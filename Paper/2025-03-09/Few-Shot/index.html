<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  Compositional Translation A Novel LLM-based Approach for Low-resource   Machine Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-74fdbea47417467763438c3c007c1516.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-09-æ›´æ–°"><a href="#2025-03-09-æ›´æ–°" class="headerlink" title="2025-03-09 æ›´æ–°"></a>2025-03-09 æ›´æ–°</h1><h2 id="Compositional-Translation-A-Novel-LLM-based-Approach-for-Low-resource-Machine-Translation"><a href="#Compositional-Translation-A-Novel-LLM-based-Approach-for-Low-resource-Machine-Translation" class="headerlink" title="Compositional Translation: A Novel LLM-based Approach for Low-resource   Machine Translation"></a>Compositional Translation: A Novel LLM-based Approach for Low-resource   Machine Translation</h2><p><strong>Authors:Armel Zebaze, BenoÃ®t Sagot, Rachel Bawden</strong></p>
<p>The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks. Machine Translation (MT) has been shown to benefit from in-context examples, in particular when they are semantically similar to the sentence to translate. In this paper, we propose a new LLM-based translation paradigm, compositional translation, to replace naive few-shot MT with similarity-based demonstrations. An LLM is used to decompose a sentence into simpler phrases, and then to translate each phrase with the help of retrieved demonstrations. Finally, the LLM is prompted to translate the initial sentence with the help of the self-generated phrase-translation pairs. Our intuition is that this approach should improve translation because these shorter phrases should be intrinsically easier to translate and easier to match with relevant examples. This is especially beneficial in low-resource scenarios, and more generally whenever the selection pool is small or out of domain. We show that compositional translation boosts LLM translation performance on a wide range of popular MT benchmarks, including FLORES 200, NTREX 128 and TICO-19. Code and outputs are available at <a target="_blank" rel="noopener" href="https://github.com/ArmelRandy/compositional-translation">https://github.com/ArmelRandy/compositional-translation</a> </p>
<blockquote>
<p>å¤§å‹ç”Ÿæˆå¼è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„èƒ½åŠ›å¼•å‘äº†å¤§é‡å…³äºå¦‚ä½•æœ€å¥½åœ°æç¤ºæ¨¡å‹è¿›è¡Œå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„ç ”ç©¶ã€‚æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ˜¾ç¤ºä»ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å—ç›Šï¼Œç‰¹åˆ«æ˜¯å½“å®ƒä»¬ä¸å¾…ç¿»è¯‘çš„å¥å­è¯­ä¹‰ç›¸ä¼¼æ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ–°ç¿»è¯‘èŒƒå¼â€”â€”ç»„åˆç¿»è¯‘ï¼Œä»¥ç”¨åŸºäºç›¸ä¼¼åº¦çš„æ¼”ç¤ºæ¥æ›¿ä»£ç®€å•çš„å‡ æ¬¡æœºå™¨ç¿»è¯‘ã€‚LLMè¢«ç”¨æ¥å°†å¥å­åˆ†è§£æˆæ›´ç®€å•çš„çŸ­è¯­ï¼Œç„¶åå€ŸåŠ©æ£€ç´¢åˆ°çš„æ¼”ç¤ºæ¥ç¿»è¯‘æ¯ä¸ªçŸ­è¯­ã€‚æœ€åï¼Œåœ¨ç”Ÿæˆçš„çŸ­è¯­ç¿»è¯‘å¯¹çš„å¸®åŠ©ä¸‹ï¼ŒLLMè¢«æç¤ºç¿»è¯‘åŸå§‹å¥å­ã€‚æˆ‘ä»¬çš„ç›´è§‰æ˜¯ï¼Œè¿™ç§æ–¹æ³•åº”è¯¥èƒ½å¤Ÿæ”¹è¿›ç¿»è¯‘ï¼Œå› ä¸ºè¿™äº›è¾ƒçŸ­çš„çŸ­è¯­æœ¬è´¨ä¸Šæ›´å®¹æ˜“ç¿»è¯‘å’Œä¸ç›¸å…³çš„ä¾‹å­ç›¸åŒ¹é…ã€‚è¿™åœ¨ä½èµ„æºåœºæ™¯ä¸­å°¤å…¶æœ‰ç›Šï¼Œè€Œä¸”åœ¨é€‰æ‹©æ± è¾ƒå°æˆ–è¶…å‡ºèŒƒå›´çš„æƒ…å†µä¸‹ä¹Ÿæ›´ä¸ºæ™®éã€‚æˆ‘ä»¬è¯æ˜äº†ç»„åˆç¿»è¯‘æé«˜äº†LLMåœ¨åŒ…æ‹¬FLORES 200ã€NTREX 128å’ŒTICO-19ç­‰å¤šä¸ªæµè¡Œçš„æœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸Šçš„ç¿»è¯‘æ€§èƒ½ã€‚ä»£ç å’Œè¾“å‡ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ArmelRandy/compositional-translation%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ArmelRandy/compositional-translationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå·²å¼•å‘å¤§é‡ç ”ç©¶å¦‚ä½•æ›´æœ‰æ•ˆåœ°å¯¹å„ç±»è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡è¿›è¡Œæ¨¡å‹æç¤ºã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹åŸºäºLLMçš„ç¿»è¯‘æ¨¡å¼â€”â€”ç»„åˆç¿»è¯‘ï¼Œæ—¨åœ¨å–ä»£åŸºäºç›¸ä¼¼åº¦æ¼”ç¤ºçš„ç®€å•å°æ ·æœ¬æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ã€‚è¯¥æ¨¡å¼åˆ©ç”¨LLMå°†å¥å­åˆ†è§£æˆæ›´ç®€å•çš„çŸ­è¯­ï¼Œå¹¶å€ŸåŠ©æ£€ç´¢åˆ°çš„æ¼”ç¤ºæ¥ç¿»è¯‘æ¯ä¸ªçŸ­è¯­ã€‚æœ€åï¼Œå€ŸåŠ©ç”Ÿæˆçš„çŸ­è¯­ç¿»è¯‘å¯¹ï¼ŒLLMè¢«æç¤ºç¿»è¯‘åŸå§‹å¥å­ã€‚æˆ‘ä»¬çš„ç›´è§‰æ˜¯ï¼Œè¿™ç§æ–¹æ³•èƒ½æé«˜ç¿»è¯‘è´¨é‡ï¼Œå› ä¸ºè¿™äº›è¾ƒçŸ­çš„çŸ­è¯­æœ¬è´¨ä¸Šæ›´å®¹æ˜“ç¿»è¯‘å’ŒåŒ¹é…ç›¸å…³ç¤ºä¾‹ã€‚è¿™åœ¨ä½èµ„æºåœºæ™¯ä»¥åŠé€‰æ‹©æ± è¾ƒå°æˆ–è¶…å‡ºèŒƒå›´çš„æƒ…å†µä¸‹å°¤ä¸ºæœ‰ç›Šã€‚æˆ‘ä»¬åœ¨å¹¿æ³›çš„æµè¡Œæœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†ç»„åˆç¿»è¯‘å¯¹LLMç¿»è¯‘æ€§èƒ½çš„æå‡ï¼ŒåŒ…æ‹¬FLORES 200ã€NTREX 128å’ŒTICO-19ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç¯å¢ƒå®Œæˆå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚</li>
<li>ç»„åˆç¿»è¯‘æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºLLMçš„ç¿»è¯‘æ¨¡å¼ï¼Œæ—¨åœ¨æ”¹è¿›å°æ ·æœ¬æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ã€‚</li>
<li>ç»„åˆç¿»è¯‘é€šè¿‡åˆ†è§£å¥å­ä¸ºç®€å•çŸ­è¯­ï¼Œå¹¶å€ŸåŠ©æ£€ç´¢åˆ°çš„æ¼”ç¤ºè¿›è¡Œç¿»è¯‘æ¥æé«˜ç¿»è¯‘è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºä½èµ„æºåœºæ™¯ï¼Œä»¥åŠé€‰æ‹©æ± è¾ƒå°æˆ–è¶…å‡ºèŒƒå›´çš„æƒ…å†µã€‚</li>
<li>ç»„åˆç¿»è¯‘åœ¨å¤šä¸ªæµè¡Œçš„æœºå™¨ç¿»è¯‘åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¯¹LLMç¿»è¯‘æ€§èƒ½çš„æå‡ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å’Œè¾“å‡ºå¯åœ¨æŒ‡å®šçš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœºå™¨ç¿»è¯‘é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2865f460a2326e03b56abf2fbee6c06b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dfebab81cf821a314847288641f3c46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83f0d0af5701b57fea0de9217c935a8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba6d7506c4b094a191daf48c78c2621f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878e52c24f2c1d182b8a2bb765aabd39.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Knowledge-Decoupled-Synergetic-Learning-An-MLLM-based-Collaborative-Approach-to-Few-shot-Multimodal-Dialogue-Intention-Recognition"><a href="#Knowledge-Decoupled-Synergetic-Learning-An-MLLM-based-Collaborative-Approach-to-Few-shot-Multimodal-Dialogue-Intention-Recognition" class="headerlink" title="Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative   Approach to Few-shot Multimodal Dialogue Intention Recognition"></a>Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative   Approach to Few-shot Multimodal Dialogue Intention Recognition</h2><p><strong>Authors:Bin Chen, Yu Zhang, Hongfei Ye, Ziyi Huang, Hongyang Chen</strong></p>
<p>Few-shot multimodal dialogue intention recognition is a critical challenge in the e-commerce domainn. Previous methods have primarily enhanced model classification capabilities through post-training techniques. However, our analysis reveals that training for few-shot multimodal dialogue intention recognition involves two interconnected tasks, leading to a seesaw effect in multi-task learning. This phenomenon is attributed to knowledge interference stemming from the superposition of weight matrix updates during the training process. To address these challenges, we propose Knowledge-Decoupled Synergetic Learning (KDSL), which mitigates these issues by utilizing smaller models to transform knowledge into interpretable rules, while applying the post-training of larger models. By facilitating collaboration between the large and small multimodal large language models for prediction, our approach demonstrates significant improvements. Notably, we achieve outstanding results on two real Taobao datasets, with enhancements of 6.37% and 6.28% in online weighted F1 scores compared to the state-of-the-art method, thereby validating the efficacy of our framework. </p>
<blockquote>
<p>åœ¨ç”µå­å•†åŠ¡é¢†åŸŸï¼Œå°æ ·æœ¬å¤šæ¨¡æ€å¯¹è¯æ„å›¾è¯†åˆ«æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦é€šè¿‡åè®­ç»ƒæŠ€æœ¯å¢å¼ºæ¨¡å‹çš„åˆ†ç±»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè®­ç»ƒç”¨äºå°æ ·æœ¬å¤šæ¨¡æ€å¯¹è¯æ„å›¾è¯†åˆ«çš„è¿‡ç¨‹æ¶‰åŠä¸¤ä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ï¼Œå¯¼è‡´å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ‘‡æ‘†æ•ˆåº”ã€‚è¿™ç§ç°è±¡å½’å› äºè®­ç»ƒè¿‡ç¨‹ä¸­æƒé‡çŸ©é˜µæ›´æ–°å åŠ äº§ç”Ÿçš„çŸ¥è¯†å¹²æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†çŸ¥è¯†è§£è€¦ååŒå­¦ä¹ ï¼ˆKDSLï¼‰ï¼Œé€šè¿‡åˆ©ç”¨è¾ƒå°çš„æ¨¡å‹å°†çŸ¥è¯†è½¬åŒ–ä¸ºå¯è§£é‡Šçš„è§„åˆ™ï¼ŒåŒæ—¶åº”ç”¨è¾ƒå¤§æ¨¡å‹çš„åæœŸè®­ç»ƒæ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚é€šè¿‡ä¿ƒè¿›å¤§å°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„é¢„æµ‹åä½œï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªçœŸå®çš„æ·˜å®æ•°æ®é›†ä¸Šå–å¾—äº†å‡ºè‰²çš„ç»“æœï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨çº¿åŠ æƒF1åˆ†æ•°æé«˜äº†6.37%å’Œ6.28%ï¼Œä»è€ŒéªŒè¯äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04201v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ç”µå­å•†åŠ¡é¢†åŸŸä¸­ï¼Œå°‘æ ·æœ¬å¤šæ¨¡æ€å¯¹è¯æ„å›¾è¯†åˆ«æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚ä»¥å¾€çš„æ–¹æ³•ä¸»è¦é€šè¿‡åè®­ç»ƒæŠ€æœ¯å¢å¼ºæ¨¡å‹åˆ†ç±»èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ¶‰åŠä¸¤ä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ï¼Œå¯¼è‡´å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„è··è··æ¿æ•ˆåº”ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºçŸ¥è¯†è§£è€¦ååŒå­¦ä¹ ï¼ˆKDSLï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å°å‹æ¨¡å‹å°†çŸ¥è¯†è½¬åŒ–ä¸ºå¯è§£é‡Šçš„è§„åˆ™ï¼ŒåŒæ—¶åº”ç”¨å¤§å‹æ¨¡å‹çš„åè®­ç»ƒï¼Œä¿ƒè¿›å¤§å‹å’Œå°å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„é¢„æµ‹åä½œï¼Œå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚åœ¨çœŸå®æ·˜å®æ•°æ®é›†ä¸Šï¼Œä¸ç°æœ‰æœ€ä½³æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨åŠ æƒF1åˆ†æ•°ä¸Šåˆ†åˆ«æé«˜äº†6.37%å’Œ6.28%ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å¤šæ¨¡æ€å¯¹è¯æ„å›¾è¯†åˆ«æ˜¯ç”µå­å•†åŠ¡é¢†åŸŸçš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡åè®­ç»ƒå¢å¼ºæ¨¡å‹åˆ†ç±»èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­æ¶‰åŠä¸¤ä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ï¼Œå¯¼è‡´å¤šä»»åŠ¡å­¦ä¹ çš„è··è··æ¿æ•ˆåº”ã€‚</li>
<li>çŸ¥è¯†è§£è€¦ååŒå­¦ä¹ ï¼ˆKDSLï¼‰æ–¹æ³•è¢«æå‡ºä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>KDSLåˆ©ç”¨å°å‹æ¨¡å‹è½¬åŒ–çŸ¥è¯†ä¸ºå¯è§£é‡Šçš„è§„åˆ™ã€‚</li>
<li>KDSLç»“åˆå¤§å‹æ¨¡å‹çš„åè®­ç»ƒï¼Œä¿ƒè¿›æ¨¡å‹é¢„æµ‹åä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04201">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74fdbea47417467763438c3c007c1516.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319095e4a9389eca80836e59d5be8719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0df7b17b82a9d1f52117b4008d3b8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a0a4aae3acba6051851b7e51c080b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DSV-LFS-Unifying-LLM-Driven-Semantic-Cues-with-Visual-Features-for-Robust-Few-Shot-Segmentation"><a href="#DSV-LFS-Unifying-LLM-Driven-Semantic-Cues-with-Visual-Features-for-Robust-Few-Shot-Segmentation" class="headerlink" title="DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for   Robust Few-Shot Segmentation"></a>DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for   Robust Few-Shot Segmentation</h2><p><strong>Authors:Amin Karimi, Charalambos Poullis</strong></p>
<p>Few-shot semantic segmentation (FSS) aims to enable models to segment novel&#x2F;unseen object classes using only a limited number of labeled examples. However, current FSS methods frequently struggle with generalization due to incomplete and biased feature representations, especially when support images do not capture the full appearance variability of the target class. To improve the FSS pipeline, we propose a novel framework that utilizes large language models (LLMs) to adapt general class semantic information to the query image. Furthermore, the framework employs dense pixel-wise matching to identify similarities between query and support images, resulting in enhanced FSS performance. Inspired by reasoning-based segmentation frameworks, our method, named DSV-LFS, introduces an additional token into the LLM vocabulary, allowing a multimodal LLM to generate a â€œsemantic promptâ€ from class descriptions. In parallel, a dense matching module identifies visual similarities between the query and support images, generating a â€œvisual promptâ€. These prompts are then jointly employed to guide the prompt-based decoder for accurate segmentation of the query image. Comprehensive experiments on the benchmark datasets Pascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves state-of-the-art performance-by a significant margin-demonstrating superior generalization to novel classes and robustness across diverse scenarios. The source code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/aminpdik/DSV-LFS%7D%7Bhttps://github.com/aminpdik/DSV-LFS%7D">https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}</a> </p>
<blockquote>
<p>å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿä»…ä½¿ç”¨æœ‰é™æ•°é‡çš„æ ‡è®°ç¤ºä¾‹æ¥åˆ†å‰²æ–°çš„&#x2F;æœªè§è¿‡çš„å¯¹è±¡ç±»åˆ«ã€‚ç„¶è€Œï¼Œç”±äºè¡¨ç¤ºä¸å®Œæ•´å’Œç‰¹å¾åè§ï¼Œå½“å‰çš„FSSæ–¹æ³•ç»å¸¸é¢ä¸´æ³›åŒ–å›°éš¾çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å½“æ”¯æŒå›¾åƒæ²¡æœ‰æ•è·ç›®æ ‡ç±»åˆ«çš„å®Œæ•´å¤–è§‚å˜åŒ–æ—¶ã€‚ä¸ºäº†æé«˜FSSç®¡é“çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥é€‚åº”é€šç”¨ç±»è¯­ä¹‰ä¿¡æ¯åˆ°æŸ¥è¯¢å›¾åƒçš„æ–°æ¡†æ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨å¯†é›†åƒç´ çº§åŒ¹é…æ¥è¯†åˆ«æŸ¥è¯¢å›¾åƒå’Œæ”¯æŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæé«˜äº†FSSçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°åŸºäºæ¨ç†çš„åˆ†å‰²æ¡†æ¶çš„å¯å‘ï¼Œè¢«å‘½åä¸ºDSV-LFSã€‚è¯¥æ–¹æ³•åœ¨LLMè¯æ±‡è¡¨ä¸­å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„ä»¤ç‰Œï¼Œå…è®¸å¤šæ¨¡æ€LLMä»ç±»åˆ«æè¿°ç”Ÿæˆâ€œè¯­ä¹‰æç¤ºâ€ã€‚åŒæ—¶ï¼Œå¯†é›†åŒ¹é…æ¨¡å—è¯†åˆ«æŸ¥è¯¢å›¾åƒå’Œæ”¯æŒå›¾åƒä¹‹é—´çš„è§†è§‰ç›¸ä¼¼æ€§ï¼Œç”Ÿæˆâ€œè§†è§‰æç¤ºâ€ã€‚ç„¶åï¼Œè¿™äº›æç¤ºè¢«å…±åŒç”¨æ¥å¼•å¯¼åŸºäºæç¤ºçš„è§£ç å™¨ï¼Œå¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œå‡†ç¡®çš„åˆ†å‰²ã€‚åœ¨Pascal-$5^{i}$å’ŒCOCO-$20^{i}$åŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶ä¸”åœ¨æ–°çš„ç±»åˆ«ä¸­è¡¨ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨ä¸åŒåœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aminpdik/DSV-LFS">https://github.com/aminpdik/DSV-LFS</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶DSV-LFSï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡åˆ©ç”¨é€šç”¨ç±»è¯­ä¹‰ä¿¡æ¯æ¥æ”¹è¿›å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥è¯­ä¹‰æç¤ºå’Œè§†è§‰æç¤ºï¼Œç»“åˆå¯†é›†åƒç´ åŒ¹é…æ¨¡å—ï¼ŒDSV-LFSæ¡†æ¶æé«˜äº†å¯¹æ–°å‹ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›å’Œåœ¨å¤šç§åœºæ™¯ä¸‹çš„ç¨³å¥æ€§ï¼Œå®ç°äº†åœ¨Pascal-$5^{i}$å’ŒCOCO-$20^{i}$ç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ¡†æ¶DSV-LFSã€‚</li>
<li>DSV-LFSæ¡†æ¶åˆ©ç”¨é€šç”¨ç±»è¯­ä¹‰ä¿¡æ¯é€‚åº”æŸ¥è¯¢å›¾åƒï¼Œä»¥æé«˜å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯†é›†åƒç´ åŒ¹é…æ¨¡å—è¯†åˆ«æŸ¥è¯¢å›¾åƒå’Œæ”¯æŒå›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰æç¤ºå’Œè§†è§‰æç¤ºï¼Œå¢å¼ºäº†æ¡†æ¶å¯¹æ–°å‹ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DSV-LFSåœ¨å¤šç§åœºæ™¯ä¸‹çš„ç¨³å¥æ€§å¾—åˆ°äº†æé«˜ã€‚</li>
<li>åœ¨Pascal-$5^{i}$å’ŒCOCO-$20^{i}$ç­‰åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-55d3527522cd7c1f9aa9d6b2e802e467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81e3fb45d20f9e4ebe9f4022ffc636ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f23573eb586dceefce36ed3d2a2d7f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LensDFF-Language-enhanced-Sparse-Feature-Distillation-for-Efficient-Few-Shot-Dexterous-Manipulation"><a href="#LensDFF-Language-enhanced-Sparse-Feature-Distillation-for-Efficient-Few-Shot-Dexterous-Manipulation" class="headerlink" title="LensDFF: Language-enhanced Sparse Feature Distillation for Efficient   Few-Shot Dexterous Manipulation"></a>LensDFF: Language-enhanced Sparse Feature Distillation for Efficient   Few-Shot Dexterous Manipulation</h2><p><strong>Authors:Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll</strong></p>
<p>Learning dexterous manipulation from few-shot demonstrations is a significant yet challenging problem for advanced, human-like robotic systems. Dense distilled feature fields have addressed this challenge by distilling rich semantic features from 2D visual foundation models into the 3D domain. However, their reliance on neural rendering models such as Neural Radiance Fields (NeRF) or Gaussian Splatting results in high computational costs. In contrast, previous approaches based on sparse feature fields either suffer from inefficiencies due to multi-view dependencies and extensive training or lack sufficient grasp dexterity. To overcome these limitations, we propose Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently distills view-consistent 2D features onto 3D points using our novel language-enhanced feature fusion strategy, thereby enabling single-view few-shot generalization. Based on LensDFF, we further introduce a few-shot dexterous manipulation framework that integrates grasp primitives into the demonstrations to generate stable and highly dexterous grasps. Moreover, we present a real2sim grasp evaluation pipeline for efficient grasp assessment and hyperparameter tuning. Through extensive simulation experiments based on the real2sim pipeline and real-world experiments, our approach achieves competitive grasping performance, outperforming state-of-the-art approaches. </p>
<blockquote>
<p>ä»å°‘é‡æ¼”ç¤ºä¸­å­¦ä¹ çµå·§æ“ä½œå¯¹äºå…ˆè¿›çš„äººå½¢æœºå™¨äººç³»ç»Ÿæ¥è¯´æ˜¯ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å¯†é›†è’¸é¦ç‰¹å¾åœºé€šè¿‡å°†ä»äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æç‚¼çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾è’¸é¦åˆ°ä¸‰ç»´é¢†åŸŸæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹ç¥ç»æ¸²æŸ“æ¨¡å‹çš„ä¾èµ–ï¼ˆä¾‹å¦‚ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆ–é«˜æ–¯å–·å°„ï¼‰å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŸºäºç¨€ç–ç‰¹å¾åœºçš„æ—©æœŸæ–¹æ³•è¦ä¹ˆå› å¤šè§†å›¾ä¾èµ–å’Œå¤§é‡è®­ç»ƒè€Œå¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œè¦ä¹ˆç¼ºä¹è¶³å¤Ÿçš„æŠ“æ¡çµå·§æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€å¢å¼ºç¨€ç–è’¸é¦ç‰¹å¾åœºï¼ˆLensDFFï¼‰ï¼Œå®ƒé€šè¿‡æˆ‘ä»¬æ–°é¢–çš„è¯­è¨€å¢å¼ºç‰¹å¾èåˆç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†è§†å›¾ä¸€è‡´çš„äºŒç»´ç‰¹å¾è’¸é¦åˆ°ä¸‰ç»´ç‚¹ä¸Šï¼Œä»è€Œå®ç°å•è§†å›¾å°æ ·æœ¬æ³›åŒ–ã€‚åŸºäºLensDFFï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§å°æ ·æœ¬çµå·§æ“ä½œæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æŠ“å–åŸè¯­é›†æˆåˆ°æ¼”ç¤ºä¸­ï¼Œä»¥ç”Ÿæˆç¨³å®šå’Œé«˜åº¦çµå·§çš„æŠ“å–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºé«˜æ•ˆæŠ“å–è¯„ä¼°å’Œè¶…å‚æ•°è°ƒæ•´æä¾›äº†ä¸€ä¸ªreal2simæŠ“å–è¯„ä¼°æµç¨‹ã€‚é€šè¿‡åŸºäºreal2simæµç¨‹å’ŒçœŸå®ä¸–ç•Œå®éªŒçš„å¹¿æ³›ä»¿çœŸå®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æŠ“å–æ€§èƒ½ï¼Œä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03890v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°‘é‡æ¼”ç¤ºæ•°æ®å®ç°é«˜çº§äººå½¢æœºå™¨äººçš„çµå·§æ“æ§æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºè¯­è¨€å¢å¼ºç¨€ç–è’¸é¦ç‰¹å¾åœºï¼ˆLensDFFï¼‰æŠ€æœ¯ï¼Œå®ƒé€šè¿‡æ–°é¢–çš„è·¨è§†è§’è¯­è¨€å¢å¼ºç‰¹å¾èåˆç­–ç•¥å®ç°äºŒç»´ç‰¹å¾çš„ç²¾å‡†æå–å¹¶è½¬è¯‘ä¸ºä¸‰ç»´æ¨¡å‹ä¸­çš„é«˜æ•ˆå§¿æ€æ“ä½œã€‚è¯¥æŠ€æœ¯ç»“åˆäº†æ¼”ç¤ºä¸­çš„æŠ“å–åŠ¨ä½œåŸå§‹ä¿¡æ¯ï¼Œç”Ÿæˆç¨³å®šä¸”é«˜åº¦çµæ´»çš„æŠ“å–åŠ¨ä½œã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªç”¨äºé«˜æ•ˆæŠ“å–è¯„ä¼°å’Œå‚æ•°è°ƒæ•´çš„çœŸå®æ¨¡æ‹ŸæŠ“å–è¯„ä¼°æµç¨‹ã€‚é€šè¿‡æ¨¡æ‹Ÿå®éªŒå’ŒçœŸå®ä¸–ç•Œå®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå‡ºè‰²çš„æŠ“å–æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦ä¹ ä»å°‘é‡æ¼”ç¤ºä¸­è¿›è¡Œçµå·§æ“æ§å¯¹äºé«˜çº§äººå½¢æœºå™¨äººç³»ç»Ÿæ˜¯é‡è¦çš„æŒ‘æˆ˜ã€‚</li>
<li>Dense distilled feature fieldsé€šè¿‡å°†äºŒç»´è§†è§‰ç‰¹å¾æ¨¡å‹ä¸­çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾è’¸é¦åˆ°ä¸‰ç»´é¢†åŸŸæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜æˆ–æ“ä½œä¸å¤Ÿçµå·§çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”è¯­è¨€å¢å¼ºç¨€ç–è’¸é¦ç‰¹å¾åœºï¼ˆLensDFFï¼‰ï¼Œå®ƒé€šè¿‡è·¨è§†è§’çš„è¯­è¨€å¢å¼ºç‰¹å¾èåˆç­–ç•¥å°†äºŒç»´ç‰¹å¾æœ‰æ•ˆåœ°è’¸é¦åˆ°ä¸‰ç»´ç‚¹ä¸Šã€‚</li>
<li>LensDFFå®ç°äº†å•è§†è§’çš„å°‘é‡ç¤ºèŒƒæ¨å¹¿èƒ½åŠ›ã€‚</li>
<li>åŸºäºLensDFFï¼Œç ”ç©¶å¼•å…¥äº†åŒ…å«æŠ“å–åŠ¨ä½œåŸå§‹ä¿¡æ¯çš„çµå·§æ“æ§æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆç¨³å®šå’Œé«˜åº¦çµæ´»çš„æŠ“å–åŠ¨ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03890">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-040facc5b1f5702b756d51daa8798234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db072406ec262f6126ce6c921abbe9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f211079570c0893c8b355448cd07d64c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a611241f4556b9783db3e0d2f4e3357.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a8a7d3b935a08bdf57e06e7c77984a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7cfef9d2e5b58bedf9d4efdeb03aa5a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model"><a href="#Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model" class="headerlink" title="Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model"></a>Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model</h2><p><strong>Authors:Steve Andreas Immanuel, Woojin Cho, Junhyuk Heo, Darongsae Kwon</strong></p>
<p>Limited data is a common problem in remote sensing due to the high cost of obtaining annotated samples. In the few-shot segmentation task, models are typically trained on base classes with abundant annotations and later adapted to novel classes with limited examples. However, this often necessitates specialized model architectures or complex training strategies. Instead, we propose a simple approach that leverages diffusion models to generate diverse variations of novel-class objects within a given scene, conditioned by the limited examples of the novel classes. By framing the problem as an image inpainting task, we synthesize plausible instances of novel classes under various environments, effectively increasing the number of samples for the novel classes and mitigating overfitting. The generated samples are then assessed using a cosine similarity metric to ensure semantic consistency with the novel classes. Additionally, we employ Segment Anything Model (SAM) to segment the generated samples and obtain precise annotations. By using high-quality synthetic data, we can directly fine-tune off-the-shelf segmentation models. Experimental results demonstrate that our method significantly enhances segmentation performance in low-data regimes, highlighting its potential for real-world remote sensing applications. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿé¢†åŸŸï¼Œç”±äºè·å–æ ‡æ³¨æ ·æœ¬çš„æˆæœ¬é«˜æ˜‚ï¼Œæœ‰é™çš„æ•°æ®æ˜¯ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚åœ¨å°‘é‡åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹é€šå¸¸ä¼šåœ¨å…·æœ‰ä¸°å¯Œæ³¨é‡Šçš„åŸºç¡€ç±»ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶åé€‚åº”å…·æœ‰æœ‰é™ä¾‹å­çš„æ–°ç±»ã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸éœ€è¦ä¸“é—¨çš„æ¨¡å‹æ¶æ„æˆ–å¤æ‚çš„è®­ç»ƒç­–ç•¥ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨ç»™å®šåœºæ™¯å†…ç”Ÿæˆæ–°ç±»å¯¹è±¡çš„å„ç§å˜ä½“ï¼Œä»¥æœ‰é™çš„ä¾‹å­ä¸ºæ¡ä»¶ã€‚é€šè¿‡å°†é—®é¢˜æ„é€ æˆå›¾åƒä¿®å¤ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨å„ç§ç¯å¢ƒä¸‹åˆæˆæ–°ç±»çš„åˆç†å®ä¾‹ï¼Œæœ‰æ•ˆåœ°å¢åŠ äº†æ–°ç±»çš„æ ·æœ¬æ•°é‡ï¼Œå¹¶å‡è½»äº†è¿‡æ‹Ÿåˆã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦åº¦é‡æ³•å¯¹ç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œè¯„ä¼°ï¼Œä»¥ç¡®ä¿å…¶ä¸æ–°ç±»çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰æ¥å¯¹ç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œåˆ†å‰²ï¼Œä»¥è·å¾—ç²¾ç¡®çš„æ³¨é‡Šã€‚é€šè¿‡ä½¿ç”¨é«˜è´¨é‡åˆæˆæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¾®è°ƒç°æˆçš„åˆ†å‰²æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œçªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œé¥æ„Ÿåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03785v1">PDF</a> Accepted to ICLRW 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ‰é™æ•°æ®çš„å¸¸è§è¿œç¨‹æ„Ÿåº”é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ï¼Œä»¥åœ¨ç»™å®šåœºæ™¯ä¸­ç”Ÿæˆå¤šæ ·åŒ–ã€å—é™å®ä¾‹æ¡ä»¶ä¸‹æ–°é¢–ç±»å¯¹è±¡çš„ä¸åŒå˜ä½“ã€‚é€šè¿‡å°†é—®é¢˜æ¡†æ¶åŒ–ä¸ºå›¾åƒä¿®å¤ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ç¯å¢ƒä¸‹åˆæˆåˆç†çš„æ–°é¢–ç±»å®ä¾‹ï¼Œæœ‰æ•ˆå¢åŠ äº†æ ·æœ¬æ•°é‡å¹¶å‡è½»äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§åº¦é‡è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬ä»¥ç¡®ä¿ä¸æ–°é¢–ç±»çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½æ•°æ®çŠ¶æ€ä¸‹æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼Œå…·æœ‰ç°å®é¥æ„Ÿåº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹é¥æ„Ÿä¸­çš„æœ‰é™æ•°æ®é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ–¹æ³•ç”Ÿæˆå¤šæ ·åŒ–ã€åŸºäºå®ä¾‹æ¡ä»¶çš„æ–°é¢–ç±»å¯¹è±¡å˜ä½“ã€‚</li>
<li>å°†é—®é¢˜æ¡†æ¶åŒ–ä¸ºå›¾åƒä¿®å¤ä»»åŠ¡ï¼Œåˆæˆåˆç†çš„æ–°é¢–ç±»å®ä¾‹ã€‚</li>
<li>é€šè¿‡å¢åŠ æ ·æœ¬æ•°é‡å‡è½»è¿‡æ‹Ÿåˆç°è±¡ã€‚</li>
<li>ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§åº¦é‡è¯„ä¼°ç”Ÿæˆçš„æ ·æœ¬ä»¥ç¡®ä¿ä¸æ–°é¢–ç±»çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>ä½¿ç”¨åˆæˆçš„é«˜å“è´¨æ•°æ®ç›´æ¥å¾®è°ƒç°æˆçš„åˆ†å‰²æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6713c5cf68538c4917bcdaf08a2892d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c2f1999084763db57955f04b7650749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2bb63831237921c2d9b431240e513a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-203caff4fab9bda8d4835d4d9a088b3c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images"><a href="#MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images" class="headerlink" title="MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images"></a>MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images</h2><p><strong>Authors:Nimra Dilawar, Sara Nadeem, Javed Iqbal, Waqas Sultani, Mohsen Ali</strong></p>
<p>Existing generic unsupervised domain adaptation approaches require access to both a large labeled source dataset and a sufficient unlabeled target dataset during adaptation. However, collecting a large dataset, even if unlabeled, is a challenging and expensive endeavor, especially in medical imaging. In addition, constraints such as privacy issues can result in cases where source data is unavailable. Taking in consideration these challenges, we propose MIAdapt, an adaptive approach for Microscopic Imagery Adaptation as a solution for Source-free Few-shot Domain Adaptive Object detection (SF-FSDA). We also define two competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot. Extensive experiments on the challenging M5-Malaria and Raabin-WBC datasets validate the effectiveness of MIAdapt. Without using any image from the source domain MIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3% mAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on Raabin-WBC. Our code and models will be publicly available. </p>
<blockquote>
<p>ç°æœ‰çš„é€šç”¨æ— ç›‘ç£åŸŸè‡ªé€‚åº”æ–¹æ³•è¦æ±‚åœ¨é€‚åº”è¿‡ç¨‹ä¸­è®¿é—®å¤§é‡æœ‰æ ‡ç­¾çš„æºæ•°æ®é›†å’Œè¶³å¤Ÿçš„æ— æ ‡ç­¾ç›®æ ‡æ•°æ®é›†ã€‚ç„¶è€Œï¼Œæ”¶é›†å¤§é‡æ•°æ®é›†ï¼Œå³ä½¿æ˜¯æœªæ ‡è®°çš„ï¼Œä¹Ÿæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ˜‚è´µçš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å½±åƒé¢†åŸŸã€‚æ­¤å¤–ï¼Œéšç§ç­‰é—®é¢˜å¯èƒ½å¯¼è‡´æºæ•°æ®æ— æ³•ä½¿ç”¨ã€‚è€ƒè™‘åˆ°è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MIAdaptï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ˜¾å¾®é•œå›¾åƒé€‚åº”çš„è‡ªé€‚åº”æ–¹æ³•ï¼Œä½œä¸ºæºè‡ªç”±å°‘åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆSF-FSDAï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸¤ä¸ªæœ‰ç«äº‰åŠ›çš„åŸºçº¿æ¨¡å‹ï¼šï¼ˆ1ï¼‰Faster-FreeShotå’Œï¼ˆ2ï¼‰MT-FreeShotã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„M5ç–Ÿç–¾å’ŒRaabin-WBCæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒéªŒè¯äº†MIAdaptçš„æœ‰æ•ˆæ€§ã€‚MIAdaptåœ¨ä¸ä½¿ç”¨æºåŸŸä¸­çš„ä»»ä½•å›¾åƒçš„æƒ…å†µä¸‹ï¼Œåœ¨Raabin-WBCæ•°æ®é›†ä¸Šçš„å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰è¶…è¿‡äº†æœ€æ–°çš„æºè‡ªç”±UDAï¼ˆSF-UDAï¼‰æ–¹æ³•+21.3%ï¼Œå¹¶è¶…è¿‡äº†å°‘åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆFSDAï¼‰çš„æ–¹æ³•+4.7%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03370v2">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MIAdaptæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ˜¾å¾®å›¾åƒè‡ªé€‚åº”çš„æºè‡ªç”±å°‘é•œå¤´åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹ï¼ˆSF-FSDAï¼‰çš„é€‚åº”æ€§æ–¹æ³•ã€‚ç”±äºæ”¶é›†å¤§é‡æ•°æ®é›†ï¼ˆå³ä½¿æ˜¯æ— æ ‡ç­¾çš„ï¼‰å…·æœ‰æŒ‘æˆ˜æ€§å’Œæ˜‚è´µæ€§ï¼Œå°¤å…¶æ˜¯åŒ»å­¦æˆåƒé¢†åŸŸï¼Œå› æ­¤è¯¥æ–¹æ³•è€ƒè™‘äº†æ— éœ€æºæ•°æ®çš„æŒ‘æˆ˜ã€‚é€šè¿‡å®šä¹‰ä¸¤æ¡ç«äº‰æ€§åŸºçº¿ï¼ˆFaster-FreeShotå’ŒMT-FreeShotï¼‰ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„M5-Malariaå’ŒRaabin-WBCæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†MIAdaptçš„æœ‰æ•ˆæ€§ã€‚åœ¨æºåŸŸå›¾åƒæœªä½¿ç”¨çš„æƒ…å†µä¸‹ï¼ŒMIAdaptçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„æºè‡ªç”±UDAï¼ˆSF-UDAï¼‰æ–¹æ³•å’Œå°‘é•œå¤´åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å¯ç”¨ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIAdaptæ˜¯ä¸€ç§é’ˆå¯¹æ˜¾å¾®å›¾åƒè‡ªé€‚åº”çš„æºè‡ªç”±å°‘é•œå¤´åŸŸè‡ªé€‚åº”ç›®æ ‡æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒMIAdaptä¸éœ€è¦ä½¿ç”¨å¤§é‡æ ‡æ³¨çš„æºæ•°æ®é›†æˆ–æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®é›†è¿›è¡Œé€‚é…ã€‚</li>
<li>ç”±äºéšç§ç­‰é—®é¢˜ï¼Œå¯èƒ½ä¼šå‡ºç°æ— æ³•è·å¾—æºæ•°æ®çš„æƒ…å†µï¼ŒMIAdaptè€ƒè™‘åˆ°äº†è¿™ä¸€ç‚¹ã€‚</li>
<li>åœ¨M5-Malariaå’ŒRaabin-WBCæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†MIAdaptçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MIAdaptçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æºè‡ªç”±UDAæ–¹æ³•å’Œå°‘é•œå¤´åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨ä¸ä½¿ç”¨æºåŸŸå›¾åƒçš„æƒ…å†µä¸‹è¶…è¿‡äº†è¿™äº›æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å¯ç”¨ï¼Œä¾¿äºå…¶ä»–äººè¿›è¡Œç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03370">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf50c1293384b7b9b24831761f296a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15bae6af356674b32e943cf399db7933.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c11912e3ba825a78e7f84a4e48e2c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58276572fd7ef9418bb2a247dae84426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e881fce8314b446a3cb04432c63a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b477433883d23a14325a3ee3fd471e9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f5f5852f7e12fab744a57b670e0630d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition (FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the high computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework (Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in two parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. </p>
<blockquote>
<p>åœ¨å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰ä¸­ï¼Œè§†é¢‘çš„é•¿å­åºåˆ—æ›´è‡ªç„¶åœ°è¡¨è¾¾äº†æ•´ä¸ªåŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸»æµåŸºäºTransformerçš„æ–¹æ³•çš„é«˜è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å…¶åº”ç”¨ã€‚æœ€è¿‘çš„Mambaåœ¨å»ºæ¨¡é•¿åºåˆ—æ–¹é¢å±•ç¤ºäº†æ•ˆç‡ï¼Œä½†ç›´æ¥å°†Mambaåº”ç”¨äºFSARå¿½ç•¥äº†å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼ŒåŒä¸€ç±»åˆ«å†…çš„é•¿å­åºåˆ—ä¼šç´¯ç§¯ç±»å†…æ–¹å·®ï¼Œè¿™å¯¹FSARæ€§èƒ½äº§ç”Ÿä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Matryoshka Mambaå’Œå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼ˆMantaï¼‰ã€‚é¦–å…ˆï¼ŒMatryoshka Mambaå¼•å…¥äº†å¤šä¸ªå†…éƒ¨æ¨¡å—æ¥å¢å¼ºå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œè€Œä¸æ˜¯ç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚å¤–éƒ¨æ¨¡å—æ•è·è¿™äº›å±€éƒ¨ç‰¹å¾ä¹‹é—´æ—¶é—´çº¿çš„ä¾èµ–æ€§ï¼Œä»¥è¿›è¡Œéšå¼æ—¶é—´å¯¹é½ã€‚å…¶æ¬¡ï¼Œç»“åˆæœ‰ç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•çš„æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼æ—¨åœ¨å‡è½»ç±»å†…æ–¹å·®ç´¯ç§¯çš„è´Ÿé¢å½±å“ã€‚Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼åœ¨Mantaçš„ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ä¸­è¿è¡Œï¼Œå¢å¼ºäº†Mambaå¯¹é•¿å­åºåˆ—çš„FSARèƒ½åŠ›ã€‚Mantaåœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°æ€§èƒ½ã€‚å¤§é‡å®è¯ç ”ç©¶è¯æ˜ï¼ŒMantaä»å¤šä¸ªè§’åº¦æ˜¾è‘—æé«˜äº†é•¿å­åºåˆ—çš„FSARæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v5">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬åŠ¨ä½œè¯†åˆ«ï¼ˆFSARï¼‰çš„æ–°æ¡†æ¶Mantaã€‚è¯¥æ¡†æ¶è§£å†³äº†é•¿åºåˆ—åŠ¨ä½œè¯†åˆ«çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å¼•å…¥Matryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œæé«˜å±€éƒ¨ç‰¹å¾å»ºæ¨¡å’Œå¯¹é½çš„èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘ç±»å†…æ–¹å·®ç§¯ç´¯å¯¹FSARæ€§èƒ½çš„å½±å“ã€‚Mantaåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Matryoshka Mambaè¢«å¼•å…¥ä»¥å¼ºåŒ–å±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œé€šè¿‡å¤šä¸ªInner Moduleså»ºæ¨¡å±€éƒ¨ç‰¹å¾ï¼Œè€Œéç›´æ¥å»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚</li>
<li>Outer Moduleç”¨äºæ•æ‰è¿™äº›å±€éƒ¨ç‰¹å¾çš„æ—¶é—´çº¿ä¾èµ–æ€§ï¼Œå®ç°éšå¼çš„æ—¶é—´å¯¹é½ã€‚</li>
<li>æå‡ºäº†æ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œç»“åˆç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ï¼Œä»¥å‡è½»ç±»å†…æ–¹å·®ç§¯ç´¯å¸¦æ¥çš„è´Ÿé¢å½±å“ã€‚</li>
<li>Mantaæ¡†æ¶ä¸­ï¼ŒMatryoshka Mambaå’Œæ··åˆå¯¹æ¯”å­¦ä¹ èŒƒå¼åœ¨ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ä¸­è¿è¡Œï¼Œå¢å¼ºäº†Mambaå¯¹é•¿å­åºåˆ—çš„FSARæ€§èƒ½ã€‚</li>
<li>Mantaåœ¨SSv2ã€Kineticsã€UCF101å’ŒHMDB51ç­‰ä¸»æµåŸºå‡†æµ‹è¯•é›†ä¸Šå®ç°äº†æœ€æ–° state-of-the-art æ€§èƒ½ã€‚</li>
<li>å¹¿æ³›çš„å®è¯ç ”ç©¶è¯æ˜ï¼ŒMantaä»å¤šä¸ªè§’åº¦æ˜¾è‘—æé«˜äº†é•¿åºåˆ—åŠ¨ä½œçš„FSARæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cdf0592535558f81b491aa1b7521988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16c2681d6549d6322cdf09a77cba898.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7850902a9c3912646aa3a165d6fc9b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HELMET-How-to-Evaluate-Long-Context-Language-Models-Effectively-and-Thoroughly"><a href="#HELMET-How-to-Evaluate-Long-Context-Language-Models-Effectively-and-Thoroughly" class="headerlink" title="HELMET: How to Evaluate Long-Context Language Models Effectively and   Thoroughly"></a>HELMET: How to Evaluate Long-Context Language Models Effectively and   Thoroughly</h2><p><strong>Authors:Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen</strong></p>
<p>Many benchmarks exist for evaluating long-context language models (LCLMs), yet developers often rely on synthetic tasks such as needle-in-a-haystack (NIAH) or an arbitrary subset of tasks. However, it remains unclear whether these benchmarks reflect the diverse downstream applications of LCLMs, and such inconsistencies further complicate model comparison. We investigate the underlying reasons behind these practices and find that existing benchmarks often provide noisy signals due to limited coverage of applications, insufficient context lengths, unreliable metrics, and incompatibility with base models. In this work, we introduce HELMET (How to Evaluate Long-context Models Effectively and Thoroughly), a comprehensive benchmark encompassing seven diverse, application-centric categories. We also address several issues in previous benchmarks by adding controllable lengths up to 128K tokens, model-based evaluation for reliable metrics, and few-shot prompting for robustly evaluating base models. Consequently, we demonstrate that HELMET offers more reliable and consistent rankings of frontier LCLMs. Through a comprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do not reliably predict downstream performance; (2) the diverse categories in HELMET exhibit distinct trends and low correlations with each other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models significantly lag behind closed ones when tasks require full-context reasoning or following complex instructions â€“ the gap widens as length increases. Finally, we recommend using our RAG tasks for fast model development, as they are easy to run and better predict other downstream performance; ultimately, we advocate for a holistic evaluation across diverse tasks. </p>
<blockquote>
<p>é’ˆå¯¹é•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰çš„è¯„ä¼°å­˜åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ï¼Œä½†å¼€å‘äººå‘˜é€šå¸¸ä¾èµ–äºåˆæˆä»»åŠ¡ï¼Œå¦‚â€œå¤§æµ·æé’ˆâ€ï¼ˆNIAHï¼‰æˆ–ä¸€ç³»åˆ—ä»»æ„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šè¿™äº›åŸºå‡†æµ‹è¯•æ˜¯å¦åæ˜ äº†LCLMçš„å¤šæ ·åŒ–ä¸‹æ¸¸åº”ç”¨ï¼Œå¹¶ä¸”è¿™ç§ä¸ä¸€è‡´æ€§è¿›ä¸€æ­¥åŠ å‰§äº†æ¨¡å‹æ¯”è¾ƒçš„éš¾åº¦ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†è¿™äº›å®è·µèƒŒåçš„æ ¹æœ¬åŸå› ï¼Œå¹¶å‘ç°ç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸ç”±äºåº”ç”¨è¦†ç›–æœ‰é™ã€ä¸Šä¸‹æ–‡é•¿åº¦ä¸è¶³ã€æŒ‡æ ‡ä¸å¯é ä»¥åŠä¸åŸºç¡€æ¨¡å‹ä¸å…¼å®¹è€Œæä¾›å˜ˆæ‚çš„ä¿¡å·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•HELMETï¼ˆå¦‚ä½•æœ‰æ•ˆä¸”å½»åº•åœ°è¯„ä¼°é•¿è¯­å¢ƒæ¨¡å‹ï¼‰ï¼ŒåŒ…å«ä¸ƒä¸ªå¤šæ ·åŒ–ã€ä»¥åº”ç”¨ä¸ºä¸­å¿ƒçš„ç±»åˆ«ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¢åŠ å¯æ§é•¿åº¦ï¼ˆæœ€å¤šè¾¾128Kä»¤ç‰Œï¼‰ã€åŸºäºæ¨¡å‹çš„è¯„ä¼°ä»¥è·å–å¯é æŒ‡æ ‡ä»¥åŠå°‘æ ·æœ¬æç¤ºæ¥è§£å†³å…ˆå‰åŸºå‡†æµ‹è¯•ä¸­çš„å‡ ä¸ªé—®é¢˜ï¼Œä»è€Œç¨³å¥åœ°è¯„ä¼°åŸºç¡€æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯æ˜äº†HELMETä¸ºå‰æ²¿LCLMæä¾›äº†æ›´å¯é ä¸”ä¸€è‡´æ€§çš„æ’åã€‚é€šè¿‡å¯¹59ä¸ªLCLMçš„ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼šï¼ˆ1ï¼‰åƒNIAHè¿™æ ·çš„åˆæˆä»»åŠ¡æ— æ³•å¯é åœ°é¢„æµ‹ä¸‹æ¸¸æ€§èƒ½ï¼›ï¼ˆ2ï¼‰HELMETä¸­çš„ä¸åŒç±»åˆ«å‘ˆç°å‡ºä¸åŒçš„è¶‹åŠ¿ï¼Œå½¼æ­¤é—´ä½ç›¸å…³æ€§ï¼›ï¼ˆ3ï¼‰è™½ç„¶å¤§å¤šæ•°LCLMåœ¨NIAHå¾—åˆ†ä¸Šè¡¨ç°å®Œç¾ï¼Œä½†åœ¨éœ€è¦å…¨è¯­å¢ƒæ¨ç†æˆ–éµå¾ªå¤æ‚æŒ‡ä»¤çš„ä»»åŠ¡ä¸Šï¼Œå¼€æºæ¨¡å‹ä¸å°é—­æ¨¡å‹çš„æ€§èƒ½å·®è·æ˜¾è‘—æ‹‰å¤§ï¼Œä¸”éšç€é•¿åº¦çš„å¢åŠ è€Œæ‰©å¤§ã€‚æœ€åï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æˆ‘ä»¬çš„RAGä»»åŠ¡è¿›è¡Œå¿«é€Ÿæ¨¡å‹å¼€å‘ï¼Œå› ä¸ºå®ƒä»¬æ˜“äºè¿è¡Œå¹¶å¯ä»¥æ›´å¥½åœ°é¢„æµ‹å…¶ä»–ä¸‹æ¸¸æ€§èƒ½ï¼›æœ€ç»ˆï¼Œæˆ‘ä»¬ä¸»å¼ åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸Šè¿›è¡Œæ•´ä½“è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02694v3">PDF</a> ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://princeton-nlp.github.io/HELMET/">https://princeton-nlp.github.io/HELMET/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¯¹é•¿æ–‡æœ¬è¯­å¢ƒæ¨¡å‹ï¼ˆLCLMï¼‰çš„è¯„ä¼°é—®é¢˜ã€‚ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•å­˜åœ¨è¯¸å¤šä¸è¶³ï¼Œå¦‚è¦†ç›–åº”ç”¨æœ‰é™ã€è¯­å¢ƒé•¿åº¦ä¸è¶³ã€è¯„ä»·æŒ‡æ ‡ä¸å¯é ç­‰ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†HELMETåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŒ…å«ä¸ƒä¸ªåº”ç”¨ç±»åˆ«ï¼Œå¹¶è§£å†³äº†ä»¥å¾€åŸºå‡†æµ‹è¯•ä¸­çš„é—®é¢˜ï¼Œå¦‚å¯æ§çš„è¯­å¢ƒé•¿åº¦ã€åŸºäºæ¨¡å‹çš„å¯é è¯„ä»·æŒ‡æ ‡å’Œå°‘æ ·æœ¬æç¤ºç­‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒHELMETä¸ºå‰æ²¿LCLMæä¾›äº†æ›´å¯é å’Œä¸€è‡´çš„æ’åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•åœ¨åæ˜ LCLMçš„å¤šæ ·ä¸‹æ¸¸åº”ç”¨æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¼€å‘è€…å¸¸ä¾èµ–åˆæˆä»»åŠ¡å¦‚â€œæµ·åº•æé’ˆâ€ï¼Œä½†è¿™äº›ä»»åŠ¡ä¸èƒ½å¯é é¢„æµ‹ä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>HELMETåŸºå‡†æµ‹è¯•åŒ…å«ä¸ƒä¸ªåº”ç”¨ç±»åˆ«ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°LCLMã€‚</li>
<li>HELMETè§£å†³äº†ä»¥å¾€åŸºå‡†æµ‹è¯•ä¸­çš„é—®é¢˜ï¼Œå¦‚è¯­å¢ƒé•¿åº¦ã€è¯„ä»·æŒ‡æ ‡çš„å¯é æ€§ç­‰ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåˆæˆä»»åŠ¡ä¸èƒ½åæ˜ LCLMçš„çœŸå®æ€§èƒ½ï¼Œä¸åŒç±»åˆ«ä¹‹é—´çš„è¶‹åŠ¿å„å¼‚ä¸”å…³è”åº¦ä½ã€‚</li>
<li>è™½ç„¶è®¸å¤šLCLMåœ¨åˆæˆä»»åŠ¡ä¸Šè¡¨ç°å®Œç¾ï¼Œä½†åœ¨éœ€è¦å…¨è¯­å¢ƒæ¨ç†æˆ–éµå¾ªå¤æ‚æŒ‡ä»¤çš„ä»»åŠ¡ä¸Šï¼Œå¼€æºæ¨¡å‹ä¸å°é—­æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸”å·®è·éšé•¿åº¦å¢åŠ è€Œæ‰©å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02694">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3db5b03b33c60155defca8831c45cfb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d22634e517f6000c3b1c138f5cc3efc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a5ae9b24d24215601e28dc20a8504a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78811e351ba329ad37aa5cd744f27fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efeadb9a4d247682c5eef8384c1b6545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d88637e9d616c090a64af58ed11cd5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>Deep neural networks have been extensively applied in the medical domain for various tasks, including image classification, segmentation, and landmark detection. However, their application is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task, specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection, a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images), surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge, this work represents the first application of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»ç–—é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œåœ°æ ‡æ£€æµ‹ã€‚ç„¶è€Œï¼Œå…¶åº”ç”¨å¾€å¾€å—åˆ°æ•°æ®å’Œå¯ç”¨æ ‡æ³¨å›¾åƒåŒ®ä¹çš„åˆ¶çº¦ã€‚æœ¬ç ”ç©¶å¼•å…¥å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰çš„æ–°åº”ç”¨ï¼Œä¸“é—¨è§£å†³Xå°„çº¿æˆåƒä¸­æ ‡æ³¨æ•°æ®æœ‰é™å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºåˆ©ç”¨DDPMsè¿›è¡Œåœ°æ ‡æ£€æµ‹çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒï¼Œè¿™æ˜¯è¯¥é¢†åŸŸä¹‹å‰æœªæ¢ç´¢è¿‡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æå°‘çš„æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼ˆä»…50å¼ å›¾åƒï¼‰ä¸‹å®ç°å‡†ç¡®çš„åœ°æ ‡æ£€æµ‹ï¼Œè¶…è¶Šäº†ImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªæˆ‘ç›‘ç£æŠ€æœ¯åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†æ‰©æ•£æ¨¡å‹åœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä¸­çš„é¦–æ¬¡åº”ç”¨äºåœ°æ ‡æ£€æµ‹ï¼Œè¿™å¯èƒ½ä¸ºæ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹æä¾›æœ‰ä»·å€¼çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·ä¾‹æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v3">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†å°†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åº”ç”¨äºåŒ»å­¦é¢†åŸŸåœ°æ ‡æ£€æµ‹ä»»åŠ¡çš„æ–°åº”ç”¨ï¼Œè§£å†³äº†åœ¨Xå°„çº¿æˆåƒä¸­æœ‰é™æ ‡æ³¨æ•°æ®çš„æŒ‘æˆ˜ã€‚é€šè¿‡è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒDDPMsï¼Œè¯¥æ–¹æ³•åœ¨ä»…å°‘é‡ï¼ˆå¦‚50å¼ ï¼‰æ ‡æ³¨å›¾åƒçš„æƒ…å†µä¸‹å®ç°äº†å‡†ç¡®çš„åœ°æ ‡æ£€æµ‹ï¼Œè¶…è¶Šäº†ImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªæˆ‘ç›‘ç£æŠ€æœ¯ï¼Œåœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚è¿™æ˜¯é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåœ°æ ‡æ£€æµ‹ä¸­çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œä¸ºç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†ä¸€ç§æœ‰ä»·å€¼çš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å°†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åº”ç”¨äºåŒ»å­¦é¢†åŸŸçš„åœ°æ ‡æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>é¢å¯¹åŒ»å­¦å›¾åƒæ ‡æ³¨å’Œå›¾åƒæ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ï¼ŒDDPMsé€šè¿‡è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒå®ç°äº†å‡†ç¡®çš„åœ°æ ‡æ£€æµ‹ã€‚</li>
<li>åœ¨ä»…å°‘é‡æ ‡æ³¨å›¾åƒï¼ˆå¦‚50å¼ ï¼‰çš„æƒ…å†µä¸‹ï¼ŒDDPMsè¡¨ç°è¶…è¶ŠImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªæˆ‘ç›‘ç£æŠ€æœ¯ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯å¹¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒåœ°æ ‡æ£€æµ‹ä¸­çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•å¯èƒ½ä¸ºç¼“è§£åŒ»å­¦å›¾åƒæ•°æ®ç¨€ç¼ºé—®é¢˜çš„é¢„è®­ç»ƒæ–¹æ¡ˆæä¾›æ–°çš„æ€è·¯ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4fe8ec52f131a12305c13150eef9066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf8299076cafa7e4723193ef80f93d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a569e2988b76382c74c267bebcb93089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e059e518c4b2016e7c6a59a420b98309.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-49f1d842eac04cb406c067daa85a7902.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic   Membrane Vessel Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6d19e10e57fe6b34da29c78421a0ded.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  Multi-Agent Inverse Q-Learning from Demonstrations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
