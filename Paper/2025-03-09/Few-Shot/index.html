<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-09  Compositional Translation A Novel LLM-based Approach for Low-resource   Machine Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-74fdbea47417467763438c3c007c1516.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-09-更新"><a href="#2025-03-09-更新" class="headerlink" title="2025-03-09 更新"></a>2025-03-09 更新</h1><h2 id="Compositional-Translation-A-Novel-LLM-based-Approach-for-Low-resource-Machine-Translation"><a href="#Compositional-Translation-A-Novel-LLM-based-Approach-for-Low-resource-Machine-Translation" class="headerlink" title="Compositional Translation: A Novel LLM-based Approach for Low-resource   Machine Translation"></a>Compositional Translation: A Novel LLM-based Approach for Low-resource   Machine Translation</h2><p><strong>Authors:Armel Zebaze, Benoît Sagot, Rachel Bawden</strong></p>
<p>The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks. Machine Translation (MT) has been shown to benefit from in-context examples, in particular when they are semantically similar to the sentence to translate. In this paper, we propose a new LLM-based translation paradigm, compositional translation, to replace naive few-shot MT with similarity-based demonstrations. An LLM is used to decompose a sentence into simpler phrases, and then to translate each phrase with the help of retrieved demonstrations. Finally, the LLM is prompted to translate the initial sentence with the help of the self-generated phrase-translation pairs. Our intuition is that this approach should improve translation because these shorter phrases should be intrinsically easier to translate and easier to match with relevant examples. This is especially beneficial in low-resource scenarios, and more generally whenever the selection pool is small or out of domain. We show that compositional translation boosts LLM translation performance on a wide range of popular MT benchmarks, including FLORES 200, NTREX 128 and TICO-19. Code and outputs are available at <a target="_blank" rel="noopener" href="https://github.com/ArmelRandy/compositional-translation">https://github.com/ArmelRandy/compositional-translation</a> </p>
<blockquote>
<p>大型生成式语言模型（LLM）在上下文学习中的能力引发了大量关于如何最好地提示模型进行各种自然语言处理任务的研究。机器翻译（MT）显示从上下文示例中受益，特别是当它们与待翻译的句子语义相似时。在本文中，我们提出了一种基于LLM的新翻译范式——组合翻译，以用基于相似度的演示来替代简单的几次机器翻译。LLM被用来将句子分解成更简单的短语，然后借助检索到的演示来翻译每个短语。最后，在生成的短语翻译对的帮助下，LLM被提示翻译原始句子。我们的直觉是，这种方法应该能够改进翻译，因为这些较短的短语本质上更容易翻译和与相关的例子相匹配。这在低资源场景中尤其有益，而且在选择池较小或超出范围的情况下也更为普遍。我们证明了组合翻译提高了LLM在包括FLORES 200、NTREX 128和TICO-19等多个流行的机器翻译基准测试上的翻译性能。代码和输出可在<a target="_blank" rel="noopener" href="https://github.com/ArmelRandy/compositional-translation%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ArmelRandy/compositional-translation找到。</a></p>
</blockquote>
<p><strong>简化版翻译</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>生成式大型语言模型（LLM）能够在上下文学习环境中表现出卓越性能，已引发大量研究如何更有效地对各类自然语言处理任务进行模型提示。本文提出一种新型基于LLM的翻译模式——组合翻译，旨在取代基于相似度演示的简单小样本机器翻译（MT）。该模式利用LLM将句子分解成更简单的短语，并借助检索到的演示来翻译每个短语。最后，借助生成的短语翻译对，LLM被提示翻译原始句子。我们的直觉是，这种方法能提高翻译质量，因为这些较短的短语本质上更容易翻译和匹配相关示例。这在低资源场景以及选择池较小或超出范围的情况下尤为有益。我们在广泛的流行机器翻译基准测试上展示了组合翻译对LLM翻译性能的提升，包括FLORES 200、NTREX 128和TICO-19。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式大型语言模型（LLM）能够通过上下文学习环境完成多种自然语言处理任务。</li>
<li>组合翻译是一种新型的基于LLM的翻译模式，旨在改进小样本机器翻译（MT）。</li>
<li>组合翻译通过分解句子为简单短语，并借助检索到的演示进行翻译来提高翻译质量。</li>
<li>该方法特别适用于低资源场景，以及选择池较小或超出范围的情况。</li>
<li>组合翻译在多个流行的机器翻译基准测试上表现出对LLM翻译性能的提升。</li>
<li>研究的代码和输出可在指定的GitHub仓库中找到。</li>
<li>该研究为机器翻译领域提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2865f460a2326e03b56abf2fbee6c06b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dfebab81cf821a314847288641f3c46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83f0d0af5701b57fea0de9217c935a8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba6d7506c4b094a191daf48c78c2621f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878e52c24f2c1d182b8a2bb765aabd39.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Knowledge-Decoupled-Synergetic-Learning-An-MLLM-based-Collaborative-Approach-to-Few-shot-Multimodal-Dialogue-Intention-Recognition"><a href="#Knowledge-Decoupled-Synergetic-Learning-An-MLLM-based-Collaborative-Approach-to-Few-shot-Multimodal-Dialogue-Intention-Recognition" class="headerlink" title="Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative   Approach to Few-shot Multimodal Dialogue Intention Recognition"></a>Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative   Approach to Few-shot Multimodal Dialogue Intention Recognition</h2><p><strong>Authors:Bin Chen, Yu Zhang, Hongfei Ye, Ziyi Huang, Hongyang Chen</strong></p>
<p>Few-shot multimodal dialogue intention recognition is a critical challenge in the e-commerce domainn. Previous methods have primarily enhanced model classification capabilities through post-training techniques. However, our analysis reveals that training for few-shot multimodal dialogue intention recognition involves two interconnected tasks, leading to a seesaw effect in multi-task learning. This phenomenon is attributed to knowledge interference stemming from the superposition of weight matrix updates during the training process. To address these challenges, we propose Knowledge-Decoupled Synergetic Learning (KDSL), which mitigates these issues by utilizing smaller models to transform knowledge into interpretable rules, while applying the post-training of larger models. By facilitating collaboration between the large and small multimodal large language models for prediction, our approach demonstrates significant improvements. Notably, we achieve outstanding results on two real Taobao datasets, with enhancements of 6.37% and 6.28% in online weighted F1 scores compared to the state-of-the-art method, thereby validating the efficacy of our framework. </p>
<blockquote>
<p>在电子商务领域，小样本多模态对话意图识别是一项关键挑战。之前的方法主要通过后训练技术增强模型的分类能力。然而，我们的分析表明，训练用于小样本多模态对话意图识别的过程涉及两个相互关联的任务，导致多任务学习中的摇摆效应。这种现象归因于训练过程中权重矩阵更新叠加产生的知识干扰。为了解决这些挑战，我们提出了知识解耦协同学习（KDSL），通过利用较小的模型将知识转化为可解释的规则，同时应用较大模型的后期训练来缓解这些问题。通过促进大小多模态大型语言模型之间的预测协作，我们的方法取得了显著的改进。值得注意的是，我们在两个真实的淘宝数据集上取得了出色的结果，与最先进的方法相比，在线加权F1分数提高了6.37%和6.28%，从而验证了我们框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04201v1">PDF</a> </p>
<p><strong>Summary</strong><br>在电子商务领域中，少样本多模态对话意图识别是一项重要挑战。以往的方法主要通过后训练技术增强模型分类能力，但我们的分析发现，训练过程中涉及两个相互关联的任务，导致多任务学习中的跷跷板效应。针对这一问题，我们提出知识解耦协同学习（KDSL）方法，通过利用小型模型将知识转化为可解释的规则，同时应用大型模型的后训练，促进大型和小型多模态语言模型的预测协作，取得了显著改进。在真实淘宝数据集上，与现有最佳方法相比，我们的框架在加权F1分数上分别提高了6.37%和6.28%，验证了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本多模态对话意图识别是电子商务领域的核心挑战。</li>
<li>现有方法主要通过后训练增强模型分类能力。</li>
<li>训练过程中涉及两个相互关联的任务，导致多任务学习的跷跷板效应。</li>
<li>知识解耦协同学习（KDSL）方法被提出以解决这个问题。</li>
<li>KDSL利用小型模型转化知识为可解释的规则。</li>
<li>KDSL结合大型模型的后训练，促进模型预测协作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04201">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-74fdbea47417467763438c3c007c1516.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319095e4a9389eca80836e59d5be8719.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d0df7b17b82a9d1f52117b4008d3b8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a0a4aae3acba6051851b7e51c080b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DSV-LFS-Unifying-LLM-Driven-Semantic-Cues-with-Visual-Features-for-Robust-Few-Shot-Segmentation"><a href="#DSV-LFS-Unifying-LLM-Driven-Semantic-Cues-with-Visual-Features-for-Robust-Few-Shot-Segmentation" class="headerlink" title="DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for   Robust Few-Shot Segmentation"></a>DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for   Robust Few-Shot Segmentation</h2><p><strong>Authors:Amin Karimi, Charalambos Poullis</strong></p>
<p>Few-shot semantic segmentation (FSS) aims to enable models to segment novel&#x2F;unseen object classes using only a limited number of labeled examples. However, current FSS methods frequently struggle with generalization due to incomplete and biased feature representations, especially when support images do not capture the full appearance variability of the target class. To improve the FSS pipeline, we propose a novel framework that utilizes large language models (LLMs) to adapt general class semantic information to the query image. Furthermore, the framework employs dense pixel-wise matching to identify similarities between query and support images, resulting in enhanced FSS performance. Inspired by reasoning-based segmentation frameworks, our method, named DSV-LFS, introduces an additional token into the LLM vocabulary, allowing a multimodal LLM to generate a “semantic prompt” from class descriptions. In parallel, a dense matching module identifies visual similarities between the query and support images, generating a “visual prompt”. These prompts are then jointly employed to guide the prompt-based decoder for accurate segmentation of the query image. Comprehensive experiments on the benchmark datasets Pascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves state-of-the-art performance-by a significant margin-demonstrating superior generalization to novel classes and robustness across diverse scenarios. The source code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/aminpdik/DSV-LFS%7D%7Bhttps://github.com/aminpdik/DSV-LFS%7D">https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}</a> </p>
<blockquote>
<p>少样本语义分割（FSS）旨在使模型能够仅使用有限数量的标记示例来分割新的&#x2F;未见过的对象类别。然而，由于表示不完整和特征偏见，当前的FSS方法经常面临泛化困难的问题，特别是当支持图像没有捕获目标类别的完整外观变化时。为了提高FSS管道的性能，我们提出了一种利用大型语言模型（LLM）来适应通用类语义信息到查询图像的新框架。此外，该框架采用密集像素级匹配来识别查询图像和支持图像之间的相似性，从而提高了FSS的性能。我们的方法受到基于推理的分割框架的启发，被命名为DSV-LFS。该方法在LLM词汇表中引入了一个额外的令牌，允许多模态LLM从类别描述生成“语义提示”。同时，密集匹配模块识别查询图像和支持图像之间的视觉相似性，生成“视觉提示”。然后，这些提示被共同用来引导基于提示的解码器，对查询图像进行准确的分割。在Pascal-$5^{i}$和COCO-$20^{i}$基准数据集上的综合实验表明，我们的框架达到了最先进的性能水平，并且在新的类别中表现出优越的泛化能力和在不同场景中的稳健性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/aminpdik/DSV-LFS">https://github.com/aminpdik/DSV-LFS</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于大语言模型的少样本语义分割框架DSV-LFS，该框架旨在通过利用通用类语义信息来改进少样本语义分割的性能。通过引入语义提示和视觉提示，结合密集像素匹配模块，DSV-LFS框架提高了对新型类别的泛化能力和在多种场景下的稳健性，实现了在Pascal-$5^{i}$和COCO-$20^{i}$等基准数据集上的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于大语言模型的少样本语义分割框架DSV-LFS。</li>
<li>DSV-LFS框架利用通用类语义信息适应查询图像，以提高少样本语义分割的性能。</li>
<li>通过密集像素匹配模块识别查询图像和支持图像之间的相似性。</li>
<li>引入语义提示和视觉提示，增强了框架对新型类别的泛化能力。</li>
<li>DSV-LFS在多种场景下的稳健性得到了提高。</li>
<li>在Pascal-$5^{i}$和COCO-$20^{i}$等基准数据集上实现了最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-55d3527522cd7c1f9aa9d6b2e802e467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81e3fb45d20f9e4ebe9f4022ffc636ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f23573eb586dceefce36ed3d2a2d7f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LensDFF-Language-enhanced-Sparse-Feature-Distillation-for-Efficient-Few-Shot-Dexterous-Manipulation"><a href="#LensDFF-Language-enhanced-Sparse-Feature-Distillation-for-Efficient-Few-Shot-Dexterous-Manipulation" class="headerlink" title="LensDFF: Language-enhanced Sparse Feature Distillation for Efficient   Few-Shot Dexterous Manipulation"></a>LensDFF: Language-enhanced Sparse Feature Distillation for Efficient   Few-Shot Dexterous Manipulation</h2><p><strong>Authors:Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll</strong></p>
<p>Learning dexterous manipulation from few-shot demonstrations is a significant yet challenging problem for advanced, human-like robotic systems. Dense distilled feature fields have addressed this challenge by distilling rich semantic features from 2D visual foundation models into the 3D domain. However, their reliance on neural rendering models such as Neural Radiance Fields (NeRF) or Gaussian Splatting results in high computational costs. In contrast, previous approaches based on sparse feature fields either suffer from inefficiencies due to multi-view dependencies and extensive training or lack sufficient grasp dexterity. To overcome these limitations, we propose Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently distills view-consistent 2D features onto 3D points using our novel language-enhanced feature fusion strategy, thereby enabling single-view few-shot generalization. Based on LensDFF, we further introduce a few-shot dexterous manipulation framework that integrates grasp primitives into the demonstrations to generate stable and highly dexterous grasps. Moreover, we present a real2sim grasp evaluation pipeline for efficient grasp assessment and hyperparameter tuning. Through extensive simulation experiments based on the real2sim pipeline and real-world experiments, our approach achieves competitive grasping performance, outperforming state-of-the-art approaches. </p>
<blockquote>
<p>从少量演示中学习灵巧操作对于先进的人形机器人系统来说是一个重要且具有挑战性的任务。密集蒸馏特征场通过将从二维视觉基础模型中提炼的丰富语义特征蒸馏到三维领域来解决这一挑战。然而，它们对神经渲染模型的依赖（例如神经辐射场（NeRF）或高斯喷射）导致了较高的计算成本。相比之下，基于稀疏特征场的早期方法要么因多视图依赖和大量训练而导致效率低下，要么缺乏足够的抓握灵巧性。为了克服这些局限性，我们提出了语言增强稀疏蒸馏特征场（LensDFF），它通过我们新颖的语言增强特征融合策略，有效地将视图一致的二维特征蒸馏到三维点上，从而实现单视图小样本泛化。基于LensDFF，我们进一步引入了一种小样本灵巧操作框架，该框架将抓取原语集成到演示中，以生成稳定和高度灵巧的抓取。此外，我们为高效抓取评估和超参数调整提供了一个real2sim抓取评估流程。通过基于real2sim流程和真实世界实验的广泛仿真实验，我们的方法实现了具有竞争力的抓取性能，优于现有先进技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03890v1">PDF</a> 8 pages</p>
<p><strong>Summary</strong></p>
<p>基于少量演示数据实现高级人形机器人的灵巧操控是一项重大挑战。为解决这一问题，研究提出语言增强稀疏蒸馏特征场（LensDFF）技术，它通过新颖的跨视角语言增强特征融合策略实现二维特征的精准提取并转译为三维模型中的高效姿态操作。该技术结合了演示中的抓取动作原始信息，生成稳定且高度灵活的抓取动作。此外，研究还建立了一个用于高效抓取评估和参数调整的真实模拟抓取评估流程。通过模拟实验和真实世界实验验证，该方法表现出出色的抓取性能，优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学习从少量演示中进行灵巧操控对于高级人形机器人系统是重要的挑战。</li>
<li>Dense distilled feature fields通过将二维视觉特征模型中的丰富语义特征蒸馏到三维领域来解决这一挑战。</li>
<li>当前方法存在计算成本高或操作不够灵巧的问题。</li>
<li>提出了一种新的方法——语言增强稀疏蒸馏特征场（LensDFF），它通过跨视角的语言增强特征融合策略将二维特征有效地蒸馏到三维点上。</li>
<li>LensDFF实现了单视角的少量示范推广能力。</li>
<li>基于LensDFF，研究引入了包含抓取动作原始信息的灵巧操控框架，用于生成稳定和高度灵活的抓取动作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-040facc5b1f5702b756d51daa8798234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db072406ec262f6126ce6c921abbe9ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f211079570c0893c8b355448cd07d64c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a611241f4556b9783db3e0d2f4e3357.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a8a7d3b935a08bdf57e06e7c77984a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7cfef9d2e5b58bedf9d4efdeb03aa5a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model"><a href="#Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model" class="headerlink" title="Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model"></a>Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model</h2><p><strong>Authors:Steve Andreas Immanuel, Woojin Cho, Junhyuk Heo, Darongsae Kwon</strong></p>
<p>Limited data is a common problem in remote sensing due to the high cost of obtaining annotated samples. In the few-shot segmentation task, models are typically trained on base classes with abundant annotations and later adapted to novel classes with limited examples. However, this often necessitates specialized model architectures or complex training strategies. Instead, we propose a simple approach that leverages diffusion models to generate diverse variations of novel-class objects within a given scene, conditioned by the limited examples of the novel classes. By framing the problem as an image inpainting task, we synthesize plausible instances of novel classes under various environments, effectively increasing the number of samples for the novel classes and mitigating overfitting. The generated samples are then assessed using a cosine similarity metric to ensure semantic consistency with the novel classes. Additionally, we employ Segment Anything Model (SAM) to segment the generated samples and obtain precise annotations. By using high-quality synthetic data, we can directly fine-tune off-the-shelf segmentation models. Experimental results demonstrate that our method significantly enhances segmentation performance in low-data regimes, highlighting its potential for real-world remote sensing applications. </p>
<blockquote>
<p>在遥感领域，由于获取标注样本的成本高昂，有限的数据是一个常见问题。在少量分割任务中，模型通常会在具有丰富注释的基础类上进行训练，然后适应具有有限例子的新类。然而，这通常需要专门的模型架构或复杂的训练策略。相反，我们提出了一种简单的方法，利用扩散模型在给定场景内生成新类对象的各种变体，以有限的例子为条件。通过将问题构造成图像修复任务，我们在各种环境下合成新类的合理实例，有效地增加了新类的样本数量，并减轻了过拟合。然后，我们使用余弦相似度度量法对生成的样本进行评估，以确保其与新类的语义一致性。此外，我们还采用了分割任何模型（SAM）来对生成的样本进行分割，以获得精确的注释。通过使用高质量合成数据，我们可以直接微调现成的分割模型。实验结果表明，我们的方法在数据稀缺的情况下显著提高了分割性能，突显了其在现实世界遥感应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03785v1">PDF</a> Accepted to ICLRW 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>基于有限数据的常见远程感应问题，文章提出了一个利用扩散模型的方法，以在给定场景中生成多样化、受限实例条件下新颖类对象的不同变体。通过将问题框架化为图像修复任务，该方法在多种环境下合成合理的新颖类实例，有效增加了样本数量并减轻了过拟合现象。使用余弦相似性度量评估生成的样本以确保与新颖类的语义一致性。实验结果表明，该方法在低数据状态下显著提高分割性能，具有现实遥感应用的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面对遥感中的有限数据问题，提出了一个基于扩散模型的解决方案。</li>
<li>方法生成多样化、基于实例条件的新颖类对象变体。</li>
<li>将问题框架化为图像修复任务，合成合理的新颖类实例。</li>
<li>通过增加样本数量减轻过拟合现象。</li>
<li>使用余弦相似性度量评估生成的样本以确保与新颖类的语义一致性。</li>
<li>使用合成的高品质数据直接微调现成的分割模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6713c5cf68538c4917bcdaf08a2892d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c2f1999084763db57955f04b7650749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2bb63831237921c2d9b431240e513a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-203caff4fab9bda8d4835d4d9a088b3c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images"><a href="#MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images" class="headerlink" title="MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images"></a>MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images</h2><p><strong>Authors:Nimra Dilawar, Sara Nadeem, Javed Iqbal, Waqas Sultani, Mohsen Ali</strong></p>
<p>Existing generic unsupervised domain adaptation approaches require access to both a large labeled source dataset and a sufficient unlabeled target dataset during adaptation. However, collecting a large dataset, even if unlabeled, is a challenging and expensive endeavor, especially in medical imaging. In addition, constraints such as privacy issues can result in cases where source data is unavailable. Taking in consideration these challenges, we propose MIAdapt, an adaptive approach for Microscopic Imagery Adaptation as a solution for Source-free Few-shot Domain Adaptive Object detection (SF-FSDA). We also define two competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot. Extensive experiments on the challenging M5-Malaria and Raabin-WBC datasets validate the effectiveness of MIAdapt. Without using any image from the source domain MIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3% mAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on Raabin-WBC. Our code and models will be publicly available. </p>
<blockquote>
<p>现有的通用无监督域自适应方法要求在适应过程中访问大量有标签的源数据集和足够的无标签目标数据集。然而，收集大量数据集，即使是未标记的，也是一项具有挑战性和昂贵的任务，特别是在医学影像领域。此外，隐私等问题可能导致源数据无法使用。考虑到这些挑战，我们提出了MIAdapt，这是一种针对显微镜图像适应的自适应方法，作为源自由少域自适应目标检测（SF-FSDA）的解决方案。我们还定义了两个有竞争力的基线模型：（1）Faster-FreeShot和（2）MT-FreeShot。在具有挑战性的M5疟疾和Raabin-WBC数据集上进行的广泛实验验证了MIAdapt的有效性。MIAdapt在不使用源域中的任何图像的情况下，在Raabin-WBC数据集上的平均精度（mAP）超过了最新的源自由UDA（SF-UDA）方法+21.3%，并超过了少域自适应目标检测（FSDA）的方法+4.7%。我们的代码和模型将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03370v2">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了MIAdapt方法，这是一种针对显微图像自适应的源自由少镜头域自适应目标检测（SF-FSDA）的适应性方法。由于收集大量数据集（即使是无标签的）具有挑战性和昂贵性，尤其是医学成像领域，因此该方法考虑了无需源数据的挑战。通过定义两条竞争性基线（Faster-FreeShot和MT-FreeShot），并在具有挑战性的M5-Malaria和Raabin-WBC数据集上进行广泛实验，验证了MIAdapt的有效性。在源域图像未使用的情况下，MIAdapt的性能超过了现有的源自由UDA（SF-UDA）方法和少镜头域自适应方法。我们的代码和模型将公开可用。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIAdapt是一种针对显微图像自适应的源自由少镜头域自适应目标检测方法。</li>
<li>与其他方法相比，MIAdapt不需要使用大量标注的源数据集或无标签的目标数据集进行适配。</li>
<li>由于隐私等问题，可能会出现无法获得源数据的情况，MIAdapt考虑到了这一点。</li>
<li>在M5-Malaria和Raabin-WBC数据集上的实验验证了MIAdapt的有效性。</li>
<li>MIAdapt的性能优于现有的源自由UDA方法和少镜头域自适应方法。具体来说，它在不使用源域图像的情况下超过了这些方法的性能。</li>
<li>代码和模型将公开可用，便于其他人进行研究和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cf50c1293384b7b9b24831761f296a4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15bae6af356674b32e943cf399db7933.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c11912e3ba825a78e7f84a4e48e2c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58276572fd7ef9418bb2a247dae84426.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e881fce8314b446a3cb04432c63a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b477433883d23a14325a3ee3fd471e9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f5f5852f7e12fab744a57b670e0630d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence"><a href="#Manta-Enhancing-Mamba-for-Few-Shot-Action-Recognition-of-Long-Sub-Sequence" class="headerlink" title="Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence"></a>Manta: Enhancing Mamba for Few-Shot Action Recognition of Long   Sub-Sequence</h2><p><strong>Authors:Wenbo Huang, Jinghui Zhang, Guang Li, Lei Zhang, Shuoyuan Wang, Fang Dong, Jiahui Jin, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In few-shot action recognition (FSAR), long sub-sequences of video naturally express entire actions more effectively. However, the high computational complexity of mainstream Transformer-based methods limits their application. Recent Mamba demonstrates efficiency in modeling long sequences, but directly applying Mamba to FSAR overlooks the importance of local feature modeling and alignment. Moreover, long sub-sequences within the same class accumulate intra-class variance, which adversely impacts FSAR performance. To solve these challenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework (Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to enhance local feature representation, rather than directly modeling global features. An Outer Module captures dependencies of timeline between these local features for implicit temporal alignment. Secondly, a hybrid contrastive learning paradigm, combining both supervised and unsupervised methods, is designed to mitigate the negative effects of intra-class variance accumulation. The Matryoshka Mamba and the hybrid contrastive learning paradigm operate in two parallel branches within Manta, enhancing Mamba for FSAR of long sub-sequence. Manta achieves new state-of-the-art performance on prominent benchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical studies prove that Manta significantly improves FSAR of long sub-sequence from multiple perspectives. </p>
<blockquote>
<p>在少样本动作识别（FSAR）中，视频的长子序列更自然地表达了整个动作。然而，主流基于Transformer的方法的高计算复杂度限制了其应用。最近的Mamba在建模长序列方面展示了效率，但直接将Mamba应用于FSAR忽略了局部特征建模和对齐的重要性。此外，同一类别内的长子序列会累积类内方差，这对FSAR性能产生不利影响。为了解决这些挑战，我们提出了Matryoshka Mamba和对比学习框架（Manta）。首先，Matryoshka Mamba引入了多个内部模块来增强局部特征表示，而不是直接建模全局特征。外部模块捕获这些局部特征之间时间线的依赖性，以进行隐式时间对齐。其次，结合有监督和无监督方法的混合对比学习范式旨在减轻类内方差累积的负面影响。Matryoshka Mamba和混合对比学习范式在Manta的两个并行分支中运行，增强了Mamba对长子序列的FSAR能力。Manta在SSv2、Kinetics、UCF101和HMDB51等主流基准测试中实现了最新性能。大量实证研究证明，Manta从多个角度显著提高了长子序列的FSAR性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07481v5">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对少样本动作识别（FSAR）的新框架Manta。该框架解决了长序列动作识别的挑战，通过引入Matryoshka Mamba和混合对比学习范式，提高局部特征建模和对齐的能力，同时减少类内方差积累对FSAR性能的影响。Manta在多个基准测试集上实现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Matryoshka Mamba被引入以强化局部特征表示，通过多个Inner Modules建模局部特征，而非直接建模全局特征。</li>
<li>Outer Module用于捕捉这些局部特征的时间线依赖性，实现隐式的时间对齐。</li>
<li>提出了混合对比学习范式，结合监督和无监督方法，以减轻类内方差积累带来的负面影响。</li>
<li>Manta框架中，Matryoshka Mamba和混合对比学习范式在两个并行分支中运行，增强了Mamba对长子序列的FSAR性能。</li>
<li>Manta在SSv2、Kinetics、UCF101和HMDB51等主流基准测试集上实现了最新 state-of-the-art 性能。</li>
<li>广泛的实证研究证明，Manta从多个角度显著提高了长序列动作的FSAR性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4cdf0592535558f81b491aa1b7521988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16c2681d6549d6322cdf09a77cba898.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7850902a9c3912646aa3a165d6fc9b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d20f12b2ec1bc52f161cf0f0f664e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3efc2920012bc9e302b18d829345f0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27e490f47ca16f70e9a33df1c71fe152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c6e325a9bcc51cecd7512552bf34d5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HELMET-How-to-Evaluate-Long-Context-Language-Models-Effectively-and-Thoroughly"><a href="#HELMET-How-to-Evaluate-Long-Context-Language-Models-Effectively-and-Thoroughly" class="headerlink" title="HELMET: How to Evaluate Long-Context Language Models Effectively and   Thoroughly"></a>HELMET: How to Evaluate Long-Context Language Models Effectively and   Thoroughly</h2><p><strong>Authors:Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen</strong></p>
<p>Many benchmarks exist for evaluating long-context language models (LCLMs), yet developers often rely on synthetic tasks such as needle-in-a-haystack (NIAH) or an arbitrary subset of tasks. However, it remains unclear whether these benchmarks reflect the diverse downstream applications of LCLMs, and such inconsistencies further complicate model comparison. We investigate the underlying reasons behind these practices and find that existing benchmarks often provide noisy signals due to limited coverage of applications, insufficient context lengths, unreliable metrics, and incompatibility with base models. In this work, we introduce HELMET (How to Evaluate Long-context Models Effectively and Thoroughly), a comprehensive benchmark encompassing seven diverse, application-centric categories. We also address several issues in previous benchmarks by adding controllable lengths up to 128K tokens, model-based evaluation for reliable metrics, and few-shot prompting for robustly evaluating base models. Consequently, we demonstrate that HELMET offers more reliable and consistent rankings of frontier LCLMs. Through a comprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do not reliably predict downstream performance; (2) the diverse categories in HELMET exhibit distinct trends and low correlations with each other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models significantly lag behind closed ones when tasks require full-context reasoning or following complex instructions – the gap widens as length increases. Finally, we recommend using our RAG tasks for fast model development, as they are easy to run and better predict other downstream performance; ultimately, we advocate for a holistic evaluation across diverse tasks. </p>
<blockquote>
<p>针对长语境语言模型（LCLM）的评估存在许多基准测试，但开发人员通常依赖于合成任务，如“大海捞针”（NIAH）或一系列任意任务。然而，尚不清楚这些基准测试是否反映了LCLM的多样化下游应用，并且这种不一致性进一步加剧了模型比较的难度。我们调查了这些实践背后的根本原因，并发现现有基准测试通常由于应用覆盖有限、上下文长度不足、指标不可靠以及与基础模型不兼容而提供嘈杂的信号。在这项工作中，我们引入了全面的基准测试HELMET（如何有效且彻底地评估长语境模型），包含七个多样化、以应用为中心的类别。我们还通过增加可控长度（最多达128K令牌）、基于模型的评估以获取可靠指标以及少样本提示来解决先前基准测试中的几个问题，从而稳健地评估基础模型。因此，我们证明了HELMET为前沿LCLM提供了更可靠且一致性的排名。通过对59个LCLM的综合研究，我们发现：（1）像NIAH这样的合成任务无法可靠地预测下游性能；（2）HELMET中的不同类别呈现出不同的趋势，彼此间低相关性；（3）虽然大多数LCLM在NIAH得分上表现完美，但在需要全语境推理或遵循复杂指令的任务上，开源模型与封闭模型的性能差距显著拉大，且随着长度的增加而扩大。最后，我们建议使用我们的RAG任务进行快速模型开发，因为它们易于运行并可以更好地预测其他下游性能；最终，我们主张在多样化任务上进行整体评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02694v3">PDF</a> ICLR 2025. Project page: <a target="_blank" rel="noopener" href="https://princeton-nlp.github.io/HELMET/">https://princeton-nlp.github.io/HELMET/</a></p>
<p><strong>Summary</strong></p>
<p>该文探讨了对长文本语境模型（LCLM）的评估问题。现有评估基准测试存在诸多不足，如覆盖应用有限、语境长度不足、评价指标不可靠等。为此，作者提出了HELMET基准测试，该测试包含七个应用类别，并解决了以往基准测试中的问题，如可控的语境长度、基于模型的可靠评价指标和少样本提示等。研究结果表明，HELMET为前沿LCLM提供了更可靠和一致的排名。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有评估基准测试在反映LCLM的多样下游应用方面存在不足。</li>
<li>开发者常依赖合成任务如“海底捞针”，但这些任务不能可靠预测下游性能。</li>
<li>HELMET基准测试包含七个应用类别，更全面地评估LCLM。</li>
<li>HELMET解决了以往基准测试中的问题，如语境长度、评价指标的可靠性等。</li>
<li>研究发现，合成任务不能反映LCLM的真实性能，不同类别之间的趋势各异且关联度低。</li>
<li>虽然许多LCLM在合成任务上表现完美，但在需要全语境推理或遵循复杂指令的任务上，开源模型与封闭模型之间存在显著差距，且差距随长度增加而扩大。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02694">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3db5b03b33c60155defca8831c45cfb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d22634e517f6000c3b1c138f5cc3efc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a5ae9b24d24215601e28dc20a8504a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78811e351ba329ad37aa5cd744f27fd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efeadb9a4d247682c5eef8384c1b6545.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d88637e9d616c090a64af58ed11cd5.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>Deep neural networks have been extensively applied in the medical domain for various tasks, including image classification, segmentation, and landmark detection. However, their application is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task, specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection, a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images), surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge, this work represents the first application of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<blockquote>
<p>深度神经网络在医疗领域得到了广泛应用，用于各种任务，包括图像分类、分割和地标检测。然而，其应用往往受到数据和可用标注图像匮乏的制约。本研究引入去噪扩散概率模型（DDPMs）的新应用，专门解决X射线成像中标注数据有限带来的挑战。我们的主要创新之处在于利用DDPMs进行地标检测的自我监督预训练，这是该领域之前未探索过的方法。该方法能够在极少的标注训练数据（仅50张图像）下实现准确的地标检测，超越了ImageNet监督预训练和传统自我监督技术在三个流行的X射线基准数据集上的表现。据我们所知，这项工作代表了扩散模型在自我监督学习中的首次应用于地标检测，这可能为数据稀缺情况下提供有价值的预训练方法，特别是在小样例情况下。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v3">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>     本文介绍了将去噪扩散概率模型（DDPMs）应用于医学领域地标检测任务的新应用，解决了在X射线成像中有限标注数据的挑战。通过自我监督预训练DDPMs，该方法在仅少量（如50张）标注图像的情况下实现了准确的地标检测，超越了ImageNet监督预训练和传统自我监督技术，在三个流行的X射线基准数据集上表现优异。这是首次将扩散模型应用于地标检测中的自我监督学习，为缓解数据稀缺问题提供了一种有价值的预训练方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究将去噪扩散概率模型（DDPMs）应用于医学领域的地标检测任务。</li>
<li>面对医学图像标注和图像数据稀缺的挑战，DDPMs通过自我监督预训练实现了准确的地标检测。</li>
<li>在仅少量标注图像（如50张）的情况下，DDPMs表现超越ImageNet监督预训练和传统自我监督技术。</li>
<li>此方法在三个流行的X射线基准数据集上进行了验证并表现优异。</li>
<li>这是首次将扩散模型应用于医学图像地标检测中的自我监督学习。</li>
<li>该方法可能为缓解医学图像数据稀缺问题的预训练方案提供新的思路。</li>
<li>该研究为深度神经网络在医学领域的应用提供了新的可能性，特别是在数据稀缺的情况下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4fe8ec52f131a12305c13150eef9066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf8299076cafa7e4723193ef80f93d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a569e2988b76382c74c267bebcb93089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e059e518c4b2016e7c6a59a420b98309.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-49f1d842eac04cb406c067daa85a7902.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-09  Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic   Membrane Vessel Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6d19e10e57fe6b34da29c78421a0ded.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-09  Multi-Agent Inverse Q-Learning from Demonstrations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
