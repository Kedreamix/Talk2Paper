<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-22  Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f657e4071acf014b3f096ccdeafde015.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    58 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-22-更新"><a href="#2025-04-22-更新" class="headerlink" title="2025-04-22 更新"></a>2025-04-22 更新</h1><h2 id="Towards-Accurate-and-Interpretable-Neuroblastoma-Diagnosis-via-Contrastive-Multi-scale-Pathological-Image-Analysis"><a href="#Towards-Accurate-and-Interpretable-Neuroblastoma-Diagnosis-via-Contrastive-Multi-scale-Pathological-Image-Analysis" class="headerlink" title="Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis"></a>Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis</h2><p><strong>Authors:Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu</strong></p>
<p>Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians’ comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to seamlessly bridge patch-level predictions to whole slide image-level classifications. We validate CMSwinKAN on the PpNTs dataset, which was collaboratively established with our partner hospital and the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/JSLiam94/CMSwinKAN">https://github.com/JSLiam94/CMSwinKAN</a>. </p>
<blockquote>
<p>神经母细胞瘤肾上腺来源是儿童最常见的实体恶性肿瘤之一，具有显著的临床异质性。从苏木精和伊红染色的全切片图像进行及时准确的病理诊断对患者的预后至关重要。然而，当前的诊断实践主要依赖于病理医师的主观手动检查，导致准确性不一致。现有的全自动切片图像分类方法面临可解释性差、特征提取能力有限以及计算成本高的挑战，限制了其在临床实践中的部署。为了克服这些局限性，我们提出了CMSwinKAN，这是一种基于对比学习的多尺度特征融合模型，专门用于病理图像分类。它通过集成内核激活网络增强了Swin Transformer架构的多层感知器和分类头模块，显著提高了可解释性和准确性。通过融合多尺度特征和利用对比学习策略，CMSwinKAN模仿了临床医生全面的方法，有效地捕捉了全局和局部组织特征。此外，我们引入了一种受临床见解启发式的软投票机制，无缝地连接了补丁级别的预测到全切片图像级别的分类。我们在与合作医院共同建立的PpNTs数据集和可公开访问的BreakHis数据集上验证了CMSwinKAN。结果表明，CMSwinKAN的性能优于在大型数据集上预训练的现有最先进的病理学专用模型。我们的源代码可在<a target="_blank" rel="noopener" href="https://github.com/JSLiam94/CMSwinKAN%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JSLiam94/CMSwinKAN上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13754v1">PDF</a> 14pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于对比学习的多尺度特征融合模型CMSwinKAN，用于神经母细胞瘤等肾上腺衍生肿瘤的病理图像分类。该模型通过整合核激活网络，增强了Swin Transformer架构的可解释性和准确性。通过多尺度特征融合和对比学习策略，CMSwinKAN有效捕捉全局和局部组织特征，模仿医生的综合诊断方法。在PpNTs和BreakHis数据集上的验证结果显示，CMSwinKAN表现优于现有的病理学特定模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经母细胞瘤是常见的儿童实体恶性肿瘤，具有显著的临床异质性，及时准确的病理诊断对预后至关重要。</li>
<li>当前诊断方法主要依赖病理医师的主观手动检查，存在准确性不一致的问题。</li>
<li>CMSwinKAN模型结合对比学习与多尺度特征融合，提高了病理图像分类的准确性和可解释性。</li>
<li>CMSwinKAN通过整合核激活网络，增强Swin Transformer架构的性能。</li>
<li>模型能捕捉全局和局部组织特征，模仿医生的综合诊断方法。</li>
<li>在PpNTs和BreakHis数据集上的验证结果显示CMSwinKAN优于现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13754">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-806109825c90292eaf19535c12b3fc14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6dbb7506a28d6fb9101b8a8b82c3a30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47b8c90fede5a73b53fa4573a332eb18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5f62ca2268c1ea6e2086d352acb3e5d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Study-of-Solar-Energetic-Particles-their-Source-Regions-Flares-and-CMEs-during-Solar-Cycles-23-24"><a href="#Study-of-Solar-Energetic-Particles-their-Source-Regions-Flares-and-CMEs-during-Solar-Cycles-23-24" class="headerlink" title="Study of Solar Energetic Particles: their Source Regions, Flares and   CMEs during Solar Cycles 23-24"></a>Study of Solar Energetic Particles: their Source Regions, Flares and   CMEs during Solar Cycles 23-24</h2><p><strong>Authors:Raj Kumar, Ramesh Chandra, Bimal Pande, Seema Pande</strong></p>
<p>In this work, we examine the association between solar active regions and 152 solar flares, coronal mass ejections, and solar energetic particle (SEP) events over solar cycles 23-24 (1997-2017). The CDAW center’s GOES data in the energy channel &gt;10 MeV (Major SEPs; solar proton events) with flux &gt;&#x3D; 10 pfu was used for our investigation. For the associated activities, we have analyzed the data from space born satellites namely: SOHO&#x2F;LASCO and SDO&#x2F;AIA. We found a moderate correlation (55 %) between SXR flux and sunspot area i.e., active regions with larger sunspot areas generally generate larger flares. We found that most of the SEPs are originated from the magnetically complex active regions i.e., hale class beta-gamma-delta and beta. Very few events were associated with unipolar active regions. Stronger GOES X-ray is linked to more impulsive events, as evidenced by the negative correlation (-0.40) between X-ray flux and SEP duration. In the active region beta-gamma-delta, the highest average SEP intensity (2051 pfu) was detected. In the data set used, only 10 % SEPs are found impulsive in nature, while the remaining 90 % are gradual in nature. All the impulsive events had SEP intensity less than 100 pfu and most of the CMEs associated with these events were decelerated CMEs. We discovered that the majority of faster CMEs are linked to the most complex magnetic active regions. This indicates that high speed CMEs are produced by magnetically complex active regions. We discovered that 58 SEP events in our data set are linked to accelerated CMEs, while 82 are linked to decelerated CMEs. The highest average CME width is found corresponding to magnetically most complex active regions beta-delta, gamma-delta, alpha-gamma-delta and beta-gamma-delta, which shows that large CMEs are the consequences of magnetically complex active regions. </p>
<blockquote>
<p>在这项工作中，我们研究了太阳活动区域与太阳周期中的太阳耀斑（solar flare）、日冕物质抛射（coronal mass ejection）以及太阳高能粒子（SEP）事件之间的关联关系，涵盖了太阳周期的第23和第24周期（即太阳活动高年，从1997年至2017年）。我们使用了CDAW中心的GOES数据对大于10MeV的质子能量进行研究，并针对其质量选择辐射通量大于等于每平方微米十亿电子伏（pfu）的数据进行分析。对于相关的活动数据，我们分析了太空探测器（如SOHO&#x2F;LASCO和SDO&#x2F;AIA）提供的数据。我们发现太阳软X射线辐射通量与太阳黑子面积之间存在中度相关性（即百分之五十五），活跃区域的太阳黑子面积越大通常引发的大型爆发越明显。大多数太阳高能粒子来自磁场复杂的活跃区域，如beta-gamma-delta型和beta型，非常少的部分来源于单极活动区。此外还发现强劲的GOES射线是与更多的突发事件紧密联系的证据表现在这两者间的负相关性（负零点四），这代表着强烈的X射线射流会导致高能粒子持续时间减少。在beta-gamma-delta型的活跃区域中检测到了最高的平均SEP强度（每平方微米两千零五十一电子伏）。在所使用的数据集中，仅有百分之十的SEP是突发性的，其余百分之九十是逐渐发展的。所有突发性事件的SEP强度均低于每平方微米一百电子伏，并且与这些事件相关的日冕物质抛射大多数是减速抛射。我们发现大多数高速日冕物质抛射与最复杂的磁场活跃区域相关联。这表明高速的日冕物质抛射是由磁场复杂的活跃区域引起的。我们发现数据集中有百分之五十八的SEP事件与加速日冕物质抛射相关，而有百分之八十二则与减速日冕物质抛射有关。对应于最复杂的活跃区域如beta-delta型、gamma-delta型以及alpha-gamma-delta型和beta-gamma-delta型时平均日冕物质抛射宽度最高，这表明大规模的日冕物质抛射是磁场复杂活跃区域的结果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13654v1">PDF</a> 26 pages, 09 figures (accepted for publication in Indian Journal of   Physics)</p>
<p><strong>摘要</strong></p>
<p>本文研究了第23至第24个太阳周期（即过去自一九九七年起的二十年）中太阳活动区域与太阳耀斑爆发、日冕物质喷射以及高能太阳粒子事件的关联。利用CDAW中心的GOES数据进行分析，主要关注了能量通道大于或等于十MeV的SEP事件。结合SOHO&#x2F;LASCO和SDO&#x2F;AIA等太空卫星的数据，研究发现太阳X射线流量与太阳黑子面积之间存在中度相关性（约百分之五十五），表明较大太阳黑子区域的活跃区域通常产生更大的耀斑。此外，大多数SEP事件源自复杂的磁活跃区域，如β-γ-δ和β类Hale区域。很少有事件与单极活跃区域相关。还发现强烈的GOES X射线与更突发性事件有关，表现为X射线流量与SEP持续时间之间的负相关性（-零点四）。在β-γ-δ活跃区域检测到最高的平均SEP强度（两千零五十一pfu）。数据集中仅有百分之十的SEP属于突发性事件，其余百分之九十则为渐进性事件。所有突发性事件的SEP强度均低于一百pfu，并且大多数与之相关的CME都是减速的CME。研究还发现，大多数高速CME与最复杂的磁活跃区域有关，表明高速CME是由复杂的磁活跃区域产生的。数据集中有百分之五十八的SEP事件与加速的CME相关，而百分之八十二与减速的CME相关。最大的CME宽度通常出现在最复杂的磁活跃区域，如β-δ、γ-δ和α-γ-δ以及β-γ-δ区域。这表明大型CME是复杂磁活跃区域的结果。总结而言，本研究探讨了太阳活动区域的磁场复杂性与高能太阳粒子事件之间的关联性及其背后的物理机制。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>太阳活动区域与太阳耀斑爆发、日冕物质喷射及高能太阳粒子事件之间存在显著关联。</li>
<li>太阳X射线流量与太阳黑子面积之间存在中度相关性，暗示大耀斑更可能出现在较大太阳黑子区域的活跃区域。</li>
<li>大部分高能太阳粒子事件源自复杂的磁活跃区域，如β-γ-δ类区域。单极活跃区域与之相关的事件很少。</li>
<li>强烈的GOES X射线流量与更突发性的事件有关，表现为X射线流量与SEP持续时间之间的负相关性。</li>
<li>数据集中仅有小部分SEP事件为突发性，大部分属于渐进性事件。与突发性事件相关的CME大多是减速的。</li>
<li>高速CME多与最复杂的磁活跃区域有关。这表明高速CME的产生与复杂的磁活跃区域密切相关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19389dcc2b0e23909ec9325175998c17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b228ba73adf98e711e887b6731576bb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-110a95e9406e261210004bd163ff26d7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Static-linear-density-response-from-X-ray-Thomson-scattering-measurements-a-case-study-of-warm-dense-beryllium"><a href="#Static-linear-density-response-from-X-ray-Thomson-scattering-measurements-a-case-study-of-warm-dense-beryllium" class="headerlink" title="Static linear density response from X-ray Thomson scattering   measurements: a case study of warm dense beryllium"></a>Static linear density response from X-ray Thomson scattering   measurements: a case study of warm dense beryllium</h2><p><strong>Authors:Sebastian Schwalbe, Hannah Bellenbaum, Tilo Döppner, Maximilian Böhme, Thomas Gawne, Dominik Kraus, Michael J. MacDonald, Zhandos Moldabekov, Panagiotis Tolias, Jan Vorberger, Tobias Dornheim</strong></p>
<p>Linear response theory is ubiquitous throughout physics and plays a central role in the theoretical description of warm dense matter – an extreme state that occurs within compact astrophysical objects and that is traversed on the compression path of a fuel capsule in inertial confinement fusion applications. Here we show how one can relate the static linear density response function to X-ray Thomson scattering (XRTS) measurements, which opens up new possibilities for the diagnostics of extreme states of matter, and for the rigorous assessment and verification of theoretical models and approximations. As a practical example, we consider an XRTS data set of warm dense beryllium taken at the National Ignition Facility [T.<del>D&quot;oppner \emph{et al.}, \textit{Nature} \textbf{618}, 270-275 (2023)]. The comparison with state-of-the-art \emph{ab initio} path integral Monte Carlo (PIMC) simulations [T.</del>Dornheim \emph{et al.}, \textit{Nature Commun.}~(in print), arXiv:2402.19113] gives us a best estimate of the mass density of $\rho&#x3D;18\pm6,$g&#x2F;cc, which is consistent with previous PIMC and density functional theory based studies, but rules out the original estimate of $\rho&#x3D;34\pm4,$g&#x2F;cc based on a Chihara model fit. </p>
<blockquote>
<p>线性响应理论在物理学中无处不在，并在描述热密物质的理论中起到核心作用。这是一种极端状态，出现在致密的天体物理对象中，也出现在惯性约束聚变应用的燃料胶囊压缩路径上。在这里，我们展示了如何将静态线性密度响应函数与X射线汤姆森散射（XRTS）测量相关联，这为极端物质状态的诊断、理论模型和近似的严格评估和验证提供了新的可能性。作为一个实际例子，我们考虑了在国家点火设施（T. Doppner等人，《自然》杂志，第618期，270-275页（2023年））获取的温热致密铍的XRTS数据集。与最新的从头开始路径积分蒙特卡洛（PIMC）模拟（Dornheim等人，《自然通讯》（印刷中），arXiv：2402.19113）的比较，为我们提供了最佳质量密度估计值ρ&#x3D;18±6 g&#x2F;cc。这与之前的PIMC和基于密度泛函理论的研究相一致，但排除了基于Chihara模型拟合的原始估计值ρ&#x3D;34±4 g&#x2F;cc。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13611v1">PDF</a> </p>
<p><strong>Summary</strong><br>     线性响应理论在物理学中普遍存在，对于描述热密物质的理论起到了核心作用。本文通过展示静态线性密度响应函数与X射线汤姆森散射测量的关系，为极端物质状态的诊断和理论模型与近似的严格评估验证提供了新的可能性。对比实验数据与最先进的从头计算路径积分蒙特卡洛模拟，得到最佳估计质量密度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>线性响应理论在描述热密物质中起到核心作用，特别是在物理和天体物理学领域。</li>
<li>X射线汤姆森散射测量与静态线性密度响应函数的关系可用于极端物质状态的诊断。</li>
<li>通过对比实验数据与最新的从头计算路径积分蒙特卡洛模拟，可以对理论模型进行验证和评估。</li>
<li>文中以热密铍为例，给出了基于最新模拟的最佳估计质量密度。</li>
<li>这一估计与之前的PIMC和密度泛函理论研究一致，排除了基于Chihara模型拟合的原始估计。</li>
<li>该研究为惯性约束聚变应用中燃料胶囊的压缩路径提供了新视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0060772abb4c3c4c2534fb3d0c3ec3fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc221e242d3621eb574c061234735577.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd360b6f56fe56da9d12d1074df1263.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Filter2Noise-Interpretable-Self-Supervised-Single-Image-Denoising-for-Low-Dose-CT-with-Attention-Guided-Bilateral-Filtering"><a href="#Filter2Noise-Interpretable-Self-Supervised-Single-Image-Denoising-for-Low-Dose-CT-with-Attention-Guided-Bilateral-Filtering" class="headerlink" title="Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for   Low-Dose CT with Attention-Guided Bilateral Filtering"></a>Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for   Low-Dose CT with Attention-Guided Bilateral Filtering</h2><p><strong>Authors:Yipeng Sun, Linda-Sophie Schneider, Mingxuan Gu, Siyuan Mei, Chengze Ye, Fabian Wagner, Siming Bayer, Andreas Maier</strong></p>
<p>Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework – Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at <a target="_blank" rel="noopener" href="https://github.com/sypsyp97/Filter2Noise.git">https://github.com/sypsyp97/Filter2Noise.git</a> . </p>
<blockquote>
<p>在低剂量计算机断层扫描（CT）中，有效的去噪对于增强细微结构和低对比度病变至关重要，同时防止诊断错误。监督方法受限于配对数据集，而自监督方法通常需要多个噪声图像，并依赖于U-Net等深度网络，对去噪机制提供很少的洞察。为了解决这些挑战，我们提出了一种可解释的自监督单图像去噪框架——Filter2Noise（F2N）。我们的方法引入了一个注意力引导双边滤波器，该滤波器可通过轻量级模块预测空间变化的滤波器参数，以适应每个噪声输入。这些参数可以在训练后进行可视化和调整，以实现用户控制的特定感兴趣区域的去噪。为了实现单图像训练，我们引入了一种新的下采样洗牌策略，以及一种新的自监督损失函数，它将Noise2Noise的概念扩展到单图像并解决空间相关噪声问题。在梅奥诊所2016年低剂量CT数据集上，F2N在PSNR方面比领先的自监督单图像方法（ZS-N2N）高出4.59 dB，同时提高了透明度、用户控制力和参数效率。这些特点对于要求精确和可解释降噪的医学应用提供了关键优势。我们的代码演示在<a target="_blank" rel="noopener" href="https://github.com/sypsyp97/Filter2Noise.git">https://github.com/sypsyp97/Filter2Noise.git</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13519v1">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Filter2Noise（F2N）的可解释性自监督单图像去噪框架。它通过引入注意力引导双边滤波器，自适应于每个噪声输入，并通过轻量级模块预测空间变化的滤波器参数，实现用户可在训练后针对特定感兴趣区域进行可视化并调整去噪效果。此外，采用了一种新的下采样洗牌策略，配合自监督损失函数，将Noise2Noise的概念扩展到单图像上，并解决空间相关噪声问题。在Mayo Clinic 2016低剂量CT数据集上，F2N表现出优于现有自监督单图像方法（ZS-N2N）的优异性能，提高了峰值信噪比（PSNR）4.59 dB，同时提高了透明度、用户控制和参数效率。这对于需要精确和可解释性降噪的医学应用具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>有效去噪在低剂量CT中至关重要，能提高细微结构和低对比度病变的识别度，防止诊断错误。</li>
<li>监督方法受限于配对数据集，自监督方法则需要多噪声图像并依赖深度网络（如U-Net），对于去噪机制提供较少见解。</li>
<li>提出的Filter2Noise（F2N）框架是一种可解释的自监督单图像去噪方法。</li>
<li>F2N通过注意力引导双边滤波器自适应于每个噪声输入，并引入轻量级模块以预测空间变化的滤波器参数，实现用户可控的特定区域去噪。</li>
<li>F2N采用新的下采样洗牌策略和自监督损失函数，扩展Noise2Noise概念至单图像，解决空间相关噪声问题。</li>
<li>在Mayo Clinic 2016低剂量CT数据集上，F2N相比现有自监督单图像方法提高了4.59 dB PSNR，同时增强透明度、用户控制和参数效率。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13519">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c912a699fc6838925628534199c6cb10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad816a6a42b79f47c49e9babc9892252.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Learning-from-Noisy-Pseudo-labels-for-All-Weather-Land-Cover-Mapping"><a href="#Learning-from-Noisy-Pseudo-labels-for-All-Weather-Land-Cover-Mapping" class="headerlink" title="Learning from Noisy Pseudo-labels for All-Weather Land Cover Mapping"></a>Learning from Noisy Pseudo-labels for All-Weather Land Cover Mapping</h2><p><strong>Authors:Wang Liu, Zhiyu Wang, Xin Guo, Puhong Duan, Xudong Kang, Shutao Li</strong></p>
<p>Semantic segmentation of SAR images has garnered significant attention in remote sensing due to the immunity of SAR sensors to cloudy weather and light conditions. Nevertheless, SAR imagery lacks detailed information and is plagued by significant speckle noise, rendering the annotation or segmentation of SAR images a formidable task. Recent efforts have resorted to annotating paired optical-SAR images to generate pseudo-labels through the utilization of an optical image segmentation network. However, these pseudo-labels are laden with noise, leading to suboptimal performance in SAR image segmentation. In this study, we introduce a more precise method for generating pseudo-labels by incorporating semi-supervised learning alongside a novel image resolution alignment augmentation. Furthermore, we introduce a symmetric cross-entropy loss to mitigate the impact of noisy pseudo-labels. Additionally, a bag of training and testing tricks is utilized to generate better land-cover mapping results. Our experiments on the GRSS data fusion contest indicate the effectiveness of the proposed method, which achieves first place. The code is available at <a target="_blank" rel="noopener" href="https://github.com/StuLiu/DFC2025Track1.git">https://github.com/StuLiu/DFC2025Track1.git</a>. </p>
<blockquote>
<p>SAR图像的语义分割在遥感领域受到了广泛关注，因为SAR传感器不受天气和光照条件的影响。然而，SAR图像缺乏详细信息，并且受到斑点噪声的困扰，这使得SAR图像的标注或分割成为一项艰巨的任务。近期的研究尝试对配对的光学SAR图像进行标注，以利用光学图像分割网络生成伪标签。然而，这些伪标签充满了噪声，导致SAR图像分割性能不佳。本研究介绍了一种更精确的方法生成伪标签，该方法结合了半监督学习以及一种新型图像分辨率对齐增强技术。此外，我们还引入了对称交叉熵损失来减轻噪声伪标签的影响。同时，我们还采用了一系列训练和测试技巧来生成更好的土地覆盖映射结果。我们在GRSS数据融合竞赛上的实验证明了所提方法的有效性，该方法取得了第一名。代码可在<a target="_blank" rel="noopener" href="https://github.com/StuLiu/DFC2025Track1.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/StuLiu/DFC2025Track1.git获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13458v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAR图像语义分割因其不受天气和光照条件影响的特性而受到遥感领域的关注。但SAR图像缺乏详细信息且易受斑点噪声影响，使得SAR图像的标注或分割成为一项艰巨的任务。本研究通过结合半监督学习和新颖的图像分辨率对齐增强方法，提出了一种更精确的生成伪标签的方法。此外，引入对称交叉熵损失以减轻噪声伪标签的影响，并使用一系列训练和测试技巧来生成更好的土地覆盖映射结果。在GRSS数据融合竞赛上的实验验证了所提方法的有效性，该方法取得了第一名，代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAR图像语义分割在遥感领域具有重要性，因为它不受天气和光照条件的限制。</li>
<li>SAR图像缺乏详细信息和易受斑点噪声影响，使得标注和分割变得困难。</li>
<li>本研究提出了一种结合半监督学习和图像分辨率对齐增强方法生成更精确伪标签的方法。</li>
<li>引入对称交叉��熵损失以减轻噪声伪标签对分割性能的影响。</li>
<li>使用一系列训练和测试技巧来提高土地覆盖映射结果的准确性。</li>
<li>在GRSS数据融合竞赛上进行的实验验证了所提出方法的有效性，并取得了第一名的好成绩。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17ea9b8a78f2eca84b0be4c7d9dfd8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63db662953a11e8a6b99e5c5c07935a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59a2af4d9ecebb9aff5a74983b70b83e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b8aa29b3b4990f5772eb852ea23b64a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Auto-FEDUS-Autoregressive-Generative-Modeling-of-Doppler-Ultrasound-Signals-from-Fetal-Electrocardiograms"><a href="#Auto-FEDUS-Autoregressive-Generative-Modeling-of-Doppler-Ultrasound-Signals-from-Fetal-Electrocardiograms" class="headerlink" title="Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound   Signals from Fetal Electrocardiograms"></a>Auto-FEDUS: Autoregressive Generative Modeling of Doppler Ultrasound   Signals from Fetal Electrocardiograms</h2><p><strong>Authors:Alireza Rafiei, Gari D. Clifford, Nasim Katebi</strong></p>
<p>Fetal health monitoring through one-dimensional Doppler ultrasound (DUS) signals offers a cost-effective and accessible approach that is increasingly gaining interest. Despite its potential, the development of machine learning based techniques to assess the health condition of mothers and fetuses using DUS signals remains limited. This scarcity is primarily due to the lack of extensive DUS datasets with a reliable reference for interpretation and data imbalance across different gestational ages. In response, we introduce a novel autoregressive generative model designed to map fetal electrocardiogram (FECG) signals to corresponding DUS waveforms (Auto-FEDUS). By leveraging a neural temporal network based on dilated causal convolutions that operate directly on the waveform level, the model effectively captures both short and long-range dependencies within the signals, preserving the integrity of generated data. Cross-subject experiments demonstrate that Auto-FEDUS outperforms conventional generative architectures across both time and frequency domain evaluations, producing DUS signals that closely resemble the morphology of their real counterparts. The realism of these synthesized signals was further gauged using a quality assessment model, which classified all as good quality, and a heart rate estimation model, which produced comparable results for generated and real data, with a Bland-Altman limit of 4.5 beats per minute. This advancement offers a promising solution for mitigating limited data availability and enhancing the training of DUS-based fetal models, making them more effective and generalizable. </p>
<blockquote>
<p>通过一维多普勒超声（DUS）信号进行胎儿健康监测是一种成本效益高且易于实施的方法，越来越受到关注。尽管其潜力巨大，但利用机器学习方法评估母亲和胎儿健康状况的DUS信号技术仍相对有限。这种缺乏主要是由于缺乏大量可靠的DUS数据集进行解读以及不同孕期数据存在不平衡的问题。为应对这一挑战，我们引入了一种新型的autoregressive生成模型（Auto-FEDUS），旨在将胎儿心电图（FECG）信号映射到相应的DUS波形上。该模型利用基于膨胀因果卷积的神经网络时序网络直接在波形层面进行操作，有效捕捉信号中的短期和长期依赖关系，保持生成数据的完整性。跨主体实验表明，在时间域和频域评估中，Auto-FEDUS均优于传统生成架构，产生的DUS信号形态与真实信号非常相似。这些合成信号的逼真性进一步通过质量评估模型进行了衡量，所有信号均被分类为高质量信号；同时，心率估计模型对生成数据和真实数据产生的结果相当，Bland-Altman限值为每分钟4.5次心跳。这一进展为解决数据有限的问题提供了一个有前景的解决方案，并有望增强基于DUS的胎儿模型的训练效果，使其更具有效性和泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13233v1">PDF</a> AAAI 2025 Workshop on Large Language Models and Generative AI for   Health</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种通过一维多普勒超声（DUS）信号进行胎儿健康监测的方法，该方法具有成本效益高、易于获取的特点，越来越受到关注。然而，由于缺乏大量的DUS数据集和可靠的解释参考以及不同孕期数据的失衡，使用机器学习方法评估母婴健康状况的技术发展受到限制。为此，研究团队提出了一种新型的自动回归生成模型Auto-FEDUS，用于将胎儿心电图（FECG）信号映射到相应的DUS波形上。该模型利用基于扩张因果卷积的神经网络时间网络直接在波形级别进行操作，可以有效地捕捉信号中的短期和长期依赖性，保持生成数据的完整性。研究表明，Auto-FEDUS在时间域和频域评估方面都优于传统生成架构，生成的DUS信号与实际信号的形态非常相似。该研究的进展为解决数据有限性问题提供了一个有前途的解决方案，并有助于提高基于DUS的胎儿模型的有效性和通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>一维多普勒超声（DUS）信号用于胎儿健康监测是一种经济、方便的方法，备受关注。</li>
<li>基于机器学习的DUS技术评估母婴健康状况仍处于起步阶段。</li>
<li>缺乏大规模的DUS数据集以及可靠的数据解释和不平衡的孕期数据限制了技术发展。</li>
<li>引入新型自动回归生成模型Auto-FEDUS，将胎儿心电图（FECG）信号映射到DUS波形上。</li>
<li>Auto-FEDUS利用神经网络时间网络捕捉信号中的短期和长期依赖性。</li>
<li>Auto-FEDUS生成的DUS信号与实际信号的形态相似，通过质量评估模型和心率估计模型验证其真实性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13233">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6ecbf5559718b1ced1a3fd7979c139fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b55f9540011050a45778f21566598fe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-191bcb12cf1c95ab982d327d5d51ae75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-790c00379ae68c7b2a3cf9c9d8d74d73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09e298cc99ba32448c22620eb2482a3a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Efficient-Brain-Tumor-Segmentation-Using-a-Dual-Decoder-3D-U-Net-with-Attention-Gates-DDUNet"><a href="#Efficient-Brain-Tumor-Segmentation-Using-a-Dual-Decoder-3D-U-Net-with-Attention-Gates-DDUNet" class="headerlink" title="Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with   Attention Gates (DDUNet)"></a>Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with   Attention Gates (DDUNet)</h2><p><strong>Authors:Mohammad Mahdi Danesh Pajouh</strong></p>
<p>Cancer remains one of the leading causes of mortality worldwide, and among its many forms, brain tumors are particularly notorious due to their aggressive nature and the critical challenges involved in early diagnosis. Recent advances in artificial intelligence have shown great promise in assisting medical professionals with precise tumor segmentation, a key step in timely diagnosis and treatment planning. However, many state-of-the-art segmentation methods require extensive computational resources and prolonged training times, limiting their practical application in resource-constrained settings. In this work, we present a novel dual-decoder U-Net architecture enhanced with attention-gated skip connections, designed specifically for brain tumor segmentation from MRI scans. Our approach balances efficiency and accuracy by achieving competitive segmentation performance while significantly reducing training demands. Evaluated on the BraTS 2020 dataset, the proposed model achieved Dice scores of 85.06% for Whole Tumor (WT), 80.61% for Tumor Core (TC), and 71.26% for Enhancing Tumor (ET) in only 50 epochs, surpassing several commonly used U-Net variants. Our model demonstrates that high-quality brain tumor segmentation is attainable even under limited computational resources, thereby offering a viable solution for researchers and clinicians operating with modest hardware. This resource-efficient model has the potential to improve early detection and diagnosis of brain tumors, ultimately contributing to better patient outcomes </p>
<blockquote>
<p>癌症仍然是全球主要的死亡原因之一，而在其多种形态中，脑肿瘤因其侵袭性以及早期诊断的艰巨挑战而特别恶名昭彰。人工智能领域的最新进展在协助医疗专业人士进行精确的肿瘤分割方面显示出巨大的潜力，这是及时诊断和治疗计划的关键步骤。然而，许多最先进的分割方法需要大量的计算资源和长时间的训练，这在资源受限的环境中限制了它们的实际应用。在这项工作中，我们提出了一种新型的双解码器U-Net架构，该架构增强了注意力门控跳跃连接，专门用于从MRI扫描中分割脑肿瘤。我们的方法通过实现竞争性的分割性能同时显著提高训练效率来平衡效率和准确性。在BraTS 2020数据集上评估，所提出模型在50个周期内实现了整体肿瘤（WT）的Dice得分为85.06%，肿瘤核心（TC）的得分为80.61%，增强肿瘤（ET）的得分为71.26%，超过了常用的一些U-Net变体。我们的模型证明了即使在有限的计算资源下也能实现高质量的脑肿瘤分割，从而为使用适度硬件的研究人员和临床医生提供了切实可行的解决方案。这种资源高效型的模型有潜力改善脑肿瘤的早期检测和诊断，最终为患者带来更好的治疗效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13200v1">PDF</a> </p>
<p><strong>Summary</strong><br>     人工智能在脑肿瘤分割上展现出巨大潜力，有助医学专业人士进行精确肿瘤分割，但现有方法计算资源需求大、训练时间长。本研究提出一种新型双解码器U-Net架构，结合注意力门控跳过连接，专为从MRI扫描中分割脑肿瘤而设计。该模型在BraTS 2020数据集上表现优越，显著减少训练需求，平衡了效率和准确性，为有限资源下研究者与临床医生提供可行解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>癌症仍是全球主要死亡原因之一，脑肿瘤因其侵袭性和早期诊断的挑战性而备受关注。</li>
<li>人工智能在脑肿瘤分割方面展现出巨大潜力，有助于医学专业人士进行精确诊断。</li>
<li>现有分割方法计算资源需求大、训练时间长，限制了其在资源有限环境下的应用。</li>
<li>本研究提出了一种新型双解码器U-Net架构，结合注意力门控跳过连接，专为脑肿瘤分割设计。</li>
<li>该模型在BraTS 2020数据集上表现优越，取得了较高的分割准确性。</li>
<li>模型在仅50个周期（epochs）内达到了高Dice得分，对整体肿瘤、肿瘤核心和增强肿瘤的分割准确率分别达到了85.06%、80.61%和71.26%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13200">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-988ef8b7dcf2a533006c3c8206376ec6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70d55ba7eb7c4a9059293fdacd537f26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73f28ba88fc4788b3333e2a6c933538a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Cardiac-MRI-Foundation-Models-Comprehensive-Visual-Tabular-Representations-for-Whole-Heart-Assessment-and-Beyond"><a href="#Towards-Cardiac-MRI-Foundation-Models-Comprehensive-Visual-Tabular-Representations-for-Whole-Heart-Assessment-and-Beyond" class="headerlink" title="Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular   Representations for Whole-Heart Assessment and Beyond"></a>Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular   Representations for Whole-Heart Assessment and Beyond</h2><p><strong>Authors:Yundi Zhang, Paul Hager, Che Liu, Suprosanna Shit, Chen Chen, Daniel Rueckert, Jiazhen Pan</strong></p>
<p>Cardiac magnetic resonance imaging is the gold standard for non-invasive cardiac assessment, offering rich spatio-temporal views of the cardiac anatomy and physiology. Patient-level health factors, such as demographics, metabolic, and lifestyle, are known to substantially influence cardiovascular health and disease risk, yet remain uncaptured by CMR alone. To holistically understand cardiac health and to enable the best possible interpretation of an individual’s disease risk, CMR and patient-level factors must be jointly exploited within an integrated framework. Recent multi-modal approaches have begun to bridge this gap, yet they often rely on limited spatio-temporal data and focus on isolated clinical tasks, thereby hindering the development of a comprehensive representation for cardiac health evaluation. To overcome these limitations, we introduce ViTa, a step toward foundation models that delivers a comprehensive representation of the heart and a precise interpretation of individual disease risk. Leveraging data from 42,000 UK Biobank participants, ViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling a complete capture of the cardiac cycle. These imaging data are then fused with detailed tabular patient-level factors, enabling context-aware insights. This multi-modal paradigm supports a wide spectrum of downstream tasks, including cardiac phenotype and physiological feature prediction, segmentation, and classification of cardiac and metabolic diseases within a single unified framework. By learning a shared latent representation that bridges rich imaging features and patient context, ViTa moves beyond traditional, task-specific models toward a universal, patient-specific understanding of cardiac health, highlighting its potential to advance clinical utility and scalability in cardiac analysis. </p>
<blockquote>
<p>心脏磁共振成像（Cardiac magnetic resonance imaging，简称CMR）是非侵入性心脏评估的金标准，能够丰富地展现心脏解剖和生理的时空视图。已知患者层面的健康因素，如人口统计学特征、新陈代谢和生活方式，会对心血管健康和疾病风险产生重大影响，但仅靠CMR无法获取这些信息。为了全面了解心脏健康并能够对个人的疾病风险进行最佳解读，必须在综合框架内共同利用CMR和患者层面的因素。最近的多模态方法已经开始弥合这一差距，但它们往往依赖于有限的时空数据，并专注于孤立的临床任务，从而阻碍了心脏健康评估的综合表现的发展。为了克服这些局限性，我们引入了ViTa，这是朝着基础模型迈出的一步，它提供了心脏的全面表征和对个体疾病风险的精确解读。ViTa利用来自42,000名英国生物银行参与者的数据，融合了短轴和长轴的3D+T电影堆栈，能够完整捕捉心脏周期。然后，这些成像数据与详细的表格患者层面因素相结合，提供情境感知的见解。这种多模态范式支持广泛的下游任务，包括心脏表型预测、生理特征预测、分割以及单一统一框架内的代谢疾病分类等。通过学习与丰富的成像特征和患者上下文相关的共享潜在表征，ViTa超越了传统的任务特定模型，朝着具有患者特异性的心脏健康通用理解的方向发展，这突显了其在心脏分析的临床实用性和可扩展性方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13037v2">PDF</a> </p>
<p><strong>Summary</strong><br>     心脏磁共振成像在非侵入性心脏评估中是金标准，但无法获取患者级别的健康因素（如人口统计、代谢和生活方式等）。为了全面了解心脏健康和解释个体疾病风险，必须联合利用心脏磁共振成像和患者级别因素。ViTa模型通过整合心脏磁共振成像数据和患者级别因素，实现了心脏的全面表示和个体疾病风险的精确解释。该模型支持广泛的下游任务，包括心脏表型预测、生理特征预测、分割和心脏代谢疾病的分类等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心脏磁共振成像在非侵入性心脏评估中具有重要地位，但缺乏对患者级别因素的捕捉。</li>
<li>患者级别的健康因素，如人口统计、代谢和生活方式，对心血管健康有重要影响。</li>
<li>ViTa模型整合了心脏磁共振成像数据和患者级别因素，为心脏健康提供了全面的表示。</li>
<li>ViTa模型利用多维时空数据支持广泛的下游任务，包括心脏表型和生理特征预测等。</li>
<li>该模型实现了从传统的任务特定模型向通用患者特定理解心脏的转化。</li>
<li>ViTa模型的潜力在于提高心脏分析的临床实用性和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13037">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f92e8639c8da09d32be8ff4c16497785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4d92fbcd48897fc9c621cb46f4b9bf.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="JWST’s-little-red-dots-an-emerging-population-of-young-low-mass-AGN-cocooned-in-dense-ionized-gas"><a href="#JWST’s-little-red-dots-an-emerging-population-of-young-low-mass-AGN-cocooned-in-dense-ionized-gas" class="headerlink" title="JWST’s little red dots: an emerging population of young, low-mass AGN   cocooned in dense ionized gas"></a>JWST’s little red dots: an emerging population of young, low-mass AGN   cocooned in dense ionized gas</h2><p><strong>Authors:V. Rusakov, D. Watson, G. P. Nikopoulos, G. Brammer, R. Gottumukkala, T. Harvey, K. E. Heintz, R. D. Nielsen, S. A. Sim, A. Sneppen, A. P. Vijayan, N. Adams, D. Austin, C. J. Conselice, C. M. Goolsby, S. Toft, J. Witstok</strong></p>
<p>JWST has uncovered large numbers of compact galaxies at high redshift with broad hydrogen&#x2F;helium lines. These include the enigmatic population known as “little red dots” (LRDs). Their nature is debated, but they are thought to be powered by supermassive black holes (SMBHs) or intense star formation. They exhibit unusual properties for SMBHs, such as black holes that are overmassive for their host galaxies and extremely weak X-ray and radio emission. Using the highest-quality JWST spectra, we show here that the lines are broadened by electron scattering with a narrow intrinsic line core. The data require high electron column densities and compact sizes (light days), which, when coupled with their high luminosities can only be explained by SMBH accretion. The narrow intrinsic cores of the lines imply upper limits on the black hole masses of $10^{5-7}$ $M_{\odot}$, two orders of magnitude lower than previous estimates. These are among the lowest mass SMBHs known at high redshift and suggest that this is a population of young, rapidly growing SMBHs. They are enshrouded in a dense cocoon of ionized gas, probably related to their youth, from which they are accreting close to the Eddington limit. Reprocessed nebular emission from the dense cocoon dominates the optical spectrum, explaining most LRD spectral characteristics and helping to suppress radio and X-ray emission. </p>
<blockquote>
<p>JWST揭示了大量高红移的紧凑星系，具有广泛的氢&#x2F;氦线。其中包括被称为“小红点”（LRDs）的神秘群体。它们的本质尚存争议，但据认为是由超大质量黑洞（SMBHs）或强烈的恒星形成所驱动的。它们展现出SMBHs的不寻常特性，例如宿主星系的超大质量黑洞和极其微弱的X射线和无线电辐射。我们在这里使用高质量的JWST光谱显示，这些线通过电子散射而展宽，具有狭窄的内在线芯。数据需要高电子柱密度和紧凑大小（光日），当与它们的高光度相结合时，只能由SMBH吸积来解释。线条的狭窄内在核心暗示黑洞质量的上限为$10^{5-7}$ $M_{\odot}$，比先前的估计低两个数量级。这些是在高红移处已知的最低质量SMBHs，表明这是一群年轻且快速生长的SMBHs。它们被密集的离子化气体斗篷所包围，可能与它们的年轻有关，它们正在接近爱丁顿极限进行吸积。来自密集斗篷的再加工星云发射物主导了光学光谱，解释了大多数LRD光谱特征并有助于抑制无线电和X射线辐射。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16595v3">PDF</a> 46 pages, 25 figures, 4 tables, submitted to Nature. Updated   spectroscopic data ID convention</p>
<p><strong>Summary</strong></p>
<p>JWST发现大量高红移处的紧凑星系，具有宽阔的氢&#x2F;氦线，包括被称为“小红点”（LRDs）的神秘群体。它们可能由超大质量黑洞（SMBHs）或强烈的恒星形成所驱动，展现出SMBHs的不寻常特性，如相对于宿主星系过于巨大的黑洞，以及极弱的X射线和无线电辐射。利用JWST最高质量的光谱数据，显示这些线路是由电子散射所加宽，具有狭窄的内在线芯。数据需要高电子柱密度和紧凑大小（光日），当与它们的高光度相结合时，只能由SMBH吸积来解释。狭窄的内在线芯对黑洞质量设定了上限，为$10^{5-7}$ $M_{\odot}$，比之前估计低两个数量级。它们是在高红移处已知质量最低的SMBHs之一，表明这是一群年轻的、快速生长的SMBHs。它们被密集的离子化气体包围，可能与它们的年轻有关，正在接近爱丁顿极限进行吸积。重新加工的星云发射从密集的包层中主导了光学光谱，解释了大多数LRD光谱特征，并有助于抑制无线电和X射线辐射。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JWST在高红移处发现了大量紧凑星系，其中含有“小红点”（LRDs）等神秘群体。</li>
<li>LRDs可能由超大质量黑洞（SMBHs）或强烈的恒星形成所驱动。</li>
<li>SMBHs展现出不同于常规的特性，如相对宿主星系过于巨大和弱X射线、无线电辐射。</li>
<li>高质量的JWST光谱数据显示，线路因电子散射而加宽，具有狭窄的内在线芯。</li>
<li>需要高电子柱密度和紧凑大小来解释数据，这暗示了SMBH吸积的可能性。</li>
<li>狭窄的内在线芯表明黑洞质量上限较低，说明这些SMBHs可能年轻且正在迅速增长。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16595">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f650ba3c9ce8c2f6d1a65c88263b035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-670fdc017abcea6ce452cc8cf850829c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d0084f54add418e33dd07f4ed3e2f61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d9830d444922ad1976caaafac8ab071.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis"><a href="#A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis" class="headerlink" title="A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis"></a>A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis</h2><p><strong>Authors:Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif</strong></p>
<p>Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of “positive” and “negative” samples, where positive pairs (e.g., variation of the same image&#x2F;object) are brought together in the embedding space, and negative pairs (e.g., views from different images&#x2F;objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models. </p>
<blockquote>
<p>自监督学习是一种机器学习的方法，它通过学习潜在的模式并从无标签数据中提取辨别特征，从而生成隐式标签，而无需手动标注。对比学习引入了“正样本”和“负样本”的概念，其中正样本对（例如，同一图像&#x2F;对象的变体）被聚集在嵌入空间中，而负样本对（例如，来自不同图像&#x2F;对象的视图）则被推开。这种方法在图像理解和图像文本分析方面取得了显著的改进，而且不需要大量依赖标注数据。在本文中，我们全面讨论了与文本-图像模型相关的对比学习的术语、最新发展以及应用。具体地，我们概述了近年来文本-图像模型中对比学习的方法。其次，我们根据不同的模型结构对方法进行了分类。此外，我们还介绍了讨论了在过程中使用的最新技术的进展，例如图像和文本的预训练任务、架构结构和关键趋势。最后，我们讨论了基于文本-图像模型的自监督对比学习的最新前沿应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11101v2">PDF</a> </p>
<p><strong>Summary</strong><br>     自监督学习通过从非标记数据中学习潜在模式和提取判别特征，生成隐式标签，无需人工标注。对比学习引入了“正样本”和“负样本”的概念，将正样本对拉近嵌入空间，将负样本对推开。此方法在图像理解和文本分析方面显著提高了效果，对标注数据的依赖较小。本文综述了文本图像模型的对比学习术语、最新发展及应用，按模型结构分类，并介绍了最新的技术进展和应用趋势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习是通过学习潜在模式和提取非标记数据的判别特征来生成隐式标签。</li>
<li>对比学习在自监督学习中引入正样本和负样本概念，用于拉近或推开样本对在嵌入空间中的距离。</li>
<li>对比学习方法在图像理解和文本分析方面显示出显著的效果提升。</li>
<li>论文全面讨论了文本图像模型的对比学习术语和最新发展。</li>
<li>论文按模型结构分类了对比学习的方法。</li>
<li>论文介绍了最新的技术进展，包括图像和文本的预训练任务、架构结构和关键趋势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1890949273184fcdfb967c547998d3ae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="An-OpenMind-for-3D-medical-vision-self-supervised-learning"><a href="#An-OpenMind-for-3D-medical-vision-self-supervised-learning" class="headerlink" title="An OpenMind for 3D medical vision self-supervised learning"></a>An OpenMind for 3D medical vision self-supervised learning</h2><p><strong>Authors:Tassilo Wald, Constantin Ulrich, Jonathan Suprijadi, Sebastian Ziegler, Michal Nohel, Robin Peretzke, Gregor Köhler, Klaus H. Maier-Hein</strong></p>
<p>The field of self-supervised learning (SSL) for 3D medical images lacks consistency and standardization. While many methods have been developed, it is impossible to identify the current state-of-the-art, due to i) varying and small pretraining datasets, ii) varying architectures, and iii) being evaluated on differing downstream datasets. In this paper, we bring clarity to this field and lay the foundation for further method advancements through three key contributions: We a) publish the largest publicly available pre-training dataset comprising 114k 3D brain MRI volumes, enabling all practitioners to pre-train on a large-scale dataset. We b) benchmark existing 3D self-supervised learning methods on this dataset for a state-of-the-art CNN and Transformer architecture, clarifying the state of 3D SSL pre-training. Among many findings, we show that pre-trained methods can exceed a strong from-scratch nnU-Net ResEnc-L baseline. Lastly, we c) publish the code of our pre-training and fine-tuning frameworks and provide the pre-trained models created during the benchmarking process to facilitate rapid adoption and reproduction. </p>
<blockquote>
<p>自监督学习（SSL）在3D医学图像领域缺乏一致性和标准化。虽然已开发了许多方法，但由于i）预训练数据集各不相同且规模较小，ii）架构各异，以及iii）在不同的下游数据集上进行评估，因此无法确定当前的最先进方法。在本文中，我们通过三个主要贡献使该领域清晰化，并为进一步的方法发展奠定基础：我们a）发布了最大的公开预训练数据集，包含114k个3D大脑MRI体积，使所有从业者都可以在大规模数据集上进行预训练。我们b）在此数据集上对现有的3D自监督学习方法进行基准测试，针对最先进的人工神经网络（CNN）和Transformer架构，明确了3D SSL预训练的状态。我们发现了许多结果，其中表明预训练方法超过了从头开始的nnU-Net ResEnc-L基线。最后，我们c）发布我们的预训练和微调框架的代码，并提供在基准测试过程中创建的预训练模型，以促进快速采用和复现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17041v2">PDF</a> Pre-Print; Dataset, Benchmark and Codebase available through   <a target="_blank" rel="noopener" href="https://github.com/MIC-DKFZ/nnssl">https://github.com/MIC-DKFZ/nnssl</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对3D医学图像的自监督学习（SSL）领域缺乏一致性和标准化的问题。文章通过三个关键贡献来澄清该领域并为进一步的方法发展奠定基础：发布最大的公开预训练数据集，包含11.4万3D脑部MRI体积，使所有实践者都可以在大规模数据集上进行预训练；在此数据集上对现有3D自监督学习方法进行基准测试，以澄清3D SSL预训练的当前状态；最后，发布预训练和微调框架的代码，并提供在基准测试过程中创建的预训练模型，以促进快速采用和复制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D医学图像自监督学习（SSL）领域缺乏一致性和标准化。</li>
<li>文章发布了包含大量数据的最大公开预训练数据集。</li>
<li>对现有3D自监督学习方法进行了基准测试，发现预训练的方法可以超越从头开始的nnU-Net ResEnc-L基线。</li>
<li>文章提供了预训练和微调框架的代码以及预训练模型，方便其他研究者使用。</li>
<li>文章强调了不同预训练数据集、架构以及下游数据集对评估自监督学习方法的影响。</li>
<li>文章通过对比不同架构（CNN和Transformer）在基准测试中的表现，为未来的研究提供了方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77145e7037ca28f3889ddb592a284be4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae1262078b4a0c900429142766aac1c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9acf4121acfec8e1c3b8212fe5cc23f7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Topograph-An-efficient-Graph-Based-Framework-for-Strictly-Topology-Preserving-Image-Segmentation"><a href="#Topograph-An-efficient-Graph-Based-Framework-for-Strictly-Topology-Preserving-Image-Segmentation" class="headerlink" title="Topograph: An efficient Graph-Based Framework for Strictly Topology   Preserving Image Segmentation"></a>Topograph: An efficient Graph-Based Framework for Strictly Topology   Preserving Image Segmentation</h2><p><strong>Authors:Laurin Lux, Alexander H. Berger, Alexander Weers, Nico Stucki, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold</strong></p>
<p>Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets. Our loss demonstrates state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods. </p>
<blockquote>
<p>拓扑正确性在许多图像分割任务中起着至关重要的作用，然而，大多数网络都是使用像素级损失函数（如Dice系数）进行训练的，忽视了拓扑准确性。现有的拓扑感知方法往往缺乏稳健的拓扑保证，仅限于特定用例，或计算成本高昂。在这项工作中，我们提出了一种新型的图论基础框架，用于拓扑准确的图像分割，既计算高效又通用性强。我们的方法构建了一个组件图，该图完全编码了预测和真实标签的拓扑信息，使我们能够高效地识别出拓扑关键区域，并根据局部邻域信息聚合损失。此外，我们引入了一种严格的拓扑度量标准，捕获预测标签对联合与交集之间的同胚等价性。我们正式证明了我们的方法的拓扑保证，并在二进制和多类别数据集上进行了实证验证。我们的损失函数表现出最佳性能，与持久同构方法相比，损失计算速度提高了五倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03228v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于图的新型拓扑精确图像分割框架，该框架能够利用拓扑信息进行高效计算并且具有广泛的应用性。通过构建组件图来全面编码预测和真实值的拓扑信息，该方法能够迅速识别出拓扑关键区域并根据局部邻域信息聚合损失。此外，引入了一种严格的拓扑度量标准，捕获预测标签对并集和交集之间的同胚等价性。该方法在二分类和多分类数据集上均表现出优异性能，且相较于持久同源性方法，损失计算速度提高了五倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>拓扑正确性在图像分割任务中起关键作用，但大多数网络使用像素级损失函数进行训练，忽略了拓扑准确性。</li>
<li>现有拓扑感知方法常常缺乏稳健的拓扑保证，仅限于特定用例，或计算成本高昂。</li>
<li>本文提出了一种新型的图基框架，用于拓扑准确的图像分割，该框架既计算高效又通用性强。</li>
<li>通过构建组件图，该方法能够全面编码预测和真实值的拓扑信息。</li>
<li>引入了一种严格的拓扑度量标准，用于捕获预测标签对的拓扑等价性。</li>
<li>该方法具有形式化的拓扑保证，并在二分类和多分类数据集上进行了实证验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03228">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62d43ee8a9cda13c3b203003c7bbd07e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3de696d0a7f06aee9a527f5f383ac089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5562bf891fc548562e732fcbdb7a09c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ea333d2705156b61a788def2d615e48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-622cfcf7d83fc9b1295d9da35a94fb83.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MambaMIM-Pre-training-Mamba-with-State-Space-Token-Interpolation-and-its-Application-to-Medical-Image-Segmentation"><a href="#MambaMIM-Pre-training-Mamba-with-State-Space-Token-Interpolation-and-its-Application-to-Medical-Image-Segmentation" class="headerlink" title="MambaMIM: Pre-training Mamba with State Space Token Interpolation and   its Application to Medical Image Segmentation"></a>MambaMIM: Pre-training Mamba with State Space Token Interpolation and   its Application to Medical Image Segmentation</h2><p><strong>Authors:Fenghe Tang, Bingkun Nian, Yingtai Li, Zihang Jiang, Jie Yang, Wei Liu, S. Kevin Zhou</strong></p>
<p>Recently, the state space model Mamba has demonstrated efficient long-sequence modeling capabilities, particularly for addressing long-sequence visual tasks in 3D medical imaging. However, existing generative self-supervised learning methods have not yet fully unleashed Mamba’s potential for handling long-range dependencies because they overlook the inherent causal properties of state space sequences in masked modeling. To address this challenge, we propose a general-purpose pre-training framework called MambaMIM, a masked image modeling method based on a novel TOKen-Interpolation strategy (TOKI) for the selective structure state space sequence, which learns causal relationships of state space within the masked sequence. Further, MambaMIM introduces a bottom-up 3D hybrid masking strategy to maintain a masking consistency across different architectures and can be used on any single or hybrid Mamba architecture to enhance its multi-scale and long-range representation capability. We pre-train MambaMIM on a large-scale dataset of 6.8K CT scans and evaluate its performance across eight public medical segmentation benchmarks. Extensive downstream experiments reveal the feasibility and advancement of using Mamba for medical image pre-training. In particular, when we apply the MambaMIM to a customized architecture that hybridizes MedNeXt and Vision Mamba, we consistently obtain the state-of-the-art segmentation performance. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/MambaMIM">https://github.com/FengheTan9/MambaMIM</a>. </p>
<blockquote>
<p>最近，状态空间模型Mamba在长时间序列建模方面表现出了高效的性能，特别是在处理三维医学影像中的长时间序列视觉任务方面。然而，现有的生成式自监督学习方法尚未充分发挥Mamba在处理长距离依赖关系方面的潜力，因为它们忽略了状态空间序列在掩模建模中的固有因果特性。为了解决这一挑战，我们提出了一种通用的预训练框架，称为MambaMIM。这是一种基于新型TOKEN插值策略（TOKI）的掩模图像建模方法，用于选择性结构状态空间序列，学习掩模序列中状态空间的因果关系。此外，MambaMIM引入了一种自下而上的三维混合掩模策略，以在不同架构之间保持掩模的一致性，并且可以在任何单一或混合Mamba架构上使用，以增强其多尺度和长距离表示能力。我们在包含六千八百个CT扫描的大规模数据集上预训练了MambaMIM，并在八个公共医学分割基准上评估了其性能。大量的下游实验验证了使用Mamba进行医学图像预训练的可行性和先进性。特别是当我们将MambaMIM应用于混合MedNeXt和视觉Mamba的定制架构时，我们始终获得最先进的分割性能。代码可从以下网站获取：<a target="_blank" rel="noopener" href="https://github.com/FengheTan9/MambaMIM">https://github.com/FengheTan9/MambaMIM</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.08070v2">PDF</a> Accepted by Medical Image Analysis. Code:   <a target="_blank" rel="noopener" href="https://github.com/FengheTan9/MambaMIM">https://github.com/FengheTan9/MambaMIM</a></p>
<p><strong>摘要</strong></p>
<p>Mamba模型具有高效的序列建模能力，特别是处理长期序列视觉任务在3D医学影像方面表现优异。但现有的生成式自监督学习方法未能充分发挥Mamba在处理长期依赖关系方面的潜力，因为它们忽视了状态空间序列的内在因果特性在遮掩建模中的作用。为此，我们提出了一种通用的预训练框架MambaMIM，这是一种基于新型Token插值策略（TOKI）的遮掩图像建模方法，用于选择性结构状态空间序列，学习遮掩序列中状态空间的因果关系。此外，MambaMIM引入了一种自下而上的3D混合遮掩策略，以保持不同架构之间的遮掩一致性，可应用于任何单一或混合Mamba架构，以增强其多尺度和长期表示能力。我们在包含大型CT扫描数据集上预训练了MambaMIM，并在八个公共医学分割基准上评估了其性能。下游实验表明使用Mamba进行医学图像预训练的可行性和先进性。特别是将MambaMIM应用于混合MedNeXt和Vision Mamba的定制架构时，我们获得了领先的分割性能。相关代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/FengheTan9/MambaMIM">https://github.com/FengheTan9/MambaMIM</a>。</p>
<pre><code>**关键见解**

1. Mamba模型在处理和解决长期序列视觉任务方面表现出强大的能力，特别是在医学成像领域。
2. 现有的生成式自监督学习方法未充分利用Mamba模型的长期依赖处理潜力，原因是忽视了其状态空间序列的因果特性。
3. 提出了一种新的预训练框架MambaMIM，结合了遮掩图像建模和一种新型的Token插值策略（TOKI）。这有助于学习状态空间序列中的因果关系。
4. MambaMIM引入了混合遮掩策略，以提高模型在多尺度和长期表示方面的能力，同时适用于多种架构。
5. 在大规模医学图像数据集上进行了预训练实验，验证了MambaMIM的有效性。
6. 在多个公共医学分割基准测试中，MambaMIM展现了卓越的性能。特别是当与其他架构结合时，其表现尤为突出。
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.08070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e546a179e2d49b5095f6518f38dbb5d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f657e4071acf014b3f096ccdeafde015.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec9a7dcec5dc00db165a3da32df15c29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-621e4cbe8f7e990b5f7441c6e916a552.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-95e95cbe0fe9d96e215934157a007fad.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-22  ChatNekoHacker Real-Time Fan Engagement with Conversational Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2b1370e32563de59b2407f6ef45fd0a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-22  Decoding Vision Transformers the Diffusion Steering Lens
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
