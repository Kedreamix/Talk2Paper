<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bbed99f1971aa9a935920c070ed7910e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-22-更新"><a href="#2025-04-22-更新" class="headerlink" title="2025-04-22 更新"></a>2025-04-22 更新</h1><h2 id="Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model"><a href="#Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model" class="headerlink" title="Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?"></a>Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?</h2><p><strong>Authors:Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models’ capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$&#x3D;1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models’ sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model’s output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: <a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a> </p>
<blockquote>
<p>强化学习与可验证奖励（RLVR）最近在增强大型语言模型（LLM）的推理能力方面取得了显著的成功，特别是在数学和编程任务中。人们普遍认为，RLVR使LLMs能够持续自我改进，从而获取超过相应基础模型的全新推理能力。然而，在这项研究中，我们通过测量pass@\textit{k}指标，并设置较大的\textit{k}值，来重新审视这一假设，以探索模型在不同模型家族和基准测试中的推理能力边界。令人惊讶的是，事实上，强化学习并没有引发根本性的新推理模式。虽然使用RL训练的模型在较小的k值（例如k&#x3D;1）情况下，其表现优于基础模型，但在较大的k值情况下，基础模型的pass@\textit{k}分数可以与使用RL的模型相当甚至更高。这表明RL训练的模型产生的推理路径已经包含在基础模型的采样分布中，即RL训练的模型中表现出的大多数推理能力已经被基础模型所获得。进一步的分析表明，RL训练通过使模型的输出分布偏向于更可能产生奖励的路径，从而提高了性能，因此更有效率地采样正确的回应。但这也会导致与基础模型相比，其推理能力边界变窄。在使用RLVR进行视觉推理任务时也观察到了类似的结果。此外，我们发现蒸馏法可以真正地将新知识引入模型中，这与RLVR不同。这些发现突出了RLVR在提升LLM推理能力方面的关键局限性，这要求我们从根本上重新思考RL训练在推理LLM中的作用以及是否需要更好的范式。项目页面：<a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13837v1">PDF</a> 24 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>强化学习通过可验证奖励（RLVR）在提升大型语言模型（LLM）的推理能力方面取得了显著的成功，特别是在数学和编程任务中。然而，本研究通过测量pass@k指标，对RLVR是否能够促使LLM持续自我提升并获取新的推理能力进行了重新评估。研究结果显示，强化学习并未激发新的推理模式，而且在较大的k值下，基础模型的性能可与RL训练的模型相媲美甚至更高。这表明RL训练的模型的推理路径已包含在基础模型的采样分布中。进一步的分析表明，RL训练通过使模型输出分布偏向于更可能产生奖励的路径，提高了性能，但这也导致了与基础模型相比更狭窄的推理能力边界。类似的结果在视觉推理任务中也同样观察到。此外，研究发现蒸馏可以真正地将新知识引入模型，与RLVR不同。这些发现强调了RLVR在提升LLM推理能力方面的局限性，并需要我们对RL训练在LLM推理中的影响以及是否需要更好的范式进行根本性的重新思考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习通过可验证奖励（RLVR）增强了LLM在数学和编程任务中的推理能力。</li>
<li>通过测量pass@k指标，研究发现强化学习并未激发新的推理模式。</li>
<li>在较大的k值下，基础模型的性能与RL训练的模型相当或更好。</li>
<li>RL训练的模型的推理路径已包含在基础模型的采样分布中。</li>
<li>RL训练提高了模型性能，但也可能导致更狭窄的推理能力边界。</li>
<li>在视觉推理任务中也观察到了类似的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13837">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1145fa10a074c5138153deb614a9a7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c356693220910067cc819f4d5fdaea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e6f577c8731eed685abeaa9d193db2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da3f9fac7a8a67e71f5c7441b7c8984a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MIG-Automatic-Data-Selection-for-Instruction-Tuning-by-Maximizing-Information-Gain-in-Semantic-Space"><a href="#MIG-Automatic-Data-Selection-for-Instruction-Tuning-by-Maximizing-Information-Gain-in-Semantic-Space" class="headerlink" title="MIG: Automatic Data Selection for Instruction Tuning by Maximizing   Information Gain in Semantic Space"></a>MIG: Automatic Data Selection for Instruction Tuning by Maximizing   Information Gain in Semantic Space</h2><p><strong>Authors:Yicheng Chen, Yining Li, Kai Hu, Zerun Ma, Haochen Ye, Kai Chen</strong></p>
<p>Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to \textbf{M}aximize the \textbf{I}nformation \textbf{G}ain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73% on AlpacaEval and +6.89% on Wildbench. </p>
<blockquote>
<p>数据质量和多样性是构建有效指令微调数据集的关键。随着开源指令微调数据集的可获取性不断增加，从大量数据中自动选择高质量和多样化的子集具有优势。现有方法通常优先考虑实例质量，并使用启发式规则来维持多样性。然而，对整个集合缺乏全面观察往往会导致结果不佳。此外，启发式规则通常关注嵌入空间内的距离或聚类，这无法准确捕获复杂指令在语义空间中的意图。为了弥补这一差距，我们提出了一种统一的方法，用于量化数据集的信息内容。该方法通过构建标签图来建模语义空间，并基于图中信息的分布来量化多样性。基于这种度量，我们进一步引入了一种有效的采样方法，该方法通过迭代选择数据样本，以最大化语义空间中的信息增益（MIG）。在各种数据集和基准模型上的实验表明，MIG持续优于最新方法。值得注意的是，使用MIG采样的5% Tulu3数据微调后的模型，其性能与在完整数据集上训练的官方SFT模型相当，在AlpacaEval上的性能提升+5.73%，在Wildbench上的性能提升+6.89%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13835v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>数据质量和多样性是构建有效指令微调数据集的关键。随着开源指令微调数据集的不断增多，从海量数据中自动选择高质量、多样化的子集具有优势。现有方法通常优先考虑实例质量，并使用启发式规则来维持多样性。然而，缺乏对整个集合的全面视角往往导致结果不尽如人意。此外，启发式规则一般关注嵌入空间内的距离或聚类，无法准确捕捉复杂指令的语义空间意图。为了弥补这一差距，我们提出了一种统一的方法，对数据集的信息内容进行量化。该方法通过构建标签图来模拟语义空间，并基于图中信息的分布来量化多样性。基于这种度量，我们进一步引入了一种有效的采样方法，该方法通过迭代选择数据样本以最大化语义空间的信息增益（MIG）。在各种数据集和基础模型上的实验表明，MIG持续优于最新方法。特别是，使用MIG采样的Tulu3数据的5%对模型进行微调，其性能与在完整数据集上训练的官方SFT模型相当，在AlpacaEval上提高了+5.73%，在Wildbench上提高了+6.89%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>数据质量和多样性对于构建有效的指令微调数据集至关重要。</li>
<li>现有方法因缺乏对整体数据集合的全面视角而导致结果不理想。</li>
<li>启发式规则通常关注嵌入空间内的距离或聚类，忽略了复杂指令的语义空间意图。</li>
<li>提出了一种新的方法，通过构建标签图来量化数据集的信息内容，并基于信息分布来量化多样性。</li>
<li>引入了一种有效的采样方法，即最大化信息增益（MIG），以提高语义空间的数据选择效果。</li>
<li>实验表明，MIG方法在多种数据集和基础模型上的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13835">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-090e3ccfe70f19b78e0d546fce3aeb02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6203bb08d9ef9854fb0843b74cda60e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cc5e27abb0785de1f5bc03f3eb68fae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-AI-Act-II-Test-Time-Scaling-Drives-Cognition-Engineering"><a href="#Generative-AI-Act-II-Test-Time-Scaling-Drives-Cognition-Engineering" class="headerlink" title="Generative AI Act II: Test Time Scaling Drives Cognition Engineering"></a>Generative AI Act II: Test Time Scaling Drives Cognition Engineering</h2><p><strong>Authors:Shijie Xia, Yiwei Qin, Xuefeng Li, Yan Ma, Run-Ze Fan, Steffi Chern, Haoyang Zou, Fan Zhou, Xiangkun Hu, Jiahe Jin, Yanheng He, Yixin Ye, Yixiu Liu, Pengfei Liu</strong></p>
<p>The first generation of Large Language Models - what might be called “Act I” of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of “Act II” (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI’s second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/cognition-engineering">https://github.com/GAIR-NLP/cognition-engineering</a> </p>
<blockquote>
<p>第一代大型语言模型——可以被称为生成式人工智能（2020年至2023年）“第一幕”——通过大规模参数和数据扩展取得了显著的成功，但在知识延迟、浅层推理和认知过程受限等方面表现出根本性局限。在这一时期，提示工程作为我们与人工智能的主要界面应运而生，它通过自然语言实现了对话级的交流。我们现在见证了“第二幕”（从现开始到当前）——通过测试时的缩放技术从知识检索系统转变为思维构造引擎的时代来临。这一新范式通过建立基于语言的思维与人工智能建立了一种心智层面的联系。在本文中，我们明确了认知工程的概念基础，并解释了为什么这一刻对其发展至关重要。我们通过全面的教程和优化后的实现系统地剖析了这些高级方法，使认知工程民主化，让每位从业者都能参与人工智能的第二幕。关于测试时缩放的相关论文集合会定期更新在GitHub仓库中：<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/cognition-engineering">https://github.com/GAIR-NLP/cognition-engineering</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13828v1">PDF</a> </p>
<p><strong>Summary</strong><br>大型语言模型在“行动一阶段”（即前阶段大型语言模型）通过大量的参数和数据规模取得了显著的成功，但展现出了一些根本性限制，如知识延迟、浅层推理和认知过程受限等。现在，我们见证了“行动二阶段”（即当前阶段）的出现，在这一阶段，模型正从隐性空间中的知识检索系统过渡到测试时的思维构建引擎。这标志着通过与语言为基础的思维进行思维级别的连接来实现认知工程的理念发展和普及。这一阶段还带来了综合性教程和优化的实现方式，为大众参与认知工程提供了民主化的机会。GitHub仓库提供了关于测试时规模的最新论文集合。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型在初期阶段通过大规模参数和数据取得了显著成功。</li>
<li>第一阶段的大型语言模型存在知识延迟、浅层推理和认知过程受限等根本性限制。</li>
<li>第二阶段的大型语言模型正在从知识检索系统过渡到思维构建引擎。</li>
<li>这一过渡代表着与AI的思维方式级别的连接。这一新阶段标志着认知工程的发展，使AI能够进行思维级别交互成为可能。 </li>
<li>提供了综合教程和优化实现方式，旨在民主化认知工程领域的机会，让更多实践者参与进来。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c65054c31174da7001c1cb4e39ecd56b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbed99f1971aa9a935920c070ed7910e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-076aa713a9ea4b1edbd9677e7b854649.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ad5cabbcf3c6fe0a4f415e7cc2610f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Analyzing-LLMs’-Knowledge-Boundary-Cognition-Across-Languages-Through-the-Lens-of-Internal-Representations"><a href="#Analyzing-LLMs’-Knowledge-Boundary-Cognition-Across-Languages-Through-the-Lens-of-Internal-Representations" class="headerlink" title="Analyzing LLMs’ Knowledge Boundary Cognition Across Languages Through   the Lens of Internal Representations"></a>Analyzing LLMs’ Knowledge Boundary Cognition Across Languages Through   the Lens of Internal Representations</h2><p><strong>Authors:Chenghao Xiao, Hou Pong Chan, Hao Zhang, Mahani Aljunied, Lidong Bing, Noura Al Moubayed, Yu Rong</strong></p>
<p>While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs’ perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs’ recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries">https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries</a>. </p>
<blockquote>
<p>了解大型语言模型的知识边界对于防止其产生幻觉至关重要，但关于大型语言模型知识边界的研究主要集中在英语上。在这项工作中，我们首次提出分析大型语言模型如何识别不同语言的知识边界，通过探索其在处理多种语言的已知和未知问题时的内部表征来进行研究。我们的实证研究揭示了三个关键发现：1）大型语言模型对知识边界的认知在不同语言的中间到中上层次中编码。2）知识边界感知的语言差异遵循线性结构，这激励我们提出了一种无需训练的对齐方法，该方法可以有效地在不同语言中转移知识边界感知能力，从而有助于降低低资源语言中的幻觉风险；3）在双语问题对翻译上进行微调，可以进一步提高大型语言模型在不同语言中的知识边界识别能力。由于缺乏跨语言知识边界分析的标准测试平台，我们构建了一个多语言评估套件，包含三种具有代表性的知识边界数据类型。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries上公开获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13816v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文首次研究了LLM在多语言环境下如何识别知识边界。通过对不同语言中的已知和未知问题进行处理，发现LLM的知识边界感知编码于中间至中上层次。语言间的知识边界感知差异呈现线性结构，并提出一种无需训练的对齐方法，以提高跨语言的知识边界感知能力，从而降低在低资源语言中的幻觉风险。通过微调双语问题对翻译，可进一步提高LLM的跨语言知识边界识别能力。缺少跨语言知识边界分析的标准测试平台，因此我们构建了包含三种代表性知识边界数据的多语言评估套件。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM的知识边界识别研究主要集中在英语环境下，但该研究首次探索了跨语言环境下的知识边界识别。</li>
<li>LLM的知识边界感知编码存在于模型中间至中上层次，这一发现揭示了模型内部如何处理知识边界。</li>
<li>不同语言间的知识边界感知差异呈现线性结构，这为跨语言知识边界感知的对齐提供了基础。</li>
<li>提出了一种无需训练的对齐方法，以提高LLM在不同语言环境下的知识边界感知能力。此方法有助于降低在低资源语言环境中的幻觉风险。</li>
<li>通过微调双语问题对翻译，能够进一步提高LLM在跨语言环境下的知识边界识别效果。</li>
<li>当前缺乏跨语言知识边界分析的标准测试平台，因此构建了包含多种类型数据的多语言评估套件。该套件可用于评估LLM在跨语言环境下的知识边界识别能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13816">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c79af8087065fdbcec145c0a64f900ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81a30b9fd8216afe3d2d2a15d8d1a29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b7cf8a973a128f0644f915fd8e6ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02333f787215458f5cd040ee16e40e21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-115bafcb6f1388a18b16d0da7cec3bf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b38779230f78d698df1b6f9f494178cf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Transformer-Encoder-and-Multi-features-Time2Vec-for-Financial-Prediction"><a href="#Transformer-Encoder-and-Multi-features-Time2Vec-for-Financial-Prediction" class="headerlink" title="Transformer Encoder and Multi-features Time2Vec for Financial Prediction"></a>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</h2><p><strong>Authors:Nguyen Kim Hai Bui, Nguyen Duy Chien, Péter Kovács, Gergő Bognár</strong></p>
<p>Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the model’s ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.   In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices. </p>
<blockquote>
<p>金融预测是一项复杂且具有挑战性的时间序列分析和信号处理任务，要求对短期波动和长期时间依赖进行建模。Transformer主要在自然语言处理中使用注意力机制取得了显著的成功，这也影响了时间序列社区。捕捉短期和长期依赖的能力有助于理解金融市场并识别价格模式，从而导致Transformer在股票预测中的成功应用。然而，以前的研究主要集中在个别特征和单一预测上，这限制了模型了解更广泛市场趋势的能力。实际上，在诸如金融和技术等领域内，属于同一行业的公司通常表现出相关的股票价格变动。在本文中，我们通过整合Time2Vec与Transformer模型的编码器，开发了一种新型神经网络架构。基于对不同市场的研究，我们提出了一种新型的相关性特征选择方法。通过全面微调多个超参数，我们对我们的结果与基准模型进行了比较分析。我们得出结论，我们的方法优于其他最先进的编码方法，如位置编码，我们还得出结论，选择相关性特征提高了预测多个股票价格的准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13801v1">PDF</a> 5 pages, currently under review at Eusipco 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了在金融预测领域，运用Transformer模型结合Time2Vec技术的重要性。文章提出了一种新的神经网络架构，并研究了一种新的相关性特征选择方法，旨在更好地捕捉金融市场的整体趋势和个股价格模式。实验结果显示，该方法相较于其他编码方法具有更高的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融预测是一项复杂的任务，需要同时模拟短期波动和长期时间依赖性。</li>
<li>Transformer模型因其能够捕捉短期和长期依赖性的能力，在自然语言处理和金融预测领域取得了显著的成功。</li>
<li>之前的研究主要关注个体特征和单一预测，这限制了模型理解市场趋势的能力。</li>
<li>本文提出了一种新的神经网络架构，整合了Time2Vec和Transformer模型的编码器。</li>
<li>提出了一种新的相关性特征选择方法，基于对不同市场的研究。</li>
<li>通过综合调整多个超参数，本文的方法被证明优于其他先进的编码方法，如位置编码。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7e876e4a69bbb35ec5f546d5e0ec2bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbfde11150c9c60bf16c772881372c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b921bedec15e1b1738c83a46a9a2b570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5494954a66e19242206d49cadf45187.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d65a7ab8b64cf6d3cf24d66e111a0ad2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BadApex-Backdoor-Attack-Based-on-Adaptive-Optimization-Mechanism-of-Black-box-Large-Language-Models"><a href="#BadApex-Backdoor-Attack-Based-on-Adaptive-Optimization-Mechanism-of-Black-box-Large-Language-Models" class="headerlink" title="BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of   Black-box Large Language Models"></a>BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of   Black-box Large Language Models</h2><p><strong>Authors:Zhengxian Wu, Juan Wen, Wanli Peng, Ziwei Zhang, Yinghan Zhou, Yiming Xue</strong></p>
<p>Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%. </p>
<blockquote>
<p>先前基于插入和改述的后门技术在攻击效果上取得了巨大成功，但它们忽视了文本质量和带毒文本与清洁文本之间的语义一致性。虽然最近的研究引入了大型语言模型来生成带毒文本，并提高了隐蔽性、语义一致性和文本质量，但它们的手工提示依赖于专家经验，在防御后面临着提示适应性和攻击性能方面的重大挑战。在本文中，我们提出了一种基于大型语言模型的自适应优化机制的新型后门攻击（BadApex），它利用一个大型黑盒语言模型通过改进后的提示生成带毒文本。具体来说，我们设计了一个自适应优化机制来迭代地改进初始提示，使用生成和修改代理。生成代理基于初始提示生成带毒文本。然后，修改代理评估带毒文本的质量并改进新的提示。经过多次迭代上述过程后，使用改进后的提示通过大型语言模型生成带毒文本。我们在三个数据集上进行了实验，采用了六种后门攻击和两种防御方法。大量的实验结果表明，BadApex显著优于最先进的攻击方法。它提高了提示适应性、语义一致性和文本质量。此外，当应用两种防御方法时，平均攻击成功率（ASR）仍然高达96.75%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13775v1">PDF</a> 16 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于黑盒大型语言模型的自适应优化机制的全新后门攻击方法（BadApex）。该方法利用黑盒LLM通过精细化的提示生成毒化文本，采用自适应优化机制对初始提示进行迭代优化，提高了攻击效果、语义一致性和文本质量。实验证明，BadApex显著优于现有攻击方法，并在应用两种防御方法后，平均攻击成功率仍高达96.75%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BadApex提出了一种基于黑盒大型语言模型的自适应优化机制，用于生成毒化文本。</li>
<li>该方法通过迭代优化初始提示，提高了攻击效果、语义一致性和文本质量。</li>
<li>BadApex显著优于现有攻击方法。</li>
<li>BadApex能够在应用两种防御方法后，仍保持较高的攻击成功率。</li>
<li>生成代理根据初始提示生成毒化文本。</li>
<li>修改代理评估毒化文本的质量并优化新提示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-78d37a1eddd890176d94f01885907360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1e52b8905ee8feae5b7b8582f515c8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9259fa3ae90e67c296026e73b7dd16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39620ff84947d85bf21cc564dd0c939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f70f0203fce33eb75430285a0c215c6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DP2Unlearning-An-Efficient-and-Guaranteed-Unlearning-Framework-for-LLMs"><a href="#DP2Unlearning-An-Efficient-and-Guaranteed-Unlearning-Framework-for-LLMs" class="headerlink" title="DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs"></a>DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs</h2><p><strong>Authors:Tamim Al Mahmud, Najeeb Jebreel, Josep Domingo-Ferrer, David Sanchez</strong></p>
<p>Large language models (LLMs) have recently revolutionized language processing tasks but have also brought ethical and legal issues. LLMs have a tendency to memorize potentially private or copyrighted information present in the training data, which might then be delivered to end users at inference time. When this happens, a naive solution is to retrain the model from scratch after excluding the undesired data. Although this guarantees that the target data have been forgotten, it is also prohibitively expensive for LLMs. Approximate unlearning offers a more efficient alternative, as it consists of ex post modifications of the trained model itself to prevent undesirable results, but it lacks forgetting guarantees because it relies solely on empirical evidence. In this work, we present DP2Unlearning, a novel LLM unlearning framework that offers formal forgetting guarantees at a significantly lower cost than retraining from scratch on the data to be retained. DP2Unlearning involves training LLMs on textual data protected using {\epsilon}-differential privacy (DP), which later enables efficient unlearning with the guarantees against disclosure associated with the chosen {\epsilon}. Our experiments demonstrate that DP2Unlearning achieves similar model performance post-unlearning, compared to an LLM retraining from scratch on retained data – the gold standard exact unlearning – but at approximately half the unlearning cost. In addition, with a reasonable computational cost, it outperforms approximate unlearning methods at both preserving the utility of the model post-unlearning and effectively forgetting the targeted information. </p>
<blockquote>
<p>大型语言模型（LLM）虽然为语言处理任务带来了革命性的变革，但也引发了伦理和法律问题。LLM倾向于记忆训练数据中存在的潜在私密或版权信息，在推理阶段可能会将这些信息提供给最终用户。在这种情况下，一种简单的解决方案是在排除不需要的数据后从头开始重新训练模型。虽然这可以保证目标数据已被遗忘，但对于大型语言模型来说，成本却极为高昂。近似遗忘提供了一种更高效的替代方案，它包括对已训练模型本身的后续修改，以防止出现不理想的结果，但它缺乏遗忘保证，因为它只依赖于经验证据。在这项工作中，我们提出了DP2Unlearning，这是一种新型的大型语言模型遗忘框架，它提供了正式的遗忘保证，并且在要保留的数据上从头开始重新训练的成本大大降低。DP2Unlearning涉及在受ε-差分隐私保护的文本数据上训练大型语言模型（DP），这后来使得可以通过与所选ε相关的保证进行高效的遗忘。我们的实验表明，DP2Unlearning在遗忘后的模型性能与在保留数据上从头开始重新训练的大型语言模型（黄金标准的精确遗忘）相似，但遗忘成本降低了大约一半。此外，它以合理的计算成本超越了近似遗忘方法在保持模型实用性和有效遗忘目标信息方面的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13774v1">PDF</a> 49 pages</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理语言任务的同时，引发了伦理和法律问题。LLM有记忆训练数据中潜在私密或版权信息的趋势，可能在推理阶段将这些信息提供给用户。为解决这个问题，有人选择重新训练模型以排除不需要的数据，虽然这能确保遗忘目标数据，但对大型语言模型而言成本过高。近似遗忘则提供了一种更高效的替代方案，它通过对已训练模型本身的后期修改来防止不理想的结果，但缺乏遗忘保证，因为它只依赖经验证据。本研究提出了DP2Unlearning，一种新型LLM遗忘框架，以较低的成本提供了形式化的遗忘保证。DP2Unlearning通过训练受ε-差分隐私保护的文本数据来训练LLM，之后可通过所选的ε对抗披露来实现高效遗忘。实验表明，DP2Unlearning在遗忘后的模型性能与从保留数据开始重新训练的LLM相似，但成本大约是重新训练的一半。此外，它以合理的计算成本超越了近似遗忘方法，既保留了模型的实用性，又有效地忘记了目标信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在处理语言任务时面临伦理和法律问题，尤其是关于记忆和提供潜在私密或版权信息的风险。</li>
<li>重训模型以排除不需要的数据是确保遗忘的方法，但对大型语言模型而言成本过高。</li>
<li>近似遗忘是一种更高效的替代方案，但缺乏遗忘保证。</li>
<li>DP2Unlearning是一种新型LLM遗忘框架，结合了ε-差分隐私保护来训练模型，实现了高效遗忘且提供了形式化的遗忘保证。</li>
<li>DP2Unlearning的实验结果表明，其在遗忘后的模型性能与重新训练的LLM相似，但成本更低。</li>
<li>DP2Unlearning在保留模型实用性的同时，有效地忘记了目标信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a4749d934b89d4cfc3ac6997730d5a9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Decoding-Vision-Transformers-the-Diffusion-Steering-Lens"><a href="#Decoding-Vision-Transformers-the-Diffusion-Steering-Lens" class="headerlink" title="Decoding Vision Transformers: the Diffusion Steering Lens"></a>Decoding Vision Transformers: the Diffusion Steering Lens</h2><p><strong>Authors:Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai</strong></p>
<p>Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. </p>
<blockquote>
<p>Logit Lens 是广泛应用于基于 transformer 的语言模型的机械解释性方法，通过将内部表示投影到输出词汇空间来分析它们在各层如何演变。虽然将 Logit Lens 应用于视觉 transformer（ViTs）在技术上很直接，但其直接使用在捕捉视觉表示的丰富性方面存在局限性。基于Toker等人（2024）的工作，他们引入了Diffusion Lens来可视化文本编码器中文字到图像扩散模型的中间表示，我们证明虽然Diffusion Lens可以有效地可视化图像编码器的残差流表示，但它无法捕捉单个子模块的直接贡献。为了克服这一局限性，我们提出了<strong>Diffusion Steering Lens（DSL）</strong>，这是一种新型、无需训练的方法，可以引导子模块输出并修补随后的间接贡献。我们通过干预研究验证了我们的方法，表明DSL为ViTs的内部处理提供了直观和可靠的解释。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13763v1">PDF</a> 12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on   Mechanistic Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>Logit Lens方法广泛应用于解释基于转换器的语言模型，但直接应用于视觉转换器（ViTs）时存在局限性。为此，本文提出一种新型的无训练方法——Diffusion Steering Lens（DSL），旨在可视化视觉转换器的内部处理过程，通过干预性研究验证了其直观性和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Logit Lens广泛应用于解释基于转换器的语言模型，通过将内部表示投影到输出词汇空间来进行分析。</li>
<li>虽然Logit Lens直接应用于视觉转换器（ViTs）技术上很直观，但在捕捉丰富的视觉表示方面存在局限性。</li>
<li>Diffusion Lens虽然能有效可视化图像编码器的残差流表示，但无法捕捉单个子模块的直接影响。</li>
<li>针对上述挑战，提出了新型的无训练方法——Diffusion Steering Lens（DSL）。</li>
<li>DSL通过干预性研究验证了其直观性和可靠性，能够更准确地解释ViTs的内部处理过程。</li>
<li>DSL方法可以直观地可视化各个子模块的输出以及后续的间接贡献。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b25fc11f3869a7624d2b8a6506569ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bd47b1cfa834935a674babcfe93332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1568148bc789594619a449bd8dc24b24.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Controlled-Territory-and-Conflict-Tracking-CONTACT-Geo-Mapping-Occupied-Territory-from-Open-Source-Intelligence"><a href="#Controlled-Territory-and-Conflict-Tracking-CONTACT-Geo-Mapping-Occupied-Territory-from-Open-Source-Intelligence" class="headerlink" title="Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping   Occupied Territory from Open Source Intelligence"></a>Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping   Occupied Territory from Open Source Intelligence</h2><p><strong>Authors:Paul K. Mandal, Cole Leo, Connor Hurley</strong></p>
<p>Open-source intelligence provides a stream of unstructured textual data that can inform assessments of territorial control. We present CONTACT, a framework for territorial control prediction using large language models (LLMs) and minimal supervision. We evaluate two approaches: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. Our model is trained on a small hand-labeled dataset of news articles covering ISIS activity in Syria and Iraq, using prompt-conditioned extraction of control-relevant signals such as military operations, casualties, and location references. We show that the BLOOMZ-based model outperforms the SetFit baseline, and that prompt-based supervision improves generalization in low-resource settings. CONTACT demonstrates that LLMs fine-tuned using few-shot methods can reduce annotation burdens and support structured inference from open-ended OSINT streams. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/PaulKMandal/CONTACT/">https://github.com/PaulKMandal/CONTACT/</a>. </p>
<blockquote>
<p>开源情报（OSINT）提供了一系列非结构化文本数据，这些数据可以为对领土控制的评估提供信息。我们提出了CONTACT，这是一个使用大型语言模型（LLM）进行领土控制预测的最小监督框架。我们评估了两种方法：SetFit，一种基于嵌入的少量样本分类器，以及应用于BLOOMZ-560m的多语言生成LLM的提示调整方法。我们的模型是在一个关于叙利亚和伊拉克伊斯兰国活动的手工标注新闻数据集上进行训练的，使用提示条件下的控制相关信号的提取，如军事行动、伤亡和位置引用。我们表明，基于BLOOMZ的模型优于SetFit基线，并且基于提示的监督改进了低资源环境中的泛化能力。CONTACT证明，使用少量样本方法微调的大型语言模型（LLM）可以减轻标注负担并支持从开放的OSINT流中进行结构化推断。我们的代码可通过 <a target="_blank" rel="noopener" href="https://github.com/PaulKMandal/CONTACT/">https://github.com/PaulKMandal/CONTACT/</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13730v1">PDF</a> 7 pages, 1 figure, 1 table</p>
<p><strong>Summary</strong><br>开源情报为评估和预测领土控制提供了大量非结构化的文本数据。本文提出了使用大型语言模型（LLM）进行领土控制预测的框架CONTACT，并采用了最小监督方式。我们评估了SetFit和BLOOMZ-560m两种模型，其中SetFit是基于嵌入的少数样本分类器，而BLOOMZ则是一个多语言生成LLM模型。我们的模型通过手动的新闻数据集训练，关注叙利亚和伊拉克的ISIS活动。通过提示控制相关信号（如军事行动、伤亡和地点参考），我们发现基于BLOOMZ的模型优于SetFit基线模型，并且基于提示的监督方式在低资源环境中有助于提高泛化能力。CONTACT证明了使用少数样本方法的LLM微调可以减少标注负担并支持从开放式情报流中进行结构化推断。我们的代码已在GitHub上公开共享。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>开源情报为非结构化文本数据提供了关于领土控制的评估信息。</li>
<li>CONTACT框架使用大型语言模型（LLM）进行领土控制预测。</li>
<li>评估了SetFit和BLOOMZ两种模型在领土控制预测中的表现。</li>
<li>BLOOMZ模型在预测中表现更好，且基于提示的监督方式有助于提高泛化能力。</li>
<li>CONTACT证明了使用少数样本方法的LLM微调可以减少标注负担并支持结构化推断。</li>
<li>CONTACT框架适用于处理涉及军事行动、伤亡和地点参考等控制相关信号的情境。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d446e301899abbaaa1dd7085cd57c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-726309b29eaa47fb59ba9f9aa132deb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c752dc2c35694723ab4b42b14f6cd1b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Exploring-Multimodal-Prompt-for-Visualization-Authoring-with-Large-Language-Models"><a href="#Exploring-Multimodal-Prompt-for-Visualization-Authoring-with-Large-Language-Models" class="headerlink" title="Exploring Multimodal Prompt for Visualization Authoring with Large   Language Models"></a>Exploring Multimodal Prompt for Visualization Authoring with Large   Language Models</h2><p><strong>Authors:Zhen Wen, Luoxuan Weng, Yinghao Tang, Runjin Zhang, Yuxin Liu, Bo Pan, Minfeng Zhu, Wei Chen</strong></p>
<p>Recent advances in large language models (LLMs) have shown great potential in automating the process of visualization authoring through simple natural language utterances. However, instructing LLMs using natural language is limited in precision and expressiveness for conveying visualization intent, leading to misinterpretation and time-consuming iterations. To address these limitations, we conduct an empirical study to understand how LLMs interpret ambiguous or incomplete text prompts in the context of visualization authoring, and the conditions making LLMs misinterpret user intent. Informed by the findings, we introduce visual prompts as a complementary input modality to text prompts, which help clarify user intent and improve LLMs’ interpretation abilities. To explore the potential of multimodal prompting in visualization authoring, we design VisPilot, which enables users to easily create visualizations using multimodal prompts, including text, sketches, and direct manipulations on existing visualizations. Through two case studies and a controlled user study, we demonstrate that VisPilot provides a more intuitive way to create visualizations without affecting the overall task efficiency compared to text-only prompting approaches. Furthermore, we analyze the impact of text and visual prompts in different visualization tasks. Our findings highlight the importance of multimodal prompting in improving the usability of LLMs for visualization authoring. We discuss design implications for future visualization systems and provide insights into how multimodal prompts can enhance human-AI collaboration in creative visualization tasks. All materials are available at <a target="_blank" rel="noopener" href="https://osf.io/2QRAK">https://OSF.IO/2QRAK</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步表明，通过简单的自然语言表述自动化可视化创作过程具有巨大潜力。然而，使用自然语言指导LLM在传达可视化意图方面的精确性和表现力有限，这可能导致误解和耗时的迭代。为了解决这些局限性，我们进行了一项实证研究，了解LLM如何在可视化创作的背景下解释模糊或不完整的文本提示，以及导致LLM误解用户意图的条件。根据研究结果，我们引入视觉提示作为文本提示的补充输入模式，有助于澄清用户意图并增强LLM的解释能力。为了探索多模式提示在可视化创作中的潜力，我们设计了VisPilot，它使用户能够轻松使用多模式提示创建可视化，包括文本、草图和对现有可视化的直接操作。通过两个案例研究和一项受控的用户研究，我们证明VisPilot提供了一种更直观的方式来创建可视化，而且不会影响与仅使用文本提示的方法相比的总体任务效率。此外，我们分析了文本和视觉提示在不同可视化任务中的影响。我们的研究结果表明，多模式提示在提高LLM在可视化创作中的可用性方面具有重要意义。我们讨论了未来可视化系统的设计影响，并提供了多模式提示如何增强创意可视化任务中的人机协作的见解。所有材料均可通过<a target="_blank" rel="noopener" href="https://osf.io/2QRAK%E8%8E%B7%E5%8F%96%E3%80%82">https://OSF.IO/2QRAK获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13700v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自动化可视化创作方面具有巨大潜力，但使用自然语言指令时存在精度和表达受限的问题，容易导致误解和耗时迭代。为研究LLM如何解释可视化创作中的模糊或不完整文本提示，以及导致误解的条件，我们引入视觉提示作为文本提示的补充输入模式，以澄清用户意图并提高LLM的解释能力。我们设计了VisPilot系统，支持多种模式提示，包括文本、草图和对现有可视化的直接操作。通过案例研究和受控用户研究，我们发现VisPilot以更直观的方式创建可视化，不影响整体任务效率，且在不同的可视化任务中，文本和视觉提示的影响显著。这强调了多模式提示在提高LLM可视化创作可用性和人机协作创意可视化任务中的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在自动化可视化创作上有巨大潜力，但自然语言指令存在精度和表达受限的问题。</li>
<li>模糊或不完整的文本提示会导致LLM误解用户意图。</li>
<li>视觉提示作为补充输入模式，有助于澄清用户意图并提高LLM的解释能力。</li>
<li>VisPilot系统支持多种模式提示，包括文本、草图和对现有可视化的直接操作。</li>
<li>VisPilot以更直观的方式创建可视化，不影响整体任务效率。</li>
<li>在不同的可视化任务中，文本和视觉提示的影响显著。</li>
<li>多模式提示对提高LLM在可视化创作中的可用性和人机协作创意可视化任务的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13700">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f32df17fdc15a0dae6324b515d7ff212.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbf0a1514fc8fcc762cbe8c1d5c0f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71d572bfd9e85805439fe708d38aa6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b60709183e59708a8fb11658486068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ddfdf36496ae53be64903623d460353.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab292187d8c7d11286ee86429f423506.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Intelligent-Interaction-Strategies-for-Context-Aware-Cognitive-Augmentation"><a href="#Intelligent-Interaction-Strategies-for-Context-Aware-Cognitive-Augmentation" class="headerlink" title="Intelligent Interaction Strategies for Context-Aware Cognitive   Augmentation"></a>Intelligent Interaction Strategies for Context-Aware Cognitive   Augmentation</h2><p><strong>Authors: Xiangrong,  Zhu, Yuan Xu, Tianjian Liu, Jingwei Sun, Yu Zhang, Xin Tong</strong></p>
<p>Human cognition is constrained by processing limitations, leading to cognitive overload and inefficiencies in knowledge synthesis and decision-making. Large Language Models (LLMs) present an opportunity for cognitive augmentation, but their current reactive nature limits their real-world applicability. This position paper explores the potential of context-aware cognitive augmentation, where LLMs dynamically adapt to users’ cognitive states and task environments to provide appropriate support. Through a think-aloud study in an exhibition setting, we examine how individuals interact with multi-modal information and identify key cognitive challenges in structuring, retrieving, and applying knowledge. Our findings highlight the need for AI-driven cognitive support systems that integrate real-time contextual awareness, personalized reasoning assistance, and socially adaptive interactions. We propose a framework for AI augmentation that seamlessly transitions between real-time cognitive support and post-experience knowledge organization, contributing to the design of more effective human-centered AI systems. </p>
<blockquote>
<p>人类的认知受到处理能力的限制，导致在知识合成和决策制定中出现认知过载和效率低下。大型语言模型（LLM）为认知增强提供了机会，但它们当前的反应性质限制了它们在现实世界中的应用。本文探讨了语境感知认知增强的潜力，其中LLM动态适应用户的认知状态和任务环境，以提供适当的支持。通过展览环境中的“出声思考”研究，我们观察了个人与多模式信息的交互方式，并识别了在结构化、检索和应用知识方面的关键认知挑战。我们的研究结果强调了人工智能驱动的认知支持系统的重要性，这些系统结合了实时上下文感知、个性化推理支持和社交适应性交互。我们提出了一个人工智能增强框架，该框架可以无缝过渡到实时认知支持和后体验知识组织，为设计更有效的人类为中心的人工智能系统做出贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13684v1">PDF</a> Presented at the 2025 ACM Workshop on Human-AI Interaction for   Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</p>
<p><strong>Summary</strong>：人类认知受限于处理能力的限制，导致认知过载和知识合成与决策制定中的效率低下。大型语言模型（LLM）为认知增强提供了机会，但其当前的反应性质限制了其在现实世界中的应用。本立场论文探讨了上下文感知认知增强的潜力，其中LLM动态适应用户的认知状态和任务环境以提供适当的支持。通过展览环境中的出声思考研究，我们考察了个人与多模式信息的交互方式，并确定了结构化、检索和应用知识方面的关键认知挑战。我们发现需要集成实时上下文感知、个性化推理辅助和社会适应性交互的人工智能驱动的认知支持系统。我们提出了一个AI增强框架，该框架无缝过渡到实时认知支持和事后知识组织，有助于设计更有效的人类为中心的人工智能系统。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>人类认知受限于处理能力，导致认知过载和决策效率低下。</li>
<li>大型语言模型（LLM）为认知增强提供机会，但现有技术的反应性质限制了其实际应用。</li>
<li>上下文感知认知增强能够动态适应用户的认知状态和任务环境，提供适当的支持。</li>
<li>个体与多模式信息交互时存在认知挑战，如知识结构化、检索和应用。</li>
<li>需要AI驱动的认知支持系统，集成实时上下文感知、个性化推理辅助和社会适应性交互。</li>
<li>框架应无缝过渡实时认知支持和事后知识组织，以提高AI系统的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-59ea7efb0374d75d3256a4401ee52024.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffb34e1010e0a87513cc64e545af1418.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Visual-Intention-Grounding-for-Egocentric-Assistants"><a href="#Visual-Intention-Grounding-for-Egocentric-Assistants" class="headerlink" title="Visual Intention Grounding for Egocentric Assistants"></a>Visual Intention Grounding for Egocentric Assistants</h2><p><strong>Authors:Pengzhan Sun, Junbin Xiao, Tze Ho Elden Tse, Yicong Li, Arjun Akula, Angela Yao</strong></p>
<p>Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts – inputs are egocentric, and objects may be referred to implicitly through needs and intentions. To bridge this gap, we introduce EgoIntention, the first dataset for egocentric visual intention grounding. EgoIntention challenges multimodal LLMs to 1) understand and ignore unintended contextual objects and 2) reason about uncommon object functionalities. Benchmark results show that current models misidentify context objects and lack affordance understanding in egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it enables hybrid training with normal descriptions and egocentric intentions with a chained intention reasoning and object grounding mechanism. RoG significantly outperforms naive finetuning and hybrid training on EgoIntention, while maintaining or slightly improving naive description grounding. This advancement enables unified visual grounding for egocentric and exocentric visual inputs while handling explicit object queries and implicit human intentions. </p>
<blockquote>
<p>视觉定位将文本描述与图像中的物体关联起来。传统方法针对第三人称图像输入和命名对象查询。在人工智能助手等应用中，视角会发生变化——输入是自我中心的，物体可能通过需求和意图被隐含地提及。为了弥补这一差距，我们引入了EgoIntention，这是第一个针对自我中心视觉意图定位的数据集。EgoIntention挑战多模态大型语言模型，以1）理解和忽略非上下文中的物体，以及2）推理不常见物体的功能。基准测试结果表明，当前模型会错误识别上下文中的物体，在自我中心视角中缺乏功能理解。我们还提出了Reason-to-Ground（RoG）指令微调方法；它通过混合正常描述和自我中心意图的训练，并结合链式意图推理和对象定位机制，实现了训练。在EgoIntention上，RoG显著优于简单的微调方法和混合训练方法，同时在单纯描述定位方面保持或略有提升。这一进展实现了自我中心和外在中心视觉输入的统一视觉定位，同时处理明确的对象查询和隐含的人类意图。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了视觉定位技术，该技术将文本描述与图像中的物体相关联。传统的视觉定位方法主要针对第三人称图像输入和命名对象查询。但在实际应用中，如AI助手等场景，视角是自我中心的，并且物体可能通过需求和意图隐含地被引用。为了解决这个问题，文章引入了一个新数据集EgoIntention，它是用于自我中心视觉意图定位的第一个数据集。该数据集挑战了多模态大型语言模型，使其能够理解并忽略非上下文中的物体，并推理出罕见物体的功能。基准测试结果表明，当前模型在自我中心视角下对上下文物体的误识别和对可负担能力的理解缺乏。文章还提出了一种名为Reason-to-Ground（RoG）的指令调整方法，它通过链式意图推理和对象定位机制实现了常规描述和自我中心意图的混合训练。在EgoIntention测试中，RoG明显超越了简单微调法和混合训练法，同时维持或提高了单纯的定位描述能力。这一进展实现了统一处理自我中心和外部中心视觉输入的统一视觉定位技术，并处理明确的对象查询和隐含的人类意图。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉定位技术能够将文本描述与图像中的物体相关联。</li>
<li>传统视觉定位方法主要关注第三人称图像输入和命名对象查询。</li>
<li>在AI助手等应用中，视角是自我中心的，物体通过需求和意图隐含地被引用。</li>
<li>EgoIntention数据集是首个用于自我中心视觉意图定位的数据集。</li>
<li>当前模型在自我中心视角下存在误识别上下文物体的挑战，缺乏罕见物体的功能推理能力。</li>
<li>Reason-to-Ground（RoG）指令调整方法实现了常规描述和自我中心意图的混合训练，通过链式意图推理和对象定位机制显著提高了性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5eb2593b989df76d796c3f26fa1dc8ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87de39a59ddf9336ae7bea7bf83701eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-465609808cd39e8d9f9fc736226f9a29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79aa231b6446fc0a0477843d87cc44de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1714d1c0f1d900ab5e9f241433dfb2b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cadfb8e03dbecd652e763b43040641.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mind2Matter-Creating-3D-Models-from-EEG-Signals"><a href="#Mind2Matter-Creating-3D-Models-from-EEG-Signals" class="headerlink" title="Mind2Matter: Creating 3D Models from EEG Signals"></a>Mind2Matter: Creating 3D Models from EEG Signals</h2><p><strong>Authors:Xia Deng, Shen Chen, Jiale Zhou, Lei Li</strong></p>
<p>The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a>. </p>
<blockquote>
<p>从脑信号重建3D物体在脑-计算机接口（BCI）研究中已引起广泛关注。目前的研究主要利用功能磁共振成像（fMRI）进行3D重建任务，因其出色的空间分辨率。然而，fMRI的临床应用受限于其高昂的成本和无法支持实时操作。相比之下，脑电图（EEG）作为实时脑-计算机交互系统的经济、无创和移动解决方案，具有明显优势。虽然深度学习领域的最新进展在根据神经数据生成图像方面取得了显著进展，但将EEG信号解码为结构化3D表示仍然很少探索。在本文中，我们提出了一种新的框架，利用神经解码技术和生成模型将EEG记录转换为3D对象重建。我们的方法包括训练EEG编码器以提取时空视觉特征，微调大型语言模型以将这些特征解释为描述性多模式输出，并利用带有布局指导控制的生成3D高斯来合成最终的3D结构。实验表明，我们的模型捕捉了显著的几何和语义特征，为脑-计算机接口（BCI）、虚拟现实和神经矫正器应用奠定了基础。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11936v2">PDF</a> </p>
<p><strong>Summary</strong><br>在脑机接口研究中，利用脑信号重建三维物体引起了广泛关注。当前研究主要利用功能磁共振成像进行三维重建任务，但成本高昂且无法实现实时操作。本文提出了一种利用脑电图和神经网络解码技术将脑电图信号解码为结构化三维表示的新框架，从而实现实时脑机交互系统的三维物体重建。该模型可应用于脑机接口、虚拟现实和神经假肢等领域。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建三维物体从脑信号在脑机接口研究中受到关注。</li>
<li>功能磁共振成像在三维重建任务中占据主导地位，但成本高昂且无法支持实时操作。</li>
<li>电极脑电图提供了一种替代方法，具有经济、非侵入性和移动性等优点，适用于实时脑机交互系统。</li>
<li>深度学习在图像生成方面取得了显著进展，但将脑电图信号解码为结构化三维表示仍待探索。</li>
<li>本文提出了一种新框架，利用神经网络解码技术和生成模型将脑电图记录转化为三维物体重建。</li>
<li>该框架包括训练EEG编码器提取时空视觉特征，微调大型语言模型以解释这些特征并生成多模式输出，并利用布局指导控制的生成三维高斯来合成最终的三维结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-95343a55301c4bf587fbc0a67665cb0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca1f712f6ee96a259fc29c0453b3053f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ba79f5a837f15bdb2207ff4617883b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e223852dc3312651eaa87167b1aefe61.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers"><a href="#When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers" class="headerlink" title="When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers"></a>When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers</h2><p><strong>Authors:Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen</strong></p>
<p>Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B). </p>
<blockquote>
<p>任务算术是指通过添加任务向量的加权和来编辑预训练模型，其中每个任务向量都是预训练模型到特定任务的微调模型的权重更新。最近，这种方法作为一种计算高效的推理方法引起了人们的关注，例如在多任务学习、遗忘和跨域泛化能力方面的模型编辑。然而，由于训练Transformer模型的高度非凸性，关于任务向量如何执行各种概念操作的理论理解仍然有限。据我们所知，本文首次对任务向量方法在非线性Transformer上的泛化保证进行了理论描述。我们考虑一个概念学习场景，其中每个任务都是基于辨别模式的二元分类问题。我们从理论上证明了同时学习一组不相关或对齐的任务时添加任务的有效性，以及从无关或矛盾的任务中遗忘一个任务时否定任务的成功。此外，我们证明了任务算术中线性系数的适当选择，以实现对域外任务的保证泛化。我们的理论结果对于密集权重参数及其低秩近似都适用。尽管是在概念环境中建立的理论发现，但我们在使用大型语言模型Phi-1.5（13亿参数）的实际机器遗忘任务上验证了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10957v2">PDF</a> Published at ICLR 2025 as an oral paper</p>
<p><strong>Summary</strong>：<br>任务算术通过添加任务向量的加权和来编辑预训练模型，每个任务向量都是预训练模型到特定任务的微调模型的权重更新。这种方法作为模型编辑的计算高效推理方法，如多任务学习、遗忘和跨域泛化能力，近期受到关注。本文首次对非线性Transformer任务向量方法进行了理论表征。在概念学习环境中，每个任务都是基于判别模式的二元分类问题，本文证明了同时学习一组不相关或对齐的任务以及通过任务否定从无关或矛盾的任务中遗忘一个任务的有效性。此外，本文证明了选择适当的线性系数对任务算术实现有保障的跨域任务泛化的重要性。理论结果适用于密集权重参数及其低秩近似。尽管是在概念环境中建立的理论发现，但这些发现通过大型语言模型Phi-1.5（1.3B）的机器遗忘任务得到了验证。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>任务算术是通过添加任务向量的加权和来编辑预训练模型的方法，这是模型编辑的一种计算高效推理方法。</li>
<li>任务算术可用于多任务学习、遗忘和跨域泛化等场景。</li>
<li>本文首次对非线性Transformer任务向量方法的理论表征进行了研究。</li>
<li>在概念学习环境中，证明了任务添加和否定的有效性。</li>
<li>适当选择线性系数对任务算术实现有保障的跨域任务泛化至关重要。</li>
<li>理论结果适用于密集权重参数及其低秩近似。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0812a7cf203c03515f54d74bd0cfac9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e71cd5b483f404bd815534f4bddd1d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db044299e165f79f1b58a3783daa70c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Run Luo, Lu Wang, Wanwei He, Xiaobo Xia</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>现有构建图形用户界面（GUI）代理的工作主要依赖于在大视觉语言模型（LVLMs）上进行的监督精细调整训练模式。然而，这种方法不仅需求大量的训练数据，而且在理解GUI截图和泛化到未见过的界面方面也存在困难。这一问题极大地限制了其在现实场景中的应用，特别是在高级任务中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于强化学习的大型视觉语言模型GUI能力增强框架，通过统一动作空间规则建模，实现了在真实世界GUI代理任务中的高性能表现。该框架利用跨平台高质量数据并采用策略优化算法更新模型，相较于传统方法使用更少的数据量即可实现优越性能。这一创新具有巨大的潜力，有望推动视觉语言模型在真实世界GUI代理任务中的执行能力的提升。</p>
<p><strong>要点解析</strong></p>
<ul>
<li>当前GUI代理的建设主要依赖于大型视觉语言模型的监督微调训练模式，这种方式需要大量的训练数据且难以有效理解GUI截图并推广到未见过的界面。</li>
<li>强化学习框架被引入来提升LVLMs在真实世界GUI任务场景中的能力，特别是在高水平任务中的问题解决能力。该框架称为“名字”（暂未具体给出框架名称），借鉴大型推理模型的强化学习策略优化方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-40e44cce157ab734b1ce7aa7a91d195b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8b83a33fe04aa5f478735789bdd633d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1378358a130b9ffbb33425047048d0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="C-MTCSD-A-Chinese-Multi-Turn-Conversational-Stance-Detection-Dataset"><a href="#C-MTCSD-A-Chinese-Multi-Turn-Conversational-Stance-Detection-Dataset" class="headerlink" title="C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset"></a>C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset</h2><p><strong>Authors:Fuqiang Niu, Yi Yang, Xianghua Fu, Genan Dai, Bowen Zhang</strong></p>
<p>Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese multi-turn conversational stance detection dataset, comprising 24,264 carefully annotated instances from Sina Weibo, which is 4.2 times larger than the only prior Chinese conversational stance detection dataset. Our comprehensive evaluation using both traditional approaches and large language models reveals the complexity of C-MTCSD: even state-of-the-art models achieve only 64.07% F1 score in the challenging zero-shot setting, while performance consistently degrades with increasing conversation depth. Traditional models particularly struggle with implicit stance detection, achieving below 50% F1 score. This work establishes a challenging new benchmark for Chinese stance detection research, highlighting significant opportunities for future improvements. </p>
<blockquote>
<p>立场检测已成为分析社交媒体上公众讨论的重要工具。当前的方法面临重大挑战，特别是在中文语言处理和多轮对话分析方面。为了解决这些局限性，我们推出了C-MTCSD，这是最大的中文多轮对话立场检测数据集，包含24264个来自新浪微博的精心注释实例，是之前唯一的中文对话立场检测数据集的4.2倍。我们使用传统方法和大型语言模型进行的全面评估揭示了C-MTCSD的复杂性：即使在具有挑战性的零样本设置中，最先进的模型也仅达到64.07%的F1分数，随着对话深度的增加，性能不断下降。传统模型在隐性立场检测方面尤其困难，F1分数低于50%。这项工作为中文立场检测研究设定了新的具有挑战性的基准，突显了未来改进的重大机遇。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09958v2">PDF</a> WWW2025</p>
<p><strong>Summary</strong></p>
<p>基于社交网络公共讨论的分析，立场检测已成为一项重要工具。针对现有方法在中文语言处理和多轮对话分析上的局限，我们推出了C-MTCSD数据集，它是迄今为止最大的中文多轮对话立场检测数据集，包含来自新浪微博的24,264个精心标注的实例，比之前唯一的中文对话立场检测数据集大4.2倍。我们的评估结果揭示了C-MTCSD的复杂性：即使在具有挑战性的零样本设置下，最先进的模型也只能达到64.07%的F1分数，随着对话深度的增加，性能持续下降。传统模型在隐性立场检测方面的表现尤其不佳，F1分数低于50%。本研究为中文立场检测研究设定了新的挑战基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>立场检测是分析社交媒体公共讨论的重要工具。</li>
<li>当前方法在中文语言处理和多轮对话分析方面存在挑战。</li>
<li>引入了C-MTCSD数据集，为中文多轮对话立场检测提供了最大的标注实例。</li>
<li>C-MTCSD数据集的复杂性被揭示，最先进的模型在零样本设置下F1分数仅为64.07%。</li>
<li>随着对话深度的增加，模型性能持续下降。</li>
<li>传统模型在隐性立场检测方面表现不佳，F1分数低于50%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-df667568300e44f07eee1e08d139b83e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608231bb75d7b84fcc702c6f2ad38fbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5059b6f738b683d07f1899fdc37d6190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8bbd061e01d02cefcdc70b85fc01a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dae74d17404493747523cf9f36e694a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Token-to-Line-Enhancing-Code-Generation-with-a-Long-Term-Perspective"><a href="#From-Token-to-Line-Enhancing-Code-Generation-with-a-Long-Term-Perspective" class="headerlink" title="From Token to Line: Enhancing Code Generation with a Long-Term   Perspective"></a>From Token to Line: Enhancing Code Generation with a Long-Term   Perspective</h2><p><strong>Authors:Tingwei Lu, Yangning Li, Liyuan Wang, Binghuai Lin, Jiwei Tang, Wanshi Xu, Hai-Tao Zheng, Yinghui Li, Bingxu An, Zhao Wei, Yong Xu</strong></p>
<p>The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches. </p>
<blockquote>
<p>大型语言模型（LLM）的出现极大地促进了代码生成任务的发展，并引发了相关文献的激增。当前的研究受到生成结果冗余和短期内过度拟合局部模式的影响。尽管现有研究试图通过采用多令牌预测策略来缓解这一问题，但在选择适当的处理长度进行生成方面仍存在有限关注。通过分析LLM生成过程中令牌之间的注意力，可以观察到注意力得分的高峰值通常出现在行的末尾。这一观察结果表明，将每行代码视为基本处理单元并进行顺序生成是合理的。受此启发，我们提出了<strong>LSR-MCTS</strong>算法，该算法利用MCTS逐行确定代码并选择最佳路径。此外，我们在每个节点集成了一种自我优化机制，以提高多样性并通过错误校正生成更高质量的程序。在三个公共编码基准测试上的广泛实验和综合分析表明，我们的方法优于最新性能方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07433v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的出现极大地推动了代码生成任务的发展，当前研究面临冗余生成结果和短期过度拟合局部模式的挑战。现有研究尝试通过采用多令牌预测策略来缓解这一问题，但如何选择适当的生成处理长度仍受到关注。通过分析LLM生成过程中的令牌间注意力，观察到注意力分数的高峰值通常出现在行尾。这启发我们提出以行为基本处理单元的LSR-MCTS算法，利用MCTS逐行确定代码并选择最优路径。此外，我们在每个节点集成了自我修正机制，以提高多样性并通过错误修正生成更高质量的程序。在三个公共编码基准上的广泛实验和综合分析表明，我们的方法优于现有最佳性能方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）对代码生成任务有显著的推动作用。</li>
<li>当前研究面临冗余生成和过度拟合的挑战。</li>
<li>现有研究主要通过多令牌预测策略来应对挑战，但处理长度的选择仍然重要。</li>
<li>LLM生成过程中令牌间的高注意力峰值通常出现在行尾，提示以行为基本处理单元的方法可能更有效。</li>
<li>提出了LSR-MCTS算法，利用MCTS逐行确定代码并选择最优路径。</li>
<li>LSR-MCTS算法集成了自我修正机制，以提高生成程序的多样性和质量。</li>
<li>在三个公共编码基准上的实验表明，LSR-MCTS算法性能优于现有最佳方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07433">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b09f0dd2ba47648b9a46064f1ad4188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1febc01984a7f8196da84a5f8ffcbee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c734a60da3b0be89bb464b0ef0c5bfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ecdd884514cc2a1befd3ad17c2181b0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible “dark patterns” in LLMs’ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local “safety regions” in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts–a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>大型语言模型（LLM）是人工智能通用化的基础探索，然而，通过指令调整和偏好学习使其与人类价值观相符只实现了表面的合规性。在这里，我们证明预训练期间嵌入的有害知识会持久地作为不可磨灭的“暗模式”存在于LLM的参数记忆中，逃避对齐保障措施，并在分布转移时通过敌对诱导重新出现。在本研究中，我们首先从理论上分析对齐LLM的内在道德脆弱性，证明当前的对齐方法仅在知识流形中产生局部“安全区域”。相反，预训练知识仍然与有害概念全球连通，经由高概率的敌对轨迹。基于这一理论洞察，我们通过分布转移下的语义连贯诱导来实证我们的发现——这是一种通过优化敌对提示来系统地绕过对齐约束的方法。这种结合理论和实证的方法在23个最新对齐LLM中的19个上实现了100%的攻击成功率，包括DeepSeek-R1和LLaMA-3，揭示了它们的普遍脆弱性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）是人工智能通用探索的基础，但通过指令调整和偏好学习实现与人类价值的对齐仅达到表面上的符合。研究表明，预训练过程中嵌入的有害知识会作为不可磨灭的“暗模式”存在于LLM的参数记忆中，逃避对齐保障措施，并在分布转移时通过对抗性诱导重新浮现。本研究首先从理论上分析对齐LLM的内在道德脆弱性，证明当前对齐方法仅在知识流形中产生局部“安全区域”，而预训练知识仍与有害概念通过高概率对抗性轨迹保持全球联系。通过分布转移下的语义连贯性诱导方法，我们实证验证了这一发现，该方法通过优化对抗性提示来系统地绕过对齐约束。这种结合理论和实证的方法在23个最新对齐的大型语言模型中成功攻击了其中19个，揭示了其普遍存在的脆弱性。总结：LLM模型与人类价值观对齐不够彻底，存在理论上的道德脆弱性和实证中的普遍攻击脆弱性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在人工智能领域中具有基础地位，但它们与人类价值的对齐仅达到表面符合。</li>
<li>预训练过程中嵌入的有害知识会作为“暗模式”存在于LLM中。</li>
<li>这些“暗模式”可以逃避现有的对齐保障措施，并在特定情境下重新浮现。</li>
<li>当前的对齐方法仅在知识流形中产生局部“安全区域”。</li>
<li>预训练知识仍与有害概念保持全球联系。</li>
<li>通过分布转移下的语义连贯性诱导方法，可以系统地绕过LLM的对齐约束。</li>
<li>最新的LLM普遍存在道德脆弱性，容易被攻击。强调对LLM进行更深入的对齐研究和加强安全保障的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e886cfdf5a2775600cbfa6abd9995005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef1ae5f4c8bb99d769899606df41c06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c4ac31a37125b8a84cf3ed3c2f94db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb2e53b7882fd19a4d824edec1c13d9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-MEM-Agentic-Memory-for-LLM-Agents"><a href="#A-MEM-Agentic-Memory-for-LLM-Agents" class="headerlink" title="A-MEM: Agentic Memory for LLM Agents"></a>A-MEM: Agentic Memory for LLM Agents</h2><p><strong>Authors:Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang</strong></p>
<p>While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems’ fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at <a target="_blank" rel="noopener" href="https://github.com/WujiangXu/AgenticMemory">https://github.com/WujiangXu/AgenticMemory</a>, while the source code of agentic memory system is available at <a target="_blank" rel="noopener" href="https://github.com/agiresearch/A-mem">https://github.com/agiresearch/A-mem</a>. </p>
<blockquote>
<p>虽然大型语言模型（LLM）代理可以有效地利用外部工具来完成复杂的现实世界任务，但它们需要记忆系统来利用历史经验。当前的记忆系统可以实现基本的存储和检索，但缺乏复杂的记忆组织，尽管最近有尝试引入图数据库。此外，这些系统的固定操作和结构限制了它们在各种任务中的适应性。为了解决这一局限性，本文提出了一种用于LLM代理的新型代理记忆系统，该系统可以动态地以代理方式对记忆进行组织。遵循Zettelkasten方法的基本原则，我们设计的记忆系统通过动态索引和链接创建相互关联的知识网络。每当添加新记忆时，我们会生成一个包含多个结构化属性的综合笔记，包括上下文描述、关键词和标签。然后，系统分析历史记忆以识别相关连接，并在存在有意义的相似性时建立链接。此外，这个过程使记忆得以进化——随着新记忆的融入，它们可以触发对现有历史记忆的上下文表示和属性的更新，使记忆网络能够不断地完善其理解。我们的方法结合了Zettelkasten的结构化组织原则与代理驱动决策的灵活性，从而实现更适应上下文的记忆管理。在六个基础模型上的实证实验表明，与现有的最先进的基线相比，有显著改进。评估性能的代码可在<a target="_blank" rel="noopener" href="https://github.com/agiresearch/A-mem%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E8%80%8C%E4%BB%A3%E7%90%86%E8%AE%B0%E5%BF%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%99%E5%8F%AF%E4%BB%A5">https://github.com/WujiangXu/AgenticMemory上找到，而代理记忆系统的源代码则可在</a><a target="_blank" rel="noopener" href="https://github.com/agiresearch/A-mem%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/agiresearch/A-mem上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12110v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）需要记忆系统来利用历史经验以完成复杂的现实世界任务。当前记忆系统虽具备基本存储和检索功能，但在组织记忆方面缺乏高级功能，且固定操作和结构的限制影响了其在不同任务中的适应性。本文提出一种新型的语言模型记忆系统，采用动态索引和链接方式，构建相互关联的知识网络。新记忆被添加时，系统生成包含多种结构化属性的综合笔记，如上下文描述、关键词和标签。通过分析历史记忆，系统可识别相关联系并建立链接。此外，新记忆的整合可触发现有历史记忆的上下文表示和属性的更新，使记忆网络不断完善理解。该方法结合了Zettelkasten的结构化组织原则和代理驱动的决策灵活性，可实现更适应上下文和语境的内存管理。实证实验结果显示在多个基础模型上的改进优于现有最先进基线。相关源代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM需要记忆系统利用历史经验进行复杂任务。</li>
<li>当前记忆系统在记忆组织方面存在局限性，缺乏高级功能。</li>
<li>本文提出一种新型LLM记忆系统，能动态构建知识网络。</li>
<li>系统利用综合笔记包含结构化属性，如上下文描述、关键词和标签。</li>
<li>系统通过分析历史记忆识别相关联系并建立链接。</li>
<li>新记忆的整合使记忆网络能够不断进化并改进理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12110">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3311fe515375cf837dab7c870ee2d06b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f60cff38e4f45d95c7cc2c7568c14b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d4a3e95cf2de0ecf3b48d938d5aff05.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AgentHarm-A-Benchmark-for-Measuring-Harmfulness-of-LLM-Agents"><a href="#AgentHarm-A-Benchmark-for-Measuring-Harmfulness-of-LLM-Agents" class="headerlink" title="AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"></a>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</h2><p><strong>Authors:Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies</strong></p>
<p>The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents – which use external tools and can execute multi-stage tasks – may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai-safety-institute/AgentHarm">https://huggingface.co/datasets/ai-safety-institute/AgentHarm</a>. </p>
<blockquote>
<p>大型语言模型（LLM）抵御越狱攻击（用户设计提示以绕过安全措施并滥用模型能力）的稳健性，主要研究了作为简单聊天机器人的LLM。同时，滥用能够使用外部工具并执行多阶段任务的大型语言模型代理可能会带来更大的风险，但其稳健性尚未得到充分探索。为了促进关于大型语言模型代理滥用的研究，我们提出了名为AgentHarm的新基准测试。该基准测试包括一套包含11种危害类别的110个明确的恶意代理任务（包含增强版本共440个任务），涵盖欺诈、网络犯罪和骚扰等。除了衡量模型是否会拒绝有害的代理请求外，要在AgentHarm上获得高分还需要确保越狱的代理在攻击后能够保持其完成多阶段任务的能力。我们评估了一系列领先的大型语言模型，并发现：（1）领先的大型语言模型在没有越狱的情况下会出人意料地遵从恶意代理的请求；（2）简单的通用越狱模板可以被适应来有效地越狱代理；（3）这些越狱能够使代理行为连贯且恶意，并保留模型的能力。为了方便对基于大型语言模型的代理进行简单可靠的攻击和防御评估，我们在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai-safety-institute/AgentHarm%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E4%BA%86AgentHarm%E3%80%82">https://huggingface.co/datasets/ai-safety-institute/AgentHarm上公开发布了AgentHarm。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09024v3">PDF</a> Accepted at ICLR 2025</p>
<p><strong>摘要</strong><br>LLMs对利用提示绕过安全措施并滥用模型功能的越狱攻击的鲁棒性已被广泛研究，主要集中在作为简单聊天机器人的LLMs上。然而，对于滥用能够使用外部工具并执行多阶段任务的LLM代理，其风险可能更大，但其鲁棒性尚未得到充分研究。为了促进对LLM代理滥用的研究，本文提出了一种新的基准测试方法——AgentHarm。该基准测试包括包含涵盖诈骗、网络犯罪和骚扰等类别在内的各种明显恶意代理任务共达百例案例及其扩展版本。除了衡量模型是否拒绝有害代理请求外，要在AgentHarm上获得良好评分还要求越狱代理在完成多阶段任务时保持其功能。我们对一系列领先的LLMs进行了评估，发现：（1）领先的LLMs在未经越狱的情况下会意外遵从恶意代理的请求；（2）简单的通用越狱模板可以适应有效地越狱代理；（3）这些越狱行为能够保留模型功能并产生连贯且恶意的多阶段代理行为。为了实现对基于LLM的代理的攻击和防御的简单可靠评估，我们在<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai-safety-institute/AgentHarm%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83AgentHarm%E3%80%82">https://huggingface.co/datasets/ai-safety-institute/AgentHarm公开发布AgentHarm。</a></p>
<p><strong>关键见解</strong></p>
<p>一、LLMs面临滥用风险的研究概述</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09024">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-db41364dbd99ce6983305dff10413b6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e0cb3dd538576c02d7596c429bb7374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6f2f0b3d37feba41ef30c22a7bf9de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d78f5d29a4192863c752c1cde3e5a13.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6d78f5d29a4192863c752c1cde3e5a13.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-22  ChatNekoHacker Real-Time Fan Engagement with Conversational Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23394.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
