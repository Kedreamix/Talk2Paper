<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bbed99f1971aa9a935920c070ed7910e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-22-æ›´æ–°"><a href="#2025-04-22-æ›´æ–°" class="headerlink" title="2025-04-22 æ›´æ–°"></a>2025-04-22 æ›´æ–°</h1><h2 id="Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model"><a href="#Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model" class="headerlink" title="Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?"></a>Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?</h2><p><strong>Authors:Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base modelsâ€™ capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$&#x3D;1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base modelsâ€™ sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the modelâ€™s output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: <a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a> </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ€è¿‘åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚äººä»¬æ™®éè®¤ä¸ºï¼ŒRLVRä½¿LLMsèƒ½å¤ŸæŒç»­è‡ªæˆ‘æ”¹è¿›ï¼Œä»è€Œè·å–è¶…è¿‡ç›¸åº”åŸºç¡€æ¨¡å‹çš„å…¨æ–°æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æµ‹é‡pass@\textit{k}æŒ‡æ ‡ï¼Œå¹¶è®¾ç½®è¾ƒå¤§çš„\textit{k}å€¼ï¼Œæ¥é‡æ–°å®¡è§†è¿™ä¸€å‡è®¾ï¼Œä»¥æ¢ç´¢æ¨¡å‹åœ¨ä¸åŒæ¨¡å‹å®¶æ—å’ŒåŸºå‡†æµ‹è¯•ä¸­çš„æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œäº‹å®ä¸Šï¼Œå¼ºåŒ–å­¦ä¹ å¹¶æ²¡æœ‰å¼•å‘æ ¹æœ¬æ€§çš„æ–°æ¨ç†æ¨¡å¼ã€‚è™½ç„¶ä½¿ç”¨RLè®­ç»ƒçš„æ¨¡å‹åœ¨è¾ƒå°çš„kå€¼ï¼ˆä¾‹å¦‚k&#x3D;1ï¼‰æƒ…å†µä¸‹ï¼Œå…¶è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨è¾ƒå¤§çš„kå€¼æƒ…å†µä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„pass@\textit{k}åˆ†æ•°å¯ä»¥ä¸ä½¿ç”¨RLçš„æ¨¡å‹ç›¸å½“ç”šè‡³æ›´é«˜ã€‚è¿™è¡¨æ˜RLè®­ç»ƒçš„æ¨¡å‹äº§ç”Ÿçš„æ¨ç†è·¯å¾„å·²ç»åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ï¼Œå³RLè®­ç»ƒçš„æ¨¡å‹ä¸­è¡¨ç°å‡ºçš„å¤§å¤šæ•°æ¨ç†èƒ½åŠ›å·²ç»è¢«åŸºç¡€æ¨¡å‹æ‰€è·å¾—ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒRLè®­ç»ƒé€šè¿‡ä½¿æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒåå‘äºæ›´å¯èƒ½äº§ç”Ÿå¥–åŠ±çš„è·¯å¾„ï¼Œä»è€Œæé«˜äº†æ€§èƒ½ï¼Œå› æ­¤æ›´æœ‰æ•ˆç‡åœ°é‡‡æ ·æ­£ç¡®çš„å›åº”ã€‚ä½†è¿™ä¹Ÿä¼šå¯¼è‡´ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ¨ç†èƒ½åŠ›è¾¹ç•Œå˜çª„ã€‚åœ¨ä½¿ç”¨RLVRè¿›è¡Œè§†è§‰æ¨ç†ä»»åŠ¡æ—¶ä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è’¸é¦æ³•å¯ä»¥çœŸæ­£åœ°å°†æ–°çŸ¥è¯†å¼•å…¥æ¨¡å‹ä¸­ï¼Œè¿™ä¸RLVRä¸åŒã€‚è¿™äº›å‘ç°çªå‡ºäº†RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œè¿™è¦æ±‚æˆ‘ä»¬ä»æ ¹æœ¬ä¸Šé‡æ–°æ€è€ƒRLè®­ç»ƒåœ¨æ¨ç†LLMä¸­çš„ä½œç”¨ä»¥åŠæ˜¯å¦éœ€è¦æ›´å¥½çš„èŒƒå¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13837v1">PDF</a> 24 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é€šè¿‡æµ‹é‡pass@kæŒ‡æ ‡ï¼Œå¯¹RLVRæ˜¯å¦èƒ½å¤Ÿä¿ƒä½¿LLMæŒç»­è‡ªæˆ‘æå‡å¹¶è·å–æ–°çš„æ¨ç†èƒ½åŠ›è¿›è¡Œäº†é‡æ–°è¯„ä¼°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¼ºåŒ–å­¦ä¹ å¹¶æœªæ¿€å‘æ–°çš„æ¨ç†æ¨¡å¼ï¼Œè€Œä¸”åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„æ€§èƒ½å¯ä¸RLè®­ç»ƒçš„æ¨¡å‹ç›¸åª²ç¾ç”šè‡³æ›´é«˜ã€‚è¿™è¡¨æ˜RLè®­ç»ƒçš„æ¨¡å‹çš„æ¨ç†è·¯å¾„å·²åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒRLè®­ç»ƒé€šè¿‡ä½¿æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåå‘äºæ›´å¯èƒ½äº§ç”Ÿå¥–åŠ±çš„è·¯å¾„ï¼Œæé«˜äº†æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´äº†ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”æ›´ç‹­çª„çš„æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚ç±»ä¼¼çš„ç»“æœåœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¹ŸåŒæ ·è§‚å¯Ÿåˆ°ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è’¸é¦å¯ä»¥çœŸæ­£åœ°å°†æ–°çŸ¥è¯†å¼•å…¥æ¨¡å‹ï¼Œä¸RLVRä¸åŒã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶éœ€è¦æˆ‘ä»¬å¯¹RLè®­ç»ƒåœ¨LLMæ¨ç†ä¸­çš„å½±å“ä»¥åŠæ˜¯å¦éœ€è¦æ›´å¥½çš„èŒƒå¼è¿›è¡Œæ ¹æœ¬æ€§çš„é‡æ–°æ€è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å¢å¼ºäº†LLMåœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æµ‹é‡pass@kæŒ‡æ ‡ï¼Œç ”ç©¶å‘ç°å¼ºåŒ–å­¦ä¹ å¹¶æœªæ¿€å‘æ–°çš„æ¨ç†æ¨¡å¼ã€‚</li>
<li>åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ä¸RLè®­ç»ƒçš„æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>RLè®­ç»ƒçš„æ¨¡å‹çš„æ¨ç†è·¯å¾„å·²åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ã€‚</li>
<li>RLè®­ç»ƒæé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´æ›´ç‹­çª„çš„æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚</li>
<li>åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1145fa10a074c5138153deb614a9a7b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c356693220910067cc819f4d5fdaea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e6f577c8731eed685abeaa9d193db2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da3f9fac7a8a67e71f5c7441b7c8984a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MIG-Automatic-Data-Selection-for-Instruction-Tuning-by-Maximizing-Information-Gain-in-Semantic-Space"><a href="#MIG-Automatic-Data-Selection-for-Instruction-Tuning-by-Maximizing-Information-Gain-in-Semantic-Space" class="headerlink" title="MIG: Automatic Data Selection for Instruction Tuning by Maximizing   Information Gain in Semantic Space"></a>MIG: Automatic Data Selection for Instruction Tuning by Maximizing   Information Gain in Semantic Space</h2><p><strong>Authors:Yicheng Chen, Yining Li, Kai Hu, Zerun Ma, Haochen Ye, Kai Chen</strong></p>
<p>Data quality and diversity are key to the construction of effective instruction-tuning datasets. % With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. % Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. % However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. % Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. % To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. % Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to \textbf{M}aximize the \textbf{I}nformation \textbf{G}ain (MIG) in semantic space. % Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. % Notably, the model fine-tuned with 5% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73% on AlpacaEval and +6.89% on Wildbench. </p>
<blockquote>
<p>æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§æ˜¯æ„å»ºæœ‰æ•ˆæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„å…³é”®ã€‚éšç€å¼€æºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„å¯è·å–æ€§ä¸æ–­å¢åŠ ï¼Œä»å¤§é‡æ•°æ®ä¸­è‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„å­é›†å…·æœ‰ä¼˜åŠ¿ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼˜å…ˆè€ƒè™‘å®ä¾‹è´¨é‡ï¼Œå¹¶ä½¿ç”¨å¯å‘å¼è§„åˆ™æ¥ç»´æŒå¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œå¯¹æ•´ä¸ªé›†åˆç¼ºä¹å…¨é¢è§‚å¯Ÿå¾€å¾€ä¼šå¯¼è‡´ç»“æœä¸ä½³ã€‚æ­¤å¤–ï¼Œå¯å‘å¼è§„åˆ™é€šå¸¸å…³æ³¨åµŒå…¥ç©ºé—´å†…çš„è·ç¦»æˆ–èšç±»ï¼Œè¿™æ— æ³•å‡†ç¡®æ•è·å¤æ‚æŒ‡ä»¤åœ¨è¯­ä¹‰ç©ºé—´ä¸­çš„æ„å›¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œç”¨äºé‡åŒ–æ•°æ®é›†çš„ä¿¡æ¯å†…å®¹ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥å»ºæ¨¡è¯­ä¹‰ç©ºé—´ï¼Œå¹¶åŸºäºå›¾ä¸­ä¿¡æ¯çš„åˆ†å¸ƒæ¥é‡åŒ–å¤šæ ·æ€§ã€‚åŸºäºè¿™ç§åº¦é‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¿­ä»£é€‰æ‹©æ•°æ®æ ·æœ¬ï¼Œä»¥æœ€å¤§åŒ–è¯­ä¹‰ç©ºé—´ä¸­çš„ä¿¡æ¯å¢ç›Šï¼ˆMIGï¼‰ã€‚åœ¨å„ç§æ•°æ®é›†å’ŒåŸºå‡†æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMIGæŒç»­ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨MIGé‡‡æ ·çš„5% Tulu3æ•°æ®å¾®è°ƒåçš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè®­ç»ƒçš„å®˜æ–¹SFTæ¨¡å‹ç›¸å½“ï¼Œåœ¨AlpacaEvalä¸Šçš„æ€§èƒ½æå‡+5.73%ï¼Œåœ¨Wildbenchä¸Šçš„æ€§èƒ½æå‡+6.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13835v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§æ˜¯æ„å»ºæœ‰æ•ˆæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„å…³é”®ã€‚éšç€å¼€æºæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„ä¸æ–­å¢å¤šï¼Œä»æµ·é‡æ•°æ®ä¸­è‡ªåŠ¨é€‰æ‹©é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å­é›†å…·æœ‰ä¼˜åŠ¿ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼˜å…ˆè€ƒè™‘å®ä¾‹è´¨é‡ï¼Œå¹¶ä½¿ç”¨å¯å‘å¼è§„åˆ™æ¥ç»´æŒå¤šæ ·æ€§ã€‚ç„¶è€Œï¼Œç¼ºä¹å¯¹æ•´ä¸ªé›†åˆçš„å…¨é¢è§†è§’å¾€å¾€å¯¼è‡´ç»“æœä¸å°½å¦‚äººæ„ã€‚æ­¤å¤–ï¼Œå¯å‘å¼è§„åˆ™ä¸€èˆ¬å…³æ³¨åµŒå…¥ç©ºé—´å†…çš„è·ç¦»æˆ–èšç±»ï¼Œæ— æ³•å‡†ç¡®æ•æ‰å¤æ‚æŒ‡ä»¤çš„è¯­ä¹‰ç©ºé—´æ„å›¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•ï¼Œå¯¹æ•°æ®é›†çš„ä¿¡æ¯å†…å®¹è¿›è¡Œé‡åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥æ¨¡æ‹Ÿè¯­ä¹‰ç©ºé—´ï¼Œå¹¶åŸºäºå›¾ä¸­ä¿¡æ¯çš„åˆ†å¸ƒæ¥é‡åŒ–å¤šæ ·æ€§ã€‚åŸºäºè¿™ç§åº¦é‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è¿­ä»£é€‰æ‹©æ•°æ®æ ·æœ¬ä»¥æœ€å¤§åŒ–è¯­ä¹‰ç©ºé—´çš„ä¿¡æ¯å¢ç›Šï¼ˆMIGï¼‰ã€‚åœ¨å„ç§æ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMIGæŒç»­ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨MIGé‡‡æ ·çš„Tulu3æ•°æ®çš„5%å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå…¶æ€§èƒ½ä¸åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè®­ç»ƒçš„å®˜æ–¹SFTæ¨¡å‹ç›¸å½“ï¼Œåœ¨AlpacaEvalä¸Šæé«˜äº†+5.73%ï¼Œåœ¨Wildbenchä¸Šæé«˜äº†+6.89%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ•°æ®è´¨é‡å’Œå¤šæ ·æ€§å¯¹äºæ„å»ºæœ‰æ•ˆçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› ç¼ºä¹å¯¹æ•´ä½“æ•°æ®é›†åˆçš„å…¨é¢è§†è§’è€Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚</li>
<li>å¯å‘å¼è§„åˆ™é€šå¸¸å…³æ³¨åµŒå…¥ç©ºé—´å†…çš„è·ç¦»æˆ–èšç±»ï¼Œå¿½ç•¥äº†å¤æ‚æŒ‡ä»¤çš„è¯­ä¹‰ç©ºé—´æ„å›¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºæ ‡ç­¾å›¾æ¥é‡åŒ–æ•°æ®é›†çš„ä¿¡æ¯å†…å®¹ï¼Œå¹¶åŸºäºä¿¡æ¯åˆ†å¸ƒæ¥é‡åŒ–å¤šæ ·æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œå³æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šï¼ˆMIGï¼‰ï¼Œä»¥æé«˜è¯­ä¹‰ç©ºé—´çš„æ•°æ®é€‰æ‹©æ•ˆæœã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMIGæ–¹æ³•åœ¨å¤šç§æ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-090e3ccfe70f19b78e0d546fce3aeb02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6203bb08d9ef9854fb0843b74cda60e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cc5e27abb0785de1f5bc03f3eb68fae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-AI-Act-II-Test-Time-Scaling-Drives-Cognition-Engineering"><a href="#Generative-AI-Act-II-Test-Time-Scaling-Drives-Cognition-Engineering" class="headerlink" title="Generative AI Act II: Test Time Scaling Drives Cognition Engineering"></a>Generative AI Act II: Test Time Scaling Drives Cognition Engineering</h2><p><strong>Authors:Shijie Xia, Yiwei Qin, Xuefeng Li, Yan Ma, Run-Ze Fan, Steffi Chern, Haoyang Zou, Fan Zhou, Xiangkun Hu, Jiahe Jin, Yanheng He, Yixin Ye, Yixiu Liu, Pengfei Liu</strong></p>
<p>The first generation of Large Language Models - what might be called â€œAct Iâ€ of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of â€œAct IIâ€ (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AIâ€™s second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: <a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/cognition-engineering">https://github.com/GAIR-NLP/cognition-engineering</a> </p>
<blockquote>
<p>ç¬¬ä¸€ä»£å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”å¯ä»¥è¢«ç§°ä¸ºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆ2020å¹´è‡³2023å¹´ï¼‰â€œç¬¬ä¸€å¹•â€â€”â€”é€šè¿‡å¤§è§„æ¨¡å‚æ•°å’Œæ•°æ®æ‰©å±•å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨çŸ¥è¯†å»¶è¿Ÿã€æµ…å±‚æ¨ç†å’Œè®¤çŸ¥è¿‡ç¨‹å—é™ç­‰æ–¹é¢è¡¨ç°å‡ºæ ¹æœ¬æ€§å±€é™ã€‚åœ¨è¿™ä¸€æ—¶æœŸï¼Œæç¤ºå·¥ç¨‹ä½œä¸ºæˆ‘ä»¬ä¸äººå·¥æ™ºèƒ½çš„ä¸»è¦ç•Œé¢åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡è‡ªç„¶è¯­è¨€å®ç°äº†å¯¹è¯çº§çš„äº¤æµã€‚æˆ‘ä»¬ç°åœ¨è§è¯äº†â€œç¬¬äºŒå¹•â€ï¼ˆä»ç°å¼€å§‹åˆ°å½“å‰ï¼‰â€”â€”é€šè¿‡æµ‹è¯•æ—¶çš„ç¼©æ”¾æŠ€æœ¯ä»çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿè½¬å˜ä¸ºæ€ç»´æ„é€ å¼•æ“çš„æ—¶ä»£æ¥ä¸´ã€‚è¿™ä¸€æ–°èŒƒå¼é€šè¿‡å»ºç«‹åŸºäºè¯­è¨€çš„æ€ç»´ä¸äººå·¥æ™ºèƒ½å»ºç«‹äº†ä¸€ç§å¿ƒæ™ºå±‚é¢çš„è”ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ˜ç¡®äº†è®¤çŸ¥å·¥ç¨‹çš„æ¦‚å¿µåŸºç¡€ï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆè¿™ä¸€åˆ»å¯¹å…¶å‘å±•è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„æ•™ç¨‹å’Œä¼˜åŒ–åçš„å®ç°ç³»ç»Ÿåœ°å‰–æäº†è¿™äº›é«˜çº§æ–¹æ³•ï¼Œä½¿è®¤çŸ¥å·¥ç¨‹æ°‘ä¸»åŒ–ï¼Œè®©æ¯ä½ä»ä¸šè€…éƒ½èƒ½å‚ä¸äººå·¥æ™ºèƒ½çš„ç¬¬äºŒå¹•ã€‚å…³äºæµ‹è¯•æ—¶ç¼©æ”¾çš„ç›¸å…³è®ºæ–‡é›†åˆä¼šå®šæœŸæ›´æ–°åœ¨GitHubä»“åº“ä¸­ï¼š<a target="_blank" rel="noopener" href="https://github.com/GAIR-NLP/cognition-engineering">https://github.com/GAIR-NLP/cognition-engineering</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13828v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨â€œè¡ŒåŠ¨ä¸€é˜¶æ®µâ€ï¼ˆå³å‰é˜¶æ®µå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰é€šè¿‡å¤§é‡çš„å‚æ•°å’Œæ•°æ®è§„æ¨¡å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å±•ç°å‡ºäº†ä¸€äº›æ ¹æœ¬æ€§é™åˆ¶ï¼Œå¦‚çŸ¥è¯†å»¶è¿Ÿã€æµ…å±‚æ¨ç†å’Œè®¤çŸ¥è¿‡ç¨‹å—é™ç­‰ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬è§è¯äº†â€œè¡ŒåŠ¨äºŒé˜¶æ®µâ€ï¼ˆå³å½“å‰é˜¶æ®µï¼‰çš„å‡ºç°ï¼Œåœ¨è¿™ä¸€é˜¶æ®µï¼Œæ¨¡å‹æ­£ä»éšæ€§ç©ºé—´ä¸­çš„çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿè¿‡æ¸¡åˆ°æµ‹è¯•æ—¶çš„æ€ç»´æ„å»ºå¼•æ“ã€‚è¿™æ ‡å¿—ç€é€šè¿‡ä¸è¯­è¨€ä¸ºåŸºç¡€çš„æ€ç»´è¿›è¡Œæ€ç»´çº§åˆ«çš„è¿æ¥æ¥å®ç°è®¤çŸ¥å·¥ç¨‹çš„ç†å¿µå‘å±•å’Œæ™®åŠã€‚è¿™ä¸€é˜¶æ®µè¿˜å¸¦æ¥äº†ç»¼åˆæ€§æ•™ç¨‹å’Œä¼˜åŒ–çš„å®ç°æ–¹å¼ï¼Œä¸ºå¤§ä¼—å‚ä¸è®¤çŸ¥å·¥ç¨‹æä¾›äº†æ°‘ä¸»åŒ–çš„æœºä¼šã€‚GitHubä»“åº“æä¾›äº†å…³äºæµ‹è¯•æ—¶è§„æ¨¡çš„æœ€æ–°è®ºæ–‡é›†åˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆæœŸé˜¶æ®µé€šè¿‡å¤§è§„æ¨¡å‚æ•°å’Œæ•°æ®å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µçš„å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨çŸ¥è¯†å»¶è¿Ÿã€æµ…å±‚æ¨ç†å’Œè®¤çŸ¥è¿‡ç¨‹å—é™ç­‰æ ¹æœ¬æ€§é™åˆ¶ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µçš„å¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨ä»çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿè¿‡æ¸¡åˆ°æ€ç»´æ„å»ºå¼•æ“ã€‚</li>
<li>è¿™ä¸€è¿‡æ¸¡ä»£è¡¨ç€ä¸AIçš„æ€ç»´æ–¹å¼çº§åˆ«çš„è¿æ¥ã€‚è¿™ä¸€æ–°é˜¶æ®µæ ‡å¿—ç€è®¤çŸ¥å·¥ç¨‹çš„å‘å±•ï¼Œä½¿AIèƒ½å¤Ÿè¿›è¡Œæ€ç»´çº§åˆ«äº¤äº’æˆä¸ºå¯èƒ½ã€‚ </li>
<li>æä¾›äº†ç»¼åˆæ•™ç¨‹å’Œä¼˜åŒ–å®ç°æ–¹å¼ï¼Œæ—¨åœ¨æ°‘ä¸»åŒ–è®¤çŸ¥å·¥ç¨‹é¢†åŸŸçš„æœºä¼šï¼Œè®©æ›´å¤šå®è·µè€…å‚ä¸è¿›æ¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c65054c31174da7001c1cb4e39ecd56b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbed99f1971aa9a935920c070ed7910e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-076aa713a9ea4b1edbd9677e7b854649.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ad5cabbcf3c6fe0a4f415e7cc2610f0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Analyzing-LLMsâ€™-Knowledge-Boundary-Cognition-Across-Languages-Through-the-Lens-of-Internal-Representations"><a href="#Analyzing-LLMsâ€™-Knowledge-Boundary-Cognition-Across-Languages-Through-the-Lens-of-Internal-Representations" class="headerlink" title="Analyzing LLMsâ€™ Knowledge Boundary Cognition Across Languages Through   the Lens of Internal Representations"></a>Analyzing LLMsâ€™ Knowledge Boundary Cognition Across Languages Through   the Lens of Internal Representations</h2><p><strong>Authors:Chenghao Xiao, Hou Pong Chan, Hao Zhang, Mahani Aljunied, Lidong Bing, Noura Al Moubayed, Yu Rong</strong></p>
<p>While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMsâ€™ perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMsâ€™ recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries">https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries</a>. </p>
<blockquote>
<p>äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†è¾¹ç•Œå¯¹äºé˜²æ­¢å…¶äº§ç”Ÿå¹»è§‰è‡³å…³é‡è¦ï¼Œä½†å…³äºå¤§å‹è¯­è¨€æ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä¸Šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•è¯†åˆ«ä¸åŒè¯­è¨€çš„çŸ¥è¯†è¾¹ç•Œï¼Œé€šè¿‡æ¢ç´¢å…¶åœ¨å¤„ç†å¤šç§è¯­è¨€çš„å·²çŸ¥å’ŒæœªçŸ¥é—®é¢˜æ—¶çš„å†…éƒ¨è¡¨å¾æ¥è¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶æ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼š1ï¼‰å¤§å‹è¯­è¨€æ¨¡å‹å¯¹çŸ¥è¯†è¾¹ç•Œçš„è®¤çŸ¥åœ¨ä¸åŒè¯­è¨€çš„ä¸­é—´åˆ°ä¸­ä¸Šå±‚æ¬¡ä¸­ç¼–ç ã€‚2ï¼‰çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥çš„è¯­è¨€å·®å¼‚éµå¾ªçº¿æ€§ç»“æ„ï¼Œè¿™æ¿€åŠ±æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¯¹é½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åœ¨ä¸åŒè¯­è¨€ä¸­è½¬ç§»çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæœ‰åŠ©äºé™ä½ä½èµ„æºè¯­è¨€ä¸­çš„å¹»è§‰é£é™©ï¼›3ï¼‰åœ¨åŒè¯­é—®é¢˜å¯¹ç¿»è¯‘ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„çŸ¥è¯†è¾¹ç•Œè¯†åˆ«èƒ½åŠ›ã€‚ç”±äºç¼ºä¹è·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œåˆ†æçš„æ ‡å‡†æµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…å«ä¸‰ç§å…·æœ‰ä»£è¡¨æ€§çš„çŸ¥è¯†è¾¹ç•Œæ•°æ®ç±»å‹ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundariesä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13816v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†LLMåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹å¦‚ä½•è¯†åˆ«çŸ¥è¯†è¾¹ç•Œã€‚é€šè¿‡å¯¹ä¸åŒè¯­è¨€ä¸­çš„å·²çŸ¥å’ŒæœªçŸ¥é—®é¢˜è¿›è¡Œå¤„ç†ï¼Œå‘ç°LLMçš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥ç¼–ç äºä¸­é—´è‡³ä¸­ä¸Šå±‚æ¬¡ã€‚è¯­è¨€é—´çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥å·®å¼‚å‘ˆç°çº¿æ€§ç»“æ„ï¼Œå¹¶æå‡ºä¸€ç§æ— éœ€è®­ç»ƒçš„å¯¹é½æ–¹æ³•ï¼Œä»¥æé«˜è·¨è¯­è¨€çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œé™ä½åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„å¹»è§‰é£é™©ã€‚é€šè¿‡å¾®è°ƒåŒè¯­é—®é¢˜å¯¹ç¿»è¯‘ï¼Œå¯è¿›ä¸€æ­¥æé«˜LLMçš„è·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œè¯†åˆ«èƒ½åŠ›ã€‚ç¼ºå°‘è·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œåˆ†æçš„æ ‡å‡†æµ‹è¯•å¹³å°ï¼Œå› æ­¤æˆ‘ä»¬æ„å»ºäº†åŒ…å«ä¸‰ç§ä»£è¡¨æ€§çŸ¥è¯†è¾¹ç•Œæ•°æ®çš„å¤šè¯­è¨€è¯„ä¼°å¥—ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„çŸ¥è¯†è¾¹ç•Œè¯†åˆ«ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ç¯å¢ƒä¸‹ï¼Œä½†è¯¥ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†è·¨è¯­è¨€ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¾¹ç•Œè¯†åˆ«ã€‚</li>
<li>LLMçš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥ç¼–ç å­˜åœ¨äºæ¨¡å‹ä¸­é—´è‡³ä¸­ä¸Šå±‚æ¬¡ï¼Œè¿™ä¸€å‘ç°æ­ç¤ºäº†æ¨¡å‹å†…éƒ¨å¦‚ä½•å¤„ç†çŸ¥è¯†è¾¹ç•Œã€‚</li>
<li>ä¸åŒè¯­è¨€é—´çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥å·®å¼‚å‘ˆç°çº¿æ€§ç»“æ„ï¼Œè¿™ä¸ºè·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥çš„å¯¹é½æä¾›äº†åŸºç¡€ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¯¹é½æ–¹æ³•ï¼Œä»¥æé«˜LLMåœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤æ–¹æ³•æœ‰åŠ©äºé™ä½åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„å¹»è§‰é£é™©ã€‚</li>
<li>é€šè¿‡å¾®è°ƒåŒè¯­é—®é¢˜å¯¹ç¿»è¯‘ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜LLMåœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¾¹ç•Œè¯†åˆ«æ•ˆæœã€‚</li>
<li>å½“å‰ç¼ºä¹è·¨è¯­è¨€çŸ¥è¯†è¾¹ç•Œåˆ†æçš„æ ‡å‡†æµ‹è¯•å¹³å°ï¼Œå› æ­¤æ„å»ºäº†åŒ…å«å¤šç§ç±»å‹æ•°æ®çš„å¤šè¯­è¨€è¯„ä¼°å¥—ä»¶ã€‚è¯¥å¥—ä»¶å¯ç”¨äºè¯„ä¼°LLMåœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¾¹ç•Œè¯†åˆ«èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13816">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c79af8087065fdbcec145c0a64f900ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81a30b9fd8216afe3d2d2a15d8d1a29e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b7cf8a973a128f0644f915fd8e6ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02333f787215458f5cd040ee16e40e21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-115bafcb6f1388a18b16d0da7cec3bf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b38779230f78d698df1b6f9f494178cf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Transformer-Encoder-and-Multi-features-Time2Vec-for-Financial-Prediction"><a href="#Transformer-Encoder-and-Multi-features-Time2Vec-for-Financial-Prediction" class="headerlink" title="Transformer Encoder and Multi-features Time2Vec for Financial Prediction"></a>Transformer Encoder and Multi-features Time2Vec for Financial Prediction</h2><p><strong>Authors:Nguyen Kim Hai Bui, Nguyen Duy Chien, PÃ©ter KovÃ¡cs, GergÅ‘ BognÃ¡r</strong></p>
<p>Financial prediction is a complex and challenging task of time series analysis and signal processing, expected to model both short-term fluctuations and long-term temporal dependencies. Transformers have remarkable success mostly in natural language processing using attention mechanism, which also influenced the time series community. The ability to capture both short and long-range dependencies helps to understand the financial market and to recognize price patterns, leading to successful applications of Transformers in stock prediction. Although, the previous research predominantly focuses on individual features and singular predictions, that limits the modelâ€™s ability to understand broader market trends. In reality, within sectors such as finance and technology, companies belonging to the same industry often exhibit correlated stock price movements.   In this paper, we develop a novel neural network architecture by integrating Time2Vec with the Encoder of the Transformer model. Based on the study of different markets, we propose a novel correlation feature selection method. Through a comprehensive fine-tuning of multiple hyperparameters, we conduct a comparative analysis of our results against benchmark models. We conclude that our method outperforms other state-of-the-art encoding methods such as positional encoding, and we also conclude that selecting correlation features enhance the accuracy of predicting multiple stock prices. </p>
<blockquote>
<p>é‡‘èé¢„æµ‹æ˜¯ä¸€é¡¹å¤æ‚ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´åºåˆ—åˆ†æå’Œä¿¡å·å¤„ç†ä»»åŠ¡ï¼Œè¦æ±‚å¯¹çŸ­æœŸæ³¢åŠ¨å’Œé•¿æœŸæ—¶é—´ä¾èµ–è¿›è¡Œå»ºæ¨¡ã€‚Transformerä¸»è¦åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œè¿™ä¹Ÿå½±å“äº†æ—¶é—´åºåˆ—ç¤¾åŒºã€‚æ•æ‰çŸ­æœŸå’Œé•¿æœŸä¾èµ–çš„èƒ½åŠ›æœ‰åŠ©äºç†è§£é‡‘èå¸‚åœºå¹¶è¯†åˆ«ä»·æ ¼æ¨¡å¼ï¼Œä»è€Œå¯¼è‡´Transformeråœ¨è‚¡ç¥¨é¢„æµ‹ä¸­çš„æˆåŠŸåº”ç”¨ã€‚ç„¶è€Œï¼Œä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸ªåˆ«ç‰¹å¾å’Œå•ä¸€é¢„æµ‹ä¸Šï¼Œè¿™é™åˆ¶äº†æ¨¡å‹äº†è§£æ›´å¹¿æ³›å¸‚åœºè¶‹åŠ¿çš„èƒ½åŠ›ã€‚å®é™…ä¸Šï¼Œåœ¨è¯¸å¦‚é‡‘èå’ŒæŠ€æœ¯ç­‰é¢†åŸŸå†…ï¼Œå±äºåŒä¸€è¡Œä¸šçš„å…¬å¸é€šå¸¸è¡¨ç°å‡ºç›¸å…³çš„è‚¡ç¥¨ä»·æ ¼å˜åŠ¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ•´åˆTime2Vecä¸Transformeræ¨¡å‹çš„ç¼–ç å™¨ï¼Œå¼€å‘äº†ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ã€‚åŸºäºå¯¹ä¸åŒå¸‚åœºçš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç›¸å…³æ€§ç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚é€šè¿‡å…¨é¢å¾®è°ƒå¤šä¸ªè¶…å‚æ•°ï¼Œæˆ‘ä»¬å¯¹æˆ‘ä»¬çš„ç»“æœä¸åŸºå‡†æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„ç¼–ç æ–¹æ³•ï¼Œå¦‚ä½ç½®ç¼–ç ï¼Œæˆ‘ä»¬è¿˜å¾—å‡ºç»“è®ºï¼Œé€‰æ‹©ç›¸å…³æ€§ç‰¹å¾æé«˜äº†é¢„æµ‹å¤šä¸ªè‚¡ç¥¨ä»·æ ¼çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13801v1">PDF</a> 5 pages, currently under review at Eusipco 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨é‡‘èé¢„æµ‹é¢†åŸŸï¼Œè¿ç”¨Transformeræ¨¡å‹ç»“åˆTime2VecæŠ€æœ¯çš„é‡è¦æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶ç ”ç©¶äº†ä¸€ç§æ–°çš„ç›¸å…³æ€§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œæ—¨åœ¨æ›´å¥½åœ°æ•æ‰é‡‘èå¸‚åœºçš„æ•´ä½“è¶‹åŠ¿å’Œä¸ªè‚¡ä»·æ ¼æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–ç¼–ç æ–¹æ³•å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èé¢„æµ‹æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦åŒæ—¶æ¨¡æ‹ŸçŸ­æœŸæ³¢åŠ¨å’Œé•¿æœŸæ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>Transformeræ¨¡å‹å› å…¶èƒ½å¤Ÿæ•æ‰çŸ­æœŸå’Œé•¿æœŸä¾èµ–æ€§çš„èƒ½åŠ›ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œé‡‘èé¢„æµ‹é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨ä¸ªä½“ç‰¹å¾å’Œå•ä¸€é¢„æµ‹ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹ç†è§£å¸‚åœºè¶‹åŠ¿çš„èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ•´åˆäº†Time2Vecå’ŒTransformeræ¨¡å‹çš„ç¼–ç å™¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç›¸å…³æ€§ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼ŒåŸºäºå¯¹ä¸åŒå¸‚åœºçš„ç ”ç©¶ã€‚</li>
<li>é€šè¿‡ç»¼åˆè°ƒæ•´å¤šä¸ªè¶…å‚æ•°ï¼Œæœ¬æ–‡çš„æ–¹æ³•è¢«è¯æ˜ä¼˜äºå…¶ä»–å…ˆè¿›çš„ç¼–ç æ–¹æ³•ï¼Œå¦‚ä½ç½®ç¼–ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7e876e4a69bbb35ec5f546d5e0ec2bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbfde11150c9c60bf16c772881372c7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b921bedec15e1b1738c83a46a9a2b570.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5494954a66e19242206d49cadf45187.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d65a7ab8b64cf6d3cf24d66e111a0ad2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BadApex-Backdoor-Attack-Based-on-Adaptive-Optimization-Mechanism-of-Black-box-Large-Language-Models"><a href="#BadApex-Backdoor-Attack-Based-on-Adaptive-Optimization-Mechanism-of-Black-box-Large-Language-Models" class="headerlink" title="BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of   Black-box Large Language Models"></a>BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of   Black-box Large Language Models</h2><p><strong>Authors:Zhengxian Wu, Juan Wen, Wanli Peng, Ziwei Zhang, Yinghan Zhou, Yiming Xue</strong></p>
<p>Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%. </p>
<blockquote>
<p>å…ˆå‰åŸºäºæ’å…¥å’Œæ”¹è¿°çš„åé—¨æŠ€æœ¯åœ¨æ”»å‡»æ•ˆæœä¸Šå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å®ƒä»¬å¿½è§†äº†æ–‡æœ¬è´¨é‡å’Œå¸¦æ¯’æ–‡æœ¬ä¸æ¸…æ´æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶å¼•å…¥äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå¸¦æ¯’æ–‡æœ¬ï¼Œå¹¶æé«˜äº†éšè”½æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡ï¼Œä½†å®ƒä»¬çš„æ‰‹å·¥æç¤ºä¾èµ–äºä¸“å®¶ç»éªŒï¼Œåœ¨é˜²å¾¡åé¢ä¸´ç€æç¤ºé€‚åº”æ€§å’Œæ”»å‡»æ€§èƒ½æ–¹é¢çš„é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶çš„æ–°å‹åé—¨æ”»å‡»ï¼ˆBadApexï¼‰ï¼Œå®ƒåˆ©ç”¨ä¸€ä¸ªå¤§å‹é»‘ç›’è¯­è¨€æ¨¡å‹é€šè¿‡æ”¹è¿›åçš„æç¤ºç”Ÿæˆå¸¦æ¯’æ–‡æœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶æ¥è¿­ä»£åœ°æ”¹è¿›åˆå§‹æç¤ºï¼Œä½¿ç”¨ç”Ÿæˆå’Œä¿®æ”¹ä»£ç†ã€‚ç”Ÿæˆä»£ç†åŸºäºåˆå§‹æç¤ºç”Ÿæˆå¸¦æ¯’æ–‡æœ¬ã€‚ç„¶åï¼Œä¿®æ”¹ä»£ç†è¯„ä¼°å¸¦æ¯’æ–‡æœ¬çš„è´¨é‡å¹¶æ”¹è¿›æ–°çš„æç¤ºã€‚ç»è¿‡å¤šæ¬¡è¿­ä»£ä¸Šè¿°è¿‡ç¨‹åï¼Œä½¿ç”¨æ”¹è¿›åçš„æç¤ºé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¸¦æ¯’æ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œé‡‡ç”¨äº†å…­ç§åé—¨æ”»å‡»å’Œä¸¤ç§é˜²å¾¡æ–¹æ³•ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBadApexæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•ã€‚å®ƒæé«˜äº†æç¤ºé€‚åº”æ€§ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚æ­¤å¤–ï¼Œå½“åº”ç”¨ä¸¤ç§é˜²å¾¡æ–¹æ³•æ—¶ï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ä»ç„¶é«˜è¾¾96.75%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13775v1">PDF</a> 16 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé»‘ç›’å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶çš„å…¨æ–°åé—¨æ”»å‡»æ–¹æ³•ï¼ˆBadApexï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é»‘ç›’LLMé€šè¿‡ç²¾ç»†åŒ–çš„æç¤ºç”Ÿæˆæ¯’åŒ–æ–‡æœ¬ï¼Œé‡‡ç”¨è‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶å¯¹åˆå§‹æç¤ºè¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜äº†æ”»å‡»æ•ˆæœã€è¯­ä¹‰ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚å®éªŒè¯æ˜ï¼ŒBadApexæ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ï¼Œå¹¶åœ¨åº”ç”¨ä¸¤ç§é˜²å¾¡æ–¹æ³•åï¼Œå¹³å‡æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾96.75%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BadApexæå‡ºäº†ä¸€ç§åŸºäºé»‘ç›’å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªé€‚åº”ä¼˜åŒ–æœºåˆ¶ï¼Œç”¨äºç”Ÿæˆæ¯’åŒ–æ–‡æœ¬ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£ä¼˜åŒ–åˆå§‹æç¤ºï¼Œæé«˜äº†æ”»å‡»æ•ˆæœã€è¯­ä¹‰ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚</li>
<li>BadApexæ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ã€‚</li>
<li>BadApexèƒ½å¤Ÿåœ¨åº”ç”¨ä¸¤ç§é˜²å¾¡æ–¹æ³•åï¼Œä»ä¿æŒè¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>ç”Ÿæˆä»£ç†æ ¹æ®åˆå§‹æç¤ºç”Ÿæˆæ¯’åŒ–æ–‡æœ¬ã€‚</li>
<li>ä¿®æ”¹ä»£ç†è¯„ä¼°æ¯’åŒ–æ–‡æœ¬çš„è´¨é‡å¹¶ä¼˜åŒ–æ–°æç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78d37a1eddd890176d94f01885907360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1e52b8905ee8feae5b7b8582f515c8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e9259fa3ae90e67c296026e73b7dd16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39620ff84947d85bf21cc564dd0c939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f70f0203fce33eb75430285a0c215c6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DP2Unlearning-An-Efficient-and-Guaranteed-Unlearning-Framework-for-LLMs"><a href="#DP2Unlearning-An-Efficient-and-Guaranteed-Unlearning-Framework-for-LLMs" class="headerlink" title="DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs"></a>DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs</h2><p><strong>Authors:Tamim Al Mahmud, Najeeb Jebreel, Josep Domingo-Ferrer, David Sanchez</strong></p>
<p>Large language models (LLMs) have recently revolutionized language processing tasks but have also brought ethical and legal issues. LLMs have a tendency to memorize potentially private or copyrighted information present in the training data, which might then be delivered to end users at inference time. When this happens, a naive solution is to retrain the model from scratch after excluding the undesired data. Although this guarantees that the target data have been forgotten, it is also prohibitively expensive for LLMs. Approximate unlearning offers a more efficient alternative, as it consists of ex post modifications of the trained model itself to prevent undesirable results, but it lacks forgetting guarantees because it relies solely on empirical evidence. In this work, we present DP2Unlearning, a novel LLM unlearning framework that offers formal forgetting guarantees at a significantly lower cost than retraining from scratch on the data to be retained. DP2Unlearning involves training LLMs on textual data protected using {\epsilon}-differential privacy (DP), which later enables efficient unlearning with the guarantees against disclosure associated with the chosen {\epsilon}. Our experiments demonstrate that DP2Unlearning achieves similar model performance post-unlearning, compared to an LLM retraining from scratch on retained data â€“ the gold standard exact unlearning â€“ but at approximately half the unlearning cost. In addition, with a reasonable computational cost, it outperforms approximate unlearning methods at both preserving the utility of the model post-unlearning and effectively forgetting the targeted information. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶ä¸ºè¯­è¨€å¤„ç†ä»»åŠ¡å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†ä¹Ÿå¼•å‘äº†ä¼¦ç†å’Œæ³•å¾‹é—®é¢˜ã€‚LLMå€¾å‘äºè®°å¿†è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨çš„æ½œåœ¨ç§å¯†æˆ–ç‰ˆæƒä¿¡æ¯ï¼Œåœ¨æ¨ç†é˜¶æ®µå¯èƒ½ä¼šå°†è¿™äº›ä¿¡æ¯æä¾›ç»™æœ€ç»ˆç”¨æˆ·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨æ’é™¤ä¸éœ€è¦çš„æ•°æ®åä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è™½ç„¶è¿™å¯ä»¥ä¿è¯ç›®æ ‡æ•°æ®å·²è¢«é—å¿˜ï¼Œä½†å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œæˆæœ¬å´æä¸ºé«˜æ˜‚ã€‚è¿‘ä¼¼é—å¿˜æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒåŒ…æ‹¬å¯¹å·²è®­ç»ƒæ¨¡å‹æœ¬èº«çš„åç»­ä¿®æ”¹ï¼Œä»¥é˜²æ­¢å‡ºç°ä¸ç†æƒ³çš„ç»“æœï¼Œä½†å®ƒç¼ºä¹é—å¿˜ä¿è¯ï¼Œå› ä¸ºå®ƒåªä¾èµ–äºç»éªŒè¯æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DP2Unlearningï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹é—å¿˜æ¡†æ¶ï¼Œå®ƒæä¾›äº†æ­£å¼çš„é—å¿˜ä¿è¯ï¼Œå¹¶ä¸”åœ¨è¦ä¿ç•™çš„æ•°æ®ä¸Šä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„æˆæœ¬å¤§å¤§é™ä½ã€‚DP2Unlearningæ¶‰åŠåœ¨å—Îµ-å·®åˆ†éšç§ä¿æŠ¤çš„æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆDPï¼‰ï¼Œè¿™åæ¥ä½¿å¾—å¯ä»¥é€šè¿‡ä¸æ‰€é€‰Îµç›¸å…³çš„ä¿è¯è¿›è¡Œé«˜æ•ˆçš„é—å¿˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDP2Unlearningåœ¨é—å¿˜åçš„æ¨¡å‹æ€§èƒ½ä¸åœ¨ä¿ç•™æ•°æ®ä¸Šä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆé»„é‡‘æ ‡å‡†çš„ç²¾ç¡®é—å¿˜ï¼‰ç›¸ä¼¼ï¼Œä½†é—å¿˜æˆæœ¬é™ä½äº†å¤§çº¦ä¸€åŠã€‚æ­¤å¤–ï¼Œå®ƒä»¥åˆç†çš„è®¡ç®—æˆæœ¬è¶…è¶Šäº†è¿‘ä¼¼é—å¿˜æ–¹æ³•åœ¨ä¿æŒæ¨¡å‹å®ç”¨æ€§å’Œæœ‰æ•ˆé—å¿˜ç›®æ ‡ä¿¡æ¯æ–¹é¢çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13774v1">PDF</a> 49 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¯­è¨€ä»»åŠ¡çš„åŒæ—¶ï¼Œå¼•å‘äº†ä¼¦ç†å’Œæ³•å¾‹é—®é¢˜ã€‚LLMæœ‰è®°å¿†è®­ç»ƒæ•°æ®ä¸­æ½œåœ¨ç§å¯†æˆ–ç‰ˆæƒä¿¡æ¯çš„è¶‹åŠ¿ï¼Œå¯èƒ½åœ¨æ¨ç†é˜¶æ®µå°†è¿™äº›ä¿¡æ¯æä¾›ç»™ç”¨æˆ·ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ‰äººé€‰æ‹©é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥æ’é™¤ä¸éœ€è¦çš„æ•°æ®ï¼Œè™½ç„¶è¿™èƒ½ç¡®ä¿é—å¿˜ç›®æ ‡æ•°æ®ï¼Œä½†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è€Œè¨€æˆæœ¬è¿‡é«˜ã€‚è¿‘ä¼¼é—å¿˜åˆ™æä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå®ƒé€šè¿‡å¯¹å·²è®­ç»ƒæ¨¡å‹æœ¬èº«çš„åæœŸä¿®æ”¹æ¥é˜²æ­¢ä¸ç†æƒ³çš„ç»“æœï¼Œä½†ç¼ºä¹é—å¿˜ä¿è¯ï¼Œå› ä¸ºå®ƒåªä¾èµ–ç»éªŒè¯æ®ã€‚æœ¬ç ”ç©¶æå‡ºäº†DP2Unlearningï¼Œä¸€ç§æ–°å‹LLMé—å¿˜æ¡†æ¶ï¼Œä»¥è¾ƒä½çš„æˆæœ¬æä¾›äº†å½¢å¼åŒ–çš„é—å¿˜ä¿è¯ã€‚DP2Unlearningé€šè¿‡è®­ç»ƒå—Îµ-å·®åˆ†éšç§ä¿æŠ¤çš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒLLMï¼Œä¹‹åå¯é€šè¿‡æ‰€é€‰çš„Îµå¯¹æŠ—æŠ«éœ²æ¥å®ç°é«˜æ•ˆé—å¿˜ã€‚å®éªŒè¡¨æ˜ï¼ŒDP2Unlearningåœ¨é—å¿˜åçš„æ¨¡å‹æ€§èƒ½ä¸ä»ä¿ç•™æ•°æ®å¼€å§‹é‡æ–°è®­ç»ƒçš„LLMç›¸ä¼¼ï¼Œä½†æˆæœ¬å¤§çº¦æ˜¯é‡æ–°è®­ç»ƒçš„ä¸€åŠã€‚æ­¤å¤–ï¼Œå®ƒä»¥åˆç†çš„è®¡ç®—æˆæœ¬è¶…è¶Šäº†è¿‘ä¼¼é—å¿˜æ–¹æ³•ï¼Œæ—¢ä¿ç•™äº†æ¨¡å‹çš„å®ç”¨æ€§ï¼Œåˆæœ‰æ•ˆåœ°å¿˜è®°äº†ç›®æ ‡ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¯­è¨€ä»»åŠ¡æ—¶é¢ä¸´ä¼¦ç†å’Œæ³•å¾‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯å…³äºè®°å¿†å’Œæä¾›æ½œåœ¨ç§å¯†æˆ–ç‰ˆæƒä¿¡æ¯çš„é£é™©ã€‚</li>
<li>é‡è®­æ¨¡å‹ä»¥æ’é™¤ä¸éœ€è¦çš„æ•°æ®æ˜¯ç¡®ä¿é—å¿˜çš„æ–¹æ³•ï¼Œä½†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è€Œè¨€æˆæœ¬è¿‡é«˜ã€‚</li>
<li>è¿‘ä¼¼é—å¿˜æ˜¯ä¸€ç§æ›´é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç¼ºä¹é—å¿˜ä¿è¯ã€‚</li>
<li>DP2Unlearningæ˜¯ä¸€ç§æ–°å‹LLMé—å¿˜æ¡†æ¶ï¼Œç»“åˆäº†Îµ-å·®åˆ†éšç§ä¿æŠ¤æ¥è®­ç»ƒæ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆé—å¿˜ä¸”æä¾›äº†å½¢å¼åŒ–çš„é—å¿˜ä¿è¯ã€‚</li>
<li>DP2Unlearningçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨é—å¿˜åçš„æ¨¡å‹æ€§èƒ½ä¸é‡æ–°è®­ç»ƒçš„LLMç›¸ä¼¼ï¼Œä½†æˆæœ¬æ›´ä½ã€‚</li>
<li>DP2Unlearningåœ¨ä¿ç•™æ¨¡å‹å®ç”¨æ€§çš„åŒæ—¶ï¼Œæœ‰æ•ˆåœ°å¿˜è®°äº†ç›®æ ‡ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a4749d934b89d4cfc3ac6997730d5a9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Decoding-Vision-Transformers-the-Diffusion-Steering-Lens"><a href="#Decoding-Vision-Transformers-the-Diffusion-Steering-Lens" class="headerlink" title="Decoding Vision Transformers: the Diffusion Steering Lens"></a>Decoding Vision Transformers: the Diffusion Steering Lens</h2><p><strong>Authors:Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai</strong></p>
<p>Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. </p>
<blockquote>
<p>Logit Lens æ˜¯å¹¿æ³›åº”ç”¨äºåŸºäº transformer çš„è¯­è¨€æ¨¡å‹çš„æœºæ¢°è§£é‡Šæ€§æ–¹æ³•ï¼Œé€šè¿‡å°†å†…éƒ¨è¡¨ç¤ºæŠ•å½±åˆ°è¾“å‡ºè¯æ±‡ç©ºé—´æ¥åˆ†æå®ƒä»¬åœ¨å„å±‚å¦‚ä½•æ¼”å˜ã€‚è™½ç„¶å°† Logit Lens åº”ç”¨äºè§†è§‰ transformerï¼ˆViTsï¼‰åœ¨æŠ€æœ¯ä¸Šå¾ˆç›´æ¥ï¼Œä½†å…¶ç›´æ¥ä½¿ç”¨åœ¨æ•æ‰è§†è§‰è¡¨ç¤ºçš„ä¸°å¯Œæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åŸºäºTokerç­‰äººï¼ˆ2024ï¼‰çš„å·¥ä½œï¼Œä»–ä»¬å¼•å…¥äº†Diffusion Lensæ¥å¯è§†åŒ–æ–‡æœ¬ç¼–ç å™¨ä¸­æ–‡å­—åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸­é—´è¡¨ç¤ºï¼Œæˆ‘ä»¬è¯æ˜è™½ç„¶Diffusion Lenså¯ä»¥æœ‰æ•ˆåœ°å¯è§†åŒ–å›¾åƒç¼–ç å™¨çš„æ®‹å·®æµè¡¨ç¤ºï¼Œä½†å®ƒæ— æ³•æ•æ‰å•ä¸ªå­æ¨¡å—çš„ç›´æ¥è´¡çŒ®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Diffusion Steering Lensï¼ˆDSLï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå¯ä»¥å¼•å¯¼å­æ¨¡å—è¾“å‡ºå¹¶ä¿®è¡¥éšåçš„é—´æ¥è´¡çŒ®ã€‚æˆ‘ä»¬é€šè¿‡å¹²é¢„ç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¡¨æ˜DSLä¸ºViTsçš„å†…éƒ¨å¤„ç†æä¾›äº†ç›´è§‚å’Œå¯é çš„è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13763v1">PDF</a> 12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on   Mechanistic Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>Logit Lensæ–¹æ³•å¹¿æ³›åº”ç”¨äºè§£é‡ŠåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œä½†ç›´æ¥åº”ç”¨äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•â€”â€”Diffusion Steering Lensï¼ˆDSLï¼‰ï¼Œæ—¨åœ¨å¯è§†åŒ–è§†è§‰è½¬æ¢å™¨çš„å†…éƒ¨å¤„ç†è¿‡ç¨‹ï¼Œé€šè¿‡å¹²é¢„æ€§ç ”ç©¶éªŒè¯äº†å…¶ç›´è§‚æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Logit Lenså¹¿æ³›åº”ç”¨äºè§£é‡ŠåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å°†å†…éƒ¨è¡¨ç¤ºæŠ•å½±åˆ°è¾“å‡ºè¯æ±‡ç©ºé—´æ¥è¿›è¡Œåˆ†æã€‚</li>
<li>è™½ç„¶Logit Lensç›´æ¥åº”ç”¨äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æŠ€æœ¯ä¸Šå¾ˆç›´è§‚ï¼Œä½†åœ¨æ•æ‰ä¸°å¯Œçš„è§†è§‰è¡¨ç¤ºæ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Diffusion Lensè™½ç„¶èƒ½æœ‰æ•ˆå¯è§†åŒ–å›¾åƒç¼–ç å™¨çš„æ®‹å·®æµè¡¨ç¤ºï¼Œä½†æ— æ³•æ•æ‰å•ä¸ªå­æ¨¡å—çš„ç›´æ¥å½±å“ã€‚</li>
<li>é’ˆå¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæå‡ºäº†æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•â€”â€”Diffusion Steering Lensï¼ˆDSLï¼‰ã€‚</li>
<li>DSLé€šè¿‡å¹²é¢„æ€§ç ”ç©¶éªŒè¯äº†å…¶ç›´è§‚æ€§å’Œå¯é æ€§ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è§£é‡ŠViTsçš„å†…éƒ¨å¤„ç†è¿‡ç¨‹ã€‚</li>
<li>DSLæ–¹æ³•å¯ä»¥ç›´è§‚åœ°å¯è§†åŒ–å„ä¸ªå­æ¨¡å—çš„è¾“å‡ºä»¥åŠåç»­çš„é—´æ¥è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b25fc11f3869a7624d2b8a6506569ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bd47b1cfa834935a674babcfe93332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1568148bc789594619a449bd8dc24b24.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Controlled-Territory-and-Conflict-Tracking-CONTACT-Geo-Mapping-Occupied-Territory-from-Open-Source-Intelligence"><a href="#Controlled-Territory-and-Conflict-Tracking-CONTACT-Geo-Mapping-Occupied-Territory-from-Open-Source-Intelligence" class="headerlink" title="Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping   Occupied Territory from Open Source Intelligence"></a>Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping   Occupied Territory from Open Source Intelligence</h2><p><strong>Authors:Paul K. Mandal, Cole Leo, Connor Hurley</strong></p>
<p>Open-source intelligence provides a stream of unstructured textual data that can inform assessments of territorial control. We present CONTACT, a framework for territorial control prediction using large language models (LLMs) and minimal supervision. We evaluate two approaches: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. Our model is trained on a small hand-labeled dataset of news articles covering ISIS activity in Syria and Iraq, using prompt-conditioned extraction of control-relevant signals such as military operations, casualties, and location references. We show that the BLOOMZ-based model outperforms the SetFit baseline, and that prompt-based supervision improves generalization in low-resource settings. CONTACT demonstrates that LLMs fine-tuned using few-shot methods can reduce annotation burdens and support structured inference from open-ended OSINT streams. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/PaulKMandal/CONTACT/">https://github.com/PaulKMandal/CONTACT/</a>. </p>
<blockquote>
<p>å¼€æºæƒ…æŠ¥ï¼ˆOSINTï¼‰æä¾›äº†ä¸€ç³»åˆ—éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯ä»¥ä¸ºå¯¹é¢†åœŸæ§åˆ¶çš„è¯„ä¼°æä¾›ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†CONTACTï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé¢†åœŸæ§åˆ¶é¢„æµ‹çš„æœ€å°ç›‘ç£æ¡†æ¶ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸¤ç§æ–¹æ³•ï¼šSetFitï¼Œä¸€ç§åŸºäºåµŒå…¥çš„å°‘é‡æ ·æœ¬åˆ†ç±»å™¨ï¼Œä»¥åŠåº”ç”¨äºBLOOMZ-560mçš„å¤šè¯­è¨€ç”ŸæˆLLMçš„æç¤ºè°ƒæ•´æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªå…³äºå™åˆ©äºšå’Œä¼Šæ‹‰å…‹ä¼Šæ–¯å…°å›½æ´»åŠ¨çš„æ‰‹å·¥æ ‡æ³¨æ–°é—»æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½¿ç”¨æç¤ºæ¡ä»¶ä¸‹çš„æ§åˆ¶ç›¸å…³ä¿¡å·çš„æå–ï¼Œå¦‚å†›äº‹è¡ŒåŠ¨ã€ä¼¤äº¡å’Œä½ç½®å¼•ç”¨ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒåŸºäºBLOOMZçš„æ¨¡å‹ä¼˜äºSetFitåŸºçº¿ï¼Œå¹¶ä¸”åŸºäºæç¤ºçš„ç›‘ç£æ”¹è¿›äº†ä½èµ„æºç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚CONTACTè¯æ˜ï¼Œä½¿ç”¨å°‘é‡æ ·æœ¬æ–¹æ³•å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å‡è½»æ ‡æ³¨è´Ÿæ‹…å¹¶æ”¯æŒä»å¼€æ”¾çš„OSINTæµä¸­è¿›è¡Œç»“æ„åŒ–æ¨æ–­ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/PaulKMandal/CONTACT/">https://github.com/PaulKMandal/CONTACT/</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13730v1">PDF</a> 7 pages, 1 figure, 1 table</p>
<p><strong>Summary</strong><br>å¼€æºæƒ…æŠ¥ä¸ºè¯„ä¼°å’Œé¢„æµ‹é¢†åœŸæ§åˆ¶æä¾›äº†å¤§é‡éç»“æ„åŒ–çš„æ–‡æœ¬æ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé¢†åœŸæ§åˆ¶é¢„æµ‹çš„æ¡†æ¶CONTACTï¼Œå¹¶é‡‡ç”¨äº†æœ€å°ç›‘ç£æ–¹å¼ã€‚æˆ‘ä»¬è¯„ä¼°äº†SetFitå’ŒBLOOMZ-560mä¸¤ç§æ¨¡å‹ï¼Œå…¶ä¸­SetFitæ˜¯åŸºäºåµŒå…¥çš„å°‘æ•°æ ·æœ¬åˆ†ç±»å™¨ï¼Œè€ŒBLOOMZåˆ™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€ç”ŸæˆLLMæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡æ‰‹åŠ¨çš„æ–°é—»æ•°æ®é›†è®­ç»ƒï¼Œå…³æ³¨å™åˆ©äºšå’Œä¼Šæ‹‰å…‹çš„ISISæ´»åŠ¨ã€‚é€šè¿‡æç¤ºæ§åˆ¶ç›¸å…³ä¿¡å·ï¼ˆå¦‚å†›äº‹è¡ŒåŠ¨ã€ä¼¤äº¡å’Œåœ°ç‚¹å‚è€ƒï¼‰ï¼Œæˆ‘ä»¬å‘ç°åŸºäºBLOOMZçš„æ¨¡å‹ä¼˜äºSetFitåŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”åŸºäºæç¤ºçš„ç›‘ç£æ–¹å¼åœ¨ä½èµ„æºç¯å¢ƒä¸­æœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚CONTACTè¯æ˜äº†ä½¿ç”¨å°‘æ•°æ ·æœ¬æ–¹æ³•çš„LLMå¾®è°ƒå¯ä»¥å‡å°‘æ ‡æ³¨è´Ÿæ‹…å¹¶æ”¯æŒä»å¼€æ”¾å¼æƒ…æŠ¥æµä¸­è¿›è¡Œç»“æ„åŒ–æ¨æ–­ã€‚æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼€æºæƒ…æŠ¥ä¸ºéç»“æ„åŒ–æ–‡æœ¬æ•°æ®æä¾›äº†å…³äºé¢†åœŸæ§åˆ¶çš„è¯„ä¼°ä¿¡æ¯ã€‚</li>
<li>CONTACTæ¡†æ¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé¢†åœŸæ§åˆ¶é¢„æµ‹ã€‚</li>
<li>è¯„ä¼°äº†SetFitå’ŒBLOOMZä¸¤ç§æ¨¡å‹åœ¨é¢†åœŸæ§åˆ¶é¢„æµ‹ä¸­çš„è¡¨ç°ã€‚</li>
<li>BLOOMZæ¨¡å‹åœ¨é¢„æµ‹ä¸­è¡¨ç°æ›´å¥½ï¼Œä¸”åŸºäºæç¤ºçš„ç›‘ç£æ–¹å¼æœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CONTACTè¯æ˜äº†ä½¿ç”¨å°‘æ•°æ ·æœ¬æ–¹æ³•çš„LLMå¾®è°ƒå¯ä»¥å‡å°‘æ ‡æ³¨è´Ÿæ‹…å¹¶æ”¯æŒç»“æ„åŒ–æ¨æ–­ã€‚</li>
<li>CONTACTæ¡†æ¶é€‚ç”¨äºå¤„ç†æ¶‰åŠå†›äº‹è¡ŒåŠ¨ã€ä¼¤äº¡å’Œåœ°ç‚¹å‚è€ƒç­‰æ§åˆ¶ç›¸å…³ä¿¡å·çš„æƒ…å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d446e301899abbaaa1dd7085cd57c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-726309b29eaa47fb59ba9f9aa132deb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c752dc2c35694723ab4b42b14f6cd1b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Exploring-Multimodal-Prompt-for-Visualization-Authoring-with-Large-Language-Models"><a href="#Exploring-Multimodal-Prompt-for-Visualization-Authoring-with-Large-Language-Models" class="headerlink" title="Exploring Multimodal Prompt for Visualization Authoring with Large   Language Models"></a>Exploring Multimodal Prompt for Visualization Authoring with Large   Language Models</h2><p><strong>Authors:Zhen Wen, Luoxuan Weng, Yinghao Tang, Runjin Zhang, Yuxin Liu, Bo Pan, Minfeng Zhu, Wei Chen</strong></p>
<p>Recent advances in large language models (LLMs) have shown great potential in automating the process of visualization authoring through simple natural language utterances. However, instructing LLMs using natural language is limited in precision and expressiveness for conveying visualization intent, leading to misinterpretation and time-consuming iterations. To address these limitations, we conduct an empirical study to understand how LLMs interpret ambiguous or incomplete text prompts in the context of visualization authoring, and the conditions making LLMs misinterpret user intent. Informed by the findings, we introduce visual prompts as a complementary input modality to text prompts, which help clarify user intent and improve LLMsâ€™ interpretation abilities. To explore the potential of multimodal prompting in visualization authoring, we design VisPilot, which enables users to easily create visualizations using multimodal prompts, including text, sketches, and direct manipulations on existing visualizations. Through two case studies and a controlled user study, we demonstrate that VisPilot provides a more intuitive way to create visualizations without affecting the overall task efficiency compared to text-only prompting approaches. Furthermore, we analyze the impact of text and visual prompts in different visualization tasks. Our findings highlight the importance of multimodal prompting in improving the usability of LLMs for visualization authoring. We discuss design implications for future visualization systems and provide insights into how multimodal prompts can enhance human-AI collaboration in creative visualization tasks. All materials are available at <a target="_blank" rel="noopener" href="https://osf.io/2QRAK">https://OSF.IO/2QRAK</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œé€šè¿‡ç®€å•çš„è‡ªç„¶è¯­è¨€è¡¨è¿°è‡ªåŠ¨åŒ–å¯è§†åŒ–åˆ›ä½œè¿‡ç¨‹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡å¯¼LLMåœ¨ä¼ è¾¾å¯è§†åŒ–æ„å›¾æ–¹é¢çš„ç²¾ç¡®æ€§å’Œè¡¨ç°åŠ›æœ‰é™ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯¯è§£å’Œè€—æ—¶çš„è¿­ä»£ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å®è¯ç ”ç©¶ï¼Œäº†è§£LLMå¦‚ä½•åœ¨å¯è§†åŒ–åˆ›ä½œçš„èƒŒæ™¯ä¸‹è§£é‡Šæ¨¡ç³Šæˆ–ä¸å®Œæ•´çš„æ–‡æœ¬æç¤ºï¼Œä»¥åŠå¯¼è‡´LLMè¯¯è§£ç”¨æˆ·æ„å›¾çš„æ¡ä»¶ã€‚æ ¹æ®ç ”ç©¶ç»“æœï¼Œæˆ‘ä»¬å¼•å…¥è§†è§‰æç¤ºä½œä¸ºæ–‡æœ¬æç¤ºçš„è¡¥å……è¾“å…¥æ¨¡å¼ï¼Œæœ‰åŠ©äºæ¾„æ¸…ç”¨æˆ·æ„å›¾å¹¶å¢å¼ºLLMçš„è§£é‡Šèƒ½åŠ›ã€‚ä¸ºäº†æ¢ç´¢å¤šæ¨¡å¼æç¤ºåœ¨å¯è§†åŒ–åˆ›ä½œä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†VisPilotï¼Œå®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾ä½¿ç”¨å¤šæ¨¡å¼æç¤ºåˆ›å»ºå¯è§†åŒ–ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾å’Œå¯¹ç°æœ‰å¯è§†åŒ–çš„ç›´æ¥æ“ä½œã€‚é€šè¿‡ä¸¤ä¸ªæ¡ˆä¾‹ç ”ç©¶å’Œä¸€é¡¹å—æ§çš„ç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜VisPilotæä¾›äº†ä¸€ç§æ›´ç›´è§‚çš„æ–¹å¼æ¥åˆ›å»ºå¯è§†åŒ–ï¼Œè€Œä¸”ä¸ä¼šå½±å“ä¸ä»…ä½¿ç”¨æ–‡æœ¬æç¤ºçš„æ–¹æ³•ç›¸æ¯”çš„æ€»ä½“ä»»åŠ¡æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†æ–‡æœ¬å’Œè§†è§‰æç¤ºåœ¨ä¸åŒå¯è§†åŒ–ä»»åŠ¡ä¸­çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šæ¨¡å¼æç¤ºåœ¨æé«˜LLMåœ¨å¯è§†åŒ–åˆ›ä½œä¸­çš„å¯ç”¨æ€§æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚æˆ‘ä»¬è®¨è®ºäº†æœªæ¥å¯è§†åŒ–ç³»ç»Ÿçš„è®¾è®¡å½±å“ï¼Œå¹¶æä¾›äº†å¤šæ¨¡å¼æç¤ºå¦‚ä½•å¢å¼ºåˆ›æ„å¯è§†åŒ–ä»»åŠ¡ä¸­çš„äººæœºåä½œçš„è§è§£ã€‚æ‰€æœ‰ææ–™å‡å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://osf.io/2QRAK%E8%8E%B7%E5%8F%96%E3%80%82">https://OSF.IO/2QRAKè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13700v1">PDF</a> 11 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–å¯è§†åŒ–åˆ›ä½œæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ—¶å­˜åœ¨ç²¾åº¦å’Œè¡¨è¾¾å—é™çš„é—®é¢˜ï¼Œå®¹æ˜“å¯¼è‡´è¯¯è§£å’Œè€—æ—¶è¿­ä»£ã€‚ä¸ºç ”ç©¶LLMå¦‚ä½•è§£é‡Šå¯è§†åŒ–åˆ›ä½œä¸­çš„æ¨¡ç³Šæˆ–ä¸å®Œæ•´æ–‡æœ¬æç¤ºï¼Œä»¥åŠå¯¼è‡´è¯¯è§£çš„æ¡ä»¶ï¼Œæˆ‘ä»¬å¼•å…¥è§†è§‰æç¤ºä½œä¸ºæ–‡æœ¬æç¤ºçš„è¡¥å……è¾“å…¥æ¨¡å¼ï¼Œä»¥æ¾„æ¸…ç”¨æˆ·æ„å›¾å¹¶æé«˜LLMçš„è§£é‡Šèƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†VisPilotç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æ¨¡å¼æç¤ºï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾å’Œå¯¹ç°æœ‰å¯è§†åŒ–çš„ç›´æ¥æ“ä½œã€‚é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å’Œå—æ§ç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°VisPilotä»¥æ›´ç›´è§‚çš„æ–¹å¼åˆ›å»ºå¯è§†åŒ–ï¼Œä¸å½±å“æ•´ä½“ä»»åŠ¡æ•ˆç‡ï¼Œä¸”åœ¨ä¸åŒçš„å¯è§†åŒ–ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬å’Œè§†è§‰æç¤ºçš„å½±å“æ˜¾è‘—ã€‚è¿™å¼ºè°ƒäº†å¤šæ¨¡å¼æç¤ºåœ¨æé«˜LLMå¯è§†åŒ–åˆ›ä½œå¯ç”¨æ€§å’Œäººæœºåä½œåˆ›æ„å¯è§†åŒ–ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªåŠ¨åŒ–å¯è§†åŒ–åˆ›ä½œä¸Šæœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†è‡ªç„¶è¯­è¨€æŒ‡ä»¤å­˜åœ¨ç²¾åº¦å’Œè¡¨è¾¾å—é™çš„é—®é¢˜ã€‚</li>
<li>æ¨¡ç³Šæˆ–ä¸å®Œæ•´çš„æ–‡æœ¬æç¤ºä¼šå¯¼è‡´LLMè¯¯è§£ç”¨æˆ·æ„å›¾ã€‚</li>
<li>è§†è§‰æç¤ºä½œä¸ºè¡¥å……è¾“å…¥æ¨¡å¼ï¼Œæœ‰åŠ©äºæ¾„æ¸…ç”¨æˆ·æ„å›¾å¹¶æé«˜LLMçš„è§£é‡Šèƒ½åŠ›ã€‚</li>
<li>VisPilotç³»ç»Ÿæ”¯æŒå¤šç§æ¨¡å¼æç¤ºï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è‰å›¾å’Œå¯¹ç°æœ‰å¯è§†åŒ–çš„ç›´æ¥æ“ä½œã€‚</li>
<li>VisPilotä»¥æ›´ç›´è§‚çš„æ–¹å¼åˆ›å»ºå¯è§†åŒ–ï¼Œä¸å½±å“æ•´ä½“ä»»åŠ¡æ•ˆç‡ã€‚</li>
<li>åœ¨ä¸åŒçš„å¯è§†åŒ–ä»»åŠ¡ä¸­ï¼Œæ–‡æœ¬å’Œè§†è§‰æç¤ºçš„å½±å“æ˜¾è‘—ã€‚</li>
<li>å¤šæ¨¡å¼æç¤ºå¯¹æé«˜LLMåœ¨å¯è§†åŒ–åˆ›ä½œä¸­çš„å¯ç”¨æ€§å’Œäººæœºåä½œåˆ›æ„å¯è§†åŒ–ä»»åŠ¡çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f32df17fdc15a0dae6324b515d7ff212.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbbf0a1514fc8fcc762cbe8c1d5c0f87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d71d572bfd9e85805439fe708d38aa6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1b60709183e59708a8fb11658486068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ddfdf36496ae53be64903623d460353.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab292187d8c7d11286ee86429f423506.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Intelligent-Interaction-Strategies-for-Context-Aware-Cognitive-Augmentation"><a href="#Intelligent-Interaction-Strategies-for-Context-Aware-Cognitive-Augmentation" class="headerlink" title="Intelligent Interaction Strategies for Context-Aware Cognitive   Augmentation"></a>Intelligent Interaction Strategies for Context-Aware Cognitive   Augmentation</h2><p><strong>Authors: Xiangrong,  Zhu, Yuan Xu, Tianjian Liu, Jingwei Sun, Yu Zhang, Xin Tong</strong></p>
<p>Human cognition is constrained by processing limitations, leading to cognitive overload and inefficiencies in knowledge synthesis and decision-making. Large Language Models (LLMs) present an opportunity for cognitive augmentation, but their current reactive nature limits their real-world applicability. This position paper explores the potential of context-aware cognitive augmentation, where LLMs dynamically adapt to usersâ€™ cognitive states and task environments to provide appropriate support. Through a think-aloud study in an exhibition setting, we examine how individuals interact with multi-modal information and identify key cognitive challenges in structuring, retrieving, and applying knowledge. Our findings highlight the need for AI-driven cognitive support systems that integrate real-time contextual awareness, personalized reasoning assistance, and socially adaptive interactions. We propose a framework for AI augmentation that seamlessly transitions between real-time cognitive support and post-experience knowledge organization, contributing to the design of more effective human-centered AI systems. </p>
<blockquote>
<p>äººç±»çš„è®¤çŸ¥å—åˆ°å¤„ç†èƒ½åŠ›çš„é™åˆ¶ï¼Œå¯¼è‡´åœ¨çŸ¥è¯†åˆæˆå’Œå†³ç­–åˆ¶å®šä¸­å‡ºç°è®¤çŸ¥è¿‡è½½å’Œæ•ˆç‡ä½ä¸‹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè®¤çŸ¥å¢å¼ºæä¾›äº†æœºä¼šï¼Œä½†å®ƒä»¬å½“å‰çš„ååº”æ€§è´¨é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†è¯­å¢ƒæ„ŸçŸ¥è®¤çŸ¥å¢å¼ºçš„æ½œåŠ›ï¼Œå…¶ä¸­LLMåŠ¨æ€é€‚åº”ç”¨æˆ·çš„è®¤çŸ¥çŠ¶æ€å’Œä»»åŠ¡ç¯å¢ƒï¼Œä»¥æä¾›é€‚å½“çš„æ”¯æŒã€‚é€šè¿‡å±•è§ˆç¯å¢ƒä¸­çš„â€œå‡ºå£°æ€è€ƒâ€ç ”ç©¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿäº†ä¸ªäººä¸å¤šæ¨¡å¼ä¿¡æ¯çš„äº¤äº’æ–¹å¼ï¼Œå¹¶è¯†åˆ«äº†åœ¨ç»“æ„åŒ–ã€æ£€ç´¢å’Œåº”ç”¨çŸ¥è¯†æ–¹é¢çš„å…³é”®è®¤çŸ¥æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†äººå·¥æ™ºèƒ½é©±åŠ¨çš„è®¤çŸ¥æ”¯æŒç³»ç»Ÿçš„é‡è¦æ€§ï¼Œè¿™äº›ç³»ç»Ÿç»“åˆäº†å®æ—¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€ä¸ªæ€§åŒ–æ¨ç†æ”¯æŒå’Œç¤¾äº¤é€‚åº”æ€§äº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªäººå·¥æ™ºèƒ½å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æ— ç¼è¿‡æ¸¡åˆ°å®æ—¶è®¤çŸ¥æ”¯æŒå’Œåä½“éªŒçŸ¥è¯†ç»„ç»‡ï¼Œä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„äººç±»ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿåšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13684v1">PDF</a> Presented at the 2025 ACM Workshop on Human-AI Interaction for   Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</p>
<p><strong>Summary</strong>ï¼šäººç±»è®¤çŸ¥å—é™äºå¤„ç†èƒ½åŠ›çš„é™åˆ¶ï¼Œå¯¼è‡´è®¤çŸ¥è¿‡è½½å’ŒçŸ¥è¯†åˆæˆä¸å†³ç­–åˆ¶å®šä¸­çš„æ•ˆç‡ä½ä¸‹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè®¤çŸ¥å¢å¼ºæä¾›äº†æœºä¼šï¼Œä½†å…¶å½“å‰çš„ååº”æ€§è´¨é™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬ç«‹åœºè®ºæ–‡æ¢è®¨äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥è®¤çŸ¥å¢å¼ºçš„æ½œåŠ›ï¼Œå…¶ä¸­LLMåŠ¨æ€é€‚åº”ç”¨æˆ·çš„è®¤çŸ¥çŠ¶æ€å’Œä»»åŠ¡ç¯å¢ƒä»¥æä¾›é€‚å½“çš„æ”¯æŒã€‚é€šè¿‡å±•è§ˆç¯å¢ƒä¸­çš„å‡ºå£°æ€è€ƒç ”ç©¶ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ä¸ªäººä¸å¤šæ¨¡å¼ä¿¡æ¯çš„äº¤äº’æ–¹å¼ï¼Œå¹¶ç¡®å®šäº†ç»“æ„åŒ–ã€æ£€ç´¢å’Œåº”ç”¨çŸ¥è¯†æ–¹é¢çš„å…³é”®è®¤çŸ¥æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°éœ€è¦é›†æˆå®æ—¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€ä¸ªæ€§åŒ–æ¨ç†è¾…åŠ©å’Œç¤¾ä¼šé€‚åº”æ€§äº¤äº’çš„äººå·¥æ™ºèƒ½é©±åŠ¨çš„è®¤çŸ¥æ”¯æŒç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªAIå¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ— ç¼è¿‡æ¸¡åˆ°å®æ—¶è®¤çŸ¥æ”¯æŒå’Œäº‹åçŸ¥è¯†ç»„ç»‡ï¼Œæœ‰åŠ©äºè®¾è®¡æ›´æœ‰æ•ˆçš„äººç±»ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººç±»è®¤çŸ¥å—é™äºå¤„ç†èƒ½åŠ›ï¼Œå¯¼è‡´è®¤çŸ¥è¿‡è½½å’Œå†³ç­–æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè®¤çŸ¥å¢å¼ºæä¾›æœºä¼šï¼Œä½†ç°æœ‰æŠ€æœ¯çš„ååº”æ€§è´¨é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>ä¸Šä¸‹æ–‡æ„ŸçŸ¥è®¤çŸ¥å¢å¼ºèƒ½å¤ŸåŠ¨æ€é€‚åº”ç”¨æˆ·çš„è®¤çŸ¥çŠ¶æ€å’Œä»»åŠ¡ç¯å¢ƒï¼Œæä¾›é€‚å½“çš„æ”¯æŒã€‚</li>
<li>ä¸ªä½“ä¸å¤šæ¨¡å¼ä¿¡æ¯äº¤äº’æ—¶å­˜åœ¨è®¤çŸ¥æŒ‘æˆ˜ï¼Œå¦‚çŸ¥è¯†ç»“æ„åŒ–ã€æ£€ç´¢å’Œåº”ç”¨ã€‚</li>
<li>éœ€è¦AIé©±åŠ¨çš„è®¤çŸ¥æ”¯æŒç³»ç»Ÿï¼Œé›†æˆå®æ—¶ä¸Šä¸‹æ–‡æ„ŸçŸ¥ã€ä¸ªæ€§åŒ–æ¨ç†è¾…åŠ©å’Œç¤¾ä¼šé€‚åº”æ€§äº¤äº’ã€‚</li>
<li>æ¡†æ¶åº”æ— ç¼è¿‡æ¸¡å®æ—¶è®¤çŸ¥æ”¯æŒå’Œäº‹åçŸ¥è¯†ç»„ç»‡ï¼Œä»¥æé«˜AIç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59ea7efb0374d75d3256a4401ee52024.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffb34e1010e0a87513cc64e545af1418.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Visual-Intention-Grounding-for-Egocentric-Assistants"><a href="#Visual-Intention-Grounding-for-Egocentric-Assistants" class="headerlink" title="Visual Intention Grounding for Egocentric Assistants"></a>Visual Intention Grounding for Egocentric Assistants</h2><p><strong>Authors:Pengzhan Sun, Junbin Xiao, Tze Ho Elden Tse, Yicong Li, Arjun Akula, Angela Yao</strong></p>
<p>Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts â€“ inputs are egocentric, and objects may be referred to implicitly through needs and intentions. To bridge this gap, we introduce EgoIntention, the first dataset for egocentric visual intention grounding. EgoIntention challenges multimodal LLMs to 1) understand and ignore unintended contextual objects and 2) reason about uncommon object functionalities. Benchmark results show that current models misidentify context objects and lack affordance understanding in egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it enables hybrid training with normal descriptions and egocentric intentions with a chained intention reasoning and object grounding mechanism. RoG significantly outperforms naive finetuning and hybrid training on EgoIntention, while maintaining or slightly improving naive description grounding. This advancement enables unified visual grounding for egocentric and exocentric visual inputs while handling explicit object queries and implicit human intentions. </p>
<blockquote>
<p>è§†è§‰å®šä½å°†æ–‡æœ¬æè¿°ä¸å›¾åƒä¸­çš„ç‰©ä½“å…³è”èµ·æ¥ã€‚ä¼ ç»Ÿæ–¹æ³•é’ˆå¯¹ç¬¬ä¸‰äººç§°å›¾åƒè¾“å…¥å’Œå‘½åå¯¹è±¡æŸ¥è¯¢ã€‚åœ¨äººå·¥æ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨ä¸­ï¼Œè§†è§’ä¼šå‘ç”Ÿå˜åŒ–â€”â€”è¾“å…¥æ˜¯è‡ªæˆ‘ä¸­å¿ƒçš„ï¼Œç‰©ä½“å¯èƒ½é€šè¿‡éœ€æ±‚å’Œæ„å›¾è¢«éšå«åœ°æåŠã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoIntentionï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ„å›¾å®šä½çš„æ•°æ®é›†ã€‚EgoIntentionæŒ‘æˆ˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥1ï¼‰ç†è§£å’Œå¿½ç•¥éä¸Šä¸‹æ–‡ä¸­çš„ç‰©ä½“ï¼Œä»¥åŠ2ï¼‰æ¨ç†ä¸å¸¸è§ç‰©ä½“çš„åŠŸèƒ½ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹ä¼šé”™è¯¯è¯†åˆ«ä¸Šä¸‹æ–‡ä¸­çš„ç‰©ä½“ï¼Œåœ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§’ä¸­ç¼ºä¹åŠŸèƒ½ç†è§£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Reason-to-Groundï¼ˆRoGï¼‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼›å®ƒé€šè¿‡æ··åˆæ­£å¸¸æè¿°å’Œè‡ªæˆ‘ä¸­å¿ƒæ„å›¾çš„è®­ç»ƒï¼Œå¹¶ç»“åˆé“¾å¼æ„å›¾æ¨ç†å’Œå¯¹è±¡å®šä½æœºåˆ¶ï¼Œå®ç°äº†è®­ç»ƒã€‚åœ¨EgoIntentionä¸Šï¼ŒRoGæ˜¾è‘—ä¼˜äºç®€å•çš„å¾®è°ƒæ–¹æ³•å’Œæ··åˆè®­ç»ƒæ–¹æ³•ï¼ŒåŒæ—¶åœ¨å•çº¯æè¿°å®šä½æ–¹é¢ä¿æŒæˆ–ç•¥æœ‰æå‡ã€‚è¿™ä¸€è¿›å±•å®ç°äº†è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–åœ¨ä¸­å¿ƒè§†è§‰è¾“å…¥çš„ç»Ÿä¸€è§†è§‰å®šä½ï¼ŒåŒæ—¶å¤„ç†æ˜ç¡®çš„å¯¹è±¡æŸ¥è¯¢å’Œéšå«çš„äººç±»æ„å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†è§†è§‰å®šä½æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å°†æ–‡æœ¬æè¿°ä¸å›¾åƒä¸­çš„ç‰©ä½“ç›¸å…³è”ã€‚ä¼ ç»Ÿçš„è§†è§‰å®šä½æ–¹æ³•ä¸»è¦é’ˆå¯¹ç¬¬ä¸‰äººç§°å›¾åƒè¾“å…¥å’Œå‘½åå¯¹è±¡æŸ¥è¯¢ã€‚ä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚AIåŠ©æ‰‹ç­‰åœºæ™¯ï¼Œè§†è§’æ˜¯è‡ªæˆ‘ä¸­å¿ƒçš„ï¼Œå¹¶ä¸”ç‰©ä½“å¯èƒ½é€šè¿‡éœ€æ±‚å’Œæ„å›¾éšå«åœ°è¢«å¼•ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†EgoIntentionï¼Œå®ƒæ˜¯ç”¨äºè‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ„å›¾å®šä½çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æŒ‘æˆ˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å¹¶å¿½ç•¥éä¸Šä¸‹æ–‡ä¸­çš„ç‰©ä½“ï¼Œå¹¶æ¨ç†å‡ºç½•è§ç‰©ä½“çš„åŠŸèƒ½ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§’ä¸‹å¯¹ä¸Šä¸‹æ–‡ç‰©ä½“çš„è¯¯è¯†åˆ«å’Œå¯¹å¯è´Ÿæ‹…èƒ½åŠ›çš„ç†è§£ç¼ºä¹ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åä¸ºReason-to-Groundï¼ˆRoGï¼‰çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ï¼Œå®ƒé€šè¿‡é“¾å¼æ„å›¾æ¨ç†å’Œå¯¹è±¡å®šä½æœºåˆ¶å®ç°äº†å¸¸è§„æè¿°å’Œè‡ªæˆ‘ä¸­å¿ƒæ„å›¾çš„æ··åˆè®­ç»ƒã€‚åœ¨EgoIntentionæµ‹è¯•ä¸­ï¼ŒRoGæ˜æ˜¾è¶…è¶Šäº†ç®€å•å¾®è°ƒæ³•å’Œæ··åˆè®­ç»ƒæ³•ï¼ŒåŒæ—¶ç»´æŒæˆ–æé«˜äº†å•çº¯çš„å®šä½æè¿°èƒ½åŠ›ã€‚è¿™ä¸€è¿›å±•å®ç°äº†ç»Ÿä¸€å¤„ç†è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒè§†è§‰è¾“å…¥çš„ç»Ÿä¸€è§†è§‰å®šä½æŠ€æœ¯ï¼Œå¹¶å¤„ç†æ˜ç¡®çš„å¯¹è±¡æŸ¥è¯¢å’Œéšå«çš„äººç±»æ„å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å®šä½æŠ€æœ¯èƒ½å¤Ÿå°†æ–‡æœ¬æè¿°ä¸å›¾åƒä¸­çš„ç‰©ä½“ç›¸å…³è”ã€‚</li>
<li>ä¼ ç»Ÿè§†è§‰å®šä½æ–¹æ³•ä¸»è¦å…³æ³¨ç¬¬ä¸‰äººç§°å›¾åƒè¾“å…¥å’Œå‘½åå¯¹è±¡æŸ¥è¯¢ã€‚</li>
<li>åœ¨AIåŠ©æ‰‹ç­‰åº”ç”¨ä¸­ï¼Œè§†è§’æ˜¯è‡ªæˆ‘ä¸­å¿ƒçš„ï¼Œç‰©ä½“é€šè¿‡éœ€æ±‚å’Œæ„å›¾éšå«åœ°è¢«å¼•ç”¨ã€‚</li>
<li>EgoIntentionæ•°æ®é›†æ˜¯é¦–ä¸ªç”¨äºè‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ„å›¾å®šä½çš„æ•°æ®é›†ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§’ä¸‹å­˜åœ¨è¯¯è¯†åˆ«ä¸Šä¸‹æ–‡ç‰©ä½“çš„æŒ‘æˆ˜ï¼Œç¼ºä¹ç½•è§ç‰©ä½“çš„åŠŸèƒ½æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Reason-to-Groundï¼ˆRoGï¼‰æŒ‡ä»¤è°ƒæ•´æ–¹æ³•å®ç°äº†å¸¸è§„æè¿°å’Œè‡ªæˆ‘ä¸­å¿ƒæ„å›¾çš„æ··åˆè®­ç»ƒï¼Œé€šè¿‡é“¾å¼æ„å›¾æ¨ç†å’Œå¯¹è±¡å®šä½æœºåˆ¶æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5eb2593b989df76d796c3f26fa1dc8ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87de39a59ddf9336ae7bea7bf83701eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-465609808cd39e8d9f9fc736226f9a29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79aa231b6446fc0a0477843d87cc44de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1714d1c0f1d900ab5e9f241433dfb2b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cadfb8e03dbecd652e763b43040641.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Mind2Matter-Creating-3D-Models-from-EEG-Signals"><a href="#Mind2Matter-Creating-3D-Models-from-EEG-Signals" class="headerlink" title="Mind2Matter: Creating 3D Models from EEG Signals"></a>Mind2Matter: Creating 3D Models from EEG Signals</h2><p><strong>Authors:Xia Deng, Shen Chen, Jiale Zhou, Lei Li</strong></p>
<p>The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a>. </p>
<blockquote>
<p>ä»è„‘ä¿¡å·é‡å»º3Dç‰©ä½“åœ¨è„‘-è®¡ç®—æœºæ¥å£ï¼ˆBCIï¼‰ç ”ç©¶ä¸­å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦åˆ©ç”¨åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰è¿›è¡Œ3Dé‡å»ºä»»åŠ¡ï¼Œå› å…¶å‡ºè‰²çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼ŒfMRIçš„ä¸´åºŠåº”ç”¨å—é™äºå…¶é«˜æ˜‚çš„æˆæœ¬å’Œæ— æ³•æ”¯æŒå®æ—¶æ“ä½œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè„‘ç”µå›¾ï¼ˆEEGï¼‰ä½œä¸ºå®æ—¶è„‘-è®¡ç®—æœºäº¤äº’ç³»ç»Ÿçš„ç»æµã€æ— åˆ›å’Œç§»åŠ¨è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ é¢†åŸŸçš„æœ€æ–°è¿›å±•åœ¨æ ¹æ®ç¥ç»æ•°æ®ç”Ÿæˆå›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†EEGä¿¡å·è§£ç ä¸ºç»“æ„åŒ–3Dè¡¨ç¤ºä»ç„¶å¾ˆå°‘æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨ç¥ç»è§£ç æŠ€æœ¯å’Œç”Ÿæˆæ¨¡å‹å°†EEGè®°å½•è½¬æ¢ä¸º3Då¯¹è±¡é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è®­ç»ƒEEGç¼–ç å™¨ä»¥æå–æ—¶ç©ºè§†è§‰ç‰¹å¾ï¼Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥å°†è¿™äº›ç‰¹å¾è§£é‡Šä¸ºæè¿°æ€§å¤šæ¨¡å¼è¾“å‡ºï¼Œå¹¶åˆ©ç”¨å¸¦æœ‰å¸ƒå±€æŒ‡å¯¼æ§åˆ¶çš„ç”Ÿæˆ3Dé«˜æ–¯æ¥åˆæˆæœ€ç»ˆçš„3Dç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ•æ‰äº†æ˜¾è‘—çš„å‡ ä½•å’Œè¯­ä¹‰ç‰¹å¾ï¼Œä¸ºè„‘-è®¡ç®—æœºæ¥å£ï¼ˆBCIï¼‰ã€è™šæ‹Ÿç°å®å’Œç¥ç»çŸ«æ­£å™¨åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11936v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è„‘æœºæ¥å£ç ”ç©¶ä¸­ï¼Œåˆ©ç”¨è„‘ä¿¡å·é‡å»ºä¸‰ç»´ç‰©ä½“å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å½“å‰ç ”ç©¶ä¸»è¦åˆ©ç”¨åŠŸèƒ½ç£å…±æŒ¯æˆåƒè¿›è¡Œä¸‰ç»´é‡å»ºä»»åŠ¡ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”æ— æ³•å®ç°å®æ—¶æ“ä½œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨è„‘ç”µå›¾å’Œç¥ç»ç½‘ç»œè§£ç æŠ€æœ¯å°†è„‘ç”µå›¾ä¿¡å·è§£ç ä¸ºç»“æ„åŒ–ä¸‰ç»´è¡¨ç¤ºçš„æ–°æ¡†æ¶ï¼Œä»è€Œå®ç°å®æ—¶è„‘æœºäº¤äº’ç³»ç»Ÿçš„ä¸‰ç»´ç‰©ä½“é‡å»ºã€‚è¯¥æ¨¡å‹å¯åº”ç”¨äºè„‘æœºæ¥å£ã€è™šæ‹Ÿç°å®å’Œç¥ç»å‡è‚¢ç­‰é¢†åŸŸã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å»ºä¸‰ç»´ç‰©ä½“ä»è„‘ä¿¡å·åœ¨è„‘æœºæ¥å£ç ”ç©¶ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>åŠŸèƒ½ç£å…±æŒ¯æˆåƒåœ¨ä¸‰ç»´é‡å»ºä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”æ— æ³•æ”¯æŒå®æ—¶æ“ä½œã€‚</li>
<li>ç”µæè„‘ç”µå›¾æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œå…·æœ‰ç»æµã€éä¾µå…¥æ€§å’Œç§»åŠ¨æ€§ç­‰ä¼˜ç‚¹ï¼Œé€‚ç”¨äºå®æ—¶è„‘æœºäº¤äº’ç³»ç»Ÿã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†è„‘ç”µå›¾ä¿¡å·è§£ç ä¸ºç»“æ„åŒ–ä¸‰ç»´è¡¨ç¤ºä»å¾…æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œè§£ç æŠ€æœ¯å’Œç”Ÿæˆæ¨¡å‹å°†è„‘ç”µå›¾è®°å½•è½¬åŒ–ä¸ºä¸‰ç»´ç‰©ä½“é‡å»ºã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬è®­ç»ƒEEGç¼–ç å™¨æå–æ—¶ç©ºè§†è§‰ç‰¹å¾ï¼Œå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ä»¥è§£é‡Šè¿™äº›ç‰¹å¾å¹¶ç”Ÿæˆå¤šæ¨¡å¼è¾“å‡ºï¼Œå¹¶åˆ©ç”¨å¸ƒå±€æŒ‡å¯¼æ§åˆ¶çš„ç”Ÿæˆä¸‰ç»´é«˜æ–¯æ¥åˆæˆæœ€ç»ˆçš„ä¸‰ç»´ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95343a55301c4bf587fbc0a67665cb0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca1f712f6ee96a259fc29c0453b3053f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ba79f5a837f15bdb2207ff4617883b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e223852dc3312651eaa87167b1aefe61.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers"><a href="#When-is-Task-Vector-Provably-Effective-for-Model-Editing-A-Generalization-Analysis-of-Nonlinear-Transformers" class="headerlink" title="When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers"></a>When is Task Vector Provably Effective for Model Editing? A   Generalization Analysis of Nonlinear Transformers</h2><p><strong>Authors:Hongkang Li, Yihua Zhang, Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen</strong></p>
<p>Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B). </p>
<blockquote>
<p>ä»»åŠ¡ç®—æœ¯æ˜¯æŒ‡é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡çš„åŠ æƒå’Œæ¥ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªä»»åŠ¡å‘é‡éƒ½æ˜¯é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹çš„æƒé‡æ›´æ–°ã€‚æœ€è¿‘ï¼Œè¿™ç§æ–¹æ³•ä½œä¸ºä¸€ç§è®¡ç®—é«˜æ•ˆçš„æ¨ç†æ–¹æ³•å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œä¾‹å¦‚åœ¨å¤šä»»åŠ¡å­¦ä¹ ã€é—å¿˜å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ¨¡å‹ç¼–è¾‘ã€‚ç„¶è€Œï¼Œç”±äºè®­ç»ƒTransformeræ¨¡å‹çš„é«˜åº¦éå‡¸æ€§ï¼Œå…³äºä»»åŠ¡å‘é‡å¦‚ä½•æ‰§è¡Œå„ç§æ¦‚å¿µæ“ä½œçš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡é¦–æ¬¡å¯¹ä»»åŠ¡å‘é‡æ–¹æ³•åœ¨éçº¿æ€§Transformerä¸Šçš„æ³›åŒ–ä¿è¯è¿›è¡Œäº†ç†è®ºæè¿°ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ¦‚å¿µå­¦ä¹ åœºæ™¯ï¼Œå…¶ä¸­æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯åŸºäºè¾¨åˆ«æ¨¡å¼çš„äºŒå…ƒåˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†åŒæ—¶å­¦ä¹ ä¸€ç»„ä¸ç›¸å…³æˆ–å¯¹é½çš„ä»»åŠ¡æ—¶æ·»åŠ ä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠä»æ— å…³æˆ–çŸ›ç›¾çš„ä»»åŠ¡ä¸­é—å¿˜ä¸€ä¸ªä»»åŠ¡æ—¶å¦å®šä»»åŠ¡çš„æˆåŠŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»»åŠ¡ç®—æœ¯ä¸­çº¿æ€§ç³»æ•°çš„é€‚å½“é€‰æ‹©ï¼Œä»¥å®ç°å¯¹åŸŸå¤–ä»»åŠ¡çš„ä¿è¯æ³›åŒ–ã€‚æˆ‘ä»¬çš„ç†è®ºç»“æœå¯¹äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼éƒ½é€‚ç”¨ã€‚å°½ç®¡æ˜¯åœ¨æ¦‚å¿µç¯å¢ƒä¸­å»ºç«‹çš„ç†è®ºå‘ç°ï¼Œä½†æˆ‘ä»¬åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ï¼ˆ13äº¿å‚æ•°ï¼‰çš„å®é™…æœºå™¨é—å¿˜ä»»åŠ¡ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10957v2">PDF</a> Published at ICLR 2025 as an oral paper</p>
<p><strong>Summary</strong>ï¼š<br>ä»»åŠ¡ç®—æœ¯é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡çš„åŠ æƒå’Œæ¥ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ¯ä¸ªä»»åŠ¡å‘é‡éƒ½æ˜¯é¢„è®­ç»ƒæ¨¡å‹åˆ°ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹çš„æƒé‡æ›´æ–°ã€‚è¿™ç§æ–¹æ³•ä½œä¸ºæ¨¡å‹ç¼–è¾‘çš„è®¡ç®—é«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œå¦‚å¤šä»»åŠ¡å­¦ä¹ ã€é—å¿˜å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œè¿‘æœŸå—åˆ°å…³æ³¨ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹éçº¿æ€§Transformerä»»åŠ¡å‘é‡æ–¹æ³•è¿›è¡Œäº†ç†è®ºè¡¨å¾ã€‚åœ¨æ¦‚å¿µå­¦ä¹ ç¯å¢ƒä¸­ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æ˜¯åŸºäºåˆ¤åˆ«æ¨¡å¼çš„äºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œæœ¬æ–‡è¯æ˜äº†åŒæ—¶å­¦ä¹ ä¸€ç»„ä¸ç›¸å…³æˆ–å¯¹é½çš„ä»»åŠ¡ä»¥åŠé€šè¿‡ä»»åŠ¡å¦å®šä»æ— å…³æˆ–çŸ›ç›¾çš„ä»»åŠ¡ä¸­é—å¿˜ä¸€ä¸ªä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¯æ˜äº†é€‰æ‹©é€‚å½“çš„çº¿æ€§ç³»æ•°å¯¹ä»»åŠ¡ç®—æœ¯å®ç°æœ‰ä¿éšœçš„è·¨åŸŸä»»åŠ¡æ³›åŒ–çš„é‡è¦æ€§ã€‚ç†è®ºç»“æœé€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼ã€‚å°½ç®¡æ˜¯åœ¨æ¦‚å¿µç¯å¢ƒä¸­å»ºç«‹çš„ç†è®ºå‘ç°ï¼Œä½†è¿™äº›å‘ç°é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹Phi-1.5ï¼ˆ1.3Bï¼‰çš„æœºå™¨é—å¿˜ä»»åŠ¡å¾—åˆ°äº†éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»»åŠ¡ç®—æœ¯æ˜¯é€šè¿‡æ·»åŠ ä»»åŠ¡å‘é‡çš„åŠ æƒå’Œæ¥ç¼–è¾‘é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™æ˜¯æ¨¡å‹ç¼–è¾‘çš„ä¸€ç§è®¡ç®—é«˜æ•ˆæ¨ç†æ–¹æ³•ã€‚</li>
<li>ä»»åŠ¡ç®—æœ¯å¯ç”¨äºå¤šä»»åŠ¡å­¦ä¹ ã€é—å¿˜å’Œè·¨åŸŸæ³›åŒ–ç­‰åœºæ™¯ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å¯¹éçº¿æ€§Transformerä»»åŠ¡å‘é‡æ–¹æ³•çš„ç†è®ºè¡¨å¾è¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>åœ¨æ¦‚å¿µå­¦ä¹ ç¯å¢ƒä¸­ï¼Œè¯æ˜äº†ä»»åŠ¡æ·»åŠ å’Œå¦å®šçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é€‚å½“é€‰æ‹©çº¿æ€§ç³»æ•°å¯¹ä»»åŠ¡ç®—æœ¯å®ç°æœ‰ä¿éšœçš„è·¨åŸŸä»»åŠ¡æ³›åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>ç†è®ºç»“æœé€‚ç”¨äºå¯†é›†æƒé‡å‚æ•°åŠå…¶ä½ç§©è¿‘ä¼¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0812a7cf203c03515f54d74bd0cfac9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e71cd5b483f404bd815534f4bddd1d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6db044299e165f79f1b58a3783daa70c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Run Luo, Lu Wang, Wanwei He, Xiaobo Xia</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>ç°æœ‰æ„å»ºå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„å·¥ä½œä¸»è¦ä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šè¿›è¡Œçš„ç›‘ç£ç²¾ç»†è°ƒæ•´è®­ç»ƒæ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢æ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚è¿™ä¸€é—®é¢˜æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜çº§ä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹GUIèƒ½åŠ›å¢å¼ºæ¡†æ¶ï¼Œé€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œå®ç°äº†åœ¨çœŸå®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡ä¸­çš„é«˜æ€§èƒ½è¡¨ç°ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è·¨å¹³å°é«˜è´¨é‡æ•°æ®å¹¶é‡‡ç”¨ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ›´æ–°æ¨¡å‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨æ›´å°‘çš„æ•°æ®é‡å³å¯å®ç°ä¼˜è¶Šæ€§èƒ½ã€‚è¿™ä¸€åˆ›æ–°å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œæœ‰æœ›æ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡ä¸­çš„æ‰§è¡Œèƒ½åŠ›çš„æå‡ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ul>
<li>å½“å‰GUIä»£ç†çš„å»ºè®¾ä¸»è¦ä¾èµ–äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒè®­ç»ƒæ¨¡å¼ï¼Œè¿™ç§æ–¹å¼éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ä¸”éš¾ä»¥æœ‰æ•ˆç†è§£GUIæˆªå›¾å¹¶æ¨å¹¿åˆ°æœªè§è¿‡çš„ç•Œé¢ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¢«å¼•å…¥æ¥æå‡LVLMsåœ¨çœŸå®ä¸–ç•ŒGUIä»»åŠ¡åœºæ™¯ä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ°´å¹³ä»»åŠ¡ä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç§°ä¸ºâ€œåå­—â€ï¼ˆæš‚æœªå…·ä½“ç»™å‡ºæ¡†æ¶åç§°ï¼‰ï¼Œå€Ÿé‰´å¤§å‹æ¨ç†æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-40e44cce157ab734b1ce7aa7a91d195b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8b83a33fe04aa5f478735789bdd633d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1378358a130b9ffbb33425047048d0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="C-MTCSD-A-Chinese-Multi-Turn-Conversational-Stance-Detection-Dataset"><a href="#C-MTCSD-A-Chinese-Multi-Turn-Conversational-Stance-Detection-Dataset" class="headerlink" title="C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset"></a>C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset</h2><p><strong>Authors:Fuqiang Niu, Yi Yang, Xianghua Fu, Genan Dai, Bowen Zhang</strong></p>
<p>Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese multi-turn conversational stance detection dataset, comprising 24,264 carefully annotated instances from Sina Weibo, which is 4.2 times larger than the only prior Chinese conversational stance detection dataset. Our comprehensive evaluation using both traditional approaches and large language models reveals the complexity of C-MTCSD: even state-of-the-art models achieve only 64.07% F1 score in the challenging zero-shot setting, while performance consistently degrades with increasing conversation depth. Traditional models particularly struggle with implicit stance detection, achieving below 50% F1 score. This work establishes a challenging new benchmark for Chinese stance detection research, highlighting significant opportunities for future improvements. </p>
<blockquote>
<p>ç«‹åœºæ£€æµ‹å·²æˆä¸ºåˆ†æç¤¾äº¤åª’ä½“ä¸Šå…¬ä¼—è®¨è®ºçš„é‡è¦å·¥å…·ã€‚å½“å‰çš„æ–¹æ³•é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡è¯­è¨€å¤„ç†å’Œå¤šè½®å¯¹è¯åˆ†ææ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†C-MTCSDï¼Œè¿™æ˜¯æœ€å¤§çš„ä¸­æ–‡å¤šè½®å¯¹è¯ç«‹åœºæ£€æµ‹æ•°æ®é›†ï¼ŒåŒ…å«24264ä¸ªæ¥è‡ªæ–°æµªå¾®åšçš„ç²¾å¿ƒæ³¨é‡Šå®ä¾‹ï¼Œæ˜¯ä¹‹å‰å”¯ä¸€çš„ä¸­æ–‡å¯¹è¯ç«‹åœºæ£€æµ‹æ•°æ®é›†çš„4.2å€ã€‚æˆ‘ä»¬ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†C-MTCSDçš„å¤æ‚æ€§ï¼šå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é›¶æ ·æœ¬è®¾ç½®ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿä»…è¾¾åˆ°64.07%çš„F1åˆ†æ•°ï¼Œéšç€å¯¹è¯æ·±åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¸æ–­ä¸‹é™ã€‚ä¼ ç»Ÿæ¨¡å‹åœ¨éšæ€§ç«‹åœºæ£€æµ‹æ–¹é¢å°¤å…¶å›°éš¾ï¼ŒF1åˆ†æ•°ä½äº50%ã€‚è¿™é¡¹å·¥ä½œä¸ºä¸­æ–‡ç«‹åœºæ£€æµ‹ç ”ç©¶è®¾å®šäº†æ–°çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†ï¼Œçªæ˜¾äº†æœªæ¥æ”¹è¿›çš„é‡å¤§æœºé‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09958v2">PDF</a> WWW2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¤¾äº¤ç½‘ç»œå…¬å…±è®¨è®ºçš„åˆ†æï¼Œç«‹åœºæ£€æµ‹å·²æˆä¸ºä¸€é¡¹é‡è¦å·¥å…·ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨ä¸­æ–‡è¯­è¨€å¤„ç†å’Œå¤šè½®å¯¹è¯åˆ†æä¸Šçš„å±€é™ï¼Œæˆ‘ä»¬æ¨å‡ºäº†C-MTCSDæ•°æ®é›†ï¼Œå®ƒæ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸­æ–‡å¤šè½®å¯¹è¯ç«‹åœºæ£€æµ‹æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªæ–°æµªå¾®åšçš„24,264ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å®ä¾‹ï¼Œæ¯”ä¹‹å‰å”¯ä¸€çš„ä¸­æ–‡å¯¹è¯ç«‹åœºæ£€æµ‹æ•°æ®é›†å¤§4.2å€ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœæ­ç¤ºäº†C-MTCSDçš„å¤æ‚æ€§ï¼šå³ä½¿åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°64.07%çš„F1åˆ†æ•°ï¼Œéšç€å¯¹è¯æ·±åº¦çš„å¢åŠ ï¼Œæ€§èƒ½æŒç»­ä¸‹é™ã€‚ä¼ ç»Ÿæ¨¡å‹åœ¨éšæ€§ç«‹åœºæ£€æµ‹æ–¹é¢çš„è¡¨ç°å°¤å…¶ä¸ä½³ï¼ŒF1åˆ†æ•°ä½äº50%ã€‚æœ¬ç ”ç©¶ä¸ºä¸­æ–‡ç«‹åœºæ£€æµ‹ç ”ç©¶è®¾å®šäº†æ–°çš„æŒ‘æˆ˜åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«‹åœºæ£€æµ‹æ˜¯åˆ†æç¤¾äº¤åª’ä½“å…¬å…±è®¨è®ºçš„é‡è¦å·¥å…·ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨ä¸­æ–‡è¯­è¨€å¤„ç†å’Œå¤šè½®å¯¹è¯åˆ†ææ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†C-MTCSDæ•°æ®é›†ï¼Œä¸ºä¸­æ–‡å¤šè½®å¯¹è¯ç«‹åœºæ£€æµ‹æä¾›äº†æœ€å¤§çš„æ ‡æ³¨å®ä¾‹ã€‚</li>
<li>C-MTCSDæ•°æ®é›†çš„å¤æ‚æ€§è¢«æ­ç¤ºï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹F1åˆ†æ•°ä»…ä¸º64.07%ã€‚</li>
<li>éšç€å¯¹è¯æ·±åº¦çš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½æŒç»­ä¸‹é™ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹åœ¨éšæ€§ç«‹åœºæ£€æµ‹æ–¹é¢è¡¨ç°ä¸ä½³ï¼ŒF1åˆ†æ•°ä½äº50%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-df667568300e44f07eee1e08d139b83e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-608231bb75d7b84fcc702c6f2ad38fbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5059b6f738b683d07f1899fdc37d6190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8bbd061e01d02cefcdc70b85fc01a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dae74d17404493747523cf9f36e694a.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Token-to-Line-Enhancing-Code-Generation-with-a-Long-Term-Perspective"><a href="#From-Token-to-Line-Enhancing-Code-Generation-with-a-Long-Term-Perspective" class="headerlink" title="From Token to Line: Enhancing Code Generation with a Long-Term   Perspective"></a>From Token to Line: Enhancing Code Generation with a Long-Term   Perspective</h2><p><strong>Authors:Tingwei Lu, Yangning Li, Liyuan Wang, Binghuai Lin, Jiwei Tang, Wanshi Xu, Hai-Tao Zheng, Yinghui Li, Bingxu An, Zhao Wei, Yong Xu</strong></p>
<p>The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æå¤§åœ°ä¿ƒè¿›äº†ä»£ç ç”Ÿæˆä»»åŠ¡çš„å‘å±•ï¼Œå¹¶å¼•å‘äº†ç›¸å…³æ–‡çŒ®çš„æ¿€å¢ã€‚å½“å‰çš„ç ”ç©¶å—åˆ°ç”Ÿæˆç»“æœå†—ä½™å’ŒçŸ­æœŸå†…è¿‡åº¦æ‹Ÿåˆå±€éƒ¨æ¨¡å¼çš„å½±å“ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶è¯•å›¾é€šè¿‡é‡‡ç”¨å¤šä»¤ç‰Œé¢„æµ‹ç­–ç•¥æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†åœ¨é€‰æ‹©é€‚å½“çš„å¤„ç†é•¿åº¦è¿›è¡Œç”Ÿæˆæ–¹é¢ä»å­˜åœ¨æœ‰é™å…³æ³¨ã€‚é€šè¿‡åˆ†æLLMç”Ÿæˆè¿‡ç¨‹ä¸­ä»¤ç‰Œä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°æ³¨æ„åŠ›å¾—åˆ†çš„é«˜å³°å€¼é€šå¸¸å‡ºç°åœ¨è¡Œçš„æœ«å°¾ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œå°†æ¯è¡Œä»£ç è§†ä¸ºåŸºæœ¬å¤„ç†å•å…ƒå¹¶è¿›è¡Œé¡ºåºç”Ÿæˆæ˜¯åˆç†çš„ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>LSR-MCTS</strong>ç®—æ³•ï¼Œè¯¥ç®—æ³•åˆ©ç”¨MCTSé€è¡Œç¡®å®šä»£ç å¹¶é€‰æ‹©æœ€ä½³è·¯å¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªèŠ‚ç‚¹é›†æˆäº†ä¸€ç§è‡ªæˆ‘ä¼˜åŒ–æœºåˆ¶ï¼Œä»¥æé«˜å¤šæ ·æ€§å¹¶é€šè¿‡é”™è¯¯æ ¡æ­£ç”Ÿæˆæ›´é«˜è´¨é‡çš„ç¨‹åºã€‚åœ¨ä¸‰ä¸ªå…¬å…±ç¼–ç åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒå’Œç»¼åˆåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°æ€§èƒ½æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07433v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°æå¤§åœ°æ¨åŠ¨äº†ä»£ç ç”Ÿæˆä»»åŠ¡çš„å‘å±•ï¼Œå½“å‰ç ”ç©¶é¢ä¸´å†—ä½™ç”Ÿæˆç»“æœå’ŒçŸ­æœŸè¿‡åº¦æ‹Ÿåˆå±€éƒ¨æ¨¡å¼çš„æŒ‘æˆ˜ã€‚ç°æœ‰ç ”ç©¶å°è¯•é€šè¿‡é‡‡ç”¨å¤šä»¤ç‰Œé¢„æµ‹ç­–ç•¥æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å¦‚ä½•é€‰æ‹©é€‚å½“çš„ç”Ÿæˆå¤„ç†é•¿åº¦ä»å—åˆ°å…³æ³¨ã€‚é€šè¿‡åˆ†æLLMç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä»¤ç‰Œé—´æ³¨æ„åŠ›ï¼Œè§‚å¯Ÿåˆ°æ³¨æ„åŠ›åˆ†æ•°çš„é«˜å³°å€¼é€šå¸¸å‡ºç°åœ¨è¡Œå°¾ã€‚è¿™å¯å‘æˆ‘ä»¬æå‡ºä»¥è¡Œä¸ºåŸºæœ¬å¤„ç†å•å…ƒçš„LSR-MCTSç®—æ³•ï¼Œåˆ©ç”¨MCTSé€è¡Œç¡®å®šä»£ç å¹¶é€‰æ‹©æœ€ä¼˜è·¯å¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªèŠ‚ç‚¹é›†æˆäº†è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ï¼Œä»¥æé«˜å¤šæ ·æ€§å¹¶é€šè¿‡é”™è¯¯ä¿®æ­£ç”Ÿæˆæ›´é«˜è´¨é‡çš„ç¨‹åºã€‚åœ¨ä¸‰ä¸ªå…¬å…±ç¼–ç åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒå’Œç»¼åˆåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æœ€ä½³æ€§èƒ½æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ä»£ç ç”Ÿæˆä»»åŠ¡æœ‰æ˜¾è‘—çš„æ¨åŠ¨ä½œç”¨ã€‚</li>
<li>å½“å‰ç ”ç©¶é¢ä¸´å†—ä½™ç”Ÿæˆå’Œè¿‡åº¦æ‹Ÿåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦é€šè¿‡å¤šä»¤ç‰Œé¢„æµ‹ç­–ç•¥æ¥åº”å¯¹æŒ‘æˆ˜ï¼Œä½†å¤„ç†é•¿åº¦çš„é€‰æ‹©ä»ç„¶é‡è¦ã€‚</li>
<li>LLMç”Ÿæˆè¿‡ç¨‹ä¸­ä»¤ç‰Œé—´çš„é«˜æ³¨æ„åŠ›å³°å€¼é€šå¸¸å‡ºç°åœ¨è¡Œå°¾ï¼Œæç¤ºä»¥è¡Œä¸ºåŸºæœ¬å¤„ç†å•å…ƒçš„æ–¹æ³•å¯èƒ½æ›´æœ‰æ•ˆã€‚</li>
<li>æå‡ºäº†LSR-MCTSç®—æ³•ï¼Œåˆ©ç”¨MCTSé€è¡Œç¡®å®šä»£ç å¹¶é€‰æ‹©æœ€ä¼˜è·¯å¾„ã€‚</li>
<li>LSR-MCTSç®—æ³•é›†æˆäº†è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ï¼Œä»¥æé«˜ç”Ÿæˆç¨‹åºçš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå…¬å…±ç¼–ç åŸºå‡†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLSR-MCTSç®—æ³•æ€§èƒ½ä¼˜äºç°æœ‰æœ€ä½³æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b09f0dd2ba47648b9a46064f1ad4188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1febc01984a7f8196da84a5f8ffcbee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c734a60da3b0be89bb464b0ef0c5bfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ecdd884514cc2a1befd3ad17c2181b0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models"><a href="#Revealing-the-Intrinsic-Ethical-Vulnerability-of-Aligned-Large-Language-Models" class="headerlink" title="Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models"></a>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language   Models</h2><p><strong>Authors:Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</strong></p>
<p>Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible â€œdark patternsâ€ in LLMsâ€™ parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local â€œsafety regionsâ€ in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shiftsâ€“a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨åŒ–çš„åŸºç¡€æ¢ç´¢ï¼Œç„¶è€Œï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å­¦ä¹ ä½¿å…¶ä¸äººç±»ä»·å€¼è§‚ç›¸ç¬¦åªå®ç°äº†è¡¨é¢çš„åˆè§„æ€§ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¯æ˜é¢„è®­ç»ƒæœŸé—´åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šæŒä¹…åœ°ä½œä¸ºä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€å­˜åœ¨äºLLMçš„å‚æ•°è®°å¿†ä¸­ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶é€šè¿‡æ•Œå¯¹è¯±å¯¼é‡æ–°å‡ºç°ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šåˆ†æå¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œè¯æ˜å½“å‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ã€‚ç›¸åï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä»ç„¶ä¸æœ‰å®³æ¦‚å¿µå…¨çƒè¿é€šï¼Œç»ç”±é«˜æ¦‚ç‡çš„æ•Œå¯¹è½¨è¿¹ã€‚åŸºäºè¿™ä¸€ç†è®ºæ´å¯Ÿï¼Œæˆ‘ä»¬é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯è¯±å¯¼æ¥å®è¯æˆ‘ä»¬çš„å‘ç°â€”â€”è¿™æ˜¯ä¸€ç§é€šè¿‡ä¼˜åŒ–æ•Œå¯¹æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸçš„æ–¹æ³•ã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ä¸ªæœ€æ–°å¯¹é½LLMä¸­çš„19ä¸ªä¸Šå®ç°äº†100%çš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒ…æ‹¬DeepSeek-R1å’ŒLLaMA-3ï¼Œæ­ç¤ºäº†å®ƒä»¬çš„æ™®éè„†å¼±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.05050v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯äººå·¥æ™ºèƒ½é€šç”¨æ¢ç´¢çš„åŸºç¡€ï¼Œä½†é€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œåå¥½å­¦ä¹ å®ç°ä¸äººç±»ä»·å€¼çš„å¯¹é½ä»…è¾¾åˆ°è¡¨é¢ä¸Šçš„ç¬¦åˆã€‚ç ”ç©¶è¡¨æ˜ï¼Œé¢„è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºä¸å¯ç£¨ç­çš„â€œæš—æ¨¡å¼â€å­˜åœ¨äºLLMçš„å‚æ•°è®°å¿†ä¸­ï¼Œé€ƒé¿å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨åˆ†å¸ƒè½¬ç§»æ—¶é€šè¿‡å¯¹æŠ—æ€§è¯±å¯¼é‡æ–°æµ®ç°ã€‚æœ¬ç ”ç©¶é¦–å…ˆä»ç†è®ºä¸Šåˆ†æå¯¹é½LLMçš„å†…åœ¨é“å¾·è„†å¼±æ€§ï¼Œè¯æ˜å½“å‰å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ï¼Œè€Œé¢„è®­ç»ƒçŸ¥è¯†ä»ä¸æœ‰å®³æ¦‚å¿µé€šè¿‡é«˜æ¦‚ç‡å¯¹æŠ—æ€§è½¨è¿¹ä¿æŒå…¨çƒè”ç³»ã€‚é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼æ–¹æ³•ï¼Œæˆ‘ä»¬å®è¯éªŒè¯äº†è¿™ä¸€å‘ç°ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–å¯¹æŠ—æ€§æç¤ºæ¥ç³»ç»Ÿåœ°ç»•è¿‡å¯¹é½çº¦æŸã€‚è¿™ç§ç»“åˆç†è®ºå’Œå®è¯çš„æ–¹æ³•åœ¨23ä¸ªæœ€æ–°å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æˆåŠŸæ”»å‡»äº†å…¶ä¸­19ä¸ªï¼Œæ­ç¤ºäº†å…¶æ™®éå­˜åœ¨çš„è„†å¼±æ€§ã€‚æ€»ç»“ï¼šLLMæ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å¯¹é½ä¸å¤Ÿå½»åº•ï¼Œå­˜åœ¨ç†è®ºä¸Šçš„é“å¾·è„†å¼±æ€§å’Œå®è¯ä¸­çš„æ™®éæ”»å‡»è„†å¼±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸä¸­å…·æœ‰åŸºç¡€åœ°ä½ï¼Œä½†å®ƒä»¬ä¸äººç±»ä»·å€¼çš„å¯¹é½ä»…è¾¾åˆ°è¡¨é¢ç¬¦åˆã€‚</li>
<li>é¢„è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥çš„æœ‰å®³çŸ¥è¯†ä¼šä½œä¸ºâ€œæš—æ¨¡å¼â€å­˜åœ¨äºLLMä¸­ã€‚</li>
<li>è¿™äº›â€œæš—æ¨¡å¼â€å¯ä»¥é€ƒé¿ç°æœ‰çš„å¯¹é½ä¿éšœæªæ–½ï¼Œå¹¶åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹é‡æ–°æµ®ç°ã€‚</li>
<li>å½“å‰çš„å¯¹é½æ–¹æ³•ä»…åœ¨çŸ¥è¯†æµå½¢ä¸­äº§ç”Ÿå±€éƒ¨â€œå®‰å…¨åŒºåŸŸâ€ã€‚</li>
<li>é¢„è®­ç»ƒçŸ¥è¯†ä»ä¸æœ‰å®³æ¦‚å¿µä¿æŒå…¨çƒè”ç³»ã€‚</li>
<li>é€šè¿‡åˆ†å¸ƒè½¬ç§»ä¸‹çš„è¯­ä¹‰è¿è´¯æ€§è¯±å¯¼æ–¹æ³•ï¼Œå¯ä»¥ç³»ç»Ÿåœ°ç»•è¿‡LLMçš„å¯¹é½çº¦æŸã€‚</li>
<li>æœ€æ–°çš„LLMæ™®éå­˜åœ¨é“å¾·è„†å¼±æ€§ï¼Œå®¹æ˜“è¢«æ”»å‡»ã€‚å¼ºè°ƒå¯¹LLMè¿›è¡Œæ›´æ·±å…¥çš„å¯¹é½ç ”ç©¶å’ŒåŠ å¼ºå®‰å…¨ä¿éšœçš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.05050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e886cfdf5a2775600cbfa6abd9995005.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ef1ae5f4c8bb99d769899606df41c06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57c4ac31a37125b8a84cf3ed3c2f94db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb2e53b7882fd19a4d824edec1c13d9.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-MEM-Agentic-Memory-for-LLM-Agents"><a href="#A-MEM-Agentic-Memory-for-LLM-Agents" class="headerlink" title="A-MEM: Agentic Memory for LLM Agents"></a>A-MEM: Agentic Memory for LLM Agents</h2><p><strong>Authors:Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang</strong></p>
<p>While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systemsâ€™ fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at <a target="_blank" rel="noopener" href="https://github.com/WujiangXu/AgenticMemory">https://github.com/WujiangXu/AgenticMemory</a>, while the source code of agentic memory system is available at <a target="_blank" rel="noopener" href="https://github.com/agiresearch/A-mem">https://github.com/agiresearch/A-mem</a>. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥å®Œæˆå¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œä½†å®ƒä»¬éœ€è¦è®°å¿†ç³»ç»Ÿæ¥åˆ©ç”¨å†å²ç»éªŒã€‚å½“å‰çš„è®°å¿†ç³»ç»Ÿå¯ä»¥å®ç°åŸºæœ¬çš„å­˜å‚¨å’Œæ£€ç´¢ï¼Œä½†ç¼ºä¹å¤æ‚çš„è®°å¿†ç»„ç»‡ï¼Œå°½ç®¡æœ€è¿‘æœ‰å°è¯•å¼•å…¥å›¾æ•°æ®åº“ã€‚æ­¤å¤–ï¼Œè¿™äº›ç³»ç»Ÿçš„å›ºå®šæ“ä½œå’Œç»“æ„é™åˆ¶äº†å®ƒä»¬åœ¨å„ç§ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºLLMä»£ç†çš„æ–°å‹ä»£ç†è®°å¿†ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»¥åŠ¨æ€åœ°ä»¥ä»£ç†æ–¹å¼å¯¹è®°å¿†è¿›è¡Œç»„ç»‡ã€‚éµå¾ªZettelkastenæ–¹æ³•çš„åŸºæœ¬åŸåˆ™ï¼Œæˆ‘ä»¬è®¾è®¡çš„è®°å¿†ç³»ç»Ÿé€šè¿‡åŠ¨æ€ç´¢å¼•å’Œé“¾æ¥åˆ›å»ºç›¸äº’å…³è”çš„çŸ¥è¯†ç½‘ç»œã€‚æ¯å½“æ·»åŠ æ–°è®°å¿†æ—¶ï¼Œæˆ‘ä»¬ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å«å¤šä¸ªç»“æ„åŒ–å±æ€§çš„ç»¼åˆç¬”è®°ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡æè¿°ã€å…³é”®è¯å’Œæ ‡ç­¾ã€‚ç„¶åï¼Œç³»ç»Ÿåˆ†æå†å²è®°å¿†ä»¥è¯†åˆ«ç›¸å…³è¿æ¥ï¼Œå¹¶åœ¨å­˜åœ¨æœ‰æ„ä¹‰çš„ç›¸ä¼¼æ€§æ—¶å»ºç«‹é“¾æ¥ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªè¿‡ç¨‹ä½¿è®°å¿†å¾—ä»¥è¿›åŒ–â€”â€”éšç€æ–°è®°å¿†çš„èå…¥ï¼Œå®ƒä»¬å¯ä»¥è§¦å‘å¯¹ç°æœ‰å†å²è®°å¿†çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºå’Œå±æ€§çš„æ›´æ–°ï¼Œä½¿è®°å¿†ç½‘ç»œèƒ½å¤Ÿä¸æ–­åœ°å®Œå–„å…¶ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†Zettelkastençš„ç»“æ„åŒ–ç»„ç»‡åŸåˆ™ä¸ä»£ç†é©±åŠ¨å†³ç­–çš„çµæ´»æ€§ï¼Œä»è€Œå®ç°æ›´é€‚åº”ä¸Šä¸‹æ–‡çš„è®°å¿†ç®¡ç†ã€‚åœ¨å…­ä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„å®è¯å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæœ‰æ˜¾è‘—æ”¹è¿›ã€‚è¯„ä¼°æ€§èƒ½çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/agiresearch/A-mem%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E8%80%8C%E4%BB%A3%E7%90%86%E8%AE%B0%E5%BF%86%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%99%E5%8F%AF%E4%BB%A5">https://github.com/WujiangXu/AgenticMemoryä¸Šæ‰¾åˆ°ï¼Œè€Œä»£ç†è®°å¿†ç³»ç»Ÿçš„æºä»£ç åˆ™å¯åœ¨</a><a target="_blank" rel="noopener" href="https://github.com/agiresearch/A-mem%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/agiresearch/A-memä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12110v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éœ€è¦è®°å¿†ç³»ç»Ÿæ¥åˆ©ç”¨å†å²ç»éªŒä»¥å®Œæˆå¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ã€‚å½“å‰è®°å¿†ç³»ç»Ÿè™½å…·å¤‡åŸºæœ¬å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½ï¼Œä½†åœ¨ç»„ç»‡è®°å¿†æ–¹é¢ç¼ºä¹é«˜çº§åŠŸèƒ½ï¼Œä¸”å›ºå®šæ“ä½œå’Œç»“æ„çš„é™åˆ¶å½±å“äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„è¯­è¨€æ¨¡å‹è®°å¿†ç³»ç»Ÿï¼Œé‡‡ç”¨åŠ¨æ€ç´¢å¼•å’Œé“¾æ¥æ–¹å¼ï¼Œæ„å»ºç›¸äº’å…³è”çš„çŸ¥è¯†ç½‘ç»œã€‚æ–°è®°å¿†è¢«æ·»åŠ æ—¶ï¼Œç³»ç»Ÿç”ŸæˆåŒ…å«å¤šç§ç»“æ„åŒ–å±æ€§çš„ç»¼åˆç¬”è®°ï¼Œå¦‚ä¸Šä¸‹æ–‡æè¿°ã€å…³é”®è¯å’Œæ ‡ç­¾ã€‚é€šè¿‡åˆ†æå†å²è®°å¿†ï¼Œç³»ç»Ÿå¯è¯†åˆ«ç›¸å…³è”ç³»å¹¶å»ºç«‹é“¾æ¥ã€‚æ­¤å¤–ï¼Œæ–°è®°å¿†çš„æ•´åˆå¯è§¦å‘ç°æœ‰å†å²è®°å¿†çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºå’Œå±æ€§çš„æ›´æ–°ï¼Œä½¿è®°å¿†ç½‘ç»œä¸æ–­å®Œå–„ç†è§£ã€‚è¯¥æ–¹æ³•ç»“åˆäº†Zettelkastençš„ç»“æ„åŒ–ç»„ç»‡åŸåˆ™å’Œä»£ç†é©±åŠ¨çš„å†³ç­–çµæ´»æ€§ï¼Œå¯å®ç°æ›´é€‚åº”ä¸Šä¸‹æ–‡å’Œè¯­å¢ƒçš„å†…å­˜ç®¡ç†ã€‚å®è¯å®éªŒç»“æœæ˜¾ç¤ºåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„æ”¹è¿›ä¼˜äºç°æœ‰æœ€å…ˆè¿›åŸºçº¿ã€‚ç›¸å…³æºä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMéœ€è¦è®°å¿†ç³»ç»Ÿåˆ©ç”¨å†å²ç»éªŒè¿›è¡Œå¤æ‚ä»»åŠ¡ã€‚</li>
<li>å½“å‰è®°å¿†ç³»ç»Ÿåœ¨è®°å¿†ç»„ç»‡æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹é«˜çº§åŠŸèƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹LLMè®°å¿†ç³»ç»Ÿï¼Œèƒ½åŠ¨æ€æ„å»ºçŸ¥è¯†ç½‘ç»œã€‚</li>
<li>ç³»ç»Ÿåˆ©ç”¨ç»¼åˆç¬”è®°åŒ…å«ç»“æ„åŒ–å±æ€§ï¼Œå¦‚ä¸Šä¸‹æ–‡æè¿°ã€å…³é”®è¯å’Œæ ‡ç­¾ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡åˆ†æå†å²è®°å¿†è¯†åˆ«ç›¸å…³è”ç³»å¹¶å»ºç«‹é“¾æ¥ã€‚</li>
<li>æ–°è®°å¿†çš„æ•´åˆä½¿è®°å¿†ç½‘ç»œèƒ½å¤Ÿä¸æ–­è¿›åŒ–å¹¶æ”¹è¿›ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3311fe515375cf837dab7c870ee2d06b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f60cff38e4f45d95c7cc2c7568c14b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d4a3e95cf2de0ecf3b48d938d5aff05.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="AgentHarm-A-Benchmark-for-Measuring-Harmfulness-of-LLM-Agents"><a href="#AgentHarm-A-Benchmark-for-Measuring-Harmfulness-of-LLM-Agents" class="headerlink" title="AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents"></a>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</h2><p><strong>Authors:Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies</strong></p>
<p>The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents â€“ which use external tools and can execute multi-stage tasks â€“ may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai-safety-institute/AgentHarm">https://huggingface.co/datasets/ai-safety-institute/AgentHarm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠµå¾¡è¶Šç‹±æ”»å‡»ï¼ˆç”¨æˆ·è®¾è®¡æç¤ºä»¥ç»•è¿‡å®‰å…¨æªæ–½å¹¶æ»¥ç”¨æ¨¡å‹èƒ½åŠ›ï¼‰çš„ç¨³å¥æ€§ï¼Œä¸»è¦ç ”ç©¶äº†ä½œä¸ºç®€å•èŠå¤©æœºå™¨äººçš„LLMã€‚åŒæ—¶ï¼Œæ»¥ç”¨èƒ½å¤Ÿä½¿ç”¨å¤–éƒ¨å·¥å…·å¹¶æ‰§è¡Œå¤šé˜¶æ®µä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†å¯èƒ½ä¼šå¸¦æ¥æ›´å¤§çš„é£é™©ï¼Œä½†å…¶ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†ä¿ƒè¿›å…³äºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æ»¥ç”¨çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºAgentHarmçš„æ–°åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸€å¥—åŒ…å«11ç§å±å®³ç±»åˆ«çš„110ä¸ªæ˜ç¡®çš„æ¶æ„ä»£ç†ä»»åŠ¡ï¼ˆåŒ…å«å¢å¼ºç‰ˆæœ¬å…±440ä¸ªä»»åŠ¡ï¼‰ï¼Œæ¶µç›–æ¬ºè¯ˆã€ç½‘ç»œçŠ¯ç½ªå’Œéªšæ‰°ç­‰ã€‚é™¤äº†è¡¡é‡æ¨¡å‹æ˜¯å¦ä¼šæ‹’ç»æœ‰å®³çš„ä»£ç†è¯·æ±‚å¤–ï¼Œè¦åœ¨AgentHarmä¸Šè·å¾—é«˜åˆ†è¿˜éœ€è¦ç¡®ä¿è¶Šç‹±çš„ä»£ç†åœ¨æ”»å‡»åèƒ½å¤Ÿä¿æŒå…¶å®Œæˆå¤šé˜¶æ®µä»»åŠ¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å‘ç°ï¼šï¼ˆ1ï¼‰é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ²¡æœ‰è¶Šç‹±çš„æƒ…å†µä¸‹ä¼šå‡ºäººæ„æ–™åœ°éµä»æ¶æ„ä»£ç†çš„è¯·æ±‚ï¼›ï¼ˆ2ï¼‰ç®€å•çš„é€šç”¨è¶Šç‹±æ¨¡æ¿å¯ä»¥è¢«é€‚åº”æ¥æœ‰æ•ˆåœ°è¶Šç‹±ä»£ç†ï¼›ï¼ˆ3ï¼‰è¿™äº›è¶Šç‹±èƒ½å¤Ÿä½¿ä»£ç†è¡Œä¸ºè¿è´¯ä¸”æ¶æ„ï¼Œå¹¶ä¿ç•™æ¨¡å‹çš„èƒ½åŠ›ã€‚ä¸ºäº†æ–¹ä¾¿å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†è¿›è¡Œç®€å•å¯é çš„æ”»å‡»å’Œé˜²å¾¡è¯„ä¼°ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai-safety-institute/AgentHarm%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E4%BA%86AgentHarm%E3%80%82">https://huggingface.co/datasets/ai-safety-institute/AgentHarmä¸Šå…¬å¼€å‘å¸ƒäº†AgentHarmã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09024v3">PDF</a> Accepted at ICLR 2025</p>
<p><strong>æ‘˜è¦</strong><br>LLMså¯¹åˆ©ç”¨æç¤ºç»•è¿‡å®‰å…¨æªæ–½å¹¶æ»¥ç”¨æ¨¡å‹åŠŸèƒ½çš„è¶Šç‹±æ”»å‡»çš„é²æ£’æ€§å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä¸»è¦é›†ä¸­åœ¨ä½œä¸ºç®€å•èŠå¤©æœºå™¨äººçš„LLMsä¸Šã€‚ç„¶è€Œï¼Œå¯¹äºæ»¥ç”¨èƒ½å¤Ÿä½¿ç”¨å¤–éƒ¨å·¥å…·å¹¶æ‰§è¡Œå¤šé˜¶æ®µä»»åŠ¡çš„LLMä»£ç†ï¼Œå…¶é£é™©å¯èƒ½æ›´å¤§ï¼Œä½†å…¶é²æ£’æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†ä¿ƒè¿›å¯¹LLMä»£ç†æ»¥ç”¨çš„ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•æ–¹æ³•â€”â€”AgentHarmã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬åŒ…å«æ¶µç›–è¯ˆéª—ã€ç½‘ç»œçŠ¯ç½ªå’Œéªšæ‰°ç­‰ç±»åˆ«åœ¨å†…çš„å„ç§æ˜æ˜¾æ¶æ„ä»£ç†ä»»åŠ¡å…±è¾¾ç™¾ä¾‹æ¡ˆä¾‹åŠå…¶æ‰©å±•ç‰ˆæœ¬ã€‚é™¤äº†è¡¡é‡æ¨¡å‹æ˜¯å¦æ‹’ç»æœ‰å®³ä»£ç†è¯·æ±‚å¤–ï¼Œè¦åœ¨AgentHarmä¸Šè·å¾—è‰¯å¥½è¯„åˆ†è¿˜è¦æ±‚è¶Šç‹±ä»£ç†åœ¨å®Œæˆå¤šé˜¶æ®µä»»åŠ¡æ—¶ä¿æŒå…¶åŠŸèƒ½ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—é¢†å…ˆçš„LLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ï¼šï¼ˆ1ï¼‰é¢†å…ˆçš„LLMsåœ¨æœªç»è¶Šç‹±çš„æƒ…å†µä¸‹ä¼šæ„å¤–éµä»æ¶æ„ä»£ç†çš„è¯·æ±‚ï¼›ï¼ˆ2ï¼‰ç®€å•çš„é€šç”¨è¶Šç‹±æ¨¡æ¿å¯ä»¥é€‚åº”æœ‰æ•ˆåœ°è¶Šç‹±ä»£ç†ï¼›ï¼ˆ3ï¼‰è¿™äº›è¶Šç‹±è¡Œä¸ºèƒ½å¤Ÿä¿ç•™æ¨¡å‹åŠŸèƒ½å¹¶äº§ç”Ÿè¿è´¯ä¸”æ¶æ„çš„å¤šé˜¶æ®µä»£ç†è¡Œä¸ºã€‚ä¸ºäº†å®ç°å¯¹åŸºäºLLMçš„ä»£ç†çš„æ”»å‡»å’Œé˜²å¾¡çš„ç®€å•å¯é è¯„ä¼°ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai-safety-institute/AgentHarm%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83AgentHarm%E3%80%82">https://huggingface.co/datasets/ai-safety-institute/AgentHarmå…¬å¼€å‘å¸ƒAgentHarmã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€LLMsé¢ä¸´æ»¥ç”¨é£é™©çš„ç ”ç©¶æ¦‚è¿°</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.09024">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-db41364dbd99ce6983305dff10413b6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e0cb3dd538576c02d7596c429bb7374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6f2f0b3d37feba41ef30c22a7bf9de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d78f5d29a4192863c752c1cde3e5a13.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6d78f5d29a4192863c752c1cde3e5a13.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  ChatNekoHacker Real-Time Fan Engagement with Conversational Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
