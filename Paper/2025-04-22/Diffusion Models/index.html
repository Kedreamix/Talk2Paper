<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-22  Decoding Vision Transformers the Diffusion Steering Lens">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a2b1370e32563de59b2407f6ef45fd0a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    65 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-22-更新"><a href="#2025-04-22-更新" class="headerlink" title="2025-04-22 更新"></a>2025-04-22 更新</h1><h2 id="Decoding-Vision-Transformers-the-Diffusion-Steering-Lens"><a href="#Decoding-Vision-Transformers-the-Diffusion-Steering-Lens" class="headerlink" title="Decoding Vision Transformers: the Diffusion Steering Lens"></a>Decoding Vision Transformers: the Diffusion Steering Lens</h2><p><strong>Authors:Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai</strong></p>
<p>Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. </p>
<blockquote>
<p>Logit Lens是广泛应用于基于转换器的语言模型的机械解释性的方法，它通过投影到输出词汇空间来分析内部表示如何在各层中演变。虽然将Logit Lens应用于视觉转换器（ViTs）在技术上很直接，但其直接使用在捕捉视觉表示的丰富性方面存在局限性。基于Toker等人（2024）的工作，他们引入了Diffusion Lens来可视化文本到图像扩散模型的文本编码器的中间表示，我们证明虽然Diffusion Lens可以有效地可视化图像编码器的剩余流表示，但它无法捕捉单个子模块的直接贡献。为了克服这一局限性，我们提出了<strong>Diffusion Steering Lens（DSL）</strong>，这是一种新型的无训练方法，用于引导子模块输出并修补随后的间接贡献。我们通过干预研究验证了我们的方法，表明DSL为ViTs的内部处理提供了直观可靠的解释。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13763v1">PDF</a> 12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on   Mechanistic Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>基于Logit Lens方法的广泛应用，尽管其应用于Vision Transformers（ViTs）在技术上较为直接，但在捕捉视觉表示的丰富性方面存在局限性。针对这一问题，本文提出一种名为Diffusion Steering Lens（DSL）的新型无训练方法，通过干预研究验证了其在直观可靠地解释ViTs内部处理方面的有效性。该方法通过控制子模块输出并调整间接贡献，有效地克服了现有方法的局限。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Logit Lens是一种广泛应用于基于转换器的语言模型的机械解释方法，用于分析内部表示如何在不同层中演化并投影到输出词汇空间。</li>
<li>虽然Logit Lens在Vision Transformers (ViTs)中的应用在技术上较为直接，但在捕捉视觉表示的丰富性方面存在局限性。</li>
<li>Diffusion Lens虽然可以有效地可视化文本编码器中的中间表示，但在捕捉图像编码器的剩余流表示方面存在不足。</li>
<li>DSL（Diffusion Steering Lens）是一种新型的无训练方法，旨在克服现有方法的局限性，通过控制子模块输出和调整间接贡献来可视化ViTs的内部处理。</li>
<li>DSL通过干预研究验证了其在直观可靠地解释ViTs内部处理方面的有效性。</li>
<li>DSL提供了一个有效的工具来深入了解Vision Transformers的工作机制，有助于进一步改进和优化这些模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13763">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b25fc11f3869a7624d2b8a6506569ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bd47b1cfa834935a674babcfe93332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1568148bc789594619a449bd8dc24b24.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ESPLoRA-Enhanced-Spatial-Precision-with-Low-Rank-Adaption-in-Text-to-Image-Diffusion-Models-for-High-Definition-Synthesis"><a href="#ESPLoRA-Enhanced-Spatial-Precision-with-Low-Rank-Adaption-in-Text-to-Image-Diffusion-Models-for-High-Definition-Synthesis" class="headerlink" title="ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in   Text-to-Image Diffusion Models for High-Definition Synthesis"></a>ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in   Text-to-Image Diffusion Models for High-Definition Synthesis</h2><p><strong>Authors:Andrea Rigo, Luca Stornaiuolo, Mauro Martino, Bruno Lepri, Nicu Sebe</strong></p>
<p>Diffusion models have revolutionized text-to-image (T2I) synthesis, producing high-quality, photorealistic images. However, they still struggle to properly render the spatial relationships described in text prompts. To address the lack of spatial information in T2I generations, existing methods typically use external network conditioning and predefined layouts, resulting in higher computational costs and reduced flexibility. Our approach builds upon a curated dataset of spatially explicit prompts, meticulously extracted and synthesized from LAION-400M to ensure precise alignment between textual descriptions and spatial layouts. Alongside this dataset, we present ESPLoRA, a flexible fine-tuning framework based on Low-Rank Adaptation, specifically designed to enhance spatial consistency in generative models without increasing generation time or compromising the quality of the outputs. In addition to ESPLoRA, we propose refined evaluation metrics grounded in geometric constraints, capturing 3D spatial relations such as \textit{in front of} or \textit{behind}. These metrics also expose spatial biases in T2I models which, even when not fully mitigated, can be strategically exploited by our TORE algorithm to further improve the spatial consistency of generated images. Our method outperforms the current state-of-the-art framework, CoMPaSS, by 13.33% on established spatial consistency benchmarks. </p>
<blockquote>
<p>扩散模型已经彻底改变了文本到图像（T2I）的合成方式，生成了高质量、逼真的图像。然而，它们在正确呈现文本提示中描述的空间关系方面仍存在困难。为了解决T2I生成中空间信息缺失的问题，现有方法通常使用外部网络条件和预定义布局，导致计算成本较高且灵活性降低。我们的方法建立在精心挑选的空间明确提示数据集上，这些数据集是从LAION-400M中提取和合成的，以确保文本描述和空间布局之间的精确对齐。除此之外，我们推出了ESPLoRA，这是一个基于低秩适应的灵活微调框架，专门设计用于提高生成模型的空间一致性，而不会增加生成时间或牺牲输出质量。除了ESPLoRA，我们还提出了基于几何约束的精炼评估指标，捕捉3D空间关系，如“在……前面”或“在……后面”。这些指标还暴露了T2I模型中的空间偏见，即使无法完全缓解，也可以被我们的TORE算法策略性利用，以进一步提高生成图像的空间一致性。我们的方法在建立的空间一致性基准测试上，比当前最先进的框架CoMPaSS高出13.33%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13745v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本到图像合成领域，扩散模型已经实现了革命性的进展，生成了高质量逼真的图像。然而，现有模型在渲染文本提示中的空间关系方面仍存在困难。针对这一问题，本文提出了一种基于LAION-400M数据集的空间明确提示的精细数据集，并引入了ESPLoRA框架，通过低秩自适应技术提高生成模型的空间一致性，无需增加生成时间并保障输出质量。此外，本文还提出了基于几何约束的评估指标和TORE算法进一步改善生成图像的空间一致性。本文方法在空间一致性基准测试中较当前先进框架CoMPaSS高出13.33%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本到图像合成领域取得显著进展，生成高质量图像。</li>
<li>现有模型在渲染文本中的空间关系时存在困难。</li>
<li>本文提出基于LAION-400M数据集的空间明确提示的精细数据集，确保文本描述与空间布局之间的精确对齐。</li>
<li>引入ESPLoRA框架，通过低秩自适应技术提高生成模型的空间一致性。</li>
<li>提出基于几何约束的评估指标，以捕捉图像中的三维空间关系。</li>
<li>本文方法较当前先进框架在空间一致性方面有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-24bee9432cfa27a28b5144af9cf8c520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5274ac26d7d6b8c17c9882ef7dc724.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3e29e6c93c7887ae87729f204e1d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03e066d3dd61c36a26790fc8d161f923.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SupResDiffGAN-a-new-approach-for-the-Super-Resolution-task"><a href="#SupResDiffGAN-a-new-approach-for-the-Super-Resolution-task" class="headerlink" title="SupResDiffGAN a new approach for the Super-Resolution task"></a>SupResDiffGAN a new approach for the Super-Resolution task</h2><p><strong>Authors:Dawid Kopeć, Wojciech Kozłowski, Maciej Wizerkaniuk, Dawid Krutul, Jan Kocoń, Maciej Zięba</strong></p>
<p>In this work, we present SupResDiffGAN, a novel hybrid architecture that combines the strengths of Generative Adversarial Networks (GANs) and diffusion models for super-resolution tasks. By leveraging latent space representations and reducing the number of diffusion steps, SupResDiffGAN achieves significantly faster inference times than other diffusion-based super-resolution models while maintaining competitive perceptual quality. To prevent discriminator overfitting, we propose adaptive noise corruption, ensuring a stable balance between the generator and the discriminator during training. Extensive experiments on benchmark datasets show that our approach outperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency and image quality. This work bridges the performance gap between diffusion- and GAN-based methods, laying the foundation for real-time applications of diffusion models in high-resolution image generation. </p>
<blockquote>
<p>在这项工作中，我们提出了SupResDiffGAN，这是一种新型混合架构，结合了生成对抗网络（GANs）和扩散模型的优点，用于超分辨率任务。通过利用潜在空间表示和减少扩散步骤的数量，SupResDiffGAN实现了比其他基于扩散的超分辨率模型更快的推理时间，同时保持了有竞争力的感知质量。为了防止判别器过度拟合，我们提出了自适应噪声腐蚀，确保在训练过程中生成器和判别器之间的稳定平衡。在基准数据集上的广泛实验表明，我们的方法在效率和图像质量方面优于传统的扩散模型，如SR3和I$^2$SB。这项工作缩小了扩散模型和基于GAN的方法之间的性能差距，为扩散模型在实时高分辨率图像生成中的应用奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13622v1">PDF</a> 25th International Conference on Computational Science</p>
<p><strong>Summary</strong></p>
<p>本文介绍了SupResDiffGAN，这是一种结合了生成对抗网络（GANs）和扩散模型优势的新型混合架构，用于超分辨率任务。通过利用潜在空间表示和减少扩散步骤的数量，SupResDiffGAN实现了比其他基于扩散的超分辨率模型更快的推理时间，同时保持了有竞争力的感知质量。为防止判别器过拟合，提出了自适应噪声腐蚀方法，确保生成器和判别器在训练过程中的稳定平衡。在基准数据集上的广泛实验表明，我们的方法在提高效率和图像质量方面优于传统的扩散模型，如SR3和I$^2$SB。这项工作缩小了扩散模型和基于GAN的方法之间的性能差距，为扩散模型在实时高分辨率图像生成中的应用奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SupResDiffGAN结合了GANs和扩散模型的优点，用于超分辨率任务。</li>
<li>通过利用潜在空间表示和减少扩散步骤，实现了快速推理。</li>
<li>自适应噪声腐蚀方法确保生成器和判别器的稳定平衡，防止判别器过拟合。</li>
<li>在基准数据集上的实验表明，SupResDiffGAN在效率和图像质量方面优于传统扩散模型。</li>
<li>该工作提高了扩散模型在实时高分辨率图像生成中的性能。</li>
<li>SupResDiffGAN的出现缩小了扩散模型和基于GAN的方法之间的性能差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81f83d558377acb0ef6c405780096589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9351b60c5020abf66de339b47383e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c662efea01d76083382b066beb715b5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="U-Shape-Mamba-State-Space-Model-for-faster-diffusion"><a href="#U-Shape-Mamba-State-Space-Model-for-faster-diffusion" class="headerlink" title="U-Shape Mamba: State Space Model for faster diffusion"></a>U-Shape Mamba: State Space Model for faster diffusion</h2><p><strong>Authors:Alex Ergasti, Filippo Botti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati</strong></p>
<p>Diffusion models have become the most popular approach for high-quality image generation, but their high computational cost still remains a significant challenge. To address this problem, we propose U-Shape Mamba (USM), a novel diffusion model that leverages Mamba-based layers within a U-Net-like hierarchical structure. By progressively reducing sequence length in the encoder and restoring it in the decoder through Mamba blocks, USM significantly lowers computational overhead while maintaining strong generative capabilities. Experimental results against Zigma, which is currently the most efficient Mamba-based diffusion model, demonstrate that USM achieves one-third the GFlops, requires less memory and is faster, while outperforming Zigma in image quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7 points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings highlight USM as a highly efficient and scalable solution for diffusion-based generative models, making high-quality image synthesis more accessible to the research community while reducing computational costs. </p>
<blockquote>
<p>扩散模型已成为高质量图像生成的最流行方法，但其高计算成本仍然是一个巨大的挑战。为了解决这个问题，我们提出了U形Mamba（USM），这是一种新型的扩散模型，它利用Mamba基础的层在一个类似U-Net的层次结构中。通过逐步减少编码器中的序列长度并在解码器中通过Mamba块进行恢复，USM在保持强大的生成能力的同时，大大降低了计算开销。与当前最有效的基于Mamba的扩散模型Zigma相比，实验结果表明，USM实现了三分之一倍的GFlops，需要的内存更少且速度更快，同时在图像质量上优于Zigma。在AFHQ、CelebAHQ和COCO数据集上，Frechet Inception Distance（FID）分别提高了15.3、0.84和2.7点。这些发现突出了USM作为基于扩散的生成模型的高效且可扩展的解决方案，使高质量图像合成更容易为研究领域所接触，同时降低了计算成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13499v1">PDF</a> Accepeted at CVPR 2025 eLVM workshop</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的扩散模型U-Shape Mamba（USM），它利用基于Mamba的层次结构来降低计算成本并维持强大的生成能力。实验结果表明，相较于当前最高效的Mamba扩散模型Zigma，USM的计算量减少了三分之一，内存需求更低，速度更快，并且在图像质量上优于Zigma。在AFHQ、CelebAHQ和COCO数据集上，Frechet Inception Distance（FID）分别提高了15.3、0.84和2.7点。这使得高效且可扩展的扩散生成模型成为可能，使高质量图像合成更加易于研究人员使用，同时降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-Shape Mamba（USM）是一种新型的扩散模型，结合了Mamba和U-Net的技术特点。</li>
<li>USM通过渐进地减少编码器中的序列长度并恢复解码器中的序列长度来显著降低计算成本。</li>
<li>USM相较于现有的Mamba扩散模型Zigma，在计算效率、内存使用和速度方面都有显著提升。</li>
<li>USM在图像质量上优于Zigma，并在AFHQ、CelebAHQ和COCO数据集上的FID得分有所提高。</li>
<li>USM提供了一个高度有效的框架，适用于扩散基础的生成模型。</li>
<li>USM降低了高质量图像合成的计算成本，更易于研究人员使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-da21137ee0b872562dfe6ac7bd945d66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50aeab6709a80cc3ffd0614f29ef05e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b13177fbcf357af3bbc8e1aa72b42c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71fdd97d39625a338bc16ebfa60281d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ca6ba7bb3fb91de61abcb156add967d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fbb93af68d7181e326d9e42b4ff27d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0b5287f4ba9e370f0e1bb0cb7a031c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1bbbcb2654e303f36eec7bec3c535e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Early-Timestep-Zero-Shot-Candidate-Selection-for-Instruction-Guided-Image-Editing"><a href="#Early-Timestep-Zero-Shot-Candidate-Selection-for-Instruction-Guided-Image-Editing" class="headerlink" title="Early Timestep Zero-Shot Candidate Selection for Instruction-Guided   Image Editing"></a>Early Timestep Zero-Shot Candidate Selection for Instruction-Guided   Image Editing</h2><p><strong>Authors:Joowon Kim, Ziseok Lee, Donghyeon Cho, Sanghyun Jo, Yeonsung Jung, Kyungsu Kim, Eunho Yang</strong></p>
<p>Despite recent advances in diffusion models, achieving reliable image generation and editing remains challenging due to the inherent diversity induced by stochastic noise in the sampling process. Instruction-guided image editing with diffusion models offers user-friendly capabilities, yet editing failures, such as background distortion, frequently occur. Users often resort to trial and error, adjusting seeds or prompts to achieve satisfactory results, which is inefficient. While seed selection methods exist for Text-to-Image (T2I) generation, they depend on external verifiers, limiting applicability, and evaluating multiple seeds increases computational complexity. To address this, we first establish a multiple-seed-based image editing baseline using background consistency scores, achieving Best-of-N performance without supervision. Building on this, we introduce ELECT (Early-timestep Latent Evaluation for Candidate Selection), a zero-shot framework that selects reliable seeds by estimating background mismatches at early diffusion timesteps, identifying the seed that retains the background while modifying only the foreground. ELECT ranks seed candidates by a background inconsistency score, filtering unsuitable samples early based on background consistency while preserving editability. Beyond standalone seed selection, ELECT integrates into instruction-guided editing pipelines and extends to Multimodal Large-Language Models (MLLMs) for joint seed and prompt selection, further improving results when seed selection alone is insufficient. Experiments show that ELECT reduces computational costs (by 41 percent on average and up to 61 percent) while improving background consistency and instruction adherence, achieving around 40 percent success rates in previously failed cases - without any external supervision or training. </p>
<blockquote>
<p>尽管扩散模型近期取得了进展，但由于采样过程中随机噪声引起的固有多样性，实现可靠的图像生成和编辑仍然具有挑战性。带有扩散模型的指令导向图像编辑提供了用户友好的功能，但编辑失败（例如背景失真）的情况仍然经常发生。用户经常需要反复试验，调整种子或提示才能获得满意的结果，这很不高效。虽然针对文本到图像（T2I）生成的种子选择方法已经存在，但它们依赖于外部验证器，限制了其适用性，并且评估多个种子会增加计算复杂性。为了解决这一问题，我们首先建立了一个基于多种子的图像编辑基线，使用背景一致性分数，无需监督即可实现最佳N性能。在此基础上，我们引入了ELECT（用于候选种子选择的早期时间步长潜在评估），这是一个零样本框架，通过估计早期扩散时间步长中的背景不匹配来选择可靠的种子，识别出保留背景而只修改前景的种子。ELECT通过背景不一致性分数对种子候选进行排名，基于早期背景一致性过滤掉不合适的样本，同时保留可编辑性。除了单独的种子选择外，ELECT还可以集成到指令导向的编辑管道中，并扩展到多模态大型语言模型（MLLMs）进行联合种子和提示选择，在仅依靠种子选择不足以提高结果时进一步改进结果。实验表明，ELECT在降低计算成本（平均降低了41%，最高可达61%）的同时，提高了背景一致性和指令遵循性，在未接受外部监督或培训的情况下，在未成功案例中实现了约40%的成功率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13490v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在扩散模型领域面临的挑战和问题，特别是在图像生成和编辑过程中由于随机噪声引起的多样性问题。文章提出了一种基于多种种子的图像编辑方法和ELECT框架，能够在无需外部监督的情况下选择可靠的种子，并在早期扩散阶段通过估计背景不一致性来选择保留背景而只修改前景的种子。ELECT不仅提高了背景一致性和指令遵循性，还降低了计算成本，使得图像编辑更加高效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成和编辑中仍存在挑战，特别是由于采样过程中的随机噪声引起的多样性问题。</li>
<li>指令引导的图像编辑具有用户友好的能力，但编辑失败（如背景失真）经常发生。</li>
<li>现有种子选择方法依赖于外部验证器，限制了其应用，而评估多个种子增加了计算复杂性。</li>
<li>建立了基于多种子的图像编辑基线，使用背景一致性分数实现最佳N性能，无需监督。</li>
<li>引入ELECT框架，通过估计早期扩散阶段背景不匹配来选择可靠种子，选择保留背景同时只修改前景的种子。</li>
<li>ELECT通过背景不一致性分数对种子候选进行排名，早期过滤不适合的样本，同时保持可编辑性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13490">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c35989f0d4b01dfe922e1be619556cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535f35b8187afdf2e7a89e98eb7abc32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-904d6adcedc579ab9b035a362255d46b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-804993e069b5c0a83327d095704aebda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-038a97d4f7f4374a1d6d38d3eaad7c62.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SMPL-GPTexture-Dual-View-3D-Human-Texture-Estimation-using-Text-to-Image-Generation-Models"><a href="#SMPL-GPTexture-Dual-View-3D-Human-Texture-Estimation-using-Text-to-Image-Generation-Models" class="headerlink" title="SMPL-GPTexture: Dual-View 3D Human Texture Estimation using   Text-to-Image Generation Models"></a>SMPL-GPTexture: Dual-View 3D Human Texture Estimation using   Text-to-Image Generation Models</h2><p><strong>Authors:Mingxiao Tu, Shuchang Ye, Hoijoon Jung, Jinman Kim</strong></p>
<p>Generating high-quality, photorealistic textures for 3D human avatars remains a fundamental yet challenging task in computer vision and multimedia field. However, real paired front and back images of human subjects are rarely available with privacy, ethical and cost of acquisition, which restricts scalability of the data. Additionally, learning priors from image inputs using deep generative models, such as GANs or diffusion models, to infer unseen regions such as the human back often leads to artifacts, structural inconsistencies, or loss of fine-grained detail. To address these issues, we present SMPL-GPTexture (skinned multi-person linear model - general purpose Texture), a novel pipeline that takes natural language prompts as input and leverages a state-of-the-art text-to-image generation model to produce paired high-resolution front and back images of a human subject as the starting point for texture estimation. Using the generated paired dual-view images, we first employ a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment between image pixels and the 3D model’s UV coordinates for each views. Second, we use an inverted rasterization technique that explicitly projects the observed colour from the input images into the UV space, thereby producing accurate, complete texture maps. Finally, we apply a diffusion-based inpainting module to fill in the missing regions, and the fusion mechanism then combines these results into a unified full texture map. Extensive experiments shows that our SMPL-GPTexture can generate high resolution texture aligned with user’s prompts. </p>
<blockquote>
<p>生成高质量、逼真的3D人类角色纹理仍是计算机视觉和多媒体领域的一项基本且具有挑战性的任务。然而，由于隐私、伦理和采集成本等原因，实际的人像前后图像很难获得，这限制了数据的可扩展性。此外，使用深度生成模型（如GAN或扩散模型）从图像输入中学习先验知识，以推断未见区域（如人的背部），通常会导致伪影、结构不一致或细节丢失。为了解决这些问题，我们提出了SMPL-GPTexture（皮肤多人线性模型-通用纹理），这是一种新型管道，它以自然语言提示为输入，利用最先进的文本到图像生成模型，生成配对的高分辨率人像前后图像，作为纹理估计的起点。使用生成的配对双视图图像，我们首先采用人体网格恢复模型，获得图像像素和每个视图的3D模型的UV坐标之间的稳健的2D-to-3D SMPL对齐。其次，我们使用反向光栅化技术，将观察到的颜色从输入图像显式投影到UV空间，从而生成准确、完整的纹理映射。最后，我们应用基于扩散的填充模块来填充缺失区域，然后融合机制将这些结果组合成统一的完整纹理映射。大量实验表明，我们的SMPL-GPTexture可以生成与用户提示对齐的高分辨率纹理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为SMPL-GPTexture的新流程，用于生成高质量的人像纹理。该流程采用自然语言提示作为输入，利用先进的文本到图像生成模型产生配对的高分辨率前后图像。通过对生成图像的2D到3D SMPL对齐，以及将观察到的颜色从输入图像投影到UV空间的反转渲染技术，最终生成准确完整的纹理映射。该流程解决了现实世界图像配对困难及深生成模型在推断未可见区域时产生的伪像问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMPL-GPTexture利用自然语言提示作为输入，生成高质量的人像纹理。</li>
<li>采用先进的文本到图像生成模型产生配对的高分辨率前后图像。</li>
<li>通过2D-to-3D SMPL对齐，实现图像像素与3D模型的UV坐标对应。</li>
<li>采用反转渲染技术将观察到的颜色从输入图像投影到UV空间，生成准确完整的纹理映射。</li>
<li>流程解决了现实世界图像配对困难的问题。</li>
<li>解决深生成模型在推断未可见区域（如人背）时产生的伪像、结构不一致或细节丢失问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ff4651ae7cc54cb8f2a55ecd57e1cbdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eeeae805e23961457d9edf1a8c44090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d18fbdd861999299a0f003280ac1f670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a98d7ee87b43b87c2aa2abdf633002.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7df3a128be5e65437aa7944bbfff542e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WaterFlow-Learning-Fast-Robust-Watermarks-using-Stable-Diffusion"><a href="#WaterFlow-Learning-Fast-Robust-Watermarks-using-Stable-Diffusion" class="headerlink" title="WaterFlow: Learning Fast &amp; Robust Watermarks using Stable Diffusion"></a>WaterFlow: Learning Fast &amp; Robust Watermarks using Stable Diffusion</h2><p><strong>Authors:Vinay Shukla, Prachee Sharma, Ryan Rossi, Sungchul Kim, Tong Yu, Aditya Grover</strong></p>
<p>The ability to embed watermarks in images is a fundamental problem of interest for computer vision, and is exacerbated by the rapid rise of generated imagery in recent times. Current state-of-the-art techniques suffer from computational and statistical challenges such as the slow execution speed for practical deployments. In addition, other works trade off fast watermarking speeds but suffer greatly in their robustness or perceptual quality. In this work, we propose WaterFlow (WF), a fast and extremely robust approach for high fidelity visual watermarking based on a learned latent-dependent watermark. Our approach utilizes a pretrained latent diffusion model to encode an arbitrary image into a latent space and produces a learned watermark that is then planted into the Fourier Domain of the latent. The transformation is specified via invertible flow layers that enhance the expressivity of the latent space of the pre-trained model to better preserve image quality while permitting robust and tractable detection. Most notably, WaterFlow demonstrates state-of-the-art performance on general robustness and is the first method capable of effectively defending against difficult combination attacks. We validate our findings on three widely used real and generated datasets: MS-COCO, DiffusionDB, and WikiArt. </p>
<blockquote>
<p>将图片嵌入水印是计算机视觉领域的一个基础且重要的问题，近年来随着生成图像技术的快速发展，这一问题变得愈发严重。当前最先进的技术面临着计算和统计方面的挑战，如在实际部署中的执行速度较慢。此外，其他方法虽然水印速度快，但在鲁棒性或感知质量方面存在很大缺陷。在这项工作中，我们提出了WaterFlow（WF），这是一种基于学习到的潜在依赖水印的快速且极其鲁棒的高保真视觉水印方法。我们的方法利用预训练的潜在扩散模型将任意图像编码到潜在空间，并产生学习的水印，然后将其植入到潜在空间的傅立叶域。通过可逆流层指定转换，增强了预训练模型的潜在空间的表现力，可以更好地保持图像质量，同时实现鲁棒和可追踪的检测。值得一提的是，WaterFlow在一般鲁棒性方面达到了最新技术水平，并且是第一种能够有效防范复杂组合攻击的方法。我们在三个广泛使用的真实和生成数据集MS-COCO、DiffusionDB和WikiArt上验证了我们的发现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12354v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于预训练扩散模型的快速且高度鲁棒的高保真视觉水印嵌入方法WaterFlow。它通过利用扩散模型将任意图像编码到潜在空间，并在潜在空间的傅立叶域中植入学习到的水印来实现高效水印嵌入。此方法使用可逆流层进行转换，提高了预训练模型的潜在空间表达能力，从而更好地保持了图像质量并实现了鲁棒和可追踪的检测。WaterFlow在一般鲁棒性方面表现出卓越的性能，并且是首个能够有效防御复杂组合攻击的方法。在MS-COCO、DiffusionDB和WikiArt三个广泛使用的真实和生成数据集上的验证结果证明了其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WaterFlow是一种基于预训练扩散模型的快速且高度鲁棒的高保真视觉水印嵌入方法。</li>
<li>WaterFlow利用扩散模型将图像编码到潜在空间，并在潜在空间的傅立叶域中植入学习到的水印。</li>
<li>WaterFlow使用可逆流层进行转换，提高了预训练模型的潜在空间表达能力。</li>
<li>WaterFlow能够很好地保持图像质量，同时实现鲁棒和可追踪的检测。</li>
<li>WaterFlow在一般鲁棒性方面表现出卓越的性能，并能够有效防御复杂组合攻击。</li>
<li>WaterFlow在MS-COCO、DiffusionDB和WikiArt三个数据集上的验证结果证明了其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12354">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4fc4e58e78154eceb96f551103f3c8c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1fe7b7107e4f0164daef59ff6269d50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b9944f9830c787d4b098b96a4294ad0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a390dda013adc4a3b98ee4627b012d53.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning"><a href="#PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning" class="headerlink" title="PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning"></a>PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning</h2><p><strong>Authors:Yaopeng Wang, Huiyu Xu, Zhibo Wang, Jiacheng Du, Zhichao Li, Yiming Li, Qiu Wang, Kui Ren</strong></p>
<p>Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency. </p>
<blockquote>
<p>针对扩散图像的水印技术已引起广泛关注，这主要是由于文本到图像扩散模型的广泛应用和对版权保护的不断增长的需求。最近，先进的水印技术，如Tree Ring，通过在扩散过程中将可追踪的模式（例如圆环）嵌入潜在分布来集成水印。这些方法由于水印导致的不可避免的分发偏移，破坏了生成图像的原语义，从而限制了它们的实用性，特别是在数字艺术创作中。在这项工作中，我们提出了语义感知的枢轴调整水印（PT-Mark），这是一种新的不可见水印方法，既能保留扩散图像语义，又能追踪水印。PT-Mark通过在整个扩散去噪过程中逐渐调整生成轨迹与原始（枢轴）轨迹的对齐，同时保持可追踪的水印，从而保留了水印图像的原始语义。为了实现这一点，我们首先计算每个扩散去噪步骤中水印的显著区域作为空间先验，以识别可以在不破坏水印模式的情况下对齐的区域。在该区域的指导下，然后我们引入了一个额外的枢轴调整分支，以优化文本嵌入，在对齐语义的同时保留水印。广泛评估表明，PT-Mark能够在集成稳健水印的同时保留扩散图像的原始语义。与最先进的水印技术相比，它在语义保留性能（即SSIM、PSNR和LPIPS）方面实现了10%的改进，同时显示出对现实世界扰动的相当稳健性，并且效率提高了四倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10853v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>文本介绍了扩散图像水印技术的新进展。由于文本到图像扩散模型广泛应用及其版权保护需求增加，水印技术受到关注。最新方法如Tree Ring通过在扩散过程中嵌入可追踪模式（如圆环）进行水印集成，但会破坏生成图像的原语义。本文提出一种新型不可见水印技术——语义感知关键调整水印（PT-Mark），该技术既保留扩散图像语义又具备水印可追踪性。PT-Mark通过逐步调整生成轨迹与原始（关键）轨迹对齐，同时在整个扩散去噪过程中保持可追踪水印，保护水印图像的原语义。为达成此目标，首先在每个扩散去噪步骤计算水印的显著区域作为空间先验，以识别可对齐区域而不干扰水印图案。受该区域引导，引入额外关键调整分支优化文本嵌入，在对齐语义的同时保留水印。评估显示，PT-Mark在集成稳健水印的同时，保留扩散图像的原语义，较先进水印方法在语义保留性能上提升10%，同时展现对现实世界扰动的稳健性并提高效率四倍。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>水印技术对于保护文本到图像扩散模型的版权至关重要。</li>
<li>最新方法如Tree Ring虽然能够集成水印，但会破坏生成图像的原语义。</li>
<li>PT-Mark是一种新型不可见水印技术，旨在解决现有技术破坏图像原语义的问题。</li>
<li>PT-Mark通过逐步对齐生成轨迹与原始轨迹，同时保持整个扩散去噪过程中的水印可追踪性，保护图像的原语义。</li>
<li>PT-Mark引入空间先验和关键调整分支来实现语义保留和水印的集成。</li>
<li>评估显示PT-Mark在语义保留、稳健性和效率方面较先进方法有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10853">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f51e42acf61f107bfa076f3a3985ff3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b0658721b6a93677247e04d42ae815.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-658c04a0563e4a55e8caaecbde0a6840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ecd0d79a7cc989cd7cf043ca2a508c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-François Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: <a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a> </p>
<blockquote>
<p>我们提出了GaSLight方法，该方法可以从常规图像生成空间变化的光照。我们的方法建议使用HDR高斯Splats作为光源表示，这是首次将常规图像作为3D渲染器的光源。我们的两阶段过程首先利用扩散模型中的先验知识，以合理且准确的方式增强图像的动态范围。接下来，我们使用高斯Splats对3D照明进行建模，以实现空间变化的光照。我们的方法在HDR估计及其应用于照明虚拟对象和场景方面产生了最先进的成果。为了对图像作为光源进行基准测试，我们引入了一个新型的校准和不饱和HDR数据集来评估图像作为光源。我们使用这个新型数据集和文献中的现有数据集来评估我们的方法。项目页面：<a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v2">PDF</a> </p>
<p><strong>Summary</strong><br>     我们提出了GaSLight方法，该方法可以从常规图像生成空间变化照明。我们的方法使用HDR高斯Splats作为光源表示，这是首次在3D渲染中使用常规图像作为光源。我们的两步过程首先利用扩散模型中的先验知识，以合理且准确的方式增强图像的动态范围。接下来，我们使用高斯Splats对3D照明进行建模，实现空间变化照明。我们的方法在HDR估计及其用于照亮虚拟对象和场景方面的应用方面达到了最新水平。为了方便将图像作为光源进行基准测试，我们引入了一个新型校准和不饱和HDR数据集来评估图像作为光源。我们使用这个新数据集和现有文献数据集来评估我们的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLight方法能够从常规图像生成空间变化照明。</li>
<li>HDR高斯Splats首次被用作光源表示，用于3D渲染中的常规图像。</li>
<li>该方法分为两个阶段：第一阶段增强图像动态范围，第二阶段使用高斯Splats进行3D照明建模，实现空间变化照明。</li>
<li>方法在HDR估计方面达到最新水平，并成功应用于照亮虚拟对象和场景。</li>
<li>为了评估图像作为光源的效果，引入了一个新型校准和不饱和HDR数据集。</li>
<li>该方法结合了新型数据集和现有数据集进行评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0acd540684c76bb65da870b875d2c04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a587eee72f338f28224d08dea7b8d954.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faad0dc0950fb458ed849d09cbe96dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b148832e6da27501a698c1717c440602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ce0b37073da87d139eff1cacc105cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606d27e2d875c7ceeb31eb0cba8ec175.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion harmoniously embeds an input reference image into arbitrary scenes described by the text prompts, producing illusion images exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion features’ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing deep fusion of the reference structural information and the textual semantic information in the diffusion model latent space. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning process, all while substantially outperforming related text-guided I2I methods in image generation quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. Our project is publically available at \href{<a target="_blank" rel="noopener" href="https://xianggao1102.github.io/PTDiffusion_webpage/%7D%7Bthis">https://xianggao1102.github.io/PTDiffusion_webpage/}{this</a> web page}. </p>
<blockquote>
<p>光学错觉隐藏图像是一种有趣的视觉感知现象，图像巧妙地融入另一幅图片中，使观众无法立即识别。我们基于现成的文本到图像（T2I）扩散模型，提出了一种无需训练、文本引导的图像到图像（I2I）转换框架，名为“阶段转移扩散模型”（PTDiffusion），用于合成隐藏艺术。PTDiffusion和谐地将输入参考图像嵌入由文本提示描述的任意场景中，生成显示参考图像隐藏视觉线索的错觉图像。我们的方法核心是一个即插即用的相位转移机制，它动态且渐进地移植扩散特征在降噪过程中的相位谱，重建将参考图像采样到生成的错觉图像中，实现在扩散模型潜在空间中参考结构信息和文本语义信息的深度融合。此外，我们提出了异步相位转移，以实现灵活控制隐藏内容可辨识度的程度。我们的方法绕过了任何模型训练和微调过程，同时在图像生成质量、文本忠实度、视觉辨识度和上下文自然度方面大大优于相关的文本引导I2I方法，为错觉图像合成提供了广泛的质量和数量实验证明。我们的项目已在以下网页公开：[<a target="_blank" rel="noopener" href="https://xianggao110/">https://xianggao110</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v5">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于现成的文本到图像（T2I）扩散模型的新型训练免文本引导图像到图像（I2I）转换框架——PTDiffusion（相位转移扩散模型），用于合成隐藏艺术图像。该模型能够将输入参考图像嵌入到文本描述的场景中，生成带有参考图像隐藏视觉线索的错觉图像。其核心机制是即插即用的相位转移机制，该机制动态地逐步转移去噪过程中的扩散特征的相位谱，实现在扩散模型潜在空间中的参考结构信息和文本语义信息的深度融合。此外，还提出了异步相位转移，以实现灵活控制隐藏内容的可辨识程度。该方法无需任何模型训练和微调过程，在图像生成质量、文本忠实度、视觉可辨识度和上下文自然度等方面均大大优于相关文本引导的I2I方法，为错觉图像合成提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用文本到图像的扩散模型开发了一种新型的文本引导图像到图像转换框架PTDiffusion。</li>
<li>PTDiffusion能够在文本描述的场景中嵌入参考图像，生成带有隐藏视觉线索的错觉图像。</li>
<li>PTDiffusion的核心机制是相位转移机制，该机制实现了在扩散模型潜在空间中的参考结构信息和文本语义信息的深度融合。</li>
<li>提出了异步相位转移，以实现对隐藏内容可辨识度的灵活控制。</li>
<li>该方法无需任何模型训练和微调过程。</li>
<li>PTDiffusion在图像生成质量、文本忠实度、视觉可辨识度和上下文自然度等方面显著优于其他相关方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-63c37dfa16485384bbd37bb46845d839.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d546ccc0900e834495405121f3861e10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6a32b6a8b666e971777898d004c84fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2e7ca8dc8a3c6bf59c59e13a1e6479a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1414e64ebf8d7c7960f3cd0f6426a083.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EditSplat-Multi-View-Fusion-and-Attention-Guided-Optimization-for-View-Consistent-3D-Scene-Editing-with-3D-Gaussian-Splatting"><a href="#EditSplat-Multi-View-Fusion-and-Attention-Guided-Optimization-for-View-Consistent-3D-Scene-Editing-with-3D-Gaussian-Splatting" class="headerlink" title="EditSplat: Multi-View Fusion and Attention-Guided Optimization for   View-Consistent 3D Scene Editing with 3D Gaussian Splatting"></a>EditSplat: Multi-View Fusion and Attention-Guided Optimization for   View-Consistent 3D Scene Editing with 3D Gaussian Splatting</h2><p><strong>Authors:Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Sangheon Shin, Sangmin Kim, Sangpil Kim</strong></p>
<p>Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR&#x2F;VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel text-driven 3D scene editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric structure inherent to 3DGS. Additionally, our AGT utilizes the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local editing. Through extensive qualitative and quantitative evaluations, EditSplat achieves state-of-the-art performance, establishing a new benchmark for text-driven 3D scene editing. </p>
<blockquote>
<p>最近的3D编辑技术进展突出了文本驱动方法在实时、用户友好的AR&#x2F;VR应用中的潜力。然而，当前的方法依赖于二维扩散模型，而没有充分考虑到多视角信息，导致多视角不一致。虽然三维高斯平铺（3DGS）技术显著提高了渲染质量和速度，但其三维编辑过程在优化方面遇到了困难，因为预训练的高斯保留了过多的源信息，阻碍了优化。为了解决这些局限性，我们提出了EditSplat，这是一种新型的文本驱动三维场景编辑框架，它集成了多视角融合指导（MFG）和注意力引导修剪（AGT）。我们的MFG通过融入扩散过程中的关键多视角信息，利用文本到图像的扩散模型的非分类器指导以及3DGS所固有的几何结构，确保多视角的一致性。此外，我们的AGT利用3DGS的显式表示进行有选择性地修剪和优化三维高斯，提高优化效率，实现精确且语义丰富的局部编辑。通过广泛的质量和数量评估，EditSplat达到了最先进的性能，为文本驱动的三维场景编辑建立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11520v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了文本驱动的3D场景编辑框架EditSplat，该框架集成了多视角融合引导（MFG）和注意力引导修剪（AGT）。EditSplat解决了现有方法在实时编辑过程中的多视角不一致和优化效率低下的问题，提高了渲染质量和速度。通过引入MFG和AGT，EditSplat实现了多视角一致性，并提高了优化效率，为精确、语义丰富的局部编辑提供了可能。经过定性和定量评估，EditSplat达到了业界领先水平，为文本驱动的3D场景编辑树立了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文本的关键要点：</p>
<ul>
<li>当前3D编辑方法存在多视角不一致的问题。</li>
<li>现有方法主要依赖二维扩散模型，忽略了多视角信息的重要性。</li>
<li>EditSplat框架集成了多视角融合引导（MFG）和注意力引导修剪（AGT），解决了上述问题。</li>
<li>MFG通过引入文本到图像的扩散模型的分类器自由引导以及3D几何结构，确保多视角一致性。</li>
<li>AGT利用显式的3DGS表示选择性地修剪和优化三维高斯分布，提高了优化效率和精确性。</li>
<li>EditSplat框架实现了精确、语义丰富的局部编辑，并达到了业界领先水平。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11520">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b396433bd0c45ba8bb75b1535ddc456.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-701ec12424a16c05b5750c70ac39a0d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1c10e5a544c37f61ceca619f09777a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b1370e32563de59b2407f6ef45fd0a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PriorDiffusion-Leverage-Language-Prior-in-Diffusion-Models-for-Monocular-Depth-Estimation"><a href="#PriorDiffusion-Leverage-Language-Prior-in-Diffusion-Models-for-Monocular-Depth-Estimation" class="headerlink" title="PriorDiffusion: Leverage Language Prior in Diffusion Models for   Monocular Depth Estimation"></a>PriorDiffusion: Leverage Language Prior in Diffusion Models for   Monocular Depth Estimation</h2><p><strong>Authors:Ziyao Zeng, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong</strong></p>
<p>Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisance. We argue that language prior can enhance monocular depth estimation by leveraging the inductive bias learned during the text-to-image pre-training of diffusion models. The ability of these models to generate images that align with text indicates that they have learned the spatial relationships, size, and shape of specified objects, which can be applied to improve depth estimation. Thus, we propose PriorDiffusion, using a pre-trained text-to-image diffusion model that takes both images and corresponding text descriptions to infer affine-invariant depth through a denoising process. We also show that language prior enhances the model’s perception of specific regions of images that users care about and describe. Simultaneously, language prior acts as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. By training on HyperSim and Virtual KITTI, we achieve faster training convergence, fewer inference diffusion steps, and state-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet. Code will be released upon acceptance. </p>
<blockquote>
<p>传统单目深度估计存在固有的模糊性和视觉干扰问题。我们认为，通过利用扩散模型在文本到图像预训练过程中学到的归纳偏见，语言先验可以增强单目深度估计。这些模型能够生成与文本相符的图像，表明它们已经学会了指定对象的空间关系、大小和形状，这可以用于改进深度估计。因此，我们提出了PriorDiffusion，它使用预训练的文本到图像扩散模型，该模型通过去噪过程推断仿射不变深度，该过程既涉及图像又涉及相应的文本描述。我们还表明，语言先验提高了模型对用户关心和描述的图像特定区域的感知能力。同时，语言先验作为约束，加速了训练和推理扩散轨迹的收敛。通过在HyperSim和Virtual KITTI上进行训练，我们实现了更快的训练收敛速度、更少的推理扩散步骤，以及在NYUv2、KITTI、ETH3D和ScanNet上的最先进的零样本性能。代码将在接受后发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16750v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出利用文本先验信息增强单目深度估计的方法，借助扩散模型的文本到图像预训练中的归纳偏置。扩散模型在生成与文本对应的图像时，已学习物体空间关系、大小和形状，可应用于深度估计。因此，本文提出PriorDiffusion，使用预训练的文本到图像扩散模型，结合图像和相应文本描述，通过去噪过程推断仿射不变深度。同时，文本先验信息提高了模型对用户关注图像特定区域的感知能力，并作为约束加速训练和推理扩散轨迹的收敛。在HyperSim和Virtual KITTI数据集上训练，实现更快的训练收敛速度、更少的推理扩散步骤，以及在NYUv2、KITTI、ETH3D和ScanNet上的零样本性能达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本先验信息可增强单目深度估计，利用扩散模型的归纳偏置。</li>
<li>扩散模型在生成图像时已学习空间关系、大小和形状，可应用于深度估计。</li>
<li>提出PriorDiffusion方法，结合图像和文本描述，通过去噪过程推断深度。</li>
<li>文本先验信息提高模型对图像特定区域的感知能力。</li>
<li>文本先验信息作为约束，加速训练和推理扩散轨迹的收敛。</li>
<li>在多个数据集上实现快速训练收敛和零样本性能领先水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16750">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c8fd58c260d43aa5f9a63e2b9779cba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afa38f027d3ad569d49e3a163a88a57c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd54206f0062ae7c74bb265a79a88033.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea9a4700fff45fbd729d408bd2defb07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3496d92bd4bde16b549b7cdcb7852a27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a239f35ddd2e127da0d69c26fc6bca.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="High-Resolution-Frame-Interpolation-with-Patch-based-Cascaded-Diffusion"><a href="#High-Resolution-Frame-Interpolation-with-Patch-based-Cascaded-Diffusion" class="headerlink" title="High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion"></a>High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion</h2><p><strong>Authors:Junhwa Hur, Charles Herrmann, Saurabh Saxena, Janne Kontkanen, Wei-Sheng Lai, Yichang Shih, Michael Rubinstein, David J. Fleet, Deqing Sun</strong></p>
<p>Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion model for high resolution frame interpolation, HIFI, that excels in these scenarios while achieving competitive performance on standard benchmarks. Cascades, which generate a series of images from low to high resolution, can help significantly with large or complex motion that require both global context for a coarse solution and detailed context for high resolution output. However, contrary to prior work on cascaded diffusion models which perform diffusion on increasingly large resolutions, we use a single model that always performs diffusion at the same resolution and upsamples by processing patches of the inputs and the prior solution. At inference time, this drastically reduces memory usage and allows a single model, solving both frame interpolation (base model’s task) and spatial up-sampling, saving training cost as well. HIFI excels at high-resolution images and complex repeated textures that require global context, achieving comparable or state-of-the-art performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We further introduce a new dataset, LaMoR, that focuses on particularly challenging cases, and HIFI significantly outperforms other baselines. Please visit our project page for video results: <a target="_blank" rel="noopener" href="https://hifi-diffusion.github.io/">https://hifi-diffusion.github.io</a> </p>
<blockquote>
<p>尽管近期有所进展，现有的帧插值方法在处理极高分辨率输入以及面对重复纹理、细薄物体和大动作等复杂情况时仍面临挑战。为了解决这些问题，我们引入了一种基于补丁的级联像素扩散模型，用于高分辨率帧插值（HIFI），该模型在这些场景中表现出色，同时在标准基准测试中实现了具有竞争力的性能。级联生成从低到高的分辨率图像系列，有助于处理需要全局上下文进行粗略解决方案和详细上下文以生成高分辨率输出的大动作或复杂动作。然而，与先前关于级联扩散模型的工作不同，这些工作在越来越高的分辨率上执行扩散，我们使用一个始终在同一分辨率上执行扩散的单一模型，并通过处理输入和先前解决方案的补丁来进行上采样。在推理时间，这大大降低了内存使用，并允许一个单一模型同时解决帧插值（基础模型的任务）和空间上采样，从而节省了训练成本。HIFI在高分辨率图像和复杂重复纹理方面表现出色，这些图像需要全局上下文，在各种基准测试（Vimeo、Xiph、X-Test和SEPE-8K）上达到了相当或最先进的性能。我们还引入了一个新的数据集LaMoR，该数据集专注于特别具有挑战性的情况，HIFI显著优于其他基线模型。有关视频结果的详细信息，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://hifi-diffusion.github.io./">https://hifi-diffusion.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11838v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hifi-diffusion.github.io/">https://hifi-diffusion.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>基于现有的帧插值方法在处理极高分辨率输入时面临的挑战，如重复纹理、薄对象和大运动等问题，我们提出了一种基于补丁的级联像素扩散模型——HIFI。它通过级联生成一系列图像，从低分辨率到高分辨率，在处理大或复杂的运动时表现出卓越性能。不同于先前的级联扩散模型，HIFI始终在相同分辨率下进行扩散，并通过处理输入和先前解决方案的补丁来进行上采样。在推断时，这大大降低了内存使用，并允许一个单一模型同时解决帧插值（基础模型的任务）和空间上采样，降低了训练成本。HIFI在高分辨率图像和复杂重复纹理上表现出卓越性能，在各种基准测试（如Vimeo、Xiph、X-Test和SEPE-8K）上达到或超越现有最佳水平。我们还引入了一个新数据集LaMoR，专注于特别具有挑战性的案例，HIFI在此数据集上的表现显著优于其他基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有帧插值方法在处理高分辨输入时存在挑战，如重复纹理、薄对象和大运动。</li>
<li>引入的HIFI模型采用基于补丁的级联像素扩散方法，针对这些挑战场景表现出卓越性能。</li>
<li>HIFI通过级联生成低分辨率到高分辨率的图像，处理大或复杂的运动。</li>
<li>与其他级联扩散模型不同，HIFI在相同分辨率下进行扩散，并通过处理补丁进行上采样，降低内存使用并降低训练成本。</li>
<li>HIFI在高分辨率图像和复杂重复纹理上表现优秀，在各种基准测试上达到或超越最佳水平。</li>
<li>HIFI在新引入的LaMoR数据集上显著优于其他基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11838">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c97c4785ff4524a762680c0a8f44082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbb10153ba35cecbc07704f375d32f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57307ec0767833b06109d1723e026779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd9ecec7e04a1f5e8997647cd2a205ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b94fd9103b1fe1b152731568c4da8d98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efd2ba8efdeafb195252938972f33cd0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining"><a href="#Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining" class="headerlink" title="Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining"></a>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining</h2><p><strong>Authors:Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao</strong></p>
<p>We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image&#x2F;multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a>. </p>
<blockquote>
<p>我们推出了Lumina-mGPT，这是一系列多模态自回归模型，能够执行各种视觉和语言任务，特别是在根据文本描述生成灵活的光栅图像方面表现出色。通过采用多模态生成预训练（mGPT）进行初始化，我们证明只使用解码器的自回归（AR）模型可以通过灵活的渐进式监督微调（FP-SFT）实现与现代扩散模型相当的高效图像生成性能。配备我们提出的明确图像表示（UniRep），Lumina-mGPT可以灵活地生成各种纵横比的高质量图像。在强大的图像生成能力基础上，我们进一步探索了全能监督微调（Omni-SFT），这是首次尝试将Lumina-mGPT提升为统一的多模态全能模型。结果证明该模型具有多种多模态功能，包括视觉生成任务（如文本到图像&#x2F;多视图生成和可控生成）、视觉识别任务（如分割和深度估计）以及视觉语言任务（如多轮视觉问答），展示了技术方向的广阔潜力。相关代码和检查点已发布在<a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT%E3%80%82">https://github.com/Alpha-VLLM/Lumina-mGPT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02657v2">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a></p>
<p><strong>Summary</strong></p>
<p>Lumina-mGPT是一个多模态自回归模型家族，擅长根据文本描述生成灵活逼真的图像。通过多模态生成预训练（mGPT）初始化，证明解码器仅自回归（AR）模型通过灵活渐进监督微调（FP-SFT）即可实现与现代扩散模型相当的高效率图像生成性能。配备提出的明确图像表示（UniRep），Lumina-mGPT可灵活生成各种纵横比的高质量图像。在强大的图像生成能力基础上，进一步探索了万能监督微调（Omni-SFT），初步尝试将Lumina-mGPT提升为统一的多模态通用模型。该模型展示多功能多模态能力，包括文本到图像&#x2F;多视图生成、可控生成、视觉识别任务如分割和深度估计，以及视觉语言任务如多轮视觉问答等。代码和检查点可通过<a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Alpha-VLLM/Lumina-mGPT获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumina-mGPT是一个多模态自回归模型，擅长文本驱动的图像生成，具有灵活性和高质量。</li>
<li>通过多模态生成预训练（mGPT）初始化，该模型实现了高效的图像生成性能。</li>
<li>Lumina-mGPT配备了明确图像表示（UniRep），能够生成不同纵横比的高质量图像。</li>
<li>模型通过灵活渐进监督微调（FP-SFT）进行优化，提高了性能。</li>
<li>模型展示了多功能多模态能力，包括文本到图像生成、视觉识别任务和视觉语言任务等。</li>
<li>Omni-SFT是一种尝试将Lumina-mGPT提升为统一多模态通用模型的初步探索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3e31bec0b582eddc8d7764e005d1dfff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a36eeccc0ac22139d093e977d319ad0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d99b6daa18c59136b99c2cd515997c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-318ef0e021430f0d8624dc5990ef9b27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3439256919e1cea1f7203273b38fbdce.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LDM-ISP-Enhancing-Neural-ISP-for-Low-Light-with-Latent-Diffusion-Models"><a href="#LDM-ISP-Enhancing-Neural-ISP-for-Low-Light-with-Latent-Diffusion-Models" class="headerlink" title="LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models"></a>LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models</h2><p><strong>Authors:Qiang Wen, Zhefan Rao, Yazhou Xing, Qifeng Chen</strong></p>
<p>Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB image is a significant challenge for modern digital cameras. Prior approaches have difficulties in recovering fine-grained details and true colors of the scene under extremely low-light environments due to near-to-zero SNR. Meanwhile, diffusion models have shown significant progress towards general domain image generation. In this paper, we propose to leverage the pre-trained latent diffusion model to perform the neural ISP for enhancing extremely low-light images. Specifically, to tailor the pre-trained latent diffusion model to operate on the RAW domain, we train a set of lightweight taming modules to inject the RAW information into the diffusion denoising process via modulating the intermediate features of UNet. We further observe different roles of UNet denoising and decoder reconstruction in the latent diffusion model, which inspires us to decompose the low-light image enhancement task into latent-space low-frequency content generation and decoding-phase high-frequency detail maintenance. Through extensive experiments on representative datasets, we demonstrate our simple design not only achieves state-of-the-art performance in quantitative evaluations but also shows significant superiority in visual comparisons over strong baselines, which highlight the effectiveness of powerful generative priors for neural ISP under extremely low-light environments. The project page is available at <a target="_blank" rel="noopener" href="https://csqiangwen.github.io/projects/ldm-isp/">https://csqiangwen.github.io/projects/ldm-isp/</a> </p>
<blockquote>
<p>将低光环境下的噪声RAW图像增强为曝光良好、清晰的sRGB图像，对于现代数码相机来说是一个巨大的挑战。由于信噪比接近零，先前的方法在极端低光环境下恢复场景的细微细节和真实颜色方面存在困难。同时，扩散模型在通用域图像生成方面取得了显著进展。在本文中，我们提出利用预训练的潜在扩散模型来进行神经网络ISP，以提升低光图像。具体来说，为了调整预训练的潜在扩散模型在RAW域上运行，我们训练了一系列轻量级驯服模块，通过将RAW信息注入扩散去噪过程，调制UNet的中间特征。我们进一步观察到UNet去噪和解码器重建在潜在扩散模型中的不同作用，这激励我们将低光图像增强任务分解为潜在空间的低频内容生成和解码阶段的高频细节维护。通过在代表性数据集上进行大量实验，我们证明了我们的简单设计不仅在定量评估中达到了最新性能，而且在视觉比较方面显著优于强大的基准模型，这突出了强大生成先验在极端低光环境下的神经网络ISP的有效性。项目页面可在<a target="_blank" rel="noopener" href="https://csqiangwen.github.io/projects/ldm-isp/%E6%89%BE%E5%88%B0%E3%80%82">https://csqiangwen.github.io/projects/ldm-isp/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01027v4">PDF</a> </p>
<p><strong>Summary</strong><br>    利用预训练潜伏扩散模型，通过神经网络ISP增强低光图像。训练轻量级驯服模块将RAW信息注入扩散去噪过程，分解低光图像增强任务为潜伏空间低频内容生成和解码阶段高频细节维护，实现先进性能并显著优于基准线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用预训练潜伏扩散模型进行神经网络ISP，以应对低光环境下的图像增强挑战。</li>
<li>训练轻量级驯服模块，将RAW信息注入扩散去噪过程。</li>
<li>分解低光图像增强任务为潜伏空间低频内容生成和解码阶段高频细节维护。</li>
<li>UNet去噪和解码器重建在潜伏扩散模型中扮演不同角色。</li>
<li>提出的简单设计在代表性数据集上进行广泛实验，实现先进性能。</li>
<li>该方法在定量评估中表现优异，并在视觉比较中显著优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01027">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-933cef0e8be94eca59a8bd7a5e137157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c33fcc562042bb7a2f8a3324d0c55b91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f6e4818d3898e818b7b5b15cff10cb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d14a4341805223f77602e6ae3cd40194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9c36aa73f73d3c8e5cec72dc6336d1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LaMD-Latent-Motion-Diffusion-for-Image-Conditional-Video-Generation"><a href="#LaMD-Latent-Motion-Diffusion-for-Image-Conditional-Video-Generation" class="headerlink" title="LaMD: Latent Motion Diffusion for Image-Conditional Video Generation"></a>LaMD: Latent Motion Diffusion for Image-Conditional Video Generation</h2><p><strong>Authors:Yaosi Hu, Zhenzhong Chen, Chong Luo</strong></p>
<p>The video generation field has witnessed rapid improvements with the introduction of recent diffusion models. While these models have successfully enhanced appearance quality, they still face challenges in generating coherent and natural movements while efficiently sampling videos. In this paper, we propose to condense video generation into a problem of motion generation, to improve the expressiveness of motion and make video generation more manageable. This can be achieved by breaking down the video generation process into latent motion generation and video reconstruction. Specifically, we present a latent motion diffusion (LaMD) framework, which consists of a motion-decomposed video autoencoder and a diffusion-based motion generator, to implement this idea. Through careful design, the motion-decomposed video autoencoder can compress patterns in movement into a concise latent motion representation. Consequently, the diffusion-based motion generator is able to efficiently generate realistic motion on a continuous latent space under multi-modal conditions, at a cost that is similar to that of image diffusion models. Results show that LaMD generates high-quality videos on various benchmark datasets, including BAIR, Landscape, NATOPS, MUG and CATER-GEN, that encompass a variety of stochastic dynamics and highly controllable movements on multiple image-conditional video generation tasks, while significantly decreases sampling time. </p>
<blockquote>
<p>视频生成领域最近引入了扩散模型，该领域已经看到了迅速的改进。虽然这些模型在提升外观质量方面取得了成功，但它们仍然面临着在有效采样视频的同时生成连贯且自然动作的挑战。在本文中，我们提出将视频生成简化为动作生成问题，以提高动作的表达能力并使视频生成更加易于管理。这可以通过将视频生成过程分解为潜在动作生成和视频重建来实现。具体来说，我们提出了一个潜在动作扩散（LaMD）框架，该框架由动作分解视频自编码器和基于扩散的动作生成器组成，以实现这个想法。通过精心设计，动作分解视频自编码器可以将运动模式压缩成简洁的潜在动作表示。因此，基于扩散的动作生成器能够在连续潜在空间下在多模态条件下高效生成逼真的动作，其成本类似于图像扩散模型。结果表明，LaMD在各种基准数据集上生成了高质量的视频，包括BAIR、Landscape、NATOPS、MUG和CATER-GEN，涵盖了多种随机动态和高度可控的动作在多个图像条件视频生成任务中，同时显著减少了采样时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.11603v2">PDF</a> accepted by IJCV</p>
<p><strong>摘要</strong><br>    最新扩散模型在视频生成领域取得了快速进展，成功提高了视频外观质量。然而，这些模型在生成连贯、自然动作以及高效采样视频方面仍面临挑战。本文提出将视频生成简化为动作生成问题，以提高动作的表达性和视频生成的易管理性。通过分解为潜在动作生成和视频重建两个步骤来实现。具体来说，我们提出了潜在动作扩散（LaMD）框架，包括运动分解视频自编码器和基于扩散的运动生成器。运动分解视频自编码器能够压缩运动模式成简洁的潜在运动表示，而基于扩散的运动生成器则能在多模态条件下在连续的潜在空间高效生成真实动作，成本类似于图像扩散模型。实验结果表明，LaMD在各种基准数据集上生成了高质量的视频，涵盖多种随机动态和高度可控的动作，显著减少了采样时间。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在视频生成领域取得显著进展，但仍面临生成连贯自然动作和高效采样视频的挑战。</li>
<li>提出将视频生成简化为动作生成问题，以提高动作表达性和视频生成的易管理性。</li>
<li>引入潜在动作扩散（LaMD）框架，包括运动分解视频自编码器和基于扩散的运动生成器。</li>
<li>运动分解视频自编码器能压缩运动模式到简洁的潜在运动表示。</li>
<li>基于扩散的运动生成器能在多模态条件下高效生成真实动作。</li>
<li>LaMD框架能在各种基准数据集上生成高质量的视频，涵盖多种随机动态和高度可控的动作。</li>
<li>LaMD框架显著减少了采样时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2304.11603">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c5fc16a8810a74ca16eab92f618ceea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0af96dde83c918f34c2d96b9b7817340.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-271274ff0cc26d994b410bdc34a3e979.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f0c8c4c529625b2ccb6e5f3eac1eb74.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f657e4071acf014b3f096ccdeafde015.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-22  Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3077e570cba8a06780060df47859346.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-22  BEV-GS Feed-forward Gaussian Splatting in Bird's-Eye-View for Road   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
