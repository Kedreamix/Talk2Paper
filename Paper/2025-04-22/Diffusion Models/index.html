<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Decoding Vision Transformers the Diffusion Steering Lens">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a2b1370e32563de59b2407f6ef45fd0a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-22-æ›´æ–°"><a href="#2025-04-22-æ›´æ–°" class="headerlink" title="2025-04-22 æ›´æ–°"></a>2025-04-22 æ›´æ–°</h1><h2 id="Decoding-Vision-Transformers-the-Diffusion-Steering-Lens"><a href="#Decoding-Vision-Transformers-the-Diffusion-Steering-Lens" class="headerlink" title="Decoding Vision Transformers: the Diffusion Steering Lens"></a>Decoding Vision Transformers: the Diffusion Steering Lens</h2><p><strong>Authors:Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai</strong></p>
<p>Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs. </p>
<blockquote>
<p>Logit Lensæ˜¯å¹¿æ³›åº”ç”¨äºåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„æœºæ¢°è§£é‡Šæ€§çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡æŠ•å½±åˆ°è¾“å‡ºè¯æ±‡ç©ºé—´æ¥åˆ†æå†…éƒ¨è¡¨ç¤ºå¦‚ä½•åœ¨å„å±‚ä¸­æ¼”å˜ã€‚è™½ç„¶å°†Logit Lensåº”ç”¨äºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨æŠ€æœ¯ä¸Šå¾ˆç›´æ¥ï¼Œä½†å…¶ç›´æ¥ä½¿ç”¨åœ¨æ•æ‰è§†è§‰è¡¨ç¤ºçš„ä¸°å¯Œæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åŸºäºTokerç­‰äººï¼ˆ2024ï¼‰çš„å·¥ä½œï¼Œä»–ä»¬å¼•å…¥äº†Diffusion Lensæ¥å¯è§†åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨çš„ä¸­é—´è¡¨ç¤ºï¼Œæˆ‘ä»¬è¯æ˜è™½ç„¶Diffusion Lenså¯ä»¥æœ‰æ•ˆåœ°å¯è§†åŒ–å›¾åƒç¼–ç å™¨çš„å‰©ä½™æµè¡¨ç¤ºï¼Œä½†å®ƒæ— æ³•æ•æ‰å•ä¸ªå­æ¨¡å—çš„ç›´æ¥è´¡çŒ®ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Diffusion Steering Lensï¼ˆDSLï¼‰</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¼•å¯¼å­æ¨¡å—è¾“å‡ºå¹¶ä¿®è¡¥éšåçš„é—´æ¥è´¡çŒ®ã€‚æˆ‘ä»¬é€šè¿‡å¹²é¢„ç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¡¨æ˜DSLä¸ºViTsçš„å†…éƒ¨å¤„ç†æä¾›äº†ç›´è§‚å¯é çš„è§£é‡Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13763v1">PDF</a> 12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on   Mechanistic Interpretability for Vision (MIV)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLogit Lensæ–¹æ³•çš„å¹¿æ³›åº”ç”¨ï¼Œå°½ç®¡å…¶åº”ç”¨äºVision Transformersï¼ˆViTsï¼‰åœ¨æŠ€æœ¯ä¸Šè¾ƒä¸ºç›´æ¥ï¼Œä½†åœ¨æ•æ‰è§†è§‰è¡¨ç¤ºçš„ä¸°å¯Œæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åä¸ºDiffusion Steering Lensï¼ˆDSLï¼‰çš„æ–°å‹æ— è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡å¹²é¢„ç ”ç©¶éªŒè¯äº†å…¶åœ¨ç›´è§‚å¯é åœ°è§£é‡ŠViTså†…éƒ¨å¤„ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡æ§åˆ¶å­æ¨¡å—è¾“å‡ºå¹¶è°ƒæ•´é—´æ¥è´¡çŒ®ï¼Œæœ‰æ•ˆåœ°å…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Logit Lensæ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºåŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹çš„æœºæ¢°è§£é‡Šæ–¹æ³•ï¼Œç”¨äºåˆ†æå†…éƒ¨è¡¨ç¤ºå¦‚ä½•åœ¨ä¸åŒå±‚ä¸­æ¼”åŒ–å¹¶æŠ•å½±åˆ°è¾“å‡ºè¯æ±‡ç©ºé—´ã€‚</li>
<li>è™½ç„¶Logit Lensåœ¨Vision Transformers (ViTs)ä¸­çš„åº”ç”¨åœ¨æŠ€æœ¯ä¸Šè¾ƒä¸ºç›´æ¥ï¼Œä½†åœ¨æ•æ‰è§†è§‰è¡¨ç¤ºçš„ä¸°å¯Œæ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Diffusion Lensè™½ç„¶å¯ä»¥æœ‰æ•ˆåœ°å¯è§†åŒ–æ–‡æœ¬ç¼–ç å™¨ä¸­çš„ä¸­é—´è¡¨ç¤ºï¼Œä½†åœ¨æ•æ‰å›¾åƒç¼–ç å™¨çš„å‰©ä½™æµè¡¨ç¤ºæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>DSLï¼ˆDiffusion Steering Lensï¼‰æ˜¯ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œé€šè¿‡æ§åˆ¶å­æ¨¡å—è¾“å‡ºå’Œè°ƒæ•´é—´æ¥è´¡çŒ®æ¥å¯è§†åŒ–ViTsçš„å†…éƒ¨å¤„ç†ã€‚</li>
<li>DSLé€šè¿‡å¹²é¢„ç ”ç©¶éªŒè¯äº†å…¶åœ¨ç›´è§‚å¯é åœ°è§£é‡ŠViTså†…éƒ¨å¤„ç†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>DSLæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·æ¥æ·±å…¥äº†è§£Vision Transformersçš„å·¥ä½œæœºåˆ¶ï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥æ”¹è¿›å’Œä¼˜åŒ–è¿™äº›æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13763">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b25fc11f3869a7624d2b8a6506569ded.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9bd47b1cfa834935a674babcfe93332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1568148bc789594619a449bd8dc24b24.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ESPLoRA-Enhanced-Spatial-Precision-with-Low-Rank-Adaption-in-Text-to-Image-Diffusion-Models-for-High-Definition-Synthesis"><a href="#ESPLoRA-Enhanced-Spatial-Precision-with-Low-Rank-Adaption-in-Text-to-Image-Diffusion-Models-for-High-Definition-Synthesis" class="headerlink" title="ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in   Text-to-Image Diffusion Models for High-Definition Synthesis"></a>ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in   Text-to-Image Diffusion Models for High-Definition Synthesis</h2><p><strong>Authors:Andrea Rigo, Luca Stornaiuolo, Mauro Martino, Bruno Lepri, Nicu Sebe</strong></p>
<p>Diffusion models have revolutionized text-to-image (T2I) synthesis, producing high-quality, photorealistic images. However, they still struggle to properly render the spatial relationships described in text prompts. To address the lack of spatial information in T2I generations, existing methods typically use external network conditioning and predefined layouts, resulting in higher computational costs and reduced flexibility. Our approach builds upon a curated dataset of spatially explicit prompts, meticulously extracted and synthesized from LAION-400M to ensure precise alignment between textual descriptions and spatial layouts. Alongside this dataset, we present ESPLoRA, a flexible fine-tuning framework based on Low-Rank Adaptation, specifically designed to enhance spatial consistency in generative models without increasing generation time or compromising the quality of the outputs. In addition to ESPLoRA, we propose refined evaluation metrics grounded in geometric constraints, capturing 3D spatial relations such as \textit{in front of} or \textit{behind}. These metrics also expose spatial biases in T2I models which, even when not fully mitigated, can be strategically exploited by our TORE algorithm to further improve the spatial consistency of generated images. Our method outperforms the current state-of-the-art framework, CoMPaSS, by 13.33% on established spatial consistency benchmarks. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„åˆæˆæ–¹å¼ï¼Œç”Ÿæˆäº†é«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ­£ç¡®å‘ˆç°æ–‡æœ¬æç¤ºä¸­æè¿°çš„ç©ºé—´å…³ç³»æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³T2Iç”Ÿæˆä¸­ç©ºé—´ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä½¿ç”¨å¤–éƒ¨ç½‘ç»œæ¡ä»¶å’Œé¢„å®šä¹‰å¸ƒå±€ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜ä¸”çµæ´»æ€§é™ä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨ç²¾å¿ƒæŒ‘é€‰çš„ç©ºé—´æ˜ç¡®æç¤ºæ•°æ®é›†ä¸Šï¼Œè¿™äº›æ•°æ®é›†æ˜¯ä»LAION-400Mä¸­æå–å’Œåˆæˆçš„ï¼Œä»¥ç¡®ä¿æ–‡æœ¬æè¿°å’Œç©ºé—´å¸ƒå±€ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ESPLoRAï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä½ç§©é€‚åº”çš„çµæ´»å¾®è°ƒæ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæé«˜ç”Ÿæˆæ¨¡å‹çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œè€Œä¸ä¼šå¢åŠ ç”Ÿæˆæ—¶é—´æˆ–ç‰ºç‰²è¾“å‡ºè´¨é‡ã€‚é™¤äº†ESPLoRAï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†åŸºäºå‡ ä½•çº¦æŸçš„ç²¾ç‚¼è¯„ä¼°æŒ‡æ ‡ï¼Œæ•æ‰3Dç©ºé—´å…³ç³»ï¼Œå¦‚â€œåœ¨â€¦â€¦å‰é¢â€æˆ–â€œåœ¨â€¦â€¦åé¢â€ã€‚è¿™äº›æŒ‡æ ‡è¿˜æš´éœ²äº†T2Iæ¨¡å‹ä¸­çš„ç©ºé—´åè§ï¼Œå³ä½¿æ— æ³•å®Œå…¨ç¼“è§£ï¼Œä¹Ÿå¯ä»¥è¢«æˆ‘ä»¬çš„TOREç®—æ³•ç­–ç•¥æ€§åˆ©ç”¨ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç”Ÿæˆå›¾åƒçš„ç©ºé—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å»ºç«‹çš„ç©ºé—´ä¸€è‡´æ€§åŸºå‡†æµ‹è¯•ä¸Šï¼Œæ¯”å½“å‰æœ€å…ˆè¿›çš„æ¡†æ¶CoMPaSSé«˜å‡º13.33%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13745v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬åˆ°å›¾åƒåˆæˆé¢†åŸŸï¼Œæ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†é©å‘½æ€§çš„è¿›å±•ï¼Œç”Ÿæˆäº†é«˜è´¨é‡é€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ¸²æŸ“æ–‡æœ¬æç¤ºä¸­çš„ç©ºé—´å…³ç³»æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLAION-400Mæ•°æ®é›†çš„ç©ºé—´æ˜ç¡®æç¤ºçš„ç²¾ç»†æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†ESPLoRAæ¡†æ¶ï¼Œé€šè¿‡ä½ç§©è‡ªé€‚åº”æŠ€æœ¯æé«˜ç”Ÿæˆæ¨¡å‹çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œæ— éœ€å¢åŠ ç”Ÿæˆæ—¶é—´å¹¶ä¿éšœè¾“å‡ºè´¨é‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†åŸºäºå‡ ä½•çº¦æŸçš„è¯„ä¼°æŒ‡æ ‡å’ŒTOREç®—æ³•è¿›ä¸€æ­¥æ”¹å–„ç”Ÿæˆå›¾åƒçš„ç©ºé—´ä¸€è‡´æ€§ã€‚æœ¬æ–‡æ–¹æ³•åœ¨ç©ºé—´ä¸€è‡´æ€§åŸºå‡†æµ‹è¯•ä¸­è¾ƒå½“å‰å…ˆè¿›æ¡†æ¶CoMPaSSé«˜å‡º13.33%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒåˆæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œç”Ÿæˆé«˜è´¨é‡å›¾åƒã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ¸²æŸ“æ–‡æœ¬ä¸­çš„ç©ºé—´å…³ç³»æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºLAION-400Mæ•°æ®é›†çš„ç©ºé—´æ˜ç¡®æç¤ºçš„ç²¾ç»†æ•°æ®é›†ï¼Œç¡®ä¿æ–‡æœ¬æè¿°ä¸ç©ºé—´å¸ƒå±€ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>å¼•å…¥ESPLoRAæ¡†æ¶ï¼Œé€šè¿‡ä½ç§©è‡ªé€‚åº”æŠ€æœ¯æé«˜ç”Ÿæˆæ¨¡å‹çš„ç©ºé—´ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºåŸºäºå‡ ä½•çº¦æŸçš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥æ•æ‰å›¾åƒä¸­çš„ä¸‰ç»´ç©ºé—´å…³ç³»ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•è¾ƒå½“å‰å…ˆè¿›æ¡†æ¶åœ¨ç©ºé—´ä¸€è‡´æ€§æ–¹é¢æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24bee9432cfa27a28b5144af9cf8c520.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac5274ac26d7d6b8c17c9882ef7dc724.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3e29e6c93c7887ae87729f204e1d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03e066d3dd61c36a26790fc8d161f923.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SupResDiffGAN-a-new-approach-for-the-Super-Resolution-task"><a href="#SupResDiffGAN-a-new-approach-for-the-Super-Resolution-task" class="headerlink" title="SupResDiffGAN a new approach for the Super-Resolution task"></a>SupResDiffGAN a new approach for the Super-Resolution task</h2><p><strong>Authors:Dawid KopeÄ‡, Wojciech KozÅ‚owski, Maciej Wizerkaniuk, Dawid Krutul, Jan KocoÅ„, Maciej ZiÄ™ba</strong></p>
<p>In this work, we present SupResDiffGAN, a novel hybrid architecture that combines the strengths of Generative Adversarial Networks (GANs) and diffusion models for super-resolution tasks. By leveraging latent space representations and reducing the number of diffusion steps, SupResDiffGAN achieves significantly faster inference times than other diffusion-based super-resolution models while maintaining competitive perceptual quality. To prevent discriminator overfitting, we propose adaptive noise corruption, ensuring a stable balance between the generator and the discriminator during training. Extensive experiments on benchmark datasets show that our approach outperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency and image quality. This work bridges the performance gap between diffusion- and GAN-based methods, laying the foundation for real-time applications of diffusion models in high-resolution image generation. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SupResDiffGANï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ··åˆæ¶æ„ï¼Œç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç”¨äºè¶…åˆ†è¾¨ç‡ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨æ½œåœ¨ç©ºé—´è¡¨ç¤ºå’Œå‡å°‘æ‰©æ•£æ­¥éª¤çš„æ•°é‡ï¼ŒSupResDiffGANå®ç°äº†æ¯”å…¶ä»–åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹æ›´å¿«çš„æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æ„ŸçŸ¥è´¨é‡ã€‚ä¸ºäº†é˜²æ­¢åˆ¤åˆ«å™¨è¿‡åº¦æ‹Ÿåˆï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”å™ªå£°è…èš€ï¼Œç¡®ä¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¹‹é—´çš„ç¨³å®šå¹³è¡¡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œå›¾åƒè´¨é‡æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚SR3å’ŒI$^2$SBã€‚è¿™é¡¹å·¥ä½œç¼©å°äº†æ‰©æ•£æ¨¡å‹å’ŒåŸºäºGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨å®æ—¶é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13622v1">PDF</a> 25th International Conference on Computational Science</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†SupResDiffGANï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹ä¼˜åŠ¿çš„æ–°å‹æ··åˆæ¶æ„ï¼Œç”¨äºè¶…åˆ†è¾¨ç‡ä»»åŠ¡ã€‚é€šè¿‡åˆ©ç”¨æ½œåœ¨ç©ºé—´è¡¨ç¤ºå’Œå‡å°‘æ‰©æ•£æ­¥éª¤çš„æ•°é‡ï¼ŒSupResDiffGANå®ç°äº†æ¯”å…¶ä»–åŸºäºæ‰©æ•£çš„è¶…åˆ†è¾¨ç‡æ¨¡å‹æ›´å¿«çš„æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†æœ‰ç«äº‰åŠ›çš„æ„ŸçŸ¥è´¨é‡ã€‚ä¸ºé˜²æ­¢åˆ¤åˆ«å™¨è¿‡æ‹Ÿåˆï¼Œæå‡ºäº†è‡ªé€‚åº”å™ªå£°è…èš€æ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šå¹³è¡¡ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ•ˆç‡å’Œå›¾åƒè´¨é‡æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ï¼Œå¦‚SR3å’ŒI$^2$SBã€‚è¿™é¡¹å·¥ä½œç¼©å°äº†æ‰©æ•£æ¨¡å‹å’ŒåŸºäºGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ‰©æ•£æ¨¡å‹åœ¨å®æ—¶é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SupResDiffGANç»“åˆäº†GANså’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç”¨äºè¶…åˆ†è¾¨ç‡ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ½œåœ¨ç©ºé—´è¡¨ç¤ºå’Œå‡å°‘æ‰©æ•£æ­¥éª¤ï¼Œå®ç°äº†å¿«é€Ÿæ¨ç†ã€‚</li>
<li>è‡ªé€‚åº”å™ªå£°è…èš€æ–¹æ³•ç¡®ä¿ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„ç¨³å®šå¹³è¡¡ï¼Œé˜²æ­¢åˆ¤åˆ«å™¨è¿‡æ‹Ÿåˆã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSupResDiffGANåœ¨æ•ˆç‡å’Œå›¾åƒè´¨é‡æ–¹é¢ä¼˜äºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>è¯¥å·¥ä½œæé«˜äº†æ‰©æ•£æ¨¡å‹åœ¨å®æ—¶é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆä¸­çš„æ€§èƒ½ã€‚</li>
<li>SupResDiffGANçš„å‡ºç°ç¼©å°äº†æ‰©æ•£æ¨¡å‹å’ŒåŸºäºGANçš„æ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81f83d558377acb0ef6c405780096589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9351b60c5020abf66de339b47383e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c662efea01d76083382b066beb715b5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="U-Shape-Mamba-State-Space-Model-for-faster-diffusion"><a href="#U-Shape-Mamba-State-Space-Model-for-faster-diffusion" class="headerlink" title="U-Shape Mamba: State Space Model for faster diffusion"></a>U-Shape Mamba: State Space Model for faster diffusion</h2><p><strong>Authors:Alex Ergasti, Filippo Botti, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati</strong></p>
<p>Diffusion models have become the most popular approach for high-quality image generation, but their high computational cost still remains a significant challenge. To address this problem, we propose U-Shape Mamba (USM), a novel diffusion model that leverages Mamba-based layers within a U-Net-like hierarchical structure. By progressively reducing sequence length in the encoder and restoring it in the decoder through Mamba blocks, USM significantly lowers computational overhead while maintaining strong generative capabilities. Experimental results against Zigma, which is currently the most efficient Mamba-based diffusion model, demonstrate that USM achieves one-third the GFlops, requires less memory and is faster, while outperforming Zigma in image quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7 points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings highlight USM as a highly efficient and scalable solution for diffusion-based generative models, making high-quality image synthesis more accessible to the research community while reducing computational costs. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆçš„æœ€æµè¡Œæ–¹æ³•ï¼Œä½†å…¶é«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Uå½¢Mambaï¼ˆUSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨MambaåŸºç¡€çš„å±‚åœ¨ä¸€ä¸ªç±»ä¼¼U-Netçš„å±‚æ¬¡ç»“æ„ä¸­ã€‚é€šè¿‡é€æ­¥å‡å°‘ç¼–ç å™¨ä¸­çš„åºåˆ—é•¿åº¦å¹¶åœ¨è§£ç å™¨ä¸­é€šè¿‡Mambaå—è¿›è¡Œæ¢å¤ï¼ŒUSMåœ¨ä¿æŒå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›çš„åŒæ—¶ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ã€‚ä¸å½“å‰æœ€æœ‰æ•ˆçš„åŸºäºMambaçš„æ‰©æ•£æ¨¡å‹Zigmaç›¸æ¯”ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒUSMå®ç°äº†ä¸‰åˆ†ä¹‹ä¸€å€çš„GFlopsï¼Œéœ€è¦çš„å†…å­˜æ›´å°‘ä¸”é€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºZigmaã€‚åœ¨AFHQã€CelebAHQå’ŒCOCOæ•°æ®é›†ä¸Šï¼ŒFrechet Inception Distanceï¼ˆFIDï¼‰åˆ†åˆ«æé«˜äº†15.3ã€0.84å’Œ2.7ç‚¹ã€‚è¿™äº›å‘ç°çªå‡ºäº†USMä½œä¸ºåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œä½¿é«˜è´¨é‡å›¾åƒåˆæˆæ›´å®¹æ˜“ä¸ºç ”ç©¶é¢†åŸŸæ‰€æ¥è§¦ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13499v1">PDF</a> Accepeted at CVPR 2025 eLVM workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹U-Shape Mambaï¼ˆUSMï¼‰ï¼Œå®ƒåˆ©ç”¨åŸºäºMambaçš„å±‚æ¬¡ç»“æ„æ¥é™ä½è®¡ç®—æˆæœ¬å¹¶ç»´æŒå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºå½“å‰æœ€é«˜æ•ˆçš„Mambaæ‰©æ•£æ¨¡å‹Zigmaï¼ŒUSMçš„è®¡ç®—é‡å‡å°‘äº†ä¸‰åˆ†ä¹‹ä¸€ï¼Œå†…å­˜éœ€æ±‚æ›´ä½ï¼Œé€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”åœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºZigmaã€‚åœ¨AFHQã€CelebAHQå’ŒCOCOæ•°æ®é›†ä¸Šï¼ŒFrechet Inception Distanceï¼ˆFIDï¼‰åˆ†åˆ«æé«˜äº†15.3ã€0.84å’Œ2.7ç‚¹ã€‚è¿™ä½¿å¾—é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ‰©æ•£ç”Ÿæˆæ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œä½¿é«˜è´¨é‡å›¾åƒåˆæˆæ›´åŠ æ˜“äºç ”ç©¶äººå‘˜ä½¿ç”¨ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>U-Shape Mambaï¼ˆUSMï¼‰æ˜¯ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†Mambaå’ŒU-Netçš„æŠ€æœ¯ç‰¹ç‚¹ã€‚</li>
<li>USMé€šè¿‡æ¸è¿›åœ°å‡å°‘ç¼–ç å™¨ä¸­çš„åºåˆ—é•¿åº¦å¹¶æ¢å¤è§£ç å™¨ä¸­çš„åºåˆ—é•¿åº¦æ¥æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>USMç›¸è¾ƒäºç°æœ‰çš„Mambaæ‰©æ•£æ¨¡å‹Zigmaï¼Œåœ¨è®¡ç®—æ•ˆç‡ã€å†…å­˜ä½¿ç”¨å’Œé€Ÿåº¦æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>USMåœ¨å›¾åƒè´¨é‡ä¸Šä¼˜äºZigmaï¼Œå¹¶åœ¨AFHQã€CelebAHQå’ŒCOCOæ•°æ®é›†ä¸Šçš„FIDå¾—åˆ†æœ‰æ‰€æé«˜ã€‚</li>
<li>USMæä¾›äº†ä¸€ä¸ªé«˜åº¦æœ‰æ•ˆçš„æ¡†æ¶ï¼Œé€‚ç”¨äºæ‰©æ•£åŸºç¡€çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>USMé™ä½äº†é«˜è´¨é‡å›¾åƒåˆæˆçš„è®¡ç®—æˆæœ¬ï¼Œæ›´æ˜“äºç ”ç©¶äººå‘˜ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-da21137ee0b872562dfe6ac7bd945d66.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50aeab6709a80cc3ffd0614f29ef05e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b13177fbcf357af3bbc8e1aa72b42c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71fdd97d39625a338bc16ebfa60281d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ca6ba7bb3fb91de61abcb156add967d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fbb93af68d7181e326d9e42b4ff27d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0b5287f4ba9e370f0e1bb0cb7a031c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1bbbcb2654e303f36eec7bec3c535e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Early-Timestep-Zero-Shot-Candidate-Selection-for-Instruction-Guided-Image-Editing"><a href="#Early-Timestep-Zero-Shot-Candidate-Selection-for-Instruction-Guided-Image-Editing" class="headerlink" title="Early Timestep Zero-Shot Candidate Selection for Instruction-Guided   Image Editing"></a>Early Timestep Zero-Shot Candidate Selection for Instruction-Guided   Image Editing</h2><p><strong>Authors:Joowon Kim, Ziseok Lee, Donghyeon Cho, Sanghyun Jo, Yeonsung Jung, Kyungsu Kim, Eunho Yang</strong></p>
<p>Despite recent advances in diffusion models, achieving reliable image generation and editing remains challenging due to the inherent diversity induced by stochastic noise in the sampling process. Instruction-guided image editing with diffusion models offers user-friendly capabilities, yet editing failures, such as background distortion, frequently occur. Users often resort to trial and error, adjusting seeds or prompts to achieve satisfactory results, which is inefficient. While seed selection methods exist for Text-to-Image (T2I) generation, they depend on external verifiers, limiting applicability, and evaluating multiple seeds increases computational complexity. To address this, we first establish a multiple-seed-based image editing baseline using background consistency scores, achieving Best-of-N performance without supervision. Building on this, we introduce ELECT (Early-timestep Latent Evaluation for Candidate Selection), a zero-shot framework that selects reliable seeds by estimating background mismatches at early diffusion timesteps, identifying the seed that retains the background while modifying only the foreground. ELECT ranks seed candidates by a background inconsistency score, filtering unsuitable samples early based on background consistency while preserving editability. Beyond standalone seed selection, ELECT integrates into instruction-guided editing pipelines and extends to Multimodal Large-Language Models (MLLMs) for joint seed and prompt selection, further improving results when seed selection alone is insufficient. Experiments show that ELECT reduces computational costs (by 41 percent on average and up to 61 percent) while improving background consistency and instruction adherence, achieving around 40 percent success rates in previously failed cases - without any external supervision or training. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹è¿‘æœŸå–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºé‡‡æ ·è¿‡ç¨‹ä¸­éšæœºå™ªå£°å¼•èµ·çš„å›ºæœ‰å¤šæ ·æ€§ï¼Œå®ç°å¯é çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¸¦æœ‰æ‰©æ•£æ¨¡å‹çš„æŒ‡ä»¤å¯¼å‘å›¾åƒç¼–è¾‘æä¾›äº†ç”¨æˆ·å‹å¥½çš„åŠŸèƒ½ï¼Œä½†ç¼–è¾‘å¤±è´¥ï¼ˆä¾‹å¦‚èƒŒæ™¯å¤±çœŸï¼‰çš„æƒ…å†µä»ç„¶ç»å¸¸å‘ç”Ÿã€‚ç”¨æˆ·ç»å¸¸éœ€è¦åå¤è¯•éªŒï¼Œè°ƒæ•´ç§å­æˆ–æç¤ºæ‰èƒ½è·å¾—æ»¡æ„çš„ç»“æœï¼Œè¿™å¾ˆä¸é«˜æ•ˆã€‚è™½ç„¶é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆçš„ç§å­é€‰æ‹©æ–¹æ³•å·²ç»å­˜åœ¨ï¼Œä½†å®ƒä»¬ä¾èµ–äºå¤–éƒ¨éªŒè¯å™¨ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ï¼Œå¹¶ä¸”è¯„ä¼°å¤šä¸ªç§å­ä¼šå¢åŠ è®¡ç®—å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªåŸºäºå¤šç§å­çš„å›¾åƒç¼–è¾‘åŸºçº¿ï¼Œä½¿ç”¨èƒŒæ™¯ä¸€è‡´æ€§åˆ†æ•°ï¼Œæ— éœ€ç›‘ç£å³å¯å®ç°æœ€ä½³Næ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ELECTï¼ˆç”¨äºå€™é€‰ç§å­é€‰æ‹©çš„æ—©æœŸæ—¶é—´æ­¥é•¿æ½œåœ¨è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ¡†æ¶ï¼Œé€šè¿‡ä¼°è®¡æ—©æœŸæ‰©æ•£æ—¶é—´æ­¥é•¿ä¸­çš„èƒŒæ™¯ä¸åŒ¹é…æ¥é€‰æ‹©å¯é çš„ç§å­ï¼Œè¯†åˆ«å‡ºä¿ç•™èƒŒæ™¯è€Œåªä¿®æ”¹å‰æ™¯çš„ç§å­ã€‚ELECTé€šè¿‡èƒŒæ™¯ä¸ä¸€è‡´æ€§åˆ†æ•°å¯¹ç§å­å€™é€‰è¿›è¡Œæ’åï¼ŒåŸºäºæ—©æœŸèƒŒæ™¯ä¸€è‡´æ€§è¿‡æ»¤æ‰ä¸åˆé€‚çš„æ ·æœ¬ï¼ŒåŒæ—¶ä¿ç•™å¯ç¼–è¾‘æ€§ã€‚é™¤äº†å•ç‹¬çš„ç§å­é€‰æ‹©å¤–ï¼ŒELECTè¿˜å¯ä»¥é›†æˆåˆ°æŒ‡ä»¤å¯¼å‘çš„ç¼–è¾‘ç®¡é“ä¸­ï¼Œå¹¶æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œè”åˆç§å­å’Œæç¤ºé€‰æ‹©ï¼Œåœ¨ä»…ä¾é ç§å­é€‰æ‹©ä¸è¶³ä»¥æé«˜ç»“æœæ—¶è¿›ä¸€æ­¥æ”¹è¿›ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒELECTåœ¨é™ä½è®¡ç®—æˆæœ¬ï¼ˆå¹³å‡é™ä½äº†41%ï¼Œæœ€é«˜å¯è¾¾61%ï¼‰çš„åŒæ—¶ï¼Œæé«˜äº†èƒŒæ™¯ä¸€è‡´æ€§å’ŒæŒ‡ä»¤éµå¾ªæ€§ï¼Œåœ¨æœªæ¥å—å¤–éƒ¨ç›‘ç£æˆ–åŸ¹è®­çš„æƒ…å†µä¸‹ï¼Œåœ¨æœªæˆåŠŸæ¡ˆä¾‹ä¸­å®ç°äº†çº¦40%çš„æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13490v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ‰©æ•£æ¨¡å‹é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜å’Œé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘è¿‡ç¨‹ä¸­ç”±äºéšæœºå™ªå£°å¼•èµ·çš„å¤šæ ·æ€§é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå¤šç§ç§å­çš„å›¾åƒç¼–è¾‘æ–¹æ³•å’ŒELECTæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€å¤–éƒ¨ç›‘ç£çš„æƒ…å†µä¸‹é€‰æ‹©å¯é çš„ç§å­ï¼Œå¹¶åœ¨æ—©æœŸæ‰©æ•£é˜¶æ®µé€šè¿‡ä¼°è®¡èƒŒæ™¯ä¸ä¸€è‡´æ€§æ¥é€‰æ‹©ä¿ç•™èƒŒæ™¯è€Œåªä¿®æ”¹å‰æ™¯çš„ç§å­ã€‚ELECTä¸ä»…æé«˜äº†èƒŒæ™¯ä¸€è‡´æ€§å’ŒæŒ‡ä»¤éµå¾ªæ€§ï¼Œè¿˜é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—å›¾åƒç¼–è¾‘æ›´åŠ é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºé‡‡æ ·è¿‡ç¨‹ä¸­çš„éšæœºå™ªå£°å¼•èµ·çš„å¤šæ ·æ€§é—®é¢˜ã€‚</li>
<li>æŒ‡ä»¤å¼•å¯¼çš„å›¾åƒç¼–è¾‘å…·æœ‰ç”¨æˆ·å‹å¥½çš„èƒ½åŠ›ï¼Œä½†ç¼–è¾‘å¤±è´¥ï¼ˆå¦‚èƒŒæ™¯å¤±çœŸï¼‰ç»å¸¸å‘ç”Ÿã€‚</li>
<li>ç°æœ‰ç§å­é€‰æ‹©æ–¹æ³•ä¾èµ–äºå¤–éƒ¨éªŒè¯å™¨ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ï¼Œè€Œè¯„ä¼°å¤šä¸ªç§å­å¢åŠ äº†è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>å»ºç«‹äº†åŸºäºå¤šç§å­çš„å›¾åƒç¼–è¾‘åŸºçº¿ï¼Œä½¿ç”¨èƒŒæ™¯ä¸€è‡´æ€§åˆ†æ•°å®ç°æœ€ä½³Næ€§èƒ½ï¼Œæ— éœ€ç›‘ç£ã€‚</li>
<li>å¼•å…¥ELECTæ¡†æ¶ï¼Œé€šè¿‡ä¼°è®¡æ—©æœŸæ‰©æ•£é˜¶æ®µèƒŒæ™¯ä¸åŒ¹é…æ¥é€‰æ‹©å¯é ç§å­ï¼Œé€‰æ‹©ä¿ç•™èƒŒæ™¯åŒæ—¶åªä¿®æ”¹å‰æ™¯çš„ç§å­ã€‚</li>
<li>ELECTé€šè¿‡èƒŒæ™¯ä¸ä¸€è‡´æ€§åˆ†æ•°å¯¹ç§å­å€™é€‰è¿›è¡Œæ’åï¼Œæ—©æœŸè¿‡æ»¤ä¸é€‚åˆçš„æ ·æœ¬ï¼ŒåŒæ—¶ä¿æŒå¯ç¼–è¾‘æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13490">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c35989f0d4b01dfe922e1be619556cfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535f35b8187afdf2e7a89e98eb7abc32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-904d6adcedc579ab9b035a362255d46b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-804993e069b5c0a83327d095704aebda.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-038a97d4f7f4374a1d6d38d3eaad7c62.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SMPL-GPTexture-Dual-View-3D-Human-Texture-Estimation-using-Text-to-Image-Generation-Models"><a href="#SMPL-GPTexture-Dual-View-3D-Human-Texture-Estimation-using-Text-to-Image-Generation-Models" class="headerlink" title="SMPL-GPTexture: Dual-View 3D Human Texture Estimation using   Text-to-Image Generation Models"></a>SMPL-GPTexture: Dual-View 3D Human Texture Estimation using   Text-to-Image Generation Models</h2><p><strong>Authors:Mingxiao Tu, Shuchang Ye, Hoijoon Jung, Jinman Kim</strong></p>
<p>Generating high-quality, photorealistic textures for 3D human avatars remains a fundamental yet challenging task in computer vision and multimedia field. However, real paired front and back images of human subjects are rarely available with privacy, ethical and cost of acquisition, which restricts scalability of the data. Additionally, learning priors from image inputs using deep generative models, such as GANs or diffusion models, to infer unseen regions such as the human back often leads to artifacts, structural inconsistencies, or loss of fine-grained detail. To address these issues, we present SMPL-GPTexture (skinned multi-person linear model - general purpose Texture), a novel pipeline that takes natural language prompts as input and leverages a state-of-the-art text-to-image generation model to produce paired high-resolution front and back images of a human subject as the starting point for texture estimation. Using the generated paired dual-view images, we first employ a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment between image pixels and the 3D modelâ€™s UV coordinates for each views. Second, we use an inverted rasterization technique that explicitly projects the observed colour from the input images into the UV space, thereby producing accurate, complete texture maps. Finally, we apply a diffusion-based inpainting module to fill in the missing regions, and the fusion mechanism then combines these results into a unified full texture map. Extensive experiments shows that our SMPL-GPTexture can generate high resolution texture aligned with userâ€™s prompts. </p>
<blockquote>
<p>ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„3Däººç±»è§’è‰²çº¹ç†ä»æ˜¯è®¡ç®—æœºè§†è§‰å’Œå¤šåª’ä½“é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºéšç§ã€ä¼¦ç†å’Œé‡‡é›†æˆæœ¬ç­‰åŸå› ï¼Œå®é™…çš„äººåƒå‰åå›¾åƒå¾ˆéš¾è·å¾—ï¼Œè¿™é™åˆ¶äº†æ•°æ®çš„å¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚GANæˆ–æ‰©æ•£æ¨¡å‹ï¼‰ä»å›¾åƒè¾“å…¥ä¸­å­¦ä¹ å…ˆéªŒçŸ¥è¯†ï¼Œä»¥æ¨æ–­æœªè§åŒºåŸŸï¼ˆå¦‚äººçš„èƒŒéƒ¨ï¼‰ï¼Œé€šå¸¸ä¼šå¯¼è‡´ä¼ªå½±ã€ç»“æ„ä¸ä¸€è‡´æˆ–ç»†èŠ‚ä¸¢å¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SMPL-GPTextureï¼ˆçš®è‚¤å¤šäººçº¿æ€§æ¨¡å‹-é€šç”¨çº¹ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç®¡é“ï¼Œå®ƒä»¥è‡ªç„¶è¯­è¨€æç¤ºä¸ºè¾“å…¥ï¼Œåˆ©ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œç”Ÿæˆé…å¯¹çš„é«˜åˆ†è¾¨ç‡äººåƒå‰åå›¾åƒï¼Œä½œä¸ºçº¹ç†ä¼°è®¡çš„èµ·ç‚¹ã€‚ä½¿ç”¨ç”Ÿæˆçš„é…å¯¹åŒè§†å›¾å›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨äººä½“ç½‘æ ¼æ¢å¤æ¨¡å‹ï¼Œè·å¾—å›¾åƒåƒç´ å’Œæ¯ä¸ªè§†å›¾çš„3Dæ¨¡å‹çš„UVåæ ‡ä¹‹é—´çš„ç¨³å¥çš„2D-to-3D SMPLå¯¹é½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨åå‘å…‰æ …åŒ–æŠ€æœ¯ï¼Œå°†è§‚å¯Ÿåˆ°çš„é¢œè‰²ä»è¾“å…¥å›¾åƒæ˜¾å¼æŠ•å½±åˆ°UVç©ºé—´ï¼Œä»è€Œç”Ÿæˆå‡†ç¡®ã€å®Œæ•´çš„çº¹ç†æ˜ å°„ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨åŸºäºæ‰©æ•£çš„å¡«å……æ¨¡å—æ¥å¡«å……ç¼ºå¤±åŒºåŸŸï¼Œç„¶åèåˆæœºåˆ¶å°†è¿™äº›ç»“æœç»„åˆæˆç»Ÿä¸€çš„å®Œæ•´çº¹ç†æ˜ å°„ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SMPL-GPTextureå¯ä»¥ç”Ÿæˆä¸ç”¨æˆ·æç¤ºå¯¹é½çš„é«˜åˆ†è¾¨ç‡çº¹ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13378v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSMPL-GPTextureçš„æ–°æµç¨‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡çš„äººåƒçº¹ç†ã€‚è¯¥æµç¨‹é‡‡ç”¨è‡ªç„¶è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹äº§ç”Ÿé…å¯¹çš„é«˜åˆ†è¾¨ç‡å‰åå›¾åƒã€‚é€šè¿‡å¯¹ç”Ÿæˆå›¾åƒçš„2Dåˆ°3D SMPLå¯¹é½ï¼Œä»¥åŠå°†è§‚å¯Ÿåˆ°çš„é¢œè‰²ä»è¾“å…¥å›¾åƒæŠ•å½±åˆ°UVç©ºé—´çš„åè½¬æ¸²æŸ“æŠ€æœ¯ï¼Œæœ€ç»ˆç”Ÿæˆå‡†ç¡®å®Œæ•´çš„çº¹ç†æ˜ å°„ã€‚è¯¥æµç¨‹è§£å†³äº†ç°å®ä¸–ç•Œå›¾åƒé…å¯¹å›°éš¾åŠæ·±ç”Ÿæˆæ¨¡å‹åœ¨æ¨æ–­æœªå¯è§åŒºåŸŸæ—¶äº§ç”Ÿçš„ä¼ªåƒé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SMPL-GPTextureåˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆé«˜è´¨é‡çš„äººåƒçº¹ç†ã€‚</li>
<li>é‡‡ç”¨å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹äº§ç”Ÿé…å¯¹çš„é«˜åˆ†è¾¨ç‡å‰åå›¾åƒã€‚</li>
<li>é€šè¿‡2D-to-3D SMPLå¯¹é½ï¼Œå®ç°å›¾åƒåƒç´ ä¸3Dæ¨¡å‹çš„UVåæ ‡å¯¹åº”ã€‚</li>
<li>é‡‡ç”¨åè½¬æ¸²æŸ“æŠ€æœ¯å°†è§‚å¯Ÿåˆ°çš„é¢œè‰²ä»è¾“å…¥å›¾åƒæŠ•å½±åˆ°UVç©ºé—´ï¼Œç”Ÿæˆå‡†ç¡®å®Œæ•´çš„çº¹ç†æ˜ å°„ã€‚</li>
<li>æµç¨‹è§£å†³äº†ç°å®ä¸–ç•Œå›¾åƒé…å¯¹å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>è§£å†³æ·±ç”Ÿæˆæ¨¡å‹åœ¨æ¨æ–­æœªå¯è§åŒºåŸŸï¼ˆå¦‚äººèƒŒï¼‰æ—¶äº§ç”Ÿçš„ä¼ªåƒã€ç»“æ„ä¸ä¸€è‡´æˆ–ç»†èŠ‚ä¸¢å¤±é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff4651ae7cc54cb8f2a55ecd57e1cbdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5eeeae805e23961457d9edf1a8c44090.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d18fbdd861999299a0f003280ac1f670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a98d7ee87b43b87c2aa2abdf633002.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7df3a128be5e65437aa7944bbfff542e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="WaterFlow-Learning-Fast-Robust-Watermarks-using-Stable-Diffusion"><a href="#WaterFlow-Learning-Fast-Robust-Watermarks-using-Stable-Diffusion" class="headerlink" title="WaterFlow: Learning Fast &amp; Robust Watermarks using Stable Diffusion"></a>WaterFlow: Learning Fast &amp; Robust Watermarks using Stable Diffusion</h2><p><strong>Authors:Vinay Shukla, Prachee Sharma, Ryan Rossi, Sungchul Kim, Tong Yu, Aditya Grover</strong></p>
<p>The ability to embed watermarks in images is a fundamental problem of interest for computer vision, and is exacerbated by the rapid rise of generated imagery in recent times. Current state-of-the-art techniques suffer from computational and statistical challenges such as the slow execution speed for practical deployments. In addition, other works trade off fast watermarking speeds but suffer greatly in their robustness or perceptual quality. In this work, we propose WaterFlow (WF), a fast and extremely robust approach for high fidelity visual watermarking based on a learned latent-dependent watermark. Our approach utilizes a pretrained latent diffusion model to encode an arbitrary image into a latent space and produces a learned watermark that is then planted into the Fourier Domain of the latent. The transformation is specified via invertible flow layers that enhance the expressivity of the latent space of the pre-trained model to better preserve image quality while permitting robust and tractable detection. Most notably, WaterFlow demonstrates state-of-the-art performance on general robustness and is the first method capable of effectively defending against difficult combination attacks. We validate our findings on three widely used real and generated datasets: MS-COCO, DiffusionDB, and WikiArt. </p>
<blockquote>
<p>å°†å›¾ç‰‡åµŒå…¥æ°´å°æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªåŸºç¡€ä¸”é‡è¦çš„é—®é¢˜ï¼Œè¿‘å¹´æ¥éšç€ç”Ÿæˆå›¾åƒæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¿™ä¸€é—®é¢˜å˜å¾—æ„ˆå‘ä¸¥é‡ã€‚å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯é¢ä¸´ç€è®¡ç®—å’Œç»Ÿè®¡æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¦‚åœ¨å®é™…éƒ¨ç½²ä¸­çš„æ‰§è¡Œé€Ÿåº¦è¾ƒæ…¢ã€‚æ­¤å¤–ï¼Œå…¶ä»–æ–¹æ³•è™½ç„¶æ°´å°é€Ÿåº¦å¿«ï¼Œä½†åœ¨é²æ£’æ€§æˆ–æ„ŸçŸ¥è´¨é‡æ–¹é¢å­˜åœ¨å¾ˆå¤§ç¼ºé™·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WaterFlowï¼ˆWFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå­¦ä¹ åˆ°çš„æ½œåœ¨ä¾èµ–æ°´å°çš„å¿«é€Ÿä¸”æå…¶é²æ£’çš„é«˜ä¿çœŸè§†è§‰æ°´å°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å°†ä»»æ„å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼Œå¹¶äº§ç”Ÿå­¦ä¹ çš„æ°´å°ï¼Œç„¶åå°†å…¶æ¤å…¥åˆ°æ½œåœ¨ç©ºé—´çš„å‚…ç«‹å¶åŸŸã€‚é€šè¿‡å¯é€†æµå±‚æŒ‡å®šè½¬æ¢ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåœ¨ç©ºé—´çš„è¡¨ç°åŠ›ï¼Œå¯ä»¥æ›´å¥½åœ°ä¿æŒå›¾åƒè´¨é‡ï¼ŒåŒæ—¶å®ç°é²æ£’å’Œå¯è¿½è¸ªçš„æ£€æµ‹ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒWaterFlowåœ¨ä¸€èˆ¬é²æ£’æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆé˜²èŒƒå¤æ‚ç»„åˆæ”»å‡»çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„çœŸå®å’Œç”Ÿæˆæ•°æ®é›†MS-COCOã€DiffusionDBå’ŒWikiArtä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12354v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿä¸”é«˜åº¦é²æ£’çš„é«˜ä¿çœŸè§†è§‰æ°´å°åµŒå…¥æ–¹æ³•WaterFlowã€‚å®ƒé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†ä»»æ„å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´çš„å‚…ç«‹å¶åŸŸä¸­æ¤å…¥å­¦ä¹ åˆ°çš„æ°´å°æ¥å®ç°é«˜æ•ˆæ°´å°åµŒå…¥ã€‚æ­¤æ–¹æ³•ä½¿ç”¨å¯é€†æµå±‚è¿›è¡Œè½¬æ¢ï¼Œæé«˜äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåœ¨ç©ºé—´è¡¨è¾¾èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°ä¿æŒäº†å›¾åƒè´¨é‡å¹¶å®ç°äº†é²æ£’å’Œå¯è¿½è¸ªçš„æ£€æµ‹ã€‚WaterFlowåœ¨ä¸€èˆ¬é²æ£’æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯é¦–ä¸ªèƒ½å¤Ÿæœ‰æ•ˆé˜²å¾¡å¤æ‚ç»„åˆæ”»å‡»çš„æ–¹æ³•ã€‚åœ¨MS-COCOã€DiffusionDBå’ŒWikiArtä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„çœŸå®å’Œç”Ÿæˆæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WaterFlowæ˜¯ä¸€ç§åŸºäºé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿä¸”é«˜åº¦é²æ£’çš„é«˜ä¿çœŸè§†è§‰æ°´å°åµŒå…¥æ–¹æ³•ã€‚</li>
<li>WaterFlowåˆ©ç”¨æ‰©æ•£æ¨¡å‹å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´çš„å‚…ç«‹å¶åŸŸä¸­æ¤å…¥å­¦ä¹ åˆ°çš„æ°´å°ã€‚</li>
<li>WaterFlowä½¿ç”¨å¯é€†æµå±‚è¿›è¡Œè½¬æ¢ï¼Œæé«˜äº†é¢„è®­ç»ƒæ¨¡å‹çš„æ½œåœ¨ç©ºé—´è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>WaterFlowèƒ½å¤Ÿå¾ˆå¥½åœ°ä¿æŒå›¾åƒè´¨é‡ï¼ŒåŒæ—¶å®ç°é²æ£’å’Œå¯è¿½è¸ªçš„æ£€æµ‹ã€‚</li>
<li>WaterFlowåœ¨ä¸€èˆ¬é²æ£’æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆé˜²å¾¡å¤æ‚ç»„åˆæ”»å‡»ã€‚</li>
<li>WaterFlowåœ¨MS-COCOã€DiffusionDBå’ŒWikiArtä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4fc4e58e78154eceb96f551103f3c8c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1fe7b7107e4f0164daef59ff6269d50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b9944f9830c787d4b098b96a4294ad0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a390dda013adc4a3b98ee4627b012d53.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning"><a href="#PT-Mark-Invisible-Watermarking-for-Text-to-image-Diffusion-Models-via-Semantic-aware-Pivotal-Tuning" class="headerlink" title="PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning"></a>PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via   Semantic-aware Pivotal Tuning</h2><p><strong>Authors:Yaopeng Wang, Huiyu Xu, Zhibo Wang, Jiacheng Du, Zhichao Li, Yiming Li, Qiu Wang, Kui Ren</strong></p>
<p>Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency. </p>
<blockquote>
<p>é’ˆå¯¹æ‰©æ•£å›¾åƒçš„æ°´å°æŠ€æœ¯å·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¹¿æ³›åº”ç”¨å’Œå¯¹ç‰ˆæƒä¿æŠ¤çš„ä¸æ–­å¢é•¿çš„éœ€æ±‚ã€‚æœ€è¿‘ï¼Œå…ˆè¿›çš„æ°´å°æŠ€æœ¯ï¼Œå¦‚Tree Ringï¼Œé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å°†å¯è¿½è¸ªçš„æ¨¡å¼ï¼ˆä¾‹å¦‚åœ†ç¯ï¼‰åµŒå…¥æ½œåœ¨åˆ†å¸ƒæ¥é›†æˆæ°´å°ã€‚è¿™äº›æ–¹æ³•ç”±äºæ°´å°å¯¼è‡´çš„ä¸å¯é¿å…çš„åˆ†å‘åç§»ï¼Œç ´åäº†ç”Ÿæˆå›¾åƒçš„åŸè¯­ä¹‰ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­—è‰ºæœ¯åˆ›ä½œä¸­ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥çš„æ¢è½´è°ƒæ•´æ°´å°ï¼ˆPT-Markï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä¸å¯è§æ°´å°æ–¹æ³•ï¼Œæ—¢èƒ½ä¿ç•™æ‰©æ•£å›¾åƒè¯­ä¹‰ï¼Œåˆèƒ½è¿½è¸ªæ°´å°ã€‚PT-Marké€šè¿‡åœ¨æ•´ä¸ªæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­é€æ¸è°ƒæ•´ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹ï¼ˆæ¢è½´ï¼‰è½¨è¿¹çš„å¯¹é½ï¼ŒåŒæ—¶ä¿æŒå¯è¿½è¸ªçš„æ°´å°ï¼Œä»è€Œä¿ç•™äº†æ°´å°å›¾åƒçš„åŸå§‹è¯­ä¹‰ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—æ¯ä¸ªæ‰©æ•£å»å™ªæ­¥éª¤ä¸­æ°´å°çš„æ˜¾è‘—åŒºåŸŸä½œä¸ºç©ºé—´å…ˆéªŒï¼Œä»¥è¯†åˆ«å¯ä»¥åœ¨ä¸ç ´åæ°´å°æ¨¡å¼çš„æƒ…å†µä¸‹å¯¹é½çš„åŒºåŸŸã€‚åœ¨è¯¥åŒºåŸŸçš„æŒ‡å¯¼ä¸‹ï¼Œç„¶åæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„æ¢è½´è°ƒæ•´åˆ†æ”¯ï¼Œä»¥ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œåœ¨å¯¹é½è¯­ä¹‰çš„åŒæ—¶ä¿ç•™æ°´å°ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPT-Markèƒ½å¤Ÿåœ¨é›†æˆç¨³å¥æ°´å°çš„åŒæ—¶ä¿ç•™æ‰©æ•£å›¾åƒçš„åŸå§‹è¯­ä¹‰ã€‚ä¸æœ€å…ˆè¿›çš„æ°´å°æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒåœ¨è¯­ä¹‰ä¿ç•™æ€§èƒ½ï¼ˆå³SSIMã€PSNRå’ŒLPIPSï¼‰æ–¹é¢å®ç°äº†10%çš„æ”¹è¿›ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºå¯¹ç°å®ä¸–ç•Œæ‰°åŠ¨çš„ç›¸å½“ç¨³å¥æ€§ï¼Œå¹¶ä¸”æ•ˆç‡æé«˜äº†å››å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10853v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†æ‰©æ•£å›¾åƒæ°´å°æŠ€æœ¯çš„æ–°è¿›å±•ã€‚ç”±äºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨åŠå…¶ç‰ˆæƒä¿æŠ¤éœ€æ±‚å¢åŠ ï¼Œæ°´å°æŠ€æœ¯å—åˆ°å…³æ³¨ã€‚æœ€æ–°æ–¹æ³•å¦‚Tree Ringé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­åµŒå…¥å¯è¿½è¸ªæ¨¡å¼ï¼ˆå¦‚åœ†ç¯ï¼‰è¿›è¡Œæ°´å°é›†æˆï¼Œä½†ä¼šç ´åç”Ÿæˆå›¾åƒçš„åŸè¯­ä¹‰ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹ä¸å¯è§æ°´å°æŠ€æœ¯â€”â€”è¯­ä¹‰æ„ŸçŸ¥å…³é”®è°ƒæ•´æ°´å°ï¼ˆPT-Markï¼‰ï¼Œè¯¥æŠ€æœ¯æ—¢ä¿ç•™æ‰©æ•£å›¾åƒè¯­ä¹‰åˆå…·å¤‡æ°´å°å¯è¿½è¸ªæ€§ã€‚PT-Marké€šè¿‡é€æ­¥è°ƒæ•´ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹ï¼ˆå…³é”®ï¼‰è½¨è¿¹å¯¹é½ï¼ŒåŒæ—¶åœ¨æ•´ä¸ªæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­ä¿æŒå¯è¿½è¸ªæ°´å°ï¼Œä¿æŠ¤æ°´å°å›¾åƒçš„åŸè¯­ä¹‰ã€‚ä¸ºè¾¾æˆæ­¤ç›®æ ‡ï¼Œé¦–å…ˆåœ¨æ¯ä¸ªæ‰©æ•£å»å™ªæ­¥éª¤è®¡ç®—æ°´å°çš„æ˜¾è‘—åŒºåŸŸä½œä¸ºç©ºé—´å…ˆéªŒï¼Œä»¥è¯†åˆ«å¯å¯¹é½åŒºåŸŸè€Œä¸å¹²æ‰°æ°´å°å›¾æ¡ˆã€‚å—è¯¥åŒºåŸŸå¼•å¯¼ï¼Œå¼•å…¥é¢å¤–å…³é”®è°ƒæ•´åˆ†æ”¯ä¼˜åŒ–æ–‡æœ¬åµŒå…¥ï¼Œåœ¨å¯¹é½è¯­ä¹‰çš„åŒæ—¶ä¿ç•™æ°´å°ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒPT-Markåœ¨é›†æˆç¨³å¥æ°´å°çš„åŒæ—¶ï¼Œä¿ç•™æ‰©æ•£å›¾åƒçš„åŸè¯­ä¹‰ï¼Œè¾ƒå…ˆè¿›æ°´å°æ–¹æ³•åœ¨è¯­ä¹‰ä¿ç•™æ€§èƒ½ä¸Šæå‡10%ï¼ŒåŒæ—¶å±•ç°å¯¹ç°å®ä¸–ç•Œæ‰°åŠ¨çš„ç¨³å¥æ€§å¹¶æé«˜æ•ˆç‡å››å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ°´å°æŠ€æœ¯å¯¹äºä¿æŠ¤æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç‰ˆæƒè‡³å…³é‡è¦ã€‚</li>
<li>æœ€æ–°æ–¹æ³•å¦‚Tree Ringè™½ç„¶èƒ½å¤Ÿé›†æˆæ°´å°ï¼Œä½†ä¼šç ´åç”Ÿæˆå›¾åƒçš„åŸè¯­ä¹‰ã€‚</li>
<li>PT-Markæ˜¯ä¸€ç§æ–°å‹ä¸å¯è§æ°´å°æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯ç ´åå›¾åƒåŸè¯­ä¹‰çš„é—®é¢˜ã€‚</li>
<li>PT-Marké€šè¿‡é€æ­¥å¯¹é½ç”Ÿæˆè½¨è¿¹ä¸åŸå§‹è½¨è¿¹ï¼ŒåŒæ—¶ä¿æŒæ•´ä¸ªæ‰©æ•£å»å™ªè¿‡ç¨‹ä¸­çš„æ°´å°å¯è¿½è¸ªæ€§ï¼Œä¿æŠ¤å›¾åƒçš„åŸè¯­ä¹‰ã€‚</li>
<li>PT-Markå¼•å…¥ç©ºé—´å…ˆéªŒå’Œå…³é”®è°ƒæ•´åˆ†æ”¯æ¥å®ç°è¯­ä¹‰ä¿ç•™å’Œæ°´å°çš„é›†æˆã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºPT-Markåœ¨è¯­ä¹‰ä¿ç•™ã€ç¨³å¥æ€§å’Œæ•ˆç‡æ–¹é¢è¾ƒå…ˆè¿›æ–¹æ³•æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f51e42acf61f107bfa076f3a3985ff3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50b0658721b6a93677247e04d42ae815.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-658c04a0563e4a55e8caaecbde0a6840.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ecd0d79a7cc989cd7cf043ca2a508c4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-FranÃ§ois Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: <a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºè®®ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¸¸è§„å›¾åƒä½œä¸º3Dæ¸²æŸ“å™¨çš„å…‰æºã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯Splatså¯¹3Dç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å®ç°ç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶åº”ç”¨äºç…§æ˜è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯æ–¹é¢äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æˆæœã€‚ä¸ºäº†å¯¹å›¾åƒä½œä¸ºå…‰æºè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ–°å‹æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–ç…§æ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨3Dæ¸²æŸ“ä¸­ä½¿ç”¨å¸¸è§„å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬çš„ä¸¤æ­¥è¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯Splatså¯¹3Dç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–ç…§æ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶ç”¨äºç…§äº®è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯æ–¹é¢çš„åº”ç”¨æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ä¸ºäº†æ–¹ä¾¿å°†å›¾åƒä½œä¸ºå…‰æºè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ–°æ•°æ®é›†å’Œç°æœ‰æ–‡çŒ®æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLightæ–¹æ³•èƒ½å¤Ÿä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–ç…§æ˜ã€‚</li>
<li>HDRé«˜æ–¯Splatsé¦–æ¬¡è¢«ç”¨ä½œå…‰æºè¡¨ç¤ºï¼Œç”¨äº3Dæ¸²æŸ“ä¸­çš„å¸¸è§„å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µå¢å¼ºå›¾åƒåŠ¨æ€èŒƒå›´ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨é«˜æ–¯Splatsè¿›è¡Œ3Dç…§æ˜å»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–ç…§æ˜ã€‚</li>
<li>æ–¹æ³•åœ¨HDRä¼°è®¡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶æˆåŠŸåº”ç”¨äºç…§äº®è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºçš„æ•ˆæœï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ–°å‹æ•°æ®é›†å’Œç°æœ‰æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0acd540684c76bb65da870b875d2c04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a587eee72f338f28224d08dea7b8d954.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faad0dc0950fb458ed849d09cbe96dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b148832e6da27501a698c1717c440602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95ce0b37073da87d139eff1cacc105cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606d27e2d875c7ceeb31eb0cba8ec175.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion harmoniously embeds an input reference image into arbitrary scenes described by the text prompts, producing illusion images exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion featuresâ€™ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing deep fusion of the reference structural information and the textual semantic information in the diffusion model latent space. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning process, all while substantially outperforming related text-guided I2I methods in image generation quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. Our project is publically available at \href{<a target="_blank" rel="noopener" href="https://xianggao1102.github.io/PTDiffusion_webpage/%7D%7Bthis">https://xianggao1102.github.io/PTDiffusion_webpage/}{this</a> web page}. </p>
<blockquote>
<p>å…‰å­¦é”™è§‰éšè—å›¾åƒæ˜¯ä¸€ç§æœ‰è¶£çš„è§†è§‰æ„ŸçŸ¥ç°è±¡ï¼Œå›¾åƒå·§å¦™åœ°èå…¥å¦ä¸€å¹…å›¾ç‰‡ä¸­ï¼Œä½¿è§‚ä¼—æ— æ³•ç«‹å³è¯†åˆ«ã€‚æˆ‘ä»¬åŸºäºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒã€æ–‡æœ¬å¼•å¯¼çš„å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æ¡†æ¶ï¼Œåä¸ºâ€œé˜¶æ®µè½¬ç§»æ‰©æ•£æ¨¡å‹â€ï¼ˆPTDiffusionï¼‰ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯ã€‚PTDiffusionå’Œè°åœ°å°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥ç”±æ–‡æœ¬æç¤ºæè¿°çš„ä»»æ„åœºæ™¯ä¸­ï¼Œç”Ÿæˆæ˜¾ç¤ºå‚è€ƒå›¾åƒéšè—è§†è§‰çº¿ç´¢çš„é”™è§‰å›¾åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œå®ƒåŠ¨æ€ä¸”æ¸è¿›åœ°ç§»æ¤æ‰©æ•£ç‰¹å¾åœ¨é™å™ªè¿‡ç¨‹ä¸­çš„ç›¸ä½è°±ï¼Œé‡å»ºå°†å‚è€ƒå›¾åƒé‡‡æ ·åˆ°ç”Ÿæˆçš„é”™è§‰å›¾åƒä¸­ï¼Œå®ç°åœ¨æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°çµæ´»æ§åˆ¶éšè—å†…å®¹å¯è¾¨è¯†åº¦çš„ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»•è¿‡äº†ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ï¼ŒåŒæ—¶åœ¨å›¾åƒç”Ÿæˆè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦æ–¹é¢å¤§å¤§ä¼˜äºç›¸å…³çš„æ–‡æœ¬å¼•å¯¼I2Iæ–¹æ³•ï¼Œä¸ºé”™è§‰å›¾åƒåˆæˆæä¾›äº†å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡å®éªŒè¯æ˜ã€‚æˆ‘ä»¬çš„é¡¹ç›®å·²åœ¨ä»¥ä¸‹ç½‘é¡µå…¬å¼€ï¼š[<a target="_blank" rel="noopener" href="https://xianggao110/">https://xianggao110</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v5">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ–°å‹è®­ç»ƒå…æ–‡æœ¬å¼•å¯¼å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æ¡†æ¶â€”â€”PTDiffusionï¼ˆç›¸ä½è½¬ç§»æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯å›¾åƒã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥åˆ°æ–‡æœ¬æè¿°çš„åœºæ™¯ä¸­ï¼Œç”Ÿæˆå¸¦æœ‰å‚è€ƒå›¾åƒéšè—è§†è§‰çº¿ç´¢çš„é”™è§‰å›¾åƒã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯å³æ’å³ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åŠ¨æ€åœ°é€æ­¥è½¬ç§»å»å™ªè¿‡ç¨‹ä¸­çš„æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ï¼Œå®ç°åœ¨æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­çš„å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„æ·±åº¦èåˆã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°çµæ´»æ§åˆ¶éšè—å†…å®¹çš„å¯è¾¨è¯†ç¨‹åº¦ã€‚è¯¥æ–¹æ³•æ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ï¼Œåœ¨å›¾åƒç”Ÿæˆè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰å¯è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦ç­‰æ–¹é¢å‡å¤§å¤§ä¼˜äºç›¸å…³æ–‡æœ¬å¼•å¯¼çš„I2Iæ–¹æ³•ï¼Œä¸ºé”™è§‰å›¾åƒåˆæˆæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å¼€å‘äº†ä¸€ç§æ–°å‹çš„æ–‡æœ¬å¼•å¯¼å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¡†æ¶PTDiffusionã€‚</li>
<li>PTDiffusionèƒ½å¤Ÿåœ¨æ–‡æœ¬æè¿°çš„åœºæ™¯ä¸­åµŒå…¥å‚è€ƒå›¾åƒï¼Œç”Ÿæˆå¸¦æœ‰éšè—è§†è§‰çº¿ç´¢çš„é”™è§‰å›¾åƒã€‚</li>
<li>PTDiffusionçš„æ ¸å¿ƒæœºåˆ¶æ˜¯ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å®ç°äº†åœ¨æ‰©æ•£æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­çš„å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„æ·±åº¦èåˆã€‚</li>
<li>æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°å¯¹éšè—å†…å®¹å¯è¾¨è¯†åº¦çš„çµæ´»æ§åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ã€‚</li>
<li>PTDiffusionåœ¨å›¾åƒç”Ÿæˆè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰å¯è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–ç›¸å…³æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-63c37dfa16485384bbd37bb46845d839.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d546ccc0900e834495405121f3861e10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6a32b6a8b666e971777898d004c84fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2e7ca8dc8a3c6bf59c59e13a1e6479a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1414e64ebf8d7c7960f3cd0f6426a083.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EditSplat-Multi-View-Fusion-and-Attention-Guided-Optimization-for-View-Consistent-3D-Scene-Editing-with-3D-Gaussian-Splatting"><a href="#EditSplat-Multi-View-Fusion-and-Attention-Guided-Optimization-for-View-Consistent-3D-Scene-Editing-with-3D-Gaussian-Splatting" class="headerlink" title="EditSplat: Multi-View Fusion and Attention-Guided Optimization for   View-Consistent 3D Scene Editing with 3D Gaussian Splatting"></a>EditSplat: Multi-View Fusion and Attention-Guided Optimization for   View-Consistent 3D Scene Editing with 3D Gaussian Splatting</h2><p><strong>Authors:Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Sangheon Shin, Sangmin Kim, Sangpil Kim</strong></p>
<p>Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR&#x2F;VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel text-driven 3D scene editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric structure inherent to 3DGS. Additionally, our AGT utilizes the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local editing. Through extensive qualitative and quantitative evaluations, EditSplat achieves state-of-the-art performance, establishing a new benchmark for text-driven 3D scene editing. </p>
<blockquote>
<p>æœ€è¿‘çš„3Dç¼–è¾‘æŠ€æœ¯è¿›å±•çªå‡ºäº†æ–‡æœ¬é©±åŠ¨æ–¹æ³•åœ¨å®æ—¶ã€ç”¨æˆ·å‹å¥½çš„AR&#x2F;VRåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºäºŒç»´æ‰©æ•£æ¨¡å‹ï¼Œè€Œæ²¡æœ‰å……åˆ†è€ƒè™‘åˆ°å¤šè§†è§’ä¿¡æ¯ï¼Œå¯¼è‡´å¤šè§†è§’ä¸ä¸€è‡´ã€‚è™½ç„¶ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰æŠ€æœ¯æ˜¾è‘—æé«˜äº†æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦ï¼Œä½†å…¶ä¸‰ç»´ç¼–è¾‘è¿‡ç¨‹åœ¨ä¼˜åŒ–æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„é«˜æ–¯ä¿ç•™äº†è¿‡å¤šçš„æºä¿¡æ¯ï¼Œé˜»ç¢äº†ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†EditSplatï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬é©±åŠ¨ä¸‰ç»´åœºæ™¯ç¼–è¾‘æ¡†æ¶ï¼Œå®ƒé›†æˆäº†å¤šè§†è§’èåˆæŒ‡å¯¼ï¼ˆMFGï¼‰å’Œæ³¨æ„åŠ›å¼•å¯¼ä¿®å‰ªï¼ˆAGTï¼‰ã€‚æˆ‘ä»¬çš„MFGé€šè¿‡èå…¥æ‰©æ•£è¿‡ç¨‹ä¸­çš„å…³é”®å¤šè§†è§’ä¿¡æ¯ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„éåˆ†ç±»å™¨æŒ‡å¯¼ä»¥åŠ3DGSæ‰€å›ºæœ‰çš„å‡ ä½•ç»“æ„ï¼Œç¡®ä¿å¤šè§†è§’çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„AGTåˆ©ç”¨3DGSçš„æ˜¾å¼è¡¨ç¤ºè¿›è¡Œæœ‰é€‰æ‹©æ€§åœ°ä¿®å‰ªå’Œä¼˜åŒ–ä¸‰ç»´é«˜æ–¯ï¼Œæé«˜ä¼˜åŒ–æ•ˆç‡ï¼Œå®ç°ç²¾ç¡®ä¸”è¯­ä¹‰ä¸°å¯Œçš„å±€éƒ¨ç¼–è¾‘ã€‚é€šè¿‡å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡è¯„ä¼°ï¼ŒEditSplatè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºæ–‡æœ¬é©±åŠ¨çš„ä¸‰ç»´åœºæ™¯ç¼–è¾‘å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11520v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬é©±åŠ¨çš„3Dåœºæ™¯ç¼–è¾‘æ¡†æ¶EditSplatï¼Œè¯¥æ¡†æ¶é›†æˆäº†å¤šè§†è§’èåˆå¼•å¯¼ï¼ˆMFGï¼‰å’Œæ³¨æ„åŠ›å¼•å¯¼ä¿®å‰ªï¼ˆAGTï¼‰ã€‚EditSplatè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å®æ—¶ç¼–è¾‘è¿‡ç¨‹ä¸­çš„å¤šè§†è§’ä¸ä¸€è‡´å’Œä¼˜åŒ–æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæé«˜äº†æ¸²æŸ“è´¨é‡å’Œé€Ÿåº¦ã€‚é€šè¿‡å¼•å…¥MFGå’ŒAGTï¼ŒEditSplatå®ç°äº†å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå¹¶æé«˜äº†ä¼˜åŒ–æ•ˆç‡ï¼Œä¸ºç²¾ç¡®ã€è¯­ä¹‰ä¸°å¯Œçš„å±€éƒ¨ç¼–è¾‘æä¾›äº†å¯èƒ½ã€‚ç»è¿‡å®šæ€§å’Œå®šé‡è¯„ä¼°ï¼ŒEditSplatè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œä¸ºæ–‡æœ¬é©±åŠ¨çš„3Dåœºæ™¯ç¼–è¾‘æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬çš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>å½“å‰3Dç¼–è¾‘æ–¹æ³•å­˜åœ¨å¤šè§†è§’ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºŒç»´æ‰©æ•£æ¨¡å‹ï¼Œå¿½ç•¥äº†å¤šè§†è§’ä¿¡æ¯çš„é‡è¦æ€§ã€‚</li>
<li>EditSplatæ¡†æ¶é›†æˆäº†å¤šè§†è§’èåˆå¼•å¯¼ï¼ˆMFGï¼‰å’Œæ³¨æ„åŠ›å¼•å¯¼ä¿®å‰ªï¼ˆAGTï¼‰ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>MFGé€šè¿‡å¼•å…¥æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹çš„åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ä»¥åŠ3Då‡ ä½•ç»“æ„ï¼Œç¡®ä¿å¤šè§†è§’ä¸€è‡´æ€§ã€‚</li>
<li>AGTåˆ©ç”¨æ˜¾å¼çš„3DGSè¡¨ç¤ºé€‰æ‹©æ€§åœ°ä¿®å‰ªå’Œä¼˜åŒ–ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒï¼Œæé«˜äº†ä¼˜åŒ–æ•ˆç‡å’Œç²¾ç¡®æ€§ã€‚</li>
<li>EditSplatæ¡†æ¶å®ç°äº†ç²¾ç¡®ã€è¯­ä¹‰ä¸°å¯Œçš„å±€éƒ¨ç¼–è¾‘ï¼Œå¹¶è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b396433bd0c45ba8bb75b1535ddc456.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-701ec12424a16c05b5750c70ac39a0d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1c10e5a544c37f61ceca619f09777a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b1370e32563de59b2407f6ef45fd0a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PriorDiffusion-Leverage-Language-Prior-in-Diffusion-Models-for-Monocular-Depth-Estimation"><a href="#PriorDiffusion-Leverage-Language-Prior-in-Diffusion-Models-for-Monocular-Depth-Estimation" class="headerlink" title="PriorDiffusion: Leverage Language Prior in Diffusion Models for   Monocular Depth Estimation"></a>PriorDiffusion: Leverage Language Prior in Diffusion Models for   Monocular Depth Estimation</h2><p><strong>Authors:Ziyao Zeng, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong</strong></p>
<p>Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisance. We argue that language prior can enhance monocular depth estimation by leveraging the inductive bias learned during the text-to-image pre-training of diffusion models. The ability of these models to generate images that align with text indicates that they have learned the spatial relationships, size, and shape of specified objects, which can be applied to improve depth estimation. Thus, we propose PriorDiffusion, using a pre-trained text-to-image diffusion model that takes both images and corresponding text descriptions to infer affine-invariant depth through a denoising process. We also show that language prior enhances the modelâ€™s perception of specific regions of images that users care about and describe. Simultaneously, language prior acts as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. By training on HyperSim and Virtual KITTI, we achieve faster training convergence, fewer inference diffusion steps, and state-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet. Code will be released upon acceptance. </p>
<blockquote>
<p>ä¼ ç»Ÿå•ç›®æ·±åº¦ä¼°è®¡å­˜åœ¨å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œè§†è§‰å¹²æ‰°é—®é¢˜ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒé¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦åˆ°çš„å½’çº³åè§ï¼Œè¯­è¨€å…ˆéªŒå¯ä»¥å¢å¼ºå•ç›®æ·±åº¦ä¼°è®¡ã€‚è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸æ–‡æœ¬ç›¸ç¬¦çš„å›¾åƒï¼Œè¡¨æ˜å®ƒä»¬å·²ç»å­¦ä¼šäº†æŒ‡å®šå¯¹è±¡çš„ç©ºé—´å…³ç³»ã€å¤§å°å’Œå½¢çŠ¶ï¼Œè¿™å¯ä»¥ç”¨äºæ”¹è¿›æ·±åº¦ä¼°è®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PriorDiffusionï¼Œå®ƒä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡å»å™ªè¿‡ç¨‹æ¨æ–­ä»¿å°„ä¸å˜æ·±åº¦ï¼Œè¯¥è¿‡ç¨‹æ—¢æ¶‰åŠå›¾åƒåˆæ¶‰åŠç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œè¯­è¨€å…ˆéªŒæé«˜äº†æ¨¡å‹å¯¹ç”¨æˆ·å…³å¿ƒå’Œæè¿°çš„å›¾åƒç‰¹å®šåŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¯­è¨€å…ˆéªŒä½œä¸ºçº¦æŸï¼ŒåŠ é€Ÿäº†è®­ç»ƒå’Œæ¨ç†æ‰©æ•£è½¨è¿¹çš„æ”¶æ•›ã€‚é€šè¿‡åœ¨HyperSimå’ŒVirtual KITTIä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å®ç°äº†æ›´å¿«çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€æ›´å°‘çš„æ¨ç†æ‰©æ•£æ­¥éª¤ï¼Œä»¥åŠåœ¨NYUv2ã€KITTIã€ETH3Då’ŒScanNetä¸Šçš„æœ€å…ˆè¿›çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16750v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨æ–‡æœ¬å…ˆéªŒä¿¡æ¯å¢å¼ºå•ç›®æ·±åº¦ä¼°è®¡çš„æ–¹æ³•ï¼Œå€ŸåŠ©æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒé¢„è®­ç»ƒä¸­çš„å½’çº³åç½®ã€‚æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆä¸æ–‡æœ¬å¯¹åº”çš„å›¾åƒæ—¶ï¼Œå·²å­¦ä¹ ç‰©ä½“ç©ºé—´å…³ç³»ã€å¤§å°å’Œå½¢çŠ¶ï¼Œå¯åº”ç”¨äºæ·±åº¦ä¼°è®¡ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºPriorDiffusionï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆå›¾åƒå’Œç›¸åº”æ–‡æœ¬æè¿°ï¼Œé€šè¿‡å»å™ªè¿‡ç¨‹æ¨æ–­ä»¿å°„ä¸å˜æ·±åº¦ã€‚åŒæ—¶ï¼Œæ–‡æœ¬å…ˆéªŒä¿¡æ¯æé«˜äº†æ¨¡å‹å¯¹ç”¨æˆ·å…³æ³¨å›¾åƒç‰¹å®šåŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶ä½œä¸ºçº¦æŸåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†æ‰©æ•£è½¨è¿¹çš„æ”¶æ•›ã€‚åœ¨HyperSimå’ŒVirtual KITTIæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå®ç°æ›´å¿«çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€æ›´å°‘çš„æ¨ç†æ‰©æ•£æ­¥éª¤ï¼Œä»¥åŠåœ¨NYUv2ã€KITTIã€ETH3Då’ŒScanNetä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å…ˆéªŒä¿¡æ¯å¯å¢å¼ºå•ç›®æ·±åº¦ä¼°è®¡ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å½’çº³åç½®ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶å·²å­¦ä¹ ç©ºé—´å…³ç³»ã€å¤§å°å’Œå½¢çŠ¶ï¼Œå¯åº”ç”¨äºæ·±åº¦ä¼°è®¡ã€‚</li>
<li>æå‡ºPriorDiffusionæ–¹æ³•ï¼Œç»“åˆå›¾åƒå’Œæ–‡æœ¬æè¿°ï¼Œé€šè¿‡å»å™ªè¿‡ç¨‹æ¨æ–­æ·±åº¦ã€‚</li>
<li>æ–‡æœ¬å…ˆéªŒä¿¡æ¯æé«˜æ¨¡å‹å¯¹å›¾åƒç‰¹å®šåŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>æ–‡æœ¬å…ˆéªŒä¿¡æ¯ä½œä¸ºçº¦æŸï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†æ‰©æ•£è½¨è¿¹çš„æ”¶æ•›ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°å¿«é€Ÿè®­ç»ƒæ”¶æ•›å’Œé›¶æ ·æœ¬æ€§èƒ½é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16750">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c8fd58c260d43aa5f9a63e2b9779cba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afa38f027d3ad569d49e3a163a88a57c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd54206f0062ae7c74bb265a79a88033.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea9a4700fff45fbd729d408bd2defb07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3496d92bd4bde16b549b7cdcb7852a27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0a239f35ddd2e127da0d69c26fc6bca.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="High-Resolution-Frame-Interpolation-with-Patch-based-Cascaded-Diffusion"><a href="#High-Resolution-Frame-Interpolation-with-Patch-based-Cascaded-Diffusion" class="headerlink" title="High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion"></a>High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion</h2><p><strong>Authors:Junhwa Hur, Charles Herrmann, Saurabh Saxena, Janne Kontkanen, Wei-Sheng Lai, Yichang Shih, Michael Rubinstein, David J. Fleet, Deqing Sun</strong></p>
<p>Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion model for high resolution frame interpolation, HIFI, that excels in these scenarios while achieving competitive performance on standard benchmarks. Cascades, which generate a series of images from low to high resolution, can help significantly with large or complex motion that require both global context for a coarse solution and detailed context for high resolution output. However, contrary to prior work on cascaded diffusion models which perform diffusion on increasingly large resolutions, we use a single model that always performs diffusion at the same resolution and upsamples by processing patches of the inputs and the prior solution. At inference time, this drastically reduces memory usage and allows a single model, solving both frame interpolation (base modelâ€™s task) and spatial up-sampling, saving training cost as well. HIFI excels at high-resolution images and complex repeated textures that require global context, achieving comparable or state-of-the-art performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We further introduce a new dataset, LaMoR, that focuses on particularly challenging cases, and HIFI significantly outperforms other baselines. Please visit our project page for video results: <a target="_blank" rel="noopener" href="https://hifi-diffusion.github.io/">https://hifi-diffusion.github.io</a> </p>
<blockquote>
<p>å°½ç®¡è¿‘æœŸæœ‰æ‰€è¿›å±•ï¼Œç°æœ‰çš„å¸§æ’å€¼æ–¹æ³•åœ¨å¤„ç†æé«˜åˆ†è¾¨ç‡è¾“å…¥ä»¥åŠé¢å¯¹é‡å¤çº¹ç†ã€ç»†è–„ç‰©ä½“å’Œå¤§åŠ¨ä½œç­‰å¤æ‚æƒ…å†µæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºè¡¥ä¸çš„çº§è”åƒç´ æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡å¸§æ’å€¼ï¼ˆHIFIï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚çº§è”ç”Ÿæˆä»ä½åˆ°é«˜çš„åˆ†è¾¨ç‡å›¾åƒç³»åˆ—ï¼Œæœ‰åŠ©äºå¤„ç†éœ€è¦å…¨å±€ä¸Šä¸‹æ–‡è¿›è¡Œç²—ç•¥è§£å†³æ–¹æ¡ˆå’Œè¯¦ç»†ä¸Šä¸‹æ–‡ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡è¾“å‡ºçš„å¤§åŠ¨ä½œæˆ–å¤æ‚åŠ¨ä½œã€‚ç„¶è€Œï¼Œä¸å…ˆå‰å…³äºçº§è”æ‰©æ•£æ¨¡å‹çš„å·¥ä½œä¸åŒï¼Œè¿™äº›å·¥ä½œåœ¨è¶Šæ¥è¶Šé«˜çš„åˆ†è¾¨ç‡ä¸Šæ‰§è¡Œæ‰©æ•£ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå§‹ç»ˆåœ¨åŒä¸€åˆ†è¾¨ç‡ä¸Šæ‰§è¡Œæ‰©æ•£çš„å•ä¸€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å¤„ç†è¾“å…¥å’Œå…ˆå‰è§£å†³æ–¹æ¡ˆçš„è¡¥ä¸æ¥è¿›è¡Œä¸Šé‡‡æ ·ã€‚åœ¨æ¨ç†æ—¶é—´ï¼Œè¿™å¤§å¤§é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œå¹¶å…è®¸ä¸€ä¸ªå•ä¸€æ¨¡å‹åŒæ—¶è§£å†³å¸§æ’å€¼ï¼ˆåŸºç¡€æ¨¡å‹çš„ä»»åŠ¡ï¼‰å’Œç©ºé—´ä¸Šé‡‡æ ·ï¼Œä»è€ŒèŠ‚çœäº†è®­ç»ƒæˆæœ¬ã€‚HIFIåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚é‡å¤çº¹ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™äº›å›¾åƒéœ€è¦å…¨å±€ä¸Šä¸‹æ–‡ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ï¼ˆVimeoã€Xiphã€X-Testå’ŒSEPE-8Kï¼‰ä¸Šè¾¾åˆ°äº†ç›¸å½“æˆ–æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†LaMoRï¼Œè¯¥æ•°æ®é›†ä¸“æ³¨äºç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„æƒ…å†µï¼ŒHIFIæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚æœ‰å…³è§†é¢‘ç»“æœçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://hifi-diffusion.github.io./">https://hifi-diffusion.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11838v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://hifi-diffusion.github.io/">https://hifi-diffusion.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºç°æœ‰çš„å¸§æ’å€¼æ–¹æ³•åœ¨å¤„ç†æé«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é‡å¤çº¹ç†ã€è–„å¯¹è±¡å’Œå¤§è¿åŠ¨ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè¡¥ä¸çš„çº§è”åƒç´ æ‰©æ•£æ¨¡å‹â€”â€”HIFIã€‚å®ƒé€šè¿‡çº§è”ç”Ÿæˆä¸€ç³»åˆ—å›¾åƒï¼Œä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡ï¼Œåœ¨å¤„ç†å¤§æˆ–å¤æ‚çš„è¿åŠ¨æ—¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä¸åŒäºå…ˆå‰çš„çº§è”æ‰©æ•£æ¨¡å‹ï¼ŒHIFIå§‹ç»ˆåœ¨ç›¸åŒåˆ†è¾¨ç‡ä¸‹è¿›è¡Œæ‰©æ•£ï¼Œå¹¶é€šè¿‡å¤„ç†è¾“å…¥å’Œå…ˆå‰è§£å†³æ–¹æ¡ˆçš„è¡¥ä¸æ¥è¿›è¡Œä¸Šé‡‡æ ·ã€‚åœ¨æ¨æ–­æ—¶ï¼Œè¿™å¤§å¤§é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œå¹¶å…è®¸ä¸€ä¸ªå•ä¸€æ¨¡å‹åŒæ—¶è§£å†³å¸§æ’å€¼ï¼ˆåŸºç¡€æ¨¡å‹çš„ä»»åŠ¡ï¼‰å’Œç©ºé—´ä¸Šé‡‡æ ·ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚HIFIåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚é‡å¤çº¹ç†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Vimeoã€Xiphã€X-Testå’ŒSEPE-8Kï¼‰ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šç°æœ‰æœ€ä½³æ°´å¹³ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°æ•°æ®é›†LaMoRï¼Œä¸“æ³¨äºç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹ï¼ŒHIFIåœ¨æ­¤æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¸§æ’å€¼æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨è¾“å…¥æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚é‡å¤çº¹ç†ã€è–„å¯¹è±¡å’Œå¤§è¿åŠ¨ã€‚</li>
<li>å¼•å…¥çš„HIFIæ¨¡å‹é‡‡ç”¨åŸºäºè¡¥ä¸çš„çº§è”åƒç´ æ‰©æ•£æ–¹æ³•ï¼Œé’ˆå¯¹è¿™äº›æŒ‘æˆ˜åœºæ™¯è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>HIFIé€šè¿‡çº§è”ç”Ÿæˆä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡çš„å›¾åƒï¼Œå¤„ç†å¤§æˆ–å¤æ‚çš„è¿åŠ¨ã€‚</li>
<li>ä¸å…¶ä»–çº§è”æ‰©æ•£æ¨¡å‹ä¸åŒï¼ŒHIFIåœ¨ç›¸åŒåˆ†è¾¨ç‡ä¸‹è¿›è¡Œæ‰©æ•£ï¼Œå¹¶é€šè¿‡å¤„ç†è¡¥ä¸è¿›è¡Œä¸Šé‡‡æ ·ï¼Œé™ä½å†…å­˜ä½¿ç”¨å¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>HIFIåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒå’Œå¤æ‚é‡å¤çº¹ç†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šæœ€ä½³æ°´å¹³ã€‚</li>
<li>HIFIåœ¨æ–°å¼•å…¥çš„LaMoRæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c97c4785ff4524a762680c0a8f44082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbb10153ba35cecbc07704f375d32f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57307ec0767833b06109d1723e026779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd9ecec7e04a1f5e8997647cd2a205ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b94fd9103b1fe1b152731568c4da8d98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efd2ba8efdeafb195252938972f33cd0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining"><a href="#Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining" class="headerlink" title="Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining"></a>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining</h2><p><strong>Authors:Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, Peng Gao</strong></p>
<p>We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image&#x2F;multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Lumina-mGPTï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œå„ç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆçµæ´»çš„å…‰æ …å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡é‡‡ç”¨å¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒï¼ˆmGPTï¼‰è¿›è¡Œåˆå§‹åŒ–ï¼Œæˆ‘ä»¬è¯æ˜åªä½¿ç”¨è§£ç å™¨çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹å¯ä»¥é€šè¿‡çµæ´»çš„æ¸è¿›å¼ç›‘ç£å¾®è°ƒï¼ˆFP-SFTï¼‰å®ç°ä¸ç°ä»£æ‰©æ•£æ¨¡å‹ç›¸å½“çš„é«˜æ•ˆå›¾åƒç”Ÿæˆæ€§èƒ½ã€‚é…å¤‡æˆ‘ä»¬æå‡ºçš„æ˜ç¡®å›¾åƒè¡¨ç¤ºï¼ˆUniRepï¼‰ï¼ŒLumina-mGPTå¯ä»¥çµæ´»åœ°ç”Ÿæˆå„ç§çºµæ¨ªæ¯”çš„é«˜è´¨é‡å›¾åƒã€‚åœ¨å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å…¨èƒ½ç›‘ç£å¾®è°ƒï¼ˆOmni-SFTï¼‰ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•å°†Lumina-mGPTæå‡ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€å…¨èƒ½æ¨¡å‹ã€‚ç»“æœè¯æ˜è¯¥æ¨¡å‹å…·æœ‰å¤šç§å¤šæ¨¡æ€åŠŸèƒ½ï¼ŒåŒ…æ‹¬è§†è§‰ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ°å›¾åƒ&#x2F;å¤šè§†å›¾ç”Ÿæˆå’Œå¯æ§ç”Ÿæˆï¼‰ã€è§†è§‰è¯†åˆ«ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼‰ä»¥åŠè§†è§‰è¯­è¨€ä»»åŠ¡ï¼ˆå¦‚å¤šè½®è§†è§‰é—®ç­”ï¼‰ï¼Œå±•ç¤ºäº†æŠ€æœ¯æ–¹å‘çš„å¹¿é˜”æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT%E3%80%82">https://github.com/Alpha-VLLM/Lumina-mGPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02657v2">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a></p>
<p><strong>Summary</strong></p>
<p>Lumina-mGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹å®¶æ—ï¼Œæ“…é•¿æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆçµæ´»é€¼çœŸçš„å›¾åƒã€‚é€šè¿‡å¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒï¼ˆmGPTï¼‰åˆå§‹åŒ–ï¼Œè¯æ˜è§£ç å™¨ä»…è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹é€šè¿‡çµæ´»æ¸è¿›ç›‘ç£å¾®è°ƒï¼ˆFP-SFTï¼‰å³å¯å®ç°ä¸ç°ä»£æ‰©æ•£æ¨¡å‹ç›¸å½“çš„é«˜æ•ˆç‡å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚é…å¤‡æå‡ºçš„æ˜ç¡®å›¾åƒè¡¨ç¤ºï¼ˆUniRepï¼‰ï¼ŒLumina-mGPTå¯çµæ´»ç”Ÿæˆå„ç§çºµæ¨ªæ¯”çš„é«˜è´¨é‡å›¾åƒã€‚åœ¨å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æ¢ç´¢äº†ä¸‡èƒ½ç›‘ç£å¾®è°ƒï¼ˆOmni-SFTï¼‰ï¼Œåˆæ­¥å°è¯•å°†Lumina-mGPTæå‡ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€é€šç”¨æ¨¡å‹ã€‚è¯¥æ¨¡å‹å±•ç¤ºå¤šåŠŸèƒ½å¤šæ¨¡æ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒ&#x2F;å¤šè§†å›¾ç”Ÿæˆã€å¯æ§ç”Ÿæˆã€è§†è§‰è¯†åˆ«ä»»åŠ¡å¦‚åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼Œä»¥åŠè§†è§‰è¯­è¨€ä»»åŠ¡å¦‚å¤šè½®è§†è§‰é—®ç­”ç­‰ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Alpha-VLLM/Lumina-mGPTè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumina-mGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œæ“…é•¿æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç”Ÿæˆï¼Œå…·æœ‰çµæ´»æ€§å’Œé«˜è´¨é‡ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒï¼ˆmGPTï¼‰åˆå§‹åŒ–ï¼Œè¯¥æ¨¡å‹å®ç°äº†é«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>Lumina-mGPTé…å¤‡äº†æ˜ç¡®å›¾åƒè¡¨ç¤ºï¼ˆUniRepï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸åŒçºµæ¨ªæ¯”çš„é«˜è´¨é‡å›¾åƒã€‚</li>
<li>æ¨¡å‹é€šè¿‡çµæ´»æ¸è¿›ç›‘ç£å¾®è°ƒï¼ˆFP-SFTï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å±•ç¤ºäº†å¤šåŠŸèƒ½å¤šæ¨¡æ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€è§†è§‰è¯†åˆ«ä»»åŠ¡å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ç­‰ã€‚</li>
<li>Omni-SFTæ˜¯ä¸€ç§å°è¯•å°†Lumina-mGPTæå‡ä¸ºç»Ÿä¸€å¤šæ¨¡æ€é€šç”¨æ¨¡å‹çš„åˆæ­¥æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e31bec0b582eddc8d7764e005d1dfff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a36eeccc0ac22139d093e977d319ad0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d99b6daa18c59136b99c2cd515997c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-318ef0e021430f0d8624dc5990ef9b27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3439256919e1cea1f7203273b38fbdce.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LDM-ISP-Enhancing-Neural-ISP-for-Low-Light-with-Latent-Diffusion-Models"><a href="#LDM-ISP-Enhancing-Neural-ISP-for-Low-Light-with-Latent-Diffusion-Models" class="headerlink" title="LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models"></a>LDM-ISP: Enhancing Neural ISP for Low Light with Latent Diffusion Models</h2><p><strong>Authors:Qiang Wen, Zhefan Rao, Yazhou Xing, Qifeng Chen</strong></p>
<p>Enhancing a low-light noisy RAW image into a well-exposed and clean sRGB image is a significant challenge for modern digital cameras. Prior approaches have difficulties in recovering fine-grained details and true colors of the scene under extremely low-light environments due to near-to-zero SNR. Meanwhile, diffusion models have shown significant progress towards general domain image generation. In this paper, we propose to leverage the pre-trained latent diffusion model to perform the neural ISP for enhancing extremely low-light images. Specifically, to tailor the pre-trained latent diffusion model to operate on the RAW domain, we train a set of lightweight taming modules to inject the RAW information into the diffusion denoising process via modulating the intermediate features of UNet. We further observe different roles of UNet denoising and decoder reconstruction in the latent diffusion model, which inspires us to decompose the low-light image enhancement task into latent-space low-frequency content generation and decoding-phase high-frequency detail maintenance. Through extensive experiments on representative datasets, we demonstrate our simple design not only achieves state-of-the-art performance in quantitative evaluations but also shows significant superiority in visual comparisons over strong baselines, which highlight the effectiveness of powerful generative priors for neural ISP under extremely low-light environments. The project page is available at <a target="_blank" rel="noopener" href="https://csqiangwen.github.io/projects/ldm-isp/">https://csqiangwen.github.io/projects/ldm-isp/</a> </p>
<blockquote>
<p>å°†ä½å…‰ç¯å¢ƒä¸‹çš„å™ªå£°RAWå›¾åƒå¢å¼ºä¸ºæ›å…‰è‰¯å¥½ã€æ¸…æ™°çš„sRGBå›¾åƒï¼Œå¯¹äºç°ä»£æ•°ç ç›¸æœºæ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç”±äºä¿¡å™ªæ¯”æ¥è¿‘é›¶ï¼Œå…ˆå‰çš„æ–¹æ³•åœ¨æç«¯ä½å…‰ç¯å¢ƒä¸‹æ¢å¤åœºæ™¯çš„ç»†å¾®ç»†èŠ‚å’ŒçœŸå®é¢œè‰²æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åŒæ—¶ï¼Œæ‰©æ•£æ¨¡å‹åœ¨é€šç”¨åŸŸå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥è¿›è¡Œç¥ç»ç½‘ç»œISPï¼Œä»¥æå‡ä½å…‰å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è°ƒæ•´é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨RAWåŸŸä¸Šè¿è¡Œï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç³»åˆ—è½»é‡çº§é©¯æœæ¨¡å—ï¼Œé€šè¿‡å°†RAWä¿¡æ¯æ³¨å…¥æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼Œè°ƒåˆ¶UNetçš„ä¸­é—´ç‰¹å¾ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°UNetå»å™ªå’Œè§£ç å™¨é‡å»ºåœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä¸åŒä½œç”¨ï¼Œè¿™æ¿€åŠ±æˆ‘ä»¬å°†ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡åˆ†è§£ä¸ºæ½œåœ¨ç©ºé—´çš„ä½é¢‘å†…å®¹ç”Ÿæˆå’Œè§£ç é˜¶æ®µçš„é«˜é¢‘ç»†èŠ‚ç»´æŠ¤ã€‚é€šè¿‡åœ¨ä»£è¡¨æ€§æ•°æ®é›†ä¸Šè¿›è¡Œå¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç®€å•è®¾è®¡ä¸ä»…åœ¨å®šé‡è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œè€Œä¸”åœ¨è§†è§‰æ¯”è¾ƒæ–¹é¢æ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºå‡†æ¨¡å‹ï¼Œè¿™çªå‡ºäº†å¼ºå¤§ç”Ÿæˆå…ˆéªŒåœ¨æç«¯ä½å…‰ç¯å¢ƒä¸‹çš„ç¥ç»ç½‘ç»œISPçš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://csqiangwen.github.io/projects/ldm-isp/%E6%89%BE%E5%88%B0%E3%80%82">https://csqiangwen.github.io/projects/ldm-isp/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01027v4">PDF</a> </p>
<p><strong>Summary</strong><br>    åˆ©ç”¨é¢„è®­ç»ƒæ½œä¼æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œISPå¢å¼ºä½å…‰å›¾åƒã€‚è®­ç»ƒè½»é‡çº§é©¯æœæ¨¡å—å°†RAWä¿¡æ¯æ³¨å…¥æ‰©æ•£å»å™ªè¿‡ç¨‹ï¼Œåˆ†è§£ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸ºæ½œä¼ç©ºé—´ä½é¢‘å†…å®¹ç”Ÿæˆå’Œè§£ç é˜¶æ®µé«˜é¢‘ç»†èŠ‚ç»´æŠ¤ï¼Œå®ç°å…ˆè¿›æ€§èƒ½å¹¶æ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ½œä¼æ‰©æ•£æ¨¡å‹è¿›è¡Œç¥ç»ç½‘ç»œISPï¼Œä»¥åº”å¯¹ä½å…‰ç¯å¢ƒä¸‹çš„å›¾åƒå¢å¼ºæŒ‘æˆ˜ã€‚</li>
<li>è®­ç»ƒè½»é‡çº§é©¯æœæ¨¡å—ï¼Œå°†RAWä¿¡æ¯æ³¨å…¥æ‰©æ•£å»å™ªè¿‡ç¨‹ã€‚</li>
<li>åˆ†è§£ä½å…‰å›¾åƒå¢å¼ºä»»åŠ¡ä¸ºæ½œä¼ç©ºé—´ä½é¢‘å†…å®¹ç”Ÿæˆå’Œè§£ç é˜¶æ®µé«˜é¢‘ç»†èŠ‚ç»´æŠ¤ã€‚</li>
<li>UNetå»å™ªå’Œè§£ç å™¨é‡å»ºåœ¨æ½œä¼æ‰©æ•£æ¨¡å‹ä¸­æ‰®æ¼”ä¸åŒè§’è‰²ã€‚</li>
<li>æå‡ºçš„ç®€å•è®¾è®¡åœ¨ä»£è¡¨æ€§æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œå®ç°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®šé‡è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨è§†è§‰æ¯”è¾ƒä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-933cef0e8be94eca59a8bd7a5e137157.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c33fcc562042bb7a2f8a3324d0c55b91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f6e4818d3898e818b7b5b15cff10cb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d14a4341805223f77602e6ae3cd40194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f9c36aa73f73d3c8e5cec72dc6336d1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LaMD-Latent-Motion-Diffusion-for-Image-Conditional-Video-Generation"><a href="#LaMD-Latent-Motion-Diffusion-for-Image-Conditional-Video-Generation" class="headerlink" title="LaMD: Latent Motion Diffusion for Image-Conditional Video Generation"></a>LaMD: Latent Motion Diffusion for Image-Conditional Video Generation</h2><p><strong>Authors:Yaosi Hu, Zhenzhong Chen, Chong Luo</strong></p>
<p>The video generation field has witnessed rapid improvements with the introduction of recent diffusion models. While these models have successfully enhanced appearance quality, they still face challenges in generating coherent and natural movements while efficiently sampling videos. In this paper, we propose to condense video generation into a problem of motion generation, to improve the expressiveness of motion and make video generation more manageable. This can be achieved by breaking down the video generation process into latent motion generation and video reconstruction. Specifically, we present a latent motion diffusion (LaMD) framework, which consists of a motion-decomposed video autoencoder and a diffusion-based motion generator, to implement this idea. Through careful design, the motion-decomposed video autoencoder can compress patterns in movement into a concise latent motion representation. Consequently, the diffusion-based motion generator is able to efficiently generate realistic motion on a continuous latent space under multi-modal conditions, at a cost that is similar to that of image diffusion models. Results show that LaMD generates high-quality videos on various benchmark datasets, including BAIR, Landscape, NATOPS, MUG and CATER-GEN, that encompass a variety of stochastic dynamics and highly controllable movements on multiple image-conditional video generation tasks, while significantly decreases sampling time. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆé¢†åŸŸæœ€è¿‘å¼•å…¥äº†æ‰©æ•£æ¨¡å‹ï¼Œè¯¥é¢†åŸŸå·²ç»çœ‹åˆ°äº†è¿…é€Ÿçš„æ”¹è¿›ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨æå‡å¤–è§‚è´¨é‡æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€åœ¨æœ‰æ•ˆé‡‡æ ·è§†é¢‘çš„åŒæ—¶ç”Ÿæˆè¿è´¯ä¸”è‡ªç„¶åŠ¨ä½œçš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå°†è§†é¢‘ç”Ÿæˆç®€åŒ–ä¸ºåŠ¨ä½œç”Ÿæˆé—®é¢˜ï¼Œä»¥æé«˜åŠ¨ä½œçš„è¡¨è¾¾èƒ½åŠ›å¹¶ä½¿è§†é¢‘ç”Ÿæˆæ›´åŠ æ˜“äºç®¡ç†ã€‚è¿™å¯ä»¥é€šè¿‡å°†è§†é¢‘ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºæ½œåœ¨åŠ¨ä½œç”Ÿæˆå’Œè§†é¢‘é‡å»ºæ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ½œåœ¨åŠ¨ä½œæ‰©æ•£ï¼ˆLaMDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”±åŠ¨ä½œåˆ†è§£è§†é¢‘è‡ªç¼–ç å™¨å’ŒåŸºäºæ‰©æ•£çš„åŠ¨ä½œç”Ÿæˆå™¨ç»„æˆï¼Œä»¥å®ç°è¿™ä¸ªæƒ³æ³•ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡ï¼ŒåŠ¨ä½œåˆ†è§£è§†é¢‘è‡ªç¼–ç å™¨å¯ä»¥å°†è¿åŠ¨æ¨¡å¼å‹ç¼©æˆç®€æ´çš„æ½œåœ¨åŠ¨ä½œè¡¨ç¤ºã€‚å› æ­¤ï¼ŒåŸºäºæ‰©æ•£çš„åŠ¨ä½œç”Ÿæˆå™¨èƒ½å¤Ÿåœ¨è¿ç»­æ½œåœ¨ç©ºé—´ä¸‹åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹é«˜æ•ˆç”Ÿæˆé€¼çœŸçš„åŠ¨ä½œï¼Œå…¶æˆæœ¬ç±»ä¼¼äºå›¾åƒæ‰©æ•£æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒLaMDåœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šç”Ÿæˆäº†é«˜è´¨é‡çš„è§†é¢‘ï¼ŒåŒ…æ‹¬BAIRã€Landscapeã€NATOPSã€MUGå’ŒCATER-GENï¼Œæ¶µç›–äº†å¤šç§éšæœºåŠ¨æ€å’Œé«˜åº¦å¯æ§çš„åŠ¨ä½œåœ¨å¤šä¸ªå›¾åƒæ¡ä»¶è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2304.11603v2">PDF</a> accepted by IJCV</p>
<p><strong>æ‘˜è¦</strong><br>    æœ€æ–°æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†å¿«é€Ÿè¿›å±•ï¼ŒæˆåŠŸæé«˜äº†è§†é¢‘å¤–è§‚è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆè¿è´¯ã€è‡ªç„¶åŠ¨ä½œä»¥åŠé«˜æ•ˆé‡‡æ ·è§†é¢‘æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºå°†è§†é¢‘ç”Ÿæˆç®€åŒ–ä¸ºåŠ¨ä½œç”Ÿæˆé—®é¢˜ï¼Œä»¥æé«˜åŠ¨ä½œçš„è¡¨è¾¾æ€§å’Œè§†é¢‘ç”Ÿæˆçš„æ˜“ç®¡ç†æ€§ã€‚é€šè¿‡åˆ†è§£ä¸ºæ½œåœ¨åŠ¨ä½œç”Ÿæˆå’Œè§†é¢‘é‡å»ºä¸¤ä¸ªæ­¥éª¤æ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨åŠ¨ä½œæ‰©æ•£ï¼ˆLaMDï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬è¿åŠ¨åˆ†è§£è§†é¢‘è‡ªç¼–ç å™¨å’ŒåŸºäºæ‰©æ•£çš„è¿åŠ¨ç”Ÿæˆå™¨ã€‚è¿åŠ¨åˆ†è§£è§†é¢‘è‡ªç¼–ç å™¨èƒ½å¤Ÿå‹ç¼©è¿åŠ¨æ¨¡å¼æˆç®€æ´çš„æ½œåœ¨è¿åŠ¨è¡¨ç¤ºï¼Œè€ŒåŸºäºæ‰©æ•£çš„è¿åŠ¨ç”Ÿæˆå™¨åˆ™èƒ½åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹åœ¨è¿ç»­çš„æ½œåœ¨ç©ºé—´é«˜æ•ˆç”ŸæˆçœŸå®åŠ¨ä½œï¼Œæˆæœ¬ç±»ä¼¼äºå›¾åƒæ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMDåœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šç”Ÿæˆäº†é«˜è´¨é‡çš„è§†é¢‘ï¼Œæ¶µç›–å¤šç§éšæœºåŠ¨æ€å’Œé«˜åº¦å¯æ§çš„åŠ¨ä½œï¼Œæ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ—¶é—´ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ç”Ÿæˆè¿è´¯è‡ªç„¶åŠ¨ä½œå’Œé«˜æ•ˆé‡‡æ ·è§†é¢‘çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºå°†è§†é¢‘ç”Ÿæˆç®€åŒ–ä¸ºåŠ¨ä½œç”Ÿæˆé—®é¢˜ï¼Œä»¥æé«˜åŠ¨ä½œè¡¨è¾¾æ€§å’Œè§†é¢‘ç”Ÿæˆçš„æ˜“ç®¡ç†æ€§ã€‚</li>
<li>å¼•å…¥æ½œåœ¨åŠ¨ä½œæ‰©æ•£ï¼ˆLaMDï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬è¿åŠ¨åˆ†è§£è§†é¢‘è‡ªç¼–ç å™¨å’ŒåŸºäºæ‰©æ•£çš„è¿åŠ¨ç”Ÿæˆå™¨ã€‚</li>
<li>è¿åŠ¨åˆ†è§£è§†é¢‘è‡ªç¼–ç å™¨èƒ½å‹ç¼©è¿åŠ¨æ¨¡å¼åˆ°ç®€æ´çš„æ½œåœ¨è¿åŠ¨è¡¨ç¤ºã€‚</li>
<li>åŸºäºæ‰©æ•£çš„è¿åŠ¨ç”Ÿæˆå™¨èƒ½åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹é«˜æ•ˆç”ŸæˆçœŸå®åŠ¨ä½œã€‚</li>
<li>LaMDæ¡†æ¶èƒ½åœ¨å„ç§åŸºå‡†æ•°æ®é›†ä¸Šç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œæ¶µç›–å¤šç§éšæœºåŠ¨æ€å’Œé«˜åº¦å¯æ§çš„åŠ¨ä½œã€‚</li>
<li>LaMDæ¡†æ¶æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2304.11603">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c5fc16a8810a74ca16eab92f618ceea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0af96dde83c918f34c2d96b9b7817340.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-271274ff0cc26d994b410bdc34a3e979.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f0c8c4c529625b2ccb6e5f3eac1eb74.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f657e4071acf014b3f096ccdeafde015.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Towards Accurate and Interpretable Neuroblastoma Diagnosis via   Contrastive Multi-scale Pathological Image Analysis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3077e570cba8a06780060df47859346.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  BEV-GS Feed-forward Gaussian Splatting in Bird's-Eye-View for Road   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
