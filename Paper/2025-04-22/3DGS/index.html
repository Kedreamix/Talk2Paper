<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-04-22  Green Robotic Mixed Reality with Gaussian Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e8e2e8fe9c1acc4ea196ef45ab5be5ac.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-22-更新"><a href="#2025-04-22-更新" class="headerlink" title="2025-04-22 更新"></a>2025-04-22 更新</h1><h2 id="Green-Robotic-Mixed-Reality-with-Gaussian-Splatting"><a href="#Green-Robotic-Mixed-Reality-with-Gaussian-Splatting" class="headerlink" title="Green Robotic Mixed Reality with Gaussian Splatting"></a>Green Robotic Mixed Reality with Gaussian Splatting</h2><p><strong>Authors:Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu</strong></p>
<p>Realizing green communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images at high frequencies through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and makes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS model which enables the simulator to opportunistically render a photo-realistic view from the robot’s pose, thereby reducing the need for excessive image uploads. Since the GS model may involve discrepancies compared to the actual environments, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation across different frames. The GSCLO problem is solved by an accelerated penalty optimization (APO) algorithm. Experiments demonstrate that the proposed GSRMR reduces the communication energy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with APO outperforms extensive baseline schemes, in terms of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). </p>
<blockquote>
<p>在机器人混合现实（RoboMR）系统中实现绿色通信是一个挑战，因为需要通过无线信道以高频率上传高分辨率图像。本文提出了高斯飞溅（GS）RoboMR（GSRMR），它降低了能耗，是朝着绿色RoboMR迈出的具体一步。GSRMR的关键是建立GS模型，使模拟器能够根据机器人的姿态随机呈现逼真的视图，从而减少过多的图像上传需求。由于GS模型可能与实际环境存在偏差，因此进一步提出了GS跨层优化（GSCLO）框架，该框架联合优化内容切换（即决定是否上传图像）和不同帧之间的功率分配。GSCLO问题通过加速惩罚优化（APO）算法解决。实验表明，与RoboMR相比，所提出的GSRMR将通信能耗降低了超过10倍。此外，与广泛的基线方案相比，带有APO的GSRMR在峰值信噪比（PSNR）和结构相似性指数度量（SSIM）方面表现出更好的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13697v1">PDF</a> 6 pages, 5 figures, accepted by IEEE INFOCOM 2025 Workshop on   Networked Robotics and Communication Systems</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于高斯涂抹技术（GS）的机器人混合现实（RoboMR）系统（GSRMR），通过构建GS模型降低高解析度图像的高频上传需求，实现绿色通信。为优化GS模型与实体环境差异，进一步提出GS跨层优化（GSCLO）框架，通过内容切换与功率分配联合优化，采用加速惩罚优化（APO）算法解决GSCLO问题。实验证明，GSRMR系统较传统RoboMR系统能减少超过10倍的通信能耗，并在峰值信噪比（PSNR）和结构相似性指数度量（SSIM）方面优于其他基线方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高频上传高解析度图像是RoboMR系统实现绿色通信的挑战。</li>
<li>GSRMR通过构建GS模型降低图像上传需求，实现绿色通信。</li>
<li>GSRMR技术能显著减少通信能耗，较传统RoboMR系统减少超过10倍。</li>
<li>为优化GS模型与实体环境的差异，提出GS跨层优化（GSCLO）框架。</li>
<li>GSCLO框架通过联合优化内容切换和功率分配来解决问题。</li>
<li>采用加速惩罚优化（APO）算法解决GSCLO问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13697">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4757e02496b8fb56280d3aa07d12c338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fcf7130b0be280fb11e9959a316b59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a68c32be138e28795f02393929a1dd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aee412e309489df75b53d8ddc962ad84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7784a6b289997572b11cef9084f7134e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4945033f9b89574380918e2a40e467f6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BEV-GS-Feed-forward-Gaussian-Splatting-in-Bird’s-Eye-View-for-Road-Reconstruction"><a href="#BEV-GS-Feed-forward-Gaussian-Splatting-in-Bird’s-Eye-View-for-Road-Reconstruction" class="headerlink" title="BEV-GS: Feed-forward Gaussian Splatting in Bird’s-Eye-View for Road   Reconstruction"></a>BEV-GS: Feed-forward Gaussian Splatting in Bird’s-Eye-View for Road   Reconstruction</h2><p><strong>Authors:Wenhua Wu, Tong Zhao, Chensheng Peng, Lei Yang, Yintao Wei, Zhe Liu, Hesheng Wang</strong></p>
<p>Road surface is the sole contact medium for wheels or robot feet. Reconstructing road surface is crucial for unmanned vehicles and mobile robots. Recent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have achieved remarkable results in scene reconstruction. However, they typically rely on multi-view image inputs and require prolonged optimization times. In this paper, we propose BEV-GS, a real-time single-frame road surface reconstruction method based on feed-forward Gaussian splatting. BEV-GS consists of a prediction module and a rendering module. The prediction module introduces separate geometry and texture networks following Bird’s-Eye-View paradigm. Geometric and texture parameters are directly estimated from a single frame, avoiding per-scene optimization. In the rendering module, we utilize grid Gaussian for road surface representation and novel view synthesis, which better aligns with road surface characteristics. Our method achieves state-of-the-art performance on the real-world dataset RSRD. The road elevation error reduces to 1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction and rendering FPS is 26, and 2061, respectively, enabling high-accuracy and real-time applications. The code will be available at: \href{<a target="_blank" rel="noopener" href="https://github.com/cat-wwh/BEV-GS%7D%7B/texttt%7Bhttps://github.com/cat-wwh/BEV-GS%7D%7D">https://github.com/cat-wwh/BEV-GS}{\texttt{https://github.com/cat-wwh/BEV-GS}}</a> </p>
<blockquote>
<p>路面是车轮或机器人脚部的唯一接触媒介。对于无人驾驶汽车和移动机器人来说，重建路面至关重要。最近关于神经辐射场（NeRF）和高斯贴图（GS）的研究在场景重建方面取得了显著成果。然而，它们通常依赖于多视角图像输入，并且需要长时间的优化。在本文中，我们提出了基于前馈高斯贴图的实时单帧路面重建方法BEV-GS。BEV-GS由预测模块和渲染模块组成。预测模块引入了遵循鸟瞰图范式的单独几何和纹理网络。几何和纹理参数直接从单帧图像中估计，避免了针对每个场景的优化。在渲染模块中，我们利用网格高斯进行路面表示和新视角合成，这与路面特性更加吻合。我们的方法在真实世界数据集RSRD上达到了最新性能。路面高程误差降低到1.73厘米，新视角合成的峰值信噪比达到28.36分贝。预测和渲染的FPS分别为26和2061，可实现高精度和实时应用。代码将在以下网址提供：<a target="_blank" rel="noopener" href="https://github.com/cat-wwh/BEV-GS">https://github.com/cat-wwh/BEV-GS</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于前馈高斯映射（Gaussian Splatting）的实时单帧路面重建方法，称为BEV-GS。该方法包括预测模块和渲染模块，预测模块采用鸟瞰图范式，引入单独的几何和纹理网络，直接从单帧图像估计几何和纹理参数，避免了逐场景的优化。渲染模块使用网格高斯进行路面表示和新颖视图合成。该方法在真实世界数据集RSRD上实现了最先进的性能，路面高程误差降低到1.73厘米，新颖视图合成的峰值信噪比达到28.36分贝，预测和渲染的帧数分别为每秒26帧和每秒2061帧，可实现高精度和实时应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了基于前馈高斯映射（Gaussian Splatting）的实时单帧路面重建方法BEV-GS。</li>
<li>BEV-GS包含预测模块和渲染模块，预测模块采用鸟瞰图范式，直接从单帧图像估计几何和纹理参数。</li>
<li>渲染模块使用网格高斯进行路面表示和新颖视图合成，与路面特性更相符。</li>
<li>在真实世界数据集RSRD上实现了最先进的性能，路面高程误差降低到1.73厘米，PSNR达到28.36分贝。</li>
<li>BEV-GS具有高的预测和渲染速度，分别为每秒26帧和每秒2061帧，适用于实时应用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-76d9128da2dc72a6601cc90236d82f78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ffaabbd78bc2b2865d01ae2b3299b68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac6342e357c4c3c645856c901a8cb6da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3077e570cba8a06780060df47859346.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EDGS-Eliminating-Densification-for-Efficient-Convergence-of-3DGS"><a href="#EDGS-Eliminating-Densification-for-Efficient-Convergence-of-3DGS" class="headerlink" title="EDGS: Eliminating Densification for Efficient Convergence of 3DGS"></a>EDGS: Eliminating Densification for Efficient Convergence of 3DGS</h2><p><strong>Authors:Dmytro Kotovenko, Olga Grebenkova, Björn Ommer</strong></p>
<p>3D Gaussian Splatting reconstructs scenes by starting from a sparse Structure-from-Motion initialization and iteratively refining under-reconstructed regions. This process is inherently slow, as it requires multiple densification steps where Gaussians are repeatedly split and adjusted, following a lengthy optimization path. Moreover, this incremental approach often leads to suboptimal renderings, particularly in high-frequency regions where detail is critical.   We propose a fundamentally different approach: we eliminate densification process with a one-step approximation of scene geometry using triangulated pixels from dense image correspondences. This dense initialization allows us to estimate rough geometry of the scene while preserving rich details from input RGB images, providing each Gaussian with well-informed colors, scales, and positions. As a result, we dramatically shorten the optimization path and remove the need for densification. Unlike traditional methods that rely on sparse keypoints, our dense initialization ensures uniform detail across the scene, even in high-frequency regions where 3DGS and other methods struggle. Moreover, since all splats are initialized in parallel at the start of optimization, we eliminate the need to wait for densification to adjust new Gaussians.   Our method not only outperforms speed-optimized models in training efficiency but also achieves higher rendering quality than state-of-the-art approaches, all while using only half the splats of standard 3DGS. It is fully compatible with other 3DGS acceleration techniques, making it a versatile and efficient solution that can be integrated with existing approaches. </p>
<blockquote>
<p>3D高斯摊铺（3DGS）从稀疏的运动结构初始化开始重建场景，并通过迭代优化细化未重建区域。这个过程本质上是缓慢的，因为它需要进行多次稠密化步骤，高斯函数需要反复分割和调整，沿着漫长的优化路径进行。此外，这种增量方法往往导致次优渲染，特别是在高频区域，细节至关重要。我们提出了一种根本不同的方法：通过密集图像对应关系的三角化像素进行场景几何的一阶近似，从而消除稠密化过程。这种密集初始化允许我们估计场景的粗略几何形状，同时保留输入RGB图像的丰富细节，为每个高斯提供颜色、尺度和位置的准确信息。因此，我们大大缩短了优化路径，并消除了对稠密化的需求。与传统的依赖于稀疏关键点的方法不同，我们的密集初始化确保了场景中的均匀细节，即使在3DGS和其他方法表现困难的高频区域也是如此。此外，由于所有摊铺都在优化开始时并行初始化，我们不需要等待稠密化来调整新的高斯函数。我们的方法不仅在训练效率上超越了经过速度优化的模型，而且达到了比最新技术更高的渲染质量，同时使用的摊铺数量只有标准3DGS的一半。它与其他3DGS加速技术完全兼容，是一种通用且高效的解决方案，可以与现有方法集成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13204v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本研究提出一种新型的基于像素稠密的初始化场景的几何方法。传统的场景重建通过稀疏SfM初始化和多次高斯分割优化来完成，速度慢且在高频区域细节渲染不足。我们的方法使用稠密图像对应的三角形像素直接完成场景几何的初步估计，实现场景的快速、全面且高质量重建。在单次操作中对场景的粗糙几何进行初始化的同时保留丰富细节，极大地简化了优化过程，避免反复迭代的必要步骤，避免了精细密度处理的繁琐过程。与传统的稀疏关键点依赖方法相比，我们的稠密初始化保证了场景的统一细节渲染效果，尤其在需要精细处理的高频区域展现了其优越性。此研究打破了现有的方法局限性，将各高斯的色彩、尺寸和位置得到更好预测。实验结果证明我们的方法在训练效率和渲染质量上都超过了现有的模型，并使用半数的标准的溅波次数达到更佳的效果，显示出极大的兼容性可与现有高效的重建方法进行整合融合以生成更高的精度重建。总结言之，此论文展现了一种新颖的建模重建思路与方法并适用于现代实际应用中的场景重建问题。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>提出一种基于稠密像素初始化场景的几何方法，避免了传统方法中多次迭代的密集化过程。</li>
<li>通过稠密图像对应的三角形像素估计场景的初步几何形态，快速准确地实现场景的重建，保持高频区域的丰富细节。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-023c73442fc935d504d4d6e11047e713.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc85d065b2b2f9fbd0e3741c0980efb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8e2e8fe9c1acc4ea196ef45ab5be5ac.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ODHSR-Online-Dense-3D-Reconstruction-of-Humans-and-Scenes-from-Monocular-Videos"><a href="#ODHSR-Online-Dense-3D-Reconstruction-of-Humans-and-Scenes-from-Monocular-Videos" class="headerlink" title="ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from   Monocular Videos"></a>ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from   Monocular Videos</h2><p><strong>Authors:Zetong Zhang, Manuel Kaufmann, Lixin Xue, Jie Song, Martin R. Oswald</strong></p>
<p>Creating a photorealistic scene and human reconstruction from a single monocular in-the-wild video figures prominently in the perception of a human-centric 3D world. Recent neural rendering advances have enabled holistic human-scene reconstruction but require pre-calibrated camera and human poses, and days of training time. In this work, we introduce a novel unified framework that simultaneously performs camera tracking, human pose estimation and human-scene reconstruction in an online fashion. 3D Gaussian Splatting is utilized to learn Gaussian primitives for humans and scenes efficiently, and reconstruction-based camera tracking and human pose estimation modules are designed to enable holistic understanding and effective disentanglement of pose and appearance. Specifically, we design a human deformation module to reconstruct the details and enhance generalizability to out-of-distribution poses faithfully. Aiming to learn the spatial correlation between human and scene accurately, we introduce occlusion-aware human silhouette rendering and monocular geometric priors, which further improve reconstruction quality. Experiments on the EMDB and NeuMan datasets demonstrate superior or on-par performance with existing methods in camera tracking, human pose estimation, novel view synthesis and runtime. Our project page is at <a target="_blank" rel="noopener" href="https://eth-ait.github.io/ODHSR">https://eth-ait.github.io/ODHSR</a>. </p>
<blockquote>
<p>创建真实场景和从单一的单眼视频重构人物对于人类为中心的三维世界的感知尤为突出。最近的神经渲染进展已经实现了全景式的人景重建，但需要预先校准的相机和人体姿势，以及长时间的训练时间。在这项工作中，我们引入了一种新型统一框架，它以在线方式同时执行相机跟踪、人体姿态估计和人与场景的重建。三维高斯混合映射被用于有效地学习人类和场景的Gaussian基元，而基于重建的相机跟踪和人体姿态估计模块的设计，旨在实现对姿势和外观的全面理解和有效分离。具体来说，我们设计了一个人体变形模块来重建细节，并增强对分布外姿势的泛化能力。为了准确学习人与场景之间的空间相关性，我们引入了遮挡感知的人形轮廓渲染和单眼几何先验，这进一步提高了重建质量。在EMDB和NeuMan数据集上的实验表明，在相机跟踪、人体姿态估计、新视角合成和运行时等方面，我们的性能优于或相当于现有方法。我们的项目页面是<a target="_blank" rel="noopener" href="https://eth-ait.github.io/ODHSR%E3%80%82">https://eth-ait.github.io/ODHSR。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13167v2">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>该文介绍了一个统一框架，能在线同时进行摄像机追踪、人体姿态估计和人机场景重建。利用3D高斯拼贴学习高斯原始人体和场景，设计重建的相机追踪和人体姿态估计模块，实现整体理解和有效的姿态与外观分离。通过设计人体变形模块来重建细节，提高泛化能力，以准确学习人与场景的空间关联。引入遮挡感知的人体轮廓渲染和单目几何先验，进一步提高重建质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个统一框架，实现摄像机追踪、人体姿态估计和人机场景重建。</li>
<li>利用3D高斯拼贴学习高斯原始人体和场景。</li>
<li>设计了基于重建的相机追踪和人体姿态估计模块，实现整体理解和姿态与外观的有效分离。</li>
<li>通过设计人体变形模块来重建细节，提高模型的泛化能力。</li>
<li>准确学习人与场景的空间关联。</li>
<li>引入遮挡感知的人体轮廓渲染来提高重建质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-360c859e292cb5ddfc2fce7531c914b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28fd95ce2d17ed0074e4aaea1197bd30.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Mind2Matter-Creating-3D-Models-from-EEG-Signals"><a href="#Mind2Matter-Creating-3D-Models-from-EEG-Signals" class="headerlink" title="Mind2Matter: Creating 3D Models from EEG Signals"></a>Mind2Matter: Creating 3D Models from EEG Signals</h2><p><strong>Authors:Xia Deng, Shen Chen, Jiale Zhou, Lei Li</strong></p>
<p>The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">https://github.com/sddwwww/Mind2Matter</a>. </p>
<blockquote>
<p>从脑电波信号重建三维物体在脑机接口（BCI）研究中受到广泛关注。目前的研究主要利用功能磁共振成像（fMRI）进行三维重建任务，因其具有出色的空间分辨率。然而，fMRI的临床应用受限于其高昂的成本和无法支持实时操作。相比之下，脑电图（EEG）作为经济实惠、非侵入式和移动式的实时脑机交互系统解决方案，具有明显优势。虽然深度学习领域的最新进展在神经网络数据生成图像方面取得了显著进展，但将脑电图信号解码为结构化三维表示仍被大大忽视。在本文中，我们提出了一种新型框架，该框架利用神经解码技术和生成模型将脑电图记录转化为三维物体重建。我们的方法包括训练EEG编码器以提取时空视觉特征，微调大型语言模型以将这些特征解释为描述性多模式输出，并利用带有布局指导控制的生成三维高斯来合成最终的三维结构。实验表明，我们的模型能够捕捉显著的几何和语义特征，为脑机接口（BCI）、虚拟现实和神经仿生器件的应用铺平道路。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sddwwww/Mind2Matter">链接</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11936v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于脑电图（EEG）记录的神经网络解码技术和生成模型的新框架，用于将EEG信号解码为三维物体重建。该框架包括训练EEG编码器提取时空视觉特征，微调大型语言模型以解释这些特征并生成多模态输出，并利用布局指导控制的生成三维高斯模型合成最终的三维结构。实验表明，该模型能够捕捉重要的几何和语义特征，为脑机接口、虚拟现实和神经仿生等领域的应用开辟了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建三维物体从脑信号在脑机接口（BCI）研究中受到关注。</li>
<li>当前研究主要利用功能磁共振成像（fMRI）进行三维重建任务，但其高昂成本和无法支持实时操作限制了临床应用。</li>
<li>与之相比，脑电图（EEG）作为一种经济、无创、可移动解决方案，在实时脑机交互系统中具有明显优势。</li>
<li>深度学习在图像生成方面具有显著进展，但将EEG信号解码为结构化三维表示仍然未被充分探索。</li>
<li>本文提出了一种新框架，该框架结合神经网络解码技术和生成模型，能将EEG记录转化为三维物体重建。</li>
<li>该框架包括训练EEG编码器、微调大型语言模型和利用生成三维高斯模型进行合成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-95343a55301c4bf587fbc0a67665cb0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca1f712f6ee96a259fc29c0453b3053f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17ba79f5a837f15bdb2207ff4617883b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e223852dc3312651eaa87167b1aefe61.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-François Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: <a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a> </p>
<blockquote>
<p>我们提出了GaSLight方法，该方法可以从常规图像生成空间变化的光照。我们的方法建议使用HDR高斯斑点作为光源表示，这标志着常规图像首次可以作为三维渲染器的光源。我们的两阶段过程首先利用扩散模型中嵌入的先验知识来增强图像的可信度和准确性，从而扩展图像的动态范围。接下来，我们使用高斯斑点对三维照明进行建模，实现空间变化的光照。我们的方法在HDR估计及其应用于照明虚拟对象和场景方面产生了最先进的成果。为了促进将图像作为基准光源的评估，我们引入了一个全新的校准和不饱和HDR数据集来评估图像作为光源。我们使用这个新数据集和文献中的现有数据集来评估我们的方法。项目页面：<a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了名为GaSLight的方法，该方法能够从常规图像生成空间变化的照明效果。该方法采用HDR高斯Splats作为光源表示，首次实现了常规图像在三维渲染器中的作为光源应用。通过利用扩散模型中的先验知识，首先增强图像的动态范围。然后，使用高斯Splats进行三维照明建模，实现空间变化照明效果。该方法在HDR估算及其虚拟对象和场景的照明应用中取得了最新结果。为了将图像作为光源进行基准测试，我们引入了一个新型的校准不饱和HDR数据集。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLight方法能够从常规图像生成空间变化的照明效果。</li>
<li>HDR高斯Splats被用作光源表示，实现了常规图像在三维渲染中的创新应用。</li>
<li>方法通过利用扩散模型中的先验知识，准确增强图像动态范围。</li>
<li>采用高斯Splats进行三维照明建模，达到空间变化照明。</li>
<li>方法在HDR估算方面取得了最新结果，并成功应用于虚拟对象和场景的照明。</li>
<li>为了评估图像作为光源的效果，引入了一个新型的校准不饱和HDR数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0acd540684c76bb65da870b875d2c04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a587eee72f338f28224d08dea7b8d954.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faad0dc0950fb458ed849d09cbe96dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b148832e6da27501a698c1717c440602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95ce0b37073da87d139eff1cacc105cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-606d27e2d875c7ceeb31eb0cba8ec175.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EditSplat-Multi-View-Fusion-and-Attention-Guided-Optimization-for-View-Consistent-3D-Scene-Editing-with-3D-Gaussian-Splatting"><a href="#EditSplat-Multi-View-Fusion-and-Attention-Guided-Optimization-for-View-Consistent-3D-Scene-Editing-with-3D-Gaussian-Splatting" class="headerlink" title="EditSplat: Multi-View Fusion and Attention-Guided Optimization for   View-Consistent 3D Scene Editing with 3D Gaussian Splatting"></a>EditSplat: Multi-View Fusion and Attention-Guided Optimization for   View-Consistent 3D Scene Editing with 3D Gaussian Splatting</h2><p><strong>Authors:Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Sangheon Shin, Sangmin Kim, Sangpil Kim</strong></p>
<p>Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR&#x2F;VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel text-driven 3D scene editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric structure inherent to 3DGS. Additionally, our AGT utilizes the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local editing. Through extensive qualitative and quantitative evaluations, EditSplat achieves state-of-the-art performance, establishing a new benchmark for text-driven 3D scene editing. </p>
<blockquote>
<p>近期三维编辑技术的进展凸显了文本驱动方法在实时、用户友好的AR&#x2F;VR应用中的潜力。然而，当前的方法依赖于二维扩散模型，而没有充分考虑到多视角信息，导致多视角不一致。虽然三维高斯摊铺（3DGS）显著提高了渲染质量和速度，但其三维编辑过程在优化方面遇到了困难，因为预训练的高斯值保留了过多的源信息，阻碍了优化。为了解决这个问题，我们提出了EditSplat，这是一种新颖的文本驱动三维场景编辑框架，它融合了多视角融合指导（MFG）和注意力引导修剪（AGT）。我们的MFG通过融入扩散过程中的关键多视角信息，利用无分类器指导的文本到图像扩散模型和固有的三维几何结构，确保多视角的一致性。此外，我们的AGT利用三维高斯表示的显式表达来选择性修剪和优化三维高斯值，提高优化效率，实现精确且语义丰富的局部编辑。通过广泛的质量和数量评估，EditSplat达到了最先进的性能水平，为文本驱动的三维场景编辑建立了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.11520v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了基于文本的实时三维场景编辑技术的前沿进展。针对现有技术的不足，如多视角信息利用不足和预训练高斯模型优化效率低下等问题，提出了名为EditSplat的新型文本驱动三维场景编辑框架。该框架结合了多视角融合引导（MFG）和注意力引导修剪（AGT）两大技术，提高了渲染质量和速度，实现了多视角一致性，优化了编辑效率，并实现了精确、语义丰富的局部编辑。通过广泛的定性和定量评估，EditSplat达到了业界领先水平，为基于文本的实时三维场景编辑树立了新的标杆。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期三维编辑技术的进步突显了文本驱动方法在实时AR&#x2F;VR应用的潜力。</li>
<li>当前方法主要依赖二维扩散模型，缺乏多视角信息的考虑，导致多视角不一致性。</li>
<li>3D高斯贴图（3DGS）能提高渲染质量和速度，但其三维编辑过程面临优化效率低下的问题。</li>
<li>EditSplat框架通过结合多视角融合引导（MFG）和注意力引导修剪（AGT）解决上述问题。</li>
<li>MFG通过融入多视角信息到扩散过程中确保多视角一致性，利用文本到图像的扩散模型的分类器引导以及固有的三维几何结构。</li>
<li>AGT利用3DGS的显式表示进行选择性修剪和优化三维高斯模型，提高优化效率并实现精确、语义丰富的局部编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.11520">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b396433bd0c45ba8bb75b1535ddc456.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-701ec12424a16c05b5750c70ac39a0d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d1c10e5a544c37f61ceca619f09777a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2b1370e32563de59b2407f6ef45fd0a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d3077e570cba8a06780060df47859346.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-22  BEV-GS Feed-forward Gaussian Splatting in Bird's-Eye-View for Road   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-994a173651d2ba11f830b9d5afae7fa6.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-04-22  Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
