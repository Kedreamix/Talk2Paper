<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-22  Meta-Learning and Knowledge Discovery based Physics-Informed Neural   Network for Remaining Useful Life Prediction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-cb21d596a538b2a51ce163b83a32f9d1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-22-更新"><a href="#2025-04-22-更新" class="headerlink" title="2025-04-22 更新"></a>2025-04-22 更新</h1><h2 id="Meta-Learning-and-Knowledge-Discovery-based-Physics-Informed-Neural-Network-for-Remaining-Useful-Life-Prediction"><a href="#Meta-Learning-and-Knowledge-Discovery-based-Physics-Informed-Neural-Network-for-Remaining-Useful-Life-Prediction" class="headerlink" title="Meta-Learning and Knowledge Discovery based Physics-Informed Neural   Network for Remaining Useful Life Prediction"></a>Meta-Learning and Knowledge Discovery based Physics-Informed Neural   Network for Remaining Useful Life Prediction</h2><p><strong>Authors:Yu Wang, Shujie Liu, Shuai Lv, Gengshuo Liu</strong></p>
<p>Predicting the remaining useful life (RUL) of rotating machinery is critical for industrial safety and maintenance, but existing methods struggle with scarce target-domain data and unclear degradation dynamics. We propose a Meta-Learning and Knowledge Discovery-based Physics-Informed Neural Network (MKDPINN) to address these challenges. The method first maps noisy sensor data to a low-dimensional hidden state space via a Hidden State Mapper (HSM). A Physics-Guided Regulator (PGR) then learns unknown nonlinear PDEs governing degradation evolution, embedding these physical constraints into the PINN framework. This integrates data-driven and physics-based approaches. The framework uses meta-learning, optimizing across source-domain meta-tasks to enable few-shot adaptation to new target tasks. Experiments on industrial data and the C-MAPSS benchmark show MKDPINN outperforms baselines in generalization and accuracy, proving its effectiveness for RUL prediction under data scarcity </p>
<blockquote>
<p>预测旋转机械的剩余使用寿命（RUL）对工业安全和维护至关重要。但现有方法在目标领域数据稀缺和退化动态不明确的情况下会遇到困难。我们提出了一种基于元学习和知识发现的物理信息神经网络（MKDPINN）来解决这些挑战。该方法首先通过隐藏状态映射器（HSM）将嘈杂的传感器数据映射到低维隐藏状态空间。然后，物理引导调节器（PGR）学习控制退化演变的未知非线性偏微分方程，将这些物理约束嵌入到PINN框架中。这结合了数据驱动和基于物理的方法。该框架使用元学习，通过优化源域元任务，使新目标任务进行少量样本即可适应。在工业数据和C-MAPSS基准测试上的实验表明，MKDPINN在泛化和准确性方面超过了基准，证明了其在数据稀缺情况下进行RUL预测的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13797v1">PDF</a> 34 pages,20 figs</p>
<p><strong>Summary</strong><br>：本文提出一种基于元学习与知识发现的物理信息神经网络（MKDPINN），用于预测旋转机械的剩余使用寿命（RUL）。该方法通过隐状态映射器（HSM）将噪声传感器数据映射到低维隐藏状态空间，并通过物理引导调节器（PGR）学习未知的支配退化演变非线性偏微分方程，将这些物理约束嵌入到PINN框架中。该框架利用元学习，通过对源域元任务进行优化，实现对新目标任务的少量适应。在工业数据和C-MAPSS基准测试上的实验表明，MKDPINN在泛化和准确性方面优于基线方法，证明其在数据稀缺情况下进行RUL预测的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MKDPINN方法结合数据驱动和物理基方法，用于预测旋转机械的剩余使用寿命（RUL）。</li>
<li>方法通过隐状态映射器（HSM）处理噪声传感器数据，将其映射到低维隐藏状态空间。</li>
<li>物理引导调节器（PGR）学习支配退化演变的未知非线性偏微分方程。</li>
<li>MKDPINN将物理约束嵌入到PINN框架中，提高模型的泛化能力和准确性。</li>
<li>该方法利用元学习，通过优化源域元任务，实现对新目标的少量适应。</li>
<li>工业数据和C-MAPSS基准测试的实验表明MKDPINN在RUL预测方面的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13797">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b75666b1c9c2bbf190994ae375ff13db.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Controlled-Territory-and-Conflict-Tracking-CONTACT-Geo-Mapping-Occupied-Territory-from-Open-Source-Intelligence"><a href="#Controlled-Territory-and-Conflict-Tracking-CONTACT-Geo-Mapping-Occupied-Territory-from-Open-Source-Intelligence" class="headerlink" title="Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping   Occupied Territory from Open Source Intelligence"></a>Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping   Occupied Territory from Open Source Intelligence</h2><p><strong>Authors:Paul K. Mandal, Cole Leo, Connor Hurley</strong></p>
<p>Open-source intelligence provides a stream of unstructured textual data that can inform assessments of territorial control. We present CONTACT, a framework for territorial control prediction using large language models (LLMs) and minimal supervision. We evaluate two approaches: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. Our model is trained on a small hand-labeled dataset of news articles covering ISIS activity in Syria and Iraq, using prompt-conditioned extraction of control-relevant signals such as military operations, casualties, and location references. We show that the BLOOMZ-based model outperforms the SetFit baseline, and that prompt-based supervision improves generalization in low-resource settings. CONTACT demonstrates that LLMs fine-tuned using few-shot methods can reduce annotation burdens and support structured inference from open-ended OSINT streams. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/PaulKMandal/CONTACT/">https://github.com/PaulKMandal/CONTACT/</a>. </p>
<blockquote>
<p>开源情报提供了一系列非结构化的文本数据，可以为地域控制评估提供信息。我们提出了CONTACT，这是一个使用大型语言模型和最小监督进行地域控制预测的框架。我们评估了两种方法：SetFit，一种基于嵌入的少量样本分类器，以及应用于BLOOMZ-560m的提示调整方法，这是一个多语言生成型LLM。我们的模型是在一个涵盖叙利亚和伊拉克ISIS活动的手工标注新闻数据集上进行训练的，通过使用提示条件下的提取与控制相关的信号，如军事行动、伤亡和位置参考。我们表明，基于BLOOMZ的模型优于SetFit基线，并且基于提示的监督可以改善低资源环境中的泛化能力。CONTACT证明，使用少量样本方法微调的大型语言模型可以减轻标注负担，并支持从开放的OSINT流中进行结构化推理。我们的代码可在<a href="https://github.com%E3%80%82com%2FPaulKMandal%2FCONTACT%2F%E6%89%BE%E5%88%B0%E3%80%82">https://github.comcom/PaulKMandal/CONTACT/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13730v1">PDF</a> 7 pages, 1 figure, 1 table</p>
<p><strong>Summary</strong></p>
<p>开源情报提供了一系列非结构化的文本数据，可为领土控制评估提供依据。本研究提出了CONTACT框架，利用大型语言模型和最小监督来进行领土控制预测。评估了两种途径：基于嵌入的少量样本分类器SetFit，以及应用于BLOOMZ-560m的多语言生成型LLM的提示调整方法。模型在少量手工标注的新闻文章数据集上进行训练，这些文章涵盖了叙利亚和伊拉克的ISIS活动，通过提示条件提取与控制相关的信号，如军事行动、伤亡和地点参考。研究结果表明，基于BLOOMZ的模型优于SetFit基线，提示监督在低资源环境中可改善泛化能力。CONTACT证明，使用少量样本方法微调的大型语言模型可以减轻标注负担，并支持从开放型OSINT流中进行结构化推理。相关代码已发布在<a target="_blank" rel="noopener" href="https://github.com/PaulKMandal/CONTACT/">https://github.com/PaulKMandal/CONTACT/</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CONTACT框架利用大型语言模型进行领土控制预测，开源情报能提供相关评估信息。</li>
<li>研究评估了SetFit和BLOOMZ-560m两种途径，前者是基于嵌入的少量样本分类器，后者是多语言生成型的大型语言模型。</li>
<li>模型训练使用了涉及ISIS在叙利亚和伊拉克活动的新闻文章数据集。</li>
<li>提示条件提取被用于提取控制相关的关键信号。</li>
<li>基于BLOOMZ的模型性能优于SetFit基线。</li>
<li>提示监督在低资源环境中提升了模型的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0d446e301899abbaaa1dd7085cd57c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-726309b29eaa47fb59ba9f9aa132deb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c752dc2c35694723ab4b42b14f6cd1b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Referring-Video-Single-and-Multi-Object-Segmentation-via-Cross-Modal-Affinity-with-Instance-Sequence-Matching"><a href="#Few-Shot-Referring-Video-Single-and-Multi-Object-Segmentation-via-Cross-Modal-Affinity-with-Instance-Sequence-Matching" class="headerlink" title="Few-Shot Referring Video Single- and Multi-Object Segmentation via   Cross-Modal Affinity with Instance Sequence Matching"></a>Few-Shot Referring Video Single- and Multi-Object Segmentation via   Cross-Modal Affinity with Instance Sequence Matching</h2><p><strong>Authors:Heng Liu, Guanghui Li, Mingqi Gao, Xiantong Zhen, Feng Zheng, Yang Wang</strong></p>
<p>Referring video object segmentation (RVOS) aims to segment objects in videos guided by natural language descriptions. We propose FS-RVOS, a Transformer-based model with two key components: a cross-modal affinity module and an instance sequence matching strategy, which extends FS-RVOS to multi-object segmentation (FS-RVMOS). Experiments show FS-RVOS and FS-RVMOS outperform state-of-the-art methods across diverse benchmarks, demonstrating superior robustness and accuracy. </p>
<blockquote>
<p>视频对象参考分割（RVOS）旨在根据自然语言描述对视频中的对象进行分割。我们提出了FS-RVOS，这是一个基于Transformer的模型，具有两个关键组件：跨模态亲和力模块和实例序列匹配策略，后者将FS-RVOS扩展到多对象分割（FS-RVMOS）。实验表明，FS-RVOS和FS-RVMOS在多种基准测试上的表现均优于现有技术，表现出更高的稳健性和准确性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13710v1">PDF</a> 23 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>基于自然语言描述的视频对象分割（RVOS）旨在实现对视频的精准对象分割。提出一种新的模型FS-RVOS，其核心为跨模态亲和模块与实例序列匹配策略。这一模型不仅能够处理单一对象分割问题，还能够进行多对象分割。实验结果证明FS-RVOS及扩展版FS-RVMOS在多种基准测试中表现优于现有技术，展现了其卓越的鲁棒性和准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RVOS旨在通过自然语言描述实现视频对象的精准分割。</li>
<li>提出了一种新的模型FS-RVOS，该模型基于Transformer技术。</li>
<li>FS-RVOS包含两个关键组件：跨模态亲和模块和实例序列匹配策略。</li>
<li>FS-RVOS不仅可以进行单一对象分割，还可以扩展到多对象分割（FS-RVMOS）。</li>
<li>实验证明FS-RVOS及FS-RVMOS在多种基准测试中表现优于现有技术。</li>
<li>FS-RVOS模型展现了卓越的鲁棒性和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13710">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-532dc64d18f74b8765a073731851dd3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-821a8fc5150e66603bd03480bc9ab713.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MEGA-Second-Order-Gradient-Alignment-for-Catastrophic-Forgetting-Mitigation-in-GFSCIL"><a href="#MEGA-Second-Order-Gradient-Alignment-for-Catastrophic-Forgetting-Mitigation-in-GFSCIL" class="headerlink" title="MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting   Mitigation in GFSCIL"></a>MEGA: Second-Order Gradient Alignment for Catastrophic Forgetting   Mitigation in GFSCIL</h2><p><strong>Authors:Jinhui Pang, Changqing Lin, Hao Lin, Jinglin He, Zhengjun Li, Zhihui Zhang, Xiaoshuai Hao</strong></p>
<p>Graph Few-Shot Class-Incremental Learning (GFSCIL) enables models to continually learn from limited samples of novel tasks after initial training on a large base dataset. Existing GFSCIL approaches typically utilize Prototypical Networks (PNs) for metric-based class representations and fine-tune the model during the incremental learning stage. However, these PN-based methods oversimplify learning via novel query set fine-tuning and fail to integrate Graph Continual Learning (GCL) techniques due to architectural constraints. To address these challenges, we propose a more rigorous and practical setting for GFSCIL that excludes query sets during the incremental training phase. Building on this foundation, we introduce Model-Agnostic Meta Graph Continual Learning (MEGA), aimed at effectively alleviating catastrophic forgetting for GFSCIL. Specifically, by calculating the incremental second-order gradient during the meta-training stage, we endow the model to learn high-quality priors that enhance incremental learning by aligning its behaviors across both the meta-training and incremental learning stages. Extensive experiments on four mainstream graph datasets demonstrate that MEGA achieves state-of-the-art results and enhances the effectiveness of various GCL methods in GFSCIL. We believe that our proposed MEGA serves as a model-agnostic GFSCIL paradigm, paving the way for future research. </p>
<blockquote>
<p>图少样本类增量学习（GFSCIL）使得模型能够在大量基础数据集进行初始训练后，继续从有限的新任务样本中学习。现有的GFSCIL方法通常利用原型网络（PNs）进行基于度量的类表示，并在增量学习阶段微调模型。然而，这些基于PN的方法通过新的查询集微调来简化学习，并由于架构约束而无法整合图持续学习（GCL）技术。为了解决这些挑战，我们为GFSCIL提出了一个更严格和实用的设置，即在增量训练阶段排除查询集。在此基础上，我们引入了模型无关的元图持续学习（MEGA），旨在有效缓解GFSCIL的灾难性遗忘问题。具体来说，通过计算元训练阶段的增量二阶梯度，我们赋予模型学习高质量先验的能力，这些先验知识通过调整元训练和增量学习阶段的行为，增强了增量学习。在四个主流图形数据集上的大量实验表明，MEGA达到了最新的结果，提高了GFSCIL中各种GCL方法的有效性。我们相信我们提出的MEGA作为一种模型无关的GFSCIL范式，为未来的研究铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13691v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>模型能够在基于大型基础数据集进行初始训练后，通过少量样本学习新任务的能力被称为图少样本类增量学习（GFSCIL）。现有方法主要使用原型网络（PNs）进行基于度量的类表示，并在增量学习阶段微调模型。然而，PNs方法简化了学习过程，未能整合图持续学习（GCL）技术。为解决这些问题，我们提出了更严格实用的GFSCIL设置，并在增量训练阶段排除查询集。在此基础上，我们引入了模型无关的元图持续学习（MEGA）方法，旨在有效缓解GFSCIL的灾难性遗忘问题。通过在元训练阶段计算增量二阶梯度，赋予模型学习能力的高质先验，提升在元训练和增量学习阶段的行为一致性。在四个主流图数据集上的实验表明，MEGA达到了最新水平的结果，提高了各种GCL方法在GFSCIL中的有效性。我们相信MEGA作为模型无关的GFSCIL范式为未来研究铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph Few-Shot Class-Incremental Learning (GFSCIL)允许模型从少量新任务样本中持续学习。</li>
<li>现有GFSCIL方法主要使用原型网络（PNs）进行类表示和微调模型。</li>
<li>PNs方法过于简化学习过程，未充分利用图持续学习（GCL）技术。</li>
<li>提出更严格的GFSCIL设置，排除增量训练阶段的查询集。</li>
<li>引入Model-Agnostic Meta Graph Continal Learning (MEGA)方法，缓解GFSCIL的灾难性遗忘问题。</li>
<li>MEGA通过计算增量二阶梯度赋予模型学习能力的高质先验，提升行为一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13691">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9aae05d8e74e83c122e997810fb479f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4cc01ed9f5550f679512d0e413cb247.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a9f616f124a98ebac35951115da90e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3886ab09926e71a5403e66075fa2820f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="KAN-or-MLP-Point-Cloud-Shows-the-Way-Forward"><a href="#KAN-or-MLP-Point-Cloud-Shows-the-Way-Forward" class="headerlink" title="KAN or MLP? Point Cloud Shows the Way Forward"></a>KAN or MLP? Point Cloud Shows the Way Forward</h2><p><strong>Authors:Yan Shi, Qingdong He, Yijun Liu, Xiaoyu Liu, Jingyong Su</strong></p>
<p>Multi-Layer Perceptrons (MLPs) have become one of the fundamental architectural component in point cloud analysis due to its effective feature learning mechanism. However, when processing complex geometric structures in point clouds, MLPs’ fixed activation functions struggle to efficiently capture local geometric features, while suffering from poor parameter efficiency and high model redundancy. In this paper, we propose PointKAN, which applies Kolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate their efficacy in hierarchical feature representation. First, we introduce a Geometric Affine Module (GAM) to transform local features, improving the model’s robustness to geometric variations. Next, in the Local Feature Processing (LFP), a parallel structure extracts both group-level features and global context, providing a rich representation of both fine details and overall structure. Finally, these features are combined and processed in the Global Feature Processing (GFP). By repeating these operations, the receptive field gradually expands, enabling the model to capture complete geometric information of the point cloud. To overcome the high parameter counts and computational inefficiency of standard KANs, we develop Efficient-KANs in the PointKAN-elite variant, which significantly reduces parameters while maintaining accuracy. Experimental results demonstrate that PointKAN outperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN, and ShapeNetPart, with particularly strong performance in Few-shot Learning task. Additionally, PointKAN achieves substantial reductions in parameter counts and computational complexity (FLOPs). This work highlights the potential of KANs-based architectures in 3D vision and opens new avenues for research in point cloud understanding. </p>
<blockquote>
<p>多层感知器（MLPs）由于其有效的特征学习机制，已成为点云分析中的基本架构组件之一。然而，在处理点云中的复杂几何结构时，MLPs的固定激活函数在有效地捕获局部几何特征方面遇到了困难，同时还存在参数效率低和模型冗余度高的问题。在本文中，我们提出了PointKAN，它将Kolmogorov-Arnold网络（KANs）应用于点云分析任务，以研究其在分层特征表示中的有效性。首先，我们引入了一个几何仿射模块（GAM）来变换局部特征，提高模型对几何变化的鲁棒性。接下来，在局部特征处理（LFP）中，并行结构提取了组级特征和全局上下文，提供了对精细细节和整体结构的丰富表示。最后，这些特征在全局特征处理（GFP）中进行组合和处理。通过重复这些操作，感受野逐渐扩大，使模型能够捕获点云的完整几何信息。为了克服标准KANs参数多、计算效率低的缺点，我们在PointKAN-elite变体中都采用了高效KANs结构显著减少了参数数量同时保持了准确性。实验结果表明，PointKAN在ModelNet40、ScanObjectNN和ShapeNetPart等基准数据集上的性能优于PointMLP，特别是在小样本学习任务中表现尤为突出。此外，PointKAN还实现了参数数量和计算复杂度的大幅降低。这项工作突出了基于KANs架构在3D视觉中的潜力，并为点云理解研究开辟了新途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13593v1">PDF</a> </p>
<p><strong>Summary</strong><br>     点云分析中，MLP因有效的特征学习机制成为基本架构之一，但在处理复杂几何结构时存在局限。本文提出PointKAN，应用Kolmogorov-Arnold网络（KANs）进行点云分析任务，并引入几何仿射模块（GAM）改进模型对几何变异的稳健性。通过局部特征处理和全局特征处理，PointKAN能捕获点云的完整几何信息。同时，PointKAN-elite通过开发Efficient-KANs在减少参数的同时保持准确性。实验结果显示PointKAN在ModelNet40、ScanObjectNN和ShapeNetPart等基准数据集上优于PointMLP，特别是在小样学习任务中表现突出。</p>
<p><strong>Key Takeaways</strong><br>     1. MLP在点云分析中是基本架构之一，但在处理复杂几何结构时存在局限性。<br>     2. PointKAN利用Kolmogorov-Arnold网络（KANs）进行点云分析，旨在提高模型对几何特征的学习能力。<br>     3. PointKAN通过几何仿射模块（GAM）增强模型对几何变异的稳健性。<br>     4. 通过局部特征处理和全局特征处理，PointKAN能够捕获点云的完整几何信息。<br>     5. Efficient-KANs的开发使得PointKAN-elite在减少参数和计算复杂性的同时保持了准确性。<br>     6. 实验结果显示PointKAN在多个基准数据集上优于PointMLP，特别是在小样学习任务中表现优异。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13593">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dbcf517c1a3f203737c226f94ef90435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fd89b1af9d0a09a5238203523fdc49e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d96f01b62b78e65427a53e967e53fd5e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MetaDSE-A-Few-shot-Meta-learning-Framework-for-Cross-workload-CPU-Design-Space-Exploration"><a href="#MetaDSE-A-Few-shot-Meta-learning-Framework-for-Cross-workload-CPU-Design-Space-Exploration" class="headerlink" title="MetaDSE: A Few-shot Meta-learning Framework for Cross-workload CPU   Design Space Exploration"></a>MetaDSE: A Few-shot Meta-learning Framework for Cross-workload CPU   Design Space Exploration</h2><p><strong>Authors:Runzhen Xue, Hao Wu, Mingyu Yan, Ziheng Xiao, Xiaochun Ye, Dongrui Fan</strong></p>
<p>Cross-workload design space exploration (DSE) is crucial in CPU architecture design. Existing DSE methods typically employ the transfer learning technique to leverage knowledge from source workloads, aiming to minimize the requirement of target workload simulation. However, these methods struggle with overfitting, data ambiguity, and workload dissimilarity.   To address these challenges, we reframe the cross-workload CPU DSE task as a few-shot meta-learning problem and further introduce MetaDSE. By leveraging model agnostic meta-learning, MetaDSE swiftly adapts to new target workloads, greatly enhancing the efficiency of cross-workload CPU DSE. Additionally, MetaDSE introduces a novel knowledge transfer method called the workload-adaptive architectural mask algorithm, which uncovers the inherent properties of the architecture. Experiments on SPEC CPU 2017 demonstrate that MetaDSE significantly reduces prediction error by 44.3% compared to the state-of-the-art. MetaDSE is open-sourced and available at this \href{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Meta_DSE-02F8%7D%7Banonymous">https://anonymous.4open.science/r/Meta_DSE-02F8}{anonymous</a> GitHub.} </p>
<blockquote>
<p>跨工作负载设计空间探索（DSE）在CPU架构设计中的重要性不言而喻。现有的DSE方法通常采用迁移学习技术，利用源工作负载的知识，旨在减少目标工作负载模拟的需求。然而，这些方法在面临过拟合、数据模糊和工作负载差异时遇到了困难。为了解决这些挑战，我们将跨工作负载CPU的DSE任务重新构建为一个小样本元学习问题，并引入了MetaDSE。通过利用模型无关的元学习，MetaDSE能够迅速适应新的目标工作负载，大大提高了跨工作负载CPU的DSE效率。此外，MetaDSE还引入了一种新的知识转移方法——工作负载自适应架构掩码算法，该算法揭示了架构的内在属性。在SPEC CPU 2017上的实验表明，与最新技术相比，MetaDSE将预测误差降低了44.3%。MetaDSE是开源的，可以在这个匿名的GitHub上找到：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Meta_DSE-02F8">https://anonymous.4open.science/r/Meta_DSE-02F8</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13568v1">PDF</a> 7 pages, 6 figures. Accepted by DAC 2025</p>
<p><strong>Summary</strong></p>
<p>基于跨工作负载设计空间探索（DSE）在CPU架构设计中的重要性，现有方法主要利用迁移学习技术，通过源工作负载的知识来提高效率。然而，这些方法面临过度拟合、数据模糊和工作负载差异等挑战。为解决这些问题，我们提出将跨工作负载CPU DSE任务重新构建为少样本元学习问题，并引入MetaDSE方法。通过利用模型无关的元学习，MetaDSE能快速适应新的目标工作负载，大大提高了跨工作负载CPU DSE的效率。此外，MetaDSE还引入了一种新的知识转移方法——工作负载自适应架构掩码算法，揭示了架构的固有属性。实验表明，与现有技术相比，MetaDSE将预测误差降低了44.3%。MetaDSE已开源，可在匿名GitHub上获得。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨工作负载设计空间探索（DSE）在CPU架构设计中至关重要。</li>
<li>现有DSE方法主要使用迁移学习技术，但面临过度拟合、数据模糊和工作负载差异的挑战。</li>
<li>提出将跨工作负载CPU DSE任务重构为少样本元学习问题。</li>
<li>引入MetaDSE方法，通过模型无关的元学习快速适应新目标工作负载。</li>
<li>MetaDSE采用新的知识转移方法——工作负载自适应架构掩码算法。</li>
<li>实验显示，MetaDSE相较于现有技术，能显著降低预测误差。</li>
<li>MetaDSE已开源，可通过匿名GitHub获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13568">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-865c17aaf40fb7adc6ebd9daf01d77f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f18a95af76a02ffa6feb782d7cd279a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4752afcb182f50d4b0fad5c1ec6aa7ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb21d596a538b2a51ce163b83a32f9d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9d9f534dd6747c54b7059a3d66be688.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a34f499ff70acf851fbb06094e9e67a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b89399939fb8fed7ad11eb740d14b1d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b6a073dc39bb8722604e54db86131d4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model’s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>传统的时间动作定位（TAL）方法依赖于大量的详细标注数据，而少样本TAL方法则通过仅使用少量的训练样本来识别未见过的动作类别，减少了这种依赖。然而，现有的少样本TAL方法通常只关注视频级信息，忽视了文本信息，而文本信息可以为定位任务提供有价值的语义支持。因此，我们提出了一种新的少样本时间动作定位方法，通过链式思维文本推理来改善定位性能。具体来说，我们设计了一种新颖的少样本学习框架，该框架利用文本语义信息来提高模型捕捉动作共性和变化的能力，其中包括一个语义感知的文本-视觉对齐模块，旨在以不同层级对齐查询和支持视频。同时，为了更好地在文本层面表达动作之间的时间依赖关系和因果关系以辅助动作定位，我们设计了一种类似链式思维（CoT）的推理方法，该方法逐步引导视觉语言模型（VLM）和大型语言模型（LLM）为视频生成CoT类似的文本描述。生成的文本可以捕获比视觉特征更多的动作变化。我们在公开可用的ActivityNet1.3和THUMOS14数据集上进行了大量实验。我们引入了名为Human-related Anomaly Localization的数据集，并探索了TAL任务在人类异常检测中的应用。实验结果表明，我们提出的方法在单实例和多实例场景中均显著优于现有方法。我们将发布我们的代码、数据和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于Chain-of-Thought文本推理的少量样本时序动作定位方法。该方法利用文本语义信息，设计了一个全新的少量样本学习框架，以增强模型捕捉动作共性和变化的能力。同时，设计了一个语义感知的文本视觉对齐模块和对齐不同级别的查询和支持视频。此外，为了更好地在文本层面表达动作的时空依赖和因果关系，设计了一种类似Chain of Thought（CoT）的推理方法，逐步引导视觉语言模型和大语言模型生成用于视频的CoT文本描述。在公开数据集ActivityNet1.3和THUMOS14上的实验表明，该方法在单实例和多实例场景中显著优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少量样本时序动作定位（few-shot TAL）方法可减少对传统大量详细标注数据的依赖，仅使用少量训练样本即可识别未见过的动作类别。</li>
<li>现有few-shot TAL方法主要关注视频层级信息，忽略了文本信息，后者可为定位任务提供有价值的语义支持。</li>
<li>提出的基于Chain-of-Thought文本推理的few-shot TAL方法，利用文本语义信息增强模型捕捉动作共性和变化的能力。</li>
<li>语义感知的文本视觉对齐模块设计用于对齐查询和支持视频的不同层级。</li>
<li>设计的CoT推理方法能在文本层面更好地表达动作的时空依赖和因果关系，辅助动作定位。</li>
<li>在ActivityNet1.3和THUMOS14公开数据集上的实验表明，该方法在单实例和多实例场景中显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-55fea05ebbb7e2ed2badab807e563848.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6088af927e869df00f843812c7c55ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c213dbbcc1e0229555b467cf4aa97d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation"><a href="#Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation" class="headerlink" title="Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation"></a>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation</h2><p><strong>Authors:T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough</strong></p>
<p>Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication. </p>
<blockquote>
<p>在现代数字通信中经常可以发现词义模糊的词语。由于数据有限，词汇的模糊性给传统的词义消歧（Word Sense Disambiguation, WSD）方法带来了挑战。因此，这些限制影响了翻译、信息检索和问答系统的效率。本研究探讨了使用大型语言模型（LLMs）结合一种新型的提示增强机制来改善词义消歧问题，该机制结合了一个由不同词义解读构成的知识库（KB）。所提出的方法采用了一种人类循环提示增强方法，该方法通过词性标注（POS）、模糊词的同义词、基于方面的词义过滤和少量提示来支持大型语言模型。通过采用基于少量思维链（COT）提示的方法，本研究展示了显著的性能提升。本研究通过FEWS测试数据和词义标签进行了评估。这项研究推进了社交媒体和数字通信中的准确词汇解读。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18337v2">PDF</a> 12 pages,6 tables, 1 figure, Proceedings of the 1st International   Conference on NLP &amp; AI for Cyber Security</p>
<p><strong>Summary</strong></p>
<p>本文探讨了现代数字通信中常见的词汇歧义问题。由于数据有限，传统的词义消歧（WSD）方法面临挑战。该研究利用大型语言模型（LLMs）结合系统提示增强机制和包含不同词义解读的知识库（KB）来改善词义消歧。方法采用人类参与的提示增强方式，借助词性标注、含糊词汇的同义词、基于方面的词义过滤和少量提示来引导LLM。通过采用基于少量思维的提示方法，该研究展示了显著的性能改进。通过FEWS测试数据和词义标签进行评估，该研究推动了社交媒体和数字通信中的准确词汇解释。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>词汇歧义在现代数字通信中是常见问题。</li>
<li>传统词义消歧（WSD）方法因数据有限而面临挑战。</li>
<li>大型语言模型（LLMs）被用于改善词义消歧。</li>
<li>系统提示增强机制与知识库（KB）相结合来提高词义消歧性能。</li>
<li>人类参与的提示增强方式包括词性标注、同义词、基于方面的词义过滤和少量提示。</li>
<li>采用基于少量思维的提示方法展示显著性能改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cbf709fa9a7dd0b311058c27411f02dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7f227667733ec050b1493a89217bb38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3bbeb948a0dce8ab986295a7dcd69b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f10c3db177cf4a894c58705c963e7489.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e223852dc3312651eaa87167b1aefe61.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-22  Mind2Matter Creating 3D Models from EEG Signals
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6d78f5d29a4192863c752c1cde3e5a13.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-04-22  ChatNekoHacker Real-Time Fan Engagement with Conversational Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24417.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
