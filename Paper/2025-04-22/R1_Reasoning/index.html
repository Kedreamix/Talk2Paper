<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-22-æ›´æ–°"><a href="#2025-04-22-æ›´æ–°" class="headerlink" title="2025-04-22 æ›´æ–°"></a>2025-04-22 æ›´æ–°</h1><h2 id="Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model"><a href="#Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model" class="headerlink" title="Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?"></a>Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?</h2><p><strong>Authors:Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base modelsâ€™ capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$&#x3D;1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base modelsâ€™ sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the modelâ€™s output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: <a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a> </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æœ€è¿‘åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚äººä»¬æ™®éè®¤ä¸ºï¼ŒRLVRä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤ŸæŒç»­è‡ªæˆ‘æ”¹è¿›ï¼Œä»è€Œè·å¾—è¶…è¿‡ç›¸åº”åŸºç¡€æ¨¡å‹å®¹é‡çš„æ–°å‹æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é€šè¿‡æµ‹é‡pass@\emph{k}æŒ‡æ ‡ï¼Œå¹¶è®¾ç½®è¾ƒå¤§çš„\emph{k}å€¼æ¥é‡æ–°è¯„ä¼°è¿™ä¸€å‡è®¾ï¼Œä»¥æ¢ç´¢æ¨¡å‹è·¨å¤šä¸ªæ¨¡å‹å®¶æ—å’ŒåŸºå‡†æµ‹è¯•é›†çš„æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒRLå¹¶æ²¡æœ‰å¼•å‘æ ¹æœ¬æ€§çš„æ–°æ¨ç†æ¨¡å¼ã€‚å°½ç®¡åœ¨è¾ƒå°çš„kå€¼ï¼ˆä¾‹å¦‚k&#x3D;1ï¼‰ä¸‹ï¼ŒRLè®­ç»ƒçš„æ¨¡å‹ä¼˜äºåŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹ç”šè‡³å¯ä»¥å–å¾—ä¸RLè®­ç»ƒæ¨¡å‹ç›¸å½“çš„ç”šè‡³æ›´é«˜çš„pass@kåˆ†æ•°ã€‚RLè®­ç»ƒæ¨¡å‹äº§ç”Ÿçš„æ¨ç†è·¯å¾„å·²åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ï¼Œè¿™è¡¨æ˜RLè®­ç»ƒæ¨¡å‹ä¸­å±•ç°çš„å¤§å¤šæ•°æ¨ç†èƒ½åŠ›å·²è¢«åŸºç¡€æ¨¡å‹æ‰€è·å¾—ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒRLè®­ç»ƒé€šè¿‡ä½¿æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåå‘äºæ›´å¯èƒ½äº§ç”Ÿå¥–åŠ±çš„è·¯å¾„ï¼Œä»è€Œæé«˜æ€§èƒ½ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°æŠ½å–æ­£ç¡®å“åº”ã€‚ä½†è¿™ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œä¹Ÿå¯¼è‡´äº†è¾ƒçª„çš„æ¨ç†èƒ½åŠ›è¾¹ç•Œã€‚åœ¨RLVRè®­ç»ƒçš„è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè§‚å¯Ÿåˆ°ç±»ä¼¼çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è’¸é¦æ³•å¯ä»¥çœŸæ­£åœ°å°†æ–°çŸ¥è¯†å¼•å…¥æ¨¡å‹ä¸­ï¼Œè¿™ä¸RLVRä¸åŒã€‚è¿™äº›å‘ç°çªå‡ºäº†RLVRåœ¨æ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œè¿™è¦æ±‚æˆ‘ä»¬ä»æ ¹æœ¬ä¸Šé‡æ–°æ€è€ƒRLè®­ç»ƒåœ¨æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä½œç”¨ä»¥åŠæ˜¯å¦éœ€è¦æ›´å¥½çš„èŒƒå¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13837v1">PDF</a> 24 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±éªŒè¯ï¼ˆRLVRï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œæœ¬ç ”ç©¶é€šè¿‡æµ‹é‡pass@\textit{k}æŒ‡æ ‡å¯¹å¤§æ¨¡å‹è¿›è¡Œæ‰¹åˆ¤æ€§è€ƒå¯Ÿï¼Œå‘ç°åœ¨å®½æ³›çš„æ¨¡å‹å®¶æ—å’ŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¼ºåŒ–å­¦ä¹ å¹¶ä¸æ¿€å‘æ–°çš„æ¨ç†æ¨¡å¼ã€‚åœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„æ€§èƒ½å¯ä¸å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç›¸å½“ç”šè‡³æ›´é«˜ã€‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒä½¿æ¨¡å‹è¾“å‡ºåˆ†å¸ƒåå‘äºæ›´æ˜“äº§ç”Ÿå¥–åŠ±çš„è·¯å¾„ï¼Œä»è€Œæé«˜æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿå¯¼è‡´ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ¨ç†èƒ½åŠ›è¾¹ç•Œè¾ƒçª„ã€‚ç±»ä¼¼çš„ç»“æœä¹Ÿå‡ºç°åœ¨ä½¿ç”¨RLVRè®­ç»ƒçš„è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°è’¸é¦å¯ä»¥çœŸæ­£åœ°ä¸ºæ¨¡å‹å¼•å…¥æ–°çŸ¥è¯†ï¼Œä¸RLVRä¸åŒã€‚è¿™äº›å‘ç°çªæ˜¾äº†RLVRåœ¨æå‡LLMæ¨ç†èƒ½åŠ›æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œéœ€è¦æˆ‘ä»¬å¯¹å¼ºåŒ–å­¦ä¹ åœ¨LLMæ¨ç†ä¸­çš„å½±å“è¿›è¡Œæ ¹æœ¬æ€§çš„é‡æ–°æ€è€ƒï¼Œå¹¶éœ€è¦æ›´å¥½çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±éªŒè¯ï¼ˆRLVRï¼‰å¢å¼ºäº†LLMåœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æµ‹é‡pass@\textit{k}æŒ‡æ ‡å‘ç°ï¼Œåœ¨è¾ƒå¤§çš„kå€¼ä¸‹ï¼ŒåŸºç¡€æ¨¡å‹çš„æ€§èƒ½å¯ä¸å¼ºåŒ–å­¦ä¹ æ¨¡å‹ç›¸å½“æˆ–æ›´é«˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒä½¿æ¨¡å‹è¾“å‡ºåå‘äºæ›´å®¹æ˜“äº§ç”Ÿå¥–åŠ±çš„è·¯å¾„ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å¹¶ä¸æ¿€å‘æ–°çš„æ¨ç†æ¨¡å¼ï¼Œå…¶å±•ç°çš„æ¨ç†è·¯å¾„å·²åŒ…å«åœ¨åŸºç¡€æ¨¡å‹çš„é‡‡æ ·åˆ†å¸ƒä¸­ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹æ¨ç†èƒ½åŠ›è¾¹ç•Œè¾ƒçª„ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹ã€‚</li>
<li>åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨RLVRè®­ç»ƒä¹Ÿå‡ºç°äº†ç±»ä¼¼çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1145fa10a074c5138153deb614a9a7b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c356693220910067cc819f4d5fdaea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e6f577c8731eed685abeaa9d193db2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da3f9fac7a8a67e71f5c7441b7c8984a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Intention-Grounding-for-Egocentric-Assistants"><a href="#Visual-Intention-Grounding-for-Egocentric-Assistants" class="headerlink" title="Visual Intention Grounding for Egocentric Assistants"></a>Visual Intention Grounding for Egocentric Assistants</h2><p><strong>Authors:Pengzhan Sun, Junbin Xiao, Tze Ho Elden Tse, Yicong Li, Arjun Akula, Angela Yao</strong></p>
<p>Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts â€“ inputs are egocentric, and objects may be referred to implicitly through needs and intentions. To bridge this gap, we introduce EgoIntention, the first dataset for egocentric visual intention grounding. EgoIntention challenges multimodal LLMs to 1) understand and ignore unintended contextual objects and 2) reason about uncommon object functionalities. Benchmark results show that current models misidentify context objects and lack affordance understanding in egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it enables hybrid training with normal descriptions and egocentric intentions with a chained intention reasoning and object grounding mechanism. RoG significantly outperforms naive finetuning and hybrid training on EgoIntention, while maintaining or slightly improving naive description grounding. This advancement enables unified visual grounding for egocentric and exocentric visual inputs while handling explicit object queries and implicit human intentions. </p>
<blockquote>
<p>è§†è§‰å®šä½æ˜¯å°†æ–‡æœ¬æè¿°ä¸å›¾åƒä¸­çš„ç‰©ä½“å…³è”èµ·æ¥ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¸»è¦é’ˆå¯¹ç¬¬ä¸‰äººç§°å›¾åƒè¾“å…¥å’Œå‘½åç‰©ä½“æŸ¥è¯¢ã€‚åœ¨äººå·¥æ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨ä¸­ï¼Œè§†è§’ä¼šå‘ç”Ÿå˜åŒ–â€”â€”è¾“å…¥æ˜¯è‡ªæˆ‘ä¸­å¿ƒçš„ï¼Œç‰©ä½“å¯èƒ½é€šè¿‡éœ€æ±‚å’Œæ„å›¾è¢«éšå«åœ°æåŠã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoIntentionï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹è‡ªæˆ‘ä¸­å¿ƒè§†è§‰æ„å›¾å®šä½çš„æ•°æ®é›†ã€‚EgoIntentionæŒ‘æˆ˜äº†è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç†è§£å’Œå¿½ç•¥éä¸Šä¸‹æ–‡ä¸­çš„ç‰©ä½“ï¼Œå¹¶å¯¹ä¸å¯»å¸¸ç‰©ä½“çš„åŠŸèƒ½è¿›è¡Œæ¨ç†ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹ä¼šé”™è¯¯è¯†åˆ«ä¸Šä¸‹æ–‡ä¸­çš„ç‰©ä½“ï¼Œåœ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§’ä¸­ç¼ºä¹åŠŸèƒ½ç†è§£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Reason-to-Groundï¼ˆRoGï¼‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ï¼›å®ƒé€šè¿‡æ··åˆæ­£å¸¸æè¿°å’Œè‡ªæˆ‘ä¸­å¿ƒæ„å›¾çš„æ··åˆè®­ç»ƒï¼Œä»¥åŠé“¾å¼æ„å›¾æ¨ç†å’Œç‰©ä½“å®šä½æœºåˆ¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ•ˆæœã€‚åœ¨EgoIntentionæ•°æ®é›†ä¸Šï¼ŒRoGç›¸è¾ƒäºç®€å•å¾®è°ƒæ¨¡å‹å’Œæ··åˆè®­ç»ƒæ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶åœ¨ç®€å•çš„æè¿°å®šä½ä¸Šèƒ½å¤Ÿä¿æŒæˆ–ç¨å¾®æé«˜æ€§èƒ½ã€‚è¿™ä¸€è¿›å±•ä½¿ç»Ÿä¸€å¤„ç†è‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒçš„è§†è§‰è¾“å…¥ï¼Œä»¥åŠå¤„ç†æ˜¾æ€§ç‰©ä½“æŸ¥è¯¢å’Œéšæ€§äººç±»æ„å›¾æˆä¸ºå¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å®šä½æ˜¯å°†æ–‡æœ¬æè¿°ä¸å›¾åƒä¸­çš„ç‰©ä½“ç›¸å…³è”ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é’ˆå¯¹ç¬¬ä¸‰äººç§°å›¾åƒè¾“å…¥å’Œå‘½åç‰©ä½“æŸ¥è¯¢ã€‚åœ¨äººå·¥æ™ºèƒ½åŠ©æ‰‹ç­‰åº”ç”¨ä¸­ï¼Œè§†è§’ä¼šå‘ç”Ÿå˜åŒ–â€”â€”è¾“å…¥æ˜¯ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ï¼Œç‰©ä½“å¯èƒ½é€šè¿‡éœ€æ±‚å’Œæ„å›¾éšå«åœ°æŒ‡ä»£ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoIntentionï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¯è§†æ„å›¾å®šä½çš„æ•°æ®é›†ã€‚EgoIntentionæŒ‘æˆ˜äº†è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥ç†è§£å’Œå¿½ç•¥æ— æ„ä¸­çš„ä¸Šä¸‹æ–‡ç‰©ä½“ä»¥åŠæ¨ç†ä¸å¯»å¸¸çš„ç‰©ä½“åŠŸèƒ½ã€‚åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨è¯†åˆ«ä¸Šä¸‹æ–‡ç‰©ä½“æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§‚ç‚¹ä¸­ç¼ºä¹æ‰¿æ‹…ç†è§£çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†Reason-to-Groundï¼ˆRoGï¼‰æŒ‡ä»¤è°ƒæ•´ï¼›å®ƒé€šè¿‡æ··åˆæ­£å¸¸æè¿°å’Œä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ„å›¾è¿›è¡Œæ··åˆè®­ç»ƒï¼Œå¹¶å¸¦æœ‰è¿é”æ„å›¾æ¨ç†å’Œç‰©ä½“å®šä½æœºåˆ¶ã€‚RoGåœ¨EgoIntentionä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç®€å•çš„å¾®è°ƒå™¨å’Œæ··åˆè®­ç»ƒï¼ŒåŒæ—¶åœ¨æè¿°å®šä½æ–¹é¢ä¿æŒæˆ–ç•¥æœ‰æ”¹è¿›ã€‚è¿™ä¸€è¿›å±•ä½¿å¾—ç»Ÿä¸€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒçš„å¯è§†è¾“å…¥çš„å®šä½æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶å¤„ç†æ˜ç¡®çš„ç‰©ä½“æŸ¥è¯¢å’Œéšå«çš„äººç±»æ„å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å®šä½ç»“åˆäº†æ–‡æœ¬æè¿°å’Œå›¾åƒä¸­çš„ç‰©ä½“ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç¬¬ä¸‰äººç§°å›¾åƒè¾“å…¥å’Œå‘½åç‰©ä½“æŸ¥è¯¢ä¸Šã€‚</li>
<li>åœ¨å¦‚AIåŠ©æ‰‹ç­‰åº”ç”¨ä¸­ï¼Œè§†è§’è½¬å˜ä¸ºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼Œç‰©ä½“é€šè¿‡éœ€æ±‚å’Œæ„å›¾éšå«æŒ‡ä»£ã€‚</li>
<li>EgoIntentionæ•°æ®é›†è§£å†³äº†ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„å¯è§†æ„å›¾å®šä½çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨è¯†åˆ«ä¸Šä¸‹æ–‡ç‰©ä½“å’Œç†è§£ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†è§’ä¸­çš„ç‰©ä½“åŠŸèƒ½æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>Reason-to-Groundï¼ˆRoGï¼‰æŒ‡ä»¤è°ƒæ•´é€šè¿‡æ··åˆè®­ç»ƒæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5eb2593b989df76d796c3f26fa1dc8ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87de39a59ddf9336ae7bea7bf83701eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465609808cd39e8d9f9fc736226f9a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79aa231b6446fc0a0477843d87cc44de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1714d1c0f1d900ab5e9f241433dfb2b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cadfb8e03dbecd652e763b43040641.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Compile-Scene-Graphs-with-Reinforcement-Learning"><a href="#Compile-Scene-Graphs-with-Reinforcement-Learning" class="headerlink" title="Compile Scene Graphs with Reinforcement Learning"></a>Compile Scene Graphs with Reinforcement Learning</h2><p><strong>Authors:Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen</strong></p>
<p>Next token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. Given the structured nature of scene graphs, we design a graph-centric reward function that integrates node-level rewards, edge-level rewards, and a format consistency reward. Our experiments demonstrate that rule-based RL substantially enhances model performance in the SGG task, achieving a zero failure rateâ€“unlike supervised fine-tuning (SFT), which struggles to generalize effectively. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG">https://github.com/gpt4vision/R1-SGG</a>. </p>
<blockquote>
<p>æ¥ä¸‹æ¥ä»¤ç‰Œé¢„æµ‹æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬åŸåˆ™ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥å¢å¼ºäº†å…¶æ¨ç†æ€§èƒ½ã€‚ä½œä¸ºå¯¹è¯­è¨€ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§æ¨¡å¼è¿›è¡Œæœ‰æ•ˆå»ºæ¨¡çš„ä¸€ç§æ‰‹æ®µï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºæå–ï¼ˆå¦‚åœºæ™¯å›¾ï¼‰ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚è¿™éœ€è¦æ¨¡å‹å‡†ç¡®ç”Ÿæˆä¸€ç»„å¯¹è±¡å’Œå…³ç³»ä¸‰å…ƒç»„ï¼Œè€Œä¸æ˜¯é€ä¸ªç”Ÿæˆæ–‡æœ¬ä»¤ç‰Œã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†R1-SGGï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆM-LLMï¼‰ï¼Œæœ€åˆé€šè¿‡åœºæ™¯å›¾æ•°æ®é›†ä¸Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œéšåä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œç²¾ç‚¼ï¼Œä»¥æé«˜å…¶ç«¯åˆ°ç«¯ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚SFTéµå¾ªä¼ ç»Ÿçš„æç¤º-å“åº”èŒƒå¼ï¼Œè€ŒRLéœ€è¦è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±ä¿¡å·ã€‚è€ƒè™‘åˆ°åœºæ™¯å›¾çš„ç»“æ„æ€§ç‰¹ç‚¹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä»¥å›¾å½¢ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°ï¼Œå®ƒç»“åˆäº†èŠ‚ç‚¹çº§å¥–åŠ±ã€è¾¹ç¼˜çº§å¥–åŠ±å’Œæ ¼å¼ä¸€è‡´æ€§å¥–åŠ±ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºè§„åˆ™çš„RLåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæé«˜äº†SGGä»»åŠ¡ä¸­çš„æ¨¡å‹æ€§èƒ½ï¼Œå®ç°äº†é›¶å¤±è´¥ç‡â€”â€”è¿™ä¸åœ¨æœ‰æ•ˆæ¨å¹¿æ–¹é¢è¡¨ç°æŒ£æ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å½¢æˆé²œæ˜å¯¹æ¯”ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gpt4vision/R1-SGGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡‡ç”¨ä¸‹ä¸€ä»£ä»¤ç‰Œé¢„æµ‹åŸåˆ™è¿›è¡Œè®­ç»ƒï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›ä¸€æ­¥æé«˜å…¶æ¨ç†æ€§èƒ½ã€‚å°½ç®¡LLMåœ¨ç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼ˆå¦‚åœºæ™¯å›¾ï¼‰æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä½†ç›¸å…³ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬ç ”ç©¶å¼•å…¥R1-SGGï¼Œä¸€ç§å¤šæ¨¡æ€LLMï¼ˆM-LLMï¼‰ï¼Œé‡‡ç”¨åœºæ™¯å›¾æ•°æ®é›†è¿›è¡ŒåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„åˆæ­¥è®­ç»ƒï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œæ—¨åœ¨æé«˜å…¶ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚ç»“åˆèŠ‚ç‚¹çº§åˆ«å’Œè¾¹ç¼˜çº§åˆ«çš„å¥–åŠ±ä»¥åŠæ ¼å¼ä¸€è‡´æ€§å¥–åŠ±ï¼Œè®¾è®¡äº†ä¸€ä¸ªä»¥å›¾å½¢ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºè§„åˆ™çš„RLåœ¨SGGä»»åŠ¡ä¸­æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œè¾¾åˆ°é›¶æ•…éšœç‡ã€‚ä»£ç å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºä¸‹ä¸€ä»£ä»¤ç‰Œé¢„æµ‹åŸåˆ™è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºLLMçš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>LLMsåœ¨ç«¯åˆ°ç«¯æå–ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼ˆå¦‚åœºæ™¯å›¾ï¼‰æ–¹é¢çš„åº”ç”¨å°šå¾…æ¢ç´¢ã€‚</li>
<li>R1-SGGæ˜¯ä¸€ç§å¤šæ¨¡æ€LLMï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆåœºæ™¯å›¾çš„èƒ½åŠ›ã€‚</li>
<li>R1-SGGé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆæ­¥è®­ç»ƒï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªä»¥å›¾å½¢ä¸ºä¸­å¿ƒçš„å¥–åŠ±å‡½æ•°ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹çº§åˆ«ã€è¾¹ç¼˜çº§åˆ«å’Œæ ¼å¼ä¸€è‡´æ€§å¥–åŠ±ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-679f83e66c9dedde7c38285f0b1755c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3815c4e5f4e07df358d37321a015b84f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CoT-RAG-Integrating-Chain-of-Thought-and-Retrieval-Augmented-Generation-to-Enhance-Reasoning-in-Large-Language-Models"><a href="#CoT-RAG-Integrating-Chain-of-Thought-and-Retrieval-Augmented-Generation-to-Enhance-Reasoning-in-Large-Language-Models" class="headerlink" title="CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation   to Enhance Reasoning in Large Language Models"></a>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation   to Enhance Reasoning in Large Language Models</h2><p><strong>Authors:Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui</strong></p>
<p>While chain-of-thought (CoT) reasoning improves the performance of large language models (LLMs) in complex tasks, it still has two main challenges: the low reliability of relying solely on LLMs to generate reasoning chains and the interference of natural language reasoning chains on the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor. We conduct a comprehensive evaluation on nine public datasets, covering three reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%. Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable accuracy and efficient execution, highlighting its strong practical applicability and scalability. </p>
<blockquote>
<p>è™½ç„¶é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œä½†å®ƒä»ç„¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä»…ä¾èµ–LLMç”Ÿæˆæ¨ç†é“¾çš„ä½å¯é æ€§ä»¥åŠè‡ªç„¶è¯­è¨€æ¨ç†é“¾å¯¹LLMæ¨ç†é€»è¾‘çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoT-RAGè¿™ä¸€æ–°å‹æ¨ç†æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸‰ä¸ªå…³é”®è®¾è®¡ï¼šï¼ˆiï¼‰çŸ¥è¯†å›¾è°±é©±åŠ¨çš„CoTç”Ÿæˆï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±æ¥è°ƒèŠ‚LLMçš„æ¨ç†é“¾ç”Ÿæˆï¼Œä»è€Œæé«˜æ¨ç†çš„å¯ä¿¡åº¦ï¼›ï¼ˆiiï¼‰å¯å­¦ä¹ çš„çŸ¥è¯†æ¡ˆä¾‹æ„ŸçŸ¥RAGï¼Œå°†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çº³å…¥çŸ¥è¯†å›¾è°±ï¼Œä»¥æ£€ç´¢ç›¸å…³çš„å­æ¡ˆä¾‹å’Œå­æè¿°ï¼Œä¸ºLLMæä¾›å¯å­¦ä¹ çš„ä¿¡æ¯ï¼›ï¼ˆiiiï¼‰ä¼ªç¨‹åºæç¤ºæ‰§è¡Œï¼Œé¼“åŠ±LLMåœ¨ä¼ªç¨‹åºä¸­æ‰§è¡Œæ¨ç†ä»»åŠ¡ï¼Œæé«˜é€»è¾‘ä¸¥è°¨æ€§ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¶µç›–ä¸‰ç§æ¨ç†é—®é¢˜ã€‚ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCoT-RAGåœ¨å‡†ç¡®ç‡ä¸Šæœ‰äº†æ˜¾è‘—çš„æé«˜ï¼ŒèŒƒå›´åœ¨4.0%åˆ°23.0%ä¹‹é—´ã€‚æ­¤å¤–ï¼Œåœ¨å››ä¸ªé¢†åŸŸç‰¹å®šçš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼ŒCoT-RAGæ˜¾ç¤ºå‡ºå“è¶Šçš„å‡†ç¡®æ€§å’Œé«˜æ•ˆçš„æ‰§è¡ŒåŠ›ï¼Œçªæ˜¾äº†å…¶å¼ºå¤§çš„å®ç”¨æ€§ã€é€‚ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13534v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è™½èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä½†ä»é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºCoT-RAGè¿™ä¸€æ–°å‹æ¨ç†æ¡†æ¶ï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±é©±åŠ¨CoTç”Ÿæˆã€å¯å­¦ä¹ çŸ¥è¯†æ¡ˆä¾‹æ„ŸçŸ¥RAGå’Œä¼ªç¨‹åºæç¤ºæ‰§è¡Œä¸‰ä¸ªå…³é”®è®¾è®¡ã€‚åœ¨ä¹ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒCoT-RAGçš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼ŒèŒƒå›´åœ¨4.0%åˆ°23.0%ä¹‹é—´ã€‚åœ¨å››ä¸ªé¢†åŸŸç‰¹å®šæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¿›ä¸€æ­¥å‡¸æ˜¾äº†å…¶å¼ºå¤§çš„å®ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œé«˜æ•ˆæ‰§è¡Œã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è™½æœ‰ä¼˜åŠ¿ï¼Œä½†ä»å­˜åœ¨å¯é æ€§ä½å’Œå¹²æ‰°æ¨ç†é“¾ç”Ÿæˆçš„é—®é¢˜ã€‚</li>
<li>æå‡ºCoT-RAGæ–°å‹æ¨ç†æ¡†æ¶ï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±é©±åŠ¨CoTç”Ÿæˆã€å¯å­¦ä¹ çŸ¥è¯†æ¡ˆä¾‹æ„ŸçŸ¥RAGå’Œä¼ªç¨‹åºæç¤ºæ‰§è¡Œä¸‰å¤§å…³é”®è®¾è®¡ã€‚</li>
<li>çŸ¥è¯†å›¾è°±ç”¨äºè°ƒåˆ¶LLMçš„æ¨ç†é“¾ç”Ÿæˆï¼Œæé«˜æ¨ç†å¯ä¿¡åº¦ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯è¢«çº³å…¥çŸ¥è¯†å›¾è°±ä¸­ï¼Œå¯æ£€ç´¢ç›¸å…³å­æ¡ˆä¾‹å’Œå­æè¿°ï¼Œä¸ºLLMæä¾›å¯å­¦ä¹ ä¿¡æ¯ã€‚</li>
<li>CoT-RAGé‡‡ç”¨ä¼ªç¨‹åºæç¤ºæ‰§è¡Œï¼Œé¼“åŠ±LLMåœ¨ä¼ªç¨‹åºä¸­æ‰§è¡Œæ¨ç†ä»»åŠ¡ï¼Œæé«˜é€»è¾‘ä¸¥è°¨æ€§ã€‚</li>
<li>åœ¨ä¹ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCoT-RAGç›¸æ¯”æœ€å…ˆè¿›çš„æ–¹æ³•å‡†ç¡®ç‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8075d9a5056ec2344954ec40883596bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12322fccc3c499be3d9118b81e987b09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d0131c37711eaae57953e5fca9b4465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c402f67fdb1347387993ebc453e23c3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SolarZip-An-Efficient-and-Adaptive-Compression-Framework-for-solar-EUV-imaging-data"><a href="#SolarZip-An-Efficient-and-Adaptive-Compression-Framework-for-solar-EUV-imaging-data" class="headerlink" title="SolarZip: An Efficient and Adaptive Compression Framework for solar EUV   imaging data"></a>SolarZip: An Efficient and Adaptive Compression Framework for solar EUV   imaging data</h2><p><strong>Authors:Zedong Liu, Song Tan, Alexander Warmuth, FrÃ©dÃ©ric Schuller, Yun Hong, Wenjing Huang, Yida Gu, Bojing Zhu, Guangming Tan, Dingwen Tao</strong></p>
<p>Context: With the advancement of solar physics research, next-generation solar space missions and ground-based telescopes face significant challenges in efficiently transmitting and&#x2F;or storing large-scale observational data. Aims: We develop an efficient compression and evaluation framework for solar EUV data, specifically optimized for Solar Orbiter Extreme Ultraviolet Imager (EUI) data, significantly reducing data volume while preserving scientific usability. Methods: We systematically evaluated four error-bounded lossy compressors across two EUI datasets. However, the existing methods cannot perfectly handle the EUI datasets (with continuously changing distance and significant resolution differences). Motivated by this, we develop an adaptive hybrid compression strategy with optimized interpolation predictors. Moreover, we designed a two-stage evaluation framework integrating distortion analysis with downstream scientific workflows, ensuring that observational analysis is not affected at high compression ratios. Results: Our framework SolarZip achieved up to 800x reduction for Full Sun Imager (FSI) data and 500x for High Resolution Imager (HRI$_{\text{EUV}}$) data. It significantly outperformed both traditional and advanced algorithms, achieving 3-50x higher compression ratios than traditional algorithms, surpassing the second-best algorithm by up to 30%. Simulation experiments verified that SolarZip can reduce data transmission time by up to 270x while ensuring the preservation of scientific usability. Conclusions: The SolarZip framework significantly enhances solar observational data compression efficiency while preserving scientific usability by dynamically selecting optimal compression methods based on observational scenarios and user requirements. This provides a promising data management solution for deep space missions like Solar Orbiter. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šéšç€å¤ªé˜³ç‰©ç†å­¦ç ”ç©¶çš„è¿›æ­¥ï¼Œä¸‹ä¸€ä»£å¤ªé˜³ç©ºé—´ä»»åŠ¡å’Œåœ°é¢æœ›è¿œé•œåœ¨æœ‰æ•ˆåœ°ä¼ è¾“å’Œ&#x2F;æˆ–å­˜å‚¨å¤§è§„æ¨¡è§‚æµ‹æ•°æ®æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç›®æ ‡ï¼šæˆ‘ä»¬ä¸ºå¤ªé˜³è½¨é“å™¨æç´«å¤–æˆåƒä»ªï¼ˆEUIï¼‰æ•°æ®å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆå‹ç¼©å’Œè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨å‡å°‘æ•°æ®é‡çš„åŒæ—¶ä¿æŒæ•°æ®çš„ç§‘å­¦å¯ç”¨æ€§ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†å››ä¸ªè¯¯å·®æœ‰ç•Œçš„æœ‰æŸå‹ç¼©æœºï¼Œè·¨è¶Šä¸¤ä¸ªEUIæ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•å®Œç¾å¤„ç†EUIæ•°æ®é›†ï¼ˆå…·æœ‰è¿ç»­å˜åŒ–çš„è·ç¦»å’Œæ˜¾è‘—çš„åˆ†è¾¨ç‡å·®å¼‚ï¼‰ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¸¦æœ‰ä¼˜åŒ–æ’å€¼é¢„æµ‹å™¨çš„è‡ªé€‚åº”æ··åˆå‹ç¼©ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ï¼Œå°†å¤±çœŸåˆ†æä¸ä¸‹æ¸¸ç§‘å­¦å·¥ä½œæµç¨‹ç›¸ç»“åˆï¼Œç¡®ä¿åœ¨é«˜å‹ç¼©æ¯”ä¸‹è§‚æµ‹åˆ†æä¸å—å½±å“ã€‚ç»“æœï¼šæˆ‘ä»¬çš„SolarZipæ¡†æ¶å®ç°äº†å¯¹å…¨æ—¥æˆåƒä»ªï¼ˆFSIï¼‰æ•°æ®é«˜è¾¾800å€çš„å‹ç¼©æ¯”ï¼Œè€Œå¯¹é«˜åˆ†è¾¨ç‡æˆåƒä»ªï¼ˆHRI$_{EUV}$ï¼‰æ•°æ®åˆ™å®ç°äº†é«˜è¾¾500å€çš„å‹ç¼©æ¯”ã€‚ä¸ä¼ ç»Ÿçš„å…ˆè¿›ç®—æ³•ç›¸æ¯”ï¼Œå®ƒå¤§å¤§ä¼˜äºè¿™äº›ç®—æ³•ï¼Œå®ç°äº†é«˜è¾¾ä¼ ç»Ÿç®—æ³•3è‡³50å€çš„é«˜å‹ç¼©æ¯”ï¼Œå¹¶è¶…è¿‡äº†ç¬¬äºŒå¥½çš„ç®—æ³•é«˜è¾¾30%ã€‚æ¨¡æ‹Ÿå®éªŒè¯å®ï¼ŒSolarZipèƒ½åœ¨ç¡®ä¿ç§‘å­¦å¯ç”¨æ€§çš„å‰æä¸‹ï¼Œå°†æ•°æ®ä¼ è¾“æ—¶é—´ç¼©çŸ­é«˜è¾¾270å€ã€‚ç»“è®ºï¼šSolarZipæ¡†æ¶é€šè¿‡æ ¹æ®è§‚æµ‹åœºæ™¯å’Œç”¨æˆ·è¦æ±‚åŠ¨æ€é€‰æ‹©æœ€ä½³å‹ç¼©æ–¹æ³•ï¼Œåœ¨ä¿æŒç§‘å­¦å¯ç”¨æ€§çš„åŒæ—¶å¤§å¤§æé«˜äº†å¤ªé˜³è§‚æµ‹æ•°æ®çš„å‹ç¼©æ•ˆç‡ã€‚è¿™ä¸ºæ·±ç©ºä»»åŠ¡å¦‚å¤ªé˜³èƒ½è½¨é“å™¨æä¾›äº†æœ‰å‰æ™¯çš„æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13504v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤ªé˜³ç‰©ç†å­¦ç ”ç©¶çš„è¿›å±•ï¼Œä¸‹ä¸€ä»£å¤ªç©ºä»»åŠ¡å’Œåœ°é¢æœ›è¿œé•œåœ¨ä¼ è¾“å’Œå­˜å‚¨å¤§è§„æ¨¡è§‚æµ‹æ•°æ®æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ç§é’ˆå¯¹å¤ªé˜³è½¨é“å™¨æç´«å¤–æˆåƒä»ªï¼ˆEUIï¼‰æ•°æ®çš„å‹ç¼©ä¸è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘æ•°æ®é‡çš„åŒæ—¶ä¿æŒç§‘å­¦å¯ç”¨æ€§ã€‚é€šè¿‡å¯¹å››ç§è¯¯å·®é™åˆ¶æœ‰æŸå‹ç¼©å™¨çš„ç³»ç»Ÿè¯„ä¼°ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”æ··åˆå‹ç¼©ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†åŒ…å«å¤±çœŸåˆ†æå’Œä¸‹æ¸¸ç§‘å­¦å·¥ä½œæµç¨‹çš„ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ã€‚ç»“æœæ˜¾ç¤ºï¼ŒSolarZipæ¡†æ¶å¯¹å…¨æ—¥æˆåƒä»ªï¼ˆFSIï¼‰æ•°æ®çš„å‹ç¼©æ¯”è¾¾åˆ°800å€ï¼Œå¯¹é«˜åˆ†è¾¨ç‡æç´«å¤–æˆåƒä»ªï¼ˆHRIï¼‰æ•°æ®çš„å‹ç¼©æ¯”ä¸º500å€ã€‚ä¸ä¼ ç»Ÿçš„å…ˆè¿›ç®—æ³•ç›¸æ¯”ï¼ŒSolarZipæ¡†æ¶å‹ç¼©æ•ˆç‡æ›´é«˜ï¼Œä¸”èƒ½ä¿æŒç§‘å­¦å¯ç”¨æ€§ã€‚æ­¤å¤–ï¼ŒSolarZipèƒ½å‡å°‘æ•°æ®ä¼ è¾“æ—¶é—´é«˜è¾¾270å€ã€‚ç»“è®ºï¼šSolarZipæ¡†æ¶é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ä½³å‹ç¼©æ–¹æ³•ï¼Œæé«˜äº†å¤ªé˜³è§‚æµ‹æ•°æ®çš„å‹ç¼©æ•ˆç‡å¹¶ä¿æŒäº†ç§‘å­¦å¯ç”¨æ€§ï¼Œä¸ºæ·±ç©ºä»»åŠ¡å¦‚Solar Orbiteræä¾›äº†æœ‰å‰æ™¯çš„æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SolarZipæ¡†æ¶æ—¨åœ¨ä¼˜åŒ–ä¸‹ä¸€ä»£å¤ªé˜³è§‚æµ‹æ•°æ®çš„å‹ç¼©æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤ªé˜³è½¨é“å™¨æç´«å¤–æˆåƒä»ªï¼ˆEUIï¼‰æ•°æ®æ—¶æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>SolarZipæ¡†æ¶é€šè¿‡å¼€å‘è‡ªé€‚åº”æ··åˆå‹ç¼©ç­–ç•¥ï¼Œä¼˜åŒ–äº†æ’å€¼é¢„æµ‹å™¨ï¼Œæé«˜äº†æ•°æ®å‹ç¼©æ¯”ã€‚</li>
<li>SolarZipæ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ç¡®ä¿äº†å‹ç¼©æ•°æ®çš„è´¨é‡å’Œç§‘å­¦å¯ç”¨æ€§ï¼ŒåŒ…æ‹¬å¤±çœŸåˆ†æå’Œä¸‹æ¸¸ç§‘å­¦å·¥ä½œæµç¨‹çš„é›†æˆã€‚</li>
<li>SolarZipå®ç°äº†é«˜è¾¾800å€çš„FSIæ•°æ®å‹ç¼©æ¯”å’Œ500å€çš„HRIæ•°æ®å‹ç¼©æ¯”ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’Œå…ˆè¿›ç®—æ³•ã€‚</li>
<li>SolarZipæ¡†æ¶åœ¨ä¿è¯ç§‘å­¦è´¨é‡çš„å‰æä¸‹ï¼Œèƒ½æœ‰æ•ˆå‡å°‘æ•°æ®ä¼ è¾“æ—¶é—´ï¼Œæœ€é«˜å¯è¾¾270å€ã€‚</li>
<li>SolarZipæ¡†æ¶æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºéœ€è¦é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡è§‚æµ‹æ•°æ®çš„æ·±ç©ºä»»åŠ¡ï¼Œå¦‚Solar Orbiterã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bc4bbb22bbcdeb146bff6dd08aebbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edf7280ad144aced663ea72e86a8406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b3d16131fb4de6b6ee96725ee15e6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bcc25d336440cc72758051a95cfd885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f389cf336e2b6b4d2ca743b938d7c94.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prejudge-Before-Think-Enhancing-Large-Language-Models-at-Test-Time-by-Process-Prejudge-Reasoning"><a href="#Prejudge-Before-Think-Enhancing-Large-Language-Models-at-Test-Time-by-Process-Prejudge-Reasoning" class="headerlink" title="Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by   Process Prejudge Reasoning"></a>Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by   Process Prejudge Reasoning</h2><p><strong>Authors:Jianing Wang, Jin Jiang, Yang Liu, Mengdi Zhang, Xunliang Cai</strong></p>
<p>In this paper, we introduce a new \emph{process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent reasoning steps, similar to people sometimes pausing to think about what mistakes may occur and how to avoid them, rather than relying solely on trial and error. Specifically, we define a prejudge node in the rationale, which represents a reasoning step, with at least one step that follows the prejudge node that has no paths toward the correct answer. To synthesize the prejudge reasoning process, we present an automated reasoning framework with a dynamic tree-searching strategy. This framework requires only one LLM to perform answer judging, response critiquing, prejudge generation, and thought completion. Furthermore, we develop a two-phase training mechanism with supervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance the reasoning capabilities of LLMs. Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs. Code and data is released at <a target="_blank" rel="noopener" href="https://github.com/wjn1996/Prejudge-Before-Think">https://github.com/wjn1996/Prejudge-Before-Think</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„â€œè¿‡ç¨‹é¢„åˆ¤â€ç­–ç•¥ï¼Œä»¥å±•ç¤ºé€šè¿‡è¿‡ç¨‹é¢„åˆ¤è¿›è¡Œå¼•å¯¼ï¼Œå¯ä»¥ä½¿LLMè‡ªé€‚åº”åœ°é¢„æµ‹åœ¨æ¨è¿›åç»­æ¨ç†æ­¥éª¤æ—¶é‡åˆ°çš„é”™è¯¯ï¼Œç±»ä¼¼äºäººä»¬æœ‰æ—¶ä¼šåœä¸‹æ¥æ€è€ƒå¯èƒ½å‡ºç°çš„é”™è¯¯ä»¥åŠå¦‚ä½•é¿å…é”™è¯¯ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–è¯•é”™ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨ç†ç”±ä¸­å®šä¹‰äº†ä¸€ä¸ªé¢„åˆ¤èŠ‚ç‚¹ï¼Œä»£è¡¨ä¸€ä¸ªæ¨ç†æ­¥éª¤ï¼Œè‡³å°‘æœ‰ä¸€ä¸ªæ­¥éª¤è·Ÿéšé¢„åˆ¤èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹æ²¡æœ‰é€šå‘æ­£ç¡®ç­”æ¡ˆçš„è·¯å¾„ã€‚ä¸ºäº†ç»¼åˆé¢„åˆ¤æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŠ¨æ€æ ‘æœç´¢ç­–ç•¥çš„è‡ªåŠ¨åŒ–æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶åªéœ€è¦ä¸€ä¸ªLLMæ¥å®Œæˆç­”æ¡ˆåˆ¤æ–­ã€å“åº”è¯„ä»·ã€é¢„åˆ¤ç”Ÿæˆå’Œæ€ç»´å®Œå–„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæœºåˆ¶ï¼Œé‡‡ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥è¿›ä¸€æ­¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚æ¥è‡ªç«èµ›çº§å¤æ‚æ¨ç†çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ•™ä¼šæ¨¡å‹åœ¨æ€è€ƒä¹‹å‰è¿›è¡Œé¢„åˆ¤ï¼Œå¹¶æ˜¾è‘—æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/wjn1996/Prejudge-Before-Think%E3%80%82">https://github.com/wjn1996/Prejudge-Before-Thinkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13500v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é¢„åˆ¤æ–­è¿‡ç¨‹ç­–ç•¥ï¼Œç”¨äºLLMæ¨ç†ã€‚é€šè¿‡å¼•å…¥é¢„åˆ¤æ–­èŠ‚ç‚¹ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢„æµ‹å¯èƒ½å‡ºç°çš„é”™è¯¯ï¼Œå¹¶è‡ªé€‚åº”åœ°è°ƒæ•´åç»­æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥é¢„åˆ¤æ–­è¿‡ç¨‹ç­–ç•¥ï¼Œä½¿LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­èƒ½å¤Ÿé¢„æµ‹å¹¶é€‚åº”å¯èƒ½å‡ºç°çš„é”™è¯¯ã€‚</li>
<li>å®šä¹‰é¢„åˆ¤æ–­èŠ‚ç‚¹ä¸ºåŒ…å«è‡³å°‘ä¸€ä¸ªåç»­æ­¥éª¤çš„èŠ‚ç‚¹ï¼Œè¿™äº›æ­¥éª¤ä¸åŒ…å«é€šå‘æ­£ç¡®ç­”æ¡ˆçš„è·¯å¾„ã€‚</li>
<li>æå‡ºè‡ªåŠ¨åŒ–æ¨ç†æ¡†æ¶ï¼Œé‡‡ç”¨åŠ¨æ€æ ‘æœç´¢ç­–ç•¥è¿›è¡Œé¢„åˆ¤æ–­æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ä»…éœ€è¦ä¸€ä¸ªLLMæ¥å®Œæˆç­”æ¡ˆåˆ¤æ–­ã€å“åº”è¯„ä»·ã€é¢„åˆ¤æ–­ç”Ÿæˆå’Œæ€ç»´è¡¥å…¨ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæœºåˆ¶ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œè¿›ä¸€æ­¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„åˆ¤æ–­è¿‡ç¨‹èƒ½å¤Ÿæé«˜LLMåœ¨ç«èµ›çº§å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b8b7624eaad004f21d7b40a438e9878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7cd6e8205a0146af0709f293e168bf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0a37720db7a13d8f11c8845ffabe660.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bab31fc5a97ca8939454ab8d21fe4c6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-16e2d7f71ebb600fe90f6546e79bd503.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Everything-You-Wanted-to-Know-About-LLM-based-Vulnerability-Detection-But-Were-Afraid-to-Ask"><a href="#Everything-You-Wanted-to-Know-About-LLM-based-Vulnerability-Detection-But-Were-Afraid-to-Ask" class="headerlink" title="Everything You Wanted to Know About LLM-based Vulnerability Detection   But Were Afraid to Ask"></a>Everything You Wanted to Know About LLM-based Vulnerability Detection   But Were Afraid to Ask</h2><p><strong>Authors:Yue Li, Xiao Li, Hao Wu, Minghui Xu, Yue Zhang, Xiuzhen Cheng, Fengyuan Xu, Sheng Zhong</strong></p>
<p>Large Language Models are a promising tool for automated vulnerability detection, thanks to their success in code generation and repair. However, despite widespread adoption, a critical question remains: Are LLMs truly effective at detecting real-world vulnerabilities? Current evaluations, which often assess models on isolated functions or files, ignore the broader execution and data-flow context essential for understanding vulnerabilities. This oversight leads to two types of misleading outcomes: incorrect conclusions and flawed rationales, collectively undermining the reliability of prior assessments. Therefore, in this paper, we challenge three widely held community beliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and (iii) performance-plateaued across model scales. We argue that these beliefs are artifacts of context-deprived evaluations. To address this, we propose CORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new evaluation framework that systematically incorporates contextual information into LLM-based vulnerability detection. We construct a context-rich dataset of 2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs across four model families. Our framework elicits both binary predictions and natural-language rationales, which are further validated using LLM-as-a-judge techniques. Our findings overturn existing misconceptions. When provided with sufficient context, SOTA LLMs achieve significantly improved performance (e.g., 0.7 F1-score on key CWEs), with 0.8 precision. We show that most false positives stem from reasoning errors rather than misclassification, and that while model and test-time scaling improve performance, they introduce diminishing returns and trade-offs in recall. Finally, we uncover new flaws in current LLM-based detection systems, such as limited generalization and overthinking biases. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨æ¼æ´æ£€æµ‹æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå…¶åœ¨ä»£ç ç”Ÿæˆå’Œä¿®å¤æ–¹é¢çš„æˆåŠŸåŠŸä¸å¯æ²¡ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶å¹¿æ³›åº”ç”¨ï¼Œä»æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜æ‚¬è€Œæœªå†³ï¼šLLMsåœ¨æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„æ¼æ´æ—¶æ˜¯å¦çœŸæ­£æœ‰æ•ˆï¼Ÿç°æœ‰çš„è¯„ä¼°é€šå¸¸åªåœ¨å­¤ç«‹çš„å‡½æ•°æˆ–æ–‡ä»¶ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¿½ç•¥äº†ç†è§£æ¼æ´æ‰€å¿…éœ€çš„æ›´å¹¿æ³›çš„æ‰§è¡Œå’Œæ•°æ®æµä¸Šä¸‹æ–‡ã€‚è¿™ç§ç›‘ç£å¯¼è‡´äº†ä¸¤ç§è¯¯å¯¼æ€§çš„ç»“æœï¼šé”™è¯¯çš„ç»“è®ºå’Œç¼ºé™·ç†ç”±ï¼Œå…±åŒå‰Šå¼±äº†å…ˆå‰è¯„ä¼°çš„å¯é æ€§ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜äº†ç¤¾åŒºä¸­ä¸‰ä¸ªæ™®éå­˜åœ¨çš„è§‚ç‚¹ï¼šï¼ˆiï¼‰LLMsä¸å¯é ï¼Œï¼ˆiiï¼‰å¯¹ä»£ç è¡¥ä¸ä¸æ•æ„Ÿï¼Œï¼ˆiiiï¼‰æ¨¡å‹è§„æ¨¡æ‰©å¤§æ€§èƒ½åœæ»ä¸å‰ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›è§‚ç‚¹éƒ½æ˜¯ç¼ºä¹ä¸Šä¸‹æ–‡è¯„ä¼°æ‰€äº§ç”Ÿçš„ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CORRECTï¼ˆåŸºäºä¿¡ä»»çš„ä¸°å¯Œä¸Šä¸‹æ–‡æ¨ç†è¯„ä¼°ä»£ç ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯åˆ°åŸºäºLLMçš„æ¼æ´æ£€æµ‹ä¸­ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«2000ä¸ªè„†å¼±è¡¥ä¸ç¨‹åºå¯¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡æ•°æ®é›†ï¼Œæ¶µç›–äº†99ä¸ªCWEï¼Œè¯„ä¼°äº†å››ä¸ªå®¶æ—ä¸­çš„13ä¸ªLLMã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å‘äº†äºŒå…ƒé¢„æµ‹å’Œè‡ªç„¶è¯­è¨€ç†ç”±ï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨LLM-as-a-judgeæŠ€æœ¯è¿›è¡ŒéªŒè¯ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æä¾›è¶³å¤Ÿä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œæœ€æ–°LLMçš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼ˆä¾‹å¦‚åœ¨å…³é”®CWEä¸Šè¾¾åˆ°0.7çš„F1åˆ†æ•°ï¼‰ï¼Œç²¾ç¡®åº¦ä¸º0.8ã€‚æˆ‘ä»¬å‘ç°å¤§å¤šæ•°è¯¯æŠ¥æºäºæ¨ç†é”™è¯¯è€Œéè¯¯åˆ†ç±»ï¼Œå¹¶ä¸”è™½ç„¶æ¨¡å‹å’Œæµ‹è¯•æ—¶çš„ç¼©æ”¾å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¼šå¸¦æ¥å›æŠ¥é€’å‡å’Œå¬å›ç‡æ–¹é¢çš„æƒè¡¡ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°äº†å½“å‰åŸºäºLLMçš„æ£€æµ‹ç³»ç»Ÿä¸­çš„æ–°æ¼æ´ï¼Œä¾‹å¦‚æœ‰é™çš„æ³›åŒ–èƒ½åŠ›å’Œè¿‡åº¦æ€è€ƒåè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶åœ¨ä»£ç ç”Ÿæˆå’Œä¿®å¤æ–¹é¢ã€‚ç„¶è€Œï¼Œå…³äºå…¶æ˜¯å¦èƒ½æœ‰æ•ˆæ£€æµ‹çœŸå®ä¸–ç•Œæ¼æ´çš„é—®é¢˜ä»å­˜åœ¨ç–‘é—®ã€‚å½“å‰è¯„ä¼°æ¨¡å‹çš„æ–¹æ³•å¸¸å¸¸å±€é™äºå¯¹å­¤ç«‹å‡½æ•°æˆ–æ–‡ä»¶çš„è¯„ä¼°ï¼Œå¿½ç•¥äº†æ‰§è¡Œå’Œæ•°æ®æµä¸Šä¸‹æ–‡çš„é‡è¦æ€§ï¼Œè¿™å¯¼è‡´è¯„ä¼°ç»“æœå­˜åœ¨è¯¯å¯¼æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶CORRECTï¼Œè¯¥æ¡†æ¶ç³»ç»Ÿåœ°ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼æ´æ£€æµ‹ã€‚é€šè¿‡æ„å»ºåŒ…å«2000ä¸ªæ¼æ´ä¿®å¤ç¨‹åºå¯¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡æ•°æ®é›†ï¼Œå¹¶å¯¹å››ä¸ªæ¨¡å‹å®¶æ—çš„13ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å½“æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡æ—¶ï¼Œå½“å‰é¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨åŒ–æ¼æ´æ£€æµ‹æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å½“å‰æ¼æ´æ£€æµ‹è¯„ä¼°æ–¹æ³•å¿½ç•¥äº†æ‰§è¡Œå’Œæ•°æ®æµä¸Šä¸‹æ–‡çš„é‡è¦æ€§ã€‚</li>
<li>æ–°çš„è¯„ä¼°æ¡†æ¶CORRECTç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹æ¼æ´æ£€æµ‹ã€‚</li>
<li>æä¾›è¶³å¤Ÿä¸Šä¸‹æ–‡æ—¶ï¼Œé¡¶å°–å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯¯æŠ¥ä¸»è¦æºäºæ¨ç†é”™è¯¯è€Œéè¯¯åˆ†ç±»ã€‚</li>
<li>æ¨¡å‹å’Œæµ‹è¯•æ—¶çš„æ‰©å±•è™½ç„¶èƒ½æé«˜æ€§èƒ½ï¼Œä½†å­˜åœ¨å›æŠ¥é€’å‡å’Œå¬å›ç‡æƒè¡¡çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fbb96b8732d54ef58b2e559a0a30921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f55aa529208c482b116bc6ab8ac230b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb0737508e214570459c163a2f9a1f41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the modelâ€™s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æ—¶é—´åŠ¨ä½œå®šä½ï¼ˆTALï¼‰æ–¹æ³•ä¾èµ–äºå¤§é‡çš„è¯¦ç»†æ ‡æ³¨æ•°æ®ï¼Œè€Œå°‘æ ·æœ¬TALåˆ™é€šè¿‡ä»…ä½¿ç”¨å°‘é‡çš„è®­ç»ƒæ ·æœ¬æ¥è¯†åˆ«æœªè§è¿‡çš„åŠ¨ä½œç±»åˆ«ï¼Œå‡å°‘äº†è¿™ç§ä¾èµ–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å°‘æ ·æœ¬TALæ–¹æ³•é€šå¸¸åªå…³æ³¨è§†é¢‘å±‚é¢çš„ä¿¡æ¯ï¼Œå¿½è§†äº†æ–‡æœ¬ä¿¡æ¯ï¼Œè¿™å¯ä»¥ä¸ºå®šä½ä»»åŠ¡æä¾›æœ‰ä»·å€¼çš„è¯­ä¹‰æ”¯æŒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ€ç»´é“¾æ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶é—´åŠ¨ä½œå®šä½æ–¹æ³•ï¼Œä»¥æé«˜å®šä½æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æ¥æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªè¯­ä¹‰æ„ŸçŸ¥çš„æ–‡æœ¬-è§†è§‰å¯¹é½æ¨¡å—ï¼Œæ—¨åœ¨åœ¨ä¸åŒå±‚æ¬¡ä¸Šå¯¹é½æŸ¥è¯¢å’Œæ”¯æŒè§†é¢‘ã€‚åŒæ—¶ï¼Œä¸ºäº†æ›´å¥½åœ°è¡¨è¾¾æ–‡æœ¬å±‚é¢ä¸ŠåŠ¨ä½œçš„æ—¶ç©ºä¾èµ–å’Œå› æœå…³ç³»ï¼Œå¸®åŠ©åŠ¨ä½œå®šä½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç±»ä¼¼æ€ç»´é“¾ï¼ˆCoTï¼‰çš„æ¨ç†æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§†é¢‘ç”Ÿæˆç±»ä¼¼æ€ç»´é“¾çš„æ–‡æœ¬æè¿°ã€‚ç”Ÿæˆçš„æ–‡æœ¬å¯ä»¥æ•æ‰æ¯”è§†è§‰ç‰¹å¾æ›´å¤šçš„åŠ¨ä½œå˜åŒ–ã€‚æˆ‘ä»¬åœ¨å…¬å¼€å¯ç”¨çš„ActivityNet1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æˆ‘ä»¬å¼•å…¥äº†åä¸ºHuman-related Anomaly Localizationçš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶æ¢ç´¢äº†TALä»»åŠ¡åœ¨äººç±»å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å•å®ä¾‹å’Œå¤šå®ä¾‹åœºæ™¯ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’ŒåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„æ–°çš„å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½æ–¹æ³•ã€‚é€šè¿‡è®¾è®¡æ–°çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶å’Œè¯­ä¹‰æ„ŸçŸ¥æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æé«˜æ¨¡å‹æ•æ‰åŠ¨ä½œå…±æ€§å’Œå˜åŒ–çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡è®¾è®¡Chain of Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•ï¼ŒååŠ©åœ¨æ–‡æœ¬å±‚é¢ä¸Šè¡¨è¾¾åŠ¨ä½œé—´çš„æ—¶åºä¾èµ–å’Œå› æœå…³ç³»ï¼Œä»¥è¾…åŠ©åŠ¨ä½œå®šä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ActivityNet1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨äººç±»å¼‚å¸¸æ£€æµ‹çš„åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºChain-of-Thoughtæ–‡æœ¬æ¨ç†çš„å°‘æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½æ–¹æ³•ã€‚</li>
<li>è®¾è®¡äº†æ–°çš„å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰æ„ŸçŸ¥æ–‡æœ¬è§†è§‰å¯¹é½æ¨¡å—ï¼Œå¯¹é½æŸ¥è¯¢å’Œæ”¯æ’‘è§†é¢‘çš„ä¸åŒå±‚æ¬¡ã€‚</li>
<li>é€šè¿‡Chain of Thoughtï¼ˆCoTï¼‰æ¨ç†æ–¹æ³•ï¼Œåœ¨æ–‡æœ¬å±‚é¢ä¸Šè¡¨è¾¾åŠ¨ä½œé—´çš„æ—¶åºä¾èµ–å’Œå› æœå…³ç³»ã€‚</li>
<li>ç”Ÿæˆçš„æ–‡æœ¬æè¿°èƒ½æ•æ‰æ¯”è§†è§‰ç‰¹å¾æ›´å¤šçš„åŠ¨ä½œå˜åŒ–ã€‚</li>
<li>åœ¨ActivityNet1.3å’ŒTHUMOS14æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55fea05ebbb7e2ed2badab807e563848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6088af927e869df00f843812c7c55ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c213dbbcc1e0229555b467cf4aa97d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SkyReels-V2-Infinite-length-Film-Generative-Model"><a href="#SkyReels-V2-Infinite-length-Film-Generative-Model" class="headerlink" title="SkyReels-V2: Infinite-length Film Generative Model"></a>SkyReels-V2: Infinite-length Film Generative Model</h2><p><strong>Authors:Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, Yahui Zhou</strong></p>
<p>Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMsâ€™ inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2">https://github.com/SkyworkAI/SkyReels-V2</a>. </p>
<blockquote>
<p>æœ€è¿‘è§†é¢‘ç”Ÿæˆçš„è¿›å±•å¾—ç›Šäºæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¡†æ¶çš„æ¨åŠ¨ï¼Œä½†ä»å­˜åœ¨å…³äºæç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨æ€å’Œæ—¶é•¿çš„åè°ƒæ–¹é¢çš„å…³é”®æŒ‘æˆ˜ï¼šä¸ºäº†å¢å¼ºæ—¶é—´è§†è§‰è´¨é‡è€Œå¦¥åè¿åŠ¨åŠ¨æ€ï¼Œå—é™åˆ¶çš„è§†é¢‘æ—¶é•¿ï¼ˆ5-10ç§’ï¼‰ä»¥ä¼˜å…ˆè€ƒè™‘åˆ†è¾¨ç‡ï¼Œä»¥åŠç”±äºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ— æ³•è§£é‡Šç”µå½±è¯­æ³•ï¼ˆå¦‚é•œå¤´æ„å›¾ã€æ¼”å‘˜è¡¨æƒ…å’Œç›¸æœºè¿åŠ¨ï¼‰è€Œå¯¼è‡´çš„æ‹æ‘„æ„è¯†ç”Ÿæˆä¸è¶³ã€‚è¿™äº›äº¤ç»‡çš„é™åˆ¶é˜»ç¢äº†ç°å®çš„é•¿å½¢å¼åˆæˆå’Œä¸“ä¸šç”µå½±é£æ ¼çš„ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SkyReels-V2ï¼Œä¸€ç§æ— é™é•¿ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒååŒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œå®ƒç»“åˆäº†å¤šæ¨¡æ€LLMçš„ä¸€èˆ¬æè¿°å’Œå­ä¸“å®¶æ¨¡å‹çš„è¯¦ç»†é•œå¤´è¯­è¨€ã€‚å€ŸåŠ©äººå·¥æ ‡æ³¨ï¼Œæˆ‘ä»¬éšåè®­ç»ƒäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘å­—å¹•å™¨ï¼Œåä¸ºSkyCaptioner-V1ï¼Œä»¥æœ‰æ•ˆåœ°æ ‡æ³¨è§†é¢‘æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ºåŸºæœ¬çš„è§†é¢‘ç”Ÿæˆå»ºç«‹äº†æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œç„¶åæ˜¯å››é˜¶æ®µçš„åè®­ç»ƒå¢å¼ºï¼šåˆå§‹æ¦‚å¿µå¹³è¡¡çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æé«˜äº†åŸºçº¿è´¨é‡ï¼›ä½¿ç”¨äººå·¥æ³¨é‡Šå’Œåˆæˆå¤±çœŸæ•°æ®çš„ç‰¹å®šè¿åŠ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè§£å†³äº†åŠ¨æ€ä¼ªå½±é—®é¢˜ï¼›æˆ‘ä»¬çš„æ‰©æ•£å¼ºåˆ¶æ¡†æ¶ä¸ä¸æ–­å¢é•¿çš„å™ªå£°æ—¶é—´è¡¨ç›¸ç»“åˆï¼Œèƒ½å¤Ÿåœ¨æœ‰æ•ˆçš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œé•¿è§†é¢‘åˆæˆï¼›æœ€åçš„é«˜è´¨é‡SFTæ”¹è¿›äº†è§†è§‰ä¿çœŸåº¦ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SkyworkAI/SkyReels-V2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13074v2">PDF</a> 31 pages,10 figures</p>
<p><strong>Summary</strong><br>     æœ€è¿‘è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„è¿›å±•ä¸»è¦å¾—ç›Šäºæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¡†æ¶ï¼Œä½†ä»é¢ä¸´æç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨åŠ›å’Œæ—¶é•¿ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†SkyReels-V2ï¼Œä¸€ç§æ— é™é•¿ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œèåˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚é€šè¿‡è®¾è®¡å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºã€è®­ç»ƒç»Ÿä¸€è§†é¢‘æ ‡æ³¨å™¨ã€å»ºç«‹æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒä»¥åŠå››ä¸ªé˜¶æ®µçš„åæœŸè®­ç»ƒå¢å¼ºç­‰æªæ–½æ¥åº”å¯¹æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”Ÿæˆé¢†åŸŸè™½æœ‰æ‰€è¿›å±•ï¼Œä½†ä»é¢ä¸´æç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨åŠ›å’Œæ—¶é•¿ç­‰æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>SkyReels-V2æ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›é™åˆ¶ï¼Œé€šè¿‡èåˆMLLMã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶æ¥æå‡è§†é¢‘ç”Ÿæˆè´¨é‡ã€‚</li>
<li>å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºç»“åˆäº†å¤šæ¨¡æ€LLMçš„ä¸€èˆ¬æè¿°å’Œå­ä¸“å®¶æ¨¡å‹çš„è¯¦ç»†é•œå¤´è¯­è¨€ã€‚</li>
<li>ä½¿ç”¨äººç±»æ³¨é‡Šè®­ç»ƒäº†ç»Ÿä¸€è§†é¢‘æ ‡æ³¨å™¨SkyCaptioner-V1ï¼Œä»¥é«˜æ•ˆæ ‡æ³¨è§†é¢‘æ•°æ®ã€‚</li>
<li>å»ºç«‹äº†æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œä¸ºåç»­çš„è§†é¢‘ç”Ÿæˆå¥ å®šåŸºç¡€ã€‚</li>
<li>å››ä¸ªé˜¶æ®µçš„åæœŸè®­ç»ƒå¢å¼ºæªæ–½ï¼ŒåŒ…æ‹¬åˆå§‹æ¦‚å¿µå¹³è¡¡çš„ç›‘ç£å¾®è°ƒã€é’ˆå¯¹è¿åŠ¨åŠ¨åŠ›çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€æ‰©æ•£å¼ºåˆ¶æ¡†æ¶ä»¥åŠæœ€ç»ˆçš„é«˜è´¨é‡ç›‘ç£å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-256858801c9eb5e0f5a46a3c88eaa9c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135bbc853f17d529787e36191b59350b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9fbf10fae977ac87c8ae1741923de30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5302e464edde26d5c78c6efa43188ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f40e05213ea0ac9ecd71eeecc2b78b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Finding-Flawed-Fictions-Evaluating-Complex-Reasoning-in-Language-Models-via-Plot-Hole-Detection"><a href="#Finding-Flawed-Fictions-Evaluating-Complex-Reasoning-in-Language-Models-via-Plot-Hole-Detection" class="headerlink" title="Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models   via Plot Hole Detection"></a>Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models   via Plot Hole Detection</h2><p><strong>Authors:Kabir Ahuja, Melanie Sclar, Yulia Tsvetkov</strong></p>
<p>Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes â€“ inconsistencies in a storyline that break the internal logic or rules of a storyâ€™s world â€“ requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMsâ€™ plot hole detection abilities in stories â€“ FlawedFictions â€“ , which is robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with more than 50% and 100% increases in plot hole detection rates with respect to human-written originals. </p>
<blockquote>
<p>æ•…äº‹æ˜¯äººç±»ç»éªŒçš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚æ·±å…¥å‚ä¸æ•…äº‹å’Œå‘ç°æƒ…èŠ‚æ¼æ´â€”â€”æ•…äº‹æƒ…èŠ‚ä¸­çš„ä¸ä¸€è‡´ä¹‹å¤„ç ´åäº†å…¶å†…éƒ¨é€»è¾‘æˆ–æ•…äº‹ä¸–ç•Œçš„è§„åˆ™â€”â€”éœ€è¦å¾®å¦™çš„æ¨ç†æŠ€èƒ½ï¼ŒåŒ…æ‹¬è¿½è¸ªå®ä½“å’Œäº‹ä»¶åŠå…¶ç›¸äº’ä½œç”¨ã€æŠ½è±¡æ€ç»´ã€å®ç”¨å™äº‹ç†è§£ã€å¸¸è¯†å’Œç¤¾ä¼šæ¨ç†ä»¥åŠå¿ƒæ™ºç†è®ºã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”Ÿæˆã€è§£é‡Šå’Œä¿®æ”¹æ–‡æœ¬ï¼Œä¸¥æ ¼è¯„ä¼°å…¶å™äº‹ä¸€è‡´æ€§ä»¥åŠæ›´æ·±å±‚æ¬¡çš„è¯­è¨€ç†è§£èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨äºè¡¨å±‚ç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºå°†æƒ…èŠ‚æ¼æ´æ£€æµ‹ä½œä¸ºè¯„ä¼°LLMçš„è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„ä»£ç†ã€‚æˆ‘ä»¬ä»‹ç»äº†FlawedFictionsMakerè¿™ä¸€æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå¯æ§ä¸”ä»”ç»†åœ°åˆæˆäººç±»å†™ä½œæ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´ã€‚ä½¿ç”¨è¯¥ç®—æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°LLMçš„æƒ…èŠ‚æ¼æ´æ£€æµ‹èƒ½åŠ›â€”â€”FlawedFictionsï¼Œè¯¥æµ‹è¯•ç¨³å¥ä¸”ä¸æ˜“å—åˆ°æ±¡æŸ“çš„å½±å“ï¼Œå¹¶é€šè¿‡äººå·¥è¿‡æ»¤ç¡®ä¿é«˜è´¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿å…è®¸å…ˆè¿›çš„LLMè¿›è¡Œæ¨ç†åŠªåŠ›ï¼Œå®ƒä»¬åœ¨è§£å†³FlawedFictionsæ–¹é¢çš„è¡¨ç°ä»ç„¶ä¸ä½³ï¼Œå¹¶ä¸”éšç€æ•…äº‹é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†åŸºäºLLMçš„æ•…äº‹æ‘˜è¦å’Œæ•…äº‹ç”Ÿæˆå®¹æ˜“å¼•å…¥æƒ…èŠ‚æ¼æ´ï¼Œä¸äººç±»åŸå§‹ä½œå“çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹ç‡ç›¸æ¯”ï¼Œå…¶æ¼æ´æ£€æµ‹ç‡åˆ†åˆ«å¢åŠ äº†50%å’Œ100%ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11900v2">PDF</a> Preprint</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡å…³æ³¨æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹ï¼Œè®¤ä¸ºè¿™æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ·±å±‚æ¬¡è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„é‡è¦æ ‡å‡†ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°ç®—æ³•FlawedFictionsMakerï¼Œç”¨äºåˆæˆå¸¦æœ‰æƒ…èŠ‚æ¼æ´çš„æ•…äº‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•FlawedFictionsï¼Œä»¥è¯„ä¼°è¯­è¨€æ¨¡å‹æ£€æµ‹æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´çš„èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨é¢ä¸´FlawedFictionsæŒ‘æˆ˜æ—¶ä¹Ÿéš¾ä»¥å‡†ç¡®è§£å†³é—®é¢˜ï¼Œéšç€æ•…äº‹é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚æ­¤å¤–ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•…äº‹æ‘˜è¦å’Œæ•…äº‹ç”Ÿæˆä¹Ÿå®¹æ˜“äº§ç”Ÿæƒ…èŠ‚æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´æ£€æµ‹æ˜¯è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ·±å±‚æ¬¡è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„é‡è¦æ ‡å‡†ã€‚</li>
<li>FlawedFictionsMakerç®—æ³•èƒ½å¤Ÿåˆæˆå¸¦æœ‰æƒ…èŠ‚æ¼æ´çš„æ•…äº‹ã€‚</li>
<li>FlawedFictionsåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹æ£€æµ‹æ•…äº‹ä¸­çš„æƒ…èŠ‚æ¼æ´çš„èƒ½åŠ›ã€‚</li>
<li>å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨FlawedFictionsæŒ‘æˆ˜æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œéšç€æ•…äº‹é•¿åº¦çš„å¢åŠ ï¼Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•…äº‹æ‘˜è¦ç”Ÿæˆæ˜“äº§ç”Ÿæƒ…èŠ‚æ¼æ´ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•…äº‹ç”Ÿæˆä¸­ä¹Ÿå®¹æ˜“å¼•å…¥æƒ…èŠ‚æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b2d26454fb7776c2e635d4c32867819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28efee1a5010ff446b3e688aa18e81d5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Run Luo, Lu Wang, Wanwei He, Xiaobo Xia</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>ç°æœ‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ„å»ºå·¥ä½œä¸»è¦ä¾èµ–äºåœ¨å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸Šè¿›è¡Œçš„æœ‰ç›‘ç£å¾®è°ƒè®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢æ–¹é¢ä¹Ÿå­˜åœ¨å›°éš¾ã€‚è¿™ä¸€é—®é¢˜æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯é«˜çº§ä»»åŠ¡ã€‚å—å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰çš„å¯å‘ï¼ˆä¾‹å¦‚DeepSeek-R1ï¼‰ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ç¯å¢ƒä¸­çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºåä¸ºâ€œåç§°â€çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒæ˜¯é¦–ä¸ªæ—¨åœ¨é€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡å¢å¼ºLVLMsåœ¨ç°å®é«˜çº§ä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨è·¨å¤šä¸ªå¹³å°ï¼ˆåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’ŒWebï¼‰çš„å°‘é‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨å¦‚é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ï¼Œâ€œåç§°â€ä»…ä½¿ç”¨0.02%çš„æ•°æ®ï¼ˆ3Kå¯¹1.3Mï¼‰å°±å®ç°äº†åœ¨è·¨è¶Šä¸‰ä¸ªä¸åŒå¹³å°ï¼ˆç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µï¼‰çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›æ–¹æ³•ï¼Œå¦‚OS-Atlasã€‚è¿™äº›ç»“æœè¯æ˜äº†åŸºäºç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡Œç°å®GUIä»£ç†ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†æ„å»ºå·¥ä½œå¤§å¤šä¾èµ–äºç›‘ç£ç²¾ç»†è°ƒæ•´çš„è®­ç»ƒæ¨¡å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸ä»…éœ€æ±‚å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œè€Œä¸”åœ¨ç†è§£GUIæˆªå›¾å’Œæ³›åŒ–åˆ°æœªè§è¿‡çš„ç•Œé¢ä¸Šå­˜åœ¨å›°éš¾ã€‚æœ¬æ–‡å—å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„å¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆRFTï¼‰çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶â€”â€”åä¸º\nameï¼Œæ—¨åœ¨é€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡ï¼Œæé«˜LVLMsåœ¨çœŸå®ä¸–ç•Œé«˜çº§ä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨è·¨å¤šä¸ªå¹³å°çš„å°é‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰ç­–ç•¥ä¼˜åŒ–ç®—æ³•æ¥æ›´æ–°æ¨¡å‹ï¼Œ\nameåœ¨ä»…ä½¿ç”¨0.02%ï¼ˆ3K vs 13Mï¼‰æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨æ¶µç›–ä¸‰ä¸ªä¸åŒå¹³å°ï¼ˆç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µï¼‰çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¼˜äºOS-Atlasç­‰ç°æœ‰å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ã€‚è¿™æ˜¾ç¤ºäº†åŸºäºç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡çš„å¼ºåŒ–å­¦ä¹ åœ¨æå‡LVLMsæ‰§è¡ŒçœŸå®ä¸–ç•ŒGUIä»£ç†ä»»åŠ¡æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUIä»£ç†çš„æ„å»ºä¸»è¦ä¾èµ–äºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç›‘ç£ç²¾ç»†è°ƒæ•´è®­ç»ƒæ¨¡å¼ï¼Œä½†éœ€è¦å¤§é‡æ•°æ®å’Œé¢ä¸´ç†è§£å’Œæ³›åŒ–éš¾é¢˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶\nameè¢«æå‡ºï¼Œç”¨äºé€šè¿‡ç»Ÿä¸€åŠ¨ä½œç©ºé—´è§„åˆ™å»ºæ¨¡æé«˜LVLMsåœ¨çœŸå®ä¸–ç•Œé«˜çº§ä»»åŠ¡åœºæ™¯ä¸­çš„GUIèƒ½åŠ›ã€‚</li>
<li>\nameä½¿ç”¨è·¨å¹³å°é«˜è´¨é‡å°é‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡ç­–ç•¥ä¼˜åŒ–ç®—æ³•å¦‚GRPOæ›´æ–°æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-40e44cce157ab734b1ce7aa7a91d195b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8b83a33fe04aa5f478735789bdd633d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1378358a130b9ffbb33425047048d0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation"><a href="#DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation" class="headerlink" title="DocAgent: A Multi-Agent System for Automated Code Documentation   Generation"></a>DocAgent: A Multi-Agent System for Automated Code Documentation   Generation</h2><p><strong>Authors:Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang</strong></p>
<p>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories. </p>
<blockquote>
<p>é«˜è´¨é‡çš„ä»£ç æ–‡æ¡£å¯¹è½¯ä»¶å¼€å‘è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚æˆ‘ä»¬å¼•å…¥äº†DocAgentï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ‹“æ‰‘ä»£ç å¤„ç†è¿›è¡Œå¢é‡ä¸Šä¸‹æ–‡æ„å»ºçš„æ–°å‹å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿã€‚ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼ˆé˜…è¯»å™¨ã€æœç´¢å™¨ã€ç¼–å†™å™¨ã€éªŒè¯å™¨ã€åè°ƒå™¨ï¼‰ååŒç”Ÿæˆæ–‡æ¡£ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå¤šæ–¹é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æ–‡æ¡£çš„å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDocAgentæŒç»­ä¸”æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯å®äº†æ‹“æ‰‘å¤„ç†é¡ºåºçš„é‡è¦ä½œç”¨ã€‚DocAgentä¸ºå¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­çš„å¯é ä»£ç æ–‡æ¡£ç”Ÿæˆæä¾›äº†ç¨³å¥çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08725v2">PDF</a> Public Repo: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/DocAgent">https://github.com/facebookresearch/DocAgent</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒé«˜è´¨é‡ä»£ç æ–‡æ¡£åœ¨è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½æ—¶ä»£ã€‚ç„¶è€Œï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•å¾€å¾€äº§ç”Ÿä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†DocAgentï¼Œä¸€ç§åˆ©ç”¨æ‹“æ‰‘ä»£ç å¤„ç†çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿï¼Œç”¨äºå¢é‡æ„å»ºä¸Šä¸‹æ–‡ã€‚é€šè¿‡ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼ˆé˜…è¯»å™¨ã€æœç´¢å™¨ã€ç¼–å†™å™¨ã€éªŒè¯å™¨ã€åè°ƒå™¨ï¼‰ååŒç”Ÿæˆæ–‡æ¡£ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§å¤šç»´è¯„ä¼°æ¡†æ¶ï¼Œè¯„ä¼°æ–‡æ¡£çš„å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDocAgentåœ¨å¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡ä»£ç æ–‡æ¡£åœ¨è½¯ä»¶å¼€å‘ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆä»£ç æ–‡æ¡£çš„ç°æœ‰æŒ‘æˆ˜ï¼šä¸å®Œæ•´ã€æ— å¸®åŠ©æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚</li>
<li>DocAgentç³»ç»Ÿä»‹ç»ï¼šåˆ©ç”¨æ‹“æ‰‘ä»£ç å¤„ç†å’Œå¤šæ™ºèƒ½ä½“åä½œç”Ÿæˆæ–‡æ¡£ã€‚</li>
<li>DocAgentç³»ç»Ÿä¸­çš„æ™ºèƒ½ä½“è§’è‰²ï¼šé˜…è¯»å™¨ã€æœç´¢å™¨ã€ç¼–å†™å™¨ã€éªŒè¯å™¨å’Œåè°ƒå™¨ã€‚</li>
<li>å¤šç»´è¯„ä¼°æ¡†æ¶ï¼šè¯„ä¼°æ–‡æ¡£çš„å®Œæ•´æ€§ã€å¸®åŠ©æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>DocAgentåœ¨å¤æ‚å’Œä¸“æœ‰å­˜å‚¨åº“ä¸­çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c961904b2d4b0a387b2ed18b9a2add40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fd0534526e9f43e1ed4683ac785090c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-933e5dcbc145e1cb1361f03356dd2b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28680bc575a8c0f69c2ab5c8868cc5f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cae1fe7424320522f3e73e14023dd673.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ReaRAG-Knowledge-guided-Reasoning-Enhances-Factuality-of-Large-Reasoning-Models-with-Iterative-Retrieval-Augmented-Generation"><a href="#ReaRAG-Knowledge-guided-Reasoning-Enhances-Factuality-of-Large-Reasoning-Models-with-Iterative-Retrieval-Augmented-Generation" class="headerlink" title="ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large   Reasoning Models with Iterative Retrieval Augmented Generation"></a>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large   Reasoning Models with Iterative Retrieval Augmented Generation</h2><p><strong>Authors:Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li</strong></p>
<p>Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAGâ€™s strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMsâ€™ factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG). </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å±•ç°å‡ºæ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦ä¾èµ–äºå‚æ•°çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†äº‹å®å‡†ç¡®æ€§ã€‚è™½ç„¶æœ€è¿‘çš„å·¥ä½œä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„LRMsé…å¤‡äº†æ£€ç´¢èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶è¿‡äºæ·±æ€ç†Ÿè™‘ï¼Œç¼ºä¹ç¨³å¥æ€§ï¼Œé™ä½äº†åœ¨é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRAGï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨ä¸è¿›è¡Œè¿‡å¤šæ¬¡è¿­ä»£çš„æƒ…å†µä¸‹æ¢ç´¢å„ç§æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸€ä¸ªå…·æœ‰æ¨ç†é“¾é•¿åº¦ä¸Šé™çš„æ–°å‹æ•°æ®æ„å»ºæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨LRMè¿›è¡Œæ·±æ€ç†Ÿè™‘çš„ç”Ÿæˆï¼Œç„¶åä»é¢„å®šçš„åŠ¨ä½œç©ºé—´ï¼ˆæœç´¢å’Œå®Œæˆï¼‰ä¸­é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚å¯¹äºæœç´¢åŠ¨ä½œï¼Œä¼šå¯¹RAGå¼•æ“æ‰§è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼Œè¿”å›çš„ç»“æœå°†ä½œä¸ºè§‚å¯Ÿç»“æœæ¥æŒ‡å¯¼åç»­çš„æ¨ç†æ­¥éª¤ã€‚è¿™ä¸ªè¿‡ç¨‹å°†ä¸€ç›´è¿­ä»£ï¼Œç›´åˆ°é€‰æ‹©ä¸€ä¸ªå®ŒæˆåŠ¨ä½œã€‚ReaRAGçš„å¼ºæ¨ç†èƒ½åŠ›ä½¿å…¶åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¿›ä¸€æ­¥çš„åˆ†æçªæ˜¾äº†å…¶å¼ºå¤§çš„åæ€èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«é”™è¯¯å¹¶ä¼˜åŒ–å…¶æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶æé«˜äº†LRMsçš„äº‹å®æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°å°†ç¨³å¥æ¨ç†æ•´åˆåˆ°å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21729v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œé™åˆ¶äº†äº‹å®å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ReaRAGï¼Œé€šè¿‡æ„å»ºæ–°å‹æ•°æ®æ¡†æ¶å¹¶é™åˆ¶æ¨ç†é“¾é•¿åº¦ï¼Œè§£å†³ç°æœ‰æ¨¡å‹è¿‡åº¦æ€è€ƒã€ç¼ºä¹ç¨³å¥æ€§çš„é—®é¢˜ã€‚ReaRAGåˆ©ç”¨LRMè¿›è¡Œæ·±æ€ç†Ÿè™‘ï¼Œä»é¢„å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆæœç´¢ä¸å®Œæˆï¼‰ä¸­é€‰æ‹©åŠ¨ä½œã€‚æœç´¢åŠ¨ä½œé€šè¿‡RAGå¼•æ“æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœæŒ‡å¯¼åç»­æ¨ç†æ­¥éª¤ã€‚æœ¬ç ”ç©¶å¢å¼ºäº†LRMçš„äº‹å®æ€§ï¼Œå¹¶æœ‰æ•ˆé›†æˆäº†ç¨³å¥æ¨ç†ï¼Œæé«˜äº†å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ä¸»è¦ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œé™åˆ¶äº†äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>ReaRAGæ¨¡å‹æ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹è¿‡åº¦æ€è€ƒã€ç¼ºä¹ç¨³å¥æ€§çš„é—®é¢˜ã€‚</li>
<li>ReaRAGé€šè¿‡æ„å»ºæ–°å‹æ•°æ®æ¡†æ¶ï¼Œå¹¶ç»“åˆé¢„å®šä¹‰åŠ¨ä½œç©ºé—´å®ç°å¢å¼ºäº‹å®æ€§çš„æ¨ç†ã€‚</li>
<li>ReaRAGåˆ©ç”¨LRMè¿›è¡Œæ·±æ€ç†Ÿè™‘ï¼Œå¹¶é€šè¿‡æœç´¢åŠ¨ä½œä¸RAGå¼•æ“äº¤äº’è·å–ç»“æœã€‚</li>
<li>ReaRAGæ¨¡å‹åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼ŒReaRAGå…·æœ‰å¼ºå¤§çš„åæ€èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«é”™è¯¯å¹¶ä¼˜åŒ–æ¨ç†è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1ce0707003e6a9caeb1c794c94d37862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5beaa5fb2d12eadd28e6fc41ce584d3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2209859632255ba8f9bfbdbb09dc5fb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ecdee8bf1f4e95a2aac8829d3e06799.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bbed99f1971aa9a935920c070ed7910e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-20  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
