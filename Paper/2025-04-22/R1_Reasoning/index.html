<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    56 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-22-更新"><a href="#2025-04-22-更新" class="headerlink" title="2025-04-22 更新"></a>2025-04-22 更新</h1><h2 id="Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model"><a href="#Does-Reinforcement-Learning-Really-Incentivize-Reasoning-Capacity-in-LLMs-Beyond-the-Base-Model" class="headerlink" title="Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?"></a>Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?</h2><p><strong>Authors:Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models’ capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$&#x3D;1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models’ sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model’s output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: <a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a> </p>
<blockquote>
<p>强化学习与可验证奖励（RLVR）最近在增强大型语言模型的推理能力方面取得了显著的成功，特别是在数学和编程任务中。人们普遍认为，RLVR使大型语言模型能够持续自我改进，从而获得超过相应基础模型容量的新型推理能力。然而，本研究通过测量pass@\emph{k}指标，并设置较大的\emph{k}值来重新评估这一假设，以探索模型跨多个模型家族和基准测试集的推理能力边界。令人惊讶的是，RL并没有引发根本性的新推理模式。尽管在较小的k值（例如k&#x3D;1）下，RL训练的模型优于基础模型，但在较大的k值下，基础模型甚至可以取得与RL训练模型相当的甚至更高的pass@k分数。RL训练模型产生的推理路径已包含在基础模型的采样分布中，这表明RL训练模型中展现的大多数推理能力已被基础模型所获得。进一步的分析表明，RL训练通过使模型输出分布偏向于更可能产生奖励的路径，从而提高性能，从而更有效地抽取正确响应。但这与基础模型相比，也导致了较窄的推理能力边界。在RLVR训练的视觉推理任务中也观察到类似的结果。此外，我们发现蒸馏法可以真正地将新知识引入模型中，这与RLVR不同。这些发现突出了RLVR在推进大型语言模型推理能力方面的关键局限性，这要求我们从根本上重新思考RL训练在推理大型语言模型中的作用以及是否需要更好的范式。项目页面：<a target="_blank" rel="noopener" href="https://limit-of-rlvr.github.io/">https://limit-of-RLVR.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13837v1">PDF</a> 24 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>强化学习通过奖励验证（RLVR）在提升大型语言模型（LLM）的推理能力方面取得了显著的成功，特别是在数学和编程任务中。然而，本研究通过测量pass@\textit{k}指标对大模型进行批判性考察，发现在宽泛的模型家族和基准测试中，强化学习并不激发新的推理模式。在较大的k值下，基础模型的性能可与强化学习模型相当甚至更高。强化学习训练使模型输出分布偏向于更易产生奖励的路径，从而提高性能，但这也导致与基础模型相比，其推理能力边界较窄。类似的结果也出现在使用RLVR训练的视觉推理任务中。此外，研究发现蒸馏可以真正地为模型引入新知识，与RLVR不同。这些发现突显了RLVR在提升LLM推理能力方面的关键局限性，需要我们对强化学习在LLM推理中的影响进行根本性的重新思考，并需要更好的范式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习通过奖励验证（RLVR）增强了LLM在数学和编程任务中的推理能力。</li>
<li>通过测量pass@\textit{k}指标发现，在较大的k值下，基础模型的性能可与强化学习模型相当或更高。</li>
<li>强化学习训练使模型输出偏向于更容易产生奖励的路径，从而提高性能。</li>
<li>强化学习并不激发新的推理模式，其展现的推理路径已包含在基础模型的采样分布中。</li>
<li>强化学习训练会导致模型推理能力边界较窄，相较于基础模型。</li>
<li>在视觉推理任务中，使用RLVR训练也出现了类似的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13837">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1145fa10a074c5138153deb614a9a7b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c356693220910067cc819f4d5fdaea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e6f577c8731eed685abeaa9d193db2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da3f9fac7a8a67e71f5c7441b7c8984a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Intention-Grounding-for-Egocentric-Assistants"><a href="#Visual-Intention-Grounding-for-Egocentric-Assistants" class="headerlink" title="Visual Intention Grounding for Egocentric Assistants"></a>Visual Intention Grounding for Egocentric Assistants</h2><p><strong>Authors:Pengzhan Sun, Junbin Xiao, Tze Ho Elden Tse, Yicong Li, Arjun Akula, Angela Yao</strong></p>
<p>Visual grounding associates textual descriptions with objects in an image. Conventional methods target third-person image inputs and named object queries. In applications such as AI assistants, the perspective shifts – inputs are egocentric, and objects may be referred to implicitly through needs and intentions. To bridge this gap, we introduce EgoIntention, the first dataset for egocentric visual intention grounding. EgoIntention challenges multimodal LLMs to 1) understand and ignore unintended contextual objects and 2) reason about uncommon object functionalities. Benchmark results show that current models misidentify context objects and lack affordance understanding in egocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it enables hybrid training with normal descriptions and egocentric intentions with a chained intention reasoning and object grounding mechanism. RoG significantly outperforms naive finetuning and hybrid training on EgoIntention, while maintaining or slightly improving naive description grounding. This advancement enables unified visual grounding for egocentric and exocentric visual inputs while handling explicit object queries and implicit human intentions. </p>
<blockquote>
<p>视觉定位是将文本描述与图像中的物体关联起来。传统的方法主要针对第三人称图像输入和命名物体查询。在人工智能助手等应用中，视角会发生变化——输入是自我中心的，物体可能通过需求和意图被隐含地提及。为了弥补这一差距，我们引入了EgoIntention，这是第一个针对自我中心视觉意图定位的数据集。EgoIntention挑战了跨模态大型语言模型，使其能够理解和忽略非上下文中的物体，并对不寻常物体的功能进行推理。基准测试结果表明，当前模型会错误识别上下文中的物体，在自我中心视角中缺乏功能理解。我们还提出了Reason-to-Ground（RoG）指令微调方法；它通过混合正常描述和自我中心意图的混合训练，以及链式意图推理和物体定位机制，实现了显著的效果。在EgoIntention数据集上，RoG相较于简单微调模型和混合训练模型具有显著优势，同时在简单的描述定位上能够保持或稍微提高性能。这一进展使统一处理自我中心和外部中心的视觉输入，以及处理显性物体查询和隐性人类意图成为可能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13621v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>视觉定位是将文本描述与图像中的物体相关联。传统方法主要针对第三人称图像输入和命名物体查询。在人工智能助手等应用中，视角会发生变化——输入是以自我为中心的，物体可能通过需求和意图隐含地指代。为了弥补这一差距，我们推出了EgoIntention，这是首个针对以自我为中心的可视意图定位的数据集。EgoIntention挑战了跨模态大型语言模型，以理解和忽略无意中的上下文物体以及推理不寻常的物体功能。基准测试结果显示，当前模型在识别上下文物体方面存在缺陷，在以自我为中心的观点中缺乏承担理解的能力。我们还提出了Reason-to-Ground（RoG）指令调整；它通过混合正常描述和以自我为中心的意图进行混合训练，并带有连锁意图推理和物体定位机制。RoG在EgoIntention上的表现显著优于简单的微调器和混合训练，同时在描述定位方面保持或略有改进。这一进展使得统一以自我为中心和以外部为中心的可视输入的定位成为可能，同时处理明确的物体查询和隐含的人类意图。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉定位结合了文本描述和图像中的物体。</li>
<li>传统方法主要集中在第三人称图像输入和命名物体查询上。</li>
<li>在如AI助手等应用中，视角转变为以自我为中心，物体通过需求和意图隐含指代。</li>
<li>EgoIntention数据集解决了以自我为中心的可视意图定位的挑战。</li>
<li>当前模型在识别上下文物体和理解以自我为中心的视角中的物体功能方面存在缺陷。</li>
<li>Reason-to-Ground（RoG）指令调整通过混合训练提高了模型的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5eb2593b989df76d796c3f26fa1dc8ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87de39a59ddf9336ae7bea7bf83701eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-465609808cd39e8d9f9fc736226f9a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79aa231b6446fc0a0477843d87cc44de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1714d1c0f1d900ab5e9f241433dfb2b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34cadfb8e03dbecd652e763b43040641.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Compile-Scene-Graphs-with-Reinforcement-Learning"><a href="#Compile-Scene-Graphs-with-Reinforcement-Learning" class="headerlink" title="Compile Scene Graphs with Reinforcement Learning"></a>Compile Scene Graphs with Reinforcement Learning</h2><p><strong>Authors:Zuyao Chen, Jinlin Wu, Zhen Lei, Marc Pollefeys, Chang Wen Chen</strong></p>
<p>Next token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. Given the structured nature of scene graphs, we design a graph-centric reward function that integrates node-level rewards, edge-level rewards, and a format consistency reward. Our experiments demonstrate that rule-based RL substantially enhances model performance in the SGG task, achieving a zero failure rate–unlike supervised fine-tuning (SFT), which struggles to generalize effectively. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG">https://github.com/gpt4vision/R1-SGG</a>. </p>
<blockquote>
<p>接下来令牌预测是训练大型语言模型（LLM）的基本原则，强化学习（RL）进一步增强了其推理性能。作为对语言、图像、视频等多种模式进行有效建模的一种手段，使用大型语言模型进行端到端的结构化视觉表示提取（如场景图）仍然缺乏研究。这需要模型准确生成一组对象和关系三元组，而不是逐个生成文本令牌。为了实现这一点，我们引入了R1-SGG，这是一个多模态大型语言模型（M-LLM），最初通过场景图数据集上的监督微调（SFT）进行训练，随后使用强化学习进行精炼，以提高其端到端生成场景图的能力。SFT遵循传统的提示-响应范式，而RL需要设计有效的奖励信号。考虑到场景图的结构性特点，我们设计了一个以图形为中心的奖励函数，它结合了节点级奖励、边缘级奖励和格式一致性奖励。我们的实验表明，基于规则的RL在很大程度上提高了SGG任务中的模型性能，实现了零失败率——这与在有效推广方面表现挣扎的监督微调（SFT）形成鲜明对比。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/gpt4vision/R1-SGG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gpt4vision/R1-SGG找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）采用下一代令牌预测原则进行训练，强化学习（RL）进一步提高其推理性能。尽管LLM在端到端提取结构化视觉表示（如场景图）方面有着广泛的应用前景，但相关研究仍然不足。本研究引入R1-SGG，一种多模态LLM（M-LLM），采用场景图数据集进行基于监督微调（SFT）的初步训练，然后使用强化学习进一步优化，旨在提高其生成场景图的能力。结合节点级别和边缘级别的奖励以及格式一致性奖励，设计了一个以图形为中心的奖励函数。实验表明，基于规则的RL在SGG任务中显著提高模型性能，达到零故障率。代码可在GitHub上获取。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）基于下一代令牌预测原则进行训练。</li>
<li>强化学习（RL）增强LLM的推理性能。</li>
<li>LLMs在端到端提取结构化视觉表示（如场景图）方面的应用尚待探索。</li>
<li>R1-SGG是一种多模态LLM，旨在提高生成场景图的能力。</li>
<li>R1-SGG通过监督微调（SFT）初步训练，并使用强化学习进一步优化。</li>
<li>设计了一个以图形为中心的奖励函数，包括节点级别、边缘级别和格式一致性奖励。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13617">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-679f83e66c9dedde7c38285f0b1755c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3815c4e5f4e07df358d37321a015b84f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CoT-RAG-Integrating-Chain-of-Thought-and-Retrieval-Augmented-Generation-to-Enhance-Reasoning-in-Large-Language-Models"><a href="#CoT-RAG-Integrating-Chain-of-Thought-and-Retrieval-Augmented-Generation-to-Enhance-Reasoning-in-Large-Language-Models" class="headerlink" title="CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation   to Enhance Reasoning in Large Language Models"></a>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation   to Enhance Reasoning in Large Language Models</h2><p><strong>Authors:Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui</strong></p>
<p>While chain-of-thought (CoT) reasoning improves the performance of large language models (LLMs) in complex tasks, it still has two main challenges: the low reliability of relying solely on LLMs to generate reasoning chains and the interference of natural language reasoning chains on the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor. We conduct a comprehensive evaluation on nine public datasets, covering three reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%. Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable accuracy and efficient execution, highlighting its strong practical applicability and scalability. </p>
<blockquote>
<p>虽然链式思维（CoT）推理在复杂任务中提高了大型语言模型（LLM）的性能，但它仍然面临两个主要挑战：仅依赖LLM生成推理链的低可靠性以及自然语言推理链对LLM推理逻辑的影响。为了解决这些问题，我们提出了CoT-RAG这一新型推理框架，它包含三个关键设计：（i）知识图谱驱动的CoT生成，利用知识图谱来调节LLM的推理链生成，从而提高推理的可信度；（ii）可学习的知识案例感知RAG，将检索增强生成（RAG）纳入知识图谱，以检索相关的子案例和子描述，为LLM提供可学习的信息；（iii）伪程序提示执行，鼓励LLM在伪程序中执行推理任务，提高逻辑严谨性。我们在九个公开数据集上进行了全面评估，涵盖三种推理问题。与最先进的方法相比，CoT-RAG在准确率上有了显著的提高，范围在4.0%到23.0%之间。此外，在四个领域特定的数据集上进行测试，CoT-RAG显示出卓越的准确性和高效的执行力，突显了其强大的实用性、适用性和可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13534v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>链式思维（CoT）推理虽能提高大型语言模型（LLM）在复杂任务中的表现，但仍面临两大挑战。为解决这些问题，提出CoT-RAG这一新型推理框架，包括知识图谱驱动CoT生成、可学习知识案例感知RAG和伪程序提示执行三个关键设计。在九个公共数据集上的全面评估表明，与现有先进方法相比，CoT-RAG的准确率显著提高，范围在4.0%到23.0%之间。在四个领域特定数据集上的测试进一步凸显了其强大的实用性、可扩展性和高效执行。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>链式思维（CoT）推理在大型语言模型（LLM）中虽有优势，但仍存在可靠性低和干扰推理链生成的问题。</li>
<li>提出CoT-RAG新型推理框架，包括知识图谱驱动CoT生成、可学习知识案例感知RAG和伪程序提示执行三大关键设计。</li>
<li>知识图谱用于调制LLM的推理链生成，提高推理可信度。</li>
<li>检索增强生成（RAG）技术被纳入知识图谱中，可检索相关子案例和子描述，为LLM提供可学习信息。</li>
<li>CoT-RAG采用伪程序提示执行，鼓励LLM在伪程序中执行推理任务，提高逻辑严谨性。</li>
<li>在九个公共数据集上的评估显示，CoT-RAG相比最先进的方法准确率显著提高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13534">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8075d9a5056ec2344954ec40883596bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12322fccc3c499be3d9118b81e987b09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d0131c37711eaae57953e5fca9b4465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c402f67fdb1347387993ebc453e23c3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SolarZip-An-Efficient-and-Adaptive-Compression-Framework-for-solar-EUV-imaging-data"><a href="#SolarZip-An-Efficient-and-Adaptive-Compression-Framework-for-solar-EUV-imaging-data" class="headerlink" title="SolarZip: An Efficient and Adaptive Compression Framework for solar EUV   imaging data"></a>SolarZip: An Efficient and Adaptive Compression Framework for solar EUV   imaging data</h2><p><strong>Authors:Zedong Liu, Song Tan, Alexander Warmuth, Frédéric Schuller, Yun Hong, Wenjing Huang, Yida Gu, Bojing Zhu, Guangming Tan, Dingwen Tao</strong></p>
<p>Context: With the advancement of solar physics research, next-generation solar space missions and ground-based telescopes face significant challenges in efficiently transmitting and&#x2F;or storing large-scale observational data. Aims: We develop an efficient compression and evaluation framework for solar EUV data, specifically optimized for Solar Orbiter Extreme Ultraviolet Imager (EUI) data, significantly reducing data volume while preserving scientific usability. Methods: We systematically evaluated four error-bounded lossy compressors across two EUI datasets. However, the existing methods cannot perfectly handle the EUI datasets (with continuously changing distance and significant resolution differences). Motivated by this, we develop an adaptive hybrid compression strategy with optimized interpolation predictors. Moreover, we designed a two-stage evaluation framework integrating distortion analysis with downstream scientific workflows, ensuring that observational analysis is not affected at high compression ratios. Results: Our framework SolarZip achieved up to 800x reduction for Full Sun Imager (FSI) data and 500x for High Resolution Imager (HRI$_{\text{EUV}}$) data. It significantly outperformed both traditional and advanced algorithms, achieving 3-50x higher compression ratios than traditional algorithms, surpassing the second-best algorithm by up to 30%. Simulation experiments verified that SolarZip can reduce data transmission time by up to 270x while ensuring the preservation of scientific usability. Conclusions: The SolarZip framework significantly enhances solar observational data compression efficiency while preserving scientific usability by dynamically selecting optimal compression methods based on observational scenarios and user requirements. This provides a promising data management solution for deep space missions like Solar Orbiter. </p>
<blockquote>
<p>背景：随着太阳物理学研究的进步，下一代太阳空间任务和地面望远镜在有效地传输和&#x2F;或存储大规模观测数据方面面临重大挑战。目标：我们为太阳轨道器极紫外成像仪（EUI）数据开发了一个高效压缩和评估框架，该框架能在减少数据量的同时保持数据的科学可用性。方法：我们系统地评估了四个误差有界的有损压缩机，跨越两个EUI数据集。然而，现有方法无法完美处理EUI数据集（具有连续变化的距离和显著的分辨率差异）。受此启发，我们开发了一种带有优化插值预测器的自适应混合压缩策略。此外，我们设计了一个两阶段的评估框架，将失真分析与下游科学工作流程相结合，确保在高压缩比下观测分析不受影响。结果：我们的SolarZip框架实现了对全日成像仪（FSI）数据高达800倍的压缩比，而对高分辨率成像仪（HRI$_{EUV}$）数据则实现了高达500倍的压缩比。与传统的先进算法相比，它大大优于这些算法，实现了高达传统算法3至50倍的高压缩比，并超过了第二好的算法高达30%。模拟实验证实，SolarZip能在确保科学可用性的前提下，将数据传输时间缩短高达270倍。结论：SolarZip框架通过根据观测场景和用户要求动态选择最佳压缩方法，在保持科学可用性的同时大大提高了太阳观测数据的压缩效率。这为深空任务如太阳能轨道器提供了有前景的数据管理解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13504v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着太阳物理学研究的进展，下一代太空任务和地面望远镜在传输和存储大规模观测数据时面临巨大挑战。本文开发了一种针对太阳轨道器极紫外成像仪（EUI）数据的压缩与评估框架，能够在减少数据量的同时保持科学可用性。通过对四种误差限制有损压缩器的系统评估，提出了一种自适应混合压缩策略，并设计了包含失真分析和下游科学工作流程的两阶段评估框架。结果显示，SolarZip框架对全日成像仪（FSI）数据的压缩比达到800倍，对高分辨率极紫外成像仪（HRI）数据的压缩比为500倍。与传统的先进算法相比，SolarZip框架压缩效率更高，且能保持科学可用性。此外，SolarZip能减少数据传输时间高达270倍。结论：SolarZip框架通过动态选择最佳压缩方法，提高了太阳观测数据的压缩效率并保持了科学可用性，为深空任务如Solar Orbiter提供了有前景的数据管理解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>SolarZip框架旨在优化下一代太阳观测数据的压缩效率，特别是在处理太阳轨道器极紫外成像仪（EUI）数据时效果显著。</li>
<li>SolarZip框架通过开发自适应混合压缩策略，优化了插值预测器，提高了数据压缩比。</li>
<li>SolarZip框架通过两阶段评估框架确保了压缩数据的质量和科学可用性，包括失真分析和下游科学工作流程的集成。</li>
<li>SolarZip实现了高达800倍的FSI数据压缩比和500倍的HRI数据压缩比，显著优于传统和先进算法。</li>
<li>SolarZip框架在保证科学质量的前提下，能有效减少数据传输时间，最高可达270倍。</li>
<li>SolarZip框架提供了一个有前途的数据管理解决方案，适用于需要高效处理大规模观测数据的深空任务，如Solar Orbiter。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13504">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1bc4bbb22bbcdeb146bff6dd08aebbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edf7280ad144aced663ea72e86a8406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b3d16131fb4de6b6ee96725ee15e6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bcc25d336440cc72758051a95cfd885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f389cf336e2b6b4d2ca743b938d7c94.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prejudge-Before-Think-Enhancing-Large-Language-Models-at-Test-Time-by-Process-Prejudge-Reasoning"><a href="#Prejudge-Before-Think-Enhancing-Large-Language-Models-at-Test-Time-by-Process-Prejudge-Reasoning" class="headerlink" title="Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by   Process Prejudge Reasoning"></a>Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by   Process Prejudge Reasoning</h2><p><strong>Authors:Jianing Wang, Jin Jiang, Yang Liu, Mengdi Zhang, Xunliang Cai</strong></p>
<p>In this paper, we introduce a new \emph{process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent reasoning steps, similar to people sometimes pausing to think about what mistakes may occur and how to avoid them, rather than relying solely on trial and error. Specifically, we define a prejudge node in the rationale, which represents a reasoning step, with at least one step that follows the prejudge node that has no paths toward the correct answer. To synthesize the prejudge reasoning process, we present an automated reasoning framework with a dynamic tree-searching strategy. This framework requires only one LLM to perform answer judging, response critiquing, prejudge generation, and thought completion. Furthermore, we develop a two-phase training mechanism with supervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance the reasoning capabilities of LLMs. Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs. Code and data is released at <a target="_blank" rel="noopener" href="https://github.com/wjn1996/Prejudge-Before-Think">https://github.com/wjn1996/Prejudge-Before-Think</a>. </p>
<blockquote>
<p>本文介绍了一种新的大型语言模型（LLM）推理中的“过程预判”策略，以展示通过过程预判进行引导，可以使LLM自适应地预测在推进后续推理步骤时遇到的错误，类似于人们有时会停下来思考可能出现的错误以及如何避免错误，而不是仅仅依赖试错。具体来说，我们在理由中定义了一个预判节点，代表一个推理步骤，至少有一个步骤跟随预判节点，该节点没有通向正确答案的路径。为了综合预判推理过程，我们提出了一个动态树搜索策略的自动化推理框架。该框架只需要一个LLM来完成答案判断、响应评价、预判生成和思维完善。此外，我们开发了一种两阶段训练机制，采用有监督微调（SFT）和强化学习（RL），以进一步提高LLM的推理能力。来自竞赛级复杂推理的实验结果表明，我们的方法能够教会模型在思考之前进行预判，并显著提高LLM的推理能力。相关代码和数据已发布在<a target="_blank" rel="noopener" href="https://github.com/wjn1996/Prejudge-Before-Think%E3%80%82">https://github.com/wjn1996/Prejudge-Before-Think。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13500v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本论文提出了一种新的预判断过程策略，用于LLM推理。通过引入预判断节点，模型能够在推理过程中预测可能出现的错误，并自适应地调整后续推理步骤，从而提高推理能力。实验结果表明，该方法能显著提高LLM的推理能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入预判断过程策略，使LLM在推理过程中能够预测并适应可能出现的错误。</li>
<li>定义预判断节点为包含至少一个后续步骤的节点，这些步骤不包含通向正确答案的路径。</li>
<li>提出自动化推理框架，采用动态树搜索策略进行预判断推理过程。</li>
<li>仅需要一个LLM来完成答案判断、响应评价、预判断生成和思维补全。</li>
<li>采用两阶段训练机制，包括监督微调（SFT）和强化学习（RL），进一步增强LLM的推理能力。</li>
<li>实验结果表明，预判断过程能够提高LLM在竞赛级复杂推理任务中的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13500">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5b8b7624eaad004f21d7b40a438e9878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7cd6e8205a0146af0709f293e168bf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0a37720db7a13d8f11c8845ffabe660.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bab31fc5a97ca8939454ab8d21fe4c6c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-16e2d7f71ebb600fe90f6546e79bd503.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Everything-You-Wanted-to-Know-About-LLM-based-Vulnerability-Detection-But-Were-Afraid-to-Ask"><a href="#Everything-You-Wanted-to-Know-About-LLM-based-Vulnerability-Detection-But-Were-Afraid-to-Ask" class="headerlink" title="Everything You Wanted to Know About LLM-based Vulnerability Detection   But Were Afraid to Ask"></a>Everything You Wanted to Know About LLM-based Vulnerability Detection   But Were Afraid to Ask</h2><p><strong>Authors:Yue Li, Xiao Li, Hao Wu, Minghui Xu, Yue Zhang, Xiuzhen Cheng, Fengyuan Xu, Sheng Zhong</strong></p>
<p>Large Language Models are a promising tool for automated vulnerability detection, thanks to their success in code generation and repair. However, despite widespread adoption, a critical question remains: Are LLMs truly effective at detecting real-world vulnerabilities? Current evaluations, which often assess models on isolated functions or files, ignore the broader execution and data-flow context essential for understanding vulnerabilities. This oversight leads to two types of misleading outcomes: incorrect conclusions and flawed rationales, collectively undermining the reliability of prior assessments. Therefore, in this paper, we challenge three widely held community beliefs: that LLMs are (i) unreliable, (ii) insensitive to code patches, and (iii) performance-plateaued across model scales. We argue that these beliefs are artifacts of context-deprived evaluations. To address this, we propose CORRECT (Context-Rich Reasoning Evaluation of Code with Trust), a new evaluation framework that systematically incorporates contextual information into LLM-based vulnerability detection. We construct a context-rich dataset of 2,000 vulnerable-patched program pairs spanning 99 CWEs and evaluate 13 LLMs across four model families. Our framework elicits both binary predictions and natural-language rationales, which are further validated using LLM-as-a-judge techniques. Our findings overturn existing misconceptions. When provided with sufficient context, SOTA LLMs achieve significantly improved performance (e.g., 0.7 F1-score on key CWEs), with 0.8 precision. We show that most false positives stem from reasoning errors rather than misclassification, and that while model and test-time scaling improve performance, they introduce diminishing returns and trade-offs in recall. Finally, we uncover new flaws in current LLM-based detection systems, such as limited generalization and overthinking biases. </p>
<blockquote>
<p>大型语言模型（LLMs）在自动漏洞检测方面展现出巨大的潜力，其在代码生成和修复方面的成功功不可没。然而，尽管其广泛应用，仍有一个关键问题悬而未决：LLMs在检测现实世界中的漏洞时是否真正有效？现有的评估通常只在孤立的函数或文件上评估模型，忽略了理解漏洞所必需的更广泛的执行和数据流上下文。这种监督导致了两种误导性的结果：错误的结论和缺陷理由，共同削弱了先前评估的可靠性。因此，本文中，我们挑战了社区中三个普遍存在的观点：（i）LLMs不可靠，（ii）对代码补丁不敏感，（iii）模型规模扩大性能停滞不前。我们认为这些观点都是缺乏上下文评估所产生的结果。为了解决这一问题，我们提出了CORRECT（基于信任的丰富上下文推理评估代码），这是一个新的评估框架，系统地结合上下文信息到基于LLM的漏洞检测中。我们构建了一个包含2000个脆弱补丁程序对的丰富上下文数据集，涵盖了99个CWE，评估了四个家族中的13个LLM。我们的框架引发了二元预测和自然语言理由，并进一步使用LLM-as-a-judge技术进行验证。我们的研究结果表明，在提供足够上下文的情况下，最新LLM的性能显著提高（例如在关键CWE上达到0.7的F1分数），精确度为0.8。我们发现大多数误报源于推理错误而非误分类，并且虽然模型和测试时的缩放可以提高性能，但它们会带来回报递减和召回率方面的权衡。最后，我们发现了当前基于LLM的检测系统中的新漏洞，例如有限的泛化能力和过度思考偏见。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在自动化漏洞检测方面展现出潜力，尤其在代码生成和修复方面。然而，关于其是否能有效检测真实世界漏洞的问题仍存在疑问。当前评估模型的方法常常局限于对孤立函数或文件的评估，忽略了执行和数据流上下文的重要性，这导致评估结果存在误导性。为解决这一问题，本文提出了一个新的评估框架CORRECT，该框架系统地结合上下文信息进行基于大型语言模型的漏洞检测。通过构建包含2000个漏洞修复程序对的丰富上下文数据集，并对四个模型家族的13个大型语言模型进行评估，发现当提供足够的上下文时，当前顶尖的大型语言模型性能显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在自动化漏洞检测方面具有潜力。</li>
<li>当前漏洞检测评估方法忽略了执行和数据流上下文的重要性。</li>
<li>新的评估框架CORRECT结合上下文信息进行大型语言模型漏洞检测。</li>
<li>提供足够上下文时，顶尖大型语言模型性能显著提升。</li>
<li>大型语言模型的误报主要源于推理错误而非误分类。</li>
<li>模型和测试时的扩展虽然能提高性能，但存在回报递减和召回率权衡的问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13474">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0fbb96b8732d54ef58b2e559a0a30921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f55aa529208c482b116bc6ab8ac230b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb0737508e214570459c163a2f9a1f41.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a0cd3eae0718cf031ebf00fa2d2899c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization"><a href="#Chain-of-Thought-Textual-Reasoning-for-Few-shot-Temporal-Action-Localization" class="headerlink" title="Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization"></a>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization</h2><p><strong>Authors:Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma</strong></p>
<p>Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model’s ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark. </p>
<blockquote>
<p>传统的时间动作定位（TAL）方法依赖于大量的详细标注数据，而少样本TAL则通过仅使用少量的训练样本来识别未见过的动作类别，减少了这种依赖。然而，现有的少样本TAL方法通常只关注视频层面的信息，忽视了文本信息，这可以为定位任务提供有价值的语义支持。因此，我们提出了一种新的基于思维链文本推理的少样本时间动作定位方法，以提高定位性能。具体来说，我们设计了一种新颖的少样本学习框架，利用文本语义信息来提高模型捕捉动作共性和变化的能力，其中包括一个语义感知的文本-视觉对齐模块，旨在在不同层次上对齐查询和支持视频。同时，为了更好地表达文本层面上动作的时空依赖和因果关系，帮助动作定位，我们设计了一种类似思维链（CoT）的推理方法，逐步引导视觉语言模型（VLM）和大型语言模型（LLM）为视频生成类似思维链的文本描述。生成的文本可以捕捉比视觉特征更多的动作变化。我们在公开可用的ActivityNet1.3和THUMOS14数据集上进行了大量实验。我们引入了名为Human-related Anomaly Localization的第一个数据集，并探索了TAL任务在人类异常检测中的应用。实验结果表明，我们提出的方法在单实例和多实例场景中均显著优于现有方法。我们将发布我们的代码、数据和基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Chain-of-Thought文本推理的新的少样本时序动作定位方法。通过设计新的少样本学习框架和语义感知文本视觉对齐模块，该方法利用文本语义信息提高模型捕捉动作共性和变化的能力。同时，通过设计Chain of Thought（CoT）推理方法，协助在文本层面上表达动作间的时序依赖和因果关系，以辅助动作定位。实验结果表明，该方法在ActivityNet1.3和THUMOS14数据集上显著优于现有方法，并在人类异常检测的应用中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于Chain-of-Thought文本推理的少样本时序动作定位方法。</li>
<li>设计了新的少样本学习框架，利用文本语义信息提高模型性能。</li>
<li>引入语义感知文本视觉对齐模块，对齐查询和支撑视频的不同层次。</li>
<li>通过Chain of Thought（CoT）推理方法，在文本层面上表达动作间的时序依赖和因果关系。</li>
<li>生成的文本描述能捕捉比视觉特征更多的动作变化。</li>
<li>在ActivityNet1.3和THUMOS14数据集上进行了广泛实验，并显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-55fea05ebbb7e2ed2badab807e563848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6088af927e869df00f843812c7c55ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c213dbbcc1e0229555b467cf4aa97d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SkyReels-V2-Infinite-length-Film-Generative-Model"><a href="#SkyReels-V2-Infinite-length-Film-Generative-Model" class="headerlink" title="SkyReels-V2: Infinite-length Film Generative Model"></a>SkyReels-V2: Infinite-length Film Generative Model</h2><p><strong>Authors:Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, Yahui Zhou</strong></p>
<p>Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs’ inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2">https://github.com/SkyworkAI/SkyReels-V2</a>. </p>
<blockquote>
<p>最近视频生成的进展得益于扩散模型和自回归框架的推动，但仍存在关于提示遵循、视觉质量、运动动态和时长的协调方面的关键挑战：为了增强时间视觉质量而妥协运动动态，受限制的视频时长（5-10秒）以优先考虑分辨率，以及由于通用多模态大型语言模型（MLLM）无法解释电影语法（如镜头构图、演员表情和相机运动）而导致的拍摄意识生成不足。这些交织的限制阻碍了现实的长形式合成和专业电影风格的生成。为了解决这些局限性，我们提出了SkyReels-V2，一种无限长电影生成模型，它协同多模态大型语言模型（MLLM）、多阶段预训练、强化学习和扩散强制框架。首先，我们设计了一种全面的视频结构表示，它结合了多模态LLM的一般描述和子专家模型的详细镜头语言。借助人工标注，我们随后训练了一个统一的视频字幕器，名为SkyCaptioner-V1，以有效地标注视频数据。其次，我们为基本的视频生成建立了渐进式分辨率预训练，然后是四阶段的后训练增强：初始概念平衡的监督微调（SFT）提高了基线质量；使用人工注释和合成失真数据的特定运动强化学习（RL）训练解决了动态伪影问题；我们的扩散强制框架与不断增长的噪声时间表相结合，能够在有效的搜索空间中进行长视频合成；最后的高质量SFT改进了视觉保真度。所有代码和模型均可在<a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SkyworkAI/SkyReels-V2找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13074v2">PDF</a> 31 pages,10 figures</p>
<p><strong>Summary</strong><br>     最近视频生成领域的进展主要得益于扩散模型和自回归框架，但仍面临提示遵循、视觉质量、运动动力和时长等方面的挑战。为解决这些限制，我们提出了SkyReels-V2，一种无限长电影生成模型，融合了多模态大型语言模型（MLLM）、多阶段预训练、强化学习和扩散强制框架。通过设计全面的视频结构表示、训练统一视频标注器、建立渐进式分辨率预训练以及四个阶段的后期训练增强等措施来应对挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频生成领域虽有所进展，但仍面临提示遵循、视觉质量、运动动力和时长等方面的挑战。</li>
<li>SkyReels-V2模型旨在解决这些限制，通过融合MLLM、多阶段预训练、强化学习和扩散强制框架来提升视频生成质量。</li>
<li>全面的视频结构表示结合了多模态LLM的一般描述和子专家模型的详细镜头语言。</li>
<li>使用人类注释训练了统一视频标注器SkyCaptioner-V1，以高效标注视频数据。</li>
<li>建立了渐进式分辨率预训练，为后续的视频生成奠定基础。</li>
<li>四个阶段的后期训练增强措施，包括初始概念平衡的监督微调、针对运动动力的强化学习训练、扩散强制框架以及最终的高质量监督微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-256858801c9eb5e0f5a46a3c88eaa9c8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135bbc853f17d529787e36191b59350b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9fbf10fae977ac87c8ae1741923de30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5302e464edde26d5c78c6efa43188ed7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32f40e05213ea0ac9ecd71eeecc2b78b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Finding-Flawed-Fictions-Evaluating-Complex-Reasoning-in-Language-Models-via-Plot-Hole-Detection"><a href="#Finding-Flawed-Fictions-Evaluating-Complex-Reasoning-in-Language-Models-via-Plot-Hole-Detection" class="headerlink" title="Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models   via Plot Hole Detection"></a>Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models   via Plot Hole Detection</h2><p><strong>Authors:Kabir Ahuja, Melanie Sclar, Yulia Tsvetkov</strong></p>
<p>Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes – inconsistencies in a storyline that break the internal logic or rules of a story’s world – requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMs’ plot hole detection abilities in stories – FlawedFictions – , which is robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with more than 50% and 100% increases in plot hole detection rates with respect to human-written originals. </p>
<blockquote>
<p>故事是人类经验的基本组成部分。深入参与故事和发现情节漏洞——故事情节中的不一致之处破坏了其内部逻辑或故事世界的规则——需要微妙的推理技能，包括追踪实体和事件及其相互作用、抽象思维、实用叙事理解、常识和社会推理以及心智理论。随着大型语言模型（LLM）越来越多地生成、解释和修改文本，严格评估其叙事一致性以及更深层次的语言理解能力变得至关重要。然而，现有的基准测试主要关注于表层理解。在这项工作中，我们提出将情节漏洞检测作为评估LLM的语言理解和推理能力的代理。我们介绍了FlawedFictionsMaker这一新型算法，该算法能够可控且仔细地合成人类写作故事中的情节漏洞。使用该算法，我们构建了一个基准测试来评估LLM的情节漏洞检测能力——FlawedFictions，该测试稳健且不易受到污染的影响，并通过人工过滤确保高质量。我们发现，即使允许先进的LLM进行推理努力，它们在解决FlawedFictions方面的表现仍然不佳，并且随着故事长度的增加，性能会显著下降。最后，我们证明了基于LLM的故事摘要和故事生成容易引入情节漏洞，与人类原始作品的情节漏洞检测率相比，其漏洞检测率分别增加了50%和100%以上。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11900v2">PDF</a> Preprint</p>
<p><strong>Summary</strong>：</p>
<p>本文关注故事中的情节漏洞检测，认为这是评估大型语言模型深层次语言理解和推理能力的重要标准。文章提出了一种新算法FlawedFictionsMaker，用于合成带有情节漏洞的故事，并构建了一个基准测试FlawedFictions，以评估语言模型检测故事中的情节漏洞的能力。实验发现，即使是最先进的大型语言模型，在面临FlawedFictions挑战时也难以准确解决问题，随着故事长度的增加，性能会显著下降。此外，基于大型语言模型的故事摘要和故事生成也容易产生情节漏洞。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>故事中的情节漏洞检测是评估大型语言模型深层次语言理解和推理能力的重要标准。</li>
<li>FlawedFictionsMaker算法能够合成带有情节漏洞的故事。</li>
<li>FlawedFictions基准测试用于评估语言模型检测故事中的情节漏洞的能力。</li>
<li>先进的大型语言模型在FlawedFictions挑战方面表现不佳，随着故事长度的增加，性能下降。</li>
<li>基于大型语言模型的故事摘要生成易产生情节漏洞。</li>
<li>大型语言模型在故事生成中也容易引入情节漏洞。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11900">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7b2d26454fb7776c2e635d4c32867819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28efee1a5010ff446b3e688aa18e81d5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents"><a href="#GUI-R1-A-Generalist-R1-Style-Vision-Language-Action-Model-For-GUI-Agents" class="headerlink" title="GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents"></a>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI   Agents</h2><p><strong>Authors:Run Luo, Lu Wang, Wanwei He, Xiaobo Xia</strong></p>
<p>Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. </p>
<blockquote>
<p>现有的图形用户界面（GUI）代理构建工作主要依赖于在大视觉语言模型（LVLMs）上进行的有监督微调训练范式。然而，这种方法不仅需求大量的训练数据，而且在理解GUI截图和泛化到未见过的界面方面也存在困难。这一问题极大地限制了其在现实场景中的应用，尤其是高级任务。受大型推理模型中的强化微调（RFT）的启发（例如DeepSeek-R1），强化学习能够提升大型语言模型在现实环境中的问题解决能力。因此，我们提出名为“名称”的强化学习框架，它是首个旨在通过统一动作空间规则建模增强LVLMs在现实高级任务场景中的GUI能力。通过利用跨多个平台（包括Windows、Linux、MacOS、Android和Web）的少量精心挑选的高质量数据，并采用如集团相对策略优化（GRPO）等策略优化算法来更新模型，“名称”仅使用0.02%的数据（3K对1.3M）就实现了在跨越三个不同平台（移动、桌面和网页）的八个基准测试中的卓越性能，超越了之前的先进方法，如OS-Atlas。这些结果证明了基于统一动作空间规则建模的强化学习在提升LVLMs执行现实GUI代理任务的能力方面具有巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10458v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型视觉语言模型（LVLMs）的图形用户界面（GUI）代理构建工作大多依赖于监督精细调整的训练模式。然而，这种方法不仅需求大量的训练数据，而且在理解GUI截图和泛化到未见过的界面上存在困难。本文受大型推理模型中的强化精细调整（RFT）的启发，提出了一个强化学习框架——名为\name，旨在通过统一动作空间规则建模，提高LVLMs在真实世界高级任务场景中的GUI能力。通过使用跨多个平台的小量精心挑选的高质量数据，并采用群体相对策略优化（GRPO）等策略优化算法来更新模型，\name在仅使用0.02%（3K vs 13M）数据的情况下，在涵盖三个不同平台（移动、桌面和网页）的八个基准测试中实现了优于OS-Atlas等现有先进方法的性能。这显示了基于统一动作空间规则建模的强化学习在提升LVLMs执行真实世界GUI代理任务方面的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUI代理的构建主要依赖于大型视觉语言模型的监督精细调整训练模式，但需要大量数据和面临理解和泛化难题。</li>
<li>强化学习框架\name被提出，用于通过统一动作空间规则建模提高LVLMs在真实世界高级任务场景中的GUI能力。</li>
<li>\name使用跨平台高质量小量数据进行训练，通过策略优化算法如GRPO更新模型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-40e44cce157ab734b1ce7aa7a91d195b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8b83a33fe04aa5f478735789bdd633d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1378358a130b9ffbb33425047048d0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation"><a href="#DocAgent-A-Multi-Agent-System-for-Automated-Code-Documentation-Generation" class="headerlink" title="DocAgent: A Multi-Agent System for Automated Code Documentation   Generation"></a>DocAgent: A Multi-Agent System for Automated Code Documentation   Generation</h2><p><strong>Authors:Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Grey Yang</strong></p>
<p>High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories. </p>
<blockquote>
<p>高质量的代码文档对软件开发至关重要，特别是在人工智能时代。然而，使用大型语言模型（LLM）自动生成文档仍然具有挑战性，因为现有方法通常会产生不完整、无帮助或事实错误的输出。我们引入了DocAgent，这是一个使用拓扑代码处理进行增量上下文构建的新型多智能体协作系统。专门的智能体（阅读器、搜索器、编写器、验证器、协调器）协同生成文档。我们还提出了一个多方面的评估框架，评估文档的完整性、帮助性和真实性。综合实验表明，DocAgent持续且显著优于基线。我们的消融研究证实了拓扑处理顺序的重要作用。DocAgent为复杂和专有存储库中的可靠代码文档生成提供了稳健的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08725v2">PDF</a> Public Repo: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/DocAgent">https://github.com/facebookresearch/DocAgent</a></p>
<p><strong>Summary</strong></p>
<p>本文强调高质量代码文档在软件开发中的重要性，特别是在人工智能时代。然而，使用大型语言模型（LLMs）自动生成文档仍然具有挑战性，因为现有方法往往产生不完整、无帮助或事实错误的输出。为此，本文介绍了DocAgent，一种利用拓扑代码处理的多智能体协作系统，用于增量构建上下文。通过专门的智能体（阅读器、搜索器、编写器、验证器、协调器）协同生成文档。同时，提出了一种多维评估框架，评估文档的完整性、帮助性和真实性。实验表明，DocAgent在复杂和专有存储库中显著优于基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高质量代码文档在软件开发中的重要性。</li>
<li>使用大型语言模型自动生成代码文档的现有挑战：不完整、无帮助或事实错误的输出。</li>
<li>DocAgent系统介绍：利用拓扑代码处理和多智能体协作生成文档。</li>
<li>DocAgent系统中的智能体角色：阅读器、搜索器、编写器、验证器和协调器。</li>
<li>多维评估框架：评估文档的完整性、帮助性和真实性。</li>
<li>DocAgent在复杂和专有存储库中的性能显著优于基线方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08725">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c961904b2d4b0a387b2ed18b9a2add40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fd0534526e9f43e1ed4683ac785090c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-933e5dcbc145e1cb1361f03356dd2b34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28680bc575a8c0f69c2ab5c8868cc5f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcf3626f73471bc8ff67f34fdec24a66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cae1fe7424320522f3e73e14023dd673.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ReaRAG-Knowledge-guided-Reasoning-Enhances-Factuality-of-Large-Reasoning-Models-with-Iterative-Retrieval-Augmented-Generation"><a href="#ReaRAG-Knowledge-guided-Reasoning-Enhances-Factuality-of-Large-Reasoning-Models-with-Iterative-Retrieval-Augmented-Generation" class="headerlink" title="ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large   Reasoning Models with Iterative Retrieval Augmented Generation"></a>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large   Reasoning Models with Iterative Retrieval Augmented Generation</h2><p><strong>Authors:Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li</strong></p>
<p>Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG’s strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs’ factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG). </p>
<blockquote>
<p>大型推理模型（LRMs）展现出显著的推理能力，但主要依赖于参数知识，这限制了事实准确性。虽然最近的工作为基于强化学习（RL）的LRMs配备了检索能力，但它们在推理时过于深思熟虑，缺乏稳健性，降低了在问答（QA）任务中的有效性。为了解决这一问题，我们提出了ReaRAG，这是一种增强事实性的推理模型，它可以在不进行过多次迭代的情况下探索各种查询。我们的解决方案包括一个具有推理链长度上限的新型数据构建框架。具体来说，我们首先利用LRM进行深思熟虑的生成，然后从预定的动作空间（搜索和完成）中选择一个动作。对于搜索动作，会对RAG引擎执行一个查询，返回的结果将作为观察结果来指导后续的推理步骤。这个过程将一直迭代，直到选择一个完成动作。ReaRAG的强推理能力使其在多跳问答任务上的表现优于现有基线。进一步的分析突显了其强大的反思能力，能够识别错误并优化其推理轨迹。我们的研究提高了LRMs的事实性，同时有效地将稳健推理整合到增强检索生成（RAG）中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21729v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型推理模型展现出卓越的推理能力，但主要依赖参数知识，限制了事实准确性。本研究提出一种增强事实性的推理模型ReaRAG，通过构建新型数据框架并限制推理链长度，解决现有模型过度思考、缺乏稳健性的问题。ReaRAG利用LRM进行深思熟虑，从预定义动作空间（搜索与完成）中选择动作。搜索动作通过RAG引擎执行查询并返回结果指导后续推理步骤。本研究增强了LRM的事实性，并有效集成了稳健推理，提高了多跳问答任务上的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型主要依赖参数知识，限制了事实准确性。</li>
<li>ReaRAG模型旨在解决现有模型过度思考、缺乏稳健性的问题。</li>
<li>ReaRAG通过构建新型数据框架，并结合预定义动作空间实现增强事实性的推理。</li>
<li>ReaRAG利用LRM进行深思熟虑，并通过搜索动作与RAG引擎交互获取结果。</li>
<li>ReaRAG模型在多跳问答任务上表现出优越性能。</li>
<li>进一步分析显示，ReaRAG具有强大的反思能力，能够识别错误并优化推理轨迹。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21729">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1ce0707003e6a9caeb1c794c94d37862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5beaa5fb2d12eadd28e6fc41ce584d3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2209859632255ba8f9bfbdbb09dc5fb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ecdee8bf1f4e95a2aac8829d3e06799.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bbed99f1971aa9a935920c070ed7910e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-22  Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1799a5c57e34d17fa4a998e98eee9508.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-20  Lightning IR Straightforward Fine-tuning and Inference of   Transformer-based Language Models for Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
