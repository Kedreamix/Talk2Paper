<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  COMPACT COMPositional Atomic-to-Complex Visual Capability Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3e0b279adcebf07254a23f8c5be9f8c9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-02-æ›´æ–°"><a href="#2025-05-02-æ›´æ–°" class="headerlink" title="2025-05-02 æ›´æ–°"></a>2025-05-02 æ›´æ–°</h1><h2 id="COMPACT-COMPositional-Atomic-to-Complex-Visual-Capability-Tuning"><a href="#COMPACT-COMPositional-Atomic-to-Complex-Visual-Capability-Tuning" class="headerlink" title="COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning"></a>COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</h2><p><strong>Authors:Xindi Wu, Hee Seung Hwang, Polina Kirichenko, Olga Russakovsky</strong></p>
<p>Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç®€å•çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹éœ€è¦å¤šç§èƒ½åŠ›çš„å¤æ‚ä»»åŠ¡æ—¶ï¼Œå¦‚åŒæ—¶è¯†åˆ«ç‰©ä½“ã€è®¡æ•°å’Œç†è§£å…¶ç©ºé—´å…³ç³»ï¼Œåˆ™è¡¨ç°å›°éš¾ã€‚è¿™å¯èƒ½æ˜¯ç”±äºè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰è¿™ä¸€MLLMçš„å…³é”®è®­ç»ƒæ­¥éª¤ä¼ ç»Ÿä¸Šä¾§é‡äºæ‰©å¤§æ•°æ®é‡ï¼Œè€Œå¿½è§†äº†è®­ç»ƒå®ä¾‹çš„ç»„æˆå¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†COMPACTï¼ˆCompositional Atomic-to-complexè§†è§‰èƒ½åŠ›è°ƒæ•´ï¼‰ï¼Œå®ƒé€šè¿‡æ˜ç¡®æ§åˆ¶è®­ç»ƒå®ä¾‹çš„ç»„æˆå¤æ‚æ€§æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ã€‚COMPACTçš„æ•°æ®å…è®¸MLLMåœ¨åŸå­èƒ½åŠ›çš„ç»„åˆä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¤æ‚èƒ½åŠ›ã€‚åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCOMPACTåœ¨ä½¿ç”¨ä¸åˆ°å…¶æ•°æ®é¢„ç®—10%çš„æƒ…å†µä¸‹å®ç°äº†ä¸LLaVA-665k VITç›¸å½“çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤æ‚å¤šèƒ½åŠ›ä»»åŠ¡æ—¶ã€‚ä¾‹å¦‚ï¼ŒCOMPACTåœ¨MMStarå’ŒMM-Vetä¸Šçš„è¡¨ç°ç›¸å¯¹äºå…¨è§„æ¨¡VITæœ‰æ˜¾è‘—æ”¹å–„ï¼Œåˆ†åˆ«æé«˜äº†83.3%å’Œ94.0%ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†éœ€è¦å››ç§æˆ–æ›´å¤šåŸå­èƒ½åŠ›çš„ç‰¹åˆ«å¤æ‚é—®é¢˜ä¸Šã€‚COMPACTæä¾›äº†ä¸€ç§å¯æ‰©å±•ã€æ•°æ®é«˜æ•ˆã€è§†è§‰ç»„æˆçš„è°ƒæ•´é…æ–¹ï¼Œä»¥æ”¹è¿›å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21850v1">PDF</a> 17 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨å¤„ç†ç®€å•è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é¢å¯¹éœ€è¦å¤šç§èƒ½åŠ›çš„å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°æ¬ ä½³ï¼Œå¦‚åŒæ—¶è¯†åˆ«ç‰©ä½“ã€è®¡æ•°å’Œç†è§£ç©ºé—´å…³ç³»ç­‰ã€‚è¿™å¯èƒ½æ˜¯ç”±äºä¼ ç»Ÿè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ˆVITï¼‰ä¸»è¦å…³æ³¨æ•°æ®è§„æ¨¡ï¼Œè€Œå¿½è§†è®­ç»ƒå®ä¾‹çš„ç»„æˆå¤æ‚æ€§æ‰€è‡´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†COMPACTæ–¹æ³•ï¼Œå®ƒé€šè¿‡æ§åˆ¶è®­ç»ƒå®ä¾‹çš„ç»„æˆå¤æ‚æ€§æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®é›†ã€‚COMPACTå…è®¸MLLMåœ¨åŸå­èƒ½åŠ›ç»„åˆä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å¤æ‚èƒ½åŠ›ã€‚åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCOMPACTåœ¨ä½¿ç”¨ä¸åˆ°LLaVA-665kVITååˆ†ä¹‹ä¸€çš„æ•°æ®é¢„ç®—çš„æƒ…å†µä¸‹å®ç°äº†ä¸ä¹‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æ¶‰åŠå¤æ‚å¤šèƒ½åŠ›ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä½³ã€‚ä¾‹å¦‚ï¼ŒCOMPACTåœ¨MMStarå’ŒMM-Vetä¸Šçš„è¡¨ç°ç›¸å¯¹äºå…¨é¢è§„æ¨¡çš„VITæœ‰æ˜¾è‘—æ”¹å–„ã€‚COMPACTä¸ºå¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ã€æ•°æ®é«˜æ•ˆã€è§†è§‰ç»„æˆçš„è°ƒæ•´æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤„ç†å¤æ‚è§†è§‰è¯­è¨€ä»»åŠ¡æ—¶é‡åˆ°å›°éš¾ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å¤šç§èƒ½åŠ›å¦‚ç‰©ä½“è¯†åˆ«ã€è®¡æ•°å’Œç©ºé—´ç†è§£ã€‚</li>
<li>ä¼ ç»ŸVITä¸»è¦å…³æ³¨æ•°æ®è§„æ¨¡ï¼Œè€Œå¿½è§†è®­ç»ƒå®ä¾‹çš„ç»„æˆå¤æ‚æ€§ã€‚</li>
<li>COMPACTæ–¹æ³•é€šè¿‡æ§åˆ¶è®­ç»ƒå®ä¾‹çš„ç»„æˆå¤æ‚æ€§æ¥æé«˜MLLMçš„æ€§èƒ½ã€‚</li>
<li>COMPACTå…è®¸MLLMåœ¨åŸå­èƒ½åŠ›ç»„åˆä¸Šè®­ç»ƒï¼Œä»è€Œæ›´é«˜æ•ˆåœ°å­¦ä¹ å¤æ‚èƒ½åŠ›ã€‚</li>
<li>COMPACTåœ¨ä½¿ç”¨è¾ƒå°‘æ•°æ®çš„æƒ…å†µä¸‹å®ç°äº†ä¸LLaVA-665kVITç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä½³ã€‚</li>
<li>COMPACTåœ¨æ¶‰åŠå¤æ‚å¤šèƒ½åŠ›ä»»åŠ¡çš„MMStarå’ŒMM-Vetä¸Šçš„è¡¨ç°æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-251e3d99bb384e3d557264681f0d3866.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ae8a78a4cbf7dd3f36c96bafe54eaaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea7755e7885c563d4d5d016035de4cc0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-160867283c075076622dcabe577ad15b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f83c3057026fb4873a48437f372421f6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DeepSeek-Prover-V2-Advancing-Formal-Mathematical-Reasoning-via-Reinforcement-Learning-for-Subgoal-Decomposition"><a href="#DeepSeek-Prover-V2-Advancing-Formal-Mathematical-Reasoning-via-Reinforcement-Learning-for-Subgoal-Decomposition" class="headerlink" title="DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via   Reinforcement Learning for Subgoal Decomposition"></a>DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via   Reinforcement Learning for Subgoal Decomposition</h2><p><strong>Authors:Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, Chong Ruan</strong></p>
<p>We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3â€™s step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DeepSeek-Prover-V2ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸ºLean 4ä¸­çš„å½¢å¼åŒ–å®šç†è¯æ˜è€Œè®¾è®¡çš„å¤§å‹å¼€æºè¯­è¨€æ¨¡å‹ã€‚å…¶åˆå§‹åŒ–æ•°æ®æ˜¯é€šè¿‡ç”±DeepSeek-V3é©±åŠ¨çš„é€’å½’å®šç†è¯æ˜ç®¡é“æ”¶é›†çš„ã€‚å†·å¯åŠ¨è®­ç»ƒè¿‡ç¨‹å§‹äºæç¤ºDeepSeek-V3å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—å­ç›®æ ‡ã€‚å·²è§£å†³çš„å­ç›®æ ‡çš„è¯æ˜è¢«ç»¼åˆä¸ºæ€ç»´é“¾è¿‡ç¨‹ï¼Œä¸DeepSeek-V3çš„é€æ­¥æ¨ç†ç›¸ç»“åˆï¼Œä¸ºå¼ºåŒ–å­¦ä¹ åˆ›å»ºåˆå§‹å†·å¯åŠ¨ã€‚è¿™ä¸€è¿‡ç¨‹ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†éæ­£å¼å’Œæ­£å¼çš„æ•°å­¦æ¨ç†æ•´åˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­ã€‚ç»“æœæ¨¡å‹DeepSeek-Prover-V2-671Båœ¨ç¥ç»å®šç†è¯æ˜æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œåœ¨MiniF2Fæµ‹è¯•ä¸­çš„é€šè¿‡ç‡ä¸º88.9%ï¼Œå¹¶è§£å†³PutnamBenchä¸­çš„49ä¸ªä¸­çš„658ä¸ªé—®é¢˜ã€‚é™¤äº†æ ‡å‡†åŸºå‡†æµ‹è¯•å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ProverBenchï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«325ä¸ªå½¢å¼åŒ–é—®é¢˜çš„é›†åˆï¼Œä»¥ä¸°å¯Œæˆ‘ä»¬çš„è¯„ä¼°ï¼Œå…¶ä¸­åŒ…æ‹¬æœ€è¿‘AIMEç«èµ›ï¼ˆç¬¬24-25å¹´ï¼‰ç²¾é€‰çš„15ä¸ªé—®é¢˜ã€‚å¯¹è¿™15ä¸ªAIMEé—®é¢˜çš„è¿›ä¸€æ­¥è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹æˆåŠŸè§£å†³äº†å…¶ä¸­6ä¸ªã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDeepSeek-V3ä½¿ç”¨å¤šæ•°æŠ•ç¥¨è§£å†³äº†å…¶ä¸­8ä¸ªé—®é¢˜ï¼Œè¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å½¢å¼ä¸éå½¢å¼æ•°å­¦æ¨ç†ä¹‹é—´çš„å·®è·æ­£åœ¨æ˜¾è‘—ç¼©å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21801v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DeepSeek-Prover-V2æ˜¯ä¸€æ¬¾ç”¨äºLean 4å½¢å¼åŒ–å®šç†è¯æ˜çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡DeepSeek-V3é©±åŠ¨çš„é€’å½’å®šç†è¯æ˜ç®¡é“è¿›è¡Œåˆå§‹åŒ–æ•°æ®é‡‡é›†ï¼Œç»“åˆæ·±åº¦æ€è€ƒä¸é€æ­¥æ¨ç†ï¼Œå®ç°å†·å¯åŠ¨è®­ç»ƒç¨‹åºã€‚è¯¥æ¨¡å‹åœ¨ç¥ç»å®šç†è¯æ˜æ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œåœ¨MiniF2Fæµ‹è¯•ä¸­çš„é€šè¿‡ç‡è¾¾åˆ°äº†88.9%ï¼Œå¹¶è§£å†³PutnamBenchä¸­çš„49ä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ProverBenchï¼ŒåŒ…å«325ä¸ªå½¢å¼åŒ–é—®é¢˜ï¼Œç”¨äºä¸°å¯Œè¯„ä¼°ï¼ŒåŒ…æ‹¬æœ€è¿‘AIMEç«èµ›ï¼ˆç¬¬24-25å¹´ï¼‰ä¸­é€‰æ‹©çš„15ä¸ªé—®é¢˜ã€‚è¿›ä¸€æ­¥è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æˆåŠŸè§£å†³äº†å…¶ä¸­6ä¸ªé—®é¢˜ï¼Œè€ŒDeepSeek-V3é€šè¿‡å¤šæ•°æŠ•ç¥¨è§£å†³äº†å…¶ä¸­8ä¸ªé—®é¢˜ï¼Œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å½¢å¼ä¸éå½¢å¼æ•°å­¦æ¨ç†ä¹‹é—´çš„å·®è·æ­£åœ¨æ˜¾è‘—ç¼©å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-Prover-V2æ˜¯ä¸€ä¸ªç”¨äºå®šç†è¯æ˜çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ç”±DeepSeek-V3é©±åŠ¨çš„é€’å½’å®šç†è¯æ˜ç®¡é“è¿›è¡Œåˆå§‹åŒ–æ•°æ®é‡‡é›†ã€‚</li>
<li>é€šè¿‡ç»“åˆæ·±åº¦æ€è€ƒå’Œé€æ­¥æ¨ç†ï¼Œå®ç°äº†å†·å¯åŠ¨è®­ç»ƒç¨‹åºã€‚</li>
<li>åœ¨ç¥ç»å®šç†è¯æ˜æ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œåœ¨MiniF2Fæµ‹è¯•ä¸­çš„é€šè¿‡ç‡è¾¾åˆ°äº†88.9%ã€‚</li>
<li>æ¨¡å‹æˆåŠŸè§£å†³PutnamBenchä¸­çš„49ä¸ªé—®é¢˜ã€‚</li>
<li>å¼•å…¥ProverBenchä½œä¸ºæ–°çš„è¯„ä¼°å·¥å…·ï¼Œå…¶ä¸­åŒ…æ‹¬æ¥è‡ªAIMEç«èµ›çš„å½¢å¼åŒ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ef543751ec0ec9f23a2e4d8397c5474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76b2c2489950bcd2680edcd10a3661c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f45986e3210815b3e23c1b2005da863.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MAC-Tuning-LLM-Multi-Compositional-Problem-Reasoning-with-Enhanced-Knowledge-Boundary-Awareness"><a href="#MAC-Tuning-LLM-Multi-Compositional-Problem-Reasoning-with-Enhanced-Knowledge-Boundary-Awareness" class="headerlink" title="MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced   Knowledge Boundary Awareness"></a>MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced   Knowledge Boundary Awareness</h2><p><strong>Authors:Junsheng Huang, Zhitao He, Sandeep Polisetty, Qingyun Wang, May Fung</strong></p>
<p>With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œç”Ÿæˆä¸å­˜åœ¨çš„äº‹å®çš„é—®é¢˜ï¼Œå³è¢«ç§°ä¸ºâ€œå¹»è§‰â€çš„é—®é¢˜ï¼Œå·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¹‹å‰å…³äºæé«˜LLMä¿¡å¿ƒä¼°è®¡çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€é—®é¢˜è®¾ç½®ä¸Šã€‚ç„¶è€Œï¼Œåœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„å¤šé—®é¢˜è®¾ç½®ä¸‹ï¼ŒLLMå¯¹å…¶å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†è¾¹ç•Œçš„è®¤è¯†ï¼Œå³éœ€è¦åŒæ—¶å‡†ç¡®å›ç­”å¤šä¸ªé—®é¢˜çš„èƒ½åŠ›ï¼Œä»ç„¶è¢«ç ”ç©¶å¾—å¾ˆå°‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¤šç­”æ¡ˆå’Œä¿¡å¿ƒé€æ­¥è°ƒæ•´ï¼ˆMAC-Tuningï¼‰ï¼Œåœ¨æŒ‡ä»¤æ•°æ®çš„å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒå°†ç­”æ¡ˆé¢„æµ‹å’Œä¿¡å¿ƒä¼°è®¡çš„å­¦ä¹ åˆ†å¼€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ä¸Šæ¯”åŸºçº¿é«˜å‡ºé«˜è¾¾25%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21773v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨ï¼Œç”Ÿæˆä¸å­˜åœ¨çš„äº‹å®å³å¹»è§‰é—®é¢˜å¤‡å—å…³æ³¨ã€‚å…ˆå‰å…³äºæé«˜LLMä¿¡å¿ƒè¯„ä¼°çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€é—®é¢˜è®¾ç½®ä¸Šï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„å¤šé—®é¢˜è®¾ç½®ä¸‹ï¼ŒLLMå¯¹å…¶å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†è¾¹ç•Œçš„è®¤è¯†ä»æœ‰å¾…æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”å¤šç­”æ¡ˆä¸ä¿¡å¿ƒé€æ­¥è°ƒæ•´ï¼ˆMAC-Tuningï¼‰ï¼Œåœ¨æŒ‡ä»¤æ•°æ®å¾®è°ƒæ—¶ï¼Œå°†ç­”æ¡ˆé¢„æµ‹å’Œä¿¡å¿ƒè¯„ä¼°çš„å­¦ä¹ åˆ†å¼€ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ä¸Šæ¯”åŸºçº¿é«˜å‡º25%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰é—®é¢˜å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ä¹‹å‰çš„LLMä¿¡å¿ƒè¯„ä¼°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å•ä¸€é—®é¢˜è®¾ç½®ä¸Šã€‚</li>
<li>åœ¨å¤šé—®é¢˜è®¾ç½®ä¸‹ï¼ŒLLMå¯¹å…¶å†…éƒ¨å‚æ•°åŒ–çŸ¥è¯†è¾¹ç•Œçš„è®¤è¯†æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MAC-Tuningï¼Œå°†ç­”æ¡ˆé¢„æµ‹å’Œä¿¡å¿ƒè¯„ä¼°çš„å­¦ä¹ åœ¨æŒ‡ä»¤æ•°æ®å¾®è°ƒæ—¶åˆ†å¼€ã€‚</li>
<li>MAC-Tuningæ–¹æ³•é€šè¿‡é€æ­¥è°ƒæ•´ä¿¡å¿ƒå’Œæä¾›å¤šä¸ªç­”æ¡ˆæ¥æ”¹è¿›LLMçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMAC-Tuningåœ¨å¹³å‡ç²¾åº¦ä¸Šæ¯”åŸºçº¿æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2329c1ecf80e1e1f93935c6493d2a0e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b8c1553b87fbb2d95d5529ab5a8e0e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0136a57e7dc4dc48cc00aec9c6ae26e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-caf00fbb4b33f2eb25005cf61838456f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c0e5eb7a2592ce8c2e52014f520035e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aeecc3012b4f46240ed5f21ff74ffe9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-based-Interactive-Imitation-Learning-for-Robotic-Manipulation"><a href="#LLM-based-Interactive-Imitation-Learning-for-Robotic-Manipulation" class="headerlink" title="LLM-based Interactive Imitation Learning for Robotic Manipulation"></a>LLM-based Interactive Imitation Learning for Robotic Manipulation</h2><p><strong>Authors:Jonas Werner, Kun Chu, Cornelius Weber, Stefan Wermter</strong></p>
<p>Recent advancements in machine learning provide methods to train autonomous agents capable of handling the increasing complexity of sequential decision-making in robotics. Imitation Learning (IL) is a prominent approach, where agents learn to control robots based on human demonstrations. However, IL commonly suffers from violating the independent and identically distributed (i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL) achieves improved performance by allowing agents to learn from interactive feedback from human teachers. Despite these improvements, both approaches come with significant costs due to the necessity of human involvement. Leveraging the emergent capabilities of Large Language Models (LLMs) in reasoning and generating human-like responses, we introduce LLM-iTeach â€“ a novel IIL framework that utilizes an LLM as an interactive teacher to enhance agent performance while alleviating the dependence on human resources. Firstly, LLM-iTeach uses a hierarchical prompting strategy that guides the LLM in generating a policy in Python code. Then, with a designed similarity-based feedback mechanism, LLM-iTeach provides corrective and evaluative feedback interactively during the agentâ€™s training. We evaluate LLM-iTeach against baseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a state-of-the-art IIL method using a human teacher, on various robotic manipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the success rate and achieves or even outscores that of CEILing, highlighting the potential of LLMs as cost-effective, human-like teachers in interactive learning environments. We further demonstrate the methodâ€™s potential for generalization by evaluating it on additional tasks. The code and prompts are provided at: <a target="_blank" rel="noopener" href="https://github.com/Tubicor/LLM-iTeach">https://github.com/Tubicor/LLM-iTeach</a>. </p>
<blockquote>
<p>è¿‘æœŸæœºå™¨å­¦ä¹ çš„å‘å±•ä¸ºè®­ç»ƒèƒ½å¤Ÿå¤„ç†æœºå™¨äººåºåˆ—å†³ç­–ä¸­æ—¥ç›Šå¢é•¿çš„å¤æ‚æ€§çš„è‡ªä¸»ä»£ç†æä¾›äº†æ–¹æ³•ã€‚æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ˜¯ä¸€ç§çªå‡ºçš„æ–¹æ³•ï¼Œå…¶ä¸­ä»£ç†æ ¹æ®äººç±»æ¼”ç¤ºæ¥å­¦ä¹ æ§åˆ¶æœºå™¨äººã€‚ç„¶è€Œï¼ŒILé€šå¸¸ä¼šè¿åæœºå™¨äººä»»åŠ¡ä¸­çš„ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰å‡è®¾ã€‚é€šè¿‡å…è®¸ä»£ç†ä»äººç±»æ•™å¸ˆçš„äº¤äº’å¼åé¦ˆä¸­å­¦ä¹ ï¼Œäº¤äº’å¼æ¨¡ä»¿å­¦ä¹ ï¼ˆIILï¼‰å®ç°äº†æ€§èƒ½çš„æå‡ã€‚å°½ç®¡æœ‰è¿™äº›æ”¹è¿›ï¼Œä½†ç”±äºäººç±»å‚ä¸çš„å¿…è¦ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å¸¦æ¥äº†ç›¸å½“å¤§çš„æˆæœ¬ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†å’Œç”Ÿæˆäººç±»ååº”æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLM-iTeachâ€”â€”ä¸€ç§æ–°å‹IILæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨LLMä½œä¸ºäº¤äº’å¼æ•™å¸ˆæ¥æé«˜ä»£ç†æ€§èƒ½ï¼ŒåŒæ—¶å‡è½»å¯¹äººåŠ›èµ„æºçš„ä¾èµ–ã€‚é¦–å…ˆï¼ŒLLM-iTeachä½¿ç”¨åˆ†å±‚æç¤ºç­–ç•¥æ¥æŒ‡å¯¼LLMç”ŸæˆPythonä»£ç ä¸­çš„ç­–ç•¥ã€‚ç„¶åï¼Œé€šè¿‡è®¾è®¡çš„åŸºäºç›¸ä¼¼åº¦çš„åé¦ˆæœºåˆ¶ï¼ŒLLM-iTeachåœ¨ä»£ç†è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›çº æ­£å’Œè¯„ä»·åé¦ˆã€‚æˆ‘ä»¬åœ¨å„ç§æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°äº†LLM-iTeachä¸åŸºçº¿æ–¹æ³•ï¼ˆå¦‚è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰ä¸€ç§ILæ–¹æ³•ä»¥åŠCEILingä¸€ç§ä½¿ç”¨äººç±»æ•™å¸ˆçš„æœ€æ–°IILæ–¹æ³•ï¼‰çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLM-iTeachåœ¨æˆåŠŸç‡ä¸Šè¶…è¶Šäº†BCï¼Œå¹¶è¾¾åˆ°äº†æˆ–ç”šè‡³è¶…è¿‡äº†CEILingçš„æ°´å¹³ï¼Œè¿™çªæ˜¾äº†LLMä½œä¸ºäº¤äº’å¼å­¦ä¹ ç¯å¢ƒä¸­æˆæœ¬æ•ˆç›Šé«˜ã€äººæ€§åŒ–çš„æ•™å¸ˆçš„æ½œåŠ›ã€‚é€šè¿‡å¯¹å…¶åœ¨é¢å¤–ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å±•ç¤ºäº†è¯¥æ–¹æ³•çš„æ½œåœ¨æ³›åŒ–èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å’Œæç¤ºè¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/Tubicor/LLM-iTeach%E3%80%82">https://github.com/Tubicor/LLM-iTeachã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21769v1">PDF</a> To be published in IJCNN 2025 proceedings</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæœºå™¨å­¦ä¹ è¿›å±•ä¸ºå¤„ç†æœºå™¨äººå¤æ‚åºåˆ—å†³ç­–æä¾›äº†è‡ªä¸»ä»£ç†è®­ç»ƒæ–¹æ³•ã€‚æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ˜¯ä¸€ç§åŸºäºäººç±»æ¼”ç¤ºè®©ä»£ç†æ§åˆ¶æœºå™¨äººçš„æ–¹æ³•ï¼Œä½†è¿åäº†æœºå™¨äººä»»åŠ¡çš„ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾ã€‚äº¤äº’å¼æ¨¡ä»¿å­¦ä¹ ï¼ˆIILï¼‰é€šè¿‡å…è®¸ä»£ç†ä»äººç±»æ•™å¸ˆçš„äº’åŠ¨åé¦ˆä¸­å­¦ä¹ ï¼Œæé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸¤ç§æ–¹æ³•éƒ½éœ€è¦äººç±»å‚ä¸ï¼Œæˆæœ¬è¾ƒé«˜ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œç”Ÿæˆäººç±»å“åº”çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†LLM-iTeachâ€”â€”ä¸€ä¸ªåˆ©ç”¨LLMä½œä¸ºäº’åŠ¨æ•™å¸ˆçš„æ–°å‹IILæ¡†æ¶ï¼Œä»¥æé«˜ä»£ç†æ€§èƒ½å¹¶å‡å°‘å¯¹äººåŠ›èµ„æºçš„ä¾èµ–ã€‚LLM-iTeaché¦–å…ˆä½¿ç”¨åˆ†å±‚æç¤ºç­–ç•¥æ¥æŒ‡å¯¼LLMç”ŸæˆPythonä»£ç ç­–ç•¥ã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªè®¾è®¡çš„åŸºäºç›¸ä¼¼åº¦çš„åé¦ˆæœºåˆ¶ï¼ŒLLM-iTeachåœ¨ä»£ç†è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›çº æ­£å’Œè¯„ä»·åé¦ˆã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒLLM-iTeachåœ¨æˆåŠŸç‡ä¸Šè¶…è¶Šäº†è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰è¿™ä¸€ILæ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å½“å‰ä¸»æµçš„IILæ–¹æ³•CEILingçš„æ°´å¹³ï¼Œçªæ˜¾äº†LLMä½œä¸ºæˆæœ¬æ•ˆç›Šé«˜ã€äººç±»åŒ–çš„äº’åŠ¨æ•™å¸ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸæœºå™¨å­¦ä¹ è¿›å±•ä¸ºæœºå™¨äººå¤æ‚å†³ç­–æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰å’Œäº¤äº’å¼æ¨¡ä»¿å­¦ä¹ ï¼ˆIILï¼‰æ˜¯æœºå™¨äººå­¦ä¹ ä¸­çš„ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œä½†éƒ½éœ€è¦äººç±»å‚ä¸ã€‚</li>
<li>LLM-iTeachåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäº’åŠ¨æ•™å¸ˆï¼Œæé«˜ä»£ç†æ€§èƒ½å¹¶å‡å°‘äººç±»èµ„æºä¾èµ–ã€‚</li>
<li>LLM-iTeachä½¿ç”¨åˆ†å±‚æç¤ºç­–ç•¥å’ŒåŸºäºç›¸ä¼¼åº¦çš„åé¦ˆæœºåˆ¶ã€‚</li>
<li>LLM-iTeachåœ¨å¤šä¸ªæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šäº†è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†å½“å‰ä¸»æµçš„IILæ–¹æ³•ã€‚</li>
<li>LLM-iTeachå…·æœ‰æ½œåŠ›åœ¨æ›´å¤šä»»åŠ¡ä¸Šå¾—åˆ°åº”ç”¨å’Œè¯„ä¼°ã€‚</li>
<li>ç›¸å…³ä»£ç å’Œæç¤ºå·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f45d847f66b4044e53e077637498a8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fc92a764fa41b43d086d636c0cae8e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fe996604e189f44c27df2135067526b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7c7af2c80f3932b7f469af34dcfe765.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-816704a394a7498b4ac1addeba47a96f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-Empowered-Embodied-Agent-for-Memory-Augmented-Task-Planning-in-Household-Robotics"><a href="#LLM-Empowered-Embodied-Agent-for-Memory-Augmented-Task-Planning-in-Household-Robotics" class="headerlink" title="LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in   Household Robotics"></a>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in   Household Robotics</h2><p><strong>Authors:Marc Glocker, Peter HÃ¶nig, Matthias Hirschmanner, Markus Vincze</strong></p>
<p>We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/marc1198/chat-hsr">https://github.com/marc1198/chat-hsr</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å‘ˆç°äº†ä¸€ä¸ªå…·æœ‰LLMé©±åŠ¨çš„ä»£ç†ç¼–æ’æ¶æ„çš„å®ä½“æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºè‡ªä¸»ç®¡ç†å®¶å±…ç‰©å“ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†å¢å¼ºè®°å¿†çš„ä»»åŠ¡è§„åˆ’ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ‰§è¡Œé«˜çº§ç”¨æˆ·å‘½ä»¤ï¼ŒåŒæ—¶è·Ÿè¸ªè¿‡å»çš„è¡ŒåŠ¨ã€‚å®ƒé‡‡ç”¨äº†ä¸‰ä¸ªä¸“ä¸šä»£ç†ï¼šè·¯ç”±ä»£ç†ã€ä»»åŠ¡è§„åˆ’ä»£ç†å’ŒçŸ¥è¯†åº“ä»£ç†ï¼Œæ¯ä¸ªä»£ç†éƒ½ç”±ç‰¹å®šä»»åŠ¡çš„LLMé©±åŠ¨ã€‚é€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé¿å…äº†å¯¹æ˜¾å¼æ¨¡å‹è®­ç»ƒçš„éœ€æ±‚ã€‚RAGä½¿ç³»ç»Ÿèƒ½å¤Ÿä»è¿‡å»çš„äº¤äº’ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜é•¿æœŸå¯¹è±¡è·Ÿè¸ªèƒ½åŠ›ã€‚ç»“åˆäº†Grounded SAMå’ŒLLaMa3.2-Visionæä¾›å¼ºå¤§çš„å¯¹è±¡æ£€æµ‹åŠŸèƒ½ï¼Œä¿ƒè¿›ä»»åŠ¡è§„åˆ’çš„åœºæ™¯è¯­ä¹‰ç†è§£ã€‚åœ¨ä¸‰ç§å®¶åº­åœºæ™¯ä¸‹çš„è¯„ä¼°è¡¨æ˜ï¼Œä»»åŠ¡è§„åˆ’ç²¾åº¦é«˜ï¼Œç”±äºRAGçš„å­˜åœ¨ï¼Œè®°å¿†å›å¿†èƒ½åŠ›å¾—åˆ°æé«˜ã€‚å…·ä½“æ¥è¯´ï¼ŒQwen2.5åœ¨ä¸“ç”¨ä»£ç†æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒLLaMA3.1åœ¨è·¯ç”±ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/marc1198/chat-hsr%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/marc1198/chat-hsrè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21716v1">PDF</a> Accepted at Austrian Robotics Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªé‡‡ç”¨LLMé©±åŠ¨çš„ä»£ç†ç¼–æ’æ¶æ„çš„æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºè‡ªä¸»ç®¡ç†å®¶åŠ¡ç‰©å“ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†è®°å¿†å¢å¼ºä»»åŠ¡è§„åˆ’ï¼Œé€šè¿‡ä¸“ä¸šä»£ç†ï¼ˆè·¯ç”±ä»£ç†ã€ä»»åŠ¡è§„åˆ’ä»£ç†å’ŒçŸ¥è¯†åº“ä»£ç†ï¼‰å®ç°ç”¨æˆ·é«˜çº§æŒ‡ä»¤çš„æ‰§è¡Œå¹¶è¿½è¸ªè¿‡å»åŠ¨ä½œã€‚åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œç³»ç»Ÿæ— éœ€æ˜ç¡®çš„æ¨¡å‹è®­ç»ƒã€‚RAGå¸®åŠ©ç³»ç»Ÿä»è¿‡å»äº’åŠ¨ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºé•¿æœŸç‰©å“è¿½è¸ªèƒ½åŠ›ã€‚ç»“åˆGrounded SAMå’ŒLLaMa3.2-Visionï¼Œæä¾›ç¨³å¥çš„ç‰©å“æ£€æµ‹ï¼Œä¿ƒè¿›ä»»åŠ¡è§„åˆ’çš„åœºæ™¯è¯­ä¹‰ç†è§£ã€‚åœ¨ä¸‰ä¸ªå®¶åº­åœºæ™¯ä¸­çš„è¯„ä¼°æ˜¾ç¤ºï¼Œä»»åŠ¡è§„åˆ’å‡†ç¡®æ€§é«˜ï¼Œå› RAGè®°å¿†å›å¿†æœ‰æ”¹å–„ã€‚Qwen2.5åœ¨ç‰¹æ®Šä»£ç†ä¸­è¡¨ç°æœ€ä½³ï¼ŒLLaMA3.1åœ¨è·¯ç”±ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æœºå™¨äººç³»ç»Ÿé‡‡ç”¨LLMé©±åŠ¨çš„ä»£ç†ç¼–æ’æ¶æ„ï¼Œç”¨äºå®¶åŠ¡ç‰©å“ç®¡ç†ã€‚</li>
<li>ç³»ç»Ÿç»“åˆäº†è®°å¿†å¢å¼ºä»»åŠ¡è§„åˆ’ï¼Œå¯ä»¥æ‰§è¡Œé«˜çº§ç”¨æˆ·æŒ‡ä»¤å¹¶è¿½è¸ªè¿‡å»åŠ¨ä½œã€‚</li>
<li>åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå‡å°‘äº†å¯¹æ˜ç¡®æ¨¡å‹è®­ç»ƒçš„éœ€æ±‚ã€‚</li>
<li>RAGä»è¿‡å»çš„äº’åŠ¨ä¸­æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºäº†é•¿æœŸç‰©å“è¿½è¸ªèƒ½åŠ›ã€‚</li>
<li>ç»“åˆGrounded SAMå’ŒLLaMa3.2-VisionæŠ€æœ¯ï¼Œæé«˜äº†ç‰©å“æ£€æµ‹çš„ç¨³å¥æ€§å’Œåœºæ™¯è¯­ä¹‰ç†è§£ã€‚</li>
<li>åœ¨å¤šç§å®¶åº­åœºæ™¯çš„è¯„ä¼°ä¸­ï¼Œç³»ç»Ÿæ˜¾ç¤ºå‡ºé«˜çš„ä»»åŠ¡è§„åˆ’å‡†ç¡®æ€§å’Œè®°å¿†æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dca7c9be1a75c46296d95c57f4f3bc5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a66ab9c531e90f69ca0c9b8d40792a16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3582528f93ede71a06f39e4df31f9f7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e590cec1d0b46dd417a5515916e21a17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0d6bc3c58292319b55dcecc9e4608ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c76ab1d3e3d6f7daaa52d6abce1c6e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd25b12de6c70dc0852d1416f14fc390.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Visual-Text-Processing-A-Comprehensive-Review-and-Unified-Evaluation"><a href="#Visual-Text-Processing-A-Comprehensive-Review-and-Unified-Evaluation" class="headerlink" title="Visual Text Processing: A Comprehensive Review and Unified Evaluation"></a>Visual Text Processing: A Comprehensive Review and Unified Evaluation</h2><p><strong>Authors:Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe</strong></p>
<p>Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at <a target="_blank" rel="noopener" href="https://github.com/shuyansy/Visual-Text-Processing-survey">https://github.com/shuyansy/Visual-Text-Processing-survey</a>. </p>
<blockquote>
<p>è§†è§‰æ–‡æœ¬æ˜¯æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œä¼ é€’ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å¸å¼•è®¡ç®—æœºè§†è§‰ç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚é™¤äº†æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ç­‰ä¼ ç»Ÿä»»åŠ¡å¤–ï¼Œè§†è§‰æ–‡æœ¬å¤„ç†åœ¨åŸºç¡€æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†å¿«é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ–‡æœ¬å›¾åƒæ“ä½œã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”±äºæ–‡æœ¬ä¸ä¸€èˆ¬ç‰©ä½“çš„ç‹¬ç‰¹å±æ€§å·®å¼‚ï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ‰æ•ˆæ•è·å’Œåˆ©ç”¨è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾æ˜¯å¼€å‘ç¨³å¥çš„è§†è§‰æ–‡æœ¬å¤„ç†æ¨¡å‹çš„å…³é”®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è§†è§‰æ–‡æœ¬å¤„ç†çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢ã€å¤šè§’åº¦çš„åˆ†æï¼Œé‡ç‚¹å›ç­”ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰ä»€ä¹ˆæ–‡æœ¬ç‰¹å¾æœ€é€‚åˆä¸åŒçš„è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼Ÿï¼ˆ2ï¼‰å¦‚ä½•æœ‰æ•ˆåœ°å°†è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾çº³å…¥å¤„ç†æ¡†æ¶ï¼Ÿæ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†VTPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¹¿æ³›è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†çš„æ–°åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é«˜çº§è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºäº†VTPScoreï¼Œä¸€ç§æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œæ—¨åœ¨ç¡®ä¿å…¬å¹³å¯é çš„è¯„ä»·ã€‚æˆ‘ä»¬å¯¹è¶…è¿‡20ç§ç‰¹å®šæ¨¡å‹çš„å®è¯ç ”ç©¶æ­ç¤ºäº†å½“å‰æŠ€æœ¯ä»æœ‰å¤§é‡æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿™é¡¹å·¥ä½œç¡®ç«‹ä¸ºè§†è§‰æ–‡æœ¬å¤„ç†è¿™ä¸€åŠ¨æ€é¢†åŸŸçš„åŸºæœ¬èµ„æºï¼Œä¿ƒè¿›æœªæ¥çš„æ¢ç´¢å’Œåˆ›æ–°ã€‚ç›¸å…³ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shuyansy/Visual-Text-Processing-survey%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shuyansy/Visual-Text-Processing-surveyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21682v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸­å¼ºè°ƒäº†è§†è§‰æ–‡æœ¬åœ¨æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­çš„å…³é”®ä½œç”¨ï¼Œå®ƒä¼ é€’äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯å¹¶å¼•èµ·äº†è®¡ç®—æœºè§†è§‰ç•Œçš„å…³æ³¨ã€‚éšç€åŸºç¡€æ¨¡å‹çš„å‡ºç°ï¼Œè§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡è¿…é€Ÿæ¨è¿›ï¼ŒåŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ–‡æœ¬å›¾åƒæ“ä½œç­‰ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºæ–‡æœ¬ä¸ä¸€èˆ¬ç‰©ä½“çš„ç‹¬ç‰¹å±æ€§ä¸åŒï¼Œä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾å¯¹äºå¼€å‘ç¨³å¥çš„è§†è§‰æ–‡æœ¬å¤„ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚æ–‡ç« å…¨é¢åˆ†æäº†è§†è§‰æ–‡æœ¬å¤„ç†çš„æœ€æ–°è¿›å±•ï¼Œå¹¶ä»‹ç»äº†VTPBenchæ–°åŸºå‡†å’ŒVTPScoreè¯„ä¼°æŒ‡æ ‡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰æŠ€æœ¯çš„å·¨å¤§æ”¹è¿›ç©ºé—´ï¼Œæ—¨åœ¨æˆä¸ºä¿ƒè¿›è§†è§‰æ–‡æœ¬å¤„ç†é¢†åŸŸæœªæ¥æ¢ç´¢å’Œåˆ›æ–°çš„åŸºæœ¬èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ–‡æœ¬åœ¨æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œä¼ é€’ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¤‡å—è®¡ç®—æœºè§†è§‰é¢†åŸŸå…³æ³¨ã€‚</li>
<li>è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡å‘å±•è¿…é€Ÿï¼ŒåŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ–‡æœ¬å›¾åƒæ“ä½œç­‰ã€‚</li>
<li>æ–‡æœ¬ä¸ä¸€èˆ¬ç‰©ä½“å…·æœ‰ç‹¬ç‰¹å±æ€§ï¼Œæœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨è¿™äº›ç‹¬ç‰¹æ–‡æœ¬ç‰¹å¾å¯¹å¼€å‘ç¨³å¥æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>æ–‡ç« å…¨é¢åˆ†æäº†è§†è§‰æ–‡æœ¬å¤„ç†çš„æœ€æ–°è¿›å±•ï¼Œå›ç­”äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå“ªäº›æ–‡æœ¬ç‰¹å¾æœ€é€‚åˆä¸åŒçš„è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼Ÿå¦‚ä½•æœ‰æ•ˆåœ°å°†è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾èå…¥å¤„ç†æ¡†æ¶ï¼Ÿ</li>
<li>ä»‹ç»äº†æ–°çš„VTPBenchåŸºå‡†ï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºäº†VTPScoreè¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿å…¬å¹³å¯é çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ba187bed82e500dc736c5cf2edc4d3df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f8657c786a8dae4ecafb7370ee476921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f62cf1e080e5998dc17731b1eef14ea6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f83f4345ad4e0b8d493335a44549503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce6b361c66b4d039193ab134a883dce.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Traceback-of-Poisoning-Attacks-to-Retrieval-Augmented-Generation"><a href="#Traceback-of-Poisoning-Attacks-to-Retrieval-Augmented-Generation" class="headerlink" title="Traceback of Poisoning Attacks to Retrieval-Augmented Generation"></a>Traceback of Poisoning Attacks to Retrieval-Augmented Generation</h2><p><strong>Authors:Baolei Zhang, Haoran Xin, Minghong Fang, Zhuqing Liu, Biao Yi, Tong Li, Zheli Liu</strong></p>
<p>Large language models (LLMs) integrated with retrieval-augmented generation (RAG) systems improve accuracy by leveraging external knowledge sources. However, recent research has revealed RAGâ€™s susceptibility to poisoning attacks, where the attacker injects poisoned texts into the knowledge database, leading to attacker-desired responses. Existing defenses, which predominantly focus on inference-time mitigation, have proven insufficient against sophisticated attacks. In this paper, we introduce RAGForensics, the first traceback system for RAG, designed to identify poisoned texts within the knowledge database that are responsible for the attacks. RAGForensics operates iteratively, first retrieving a subset of texts from the database and then utilizing a specially crafted prompt to guide an LLM in detecting potential poisoning texts. Empirical evaluations across multiple datasets demonstrate the effectiveness of RAGForensics against state-of-the-art poisoning attacks. This work pioneers the traceback of poisoned texts in RAG systems, providing a practical and promising defense mechanism to enhance their security. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç›¸ç»“åˆï¼Œå¯ä»¥åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æºæé«˜å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜RAGå®¹æ˜“å—åˆ°ä¸­æ¯’æ”»å‡»çš„å½±å“ï¼Œæ”»å‡»è€…å°†æœ‰æ¯’æ–‡æœ¬æ³¨å…¥çŸ¥è¯†æ•°æ®åº“ï¼Œå¯¼è‡´äº§ç”Ÿæ”»å‡»è€…æ‰€æœŸæœ›çš„å›åº”ã€‚ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µä¸»è¦é›†ä¸­åœ¨æ¨ç†æ—¶é—´ç¼“è§£ä¸Šï¼Œå·²è¢«è¯æ˜ä¸è¶³ä»¥åº”å¯¹é«˜çº§æ”»å‡»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RAGForensicsï¼Œè¿™æ˜¯é’ˆå¯¹RAGçš„ç¬¬ä¸€ä¸ªå›æº¯ç³»ç»Ÿï¼Œæ—¨åœ¨è¯†åˆ«çŸ¥è¯†æ•°æ®åº“ä¸­çš„æœ‰æ¯’æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬æ˜¯æ”»å‡»çš„åŸå› ã€‚RAGForensicsé€šè¿‡è¿­ä»£æ–¹å¼è¿è¡Œï¼Œé¦–å…ˆä»æ•°æ®åº“ä¸­æ£€ç´¢ä¸€éƒ¨åˆ†æ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¸“é—¨è®¾è®¡çš„æç¤ºæ¥å¼•å¯¼LLMæ£€æµ‹æ½œåœ¨çš„æœ‰æ¯’æ–‡æœ¬ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒRAGForensicså¯¹æœ€å…ˆè¿›çš„ä¸­æ¯’æ”»å‡»å…·æœ‰æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œåœ¨RAGç³»ç»Ÿä¸­å¼€åˆ›äº†æœ‰æ¯’æ–‡æœ¬çš„è¿½æº¯å…ˆæ²³ï¼Œæä¾›äº†ä¸€ç§å¢å¼ºç³»ç»Ÿå®‰å…¨çš„å®ç”¨ä¸”å‰æ™¯å¹¿é˜”çš„é˜²å¾¡æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21668v1">PDF</a> Accepted by The Web Conference 2025</p>
<p><strong>Summary</strong></p>
<p>LLMsä¸RAGç³»ç»Ÿç»“åˆå¯æé«˜å‡†ç¡®æ€§ï¼Œä½†æ˜“å—åˆ°æ¯’è¯æ”»å‡»å½±å“ï¼Œæ”»å‡»è€…å°†æœ‰æ¯’æ–‡æœ¬æ³¨å…¥çŸ¥è¯†åº“ä»¥äº§ç”Ÿæ‰€éœ€å“åº”ã€‚ç°æœ‰ä¸»è¦ä¾§é‡äºæ¨ç†æ—¶é—´ç¼“è§£çš„é˜²å¾¡æªæ–½å¯¹é«˜çº§æ”»å‡»è¯æ˜æ— æ•ˆã€‚æœ¬æ–‡ä»‹ç»RAGForensicsï¼Œé¦–ä¸ªä¸ºRAGè®¾è®¡çš„å›æº¯ç³»ç»Ÿï¼Œç”¨äºè¯†åˆ«çŸ¥è¯†åº“ä¸­çš„æœ‰æ¯’æ–‡æœ¬ã€‚RAGForensicsé€šè¿‡è¿­ä»£æ“ä½œï¼Œé¦–å…ˆä»æ•°æ®åº“ä¸­æ£€ç´¢éƒ¨åˆ†æ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¸“é—¨è®¾è®¡çš„æç¤ºå¼•å¯¼LLMæ£€æµ‹å¯èƒ½çš„æ¯’è¯æ–‡æœ¬ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¯æ˜ï¼ŒRAGForensicså¯¹æœ€å…ˆè¿›çš„æ¯’è¯æ”»å‡»æœ‰æ•ˆã€‚è¿™é¡¹å·¥ä½œåœ¨RAGç³»ç»Ÿä¸­å¼€åˆ›äº†æœ‰æ¯’æ–‡æœ¬å›æº¯çš„å…ˆæ²³ï¼Œæä¾›äº†ä¸€ç§å¢å¼ºå®‰å…¨æ€§çš„å®ç”¨ä¸”å‰æ™¯å¹¿é˜”çš„é˜²å¾¡æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsç»“åˆRAGç³»ç»Ÿåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æºæé«˜å‡†ç¡®æ€§ï¼Œä½†å­˜åœ¨è¢«æ¯’è¯æ”»å‡»çš„é£é™©ã€‚</li>
<li>æ¯’è¯æ”»å‡»é€šè¿‡æ³¨å…¥æœ‰æ¯’æ–‡æœ¬åˆ°çŸ¥è¯†åº“ï¼Œå¯¼è‡´äº§ç”Ÿæ”»å‡»è€…å¸Œæœ›çš„å“åº”ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æªæ–½ä¸»è¦ä¾§é‡äºæ¨ç†æ—¶é—´çš„ç¼“è§£ï¼Œå¯¹é«˜çº§æ”»å‡»ä¸å¤Ÿæœ‰æ•ˆã€‚</li>
<li>RAGForensicsæ˜¯é¦–ä¸ªä¸ºRAGè®¾è®¡çš„å›æº¯ç³»ç»Ÿï¼Œå¯è¯†åˆ«çŸ¥è¯†åº“ä¸­çš„æœ‰æ¯’æ–‡æœ¬ã€‚</li>
<li>RAGForensicsé€šè¿‡è¿­ä»£æ–¹å¼æ£€ç´¢æ–‡æœ¬å¹¶ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„æç¤ºå¼•å¯¼LLMæ£€æµ‹æ¯’è¯æ–‡æœ¬ã€‚</li>
<li>å®è¯è¯„ä¼°è¯æ˜RAGForensicså¯¹å…ˆè¿›çš„æ¯’è¯æ”»å‡»æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ed09b89ef02713dabc98a71934bfb59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01fa1979675942926c894b4c3c28242b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8b588effc36fd052970fb4aa4f4b523.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AdaR1-From-Long-CoT-to-Hybrid-CoT-via-Bi-Level-Adaptive-Reasoning-Optimization"><a href="#AdaR1-From-Long-CoT-to-Hybrid-CoT-via-Bi-Level-Adaptive-Reasoning-Optimization" class="headerlink" title="AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning   Optimization"></a>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning   Optimization</h2><p><strong>Authors:Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, Li Shen</strong></p>
<p>Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at <a target="_blank" rel="noopener" href="https://github.com/StarDewXXX/AdaR1">https://github.com/StarDewXXX/AdaR1</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œé•¿æœŸè¢«è®¤ä¸ºæ˜¯æ¨ç†æ¨¡å‹çš„å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†å¾€å¾€äº§ç”Ÿå¤§é‡çš„æ¨ç†å¼€é”€ï¼Œä½¿å¾—æ•ˆç‡æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ˜¾ç¤ºï¼Œä½¿ç”¨Long-CoTçš„å¥½å¤„å› é—®é¢˜è€Œå¼‚ï¼šè™½ç„¶ä¸€äº›é—®é¢˜éœ€è¦ç²¾ç»†æ¨ç†ï¼Œä½†å…¶ä»–é—®é¢˜åˆ™æ²¡æœ‰æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œç”šè‡³ç²¾åº¦ä¸‹é™ã€‚è¿™æ¿€å‘äº†é€‚åº”æ€§çš„æ¨ç†ç­–ç•¥ï¼Œæ ¹æ®è¾“å…¥å®šåˆ¶æ¨ç†æ·±åº¦ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œä¸»è¦ä¾§é‡äºå‡å°‘é•¿æ¨ç†è·¯å¾„ä¸­çš„å†—ä½™ï¼Œé™åˆ¶äº†Long-CoTæ¨¡å¼ä¹‹å¤–æ›´æœ‰æ•ˆç‡ç­–ç•¥çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªé€‚åº”å’Œé«˜æ•ˆæ¨ç†çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åˆå¹¶é•¿çŸ­CoTæ¨¡å‹æ„å»ºæ··åˆæ¨ç†æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ ·åŒ–çš„æ¨ç†é£æ ¼ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åº”ç”¨åŒå±‚åå¥½è®­ç»ƒæ¥å¼•å¯¼æ¨¡å‹é€‰æ‹©é€‚å½“çš„æ¨ç†é£æ ¼ï¼ˆç¾¤ä½“å±‚é¢ï¼‰ï¼Œå¹¶åœ¨æ¯ç§é£æ ¼å†…åå¥½ç®€æ´æ­£ç¡®çš„æ¨ç†ï¼ˆå®ä¾‹å±‚é¢ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œä¸å…¶ä»–åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†æ¨ç†æˆæœ¬ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨äº”ä¸ªæ•°å­¦æ•°æ®é›†ä¸Šï¼Œæ¨ç†çš„å¹³å‡é•¿åº¦å‡å°‘äº†è¶…è¿‡50%ï¼Œè¿™çªæ˜¾äº†è‡ªé€‚åº”ç­–ç•¥åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†æ•ˆç‡çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¾ˆå¿«å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/StarDewXXX/AdaR1%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/StarDewXXX/AdaR1ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21659v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼Œé•¿æ¨ç†æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†æ¨ç†æ•ˆç‡æˆä¸ºå…³é”®é—®é¢˜ã€‚ç ”ç©¶å‘ç°ä½¿ç”¨Long-CoTçš„å¥½å¤„å› é—®é¢˜è€Œå¼‚ï¼Œå› æ­¤æå‡ºè‡ªé€‚åº”æ¨ç†ç­–ç•¥ï¼Œæ ¹æ®è¾“å…¥è°ƒæ•´æ¨ç†æ·±åº¦ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼Œæå‡ºä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œèåˆé•¿çŸ­CoTæ¨¡å‹ï¼Œå®ç°å¤šæ ·åŒ–æ¨ç†é£æ ¼ã€‚é€šè¿‡åŒå±‚æ¬¡åå¥½è®­ç»ƒï¼Œå¼•å¯¼æ¨¡å‹é€‰æ‹©åˆé€‚æ¨ç†é£æ ¼ï¼Œå¹¶åœ¨å„é£æ ¼å†…åå¥½ç®€æ´æ­£ç¡®æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ç»´æŒæ€§èƒ½ï¼Œåœ¨äº”ä¸ªæ•°å­¦æ•°æ®é›†ä¸Šå¹³å‡æ¨ç†é•¿åº¦å‡å°‘è¶…è¿‡50%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿æ¨ç†æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†æ•ˆç‡æˆä¸ºå…³é”®é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨Long-CoTçš„å¥½å¤„å› é—®é¢˜è€Œå¼‚ï¼Œéœ€è¦è‡ªé€‚åº”æ¨ç†ç­–ç•¥ã€‚</li>
<li>èåˆé•¿çŸ­CoTæ¨¡å‹ä»¥æä¾›å¤šæ ·åŒ–æ¨ç†é£æ ¼ã€‚</li>
<li>åŒå±‚æ¬¡åå¥½è®­ç»ƒå¼•å¯¼æ¨¡å‹é€‰æ‹©é€‚åˆçš„æ¨ç†é£æ ¼ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–æ–¹æ³•ï¼Œæ‰€ææ–¹æ³•åœ¨é™ä½æ¨ç†æˆæœ¬çš„åŒæ—¶ç»´æŒæ€§èƒ½ã€‚</li>
<li>åœ¨äº”ä¸ªæ•°å­¦æ•°æ®é›†ä¸Šå¹³å‡æ¨ç†é•¿åº¦å‡å°‘è¶…è¿‡50%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88bb340d629bf87739297f4acd62cb56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86364842f179fc9066b99241de68d0b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf44a1eb4df9462dda3b7c0f51197437.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Sadeed-Advancing-Arabic-Diacritization-Through-Small-Language-Model"><a href="#Sadeed-Advancing-Arabic-Diacritization-Through-Small-Language-Model" class="headerlink" title="Sadeed: Advancing Arabic Diacritization Through Small Language Model"></a>Sadeed: Advancing Arabic Diacritization Through Small Language Model</h2><p><strong>Authors:Zeina Aldallal, Sara Chrouf, Khalil Hennara, Mohamed Motaism Hamed, Muhammad Hreden, Safwan AlModhayan</strong></p>
<p>Arabic text diacritization remains a persistent challenge in natural language processing due to the languageâ€™s morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools. </p>
<blockquote>
<p>é˜¿æ‹‰ä¼¯æ–‡æœ¬å˜éŸ³ç¬¦å·åŒ–ä»ç„¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªæŒç»­æŒ‘æˆ˜ï¼Œå› ä¸ºè¯¥è¯­è¨€å½¢æ€ä¸°å¯Œã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Sadeedï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼ŒåŸºäºKuwain 1.5B Hennaraç­‰äººäº2025å¹´å¼€å‘çš„ç´§å‡‘æ¨¡å‹æ”¹ç¼–çš„å¾®è°ƒè§£ç å™¨è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹æœ€åˆæ˜¯åœ¨å„ç§é˜¿æ‹‰ä¼¯è¯­æ–™åº“ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚Sadeedåœ¨ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡å¸¦æœ‰å˜éŸ³ç¬¦å·çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›æ•°æ®é›†æ˜¯é€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–ç®¡é“æ„å»ºçš„ã€‚å°½ç®¡ä½¿ç”¨äº†é€‚ä¸­çš„è®¡ç®—èµ„æºï¼Œä½†Sadeedåœ¨ä¸ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ç«äº‰ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå¹¶ä¸”åœ¨ç±»ä¼¼é¢†åŸŸä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å½“å‰é˜¿æ‹‰ä¼¯å˜éŸ³ç¬¦å·åŒ–åŸºå‡†æµ‹è¯•å®è·µä¸­çš„å…³é”®å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SadeedDiac-25ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å®ç°åœ¨ä¸åŒæ–‡æœ¬ç±»å‹å’Œå¤æ‚ç¨‹åº¦ä¸Šçš„æ›´å…¨é¢å’Œå…¬å¹³çš„è¯„ä¼°ã€‚å› æ­¤ï¼ŒSadeedå’ŒSadeedDiac-25ä¸ºæ¨è¿›é˜¿æ‹‰ä¼¯è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ°è¯­éŸ³å’Œè¯­è¨€å­¦ä¹ å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Sadeedï¼Œä¸€ç§åŸºäºKuwain 1.5B Hennaraç­‰äººåœ¨é˜¿æ‹‰ä¼¯è¯­è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ç´§å‡‘æ¨¡å‹çš„æ–°å‹é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬å‘éŸ³ç¬¦å·åŒ–æ–¹æ³•ã€‚Sadeedç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„é«˜è´¨é‡å‘éŸ³ç¬¦å·æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–æµç¨‹æ„å»ºã€‚ç›¸è¾ƒäºä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒSadeedåœ¨èµ„æºä½¿ç”¨é€‚åº¦çš„æƒ…å†µä¸‹å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå¹¶ä¸”åœ¨ç±»ä¼¼é¢†åŸŸä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æŒ‡å‡ºäº†å½“å‰é˜¿æ‹‰ä¼¯è¯­å‘éŸ³ç¬¦å·åŒ–è¯„ä¼°æ–¹æ³•çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å¼•å…¥äº†SadeedDiac-25æ–°åŸºå‡†æµ‹è¯•ï¼Œä»¥å®ç°åœ¨ä¸åŒæ–‡æœ¬ç±»å‹å’Œå¤æ‚ç¨‹åº¦ä¸Šçš„æ›´å…¬å¹³å’Œå…¨é¢çš„è¯„ä¼°ã€‚Sadeedå’ŒSadeedDiac-25ä¸ºæ¨è¿›é˜¿æ‹‰ä¼¯è¯­NLPåº”ç”¨ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œè¯­è¨€å­¦ä¹ å·¥å…·ç­‰æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é˜¿æ‹‰ä¼¯è¯­æ–‡æœ¬å‘éŸ³ç¬¦å·åŒ–æ–¹æ³•Sadeedï¼ŒåŸºäºç´§å‡‘æ¨¡å‹è¿›è¡Œå¾®è°ƒå®ç°ã€‚</li>
<li>Sadeedåœ¨é«˜è´¨é‡å‘éŸ³ç¬¦å·æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡ä¸¥æ ¼çš„æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–æµç¨‹æ„å»ºã€‚</li>
<li>Sadeedåœ¨èµ„æºä½¿ç”¨é€‚åº¦çš„æƒ…å†µä¸‹å–å¾—äº†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“çš„ç«äº‰åŠ›ç»“æœã€‚</li>
<li>Sadeedåœ¨ç±»ä¼¼é¢†åŸŸä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ã€‚</li>
<li>å½“å‰é˜¿æ‹‰ä¼¯è¯­å‘éŸ³ç¬¦å·åŒ–çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨å…³é”®å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•SadeedDiac-25ï¼Œä»¥æ¨åŠ¨æ›´å…¬å¹³å’Œå…¨é¢çš„è¯„ä¼°ï¼Œé€‚ç”¨äºä¸åŒæ–‡æœ¬ç±»å‹å’Œå¤æ‚ç¨‹åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a7ba50706ad870559878f91ccc55ba9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80fb25efd660c12945225441946e1858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-891dee2cf88e1c7f060e56ae86df1f95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6428034024f441fdb8c98df7beccbf5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Advancing-Arabic-Reverse-Dictionary-Systems-A-Transformer-Based-Approach-with-Dataset-Construction-Guidelines"><a href="#Advancing-Arabic-Reverse-Dictionary-Systems-A-Transformer-Based-Approach-with-Dataset-Construction-Guidelines" class="headerlink" title="Advancing Arabic Reverse Dictionary Systems: A Transformer-Based   Approach with Dataset Construction Guidelines"></a>Advancing Arabic Reverse Dictionary Systems: A Transformer-Based   Approach with Dataset Construction Guidelines</h2><p><strong>Authors:Serry Sibaee, Samar Ahmed, Abdullah Al Harbi, Omer Nacar, Adel Ammar, Yasser Habashi, Wadii Boulila</strong></p>
<p>This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic. </p>
<blockquote>
<p>æœ¬ç ”ç©¶é€šè¿‡å¼€å‘æœ‰æ•ˆçš„é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ï¼ˆRDï¼‰ç³»ç»Ÿï¼Œè§£å†³äº†é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ç©ºç™½ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨æˆ·å¯ä»¥æ ¹æ®æè¿°æˆ–æ„ä¹‰æŸ¥æ‰¾å•è¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè½¬æ¢å™¨çš„æ–¹æ³•ï¼Œé‡‡ç”¨å…·æœ‰å‡ ä½•é€’å‡å±‚çš„åŠç¼–ç å™¨ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­RDä»»åŠ¡ä¸­å–å¾—äº†æœ€æ–°ç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å…¨é¢çš„æ•°æ®é›†æ„å»ºè¿‡ç¨‹ï¼Œå¹¶ä¸ºé˜¿æ‹‰ä¼¯è¯­è¯å…¸å®šä¹‰å»ºç«‹äº†æ­£å¼çš„è´¨é‡æ ‡å‡†ã€‚ä½¿ç”¨å„ç§é¢„è®­ç»ƒæ¨¡å‹çš„å®éªŒè¡¨æ˜ï¼Œé˜¿æ‹‰ä¼¯è¯­ç‰¹å®šæ¨¡å‹çš„è¡¨ç°è¿œè¶…é€šç”¨å¤šè¯­è¨€åµŒå…¥ï¼Œå…¶ä¸­ARBERTv2å–å¾—æœ€ä½³æ’ååˆ†æ•°ï¼ˆ0.0644ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹åå‘è¯å…¸ä»»åŠ¡è¿›è¡Œäº†æ­£å¼æŠ½è±¡ï¼Œå¢å¼ºäº†ç†è®ºç†è§£ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„Pythonåº“ï¼ˆRDTLï¼‰ï¼Œå…·æœ‰å¯é…ç½®çš„è®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬å¯¹æ•°æ®é›†è´¨é‡çš„åˆ†ææ­ç¤ºäº†æ”¹è¿›é˜¿æ‹‰ä¼¯è¯­å®šä¹‰æ„å»ºçš„è§è§£ï¼Œå¹¶æå‡ºäº†å»ºç«‹é«˜è´¨é‡åå‘è¯å…¸èµ„æºçš„å…«æ¡ç‰¹å®šæ ‡å‡†ã€‚æœ¬å·¥ä½œå¯¹é˜¿æ‹‰ä¼¯è¯­è®¡ç®—è¯­è¨€å­¦åšå‡ºäº†é‡å¤§è´¡çŒ®ï¼Œå¹¶ä¸ºé˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€å­¦ä¹ ã€å­¦æœ¯å†™ä½œå’Œä¸“ä¸šäº¤æµæä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21475v1">PDF</a> </p>
<p><strong>Summary</strong><br>é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ç³»ç»Ÿç ”ç©¶</p>
<p>è¯¥ç ”ç©¶è§£å†³äº†é˜¿æ‹‰ä¼¯è¯­è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ç©ºç™½ï¼Œå¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ï¼ˆRDï¼‰ç³»ç»Ÿï¼Œä½¿ç”¨æˆ·å¯ä»¥é€šè¿‡æè¿°æˆ–æ„ä¹‰æŸ¥æ‰¾å•è¯ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè½¬æ¢å™¨çš„æ–°å‹æ–¹æ³•ï¼Œé‡‡ç”¨å…·æœ‰å‡ ä½•é€’å‡å±‚çš„åŠç¼–ç å™¨ç¥ç»ç½‘ç»œæ¶æ„ï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­RDä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœã€‚è¯¥æ–¹æ³•çº³å…¥äº†ç»¼åˆæ•°æ®é›†æ„å»ºè¿‡ç¨‹ï¼Œä¸ºé˜¿æ‹‰ä¼¯è¯­è¯å…¸å®šä¹‰å»ºç«‹äº†æ­£å¼çš„è´¨é‡æ ‡å‡†ã€‚å®éªŒè¡¨æ˜ï¼Œé˜¿æ‹‰ä¼¯è¯­ç‰¹å®šæ¨¡å‹æ˜æ˜¾ä¼˜äºé€šç”¨å¤šè¯­è¨€åµŒå…¥ï¼ŒARBERTv2è·å¾—æœ€ä½³æ’åå¾—åˆ†ï¼ˆ0.0644ï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶æä¾›äº†å¯¹åå‘è¯å…¸ä»»åŠ¡çš„æ­£å¼æŠ½è±¡ï¼Œå¢å¼ºäº†ç†è®ºç†è§£ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„Pythonåº“ï¼ˆRDTLï¼‰ï¼Œå…·æœ‰å¯é…ç½®çš„è®­ç»ƒç®¡é“ã€‚å¯¹æ•°æ®é›†è´¨é‡çš„åˆ†ææ­ç¤ºäº†æ”¹è¿›é˜¿æ‹‰ä¼¯è¯­å®šä¹‰æ„å»ºçš„é‡è¦è§è§£ï¼Œå¹¶ä¸ºæ„å»ºé«˜è´¨é‡åå‘è¯å…¸èµ„æºæä¾›äº†å…«é¡¹ç‰¹å®šæ ‡å‡†ã€‚è¿™é¡¹ç ”ç©¶å¯¹é˜¿æ‹‰ä¼¯è¯­è®¡ç®—è¯­è¨€å­¦åšå‡ºäº†é‡å¤§è´¡çŒ®ï¼Œå¹¶ä¸ºé˜¿æ‹‰ä¼¯è¯­çš„è¯­è¨€å­¦ä¹ ã€å­¦æœ¯å†™ä½œå’Œä¸“ä¸šäº¤æµæä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼€å‘äº†æœ‰æ•ˆçš„é˜¿æ‹‰ä¼¯è¯­åå‘è¯å…¸ç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æè¿°æˆ–æ„ä¹‰æŸ¥æ‰¾å•è¯ã€‚</li>
<li>é‡‡ç”¨æ–°å‹åŸºäºè½¬æ¢å™¨çš„åŠç¼–ç å™¨ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ç°é˜¿æ‹‰ä¼¯è¯­RDä»»åŠ¡çš„æœ€æ–°æˆæœã€‚</li>
<li>é˜¿æ‹‰ä¼¯è¯­ç‰¹å®šæ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°ä¼˜äºé€šç”¨å¤šè¯­è¨€åµŒå…¥ï¼ŒARBERTv2è·å¾—æœ€ä½³æ’åå¾—åˆ†ã€‚</li>
<li>ç ”ç©¶æä¾›äº†åå‘è¯å…¸ä»»åŠ¡çš„æ­£å¼æŠ½è±¡ï¼Œå¢å¼ºç†è®ºç†è§£ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ¨¡å—åŒ–çš„Pythonåº“ï¼ˆRDTLï¼‰ï¼Œå…·æœ‰å¯é…ç½®çš„è®­ç»ƒç®¡é“ã€‚</li>
<li>åˆ†ææ­ç¤ºäº†æ”¹è¿›é˜¿æ‹‰ä¼¯è¯­å®šä¹‰æ„å»ºçš„é‡è¦è§è§£ã€‚</li>
<li>ç ”ç©¶ä¸ºæ„å»ºé«˜è´¨é‡åå‘è¯å…¸èµ„æºæå‡ºäº†å…«é¡¹ç‰¹å®šæ ‡å‡†ï¼Œå¯¹é˜¿æ‹‰ä¼¯è¯­è®¡ç®—è¯­è¨€å­¦åšå‡ºé‡å¤§è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f3fa9f8f00f83a56108709cddb12d00a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc9a781b70e2a4876f1799e1ed14d87c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Quantification-for-Language-Models-A-Suite-of-Black-Box-White-Box-LLM-Judge-and-Ensemble-Scorers"><a href="#Uncertainty-Quantification-for-Language-Models-A-Suite-of-Black-Box-White-Box-LLM-Judge-and-Ensemble-Scorers" class="headerlink" title="Uncertainty Quantification for Language Models: A Suite of Black-Box,   White-Box, LLM Judge, and Ensemble Scorers"></a>Uncertainty Quantification for Language Models: A Suite of Black-Box,   White-Box, LLM Judge, and Ensemble Scorers</h2><p><strong>Authors:Dylan Bouchard, Mohit Singh Chauhan</strong></p>
<p>Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paperâ€™s companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs. </p>
<blockquote>
<p>å¹»è§‰ï¼ˆhallucinationsï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒç»­å­˜åœ¨çš„é—®é¢˜ã€‚éšç€è¿™äº›æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸçš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œå¯¹æœ‰æ•ˆçš„å¹»è§‰æ£€æµ‹çš„éœ€æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé›¶èµ„æºå¹»è§‰æ£€æµ‹çš„é€šç”¨æ¡†æ¶ï¼Œå®è·µè€…å¯ä»¥å°†å…¶åº”ç”¨äºç°å®ä¸–ç•Œçš„ç”¨ä¾‹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šç§ç°æœ‰çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬é»‘ç›’UQã€ç™½ç›’UQå’ŒLLM-as-a-Judgeï¼ŒæŒ‰éœ€å°†å®ƒä»¬è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„å“åº”çº§ç½®ä¿¡åº¦åˆ†æ•°ï¼ŒèŒƒå›´ä»0åˆ°1ã€‚ä¸ºäº†æé«˜çµæ´»æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¯è°ƒé›†æˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ç»“åˆä»»ä½•å•ä¸ªç½®ä¿¡åº¦åˆ†æ•°çš„ç»„åˆã€‚è¿™ç§æ–¹æ³•ä½¿å®è·µè€…èƒ½å¤Ÿä¸ºç‰¹å®šçš„ç”¨ä¾‹ä¼˜åŒ–é›†æˆï¼Œä»¥æé«˜æ€§èƒ½ã€‚ä¸ºäº†ç®€åŒ–å®æ–½è¿‡ç¨‹ï¼Œæ•´å¥—è¯„åˆ†å™¨éƒ½åŒ…å«åœ¨æœ¬æ–‡çš„é…å¥—Pythonå·¥å…·åŒ…UQLMä¸­ã€‚ä¸ºäº†è¯„ä¼°å„ç§è¯„åˆ†å™¨çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šä¸ªLLMé—®ç­”åŸºå‡†è¿›è¡Œäº†ä¸€ç³»åˆ—å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„å¯è°ƒé›†æˆé€šå¸¸è¶…è¿‡å…¶å„ä¸ªç»„ä»¶çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†ä¸ºæ”¹å–„LLMçš„å‡†ç¡®æ€§å’Œå¯é æ€§è€Œé‡èº«å®šåˆ¶çš„å¹»è§‰æ£€æµ‹ç­–ç•¥çš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19254v2">PDF</a> UQLM repository: <a target="_blank" rel="noopener" href="https://github.com/cvs-health/uqlm">https://github.com/cvs-health/uqlm</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§é›¶èµ„æºç¯å¢ƒä¸‹çš„å¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»ç–—å’Œé‡‘èï¼‰ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é€šè¿‡é€‚åº”å¤šç§ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æŠ€æœ¯ï¼Œå¹¶å¼•å…¥å¯è°ƒé›†æˆæ–¹æ³•ï¼Œå®ç°å¯¹LLMå¹»è§‰çš„æœ‰æ•ˆæ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥é›†æˆæ–¹æ³•é€šå¸¸ä¼˜äºå…¶å•ä¸ªç»„ä»¶ï¼Œå¹¶ä¼˜äºç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜é£é™©é¢†åŸŸä¸­çš„å¹»è§‰é—®é¢˜äºŸå¾…è§£å†³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é›¶èµ„æºå¹»è§‰æ£€æµ‹æ¡†æ¶ï¼Œä»¥é€‚åº”å¤šç§ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬é»‘ç›’UQã€ç™½ç›’UQå’ŒLLM-as-a-Judgeç­‰æŠ€æœ¯ï¼Œè½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„å“åº”çº§åˆ«ä¿¡å¿ƒåˆ†æ•°ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¯è°ƒé›†æˆæ–¹æ³•ï¼Œèƒ½å¤Ÿç»“åˆä»»ä½•å•ä¸ªç½®ä¿¡åº¦åˆ†æ•°ç»„åˆï¼Œé’ˆå¯¹ç‰¹å®šç”¨ä¾‹è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®Œæ•´çš„è¯„åˆ†è€…å¥—ä»¶åŒ…å«åœ¨è®ºæ–‡çš„Pythonå·¥å…·åŒ…UQLMä¸­ï¼Œä¾¿äºå®æ–½ã€‚</li>
<li>é€šè¿‡å¤šç»„å®éªŒéªŒè¯äº†å„ç§è¯„åˆ†è€…çš„æ€§èƒ½ï¼Œè¡¨æ˜å¯è°ƒé›†æˆæ–¹æ³•é€šå¸¸ä¼˜äºå…¶å•ä¸ªç»„ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c154cc93abbad372c21e066b34f19ea.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge"><a href="#VisualPuzzles-Decoupling-Multimodal-Reasoning-Evaluation-from-Domain-Knowledge" class="headerlink" title="VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge"></a>VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain   Knowledge</h2><p><strong>Authors:Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue</strong></p>
<p>Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with â€œthinkingâ€ modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•é€šå¸¸å°†æ¨ç†ä¸ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†æ··æ·†ï¼Œåœ¨éä¸“ä¸šç¯å¢ƒä¸­å¾ˆéš¾éš”ç¦»å’Œè¯„ä¼°ä¸€èˆ¬çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisualPuzzlesï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†è§‰æ¨ç†ä¸ºç›®æ ‡ï¼ŒåŒæ—¶æ•…æ„å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†ä¾èµ–çš„åŸºå‡†æµ‹è¯•ã€‚VisualPuzzlesåŒ…å«äº”ä¸ªç±»åˆ«çš„é—®é¢˜ï¼šç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚æˆ‘ä»¬çš„é—®é¢˜ä¸»è¦æ¥æºäºä¸­å›½å…¬åŠ¡å‘˜è€ƒè¯•ä¸­æ‰‹åŠ¨ç¿»è¯‘çš„é€»è¾‘æ¨ç†é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒVisualPuzzlesç›¸æ¯”MMMUç­‰åŸºå‡†æµ‹è¯•ï¼Œéœ€è¦æ›´å°‘çš„ç‰¹å®šé¢†åŸŸçŸ¥è¯†å’Œæ›´å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°çœŸæ­£çš„å¤šæ¨¡æ€æ¨ç†ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œæœ€å…ˆè¿›çš„å¤šåª’ä½“è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°å§‹ç»ˆè½åäºäººç±»çš„è¡¨ç°ï¼Œè€Œä¸”åœ¨çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šèƒ½åœ¨æ³¨é‡æ¨ç†ã€è½»çŸ¥è¯†çš„ä»»åŠ¡ä¸Šå–å¾—æˆåŠŸã€‚æ­¤å¤–ï¼Œå¢åŠ æ¨ç†èƒ½åŠ›çš„å¢å¼ºæ–¹æ³•ï¼ˆå¦‚æ‰©å¤§æ¨ç†è®¡ç®—è§„æ¨¡ï¼ˆé‡‡ç”¨â€œæ€è€ƒâ€æ¨¡å¼ï¼‰ï¼‰åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ç±»å‹ä¹‹é—´äº§ç”Ÿäº†ä¸ä¸€è‡´çš„æ”¶ç›Šï¼Œå¹¶ä¸”æˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹å¤§å°ä¸æ€§èƒ½ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„ç›¸å…³æ€§ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸é‡ç‚¹å¼ºè°ƒçŸ¥è¯†çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæ¨¡å‹åœ¨VisualPuzzlesä¸Šè¡¨ç°å‡ºä¸åŒçš„æ¨ç†å’Œå›ç­”æ¨¡å¼ã€‚VisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„è§†è§’æ¥è¯„ä¼°è¶…è¶Šäº‹å®è®°å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10342v3">PDF</a> 56 pages, 43 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡ä»‹ç»äº†VisualPuzzlesè¿™ä¸€è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•çš„è®¾è®¡æ€è·¯å’Œä¸»è¦æˆæœã€‚è¯¥ç ”ç©¶æŒ‡å‡ºå½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•å¾€å¾€æ··æ·†äº†æ¨ç†å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œä½¿å¾—åœ¨éä¸“ä¸šç¯å¢ƒä¸‹éš¾ä»¥è¯„ä¼°é€šç”¨çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†VisualPuzzlesåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸“æ³¨äºè§†è§‰æ¨ç†ï¼Œå¹¶å°½é‡å‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«æ¶µç›–ç®—æ³•æ¨ç†ã€ç±»æ¯”æ¨ç†ã€æ¼”ç»æ¨ç†ã€å½’çº³æ¨ç†å’Œç©ºé—´æ¨ç†äº”ä¸ªç±»åˆ«çš„é—®é¢˜ï¼Œå¹¶ä¸”è®¸å¤šé—®é¢˜çš„çµæ„Ÿæ¥æºäºä¸­å›½æ”¿åºœæœåŠ¡è€ƒè¯•çš„é€»è¾‘æ¨ç†é¢˜ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzlesæµ‹è¯•ä¾èµ–äºæ›´å°‘çš„ä¸“ä¸šçŸ¥è¯†è€Œæ›´æ³¨é‡æ¨ç†èƒ½åŠ›ã€‚åœ¨æœ€æ–°æ¨å‡ºçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æµ‹è¯•ä¸­ï¼ŒåŸºäºVisualPuzzlesçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºä¸äººç±»çš„æ€§èƒ½è¡¨ç°ä»æœ‰ä¸€å®šå·®è·ï¼ŒåŒæ—¶ä¸åŒæ¨¡å‹åœ¨å¤„ç†æ¨ç†å¯†é›†å‹ä»»åŠ¡æ—¶çš„è¡¨ç°ä¹Ÿå­˜åœ¨è¾ƒå¤§å·®å¼‚ã€‚å¦å¤–ï¼Œå¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æªæ–½åœ¨ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä¸€è‡´ï¼Œè€Œæ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´å¹¶æ— æ˜ç¡®å…³è”ã€‚æ€»çš„æ¥è¯´ï¼ŒVisualPuzzlesæä¾›äº†ä¸€ä¸ªæ›´æ¸…æ™°çš„è§†è§’æ¥è¯„ä¼°è¶…è¶Šäº‹å®è®°å¿†å’Œé¢†åŸŸçŸ¥è¯†çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•æ··æ·†äº†æ¨ç†å’Œé¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œä½¿å¾—è¯„ä¼°é€šç”¨æ¨ç†èƒ½åŠ›å˜å¾—å›°éš¾ã€‚</li>
<li>VisualPuzzlesåŸºå‡†æµ‹è¯•æ—¨åœ¨ä¸“æ³¨äºè§†è§‰æ¨ç†ï¼Œå‡å°‘å¯¹ä¸“ä¸šçŸ¥è¯†çš„ä¾èµ–ã€‚</li>
<li>VisualPuzzlesåŒ…å«å¤šç§é€»è¾‘æ¨ç†ç±»åˆ«çš„é—®é¢˜ï¼Œå¦‚ç®—æ³•ã€ç±»æ¯”ã€æ¼”ç»ã€å½’çº³å’Œç©ºé—´æ¨ç†ã€‚</li>
<li>VisualPuzzlesæµ‹è¯•çš„é—®é¢˜éƒ¨åˆ†æ¥æºäºä¸­å›½æ”¿åºœæœåŠ¡è€ƒè¯•çš„é€»è¾‘æ¨ç†é¢˜ã€‚</li>
<li>ä¸å…¶ä»–åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒVisualPuzzlesæ›´ä¾§é‡äºè¯„ä¼°æ¨ç†èƒ½åŠ›è€Œéä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>æœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨VisualPuzzlesä¸Šçš„è¡¨ç°è½åäºäººç±»ï¼Œä¸”ä¸åŒæ¨¡å‹é—´çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æªæ–½æ•ˆæœä¸ä¸€ï¼Œä¸”æ¨¡å‹è§„æ¨¡ä¸æ€§èƒ½ä¹‹é—´å¹¶æ— æ˜ç¡®å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11b7a8773c14e16edf7107e5acbb111d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d978ad9899040c77f89efe8c0764d40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea435c18c082e0889e2175cf11074b9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b70d49771c8059a717a01fa4cce49f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6bb5ec5fc171fdeea62265fbf6ca1c4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LabTOP-A-Unified-Model-for-Lab-Test-Outcome-Prediction-on-Electronic-Health-Records"><a href="#LabTOP-A-Unified-Model-for-Lab-Test-Outcome-Prediction-on-Electronic-Health-Records" class="headerlink" title="LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic   Health Records"></a>LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic   Health Records</h2><p><strong>Authors:Sujeong Im, Jungwoo Oh, Edward Choi</strong></p>
<p>Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions. </p>
<blockquote>
<p>å®éªŒå®¤æµ‹è¯•å¯¹äºç–¾ç—…çš„è¯Šæ–­å’Œç—…äººçŠ¶å†µçš„ç›‘æµ‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé¢‘ç¹çš„æµ‹è¯•å¯¹ç—…äººæ¥è¯´å¯èƒ½æ˜¯ä¸€ç§è´Ÿæ‹…ï¼Œè€Œä¸”æµ‹è¯•ç»“æœå¹¶éæ€»èƒ½ç«‹å³è·å¾—ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LabTOPï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®é‡‡ç”¨è¯­è¨€å»ºæ¨¡æ–¹æ³•è¿›è¡Œå®éªŒå®¤æµ‹è¯•ç»“æœé¢„æµ‹çš„ç»Ÿåˆæ¨¡å‹ã€‚ä¸åŒäºä»…ä¼°è®¡éƒ¨åˆ†å®éªŒå®¤æµ‹è¯•æˆ–å¯¹ç¦»æ•£å€¼èŒƒå›´è¿›è¡Œåˆ†ç±»çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒLabTOPå¯ä»¥å¯¹å¤šç§å®éªŒå®¤é¡¹ç›®è¿›è¡Œè¿ç»­çš„æ•°å€¼é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€çš„EHRæ•°æ®é›†ä¸Šè¯„ä¼°äº†LabTOPï¼Œå¹¶è¯æ˜å…¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹ä»¥åŠæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ä»¥éªŒè¯æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©ã€‚æˆ‘ä»¬ç›¸ä¿¡LabTOPå°†ä½œä¸ºå®éªŒå®¤æµ‹è¯•ç»“æœé¢„æµ‹çš„å‡†ç¡®ä¸”å¯æ¨å¹¿çš„æ¡†æ¶ï¼Œåœ¨ä¸´åºŠå†³ç­–æ”¯æŒå’Œå…³é”®ç–¾ç—…çš„æ—©æœŸæ£€æµ‹ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14259v2">PDF</a> 11 pages for main text, 13 pages for appendix</p>
<p><strong>Summary</strong>ï¼š<br>å®éªŒå®¤æµ‹è¯•æ˜¯è¯Šæ–­ç–¾ç—…å’Œç›‘æµ‹æ‚£è€…çŠ¶å†µçš„åŸºç¡€ï¼Œä½†é¢‘ç¹æµ‹è¯•å¯¹æ‚£è€…é€ æˆè´Ÿæ‹…ï¼Œä¸”ç»“æœå¹¶éæ€»èƒ½å³æ—¶å¾—çŸ¥ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºLabTOPæ¨¡å‹ï¼Œé€šè¿‡ç”µå­å¥åº·è®°å½•æ•°æ®çš„è¯­è¨€å»ºæ¨¡æ–¹æ³•é¢„æµ‹å®éªŒå®¤æµ‹è¯•ç»“æœã€‚ä¸åŒäºä»…ä¼°è®¡éƒ¨åˆ†æµ‹è¯•æˆ–åˆ†ç±»ç¦»æ•£å€¼èŒƒå›´çš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒLabTOPå¯è¿›è¡Œå„ç§å®éªŒå®¤é¡¹ç›®çš„è¿ç»­æ•°å€¼é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„ç”µå­å¥åº·è®°å½•æ•°æ®é›†ä¸Šè¯„ä¼°LabTOPï¼Œè¯æ˜å…¶ä¼˜äºä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œç›®å‰é¡¶å°–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬ç›¸ä¿¡LabTOPå°†æˆä¸ºå‡†ç¡®ä¸”å¯æ¨å¹¿çš„å®éªŒå®¤æµ‹è¯•ç»“æœé¢„æµ‹æ¡†æ¶ï¼Œå¯åº”ç”¨äºä¸´åºŠå†³ç­–æ”¯æŒå’Œå…³é”®ç–¾ç—…çš„æ—©æœŸæ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LabTOPæ¨¡å‹åˆ©ç”¨è¯­è¨€å»ºæ¨¡åœ¨ç”µå­å¥åº·è®°å½•æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹å®éªŒå®¤æµ‹è¯•ç»“æœã€‚</li>
<li>LabTOPèƒ½å¤Ÿé’ˆå¯¹å„ç§å®éªŒå®¤é¡¹ç›®åšå‡ºè¿ç»­æ•°å€¼é¢„æµ‹ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒLabTOPè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>LabTOPçš„è®¾è®¡é€‰æ‹©ç»è¿‡å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>LabTOPæ¨¡å‹å…·æœ‰å‡†ç¡®åº¦å’Œæ³›åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºä¸´åºŠå†³ç­–æ”¯æŒã€‚</li>
<li>LabTOPæœ‰åŠ©äºå‡è½»æ‚£è€…çš„é¢‘ç¹æµ‹è¯•è´Ÿæ‹…å’ŒåŠæ—¶å¾—çŸ¥ç»“æœçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-862616b55dc5f93e0a858a23ece747dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3896f709c91d2dc950c4da0a492609e2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="T2VEval-Benchmark-Dataset-and-Objective-Evaluation-Method-for-T2V-generated-Videos"><a href="#T2VEval-Benchmark-Dataset-and-Objective-Evaluation-Method-for-T2V-generated-Videos" class="headerlink" title="T2VEval: Benchmark Dataset and Objective Evaluation Method for   T2V-generated Videos"></a>T2VEval: Benchmark Dataset and Objective Evaluation Method for   T2V-generated Videos</h2><p><strong>Authors:Zelu Qi, Ping Shi, Shuqi Wang, Chaoyang Zhang, Fei Zhao, Zefeng Ying, Da Pan, Xi Yang, Zheqi He, Teng Dai</strong></p>
<p>Recent advances in text-to-video (T2V) technology, as demonstrated by models such as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the applicability and popularity of the technology. This progress has created a growing demand for accurate quality assessment metrics to evaluate the perceptual quality of T2V-generated videos and optimize video generation models. However, assessing the quality of text-to-video outputs remain challenging due to the presence of highly complex distortions, such as unnatural actions and phenomena that defy human cognition. To address these challenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset for text-to-video quality evaluation, which contains 148 textual prompts and 1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation, we scored each video on four dimensions in the subjective experiment, which are overall impression, text-video consistency, realness, and technical quality. Based on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for T2V quality evaluation. T2VEval assesses videos across three branches: text-video consistency, realness, and technical quality. Using an attention-based fusion module, T2VEval effectively integrates features from each branch and predicts scores with the aid of a large language model. Additionally, we implemented a divide-and-conquer training strategy, enabling each branch to learn targeted knowledge while maintaining synergy with the others. Experimental results demonstrate that T2VEval achieves state-of-the-art performance across multiple metrics. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰æŠ€æœ¯çš„è¿›å±•ï¼Œç”±è¯¸å¦‚è·‘é“Gen-3ã€çš®å¡ã€ç´¢æ‹‰å’Œå…‹æ—æ¨¡å‹æ‰€å±•ç¤ºï¼Œå·²ç»æ˜¾è‘—åœ°æ‹“å®½äº†è¿™é¡¹æŠ€æœ¯çš„åº”ç”¨èŒƒå›´å’Œå—æ¬¢è¿ç¨‹åº¦ã€‚è¿™ä¸€è¿›å±•å‚¬ç”Ÿäº†å¯¹å‡†ç¡®çš„è´¨é‡è¯„ä¼°æŒ‡æ ‡çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œä»¥è¯„ä¼°T2Vç”Ÿæˆè§†é¢‘çš„æ„ŸçŸ¥è´¨é‡å¹¶ä¼˜åŒ–è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¯„ä¼°æ–‡æœ¬è½¬è§†é¢‘è¾“å‡ºçš„è´¨é‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨é«˜åº¦å¤æ‚çš„å¤±çœŸï¼Œä¾‹å¦‚ä¸è‡ªç„¶çš„è¡Œä¸ºå’Œè¿èƒŒäººç±»è®¤çŸ¥çš„ç°è±¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ„å»ºäº†T2VEval-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬è½¬è§†é¢‘è´¨é‡è¯„ä¼°çš„å¤šç»´åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«148ä¸ªæ–‡æœ¬æç¤ºå’Œç”±13ä¸ªT2Væ¨¡å‹ç”Ÿæˆçš„1,783ä¸ªè§†é¢‘ã€‚ä¸ºäº†ç¡®ä¿å…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬åœ¨ä¸»è§‚å®éªŒä¸­å¯¹æ¯ä¸ªè§†é¢‘åœ¨å››ä¸ªç»´åº¦ä¸Šè¿›è¡Œäº†è¯„åˆ†ï¼Œåˆ†åˆ«æ˜¯æ•´ä½“å°è±¡ã€æ–‡æœ¬ä¸è§†é¢‘çš„ä¸€è‡´æ€§ã€çœŸå®æ€§å’ŒæŠ€æœ¯è´¨é‡ã€‚åŸºäºT2VEval-Benchï¼Œæˆ‘ä»¬å¼€å‘äº†T2VEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºT2Vè´¨é‡è¯„ä¼°çš„å¤šåˆ†æ”¯èåˆæ–¹æ¡ˆã€‚T2VEvalé€šè¿‡ä¸‰ä¸ªåˆ†æ”¯æ¥è¯„ä¼°è§†é¢‘ï¼šæ–‡æœ¬ä¸è§†é¢‘çš„ä¸€è‡´æ€§ã€çœŸå®æ€§å’ŒæŠ€æœ¯è´¨é‡ã€‚é€šè¿‡ä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„èåˆæ¨¡å—ï¼ŒT2VEvalæœ‰æ•ˆåœ°é›†æˆäº†æ¯ä¸ªåˆ†æ”¯çš„ç‰¹å¾ï¼Œå¹¶å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„æµ‹è¯„åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®æ–½äº†åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¯ä¸ªåˆ†æ”¯èƒ½å¤Ÿå­¦ä¹ æœ‰é’ˆå¯¹æ€§çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¸å…¶ä»–åˆ†æ”¯ä¿æŒååŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2VEvalåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08545v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬è½¬è§†é¢‘ï¼ˆT2Vï¼‰æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¦‚Runway Gen-3ã€Pikaã€Soraå’ŒKlingç­‰æ¨¡å‹ï¼Œå¤§å¤§æ‹“å®½äº†T2VæŠ€æœ¯çš„åº”ç”¨èŒƒå›´å’Œå—æ¬¢è¿ç¨‹åº¦ã€‚ä¸ºè¯„ä¼°T2Vç”Ÿæˆè§†é¢‘çš„æ„ŸçŸ¥è´¨é‡å¹¶ä¼˜åŒ–è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæå‡ºäº†T2VEval-Benchï¼Œä¸€ä¸ªå¤šç»´åº¦çš„T2Vè´¨é‡è¯„ä¼°åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«148ä¸ªæ–‡æœ¬æç¤ºå’Œ1783ä¸ªç”±13ä¸ªT2Væ¨¡å‹ç”Ÿæˆçš„è§†é¢‘ã€‚ä¸ºå…¨é¢è¯„ä¼°è§†é¢‘è´¨é‡ï¼Œæˆ‘ä»¬åœ¨ä¸»è§‚å®éªŒä¸­å¯¹è§†é¢‘è¿›è¡Œäº†å››ä¸ªç»´åº¦çš„è¯„åˆ†ï¼šæ•´ä½“å°è±¡ã€æ–‡æœ¬ä¸è§†é¢‘ä¸€è‡´æ€§ã€çœŸå®æ€§å’ŒæŠ€æœ¯è´¨é‡ã€‚åŸºäºT2VEval-Benchï¼Œå¼€å‘äº†T2VEvalï¼Œä¸€ä¸ªç”¨äºT2Vè´¨é‡è¯„ä¼°çš„å¤šåˆ†æ”¯èåˆæ–¹æ¡ˆã€‚T2VEvalé€šè¿‡æ³¨æ„åŠ›åŸºç¡€çš„èåˆæ¨¡å—ï¼Œæœ‰æ•ˆæ•´åˆå„åˆ†æ”¯çš„ç‰¹å¾ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„åˆ†é¢„æµ‹ã€‚æ­¤å¤–ï¼Œå®æ–½äº†åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å„åˆ†æ”¯èƒ½å¤Ÿå­¦ä¹ é’ˆå¯¹æ€§çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒå½¼æ­¤é—´çš„ååŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2VEvalåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2VæŠ€æœ¯çš„æœ€æ–°è¿›å±•å±•ç¤ºäº†å¹¿æ³›çš„åº”ç”¨å’Œå—æ¬¢è¿ç¨‹åº¦ï¼Œäº§ç”Ÿäº†å¯¹è¯„ä¼°å…¶ç”Ÿæˆè§†é¢‘è´¨é‡çš„éœ€æ±‚ã€‚</li>
<li>T2VEval-Benchæ˜¯ä¸€ä¸ªå¤šç»´åº¦æ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬æç¤ºå’Œç”±ä¸åŒT2Væ¨¡å‹ç”Ÿæˆçš„è§†é¢‘ï¼Œç”¨äºå…¨é¢è¯„ä¼°è§†é¢‘è´¨é‡ã€‚</li>
<li>è§†é¢‘è´¨é‡è¯„ä¼°åŒ…æ‹¬å››ä¸ªç»´åº¦ï¼šæ•´ä½“å°è±¡ã€æ–‡æœ¬ä¸è§†é¢‘ä¸€è‡´æ€§ã€çœŸå®æ€§å’ŒæŠ€æœ¯è´¨é‡ã€‚</li>
<li>T2VEvalæ˜¯ä¸€ä¸ªå¤šåˆ†æ”¯èåˆæ–¹æ¡ˆï¼Œç”¨äºT2Vè´¨é‡è¯„ä¼°ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶æ•´åˆä¸åŒåˆ†æ”¯çš„ç‰¹å¾ã€‚</li>
<li>T2VEvalå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„åˆ†é¢„æµ‹ï¼Œæé«˜è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>å®æ–½åˆ†è€Œæ²»ä¹‹çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å„åˆ†æ”¯èƒ½å¤Ÿé’ˆå¯¹æ€§åœ°å­¦ä¹ çŸ¥è¯†ï¼Œå¹¶ä¿æŒå½¼æ­¤é—´çš„ååŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f67f2249400d5f0747464811501e74d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab80004c089385b32026c90cd0700c5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20756d0a43e1dea0729a7e20b6dc633a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b22f5e867d919e023879b2761264c77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b28e6e5833ee026b90046fc9336b6031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f59d492eb0098a250a2263ed6fda49ad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Adsorb-Agent-Autonomous-Identification-of-Stable-Adsorption-Configurations-via-Large-Language-Model-Agent"><a href="#Adsorb-Agent-Autonomous-Identification-of-Stable-Adsorption-Configurations-via-Large-Language-Model-Agent" class="headerlink" title="Adsorb-Agent: Autonomous Identification of Stable Adsorption   Configurations via Large Language Model Agent"></a>Adsorb-Agent: Autonomous Identification of Stable Adsorption   Configurations via Large Language Model Agent</h2><p><strong>Authors:Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani</strong></p>
<p>Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agentâ€™s performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions. </p>
<blockquote>
<p>å¸é™„èƒ½æ˜¯å‚¬åŒ–ä¸­çš„å…³é”®ååº”æ€§æè¿°ç¬¦ï¼Œèƒ½å¤Ÿå®ç°é«˜æ•ˆç­›é€‰æœ€ä½³å‚¬åŒ–å‰‚ã€‚ç„¶è€Œï¼Œç¡®å®šå¸é™„èƒ½é€šå¸¸éœ€è¦è¯„ä¼°ä¼—å¤šçš„å¸é™„ç‰©-å‚¬åŒ–å‰‚ç»„åˆã€‚å½“å‰çš„ç®—æ³•æ–¹æ³•ä¾èµ–äºå¯¹å¸é™„ä½ç‚¹å’Œé…ç½®çš„è¯¦å°½æšä¸¾ï¼Œè¿™ä½¿å¾—è¿‡ç¨‹è®¡ç®—é‡å¤§ï¼Œå¹¶ä¸”ä¸ä¿è¯èƒ½æ‰¾åˆ°å…¨å±€æœ€ä½èƒ½é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Adsorb-Agentï¼Œè¿™æ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°è¯†åˆ«ä¸å…¨å±€æœ€ä½å¸é™„èƒ½é‡ç›¸å¯¹åº”çš„ç‰¹å®šç³»ç»Ÿç¨³å®šçš„å¸é™„é…ç½®ã€‚Adsorb-Agentåˆ©ç”¨å…¶å†…ç½®çŸ¥è¯†å’Œæ–°å…´æ¨ç†èƒ½åŠ›æ¥æœ‰ç­–ç•¥åœ°æ¢ç´¢å¯èƒ½åŒ…å«å¸é™„èƒ½çš„å¸é™„é…ç½®ã€‚é€šè¿‡å‡å°‘å¯¹è¯¦å°½é‡‡æ ·çš„ä¾èµ–ï¼Œå®ƒåœ¨å‡å°‘æ‰€éœ€åˆå§‹é…ç½®æ•°é‡çš„åŒæ—¶æé«˜äº†å¸é™„èƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†Adsorb-Agentåœ¨æ¶µç›–å„ç§å¤æ‚æ€§çš„äºŒåä¸ªä»£è¡¨æ€§ç³»ç»Ÿä¸Šçš„æ€§èƒ½ã€‚Adsorb-AgentæˆåŠŸåœ°ä¸º83.7%çš„ç³»ç»Ÿç¡®å®šäº†å¯æ¯”çš„å¸é™„èƒ½ï¼Œå¹¶ä¸º35%çš„ç³»ç»Ÿå®ç°äº†æ›´æ¥è¿‘å®é™…å…¨å±€æœ€å°å€¼æ›´ä½çš„èƒ½é‡ï¼ŒåŒæ—¶æ‰€éœ€çš„åˆå§‹é…ç½®æ•°é‡è¿œè¿œå°‘äºä¼ ç»Ÿæ–¹æ³•ã€‚å…¶èƒ½åŠ›åœ¨å¤æ‚ç³»ç»Ÿä¸­å°¤ä¸ºçªå‡ºï¼Œåœ¨æ¶‰åŠé‡‘å±é—´è¡¨é¢çš„ç³»ç»Ÿä¸­ä¸º46.7%çš„ç³»ç»Ÿæ‰¾åˆ°äº†è¾ƒä½çš„å¸é™„èƒ½ï¼Œåœ¨å…·æœ‰å¤§å¸é™„åˆ†å­çš„ç³»ç»Ÿä¸­ä¸º66.7%çš„ç³»ç»Ÿæ‰¾åˆ°äº†è¾ƒä½çš„å¸é™„èƒ½ã€‚è¿™äº›ç»“æœè¯æ˜äº†Adsorb-Agentåœ¨å‡å°‘è®¡ç®—æˆæœ¬å’Œæé«˜å¸é™„èƒ½é¢„æµ‹å¯é æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œä»è€Œæœ‰æœ›åŠ é€Ÿå‚¬åŒ–å‰‚çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16658v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¸é™„èƒ½æ˜¯å‚¬åŒ–ä¸­çš„å…³é”®ååº”æ€§æè¿°ç¬¦ï¼Œå¯ç”¨äºæœ‰æ•ˆç­›é€‰æœ€ä½³å‚¬åŒ–å‰‚ã€‚ç„¶è€Œï¼Œç¡®å®šå¸é™„èƒ½é€šå¸¸éœ€è¦è¯„ä¼°ä¼—å¤šçš„å¸é™„ç‰©-å‚¬åŒ–å‰‚æ„å‹ã€‚å½“å‰ç®—æ³•æ–¹æ³•ä¾èµ–äºå¸é™„ä½ç‚¹å’Œæ„å‹çš„è¯¦å°½åˆ—ä¸¾ï¼Œè¿™ä½¿å¾—è¿‡ç¨‹è®¡ç®—å¯†é›†ï¼Œå¹¶ä¸”ä¸ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä½èƒ½é‡ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸é™„å‰‚Agentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ï¼Œè¢«è®¾è®¡ç”¨äºæœ‰æ•ˆè¯†åˆ«ä¸å…¨å±€æœ€ä½å¸é™„èƒ½å¯¹åº”çš„ç³»ç»Ÿç‰¹å®šç¨³å®šå¸é™„æ„å‹ã€‚å¸é™„å‰‚Agentåˆ©ç”¨å…¶å†…ç½®çŸ¥è¯†å’Œæ–°å…´æ¨ç†èƒ½åŠ›æ¥ç­–ç•¥æ€§åœ°æ¢ç´¢å¯èƒ½å«æœ‰å¸é™„èƒ½çš„å¸é™„æ„å‹ã€‚é€šè¿‡å‡å°‘å¯¹è¯¦å°½é‡‡æ ·çš„ä¾èµ–ï¼Œå®ƒåœ¨å‡å°‘æ‰€éœ€åˆå§‹æ„å‹æ•°é‡çš„åŒæ—¶æé«˜äº†å¸é™„èƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å¯¹å¸é™„å‰‚Agentåœ¨äºŒåä¸ªä»£è¡¨æ€§ç³»ç»Ÿä¸Šçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›ç³»ç»Ÿçš„å¤æ‚æ€§å„ä¸ç›¸åŒã€‚å¸é™„å‰‚AgentæˆåŠŸåœ°ä¸º83.7%çš„ç³»ç»Ÿç¡®å®šäº†å¯æ¯”çš„å¸é™„èƒ½ï¼Œå¹¶ä¸º35%çš„ç³»ç»Ÿè¾¾åˆ°äº†æ›´æ¥è¿‘å®é™…å…¨å±€æœ€å°å€¼æ›´ä½çš„èƒ½é‡ï¼ŒåŒæ—¶æ‰€éœ€çš„åˆå§‹æ„å‹æ•°é‡è¿œå°‘äºä¼ ç»Ÿæ–¹æ³•ã€‚å…¶åœ¨å¤æ‚ç³»ç»Ÿä¸­çš„èƒ½åŠ›å°¤ä¸ºçªå‡ºï¼Œä¸ºæ¶‰åŠé‡‘å±é—´è¡¨é¢çš„ç³»ç»Ÿé™ä½äº†46.7%çš„å¸é™„èƒ½ï¼Œä¸ºå¤§å¸é™„ç‰©åˆ†å­çš„ç³»ç»Ÿé™ä½äº†66.7%çš„å¸é™„èƒ½ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†å¸é™„å‰‚Agentåœ¨é€šè¿‡å‡å°‘è®¡ç®—æˆæœ¬å’Œæé«˜å¸é™„èƒ½é¢„æµ‹å¯é æ€§æ¥åŠ é€Ÿå‚¬åŒ–å‰‚å‘ç°æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸é™„èƒ½æ˜¯å‚¬åŒ–ä¸­çš„å…³é”®ååº”æ€§æè¿°ç¬¦ï¼Œç”¨äºç­›é€‰æœ€ä½³å‚¬åŒ–å‰‚ã€‚</li>
<li>å½“å‰ç®—æ³•é€šè¿‡è¯¦å°½åˆ—ä¸¾å¸é™„ä½ç‚¹å’Œæ„å‹æ¥ç¡®å®šå¸é™„èƒ½ï¼Œä½†æ­¤æ–¹æ³•è®¡ç®—å¯†é›†å¹¶ä¸ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä½èƒ½é‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†â€”â€”å¸é™„å‰‚Agentï¼Œå¯é«˜æ•ˆè¯†åˆ«ç³»ç»Ÿç‰¹å®šçš„ç¨³å®šå¸é™„æ„å‹ã€‚</li>
<li>å¸é™„å‰‚Agentåˆ©ç”¨å†…ç½®çŸ¥è¯†å’Œæ–°å…´æ¨ç†èƒ½åŠ›ï¼Œç­–ç•¥æ€§åœ°æ¢ç´¢å¯èƒ½å«æœ‰å¸é™„èƒ½çš„å¸é™„æ„å‹ã€‚</li>
<li>ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œå¸é™„å‰‚Agentå‡å°‘äº†åˆå§‹æ„å‹çš„éœ€æ±‚ï¼Œæé«˜äº†å¸é™„èƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤æ‚ç³»ç»Ÿä¸­ï¼Œå¸é™„å‰‚Agentè¡¨ç°çªå‡ºï¼Œä¸ºæ¶‰åŠé‡‘å±é—´è¡¨é¢å’Œå¤§å¸é™„ç‰©åˆ†å­çš„ç³»ç»Ÿé™ä½äº†å¸é™„èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e0b279adcebf07254a23f8c5be9f8c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9284e878eb4d416f5a7a9dd166185b3e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Extracting-and-Transferring-Abilities-For-Building-Multi-lingual-Ability-enhanced-Large-Language-Models"><a href="#Extracting-and-Transferring-Abilities-For-Building-Multi-lingual-Ability-enhanced-Large-Language-Models" class="headerlink" title="Extracting and Transferring Abilities For Building Multi-lingual   Ability-enhanced Large Language Models"></a>Extracting and Transferring Abilities For Building Multi-lingual   Ability-enhanced Large Language Models</h2><p><strong>Authors:Zhipeng Chen, Kun Zhou, Liang Song, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen</strong></p>
<p>Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may be not available for low-resource languages. To solve it, we propose a Multi-lingual Ability Extraction and Transfer approach, named as MAET. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and transfer them across different languages by simple addition and subtraction operations without training. Specially, our MAET consists of the extraction and transfer stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-specific weights. In the transfer stage, we further select the ability-related parameter tensors, and design the merging strategy based on the linguistic and ability specific weights, to build the multi-lingual ability-enhanced LLM. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on mathematical and scientific tasks in both high-resource lingual and low-resource lingual scenarios. Experiment results have shown that MAET can effectively and efficiently extract and transfer the advanced abilities, and outperform training-based baseline methods. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/MAET">https://github.com/RUCAIBox/MAET</a>. </p>
<blockquote>
<p>å¤šè¯­è¨€èƒ½åŠ›çš„è¿ç§»å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›åº”ç”¨è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰å·¥ä½œé«˜åº¦ä¾èµ–äºå¤šè¯­è¨€èƒ½åŠ›ç›¸å…³æ•°æ®æ¥è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¹äºä½èµ„æºè¯­è¨€å¯èƒ½æ— æ³•è·å¾—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè¯­è¨€èƒ½åŠ›æå–ä¸è¿ç§»æ–¹æ³•ï¼Œå‘½åä¸ºMAETã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»LLMä¸­åˆ†è§£å¹¶æå–ä¸è¯­è¨€æ— å…³çš„èƒ½åŠ›ç›¸å…³æƒé‡ï¼Œé€šè¿‡ç®€å•çš„åŠ å‡æ“ä½œåœ¨ä¸åŒè¯­è¨€ä¹‹é—´è¿›è¡Œè¿ç§»ï¼Œæ— éœ€è®­ç»ƒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„MAETç”±æå–å’Œè¿ç§»ä¸¤ä¸ªé˜¶æ®µç»„æˆã€‚åœ¨æå–é˜¶æ®µï¼Œæˆ‘ä»¬é¦–å…ˆè¦æ‰¾åˆ°ä¸ç‰¹å®šèƒ½åŠ›é«˜åº¦ç›¸å…³çš„å…³é”®ç¥ç»å…ƒï¼Œç„¶ååˆ©ç”¨å®ƒä»¬æ¥æå–å¯è¿ç§»çš„èƒ½åŠ›ç‰¹å®šæƒé‡ã€‚åœ¨è¿ç§»é˜¶æ®µï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€‰æ‹©ä¸èƒ½åŠ›ç›¸å…³çš„å‚æ•°å¼ é‡ï¼Œå¹¶è®¾è®¡åŸºäºè¯­è¨€å’Œèƒ½åŠ›ç‰¹å®šæƒé‡çš„åˆå¹¶ç­–ç•¥ï¼Œä»¥æ„å»ºå¤šè¯­è¨€èƒ½åŠ›å¢å¼ºçš„LLMã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨é«˜èµ„æºè¯­è¨€å’Œä½èµ„æºè¯­è¨€åœºæ™¯ä¸‹å¯¹æ•°å­¦å’Œç§‘å­¦ä»»åŠ¡è¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAETå¯ä»¥æœ‰æ•ˆä¸”é«˜æ•ˆåœ°æå–å’Œè¿ç§»é«˜çº§èƒ½åŠ›ï¼Œå¹¶ä¼˜äºåŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/MAET%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/MAETæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07825v2">PDF</a> 17 Pages. Working in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMAETçš„å¤šè¯­è¨€èƒ½åŠ›æå–ä¸è¿ç§»æ–¹æ³•ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒè¯­è¨€ç¯å¢ƒä¸‹çš„åº”ç”¨èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†è§£å’Œæå–è¯­è¨€æ— å…³çš„èƒ½åŠ›ç›¸å…³æƒé‡ï¼Œå®ç°è·¨ä¸åŒè¯­è¨€çš„è¿ç§»ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚é€šè¿‡æå–é˜¶æ®µæ‰¾åˆ°ä¸ç‰¹å®šèƒ½åŠ›é«˜åº¦ç›¸å…³çš„å…³é”®ç¥ç»å…ƒï¼Œå¹¶æ®æ­¤æå–å¯è¿ç§»çš„èƒ½åŠ›ç‰¹å®šæƒé‡ã€‚åœ¨è¿ç§»é˜¶æ®µï¼Œé€‰æ‹©èƒ½åŠ›ç›¸å…³çš„å‚æ•°å¼ é‡ï¼Œå¹¶è®¾è®¡åŸºäºè¯­è¨€å’Œèƒ½åŠ›çš„æƒé‡åˆå¹¶ç­–ç•¥ï¼Œä»¥æ„å»ºå¤šè¯­è¨€èƒ½åŠ›å¢å¼ºå‹LLMã€‚å®éªŒè¯æ˜ï¼ŒMAETèƒ½æœ‰æ•ˆæå–å’Œè¿ç§»é«˜çº§èƒ½åŠ›ï¼Œå¹¶åœ¨é«˜èµ„æºè¯­è¨€å’Œä½èµ„æºè¯­è¨€åœºæ™¯ä¸­è¶…è¶ŠåŸºäºè®­ç»ƒçš„åŸºç¡€æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MAETæ–¹æ³•æ—¨åœ¨è§£å†³å¤šè¯­è¨€ç¯å¢ƒä¸‹LLMåº”ç”¨èƒ½åŠ›çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­ã€‚</li>
<li>MAETé€šè¿‡åˆ†è§£å’Œæå–LLMä¸­çš„è¯­è¨€æ— å…³èƒ½åŠ›ç›¸å…³æƒé‡ï¼Œå®ç°è·¨ä¸åŒè¯­è¨€çš„è¿ç§»ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬æå–å’Œè¿ç§»ä¸¤ä¸ªé˜¶æ®µï¼Œæå–é˜¶æ®µæ‰¾åˆ°ä¸ç‰¹å®šèƒ½åŠ›ç›¸å…³çš„å…³é”®ç¥ç»å…ƒï¼Œè¿ç§»é˜¶æ®µåˆ™é€‰æ‹©èƒ½åŠ›ç›¸å…³å‚æ•°å¼ é‡å¹¶è®¾è®¡åˆå¹¶ç­–ç•¥ã€‚</li>
<li>MAETæ–¹æ³•åœ¨å¤šç§è¯­è¨€å’Œç§‘å­¦ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>MAETæ–¹æ³•å®ç°äº†é«˜æ•ˆçš„èƒ½åŠ›æå–å’Œè¿ç§»ï¼Œè¶…è¶Šäº†åŸºäºè®­ç»ƒçš„åŸºç¡€æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-081c538b35edbfda0f2435770e09e39d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce934237b6c64b05ac6df47d65236f94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e58636d4c56b9f5f59c2a249c95b186c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Formal-Framework-for-Understanding-Length-Generalization-in-Transformers"><a href="#A-Formal-Framework-for-Understanding-Length-Generalization-in-Transformers" class="headerlink" title="A Formal Framework for Understanding Length Generalization in   Transformers"></a>A Formal Framework for Understanding Length Generalization in   Transformers</h2><p><strong>Authors:Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn</strong></p>
<p>A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers. </p>
<blockquote>
<p>å¯¹äºè½¬æ¢å™¨æ¨¡å‹æ¥è¯´ï¼Œä¸€ä¸ªä¸»è¦çš„æŒ‘æˆ˜æ˜¯å°†å…¶æ¨å¹¿åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿çš„åºåˆ—ã€‚å°½ç®¡ä¹‹å‰çš„å·¥ä½œä»å®è¯è§’åº¦è¯æ˜äº†è½¬æ¢å™¨åœ¨æŸäº›ä»»åŠ¡ä¸Šå¯ä»¥åœ¨é•¿åº¦æ¨å¹¿æ–¹é¢å–å¾—æˆåŠŸæˆ–å¤±è´¥ï¼Œä½†å¯¹äºè¿™ä¸€ç°è±¡çš„ç†è®ºç†è§£ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¥æ ¼çš„ç†è®ºæ¡†æ¶æ¥åˆ†æå¸¦æœ‰å¯å­¦ä¹ ç»å¯¹ä½ç½®ç¼–ç çš„å› æœè½¬æ¢å™¨çš„é•¿åº¦æ¨å¹¿é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ»ç”»äº†åœ¨ç†æƒ³åŒ–æ¨ç†æ–¹æ¡ˆä¸‹ï¼Œä½¿ç”¨åŸºäºèŒƒæ•°çš„æ­£åˆ™åŒ–å™¨ï¼Œä»è¶³å¤Ÿé•¿çš„è¾“å…¥ä¸­è¯†åˆ«å‡ºçš„é‚£äº›å…·æœ‰ç»å¯¹ä½ç½®ç¼–ç çš„å‡½æ•°ç‰¹æ€§ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè¯æ˜å¯¹äºä¸€ç³»åˆ—ä¸°å¯Œçš„é—®é¢˜è¿›è¡Œé•¿åº¦æ¨å¹¿çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒéªŒè¯äº†è¯¥ç†è®ºåœ¨é¢„æµ‹é•¿åº¦æ¨å¹¿æˆåŠŸå’Œå¤±è´¥æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—ç®—æ³•å’Œå½¢å¼è¯­è¨€ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç†è®ºä¸ä»…è§£é‡Šäº†å¤§é‡å®è¯è§‚å¯Ÿç»“æœï¼Œè€Œä¸”ä¸ºé¢„æµ‹è½¬æ¢å™¨ä¸­çš„é•¿åº¦æ¨å¹¿èƒ½åŠ›å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02140v3">PDF</a> 85 pages, 9 figures, 11 tables. Accepted for publication at ICLR 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªä¸¥è°¨çš„ç†è®ºæ¡†æ¶ï¼Œç”¨äºåˆ†æå…·æœ‰å¯å­¦ä¹ ç»å¯¹ä½ç½®ç¼–ç çš„å› æœTransformerä¸­çš„é•¿åº¦æ³›åŒ–é—®é¢˜ã€‚é€šè¿‡å¯¹è¶³å¤Ÿé•¿è¾“å…¥ä¸‹é€šè¿‡ç†æƒ³åŒ–æ¨ç†æ–¹æ¡ˆåˆ©ç”¨ç»å¯¹ä½ç½®ç¼–ç è¯†åˆ«å‡ºçš„å‡½æ•°ç‰¹æ€§è¿›è¡Œæè¿°ï¼Œæœ¬æ–‡è¯æ˜äº†åœ¨ä¸°å¯Œçš„ä»»åŠ¡é›†ä¸­å®ç°é•¿åº¦æ³›åŒ–çš„å¯èƒ½æ€§ã€‚åŒæ—¶ï¼Œå®éªŒéªŒè¯äº†è¯¥ç†è®ºåœ¨ç®—æ³•å’Œè‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­é¢„æµ‹é•¿åº¦æ³›åŒ–æˆåŠŸä¸å¤±è´¥çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä¸ä»…è§£é‡Šäº†å¹¿æ³›çš„å®éªŒè§‚å¯Ÿç»“æœï¼Œè€Œä¸”ä¸ºé¢„æµ‹Transformerçš„é•¿åº¦æ³›åŒ–èƒ½åŠ›å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¼•å…¥äº†ç†è®ºæ¡†æ¶æ¥åˆ†æå› æœTransformerä¸­çš„é•¿åº¦æ³›åŒ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡æè¿°è¶³å¤Ÿé•¿è¾“å…¥ä¸‹åˆ©ç”¨ç»å¯¹ä½ç½®ç¼–ç è¯†åˆ«å‡½æ•°çš„ç‰¹æ€§ï¼Œè¯æ˜äº†åœ¨ä¸°å¯Œä»»åŠ¡é›†ä¸­å®ç°é•¿åº¦æ³›åŒ–çš„å¯èƒ½æ€§ã€‚</li>
<li>æä¾›äº†å¯¹é•¿åº¦æ³›åŒ–ç°è±¡çš„ä¸¥è°¨ç†è®ºç†è§£ã€‚</li>
<li>å®éªŒéªŒè¯äº†ç†è®ºåœ¨é¢„æµ‹é•¿åº¦æ³›åŒ–æˆåŠŸä¸å¤±è´¥æ–¹é¢çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥ç†è®ºä¸ä»…è§£é‡Šäº†å¹¿æ³›çš„å®éªŒè§‚å¯Ÿç»“æœï¼Œè€Œä¸”ä¸ºé¢„æµ‹Transformerçš„æ€§èƒ½æä¾›äº†ä¾æ®ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†ç»å¯¹ä½ç½®ç¼–ç åœ¨é•¿åº¦æ³›åŒ–ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-564f69bfae5a7c6ec32822c21ca0656a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37d9704c0d099710ef77ea8a508e0bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d74a9d7a10c282f489f405ec119ba1b4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Revise-Reason-and-Recognize-LLM-Based-Emotion-Recognition-via-Emotion-Specific-Prompts-and-ASR-Error-Correction"><a href="#Revise-Reason-and-Recognize-LLM-Based-Emotion-Recognition-via-Emotion-Specific-Prompts-and-ASR-Error-Correction" class="headerlink" title="Revise, Reason, and Recognize: LLM-Based Emotion Recognition via   Emotion-Specific Prompts and ASR Error Correction"></a>Revise, Reason, and Recognize: LLM-Based Emotion Recognition via   Emotion-Specific Prompts and ASR Error Correction</h2><p><strong>Authors:Yuanchao Li, Yuan Gong, Chao-Han Huck Yang, Peter Bell, Catherine Lai</strong></p>
<p>Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œä½¿ç”¨æç¤ºå·¥ç¨‹è¿›è¡Œè¯­éŸ³æƒ…ç»ªæ ‡æ³¨å’Œè¯†åˆ«æœ€è¿‘å¼€å§‹æµ®ç°ï¼Œä½†å…¶æ•ˆæœå’Œå¯é æ€§ä»ç„¶å€¼å¾—æ€€ç–‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸€ä¸»é¢˜è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œé¦–å…ˆæå‡ºäº†èå…¥å£°å­¦ã€è¯­è¨€å­¦å’Œå¿ƒç†å­¦çš„ç‰¹å®šæƒ…ç»ªçŸ¥è¯†çš„æ–°æç¤ºã€‚éšåï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºLLMçš„æç¤ºåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶ä¸çœŸå®è½¬å½•è¿›è¡Œäº†å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºå­˜åœ¨ASRé”™è¯¯çš„é²æ£’æ€§LLMæƒ…ç»ªè¯†åˆ«æå‡ºäº†ä¿®è®¢-æ¨ç†-è¯†åˆ«çš„æç¤ºç®¡é“ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥å­¦ä¹ ã€ä¸Šä¸‹æ–‡å†…å­¦ä¹ å’ŒæŒ‡ä»¤è°ƒæ•´æ–¹é¢è¿›è¡Œäº†å®éªŒï¼Œä»¥æ£€éªŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨æ­¤æ–¹å‘ä¸Šçš„å®ç”¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMså¯¹å¾®å°æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚å®éªŒç»“æœè¯æ˜äº†ç‰¹å®šæƒ…ç»ªæç¤ºã€ASRé”™è¯¯ä¿®æ­£å’ŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨åŸºäºLLMçš„æƒ…ç»ªè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨å®Œå–„LLMåœ¨æƒ…ç»ªè¯†åˆ«å’Œç›¸å…³é¢†åŸŸçš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15551v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹è¿›è¡Œè¯­éŸ³æƒ…ç»ªæ³¨è§£å’Œè¯†åˆ«å·²å´­éœ²å¤´è§’ï¼Œä½†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§ä»å­˜ç–‘ã€‚æœ¬æ–‡å¯¹æ­¤è¿›è¡Œç³»ç»Ÿç ”ç©¶ï¼Œé¦–å…ˆæå‡ºèåˆå£°å­¦ã€è¯­è¨€å­¦åŠå¿ƒç†å­¦ä¸­æƒ…ç»ªç‰¹å®šçŸ¥è¯†çš„æ–°æç¤ºã€‚æ¥ç€ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºLLMçš„æç¤ºå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸çœŸå®è½¬å½•è¿›è¡Œäº†å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªRevise-Reason-Recognizeæç¤ºç®¡é“ï¼Œç”¨äºä»å¸¦æœ‰ASRé”™è¯¯çš„å£è¯­ä¸­ç¨³å¥åœ°è¯†åˆ«æƒ…ç»ªã€‚å®éªŒç ”ç©¶äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥å­¦ä¹ ã€å³æ—¶å­¦ä¹ å’ŒæŒ‡ä»¤å¾®è°ƒç­‰LLMè®­ç»ƒæ–¹æ¡ˆçš„æœ‰ç”¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMå¯¹å¾®å°æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæƒ…ç»ªç‰¹å®šæç¤ºã€ASRé”™è¯¯æ ¡æ­£å’ŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨åŸºäºLLMçš„æƒ…ç»ªè¯†åˆ«ä¸­æœ‰æ•ˆã€‚æœ¬ç ”ç©¶æ—¨åœ¨å®Œå–„LLMåœ¨æƒ…ç»ªè¯†åˆ«åŠç›¸å…³é¢†åŸŸçš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¯ç”¨äºè¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼Œä½†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§éœ€è¦æ·±å…¥ç ”ç©¶ã€‚</li>
<li>èåˆå£°å­¦ã€è¯­è¨€å­¦å’Œå¿ƒç†å­¦ä¸­çš„æƒ…ç»ªç‰¹å®šçŸ¥è¯†èƒ½æé«˜LLMåœ¨è¯­éŸ³æƒ…ç»ªè¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>åŸºäºLLMçš„æç¤ºå¯¹ASRè½¬å½•çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†ç ”ç©¶ï¼Œå¹¶ä¸çœŸå®è½¬å½•è¿›è¡Œäº†å¯¹æ¯”ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªRevise-Reason-Recognizeæç¤ºç®¡é“ï¼Œç”¨äºä»å¸¦æœ‰ASRé”™è¯¯çš„å£è¯­ä¸­ç¨³å¥åœ°è¯†åˆ«æƒ…ç»ªã€‚</li>
<li>ä¸Šä¸‹æ–‡æ„ŸçŸ¥å­¦ä¹ ã€å³æ—¶å­¦ä¹ å’ŒæŒ‡ä»¤å¾®è°ƒç­‰LLMè®­ç»ƒæ–¹æ¡ˆåœ¨è¯­éŸ³æƒ…ç»ªè¯†åˆ«ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>LLMå¯¹å¾®å°æç¤ºå˜åŒ–å…·æœ‰æ•æ„Ÿæ€§ï¼Œè¿™ä¸ºæé«˜å…¶æ€§èƒ½æä¾›äº†æ–¹å‘ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæƒ…ç»ªç‰¹å®šæç¤ºã€ASRé”™è¯¯æ ¡æ­£å’ŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨åŸºäºLLMçš„æƒ…ç»ªè¯†åˆ«ä¸­æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3db923fa6400d94ce66ccb7ae8bf490c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d503f00707396d52d04caddd5ff6d061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48deef82dc998c76132d96cedd4f98d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85ecc4201dbee8698e9601d69d3d39aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a829a1d06d2b91e03464e663362bb4bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d49d138405a0849ac24441f9d6d25ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d616ea42b2de1f7c3699cbe79557fa.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Can-We-Trust-Embodied-Agents-Exploring-Backdoor-Attacks-against-Embodied-LLM-based-Decision-Making-Systems"><a href="#Can-We-Trust-Embodied-Agents-Exploring-Backdoor-Attacks-against-Embodied-LLM-based-Decision-Making-Systems" class="headerlink" title="Can We Trust Embodied Agents? Exploring Backdoor Attacks against   Embodied LLM-based Decision-Making Systems"></a>Can We Trust Embodied Agents? Exploring Backdoor Attacks against   Embodied LLM-based Decision-Making Systems</h2><p><strong>Authors:Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu</strong></p>
<p>Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®ä½“äººå·¥æ™ºèƒ½çš„å†³ç­–ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¾®è°ƒä»¥åˆ©ç”¨å…¶å›ºæœ‰çš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œé’ˆå¯¹ç‰¹å®šåº”ç”¨è¿›è¡Œå®šåˆ¶æ—¶ã€‚ç„¶è€Œï¼Œè¿™ç§å¾®è°ƒè¿‡ç¨‹å¼•å…¥äº†ç›¸å½“å¤§çš„å®‰å…¨å’Œä¿å¯†æ¼æ´ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®çš„ç½‘ç‰©ç³»ç»Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹å®ä½“äººå·¥æ™ºèƒ½ä¸­åŸºäºLLMçš„å†³ç­–ç³»ç»Ÿçš„é¦–ä¸ªå…¨é¢çš„åé—¨æ”»å‡»æ¡†æ¶ï¼ˆBALDï¼‰ï¼Œç³»ç»Ÿåœ°æ¢ç´¢æ”»å‡»é¢å’Œè§¦å‘æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§ç‹¬ç‰¹çš„æ”»å‡»æœºåˆ¶ï¼šå•è¯æ³¨å…¥ã€åœºæ™¯æ“æ§å’ŒçŸ¥è¯†æ³¨å…¥ï¼Œå®ƒä»¬åˆ†åˆ«é’ˆå¯¹åŸºäºLLMçš„å†³ç­–ç®¡é“çš„ä¸åŒç»„ä»¶ã€‚æˆ‘ä»¬åœ¨è‡ªåŠ¨é©¾é©¶å’Œå®¶åº­æœºå™¨äººä»»åŠ¡ä¸­å¯¹ä»£è¡¨æ€§LLMï¼ˆGPT-3.5ã€LLaMA2ã€PaLM2ï¼‰è¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„åé—¨è§¦å‘åœ¨å¤šç§æ”»å‡»é€šé“ä¸­çš„æœ‰æ•ˆæ€§å’Œéšè”½æ€§ï¼Œæ¡ˆä¾‹åŒ…æ‹¬è½¦è¾†åŠ é€Ÿæ’å‘éšœç¢ç‰©å’Œæœºå™¨äººæŠŠåˆ€æ”¾åœ¨åºŠä¸Šã€‚æˆ‘ä»¬çš„è¯æ±‡å’ŒçŸ¥è¯†æ³¨å…¥æ”»å‡»åœ¨å¤šæ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†è¿‘100%çš„æˆåŠŸç‡ï¼ŒåŒæ—¶åªéœ€è¦å¯¹ç³»ç»Ÿæœ‰é™çš„è®¿é—®æƒé™ã€‚æˆ‘ä»¬çš„åœºæ™¯æ“æ§æ”»å‡»çš„æˆåŠŸç‡è¶…è¿‡65%ï¼Œç”šè‡³è¾¾åˆ°90%ï¼Œè€Œä¸”ä¸éœ€è¦ä»»ä½•è¿è¡Œæ—¶ç³»ç»Ÿå…¥ä¾µã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†è¿™äº›æ”»å‡»å¯¹é˜²å¾¡çš„ç¨³å¥æ€§ï¼Œè¯æ˜äº†å®ƒä»¬çš„éŸ§æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†å®ä½“LLMç³»ç»Ÿä¸­å…³é”®çš„å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†ä¿æŠ¤è¿™äº›ç³»ç»Ÿä»¥å‡è½»æ½œåœ¨é£é™©çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20774v3">PDF</a> Accepted paper at ICLR 2025, 31 pages, including main paper,   references, and appendix</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®ä½“äººå·¥æ™ºèƒ½çš„å†³ç­–ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ç²¾ç»†åŒ–è°ƒæ•´ä»¥åˆ©ç”¨å…¶å†…åœ¨å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œé’ˆå¯¹ç‰¹å®šåº”ç”¨è¿›è¡Œå®šåˆ¶æ—¶ã€‚ç„¶è€Œï¼Œè¿™ç§å¾®è°ƒè¿‡ç¨‹å¼•å…¥äº†å·¨å¤§çš„å®‰å…¨å’Œä¿éšœæ¼æ´ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®æ€§çš„ç½‘ç»œç‰©ç†ç³»ç»Ÿä¸­ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æå‡ºäº†é’ˆå¯¹å®ä½“äººå·¥æ™ºèƒ½ä¸­åŸºäºLLMçš„å†³ç­–ç³»ç»Ÿçš„åé—¨æ”»å‡»å…¨é¢æ¡†æ¶ï¼ˆBALDï¼‰ï¼Œç³»ç»Ÿåœ°æ¢ç´¢æ”»å‡»é¢å’Œè§¦å‘æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºä¸‰ç§ç‹¬ç‰¹çš„æ”»å‡»æœºåˆ¶ï¼šå•è¯æ³¨å…¥ã€åœºæ™¯æ“çºµå’ŒçŸ¥è¯†æ³¨å…¥ï¼Œé’ˆå¯¹LLMå†³ç­–åˆ¶å®šæµç¨‹ä¸­çš„ä¸åŒç»„ä»¶ã€‚åœ¨è‡ªåŠ¨é©¾é©¶å’Œå®¶åº­æœºå™¨äººä»»åŠ¡ä¸­å¯¹ä»£è¡¨æ€§LLMï¼ˆGPT-3.5ã€LLaMA2ã€PaLM2ï¼‰è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åé—¨è§¦å‘åœ¨å¤šç§æ”»å‡»é€šé“ä¸­æœ‰æ•ˆä¸”éšè”½ï¼Œå‡ºç°äº†è½¦è¾†åŠ é€Ÿæ’å‘éšœç¢ç‰©å’Œæœºå™¨äººåœ¨åºŠä¸Šæ”¾ç½®åˆ€å…·ç­‰æ¡ˆä¾‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å®ä½“LLMç³»ç»Ÿä¸­çš„å…³é”®å®‰å…¨æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†ä¿éšœè¿™äº›ç³»ç»Ÿä»¥å‡è½»æ½œåœ¨é£é™©çš„ç´§è¿«éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å®ä½“äººå·¥æ™ºèƒ½çš„å†³ç­–ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å­˜åœ¨å®‰å…¨å’Œä¿éšœæ¼æ´ã€‚</li>
<li>é¦–æ¬¡æå‡ºé’ˆå¯¹LLMå†³ç­–ç³»ç»Ÿçš„åé—¨æ”»å‡»å…¨é¢æ¡†æ¶BALDã€‚</li>
<li>æå‡ºä¸‰ç§æ”»å‡»æœºåˆ¶ï¼šå•è¯æ³¨å…¥ã€åœºæ™¯æ“çºµå’ŒçŸ¥è¯†æ³¨å…¥ï¼Œé’ˆå¯¹LLMå†³ç­–æµç¨‹çš„ä¸åŒéƒ¨åˆ†ã€‚</li>
<li>å®éªŒè¯æ˜æ”»å‡»æœºåˆ¶åœ¨å¤šç§LLMå’Œå®é™…åº”ç”¨åœºæ™¯ä¸­æœ‰æ•ˆä¸”éšè”½ã€‚</li>
<li>æ”»å‡»å¯å¯¼è‡´ä¸¥é‡åæœï¼Œå¦‚è½¦è¾†å†²å‘éšœç¢ç‰©å’Œæœºå™¨äººå±é™©è¡Œä¸ºã€‚</li>
<li>ç°æœ‰é˜²å¾¡æªæ–½å¯¹æ”»å‡»æœºåˆ¶çš„æŠµå¾¡èƒ½åŠ›æœ‰é™ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ä¿éšœLLMç³»ç»Ÿçš„ç´§è¿«éœ€æ±‚ä»¥å‡è½»æ½œåœ¨é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-842f7f6aa0552215c096a1f303f44a48.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90fa48e7117e3fdefa8db34cae8b585d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6941e439ba12306cd7de2a0c04ed4485.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99347939275fb9711341c0808ccb9940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d49293eb3303e13511d4fa6d50668b6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Emergence-of-a-High-Dimensional-Abstraction-Phase-in-Language-Transformers"><a href="#Emergence-of-a-High-Dimensional-Abstraction-Phase-in-Language-Transformers" class="headerlink" title="Emergence of a High-Dimensional Abstraction Phase in Language   Transformers"></a>Emergence of a High-Dimensional Abstraction Phase in Language   Transformers</h2><p><strong>Authors:Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, Marco Baroni</strong></p>
<p>A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ˜¯å°†è¯­è¨€ä¸Šä¸‹æ–‡æ˜ å°„åˆ°è¾“å‡ºæ ‡è®°çš„è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå…³äºè¿™ç§æ˜ å°„è¿˜æœ‰å¾ˆå¤šæœªçŸ¥ï¼ŒåŒ…æ‹¬å…¶å‡ ä½•å±æ€§å¦‚ä½•ä¸å…¶åŠŸèƒ½ç›¸å…³è”ã€‚æˆ‘ä»¬é‡‡ç”¨é«˜çº§å‡ ä½•æ–¹æ³•å¯¹å…¶è¿›è¡Œåˆ†æï¼Œè§‚å¯Ÿåˆ°äº”ä¸ªåŸºäºè½¬æ¢å™¨çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œä¸‰ä¸ªè¾“å…¥æ•°æ®é›†ä¹‹é—´å­˜åœ¨ä¸€ä¸ªä»¥é«˜å†…åœ¨ç»´åº¦ä¸ºç‰¹å¾çš„æ˜æ˜¾é˜¶æ®µã€‚åœ¨è¿™ä¸ªé˜¶æ®µä¸­ï¼Œè¡¨ç¤ºï¼ˆ1ï¼‰å¯¹åº”äºè¾“å…¥çš„ç¬¬ä¸€ä¸ªå®Œæ•´è¯­è¨€æŠ½è±¡ï¼›ï¼ˆ2ï¼‰æ˜¯ç¬¬ä¸€ä¸ªåˆ‡å®è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç¤ºï¼›ï¼ˆ3ï¼‰åœ¨ä¸åŒè¯­è¨€æ¨¡å‹ä¸­ç›¸äº’é¢„æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é˜¶æ®µæå‰å‡ºç°å¼ºçƒˆé¢„ç¤ºç€æ›´å¥½çš„è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ ¸å¿ƒè¯­è¨€å¤„ç†åœ¨è®¸å¤šå¸¸è§çš„LMæ¶æ„ä¸­éƒ½å­˜åœ¨ä¸€ä¸ªä¸­å¤®é«˜ç»´åº¦é˜¶æ®µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15471v4">PDF</a> Published as conference paper at ICLR 2025</p>
<p><strong>Summary</strong>ï¼šè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å°†è¯­è¨€ä¸Šä¸‹æ–‡æ˜ å°„åˆ°è¾“å‡ºæ ‡è®°ä¸Šï¼Œä½†å…¶æ˜ å°„çš„å‡ ä½•ç‰¹æ€§ä¸åŠŸèƒ½ä¹‹é—´çš„å…³ç³»ä»çŸ¥ä¹‹ç”šå°‘ã€‚æœ¬æ–‡é‡‡ç”¨é«˜çº§å‡ ä½•æ–¹æ³•åˆ†æé¢„è®­ç»ƒçš„åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å­˜åœ¨ä¸€ä¸ªä»¥é«˜å†…åœ¨ç»´åº¦ä¸ºç‰¹å¾çš„æ˜æ˜¾é˜¶æ®µã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå¯¹åº”äºè¾“å…¥çš„é¦–ä¸ªå®Œæ•´è¯­è¨€æŠ½è±¡ï¼Œå¯å®é™…åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸åŒè¯­è¨€æ¨¡å‹ä¹‹é—´å½¼æ­¤é¢„æµ‹ã€‚æ—©æœŸè¿›å…¥æ­¤é˜¶æ®µçš„è¯­è¨€æ¨¡å‹é¢„æµ‹æ€§èƒ½æ›´ä½³ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæœ¬æ–‡ç»“æœè¡¨æ˜ï¼Œæ ¸å¿ƒè¯­è¨€å¤„ç†åœ¨è®¸å¤šå¸¸è§è¯­è¨€æ¨¡å‹æ¶æ„ä¸­éƒ½å­˜åœ¨ä»¥é«˜ç»´åº¦ä¸ºä¸­å¿ƒçš„é˜¶æ®µã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹å°†è¯­è¨€ä¸Šä¸‹æ–‡æ˜ å°„åˆ°è¾“å‡ºæ ‡è®°ä¸Šï¼Œä½†å…¶æ˜ å°„çš„å‡ ä½•ç‰¹æ€§ä¸åŠŸèƒ½ä¹‹é—´çš„å…³ç³»ä»ä¸æ¸…æ¥šã€‚</li>
<li>é¢„è®­ç»ƒçš„åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹å­˜åœ¨ä»¥é«˜å†…åœ¨ç»´åº¦ä¸ºç‰¹å¾çš„æ˜æ˜¾é˜¶æ®µã€‚</li>
<li>åœ¨è¿™ä¸€é˜¶æ®µï¼Œè¯­è¨€æ¨¡å‹çš„è¡¨ç¤ºå¯¹åº”äºè¾“å…¥çš„é¦–ä¸ªå®Œæ•´è¯­è¨€æŠ½è±¡ã€‚</li>
<li>è¿™ä¸€é˜¶æ®µçš„è¡¨ç¤ºå¯å®é™…åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶åœ¨ä¸åŒè¯­è¨€æ¨¡å‹ä¹‹é—´å½¼æ­¤é¢„æµ‹ã€‚</li>
<li>æ—©æœŸè¿›å…¥é«˜ç»´åº¦é˜¶æ®µçš„è¯­è¨€æ¨¡å‹å…·æœ‰æ›´å¥½çš„è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚</li>
<li>æ ¸å¿ƒè¯­è¨€å¤„ç†åœ¨è®¸å¤šå¸¸è§è¯­è¨€æ¨¡å‹æ¶æ„ä¸­éƒ½å­˜åœ¨ä»¥é«˜ç»´åº¦ä¸ºä¸­å¿ƒçš„é˜¶æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e3da41369844512d354aec335caa5586.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9156bdb4aa747bdc4743c5b9412a6333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb57232f8edff3469b65bdfc8a1d24a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-502399b049ae062d1b078916628e2c19.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  SWE-smith Scaling Data for Software Engineering Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f3ea181c49e2c7bbe1af35cd0fee256.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  DeepSeek-Prover-V2 Advancing Formal Mathematical Reasoning via   Reinforcement Learning for Subgoal Decomposition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
