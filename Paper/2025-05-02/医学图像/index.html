<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  A simple and effective approach for body part recognition on CT scans   based on projection estimation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d92fc23fea6223950917f3abf2912343.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    66 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-02-æ›´æ–°"><a href="#2025-05-02-æ›´æ–°" class="headerlink" title="2025-05-02 æ›´æ–°"></a>2025-05-02 æ›´æ–°</h1><h2 id="A-simple-and-effective-approach-for-body-part-recognition-on-CT-scans-based-on-projection-estimation"><a href="#A-simple-and-effective-approach-for-body-part-recognition-on-CT-scans-based-on-projection-estimation" class="headerlink" title="A simple and effective approach for body part recognition on CT scans   based on projection estimation"></a>A simple and effective approach for body part recognition on CT scans   based on projection estimation</h2><p><strong>Authors:Franko Hrzic, Mohammadreza Movahhedi, Ophelie Lavoie-Gagne, Ata Kiapour</strong></p>
<p>It is well known that machine learning models require a high amount of annotated data to obtain optimal performance. Labelling Computed Tomography (CT) data can be a particularly challenging task due to its volumetric nature and often missing and$&#x2F;$or incomplete associated meta-data. Even inspecting one CT scan requires additional computer software, or in the case of programming languages $-$ additional programming libraries. This study proposes a simple, yet effective approach based on 2D X-ray-like estimation of 3D CT scans for body region identification. Although body region is commonly associated with the CT scan, it often describes only the focused major body region neglecting other anatomical regions present in the observed CT. In the proposed approach, estimated 2D images were utilized to identify 14 distinct body regions, providing valuable information for constructing a high-quality medical dataset. To evaluate the effectiveness of the proposed method, it was compared against 2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed the others, where it came on top with statistical significance and F1-Score for the best-performing model EffNet-B0 of 0.980 $\pm$ 0.016 in comparison to the 0.840 $\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\pm$ 0.096 (3D VoxCNN), and 0.852 $\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three different clinical centers and counted 15,622 CT scans (44,135 labels). </p>
<blockquote>
<p>ä¼—æ‰€å‘¨çŸ¥ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®æ‰èƒ½è·å¾—æœ€ä½³æ€§èƒ½ã€‚ç”±äºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®çš„ä½“ç§¯æ€§è´¨ä»¥åŠç»å¸¸ç¼ºå¤±å’Œ&#x2F;æˆ–ä¸å®Œæ•´çš„å…³è”å…ƒæ•°æ®ï¼Œå¯¹CTæ•°æ®è¿›è¡Œæ ‡æ³¨å¯èƒ½æ˜¯ä¸€é¡¹ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å³ä½¿æ£€æŸ¥ä¸€æ¬¡CTæ‰«æä¹Ÿéœ€è¦é¢å¤–çš„è®¡ç®—æœºè½¯ä»¶ï¼Œæˆ–è€…åœ¨ç¼–ç¨‹è¯­è¨€çš„æƒ…å†µä¸‹éœ€è¦é¢å¤–çš„ç¼–ç¨‹åº“ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼ŒåŸºäºåŸºäºä½“ä¸‰ç»´CTæ‰«æçš„äºŒç»´Xå°„çº¿å¼ä¼°è®¡è¿›è¡Œèº«ä½“éƒ¨ä½è¯†åˆ«ã€‚è™½ç„¶èº«ä½“éƒ¨ä½é€šå¸¸ä¸CTæ‰«æç›¸å…³è”ï¼Œä½†å®ƒé€šå¸¸ä»…æè¿°å…³æ³¨çš„ä¸»ä½“éƒ¨ä½ï¼Œè€Œå¿½ç•¥äº†è§‚å¯Ÿåˆ°çš„CTä¸­å­˜åœ¨çš„å…¶ä»–è§£å‰–éƒ¨ä½ã€‚åœ¨æå‡ºçš„æ–¹æ³•ä¸­ï¼Œä¼°è®¡çš„äºŒç»´å›¾åƒè¢«ç”¨æ¥è¯†åˆ«å‡ºä¸åŒçš„14ä¸ªèº«ä½“éƒ¨ä½ï¼Œä¸ºæ„å»ºé«˜è´¨é‡åŒ»å­¦æ•°æ®é›†æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ƒä¸åŸºäº2.5Dã€3Då’ŒåŸºç¡€æ¨¡å‹ï¼ˆMI2ï¼‰çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåœ¨ç»Ÿè®¡æ˜¾è‘—æ€§å’Œæœ€ä½³æ€§èƒ½æ¨¡å‹EffNet-B0çš„F1åˆ†æ•°æ–¹é¢ä½å±…æ¦œé¦–ï¼Œè¾¾åˆ°äº†0.980Â±0.016ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¶ä»–æ¨¡å‹åˆ†åˆ«ä¸ºï¼š2.5D DenseNet-161çš„0.840Â±0.114ã€3D VoxCNNçš„0.854Â±0.096ä»¥åŠMI2åŸºç¡€æ¨¡å‹çš„0.852Â±0.104ã€‚æ‰€ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«ä¸‰ä¸ªä¸åŒçš„ä¸´åºŠä¸­å¿ƒï¼Œå…±æœ‰CTæ‰«æå›¾åƒå…±15,622å¼ ï¼ˆæ ‡ç­¾æ•°é‡ä¸º44,135ä¸ªï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21810v1">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e423b5d604bc516294173c53f8243900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d40c4dbd89ff794bec7d47b2533378f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b6d057593dadef9fc9547db869e684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44719e232f19b1da0bf2bbeccb39398b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Anomaly-Driven-Approach-for-Enhanced-Prostate-Cancer-Segmentation"><a href="#Anomaly-Driven-Approach-for-Enhanced-Prostate-Cancer-Segmentation" class="headerlink" title="Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation"></a>Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation</h2><p><strong>Authors:Alessia Hu, Regina Beets-Tan, Lishan Cai, Eduardo Pooch</strong></p>
<p>Magnetic Resonance Imaging (MRI) plays an important role in identifying clinically significant prostate cancer (csPCa), yet automated methods face challenges such as data imbalance, variable tumor sizes, and a lack of annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which incorporates anomaly maps derived from biparametric MRI sequences into a deep learning-based segmentation framework to improve csPCa identification. We conduct a comparative analysis of anomaly detection methods and evaluate the integration of anomaly maps into the segmentation pipeline. Anomaly maps, generated using Fixed-Point GAN reconstruction, highlight deviations from normal prostate tissue, guiding the segmentation model to potential cancerous regions. We compare the performance by using the average score, computed as the mean of the AUROC and Average Precision (AP). On the external test set, adU-Net achieves the best average score of 0.618, outperforming the baseline nnU-Net model (0.605). The results demonstrate that incorporating anomaly detection into segmentation improves generalization and performance, particularly with ADC-based anomaly maps, offering a promising direction for automated csPCa identification. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è¯†åˆ«å…·æœ‰é‡è¦ä¸´åºŠæ„ä¹‰çš„å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œè‡ªåŠ¨åŒ–æ–¹æ³•é¢ä¸´æ•°æ®ä¸å¹³è¡¡ã€è‚¿ç˜¤å¤§å°å¯å˜ä»¥åŠç¼ºä¹æ³¨é‡Šæ•°æ®ç­‰æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†Anomaly-Driven U-Netï¼ˆadU-Netï¼‰ï¼Œå®ƒå°†æ¥è‡ªåŒå‚æ•°MRIåºåˆ—çš„å¼‚å¸¸æ˜ å°„é›†æˆåˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æ¡†æ¶ä¸­ï¼Œä»¥æé«˜csPCaçš„è¯†åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å¼‚å¸¸æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œå¹¶è¯„ä¼°äº†å¼‚å¸¸æ˜ å°„é›†æˆåˆ°åˆ†å‰²ç®¡é“ä¸­çš„æ•ˆæœã€‚ä½¿ç”¨Fixed-Point GANé‡å»ºæŠ€æœ¯ç”Ÿæˆçš„å¼‚å¸¸æ˜ å°„çªå‡ºäº†å‰åˆ—è…ºç»„ç»‡ä¸­çš„å¼‚å¸¸åç¦»ï¼Œå¼•å¯¼åˆ†å‰²æ¨¡å‹å®šä½æ½œåœ¨ç™Œå˜åŒºåŸŸã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¹³å‡å¾—åˆ†ï¼ˆè®¡ç®—ä¸ºAUROCå’Œå¹³å‡ç²¾åº¦ï¼ˆAPï¼‰çš„å¹³å‡å€¼ï¼‰æ¥æ¯”è¾ƒæ€§èƒ½ã€‚åœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šï¼ŒadU-Netå–å¾—äº†æœ€ä½³å¹³å‡å¾—åˆ†0.618ï¼Œä¼˜äºåŸºçº¿nnU-Netæ¨¡å‹ï¼ˆ0.605ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œå°†å¼‚å¸¸æ£€æµ‹çº³å…¥åˆ†å‰²å¯ä»¥æé«˜æ³›åŒ–å’Œæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åŸºäºADCçš„å¼‚å¸¸æ˜ å°„ï¼Œä¸ºè‡ªåŠ¨csPCaè¯†åˆ«æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21789v1">PDF</a> Paper accepted for publication at 2025 47th Annual International   Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)   Copyright 2025 IEEE. Personal use of this material is permitted. Permission   from IEEE must be obtained for all other uses, in any current or future media</p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ çš„U-Netæ¨¡å‹ç»“åˆåŒå‚æ•°MRIåºåˆ—çš„å¼‚å¸¸å›¾ï¼Œç”¨äºæé«˜ä¸´åºŠæ˜¾è‘—å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰çš„è¯†åˆ«ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå¹¶å°†å¼‚å¸¸å›¾èå…¥åˆ†å‰²æµç¨‹ä¸­ï¼Œæé«˜äº†æ¨¡å‹è¯†åˆ«å‰åˆ—è…ºåŒºåŸŸå†…ç™Œç—‡ç—…ç¶çš„æ€§èƒ½ã€‚æ¨¡å‹æ€§èƒ½ä¼˜å¼‚ï¼Œä¸ºæœªæ¥è‡ªåŠ¨åŒ–è¯†åˆ«csPCaæä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIåœ¨è¯†åˆ«ä¸´åºŠæ˜¾è‘—å‰åˆ—è…ºç™Œï¼ˆcsPCaï¼‰ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ–¹æ³•é¢ä¸´æ•°æ®ä¸å¹³è¡¡ã€è‚¿ç˜¤å¤§å°å˜åŒ–å’Œç¼ºä¹æ ‡æ³¨æ•°æ®ç­‰æŒ‘æˆ˜ã€‚</li>
<li>Anomaly-Driven U-Netï¼ˆadU-Netï¼‰æ¨¡å‹ç»“åˆäº†åŒå‚æ•°MRIåºåˆ—çš„å¼‚å¸¸å›¾æ¥æé«˜csPCaè¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨Fixed-Point GANé‡å»ºç”Ÿæˆå¼‚å¸¸å›¾ï¼Œçªå‡ºæ­£å¸¸å‰åˆ—è…ºç»„ç»‡ä¸­çš„åå·®ï¼ŒæŒ‡å¯¼åˆ†å‰²æ¨¡å‹æ‰¾åˆ°æ½œåœ¨çš„ç™Œç—‡åŒºåŸŸã€‚</li>
<li>é€šè¿‡å¹³å‡å¾—åˆ†ï¼ˆå¹³å‡AUROCå’Œå¹³å‡ç²¾åº¦ï¼‰è¯„ä¼°æ€§èƒ½ï¼ŒadU-Netåœ¨å¤–éƒ¨æµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€ä½³å¹³å‡å¾—åˆ†0.618ã€‚</li>
<li>ä¸åŸºçº¿nnU-Netæ¨¡å‹ç›¸æ¯”ï¼ŒadU-Netçš„æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œå®ç°äº†æ›´é«˜çš„å¹³å‡å¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11f42baf3d547344404f8cd01d53e008.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-205d539b88716acdee71080f3b5770ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d92fc23fea6223950917f3abf2912343.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e360c2c6a6467346349c1850d5f54a41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9948efb28a7312d4d1aefffabe8da48e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2142b52f30123078b656b5c07c565a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5d207aaaf3e0020b152323d295d67ba.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Anatomical-Similarity-as-a-New-Metric-to-Evaluate-Brain-Generative-Models"><a href="#Anatomical-Similarity-as-a-New-Metric-to-Evaluate-Brain-Generative-Models" class="headerlink" title="Anatomical Similarity as a New Metric to Evaluate Brain Generative   Models"></a>Anatomical Similarity as a New Metric to Evaluate Brain Generative   Models</h2><p><strong>Authors:Bahram Jafrasteh, Wei Peng, Cheng Wan, Yimin Luo, Ehsan Adeli, Qingyu Zhao</strong></p>
<p>Generative models enhance neuroimaging through data augmentation, quality improvement, and rare condition studies. Despite advances in realistic synthetic MRIs, evaluations focus on texture and perception, lacking sensitivity to crucial anatomical fidelity. This study proposes a new metric, called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the anatomical realism of synthetic brain MRIs. WASABI leverages \textit{SynthSeg}, a deep learning-based brain parcellation tool, to derive volumetric measures of brain regions in each MRI and uses the multivariate Wasserstein distance to compare distributions between real and synthetic anatomies. Based on controlled experiments on two real datasets and synthetic MRIs from five generative models, WASABI demonstrates higher sensitivity in quantifying anatomical discrepancies compared to traditional image-level metrics, even when synthetic images achieve near-perfect visual quality. Our findings advocate for shifting the evaluation paradigm beyond visual inspection and conventional metrics, emphasizing anatomical fidelity as a crucial benchmark for clinically meaningful brain MRI synthesis. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BahramJafrasteh/wasabi-mri">https://github.com/BahramJafrasteh/wasabi-mri</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹é€šè¿‡æ•°æ®å¢å¼ºã€è´¨é‡æ”¹è¿›å’Œç½•è§ç–¾ç—…ç ”ç©¶å¢å¼ºäº†ç¥ç»æˆåƒã€‚å°½ç®¡åœ¨é€¼çœŸçš„åˆæˆMRIæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨çº¹ç†å’Œæ„ŸçŸ¥ä¸Šï¼Œç¼ºä¹å¯¹å…³é”®è§£å‰–é€¼çœŸåº¦çš„æ•æ„Ÿæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ–¹æ³•ï¼Œç§°ä¸ºWASABIï¼ˆåŸºäºWassersteinçš„è§£å‰–è„‘æŒ‡æ•°ï¼‰ï¼Œä»¥è¯„ä¼°åˆæˆè„‘MRIçš„è§£å‰–é€¼çœŸåº¦ã€‚WASABIåˆ©ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„è„‘åˆ†å‰²å·¥å…·\textit{SynthSeg}ï¼Œä»æ¯ä¸ªMRIä¸­å¯¼å‡ºè„‘åŒºçš„ä½“ç§¯æµ‹é‡å€¼ï¼Œå¹¶ä½¿ç”¨å¤šå…ƒWassersteinè·ç¦»æ¯”è¾ƒçœŸå®å’Œåˆæˆè§£å‰–ä¹‹é—´çš„åˆ†å¸ƒã€‚åŸºäºä¸¤ä¸ªçœŸå®æ•°æ®é›†å’Œäº”ä¸ªç”Ÿæˆæ¨¡å‹åˆæˆçš„MRIè¿›è¡Œçš„æ§åˆ¶å®éªŒè¡¨æ˜ï¼ŒWASABIåœ¨é‡åŒ–è§£å‰–å·®å¼‚æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„æ•æ„Ÿæ€§ï¼Œå³ä½¿åˆæˆå›¾åƒè¾¾åˆ°è¿‘ä¹å®Œç¾çš„è§†è§‰è´¨é‡äº¦æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œéœ€è¦è¶…è¶Šè§†è§‰æ£€æŸ¥å’Œä¼ ç»ŸæŒ‡æ ‡çš„è¯„ä¼°æ¨¡å¼ï¼Œå¼ºè°ƒè§£å‰–é€¼çœŸåº¦ä½œä¸ºä¸´åºŠä¸Šå…·æœ‰æ„ä¹‰çš„è„‘MRIåˆæˆçš„å…³é”®åŸºå‡†ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BahramJafrasteh/wasabi-mri%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BahramJafrasteh/wasabi-mriæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21771v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆæ¨¡å‹é€šè¿‡æ•°æ®å¢å¼ºã€è´¨é‡æå‡å’Œç½•è§ç–¾ç—…ç ”ç©¶å¢å¼ºäº†ç¥ç»å½±åƒæŠ€æœ¯ã€‚ä¸ºè¯„ä¼°åˆæˆè„‘MRIçš„è§£å‰–çœŸå®æ€§ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°çš„æŒ‡æ ‡WASABIï¼ˆåŸºäºWassersteinçš„è„‘è§£å‰–æŒ‡æ•°ï¼‰ï¼Œå¹¶ç»“åˆæ·±åº¦å­¦ä¹ è„‘åˆ†å‰²å·¥å…·SynthSegå’Œå¤šå…ƒWassersteinè·ç¦»è¡¡é‡çœŸå®ä¸åˆæˆMRIè§£å‰–åˆ†å¸ƒçš„å·®å¼‚ã€‚å®éªŒè¯æ˜ï¼ŒWASABIç›¸è¾ƒäºä¼ ç»Ÿå›¾åƒçº§æŒ‡æ ‡ï¼Œåœ¨é‡åŒ–è§£å‰–å·®å¼‚æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„çµæ•åº¦ï¼Œå³ä½¿åˆæˆå›¾åƒå‡ ä¹è¾¾åˆ°å®Œç¾è§†è§‰è´¨é‡ã€‚å¼ºè°ƒè§£å‰–çœŸå®æ€§æ˜¯ä¸´åºŠæœ‰æ„ä¹‰çš„è„‘MRIåˆæˆçš„å…³é”®æŒ‡æ ‡ï¼Œå¹¶æå€¡è¯„ä¼°èŒƒå¼ä»è§†è§‰æ£€æŸ¥å’Œä¼ ç»ŸæŒ‡æ ‡è½¬å‘æ­¤å…³é”®è¦ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹åœ¨ç¥ç»å½±åƒä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œé€šè¿‡æ•°æ®å¢å¼ºã€è´¨é‡æå‡å’Œç½•è§ç–¾ç—…ç ”ç©¶å¸¦æ¥è¿›æ­¥ã€‚</li>
<li>è¯„ä¼°åˆæˆè„‘MRIçš„çœŸå®æ€§æ—¶ï¼Œç°æœ‰æŒ‡æ ‡ä¸»è¦å…³æ³¨çº¹ç†å’Œæ„ŸçŸ¥ï¼Œç¼ºä¹å¯¹è§£å‰–çœŸå®æ€§çš„æ•æ„Ÿã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºæ–°çš„è¯„ä¼°æŒ‡æ ‡WASABIï¼Œç»“åˆæ·±åº¦å­¦ä¹ è„‘åˆ†å‰²å·¥å…·å’Œå¤šå…ƒWassersteinè·ç¦»æ¥è¡¡é‡çœŸå®ä¸åˆæˆMRIä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>WASABIåœ¨é‡åŒ–è§£å‰–å·®å¼‚æ–¹é¢è¡¨ç°å‡ºé«˜çµæ•åº¦ï¼Œå³ä½¿åˆæˆå›¾åƒè§†è§‰è´¨é‡è¿‘ä¹å®Œç¾ã€‚</li>
<li>å¼ºè°ƒè§£å‰–çœŸå®æ€§æ˜¯è¯„ä»·åˆæˆè„‘MRIä¸´åºŠæ„ä¹‰çš„å…³é”®æŒ‡æ ‡ã€‚</li>
<li>æå€¡æ”¹å˜è¯„ä¼°èŒƒå¼ï¼Œä»è§†è§‰æ£€æŸ¥å’Œä¼ ç»ŸæŒ‡æ ‡è½¬å‘è§£å‰–çœŸå®æ€§è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c62a20fc7f998f5989fced90bb23e35e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30fa356570de47ced4909a80127b0d9d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Visual-Text-Processing-A-Comprehensive-Review-and-Unified-Evaluation"><a href="#Visual-Text-Processing-A-Comprehensive-Review-and-Unified-Evaluation" class="headerlink" title="Visual Text Processing: A Comprehensive Review and Unified Evaluation"></a>Visual Text Processing: A Comprehensive Review and Unified Evaluation</h2><p><strong>Authors:Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe</strong></p>
<p>Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at <a target="_blank" rel="noopener" href="https://github.com/shuyansy/Visual-Text-Processing-survey">https://github.com/shuyansy/Visual-Text-Processing-survey</a>. </p>
<blockquote>
<p>è§†è§‰æ–‡æœ¬åœ¨æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­éƒ½æ˜¯è‡³å…³é‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä¼ é€’äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å¼•èµ·äº†è®¡ç®—æœºè§†è§‰ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚é™¤äº†æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ç­‰ä¼ ç»Ÿä»»åŠ¡å¤–ï¼Œè§†è§‰æ–‡æœ¬å¤„ç†åœ¨åŸºç¡€æ¨¡å‹çš„æ¨åŠ¨ä¸‹å®ç°äº†å¿«é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ–‡æœ¬å›¾åƒæ“ä½œã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”±äºæ–‡æœ¬ä¸ä¸€èˆ¬ç‰©ä½“çš„ç‹¬ç‰¹å±æ€§å·®å¼‚ï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ‰æ•ˆåœ°æ•è·å’Œåˆ©ç”¨è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾å¯¹äºå¼€å‘ç¨³å¥çš„è§†è§‰æ–‡æœ¬å¤„ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è§†è§‰æ–‡æœ¬å¤„ç†çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢ã€å¤šè§†è§’çš„åˆ†æï¼Œé‡ç‚¹å›ç­”ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰ä¸åŒçš„è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡æœ€é€‚åˆå“ªäº›æ–‡æœ¬ç‰¹å¾ï¼Ÿï¼ˆ2ï¼‰å¦‚ä½•æœ‰æ•ˆåœ°å°†è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾çº³å…¥å¤„ç†æ¡†æ¶ï¼Ÿæ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†VTPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¹¿æ³›è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†çš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºäº†VTPScoreï¼Œä¸€ç§æ—¨åœ¨ç¡®ä¿å…¬å¹³å¯é è¯„ä¼°çš„æ–°å‹è¯„ä»·æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯¹è¶…è¿‡20ç§ç‰¹å®šæ¨¡å‹çš„å®è¯ç ”ç©¶æ­ç¤ºäº†å½“å‰æŠ€æœ¯ä»æœ‰å¤§é‡æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿™é¡¹å·¥ä½œå»ºç«‹ä¸ºè§†è§‰æ–‡æœ¬å¤„ç†è¿™ä¸€åŠ¨æ€é¢†åŸŸçš„åŸºæœ¬èµ„æºï¼Œä¿ƒè¿›æœªæ¥çš„æ¢ç´¢å’Œåˆ›æ–°ã€‚ç›¸å…³ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shuyansy/Visual-Text-Processing-survey">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21682v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¦‚è¿°äº†è§†è§‰æ–‡æœ¬å¤„ç†é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä»‹ç»äº†å¤„ç†æ–‡æœ¬å›¾åƒçš„é‡è¦ç‰¹æ€§ä¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç‰¹å¾çš„é€‰æ‹©ä¸èå…¥å¤„ç†æ¡†æ¶çš„æ–¹å¼ã€‚æå‡ºVTPBenchæ–°åŸºå‡†ï¼Œæ¶µç›–å¹¿æ³›çš„è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†ï¼Œå¹¶åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºVTPScoreè¯„ä¼°æŒ‡æ ‡ï¼Œæ—¨åœ¨ç¡®ä¿å…¬å¹³å¯é çš„è¯„ä¼°ã€‚å¯¹è¶…è¿‡20ç§ç‰¹å®šæ¨¡å‹çš„å®è¯ç ”ç©¶æ˜¾ç¤ºå‡ºå½“å‰æŠ€æœ¯ä»æœ‰å¤§é‡æå‡ç©ºé—´ï¼Œæ—¨åœ¨ä¸ºè§†è§‰æ–‡æœ¬å¤„ç†é¢†åŸŸçš„æœªæ¥æ¢ç´¢å’Œåˆ›æ–°æä¾›åŸºç¡€èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ–‡æœ¬æ˜¯æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼ŒåŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¤‡å—å…³æ³¨ã€‚</li>
<li>è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸ä»…é™äºä¼ ç»Ÿçš„æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ï¼Œè¿˜åŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ“çºµç­‰ã€‚</li>
<li>è§†è§‰æ–‡æœ¬å¤„ç†é¢†åŸŸè™½å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ï¼Œæœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨æ–‡æœ¬çš„ç‹¬ç‰¹ç‰¹æ€§å¯¹äºå¼€å‘ç¨³å¥çš„æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>æ–‡ä¸­å¯¹è§†è§‰æ–‡æœ¬å¤„ç†çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢ã€å¤šè§†è§’çš„åˆ†æï¼Œå¹¶å›ç­”äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå“ªäº›æ–‡æœ¬ç‰¹å¾æœ€é€‚åˆä¸åŒçš„è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼Ÿå¦‚ä½•æœ‰æ•ˆåœ°å°†è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾èå…¥å¤„ç†æ¡†æ¶ï¼Ÿ</li>
<li>ä»‹ç»äº†æ–°çš„VTPBenchåŸºå‡†ï¼Œæ¶µç›–å¹¿æ³›çš„è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†ï¼Œä¸ºç ”ç©¶å’Œè¯„ä¼°æä¾›ä¸°å¯Œèµ„æºã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºäº†VTPScoreè¯„ä¼°æŒ‡æ ‡ï¼Œç¡®ä¿å…¬å¹³å’Œå¯é çš„è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ba187bed82e500dc736c5cf2edc4d3df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8657c786a8dae4ecafb7370ee476921.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f62cf1e080e5998dc17731b1eef14ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f83f4345ad4e0b8d493335a44549503.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ce6b361c66b4d039193ab134a883dce.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Broadband-study-of-the-SMC-pulsar-RX-J0032-9-7348-during-its-X-ray-brightening-in-2024"><a href="#Broadband-study-of-the-SMC-pulsar-RX-J0032-9-7348-during-its-X-ray-brightening-in-2024" class="headerlink" title="Broadband study of the SMC pulsar RX J0032.9-7348 during its X-ray   brightening in 2024"></a>Broadband study of the SMC pulsar RX J0032.9-7348 during its X-ray   brightening in 2024</h2><p><strong>Authors:Birendra Chhotaray, Gaurava K. Jaisawal, Sachindra Naik, Arghajit Jana</strong></p>
<p>We present the results of the broadband timing and spectral analysis of the poorly understood SMC pulsar RX J0032.9-7348 (&#x3D; SXP 7.02) using NuSTAR and NICER observations during its X-ray brightening in 2024. Our timing analysis revealed a pulsation period of approximately 7.02 s in the X-ray light curve. The pulse profile obtained in the broad energy range is double-peaked and asymmetric in nature and shows moderate variation with the energy. An absorbed power-law model describes the 0.5-8 keV NICER spectra well. The 3-50 keV NuSTAR spectrum is best described with an absorbed power-law modified with a high-energy cutoff model. We find no evidence of iron or cyclotron line features in the energy spectrum. During our observation period, the 0.5-50 keV luminosity varies in the range of $\sim 8\times10^{36} - 4\times10^{37}$ erg s$^{-1}$. We also discuss the dependence of spectral parameters on the rotational phase of the pulsar through phase-resolved spectroscopy. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†åœ¨Xå°„çº¿å¢äº®æœŸï¼ˆå‘ç”Ÿåœ¨2024å¹´ï¼‰ä½¿ç”¨NuSTARå’ŒNICERè§‚æµ‹ç»“æœå¯¹SMCè„‰å†²æ˜ŸRX J0032.9-7348ï¼ˆå³SXP 7.02ï¼‰çš„å®½å¸¦è®¡æ—¶å’Œå…‰è°±åˆ†æçš„ç»“æœã€‚æˆ‘ä»¬çš„è®¡æ—¶åˆ†æå‘ç°Xå°„çº¿å…‰å˜æ›²çº¿ä¸­çš„è„‰å†²å‘¨æœŸçº¦ä¸º7.02ç§’ã€‚åœ¨å®½èƒ½èŒƒå›´å†…è·å¾—çš„è„‰å†²è½®å»“å…·æœ‰åŒå³°å’Œä¸å¯¹ç§°æ€§è´¨ï¼Œå¹¶éšèƒ½é‡çš„å˜åŒ–è€Œé€‚åº¦å˜åŒ–ã€‚å¸æ”¶å¹‚å¾‹æ¨¡å‹å¾ˆå¥½åœ°æè¿°äº†NICERåœ¨0.5-8åƒç”µå­ä¼ç‰¹èŒƒå›´å†…çš„å…‰è°±ã€‚NuSTARåœ¨3-50åƒç”µå­ä¼ç‰¹èŒƒå›´å†…çš„å…‰è°±æœ€å¥½ç”¨å¸æ”¶å¹‚å¾‹æ¨¡å‹æ¥æè¿°ï¼Œè¯¥æ¨¡å‹é€šè¿‡é«˜èƒ½æˆªæ­¢æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚æˆ‘ä»¬åœ¨èƒ½é‡è°±ä¸­æ²¡æœ‰å‘ç°é“æˆ–å›æ—‹çº¿ç‰¹å¾ã€‚åœ¨æˆ‘ä»¬çš„è§‚æµ‹æœŸé—´ï¼Œè§‚å¯Ÿåˆ°åœ¨å¤§çº¦èŒƒå›´ä»$ 8\times10^{36}è‡³ 4\times10^{37}$å°”æ ¼æ¯ç§’çš„è¾å°„å…‰åº¦ï¼ˆä»ç´«å¤–åˆ°è¿‘çº¢å¤–ï¼‰ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ç›¸ä½è§£æå…‰è°±è®¨è®ºäº†è°±å‚æ•°å¯¹è„‰å†²æ˜Ÿè‡ªè½¬å‘¨æœŸçš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21671v1">PDF</a> Accepted for publication in MNRAS</p>
<p><strong>Summary</strong><br>    RX J0032.9-7348è„‰å†²æ˜Ÿçš„å®½é¢‘æ—¶åºå’Œå…‰è°±åˆ†æç»“æœã€‚å‘ç°7.02ç§’è„‰å†²å‘¨æœŸï¼ŒXå°„çº¿å…‰å˜æ›²çº¿å‘ˆåŒå³°ã€ä¸å¯¹ç§°ç‰¹å¾ï¼Œèƒ½è°±ç‰¹å¾ä»¥å¸æ”¶æ€§å¹‚å¾‹æ¨¡å‹ä¸ºä¸»ï¼Œæ— æ˜æ˜¾çš„é“æˆ–å›æ—‹çº¿ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RX J0032.9-7348è„‰å†²æ˜Ÿçš„è„‰å†²å‘¨æœŸçº¦ä¸º7.02ç§’ã€‚</li>
<li>Xå°„çº¿å…‰å˜æ›²çº¿å‘ˆç°åŒå³°ã€ä¸å¯¹ç§°çš„è„‰å†²ç‰¹å¾ã€‚</li>
<li>èƒ½è°±ç‰¹å¾åœ¨0.5-8 keVèŒƒå›´å†…å¯ç”¨å¸æ”¶æ€§å¹‚å¾‹æ¨¡å‹æè¿°ã€‚</li>
<li>åœ¨3-50 keVçš„NuSTARå…‰è°±æ•°æ®éœ€è¦é‡‡ç”¨å¸¦æœ‰é«˜èƒ½æˆªæ­¢çš„æ¨¡å‹è¿›è¡Œæè¿°ã€‚</li>
<li>æ— æ˜æ˜¾çš„é“æˆ–å›æ—‹çº¿ç‰¹å¾åœ¨èƒ½è°±ä¸­è¢«å‘ç°ã€‚</li>
<li>è„‰å†²æ˜Ÿçš„0.5-50 keVå…‰åº¦åœ¨è§‚æµ‹æœŸé—´å†…å˜åŒ–èŒƒå›´çº¦ä¸º (8\times10^{36} - 4\times10^{37}) erg s(^{-1})ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9d2d6fd1f36a92679610c0c17e55ddd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5edaa8acbff4fbc7837f365b3491db87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23fa4751f9c1dec99e7d3246de618bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1d20d92c2cdf1a62bd357ca7b121769.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a46d597e87bbf064e51b28a488534a38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0bd15e0dcb0bbbe3de56ce62145f4d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50dcf0005e91a1a459a7f9167e35f69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6aa07883082eb18884ca52508e405d5e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Revisiting-Diffusion-Autoencoder-Training-for-Image-Reconstruction-Quality"><a href="#Revisiting-Diffusion-Autoencoder-Training-for-Image-Reconstruction-Quality" class="headerlink" title="Revisiting Diffusion Autoencoder Training for Image Reconstruction   Quality"></a>Revisiting Diffusion Autoencoder Training for Image Reconstruction   Quality</h2><p><strong>Authors:Pramook Khungurn, Sukit Seripanitkarn, Phonphrm Thawatdamrongkit, Supasorn Suwajanakorn</strong></p>
<p>Diffusion autoencoders (DAEs) are typically formulated as a noise prediction model and trained with a linear-$\beta$ noise schedule that spends much of its sampling steps at high noise levels. Because high noise levels are associated with recovering large-scale image structures and low noise levels with recovering details, this configuration can result in low-quality and blurry images. However, it should be possible to improve details while spending fewer steps recovering structures because the latent code should already contain structural information. Based on this insight, we propose a new DAE training method that improves the quality of reconstructed images. We divide training into two phases. In the first phase, the DAE is trained as a vanilla autoencoder by always setting the noise level to the highest, forcing the encoder and decoder to populate the latent code with structural information. In the second phase, we incorporate a noise schedule that spends more time in the low-noise region, allowing the DAE to learn how to perfect the details. Our method results in images that have accurate high-level structures and low-level details while still preserving useful properties of the latent codes. </p>
<blockquote>
<p>æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEsï¼‰é€šå¸¸è¢«åˆ¶å®šä¸ºå™ªå£°é¢„æµ‹æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨çº¿æ€§-$\beta$å™ªå£°æ—¶é—´è¡¨è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡å‹åœ¨é‡‡æ ·æ­¥éª¤ä¸­å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨é«˜å™ªå£°æ°´å¹³ä¸‹è¿è¡Œã€‚ç”±äºé«˜å™ªå£°æ°´å¹³ä¸æ¢å¤å¤§è§„æ¨¡å›¾åƒç»“æ„æœ‰å…³ï¼Œè€Œä½å™ªå£°æ°´å¹³ä¸æ¢å¤ç»†èŠ‚æœ‰å…³ï¼Œè¿™ç§é…ç½®å¯èƒ½ä¼šå¯¼è‡´å›¾åƒè´¨é‡ä½ä¸‹å’Œæ¨¡ç³Šã€‚ç„¶è€Œï¼Œåº”è¯¥æœ‰å¯èƒ½åœ¨æ¢å¤ç»“æ„æ—¶å‡å°‘æ­¥éª¤æ•°é‡ï¼ŒåŒæ—¶æ”¹è¿›ç»†èŠ‚ï¼Œå› ä¸ºæ½œåœ¨ä»£ç åº”è¯¥å·²ç»åŒ…å«ç»“æ„ä¿¡æ¯ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„DAEè®­ç»ƒæ–¹æ³•ï¼Œä»¥æé«˜é‡å»ºå›¾åƒçš„è´¨é‡ã€‚æˆ‘ä»¬å°†è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å°†DAEè®­ç»ƒä¸ºæ™®é€šè‡ªç¼–ç å™¨ï¼Œå§‹ç»ˆå°†å™ªå£°æ°´å¹³è®¾ç½®ä¸ºæœ€é«˜ï¼Œè¿«ä½¿ç¼–ç å™¨å’Œè§£ç å™¨åœ¨æ½œåœ¨ä»£ç ä¸­å¡«å……ç»“æ„ä¿¡æ¯ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§åœ¨ä½å£°åŒºèŠ±è´¹æ›´å¤šæ—¶é—´çš„å™ªå£°æ—¶é—´è¡¨ï¼Œè®©DAEå­¦ä¹ å¦‚ä½•å®Œå–„ç»†èŠ‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„å›¾åƒæ—¢å…·æœ‰å‡†ç¡®çš„é«˜çº§ç»“æ„ï¼Œåˆå…·æœ‰ä½çº§çš„ç»†èŠ‚ï¼ŒåŒæ—¶è¿˜ä¿ç•™äº†æ½œåœ¨ä»£ç çš„æœ‰ç”¨å±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21368v1">PDF</a> AI for Content Creation (AI4CC) Workshop at CVPR 2025</p>
<p><strong>Summary</strong><br>     æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEsï¼‰ä½œä¸ºå™ªå£°é¢„æµ‹æ¨¡å‹ï¼Œé€šå¸¸é‡‡ç”¨çº¿æ€§-$\beta$å™ªå£°è°ƒåº¦è¿›è¡Œè®­ç»ƒï¼Œå…¶é‡‡æ ·æ­¥éª¤å¤šæ•°é›†ä¸­åœ¨é«˜å™ªå£°æ°´å¹³ã€‚è¿™å¯¼è‡´åœ¨æ¢å¤å›¾åƒç»“æ„æ—¶è´¨é‡è¾ƒä½ä¸”å›¾åƒæ¨¡ç³Šã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„DAEè®­ç»ƒæ–¹æ³•ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µä»¥æé«˜é‡å»ºå›¾åƒçš„è´¨é‡ã€‚ç¬¬ä¸€é˜¶æ®µè®­ç»ƒDAEä½œä¸ºæ™®é€šè‡ªç¼–ç å™¨ï¼Œå§‹ç»ˆè®¾ç½®æœ€é«˜å™ªå£°æ°´å¹³ï¼Œä½¿ç¼–ç å™¨å’Œè§£ç å™¨åœ¨æ½œåœ¨ä»£ç ä¸­å¡«å……ç»“æ„ä¿¡æ¯ã€‚ç¬¬äºŒé˜¶æ®µå¼•å…¥å™ªå£°è°ƒåº¦ï¼Œæ›´å¤šåœ°å…³æ³¨ä½å™ªå£°åŒºåŸŸï¼Œä½¿DAEå­¦ä¹ å®Œå–„ç»†èŠ‚ã€‚è¯¥æ–¹æ³•ç”Ÿæˆçš„å›¾åƒæ—¢å…·æœ‰å‡†ç¡®çš„é«˜çº§ç»“æ„ï¼Œåˆä¿ç•™äº†æ½œåœ¨ä»£ç çš„æœ‰ç”¨å±æ€§ï¼ŒåŒæ—¶ç»†èŠ‚ä¸°å¯Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEsï¼‰åœ¨å™ªå£°é¢„æµ‹æ¨¡å‹ä¸­é€šå¸¸é‡‡ç”¨çº¿æ€§-$\beta$å™ªå£°è°ƒåº¦è®­ç»ƒï¼Œé‡‡æ ·æ­¥éª¤å¤šæ•°é›†ä¸­åœ¨é«˜å™ªå£°æ°´å¹³ï¼Œå¯¼è‡´å›¾åƒè´¨é‡ä½ä¸”æ¨¡ç³Šã€‚</li>
<li>é«˜å™ªå£°æ°´å¹³å…³è”æ¢å¤å¤§å°ºåº¦å›¾åƒç»“æ„ï¼Œä½å™ªå£°æ°´å¹³å…³è”æ¢å¤ç»†èŠ‚ã€‚</li>
<li>æå‡ºçš„æ–°è®­ç»ƒæ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µé‡ç‚¹è®­ç»ƒDAEä½œä¸ºæ™®é€šè‡ªç¼–ç å™¨ï¼Œä½¿ç¼–ç å™¨å’Œè§£ç å™¨åœ¨æ½œåœ¨ä»£ç ä¸­å¡«å……ç»“æ„ä¿¡æ¯ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µå¼•å…¥çš„å™ªå£°è°ƒåº¦æ›´å¤šå…³æ³¨ä½å™ªå£°åŒºåŸŸï¼Œæé«˜ç»†èŠ‚è´¨é‡ã€‚</li>
<li>é€šè¿‡æ–°è®­ç»ƒæ–¹æ³•ï¼Œç”Ÿæˆçš„å›¾åƒå…·æœ‰å‡†ç¡®çš„é«˜çº§ç»“æ„å’Œä¸°å¯Œçš„ç»†èŠ‚ã€‚</li>
<li>æ–°æ–¹æ³•ä¿ç•™æ½œåœ¨ä»£ç çš„æœ‰ç”¨å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba86ea9628caedbe4d9af973f8de7413.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80b7fac8d8d1b52bff977e3f7a55bd73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8031aab809acb732a060f184d09ed4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-793b16a9a1019eb5f2b82257533e71ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45705d99638f9aca19d5e1d80b3e8fc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Model-Based-Semantic-Guided-Imaging-Biomarker-for-Early-Lung-Cancer-Detection"><a href="#Vision-Language-Model-Based-Semantic-Guided-Imaging-Biomarker-for-Early-Lung-Cancer-Detection" class="headerlink" title="Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early   Lung Cancer Detection"></a>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early   Lung Cancer Detection</h2><p><strong>Authors:Luoting Zhuang, Seyed Mohammad Hossein Tabatabaei, Ramin Salehi-Rad, Linh M. Tran, Denise R. Aberle, Ashley E. Prosper, William Hsu</strong></p>
<p>Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologistsâ€™ assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings. </p>
<blockquote>
<p>ç›®æ ‡ï¼šè®¸å¤šæœºå™¨å­¦ä¹ æ¨¡å‹å·²ç»åˆ©ç”¨è¯­ä¹‰ç‰¹å¾ã€æ·±åº¦ç‰¹å¾æˆ–ä¸¤è€…æ¥è¯„ä¼°è‚ºç»“èŠ‚çš„æ¶æ€§ç¨‹åº¦ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ã€è§£é‡Šæ€§æœ‰é™ä»¥åŠå¯¹æˆåƒå˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨ç»“åˆæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹ç»“èŠ‚çš„è¯„ä¼°æ‰€å¾—å‡ºçš„è¯­ä¹‰ç‰¹å¾ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç”¨äºé¢„æµ‹è‚ºç™Œçš„ä¸´åºŠç›¸å…³ã€ç¨³å¥å’Œå¯è§£é‡Šçš„ç‰¹å¾ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬ä»å›½å®¶è‚ºç™Œç­›æŸ¥è¯•éªŒä¸­è·å¾—938ä¾‹ä½å‰‚é‡CTæ‰«æç»“æœï¼ŒåŒ…å«1246ä¸ªç»“èŠ‚å’Œè¯­ä¹‰ç‰¹å¾ã€‚è‚ºéƒ¨å›¾åƒæ•°æ®åº“è”ç›Ÿæ•°æ®é›†åŒ…å«1018ä¾‹CTæ‰«æç»“æœï¼Œå¯¹2625ä¸ªç—…å˜è¿›è¡Œäº†ç»“èŠ‚ç‰¹å¾æ ‡æ³¨ã€‚å¦å¤–ä¸‰ä¸ªå¤–éƒ¨æ•°æ®é›†åˆ†åˆ«æ¥è‡ªåŠ å·å¤§å­¦æ´›æ‰çŸ¶åˆ†æ ¡å¥åº·ä¸­å¿ƒã€LUNGxæŒ‘æˆ˜èµ›å’Œæœå…‹è‚ºç™Œç­›æŸ¥ã€‚æˆ‘ä»¬ä½¿ç”¨å¾®è°ƒåçš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œé‡‡ç”¨é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯¹é½æˆåƒå’Œè¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é¢„æµ‹ä¸€å¹´å†…è‚ºç™Œè¯Šæ–­ç»“æœã€‚</p>
<p>ç»“æœï¼šæˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹å¯¹ä¸€å¹´å†…è‚ºç™Œè¯Šæ–­çš„æ€§èƒ½ï¼Œä½¿ç”¨AUROCå’ŒAUPRCè¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸ä¸‰ç§æœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹AUROCè¾¾åˆ°0.90ï¼ŒAUPRCè¾¾åˆ°0.78ï¼Œåœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†åŸºçº¿æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚ä½¿ç”¨CLIPï¼Œæˆ‘ä»¬è¿˜è·å¾—äº†å…³äºè¯­ä¹‰ç‰¹å¾çš„é¢„æµ‹ï¼Œå¦‚ç»“èŠ‚è¾¹ç¼˜ï¼ˆAUROCï¼š0.81ï¼‰ã€ç»“èŠ‚ä¸€è‡´æ€§ï¼ˆ0.81ï¼‰å’Œèƒ¸è†œé™„ç€ï¼ˆ0.84ï¼‰ï¼Œè¿™äº›ç‰¹å¾å¯ç”¨äºè§£é‡Šæ¨¡å‹é¢„æµ‹ã€‚</p>
<p>ç»“è®ºï¼šæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®åœ°å°†è‚ºç»“èŠ‚åˆ†ç±»ä¸ºè‰¯æ€§æˆ–æ¶æ€§ï¼Œæä¾›å¯è§£é‡Šçš„è¾“å‡ºç»“æœï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿç†è§£æ¨¡å‹é¢„æµ‹ç»“æœèƒŒåçš„å«ä¹‰ã€‚è¿™ç§æ–¹æ³•è¿˜é˜²æ­¢äº†æ¨¡å‹å­¦ä¹ æ·å¾„ï¼Œå¹¶åœ¨å„ç§ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰é€šç”¨æ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21344v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶èåˆæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹ç»“èŠ‚çš„è¯„ä¼°æ‰€æå–çš„è¯­ä¹‰ç‰¹å¾ï¼Œè®­ç»ƒæ¨¡å‹ä»¥é¢„æµ‹è‚ºç™Œã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¯¹é½å›¾åƒå’Œè¯­ä¹‰ç‰¹å¾ï¼Œé¢„æµ‹ä¸€å¹´å†…è‚ºç™Œè¯Šæ–­ã€‚æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæä¾›è§£é‡Šæ€§è¾“å‡ºï¼Œå¸®åŠ©åŒ»ç”Ÿç†è§£æ¨¡å‹é¢„æµ‹çš„å†…åœ¨å«ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨è§£å†³ç°æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é¢„æµ‹è‚ºç»“èŠ‚æ¶æ€§ç¨‹åº¦æ—¶å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚ä¾èµ–æ‰‹åŠ¨æ³¨é‡Šã€è§£é‡Šæ€§æœ‰é™å’Œå¯¹æˆåƒå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆè¯­ä¹‰ç‰¹å¾å’Œæ·±åº¦å­¦ä¹ ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ”¾å°„ç§‘åŒ»ç”Ÿå¯¹ç»“èŠ‚çš„è¯„ä¼°æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ ä¸´åºŠç›¸å…³ã€ç¨³å¥å’Œå¯è§£é‡Šçš„ç‰¹å¾ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†å¤šä¸ªæ•°æ®é›†ï¼ŒåŒ…æ‹¬æ¥è‡ªå›½å®¶è‚ºç™Œç­›æŸ¥è¯•éªŒã€è‚ºå›¾åƒæ•°æ®åº“è”ç›Ÿä»¥åŠä¸‰ä¸ªå¤–éƒ¨æ•°æ®é›†ï¼ˆUCLAå¥åº·ã€LUNGxæŒ‘æˆ˜å’Œæœå…‹è‚ºç™Œç­›æŸ¥ï¼‰çš„ä½å‰‚é‡CTæ‰«æå›¾åƒã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨é¢„è®­ç»ƒçš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¯¹é½å›¾åƒå’Œè¯­ä¹‰ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨é¢„æµ‹ä¸€å¹´å†…è‚ºç™Œè¯Šæ–­æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´å¥½çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿè§£é‡Šå…¶é¢„æµ‹ç»“æœï¼Œå¦‚ç»“èŠ‚è¾¹ç¼˜ã€ç»“èŠ‚ä¸€è‡´æ€§å’Œèƒ¸è†œé™„ç€ç­‰è¯­ä¹‰ç‰¹å¾ï¼Œæœ‰åŠ©äºåŒ»ç”Ÿç†è§£æ¨¡å‹é¢„æµ‹çš„å†…åœ¨å«ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1b53539bd8103ee141e9638d004fb55a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Improved-Cervical-Cancer-Screening-Vision-Transformer-Based-Classification-and-Interpretability"><a href="#Towards-Improved-Cervical-Cancer-Screening-Vision-Transformer-Based-Classification-and-Interpretability" class="headerlink" title="Towards Improved Cervical Cancer Screening: Vision Transformer-Based   Classification and Interpretability"></a>Towards Improved Cervical Cancer Screening: Vision Transformer-Based   Classification and Interpretability</h2><p><strong>Authors:Khoa Tuan Nguyen, Ho-min Park, Gaeun Oh, Joris Vankerschaver, Wesley De Neve</strong></p>
<p>We propose a novel approach to cervical cell image classification for cervical cancer screening using the EVA-02 transformer model. We developed a four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important features through multiple machine learning models, and training a new artificial neural network with optional loss weighting for improved generalization. With this design, our best model achieved an F1-score of 0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized Kernel SHAP analysis and identified key features correlating with cell morphology and staining characteristics, providing interpretable insights into the decision-making process of the fine-tuned model. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Khoa-NT/isbi2025_ps3c">https://github.com/Khoa-NT/isbi2025_ps3c</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨EVA-02è½¬æ¢å™¨æ¨¡å‹è¿›è¡Œå®«é¢ˆç™Œç­›æŸ¥çš„å®«é¢ˆç»†èƒå›¾åƒåˆ†ç±»æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå››æ­¥æµç¨‹ï¼šå¾®è°ƒEVA-02ï¼Œç‰¹å¾æå–ï¼Œé€šè¿‡å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œä»¥åŠä½¿ç”¨å¯é€‰çš„æŸå¤±æƒé‡è®­ç»ƒæ–°çš„ç¥ç»ç½‘ç»œä»¥æ”¹å–„å…¶æ³›åŒ–èƒ½åŠ›ã€‚å‡­å€Ÿè¿™ç§è®¾è®¡ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹å®ç°äº†F1åˆ†æ•°ä¸º0.85227ï¼Œè¶…è¿‡äº†åŸºçº¿EVA-02æ¨¡å‹ï¼ˆ0.84878ï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨Kernel SHAPåˆ†æç¡®å®šäº†ä¸ç»†èƒå½¢æ€å’ŒæŸ“è‰²ç‰¹å¾ç›¸å…³çš„å…³é”®ç‰¹å¾ï¼Œä¸ºå¾®è°ƒæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹æä¾›äº†å¯è§£é‡Šæ€§çš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Khoa-NT/isbi2025_ps3c%E3%80%82">https://github.com/Khoa-NT/isbi2025_ps3cã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21340v1">PDF</a> Accepted at ISBI 2025 â€œChallenge 2: Pap Smear Cell Classification   Challengeâ€</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨EVA-02å˜å‹å™¨æ¨¡å‹è¿›è¡Œå®«é¢ˆç»†èƒå›¾åƒåˆ†ç±»ä»¥è¿›è¡Œå®«é¢ˆç™Œç­›æŸ¥çš„æ–°æ–¹æ³•ã€‚è®¾è®¡äº†ä¸€ä¸ªå››æ­¥æµç¨‹ï¼šå¾®è°ƒEVA-02ã€ç‰¹å¾æå–ã€é€šè¿‡å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹é€‰æ‹©é‡è¦ç‰¹å¾ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªæ–°çš„å¸¦æœ‰å¯é€‰æŸå¤±æƒé‡çš„ç¥ç»ç½‘ç»œä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°è¾¾åˆ°0.85227ï¼Œä¼˜äºåŸºçº¿EVA-02æ¨¡å‹ï¼ˆ0.84878ï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨Kernel SHAPåˆ†æç¡®å®šäº†ä¸ç»†èƒå½¢æ€å’ŒæŸ“è‰²ç‰¹å¾ç›¸å…³çš„å…³é”®ç‰¹å¾ï¼Œä¸ºå¾®è°ƒæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹æä¾›äº†å¯è§£é‡Šæ€§çš„è§è§£ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Khoa-NT/isbi2025_ps3c%E3%80%82">https://github.com/Khoa-NT/isbi2025_ps3cã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºEVA-02å˜å‹å™¨æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç”¨äºå®«é¢ˆç»†èƒå›¾åƒåˆ†ç±»çš„å®«é¢ˆç™Œç­›æŸ¥ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªå››æ­¥æµç¨‹ï¼ŒåŒ…æ‹¬å¾®è°ƒEVA-02æ¨¡å‹ã€ç‰¹å¾æå–å’Œé€‰æ‹©é‡è¦ç‰¹å¾ç­‰æ­¥éª¤ã€‚</li>
<li>é€šè¿‡å¤šæœºå™¨å­¦ä¹ æ¨¡å‹é€‰æ‹©å…³é”®ç‰¹å¾ï¼Œè®­ç»ƒæ–°çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°ä¸º0.85227ï¼Œæ€§èƒ½ä¼˜äºåŸºçº¿EVA-02æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨Kernel SHAPåˆ†æç¡®å®šäº†ä¸ç»†èƒå½¢æ€å’ŒæŸ“è‰²ç‰¹å¾ç›¸å…³çš„å…³é”®ç‰¹å¾ã€‚</li>
<li>æä¾›äº†æ¨¡å‹å†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§è§è§£ã€‚</li>
<li>ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šä¾›å…¬ä¼—è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed7fc65b52e2dd3427ca79e7d06ef718.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdb3fb6522e7305eee66788477864172.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87e8394163f2143d0087a581da0477e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d05be88e15a8a797944b624718b8e798.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a8d7e51ca9bd45b26b46ab31922070a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="UniBiomed-A-Universal-Foundation-Model-for-Grounded-Biomedical-Image-Interpretation"><a href="#UniBiomed-A-Universal-Foundation-Model-for-Grounded-Biomedical-Image-Interpretation" class="headerlink" title="UniBiomed: A Universal Foundation Model for Grounded Biomedical Image   Interpretation"></a>UniBiomed: A Universal Foundation Model for Grounded Biomedical Image   Interpretation</h2><p><strong>Authors:Linshan Wu, Yuxiang Nie, Sunan He, Jiaxin Zhuang, Hao Chen</strong></p>
<p>Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦å›¾åƒçš„å¤šæ¨¡æ€è§£è¯»ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ–°çš„æœºä¼šã€‚ä¼ ç»Ÿçš„AIæ–¹æ³•é€šå¸¸ä¾èµ–äºåˆ†ç«‹è®­ç»ƒï¼Œå³ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¸´åºŠæ–‡æœ¬ç”Ÿæˆå’Œåˆ†å‰²æ¨¡å‹è¿›è¡Œç›®æ ‡æå–ï¼Œè¿™å¯¼è‡´åœ¨å®é™…éƒ¨ç½²ä¸­çš„ä¸çµæ´»æ€§å’Œæ— æ³•åˆ©ç”¨æ•´ä½“ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniBiomedï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºåŸºäºåŸºç¡€ç”Ÿç‰©åŒ»å­¦å›¾åƒè§£è¯»çš„é€šç”¨åŸºç¡€æ¨¡å‹ã€‚UniBiomedåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œåˆ†æ®µä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆSAMï¼‰çš„æ–°é¢–é›†æˆï¼Œæœ‰æ•ˆåœ°ç»Ÿä¸€äº†ä¸´åºŠæ–‡æœ¬çš„ç”Ÿæˆå’Œç›¸åº”ç”Ÿç‰©åŒ»å­¦å¯¹è±¡çš„åˆ†å‰²ï¼Œä»¥å®ç°åŸºäºè§£è¯»çš„è§£è¯»ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒUniBiomedèƒ½å¤Ÿå¤„ç†æ¶‰åŠåç§ä¸åŒç”Ÿç‰©åŒ»å­¦æˆåƒæ¨¡æ€çš„å¹¿æ³›ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ã€‚ä¸ºäº†å¼€å‘UniBiomedï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«åç§æˆåƒæ¨¡æ€çš„å›¾åƒã€æ³¨è§£å’Œæ–‡æœ¬æè¿°è¶…è¿‡2700ä¸‡ç»„æ•°æ®ã€‚åœ¨å†…éƒ¨å’Œå¤–éƒ¨çš„84ä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›éªŒè¯è¡¨æ˜ï¼ŒUniBiomedåœ¨åˆ†å‰²ã€ç–¾ç—…è¯†åˆ«ã€åŒºåŸŸæ„ŸçŸ¥è¯Šæ–­ã€è§†è§‰é—®ç­”å’ŒæŠ¥å‘Šç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ä¹‹å‰ä¾èµ–äºä¸´åºŠä¸“å®¶è¿›è¡Œé¢„å…ˆè¯Šæ–­å›¾åƒå’Œæ‰‹åŠ¨ç²¾ç¡®åˆ¶ä½œæ–‡æœ¬æˆ–è§†è§‰æç¤ºçš„æ¨¡å‹ä¸åŒï¼ŒUniBiomedå¯ä»¥ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææä¾›è‡ªåŠ¨åŒ–å’Œç«¯åˆ°ç«¯çš„åŸºäºåŸºç¡€è§£è¯»ã€‚è¿™ä»£è¡¨äº†ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„æ–°èŒƒå¼è½¬å˜ï¼Œå°†æ˜¾è‘—æé«˜è¯Šæ–­æ•ˆç‡ã€‚æ€»ä¹‹ï¼ŒUniBiomedä»£è¡¨äº†ç”Ÿç‰©åŒ»å­¦AIçš„ä¸€ä¸ªå…¨æ–°çªç ´ï¼Œè§£é”äº†å¼ºå¤§çš„åŸºäºè§£è¯»çš„èƒ½åŠ›ï¼Œä¸ºæ›´å‡†ç¡®å’Œé«˜æ•ˆç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21336v1">PDF</a> The first universal foundation model for grounded biomedical image   interpretation</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿç‰©åŒ»å­¦å›¾åƒå¤šæ¨¡æ€è§£è¯»çš„æ–°æœºä¼šã€‚ä¼ ç»Ÿçš„AIæ–¹æ³•é€šå¸¸é‡‡ç”¨åˆ†ç«‹è®­ç»ƒçš„æ–¹å¼ï¼Œæ— æ³•å®ç°å…¨é¢åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯ï¼Œå­˜åœ¨çµæ´»æ€§é—®é¢˜ã€‚å› æ­¤å¼•å…¥äº†UniBiomedï¼Œä¸€ä¸ªç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒè§£è¯»çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹ã€‚å®ƒé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å’Œåˆ†å‰²ä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆSAMï¼‰çš„æœ‰æ•ˆé›†æˆï¼Œå®ç°äº†ä¸´åºŠæ–‡æœ¬ç”Ÿæˆå’Œå¯¹åº”ç”Ÿç‰©åŒ»å­¦å¯¹è±¡çš„åˆ†å‰²çš„ç»Ÿä¸€ã€‚UniBiomedèƒ½å¤Ÿåœ¨å¤šç§ç”Ÿç‰©åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šå®Œæˆä¸€ç³»åˆ—ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ•°æ®é›†ä¸Šå®ç°äº†åˆ†å‰²ã€ç–¾ç—…è¯†åˆ«ç­‰ä»»åŠ¡çš„é¡¶å°–æ€§èƒ½ï¼Œèƒ½å¤Ÿæä¾›è‡ªåŠ¨åŒ–å’Œç«¯åˆ°ç«¯çš„è§£è¯»ï¼Œä»£è¡¨ä¸´åºŠå·¥ä½œæµç¨‹çš„æ–°èŒƒå¼è½¬å˜ã€‚æ€»ä¹‹ï¼ŒUniBiomedæ˜¯ç”Ÿç‰©åŒ»å­¦AIé¢†åŸŸçš„ä¸€é¡¹é‡å¤§çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è§£è¯»ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ææä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>ä¼ ç»ŸAIæ–¹æ³•å­˜åœ¨çµæ´»æ€§é—®é¢˜ï¼Œæ— æ³•å®ç°å…¨é¢åˆ©ç”¨ç”Ÿç‰©åŒ»å­¦ä¿¡æ¯ã€‚</li>
<li>UniBiomedæ˜¯ä¸€ä¸ªç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒè§£è¯»çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>UniBiomedé€šè¿‡MLLMå’ŒSAMçš„é›†æˆï¼Œå®ç°äº†ä¸´åºŠæ–‡æœ¬ç”Ÿæˆå’Œç”Ÿç‰©åŒ»å­¦å¯¹è±¡åˆ†å‰²çš„ç»Ÿä¸€ã€‚</li>
<li>UniBiomedèƒ½å¤Ÿåœ¨å¤šç§ç”Ÿç‰©åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šå®Œæˆä¸€ç³»åˆ—ä»»åŠ¡ã€‚</li>
<li>UniBiomedåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå®ç°äº†é¡¶å°–æ€§èƒ½ï¼ŒåŒ…æ‹¬åˆ†å‰²ã€ç–¾ç—…è¯†åˆ«ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2a2c5e2d96da259ac765ac4eceb4da6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-808b4956e5a32836d75445efdf8f9755.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mamba-Based-Feature-Extraction-And-Adaptive-Multilevel-Feature-Fusion-For-3D-Tumor-Segmentation-From-Multi-modal-Medical-Image"><a href="#Mamba-Based-Feature-Extraction-And-Adaptive-Multilevel-Feature-Fusion-For-3D-Tumor-Segmentation-From-Multi-modal-Medical-Image" class="headerlink" title="Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion   For 3D Tumor Segmentation From Multi-modal Medical Image"></a>Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion   For 3D Tumor Segmentation From Multi-modal Medical Image</h2><p><strong>Authors:Zexin Ji, Beiji Zou, Xiaoyan Kui, Hua Li, Pierre Vera, Su Ruan</strong></p>
<p>Multi-modal 3D medical image segmentation aims to accurately identify tumor regions across different modalities, facing challenges from variations in image intensity and tumor morphology. Traditional convolutional neural network (CNN)-based methods struggle with capturing global features, while Transformers-based methods, despite effectively capturing global context, encounter high computational costs in 3D medical image segmentation. The Mamba model combines linear scalability with long-distance modeling, making it a promising approach for visual representation learning. However, Mamba-based 3D multi-modal segmentation still struggles to leverage modality-specific features and fuse complementary information effectively. In this paper, we propose a Mamba based feature extraction and adaptive multilevel feature fusion for 3D tumor segmentation using multi-modal medical image. We first develop the specific modality Mamba encoder to efficiently extract long-range relevant features that represent anatomical and pathological structures present in each modality. Moreover, we design an bi-level synergistic integration block that dynamically merges multi-modal and multi-level complementary features by the modality attention and channel attention learning. Lastly, the decoder combines deep semantic information with fine-grained details to generate the tumor segmentation map. Experimental results on medical image datasets (PET&#x2F;CT and MRI multi-sequence) show that our approach achieve competitive performance compared to the state-of-the-art CNN, Transformer, and Mamba-based approaches. </p>
<blockquote>
<p>å¤šæ¨¡æ€3DåŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨å‡†ç¡®è¯†åˆ«ä¸åŒæ¨¡æ€ä¸‹çš„è‚¿ç˜¤åŒºåŸŸï¼Œé¢ä¸´æ¥è‡ªå›¾åƒå¼ºåº¦å’Œè‚¿ç˜¤å½¢æ€å˜åŒ–çš„æŒ‘æˆ˜ã€‚ä¼ ç»ŸåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ–¹æ³•åœ¨æ•æ‰å…¨å±€ç‰¹å¾æ—¶é‡åˆ°å›°éš¾ï¼Œè€ŒåŸºäºTransformerçš„æ–¹æ³•è™½ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬çš„é—®é¢˜ã€‚Mambaæ¨¡å‹ç»“åˆäº†çº¿æ€§å¯æ‰©å±•æ€§å’Œé•¿è·ç¦»å»ºæ¨¡ï¼Œä½¿å…¶æˆä¸ºè§†è§‰è¡¨ç¤ºå­¦ä¹ çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŸºäºMambaçš„3Då¤šæ¨¡æ€åˆ†å‰²ä»ç„¶éš¾ä»¥åˆ©ç”¨ç‰¹å®šæ¨¡æ€çš„ç‰¹å¾å¹¶æœ‰æ•ˆåœ°èåˆäº’è¡¥ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMambaçš„ç‰¹å¾æå–å’Œè‡ªé€‚åº”å¤šçº§ç‰¹å¾èåˆæ–¹æ³•ï¼Œç”¨äºä½¿ç”¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒè¿›è¡Œ3Dè‚¿ç˜¤åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ç§ç‰¹å®šæ¨¡æ€çš„Mambaç¼–ç å™¨ï¼Œä»¥æœ‰æ•ˆåœ°æå–ä»£è¡¨æ¯ç§æ¨¡æ€ä¸­å­˜åœ¨çš„è§£å‰–å’Œç—…ç†ç»“æ„çš„é•¿ç¨‹ç›¸å…³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤çº§ååŒé›†æˆå—ï¼Œé€šè¿‡æ¨¡æ€æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›å­¦ä¹ æ¥åŠ¨æ€èåˆå¤šæ¨¡æ€å’Œå¤šçº§äº’è¡¥ç‰¹å¾ã€‚æœ€åï¼Œè§£ç å™¨å°†æ·±åº¦è¯­ä¹‰ä¿¡æ¯ä¸ç»†èŠ‚ç›¸ç»“åˆï¼Œç”Ÿæˆè‚¿ç˜¤åˆ†å‰²å›¾ã€‚åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼ˆPET&#x2F;CTå’ŒMRIå¤šåºåˆ—ï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€å…ˆè¿›çš„CNNã€Transformerå’ŒMambaæ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21281v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€3DåŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´ä¸åŒæ¨¡æ€ä¸‹è‚¿ç˜¤åŒºåŸŸå‡†ç¡®è¯†åˆ«ä»¥åŠå›¾åƒå¼ºåº¦å˜åŒ–å’Œè‚¿ç˜¤å½¢æ€å˜åŒ–çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ–¹æ³•éš¾ä»¥æ•æ‰å…¨å±€ç‰¹å¾ï¼Œè€ŒåŸºäºTransformerçš„æ–¹æ³•è™½ç„¶èƒ½æœ‰æ•ˆæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨3DåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚Mambaæ¨¡å‹å…·æœ‰çº¿æ€§å¯æ‰©å±•æ€§å’Œè¿œè·ç¦»å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ ä¸­å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼ŒåŸºäºMambaçš„3Då¤šæ¨¡æ€åˆ†å‰²åœ¨åˆ©ç”¨ç‰¹å®šæ¨¡æ€ç‰¹å¾å’Œæœ‰æ•ˆèåˆäº’è¡¥ä¿¡æ¯æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMambaçš„ç‰¹å¾æå–å’Œè‡ªé€‚åº”å¤šçº§ç‰¹å¾èåˆæ–¹æ³•ï¼Œç”¨äºä½¿ç”¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒè¿›è¡Œ3Dè‚¿ç˜¤åˆ†å‰²ã€‚é¦–å…ˆå¼€å‘ç‰¹å®šæ¨¡æ€çš„Mambaç¼–ç å™¨ï¼Œä»¥æœ‰æ•ˆæå–ä»£è¡¨æ¯ç§æ¨¡æ€çš„è§£å‰–å’Œç—…ç†ç»“æ„çš„é•¿æœŸç›¸å…³ç‰¹å¾ã€‚å…¶æ¬¡è®¾è®¡äº†ä¸€ç§ä¸¤çº§ååŒé›†æˆæ¨¡å—ï¼Œé€šè¿‡æ¨¡æ€æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›å­¦ä¹ åŠ¨æ€èåˆå¤šæ¨¡æ€å’Œå¤šçº§äº’è¡¥ç‰¹å¾ã€‚æœ€åï¼Œè§£ç å™¨ç»“åˆæ·±å±‚è¯­ä¹‰ä¿¡æ¯å’Œç»†èŠ‚ç”Ÿæˆè‚¿ç˜¤åˆ†å‰²å›¾ã€‚åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼ˆPET&#x2F;CTå’ŒMRIå¤šåºåˆ—ï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸æœ€æ–°çš„CNNã€Transformerå’ŒMambaæ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€3DåŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨å‡†ç¡®è¯†åˆ«ä¸åŒæ¨¡æ€ä¸‹çš„è‚¿ç˜¤åŒºåŸŸï¼Œé¢ä¸´å›¾åƒå¼ºåº¦å˜åŒ–å’Œè‚¿ç˜¤å½¢æ€å˜åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸCNNæ–¹æ³•éš¾ä»¥æ•æ‰å…¨å±€ç‰¹å¾ï¼Œè€ŒåŸºäºTransformerçš„æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>Mambaæ¨¡å‹å…·æœ‰çº¿æ€§å¯æ‰©å±•æ€§å’Œè¿œè·ç¦»å»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>åŸºäºMambaçš„3Då¤šæ¨¡æ€åˆ†å‰²åœ¨åˆ©ç”¨ç‰¹å®šæ¨¡æ€ç‰¹å¾å’Œèåˆäº’è¡¥ä¿¡æ¯æ–¹é¢ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºMambaçš„ç‰¹å¾æå–å’Œè‡ªé€‚åº”å¤šçº§ç‰¹å¾èåˆæ–¹æ³•ï¼Œç»“åˆç‰¹å®šæ¨¡æ€çš„Mambaç¼–ç å™¨å’ŒååŒé›†æˆæ¨¡å—å®ç°ç²¾å‡†åˆ†å‰²ã€‚</li>
<li>è®ºæ–‡è®¾è®¡çš„æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21281">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-86697a602fdc94af6dd0ab96cdffff6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79f50c56a243bdd3a792d5d8e46f3554.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Modeling-and-Performance-Analysis-for-Semantic-Communications-Based-on-Empirical-Results"><a href="#Modeling-and-Performance-Analysis-for-Semantic-Communications-Based-on-Empirical-Results" class="headerlink" title="Modeling and Performance Analysis for Semantic Communications Based on   Empirical Results"></a>Modeling and Performance Analysis for Semantic Communications Based on   Empirical Results</h2><p><strong>Authors:Shuai Ma, Bin Shen, Chuanhui Zhang, Youlong Wu, Hang Li, Shiyin Li, Guangming Shi, Naofal Al-Dhahir</strong></p>
<p>Due to the black-box characteristics of deep learning based semantic encoders and decoders, finding a tractable method for the performance analysis of semantic communications is a challenging problem. In this paper, we propose an Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end measurement and SNR, which can be applied for both image reconstruction tasks and inference tasks. Specifically, for image reconstruction tasks, the proposed ABG formula can well fit the commonly used DL networks, such as SCUNet, and Vision Transformer, for semantic encoding with the multi scale-structural similarity index measure (MS-SSIM) measurement. Furthermore, we find that the upper bound of the MS-SSIM depends on the number of quantized output bits of semantic encoders, and we also propose a closed-form expression to fit the relationship between the MS-SSIM and quantized output bits. To the best of our knowledge, this is the first theoretical expression between end-to-end performance metrics and SNR for semantic communications. Based on the proposed ABG formula, we investigate an adaptive power control scheme for semantic communications over random fading channels, which can effectively guarantee quality of service (QoS) for semantic communications, and then design the optimal power allocation scheme to maximize the energy efficiency of the semantic communication system. Furthermore, by exploiting the bisection algorithm, we develop the power allocation scheme to maximize the minimum QoS of multiple users for OFDMA downlink semantic communication Extensive simulations verify the effectiveness and superiority of the proposed ABG formula and power allocation schemes. </p>
<blockquote>
<p>ç”±äºåŸºäºæ·±åº¦å­¦ä¹ çš„è¯­ä¹‰ç¼–ç å™¨å’Œè§£ç å™¨çš„é»‘ç®±ç‰¹æ€§ï¼Œæ‰¾åˆ°ä¸€ç§ç”¨äºè¯­ä¹‰é€šä¿¡æ€§èƒ½åˆ†æçš„å¯è¡Œæ–¹æ³•æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªAlpha-Beta-Gammaï¼ˆABGï¼‰å…¬å¼ï¼Œç”¨äºå»ºæ¨¡ç«¯åˆ°ç«¯æµ‹é‡ä¸ä¿¡å™ªæ¯”ä¹‹é—´çš„å…³ç³»ï¼Œè¯¥å…¬å¼æ—¢å¯ç”¨äºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œä¹Ÿå¯ç”¨äºæ¨ç†ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œæå‡ºçš„ABGå…¬å¼èƒ½å¤Ÿå¾ˆå¥½åœ°é€‚åº”å¸¸ç”¨çš„DLç½‘ç»œï¼Œå¦‚SCUNetå’ŒVision Transformerï¼Œè¿›è¡Œå…·æœ‰å¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆMS-SSIMï¼‰çš„è¯­ä¹‰ç¼–ç ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°MS-SSIMçš„ä¸Šé™å–å†³äºè¯­ä¹‰ç¼–ç å™¨çš„é‡åŒ–è¾“å‡ºä½æ•°ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå°é—­å½¢å¼çš„è¡¨è¾¾å¼æ¥æ‹ŸåˆMS-SSIMä¸é‡åŒ–è¾“å‡ºä½ä¹‹é—´çš„å…³ç³»ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¯­ä¹‰é€šä¿¡ç«¯åˆ°ç«¯æ€§èƒ½æŒ‡æ ‡å’Œä¿¡å™ªæ¯”ä¹‹é—´çš„ç¬¬ä¸€ä¸ªç†è®ºè¡¨è¾¾å¼ã€‚åŸºäºæå‡ºçš„ABGå…¬å¼ï¼Œæˆ‘ä»¬ç ”ç©¶äº†éšæœºè¡°è½ä¿¡é“ä¸Šçš„è¯­ä¹‰é€šä¿¡çš„è‡ªé€‚åº”åŠŸç‡æ§åˆ¶æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥æœ‰æ•ˆåœ°ä¿è¯è¯­ä¹‰é€šä¿¡çš„æœåŠ¡è´¨é‡ï¼ˆQoSï¼‰ï¼Œç„¶åè®¾è®¡æœ€ä¼˜åŠŸç‡åˆ†é…æ–¹æ¡ˆä»¥æœ€å¤§åŒ–è¯­ä¹‰é€šä¿¡ç³»ç»Ÿçš„èƒ½æºæ•ˆç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨äºŒåˆ†ç®—æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†åŠŸç‡åˆ†é…æ–¹æ¡ˆï¼Œä»¥æœ€å¤§åŒ–å¤šä¸ªç”¨æˆ·åœ¨OFDMAä¸‹è¡Œé“¾è·¯è¯­ä¹‰é€šä¿¡ä¸­çš„æœ€ä½QoSã€‚å¤§é‡ä»¿çœŸéªŒè¯äº†æ‰€æå‡ºçš„ABGå…¬å¼å’ŒåŠŸç‡åˆ†é…æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§Alpha-Beta-Gammaï¼ˆABGï¼‰å…¬å¼ï¼Œç”¨äºå»ºæ¨¡è¯­ä¹‰é€šä¿¡ç«¯åˆ°ç«¯æµ‹é‡ä¸ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä¹‹é—´çš„å…³ç³»ï¼Œé€‚ç”¨äºå›¾åƒé‡å»ºå’Œæ¨ç†ä»»åŠ¡ã€‚å¯¹äºå›¾åƒé‡å»ºä»»åŠ¡ï¼ŒABGå…¬å¼èƒ½å¾ˆå¥½åœ°é€‚åº”å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ ç½‘ç»œï¼ˆå¦‚SCUNetå’ŒVision Transformerï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œå¤šå°ºåº¦ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆMS-SSIMï¼‰çš„ä¸Šé™å–å†³äºè¯­ä¹‰ç¼–ç å™¨çš„é‡åŒ–è¾“å‡ºä½æ•°ï¼Œå¹¶æå‡ºä¸€ä¸ªå°é—­å½¢å¼çš„è¡¨è¾¾å¼æ¥æè¿°å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼ŒåŸºäºABGå…¬å¼ï¼Œç ”ç©¶äº†ä¸€ç§é’ˆå¯¹éšæœºè¡°è½ä¿¡é“çš„è‡ªé€‚åº”åŠŸç‡æ§åˆ¶æ–¹æ¡ˆï¼Œä¿è¯è¯­ä¹‰é€šä¿¡çš„æœåŠ¡è´¨é‡ï¼ˆQoSï¼‰ï¼Œå¹¶è®¾è®¡äº†æœ€ä¼˜åŠŸç‡åˆ†é…æ–¹æ¡ˆï¼Œæœ€å¤§åŒ–è¯­ä¹‰é€šä¿¡ç³»ç»Ÿçš„èƒ½æºæ•ˆç‡ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ABGå…¬å¼å’ŒåŠŸç‡åˆ†é…æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºAlpha-Beta-Gammaï¼ˆABGï¼‰å…¬å¼ï¼Œç”¨äºå»ºæ¨¡è¯­ä¹‰é€šä¿¡ç«¯åˆ°ç«¯æµ‹é‡ä¸ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰çš„å…³ç³»ã€‚</li>
<li>ABGå…¬å¼é€‚ç”¨äºå›¾åƒé‡å»ºå’Œæ¨ç†ä»»åŠ¡ï¼Œå¹¶èƒ½é€‚åº”å¸¸è§çš„æ·±åº¦å­¦ä¹ ç½‘ç»œã€‚</li>
<li>MS-SSIMçš„ä¸Šé™å—è¯­ä¹‰ç¼–ç å™¨çš„é‡åŒ–è¾“å‡ºä½æ•°å½±å“ã€‚</li>
<li>æå‡ºå°é—­å½¢å¼çš„è¡¨è¾¾å¼æè¿°MS-SSIMä¸é‡åŒ–è¾“å‡ºä½æ•°çš„å…³ç³»ã€‚</li>
<li>åŸºäºABGå…¬å¼ï¼Œç ”ç©¶äº†ä¸€ç§è‡ªé€‚åº”åŠŸç‡æ§åˆ¶æ–¹æ¡ˆï¼Œä¿è¯è¯­ä¹‰é€šä¿¡çš„æœåŠ¡è´¨é‡ã€‚</li>
<li>è®¾è®¡äº†æœ€ä¼˜åŠŸç‡åˆ†é…æ–¹æ¡ˆï¼Œä»¥æœ€å¤§åŒ–è¯­ä¹‰é€šä¿¡ç³»ç»Ÿçš„èƒ½æºæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db118acb7b2de3ea12fafe2b8951df53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d047e0f61325503188196a44896de63e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9858e2b87d35e3cebbee545fdf1816af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-058e7051fc0c67ca38e59fe96f8a342e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1373e631533280e5ce26a621dd042aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce02a9f0f4d41b9d9e25592d3a408b4b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="BiPrompt-SAM-Enhancing-Image-Segmentation-via-Explicit-Selection-between-Point-and-Text-Prompts"><a href="#BiPrompt-SAM-Enhancing-Image-Segmentation-via-Explicit-Selection-between-Point-and-Text-Prompts" class="headerlink" title="BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection   between Point and Text Prompts"></a>BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection   between Point and Text Prompts</h2><p><strong>Authors:Suzhe Xu, Jialin Peng, Chengyuan Zhang</strong></p>
<p>Segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. The Segment Anything Model (SAM) excels at point-prompted segmentation, while text-based models, often leveraging powerful multimodal encoders like BEIT-3, provide rich semantic understanding. However, effectively combining these complementary modalities remains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal prompt segmentation framework employing an explicit selection mechanism. We leverage SAMâ€™s ability to generate multiple mask candidates from a single point prompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select the point-generated mask that best aligns spatially, measured by Intersection over Union (IoU). This approach, interpretable as a simplified Mixture of Experts (MoE), effectively fuses spatial precision and semantic context without complex model modifications. Notably, our method achieves strong zero-shot performance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using only a single point prompt per instance. This significantly reduces annotation burden compared to bounding boxes and aligns better with practical clinical workflows, demonstrating the methodâ€™s effectiveness without domain-specific training. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8% IoU, significantly outperforming existing approaches. Experiments show BiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic disambiguation, offering a simple, effective, and interpretable perspective on multi-modal prompt fusion. </p>
<blockquote>
<p>åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œç”±äºçµæ´»æ€§ï¼ŒåŸºäºæç¤ºçš„æ–¹æ³•é€æ¸å—åˆ°é‡è§†ã€‚Segment Anything Modelï¼ˆSAMï¼‰åœ¨ç‚¹æç¤ºåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè€ŒåŸºäºæ–‡æœ¬æ¨¡å‹åˆ™ç»å¸¸åˆ©ç”¨å¼ºå¤§çš„å¤šæ¨¡æ€ç¼–ç å™¨ï¼ˆå¦‚BEIT-3ï¼‰æä¾›ä¸°å¯Œçš„è¯­ä¹‰ç†è§£ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°ç»“åˆè¿™äº›äº’è¡¥æ¨¡æ€ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†BiPrompt-SAMï¼Œè¿™æ˜¯ä¸€ç§é‡‡ç”¨æ˜¾å¼é€‰æ‹©æœºåˆ¶çš„æ–°å‹åŒæ¨¡æ€æç¤ºåˆ†å‰²æ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨SAMä»å•ä¸ªç‚¹æç¤ºç”Ÿæˆå¤šä¸ªæ©ç å€™é€‰çš„èƒ½åŠ›ï¼Œå¹¶ä½¿ç”¨æ–‡æœ¬å¼•å¯¼æ©ç ï¼ˆé€šè¿‡EVF-SAMä¸BEIT-3ç”Ÿæˆï¼‰é€‰æ‹©ç©ºé—´ä¸Šæœ€ä½³å¯¹é½çš„ç‚¹ç”Ÿæˆæ©ç ï¼Œé€šè¿‡äº¤é›†å¹¶é›†ï¼ˆIoUï¼‰æ¥è¡¡é‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥è§£é‡Šä¸ºç®€åŒ–çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°èåˆç©ºé—´ç²¾åº¦å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œè€Œæ— éœ€å¤æ‚çš„æ¨¡å‹ä¿®æ”¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Endovis17åŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼ˆmDiceä¸º89.55%ï¼ŒmIoUä¸º81.46%ï¼‰ï¼Œæ¯ä¸ªå®ä¾‹ä»…ä½¿ç”¨å•ä¸ªç‚¹æç¤ºã€‚è¿™å¤§å¤§é™ä½äº†ä¸è¾¹ç•Œæ¡†ç›¸æ¯”çš„æ³¨é‡Šè´Ÿæ‹…ï¼Œå¹¶æ›´å¥½åœ°ä¸å®é™…çš„ä¸´åºŠå·¥ä½œæµç¨‹å¯¹é½ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ— éœ€ç‰¹å®šé¢†åŸŸè®­ç»ƒçš„æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚åœ¨RefCOCOç³»åˆ—ä¸Šï¼ŒBiPrompt-SAMè¾¾åˆ°äº†87.1%ï¼Œ86.5%å’Œ85.8%çš„IoUï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒBiPrompt-SAMåœ¨éœ€è¦ç©ºé—´å‡†ç¡®æ€§å’Œè¯­ä¹‰æ¶ˆæ­§çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºå¤šæ¨¡æ€æç¤ºèåˆæä¾›äº†ç®€å•ã€æœ‰æ•ˆå’Œå¯è§£é‡Šçš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19769v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†BiPrompt-SAMï¼Œä¸€ç§æ–°å‹çš„åŒæ¨¡æ€æç¤ºåˆ†å‰²æ¡†æ¶ï¼Œå®ƒç»“åˆäº†Segment Anything Modelï¼ˆSAMï¼‰å’Œæ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨SAMç”Ÿæˆå¤šä¸ªæ©è†œå€™é€‰ï¼Œå¹¶ç»“åˆæ–‡æœ¬æŒ‡å¯¼æ©è†œé€‰æ‹©æœ€ä½³çš„ç©ºé—´å¯¹é½æ©è†œï¼Œå®ç°äº†ç©ºé—´ç²¾åº¦å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡çš„èåˆã€‚åœ¨åŒ»å­¦æ•°æ®é›†Endovis17å’ŒRefCOCOç³»åˆ—ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸éœ€è¦ç‰¹å®šé¢†åŸŸè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BiPrompt-SAMç»“åˆäº†ç‚¹æç¤ºåˆ†å‰²æ–¹æ³•å’Œæ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•ï¼Œæ˜¯ä¸€ç§æ–°å‹çš„åŒæ¨¡æ€æç¤ºåˆ†å‰²æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨SAMç”Ÿæˆå¤šä¸ªæ©è†œå€™é€‰ï¼Œé€šè¿‡æ–‡æœ¬æŒ‡å¯¼é€‰æ‹©æœ€ä½³çš„ç©ºé—´å¯¹é½æ©è†œã€‚</li>
<li>æ–¹æ³•å®ç°äº†ç©ºé—´ç²¾åº¦å’Œè¯­ä¹‰ä¸Šä¸‹æ–‡çš„èåˆï¼Œå¯çœ‹ä½œæ˜¯ä¸€ç§ç®€åŒ–çš„Mixture of Experts (MoE)ã€‚</li>
<li>åœ¨åŒ»å­¦æ•°æ®é›†Endovis17ä¸Šå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½¿ç”¨å•ä¸ªç‚¹æç¤ºé™ä½äº†æ ‡æ³¨è´Ÿæ‹…ï¼Œæ›´ç¬¦åˆå®é™…ä¸´åºŠå·¥ä½œæµç¨‹ã€‚</li>
<li>åœ¨RefCOCOç³»åˆ—ä¸Šï¼ŒBiPrompt-SAMæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒBiPrompt-SAMåœ¨éœ€è¦ç©ºé—´ç²¾åº¦å’Œè¯­ä¹‰è¾¨æçš„åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f71730ec82b776b575e3202e003c202.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3bff661bf4102ab0fa63b34f0df1c50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e54ad8684c3157b93f443320d2a40d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e34ead18155450e15b6544dc2d89b364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bfd33ab7f45e3c71ca08140bd372b1a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAD-Unet-A-Capsule-Network-Enhanced-Unet-Architecture-for-Accurate-Segmentation-of-COVID-19-Lung-Infections-from-CT-Images"><a href="#CAD-Unet-A-Capsule-Network-Enhanced-Unet-Architecture-for-Accurate-Segmentation-of-COVID-19-Lung-Infections-from-CT-Images" class="headerlink" title="CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate   Segmentation of COVID-19 Lung Infections from CT Images"></a>CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate   Segmentation of COVID-19 Lung Infections from CT Images</h2><p><strong>Authors:Yijie Dang, Weijun Ma, Xiaohu Luo, Huaizhu Wang</strong></p>
<p>Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: <a target="_blank" rel="noopener" href="https://github.com/AmanoTooko-jie/CAD-Unet">https://github.com/AmanoTooko-jie/CAD-Unet</a>. </p>
<blockquote>
<p>è‡ª2019å¹´COVID-19ç–«æƒ…çˆ†å‘ä»¥æ¥ï¼ŒåŒ»å­¦æˆåƒå·²æˆä¸ºè¯Šæ–­COVID-19è‚ºç‚çš„ä¸»è¦æ–¹å¼ã€‚åœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œä»è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒä¸­å¯¹è‚ºéƒ¨æ„ŸæŸ“è¿›è¡Œåˆ†å‰²ï¼Œèƒ½å¤Ÿå®ç°COVID-19çš„å¿«é€Ÿå’Œå‡†ç¡®é‡åŒ–åŠè¯Šæ–­ã€‚COVID-19è‚ºéƒ¨æ„ŸæŸ“åˆ†å‰²é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç£¨ç»ç’ƒæ ·æ··æµŠè¡¨ç°çš„ç‰¹å¾è¾¹ç•Œä¸æ¸…å’Œå¯¹æ¯”åº¦æœ‰é™ã€‚æ­¤å¤–ï¼Œæµ¸æ¶¦ã€è‚ºç»„ç»‡å’Œè‚ºå£ä¹‹é—´çš„æ··æ·†ç›¸ä¼¼æ€§è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€åˆ†å‰²ä»»åŠ¡çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ·±åº¦ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºCAD-Unetï¼Œç”¨äºåˆ†å‰²COVID-19è‚ºéƒ¨æ„ŸæŸ“ã€‚è¯¥æ¶æ„ç»“åˆäº†ç°æœ‰çš„Unetæ¡†æ¶å¹¶èå…¥äº†èƒ¶å›Šç½‘ç»œã€‚èƒ¶å›Šç½‘ç»œä»£è¡¨äº†ä¸€ç§ä¸åŒäºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œçš„æ–°å‹ç½‘ç»œæ¶æ„ã€‚å®ƒä»¬ä½¿ç”¨å‘é‡è¿›è¡Œèƒ¶å›Šä¹‹é—´çš„ä¿¡æ¯ä¼ é€’ï¼Œä¾¿äºæå–å¤æ‚çš„ç—…å˜ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªèƒ¶å›Šç¼–ç å™¨è·¯å¾„ï¼Œå¹¶åœ¨Unetç¼–ç å™¨å’Œèƒ¶å›Šç¼–ç å™¨ä¹‹é—´å»ºç«‹äº†ä¸€æ¡è€¦åˆè·¯å¾„ã€‚è¿™ä¸€è®¾è®¡æœ€å¤§é™åº¦åœ°å‘æŒ¥äº†ä¸¤ç§ç½‘ç»œç»“æ„çš„äº’è¡¥ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¿¡æ¯èåˆã€‚æœ€åï¼Œåœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬äºŒå…ƒåˆ†å‰²ä»»åŠ¡å’Œå¤šç±»åˆ†å‰²ä»»åŠ¡ã€‚å®éªŒç»“æœè¯æ˜äº†æ‰€æå‡ºæ¨¡å‹çš„å“è¶Šåˆ†å‰²æ€§èƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/AmanoTooko-jie/CAD-Unet%E3%80%82]">https://github.com/AmanoTooko-jie/CAD-Unetã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06314v2">PDF</a> Published in Medical Image Analysis, Volume 103, 2025, Pages 103583.   DOI: 10.1016&#x2F;j.media.2025.103583 This is the authorâ€™s pre-print version prior   to final journal edits. Final published version available at:   <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S1361841525001306">https://www.sciencedirect.com/science/article/pii/S1361841525001306</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†COVID-19ç–«æƒ…æœŸé—´ï¼ŒåŒ»å­¦æˆåƒæˆä¸ºè¯Šæ–­æ–°å† è‚ºç‚çš„ä¸»è¦æ‰‹æ®µä¹‹ä¸€ã€‚æ–‡ä¸­æå‡ºä¸€ç§æ–°çš„æ·±åº¦ç½‘ç»œæ¶æ„CAD-Unetï¼Œç”¨äºåˆ†å‰²è‚ºéƒ¨COVID-19æ„ŸæŸ“åŒºåŸŸã€‚è¯¥æ¶æ„ç»“åˆäº†èƒ¶å›Šç½‘ç»œä¸ä¼ ç»ŸUnetæ¡†æ¶ï¼Œæé«˜äº†ä¿¡æ¯èåˆæ•ˆç‡ï¼Œå®ç°äº†è‚ºéƒ¨æ„ŸæŸ“åŒºåŸŸçš„ç²¾å‡†åˆ†å‰²ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒäºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>COVID-19ç–«æƒ…æœŸé—´ï¼ŒåŒ»å­¦æˆåƒæˆä¸ºè¯Šæ–­æ–°å† è‚ºç‚çš„ä¸»è¦æ‰‹æ®µä¹‹ä¸€ã€‚</li>
<li>è‚ºéƒ¨æ„ŸæŸ“åŒºåŸŸåˆ†å‰²æ˜¯è¯Šæ–­çš„å…³é”®æŒ‘æˆ˜ï¼Œå› è¾¹ç•Œæ¨¡ç³Šå’Œå¯¹æ¯”åº¦æœ‰é™ã€‚</li>
<li>å¼•å…¥æ–°å‹æ·±åº¦ç½‘ç»œæ¶æ„CAD-Unetï¼Œç»“åˆèƒ¶å›Šç½‘ç»œä¸Unetæ¡†æ¶è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>èƒ¶å›Šç½‘ç»œèƒ½æå–ç—…å˜çš„ç©ºé—´ä¿¡æ¯ï¼Œæé«˜ä¿¡æ¯èåˆæ•ˆç‡ã€‚</li>
<li>CAD-Unetå®ç°äº†é«˜æ•ˆçš„ä¿¡æ¯èåˆï¼Œå……åˆ†åˆ©ç”¨ä¸¤ç§ç½‘ç»œç»“æ„çš„ä¼˜åŠ¿ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e495820fe62e597a3552a724d80c4495.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a4fd009d38a49959512eee8d67b81c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6f8dfef10a6f2b9c3f8ece688ee2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-651d9ef2ff2c5cfbf8e67385da744205.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="A-Density-Functional-Theory-Study-of-Magnetic-Transition-in-MnO2-adsorbed-Vanadium-Carbide-V-2-C-MXene"><a href="#A-Density-Functional-Theory-Study-of-Magnetic-Transition-in-MnO2-adsorbed-Vanadium-Carbide-V-2-C-MXene" class="headerlink" title="A Density Functional Theory Study of Magnetic Transition in MnO2   adsorbed Vanadium Carbide (V$_2$C) MXene"></a>A Density Functional Theory Study of Magnetic Transition in MnO2   adsorbed Vanadium Carbide (V$_2$C) MXene</h2><p><strong>Authors:Mahjabeen Fatima, Saleem Ayaz Khan, Syed Rizwan</strong></p>
<p>The work reports nonmagnetic behavior (0.04 $\mu$B) in two-dimensional (2D) V2C-OF MXene and ferromagnetism in MnO$_2$ adsorbed V2C-OF MXene. The density functional theory (DFT) calculations were carried out to study the magnetic moments of V$_2$C-OF and MnO$_2$@V$_2$C-OF MXene. The MXene, which is derived from the exfoliation of its parent V$_2$AlC MAX phase, shows a good potential to be a ferromagnetic when MnO$_2$ is adsorbed on it. The V$_2$C MXene and MnO$_2$ adsorbed V$_2$C MXene were successfully synthesized, as characterized using X-ray diffraction, showing an increased c-lattice parameter from 22.6{\AA} to 27.2{\AA} after MnO$_2$ adsorption. The DFT study confirmed that MnO$_2$ adsorbed V$_2$C MXene changed from nonmagnetic (in V$_2$C MXene) to a strong ferromagnetic with a magnetic moment of 4.48$\mu$B for Mn adsorbed V$_2$C-OF MXene. The current work is a step-forward towards understanding of magnetism in two-dimensional materials for future 2D spintronics. </p>
<blockquote>
<p>è¿™ç¯‡è®ºæ–‡æŠ¥é“äº†äºŒç»´V2C-OF MXeneçš„éç£æ€§è¡Œä¸ºï¼ˆ0.04Î¼Bï¼‰ï¼Œä»¥åŠåœ¨MnO2å¸é™„çš„V2C-OF MXeneä¸Šè¡¨ç°å‡ºé“ç£æ€§ã€‚é€šè¿‡å¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è®¡ç®—ï¼Œç ”ç©¶äº†V2C-OFå’ŒMnO2@V2C-OF MXeneçš„ç£çŸ©ã€‚MXeneæ˜¯ä»å…¶æ¯ä½“V2AlC MAXç›¸å‰¥ç¦»è€Œæ¥ï¼Œå½“MnO2å¸é™„åœ¨å…¶ä¸Šæ—¶ï¼Œå…·æœ‰æˆä¸ºé“ç£ä½“çš„æ½œåŠ›ã€‚V2C MXeneå’ŒMnO2å¸é™„çš„V2C MXeneå·²æˆåŠŸåˆæˆï¼Œç»Xå°„çº¿è¡å°„è¡¨å¾ï¼ŒMnO2å¸é™„åcæ™¶æ ¼å‚æ•°ä»22.6Ã…å¢åŠ åˆ°27.2Ã…ã€‚DFTç ”ç©¶è¯å®ï¼ŒMnO2å¸é™„çš„V2C MXeneä»éç£æ€§ï¼ˆåœ¨V2C MXeneä¸­ï¼‰è½¬å˜ä¸ºå¼ºé“ç£æ€§ï¼Œå¯¹äºMnå¸é™„çš„V2C-OF MXeneï¼Œç£çŸ©ä¸º4.48Î¼Bã€‚å½“å‰çš„å·¥ä½œæ˜¯æœç€ç†è§£äºŒç»´ææ–™ç£æ€§çš„æ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥ï¼Œä¸ºæœªæ¥äºŒç»´è‡ªæ—‹ç”µå­å­¦çš„å‘å±•å¥ å®šåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09536v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†äºŒç»´Vâ‚‚C-OF MXeneçš„éç£æ€§è¡Œä¸ºå’ŒMnOâ‚‚å¸é™„åœ¨Vâ‚‚C-OF MXeneä¸Šçš„é“ç£æ€§ã€‚é€šè¿‡å¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è®¡ç®—äº†Vâ‚‚C-OFå’ŒMnOâ‚‚@Vâ‚‚C-OF MXeneçš„ç£çŸ©ã€‚MXeneä»å…¶æ¯ä½“Vâ‚‚AlC MAXç›¸ä¸­å‰¥ç¦»å‡ºæ¥ï¼Œå½“MnOâ‚‚å¸é™„åœ¨å…¶ä¸Šæ—¶ï¼Œå…·æœ‰æˆä¸ºé“ç£ä½“çš„æ½œåŠ›ã€‚æˆåŠŸåˆæˆVâ‚‚C MXeneå’ŒMnOâ‚‚å¸é™„çš„Vâ‚‚C MXeneï¼Œç»Xå°„çº¿è¡å°„è¡¨å¾ï¼ŒMnOâ‚‚å¸é™„åcæ™¶æ ¼å‚æ•°ä»22.6Ã…å¢åŠ åˆ°27.2Ã…ã€‚DFTç ”ç©¶è¡¨æ˜ï¼ŒMnOâ‚‚å¸é™„çš„Vâ‚‚C MXeneä»éç£æ€§ï¼ˆåœ¨Vâ‚‚C MXeneä¸­ï¼‰å˜ä¸ºå¼ºé“ç£æ€§ï¼Œç£çŸ©ä¸ºMnå¸é™„çš„Vâ‚‚C-OF MXeneä¸º4.48Î¼Bã€‚è¿™é¡¹å·¥ä½œæ˜¯ç†è§£äºŒç»´ææ–™ç£æ€§çš„é‡è¦ä¸€æ­¥ï¼Œä¸ºæœªæ¥äºŒç»´è‡ªæ—‹ç”µå­å­¦æä¾›äº†å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vâ‚‚C-OF MXeneè¡¨ç°å‡ºéç£æ€§è¡Œä¸ºï¼Œè€ŒMnOâ‚‚å¸é™„åœ¨å…¶ä¸Šæ—¶è¡¨ç°å‡ºé“ç£æ€§ã€‚</li>
<li>é€šè¿‡å¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰è®¡ç®—äº†Vâ‚‚C-OFå’ŒMnOâ‚‚@Vâ‚‚C-OF MXeneçš„ç£çŸ©ã€‚</li>
<li>MXeneæ˜¯ä»Vâ‚‚AlC MAXç›¸å‰¥ç¦»å¾—åˆ°çš„ï¼Œå…·æœ‰æ½œåœ¨çš„é“ç£æ€§ã€‚</li>
<li>æˆåŠŸåˆæˆVâ‚‚C MXeneå’ŒMnOâ‚‚å¸é™„çš„Vâ‚‚C MXeneï¼Œå¹¶é€šè¿‡Xå°„çº¿è¡å°„è¿›è¡Œäº†è¡¨å¾ã€‚</li>
<li>MnOâ‚‚å¸é™„åï¼Œcæ™¶æ ¼å‚æ•°æœ‰æ‰€å¢åŠ ã€‚</li>
<li>DFTç ”ç©¶æ˜¾ç¤ºMnOâ‚‚å¸é™„çš„Vâ‚‚C MXeneå…·æœ‰å¼ºé“ç£æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1f9cfa6cb6ced2376d3a44b1d908652.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c75df3726a543afcefdb6acef3b18df0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-639da898236ffe6200b602658a9cab67.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SmoothSegNet-A-Global-Local-Framework-for-Liver-Tumor-Segmentation-with-Clinical-KnowledgeInformed-Label-Smoothing"><a href="#SmoothSegNet-A-Global-Local-Framework-for-Liver-Tumor-Segmentation-with-Clinical-KnowledgeInformed-Label-Smoothing" class="headerlink" title="SmoothSegNet: A Global-Local Framework for Liver Tumor Segmentation with   Clinical KnowledgeInformed Label Smoothing"></a>SmoothSegNet: A Global-Local Framework for Liver Tumor Segmentation with   Clinical KnowledgeInformed Label Smoothing</h2><p><strong>Authors:Hairong Wang, Lingchao Mao, Zihan Zhang, Jing Li</strong></p>
<p>Liver cancer is a leading cause of mortality worldwide, and accurate Computed Tomography (CT)-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present SmoothSegNet, a novel deep learning framework that addresses these challenges with the three key designs: (1) A novel knowledge-informed label smoothing technique that distills knowledge from clinical data to generate smooth labels, which are used to regularize model training, reducing the overfitting risk and enhancing model performance; (2) A global and local segmentation framework that breaks down the main task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask aimed to enhance tumor visibility and refines tumor boundaries. We apply the proposed model on a challenging HCC-TACE-Seg dataset and show that SmoothSegNet outperformed various benchmarks in segmentation performance, particularly at smaller tumors (&lt;10cm). Our ablation studies show that the three design components complementarily contribute to the model improved performance. Code for the proposed method are available at <a target="_blank" rel="noopener" href="https://github.com/lingchm/medassist-liver-cancer">https://github.com/lingchm/medassist-liver-cancer</a>. </p>
<blockquote>
<p>è‚ç™Œæ˜¯å…¨çƒä¸»è¦çš„è‡´æ­»åŸå› ä¹‹ä¸€ï¼Œè€ŒåŸºäºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„ç²¾ç¡®è‚¿ç˜¤åˆ†å‰²å¯¹äºè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚æ‰‹åŠ¨æç»˜æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºç°åå·®ï¼Œè¿™çªæ˜¾äº†å¯é è‡ªåŠ¨åŒ–éœ€æ±‚çš„å¿…è¦æ€§ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨è‚è„åˆ†å‰²çš„è‡ªåŠ¨åŒ–æ–¹é¢å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºè‚¿ç˜¤çš„å¼‚è´¨æ€§ã€è‚¿ç˜¤è¾¹ç•Œçš„ä¸ç²¾ç¡®ä»¥åŠæ ‡è®°æ•°æ®çš„æœ‰é™æ€§ï¼Œç²¾ç¡®çš„è‚è„è‚¿ç˜¤åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†SmoothSegNetï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®è®¾è®¡æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä¸€ç§æ–°å‹çš„çŸ¥è¯†ä¿¡æ¯æ ‡ç­¾å¹³æ»‘æŠ€æœ¯ï¼Œä»ä¸´åºŠæ•°æ®ä¸­æç‚¼çŸ¥è¯†ä»¥ç”Ÿæˆå¹³æ»‘æ ‡ç­¾ï¼Œç”¨äºè§„èŒƒæ¨¡å‹è®­ç»ƒï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ï¼›ï¼ˆ2ï¼‰å…¨å±€å’Œå±€éƒ¨åˆ†å‰²æ¡†æ¶å°†ä¸»è¦ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªæ›´ç®€å•çš„å­ä»»åŠ¡ï¼Œå…è®¸é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œä¼˜åŒ–é¢„å¤„ç†å’Œè®­ç»ƒï¼›ï¼ˆ3ï¼‰é’ˆå¯¹æ¯ä¸ªå­ä»»åŠ¡æŒ‘æˆ˜çš„é¢„å¤„ç†å’Œåå¤„ç†ç®¡é“ï¼Œæ—¨åœ¨æé«˜è‚¿ç˜¤å¯è§æ€§å¹¶ç»†åŒ–è‚¿ç˜¤è¾¹ç•Œã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„HCC-TACE-Segæ•°æ®é›†ä¸Šåº”ç”¨æ‰€æå‡ºçš„æ¨¡å‹ï¼Œå¹¶è¯æ˜SmoothSegNetåœ¨åˆ†å‰²æ€§èƒ½æ–¹é¢è¶…è¿‡äº†å„ç§åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒå°çš„è‚¿ç˜¤ï¼ˆ&lt;10cmï¼‰ä¸Šã€‚æˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œä¸‰ä¸ªè®¾è®¡ç»„ä»¶ä»¥äº’è¡¥çš„æ–¹å¼ä¿ƒè¿›äº†æ¨¡å‹æ€§èƒ½çš„æé«˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lingchm/medassist-liver-cancer%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lingchm/medassist-liver-cancerä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10005v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‚è„è‚¿ç˜¤åˆ†å‰²æ–°æ–¹æ³•SmoothSegNetï¼Œè¯¥æ–¹æ³•é€šè¿‡é‡‡ç”¨çŸ¥è¯†æŒ‡å¯¼çš„æ ‡ç­¾å¹³æ»‘æŠ€æœ¯ã€å…¨å±€ä¸å±€éƒ¨åˆ†å‰²æ¡†æ¶ä»¥åŠé’ˆå¯¹å­ä»»åŠ¡çš„é¢„å¤„ç†å’Œåå¤„ç†ç®¡é“ï¼Œè§£å†³äº†æ‰‹åŠ¨åˆ†å‰²è€—æ—¶ã€æ˜“å‡ºé”™å’Œç¼ºä¹å¯é è‡ªåŠ¨åŒ–çš„é—®é¢˜ã€‚åœ¨HCC-TACE-Segæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSmoothSegNetåœ¨åˆ†å‰²æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å°äº10å˜ç±³çš„è‚¿ç˜¤ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmoothSegNetæ˜¯ä¸€ç§ç”¨äºè‚è„è‚¿ç˜¤åˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰‹åŠ¨åˆ†å‰²çš„è€—æ—¶ã€æ˜“é”™å’Œç¼ºä¹è‡ªåŠ¨åŒ–çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨äº†çŸ¥è¯†æŒ‡å¯¼çš„æ ‡ç­¾å¹³æ»‘æŠ€æœ¯ï¼Œé€šè¿‡ä¸´åºŠæ•°æ®çš„çŸ¥è¯†è’¸é¦ç”Ÿæˆå¹³æ»‘æ ‡ç­¾ï¼Œä»¥è§„èŒƒåŒ–æ¨¡å‹è®­ç»ƒï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SmoothSegNeté‡‡ç”¨å…¨å±€å’Œå±€éƒ¨åˆ†å‰²æ¡†æ¶ï¼Œå°†ä¸»è¦ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªæ›´ç®€å•çš„å­ä»»åŠ¡ï¼Œä»¥ä¾¿å¯¹æ¯ä¸ªå­ä»»åŠ¡è¿›è¡Œä¼˜åŒ–çš„é¢„å¤„ç†å’Œè®­ç»ƒã€‚</li>
<li>è¯¥æ–¹æ³•è¿˜åŒ…å«é’ˆå¯¹å­ä»»åŠ¡çš„é¢„å¤„ç†å’Œåå¤„ç†ç®¡é“ï¼Œæ—¨åœ¨æé«˜è‚¿ç˜¤å¯è§æ€§å¹¶ä¼˜åŒ–è‚¿ç˜¤è¾¹ç•Œã€‚</li>
<li>åœ¨HCC-TACE-Segæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSmoothSegNetåœ¨åˆ†å‰²æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚</li>
<li>ç‰¹åˆ«åœ°ï¼Œè¯¥æ–¹æ³•åœ¨å°äº10å˜ç±³çš„è‚¿ç˜¤ä¸Šçš„è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1640145918feeb507ef563853b7cb7fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95147d1c876c269539e120410d96ed1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01dc1a6d1081ca2559c9ca5aaa27cd9d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MeDSLIP-Medical-Dual-Stream-Language-Image-Pre-training-with-Pathology-Anatomy-Semantic-Alignment"><a href="#MeDSLIP-Medical-Dual-Stream-Language-Image-Pre-training-with-Pathology-Anatomy-Semantic-Alignment" class="headerlink" title="MeDSLIP: Medical Dual-Stream Language-Image Pre-training with   Pathology-Anatomy Semantic Alignment"></a>MeDSLIP: Medical Dual-Stream Language-Image Pre-training with   Pathology-Anatomy Semantic Alignment</h2><p><strong>Authors:Wenrui Fan, Mohammod N. I. Suvon, Shuo Zhou, Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping Lu</strong></p>
<p>Pathology and anatomy are two essential groups of semantics in medical data. Pathology describes what the diseases are, while anatomy explains where the diseases occur. They describe diseases from different perspectives, providing complementary insights into diseases. Thus, properly understanding these semantics and their relationships can enhance medical vision-language models (VLMs). However, pathology and anatomy semantics are usually entangled in medical data, hindering VLMs from explicitly modeling these semantics and their relationships. To address this challenge, we propose MeDSLIP, a novel Medical Dual-Stream Language-Image Pre-training pipeline, to disentangle pathology and anatomy semantics and model the relationships between them. We introduce a dual-stream mechanism in MeDSLIP to explicitly disentangle medical semantics into pathology-relevant and anatomy-relevant streams and align visual and textual information within each stream. Furthermore, we propose an interaction modeling module with prototypical contrastive learning loss and intra-image contrastive learning loss to regularize the relationships between pathology and anatomy semantics. We apply MeDSLIP to chest X-ray analysis and conduct comprehensive evaluations with four benchmark datasets: NIH CXR14, RSNA Pneumonia, SIIM-ACR Pneumothorax, and COVIDx CXR-4. The results demonstrate MeDSLIPâ€™s superior generalizability and transferability across different scenarios. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Shef-AIRE/MeDSLIP">https://github.com/Shef-AIRE/MeDSLIP</a>, and the pre-trained model is released at <a target="_blank" rel="noopener" href="https://huggingface.co/pykale/MeDSLIP">https://huggingface.co/pykale/MeDSLIP</a>. </p>
<blockquote>
<p>ç—…ç†å­¦å’Œè§£å‰–å­¦æ˜¯åŒ»ç–—æ•°æ®ä¸­ä¸¤ä¸ªé‡è¦çš„è¯­ä¹‰ç±»åˆ«ã€‚ç—…ç†å­¦æè¿°ç–¾ç—…çš„æœ¬è´¨ï¼Œè€Œè§£å‰–å­¦è§£é‡Šç–¾ç—…å‘ç”Ÿçš„éƒ¨ä½ã€‚å®ƒä»¬ä»ä¸åŒè§’åº¦æè¿°ç–¾ç—…ï¼Œä¸ºç–¾ç—…ç ”ç©¶æä¾›äº’è¡¥çš„è§è§£ã€‚å› æ­¤ï¼Œæ­£ç¡®ç†è§£è¿™äº›è¯­ä¹‰åŠå…¶å…³ç³»å¯ä»¥å¢å¼ºåŒ»ç–—è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç—…ç†å­¦å’Œè§£å‰–å­¦è¯­ä¹‰é€šå¸¸äº¤ç»‡åœ¨åŒ»ç–—æ•°æ®ä¸­ï¼Œé˜»ç¢VLMsæ˜ç¡®åœ°å¯¹è¿™äº›è¯­ä¹‰åŠå…¶å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MeDSLIPï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åŒ»ç–—åŒæµè¯­è¨€-å›¾åƒé¢„è®­ç»ƒç®¡é“ï¼Œç”¨äºåˆ†ç¦»ç—…ç†å­¦å’Œè§£å‰–å­¦è¯­ä¹‰ï¼Œå¹¶å»ºæ¨¡å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬åœ¨MeDSLIPä¸­å¼•å…¥äº†åŒæµæœºåˆ¶ï¼Œå°†åŒ»ç–—è¯­ä¹‰æ˜ç¡®åœ°åˆ†ç¦»ä¸ºä¸ç—…ç†å­¦ç›¸å…³çš„æµå’Œä¸è§£å‰–å­¦ç›¸å…³çš„æµï¼Œå¹¶åœ¨æ¯ä¸ªæµå†…å¯¹é½è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªäº¤äº’å»ºæ¨¡æ¨¡å—ï¼Œé‡‡ç”¨åŸå‹å¯¹æ¯”å­¦ä¹ æŸå¤±å’Œå›¾åƒå†…å¯¹æ¯”å­¦ä¹ æŸå¤±æ¥è§„èŒƒç—…ç†å­¦å’Œè§£å‰–å­¦è¯­ä¹‰ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬å°†MeDSLIPåº”ç”¨äºèƒ¸éƒ¨Xå°„çº¿åˆ†æï¼Œå¹¶ä½¿ç”¨å››ä¸ªåŸºå‡†æ•°æ®é›†è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼šNIH CXR14ã€RSNAè‚ºç‚ã€SIIM-ACRæ°”èƒ¸å’ŒCOVIDx CXR-4ã€‚ç»“æœè¡¨æ˜ï¼ŒMeDSLIPåœ¨ä¸åŒåœºæ™¯ä¸­å…·æœ‰å‡ºè‰²çš„é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Shef-AIRE/MeDSLIP%E6%89%BE%E5%88%B0%EF%BC%8CHttps:%E4%BA%9C%E9%A6%ACface.co%2Fpykale%2FMeDSLIP%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Shef-AIRE/MeDSLIPæ‰¾åˆ°ï¼Œé¢„è®­ç»ƒæ¨¡å‹å·²åœ¨https://huggingface.co/pykale/MeDSLIPå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.10635v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦æ•°æ®ä¸­çš„ç—…ç†å­¦ä¸è§£å‰–å­¦è¯­ä¹‰æ˜¯ç–¾ç—…æè¿°çš„ä¸¤ä¸ªé‡è¦æ–¹é¢ï¼Œåˆ†åˆ«å…³æ³¨ç–¾ç—…çš„æœ¬è´¨å’Œå‘ç”Ÿä½ç½®ã€‚äºŒè€…ç»“åˆæä¾›äº†äº’è¡¥çš„ç–¾ç—…æ´å¯Ÿã€‚ä¸ºäº†å¢å¼ºåŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹è¿™ä¸¤è€…åŠå…³ç³»çš„ç†è§£ï¼Œæˆ‘ä»¬é¢ä¸´è¯­ä¹‰çº ç¼ çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºMeDSLIPï¼Œä¸€ç§æ–°é¢–çš„åŒ»ç–—åŒæµè¯­è¨€å›¾åƒé¢„è®­ç»ƒæµç¨‹ï¼Œç”¨äºè§£æç—…ç†å­¦ä¸è§£å‰–å­¦è¯­ä¹‰å¹¶å»ºæ¨¡å…¶å…³ç³»ã€‚å®ƒé‡‡ç”¨åŒæµæœºåˆ¶ï¼Œå°†åŒ»å­¦è¯­ä¹‰åˆ†ä¸ºä¸ç—…ç†å­¦ç›¸å…³å’Œä¸è§£å‰–å­¦ç›¸å…³çš„æµï¼Œå¹¶åœ¨æ¯ä¸ªæµå†…å¯¹é½è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŸå‹å¯¹æ¯”å­¦ä¹ æŸå¤±å’Œå›¾åƒå†…å¯¹æ¯”å­¦ä¹ æŸå¤±æå‡ºäº†äº¤äº’å»ºæ¨¡æ¨¡å—æ¥è§„èŒƒä¸¤è€…ä¹‹é—´çš„å…³ç³»ã€‚åœ¨èƒ¸éƒ¨Xå…‰åˆ†æä¸­ï¼ŒMeDSLIPåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°è¯æ˜äº†å…¶å‡ºè‰²çš„é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ã€‚ä»£ç ä¸é¢„è®­ç»ƒæ¨¡å‹å·²å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å­¦å’Œè§£å‰–å­¦æ˜¯åŒ»å­¦æ•°æ®ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒè¯­ä¹‰é¢†åŸŸï¼Œåˆ†åˆ«å…³æ³¨ç–¾ç—…çš„æœ¬è´¨å’Œå‘ç”Ÿéƒ¨ä½ï¼Œä¸¤è€…æä¾›å¯¹ç–¾ç—…çš„äº’è¡¥è§†è§’ã€‚</li>
<li>ç°æœ‰çš„åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å’Œå»ºæ¨¡è¿™äº›è¯­ä¹‰åŠå…¶å…³ç³»æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯ç—…ç†å­¦å’Œè§£å‰–å­¦è¯­ä¹‰åœ¨åŒ»å­¦æ•°æ®ä¸­çš„çº ç¼ ã€‚</li>
<li>MeDSLIPé€šè¿‡åŒæµæœºåˆ¶è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°†åŒ»å­¦è¯­ä¹‰åˆ†ä¸ºç—…ç†ç›¸å…³å’Œè§£å‰–ç›¸å…³ä¸¤ä¸ªæµï¼Œå¹¶åœ¨æ¯ä¸ªæµå†…å¯¹é½è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>MeDSLIPè¿˜åŒ…æ‹¬ä¸€ä¸ªäº¤äº’å»ºæ¨¡æ¨¡å—ï¼Œä½¿ç”¨åŸå‹å¯¹æ¯”å­¦ä¹ æŸå¤±å’Œå›¾åƒå†…å¯¹æ¯”å­¦ä¹ æŸå¤±æ¥è§„èŒƒç—…ç†ä¸è§£å‰–å­¦è¯­ä¹‰ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>MeDSLIPåœ¨èƒ¸éƒ¨Xå…‰åˆ†æé¢†åŸŸè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ã€‚</li>
<li>MeDSLIPçš„ä»£ç å·²å…¬å¼€å¯ç”¨ï¼Œä¸ºç”¨æˆ·æä¾›å®ç°ç»†èŠ‚å’Œè‡ªå®šä¹‰æ¨¡å‹çš„æœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.10635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bea1f0432b096688d4198e71267f412d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff66c40faaf376541b9e20e91eaa9732.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83a786a9170e5a4882886c403f20549b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2adc12566bb1d5990fce5867d1e4ac79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335af4ff2d958438f68567a9380b982.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8438f937d5ac6c6783a8c722d0cdca43.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  Sadeed Advancing Arabic Diacritization Through Small Language Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e403b5782e7295d29c441e83aa07c4aa.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  HoloTime Taming Video Diffusion Models for Panoramic 4D Scene   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
