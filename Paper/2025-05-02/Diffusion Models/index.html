<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-05-02  HoloTime Taming Video Diffusion Models for Panoramic 4D Scene   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e403b5782e7295d29c441e83aa07c4aa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-02-更新"><a href="#2025-05-02-更新" class="headerlink" title="2025-05-02 更新"></a>2025-05-02 更新</h1><h2 id="HoloTime-Taming-Video-Diffusion-Models-for-Panoramic-4D-Scene-Generation"><a href="#HoloTime-Taming-Video-Diffusion-Models-for-Panoramic-4D-Scene-Generation" class="headerlink" title="HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene   Generation"></a>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene   Generation</h2><p><strong>Authors:Haiyang Zhou, Wangbo Yu, Jiawen Guan, Xinhua Cheng, Yonghong Tian, Li Yuan</strong></p>
<p>The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method’s capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications. </p>
<blockquote>
<p>扩散模型的快速发展有望彻底改变VR和AR技术的应用前景，这些技术通常需要场景级的四维资产来提升用户体验。然而，现有的扩散模型主要集中在静态三维场景的建模或对象级别的动态建模上，这限制了它们提供真正沉浸式体验的能力。为了解决这个问题，我们提出了HoloTime框架，它结合了视频扩散模型来生成全景视频，从一个单一的提示或参考图像开始，并使用一种全景的4D场景重建方法，将生成的全景视频无缝地转化为四维资产，为用户带来全面的沉浸式体验。具体来说，为了驾驭视频扩散模型以生成高质量全景视频，我们引入了首个全景视频数据集360World数据集，适合用于下游四维场景重建任务。有了这个定制的数据集，我们提出了全景动画生成器（Panoramic Animator），一个两阶段的图像到视频的扩散模型，能够将全景图像转换为高质量全景视频。随后，我们提出了全景时空重建（Panoramic Space-Time Reconstruction），利用时空深度估计方法将生成的全景视频转化为四维点云，并优化整体的四维高斯溅射表示法来重建空间和时间上连贯的四维场景。为了验证我们方法的有效性，我们与现有方法进行了比较分析，结果显示我们的方法在全景视频生成和四维场景重建方面都表现出卓越的性能。这表明我们的方法能够创建更具吸引力和更逼真的沉浸式环境，从而增强VR和AR应用程序中的用户体验。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21650v1">PDF</a> Project homepage: <a target="_blank" rel="noopener" href="https://zhouhyocean.github.io/holotime/">https://zhouhyocean.github.io/holotime/</a></p>
<p><strong>Summary</strong></p>
<p>扩散模型的快速发展为VR和AR技术的广泛应用带来了革命性的希望。现有的扩散模型主要关注静态三维场景或对象级别的动态建模，难以提供真正的沉浸式体验。本文提出HoloTime框架，通过整合视频扩散模型生成全景视频，并采用360度四维场景重建方法，将生成的全景视频无缝转换为四维资产，为用户带来全新的沉浸式四维体验。为了生成高质量的全景视频，引入了360World数据集，并提出全景动画师（Panoramic Animator）模型。接着使用时空重建技术将全景视频转化为四维点云，最终优化整体四维高斯描绘（Gaussian Splatting）来实现空间和时间上一致的场景重建。本研究验证了所提出方法在全景视频生成和四维场景重建方面的优势，增强了VR和AR应用的用户体验。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型的进步为VR和AR技术的沉浸式体验提供了巨大的潜力和机遇。</li>
<li>当前扩散模型主要集中在静态三维场景和对象级别的动态建模上，难以满足真正的沉浸式需求。</li>
<li>HoloTime框架整合视频扩散模型生成全景视频，实现单一提示或参考图像的场景重建。</li>
<li>引入的360World数据集为全景视频生成提供了高质量资源。</li>
<li>提出全景动画师（Panoramic Animator）模型，用于将全景图像转换为高质量全景视频。</li>
<li>利用时空重建技术将全景视频转化为四维点云，进一步优化了四维场景重建的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-108d8be07258f28f73c4a9781b72eae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404094d08fe4de07420ee6d582f758d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c94392690dc8358107d9f8dd554658.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cc9ac35ee0db72fbe3d4e35aa6b32bf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-Adversarial-Identity-Manipulation-for-Facial-Privacy-Protection"><a href="#Diffusion-based-Adversarial-Identity-Manipulation-for-Facial-Privacy-Protection" class="headerlink" title="Diffusion-based Adversarial Identity Manipulation for Facial Privacy   Protection"></a>Diffusion-based Adversarial Identity Manipulation for Facial Privacy   Protection</h2><p><strong>Authors:Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</strong></p>
<p>The success of face recognition (FR) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. Existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. In this paper, we propose diffusion-based adversarial identity manipulation (DiffAIM) to generate natural and highly transferable adversarial faces against malicious FR systems. To be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. This involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. The guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. We further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. Extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger black-box attack transferability while maintaining superior visual quality. We also demonstrate the effectiveness of the proposed approach for commercial FR APIs, including Face++ and Aliyun. </p>
<blockquote>
<p>人脸识别（FR）系统的成功引发了严重的隐私担忧，因为可能存在未经授权的监督和社会网络上的用户跟踪。现有的增强隐私的方法无法生成能够保护面部隐私的自然面部图像。在本文中，我们提出了基于扩散的对抗性身份操纵（DiffAIM）方法，生成自然且高度可转移的对抗性面部图像，以对抗恶意FR系统。具体来说，我们在扩散模型的低维潜在空间内对身份进行操纵。这涉及在反向扩散过程中迭代注入基于梯度的对抗性身份指导，逐步引导生成走向目标对抗性面部图像。指导优化旨在使身份收敛于目标，同时促进与源之间的语义发散，从而在保持视觉自然性的同时实现有效的伪装。我们进一步引入了结构保持正则化，以保持面部结构一致性在操纵过程中。对面部验证和识别任务的广泛实验表明，与最新技术相比，DiffAIM实现了更强的黑盒攻击转移性，同时保持了卓越的可视质量。我们还证明了所提出方法在包括Face++和阿里云在内的商业FR API上的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21646v1">PDF</a> </p>
<p><strong>Summary</strong><br>人脸识别系统的成功引发了关于个人隐私的严重关注，因为存在潜在的未经授权监控和用户社交网络追踪。现有的隐私保护方法无法生成能保护面部隐私的自然面部图像。本研究提出了一种基于扩散对抗身份操控（DiffAIM）的方法，用于生成对抗性面部图像，以抵御恶意人脸识别系统。DiffAIM在低维潜在空间内操纵面部身份，并在反向扩散过程中注入基于梯度的对抗性身份指导，逐步引导生成图像朝向目标对抗性面部。此方法优化了身份收敛至目标的同时，促进与源图像的语义差异，以实现有效伪装并维持视觉自然性。此外，还融入了结构保持正则化，以保持面部结构一致性。实验表明，相较于其他顶尖方法，DiffAIM在面部验证和识别任务上实现了更强的黑盒攻击转移能力，同时保持了卓越视觉质量。该策略对包括Face++和阿里云在内的人脸识别API同样有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人脸识别系统的成功引发了关于隐私的重大担忧，特别是未经授权的监控和社交网络追踪。</li>
<li>现有隐私保护方法无法自然生成能保护面部隐私的图像。</li>
<li>DiffAIM方法基于扩散模型，在低维潜在空间内操控面部身份。</li>
<li>DiffAIM通过注入梯度基于的对抗性身份指导，逐步生成对抗性面部图像。</li>
<li>DiffAIM在优化身份收敛至目标的同时，促进与源图像的语义差异，维持视觉自然性。</li>
<li>结构保持正则化被用于保持面部结构一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-94eb472108f8f00e94d5c6e05491eb21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3d82d52452e5f49bfc0f3a224c873176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58cc7ead29eb1b6f99da38c2be1ad145.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c3ffd0ffde01cea604bc3781c6b8a48.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Latent-Feature-Guided-Conditional-Diffusion-for-High-Fidelity-Generative-Image-Semantic-Communication"><a href="#Latent-Feature-Guided-Conditional-Diffusion-for-High-Fidelity-Generative-Image-Semantic-Communication" class="headerlink" title="Latent Feature-Guided Conditional Diffusion for High-Fidelity Generative   Image Semantic Communication"></a>Latent Feature-Guided Conditional Diffusion for High-Fidelity Generative   Image Semantic Communication</h2><p><strong>Authors:Zehao Chen, Xinfeng Wei, Haonan Tong, Zhaohui Yang, Changchuan Yin</strong></p>
<p>Semantic communication is proposed and expected to improve the efficiency and effectiveness of massive data transmission over sixth generation (6G) networks. However, existing deep learning-based joint source and channel coding (DeepJSCC) image semantic communication scheme predominantly focuses on optimizing pixel-level metrics, and neglects human perceptual requirements, which results in degraded perceptual quality. To address this issue, we propose a latent representation-oriented image semantic communication (LRISC) system, which transmits latent semantic features for image generation with semantic consistency, thereby ensuring the perceptual quality at the receiver. In particular, we first map the source image to latent features in a high-dimensional semantic space via a neural network (NN)- based non-linear transformation. Subsequently, these features are encoded using a joint source and channel coding (JSCC) scheme with adaptive coding length for efficient transmission over a wireless channel. At the receiver, a conditional diffusion model is developed by using the received latent features as conditional guidance to steer the reverse diffusion process, progressively reconstructing high-fidelity images while preserving semantic consistency. Moreover, we introduce a channel signal-to-noise ratio (SNR) adaptation mechanism, allowing one model to work across various channel states. Experiments show that the proposed method significantly outperforms existing methods, in terms of learned perceptual image patch similarity (LPIPS) and robustness against channel noise, with an average LPIPS reduction of 43.3% compared to DeepJSCC, while guaranteeing the semantic consistency. </p>
<blockquote>
<p>语义通信旨在提高第六代（6G）网络上大规模数据传输的效率和效果。然而，现有的基于深度学习的联合源信道编码（DeepJSCC）图像语义通信方案主要关注像素级的指标优化，忽视了人类感知的需求，导致感知质量下降。为了解决这一问题，我们提出了一种面向潜在表示的图像语义通信（LRISC）系统，该系统传输用于图像生成的潜在语义特征，以确保接收端的感知质量具有语义一致性。具体而言，我们首先将源图像映射到高维语义空间中的潜在特征，通过神经网络（NN）进行非线性变换。随后，这些特征使用具有自适应编码长度的联合源信道编码（JSCC）方案进行编码，以便在无线信道上进行有效传输。在接收端，我们利用接收到的潜在特征作为条件指导，开发了一种条件扩散模型，引导反向扩散过程，逐步重建高质量图像，同时保持语义一致性。此外，我们还引入了一种信道信噪比（SNR）自适应机制，使模型能够在各种信道状态下工作。实验表明，所提方法在学习的感知图像补丁相似性（LPIPS）和对抗信道噪声的稳健性方面都显著优于现有方法，与DeepJSCC相比，平均LPIPS降低了43.3%，同时保证了语义一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21577v1">PDF</a> 6 pages, 6 figures</p>
<p><strong>摘要</strong><br>    针对第六代（6G）网络中的大规模数据传输，提出语义通信能提高传输效率和效果。然而，现有的基于深度学习的联合源信道编码（DeepJSCC）图像语义通信方案主要关注像素级指标优化，忽视了人类感知需求，导致感知质量下降。为解决这一问题，我们提出了一个基于潜在表示的图像语义通信系统（LRISC），通过传输潜在语义特征来生成图像，确保接收端的感知质量。具体而言，我们首先将源图像映射到高维语义空间的潜在特征上，然后通过联合源信道编码进行编码以适应无线信道的传输。在接收端，利用接收到的潜在特征作为条件指导，发展了一种条件扩散模型，以逆向扩散过程逐步重建高质量图像并保持语义一致性。此外，我们引入了信道信噪比（SNR）自适应机制，使模型能适应各种信道状态。实验表明，相较于DeepJSCC，所提方法在感知图像补丁相似性（LPIPS）上平均降低了43.3%，在抵抗信道噪声方面更加稳健，同时保证了语义一致性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>语义通信在6G网络中对于大规模数据传输的效率与效果提升至关重要。</li>
<li>现有DeepJSCC方案主要关注像素级优化，忽视了人类感知需求，造成感知质量下降。</li>
<li>LRISC系统通过传输潜在语义特征来确保图像的语义一致性和接收端的感知质量。</li>
<li>源图像首先被映射到高维语义空间的潜在特征上，然后通过联合源信道编码进行编码以适应无线传输。</li>
<li>接收端采用条件扩散模型，利用潜在特征逐步重建高质量图像。</li>
<li>引入信道SNR自适应机制，增强模型对各种信道状态的适应性。</li>
<li>实验显示LRISC在LPIPS指标上显著优于DeepJSCC，且更抗信道噪声干扰。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f5dc5e5fdd60a21cd2a12c32814006b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd9390a281ba11735752e5850746e20c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23a8abc6e19c7394ba9a3f1443f4f237.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f9eeca927664cf00d1efd7050ea90ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e403b5782e7295d29c441e83aa07c4aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f27ffbd80653c75f77a8cdc3e61ac46.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MagicPortrait-Temporally-Consistent-Face-Reenactment-with-3D-Geometric-Guidance"><a href="#MagicPortrait-Temporally-Consistent-Face-Reenactment-with-3D-Geometric-Guidance" class="headerlink" title="MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric   Guidance"></a>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric   Guidance</h2><p><strong>Authors:Mengting Wei, Yante Li, Tuomas Varanka, Yan Jiang, Licai Sun, Guoying Zhao</strong></p>
<p>In this paper, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This enables precise extraction of detailed face geometry and motion features from driving videos. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. A multi-layer face movements fusion module with integrated self-attention mechanisms is used to combine identity and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/weimengting/MagicPortrait">https://github.com/weimengting/MagicPortrait</a>. </p>
<blockquote>
<p>本文提出了一种将3D人脸参数模型集成到潜在扩散框架中的视频人脸再现方法。该方法旨在提高现有视频人脸生成方法中的形状一致性和运动控制。我们的方法采用FLAME（带有关节模型和表情的人脸学习）模型作为3D人脸参数表示，为面部表情和头部姿态建模提供了一个统一框架。这可以从驱动视频中提取详细的面部几何和运动特征。具体来说，我们通过融入从FLAME序列派生的深度图、法线图以及渲染图，丰富了潜在扩散模型的3D表情和详细姿态信息。采用具有集成自注意力机制的多层面部运动融合模块，在空域内结合身份和运动潜在特征。通过利用3D人脸参数模型作为运动指导，我们的方法能够在参考图像和从驱动视频中捕获的运动之间实现人脸身份的参数对齐。在基准数据集上的实验结果表明，我们的方法在生成高质量的人脸动画、精确的表情和头部姿态变化建模方面表现出色。此外，它在域外图像上表现出强大的泛化性能。相关代码已公开在<a target="_blank" rel="noopener" href="https://github.com/weimengting/MagicPortrait%E4%B8%8A%E3%80%82">https://github.com/weimengting/MagicPortrait上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21497v1">PDF</a> </p>
<p><strong>Summary</strong><br>新一代视频人脸替换技术结合了三维人脸参数模型与潜在扩散框架，旨在改进现有视频人脸生成方法中的形状一致性和运动控制。通过采用FLAME模型作为三维人脸参数表示，该方法为面部表情和头部姿态建模提供了统一框架，从而精确地提取了驱动视频中的面部几何和运动特征。此外，该方法还通过融入深度图、法线图以及从FLAME序列中得到的渲染图增强了潜在扩散模型的丰富三维表情和精细姿态信息。利用三维人脸参数模型作为运动指导，该方法实现了参考图像与从驱动视频中捕获的运动之间的面部身份参数对齐。实验结果表明，该方法在生成高质量面部动画、精确表达及头部姿态变化建模方面表现出色，且在跨域图像上具有较强的泛化性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种将3D人脸参数模型融入潜在扩散框架的视频人脸替换新方法。</li>
<li>采用FLAME模型作为3D人脸参数表示，为面部表情和头部姿态建模提供统一框架。</li>
<li>通过融入深度图、法线图和渲染图增强潜在扩散模型的丰富三维表情和精细姿态信息。</li>
<li>利用三维人脸参数模型作为运动指导，实现面部身份与运动的参数对齐。</li>
<li>该方法在生成高质量面部动画、精确表达及头部姿态变化建模方面表现优异。</li>
<li>该方法具有强大的泛化能力，适用于跨域图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21497">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-50daa0821b58ae4c4360e8d879fb4d5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a98f99abe6f13361d5376728b9b6229c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-475855bb8d6048f8dc11b0fa9c396653.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-814454a25c5df812963c2d907fbd77d8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DGSolver-Diffusion-Generalist-Solver-with-Universal-Posterior-Sampling-for-Image-Restoration"><a href="#DGSolver-Diffusion-Generalist-Solver-with-Universal-Posterior-Sampling-for-Image-Restoration" class="headerlink" title="DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling   for Image Restoration"></a>DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling   for Image Restoration</h2><p><strong>Authors:Hebaixu Wang, Jing Zhang, Haonan Guo, Di Wang, Jiayi Ma, Bo Du</strong></p>
<p>Diffusion models have achieved remarkable progress in universal image restoration. While existing methods speed up inference by reducing sampling steps, substantial step intervals often introduce cumulative errors. Moreover, they struggle to balance the commonality of degradation representations and restoration quality. To address these challenges, we introduce \textbf{DGSolver}, a diffusion generalist solver with universal posterior sampling. We first derive the exact ordinary differential equations for generalist diffusion models and tailor high-order solvers with a queue-based accelerated sampling strategy to improve both accuracy and efficiency. We then integrate universal posterior sampling to better approximate manifold-constrained gradients, yielding a more accurate noise estimation and correcting errors in inverse inference. Extensive experiments show that DGSolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability, both qualitatively and quantitatively. Code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/MiliLab/DGSolver">https://github.com/MiliLab/DGSolver</a>. </p>
<blockquote>
<p>扩散模型在通用图像修复方面取得了显著的进步。虽然现有方法通过减少采样步骤来加速推理，但较大的步骤间隔通常会引入累积误差。此外，它们很难在退化表示的共同性和修复质量之间取得平衡。为了解决这些挑战，我们引入了<strong>DGSolver</strong>，这是一种带有通用后验采样的扩散通用求解器。我们首先为通用扩散模型推导出精确的常微分方程，并使用基于队列的加速采样策略定制高阶求解器，以提高准确性和效率。然后，我们融入通用后验采样，以更好地近似流形约束梯度，从而得到更准确的噪声估计和逆向推理中的误差校正。大量实验表明，无论是在定性还是定量方面，DGSolver在修复准确性、稳定性和可扩展性方面都优于最先进的方法。相关代码和模型将在<a target="_blank" rel="noopener" href="https://github.com/MiliLab/DGSolver%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/MiliLab/DGSolver上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21487v1">PDF</a> </p>
<p><strong>Summary</strong>：针对通用扩散模型在图像修复方面的不足，提出一种名为DGSolver的通用扩散求解器，采用精确常微分方程和高阶求解器来提高修复准确性和效率。结合通用后采样策略进行噪声估计和逆向推理误差校正，提高修复质量。在多项实验中，DGSolver在修复准确性、稳定性和可扩展性方面均优于现有方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>现有扩散模型在图像修复中面临挑战，如累积误差和平衡退化表示与修复质量的问题。</li>
<li>提出DGSolver，采用精确常微分方程和高阶求解器改善修复效率和准确性。</li>
<li>采用基于队列的加速采样策略提高性能。</li>
<li>结合通用后采样以更好地近似流形约束梯度，准确估计噪声并纠正逆向推理误差。</li>
<li>DGSolver在多项实验中的修复准确性、稳定性和可扩展性优于现有方法。</li>
<li>DGSolver代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/MiliLab/DGSolver%E4%B8%8A%E8%8E%B7%E3%80%82">https://github.com/MiliLab/DGSolver上获取。</a></li>
<li>DGSolver设计具有普适性，可应用于多种图像修复任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21487">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f045a63e71be310818c0262a3fe65eef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ea14c0d296a3d3293494d893af5dcb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7defe55add8baabc0eafec95e88b12d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8577738c701f00cdf6e825c271f94fdc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="IDDM-Bridging-Synthetic-to-Real-Domain-Gap-from-Physics-Guided-Diffusion-for-Real-world-Image-Dehazing"><a href="#IDDM-Bridging-Synthetic-to-Real-Domain-Gap-from-Physics-Guided-Diffusion-for-Real-world-Image-Dehazing" class="headerlink" title="IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided   Diffusion for Real-world Image Dehazing"></a>IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided   Diffusion for Real-world Image Dehazing</h2><p><strong>Authors:Shijun Zhou, Yajing Liu, Chunhui Hao, Zhiyuan Liu, Jiandong Tian</strong></p>
<p>Due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. To address this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion \textbf{M}odels (IDDM), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. IDDM aims to use the gradual haze formation process to help the denoising Unet robustly learn the distribution of clear images from the conditional input hazy images. We design a specialized training strategy centered around IDDM. Diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. During the forward process, IDDM simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. By training with physics-guided information, IDDM shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. Extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches. </p>
<blockquote>
<p>由于真实世界和合成雾图像之间的领域差距，当前基于合成数据集训练的数据驱动去雾算法在合成数据上表现良好，但在真实场景中的泛化能力较差。为了应对这一挑战，我们提出了图像去雾扩散模型（IDDM），这是一种新的扩散过程，将大气散射模型融入噪声扩散中。IDDM旨在利用逐步的雾形成过程，帮助去噪U-Net从条件输入的有雾图像中稳健地学习清晰图像分布。我们围绕IDDM设计了一种专门的训练策略。扩散模型被用来弥合合成和真实世界之间的领域差距，而大气散射模型则为雾的形成提供了物理指导。在正向过程中，IDDM同时将雾和噪声引入清晰图像，然后在采样过程中稳健地分离它们。通过物理指导信息进行训练，IDDM展现出领域泛化的能力，即使在合成数据集上训练，也能有效地恢复真实世界的雾图像。大量实验表明，我们的方法与最先进的方法相比，在定量和定性比较中都表现出有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21385v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为图像去雾扩散模型（IDDM）的新方法，旨在解决合成数据集训练的现有数据驱动去雾算法在真实世界场景中的泛化性能挑战。通过将大气散射模型融入噪声扩散，IDDM利用渐进的雾形成过程帮助去噪Unet从条件输入雾图像中学习清晰图像分布。通过设计以IDDM为中心的训练策略，利用扩散模型缩小合成数据与真实世界之间的领域差距，同时大气散射模型为雾的形成提供物理指导。实验证明，该方法在合成数据集上训练后，能有效恢复真实世界中的雾图像。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前数据驱动的去雾算法在真实世界场景中泛化性能较差，主要原因是合成图像和真实世界图像之间的域差距。</li>
<li>提出了一种新的图像去雾扩散模型（IDDM），将大气散射模型融入噪声扩散中。</li>
<li>IDDM利用渐进的雾形成过程帮助去噪Unet从条件输入雾图像中学习清晰图像分布。</li>
<li>设计了以IDDM为中心的训练策略，利用扩散模型缩小合成数据和真实世界之间的领域差距。</li>
<li>大气散射模型为雾的形成提供物理指导，使得IDDM能够更有效地恢复真实世界的雾图像。</li>
<li>IDDM在定量和定性比较方面均显示出其有效性，与现有先进方法相比具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21385">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-26d0939fc277001a4bbd70efb0d190c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1630cdd4dd3e88421d9f411362d3d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-052162f8e25e52986f17d40ae111114e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d192fbf780041bfa2e76088cfb5b5cf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67b2022990e1105873930134229afa08.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Sparse-to-Sparse-Training-of-Diffusion-Models"><a href="#Sparse-to-Sparse-Training-of-Diffusion-Models" class="headerlink" title="Sparse-to-Sparse Training of Diffusion Models"></a>Sparse-to-Sparse Training of Diffusion Models</h2><p><strong>Authors:Inês Cardoso Oliveira, Decebal Constantin Mocanu, Luis A. Leiva</strong></p>
<p>Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and often outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs. </p>
<blockquote>
<p>扩散模型（DMs）是一种强大的生成模型，在各种图像合成任务中达到了最先进的水平，并在其他领域如自然语言处理和时序数据建模中显示出潜力。尽管DMs具有稳定的训练动力并能够产生多样化的高质量样本，但它们却以在训练和推理阶段都需要大量计算资源而闻名。以前的工作主要集中在提高模型推理的效率。本文首次将稀疏到稀疏的训练范式引入扩散模型，旨在提高训练和推理的效率。我们关注无条件生成，从零开始训练稀疏扩散模型（潜在扩散和ChiroDiff），在六个数据集上使用三种不同的方法（Static-DM、RigL-DM和MagRan-DM）来研究稀疏性对模型性能的影响。我们的实验表明，稀疏DMs能够匹配甚至超越其密集对应模型的表现，同时大大减少可训练参数和浮点运算次数。我们还确定了进行DMs的稀疏到稀疏训练的安全和有效值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21380v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型（DMs）是一种强大的生成模型，在各种图像合成任务中取得了最新成果，并在自然语言处理和时序数据建模等领域显示出潜力。尽管DMs具有稳定的训练动力学和产生多样化高质量样本的能力，但它们却在训练和推理阶段都需要大量的计算资源。以前的工作主要集中在提高模型推理的效率。本文首次将稀疏到稀疏的训练范式引入到DMs中，旨在提高训练和推理的效率。我们专注于无条件生成，使用三种不同的方法（Static-DM、RigL-DM和MagRan-DM）在六个数据集上从头开始训练稀疏DMs（潜在扩散和ChiroDiff）。实验表明，稀疏DMs能够匹配甚至超越其密集对应模型的表现，同时大幅减少可训练参数和浮点运算次数。我们还确定了进行DMs的稀疏到稀疏训练的安全和有效值。</p>
<p><strong>要点</strong></p>
<ol>
<li>扩散模型（DMs）在图像合成等领域表现出强大的生成能力。</li>
<li>DMs在训练和推理阶段都需要大量计算资源。</li>
<li>本文首次引入稀疏到稀疏的训练范式到DMs中，旨在提高训练和推理的效率。</li>
<li>在多个数据集上实验表明，稀疏DMs能匹配甚至超越密集DMs的表现。</li>
<li>稀疏DMs能大幅减少可训练参数和浮点运算次数。</li>
<li>本文提供了进行DMs的稀疏到稀疏训练的有效方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-61f78d34c780d486569ba18ed648bc9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5549363362063e5e22423667c4cf7aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74bece3d5e501392582b911afeed596c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Nexus-Gen-A-Unified-Model-for-Image-Understanding-Generation-and-Editing"><a href="#Nexus-Gen-A-Unified-Model-for-Image-Understanding-Generation-and-Editing" class="headerlink" title="Nexus-Gen: A Unified Model for Image Understanding, Generation, and   Editing"></a>Nexus-Gen: A Unified Model for Image Understanding, Generation, and   Editing</h2><p><strong>Authors:Hong Zhang, Zhongjie Duan, Xingjun Wang, Yingda Chen, Yuze Zhao, Yu Zhang</strong></p>
<p>Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigm’s training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at <a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen.git">https://github.com/modelscope/Nexus-Gen.git</a> to facilitate further advancements across the field. </p>
<blockquote>
<p>统一多模态大型语言模型（MLLM）旨在通过单一框架整合多模态理解和生成能力。尽管它们具有多功能性，但现有的开源统一模型与特定领域的架构之间仍存在一定的性能差距。为了弥补这一差距，我们提出了Nexus-Gen，这是一款统一模型，它协同了LLM的语言推理能力与扩散模型的图像合成能力。为了对齐LLM和扩散模型的嵌入空间，我们进行了双阶段对齐训练过程。（1）自回归LLM学习根据多模态输入预测图像嵌入，（2）视觉解码器则训练从这些嵌入中重建高保真图像。在训练LLM的过程中，我们发现自回归范式训练与推理阶段之间存在关键差异，连续嵌入空间中的误差累积会严重降低生成质量。为了避免这个问题，我们引入了一种预填充自回归策略，该策略用位置嵌入特殊令牌预填充输入序列，而不是连续嵌入。通过双阶段训练，Nexus-Gen已经具备了全面解决图像理解、生成和编辑任务的能力。所有模型、数据集和代码都已发布在<a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen.git%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%91%E5%B1%95%E3%80%82">https://github.com/modelscope/Nexus-Gen.git，以促进该领域的进一步发展。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21356v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于统一多模态大型语言模型（MLLMs）的目标是通过单一框架整合多模态理解和生成能力。现有开源统一模型在领域特定架构方面存在性能差距。为了缩小这一差距，我们提出了Nexus-Gen，一个融合了语言推理能力和扩散模型图像合成能力的统一模型。通过双阶段对齐训练过程，实现了LLM和扩散模型的嵌入空间对齐。第一阶段是自适应回归LLM学习基于多模态输入的图像嵌入预测，第二阶段是视觉解码器从这些嵌入中重建高保真图像。在训练LLM过程中，我们发现自适应回归范式在训练和推理阶段之间存在关键差异，连续嵌入空间中的误差累积严重降低了生成质量。为了避免这个问题，我们引入了一种预填充自适应回归策略，该策略用位置嵌入特殊令牌预填充输入序列，而不是连续嵌入。通过双阶段训练，Nexus-Gen具备了全面解决图像理解、生成和编辑任务的综合能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>统一多模态大型语言模型（MLLMs）的目标是整合多模态理解和生成能力。</li>
<li>现有开源统一模型在特定领域性能方面存在差距。</li>
<li>Nexus-Gen融合了语言推理能力和扩散模型的图像合成能力。</li>
<li>通过双阶段对齐训练过程，实现了LLM和扩散模型的嵌入空间对齐。</li>
<li>在训练LLM过程中，发现并解决了自适应回归范式的关键差异问题。</li>
<li>引入了预填充自适应回归策略，以提高生成质量。</li>
<li>Nexus-Gen具备图像理解、生成和编辑的综合能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21356">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8b08bca29f0be492d8a0eaa935de8e3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e38ca9a369d5342fcb3eaac097539f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f870bede37548c9a3885c4ff0fbd97ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ce13c70f0d32254591fab13dabe9ae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09a436404eaa50247abe30ab1918b1b8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Efficient-Diffusion-Models-A-Survey"><a href="#Efficient-Diffusion-Models-A-Survey" class="headerlink" title="Efficient Diffusion Models: A Survey"></a>Efficient Diffusion Models: A Survey</h2><p><strong>Authors:Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang</strong></p>
<p>Diffusion models have emerged as powerful generative models capable of producing high-quality contents such as images, videos, and audio, demonstrating their potential to revolutionize digital content creation. However, these capabilities come at the cost of their significant computational resources and lengthy generation time, underscoring the critical need to develop efficient techniques for practical deployment. In this survey, we provide a systematic and comprehensive review of research on efficient diffusion models. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient diffusion model topics from algorithm-level, system-level, and framework perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at <a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey</a>. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient diffusion model research and inspire them to contribute to this important and exciting field. </p>
<blockquote>
<p>扩散模型作为强大的生成模型已经崭露头角，能够生成高质量的内容，如图像、视频和音频，显示出它们有潜力彻底改变数字内容的创作方式。然而，这些能力需要耗费大量的计算资源和漫长的生成时间，这凸显了开发高效技术以用于实际部署的迫切需求。在这篇综述中，我们对高效的扩散模型研究进行了系统而全面的回顾。我们从算法层面、系统层面和框架视角三个主要类别对文献进行了分类，涵盖了不同但相互关联的高效扩散模型主题。我们还创建了一个GitHub仓库，其中整理了本综述中提到的论文，地址为：<a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%AE%E5%B8%AE%E5%AF%BC%E5%B9%BF%E5%BA%AB%E6%9D%BF%E7%BB%BC%E8%BF%BD%E7%BB%AD%E5%9C%B0%E4%BA%86%E8%A7%A3%E5%AE%BD%E6%B5%94%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A0%94%E7%A9%B6%EF%BC%8C%E5%B9%B6%E6%BF%80%E5%8F%91%E4%BB%96%E4%BB%AC%E5%9C%A8%E8%BF%99%E4%B8%AA%E9%87%8D%E8%A6%81%E4%B8%94%E5%85%B7%E6%9C%89%E6%8C%91%E6%88%98%E6%80%A7%E7%9A%84%E9%A2%86%E5%9F%9F%E8%BF%9B%E8%A1%8C%E5%88%9B%E6%96%B0%E3%80%82">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey。我们希望这篇综述能够作为有价值的资源，帮助研究人员和从业者系统地了解高效的扩散模型研究，并激发他们为这个重要而激动人心的领域做出贡献。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06805v2">PDF</a> Published in Transactions on Machine Learning Research (TMLR-2025)</p>
<p><strong>Summary</strong></p>
<p>扩散模型作为强大的生成模型，在图像、视频和音频等内容创作方面表现出卓越的能力，具有革新数字内容创作的潜力。然而，其强大的能力需要消耗大量的计算资源和时间，因此开发高效的技术以实际应用显得尤为重要。本次调研系统地综述了关于高效扩散模型的研究，将文献分为算法层面、系统层面和框架层面的三个主要类别，以全面覆盖不同但相互关联的高效扩散模型主题。我们创建了GitHub仓库，整理并分享了本次调研中的论文，网址为：[<a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E6%9C%AC%E6%AC%A1%E8%B0%83%E7%A0%94%E8%83%BD%E4%B8%BA%E7%A0%94%E7%A9%B6%E8%80%85%E5%92%8C%E4%BB%8E%E4%B8%9A%E8%80%85%E6%8F%90%E4%BE%9B%E5%AF%B9%E9%AB%98%E6%95%88%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3%EF%BC%8C%E5%B9%B6%E6%BF%80%E5%8F%91%E4%BB%96%E4%BB%AC%E5%9C%A8%E8%BF%99%E4%B8%80%E9%87%8D%E8%A6%81%E4%B8%94%E6%BF%80%E5%8A%A8%E4%BA%BA%E5%BF%83%E7%9A%84%E9%A2%86%E5%9F%9F%E5%81%9A%E5%87%BA%E8%B4%A1%E7%8C%AE%E3%80%82]">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey。我们希望本次调研能为研究者和从业者提供对高效扩散模型的全面理解，并激发他们在这一重要且激动人心的领域做出贡献。]</a>(<a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%AE%E5%A4%9A%E5%AF%BB%E7%A0%94%E6%B5%B7%E5%AF%BC%E5%AF%BC%E5%AD%A6%E8%BF%BD%E5%AF%BB%E6%B3%A8%E5%AE%9E%E7%94%A8%E7%9A%84%E6%9C%AC%%EF%BC%88%E4%B8%AD%E6%96%87%E4%B8%BA%E2%80%9C%E6%88%91%E4%BB%AC%E5%B8%8C%E6%9C%9B%E9%80%9A%E8%BF%87%E6%9C%AC%E6%AC%A1%E8%B0%83%E7%A0%94%E5%BC%95%E9%A2%86%E5%AD%A6%E8%80%85%E5%92%8C%E4%BB%8E%E4%B8%9A%E8%80%85%E4%BA%86%E8%A7%A3%E9%AB%98%E6%95%88%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E5%B9%B6%E6%BF%80%E5%8F%91%E4%BB%96%E4%BB%AC%E5%AF%B9%E8%BF%99%E4%B8%80%E9%87%8D%E8%A6%81%E9%A2%86%E5%9F%9F%E7%9A%84%E8%B4%A1%E7%8C%AE%E3%80%82%E2%80%9D%EF%BC%89%E6%AD%A4%E5%8F%A5%E8%AF%9D%E7%BB%93%E5%B0%BE%E6%9C%89%E7%9C%81%E7%95%A5%E5%8F%B7%E6%BC%8F%E8%AF%91%E6%83%85%E5%86%B5%EF%BC%89%E6%AD%A4%E5%A4%84%E4%B8%BA%E6%82%A8%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%AE%8C%E6%95%B4%E8%A1%A5%E5%85%85%E3%80%82%E5%BD%93%E5%89%8D%E8%8B%B1%E6%96%87%E9%83%A8%E5%88%86%E5%85%A8%E9%83%A8%E4%B8%BA%E5%AE%8C%E6%95%B4%E7%BF%BB%E8%AF%91%E7%BB%93%E6%9E%9C%E3%80%82%E6%88%91%E4%BB%AC%E6%95%B4%E7%90%86%E7%9A%84%E8%B5%84%E6%BA%90%E8%83%BD%E5%A4%9F%E5%B8%AE%E5%8A%A9%E5%A4%A7%E5%AE%B6%E6%9B%B4%E5%A5%BD%E5%9C%B0%E7%90%86%E8%A7%A3%E5%92%8C%E7%A0%94%E7%A9%B6%E9%AB%98%E6%95%88%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E6%8E%A8%E5%8A%A8%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%91%E5%B1%95%E3%80%82%E5%B8%8C%E6%9C%9B%E6%88%91%E4%BB%AC%E7%9A%84%E8%B0%83%E7%A0%94%E8%83%BD%E4%B8%BA%E5%A4%A7%E5%AE%B6%E6%8F%90%E4%BE%9B%E6%9C%89%E4%BB%B7%E5%80%BC%E7%9A%84%E5%8F%82%E8%80%83%E5%92%8C%E5%B8%AE%E5%8A%A9%E3%80%82">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey%E3%80%82%E6%88%91%E4%BB%AC%E5%B8%AE%E5%A4%9A%E5%AF%BB%E7%A0%94%E6%B5%B7%E5%AF%BC%E5%AF%BC%E5%AD%A6%E8%BF%BD%E5%AF%BB%E6%B3%A8%E5%AE%9E%E7%94%A8%E7%9A%84%E6%9C%AC%（中文为“我们希望通过本次调研引领学者和从业者了解高效扩散模型的最新进展并激发他们对这一重要领域的贡献。”）此句话结尾有省略号漏译情况）此处为您进行了完整补充。当前英文部分全部为完整翻译结果。我们整理的资源能够帮助大家更好地理解和研究高效扩散模型，并推动这一领域的进一步发展。希望我们的调研能为大家提供有价值的参考和帮助。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型具有强大的生成能力，尤其在图像、视频和音频创作领域。</li>
<li>扩散模型的计算资源消耗量大，生成时间长，需开发高效技术满足实际应用需求。</li>
<li>本次调研系统地总结了关于高效扩散模型的研究文献，分为三个主要类别：算法层面、系统层面和框架层面。</li>
<li>创建了GitHub仓库分享调研论文，便于研究者和从业者获取资源。</li>
<li>调研旨在帮助理解高效扩散模型的最新研究进展。</li>
<li>激发对高效扩散模型领域的贡献和创新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06805">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b52459d56e423d5a44382a950ed3fdf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d5f08f168cc71a3bf5f02a365c7d57e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Diffusion-Bridge-Implicit-Models"><a href="#Diffusion-Bridge-Implicit-Models" class="headerlink" title="Diffusion Bridge Implicit Models"></a>Diffusion Bridge Implicit Models</h2><p><strong>Authors:Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, Jun Zhu</strong></p>
<p>Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/thu-ml/DiffusionBridge">https://github.com/thu-ml/DiffusionBridge</a>. </p>
<blockquote>
<p>去噪扩散桥模型（DDBMs）是在给定两个任意配对分布作为端点时，用于进行插值的扩散模型的强大变体。尽管它们在图像翻译等任务中表现出有希望的性能，但DDBMs需要进行计算密集型的采样过程，这涉及通过数百次网络评估来模拟（随机）微分方程。在这项工作中，我们采取了加快DDBMs采样的第一步，无需额外的训练，这是由扩散模型的既定配方所激发的。我们通过定义一类关于采样离散时间步长的非马尔可夫扩散桥来推广DDBMs，这些桥具有相同的边缘分布和训练目标，产生从随机到确定的生成过程，并导致扩散桥隐式模型（DBIMs）的出现。DBIMs不仅比DDBMs的原生采样器快达25倍，而且还引出了一种新的、简单且富有洞察力的常微分方程（ODE），这激发了高阶数值求解器的灵感。此外，DBIMs以独特的方式保持生成多样性，通过在初始采样步骤中使用引导噪声，这使得在图像翻译任务中能够实现忠实编码、重建和语义插值。代码可在<a target="_blank" rel="noopener" href="https://github.com/thu-ml/DiffusionBridge%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/thu-ml/DiffusionBridge找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15885v6">PDF</a> Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>DDBMs（去噪扩散桥梁模型）是一种强大的扩散模型，能够在两个任意配对分布之间进行插值。但其采样过程计算密集，涉及模拟微分方程和大量网络评估。本研究首次尝试对DDBMs进行快速采样，无需额外训练。通过定义非马尔可夫扩散桥梁的类别，我们提出了扩散桥梁隐模型（DBIMs），它们具有相同的边际分布和培训目标，生成过程从随机到确定性不等。DBIMs不仅使采样速度提高了25倍，还启发了一种新型简单的常微分方程（ODE）。此外，通过初始采样步骤中的引导噪声，DBIMs以独特的方式保持了生成的多样性，使图像翻译任务中的编码、重建和语义插值更加真实。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/thu-ml/DiffusionBridge">链接</a>找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDBMs 是一种能够在两个任意配对分布之间插值的强大扩散模型。</li>
<li>DDBMs 的采样过程计算密集，涉及模拟微分方程和大量网络评估。</li>
<li>本研究首次尝试对 DDBMs 进行快速采样，提出了扩散桥梁隐模型（DBIMs）。</li>
<li>DBIMs 通过定义非马尔可夫扩散桥梁实现快速采样，具有相同的边际分布和培训目标。</li>
<li>DBIMs 的生成过程包括从随机到确定性的多种形态。</li>
<li>DBIMs 提高了采样速度，同时保持了生成的多样性，通过初始采样步骤中的引导噪声实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15885">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-02081eaef27d75b9cc468b098557d710.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2168847ad73418759c0cf49bea42be57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c68df5f067cc793625a158171eb849a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5742d0aa04c7dfd7f20275519faf3475.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SignDiff-Diffusion-Model-for-American-Sign-Language-Production"><a href="#SignDiff-Diffusion-Model-for-American-Sign-Language-Production" class="headerlink" title="SignDiff: Diffusion Model for American Sign Language Production"></a>SignDiff: Diffusion Model for American Sign Language Production</h2><p><strong>Authors:Sen Fang, Chunyu Sui, Yanghao Zhou, Xuedong Zhang, Hongbin Zhong, Yapeng Tian, Chen Chen</strong></p>
<p>In this paper, we propose a dual-condition diffusion pre-training model named SignDiff that can generate human sign language speakers from a skeleton pose. SignDiff has a novel Frame Reinforcement Network called FR-Net, similar to dense human pose estimation work, which enhances the correspondence between text lexical symbols and sign language dense pose frames, reduces the occurrence of multiple fingers in the diffusion model. In addition, we propose a new method for American Sign Language Production (ASLP), which can generate ASL skeletal pose videos from text input, integrating two new improved modules and a new loss function to improve the accuracy and quality of sign language skeletal posture and enhance the ability of the model to train on large-scale data. We propose the first baseline for ASL production and report the scores of 17.19 and 12.85 on BLEU-4 on the How2Sign dev&#x2F;test sets. We evaluated our model on the previous mainstream dataset PHOENIX14T, and the experiments achieved the SOTA results. In addition, our image quality far exceeds all previous results by 10 percentage points in terms of SSIM. </p>
<blockquote>
<p>本文提出了一种名为SignDiff的双条件扩散预训练模型，该模型可以从骨架姿态生成人类手语。SignDiff具有一个名为FR-Net的新型框架强化网络，类似于密集人体姿态估计工作，它增强了文本词汇符号与手语密集姿态框架之间的对应关系，减少了扩散模型中多个手指的出现。此外，我们提出了一种新的美国手语生产（ASLP）方法，可以从文本输入生成ASL骨架姿态视频，集成两个新改进模块和新的损失函数，以提高手语骨架姿态的准确性和质量，并增强模型在大规模数据上的训练能力。我们为ASL生产提出了第一个基准线，并在How2Sign的dev&#x2F;test集上报告了BLEU-4得分为17.19和12.85。我们在之前的主流数据集PHOENIX14T上评估了我们的模型，实验达到了SOTA结果。此外，我们的图像质量在SSIM方面超过了之前所有结果，提高了10个百分点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16082v4">PDF</a> Camera-Ready Version; Project Page at <a target="_blank" rel="noopener" href="https://signdiff.github.io/">https://signdiff.github.io</a></p>
<p><strong>Summary</strong><br>     该论文提出了一种名为SignDiff的双条件扩散预训练模型，能够从骨架姿态生成人类手势语言。SignDiff具有新型帧强化网络FR-Net，与密集人类姿态估计工作类似，增强了文本词汇符号与手势语言密集姿态帧之间的对应关系，减少了扩散模型中多个手指的出现。此外，论文还提出了美式手势语言生产（ASLP）的新方法，可从文本输入生成ASL骨架姿态视频，集成两个新改进模块和新的损失函数，提高手势语言骨架姿态的准确性和质量，并增强模型在大规模数据上的训练能力。论文首次提出ASL生产的基线，并在How2Sign的dev&#x2F;test集上取得BLEU-4评分为17.19和12.85。在PHOENIX14T主流数据集上的实验达到了最佳结果，且图像质量在SSIM方面超过了之前所有结果，提高了10个百分点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SignDiff模型能够基于骨架姿态生成人类手势语言。</li>
<li>SignDiff拥有名为FR-Net的新型Frame Reinforcement网络，增强了文本与手势语言姿态之间的对应关系。</li>
<li>FR-Net减少了扩散模型中多个手指的出现。</li>
<li>提出了美式手势语言生产（ASLP）的新方法，集成了两个改进模块和新的损失函数。</li>
<li>ASLP方法能提高手势语言骨架姿态的准确性和质量。</li>
<li>论文在How2Sign数据集上取得了较高的BLEU-4评分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.16082">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-8383cad4a8dcb42bcf6225d684f23e23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab850608039f4e08a01a18f850fbceee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84b78c46d413ca62b84dc0bef2467a05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-748dcf45ecaef95aaca763e16ac8899b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec776f38ce95833bc406ba7f922d20b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fbbcc95c3f3436c2c2eb6fdb3adc84d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Generalizable-Synthetic-Image-Detection-via-Language-guided-Contrastive-Learning"><a href="#Generalizable-Synthetic-Image-Detection-via-Language-guided-Contrastive-Learning" class="headerlink" title="Generalizable Synthetic Image Detection via Language-guided Contrastive   Learning"></a>Generalizable Synthetic Image Detection via Language-guided Contrastive   Learning</h2><p><strong>Authors:Haiwei Wu, Jiantao Zhou, Shile Zhang</strong></p>
<p>The heightened realism of AI-generated images can be attributed to the rapid development of synthetic models, including generative adversarial networks (GANs) and diffusion models (DMs). The malevolent use of synthetic images, such as the dissemination of fake news or the creation of fake profiles, however, raises significant concerns regarding the authenticity of images. Though many forensic algorithms have been developed for detecting synthetic images, their performance, especially the generalization capability, is still far from being adequate to cope with the increasing number of synthetic models. In this work, we propose a simple yet very effective synthetic image detection method via a language-guided contrastive learning. Specifically, we augment the training images with carefully-designed textual labels, enabling us to use a joint visual-language contrastive supervision for learning a forensic feature space with better generalization. It is shown that our proposed LanguAge-guided SynThEsis Detection (LASTED) model achieves much improved generalizability to unseen image generation models and delivers promising performance that far exceeds state-of-the-art competitors over four datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HighwayWu/LASTED">https://github.com/HighwayWu/LASTED</a>. </p>
<blockquote>
<p>人工智能生成图像的高度逼真性可归功于合成模型的快速发展，包括生成对抗网络（GAN）和扩散模型（DM）。然而，合成图像被恶意使用的现象，如传播假新闻或创建虚假个人形象，引发了人们对图像真实性的严重关注。尽管已经开发了许多用于检测合成图像的法医算法，但它们的性能，特别是其泛化能力，仍然远远不足以应对日益增多的合成模型数量。在本文中，我们提出了一种简单而有效的合成图像检测方法，该方法采用语言引导对比学习。具体来说，我们通过精心设计的文本标签扩充训练图像，使我们能够使用联合视觉语言对比监督来学习具有更好泛化的法医特征空间。结果表明，我们提出的LanguAge引导的合成图像检测（LASTED）模型在未见过的图像生成模型上实现了显著的可泛化性提升，并且在四个数据集上的性能远超最新竞争对手。代码可通过<a target="_blank" rel="noopener" href="https://github.com/HighwayWu/LASTED%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HighwayWu/LASTED获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.13800v2">PDF</a> </p>
<p><strong>Summary</strong><br>     人工智能图像生成技术的快速发展，如生成对抗网络（GANs）和扩散模型（DMs），提高了图像的真实性。然而，恶意使用合成图像，如传播假新闻或创建虚假个人资料，引发了人们对图像真实性的关注。虽然已开发出许多用于检测合成图像的前端算法，但其性能尤其是泛化能力仍不足以应对不断增多的合成模型。本研究提出了一种简单有效的合成图像检测方法——通过语言引导对比学习。通过增强训练图像并设计巧妙的文本标签，我们能够使用联合视觉语言对比监督来学习具有更好泛化能力的法医特征空间。所提出的LanguAge引导的合成图像检测（LASTED）模型在未见过的图像生成模型上表现出更高的泛化能力，并在四个数据集上的性能远超最新竞争对手。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/HighwayWu/LASTED%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HighwayWu/LASTED获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI图像生成技术如GANs和DMs提高了图像的真实性。</li>
<li>合成图像的恶意使用引发了关于图像真实性的担忧。</li>
<li>尽管已有许多前端算法用于检测合成图像，但其泛化能力仍然不足。</li>
<li>本研究提出了一种新的合成图像检测方法——通过语言引导对比学习。</li>
<li>通过设计文本标签增强训练图像，以提高模型的泛化能力。</li>
<li>LASTED模型在未见过的图像生成模型上表现出较高的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.13800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6c924b1d6d8939ccc18f7c5e62d4de88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7285e9f9a560ad6d403326c10007f64f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1723ea7ed3e29b53b6f94e182918937.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-231c7fe1970d47a90270147be514c569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b53c51a7b1945209f6dcbc357dffc8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74a6a4da279abaf3fbc7b7b78d951875.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b38c7b72c550016c4cf3d15e1331248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-169ff44f83247c27acd78179f15c3910.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51561b2ec6bac160d8d20d9dba641346.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d92fc23fea6223950917f3abf2912343.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-05-02  A simple and effective approach for body part recognition on CT scans   based on projection estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bd6e08e666d2413b081672afa5619a13.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-05-02  SMOGAN Synthetic Minority Oversampling with GAN Refinement for   Imbalanced Regression
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
