<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-05-02  SWE-smith Scaling Data for Software Engineering Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-502399b049ae062d1b078916628e2c19.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-02-更新"><a href="#2025-05-02-更新" class="headerlink" title="2025-05-02 更新"></a>2025-05-02 更新</h1><h2 id="SWE-smith-Scaling-Data-for-Software-Engineering-Agents"><a href="#SWE-smith-Scaling-Data-for-Software-Engineering-Agents" class="headerlink" title="SWE-smith: Scaling Data for Software Engineering Agents"></a>SWE-smith: Scaling Data for Software Engineering Agents</h2><p><strong>Authors:John Yang, Kilian Leret, Carlos E. Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, Diyi Yang</strong></p>
<p>Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at <a target="_blank" rel="noopener" href="https://swesmith.com/">https://swesmith.com</a>. </p>
<blockquote>
<p>尽管最近在软件工程语言模型（LMs）方面取得了进展，但收集训练数据仍然是一个痛点。现有数据集很小，最多只有来自11个或更少GitHub仓库的数千个训练实例。整理此类数据集的流程通常很复杂，需要数百小时的人工劳动；伴随的执行环境也需要占用数兆字节的存储空间，严重限制了其可扩展性和可用性。为了解决这一痛点，我们引入了SWE-smith，这是一个大规模生成软件工程训练数据的新型管道。给定任何Python代码库，SWE-smith构建相应的执行环境，然后自动合成数百到数千个任务实例，这些实例会破坏代码库中的现有测试。使用SWE-smith，我们创建了一个包含5万个实例的数据集，这些实例来源于128个GitHub仓库，规模比以前的所有工作都大一个数量级。我们训练了SWE-agent-LM-32B模型，在SWE-bench经过验证的基准测试中达到了40.2%的Pass@1解决率，在开源模型中处于最先进水平。为了降低语言模型系统在自动软件工程方面的入门门槛，我们将SWE-smith（收集流程、任务实例、轨迹、模型）开源。所有资源均可在<a target="_blank" rel="noopener" href="https://swesmith.com找到./">https://swesmith.com找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21798v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对软件工程技术训练数据收集难题的新型解决方案SWE-smith。SWE-smith能够自动生成大量的任务实例，打破了以往测试集的限制，从GitHub仓库中构建了相应的执行环境。通过SWE-smith生成的包含5万个实例的数据集显著提高了软件工程技术训练数据规模。并且开源模型SWE-agent-LM-32B在SWE-bench Verified基准测试中实现了较高的性能表现。文章的所有资源均已在<a target="_blank" rel="noopener" href="https://swesmith.com上公开./">https://swesmith.com上公开。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SWE-smith是一种新型软件工程技术训练数据生成管道，能够自动生成大量任务实例。</li>
<li>该方法通过构建执行环境，打破了以往测试集的限制。</li>
<li>使用SWE-smith生成的数据集包含5万个实例，来自128个GitHub仓库，规模显著。</li>
<li>开源模型SWE-agent-LM-32B在基准测试中性能领先。</li>
<li>SWE-smith的开源化降低了进入门槛，便于研究LM系统在自动化软件工程中的应用。</li>
<li>所有相关资源均可在<a target="_blank" rel="noopener" href="https://swesmith.com上找到./">https://swesmith.com上找到。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21798">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d199e7b46a64d1c50b45928415cfd4e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8207f403a642629b35b2aa369b289803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-675d21f975d7aabeae134c524d886f36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39657d3fac58f7c96868c173bc2ccb0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4be2b64492da148d6329a62a8cce6f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8d4888217bb4d60e95f2c46a06ba0e5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LLM-Empowered-Embodied-Agent-for-Memory-Augmented-Task-Planning-in-Household-Robotics"><a href="#LLM-Empowered-Embodied-Agent-for-Memory-Augmented-Task-Planning-in-Household-Robotics" class="headerlink" title="LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in   Household Robotics"></a>LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in   Household Robotics</h2><p><strong>Authors:Marc Glocker, Peter Hönig, Matthias Hirschmanner, Markus Vincze</strong></p>
<p>We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/marc1198/chat-hsr">https://github.com/marc1198/chat-hsr</a>. </p>
<blockquote>
<p>我们呈现了一个具有LLM驱动的主体编排架构的实体机器人系统，用于自主管理家居物品。该系统结合了增强记忆的任务规划，使机器人能够执行高级用户命令，同时跟踪过去的动作。它采用了三个专业代理：路由代理、任务规划代理和知识库代理，每个代理都由特定任务的LLM提供支持。通过利用上下文学习，我们的系统避免了对明确模型训练的需求。RAG使系统能够从过去的交互中检索上下文，从而提高长期物体跟踪能力。Grounded SAM和LLaMa3.2-Vision的结合提供了强大的物体检测功能，促进任务规划的语义场景理解。在三个家庭场景中的评估表明，任务规划精度高，且由于RAG的参与，记忆回忆能力有所提高。具体来说，Qwen2.5在专业代理方面表现最佳，而LLaMA3.1在路由任务方面表现出色。源代码可在<a target="_blank" rel="noopener" href="https://github.com/marc1198/chat-hsr%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/marc1198/chat-hsr上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21716v1">PDF</a> Accepted at Austrian Robotics Workshop 2025</p>
<p><strong>Summary</strong></p>
<p>一个具有LLM驱动代理编排架构的实体机器人系统，用于自主管理家居物品。该系统结合了记忆增强的任务规划，使机器人能够执行高级用户命令并跟踪过去的行动。通过利用上下文学习，该系统避免了显式模型训练的需要。RAG使系统能够从过去的互动中检索上下文，提高长期物品跟踪能力。结合Grounded SAM和LLaMa3.2-Vision，提供稳健的对象检测，促进场景语义理解以进行任务规划。在三个家庭场景中的评估显示，任务规划精度高，且由于RAG的存在，记忆回忆有所改善。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该系统是一个实体机器人平台，旨在通过LLM驱动代理编排架构进行自主家居物品管理。</li>
<li>集成记忆增强的任务规划功能，使机器人能执行高级用户命令并跟踪过去的行为。</li>
<li>系统采用上下文学习，无需明确的模型训练。</li>
<li>RAG能够从过去的互动中检索上下文，提高长期物品跟踪能力。</li>
<li>Grounded SAM和LLaMa3.2-Vision的结合为系统提供了稳健的对象检测功能。</li>
<li>系统在三个家庭场景中的评估结果表明，任务规划精度高，记忆召回也有所改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dca7c9be1a75c46296d95c57f4f3bc5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a66ab9c531e90f69ca0c9b8d40792a16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3582528f93ede71a06f39e4df31f9f7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e590cec1d0b46dd417a5515916e21a17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d6bc3c58292319b55dcecc9e4608ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c76ab1d3e3d6f7daaa52d6abce1c6e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd25b12de6c70dc0852d1416f14fc390.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Iterative-Trajectory-Exploration-for-Multimodal-Agents"><a href="#Iterative-Trajectory-Exploration-for-Multimodal-Agents" class="headerlink" title="Iterative Trajectory Exploration for Multimodal Agents"></a>Iterative Trajectory Exploration for Multimodal Agents</h2><p><strong>Authors:Pengxiang Li, Zhi Gao, Bofei Zhang, Yapeng Mi, Xiaojian Ma, Chenrui Shi, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, Qing Li</strong></p>
<p>Multimodal agents, which integrate a controller (e.g., a large language model) with external tools, have demonstrated remarkable capabilities in tackling complex tasks. However, existing agents need to collect a large number of expert data for fine-tuning to adapt to new environments. In this paper, we propose an online self-exploration method for multimodal agents, namely SPORT, via step-wise preference optimization to refine the trajectories of agents, which automatically generates tasks and learns from solving the generated tasks, without any expert annotation. SPORT operates through four iterative components: task synthesis, step sampling, step verification, and preference tuning. First, we synthesize multi-modal tasks using language models. Then, we introduce a novel search scheme, where step sampling and step verification are executed alternately to solve each generated task. We employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller’s policy through preference tuning, producing a SPORT Agent. By interacting with real environments, the SPORT Agent evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks show that the SPORT Agent achieves 6.41% and 3.64% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is <a target="_blank" rel="noopener" href="https://sport-agents.github.io/">https://SPORT-Agents.github.io</a>. </p>
<blockquote>
<p>多模态智能体通过整合控制器（如大型语言模型）与外部工具，在应对复杂任务方面展现出卓越的能力。然而，现有智能体需要收集大量专家数据来进行微调以适应新环境。在本文中，我们提出了一种在线自我探索的多模态智能体方法，名为SPORT，通过逐步偏好优化来优化智能体的轨迹，该方法可以自动生成任务并从解决生成的任务中学习，无需任何专家标注。SPORT通过四个迭代组件进行操作：任务合成、步骤采样、步骤验证和偏好调整。首先，我们使用语言模型合成多模态任务。然后，我们引入了一种新颖的搜索方案，该方案交替执行步骤采样和步骤验证来解决每个生成的任务。我们采用验证器为人工智能提供反馈来构建逐步偏好数据。随后，这些数据被用来通过偏好调整更新控制器的策略，从而产生一个SPORT智能体。通过与真实环境进行交互，SPORT智能体进化成一个更加精细和强大的系统。在GTA和GAIA基准测试中的评估显示，SPORT智能体的改进率达到6.41%和3.64%，凸显了我们方法的通用性和有效性。项目页面为<a target="_blank" rel="noopener" href="https://sport-agents.github.io./">https://SPORT-Agents.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21561v1">PDF</a> 16 pages, 8 figures</p>
<p><strong>Summary</strong><br>多模态智能体通过与外部工具集成大型语言模型控制器，展现出处理复杂任务的能力。然而，现有智能体需要收集大量专家数据进行微调以适应新环境。本研究提出一种在线自我探索方法SPORT，通过步骤偏好优化为智能体调整轨迹，使其自动生成任务并从解决的任务中学习，无需专家标注。SPORT通过任务合成、步骤采样、步骤验证和偏好调整四个迭代环节操作。利用语言模型合成多模态任务，引入新型搜索方案，交替执行步骤采样和验证来解决每个生成的任务。采用验证器提供AI反馈来构建步骤偏好数据，随后用于通过偏好调整更新控制器的策略，产生SPORT智能体。在与实际环境互动中，SPORT智能体逐渐进化为更精细、更强大的系统。在GTA和GAIA基准测试中，SPORT智能体的表现证明了该方法带来的推广和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态智能体通过集成大型语言模型控制器和外部工具，具备处理复杂任务的能力。</li>
<li>现有智能体需要大量专家数据进行微调以适应新环境。</li>
<li>SPORT是一种在线自我探索方法，为智能体调整轨迹，无需专家标注即可自动生成任务并学习。</li>
<li>SPORT通过任务合成、步骤采样、步骤验证和偏好调整四个迭代环节操作。</li>
<li>采用语言模型合成多模态任务，并引入新型搜索方案解决生成的任务。</li>
<li>通过AI反馈构建步骤偏好数据，用于更新控制器的策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21561">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6dba05ca9c947ea9ad6bd4dc9efe0cb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dea9dfff1165c627098ac16bf0c0510d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5039b2f01305fb73e4b911ba2ca2526e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ee56d2bda0dbb7beaafbf4bc65b58781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3811ed6d42d50432abac83478bd9ea82.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Feature-Transformation-via-In-context-Generation-Generator-critic-LLM-Agents-and-Duet-play-Teaming"><a href="#Unsupervised-Feature-Transformation-via-In-context-Generation-Generator-critic-LLM-Agents-and-Duet-play-Teaming" class="headerlink" title="Unsupervised Feature Transformation via In-context Generation,   Generator-critic LLM Agents, and Duet-play Teaming"></a>Unsupervised Feature Transformation via In-context Generation,   Generator-critic LLM Agents, and Duet-play Teaming</h2><p><strong>Authors:Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haoyue Bai, Sixun Dong, Haifeng Chen, Yanjie Fu</strong></p>
<p>Feature transformation involves generating a new set of features from the original dataset to enhance the data’s utility. In certain domains like material performance screening, dimensionality is large and collecting labels is expensive and lengthy. It highly necessitates transforming feature spaces efficiently and without supervision to enhance data readiness and AI utility. However, existing methods fall short in efficient navigation of a vast space of feature combinations, and are mostly designed for supervised settings. To fill this gap, our unique perspective is to leverage a generator-critic duet-play teaming framework using LLM agents and in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: (1) Critic agent diagnoses data to generate actionable advice, (2) Generator agent produces tokenized feature transformations guided by the critic’s advice, and (3) Iterative refinement ensures continuous improvement through feedback between agents. The generator-critic framework can be generalized to human-agent collaborative generation, by replacing the critic agent with human experts. Extensive experiments demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability across diverse datasets. </p>
<blockquote>
<p>特征转换涉及从原始数据集中生成新的特征集，以提高数据的效用。在某些领域，如材料性能筛选中，维度很大，收集标签既昂贵又耗时。这强烈需要有效地进行特征空间转换，并且无需监督来提高数据准备和人工智能的效用。然而，现有方法在有效地遍历大量的特征组合空间方面存在不足，并且大多是为有监督环境设计的。为了填补这一空白，我们的独特视角是利用生成器-评论家二重奏协作框架，使用大型语言模型（LLM）代理和上下文学习，从无监督数据中获取伪监督。该框架包含三个相互关联的步骤：（1）评论家代理诊断数据以生成可操作的建议，（2）生成器代理根据评论家的建议生成标记化的特征转换，（3）迭代优化确保通过代理之间的反馈进行持续改进。生成器-评论家框架可以通过用人类专家替换评论家代理来推广到人类-代理协作生成。大量实验表明，所提出的框架在特征转换效率、稳健性和实际应用的适用性方面甚至超过了有监督的基线模型，适用于多种数据集。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21304v1">PDF</a> Accepted to IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>特征转换通过从原始数据集中生成新的特征集来提高数据效用。在材料性能筛选等领域，由于维度大且收集标签的成本高且耗时，高效且无监督的特征空间转换变得至关重要。现有方法难以在大量的特征组合空间中进行有效导航，并且大多设计用于监督环境。为解决此空白，我们利用生成器-评论家二重奏的框架，结合大型语言模型（LLM）代理和上下文学习，从无监督数据中衍生伪监督。该框架包含三个相互关联的步骤：评论家代理诊断数据以生成可操作的建议，生成器代理根据评论家的建议产生标记化的特征转换，以及迭代优化确保通过代理之间的反馈持续改进。生成器-评论家框架可推广至人机协作生成，以人类专家替代评论家代理。大量实验表明，该框架在特征转换效率、稳健性和实际应用的多样性方面，甚至超越了监督基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>特征转换能提高数据的效用，特别是在维度大、标签收集成本高的领域。</li>
<li>现有特征转换方法存在对大量特征组合空间导航困难的问题，且主要适用于监督环境。</li>
<li>利用生成器-评论家二重奏框架，结合大型语言模型代理和上下文学习，可从无监督数据中衍生伪监督。</li>
<li>框架包含三个主要步骤：数据诊断、特征生成和迭代优化。</li>
<li>生成器-评论家框架可推广至人机协作生成，以人类专家替代评论家代理。</li>
<li>该框架在特征转换效率、稳健性和实际应用的多样性方面表现优异，超越监督基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21304">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6f4147f076c3a035a438ade3f815503e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c7227c2d89d0f0bff15a3621429f7d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f7353bdbd45197c006f6808a616498b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-498edde4723a82591a80f8345822949e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abdb33736677fa09d4399cd1b428c921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d5fc0a3972f02f864fdfe46796dfdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43e0c00251478e97492ffe03e014c64f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Robust-Multi-agent-Communication-Based-on-Decentralization-Oriented-Adversarial-Training"><a href="#Robust-Multi-agent-Communication-Based-on-Decentralization-Oriented-Adversarial-Training" class="headerlink" title="Robust Multi-agent Communication Based on Decentralization-Oriented   Adversarial Training"></a>Robust Multi-agent Communication Based on Decentralization-Oriented   Adversarial Training</h2><p><strong>Authors:Xuyan Ma, Yawen Wang, Junjie Wang, Xiaofei Xie, Boyu Wu, Shoubin Li, Fanjiang Xu, Qing Wang</strong></p>
<p>In typical multi-agent reinforcement learning (MARL) problems, communication is important for agents to share information and make the right decisions. However, due to the complexity of training multi-agent communication, existing methods often fall into the dilemma of local optimization, which leads to the concentration of communication in a limited number of channels and presents an unbalanced structure. Such unbalanced communication policy are vulnerable to abnormal conditions, where the damage of critical communication channels can trigger the crash of the entire system. Inspired by decentralization theory in sociology, we propose DMAC, which enhances the robustness of multi-agent communication policies by retraining them into decentralized patterns. Specifically, we train an adversary DMAC_Adv which can dynamically identify and mask the critical communication channels, and then apply the adversarial samples generated by DMAC_Adv to the adversarial learning of the communication policy to force the policy in exploring other potential communication schemes and transition to a decentralized structure. As a training method to improve robustness, DMAC can be fused with any learnable communication policy algorithm. The experimental results in two communication policies and four multi-agent tasks demonstrate that DMAC achieves higher improvement on robustness and performance of communication policy compared with two state-of-the-art and commonly-used baselines. Also, the results demonstrate that DMAC can achieve decentralized communication structure with acceptable communication cost. </p>
<blockquote>
<p>在多智能体强化学习（MARL）的典型问题中，通信对于智能体共享信息和做出正确决策至关重要。然而，由于训练多智能体通信的复杂性，现有方法往往陷入局部优化的困境，导致通信集中在有限的通道上，呈现出不平衡的结构。这种不平衡的通信策略在异常条件下很脆弱，关键通信通道的损坏可能会引发整个系统的崩溃。受社会学中分散化理论的启发，我们提出了DMAC（分散多智能体通信）算法，它通过重新训练智能体通信策略来提高其鲁棒性，使它们呈现分散的模式。具体来说，我们训练了一个对手DMAC_Adv，它能够动态地识别和屏蔽关键的通信通道，然后将由DMAC_Adv生成的对抗样本应用于通信策略的对抗性学习，迫使策略探索其他潜在的通信方案，并转向分散的结构。作为一种提高鲁棒性的训练方法，DMAC可以与任何可学习的通信策略算法相结合。在两个通信策略和四个多智能体任务的实验结果证明了DMAC在提高通信策略的鲁棒性和性能方面的优势，相较于两个先进且常用的基线方法，DMAC取得了更高的改进。此外，结果还表明DMAC能够实现具有可接受的通信成本的分散式通信结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多智能体强化学习中的通信至关重要，但现有方法易陷入局部优化困境，导致通信不平衡。本文受社会学中分权理论启发，提出DMAC方法，通过重新训练智能体通信策略以增强其鲁棒性，并采用对抗性训练方式促使智能体探索其他潜在通信方案并实现去中心化结构。实验表明，DMAC能提高通信策略的鲁棒性和性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习中，通信对智能体决策至关重要。</li>
<li>现有方法面临局部优化问题，导致通信不平衡和结构脆弱。</li>
<li>DMAC方法受社会学中分权理论启发，旨在增强智能体通信策略的鲁棒性。</li>
<li>DMAC通过重新训练通信策略，促使智能体实现去中心化结构。</li>
<li>DMAC采用对抗性训练方式，使智能体能动态识别并屏蔽关键通信通道。</li>
<li>实验表明，DMAC相较于两种先进基线方法，在提高通信策略的鲁棒性和性能方面有更高成效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e49cf3a91dfba59b6fc9f0d69daaf261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81c590ba3880e72522fd22b5326f58bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98b83501f415ee3baee1770ed6d5cb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2f9aec2d36b98a1aad6f4927d9699f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef574ff608a77dec178ce566dfdc0cc1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Talk-Before-You-Retrieve-Agent-Led-Discussions-for-Better-RAG-in-Medical-QA"><a href="#Talk-Before-You-Retrieve-Agent-Led-Discussions-for-Better-RAG-in-Medical-QA" class="headerlink" title="Talk Before You Retrieve: Agent-Led Discussions for Better RAG in   Medical QA"></a>Talk Before You Retrieve: Agent-Led Discussions for Better RAG in   Medical QA</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Hao Wang, Xiwen Chen, Peijie Qiu, Rui Yin, Yi Su, Yalin Wang</strong></p>
<p>Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/Discuss-RAG">https://github.com/LLM-VLM-GSL/Discuss-RAG</a>. </p>
<blockquote>
<p>医疗问答（QA）是一项需要大量推理的任务，对于大型语言模型（LLM）来说仍然具有挑战性，因为存在虚构和过时的领域知识。检索增强生成（RAG）通过利用外部知识提供了一种有前途的后期训练解决方案。然而，现有的医疗RAG系统存在两个主要局限性：（1）在信息检索过程中缺乏对人类式推理行为的建模；（2）依赖于次优的医疗语料库，这往往导致检索到不相关或嘈杂的片段。为了克服这些挑战，我们提出了Discuss-RAG，这是一个即插即用的模块，旨在通过基于协作代理的推理来增强医疗问答RAG系统。我们的方法引入了一个总结代理，该代理协调医疗专家团队进行多次头脑风暴，从而提高检索内容的关联性。此外，一个决策代理在最终整合之前评估检索到的片段。在四个基准医疗问答数据集上的实验结果表明，Discuss-RAG持续优于MedRAG，特别是在BioASQ上答案准确率提高了高达16.67%，在PubMedQA上提高了12.20%。代码可在：<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/Discuss-RAG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LLM-VLM-GSL/Discuss-RAG找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21252v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>医学问答是一个需要大量推理的任务，对于大型语言模型来说仍然具有挑战性，因为存在虚构和过时的领域知识问题。检索增强生成（RAG）通过利用外部知识提供了一种有前景的后期训练解决方案。然而，现有的医学RAG系统存在两个主要局限性：（1）在信息检索过程中缺乏人类式推理行为的建模；（2）依赖于次优医学语料库，这通常会导致检索到不相关或嘈杂的片段。为了克服这些挑战，我们提出了Discuss-RAG，这是一个即插即用的模块，旨在通过基于协作代理的推理增强医学问答RAG系统。我们的方法引入了一个总结代理，该代理协调医学专家团队进行多轮讨论，从而提高检索内容的关联性。此外，一个决策代理对检索到的片段进行评估，然后再进行最终集成。在四个基准医学问答数据集上的实验结果表明，Discuss-RAG的性能持续优于MedRAG，尤其是在BioASQ上答案准确率提高高达16.67%，在PubMedQA上提高12.20%。相关代码可在：<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/Discuss-RAG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LLM-VLM-GSL/Discuss-RAG获取。</a></p>
<p><strong>要点</strong></p>
<ol>
<li>医学问答是一个对大型语言模型具有挑战性的任务，因为存在虚构和领域知识过时的问题。</li>
<li>检索增强生成（RAG）是一种利用外部知识的后期训练解决方案。</li>
<li>现有医学RAG系统存在两个主要局限性：缺乏人类式推理行为的建模和依赖于次优医学语料库。</li>
<li>Discuss-RAG通过引入总结代理和决策代理，旨在解决这些挑战。</li>
<li>总结代理协调医学专家团队进行多轮讨论，提高检索内容的关联性。</li>
<li>决策代理对检索到的片段进行评估，再进行最终集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21252">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9615066a83ce517a9bd2181d1572205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c4f626f9094f6d3b55b48e201d7500f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d094249ff31a94fc16939de87bc4f6e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22cee3d38ea983dc8524663713d51b0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b63636f493f1b01c25fd0e31cd7d4f3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VideoMultiAgents-A-Multi-Agent-Framework-for-Video-Question-Answering"><a href="#VideoMultiAgents-A-Multi-Agent-Framework-for-Video-Question-Answering" class="headerlink" title="VideoMultiAgents: A Multi-Agent Framework for Video Question Answering"></a>VideoMultiAgents: A Multi-Agent Framework for Video Question Answering</h2><p><strong>Authors:Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Yasunori Ishii, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli</strong></p>
<p>Video Question Answering (VQA) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. However, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. To address this limitation, we introduce VideoMultiAgents, a framework that integrates specialized agents for vision, scene graph analysis, and text processing. It enhances video understanding leveraging complementary multimodal reasoning from independently operating agents. Our approach is also supplemented with a question-guided caption generation, which produces captions that highlight objects, actions, and temporal transitions directly relevant to a given query, thus improving the answer accuracy. Experimental results demonstrate that our method achieves state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA), EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%). The source code is available at <a target="_blank" rel="noopener" href="https://github.com/PanasonicConnect/VideoMultiAgents">https://github.com/PanasonicConnect/VideoMultiAgents</a>. </p>
<blockquote>
<p>视频问答（VQA）本质上依赖于多模态推理，它结合了视觉、时间和语言线索，以实现更深层次的视频内容理解。然而，许多现有方法依赖于将帧级字幕输入单一模型，这使得难以充分捕获时间和交互上下文。为了解决这一局限性，我们引入了VideoMultiAgents框架，该框架集成了用于视觉、场景图分析和文本处理的专门代理。它通过独立运行的代理的互补多模态推理来增强视频理解。我们的方法还辅以问题引导的字幕生成，生成的字幕突出了与给定查询直接相关的对象、动作和时间过渡，从而提高了答案的准确性。实验结果表明，我们的方法在Intent-QA（意图问答，79.0%，较之前的最优方法高出6.2%）、EgoSchema子集（75.4%，高出3.4%）和NExT-QA（下一个问答，79.6%，高出0.4%）上达到了最新技术水平。源代码可在<a target="_blank" rel="noopener" href="https://github.com/PanasonicConnect/VideoMultiAgents%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/PanasonicConnect/VideoMultiAgents获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20091v2">PDF</a> </p>
<p><strong>Summary</strong><br>视频问答（VQA）需要依靠对视频内容的多模态推理进行回答，包括对视觉、时间和语言线索的整合。然而，现有方法大多依赖于将帧级字幕输入单一模型，难以充分捕捉时间和交互上下文。为解决此问题，我们推出VideoMultiAgents框架，集成专门用于视觉、场景图分析和文本处理的代理。它通过独立操作的代理的互补多模态推理增强视频理解。此外，辅以问题导向的标题生成，生成与查询直接相关的物体、动作和时间过渡的标题，从而提高答案的准确性。实验结果证明，我们的方法在Intent-QA（提升6.2%，达到79.0%）、EgoSchema子集（提升3.4%，达到75.4%）和NExT-QA（提升0.4%，达到79.6%）上达到了最新的水平。代码可在PanasonicConnect公司的VideoMultiAgents项目中找到。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VQA依赖于多模态推理，需要整合视觉、时间和语言线索。</li>
<li>现有方法主要依赖单一模型处理帧级字幕，难以捕捉时间和交互上下文。</li>
<li>VideoMultiAgents框架集成了专门用于视觉、场景图分析和文本处理的代理，通过独立操作的代理的互补进行视频理解。</li>
<li>问题导向的标题生成技术可以提高答案的准确性。</li>
<li>VideoMultiAgents在多个数据集上取得了最新成果，包括Intent-QA、EgoSchema子集和NExT-QA。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-502399b049ae062d1b078916628e2c19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76608eb954f541943dcb8b3274cf1e3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-015b959e445ff6acd218f29ef8902276.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OS-Genesis-Automating-GUI-Agent-Trajectory-Construction-via-Reverse-Task-Synthesis"><a href="#OS-Genesis-Automating-GUI-Agent-Trajectory-Construction-via-Reverse-Task-Synthesis" class="headerlink" title="OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse   Task Synthesis"></a>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse   Task Synthesis</h2><p><strong>Authors:Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, Zhiyong Wu</strong></p>
<p>Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis’s efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at <a target="_blank" rel="noopener" href="https://qiushisun.github.io/OS-Genesis-Home/">https://qiushisun.github.io/OS-Genesis-Home/</a>. </p>
<blockquote>
<p>由视觉语言模型（VLMs）驱动的图形用户界面（GUI）代理已经展现出人类般的计算机控制能力。尽管它们在推进数字自动化方面很有用，但仍然存在一个关键瓶颈：收集高质量轨迹数据进行训练。目前常见的收集此类数据的方法依赖于人工监督或通过执行预定义任务来生成合成数据，这些方法要么资源密集，要么无法保证数据质量。此外，这些方法还遭受数据多样性有限以及合成数据与真实世界环境之间显著差异的困扰。为了应对这些挑战，我们提出了OS-Genesis，一种新型GUI数据合成管道，它颠覆了传统的轨迹收集过程。不同于依赖预定义任务的方法，OS-Genesis使代理首先感知环境并执行分步交互，然后回顾性地推导高质量任务以实现轨迹层面的探索。接着，采用轨迹奖励模型来保证生成轨迹的质量。我们证明，使用OS-Genesis训练的GUI代理在高度挑战性的在线基准测试中显著提高了性能。深入分析进一步验证了OS-Genesis的效率及其相较于现有合成方法在数据质量和多样性方面的优越性。我们的代码、数据和检查点位于：<a target="_blank" rel="noopener" href="https://qiushisun.github.io/OS-Genesis-Home%E3%80%82">https://qiushisun.github.io/OS-Genesis-Home/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19723v2">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>基于Vision-Language Models（VLMs）的图形用户界面（GUI）代理展现了人类般的计算机控制功能。然而，收集高质量轨迹数据以进行训练仍是瓶颈。常规的数据收集方法依赖于人工监督或执行预定义任务生成合成数据，这些方法要么资源消耗大，要么无法保证数据质量。为解决这些问题，本文提出了OS-Genesis，一种新型的GUI数据合成流程。OS-Genesis改变了传统轨迹收集过程，让代理首先感知环境并执行逐步交互，然后回顾性地生成高质量任务以实现轨迹层面的探索。使用轨迹奖励模型确保生成轨迹的质量。实验证明，使用OS-Genesis训练的GUI代理在具有挑战性的在线基准测试中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GUI代理利用Vision-Language Models（VLMs）实现了人类般的计算机控制功能。</li>
<li>收集高质量轨迹数据是训练GUI代理的瓶颈。</li>
<li>传统数据收集方法依赖人工监督或预定义任务生成合成数据，存在资源消耗大、数据质量难以保证的问题。</li>
<li>OS-Genesis是一种新型的GUI数据合成流程，改变了轨迹收集方式，让代理通过感知环境和逐步交互来生成高质量任务。</li>
<li>OS-Genesis使用轨迹奖励模型确保生成轨迹的质量。</li>
<li>使用OS-Genesis训练的GUI代理在在线基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19723">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9b95e3404a2256293bb6c7fe11e915b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1700a31e10b46cddf93e69f2e267ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a5e1826f4a8033f3903760f890bd0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6f45987406f2bc882472004fb4ed52b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="You-Name-It-I-Run-It-An-LLM-Agent-to-Execute-Tests-of-Arbitrary-Projects"><a href="#You-Name-It-I-Run-It-An-LLM-Agent-to-Execute-Tests-of-Arbitrary-Projects" class="headerlink" title="You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary   Projects"></a>You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary   Projects</h2><p><strong>Authors:Islem Bouzenia, Michael Pradel</strong></p>
<p>The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33&#x2F;50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects. </p>
<blockquote>
<p>执行项目测试集的能力在许多场景中都是至关重要的，例如评估代码质量和代码覆盖率、验证开发人员或自动化工具所做的代码更改，以及确保与依赖项的兼容性。尽管其重要性不言而喻，但在实践中执行项目的测试集可能会面临挑战，因为不同的项目会使用不同的编程语言、软件生态系统、构建系统、测试框架和其他工具。这些挑战使得创建一个可靠、通用的跨项目测试执行方法变得困难。</p>
</blockquote>
<p>本文介绍了ExecutionAgent，这是一种自动化技术，用于为从源代码构建任意项目并运行其测试用例生成脚本。我们的方法灵感来自于人类开发者如何完成此任务，它是一个基于大型语言模型（LLM）的代理，可以自主执行命令并与主机系统交互。该代理使用元提示来收集有关给定项目的最新技术指南，并基于上一步的反馈来迭代优化其流程。</p>
<p>我们的评估将ExecutionAgent应用于50个使用14种不同编程语言以及许多不同构建和测试工具的开源项目。该方法成功执行了33&#x2F;50个项目的测试集，与基准测试集执行结果的匹配度达到了仅7.5%的偏差。这些结果比之前可用的最佳技术有了6.6倍的提升。该方法的成本是合理的，每个项目的平均执行时间为74分钟，LLM的成本为0.16美元。</p>
<p>我们期望ExecutionAgent能成为开发者、自动化编程工具和研究人员执行广泛项目测试的有价值工具。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.10133v2">PDF</a> PUBLISHED AT ISSTA 2025</p>
<p><strong>Summary</strong><br>项目测试套件执行能力在许多场景中至关重要，如评估代码质量和覆盖率、验证开发者和自动化工具所做的代码更改以及确保与依赖项的兼容性。然而，执行项目测试套件在实践中可能具有挑战性，因为不同项目使用的编程语言、软件生态系统、构建系统、测试框架和其他工具都有所不同。本文提出ExecutionAgent，一种自动化技术，用于为任意项目从源代码构建脚本并运行其测试用例。该方法是基于大型语言模型（LLM）的代理，可自主执行命令并与主机系统交互。通过对50个使用不同编程语言和构建及测试工具的开源项目进行评估，该方法成功执行了其中33个项目的测试套件，与真实测试套件执行结果的偏差仅为7.5%。相较于之前最佳的技术方法，这一结果提高了6.6倍。该方法的执行时间和LLM成本合理。预期ExecutionAgent能为需要跨多种项目执行测试的开发者、自动化编程工具和研究者提供有价值的工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>执行测试套件对于评估代码质量、验证代码更改和确保依赖兼容性至关重要。</li>
<li>不同项目在执行测试套件时面临多种挑战，如使用的编程语言、软件生态系统等差异。</li>
<li>ExecutionAgent是一种自动化技术，可以从源代码为任意项目构建脚本并运行其测试用例。</li>
<li>ExecutionAgent基于大型语言模型（LLM），自主执行命令并与主机系统交互。</li>
<li>该方法成功执行了33个项目的测试套件，与真实结果的偏差仅为7.5%。</li>
<li>ExecutionAgent相较于之前的技术方法有显著提高，执行时间合理，且LLM成本不高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.10133">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dd9b0910e499c3df86e5b013dd2042ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-081fe0fb00f5659d83f9859b97f5bc27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e2016364c4ce18c327939cab4898b71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7458436ca34177ab0a697086fdbb3dc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Adsorb-Agent-Autonomous-Identification-of-Stable-Adsorption-Configurations-via-Large-Language-Model-Agent"><a href="#Adsorb-Agent-Autonomous-Identification-of-Stable-Adsorption-Configurations-via-Large-Language-Model-Agent" class="headerlink" title="Adsorb-Agent: Autonomous Identification of Stable Adsorption   Configurations via Large Language Model Agent"></a>Adsorb-Agent: Autonomous Identification of Stable Adsorption   Configurations via Large Language Model Agent</h2><p><strong>Authors:Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani</strong></p>
<p>Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent’s performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions. </p>
<blockquote>
<p>吸附能是催化中的关键反应性描述符，能够实现高效筛选最佳催化剂。然而，确定吸附能通常需要评估大量的吸附物-催化剂构型。当前的算法方法依赖于吸附位点和构型的详尽列举，这使得过程计算密集，并且不保证能固有地识别全局最低能量。在这项工作中，我们引入了Adsorb-Agent，这是一个大型语言模型（LLM）代理，旨在有效地识别与全局最低吸附能相对应的特定系统稳定吸附构型。Adsorb-Agent利用其内置知识和新兴推理能力来有策略地探索可能含有吸附能的吸附构型。通过减少对详尽采样的依赖，它在减少所需初始构型数量的同时提高了吸附能预测的准确性。我们评估了Adsorb-Agent在涵盖各种复杂性的二十个代表性系统上的性能。Adsorb-Agent成功地为83.7%的系统确定了相当的吸附能，并为35%的系统实现了更接近实际全局最低值的能量，同时所需的初始构型远少于传统方法。其能力在复杂系统中尤其明显，为涉及金属间表面的系统中有46.7%和具有大吸附分子的系统中66.7%确定了较低的吸附能。这些结果证明了Adsorb-Agent在加速催化剂发现方面的潜力，可以降低计算成本并提高吸附能预测的可信度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16658v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>吸附能是催化中的重要反应描述符，用于有效筛选最佳催化剂。然而，确定吸附能通常需要评估许多吸附质-催化剂配置。当前算法方法依赖于吸附位点和配置的详尽列举，这使得过程计算密集，并不保证找到全局最低能量。在本研究中，我们引入了吸附剂代理（Adsorb-Agent），这是一个大型语言模型（LLM）代理，旨在有效识别与全局最低吸附能对应的系统特定稳定吸附配置。它通过利用内置知识和新兴推理能力来策略性地探索可能包含吸附能的吸附配置，减少了对详尽采样的依赖，在减少所需初始配置数量的同时提高了吸附能预测的准确性。我们对Adsorb-Agent的性能在二十个代表性系统上进行评估，涵盖了一系列复杂性。Adsorb-Agent成功为83.7%的系统确定了可比的吸附能，为35%的系统达到更接近实际全局最小值的能量，同时比传统方法需要的初始配置要少得多。其在复杂系统中的能力特别明显，在涉及金属间表面的系统中为46.7%的系统以及具有大吸附质分子的系统中为66.7%的系统确定了较低的吸附能。这些结果证明了Adsorb-Agent在加速催化剂发现方面的潜力，可降低计算成本并提高吸附能预测的可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>吸附能是催化中重要的反应描述符，对于筛选最佳催化剂至关重要。</li>
<li>当前算法在确定吸附能时依赖详尽的吸附位点和配置列举，计算密集且不一定找到全局最低能量。</li>
<li>Adsorb-Agent是一个大型语言模型代理，可高效识别系统特定的稳定吸附配置和全局最低吸附能。</li>
<li>Adsorb-Agent利用内置知识和新兴推理能力来策略性探索吸附配置。</li>
<li>该代理减少了对详尽采样的依赖，提高了预测吸附能的准确性和效率。</li>
<li>在多个代表性系统上评估，Adsorb-Agent在复杂系统中表现特别出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16658">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3e0b279adcebf07254a23f8c5be9f8c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9284e878eb4d416f5a7a9dd166185b3e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Can-We-Trust-Embodied-Agents-Exploring-Backdoor-Attacks-against-Embodied-LLM-based-Decision-Making-Systems"><a href="#Can-We-Trust-Embodied-Agents-Exploring-Backdoor-Attacks-against-Embodied-LLM-based-Decision-Making-Systems" class="headerlink" title="Can We Trust Embodied Agents? Exploring Backdoor Attacks against   Embodied LLM-based Decision-Making Systems"></a>Can We Trust Embodied Agents? Exploring Backdoor Attacks against   Embodied LLM-based Decision-Making Systems</h2><p><strong>Authors:Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu</strong></p>
<p>Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks. </p>
<blockquote>
<p>大型语言模型（LLM）在实体人工智能的现实决策任务中显示出巨大的潜力，尤其是在微调以利用其固有的常识和推理能力的同时，针对特定应用进行定制时。然而，这种微调过程引入了相当大的安全和漏洞隐患，特别是在安全关键的网物系统。在这项工作中，我们提出了针对基于LLM的决策系统（BALD）的第一个全面的框架，系统地探索攻击面和触发机制。具体来说，我们提出了三种独特的攻击机制：单词注入、场景操控和知识注入，针对基于LLM的决策管道的不同组件。我们对具有代表性的LLM（GPT-3.5、LLaMA2、PaLM2）在自动驾驶和家庭机器人任务上进行了广泛的实验，证明了我们的后门触发在通过各种攻击渠道时的有效性和隐蔽性，包括车辆加速冲向障碍和机器人在床上放置刀具等案例。我们的单词和知识注入攻击在多个模型和数据集上成功率接近100%，而且只需要对系统有限的访问权限。我们的场景操控攻击成功率超过65%，最高可达90%，而且不需要任何运行时系统入侵。我们还对攻击进行了防御评估，揭示了它们的韧性。我们的研究发现了实体LLM系统中关键的安全漏洞，并强调了保护这些系统以减轻潜在风险的紧迫需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20774v3">PDF</a> Accepted paper at ICLR 2025, 31 pages, including main paper,   references, and appendix</p>
<p><strong>Summary</strong></p>
<p>大型语言模型在实体人工智能的决策任务中展现出巨大潜力，但精细调整过程会引发安全和保障漏洞，特别是在安全关键的网路物理系统中。本研究首次提出针对基于LLM的决策系统的后门攻击（BALD）的全面框架，针对LLM决策管道的不同组件，提出三种独特的攻击机制：单词注入、场景操控和知识注入。我们在自动驾驶和家庭机器人任务中对代表性LLM（GPT-3.5、LLaMA2、PaLM2）进行了广泛实验，证明了我们的后门触发机制在各种攻击通道中的有效性和隐蔽性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在实体人工智能的决策任务中具有显著潜力，但精细调整过程引发安全和保障漏洞。</li>
<li>首次提出针对LLM决策系统的后门攻击（BALM）的全面框架。</li>
<li>提出三种攻击机制：单词注入、场景操控和知识注入，可针对LLM决策管道的不同部分。</li>
<li>实验证明，在自动驾驶和家庭机器人任务中，所提攻击方法具有高效性和隐蔽性。</li>
<li>单词注入和知识注入攻击的成功率高达近100%，场景操控攻击的成功率超过65%，甚至达到90%。</li>
<li>这些攻击对系统的防御具有抗性，显示出其稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-842f7f6aa0552215c096a1f303f44a48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90fa48e7117e3fdefa8db34cae8b585d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6941e439ba12306cd7de2a0c04ed4485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99347939275fb9711341c0808ccb9940.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d49293eb3303e13511d4fa6d50668b6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Variational-Offline-Multi-agent-Skill-Discovery"><a href="#Variational-Offline-Multi-agent-Skill-Discovery" class="headerlink" title="Variational Offline Multi-agent Skill Discovery"></a>Variational Offline Multi-agent Skill Discovery</h2><p><strong>Authors:Jiayu Chen, Tian Lan, Vaneet Aggarwal</strong></p>
<p>Skills are effective temporal abstractions established for sequential decision making, which enable efficient hierarchical learning for long-horizon tasks and facilitate multi-task learning through their transferability. Despite extensive research, research gaps remain in multi-agent scenarios, particularly for automatically extracting subgroup coordination patterns in a multi-agent task. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and VO-MASD-Hier, to simultaneously capture subgroup- and temporal-level abstractions and form multi-agent skills, which firstly solves the aforementioned challenge. An essential algorithm component of these schemes is a dynamic grouping function that can automatically detect latent subgroups based on agent interactions in a task. Further, our method can be applied to offline multi-task data, and the discovered subgroup skills can be transferred across relevant tasks without retraining. Empirical evaluations on StarCraft tasks indicate that our approach significantly outperforms existing hierarchical multi-agent reinforcement learning (MARL) methods. Moreover, skills discovered using our method can effectively reduce the learning difficulty in MARL scenarios with delayed and sparse reward signals. The codebase is available at <a target="_blank" rel="noopener" href="https://github.com/LucasCJYSDL/VOMASD">https://github.com/LucasCJYSDL/VOMASD</a>. </p>
<blockquote>
<p>技能是为顺序决策而建立的有效时间抽象，它们为长期任务提供了高效的层次化学习，并通过其可转移性促进了多任务学习。尽管已有大量研究，但在多智能体场景中仍存在研究空白，特别是在多智能体任务中自动提取子组协调模式方面。针对这种情况，我们提出了两种新型自动编码器方案：VO-MASD-3D和VO-MASD-Hier，它们可以同时捕获子组和时间层面的抽象，形成多智能体技能，首先解决了上述挑战。这些方案的一个基本算法组件是一个动态分组函数，该函数可以基于任务中的智能体交互自动检测潜在子组。此外，我们的方法可以应用于离线多任务数据，并且发现的子组技能可以在相关任务之间转移而无需重新训练。在星际争霸任务上的经验评估表明，我们的方法显著优于现有的分层多智能体强化学习（MARL）方法。此外，使用我们的方法发现的技能可以有效地降低具有延迟和稀疏奖励信号的多智能体场景中的学习难度。代码库位于<a target="_blank" rel="noopener" href="https://github.com/LucasCJYSDL/VOMASD%E3%80%82">https://github.com/LucasCJYSDL/VOMASD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.16386v3">PDF</a> This work will appear in the proceedings of IJCAI 2025</p>
<p><strong>Summary</strong><br>技能是有效的时序抽象，用于序列决策制定，能够实现长周期任务的高效层次学习，并通过其可转移性促进多任务学习。然而，在多智能体场景中，自动提取智能体协调模式的研究还存在缺口。针对此问题，我们提出了两种新型自动编码器方案：VO-MASD-3D和VO-MASD-Hier，它们可以同时捕获子群体和时序层面的抽象信息，形成多智能体技能。关键算法组件是动态分组功能，可以基于任务中智能体的交互自动检测潜在子群体。此外，我们的方法可应用于离线多任务数据，发现的子群体技能可以在相关任务之间转移而无需重新训练。在星际争霸任务上的实证评估表明，我们的方法显著优于现有的分层多智能体强化学习（MARL）方法。此外，使用我们的方法发现的技能可以有效地降低MARL场景中延迟和稀疏奖励信号的学习难度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>技能是时序抽象，用于序列决策制定，促进长周期任务的高效层次学习。</li>
<li>在多智能体场景中，自动提取智能体协调模式的研究存在缺口。</li>
<li>提出了VO-MASD-3D和VO-MASD-Hier两种新型自动编码器方案，解决自动提取智能体协调模式的问题。</li>
<li>动态分组功能可自动检测潜在子群体，基于任务中智能体的交互。</li>
<li>方法可应用于离线多任务数据，并可实现子群体技能在不同任务间的转移。</li>
<li>在星际争霸任务上的实证评估显示，该方法优于现有分层多智能体强化学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.16386">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0a8be7e31993a4239e674834d5250d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ba1d4949c66ed769530ca4cffbfe470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99706e16fc96cf352c4a0d40b7a101d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f374dd307f94b1dafe02e846899e0c5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-845db53a71fc9b124551218313b4d8d8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5ace09a772af15c1e1590e4016d55c8c.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-02  DNB-AI-Project at SemEval-2025 Task 5 An LLM-Ensemble Approach for   Automated Subject Indexing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3e0b279adcebf07254a23f8c5be9f8c9.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-05-02  COMPACT COMPositional Atomic-to-Complex Visual Capability Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17447.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
