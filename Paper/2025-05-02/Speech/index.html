<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  Enhancing Health Mention Classification Performance A Study on   Advancements in Parameter Efficient Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-37d616ea42b2de1f7c3699cbe79557fa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-02-æ›´æ–°"><a href="#2025-05-02-æ›´æ–°" class="headerlink" title="2025-05-02 æ›´æ–°"></a>2025-05-02 æ›´æ–°</h1><h2 id="Enhancing-Health-Mention-Classification-Performance-A-Study-on-Advancements-in-Parameter-Efficient-Tuning"><a href="#Enhancing-Health-Mention-Classification-Performance-A-Study-on-Advancements-in-Parameter-Efficient-Tuning" class="headerlink" title="Enhancing Health Mention Classification Performance: A Study on   Advancements in Parameter Efficient Tuning"></a>Enhancing Health Mention Classification Performance: A Study on   Advancements in Parameter Efficient Tuning</h2><p><strong>Authors:Reem Abdel-Salam, Mary Adewunmi</strong></p>
<p>Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency. </p>
<blockquote>
<p>å¥åº·æåŠåˆ†ç±»ï¼ˆHMCï¼‰åœ¨åˆ©ç”¨ç¤¾äº¤åª’ä½“å¸–å­è¿›è¡Œå®æ—¶è·Ÿè¸ªå’Œå…¬å…±å«ç”Ÿç›‘æµ‹æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼ŒHMCçš„è¿‡ç¨‹ç”±äºå…¶å¤æ‚æ€§è´¨è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜ä¸»è¦æºäºå¥åº·æåŠçš„ä¸Šä¸‹æ–‡æ–¹é¢ï¼Œå¦‚å›¾é‡Šè¯­è¨€å’Œæè¿°æ€§æœ¯è¯­ï¼Œè€Œä¸æ˜¯æ˜ç¡®åæ˜ ä¸ªäººç–¾ç—…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®¤ä¸ºé€šè¿‡ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€æ–¹æ³•ï¼ˆNLPï¼‰çš„å¢å¼ºå‚æ•°çš„å¸¸è§„å¾®è°ƒå¯ä»¥å®ç°æ›´æ¸…æ™°çš„æåŠã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„æŠ€æœ¯ï¼Œå¦‚åˆ©ç”¨è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯ï¼Œæ”¹è¿›PEFTæŠ€æœ¯ï¼Œä»¥åŠå®ƒä»¬çš„ä¸åŒç»„åˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†RHDMã€PHMå’ŒIllnessä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚ç»“åˆè¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯å’Œåˆ©ç”¨PEFTæŠ€æœ¯ï¼Œåœ¨F1åˆ†æ•°æ–¹é¢æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œä¸ä¸‰ä¸ªæ•°æ®é›†ä¸­çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œé€šè¿‡ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹å’Œæœ‰æ•ˆçš„è®­ç»ƒå–å¾—äº†æ›´å¥½çš„æˆç»©ã€‚æ­¤å¤–ï¼Œç ”ç©¶ç»“æœå¼ºè°ƒäº†çº³å…¥è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯å’Œåˆ©ç”¨PEFTæŠ€æœ¯åœ¨HMCä¸­çš„æœ‰æ•ˆæ€§ã€‚æ€»ä¹‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ºå‡†ç¡®åˆ†ç±»ç¤¾äº¤åª’ä½“å¸–å­ä¸­çš„å¥åº·æåŠæä¾›äº†ä¸€ç§æ½œåœ¨çš„æœ‰æ•ˆæ–¹æ³•ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21685v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¤¾äº¤åª’ä½“å¸–å­è¿›è¡Œå®æ—¶è¿½è¸ªå’Œå…¬å…±å«ç”Ÿç›‘æµ‹æ—¶ï¼Œå¥åº·æåŠåˆ†ç±»ï¼ˆHMCï¼‰å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºå¥åº·æåŠçš„ä¸Šä¸‹æ–‡å› ç´ ï¼Œå¦‚æ¯”å–»è¯­è¨€å’Œæè¿°æ€§æœ¯è¯­ï¼Œè€Œéä¸ªäººç–¾ç—…çš„æ˜ç¡®åæ˜ ï¼ŒHMCè¿‡ç¨‹é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶é€šè¿‡æ”¹è¿›å‚æ•°å’Œé‡‡ç”¨ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ï¼ˆNLPï¼‰çš„ä¼ ç»Ÿå¾®è°ƒæ–¹å¼ï¼ŒåŠ›æ±‚å®ç°æ›´æ¸…æ™°çš„æåŠã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä¸åŒæŠ€æœ¯ï¼Œå¦‚åˆ©ç”¨è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯ã€æ”¹è¿›PEFTæŠ€æœ¯ä»¥åŠå®ƒä»¬çš„ç»„åˆä½¿ç”¨ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œèå…¥è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯å’Œåˆ©ç”¨PEFTæŠ€æœ¯èƒ½æ˜¾è‘—æé«˜F1åˆ†æ•°æ–¹é¢çš„æ€§èƒ½è¡¨ç°ï¼Œç›¸è¾ƒäºæ‰€æœ‰å¯¹æ¯”æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼ŒåŒæ—¶å®ç°äº†æ¨¡å‹çš„å°å‹åŒ–å’Œè®­ç»ƒæ•ˆç‡çš„æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°èå…¥è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯å’Œåˆ©ç”¨PEFTæŠ€æœ¯å¯¹äºHMCçš„æœ‰æ•ˆæ€§ã€‚æ€»ä¹‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ºå‡†ç¡®åˆ†ç±»ç¤¾äº¤åª’ä½“å¸–å­ä¸­çš„å¥åº·æåŠæä¾›äº†ä¸€ç§æ½œåœ¨çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥åº·æåŠåˆ†ç±»ï¼ˆHMCï¼‰åœ¨å®æ—¶è¿½è¸ªå’Œå…¬å…±å«ç”Ÿç›‘æµ‹æ–¹é¢åº”ç”¨å¹¿æ³›ã€‚</li>
<li>HMCé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºè¯†åˆ«ä¸Šä¸‹æ–‡å› ç´ ï¼ˆå¦‚æ¯”å–»è¯­è¨€å’Œæè¿°æ€§æœ¯è¯­ï¼‰å¯¼è‡´çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å®éªŒæ¢ç´¢äº†ä¸åŒæŠ€æœ¯è§£å†³æ­¤æŒ‘æˆ˜çš„æ–¹æ³•ï¼Œå¦‚è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯çš„åˆ©ç”¨å’ŒPEFTæŠ€æœ¯çš„æ”¹è¿›ç­‰ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜èå…¥è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯å’Œåˆ©ç”¨PEFTæŠ€æœ¯å¯æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•å±•ç°å‡ºå®ç°æ›´å°æ¨¡å‹å’Œæ›´é«˜æ•ˆè®­ç»ƒçš„å¯èƒ½æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜èå…¥è¯æ€§æ ‡æ³¨å™¨ä¿¡æ¯å’Œåˆ©ç”¨PEFTæŠ€æœ¯å¯¹äºHMCéå¸¸æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-275c02b592d8f85ad41d5e8b7cb480bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe153dbda787caf30be10ca30582ef95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-698d7d0196876147294d36e639828022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5b6b2a70275afbfc74ebeaae9cf6322.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="End-to-end-Audio-Deepfake-Detection-from-RAW-Waveforms-a-RawNet-Based-Approach-with-Cross-Dataset-Evaluation"><a href="#End-to-end-Audio-Deepfake-Detection-from-RAW-Waveforms-a-RawNet-Based-Approach-with-Cross-Dataset-Evaluation" class="headerlink" title="End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based   Approach with Cross-Dataset Evaluation"></a>End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based   Approach with Cross-Dataset Evaluation</h2><p><strong>Authors:Andrea Di Pierno, Luca Guarnera, Dario Allegra, Sebastiano Battiato</strong></p>
<p>Audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. Detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. In this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. Our model, RawNetLite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. To enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts Focal Loss to emphasize difficult or ambiguous samples. We further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging out-of-distribution test set (AVSpoof2021 + CodecFake). These findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/">https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/</a>. </p>
<blockquote>
<p>éŸ³é¢‘æ·±åº¦ä¼ªé€ æŠ€æœ¯å¯¹æ•°å­—å®‰å…¨å’Œä¿¡ä»»æ„æˆäº†æ—¥ç›Šå¢é•¿çš„å¨èƒï¼Œå®ƒåˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹äº§ç”Ÿåˆæˆè¯­éŸ³ï¼Œè¯¥è¯­éŸ³ç´§å¯†æ¨¡ä»¿çœŸå®çš„äººå£°ã€‚åœ¨å¼€æ”¾ä¸–ç•Œæ¡ä»¶ä¸‹æ£€æµ‹æ­¤ç±»æ“çºµå°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåœ¨æµ‹è¯•æœŸé—´é‡åˆ°çš„æ¬ºéª—æ–¹æ³•å¯èƒ½ä¸è®­ç»ƒæœŸé—´æ‰€è§çš„æœ‰æ‰€ä¸åŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºéŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹çš„ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç›´æ¥åœ¨åŸå§‹æ³¢å½¢ä¸Šè¿›è¡Œæ“ä½œã€‚æˆ‘ä»¬çš„æ¨¡å‹RawNetLiteæ˜¯ä¸€ä¸ªè½»é‡çº§çš„å·ç§¯é€’å½’æ¶æ„ï¼Œæ—¨åœ¨æ•æ‰å…‰è°±å’Œæ—¶åºç‰¹å¾ï¼Œæ— éœ€æ‰‹å·¥é¢„å¤„ç†ã€‚ä¸ºäº†æé«˜ç¨³å¥æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆå¤šä¸ªåŸŸæ•°æ®çš„è®­ç»ƒç­–ç•¥ï¼Œå¹¶é‡‡ç”¨Focal Lossæ¥å¼ºè°ƒå›°éš¾æˆ–æ¨¡ç³Šæ ·æœ¬ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œåœ¨åŸºäºç¼–ç å™¨çš„æ“ä½œå’Œæ³¢å½¢çº§åˆ«çš„éŸ³é¢‘å¢å¼ºï¼ˆä¾‹å¦‚éŸ³è°ƒè½¬æ¢ã€å™ªå£°å’Œæ—¶é—´æ‹‰ä¼¸ï¼‰çš„åŠ æŒä¸‹ï¼Œå¯ä»¥åœ¨ç°å®å£°å­¦æ¡ä»¶ä¸‹å®ç°æ˜¾è‘—çš„æ€»ä½“åŒ–æ”¹å–„ã€‚æ‰€æå‡ºæ¨¡å‹åœ¨åŸŸå†…æ•°æ®ï¼ˆFakeOrRealï¼‰ä¸Šå®ç°äº†è¶…è¿‡99.7%çš„F1åˆ†æ•°å’Œ0.25%çš„EERï¼ˆç­‰è¯¯ç‡ï¼‰ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸŸå¤–æµ‹è¯•é›†ï¼ˆAVSpoof2021 + CodecFakeï¼‰ä¸Šè¾¾åˆ°äº†83.4%çš„F1åˆ†æ•°å’Œ16.4%çš„EERã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¤šæ ·è®­ç»ƒæ•°æ®ã€å®šåˆ¶ç›®æ ‡å‡½æ•°å’ŒéŸ³é¢‘å¢å¼ºåœ¨æ„å»ºåšéŸ§ä¸”å¯æ³›åŒ–çš„éŸ³é¢‘ä¼ªé€ æ£€æµ‹å™¨ä¸­çš„é‡è¦æ€§ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/%E6%89%BE%E5%88%B0%E3%80%82">https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20923v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘æ·±åº¦ä¼ªé€ å¯¹æ•°å­—å®‰å…¨å’Œä¿¡ä»»æ„æˆçš„å¨èƒï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æ¡†æ¶RawNetLiteã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨åŸå§‹æ³¢å½¢ä¸Šç›´æ¥æ“ä½œï¼Œå…·æœ‰è½»é‡çº§å·ç§¯å¾ªç¯æ¶æ„ï¼Œå¯æ•è·é¢‘è°±å’Œæ—¶åºç‰¹å¾ï¼Œæ— éœ€æ‰‹å·¥é¢„å¤„ç†ã€‚é€šè¿‡ç»“åˆå¤šåŸŸæ•°æ®å’Œé‡‡ç”¨Focal Lossçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠåŸºäºç¼–ç å™¨çš„æ“ä½œå’Œæ³¢å½¢çº§åˆ«çš„éŸ³é¢‘å¢å¼ºæŠ€æœ¯ï¼Œè¯¥æ¨¡å‹åœ¨çœŸå®å£°å­¦æ¡ä»¶ä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šï¼Œå…¶F1åˆ†æ•°è¶…è¿‡99.7%ï¼ŒEERä¸º0.25%ï¼Œè€Œåœ¨è·¨åˆ†å¸ƒæµ‹è¯•é›†ä¸Šï¼ŒF1åˆ†æ•°è¾¾åˆ°83.4%ï¼ŒEERä¸º16.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘æ·±åº¦ä¼ªé€ å·²æˆä¸ºæ•°å­—å®‰å…¨å’Œä¿¡ä»»çš„é‡å¤§å¨èƒï¼Œéœ€è¦æœ‰æ•ˆçš„æ£€æµ‹æ‰‹æ®µã€‚</li>
<li>æå‡ºçš„RawNetLiteæ¨¡å‹èƒ½åœ¨åŸå§‹æ³¢å½¢ä¸Šæ“ä½œï¼Œç›´æ¥æ•è·éŸ³é¢‘ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹ç»“åˆå¤šåŸŸæ•°æ®å’ŒFocal Lossè®­ç»ƒç­–ç•¥ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥ç¼–ç å™¨çš„æ“ä½œå’Œæ³¢å½¢çº§åˆ«çš„éŸ³é¢‘å¢å¼ºï¼Œæ¨¡å‹åœ¨çœŸå®å£°å­¦æ¡ä»¶ä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>RawNetLiteæ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒF1åˆ†æ•°è¶…è¿‡99.7%ï¼ŒEERä½äº0.25%ã€‚</li>
<li>åœ¨è·¨åˆ†å¸ƒæµ‹è¯•é›†ä¸Šï¼Œæ¨¡å‹ä»è¡¨ç°å‡ºè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼ŒF1åˆ†æ•°è¾¾åˆ°83.4%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-14299610e7c842c8112447cf64391709.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed41944f3802c968d737bda348d41a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0baddf35a3a86fde89bfe76ffc33377c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93e7529f909222e5689d3529fdf7b97f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ea745dd90efaebfb3e755686aa2ce04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daa7f1e53cfd6fcbee768e3126d02768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd2040834139d25cf005474db16fee33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c0520245c4a8dfd88e9c9b9957afcf8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c2857cd1f4d5e29eb8333ef372f76e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cde12d0ca61dd67e19593f7453a4f255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abb2198e133ec70326afd8821e95ebe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-Task-Corrupted-Prediction-for-Learning-Robust-Audio-Visual-Speech-Representation"><a href="#Multi-Task-Corrupted-Prediction-for-Learning-Robust-Audio-Visual-Speech-Representation" class="headerlink" title="Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech   Representation"></a>Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech   Representation</h2><p><strong>Authors:Sungnyun Kim, Sungwoo Cho, Sangmin Bae, Kangwook Jang, Se-Young Yun</strong></p>
<p>Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sungnyun/cav2vec">https://github.com/sungnyun/cav2vec</a>. </p>
<blockquote>
<p>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†å¬è§‰å’Œè§†è§‰æ¨¡å¼æ¥æé«˜è¯†åˆ«ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å˜ˆæ‚çš„ç¯å¢ƒä¸­ï¼Œä»…ä½¿ç”¨éŸ³é¢‘çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ˜¯ä¸å¤Ÿçš„ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶ä¸»è¦è§£å†³äº†éŸ³é¢‘å¹²æ‰°é—®é¢˜ï¼Œä½†å¾ˆå°‘æœ‰ç ”ç©¶å¤„ç†è§†è§‰ä¸Šçš„è…è´¥ï¼Œä¾‹å¦‚å˜´å”‡é®æŒ¡æˆ–æ¨¡ç³Šçš„è§†é¢‘ï¼Œè¿™äº›ä¹Ÿä¼šå¯¹è¯†åˆ«é€ æˆæŸå®³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç°å®æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAV2vecï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†éŸ³é¢‘è§†è§‰è”åˆè…è´¥ã€‚CAV2vecé‡‡ç”¨è‡ªè’¸é¦æ³•ï¼Œé€šè¿‡è…è´¥é¢„æµ‹ä»»åŠ¡ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ é¢„æµ‹ç”±æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„å¹²å‡€ç›®æ ‡ï¼ŒåŒæ—¶è¾“å…¥å¸¦æœ‰è…è´¥çš„å¸§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨å•æ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ ï¼Œé€šè¿‡é¢„æµ‹å¸¦æœ‰è…è´¥è§†é¢‘çš„å¹²å‡€éŸ³é¢‘ç›®æ ‡å’Œå¸¦æœ‰è…è´¥éŸ³é¢‘çš„å¹²å‡€è§†é¢‘ç›®æ ‡ï¼Œæç‚¼è·¨æ¨¡æ€çŸ¥è¯†å¹¶å¯¹é½è…è´¥æ¨¡å¼ã€‚è¿™ä¸€ç­–ç•¥å‡è½»äº†ç”±è…è´¥æ¨¡å¼é€ æˆçš„è¡¨ç¤ºç©ºé—´åˆ†æ•£é—®é¢˜ï¼Œå®ç°äº†æ›´å¯é å’Œç¨³å¥çš„è§†å¬èåˆã€‚æˆ‘ä»¬åœ¨ç¨³å¥çš„AVSRåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè…è´¥è¡¨ç¤ºå­¦ä¹ æ–¹æ³•åœ¨æ¶‰åŠå¤šç§ç±»å‹è…è´¥çš„é€šç”¨ç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†è¯†åˆ«ç²¾åº¦ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sungnyun/cav2vec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sungnyun/cav2vecæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18539v2">PDF</a> ICLR 2025; 22 pages, 6 figures, 14 tables</p>
<p><strong>Summary</strong></p>
<p>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†å¬è§‰å’Œè§†è§‰æ¨¡æ€ï¼Œä»¥æé«˜è¯†åˆ«ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°ç¯å¢ƒä¸­ï¼Œä»…ä½¿ç”¨éŸ³é¢‘çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ˜¯ä¸å¤Ÿçš„ã€‚é’ˆå¯¹è§†è§‰å¤±çœŸï¼ˆå¦‚å˜´å”‡é®æŒ¡æˆ–æ¨¡ç³Šè§†é¢‘ï¼‰ç­‰ç°å®æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è‡ªæˆ‘ç›‘ç£è¯­éŸ³è¡¨å¾å­¦ä¹ æ¡†æ¶CAV2vecã€‚CAV2vecé‡‡ç”¨è‡ªæˆ‘è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡é¢„æµ‹å¸¦æœ‰å™ªå£°è¾“å…¥å¸§çš„æ¸…æ´ç›®æ ‡ï¼Œä½¿å­¦ç”Ÿæ¨¡å‹å­¦ä¹ é¢„æµ‹æ¸…æ´ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å•æ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ ï¼Œå®ƒé€šè¿‡è’¸é¦è·¨æ¨¡æ€çŸ¥è¯†å¹¶å¯¹é½å™ªå£°æ¨¡æ€ï¼Œé€šè¿‡é¢„æµ‹å¸¦æœ‰å™ªå£°è§†é¢‘çš„æ¸…æ´éŸ³é¢‘ç›®æ ‡å’Œå¸¦æœ‰å™ªå£°éŸ³é¢‘çš„æ¸…æ´è§†é¢‘ç›®æ ‡æ¥å®ç°ã€‚è¿™ç§æ–¹æ³•å‡è½»äº†å™ªå£°æ¨¡æ€å¼•èµ·çš„è¡¨ç¤ºç©ºé—´åˆ†æ•£é—®é¢˜ï¼Œå®ç°äº†æ›´å¯é å’Œç¨³å¥çš„è§†å¬èåˆã€‚å®éªŒè¯æ˜ï¼Œåœ¨æ¶‰åŠå¤šç§ç±»å‹å™ªå£°çš„é€šç”¨ç¯å¢ƒä¸­ï¼Œè¢«å™ªå£°å½±å“çš„è¡¨å¾å­¦ä¹ æ–¹æ³•æ˜¾è‘—æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚ä»£ç å¯ä»ç›¸å…³ç½‘ç«™è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰èåˆå¬è§‰å’Œè§†è§‰æ¨¡æ€æé«˜è¯†åˆ«ç²¾åº¦ï¼Œå°¤å…¶åœ¨å™ªå£°ç¯å¢ƒä¸‹ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨éŸ³é¢‘å¹²æ‰°ï¼Œä½†è§†è§‰å¤±çœŸï¼ˆå¦‚å˜´å”‡é®æŒ¡ã€æ¨¡ç³Šè§†é¢‘ç­‰ï¼‰åŒæ ·å¯¹è¯†åˆ«é€ æˆè´Ÿé¢å½±å“ã€‚</li>
<li>CAV2vecæ˜¯ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨å¾å­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡æ¥å¤„ç†è§†å¬è”åˆå¤±çœŸã€‚</li>
<li>CAV2vecé‡‡ç”¨è‡ªæˆ‘è’¸é¦æ–¹æ³•å’Œé¢„æµ‹å¸¦æœ‰å™ªå£°è¾“å…¥å¸§çš„æ¸…æ´ç›®æ ‡ç­–ç•¥ã€‚</li>
<li>å•æ¨¡æ€å¤šä»»åŠ¡å­¦ä¹ é€šè¿‡è’¸é¦è·¨æ¨¡æ€çŸ¥è¯†å’Œå¯¹é½å™ªå£°æ¨¡æ€å®ç°å¯é å’Œç¨³å¥çš„è§†å¬èåˆã€‚</li>
<li>CAV2vecé€šè¿‡åœ¨é¢„æµ‹æ¸…æ´éŸ³é¢‘ç›®æ ‡æ—¶ä½¿ç”¨å™ªå£°è§†é¢‘å’Œé¢„æµ‹æ¸…æ´è§†é¢‘ç›®æ ‡æ—¶ä½¿ç”¨å™ªå£°éŸ³é¢‘æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2f4b9d4d9a31afd4f4d98987acb011b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-920f0ebd907c0f7684aec2e1315e7afd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bed7689f2e0e2c0f447f681a2c35a4f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd518048ff155a6f9e63f563df9a028d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4adcd2ad70307c0e3e7d40db1bb8ebf0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts"><a href="#FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts" class="headerlink" title="FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"></a>FleSpeech: Flexibly Controllable Speech Generation with Various Prompts</h2><p><strong>Authors:Hanzhao Li, Yuke Li, Xinsheng Wang, Jingbin Hu, Qicong Xie, Shan Yang, Lei Xie</strong></p>
<p>Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speakerâ€™s timbre, or choosing a style and generating a voice that matches a characterâ€™s visual appearance. To overcome these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at <a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech/">https://kkksuper.github.io/FleSpeech/</a> </p>
<blockquote>
<p>å¯æ§è¯­éŸ³ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€æˆ–å›ºå®šçš„æç¤ºï¼Œè¿™é™åˆ¶äº†åˆ›é€ æ€§å’Œçµæ´»æ€§ã€‚è¿™äº›å±€é™æ€§ä½¿å¾—åœ¨æŸäº›åœºæ™¯ä¸‹éš¾ä»¥æ»¡è¶³ç‰¹å®šç”¨æˆ·çš„éœ€æ±‚ï¼Œä¾‹å¦‚åœ¨ä¿ç•™é€‰å®šæ¼”è®²è€…éŸ³è´¨çš„åŒæ—¶è°ƒæ•´é£æ ¼ï¼Œæˆ–è€…é€‰æ‹©é£æ ¼å¹¶ç”Ÿæˆä¸è§’è‰²å¤–è§‚ç›¸åŒ¹é…çš„è¯­éŸ³ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FleSpeechï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šé˜¶æ®µè¯­éŸ³ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå„ç§æ§åˆ¶å½¢å¼ï¼Œå…è®¸æ›´çµæ´»åœ°æ“ä½œè¯­éŸ³å±æ€§ã€‚FleSpeeché‡‡ç”¨å¤šæ¨¡æ€æç¤ºç¼–ç å™¨ï¼Œå¤„ç†å’Œç»Ÿä¸€ä¸åŒçš„æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æç¤ºï¼Œå½¢æˆè¿è´¯çš„è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¢å¼ºäº†è¯­éŸ³åˆæˆçš„é€‚åº”æ€§ï¼Œæ”¯æŒå¯¹ç”Ÿæˆè¯­éŸ³è¿›è¡Œåˆ›é€ æ€§å’Œç²¾ç¡®çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†å¤šæ¨¡æ€æ•°æ®é›†çš„æ•°æ®æ”¶é›†ç®¡é“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚ç»¼åˆçš„ä¸»è§‚å’Œå®¢è§‚å®éªŒè¯æ˜äº†FleSpeechçš„æœ‰æ•ˆæ€§ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://kkksuper.github.io/FleSpeech/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04644v2">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong><br>è¯­éŸ³ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºå•ä¸€æˆ–å›ºå®šçš„æç¤ºï¼Œé™åˆ¶äº†åˆ›é€ æ€§å’Œçµæ´»æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºFleSpeechï¼Œä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µè¯­éŸ³ç”Ÿæˆæ¡†æ¶ï¼Œé€šè¿‡æ•´åˆå„ç§å½¢å¼çš„æ§åˆ¶ï¼Œå…è®¸æ›´çµæ´»åœ°æ“ä½œè¯­éŸ³å±æ€§ã€‚FleSpeeché‡‡ç”¨å¤šæ¨¡æ€æç¤ºç¼–ç å™¨ï¼Œå¤„ç†å’Œç»Ÿä¸€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰æç¤ºï¼Œç”Ÿæˆè¿è´¯çš„è¯­éŸ³ã€‚è¯¥æ¡†æ¶æé«˜äº†è¯­éŸ³åˆæˆçš„é€‚åº”æ€§ï¼Œæ”¯æŒå¯¹ç”Ÿæˆè¯­éŸ³çš„åˆ›é€ æ€§ä¸ç²¾ç¡®æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­éŸ³ç”Ÿæˆæ–¹æ³•å­˜åœ¨åˆ›é€ æ€§å’Œçµæ´»æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>FleSpeechæ˜¯ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µè¯­éŸ³ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>FleSpeeché€šè¿‡å¤šæ¨¡æ€æç¤ºç¼–ç å™¨ï¼Œæ•´åˆä¸åŒå½¢å¼çš„æ§åˆ¶ï¼Œå®ç°æ›´çµæ´»çš„è¯­éŸ³å±æ€§æ“ä½œã€‚</li>
<li>FleSpeeché‡‡ç”¨æ•°æ®æ”¶é›†ç®¡é“ï¼Œç”¨äºæ”¶é›†å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
<li>FleSpeechæé«˜äº†è¯­éŸ³åˆæˆçš„é€‚åº”æ€§ï¼Œå¹¶æ”¯æŒå¯¹ç”Ÿæˆè¯­éŸ³çš„åˆ›é€ æ€§ä¸ç²¾ç¡®æ§åˆ¶ã€‚</li>
<li>é€šè¿‡ç»¼åˆçš„ä¸»è§‚å’Œå®¢è§‚å®éªŒéªŒè¯ï¼Œè¯æ˜äº†FleSpeechçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04644">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-32f8dbe6366baf9fb97790e7f970cd96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-682fe78391efffab7b91b971a10ef258.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Exploring-Acoustic-Similarity-in-Emotional-Speech-and-Music-via-Self-Supervised-Representations"><a href="#Exploring-Acoustic-Similarity-in-Emotional-Speech-and-Music-via-Self-Supervised-Representations" class="headerlink" title="Exploring Acoustic Similarity in Emotional Speech and Music via   Self-Supervised Representations"></a>Exploring Acoustic Similarity in Emotional Speech and Music via   Self-Supervised Representations</h2><p><strong>Authors:Yujia Sun, Zeyu Zhao, Korin Richmond, Yuanchao Li</strong></p>
<p>Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems. </p>
<blockquote>
<p>è¯­éŸ³å’ŒéŸ³ä¹çš„æƒ…æ„Ÿè¯†åˆ«ç”±äºå£°éŸ³é‡å è€Œå…·æœ‰ç›¸ä¼¼æ€§ï¼Œè¿™å¼•å‘äº†åœ¨è¿™ä¸¤ä¸ªé¢†åŸŸä¹‹é—´è½¬ç§»çŸ¥è¯†çš„å…´è¶£ã€‚ç„¶è€Œï¼Œè¯­éŸ³å’ŒéŸ³ä¹ä¹‹é—´çš„å…±äº«å£°å­¦çº¿ç´¢ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ç”±è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ç¼–ç çš„çº¿ç´¢ï¼Œä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚è€ƒè™‘åˆ°è¯­éŸ³å’ŒéŸ³ä¹çš„SSLæ¨¡å‹å¾ˆå°‘åº”ç”¨äºè·¨åŸŸç ”ç©¶ï¼Œè¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å…³æ³¨æƒ…æ„Ÿè¯­éŸ³å’ŒéŸ³ä¹ä¹‹é—´çš„å£°å­¦ç›¸ä¼¼æ€§ï¼Œä»åˆ†æç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å’ŒéŸ³ä¹æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰çš„SSLæ¨¡å‹çš„å±‚çº§è¡Œä¸ºå¼€å§‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹çš„å‡ ç§æ–¹æ³•çš„æ¯”è¾ƒæ¥è¿›è¡Œè·¨åŸŸé€‚åº”ï¼Œç ”ç©¶å¦‚ä½•åˆ©ç”¨éŸ³ä¹è¿›è¡ŒSERå’Œåˆ©ç”¨è¯­éŸ³è¿›è¡ŒMERçš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨FrechetéŸ³é¢‘è·ç¦»æ¥æ¢ç´¢æƒ…æ„Ÿè¯­éŸ³å’ŒéŸ³ä¹ä¹‹é—´çš„å£°å­¦ç›¸ä¼¼æ€§ï¼Œæ­ç¤ºäº†åœ¨è¯­éŸ³å’ŒéŸ³ä¹SSLæ¨¡å‹ä¸­çš„æƒ…æ„Ÿåè§é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¯­éŸ³å’ŒéŸ³ä¹çš„SSLæ¨¡å‹ç¡®å®æ•æ‰åˆ°äº†å…±äº«çš„å£°å­¦ç‰¹å¾ï¼Œä½†ç”±äºå…¶è®­ç»ƒç­–ç•¥å’Œé¢†åŸŸç‰¹å¼‚æ€§ï¼Œå®ƒä»¬çš„è¡Œä¸ºå¯èƒ½ä¼šå› ä¸åŒæƒ…æ„Ÿè€Œæœ‰æ‰€å˜åŒ–ã€‚æ­¤å¤–ï¼Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒå¯ä»¥é€šè¿‡åˆ©ç”¨å½¼æ­¤çš„çŸ¥è¯†æ¥æé«˜SERå’ŒMERçš„æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶æä¾›äº†æƒ…æ„Ÿè¯­éŸ³å’ŒéŸ³ä¹ä¹‹é—´å£°å­¦ç›¸ä¼¼æ€§çš„æ–°è§è§£ï¼Œå¹¶çªå‡ºäº†è·¨åŸŸæ³›åŒ–åœ¨æ”¹è¿›SERå’ŒMERç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17899v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³ä¸éŸ³ä¹çš„æƒ…æ„Ÿè¯†åˆ«å› å£°å­¦é‡å è€Œå…·æœ‰ç›¸ä¼¼æ€§ï¼Œå¼•å‘äº†è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»çš„å…´è¶£ã€‚ç„¶è€Œï¼Œè¯­éŸ³ä¸éŸ³ä¹ä¹‹é—´å…±äº«çš„å£°å­¦çº¿ç´¢ï¼Œç‰¹åˆ«æ˜¯è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹æ‰€ç¼–ç çš„çº¿ç´¢ï¼Œåœ¨è·¨åŸŸç ”ç©¶ä¸­åº”ç”¨è¾ƒå°‘ã€‚æœ¬æ–‡é‡æ–°æ¢è®¨äº†æƒ…æ„Ÿè¯­éŸ³å’ŒéŸ³ä¹ä¹‹é—´çš„å£°å­¦ç›¸ä¼¼æ€§ï¼Œåˆ†æäº†SSLæ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å’ŒéŸ³ä¹æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ä¸­çš„é€å±‚è¡Œä¸ºã€‚é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹çš„å¤šç§æ–¹æ³•è¿›è¡Œè·¨åŸŸé€‚é…ï¼Œå¹¶æ¢ç´¢äº†æœ‰æ•ˆçš„æ–¹å¼åˆ©ç”¨éŸ³ä¹è¿›è¡ŒSERå’Œè¯­éŸ³è¿›è¡ŒMERã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä½¿ç”¨FrechetéŸ³é¢‘è·ç¦»ç ”ç©¶è¯­éŸ³å’ŒéŸ³ä¹åœ¨ä¸ªä½“æƒ…æ„Ÿä¸Šçš„å£°å­¦ç›¸ä¼¼æ€§ï¼Œå¹¶å‘ç°æƒ…æ„Ÿåè§é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶è¯­éŸ³å’ŒéŸ³ä¹SSLæ¨¡å‹ç¡®å®æ•æ‰åˆ°äº†å…±äº«çš„å£°å­¦ç‰¹å¾ï¼Œä½†å®ƒä»¬åœ¨ä¸åŒæƒ…æ„Ÿä¸Šçš„è¡¨ç°å¯èƒ½ä¼šå› è®­ç»ƒç­–ç•¥å’Œé¢†åŸŸç‰¹æ€§è€Œæœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯å¯ä»¥æé«˜è·¨é¢†åŸŸçš„æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚è¯¥ç ”ç©¶æä¾›äº†è¯­éŸ³å’Œæƒ…æ„ŸéŸ³ä¹å£°å­¦ç›¸ä¼¼æ€§çš„æ–°è§†è§’ï¼Œå¹¶çªæ˜¾äº†è·¨é¢†åŸŸæ³›åŒ–åœ¨æ”¹è¿›SERå’ŒMERç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ä¸éŸ³ä¹åœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢å­˜åœ¨å£°å­¦ç›¸ä¼¼æ€§ï¼Œå¼•å‘è·¨é¢†åŸŸçŸ¥è¯†è¿ç§»çš„å…´è¶£ã€‚</li>
<li>SSLæ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å’ŒéŸ³ä¹æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ä¸­çš„è¡Œä¸ºåˆ†ææ˜¾ç¤ºï¼Œå®ƒä»¬ç¡®å®æ•æ‰åˆ°äº†å…±äº«çš„å£°å­¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹è¿›è¡Œè·¨åŸŸé€‚é…ï¼Œå‘ç°åˆ©ç”¨éŸ³ä¹è¿›è¡ŒSERå’Œè¯­éŸ³è¿›è¡ŒMERçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨FrechetéŸ³é¢‘è·ç¦»ç ”ç©¶è¯­éŸ³å’ŒéŸ³ä¹åœ¨ä¸ªä½“æƒ…æ„Ÿä¸Šçš„å£°å­¦ç›¸ä¼¼æ€§ï¼Œæ­ç¤ºäº†æƒ…æ„Ÿåè§é—®é¢˜ã€‚</li>
<li>SSLæ¨¡å‹çš„è¡Œä¸ºåœ¨ä¸åŒæƒ…æ„Ÿä¸Šå¯èƒ½å› è®­ç»ƒç­–ç•¥å’Œé¢†åŸŸç‰¹æ€§è€Œæœ‰æ‰€ä¸åŒã€‚</li>
<li>å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯å¯ä»¥æé«˜è·¨é¢†åŸŸçš„æƒ…æ„Ÿè¯†åˆ«æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c28d276e46b51bc69b84373cbc423ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d4300bba03773e3de75e1463aa46c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff13a5e136d13e80c0acfd352c53f04c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-505eb59a7f9fd2c836b0eca6b9ac18c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6f95e8d82eb4ac91a6c6d632d46a676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85362bb5e11d8eee119a101e46631d69.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Cognitive-State-Classification-from-Speech-with-Multi-View-Pseudo-Labeling"><a href="#Semi-Supervised-Cognitive-State-Classification-from-Speech-with-Multi-View-Pseudo-Labeling" class="headerlink" title="Semi-Supervised Cognitive State Classification from Speech with   Multi-View Pseudo-Labeling"></a>Semi-Supervised Cognitive State Classification from Speech with   Multi-View Pseudo-Labeling</h2><p><strong>Authors:Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai</strong></p>
<p>The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines. </p>
<blockquote>
<p>åœ¨è¯­éŸ³åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œç¼ºä¹æ ‡æ³¨æ•°æ®æ˜¯ä¸€ä¸ªå¸¸è§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è¿›è¡Œå¤§é‡ä¸»è§‚è¯„ä¼°çš„ä»»åŠ¡ä¸­ï¼Œå¦‚è®¤çŸ¥çŠ¶æ€åˆ†ç±»ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šè§†è§’ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å£°å­¦å’Œè¯­è¨€å­¦ç‰¹å¾æ¥é€‰æ‹©æœ€ç¡®ä¿¡çš„æ•°æ®æ¥è®­ç»ƒåˆ†ç±»æ¨¡å‹ã€‚åœ¨å£°å­¦æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨FrechetéŸ³é¢‘è·ç¦»æ¯”è¾ƒæœªæ ‡è®°æ•°æ®å’Œå·²æ ‡è®°æ•°æ®ï¼Œè¯¥è·ç¦»æ˜¯æ ¹æ®å¤šä¸ªéŸ³é¢‘ç¼–ç å™¨ç”Ÿæˆçš„åµŒå…¥è®¡ç®—å¾—å‡ºçš„ã€‚åœ¨è¯­è¨€å­¦æ–¹é¢ï¼Œæˆ‘ä»¬æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¿®è®¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è½¬å½•ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬æå‡ºçš„ç‰¹å®šä»»åŠ¡çŸ¥è¯†é¢„æµ‹æ ‡ç­¾ã€‚å½“ä¸¤ä¸ªæ¥æºçš„ä¼ªæ ‡ç­¾ä¸€è‡´æ—¶ï¼Œå³å¯ç¡®å®šé«˜ä¿¡å¿ƒæ•°æ®ï¼Œè€Œä¸åŒ¹é…çš„æ•°æ®åˆ™è¢«è§†ä¸ºä½ä¿¡å¿ƒæ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªåŒæ¨¡æ€åˆ†ç±»å™¨ï¼Œä»¥è¿­ä»£æ–¹å¼å¯¹ä½ä¿¡å¿ƒæ•°æ®è¿›è¡Œæ ‡è®°ï¼Œç›´åˆ°æ»¡è¶³é¢„è®¾æ ‡å‡†ã€‚æˆ‘ä»¬åœ¨æƒ…æ„Ÿè¯†åˆ«å’Œç—´å‘†æ£€æµ‹ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„SSLæ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä½¿ç”¨ä»…30%çš„æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¸å…¨ç›‘ç£å­¦ä¹ ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”æ˜¾è‘—ä¼˜äºä¸¤ä¸ªé€‰å®šçš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16937v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ¡†æ¶ï¼Œé’ˆå¯¹è¯­éŸ³åˆ†ç±»ä»»åŠ¡ä¸­ç¼ºä¹æ ‡ç­¾æ•°æ®çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯éœ€è¦ä¸»è§‚è¯„ä¼°çš„è®¤çŸ¥çŠ¶æ€åˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¤šè§†è§’ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œåˆ©ç”¨å£°å­¦åŠè¯­è¨€ç‰¹æ€§é€‰æ‹©æœ€ç¡®å®šçš„æ•°æ®æ¥è®­ç»ƒåˆ†ç±»æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æƒ…ç»ªè¯†åˆ«å’Œç—´å‘†æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸å®Œå…¨ç›‘ç£å­¦ä¹ ç›¸æ¯”ä»…ä½¿ç”¨30%çš„æ ‡ç­¾æ•°æ®å³å¯è¾¾åˆ°ç«äº‰æ°´å¹³ï¼Œå¹¶æ˜¾è‘—ä¼˜äºä¸¤ä¸ªé€‰å®šçš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹æ ‡ç­¾æ•°æ®æ˜¯è¯­éŸ³åˆ†ç±»ä»»åŠ¡çš„å¸¸è§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä¸»è§‚è¯„ä¼°çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>æå‡ºäº†åŸºäºåŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å£°å­¦åŠè¯­è¨€ç‰¹æ€§æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†æ–°é¢–çš„å¤šè§†è§’ä¼ªæ ‡ç­¾æ–¹æ³•ï¼Œç»“åˆå£°å­¦åŠè¯­è¨€ç‰¹æ€§é€‰æ‹©æœ€ç¡®å®šçš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>é€šè¿‡FrechetéŸ³é¢‘è·ç¦»å’Œå¤šä¸ªéŸ³é¢‘ç¼–ç å™¨ç”Ÿæˆçš„åµŒå…¥è¿›è¡Œæ¯”è¾ƒï¼Œå¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œå£°å­¦è¯„ä¼°ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¿®è®¢è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è½¬å½•å¹¶åŸºäºç‰¹å®šä»»åŠ¡çŸ¥è¯†é¢„æµ‹æ ‡ç­¾ã€‚</li>
<li>å½“ä¼ªæ ‡ç­¾ä»ä¸¤ä¸ªæºå¯¹é½æ—¶ï¼Œç¡®å®šé«˜ç½®ä¿¡åº¦æ•°æ®ï¼Œè€Œå°†ä¸åŒ¹é…çš„æ•°æ®è§†ä¸ºä½ç½®ä¿¡åº¦æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16937">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1efe05a2b871868c9f2ab60a8916289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96ee64cece6972217ad2925293c58fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f1f1a1aca9848c96305dd64c9893bac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85bba63cef4ebe15b0b880bf2de53d4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cab75ebaf57ba910db551b76f53b652b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af95c52c2f7f2040a77b3ab74d63c37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-617a1e60214fd1c767ce4f2353f37a6b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Speech-Emotion-Recognition-Humans-vs-Self-Supervised-Models"><a href="#Cross-Lingual-Speech-Emotion-Recognition-Humans-vs-Self-Supervised-Models" class="headerlink" title="Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised   Models"></a>Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised   Models</h2><p><strong>Authors:Zhichen Han, Tianqi Geng, Hui Feng, Jiahong Yuan, Korin Richmond, Yuanchao Li</strong></p>
<p>Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception. </p>
<blockquote>
<p>åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹è¿›è¡Œè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰å·²ç»è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†å…³äºè·¨è¯­è¨€åœºæ™¯çš„ç ”ç©¶æœ‰é™ã€‚æœ¬ç ”ç©¶é€šè¿‡å¯¹äººç±»è¡¨ç°å’ŒSSLæ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œä»é€å±‚åˆ†æçš„è§’åº¦æ¢ç´¢äº†åœ¨å•è¯­ã€è·¨è¯­å’Œè¿ç§»å­¦ä¹ èƒŒæ™¯ä¸‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨è¯­å¥å’Œåˆ†æ®µä¸¤ä¸ªå±‚é¢ä¸Šæ¯”è¾ƒäº†æ¨¡å‹å’Œäººç±»çš„SERèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡äººç±»è¯„ä¼°æ¢è®¨äº†æ–¹è¨€å¯¹è·¨è¯­è¨€SERçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨é€‚å½“çš„çŸ¥è¯†è¿ç§»ä¸‹ï¼Œæ¨¡å‹èƒ½å¤Ÿé€‚åº”ç›®æ ‡è¯­è¨€å¹¶è¾¾åˆ°ä¸æ¯è¯­è€…ç›¸å½“çš„è¡¨ç°æ°´å¹³ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æ–¹è¨€å¯¹äºç¼ºä¹å…ˆéªŒè¯­è¨€å’Œè¯­è¨€è¾…åŠ©èƒŒæ™¯çš„ä¸ªä½“åœ¨SERæ–¹é¢çš„é‡å¤§å½±å“ã€‚æ­¤å¤–ï¼Œæ— è®ºæ˜¯äººç±»è¿˜æ˜¯æ¨¡å‹åœ¨ä¸åŒæƒ…æ„Ÿæ–¹é¢éƒ½è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºã€‚è¿™äº›ç»“æœä¸ºæˆ‘ä»¬æä¾›äº†å…³äºSSLæ¨¡å‹çš„è·¨è¯­è¨€SERèƒ½åŠ›çš„å…¨æ–°è§è§£ï¼Œå¹¶å¼ºè°ƒäº†å®ƒä»¬ä¸äººç±»æƒ…æ„Ÿæ„ŸçŸ¥çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16920v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„ç ”ç©¶ã€‚æ–‡ç« å¯¹æ¯”äº†äººç±»ä¸SSLæ¨¡å‹åœ¨å•è¯­ã€è·¨è¯­è¨€å’Œè¿ç§»å­¦ä¹ èƒŒæ™¯ä¸‹çš„æ€§èƒ½ï¼Œå¹¶è¿›è¡Œäº†åˆ†å±‚åˆ†æå’Œå‚æ•°ä¼˜åŒ–å¾®è°ƒç­–ç•¥çš„æ¢ç´¢ã€‚ç ”ç©¶å‘ç°ï¼Œé€‚å½“çš„çŸ¥è¯†è¿ç§»å¯ä»¥ä½¿æ¨¡å‹é€‚åº”ç›®æ ‡è¯­è¨€ï¼Œå¹¶è¾¾åˆ°æ¥è¿‘æ¯è¯­è€…çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†æ–¹è¨€å¯¹è·¨è¯­è¨€SERçš„å½±å“ï¼Œå¹¶å±•ç¤ºäº†äººç±»è¯„ä¼°ç»“æœã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œäººç±»å’Œæ¨¡å‹åœ¨ä¸åŒæƒ…æ„Ÿä¸‹è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>è·¨è¯­è¨€æƒ…å¢ƒä¸‹çš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç ”ç©¶ä»ç„¶æœ‰é™ã€‚</li>
<li>è¿›è¡Œäº†äººç±»ä¸è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”åˆ†æï¼ŒåŒ…æ‹¬å±‚çº§åˆ†æå’Œå‚æ•°ä¼˜åŒ–å¾®è°ƒç­–ç•¥ã€‚</li>
<li>é€‚å½“çš„çŸ¥è¯†è¿ç§»ä½¿æ¨¡å‹èƒ½é€‚åº”ç›®æ ‡è¯­è¨€ï¼Œå¹¶æ¥è¿‘æ¯è¯­è€…çš„æ€§èƒ½ã€‚</li>
<li>æ–¹è¨€å¯¹è·¨è¯­è¨€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æœ‰é‡è¦å½±å“ï¼Œç‰¹åˆ«æ˜¯å¯¹é‚£äº›æ²¡æœ‰è¯­è¨€å­¦å’Œè¯­ç”¨å­¦èƒŒæ™¯çš„äººã€‚</li>
<li>äººç±»å’Œæ¨¡å‹åœ¨ä¸åŒæƒ…æ„Ÿä¸‹è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac1d9653d2692844f5a4f885d55673c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0d84164cc09ad79177a5e016c86c3ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-526bebed1c9775dbb61b484bd18fa458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-087e8415f393afbf8ecbb604fea6134e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revise-Reason-and-Recognize-LLM-Based-Emotion-Recognition-via-Emotion-Specific-Prompts-and-ASR-Error-Correction"><a href="#Revise-Reason-and-Recognize-LLM-Based-Emotion-Recognition-via-Emotion-Specific-Prompts-and-ASR-Error-Correction" class="headerlink" title="Revise, Reason, and Recognize: LLM-Based Emotion Recognition via   Emotion-Specific Prompts and ASR Error Correction"></a>Revise, Reason, and Recognize: LLM-Based Emotion Recognition via   Emotion-Specific Prompts and ASR Error Correction</h2><p><strong>Authors:Yuanchao Li, Yuan Gong, Chao-Han Huck Yang, Peter Bell, Catherine Lai</strong></p>
<p>Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥ï¼Œä½¿ç”¨æç¤ºå·¥ç¨‹è¿›è¡Œè¯­éŸ³æƒ…æ„Ÿçš„æ ‡æ³¨å’Œè¯†åˆ«æœ€è¿‘å¼€å§‹æ˜¾ç°ï¼Œä½†å…¶æœ‰æ•ˆæ€§å’Œå¯é æ€§ä»å­˜åœ¨ç–‘é—®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸€ä¸»é¢˜è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ï¼Œé¦–å…ˆæå‡ºäº†èå…¥å£°å­¦ã€è¯­è¨€å­¦å’Œå¿ƒç†å­¦ä¸­çš„æƒ…æ„Ÿç‰¹å®šçŸ¥è¯†çš„æ–°æç¤ºã€‚éšåï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºLLMçš„æç¤ºåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸Šçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶ä¸çœŸå®è½¬å½•è¿›è¡Œäº†å¯¹æ¯”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºå­˜åœ¨ASRé”™è¯¯çš„å£è¯­æƒ…æ„Ÿè¯†åˆ«æå‡ºäº†ä¿®è®¢ã€æ¨ç†ã€è¯†åˆ«çš„æç¤ºç®¡é“ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…³äºæƒ…å¢ƒæ„ŸçŸ¥å­¦ä¹ ã€ä¸Šä¸‹æ–‡å­¦ä¹ å’ŒæŒ‡ä»¤è°ƒæ•´çš„è¯•éªŒï¼Œä»¥æ£€éªŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨æ­¤æ–¹å‘ä¸Šçš„å®ç”¨æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMså¯¹å¾®å°æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚å®éªŒç»“æœè¯æ˜äº†æƒ…æ„Ÿç‰¹å®šæç¤ºã€ASRé”™è¯¯ä¿®æ­£å’ŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨åŸºäºLLMçš„æƒ…æ„Ÿè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å®Œå–„LLMsåœ¨æƒ…æ„Ÿè¯†åˆ«å’Œç›¸å…³é¢†åŸŸçš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15551v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹è¿›è¡Œè¯­éŸ³æƒ…æ„Ÿæ ‡æ³¨å’Œè¯†åˆ«å·²æˆä¸ºæ–°å…´ç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç³»åˆ—åŒ…å«å£°å­¦ã€è¯­è¨€å­¦å’Œå¿ƒç†å­¦æƒ…æ„Ÿç‰¹å®šçŸ¥è¯†çš„æ–°å‹æç¤ºï¼Œå¹¶ç³»ç»Ÿåœ°ç ”ç©¶äº†å…¶åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸Šçš„æ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§Revise-Reason-Recognizeçš„æç¤ºæµç¨‹ï¼Œç”¨äºåœ¨å­˜åœ¨è¯­éŸ³è¯†åˆ«é”™è¯¯çš„æƒ…å†µä¸‹ï¼Œå®ç°ç¨³å¥çš„åŸºäºLLMçš„æƒ…æ„Ÿè¯†åˆ«ã€‚é€šè¿‡å®éªŒï¼Œæœ¬æ–‡éªŒè¯äº†æƒ…æ„Ÿç‰¹å®šæç¤ºã€ASRè¯¯å·®æ ¡æ­£å’ŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨LLMæƒ…æ„Ÿè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­é€šè¿‡æç¤ºå·¥ç¨‹è¿›è¡Œåº”ç”¨ã€‚</li>
<li>æå‡ºäº†åŒ…å«å£°å­¦ã€è¯­è¨€å­¦å’Œå¿ƒç†å­¦æƒ…æ„Ÿç‰¹å®šçŸ¥è¯†çš„æ–°å‹æç¤ºã€‚</li>
<li>ç ”ç©¶äº†æ–°å‹æç¤ºåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•ä¸Šçš„æ•ˆæœã€‚</li>
<li>æå‡ºäº†Revise-Reason-Recognizeçš„æç¤ºæµç¨‹ï¼Œç”¨äºç¨³å¥çš„åŸºäºLLMçš„æƒ…æ„Ÿè¯†åˆ«ã€‚</li>
<li>éªŒè¯äº†æƒ…æ„Ÿç‰¹å®šæç¤ºã€ASRè¯¯å·®æ ¡æ­£å’ŒLLMè®­ç»ƒæ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¿›è¡Œäº†å…³äºä¸Šä¸‹æ–‡å­¦ä¹ ã€å³æ—¶å­¦ä¹ å’ŒæŒ‡ä»¤è°ƒæ•´çš„å®éªŒï¼Œä»¥æ£€éªŒLLMè®­ç»ƒæ–¹æ¡ˆåœ¨æ­¤æ–¹å‘çš„æœ‰ç”¨æ€§ã€‚</li>
<li>ç ”ç©¶äº†LLMså¯¹å¾®å°æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3db923fa6400d94ce66ccb7ae8bf490c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d503f00707396d52d04caddd5ff6d061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48deef82dc998c76132d96cedd4f98d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85ecc4201dbee8698e9601d69d3d39aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a829a1d06d2b91e03464e663362bb4bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d49d138405a0849ac24441f9d6d25ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d616ea42b2de1f7c3699cbe79557fa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3b75748c2e96be8a9e8231da4a1c2fc7.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0a4a465482b112bfb4412bf6ff31a79d.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-02  A Contrast-Agnostic Method for Ultra-High Resolution Claustrum   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
