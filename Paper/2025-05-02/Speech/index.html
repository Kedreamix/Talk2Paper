<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-05-02  Enhancing Health Mention Classification Performance A Study on   Advancements in Parameter Efficient Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-37d616ea42b2de1f7c3699cbe79557fa.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-02-更新"><a href="#2025-05-02-更新" class="headerlink" title="2025-05-02 更新"></a>2025-05-02 更新</h1><h2 id="Enhancing-Health-Mention-Classification-Performance-A-Study-on-Advancements-in-Parameter-Efficient-Tuning"><a href="#Enhancing-Health-Mention-Classification-Performance-A-Study-on-Advancements-in-Parameter-Efficient-Tuning" class="headerlink" title="Enhancing Health Mention Classification Performance: A Study on   Advancements in Parameter Efficient Tuning"></a>Enhancing Health Mention Classification Performance: A Study on   Advancements in Parameter Efficient Tuning</h2><p><strong>Authors:Reem Abdel-Salam, Mary Adewunmi</strong></p>
<p>Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency. </p>
<blockquote>
<p>健康提及分类（HMC）在利用社交媒体帖子进行实时跟踪和公共卫生监测方面发挥着关键作用。然而，HMC的过程由于其复杂性质而面临重大挑战，这些挑战主要源于健康提及的上下文方面，如图释语言和描述性术语，而不是明确反映个人疾病。为了解决这个问题，我们认为通过生物医学自然语言方法（NLP）的增强参数的常规微调可以实现更清晰的提及。在这项研究中，我们探索了不同的技术，如利用词性标注器信息，改进PEFT技术，以及它们的不同组合。我们在三个广泛使用的数据集RHDM、PHM和Illness上进行了大量实验。结合词性标注器信息和利用PEFT技术，在F1分数方面显著提高了性能，与三个数据集中的最新方法相比，通过使用较小的模型和有效的训练取得了更好的成绩。此外，研究结果强调了纳入词性标注器信息和利用PEFT技术在HMC中的有效性。总之，所提出的方法为准确分类社交媒体帖子中的健康提及提供了一种潜在的有效方法，同时优化了模型大小和训练效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21685v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>基于社交媒体帖子进行实时追踪和公共卫生监测时，健康提及分类（HMC）发挥着至关重要的作用。然而，由于健康提及的上下文因素，如比喻语言和描述性术语，而非个人疾病的明确反映，HMC过程面临巨大挑战。为解决这一问题，本研究通过改进参数和采用生物医学自然语言处理方法（NLP）的传统微调方式，力求实现更清晰的提及。本研究探索了不同技术，如利用词性标注器信息、改进PEFT技术以及它们的组合使用。在三个广泛使用数据集上的大量实验表明，融入词性标注器信息和利用PEFT技术能显著提高F1分数方面的性能表现，相较于所有对比方法表现更优，同时实现了模型的小型化和训练效率的提升。此外，研究还发现融入词性标注器信息和利用PEFT技术对于HMC的有效性。总之，所提出的方法为准确分类社交媒体帖子中的健康提及提供了一种潜在的有效方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>健康提及分类（HMC）在实时追踪和公共卫生监测方面应用广泛。</li>
<li>HMC面临的主要挑战在于识别上下文因素（如比喻语言和描述性术语）导致的复杂性和多样性。</li>
<li>研究通过实验探索了不同技术解决此挑战的方法，如词性标注器信息的利用和PEFT技术的改进等。</li>
<li>在三个数据集上的实验表明融入词性标注器信息和利用PEFT技术可显著提高模型性能。</li>
<li>提出的方法展现出实现更小模型和更高效训练的可能性。</li>
<li>实验结果表明融入词性标注器信息和利用PEFT技术对于HMC非常有效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21685">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-275c02b592d8f85ad41d5e8b7cb480bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe153dbda787caf30be10ca30582ef95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-698d7d0196876147294d36e639828022.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d5b6b2a70275afbfc74ebeaae9cf6322.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="End-to-end-Audio-Deepfake-Detection-from-RAW-Waveforms-a-RawNet-Based-Approach-with-Cross-Dataset-Evaluation"><a href="#End-to-end-Audio-Deepfake-Detection-from-RAW-Waveforms-a-RawNet-Based-Approach-with-Cross-Dataset-Evaluation" class="headerlink" title="End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based   Approach with Cross-Dataset Evaluation"></a>End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based   Approach with Cross-Dataset Evaluation</h2><p><strong>Authors:Andrea Di Pierno, Luca Guarnera, Dario Allegra, Sebastiano Battiato</strong></p>
<p>Audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. Detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. In this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. Our model, RawNetLite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. To enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts Focal Loss to emphasize difficult or ambiguous samples. We further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging out-of-distribution test set (AVSpoof2021 + CodecFake). These findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. Code and pretrained models are available at <a target="_blank" rel="noopener" href="https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/">https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/</a>. </p>
<blockquote>
<p>音频深度伪造技术对数字安全和信任构成了日益增长的威胁，它利用先进的生成模型产生合成语音，该语音紧密模仿真实的人声。在开放世界条件下检测此类操纵尤其具有挑战性，因为在测试期间遇到的欺骗方法可能与训练期间所见的有所不同。在这项工作中，我们提出了一种用于音频深度伪造检测的端到端深度学习框架，它直接在原始波形上进行操作。我们的模型RawNetLite是一个轻量级的卷积递归架构，旨在捕捉光谱和时序特征，无需手工预处理。为了提高稳健性，我们引入了一种结合多个域数据的训练策略，并采用Focal Loss来强调困难或模糊样本。我们进一步证明，在基于编码器的操作和波形级别的音频增强（例如音调转换、噪声和时间拉伸）的加持下，可以在现实声学条件下实现显著的总体化改善。所提出模型在域内数据（FakeOrReal）上实现了超过99.7%的F1分数和0.25%的EER（等误率），在具有挑战性的域外测试集（AVSpoof2021 + CodecFake）上达到了83.4%的F1分数和16.4%的EER。这些发现强调了多样训练数据、定制目标函数和音频增强在构建坚韧且可泛化的音频伪造检测器中的重要性。代码和预先训练好的模型可在<a target="_blank" rel="noopener" href="https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/%E6%89%BE%E5%88%B0%E3%80%82">https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20923v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了音频深度伪造对数字安全和信任构成的威胁，并提出了一种基于深度学习的音频深度伪造检测框架RawNetLite。该模型能够在原始波形上直接操作，具有轻量级卷积循环架构，可捕获频谱和时序特征，无需手工预处理。通过结合多域数据和采用Focal Loss的训练策略，以及基于编码器的操作和波形级别的音频增强技术，该模型在真实声学条件下具有良好的泛化性能。在特定领域数据上，其F1分数超过99.7%，EER为0.25%，而在跨分布测试集上，F1分数达到83.4%，EER为16.4%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频深度伪造已成为数字安全和信任的重大威胁，需要有效的检测手段。</li>
<li>提出的RawNetLite模型能在原始波形上操作，直接捕获音频特征。</li>
<li>模型结合多域数据和Focal Loss训练策略，增强了模型的稳健性和泛化能力。</li>
<li>通过引入编码器的操作和波形级别的音频增强，模型在真实声学条件下表现更佳。</li>
<li>RawNetLite模型在特定领域数据上表现优异，F1分数超过99.7%，EER低于0.25%。</li>
<li>在跨分布测试集上，模型仍表现出较强的泛化能力，F1分数达到83.4%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20923">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-14299610e7c842c8112447cf64391709.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed41944f3802c968d737bda348d41a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0baddf35a3a86fde89bfe76ffc33377c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93e7529f909222e5689d3529fdf7b97f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ea745dd90efaebfb3e755686aa2ce04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daa7f1e53cfd6fcbee768e3126d02768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd2040834139d25cf005474db16fee33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c0520245c4a8dfd88e9c9b9957afcf8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c2857cd1f4d5e29eb8333ef372f76e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cde12d0ca61dd67e19593f7453a4f255.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0abb2198e133ec70326afd8821e95ebe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multi-Task-Corrupted-Prediction-for-Learning-Robust-Audio-Visual-Speech-Representation"><a href="#Multi-Task-Corrupted-Prediction-for-Learning-Robust-Audio-Visual-Speech-Representation" class="headerlink" title="Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech   Representation"></a>Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech   Representation</h2><p><strong>Authors:Sungnyun Kim, Sungwoo Cho, Sangmin Bae, Kangwook Jang, Se-Young Yun</strong></p>
<p>Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sungnyun/cav2vec">https://github.com/sungnyun/cav2vec</a>. </p>
<blockquote>
<p>音频视觉语音识别（AVSR）结合了听觉和视觉模式来提高识别精度，特别是在嘈杂的环境中，仅使用音频的语音识别系统是不够的。虽然之前的研究主要解决了音频干扰问题，但很少有研究处理视觉上的腐败，例如嘴唇遮挡或模糊的视频，这些也会对识别造成损害。为了解决这一现实挑战，我们提出了CAV2vec，这是一种新型的自监督语音表示学习框架，专门设计用于处理音频视觉联合腐败。CAV2vec采用自蒸馏法，通过腐败预测任务，使学生模型学习预测由教师模型生成的干净目标，同时输入带有腐败的帧。具体来说，我们建议使用单模态多任务学习，通过预测带有腐败视频的干净音频目标和带有腐败音频的干净视频目标，提炼跨模态知识并对齐腐败模式。这一策略减轻了由腐败模式造成的表示空间分散问题，实现了更可靠和稳健的视听融合。我们在稳健的AVSR基准测试上的实验表明，腐败表示学习方法在涉及多种类型腐败的通用环境中显著提高了识别精度。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sungnyun/cav2vec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sungnyun/cav2vec找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18539v2">PDF</a> ICLR 2025; 22 pages, 6 figures, 14 tables</p>
<p><strong>Summary</strong></p>
<p>音频视觉语音识别（AVSR）结合了听觉和视觉模态，以提高识别精度，特别是在噪声环境中，仅使用音频的语音识别系统是不够的。针对视觉失真（如嘴唇遮挡或模糊视频）等现实挑战，提出一种新型的自我监督语音表征学习框架CAV2vec。CAV2vec采用自我蒸馏方法，通过预测带有噪声输入帧的清洁目标，使学生模型学习预测清洁目标。具体来说，我们提出了一种单模态多任务学习，它通过蒸馏跨模态知识并对齐噪声模态，通过预测带有噪声视频的清洁音频目标和带有噪声音频的清洁视频目标来实现。这种方法减轻了噪声模态引起的表示空间分散问题，实现了更可靠和稳健的视听融合。实验证明，在涉及多种类型噪声的通用环境中，被噪声影响的表征学习方法显著提高识别准确性。代码可从相关网站获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音频视觉语音识别（AVSR）融合听觉和视觉模态提高识别精度，尤其在噪声环境下。</li>
<li>现有研究主要关注音频干扰，但视觉失真（如嘴唇遮挡、模糊视频等）同样对识别造成负面影响。</li>
<li>CAV2vec是一种自我监督的语音表征学习框架，专门设计来处理视听联合失真。</li>
<li>CAV2vec采用自我蒸馏方法和预测带有噪声输入帧的清洁目标策略。</li>
<li>单模态多任务学习通过蒸馏跨模态知识和对齐噪声模态实现可靠和稳健的视听融合。</li>
<li>CAV2vec通过在预测清洁音频目标时使用噪声视频和预测清洁视频目标时使用噪声音频来实现这一点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18539">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f2f4b9d4d9a31afd4f4d98987acb011b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-920f0ebd907c0f7684aec2e1315e7afd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bed7689f2e0e2c0f447f681a2c35a4f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd518048ff155a6f9e63f563df9a028d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4adcd2ad70307c0e3e7d40db1bb8ebf0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts"><a href="#FleSpeech-Flexibly-Controllable-Speech-Generation-with-Various-Prompts" class="headerlink" title="FleSpeech: Flexibly Controllable Speech Generation with Various Prompts"></a>FleSpeech: Flexibly Controllable Speech Generation with Various Prompts</h2><p><strong>Authors:Hanzhao Li, Yuke Li, Xinsheng Wang, Jingbin Hu, Qicong Xie, Shan Yang, Lei Xie</strong></p>
<p>Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker’s timbre, or choosing a style and generating a voice that matches a character’s visual appearance. To overcome these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at <a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech/">https://kkksuper.github.io/FleSpeech/</a> </p>
<blockquote>
<p>可控语音生成方法通常依赖于单一或固定的提示，这限制了创造性和灵活性。这些局限性使得在某些场景下难以满足特定用户的需求，例如在保留选定演讲者音质的同时调整风格，或者选择风格并生成与角色外观相匹配的语音。为了克服这些挑战，我们提出了FleSpeech，这是一个新型的多阶段语音生成框架，通过整合各种控制形式，允许更灵活地操作语音属性。FleSpeech采用多模态提示编码器，处理和统一不同的文本、音频和视觉提示，形成连贯的表示。这种方法增强了语音合成的适应性，支持对生成语音进行创造性和精确的控制。此外，我们还开发了多模态数据集的数据收集管道，以促进该领域的进一步研究和应用。综合的主观和客观实验证明了FleSpeech的有效性。音频样本可在<a target="_blank" rel="noopener" href="https://kkksuper.github.io/FleSpeech%E6%89%BE%E5%88%B0%E3%80%82">https://kkksuper.github.io/FleSpeech/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.04644v2">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong><br>语音生成方法通常依赖于单一或固定的提示，限制了创造性和灵活性。为解决这一问题，我们提出FleSpeech，一种新型的多阶段语音生成框架，通过整合各种形式的控制，允许更灵活地操作语音属性。FleSpeech采用多模态提示编码器，处理和统一文本、音频和视觉提示，生成连贯的语音。该框架提高了语音合成的适应性，支持对生成语音的创造性与精确控制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有语音生成方法存在创造性和灵活性方面的局限性。</li>
<li>FleSpeech是一种新型的多阶段语音生成框架，旨在解决上述问题。</li>
<li>FleSpeech通过多模态提示编码器，整合不同形式的控制，实现更灵活的语音属性操作。</li>
<li>FleSpeech采用数据收集管道，用于收集多模态数据集，以促进该领域的研究和应用。</li>
<li>FleSpeech提高了语音合成的适应性，并支持对生成语音的创造性与精确控制。</li>
<li>通过综合的主观和客观实验验证，证明了FleSpeech的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.04644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-32f8dbe6366baf9fb97790e7f970cd96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-682fe78391efffab7b91b971a10ef258.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Exploring-Acoustic-Similarity-in-Emotional-Speech-and-Music-via-Self-Supervised-Representations"><a href="#Exploring-Acoustic-Similarity-in-Emotional-Speech-and-Music-via-Self-Supervised-Representations" class="headerlink" title="Exploring Acoustic Similarity in Emotional Speech and Music via   Self-Supervised Representations"></a>Exploring Acoustic Similarity in Emotional Speech and Music via   Self-Supervised Representations</h2><p><strong>Authors:Yujia Sun, Zeyu Zhao, Korin Richmond, Yuanchao Li</strong></p>
<p>Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems. </p>
<blockquote>
<p>语音和音乐的情感识别由于声音重叠而具有相似性，这引发了在这两个领域之间转移知识的兴趣。然而，语音和音乐之间的共享声学线索，特别是那些由自监督学习（SSL）模型编码的线索，仍然在很大程度上未被探索。考虑到语音和音乐的SSL模型很少应用于跨域研究，这项工作中，我们重新关注情感语音和音乐之间的声学相似性，从分析用于语音情感识别（SER）和音乐情感识别（MER）的SSL模型的层级行为开始。此外，我们通过一个两阶段微调过程的几种方法的比较来进行跨域适应，研究如何利用音乐进行SER和利用语音进行MER的有效方法。最后，我们使用Frechet音频距离来探索情感语音和音乐之间的声学相似性，揭示了在语音和音乐SSL模型中的情感偏见问题。我们的研究结果表明，虽然语音和音乐的SSL模型确实捕捉到了共享的声学特征，但由于其训练策略和领域特异性，它们的行为可能会因不同情感而有所变化。此外，参数高效的微调可以通过利用彼此的知识来提高SER和MER的性能。这项研究提供了情感语音和音乐之间声学相似性的新见解，并突出了跨域泛化在改进SER和MER系统方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17899v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>语音与音乐的情感识别因声学重叠而具有相似性，引发了跨领域知识迁移的兴趣。然而，语音与音乐之间共享的声学线索，特别是自监督学习（SSL）模型所编码的线索，在跨域研究中应用较少。本文重新探讨了情感语音和音乐之间的声学相似性，分析了SSL模型在语音情感识别（SER）和音乐情感识别（MER）中的逐层行为。通过两阶段微调过程的多种方法进行跨域适配，并探索了有效的方式利用音乐进行SER和语音进行MER。此外，本文使用Frechet音频距离研究语音和音乐在个体情感上的声学相似性，并发现情感偏见问题。研究发现，虽然语音和音乐SSL模型确实捕捉到了共享的声学特征，但它们在不同情感上的表现可能会因训练策略和领域特性而有所不同。此外，通过参数高效的微调技术可以提高跨领域的情感识别性能。该研究提供了语音和情感音乐声学相似性的新视角，并突显了跨领域泛化在改进SER和MER系统方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音与音乐在情感识别方面存在声学相似性，引发跨领域知识迁移的兴趣。</li>
<li>SSL模型在语音情感识别（SER）和音乐情感识别（MER）中的行为分析显示，它们确实捕捉到了共享的声学特征。</li>
<li>通过两阶段微调过程进行跨域适配，发现利用音乐进行SER和语音进行MER的有效方法。</li>
<li>使用Frechet音频距离研究语音和音乐在个体情感上的声学相似性，揭示了情感偏见问题。</li>
<li>SSL模型的行为在不同情感上可能因训练策略和领域特性而有所不同。</li>
<li>参数高效的微调技术可以提高跨领域的情感识别性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17899">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c28d276e46b51bc69b84373cbc423ee6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d4300bba03773e3de75e1463aa46c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff13a5e136d13e80c0acfd352c53f04c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-505eb59a7f9fd2c836b0eca6b9ac18c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6f95e8d82eb4ac91a6c6d632d46a676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85362bb5e11d8eee119a101e46631d69.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Cognitive-State-Classification-from-Speech-with-Multi-View-Pseudo-Labeling"><a href="#Semi-Supervised-Cognitive-State-Classification-from-Speech-with-Multi-View-Pseudo-Labeling" class="headerlink" title="Semi-Supervised Cognitive State Classification from Speech with   Multi-View Pseudo-Labeling"></a>Semi-Supervised Cognitive State Classification from Speech with   Multi-View Pseudo-Labeling</h2><p><strong>Authors:Yuanchao Li, Zixing Zhang, Jing Han, Peter Bell, Catherine Lai</strong></p>
<p>The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines. </p>
<blockquote>
<p>在语音分类任务中，缺乏标注数据是一个常见的挑战，特别是在需要进行大量主观评估的任务中，如认知状态分类。在这项工作中，我们提出了一个半监督学习（SSL）框架，引入了一种新型的多视角伪标签方法，该方法利用声学和语言学特征来选择最确信的数据来训练分类模型。在声学方面，我们使用Frechet音频距离比较未标记数据和已标记数据，该距离是根据多个音频编码器生成的嵌入计算得出的。在语言学方面，我们提示大型语言模型修订自动语音识别转录，并根据我们提出的特定任务知识预测标签。当两个来源的伪标签一致时，即可确定高信心数据，而不匹配的数据则被视为低信心数据。然后，我们训练一个双模态分类器，以迭代方式对低信心数据进行标记，直到满足预设标准。我们在情感识别和痴呆检测任务上评估了我们的SSL框架。实验结果表明，我们的方法在使用仅30%的标注数据的情况下，与全监督学习相比具有竞争力，并且显著优于两个选定的基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16937v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种基于半监督学习（SSL）的框架，针对语音分类任务中缺乏标签数据的问题，特别是需要主观评估的认知状态分类任务。该框架引入了一种新颖的多视角伪标签方法，利用声学及语言特性选择最确定的数据来训练分类模型。实验结果显示，该方法在情绪识别和痴呆检测任务上表现出优异的性能，与完全监督学习相比仅使用30%的标签数据即可达到竞争水平，并显著优于两个选定的基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>缺乏标签数据是语音分类任务的常见挑战，特别是在需要主观评估的任务中。</li>
<li>提出了基于半监督学习（SSL）的框架，该框架利用声学及语言特性来解决这一问题。</li>
<li>引入了新颖的多视角伪标签方法，结合声学及语言特性选择最确定的数据进行模型训练。</li>
<li>通过Frechet音频距离和多个音频编码器生成的嵌入进行比较，对未标记数据进行声学评估。</li>
<li>利用大型语言模型修订自动语音识别转录并基于特定任务知识预测标签。</li>
<li>当伪标签从两个源对齐时，确定高置信度数据，而将不匹配的数据视为低置信度数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a1efe05a2b871868c9f2ab60a8916289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96ee64cece6972217ad2925293c58fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f1f1a1aca9848c96305dd64c9893bac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85bba63cef4ebe15b0b880bf2de53d4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cab75ebaf57ba910db551b76f53b652b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af95c52c2f7f2040a77b3ab74d63c37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-617a1e60214fd1c767ce4f2353f37a6b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Lingual-Speech-Emotion-Recognition-Humans-vs-Self-Supervised-Models"><a href="#Cross-Lingual-Speech-Emotion-Recognition-Humans-vs-Self-Supervised-Models" class="headerlink" title="Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised   Models"></a>Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised   Models</h2><p><strong>Authors:Zhichen Han, Tianqi Geng, Hui Feng, Jiahong Yuan, Korin Richmond, Yuanchao Li</strong></p>
<p>Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception. </p>
<blockquote>
<p>利用自监督学习（SSL）模型进行语音情感识别（SER）已经证明是有效的，但关于跨语言场景的研究有限。本研究通过对人类表现和SSL模型进行对比分析，从逐层分析的角度探索了在单语、跨语和迁移学习背景下的参数高效微调策略。我们进一步在语句和分段两个层面上比较了模型和人类的SER能力。此外，我们还通过人类评估探讨了方言对跨语言SER的影响。我们的研究结果表明，在适当的知识迁移下，模型能够适应目标语言并达到与母语者相当的表现水平。我们还证明了方言对于缺乏先验语言和语言辅助背景的个体在SER方面的重大影响。此外，无论是人类还是模型在不同情感方面都表现出不同的行为。这些结果为我们提供了关于SSL模型的跨语言SER能力的全新见解，并强调了它们与人类情感感知的相似性和差异性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16920v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用自监督学习（SSL）模型进行跨语言语音情感识别（SER）的研究。文章对比了人类与SSL模型在单语、跨语言和迁移学习背景下的性能，并进行了分层分析和参数优化微调策略的探索。研究发现，适当的知识迁移可以使模型适应目标语言，并达到接近母语者的性能。此外，文章还探讨了方言对跨语言SER的影响，并展示了人类评估结果。研究结果表明，人类和模型在不同情感下表现出不同的行为特征。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习模型在语音情感识别中表现出有效性。</li>
<li>跨语言情境下的语音情感识别研究仍然有限。</li>
<li>进行了人类与自监督学习模型的性能对比分析，包括层级分析和参数优化微调策略。</li>
<li>适当的知识迁移使模型能适应目标语言，并接近母语者的性能。</li>
<li>方言对跨语言语音情感识别有重要影响，特别是对那些没有语言学和语用学背景的人。</li>
<li>人类和模型在不同情感下表现出不同的行为特征。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.16920">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ac1d9653d2692844f5a4f885d55673c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0d84164cc09ad79177a5e016c86c3ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-526bebed1c9775dbb61b484bd18fa458.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-087e8415f393afbf8ecbb604fea6134e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revise-Reason-and-Recognize-LLM-Based-Emotion-Recognition-via-Emotion-Specific-Prompts-and-ASR-Error-Correction"><a href="#Revise-Reason-and-Recognize-LLM-Based-Emotion-Recognition-via-Emotion-Specific-Prompts-and-ASR-Error-Correction" class="headerlink" title="Revise, Reason, and Recognize: LLM-Based Emotion Recognition via   Emotion-Specific Prompts and ASR Error Correction"></a>Revise, Reason, and Recognize: LLM-Based Emotion Recognition via   Emotion-Specific Prompts and ASR Error Correction</h2><p><strong>Authors:Yuanchao Li, Yuan Gong, Chao-Han Huck Yang, Peter Bell, Catherine Lai</strong></p>
<p>Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains. </p>
<blockquote>
<p>随着大型语言模型（LLMs）的进步，使用提示工程进行语音情感的标注和识别最近开始显现，但其有效性和可靠性仍存在疑问。在本文中，我们对这一主题进行了系统研究，首先提出了融入声学、语言学和心理学中的情感特定知识的新提示。随后，我们研究了基于LLM的提示在自动语音识别（ASR）转录上的有效性，并将其与真实转录进行了对比。此外，我们还为存在ASR错误的口语情感识别提出了修订、推理、识别的提示管道。另外，我们还进行了关于情境感知学习、上下文学习和指令调整的试验，以检验LLM训练方案在此方向上的实用性。最后，我们研究了LLMs对微小提示变化的敏感性。实验结果证明了情感特定提示、ASR错误修正和LLM训练方案在基于LLM的情感识别中的有效性。本研究旨在完善LLMs在情感识别和相关领域的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.15551v2">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLMs）的发展，通过提示工程进行语音情感标注和识别已成为新兴研究领域。本文提出了一系列包含声学、语言学和心理学情感特定知识的新型提示，并系统地研究了其在自动语音识别（ASR）转录上的效果。此外，本文还提出了一种Revise-Reason-Recognize的提示流程，用于在存在语音识别错误的情况下，实现稳健的基于LLM的情感识别。通过实验，本文验证了情感特定提示、ASR误差校正和LLM训练方案在LLM情感识别中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在语音情感识别中通过提示工程进行应用。</li>
<li>提出了包含声学、语言学和心理学情感特定知识的新型提示。</li>
<li>研究了新型提示在自动语音识别（ASR）转录上的效果。</li>
<li>提出了Revise-Reason-Recognize的提示流程，用于稳健的基于LLM的情感识别。</li>
<li>验证了情感特定提示、ASR误差校正和LLM训练方案的有效性。</li>
<li>进行了关于上下文学习、即时学习和指令调整的实验，以检验LLM训练方案在此方向的有用性。</li>
<li>研究了LLMs对微小提示变化的敏感性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.15551">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3db923fa6400d94ce66ccb7ae8bf490c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d503f00707396d52d04caddd5ff6d061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48deef82dc998c76132d96cedd4f98d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85ecc4201dbee8698e9601d69d3d39aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a829a1d06d2b91e03464e663362bb4bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7d49d138405a0849ac24441f9d6d25ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37d616ea42b2de1f7c3699cbe79557fa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-02/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3b75748c2e96be8a9e8231da4a1c2fc7.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-05-02  Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-02/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0a4a465482b112bfb4412bf6ff31a79d.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-05-02  A Contrast-Agnostic Method for Ultra-High Resolution Claustrum   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30055.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
