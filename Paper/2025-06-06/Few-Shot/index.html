<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-06-06  QQSUM A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8105f9a9ebbd28b31395c9f34719b340.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-06-更新"><a href="#2025-06-06-更新" class="headerlink" title="2025-06-06 更新"></a>2025-06-06 更新</h1><h2 id="QQSUM-A-Novel-Task-and-Model-of-Quantitative-Query-Focused-Summarization-for-Review-based-Product-Question-Answering"><a href="#QQSUM-A-Novel-Task-and-Model-of-Quantitative-Query-Focused-Summarization-for-Review-based-Product-Question-Answering" class="headerlink" title="QQSUM: A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering"></a>QQSUM: A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering</h2><p><strong>Authors:An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh, Zhuang Li</strong></p>
<p>Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: <a target="_blank" rel="noopener" href="https://github.com/antangrocket1312/QQSUMM">https://github.com/antangrocket1312/QQSUMM</a> </p>
<blockquote>
<p>基于评论的产品问答（PQA）允许电子商务平台利用用户评论的见解来自动回答客户问题。然而，现有的PQA系统仅从一个角度生成答案，无法捕捉客户意见的多样性。本文介绍了一项新任务：定量查询聚焦摘要（QQSUM），旨在将多样化的客户意见总结为代表性的关键点（KPs），并量化它们的普遍性，以有效地回答用户查询。虽然增强检索生成（RAG）在PQA中显示出潜力，但其生成的答案仍未能捕捉到观点的全貌。为了应对这一挑战，我们的模型QQSUM-RAG扩展了RAG，采用小样本学习来联合训练一个面向关键点的检索器和一个关键点摘要生成器，从而基于关键点生成能够捕捉多样化和代表性意见的摘要。实验结果表明，与最新的RAG基线相比，QQSUM-RAG在文本质量和意见量化准确性方面都实现了卓越的性能。我们的源代码可访问于：<a target="_blank" rel="noopener" href="https://github.com/antangrocket1312/QQSUMM">https://github.com/antangrocket1312/QQSUMM</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04020v1">PDF</a> Paper accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>基于用户评论的自动问答（PQA）技术可为电商平台提供自动回答客户问题的能力，但现有系统往往只从单一视角生成答案，忽略了客户意见多样性。本研究提出了一个新的任务——定量查询焦点摘要（QQSUM），旨在通过抽取和量化不同客户的观点，更全面地回答问题。实验结果显示，与当前流行的模型相比，我们的QQSUM-RAG模型在文本质量和意见量化的准确性上都取得了显著的改进。更多详细信息请访问我们的GitHub页面：<a target="_blank" rel="noopener" href="https://github.com/antangrocket1312/QQSUMM">https://github.com/antangrocket1312/QQSUMM</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PQA技术允许电商平台自动回答客户问题，但现有系统缺乏捕捉客户意见多样性的能力。</li>
<li>引入新的任务QQSUM，旨在从多种观点中总结代表性关键点并量化其普遍性。</li>
<li>QQSUM-RAG模型通过少量学习样本即可实现高效率和准确的训练。这种模型结合了一个关键点导向的检索器和一个摘要生成器。</li>
<li>QQSUM-RAG在文本质量和意见量化的准确性上超越了当前先进的模型。这表明它能够在多样化和代表性的观点中有效地捕捉信息。</li>
<li>模型使用了创新的策略来捕捉和理解用户评论中的复杂信息，这对于电商平台来说非常重要，因为它们需要理解并响应客户的各种需求和反馈。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6881ac1b5cb1b3266e3730667f72d613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018ca3b59ce03850636fc6d36334342d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3290ef5936fecf3540fabec2ffbedcb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7a746bf5a6f687b7300a907748666dd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vocabulary-free-few-shot-learning-for-Vision-Language-Models"><a href="#Vocabulary-free-few-shot-learning-for-Vision-Language-Models" class="headerlink" title="Vocabulary-free few-shot learning for Vision-Language Models"></a>Vocabulary-free few-shot learning for Vision-Language Models</h2><p><strong>Authors:Maxime Zanella, Clément Fuchs, Ismail Ben Ayed, Christophe De Vleeschouwer</strong></p>
<p>Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have greatly expanded their ability to generalize across tasks using only a few labeled examples. However, existing approaches primarily build upon the strong zero-shot priors of these models by leveraging carefully designed, task-specific prompts. This dependence on predefined class names can restrict their applicability, especially in scenarios where exact class names are unavailable or difficult to specify. To address this limitation, we introduce vocabulary-free few-shot learning for VLMs, a setting where target class instances - that is, images - are available but their corresponding names are not. We propose Similarity Mapping (SiM), a simple yet effective baseline that classifies target instances solely based on similarity scores with a set of generic prompts (textual or visual), eliminating the need for carefully handcrafted prompts. Although conceptually straightforward, SiM demonstrates strong performance, operates with high computational efficiency (learning the mapping typically takes less than one second), and provides interpretability by linking target classes to generic prompts. We believe that our approach could serve as an important baseline for future research in vocabulary-free few-shot learning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MaxZanella/vocabulary-free-FSL">https://github.com/MaxZanella/vocabulary-free-FSL</a>. </p>
<blockquote>
<p>近期视觉语言模型（VLMs）在少量样本适应方面的进展极大地扩展了它们仅使用少量有标签样本即可跨任务泛化的能力。然而，现有方法主要依赖于这些模型的强大零样本先验知识，通过利用精心设计的任务特定提示来实现。对预设类名的这种依赖限制了其适用性，特别是在没有确切类名或难以指定的场景中。为了解决这个问题，我们为VLMs引入了无词汇少量样本学习，在这种设置中，目标类实例（即图像）是可用的，但它们相应的名称不可用。我们提出了基于相似性映射（SiM）的简单有效的基线方法，该方法仅根据目标实例与一组通用提示（文本或视觉）的相似性得分进行分类，从而消除了对精心制作的提示的需求。尽管概念上很简单，但SiM表现出强大的性能，计算效率高（学习映射通常不到一秒），并通过将目标类别与通用提示联系起来提供了解释性。我们相信，我们的方法可以为未来的无词汇少量样本学习研究提供重要的基线。代码可在<a target="_blank" rel="noopener" href="https://github.com/MaxZanella/vocabulary-free-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MaxZanella/vocabulary-free-FSL找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04005v1">PDF</a> Accepted at CVPR Workshops 2025</p>
<p><strong>Summary</strong></p>
<p>该摘要介绍了针对视觉语言模型（VLMs）的词汇无关少样本学习的新进展。针对现有方法依赖预设类别名称的问题，提出了一种基于相似性映射（SiM）的词汇无关少样本学习方法。该方法无需精心设计任务特定提示，仅通过目标实例与一组通用提示的相似性得分进行分类，从而消除了对精确类别名称的需求。SiM具有强大的性能、高计算效率和可解释性，为未来研究提供了一个重要的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本适应技术已显著提高了视觉语言模型（VLMs）在仅使用少量标记示例时跨任务泛化的能力。</li>
<li>现有方法主要依赖于模型的零样本先验知识，通过利用精心设计的任务特定提示来构建。</li>
<li>对预设类别名称的依赖限制了其在特定场景下的适用性，特别是在无法获得或难以指定确切类别名称的情况下。</li>
<li>引入了词汇无关的少样本学习，其中目标类别实例可用，但对应的名称不可用。</li>
<li>提出了相似性映射（SiM）方法，该方法仅基于目标实例与通用提示的相似性得分进行分类，无需精心制作的提示。</li>
<li>SiM具有强大的性能、高计算效率和可解释性，通过链接目标类别到通用提示来提供分类理由。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b7842b8c5d80453c20305fc060ef03f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21c4eea25edfb3a3e9059a3c32405842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10cdff61bf1e12e8c0bd98e98544a83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4652140286a200870aecb8477291c1bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4267914babde5792f087c91158c33daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3011b332a65e051a9f47b21c08c28835.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multiple-Stochastic-Prompt-Tuning-for-Practical-Cross-Domain-Few-Shot-Learning"><a href="#Multiple-Stochastic-Prompt-Tuning-for-Practical-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot   Learning"></a>Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot   Learning</h2><p><strong>Authors:Debarshi Brahma, Soma Biswas</strong></p>
<p>In this work, we propose a practical cross-domain few-shot learning (pCDFSL) task, where a large-scale pre-trained model like CLIP can be easily deployed on a target dataset. The goal is to simultaneously classify all unseen classes under extreme domain shifts, by utilizing only a few labeled samples per class. The pCDFSL paradigm is source-free and moves beyond artificially created episodic training and testing regimes followed by existing CDFSL frameworks, making it more challenging and relevant to real-world applications. Towards that goal, we propose a novel framework, termed MIST (MultIple STochastic Prompt tuning), where multiple stochastic prompts are utilized to handle significant domain and semantic shifts. Specifically, multiple prompts are learnt for each class, effectively capturing multiple peaks in the input data. Furthermore, instead of representing the weights of the multiple prompts as point-estimates, we model them as learnable Gaussian distributions with two different strategies, encouraging an efficient exploration of the prompt parameter space, which mitigate overfitting due to the few labeled training samples. Extensive experiments and comparison with the state-of-the-art methods on four CDFSL benchmarks adapted to this setting, show the effectiveness of the proposed framework. </p>
<blockquote>
<p>在这项工作中，我们提出了一个实用的跨域小样本学习（pCDFSL）任务，其中可以很容易地在目标数据集上部署CLIP等大型预训练模型。目标是利用每个类别仅有的少量标记样本，同时对极端域偏移下的所有未见类别进行分类。pCDFSL范式是无源的，超越了现有的CDFSL框架所遵循的人工创建的事件性训练和测试制度，使其更具挑战性和与真实世界应用的相关性。朝着这一目标，我们提出了一种新型框架，称为MIST（多重随机提示调整），其中利用多个随机提示来处理重大的域和语义变化。具体来说，为每个类别学习多个提示，有效地捕捉输入数据中的多个峰值。此外，我们不是将多个提示的权重表示为点估计，而是将它们建模为两种不同策略的可学习高斯分布，鼓励有效地探索提示参数空间，从而减轻由于少量标记训练样本而导致的过拟合。在适应此设置的四个CDFSL基准测试上的广泛实验以及与最新方法的比较，证明了所提出框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03926v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于跨域小样本学习（pCDFSL）任务，我们提出了一个实用框架MIST。此框架适用于部署大型预训练模型（如CLIP）在目标数据集上，目标是通过每个类别仅有少量标注样本，同时对所有未见类别进行分类。MIST采用多个随机提示（prompt）来处理显著的域和语义变化，并为每个类别学习多个提示，有效地捕捉输入数据中的多个峰值。此外，我们使用两种策略将提示权重建模为可学习的高斯分布，以高效探索提示参数空间并缓解因少量标注样本引起的过拟合问题。在四个CDFSL基准测试上的实验和对比显示了我们框架的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于跨域小样本学习（pCDFSL）任务的实用框架MIST，适用于预训练模型在目标数据集上的应用。</li>
<li>MIST框架能够同时分类所有未见类别，且仅使用每个类别的少量标注样本。</li>
<li>MIST采用多个随机提示（prompt）来处理域和语义的显著变化。</li>
<li>为每个类别学习多个提示，以捕捉输入数据中的多个峰值。</li>
<li>提示权重被建模为可学习的高斯分布，以高效探索提示参数空间。</li>
<li>该框架在四个CDFSL基准测试上的实验结果表明其有效性。</li>
<li>与现有方法相比，该框架展现出优越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03926">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc32079d7446e6ef383c13f5119be1f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01a5096db6329db54de4b0d72e400ffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29c061536d0435fe9897146400a11d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9089c1429bd8fdb47518d47327a7504d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a92c6abfbf6d90bc7dd463a22aa77bed.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="INP-Former-Advancing-Universal-Anomaly-Detection-via-Intrinsic-Normal-Prototypes-and-Residual-Learning"><a href="#INP-Former-Advancing-Universal-Anomaly-Detection-via-Intrinsic-Normal-Prototypes-and-Residual-Learning" class="headerlink" title="INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal   Prototypes and Residual Learning"></a>INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal   Prototypes and Residual Learning</h2><p><strong>Authors:Wei Luo, Haiming Yao, Yunkang Cao, Qiyu Chen, Ang Gao, Weiming Shen, Weihang Zhang, Wenyong Yu</strong></p>
<p>Anomaly detection (AD) is essential for industrial inspection and medical diagnosis, yet existing methods typically rely on &#96;&#96;comparing’’ test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Furthermore, we propose a soft version of the INP Coherence Loss and enhance INP-Former by incorporating residual learning, leading to the development of INP-Former++. The proposed method significantly improves detection performance across single-class, multi-class, semi-supervised, few-shot, and zero-shot settings. </p>
<blockquote>
<p>异常检测（AD）对于工业检测和医疗诊断至关重要。然而，现有方法通常依赖于将测试图像与训练集中的正常参考图像进行“比较”。但是，外观和位置的变化往往使这些参考图像与测试图像的对齐变得复杂，从而限制了检测精度。我们观察到，大多数异常表现为局部变化，这意味着即使在异常图像内部，仍有宝贵的正常信息存在。我们认为这些信息是有用的，并且可能与异常更加对齐，因为异常和正常信息都来自同一图像。因此，我们提出了一种新方法INP-Former，它直接从测试图像中提取内在正常原型（INPs）。具体来说，我们引入了INP提取器，该提取器通过线性组合正常令牌来表示INPs。我们还提出了一种INP一致性损失，以确保INPs能够忠实地代表测试图像的正常性。这些INPs然后引导INP引导解码器仅重建正常令牌，重建误差作为异常分数。此外，我们还提出了一种软挖掘损失，以在训练过程中优先处理难以优化的样本。INP-Former在MVTec-AD、VisA和Real-IAD上的单类、多类、和少样本AD任务上实现了最先进的性能表现，成为AD的通用解决方案。值得一提的是，INP-Former还表现出了一些零样本AD的能力。此外，我们提出了INP一致性损失的软版本，并通过结合残差学习增强了INP-Former，从而发展了INP-Former++。所提出的方法在单类、多类、半监督、少样本和零样本设置上显著提高了检测性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03660v1">PDF</a> 15 pages, 11 figures, 13 tables</p>
<p><strong>Summary</strong></p>
<p>该文本提出了一种新型异常检测（Anomaly Detection，AD）方法——INP-Former。该方法直接从测试图像中提取内在正常原型（Intrinsic Normal Prototypes，INPs），无需依赖训练集中的外部正常参考。通过引入INP提取器和INP相干性损失，INPs能忠实代表测试图像的正常性。利用这些INPs，指导解码器仅重建正常令牌，重建误差作为异常分数。此外，还提出了软挖掘损失，以在训练期间优先处理难以优化的样本。该方法在MVTec-AD、VisA和Real-IAD等多个数据集上实现了单类、多类及少样本异常检测的卓越性能，并具有零样本异常检测能力。通过引入软化版本的INP相干性损失并结合残差学习，进一步开发了改进的INP-Former++，提高了在多种设置下的检测性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有异常检测方法常依赖于与训练集中的正常参考图像进行比较，但外观和位置的变化使得对齐变得困难，限制了检测精度。</li>
<li>INP-Former方法直接从测试图像中提取内在正常原型（INPs），无需外部正常参考。</li>
<li>引入INP提取器和INP相干性损失，确保INPs能忠实代表测试图像的正常性。</li>
<li>利用INPs指导解码器仅重建正常令牌，重建误差作为异常分数。</li>
<li>提出软挖掘损失，以优先处理训练中的困难样本。</li>
<li>INP-Former在多个数据集上实现了卓越的性能，包括单类、多类及少样本异常检测，并具备零样本异常检测能力。</li>
<li>通过引入软化版本的INP相干性损失并结合残差学习，开发了改进的INP-Former++。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03660">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-036ea36231060819b54a5574902a9781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a983b37a2bd9c2b5eb93117fbab3937c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac4154d338eedd8b870f84cff1629bb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98001f37b2b498380406311590aab93e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CHIME-Conditional-Hallucination-and-Integrated-Multi-scale-Enhancement-for-Time-Series-Diffusion-Model"><a href="#CHIME-Conditional-Hallucination-and-Integrated-Multi-scale-Enhancement-for-Time-Series-Diffusion-Model" class="headerlink" title="CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement   for Time Series Diffusion Model"></a>CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement   for Time Series Diffusion Model</h2><p><strong>Authors:Yuxuan Chen, Haipeng Xie</strong></p>
<p>The denoising diffusion probabilistic model has become a mainstream generative model, achieving significant success in various computer vision tasks. Recently, there has been initial exploration of applying diffusion models to time series tasks. However, existing studies still face challenges in multi-scale feature alignment and generative capabilities across different entities and long-time scales. In this paper, we propose CHIME, a conditional hallucination and integrated multi-scale enhancement framework for time series diffusion models. By employing multi-scale decomposition and adaptive integration, CHIME captures the decomposed features of time series, achieving in-domain distribution alignment between generated and original samples. In addition, we introduce a feature hallucination module in the conditional denoising process, enabling the transfer of temporal features through the training of category-independent transformation layers. Experimental results on publicly available real-world datasets demonstrate that CHIME achieves state-of-the-art performance and exhibits excellent generative generalization capabilities in few-shot scenarios. </p>
<blockquote>
<p>去噪扩散概率模型已经成为主流生成模型，在各种计算机视觉任务中取得了巨大成功。最近，人们开始探索将扩散模型应用于时间序列任务。然而，现有研究在多尺度特征对齐和跨不同实体和长时间尺度的生成能力方面仍面临挑战。在本文中，我们提出了CHIME，这是一个用于时间序列扩散模型的基于条件幻觉和集成多尺度增强框架。通过采用多尺度分解和自适应集成，CHIME捕捉时间序列的分解特征，实现生成样本和原始样本之间的域内分布对齐。此外，我们在条件去噪过程中引入了一个特征幻觉模块，通过训练类别独立的转换层，实现了时间特征的转移。在公开可用的真实世界数据集上的实验结果表明，CHIME达到了最先进的性能，并在小样本场景中表现出优秀的生成泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散概率模型已成为主流生成模型，在计算机视觉任务中取得了巨大成功。最近，人们开始探索将扩散模型应用于时间序列任务，但仍面临多尺度特征对齐和跨不同实体及长时间尺度的生成能力挑战。本文提出CHIME框架，采用多尺度分解和自适应集成，实现时间序列的分解特征捕捉，生成样本与原始样本之间的域内分布对齐。此外，我们在条件去噪过程中引入了特征幻觉模块，通过训练类别独立转换层，实现时间特征的转移。在公开的真实世界数据集上的实验结果表明，CHIME达到了最先进的性能，并在小样本场景中表现出优秀的生成泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散概率模型已成为主流生成模型，广泛应用于计算机视觉任务。</li>
<li>将扩散模型应用于时间序列任务尚处于初步探索阶段。</li>
<li>现有研究面临多尺度特征对齐和跨不同实体及长时间尺度的生成能力挑战。</li>
<li>CHIME框架通过多尺度分解和自适应集成实现时间序列的分解特征捕捉。</li>
<li>CHIME实现了生成样本与原始样本之间的域内分布对齐。</li>
<li>CHIME引入特征幻觉模块，通过训练类别独立转换层，实现时间特征的转移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dde9f87a66ce1367497a6084caee8566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25486974e1d9dc2869d277bee77b9df2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-319123ee9abdb43bbdc80f051bd8721c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RoNFA-Robust-Neural-Field-based-Approach-for-Few-Shot-Image-Classification-with-Noisy-Labels"><a href="#RoNFA-Robust-Neural-Field-based-Approach-for-Few-Shot-Image-Classification-with-Noisy-Labels" class="headerlink" title="RoNFA: Robust Neural Field-based Approach for Few-Shot Image   Classification with Noisy Labels"></a>RoNFA: Robust Neural Field-based Approach for Few-Shot Image   Classification with Noisy Labels</h2><p><strong>Authors:Nan Xiang, Lifeng Xing, Dequan Jin</strong></p>
<p>In few-shot learning (FSL), the labeled samples are scarce. Thus, label errors can significantly reduce classification accuracy. Since label errors are inevitable in realistic learning tasks, improving the robustness of the model in the presence of label errors is critical. This paper proposes a new robust neural field-based image approach (RoNFA) for few-shot image classification with noisy labels. RoNFA consists of two neural fields for feature and category representation. They correspond to the feature space and category set. Each neuron in the field for category representation (FCR) has a receptive field (RF) on the field for feature representation (FFR) centered at the representative neuron for its category generated by soft clustering. In the prediction stage, the range of these receptive fields adapts according to the neuronal activation in FCR to ensure prediction accuracy. These learning strategies provide the proposed model with excellent few-shot learning capability and strong robustness against label noises. The experimental results on real-world FSL datasets with three different types of label noise demonstrate that the proposed method significantly outperforms state-of-the-art FSL methods. Its accuracy obtained in the presence of noisy labels even surpasses the results obtained by state-of-the-art FSL methods trained on clean support sets, indicating its strong robustness against noisy labels. </p>
<blockquote>
<p>在少量学习（FSL）中，标注样本非常稀缺。因此，标签错误可能会显著降低分类精度。由于在实际学习任务中标签错误不可避免，因此在存在标签错误的情况下提高模型的稳健性至关重要。本文针对带有噪声标签的少量图像分类问题，提出了一种新的基于稳健神经场的图像方法（RoNFA）。RoNFA由两个用于特征和类别表示的神经场组成。它们对应于特征空间和类别集。类别表示字段（FCR）中的每个神经元在其特征表示字段（FFR）上具有一个以通过软聚类生成的类别代表神经元为中心的接受场（RF）。在预测阶段，这些接受场的范围会根据FCR中的神经元激活情况进行调整，以确保预测精度。这些学习策略使所提模型具有出色的少量学习能力，并对标签噪声具有很强的稳健性。在具有三种不同类型标签噪声的真实世界FSL数据集上的实验结果表明，所提方法显著优于最先进的FSL方法。在存在噪声标签的情况下获得其精度甚至超过了在干净的支撑集上训练的最新FSL方法的结果，这表明其对噪声标签的稳健性很强。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03461v1">PDF</a> 7 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于神经场的稳健图像分类方法（RoNFA），用于小样本学习（FSL）中的带噪声标签问题。该方法通过特征表示和类别表示的两个神经场来构建模型，具有优秀的少样本学习能力，以及对标签噪声的强鲁棒性。实验结果表明，该方法在真实世界的FSL数据集上，对于三种不同类型的标签噪声，显著优于现有的FSL方法。即使在存在噪声标签的情况下，其准确性也超过了在干净的支持集上训练的现有FSL方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RoNFA是一种基于神经场的图像分类方法，用于处理小样本次学习中的标签噪声问题。</li>
<li>RoNFA包含两个神经场：一个用于特征表示（FFR），另一个用于类别表示（FCR）。</li>
<li>FCR中的每个神经元在FFR上都有一个以该类别的代表性神经元为中心的接收域（RF）。</li>
<li>在预测阶段，这些接收域的范围会根据FCR中的神经元激活进行适应，以确保预测的准确性。</li>
<li>RoNFA具有优秀的少样本学习能力，以及对标签噪声的强鲁棒性。</li>
<li>实验结果表明，RoNFA在真实世界的FSL数据集上显著优于现有的FSL方法，特别是在处理标签噪声方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03461">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8a6961a27bf1e749d3acdb7d79efb7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43dfbb1e76afc8e79f30e4b8a4e0a510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8105f9a9ebbd28b31395c9f34719b340.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f165c75e53c392aeddab3fdd6a131bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed49dc7c6b43dd933273e13d4c2f1168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d84b01d54568157152e76d241c410cff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708e318431cf59c9f094ae1f32a417d3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-AI-Agent-Architectures-for-Entity-Relationship-Classification"><a href="#Comparative-Analysis-of-AI-Agent-Architectures-for-Entity-Relationship-Classification" class="headerlink" title="Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification"></a>Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification</h2><p><strong>Authors:Maryam Berijanian, Kuldeep Singh, Amin Sehati</strong></p>
<p>Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/maryambrj/ALIEN.git">https://github.com/maryambrj/ALIEN.git</a>. </p>
<blockquote>
<p>实体关系分类在信息提取中仍然是一项具有挑战性的任务，特别是在标签数据有限和复杂关系结构的情况下。在这项研究中，我们对三种用于执行关系分类的AI代理架构进行了比较分析，这些架构使用大型语言模型（LLM）。探索的代理架构包括（1）反思自我评价，（2）层次任务分解，以及（3）一种新的多代理动态示例生成机制，每种机制都利用不同的推理和提示适应模式。特别是，我们的动态示例生成方法引入了实时合作和对抗性提示。我们在多个领域和模型后端系统地比较了它们的性能。实验表明，多代理协作始终优于标准的小样本提示，并接近微调模型的性能。这些发现为设计用于结构化关系提取的模块化、通用化LLM系统提供了实际指导。源代码和数据集可通过<a target="_blank" rel="noopener" href="https://github.com/maryambrj/ALIEN.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/maryambrj/ALIEN.git获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02426v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究探讨了三种不同的AI代理架构在关系分类任务中的表现，这三种架构分别是反射自我评价、层次任务分解和新型多代理动态示例生成机制。研究通过系统比较这些架构在多领域和模型后端的表现，发现多代理协调在有限标记数据和复杂关系结构下始终优于标准少样本提示方法，并接近精细调整模型的性能。这为设计模块化、通用化的基于大型语言模型的系统提供了实践指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究对比分析了三种AI代理架构在关系分类任务中的应用效果。</li>
<li>这些架构包括反射自我评价、层次任务分解和多代理动态示例生成机制。</li>
<li>多代理动态示例生成机制引入实时合作和对抗性提示，提高了关系分类的准确性。</li>
<li>研究通过系统比较这些架构在多个领域和模型后端的表现，发现多代理协调表现最佳。</li>
<li>多代理协调在有限标记数据和复杂关系结构下优于标准少样本提示方法。</li>
<li>多代理协调方法接近精细调整模型的性能，为设计模块化、通用化的大型语言模型系统提供了实践指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02426">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b8f5346e14c5399d820c5e96a50a22f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea697f76100335fdf89746282a33143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c560b9afb5054fbd70c7ab48032fb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7218792e58acd738a798fe0b1b511b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79971973db5de7b925deef4f1ab0716c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-505b5150ebb1833e5037a144bdcfa5ad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DOVE-A-Large-Scale-Multi-Dimensional-Predictions-Dataset-Towards-Meaningful-LLM-Evaluation"><a href="#DOVE-A-Large-Scale-Multi-Dimensional-Predictions-Dataset-Towards-Meaningful-LLM-Evaluation" class="headerlink" title="DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation"></a>DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation</h2><p><strong>Authors:Eliya Habba, Ofir Arviv, Itay Itzhak, Yotam Perlitz, Elron Bandel, Leshem Choshen, Michal Shmueli-Scheuer, Gabriel Stanovsky</strong></p>
<p>Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation.   Browse the data, contribute, and more: <a target="_blank" rel="noopener" href="https://slab-nlp.github.io/DOVE/">https://slab-nlp.github.io/DOVE/</a> </p>
<blockquote>
<p>最近的研究发现，大型语言模型（LLMs）对各种任意的提示维度都非常敏感，包括分隔符的类型、答案枚举器、指令措辞等等。这引发了人们对流行的单一提示评估方法的质疑。我们推出了DOVE（变异评估数据集），这是一个大规模的数据集，包含了各种评估基准测试的提示扰动。与以前的工作相比，我们从整体的角度来审视LLM的敏感性，并评估了不同维度扰动联合效应，导致每个实例都有成千上万的扰动。我们在DOVE上评估了几个模型家族，得到了一些发现，包括选择表现良好的提示的有效方法，观察到少量示例可以减少敏感性，并识别出在所有扰动中固有的难以解决的实例。DOVE包含超过25亿个提示扰动和模型输出，我们将其公开提供，以激发社区朝着有意义、稳健和高效的评估方向共同努力。浏览数据、作出贡献等等：<a target="_blank" rel="noopener" href="https://slab-nlp.github.io/DOVE/">https://slab-nlp.github.io/DOVE/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01622v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）对提示维度敏感，包括分隔符、答案枚举器、指令措辞等。DOVE数据集用于评估模型对各种提示扰动的敏感性，从整体性角度考察LLM的敏感性，并评估各维度扰动的联合效应。该数据集包含超过2.5亿个提示扰动和模型输出，提供公众访问，以推动有意义、稳健和高效的评估工作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs对提示维度敏感，包括分隔符、答案枚举器和指令措辞等。</li>
<li>DOVE数据集包含大量的提示扰动，用于评估LLM的敏感性。</li>
<li>DOVE数据集从整体性角度考察LLM的敏感性，并评估各维度扰动的联合效应。</li>
<li>DOVE数据集包含超过250M的提示扰动和模型输出，可供公众访问。</li>
<li>通过DOVE数据集，发现了选择表现良好的提示的有效方法。</li>
<li>少量样本可以减少LLM的敏感性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01622">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5660e8ae6f0613a564380d952d42002f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17b48d6c2121ca5d8f5f783e73cfe4a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-091f74d42c207c13f3d0d965c3d6f9c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79be5c30bea784efc34f17ad0cc26ed8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4465bff72aafe1e8661dfbde6810acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8ad25e32cd3edbbc1b1d0c8ca4c6fdb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improving-the-Language-Understanding-Capabilities-of-Large-Language-Models-Using-Reinforcement-Learning"><a href="#Improving-the-Language-Understanding-Capabilities-of-Large-Language-Models-Using-Reinforcement-Learning" class="headerlink" title="Improving the Language Understanding Capabilities of Large Language   Models Using Reinforcement Learning"></a>Improving the Language Understanding Capabilities of Large Language   Models Using Reinforcement Learning</h2><p><strong>Authors:Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie</strong></p>
<p>Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4% on average across sentiment and natural language inference tasks, including gains of 7.3% on the Mental Health dataset and 10.9% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation. </p>
<blockquote>
<p>指令微调的大型语言模型（LLM）在14B参数以下，在自然语言理解（NLU）任务上表现持续不佳，通常在GLUE和SuperGLUE等基准测试上落后于较小的模型，如BERT-base。受到强化学习在推理任务（例如DeepSeek）中成功的启发，我们探索近端策略优化（PPO）作为框架，以提高LLM的NLU能力。我们将NLU作为一个强化学习环境，将令牌生成作为一系列动作，并基于与真实标签的对齐情况优化奖励信号。PPO始终优于监督微调，在GLUE上平均提高了6.3分，并分别超过了零样本和少样本提示38.7分和26.1分。值得注意的是，通过PPO训练的模型在情感和自然语言推理任务上的平均表现超过了GPT-4o超过4%，包括在心理健康数据集上提高7.3%，在SIGA-nli上提高10.9%。这项工作突出了通过重新构建为强化学习问题来适应LLM执行新任务的前景，使学习能够通过简单的终端任务奖励而不是大量的数据收集来进行。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11020v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLM）在少于14B参数的情况下，在自然语言理解（NLU）任务上的表现仍然不尽如人意，常常在GLUE和SuperGLUE等基准测试中落后于较小的模型，如BERT-base。本研究受深度学习在推理任务中成功应用的启发，探索使用近端策略优化（PPO）框架来提升LLM的NLU能力。本研究将NLU任务视为一个强化学习环境，将令牌生成视为一系列行动，并基于与真实标签的对齐情况来优化奖励信号。PPO在GLUE基准测试上的平均表现优于监督微调，提升了6.3个点；并且在零样本和少样本提示下的表现分别提升了38.7和26.1个点。值得注意的是，使用PPO优化的模型在情感和自然语言推理任务上的平均表现超过GPT-4o超过4%，其中在精神健康数据集上提升了7.3%，在SIGA-nli上提升了10.9%。这表明将LLM重新定位为强化学习问题是一个有前景的方向，可以通过简单的终端任务奖励进行学习，而无需大量数据整理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）在少于14B参数时，在自然语言理解（NLU）任务上的表现较差，常落后于较小模型。</li>
<li>研究采用强化学习中的近端策略优化（PPO）框架来提升LLM的NLU能力。</li>
<li>将NLU任务视为强化学习环境，将令牌生成视为一系列行动，并优化奖励信号以对齐真实标签。</li>
<li>PPO在GLUE基准测试上的表现优于监督微调和其他方法。</li>
<li>PPO优化的模型在情感和自然语言推理任务上的平均表现超过GPT-4o超过4%，在某些数据集上的提升显著。</li>
<li>研究结果表明，将LLM重新定位为强化学习问题是一个有前景的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dadb15198773b831df1ac1e0a29e696f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9c217131817731f7f05cee1691eed18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53a89ad01efdf8b76d5a47ed931d54b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0550e62e6a2671a165c6a6c5025c0cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0f7d5ba4831bfe386f88647faff46233.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-06-06  ATI Any Trajectory Instruction for Controllable Video Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab60ebc7690d7c235db9dbfc46519a2b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-06  OWMM-Agent Open World Mobile Manipulation With Multi-modal Agentic Data   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
