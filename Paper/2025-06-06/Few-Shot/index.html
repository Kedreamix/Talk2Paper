<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  QQSUM A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-8105f9a9ebbd28b31395c9f34719b340.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-06-æ›´æ–°"><a href="#2025-06-06-æ›´æ–°" class="headerlink" title="2025-06-06 æ›´æ–°"></a>2025-06-06 æ›´æ–°</h1><h2 id="QQSUM-A-Novel-Task-and-Model-of-Quantitative-Query-Focused-Summarization-for-Review-based-Product-Question-Answering"><a href="#QQSUM-A-Novel-Task-and-Model-of-Quantitative-Query-Focused-Summarization-for-Review-based-Product-Question-Answering" class="headerlink" title="QQSUM: A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering"></a>QQSUM: A Novel Task and Model of Quantitative Query-Focused   Summarization for Review-based Product Question Answering</h2><p><strong>Authors:An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh, Zhuang Li</strong></p>
<p>Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: <a target="_blank" rel="noopener" href="https://github.com/antangrocket1312/QQSUMM">https://github.com/antangrocket1312/QQSUMM</a> </p>
<blockquote>
<p>åŸºäºè¯„è®ºçš„äº§å“é—®ç­”ï¼ˆPQAï¼‰å…è®¸ç”µå­å•†åŠ¡å¹³å°åˆ©ç”¨ç”¨æˆ·è¯„è®ºçš„è§è§£æ¥è‡ªåŠ¨å›ç­”å®¢æˆ·é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„PQAç³»ç»Ÿä»…ä»ä¸€ä¸ªè§’åº¦ç”Ÿæˆç­”æ¡ˆï¼Œæ— æ³•æ•æ‰å®¢æˆ·æ„è§çš„å¤šæ ·æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°ä»»åŠ¡ï¼šå®šé‡æŸ¥è¯¢èšç„¦æ‘˜è¦ï¼ˆQQSUMï¼‰ï¼Œæ—¨åœ¨å°†å¤šæ ·åŒ–çš„å®¢æˆ·æ„è§æ€»ç»“ä¸ºä»£è¡¨æ€§çš„å…³é”®ç‚¹ï¼ˆKPsï¼‰ï¼Œå¹¶é‡åŒ–å®ƒä»¬çš„æ™®éæ€§ï¼Œä»¥æœ‰æ•ˆåœ°å›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚è™½ç„¶å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰åœ¨PQAä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶ç”Ÿæˆçš„ç­”æ¡ˆä»æœªèƒ½æ•æ‰åˆ°è§‚ç‚¹çš„å…¨è²Œã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹QQSUM-RAGæ‰©å±•äº†RAGï¼Œé‡‡ç”¨å°æ ·æœ¬å­¦ä¹ æ¥è”åˆè®­ç»ƒä¸€ä¸ªé¢å‘å…³é”®ç‚¹çš„æ£€ç´¢å™¨å’Œä¸€ä¸ªå…³é”®ç‚¹æ‘˜è¦ç”Ÿæˆå™¨ï¼Œä»è€ŒåŸºäºå…³é”®ç‚¹ç”Ÿæˆèƒ½å¤Ÿæ•æ‰å¤šæ ·åŒ–å’Œä»£è¡¨æ€§æ„è§çš„æ‘˜è¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„RAGåŸºçº¿ç›¸æ¯”ï¼ŒQQSUM-RAGåœ¨æ–‡æœ¬è´¨é‡å’Œæ„è§é‡åŒ–å‡†ç¡®æ€§æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æºä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/antangrocket1312/QQSUMM">https://github.com/antangrocket1312/QQSUMM</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04020v1">PDF</a> Paper accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”¨æˆ·è¯„è®ºçš„è‡ªåŠ¨é—®ç­”ï¼ˆPQAï¼‰æŠ€æœ¯å¯ä¸ºç”µå•†å¹³å°æä¾›è‡ªåŠ¨å›ç­”å®¢æˆ·é—®é¢˜çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰ç³»ç»Ÿå¾€å¾€åªä»å•ä¸€è§†è§’ç”Ÿæˆç­”æ¡ˆï¼Œå¿½ç•¥äº†å®¢æˆ·æ„è§å¤šæ ·æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡â€”â€”å®šé‡æŸ¥è¯¢ç„¦ç‚¹æ‘˜è¦ï¼ˆQQSUMï¼‰ï¼Œæ—¨åœ¨é€šè¿‡æŠ½å–å’Œé‡åŒ–ä¸åŒå®¢æˆ·çš„è§‚ç‚¹ï¼Œæ›´å…¨é¢åœ°å›ç­”é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸å½“å‰æµè¡Œçš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„QQSUM-RAGæ¨¡å‹åœ¨æ–‡æœ¬è´¨é‡å’Œæ„è§é‡åŒ–çš„å‡†ç¡®æ€§ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„GitHubé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/antangrocket1312/QQSUMM">https://github.com/antangrocket1312/QQSUMM</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PQAæŠ€æœ¯å…è®¸ç”µå•†å¹³å°è‡ªåŠ¨å›ç­”å®¢æˆ·é—®é¢˜ï¼Œä½†ç°æœ‰ç³»ç»Ÿç¼ºä¹æ•æ‰å®¢æˆ·æ„è§å¤šæ ·æ€§çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ–°çš„ä»»åŠ¡QQSUMï¼Œæ—¨åœ¨ä»å¤šç§è§‚ç‚¹ä¸­æ€»ç»“ä»£è¡¨æ€§å…³é”®ç‚¹å¹¶é‡åŒ–å…¶æ™®éæ€§ã€‚</li>
<li>QQSUM-RAGæ¨¡å‹é€šè¿‡å°‘é‡å­¦ä¹ æ ·æœ¬å³å¯å®ç°é«˜æ•ˆç‡å’Œå‡†ç¡®çš„è®­ç»ƒã€‚è¿™ç§æ¨¡å‹ç»“åˆäº†ä¸€ä¸ªå…³é”®ç‚¹å¯¼å‘çš„æ£€ç´¢å™¨å’Œä¸€ä¸ªæ‘˜è¦ç”Ÿæˆå™¨ã€‚</li>
<li>QQSUM-RAGåœ¨æ–‡æœ¬è´¨é‡å’Œæ„è§é‡åŒ–çš„å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†å½“å‰å…ˆè¿›çš„æ¨¡å‹ã€‚è¿™è¡¨æ˜å®ƒèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–å’Œä»£è¡¨æ€§çš„è§‚ç‚¹ä¸­æœ‰æ•ˆåœ°æ•æ‰ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨äº†åˆ›æ–°çš„ç­–ç•¥æ¥æ•æ‰å’Œç†è§£ç”¨æˆ·è¯„è®ºä¸­çš„å¤æ‚ä¿¡æ¯ï¼Œè¿™å¯¹äºç”µå•†å¹³å°æ¥è¯´éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ç†è§£å¹¶å“åº”å®¢æˆ·çš„å„ç§éœ€æ±‚å’Œåé¦ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6881ac1b5cb1b3266e3730667f72d613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-018ca3b59ce03850636fc6d36334342d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3290ef5936fecf3540fabec2ffbedcb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7a746bf5a6f687b7300a907748666dd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Vocabulary-free-few-shot-learning-for-Vision-Language-Models"><a href="#Vocabulary-free-few-shot-learning-for-Vision-Language-Models" class="headerlink" title="Vocabulary-free few-shot learning for Vision-Language Models"></a>Vocabulary-free few-shot learning for Vision-Language Models</h2><p><strong>Authors:Maxime Zanella, ClÃ©ment Fuchs, Ismail Ben Ayed, Christophe De Vleeschouwer</strong></p>
<p>Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have greatly expanded their ability to generalize across tasks using only a few labeled examples. However, existing approaches primarily build upon the strong zero-shot priors of these models by leveraging carefully designed, task-specific prompts. This dependence on predefined class names can restrict their applicability, especially in scenarios where exact class names are unavailable or difficult to specify. To address this limitation, we introduce vocabulary-free few-shot learning for VLMs, a setting where target class instances - that is, images - are available but their corresponding names are not. We propose Similarity Mapping (SiM), a simple yet effective baseline that classifies target instances solely based on similarity scores with a set of generic prompts (textual or visual), eliminating the need for carefully handcrafted prompts. Although conceptually straightforward, SiM demonstrates strong performance, operates with high computational efficiency (learning the mapping typically takes less than one second), and provides interpretability by linking target classes to generic prompts. We believe that our approach could serve as an important baseline for future research in vocabulary-free few-shot learning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/MaxZanella/vocabulary-free-FSL">https://github.com/MaxZanella/vocabulary-free-FSL</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°‘é‡æ ·æœ¬é€‚åº”æ–¹é¢çš„è¿›å±•æå¤§åœ°æ‰©å±•äº†å®ƒä»¬ä»…ä½¿ç”¨å°‘é‡æœ‰æ ‡ç­¾æ ·æœ¬å³å¯è·¨ä»»åŠ¡æ³›åŒ–çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºè¿™äº›æ¨¡å‹çš„å¼ºå¤§é›¶æ ·æœ¬å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ç‰¹å®šæç¤ºæ¥å®ç°ã€‚å¯¹é¢„è®¾ç±»åçš„è¿™ç§ä¾èµ–é™åˆ¶äº†å…¶é€‚ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰ç¡®åˆ‡ç±»åæˆ–éš¾ä»¥æŒ‡å®šçš„åœºæ™¯ä¸­ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºVLMså¼•å…¥äº†æ— è¯æ±‡å°‘é‡æ ·æœ¬å­¦ä¹ ï¼Œåœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œç›®æ ‡ç±»å®ä¾‹ï¼ˆå³å›¾åƒï¼‰æ˜¯å¯ç”¨çš„ï¼Œä½†å®ƒä»¬ç›¸åº”çš„åç§°ä¸å¯ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºç›¸ä¼¼æ€§æ˜ å°„ï¼ˆSiMï¼‰çš„ç®€å•æœ‰æ•ˆçš„åŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…æ ¹æ®ç›®æ ‡å®ä¾‹ä¸ä¸€ç»„é€šç”¨æç¤ºï¼ˆæ–‡æœ¬æˆ–è§†è§‰ï¼‰çš„ç›¸ä¼¼æ€§å¾—åˆ†è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ç²¾å¿ƒåˆ¶ä½œçš„æç¤ºçš„éœ€æ±‚ã€‚å°½ç®¡æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼Œä½†SiMè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼ˆå­¦ä¹ æ˜ å°„é€šå¸¸ä¸åˆ°ä¸€ç§’ï¼‰ï¼Œå¹¶é€šè¿‡å°†ç›®æ ‡ç±»åˆ«ä¸é€šç”¨æç¤ºè”ç³»èµ·æ¥æä¾›äº†è§£é‡Šæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸ºæœªæ¥çš„æ— è¯æ±‡å°‘é‡æ ·æœ¬å­¦ä¹ ç ”ç©¶æä¾›é‡è¦çš„åŸºçº¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MaxZanella/vocabulary-free-FSL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MaxZanella/vocabulary-free-FSLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04005v1">PDF</a> Accepted at CVPR Workshops 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ‘˜è¦ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¯æ±‡æ— å…³å°‘æ ·æœ¬å­¦ä¹ çš„æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–é¢„è®¾ç±»åˆ«åç§°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç›¸ä¼¼æ€§æ˜ å°„ï¼ˆSiMï¼‰çš„è¯æ±‡æ— å…³å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ— éœ€ç²¾å¿ƒè®¾è®¡ä»»åŠ¡ç‰¹å®šæç¤ºï¼Œä»…é€šè¿‡ç›®æ ‡å®ä¾‹ä¸ä¸€ç»„é€šç”¨æç¤ºçš„ç›¸ä¼¼æ€§å¾—åˆ†è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹ç²¾ç¡®ç±»åˆ«åç§°çš„éœ€æ±‚ã€‚SiMå…·æœ‰å¼ºå¤§çš„æ€§èƒ½ã€é«˜è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªé‡è¦çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬é€‚åº”æŠ€æœ¯å·²æ˜¾è‘—æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡è®°ç¤ºä¾‹æ—¶è·¨ä»»åŠ¡æ³›åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæ¨¡å‹çš„é›¶æ ·æœ¬å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ç‰¹å®šæç¤ºæ¥æ„å»ºã€‚</li>
<li>å¯¹é¢„è®¾ç±»åˆ«åç§°çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„é€‚ç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•è·å¾—æˆ–éš¾ä»¥æŒ‡å®šç¡®åˆ‡ç±»åˆ«åç§°çš„æƒ…å†µä¸‹ã€‚</li>
<li>å¼•å…¥äº†è¯æ±‡æ— å…³çš„å°‘æ ·æœ¬å­¦ä¹ ï¼Œå…¶ä¸­ç›®æ ‡ç±»åˆ«å®ä¾‹å¯ç”¨ï¼Œä½†å¯¹åº”çš„åç§°ä¸å¯ç”¨ã€‚</li>
<li>æå‡ºäº†ç›¸ä¼¼æ€§æ˜ å°„ï¼ˆSiMï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»…åŸºäºç›®æ ‡å®ä¾‹ä¸é€šç”¨æç¤ºçš„ç›¸ä¼¼æ€§å¾—åˆ†è¿›è¡Œåˆ†ç±»ï¼Œæ— éœ€ç²¾å¿ƒåˆ¶ä½œçš„æç¤ºã€‚</li>
<li>SiMå…·æœ‰å¼ºå¤§çš„æ€§èƒ½ã€é«˜è®¡ç®—æ•ˆç‡å’Œå¯è§£é‡Šæ€§ï¼Œé€šè¿‡é“¾æ¥ç›®æ ‡ç±»åˆ«åˆ°é€šç”¨æç¤ºæ¥æä¾›åˆ†ç±»ç†ç”±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7842b8c5d80453c20305fc060ef03f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21c4eea25edfb3a3e9059a3c32405842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d10cdff61bf1e12e8c0bd98e98544a83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4652140286a200870aecb8477291c1bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4267914babde5792f087c91158c33daa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3011b332a65e051a9f47b21c08c28835.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Multiple-Stochastic-Prompt-Tuning-for-Practical-Cross-Domain-Few-Shot-Learning"><a href="#Multiple-Stochastic-Prompt-Tuning-for-Practical-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot   Learning"></a>Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot   Learning</h2><p><strong>Authors:Debarshi Brahma, Soma Biswas</strong></p>
<p>In this work, we propose a practical cross-domain few-shot learning (pCDFSL) task, where a large-scale pre-trained model like CLIP can be easily deployed on a target dataset. The goal is to simultaneously classify all unseen classes under extreme domain shifts, by utilizing only a few labeled samples per class. The pCDFSL paradigm is source-free and moves beyond artificially created episodic training and testing regimes followed by existing CDFSL frameworks, making it more challenging and relevant to real-world applications. Towards that goal, we propose a novel framework, termed MIST (MultIple STochastic Prompt tuning), where multiple stochastic prompts are utilized to handle significant domain and semantic shifts. Specifically, multiple prompts are learnt for each class, effectively capturing multiple peaks in the input data. Furthermore, instead of representing the weights of the multiple prompts as point-estimates, we model them as learnable Gaussian distributions with two different strategies, encouraging an efficient exploration of the prompt parameter space, which mitigate overfitting due to the few labeled training samples. Extensive experiments and comparison with the state-of-the-art methods on four CDFSL benchmarks adapted to this setting, show the effectiveness of the proposed framework. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆpCDFSLï¼‰ä»»åŠ¡ï¼Œå…¶ä¸­å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šéƒ¨ç½²CLIPç­‰å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ã€‚ç›®æ ‡æ˜¯åˆ©ç”¨æ¯ä¸ªç±»åˆ«ä»…æœ‰çš„å°‘é‡æ ‡è®°æ ·æœ¬ï¼ŒåŒæ—¶å¯¹æç«¯åŸŸåç§»ä¸‹çš„æ‰€æœ‰æœªè§ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚pCDFSLèŒƒå¼æ˜¯æ— æºçš„ï¼Œè¶…è¶Šäº†ç°æœ‰çš„CDFSLæ¡†æ¶æ‰€éµå¾ªçš„äººå·¥åˆ›å»ºçš„äº‹ä»¶æ€§è®­ç»ƒå’Œæµ‹è¯•åˆ¶åº¦ï¼Œä½¿å…¶æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸çœŸå®ä¸–ç•Œåº”ç”¨çš„ç›¸å…³æ€§ã€‚æœç€è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç§°ä¸ºMISTï¼ˆå¤šé‡éšæœºæç¤ºè°ƒæ•´ï¼‰ï¼Œå…¶ä¸­åˆ©ç”¨å¤šä¸ªéšæœºæç¤ºæ¥å¤„ç†é‡å¤§çš„åŸŸå’Œè¯­ä¹‰å˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºæ¯ä¸ªç±»åˆ«å­¦ä¹ å¤šä¸ªæç¤ºï¼Œæœ‰æ•ˆåœ°æ•æ‰è¾“å…¥æ•°æ®ä¸­çš„å¤šä¸ªå³°å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸æ˜¯å°†å¤šä¸ªæç¤ºçš„æƒé‡è¡¨ç¤ºä¸ºç‚¹ä¼°è®¡ï¼Œè€Œæ˜¯å°†å®ƒä»¬å»ºæ¨¡ä¸ºä¸¤ç§ä¸åŒç­–ç•¥çš„å¯å­¦ä¹ é«˜æ–¯åˆ†å¸ƒï¼Œé¼“åŠ±æœ‰æ•ˆåœ°æ¢ç´¢æç¤ºå‚æ•°ç©ºé—´ï¼Œä»è€Œå‡è½»ç”±äºå°‘é‡æ ‡è®°è®­ç»ƒæ ·æœ¬è€Œå¯¼è‡´çš„è¿‡æ‹Ÿåˆã€‚åœ¨é€‚åº”æ­¤è®¾ç½®çš„å››ä¸ªCDFSLåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒä»¥åŠä¸æœ€æ–°æ–¹æ³•çš„æ¯”è¾ƒï¼Œè¯æ˜äº†æ‰€æå‡ºæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03926v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºè·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆpCDFSLï¼‰ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®ç”¨æ¡†æ¶MISTã€‚æ­¤æ¡†æ¶é€‚ç”¨äºéƒ¨ç½²å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šï¼Œç›®æ ‡æ˜¯é€šè¿‡æ¯ä¸ªç±»åˆ«ä»…æœ‰å°‘é‡æ ‡æ³¨æ ·æœ¬ï¼ŒåŒæ—¶å¯¹æ‰€æœ‰æœªè§ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚MISTé‡‡ç”¨å¤šä¸ªéšæœºæç¤ºï¼ˆpromptï¼‰æ¥å¤„ç†æ˜¾è‘—çš„åŸŸå’Œè¯­ä¹‰å˜åŒ–ï¼Œå¹¶ä¸ºæ¯ä¸ªç±»åˆ«å­¦ä¹ å¤šä¸ªæç¤ºï¼Œæœ‰æ•ˆåœ°æ•æ‰è¾“å…¥æ•°æ®ä¸­çš„å¤šä¸ªå³°å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ç§ç­–ç•¥å°†æç¤ºæƒé‡å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„é«˜æ–¯åˆ†å¸ƒï¼Œä»¥é«˜æ•ˆæ¢ç´¢æç¤ºå‚æ•°ç©ºé—´å¹¶ç¼“è§£å› å°‘é‡æ ‡æ³¨æ ·æœ¬å¼•èµ·çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚åœ¨å››ä¸ªCDFSLåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒå’Œå¯¹æ¯”æ˜¾ç¤ºäº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºè·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆpCDFSLï¼‰ä»»åŠ¡çš„å®ç”¨æ¡†æ¶MISTï¼Œé€‚ç”¨äºé¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šçš„åº”ç”¨ã€‚</li>
<li>MISTæ¡†æ¶èƒ½å¤ŸåŒæ—¶åˆ†ç±»æ‰€æœ‰æœªè§ç±»åˆ«ï¼Œä¸”ä»…ä½¿ç”¨æ¯ä¸ªç±»åˆ«çš„å°‘é‡æ ‡æ³¨æ ·æœ¬ã€‚</li>
<li>MISTé‡‡ç”¨å¤šä¸ªéšæœºæç¤ºï¼ˆpromptï¼‰æ¥å¤„ç†åŸŸå’Œè¯­ä¹‰çš„æ˜¾è‘—å˜åŒ–ã€‚</li>
<li>ä¸ºæ¯ä¸ªç±»åˆ«å­¦ä¹ å¤šä¸ªæç¤ºï¼Œä»¥æ•æ‰è¾“å…¥æ•°æ®ä¸­çš„å¤šä¸ªå³°å€¼ã€‚</li>
<li>æç¤ºæƒé‡è¢«å»ºæ¨¡ä¸ºå¯å­¦ä¹ çš„é«˜æ–¯åˆ†å¸ƒï¼Œä»¥é«˜æ•ˆæ¢ç´¢æç¤ºå‚æ•°ç©ºé—´ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å››ä¸ªCDFSLåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc32079d7446e6ef383c13f5119be1f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01a5096db6329db54de4b0d72e400ffa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29c061536d0435fe9897146400a11d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9089c1429bd8fdb47518d47327a7504d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a92c6abfbf6d90bc7dd463a22aa77bed.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="INP-Former-Advancing-Universal-Anomaly-Detection-via-Intrinsic-Normal-Prototypes-and-Residual-Learning"><a href="#INP-Former-Advancing-Universal-Anomaly-Detection-via-Intrinsic-Normal-Prototypes-and-Residual-Learning" class="headerlink" title="INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal   Prototypes and Residual Learning"></a>INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal   Prototypes and Residual Learning</h2><p><strong>Authors:Wei Luo, Haiming Yao, Yunkang Cao, Qiyu Chen, Ang Gao, Weiming Shen, Weihang Zhang, Wenyong Yu</strong></p>
<p>Anomaly detection (AD) is essential for industrial inspection and medical diagnosis, yet existing methods typically rely on &#96;&#96;comparingâ€™â€™ test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Furthermore, we propose a soft version of the INP Coherence Loss and enhance INP-Former by incorporating residual learning, leading to the development of INP-Former++. The proposed method significantly improves detection performance across single-class, multi-class, semi-supervised, few-shot, and zero-shot settings. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰å¯¹äºå·¥ä¸šæ£€æµ‹å’ŒåŒ»ç–—è¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†æµ‹è¯•å›¾åƒä¸è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œâ€œæ¯”è¾ƒâ€ã€‚ä½†æ˜¯ï¼Œå¤–è§‚å’Œä½ç½®çš„å˜åŒ–å¾€å¾€ä½¿è¿™äº›å‚è€ƒå›¾åƒä¸æµ‹è¯•å›¾åƒçš„å¯¹é½å˜å¾—å¤æ‚ï¼Œä»è€Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¤§å¤šæ•°å¼‚å¸¸è¡¨ç°ä¸ºå±€éƒ¨å˜åŒ–ï¼Œè¿™æ„å‘³ç€å³ä½¿åœ¨å¼‚å¸¸å›¾åƒå†…éƒ¨ï¼Œä»æœ‰å®è´µçš„æ­£å¸¸ä¿¡æ¯å­˜åœ¨ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¸å¼‚å¸¸æ›´åŠ å¯¹é½ï¼Œå› ä¸ºå¼‚å¸¸å’Œæ­£å¸¸ä¿¡æ¯éƒ½æ¥è‡ªåŒä¸€å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•INP-Formerï¼Œå®ƒç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†INPæå–å™¨ï¼Œè¯¥æå–å™¨é€šè¿‡çº¿æ€§ç»„åˆæ­£å¸¸ä»¤ç‰Œæ¥è¡¨ç¤ºINPsã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§INPä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿INPsèƒ½å¤Ÿå¿ å®åœ°ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚è¿™äº›INPsç„¶åå¼•å¯¼INPå¼•å¯¼è§£ç å™¨ä»…é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½¯æŒ–æ˜æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚INP-Formeråœ¨MVTec-ADã€VisAå’ŒReal-IADä¸Šçš„å•ç±»ã€å¤šç±»ã€å’Œå°‘æ ·æœ¬ADä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œæˆä¸ºADçš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒINP-Formerè¿˜è¡¨ç°å‡ºäº†ä¸€äº›é›¶æ ·æœ¬ADçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†INPä¸€è‡´æ€§æŸå¤±çš„è½¯ç‰ˆæœ¬ï¼Œå¹¶é€šè¿‡ç»“åˆæ®‹å·®å­¦ä¹ å¢å¼ºäº†INP-Formerï¼Œä»è€Œå‘å±•äº†INP-Former++ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å•ç±»ã€å¤šç±»ã€åŠç›‘ç£ã€å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸Šæ˜¾è‘—æé«˜äº†æ£€æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03660v1">PDF</a> 15 pages, 11 figures, 13 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°å‹å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detectionï¼ŒADï¼‰æ–¹æ³•â€”â€”INP-Formerã€‚è¯¥æ–¹æ³•ç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆIntrinsic Normal Prototypesï¼ŒINPsï¼‰ï¼Œæ— éœ€ä¾èµ–è®­ç»ƒé›†ä¸­çš„å¤–éƒ¨æ­£å¸¸å‚è€ƒã€‚é€šè¿‡å¼•å…¥INPæå–å™¨å’ŒINPç›¸å¹²æ€§æŸå¤±ï¼ŒINPsèƒ½å¿ å®ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚åˆ©ç”¨è¿™äº›INPsï¼ŒæŒ‡å¯¼è§£ç å™¨ä»…é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†è½¯æŒ–æ˜æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒæœŸé—´ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚è¯¥æ–¹æ³•åœ¨MVTec-ADã€VisAå’ŒReal-IADç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å•ç±»ã€å¤šç±»åŠå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹çš„å“è¶Šæ€§èƒ½ï¼Œå¹¶å…·æœ‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥è½¯åŒ–ç‰ˆæœ¬çš„INPç›¸å¹²æ€§æŸå¤±å¹¶ç»“åˆæ®‹å·®å­¦ä¹ ï¼Œè¿›ä¸€æ­¥å¼€å‘äº†æ”¹è¿›çš„INP-Former++ï¼Œæé«˜äº†åœ¨å¤šç§è®¾ç½®ä¸‹çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¼‚å¸¸æ£€æµ‹æ–¹æ³•å¸¸ä¾èµ–äºä¸è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œä½†å¤–è§‚å’Œä½ç½®çš„å˜åŒ–ä½¿å¾—å¯¹é½å˜å¾—å›°éš¾ï¼Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>INP-Formeræ–¹æ³•ç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ï¼Œæ— éœ€å¤–éƒ¨æ­£å¸¸å‚è€ƒã€‚</li>
<li>å¼•å…¥INPæå–å™¨å’ŒINPç›¸å¹²æ€§æŸå¤±ï¼Œç¡®ä¿INPsèƒ½å¿ å®ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚</li>
<li>åˆ©ç”¨INPsæŒ‡å¯¼è§£ç å™¨ä»…é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚</li>
<li>æå‡ºè½¯æŒ–æ˜æŸå¤±ï¼Œä»¥ä¼˜å…ˆå¤„ç†è®­ç»ƒä¸­çš„å›°éš¾æ ·æœ¬ã€‚</li>
<li>INP-Formeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•ç±»ã€å¤šç±»åŠå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼Œå¹¶å…·å¤‡é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¼•å…¥è½¯åŒ–ç‰ˆæœ¬çš„INPç›¸å¹²æ€§æŸå¤±å¹¶ç»“åˆæ®‹å·®å­¦ä¹ ï¼Œå¼€å‘äº†æ”¹è¿›çš„INP-Former++ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-036ea36231060819b54a5574902a9781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a983b37a2bd9c2b5eb93117fbab3937c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac4154d338eedd8b870f84cff1629bb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-98001f37b2b498380406311590aab93e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CHIME-Conditional-Hallucination-and-Integrated-Multi-scale-Enhancement-for-Time-Series-Diffusion-Model"><a href="#CHIME-Conditional-Hallucination-and-Integrated-Multi-scale-Enhancement-for-Time-Series-Diffusion-Model" class="headerlink" title="CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement   for Time Series Diffusion Model"></a>CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement   for Time Series Diffusion Model</h2><p><strong>Authors:Yuxuan Chen, Haipeng Xie</strong></p>
<p>The denoising diffusion probabilistic model has become a mainstream generative model, achieving significant success in various computer vision tasks. Recently, there has been initial exploration of applying diffusion models to time series tasks. However, existing studies still face challenges in multi-scale feature alignment and generative capabilities across different entities and long-time scales. In this paper, we propose CHIME, a conditional hallucination and integrated multi-scale enhancement framework for time series diffusion models. By employing multi-scale decomposition and adaptive integration, CHIME captures the decomposed features of time series, achieving in-domain distribution alignment between generated and original samples. In addition, we introduce a feature hallucination module in the conditional denoising process, enabling the transfer of temporal features through the training of category-independent transformation layers. Experimental results on publicly available real-world datasets demonstrate that CHIME achieves state-of-the-art performance and exhibits excellent generative generalization capabilities in few-shot scenarios. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹å·²ç»æˆä¸ºä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚æœ€è¿‘ï¼Œäººä»¬å¼€å§‹æ¢ç´¢å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶åœ¨å¤šå°ºåº¦ç‰¹å¾å¯¹é½å’Œè·¨ä¸åŒå®ä½“å’Œé•¿æ—¶é—´å°ºåº¦çš„ç”Ÿæˆèƒ½åŠ›æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CHIMEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ—¶é—´åºåˆ—æ‰©æ•£æ¨¡å‹çš„åŸºäºæ¡ä»¶å¹»è§‰å’Œé›†æˆå¤šå°ºåº¦å¢å¼ºæ¡†æ¶ã€‚é€šè¿‡é‡‡ç”¨å¤šå°ºåº¦åˆ†è§£å’Œè‡ªé€‚åº”é›†æˆï¼ŒCHIMEæ•æ‰æ—¶é—´åºåˆ—çš„åˆ†è§£ç‰¹å¾ï¼Œå®ç°ç”Ÿæˆæ ·æœ¬å’ŒåŸå§‹æ ·æœ¬ä¹‹é—´çš„åŸŸå†…åˆ†å¸ƒå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸€ä¸ªç‰¹å¾å¹»è§‰æ¨¡å—ï¼Œé€šè¿‡è®­ç»ƒç±»åˆ«ç‹¬ç«‹çš„è½¬æ¢å±‚ï¼Œå®ç°äº†æ—¶é—´ç‰¹å¾çš„è½¬ç§»ã€‚åœ¨å…¬å¼€å¯ç”¨çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCHIMEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å°æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„ç”Ÿæˆæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¦‚ç‡æ¨¡å‹å·²æˆä¸ºä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸã€‚æœ€è¿‘ï¼Œäººä»¬å¼€å§‹æ¢ç´¢å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡ï¼Œä½†ä»é¢ä¸´å¤šå°ºåº¦ç‰¹å¾å¯¹é½å’Œè·¨ä¸åŒå®ä½“åŠé•¿æ—¶é—´å°ºåº¦çš„ç”Ÿæˆèƒ½åŠ›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºCHIMEæ¡†æ¶ï¼Œé‡‡ç”¨å¤šå°ºåº¦åˆ†è§£å’Œè‡ªé€‚åº”é›†æˆï¼Œå®ç°æ—¶é—´åºåˆ—çš„åˆ†è§£ç‰¹å¾æ•æ‰ï¼Œç”Ÿæˆæ ·æœ¬ä¸åŸå§‹æ ·æœ¬ä¹‹é—´çš„åŸŸå†…åˆ†å¸ƒå¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¡ä»¶å»å™ªè¿‡ç¨‹ä¸­å¼•å…¥äº†ç‰¹å¾å¹»è§‰æ¨¡å—ï¼Œé€šè¿‡è®­ç»ƒç±»åˆ«ç‹¬ç«‹è½¬æ¢å±‚ï¼Œå®ç°æ—¶é—´ç‰¹å¾çš„è½¬ç§»ã€‚åœ¨å…¬å¼€çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCHIMEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å°æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°å‡ºä¼˜ç§€çš„ç”Ÿæˆæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¦‚ç‡æ¨¡å‹å·²æˆä¸ºä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚</li>
<li>å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºæ—¶é—´åºåˆ—ä»»åŠ¡å°šå¤„äºåˆæ­¥æ¢ç´¢é˜¶æ®µã€‚</li>
<li>ç°æœ‰ç ”ç©¶é¢ä¸´å¤šå°ºåº¦ç‰¹å¾å¯¹é½å’Œè·¨ä¸åŒå®ä½“åŠé•¿æ—¶é—´å°ºåº¦çš„ç”Ÿæˆèƒ½åŠ›æŒ‘æˆ˜ã€‚</li>
<li>CHIMEæ¡†æ¶é€šè¿‡å¤šå°ºåº¦åˆ†è§£å’Œè‡ªé€‚åº”é›†æˆå®ç°æ—¶é—´åºåˆ—çš„åˆ†è§£ç‰¹å¾æ•æ‰ã€‚</li>
<li>CHIMEå®ç°äº†ç”Ÿæˆæ ·æœ¬ä¸åŸå§‹æ ·æœ¬ä¹‹é—´çš„åŸŸå†…åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>CHIMEå¼•å…¥ç‰¹å¾å¹»è§‰æ¨¡å—ï¼Œé€šè¿‡è®­ç»ƒç±»åˆ«ç‹¬ç«‹è½¬æ¢å±‚ï¼Œå®ç°æ—¶é—´ç‰¹å¾çš„è½¬ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dde9f87a66ce1367497a6084caee8566.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25486974e1d9dc2869d277bee77b9df2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-319123ee9abdb43bbdc80f051bd8721c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RoNFA-Robust-Neural-Field-based-Approach-for-Few-Shot-Image-Classification-with-Noisy-Labels"><a href="#RoNFA-Robust-Neural-Field-based-Approach-for-Few-Shot-Image-Classification-with-Noisy-Labels" class="headerlink" title="RoNFA: Robust Neural Field-based Approach for Few-Shot Image   Classification with Noisy Labels"></a>RoNFA: Robust Neural Field-based Approach for Few-Shot Image   Classification with Noisy Labels</h2><p><strong>Authors:Nan Xiang, Lifeng Xing, Dequan Jin</strong></p>
<p>In few-shot learning (FSL), the labeled samples are scarce. Thus, label errors can significantly reduce classification accuracy. Since label errors are inevitable in realistic learning tasks, improving the robustness of the model in the presence of label errors is critical. This paper proposes a new robust neural field-based image approach (RoNFA) for few-shot image classification with noisy labels. RoNFA consists of two neural fields for feature and category representation. They correspond to the feature space and category set. Each neuron in the field for category representation (FCR) has a receptive field (RF) on the field for feature representation (FFR) centered at the representative neuron for its category generated by soft clustering. In the prediction stage, the range of these receptive fields adapts according to the neuronal activation in FCR to ensure prediction accuracy. These learning strategies provide the proposed model with excellent few-shot learning capability and strong robustness against label noises. The experimental results on real-world FSL datasets with three different types of label noise demonstrate that the proposed method significantly outperforms state-of-the-art FSL methods. Its accuracy obtained in the presence of noisy labels even surpasses the results obtained by state-of-the-art FSL methods trained on clean support sets, indicating its strong robustness against noisy labels. </p>
<blockquote>
<p>åœ¨å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰ä¸­ï¼Œæ ‡æ³¨æ ·æœ¬éå¸¸ç¨€ç¼ºã€‚å› æ­¤ï¼Œæ ‡ç­¾é”™è¯¯å¯èƒ½ä¼šæ˜¾è‘—é™ä½åˆ†ç±»ç²¾åº¦ã€‚ç”±äºåœ¨å®é™…å­¦ä¹ ä»»åŠ¡ä¸­æ ‡ç­¾é”™è¯¯ä¸å¯é¿å…ï¼Œå› æ­¤åœ¨å­˜åœ¨æ ‡ç­¾é”™è¯¯çš„æƒ…å†µä¸‹æé«˜æ¨¡å‹çš„ç¨³å¥æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡é’ˆå¯¹å¸¦æœ‰å™ªå£°æ ‡ç­¾çš„å°‘é‡å›¾åƒåˆ†ç±»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºäºç¨³å¥ç¥ç»åœºçš„å›¾åƒæ–¹æ³•ï¼ˆRoNFAï¼‰ã€‚RoNFAç”±ä¸¤ä¸ªç”¨äºç‰¹å¾å’Œç±»åˆ«è¡¨ç¤ºçš„ç¥ç»åœºç»„æˆã€‚å®ƒä»¬å¯¹åº”äºç‰¹å¾ç©ºé—´å’Œç±»åˆ«é›†ã€‚ç±»åˆ«è¡¨ç¤ºå­—æ®µï¼ˆFCRï¼‰ä¸­çš„æ¯ä¸ªç¥ç»å…ƒåœ¨å…¶ç‰¹å¾è¡¨ç¤ºå­—æ®µï¼ˆFFRï¼‰ä¸Šå…·æœ‰ä¸€ä¸ªä»¥é€šè¿‡è½¯èšç±»ç”Ÿæˆçš„ç±»åˆ«ä»£è¡¨ç¥ç»å…ƒä¸ºä¸­å¿ƒçš„æ¥å—åœºï¼ˆRFï¼‰ã€‚åœ¨é¢„æµ‹é˜¶æ®µï¼Œè¿™äº›æ¥å—åœºçš„èŒƒå›´ä¼šæ ¹æ®FCRä¸­çš„ç¥ç»å…ƒæ¿€æ´»æƒ…å†µè¿›è¡Œè°ƒæ•´ï¼Œä»¥ç¡®ä¿é¢„æµ‹ç²¾åº¦ã€‚è¿™äº›å­¦ä¹ ç­–ç•¥ä½¿æ‰€ææ¨¡å‹å…·æœ‰å‡ºè‰²çš„å°‘é‡å­¦ä¹ èƒ½åŠ›ï¼Œå¹¶å¯¹æ ‡ç­¾å™ªå£°å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚åœ¨å…·æœ‰ä¸‰ç§ä¸åŒç±»å‹æ ‡ç­¾å™ªå£°çš„çœŸå®ä¸–ç•ŒFSLæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„FSLæ–¹æ³•ã€‚åœ¨å­˜åœ¨å™ªå£°æ ‡ç­¾çš„æƒ…å†µä¸‹è·å¾—å…¶ç²¾åº¦ç”šè‡³è¶…è¿‡äº†åœ¨å¹²å‡€çš„æ”¯æ’‘é›†ä¸Šè®­ç»ƒçš„æœ€æ–°FSLæ–¹æ³•çš„ç»“æœï¼Œè¿™è¡¨æ˜å…¶å¯¹å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§å¾ˆå¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03461v1">PDF</a> 7 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»åœºçš„ç¨³å¥å›¾åƒåˆ†ç±»æ–¹æ³•ï¼ˆRoNFAï¼‰ï¼Œç”¨äºå°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰ä¸­çš„å¸¦å™ªå£°æ ‡ç­¾é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾è¡¨ç¤ºå’Œç±»åˆ«è¡¨ç¤ºçš„ä¸¤ä¸ªç¥ç»åœºæ¥æ„å»ºæ¨¡å‹ï¼Œå…·æœ‰ä¼˜ç§€çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä»¥åŠå¯¹æ ‡ç­¾å™ªå£°çš„å¼ºé²æ£’æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„FSLæ•°æ®é›†ä¸Šï¼Œå¯¹äºä¸‰ç§ä¸åŒç±»å‹çš„æ ‡ç­¾å™ªå£°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„FSLæ–¹æ³•ã€‚å³ä½¿åœ¨å­˜åœ¨å™ªå£°æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œå…¶å‡†ç¡®æ€§ä¹Ÿè¶…è¿‡äº†åœ¨å¹²å‡€çš„æ”¯æŒé›†ä¸Šè®­ç»ƒçš„ç°æœ‰FSLæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RoNFAæ˜¯ä¸€ç§åŸºäºç¥ç»åœºçš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œç”¨äºå¤„ç†å°æ ·æœ¬æ¬¡å­¦ä¹ ä¸­çš„æ ‡ç­¾å™ªå£°é—®é¢˜ã€‚</li>
<li>RoNFAåŒ…å«ä¸¤ä¸ªç¥ç»åœºï¼šä¸€ä¸ªç”¨äºç‰¹å¾è¡¨ç¤ºï¼ˆFFRï¼‰ï¼Œå¦ä¸€ä¸ªç”¨äºç±»åˆ«è¡¨ç¤ºï¼ˆFCRï¼‰ã€‚</li>
<li>FCRä¸­çš„æ¯ä¸ªç¥ç»å…ƒåœ¨FFRä¸Šéƒ½æœ‰ä¸€ä¸ªä»¥è¯¥ç±»åˆ«çš„ä»£è¡¨æ€§ç¥ç»å…ƒä¸ºä¸­å¿ƒçš„æ¥æ”¶åŸŸï¼ˆRFï¼‰ã€‚</li>
<li>åœ¨é¢„æµ‹é˜¶æ®µï¼Œè¿™äº›æ¥æ”¶åŸŸçš„èŒƒå›´ä¼šæ ¹æ®FCRä¸­çš„ç¥ç»å…ƒæ¿€æ´»è¿›è¡Œé€‚åº”ï¼Œä»¥ç¡®ä¿é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>RoNFAå…·æœ‰ä¼˜ç§€çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä»¥åŠå¯¹æ ‡ç­¾å™ªå£°çš„å¼ºé²æ£’æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRoNFAåœ¨çœŸå®ä¸–ç•Œçš„FSLæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„FSLæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ ‡ç­¾å™ªå£°æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03461">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8a6961a27bf1e749d3acdb7d79efb7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43dfbb1e76afc8e79f30e4b8a4e0a510.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8105f9a9ebbd28b31395c9f34719b340.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f165c75e53c392aeddab3fdd6a131bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed49dc7c6b43dd933273e13d4c2f1168.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d84b01d54568157152e76d241c410cff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708e318431cf59c9f094ae1f32a417d3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-AI-Agent-Architectures-for-Entity-Relationship-Classification"><a href="#Comparative-Analysis-of-AI-Agent-Architectures-for-Entity-Relationship-Classification" class="headerlink" title="Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification"></a>Comparative Analysis of AI Agent Architectures for Entity Relationship   Classification</h2><p><strong>Authors:Maryam Berijanian, Kuldeep Singh, Amin Sehati</strong></p>
<p>Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/maryambrj/ALIEN.git">https://github.com/maryambrj/ALIEN.git</a>. </p>
<blockquote>
<p>å®ä½“å…³ç³»åˆ†ç±»åœ¨ä¿¡æ¯æå–ä¸­ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™å’Œå¤æ‚å…³ç³»ç»“æ„çš„æƒ…å†µä¸‹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸‰ç§ç”¨äºæ‰§è¡Œå…³ç³»åˆ†ç±»çš„AIä»£ç†æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œè¿™äº›æ¶æ„ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æ¢ç´¢çš„ä»£ç†æ¶æ„åŒ…æ‹¬ï¼ˆ1ï¼‰åæ€è‡ªæˆ‘è¯„ä»·ï¼Œï¼ˆ2ï¼‰å±‚æ¬¡ä»»åŠ¡åˆ†è§£ï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸€ç§æ–°çš„å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ï¼Œæ¯ç§æœºåˆ¶éƒ½åˆ©ç”¨ä¸åŒçš„æ¨ç†å’Œæç¤ºé€‚åº”æ¨¡å¼ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæ–¹æ³•å¼•å…¥äº†å®æ—¶åˆä½œå’Œå¯¹æŠ—æ€§æç¤ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡å‹åç«¯ç³»ç»Ÿåœ°æ¯”è¾ƒäº†å®ƒä»¬çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œå¤šä»£ç†åä½œå§‹ç»ˆä¼˜äºæ ‡å‡†çš„å°æ ·æœ¬æç¤ºï¼Œå¹¶æ¥è¿‘å¾®è°ƒæ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸ºè®¾è®¡ç”¨äºç»“æ„åŒ–å…³ç³»æå–çš„æ¨¡å—åŒ–ã€é€šç”¨åŒ–LLMç³»ç»Ÿæä¾›äº†å®é™…æŒ‡å¯¼ã€‚æºä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/maryambrj/ALIEN.git%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/maryambrj/ALIEN.gitè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02426v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä¸‰ç§ä¸åŒçš„AIä»£ç†æ¶æ„åœ¨å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿™ä¸‰ç§æ¶æ„åˆ†åˆ«æ˜¯åå°„è‡ªæˆ‘è¯„ä»·ã€å±‚æ¬¡ä»»åŠ¡åˆ†è§£å’Œæ–°å‹å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ¯”è¾ƒè¿™äº›æ¶æ„åœ¨å¤šé¢†åŸŸå’Œæ¨¡å‹åç«¯çš„è¡¨ç°ï¼Œå‘ç°å¤šä»£ç†åè°ƒåœ¨æœ‰é™æ ‡è®°æ•°æ®å’Œå¤æ‚å…³ç³»ç»“æ„ä¸‹å§‹ç»ˆä¼˜äºæ ‡å‡†å°‘æ ·æœ¬æç¤ºæ–¹æ³•ï¼Œå¹¶æ¥è¿‘ç²¾ç»†è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ä¸ºè®¾è®¡æ¨¡å—åŒ–ã€é€šç”¨åŒ–çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å¯¹æ¯”åˆ†æäº†ä¸‰ç§AIä»£ç†æ¶æ„åœ¨å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>è¿™äº›æ¶æ„åŒ…æ‹¬åå°„è‡ªæˆ‘è¯„ä»·ã€å±‚æ¬¡ä»»åŠ¡åˆ†è§£å’Œå¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶ã€‚</li>
<li>å¤šä»£ç†åŠ¨æ€ç¤ºä¾‹ç”Ÿæˆæœºåˆ¶å¼•å…¥å®æ—¶åˆä½œå’Œå¯¹æŠ—æ€§æç¤ºï¼Œæé«˜äº†å…³ç³»åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç³»ç»Ÿæ¯”è¾ƒè¿™äº›æ¶æ„åœ¨å¤šä¸ªé¢†åŸŸå’Œæ¨¡å‹åç«¯çš„è¡¨ç°ï¼Œå‘ç°å¤šä»£ç†åè°ƒè¡¨ç°æœ€ä½³ã€‚</li>
<li>å¤šä»£ç†åè°ƒåœ¨æœ‰é™æ ‡è®°æ•°æ®å’Œå¤æ‚å…³ç³»ç»“æ„ä¸‹ä¼˜äºæ ‡å‡†å°‘æ ·æœ¬æç¤ºæ–¹æ³•ã€‚</li>
<li>å¤šä»£ç†åè°ƒæ–¹æ³•æ¥è¿‘ç²¾ç»†è°ƒæ•´æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºè®¾è®¡æ¨¡å—åŒ–ã€é€šç”¨åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8f5346e14c5399d820c5e96a50a22f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dea697f76100335fdf89746282a33143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87c560b9afb5054fbd70c7ab48032fb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7218792e58acd738a798fe0b1b511b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79971973db5de7b925deef4f1ab0716c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-505b5150ebb1833e5037a144bdcfa5ad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DOVE-A-Large-Scale-Multi-Dimensional-Predictions-Dataset-Towards-Meaningful-LLM-Evaluation"><a href="#DOVE-A-Large-Scale-Multi-Dimensional-Predictions-Dataset-Towards-Meaningful-LLM-Evaluation" class="headerlink" title="DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation"></a>DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards   Meaningful LLM Evaluation</h2><p><strong>Authors:Eliya Habba, Ofir Arviv, Itay Itzhak, Yotam Perlitz, Elron Bandel, Leshem Choshen, Michal Shmueli-Scheuer, Gabriel Stanovsky</strong></p>
<p>Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation.   Browse the data, contribute, and more: <a target="_blank" rel="noopener" href="https://slab-nlp.github.io/DOVE/">https://slab-nlp.github.io/DOVE/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹å„ç§ä»»æ„çš„æç¤ºç»´åº¦éƒ½éå¸¸æ•æ„Ÿï¼ŒåŒ…æ‹¬åˆ†éš”ç¬¦çš„ç±»å‹ã€ç­”æ¡ˆæšä¸¾å™¨ã€æŒ‡ä»¤æªè¾ç­‰ç­‰ã€‚è¿™å¼•å‘äº†äººä»¬å¯¹æµè¡Œçš„å•ä¸€æç¤ºè¯„ä¼°æ–¹æ³•çš„è´¨ç–‘ã€‚æˆ‘ä»¬æ¨å‡ºäº†DOVEï¼ˆå˜å¼‚è¯„ä¼°æ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«äº†å„ç§è¯„ä¼°åŸºå‡†æµ‹è¯•çš„æç¤ºæ‰°åŠ¨ã€‚ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬ä»æ•´ä½“çš„è§’åº¦æ¥å®¡è§†LLMçš„æ•æ„Ÿæ€§ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒç»´åº¦æ‰°åŠ¨è”åˆæ•ˆåº”ï¼Œå¯¼è‡´æ¯ä¸ªå®ä¾‹éƒ½æœ‰æˆåƒä¸Šä¸‡çš„æ‰°åŠ¨ã€‚æˆ‘ä»¬åœ¨DOVEä¸Šè¯„ä¼°äº†å‡ ä¸ªæ¨¡å‹å®¶æ—ï¼Œå¾—åˆ°äº†ä¸€äº›å‘ç°ï¼ŒåŒ…æ‹¬é€‰æ‹©è¡¨ç°è‰¯å¥½çš„æç¤ºçš„æœ‰æ•ˆæ–¹æ³•ï¼Œè§‚å¯Ÿåˆ°å°‘é‡ç¤ºä¾‹å¯ä»¥å‡å°‘æ•æ„Ÿæ€§ï¼Œå¹¶è¯†åˆ«å‡ºåœ¨æ‰€æœ‰æ‰°åŠ¨ä¸­å›ºæœ‰çš„éš¾ä»¥è§£å†³çš„å®ä¾‹ã€‚DOVEåŒ…å«è¶…è¿‡25äº¿ä¸ªæç¤ºæ‰°åŠ¨å’Œæ¨¡å‹è¾“å‡ºï¼Œæˆ‘ä»¬å°†å…¶å…¬å¼€æä¾›ï¼Œä»¥æ¿€å‘ç¤¾åŒºæœç€æœ‰æ„ä¹‰ã€ç¨³å¥å’Œé«˜æ•ˆçš„è¯„ä¼°æ–¹å‘å…±åŒåŠªåŠ›ã€‚æµè§ˆæ•°æ®ã€ä½œå‡ºè´¡çŒ®ç­‰ç­‰ï¼š<a target="_blank" rel="noopener" href="https://slab-nlp.github.io/DOVE/">https://slab-nlp.github.io/DOVE/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01622v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹æç¤ºç»´åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬åˆ†éš”ç¬¦ã€ç­”æ¡ˆæšä¸¾å™¨ã€æŒ‡ä»¤æªè¾ç­‰ã€‚DOVEæ•°æ®é›†ç”¨äºè¯„ä¼°æ¨¡å‹å¯¹å„ç§æç¤ºæ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œä»æ•´ä½“æ€§è§’åº¦è€ƒå¯ŸLLMçš„æ•æ„Ÿæ€§ï¼Œå¹¶è¯„ä¼°å„ç»´åº¦æ‰°åŠ¨çš„è”åˆæ•ˆåº”ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡2.5äº¿ä¸ªæç¤ºæ‰°åŠ¨å’Œæ¨¡å‹è¾“å‡ºï¼Œæä¾›å…¬ä¼—è®¿é—®ï¼Œä»¥æ¨åŠ¨æœ‰æ„ä¹‰ã€ç¨³å¥å’Œé«˜æ•ˆçš„è¯„ä¼°å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯¹æç¤ºç»´åº¦æ•æ„Ÿï¼ŒåŒ…æ‹¬åˆ†éš”ç¬¦ã€ç­”æ¡ˆæšä¸¾å™¨å’ŒæŒ‡ä»¤æªè¾ç­‰ã€‚</li>
<li>DOVEæ•°æ®é›†åŒ…å«å¤§é‡çš„æç¤ºæ‰°åŠ¨ï¼Œç”¨äºè¯„ä¼°LLMçš„æ•æ„Ÿæ€§ã€‚</li>
<li>DOVEæ•°æ®é›†ä»æ•´ä½“æ€§è§’åº¦è€ƒå¯ŸLLMçš„æ•æ„Ÿæ€§ï¼Œå¹¶è¯„ä¼°å„ç»´åº¦æ‰°åŠ¨çš„è”åˆæ•ˆåº”ã€‚</li>
<li>DOVEæ•°æ®é›†åŒ…å«è¶…è¿‡250Mçš„æç¤ºæ‰°åŠ¨å’Œæ¨¡å‹è¾“å‡ºï¼Œå¯ä¾›å…¬ä¼—è®¿é—®ã€‚</li>
<li>é€šè¿‡DOVEæ•°æ®é›†ï¼Œå‘ç°äº†é€‰æ‹©è¡¨ç°è‰¯å¥½çš„æç¤ºçš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å°‘é‡æ ·æœ¬å¯ä»¥å‡å°‘LLMçš„æ•æ„Ÿæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5660e8ae6f0613a564380d952d42002f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17b48d6c2121ca5d8f5f783e73cfe4a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-091f74d42c207c13f3d0d965c3d6f9c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79be5c30bea784efc34f17ad0cc26ed8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4465bff72aafe1e8661dfbde6810acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8ad25e32cd3edbbc1b1d0c8ca4c6fdb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improving-the-Language-Understanding-Capabilities-of-Large-Language-Models-Using-Reinforcement-Learning"><a href="#Improving-the-Language-Understanding-Capabilities-of-Large-Language-Models-Using-Reinforcement-Learning" class="headerlink" title="Improving the Language Understanding Capabilities of Large Language   Models Using Reinforcement Learning"></a>Improving the Language Understanding Capabilities of Large Language   Models Using Reinforcement Learning</h2><p><strong>Authors:Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie</strong></p>
<p>Instruction-fine-tuned large language models (LLMs) under 14B parameters continue to underperform on natural language understanding (NLU) tasks, often trailing smaller models like BERT-base on benchmarks such as GLUE and SuperGLUE. Motivated by the success of reinforcement learning in reasoning tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a framework to improve the NLU capabilities of LLMs. We frame NLU as a reinforcement learning environment, treating token generation as a sequence of actions and optimizing for reward signals based on alignment with ground-truth labels. PPO consistently outperforms supervised fine-tuning, yielding an average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models outperform GPT-4o by over 4% on average across sentiment and natural language inference tasks, including gains of 7.3% on the Mental Health dataset and 10.9% on SIGA-nli. This work highlights a promising direction for adapting LLMs to new tasks by reframing them as reinforcement learning problems, enabling learning through simple end-task rewards rather than extensive data curation. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨14Bå‚æ•°ä»¥ä¸‹ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸Šè¡¨ç°æŒç»­ä¸ä½³ï¼Œé€šå¸¸åœ¨GLUEå’ŒSuperGLUEç­‰åŸºå‡†æµ‹è¯•ä¸Šè½åäºè¾ƒå°çš„æ¨¡å‹ï¼Œå¦‚BERT-baseã€‚å—åˆ°å¼ºåŒ–å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ï¼ˆä¾‹å¦‚DeepSeekï¼‰ä¸­æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰ä½œä¸ºæ¡†æ¶ï¼Œä»¥æé«˜LLMçš„NLUèƒ½åŠ›ã€‚æˆ‘ä»¬å°†NLUä½œä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œå°†ä»¤ç‰Œç”Ÿæˆä½œä¸ºä¸€ç³»åˆ—åŠ¨ä½œï¼Œå¹¶åŸºäºä¸çœŸå®æ ‡ç­¾çš„å¯¹é½æƒ…å†µä¼˜åŒ–å¥–åŠ±ä¿¡å·ã€‚PPOå§‹ç»ˆä¼˜äºç›‘ç£å¾®è°ƒï¼Œåœ¨GLUEä¸Šå¹³å‡æé«˜äº†6.3åˆ†ï¼Œå¹¶åˆ†åˆ«è¶…è¿‡äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤º38.7åˆ†å’Œ26.1åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡PPOè®­ç»ƒçš„æ¨¡å‹åœ¨æƒ…æ„Ÿå’Œè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°è¶…è¿‡äº†GPT-4oè¶…è¿‡4%ï¼ŒåŒ…æ‹¬åœ¨å¿ƒç†å¥åº·æ•°æ®é›†ä¸Šæé«˜7.3%ï¼Œåœ¨SIGA-nliä¸Šæé«˜10.9%ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†é€šè¿‡é‡æ–°æ„å»ºä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜æ¥é€‚åº”LLMæ‰§è¡Œæ–°ä»»åŠ¡çš„å‰æ™¯ï¼Œä½¿å­¦ä¹ èƒ½å¤Ÿé€šè¿‡ç®€å•çš„ç»ˆç«¯ä»»åŠ¡å¥–åŠ±è€Œä¸æ˜¯å¤§é‡çš„æ•°æ®æ”¶é›†æ¥è¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11020v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°‘äº14Bå‚æ•°çš„æƒ…å†µä¸‹ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»ç„¶ä¸å°½å¦‚äººæ„ï¼Œå¸¸å¸¸åœ¨GLUEå’ŒSuperGLUEç­‰åŸºå‡†æµ‹è¯•ä¸­è½åäºè¾ƒå°çš„æ¨¡å‹ï¼Œå¦‚BERT-baseã€‚æœ¬ç ”ç©¶å—æ·±åº¦å­¦ä¹ åœ¨æ¨ç†ä»»åŠ¡ä¸­æˆåŠŸåº”ç”¨çš„å¯å‘ï¼Œæ¢ç´¢ä½¿ç”¨è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¡†æ¶æ¥æå‡LLMçš„NLUèƒ½åŠ›ã€‚æœ¬ç ”ç©¶å°†NLUä»»åŠ¡è§†ä¸ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œå°†ä»¤ç‰Œç”Ÿæˆè§†ä¸ºä¸€ç³»åˆ—è¡ŒåŠ¨ï¼Œå¹¶åŸºäºä¸çœŸå®æ ‡ç­¾çš„å¯¹é½æƒ…å†µæ¥ä¼˜åŒ–å¥–åŠ±ä¿¡å·ã€‚PPOåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡è¡¨ç°ä¼˜äºç›‘ç£å¾®è°ƒï¼Œæå‡äº†6.3ä¸ªç‚¹ï¼›å¹¶ä¸”åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºä¸‹çš„è¡¨ç°åˆ†åˆ«æå‡äº†38.7å’Œ26.1ä¸ªç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨PPOä¼˜åŒ–çš„æ¨¡å‹åœ¨æƒ…æ„Ÿå’Œè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°è¶…è¿‡GPT-4oè¶…è¿‡4%ï¼Œå…¶ä¸­åœ¨ç²¾ç¥å¥åº·æ•°æ®é›†ä¸Šæå‡äº†7.3%ï¼Œåœ¨SIGA-nliä¸Šæå‡äº†10.9%ã€‚è¿™è¡¨æ˜å°†LLMé‡æ–°å®šä½ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¯ä»¥é€šè¿‡ç®€å•çš„ç»ˆç«¯ä»»åŠ¡å¥–åŠ±è¿›è¡Œå­¦ä¹ ï¼Œè€Œæ— éœ€å¤§é‡æ•°æ®æ•´ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°‘äº14Bå‚æ•°æ—¶ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œå¸¸è½åäºè¾ƒå°æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æ¡†æ¶æ¥æå‡LLMçš„NLUèƒ½åŠ›ã€‚</li>
<li>å°†NLUä»»åŠ¡è§†ä¸ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼Œå°†ä»¤ç‰Œç”Ÿæˆè§†ä¸ºä¸€ç³»åˆ—è¡ŒåŠ¨ï¼Œå¹¶ä¼˜åŒ–å¥–åŠ±ä¿¡å·ä»¥å¯¹é½çœŸå®æ ‡ç­¾ã€‚</li>
<li>PPOåœ¨GLUEåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç›‘ç£å¾®è°ƒå’Œå…¶ä»–æ–¹æ³•ã€‚</li>
<li>PPOä¼˜åŒ–çš„æ¨¡å‹åœ¨æƒ…æ„Ÿå’Œè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°è¶…è¿‡GPT-4oè¶…è¿‡4%ï¼Œåœ¨æŸäº›æ•°æ®é›†ä¸Šçš„æå‡æ˜¾è‘—ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†LLMé‡æ–°å®šä½ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.11020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dadb15198773b831df1ac1e0a29e696f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9c217131817731f7f05cee1691eed18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53a89ad01efdf8b76d5a47ed931d54b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0550e62e6a2671a165c6a6c5025c0cd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0f7d5ba4831bfe386f88647faff46233.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  ATI Any Trajectory Instruction for Controllable Video Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab60ebc7690d7c235db9dbfc46519a2b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  OWMM-Agent Open World Mobile Manipulation With Multi-modal Agentic Data   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31086.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
