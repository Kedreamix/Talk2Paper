<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  Language-Image Alignment with Fixed Text Encoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6695769443305db9e6b95213c3d323bb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-06-æ›´æ–°"><a href="#2025-06-06-æ›´æ–°" class="headerlink" title="2025-06-06 æ›´æ–°"></a>2025-06-06 æ›´æ–°</h1><h2 id="Language-Image-Alignment-with-Fixed-Text-Encoders"><a href="#Language-Image-Alignment-with-Fixed-Text-Encoders" class="headerlink" title="Language-Image Alignment with Fixed Text Encoders"></a>Language-Image Alignment with Fixed Text Encoders</h2><p><strong>Authors:Jingfeng Yang, Ziyang Wu, Yue Zhao, Yi Ma</strong></p>
<p>Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations. </p>
<blockquote>
<p>ç›®å‰ï¼Œå»ºç«‹è¯­è¨€-å›¾åƒå¯¹é½çš„æœ€ä¸»æµæ–¹æ³•æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ è”åˆé¢„è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œå¦‚CLIPåŠå…¶å˜ä½“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è´¨ç–‘è¿™ç§æ˜‚è´µçš„è”åˆè®­ç»ƒæ˜¯å¦æœ‰å¿…è¦ã€‚æˆ‘ä»¬ç‰¹åˆ«è°ƒæŸ¥é¢„è®­ç»ƒçš„å›ºå®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦æä¾›äº†è¶³å¤Ÿå¥½çš„æ–‡æœ¬ç¼–ç å™¨æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†åªè®­ç»ƒå›¾åƒç¼–ç å™¨ï¼Œåˆ©ç”¨LLMçš„å›ºå®šæ–‡æœ¬ç¼–ç å™¨æ¥å­¦ä¹ è¯­è¨€-å›¾åƒå¯¹é½ï¼ˆLIFTï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œé€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•å’Œæ¶ˆèç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°è¿™ç§ç®€åŒ–çš„LIFTæ¡†æ¶éå¸¸æœ‰æ•ˆï¼Œåœ¨æ¶‰åŠç»„åˆç†è§£å’Œé•¿å­—å¹•çš„å¤šæ•°åœºæ™¯ä¸­ï¼Œå®ƒéƒ½ä¼˜äºCLIPï¼ŒåŒæ—¶å®ç°äº†è®¡ç®—æ•ˆç‡ä¸Šçš„æ˜¾è‘—æ”¶ç›Šã€‚æˆ‘ä»¬çš„å·¥ä½œæœç€ç³»ç»Ÿåœ°æ¢ç´¢LLMä¸­çš„æ–‡æœ¬åµŒå…¥å¦‚ä½•æŒ‡å¯¼è§†è§‰å­¦ä¹ è¿ˆå‡ºäº†ç¬¬ä¸€æ­¥ï¼Œå¹¶ä¸ºå­¦ä¹ è¯­è¨€å¯¹é½çš„è§†è§‰è¡¨ç¤ºæä¾›äº†å¦ä¸€ç§è®¾è®¡é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04209v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨é¢„è®­ç»ƒçš„å›ºå®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¯­è¨€å›¾åƒå¯¹é½çš„å¯è¡Œæ€§ï¼Œæå‡ºäº†ä¸€ç§ä»…è®­ç»ƒå›¾åƒç¼–ç å™¨çš„æ–¹æ³•ï¼ˆç§°ä¸ºLIFTï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æ¶‰åŠç»„åˆç†è§£å’Œé•¿æ–‡æœ¬æè¿°çš„åœºæ™¯ä¸­ï¼Œè¡¨ç°ä¼˜äºCLIPæ¨¡å‹ï¼Œå¹¶å®ç°äº†è®¡ç®—æ•ˆç‡çš„æ˜¾è‘—æé«˜ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°æ¢ç´¢äº†LLMæ–‡æœ¬åµŒå…¥å¦‚ä½•æŒ‡å¯¼è§†è§‰å­¦ä¹ ï¼Œå¹¶ä¸ºè¯­è¨€å¯¹é½çš„è§†è§‰è¡¨ç¤ºå­¦ä¹ æä¾›äº†å¦ä¸€ç§è®¾è®¡é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®å‰ä¸»æµçš„è¯­è¨€å›¾åƒå¯¹é½æ–¹æ³•æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ è”åˆé¢„è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨ï¼Œå¦‚CLIPåŠå…¶å˜ä½“ã€‚</li>
<li>æœ¬æ–‡è´¨ç–‘æ˜¯å¦å¿…é¡»è¿›è¡Œè¿™ç§æ˜‚è´µçš„è”åˆè®­ç»ƒã€‚</li>
<li>æå‡ºä»…é€šè¿‡é¢„è®­ç»ƒçš„å›ºå®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ–‡æœ¬ç¼–ç å™¨æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºLIFTã€‚</li>
<li>LIFTæ–¹æ³•é€šè¿‡ç®€åŒ–çš„æ¡†æ¶å®ç°äº†ä¸CLIPç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ¶‰åŠç»„åˆç†è§£å’Œé•¿æ–‡æœ¬æè¿°çš„åœºæ™¯ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
<li>LIFTæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°æ¢ç´¢äº†LLMæ–‡æœ¬åµŒå…¥åœ¨æŒ‡å¯¼è§†è§‰å­¦ä¹ æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04209">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e35dd00dafa87d19ae5cdb2ef4db2c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08f71da0d9182562150788394cc22b7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7625d4d2b7bd22b0e72e62f64929559d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d83f766298684ab97e7007eeb17d817.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c64e4f027558ed0d577302978ed9d503.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8730eae0827117c25c0e102a735d373d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a57fc35e1379ffb11109a6822a1f03cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bdc57b842e28073c0ec8f285b5fc1c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Advancing-Multimodal-Reasoning-From-Optimized-Cold-Start-to-Staged-Reinforcement-Learning"><a href="#Advancing-Multimodal-Reasoning-From-Optimized-Cold-Start-to-Staged-Reinforcement-Learning" class="headerlink" title="Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning"></a>Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning</h2><p><strong>Authors:Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng</strong></p>
<p>Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025. </p>
<blockquote>
<p>å—åˆ°Deepseek-R1åœ¨å¤„ç†å¤æ‚æ–‡æœ¬ä»»åŠ¡ä¸­çš„å‡ºè‰²æ¨ç†èƒ½åŠ›çš„å¯å‘ï¼Œè®¸å¤šå·¥ä½œå°è¯•é€šè¿‡ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æ¿€åŠ±ç±»ä¼¼çš„èƒ½åŠ›äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚ç„¶è€Œï¼Œä»–ä»¬åœ¨æ¿€æ´»å¤æ‚æ¨ç†æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å­¤ç«‹åœ°ç ”ç©¶å¤šæ¨¡æ€RLï¼Œè€Œæ˜¯æ·±å…¥ç ”ç©¶äº†å½“å‰çš„è®­ç»ƒç®¡é“ï¼Œå¹¶å‘ç°äº†ä¸‰ä¸ªå…³é”®ç°è±¡ï¼š1ï¼‰æœ‰æ•ˆçš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹äºå¢å¼ºMLLMæ¨ç†è‡³å…³é‡è¦ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä»…é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®è¿›è¡Œåˆå§‹åŒ–å¯ä»¥å¯¼è‡´æ€§èƒ½è¶…è¶Šè®¸å¤šæœ€æ–°çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œç”šè‡³åœ¨å¤šæ¨¡æ€RLä¹‹å‰ã€‚2ï¼‰åº”ç”¨äºå¤šæ¨¡æ€RLçš„æ ‡å‡†GRPOé­å—æ¢¯åº¦åœæ»çš„å›°æ‰°ï¼Œè¿™é™ä½äº†è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚3ï¼‰åœ¨å¤šæ¨¡æ€RLé˜¶æ®µä¹‹åè¿›è¡Œçš„çº¯æ–‡æœ¬RLè®­ç»ƒå¯è¿›ä¸€æ­¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚è¿™ç§åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†æ„ŸçŸ¥å®šä½ä¸è®¤çŸ¥æ¨ç†å‘å±•ã€‚é€šè¿‡èå…¥ä¸Šè¿°è§è§£å¹¶è§£å†³å¤šæ¨¡æ€RLçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ReVisual-R1ï¼Œåœ¨åŒ…æ‹¬MathVerseã€MathVisionã€WeMathã€LogicVistaã€DynaMathä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„AIME2024å’ŒAIME2025ç­‰å¼€æ”¾æºä»£ç çš„7B MLLMsä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04207v1">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æ¿€åŠ±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œæœ‰æ•ˆçš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹å¢å¼ºMLLMæ¨ç†è‡³å…³é‡è¦ã€‚é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®åˆå§‹åŒ–å¯ä»¥è¶…è¶Šè®¸å¤šæœ€æ–°çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚åŒæ—¶ï¼Œå‘ç°æ ‡å‡†GRPOåœ¨åº”ç”¨äºå¤šæ¨¡æ€RLæ—¶å­˜åœ¨æ¢¯åº¦åœæ»é—®é¢˜ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚éšåæ–‡æœ¬åªæœ‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œåœ¨å¤šå±‚æ¬¡å¼ºåŒ–å­¦ä¹ é˜¶æ®µåï¼Œèƒ½è¿›ä¸€æ­¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚é€šè¿‡ç»“åˆè¿™äº›è§è§£å¹¶è§£å†³å¤šæ¨¡æ€RLé—®é¢˜ï¼Œæ¨å‡ºäº†ReVisual-R1æ¨¡å‹ï¼Œåœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¼€æº7B MLLMçš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ‰æ•ˆå†·å¯åŠ¨åˆå§‹åŒ–å¯¹å¢å¼ºMLLMæ¨ç†è‡³å…³é‡è¦ï¼Œé€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®åˆå§‹åŒ–æ€§èƒ½å¯è¶…è¶Šè®¸å¤šå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚</li>
<li>æ ‡å‡†GRPOåœ¨å¤šæ¨¡æ€RLä¸­å­˜åœ¨æ¢¯åº¦åœæ»é—®é¢˜ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>æ–‡æœ¬åªæœ‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒåœ¨å¤šæ¨¡æ€RLé˜¶æ®µä¹‹åèƒ½è¿›ä¸€æ­¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>å¹³è¡¡æ„ŸçŸ¥åŸºç¡€å’Œè®¤çŸ¥æ¨ç†å‘å±•æ˜¯å…³é”®ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•ã€‚</li>
<li>ReVisual-R1æ¨¡å‹ç»“åˆä»¥ä¸Šè§è§£è§£å†³å¤šæ¨¡æ€RLé—®é¢˜ï¼Œå®ç°å¼€æº7B MLLMçš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>ReVisual-R1æ¨¡å‹åœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬MathVerse, MathVision, WeMath, LogicVista, DynaMathåŠAIME2024å’ŒAIME2025ã€‚</li>
<li>æ­¤ç ”ç©¶æˆæœå¯¹äºæ¨åŠ¨MLLMsçš„å¤æ‚æ¨ç†èƒ½åŠ›æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c372b9949f00003cd9668401c8ba805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3df634d151ee7b5f86f0bb59474c69b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4d7104ecdbac515058be32687c3577.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation"><a href="#EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation" class="headerlink" title="EPiC: Towards Lossless Speedup for Reasoning Training through   Edge-Preserving CoT Condensation"></a>EPiC: Towards Lossless Speedup for Reasoning Training through   Edge-Preserving CoT Condensation</h2><p><strong>Authors:Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu</strong></p>
<p>Large language models (LLMs) have shown remarkable reasoning capabilities when trained with chain-of-thought (CoT) supervision. However, the long and verbose CoT traces, especially those distilled from large reasoning models (LRMs) such as DeepSeek-R1, significantly increase training costs during the distillation process, where a non-reasoning base model is taught to replicate the reasoning behavior of an LRM. In this work, we study the problem of CoT condensation for resource-efficient reasoning training, aimed at pruning intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling supervised model training on length-reduced CoT data while preserving both answer accuracy and the modelâ€™s ability to generate coherent reasoning. Our rationale is that CoT traces typically follow a three-stage structure: problem understanding, exploration, and solution convergence. Through empirical analysis, we find that retaining the structure of the reasoning trace, especially the early stage of problem understanding (rich in reflective cues) and the final stage of solution convergence, is sufficient to achieve lossless reasoning supervision. To this end, we propose an Edge-Preserving Condensation method, EPiC, which selectively retains only the initial and final segments of each CoT trace while discarding the middle portion. This design draws an analogy to preserving the â€œedgeâ€ of a reasoning trajectory, capturing both the initial problem framing and the final answer synthesis, to maintain logical continuity. Experiments across multiple model families (Qwen and LLaMA) and benchmarks show that EPiC reduces training time by over 34% while achieving lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To the best of our knowledge, this is the first study to explore thought-level CoT condensation for efficient reasoning model distillation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå±•ç°å‡ºäº†æ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ€ç»´é“¾è½¨è¿¹ï¼Œå°¤å…¶æ˜¯ä»æ·±åº¦å¯»æ±‚R1ç­‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ä¸­æç‚¼å‡ºçš„è½¨è¿¹ï¼Œåœ¨è’¸é¦è¿‡ç¨‹ä¸­æ˜¾è‘—å¢åŠ äº†è®­ç»ƒæˆæœ¬ï¼Œå…¶ä¸­éæ¨ç†åŸºç¡€æ¨¡å‹è¢«æ•™å¯¼å¤åˆ¶LRMçš„æ¨ç†è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é¢å‘èµ„æºé«˜æ•ˆæ¨ç†è®­ç»ƒçš„æ€ç»´é“¾å‡ç»“é—®é¢˜ï¼Œæ—¨åœ¨åˆ é™¤æ€ç»´é“¾è½¨è¿¹ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆå³æ€ç»´ï¼‰ï¼Œä»è€Œåœ¨ä¿æŒç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨¡å‹ç”Ÿæˆè¿è´¯æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°åœ¨ç¼©å‡é•¿åº¦åçš„æ€ç»´é“¾æ•°æ®ä¸Šç›‘ç£æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬çš„ç†å¿µæ˜¯ï¼Œæ€ç»´é“¾è½¨è¿¹é€šå¸¸éµå¾ªä¸‰é˜¶æ®µç»“æ„ï¼šç†è§£é—®é¢˜ã€æ¢ç´¢å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¿ç•™æ¨ç†è½¨è¿¹çš„ç»“æ„ï¼Œç‰¹åˆ«æ˜¯ç†è§£é—®é¢˜çš„æ—©æœŸé˜¶æ®µï¼ˆå¯Œå«åæ€çº¿ç´¢ï¼‰å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›çš„æœ€åä¸€ä¸ªé˜¶æ®µï¼Œè¶³ä»¥å®ç°æ— æŸæ¨ç†ç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾¹ç¼˜ä¿ç•™å‡ç»“æ–¹æ³•ï¼ˆEPiCï¼‰ï¼Œè¯¥æ–¹æ³•æœ‰é€‰æ‹©åœ°ä»…ä¿ç•™æ¯ä¸ªæ€ç»´é“¾è½¨è¿¹çš„åˆå§‹å’Œæœ€ç»ˆéƒ¨åˆ†ï¼ŒåŒæ—¶ä¸¢å¼ƒä¸­é—´éƒ¨åˆ†ã€‚è¿™ç§è®¾è®¡ç±»ä¼¼äºä¿ç•™æ¨ç†è½¨è¿¹çš„â€œè¾¹ç¼˜â€ï¼Œæ•æ‰åˆå§‹é—®é¢˜æ¡†æ¶å’Œæœ€ç»ˆç­”æ¡ˆåˆæˆï¼Œä»¥ä¿æŒé€»è¾‘è¿ç»­æ€§ã€‚åœ¨å¤šæ¨¡å‹å®¶æ—ï¼ˆQwenå’ŒLLaMAï¼‰å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEPiCå°†è®­ç»ƒæ—¶é—´å‡å°‘äº†34%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨MATH500ä¸Šå®ç°äº†ä¸å®Œæ•´æ€ç»´é“¾ç›‘ç£ç›¸å½“çš„æ¨ç†å‡†ç¡®æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹æ¢ç´¢é«˜æ•ˆæ¨ç†æ¨¡å‹è’¸é¦ä¸­çš„æ€ç»´æ°´å¹³æ€ç»´é“¾å‡ç»“çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04205v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£è®­ç»ƒå±•ç°å‡ºå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°¤å…¶æ˜¯åœ¨ä»æ·±åº¦æœç´¢Rç­‰å¤§æ•°æ®æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æç‚¼å‡ºæ¥çš„é•¿ä¸”å†—é•¿çš„CoTè½¨è¿¹ä¸­ï¼Œæ˜¾è‘—å¢åŠ äº†è’¸é¦è¿‡ç¨‹ä¸­çš„è®­ç»ƒæˆæœ¬ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶å‡å°‘èµ„æºæ¶ˆè€—çš„æ¨ç†è®­ç»ƒä¸­çš„æ€ç»´é“¾ï¼ˆCoTï¼‰å‡èšé—®é¢˜ï¼Œæ—¨åœ¨ä¿®å‰ªCoTè½¨è¿¹ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä»è€Œåœ¨ä¿æŒç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨¡å‹ç”Ÿæˆè¿è´¯æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå¯¹é•¿åº¦ç¼©å‡çš„CoTæ•°æ®è¿›è¡Œç›‘ç£æ¨¡å‹è®­ç»ƒã€‚æœ¬æ–‡çš„æ¨ç†è½¨è¿¹é€šå¸¸éµå¾ªç†è§£é—®é¢˜ã€æ¢ç´¢ã€å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›çš„ä¸‰é˜¶æ®µç»“æ„ã€‚é€šè¿‡å®éªŒåˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¿ç•™ç†è§£é—®é¢˜çš„åˆæ­¥é˜¶æ®µä»¥åŠè§£å†³æ–¹æ¡ˆæ”¶æ•›çš„æœ€ç»ˆé˜¶æ®µæ˜¯ç¡®ä¿æ— æŸå¤±æ¨ç†ç›‘ç£çš„å…³é”®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾¹ç¼˜ä¿ç•™å‡èšæ–¹æ³•ï¼ˆEPiCï¼‰ï¼Œè¯¥æ–¹æ³•æœ‰é€‰æ‹©åœ°ä¿ç•™æ¯ä¸ªCoTè½¨è¿¹çš„åˆå§‹å’Œæœ€ç»ˆéƒ¨åˆ†ï¼ŒåŒæ—¶ä¸¢å¼ƒä¸­é—´éƒ¨åˆ†ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºæ•æ‰æ¨ç†è½¨è¿¹çš„â€œè¾¹ç¼˜â€ï¼Œæ—¢èƒ½ä¿æŒé—®é¢˜çš„åˆå§‹æ¡†æ¶ï¼Œåˆèƒ½ç»´æŒæœ€ç»ˆçš„ç­”æ¡ˆåˆæˆï¼Œä»è€Œç»´æŒé€»è¾‘è¿è´¯æ€§ã€‚å®éªŒæ˜¾ç¤ºï¼ŒEPiCåœ¨å¤šæ¨¡å‹å®¶æ—å’ŒåŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†è¶…è¿‡34%çš„è®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶åœ¨MATH500ä¸Šå®ç°äº†æ— æŸæ¨ç†å‡†ç¡®æ€§ï¼Œä¸å®Œæ•´çš„CoTç›‘ç£ç›¸å½“ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹å…³äºæœ‰æ•ˆæ¨ç†æ¨¡å‹è’¸é¦ä¸­æ€ç»´çº§CoTå‡èšçš„ç ”ç©¶ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é“¾å¼æ€ç»´ç›‘ç£ä¸‹å±•ç°å‡ºå“è¶Šæ¨ç†èƒ½åŠ›ï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨é«˜æˆæœ¬é—®é¢˜ã€‚</li>
<li>ç ”ç©¶èšç„¦äºæ€ç»´é“¾å‡èšä»¥å‡å°‘èµ„æºæ¶ˆè€—ï¼Œæ—¨åœ¨ä¿®å‰ªä¸­é—´æ¨ç†æ­¥éª¤ã€‚</li>
<li>æ€ç»´é“¾è½¨è¿¹éµå¾ªä¸‰é˜¶æ®µç»“æ„ï¼šç†è§£é—®é¢˜ã€æ¢ç´¢å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›ã€‚</li>
<li>ä¿ç•™é—®é¢˜ç†è§£é˜¶æ®µå’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›é˜¶æ®µæ˜¯ç¡®ä¿æ— æŸæ¨ç†ç›‘ç£çš„å…³é”®ã€‚</li>
<li>æå‡ºè¾¹ç¼˜ä¿ç•™å‡èšæ–¹æ³•ï¼ˆEPiCï¼‰ï¼Œé€‰æ‹©æ€§ä¿ç•™æ€ç»´è½¨è¿¹çš„åˆå§‹å’Œæœ€ç»ˆéƒ¨åˆ†ã€‚</li>
<li>EPiCæ–¹æ³•æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†MATH500ä¸Šçš„æ— æŸæ¨ç†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55aca2880c7d30e7d16c63af14f1b211.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9af991d4c026c1252da709b661ce928.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bbc31fcad4fcc0d988bc4a1c71c5b6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86ae84eafc90ce41080ecab61e15d41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da85968f48f5e69fb97cfa1a3a40f70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aafeaa0f991135cc33b51381844ec1d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b74f08e0285f605dbb2e1f49223bac5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cascadia-A-Cascade-Serving-System-for-Large-Language-Models"><a href="#Cascadia-A-Cascade-Serving-System-for-Large-Language-Models" class="headerlink" title="Cascadia: A Cascade Serving System for Large Language Models"></a>Cascadia: A Cascade Serving System for Large Language Models</h2><p><strong>Authors:Youhe Jiang, Fangcheng Fu, Wanru Zhao, Stephan Rabanser, Nicholas D. Lane, Binhang Yuan</strong></p>
<p>Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality answers. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet less capable. Recent work proposes balancing this latency-quality trade-off using model cascades, which route simpler queries to smaller models and more complex ones to larger models. However, enabling efficient cascade serving remains challenging. Current frameworks lack effective mechanisms for handling (i) the huge and varying resource demands of different LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the co-optimization of system deployment and routing strategy. Motivated by these observations, we introduce Cascadia, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. Cascadia employs a bi-level optimization method: at the inner level, it uses a mixed-integer linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the outer level, it applies a weighted Tchebycheff algorithm to iteratively co-optimize the routing strategy and the system deployment produced by the inner level. Our extensive evaluation on diverse workload traces and different model cascades (DeepSeek and the Llama series) demonstrates that Cascadia significantly outperforms both single-model deployments and the state-of-the-art cascade serving baseline, achieving up to 4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher throughput while maintaining target answer quality. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åŠ å‰§äº†å¯¹å¿«é€Ÿå“åº”å’Œé«˜è´¨ç­”æ¡ˆçš„éœ€æ±‚ã€‚æ›´å¼ºå¤§çš„æ¨¡å‹è™½ç„¶èƒ½äº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œä½†ä¼šå¯¼è‡´æ›´é«˜çš„æ¨ç†å»¶è¿Ÿï¼Œè€Œè¾ƒå°çš„æ¨¡å‹è™½ç„¶é€Ÿåº¦æ›´å¿«ï¼Œä½†èƒ½åŠ›è¾ƒå¼±ã€‚æœ€è¿‘çš„å·¥ä½œæå‡ºäº†ä½¿ç”¨æ¨¡å‹çº§è”æ¥å¹³è¡¡è¿™ç§å»¶è¿Ÿä¸è´¨é‡çš„æƒè¡¡ï¼Œå°†ç®€å•çš„æŸ¥è¯¢è·¯ç”±åˆ°è¾ƒå°çš„æ¨¡å‹ï¼Œå°†å¤æ‚çš„æŸ¥è¯¢è·¯ç”±åˆ°è¾ƒå¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®ç°é«˜æ•ˆçš„çº§è”æœåŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å½“å‰æ¡†æ¶ç¼ºä¹æœ‰æ•ˆæœºåˆ¶æ¥å¤„ç†ï¼ˆiï¼‰ä¸åŒLLMçš„å·¨å¤§ä¸”å¤šå˜çš„èµ„æºéœ€æ±‚ï¼Œï¼ˆiiï¼‰LLMå·¥ä½œè´Ÿè½½çš„å›ºæœ‰å¼‚è´¨æ€§ï¼Œä»¥åŠï¼ˆiiiï¼‰ç³»ç»Ÿéƒ¨ç½²å’Œè·¯ç”±ç­–ç•¥çš„ååŒä¼˜åŒ–ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†Cascadiaï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè°ƒåº¦è¯·æ±‚è·¯ç”±å’Œéƒ¨ç½²æ¨¡å‹çº§è”çš„å¿«é€Ÿã€ä¿è´¨çš„LLMæœåŠ¡çš„æ–°å‹çº§è”æœåŠ¡æ¡†æ¶ã€‚Cascadiaé‡‡ç”¨ä¸¤çº§ä¼˜åŒ–æ–¹æ³•ï¼šåœ¨å†…å±‚ï¼Œå®ƒä½¿ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼ŒåŸºäºLLMä¿¡æ¯å’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹©èµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥ï¼›åœ¨å¤–å±‚ï¼Œå®ƒåº”ç”¨åŠ æƒTchebycheffç®—æ³•ï¼Œè¿­ä»£åœ°ååŒä¼˜åŒ–ç”±å†…éƒ¨çº§åˆ«äº§ç”Ÿçš„è·¯ç”±ç­–ç•¥å’Œç³»ç»Ÿéƒ¨ç½²ã€‚æˆ‘ä»¬å¯¹ä¸åŒçš„å·¥ä½œè´Ÿè½½è·Ÿè¸ªå’Œæ¨¡å‹çº§è”ï¼ˆDeepSeekå’ŒLlamaç³»åˆ—ï¼‰è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœè¡¨æ˜Cascadiaæ˜¾è‘—ä¼˜äºå•æ¨¡å‹éƒ¨ç½²å’Œæœ€æ–°çš„çº§è”æœåŠ¡åŸºå‡†æµ‹è¯•ï¼Œå®ç°äº†é«˜è¾¾4å€ï¼ˆå¹³å‡2.3å€ï¼‰çš„æ›´ç´§å»¶è¿ŸSLOå’Œé«˜è¾¾5å€ï¼ˆå¹³å‡2.4å€ï¼‰çš„æ›´é«˜ååé‡ï¼ŒåŒæ—¶ä¿æŒç›®æ ‡ç­”æ¡ˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04203v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åŠ å‰§äº†å¯¹å¿«é€Ÿå“åº”å’Œé«˜è´¨å›ç­”çš„éœ€æ±‚ã€‚æ›´å¼ºå¤§çš„æ¨¡å‹è™½ç„¶èƒ½äº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œä½†æ¨ç†å»¶è¿Ÿè¾ƒé«˜ï¼Œè€Œè¾ƒå°çš„æ¨¡å‹è™½ç„¶é€Ÿåº¦æ›´å¿«ï¼Œä½†èƒ½åŠ›æœ‰é™ã€‚ä¸ºå¹³è¡¡å»¶è¿Ÿä¸è´¨é‡çš„æƒè¡¡ï¼Œæå‡ºäº†ä½¿ç”¨æ¨¡å‹çº§è”çš„æ–¹æ³•ï¼Œå°†ç®€å•çš„æŸ¥è¯¢è·¯ç”±åˆ°è¾ƒå°çš„æ¨¡å‹ï¼Œå¤æ‚çš„æŸ¥è¯¢è·¯ç”±åˆ°è¾ƒå¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå®ç°é«˜æ•ˆçš„çº§è”æœåŠ¡ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚å½“å‰æ¡†æ¶ç¼ºä¹å¤„ç†LLMä¸åŒèµ„æºéœ€æ±‚çš„å·¨å¤§å·®å¼‚ã€LLMå·¥ä½œè´Ÿè½½çš„å†…åœ¨å¼‚è´¨æ€§å’Œç³»ç»Ÿéƒ¨ç½²ä¸è·¯ç”±ç­–ç•¥çš„ååŒä¼˜åŒ–çš„æœ‰æ•ˆæœºåˆ¶ã€‚æœ¬æ–‡ä»‹ç»Cascadiaï¼Œä¸€ç§ä¸“é—¨è®¾è®¡ç”¨äºè°ƒåº¦è¯·æ±‚è·¯ç”±å’Œéƒ¨ç½²æ¨¡å‹çº§è”çš„å¿«é€Ÿã€é«˜è´¨é‡ä¿ç•™çš„LLMæœåŠ¡çº§è”æ¡†æ¶ã€‚Cascadiaé‡‡ç”¨ä¸¤çº§ä¼˜åŒ–æ–¹æ³•ï¼šå†…éƒ¨ä½¿ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’æ ¹æ®LLMä¿¡æ¯å’Œå·¥ä½œè´Ÿè½½ç‰¹æ€§é€‰æ‹©èµ„æºåˆ†é…å’Œå¹¶è¡Œç­–ç•¥ï¼›å¤–éƒ¨é‡‡ç”¨åŠ æƒåˆ‡æ¯”é›ªå¤«ç®—æ³•è¿­ä»£ä¼˜åŒ–è·¯ç”±ç­–ç•¥å’Œå†…éƒ¨äº§ç”Ÿçš„ç³»ç»Ÿéƒ¨ç½²ã€‚åœ¨å¤šç§å·¥ä½œè´Ÿè½½è¿½è¸ªå’Œä¸åŒçš„æ¨¡å‹çº§è”ï¼ˆDeepSeekå’ŒLlamaç³»åˆ—ï¼‰ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒCascadiaæ˜¾è‘—ä¼˜äºå•æ¨¡å‹éƒ¨ç½²å’Œå½“å‰æœ€å…ˆè¿›çš„çº§è”æœåŠ¡åŸºçº¿ï¼Œåœ¨ç»´æŒç›®æ ‡ç­”æ¡ˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°å»¶è¿ŸSLOçš„4å€ï¼ˆå¹³å‡æé«˜2.3å€ï¼‰å’Œååé‡æé«˜5å€ï¼ˆå¹³å‡æé«˜2.4å€ï¼‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´å¿«é€Ÿå“åº”å’Œé«˜è´¨å›ç­”çš„éœ€æ±‚å¹³è¡¡é—®é¢˜ã€‚</li>
<li>æ¨¡å‹çº§è”æ˜¯è§£å†³å»¶è¿Ÿä¸è´¨é‡é—®é¢˜çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å½“å‰æ¡†æ¶åœ¨å¤„ç†LLMçš„èµ„æºéœ€æ±‚ã€å·¥ä½œè´Ÿè½½å¼‚è´¨æ€§å’Œç³»ç»Ÿéƒ¨ç½²ä¸è·¯ç”±ç­–ç•¥çš„ååŒä¼˜åŒ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Cascadiaæ¡†æ¶é€šè¿‡ä¸¤çº§ä¼˜åŒ–æ–¹æ³•è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°é«˜æ•ˆã€é«˜è´¨é‡çš„LLMæœåŠ¡ã€‚</li>
<li>Cascadiaé‡‡ç”¨æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’å’ŒåŠ æƒåˆ‡æ¯”é›ªå¤«ç®—æ³•è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>Cascadiaåœ¨å¤šç§å·¥ä½œè´Ÿè½½å’Œæ¨¡å‹çº§è”ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºå•æ¨¡å‹éƒ¨ç½²å’Œç°æœ‰åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d6209803dfb16b0c32700f7f605a6420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-374e38cb86bed8c62d13780d36e400cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6710574e883d22e435ac9ae3a8d2e7ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80b2ae9ee7a95c9dbc733bf335e34d73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-221d6563c6fffdb8e84b7fafee42aa05.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TracLLM-A-Generic-Framework-for-Attributing-Long-Context-LLMs"><a href="#TracLLM-A-Generic-Framework-for-Attributing-Long-Context-LLMs" class="headerlink" title="TracLLM: A Generic Framework for Attributing Long Context LLMs"></a>TracLLM: A Generic Framework for Attributing Long Context LLMs</h2><p><strong>Authors:Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia</strong></p>
<p>Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and&#x2F;or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble&#x2F;denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: <a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM">https://github.com/Wang-Yanting/TracLLM</a>. </p>
<blockquote>
<p>é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«éƒ¨ç½²åœ¨è®¸å¤šçœŸå®ä¸–ç•Œåº”ç”¨ä¸­ï¼Œå¦‚RAGã€æ™ºèƒ½ä»£ç†å’Œå¹¿æ³›çš„LLMé›†æˆåº”ç”¨ã€‚ç»™å®šä¸€ä¸ªæŒ‡ä»¤å’Œé•¿è¯­å¢ƒï¼ˆä¾‹å¦‚æ–‡æ¡£ã€PDFæ–‡ä»¶ã€ç½‘é¡µï¼‰ï¼Œé•¿è¯­å¢ƒLLMå¯ä»¥åŸºäºæä¾›çš„è¯­å¢ƒç”Ÿæˆè¾“å‡ºï¼Œæ—¨åœ¨æä¾›æ›´å‡†ç¡®ã€æœ€æ–°å’Œå¯éªŒè¯çš„è¾“å‡ºï¼ŒåŒæ—¶å‡å°‘å¹»è§‰å’Œæœªç»è¯å®çš„å£°æ˜ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ç¡®å®šè¯­å¢ƒä¸­çš„æ–‡æœ¬ï¼ˆä¾‹å¦‚å¥å­ã€æ®µè½ï¼‰å¯¹LLMç”Ÿæˆçš„è¾“å‡ºè´¡çŒ®æœ€å¤§æˆ–è´Ÿè´£ï¼Ÿæˆ‘ä»¬ç§°è¿™ä¸ªè¿‡ç¨‹ä¸ºä¸Šä¸‹æ–‡å›æº¯ï¼Œå…·æœ‰å„ç§çœŸå®ä¸–ç•Œåº”ç”¨ï¼Œä¾‹å¦‚1ï¼‰è°ƒè¯•åŸºäºLLMçš„ç³»ç»Ÿï¼Œ2ï¼‰å¯¹LLMè¿›è¡Œæ”»å‡»åçš„æ³•åŒ»åˆ†æï¼ˆä¾‹å¦‚æç¤ºæ³¨å…¥æ”»å‡»ã€çŸ¥è¯†è…è´¥æ”»å‡»ï¼‰ï¼Œä»¥åŠ3ï¼‰çªå‡ºæ˜¾ç¤ºçŸ¥è¯†æ¥æºï¼Œä»¥å¢å¼ºç”¨æˆ·å¯¹LLMç”Ÿæˆè¾“å‡ºçš„ä¿¡ä»»ã€‚å½“åº”ç”¨äºé•¿è¯­å¢ƒLLMçš„ä¸Šä¸‹æ–‡å›æº¯æ—¶ï¼Œç°æœ‰çš„ç‰¹å¾å½’å› æ–¹æ³•ï¼ˆå¦‚Shapleyï¼‰æ€§èƒ½ä¸ä½³ä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†TracLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹é•¿è¯­å¢ƒLLMçš„é€šç”¨ä¸Šä¸‹æ–‡å›æº¯æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æé«˜ç°æœ‰ç‰¹å¾å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬åœ¨TracLLMä¸­å¼€å‘äº†åŸºäºä¿¡æ¯æœç´¢çš„ç®—æ³•ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†è´¡çŒ®åˆ†æ•°é›†æˆ&#x2F;é™å™ªæŠ€æœ¯ï¼Œä»¥æé«˜TracLLMçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒTracLLMå¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«é•¿è¯­å¢ƒä¸­å¯¼è‡´LLMè¾“å‡ºçš„æ–‡æœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM%E3%80%82">https://github.com/Wang-Yanting/TracLLMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04202v1">PDF</a> To appear in USENIX Security Symposium 2025. The code and data are   at: <a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM">https://github.com/Wang-Yanting/TracLLM</a></p>
<p><strong>Summary</strong></p>
<p>é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºRAGã€æ™ºèƒ½ä»£ç†å’Œå¹¿æ³›çš„LLMé›†æˆåº”ç”¨ç­‰å®é™…åœºæ™¯ä¸­ã€‚é’ˆå¯¹LLMå¦‚ä½•å®šä½ç”Ÿæˆè¾“å‡ºä¸­è´¡çŒ®æœ€å¤§çš„æ–‡æœ¬ï¼ˆå¦‚å¥å­ã€æ®µè½ç­‰ï¼‰çš„é—®é¢˜ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºä¸Šä¸‹æ–‡å›æº¯ï¼Œå…·æœ‰é‡è¦çš„ç°å®æ„ä¹‰ï¼Œå¦‚è°ƒè¯•LLMç³»ç»Ÿã€è¿›è¡Œæ”»å‡»åçš„æ³•åŒ»å­¦åˆ†æä»¥åŠçªå‡ºçŸ¥è¯†æ¥æºä»¥å¢å¼ºç”¨æˆ·å¯¹LLMç”Ÿæˆè¾“å‡ºçš„ä¿¡ä»»ã€‚ç°æœ‰ç‰¹å¾å½’å› æ–¹æ³•å¦‚Shapleyåœ¨åº”ç”¨äºé•¿è¯­å¢ƒLLMçš„ä¸Šä¸‹æ–‡å›æº¯æ—¶æ•ˆæœä¸ç†æƒ³ä¸”è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹é•¿è¯­å¢ƒLLMçš„é€šç”¨ä¸Šä¸‹æ–‡å›æº¯æ¡†æ¶TracLLMï¼Œæ”¹è¿›äº†ç°æœ‰ç‰¹å¾å½’å› æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚é€šè¿‡ç®—æ³•ä¼˜åŒ–å’ŒæŠ€æœ¯æ”¹è¿›ï¼ŒTracLLMèƒ½å‡†ç¡®è¯†åˆ«å¯¼è‡´LLMè¾“å‡ºçš„æ–‡æœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså·²è¢«å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œå…·æœ‰å¼ºå¤§çš„è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨æä¾›é•¿è¯­å¢ƒçš„æƒ…å†µä¸‹ï¼ŒLLMsèƒ½å¤Ÿæä¾›æ›´å‡†ç¡®ã€å®æ—¶å’Œå¯éªŒè¯çš„è¾“å‡ºã€‚</li>
<li>â€œä¸Šä¸‹æ–‡å›æº¯â€æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨ç¡®å®šLLMç”Ÿæˆè¾“å‡ºæ‰€ä¾èµ–çš„æ–‡æœ¬å†…å®¹ã€‚</li>
<li>ä¸Šä¸‹æ–‡å›æº¯çš„åº”ç”¨åŒ…æ‹¬è°ƒè¯•LLMç³»ç»Ÿã€æ”»å‡»åçš„æ³•åŒ»å­¦åˆ†æå’Œå¢å¼ºç”¨æˆ·å¯¹LLMçš„ä¿¡ä»»ç­‰ã€‚</li>
<li>ç°æœ‰ç‰¹å¾å½’å› æ–¹æ³•åœ¨é•¿è¯­å¢ƒLLMçš„ä¸Šä¸‹æ–‡å›æº¯æ–¹é¢å­˜åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>æ–°å¼€å‘çš„TracLLMæ¡†æ¶æ—¨åœ¨æ”¹è¿›è¿™äº›æ–¹æ³•ï¼Œå¹¶é€šè¿‡ç®—æ³•ä¼˜åŒ–å’ŒæŠ€æœ¯æ”¹è¿›æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-682ca0f3225d5bcb356e76b050c5b746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f013b6f939e53ca40d5a688f2bf64e69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68045b6106fec5bfb9494f37f3309866.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="R-Search-Empowering-LLM-Reasoning-with-Search-via-Multi-Reward-Reinforcement-Learning"><a href="#R-Search-Empowering-LLM-Reasoning-with-Search-via-Multi-Reward-Reinforcement-Learning" class="headerlink" title="R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning"></a>R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning</h2><p><strong>Authors:Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu</strong></p>
<p>Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/QingFei1/R-Search">https://github.com/QingFei1/R-Search</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ­¥éª¤å’Œé•¿é“¾æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†å…¶æ¨ç†èƒ½åŠ›æ‰©å±•ä»¥æ¶µç›–ä¸æœç´¢çš„æ·±å±‚æ¬¡äº¤äº’ä»ç„¶æ˜¯ä¸€ä¸ªä¸å°çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹é€šå¸¸æ— æ³•è¯†åˆ«å‡ºæœ€ä½³çš„æ¨ç†-æœç´¢äº¤äº’è½¨è¿¹ï¼Œä»è€Œå¯¼è‡´å“åº”ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†R-Searchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ¨ç†-æœç´¢é›†æˆçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå…·æœ‰æ·±å±‚æœç´¢äº¤äº’çš„å¤šæ­¥éª¤æ¨ç†ï¼Œå¹¶é€šè¿‡å¤šå¥–åŠ±ä¿¡å·å­¦ä¹ æœ€ä½³çš„æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ï¼Œä»è€Œåœ¨å¤æ‚çš„é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­æé«˜å“åº”è´¨é‡ã€‚R-Searchå¼•å¯¼LLMåŠ¨æ€å†³å®šä½•æ—¶è¿›è¡Œæ£€ç´¢æˆ–æ¨ç†ï¼ŒåŒæ—¶å…¨å±€é›†æˆå…³é”®è¯æ®ï¼Œå¢å¼ºæ¨ç†å’Œæœç´¢ä¹‹é—´çš„æ·±å±‚çŸ¥è¯†äº¤äº’ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒR-Searchæä¾›å¤šé˜¶æ®µã€å¤šç±»å‹çš„å¥–åŠ±æ¥å…±åŒä¼˜åŒ–æ¨ç†-æœç´¢è½¨è¿¹ã€‚åœ¨7ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒR-Searchè¾ƒå…ˆè¿›çš„RAGåŸºå‡†æµ‹è¯•é«˜å‡º32.2%ï¼ˆé¢†åŸŸå†…ï¼‰å’Œ25.1%ï¼ˆè·¨é¢†åŸŸï¼‰ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/QingFei1/R-Search%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/QingFei1/R-Searchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04185v1">PDF</a> 16 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ­¥å’Œé•¿é“¾æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä¸æœç´¢çš„æ·±åº¦äº¤äº’æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æå‡ºR-Searchæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å®ç°æ¨ç†ä¸æœç´¢çš„é›†æˆï¼Œä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¤šæ­¥æ¨ç†å¹¶æ·±åº¦æœç´¢äº¤äº’ï¼Œé€šè¿‡å¤šå¥–åŠ±ä¿¡å·å­¦ä¹ æœ€ä¼˜æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ï¼Œæé«˜åœ¨å¤æ‚é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å“åº”è´¨é‡ã€‚R-Searchæ¡†æ¶å¯æŒ‡å¯¼LLMåŠ¨æ€å†³å®šä½•æ—¶æ£€ç´¢æˆ–æ¨ç†ï¼ŒåŒæ—¶å…¨å±€æ•´åˆå…³é”®è¯æ®ï¼Œå¢å¼ºæ¨ç†ä¸æœç´¢ä¹‹é—´çš„æ·±åº¦çŸ¥è¯†äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR-Searchåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆè¿›çš„RAGåŸºå‡†æµ‹è¯•ï¼Œæœ€é«˜æå‡ç‡è¾¾32.2%ï¼ˆé¢†åŸŸå†…ï¼‰å’Œ25.1%ï¼ˆè·¨é¢†åŸŸï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šæ­¥å’Œé•¿é“¾æ¨ç†ä¸Šæœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨ä¸æœç´¢çš„æ·±åº¦äº¤äº’æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>R-Searchæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ¨ç†ä¸æœç´¢çš„é›†æˆã€‚</li>
<li>R-Searchä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¤šæ­¥æ¨ç†ï¼Œå¹¶æ·±åº¦æœç´¢äº¤äº’ã€‚</li>
<li>é€šè¿‡å¤šå¥–åŠ±ä¿¡å·å­¦ä¹ æœ€ä¼˜æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ã€‚</li>
<li>R-Searchæé«˜äº†åœ¨å¤æ‚é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å“åº”è´¨é‡ã€‚</li>
<li>R-Searchæ¡†æ¶å¯åŠ¨æ€è°ƒæ•´æ£€ç´¢ä¸æ¨ç†çš„æ—¶æœºï¼Œå¹¶å…¨å±€æ•´åˆå…³é”®è¯æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cac3135fed19307e49f5a5ec1b29f3de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1494d44782cd1eec136119a2c80f523.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SuperWriter-Reflection-Driven-Long-Form-Generation-with-Large-Language-Models"><a href="#SuperWriter-Reflection-Driven-Long-Form-Generation-with-Large-Language-Models" class="headerlink" title="SuperWriter: Reflection-Driven Long-Form Generation with Large Language   Models"></a>SuperWriter: Reflection-Driven Long-Form Generation with Large Language   Models</h2><p><strong>Authors:Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee</strong></p>
<p>Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation. </p>
<blockquote>
<p>é•¿æ–‡æœ¬ç”Ÿæˆå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒè¿è´¯æ€§ã€ç¡®ä¿é€»è¾‘ä¸€è‡´æ€§å’Œéšç€åºåˆ—é•¿åº¦å¢åŠ è€Œä¿æŒæ–‡æœ¬è´¨é‡æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SuperWriter-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»£ç†çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚SuperWriter-Agentå°†æ˜ç¡®çš„ç»“æ„åŒ–æ€è€ƒå¼•å…¥ç”Ÿæˆç®¡é“ï¼Œé€šè¿‡è§„åˆ’å’Œç»†åŒ–é˜¶æ®µï¼Œå¼•å¯¼æ¨¡å‹éµå¾ªä¸€ä¸ªæ›´åŠ æ·±æ€ç†Ÿè™‘å’Œè®¤çŸ¥åŸºç¡€çš„è¿‡ç¨‹ï¼Œç±»ä¼¼äºä¸“ä¸šä½œå®¶çš„è¿‡ç¨‹ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼Œä»¥è®­ç»ƒä¸€ä¸ª7Bçš„SuperWriter-LMã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§åˆ†å±‚çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç¨‹åºï¼Œè¯¥ç¨‹åºä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¥ä¼ æ’­æœ€ç»ˆè´¨é‡è¯„ä¼°å¹¶ç›¸åº”åœ°ä¼˜åŒ–æ¯ä¸ªç”Ÿæˆæ­¥éª¤ã€‚åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒSuperWriter-LMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ä»…åœ¨è‡ªåŠ¨è¯„ä¼°ä¸­è€Œä¸”åœ¨äººç±»è¯„ä¼°ä¸­è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„åŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå…¨é¢çš„æ¶ˆèç ”ç©¶è¯æ˜äº†åˆ†å±‚DPOçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒäº†èå…¥ç»“æ„åŒ–æ€è€ƒæ­¥éª¤ä»¥æé«˜é•¿æ–‡æœ¬ç”Ÿæˆè´¨é‡çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚ä¿æŒè¿è´¯æ€§ã€é€»è¾‘ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚ä¸ºæ­¤ï¼Œæå‡ºSuperWriter-Agentæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥ç»“æ„åŒ–æ€è€ƒé˜¶æ®µï¼Œæé«˜é•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œè®­ç»ƒäº†7Bçš„SuperWriter-LMï¼Œå¹¶é‡‡ç”¨åˆ†å±‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œä¼˜åŒ–ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒSuperWriter-LMåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­è¶…è¶Šäº†æ›´å¤§è§„æ¨¡çš„åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è¿è´¯æ€§ã€é€»è¾‘ä¸€è‡´æ€§å’Œæ–‡æœ¬è´¨é‡ã€‚</li>
<li>SuperWriter-Agentæ¡†æ¶é€šè¿‡å¼•å…¥ç»“æ„åŒ–æ€è€ƒé˜¶æ®µï¼Œå¢å¼ºé•¿æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚</li>
<li>æ„å»ºäº†SuperWriter-LMï¼Œä½¿ç”¨åˆ†å±‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>SuperWriter-LMåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè¶…è¶Šå¤§è§„æ¨¡åŸºå‡†æ¨¡å‹ã€‚</li>
<li>å®è¯ç»“æœè¯æ˜äº†SuperWriter-LMçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>åˆ†å±‚DPOæ–¹æ³•å¯¹äºæé«˜æ–‡æœ¬ç”Ÿæˆè´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-350699db7802966cd283e482f66fc7e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd7017a98e2a4136b192be26d92132c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b8f79ea31089a19e91dfa2a3c48b50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca497570e81c183cfeeb3ae59ca42905.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SkipGPT-Dynamic-Layer-Pruning-Reinvented-with-Token-Awareness-and-Module-Decoupling"><a href="#SkipGPT-Dynamic-Layer-Pruning-Reinvented-with-Token-Awareness-and-Module-Decoupling" class="headerlink" title="SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and   Module Decoupling"></a>SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and   Module Decoupling</h2><p><strong>Authors:Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen</strong></p>
<p>Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens, and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% of model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/SkipGPT">https://github.com/EIT-NLP/SkipGPT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†ç”±äºå…¶æ·±å±‚ã€å¤šå±‚æ¶æ„ï¼Œäº§ç”Ÿäº†å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚å±‚å‰ªæä½œä¸ºä¸€ç§ç¼“è§£è¿™äº›ä½æ•ˆæ€§çš„ç­–ç•¥å·²ç»å‡ºç°ï¼Œä½†ä¼ ç»Ÿçš„é™æ€å‰ªææ–¹æ³•å¿½ç•¥äº†LLMæ¨ç†ä¸­å›ºæœ‰çš„ä¸¤ä¸ªå…³é”®åŠ¨æ€ï¼šï¼ˆ1ï¼‰æ°´å¹³åŠ¨æ€ï¼Œå…¶ä¸­ä»¤ç‰Œçº§åˆ«çš„å¼‚è´¨æ€§è¦æ±‚ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å‰ªæå†³ç­–ï¼›ï¼ˆ2ï¼‰å‚ç›´åŠ¨æ€ï¼Œå…¶ä¸­MLPå’Œè‡ªæˆ‘æ³¨æ„å±‚çš„ä¸åŒåŠŸèƒ½è§’è‰²éœ€è¦ç‰¹å®šçš„ç»„ä»¶å‰ªæç­–ç•¥ã€‚æˆ‘ä»¬å¼•å…¥äº†SkipGPTï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€å±‚å‰ªææ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°æ¥ä¼˜åŒ–è®¡ç®—èµ„æºåˆ†é…ï¼šï¼ˆ1ï¼‰å…¨å±€ä»¤ç‰Œæ„ŸçŸ¥è·¯ç”±ä»¥ä¼˜å…ˆå¤„ç†å…³é”®ä»¤ç‰Œï¼›ï¼ˆ2ï¼‰ä¸ºMLPå’Œè‡ªæˆ‘æ³¨æ„ç»„ä»¶æä¾›è§£è€¦çš„å‰ªæç­–ç•¥ã€‚ä¸ºäº†å‡è½»è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µä¼˜åŒ–èŒƒå¼ï¼šé¦–å…ˆæ˜¯ä¸€ä¸ªè§£è€¦çš„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡è½¯å‚æ•°åŒ–å­¦ä¹ è·¯ç”±ç­–ç•¥ï¼Œä»¥é¿å…è¿‡æ—©çš„å‰ªæå†³ç­–ï¼Œç„¶åæ˜¯å‚æ•°é«˜æ•ˆçš„LoRAå¾®è°ƒæ¥æ¢å¤å› å±‚ç§»é™¤è€Œå½±å“æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSkipGPTåœ¨å‡å°‘è¶…è¿‡40%çš„æ¨¡å‹å‚æ•°çš„åŒæ—¶ï¼Œåœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸åŸå§‹å¯†é›†æ¨¡å‹ç›¸åŒ¹é…æˆ–æ›´é«˜çš„æ€§èƒ½ã€‚é€šè¿‡åè°ƒåŠ¨æ€æ•ˆç‡ä¸ä¿ç•™çš„è¡¨ç°åŠ›ï¼ŒSkipGPTæ¨åŠ¨äº†å¯æ‰©å±•ã€èµ„æºæ„ŸçŸ¥çš„LLMçš„å®é™…éƒ¨ç½²ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/SkipGPT%E3%80%82">https://github.com/EIT-NLP/SkipGPTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04179v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¡ç®—èµ„æºæ¶ˆè€—æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå…¶æ·±åº¦å¤šå±‚æ¶æ„å¯¼è‡´é«˜è®¡ç®—æˆæœ¬ã€‚ä¸ºä¼˜åŒ–èµ„æºåˆ†é…ï¼Œæå‡ºSkipGPTåŠ¨æ€å±‚å‰ªææ¡†æ¶ï¼Œé€šè¿‡å…¨å±€ä»¤ç‰Œæ„ŸçŸ¥è·¯ç”±å’Œç‹¬ç«‹å‰ªæç­–ç•¥ï¼Œå®ç°èµ„æºä¼˜åŒ–åŒæ—¶ä¿æŒæ€§èƒ½ã€‚å¼•å…¥ä¸¤é˜¶æ®µä¼˜åŒ–èŒƒå¼ï¼Œå…ˆè¿›è¡Œéè€¦åˆè®­ç»ƒå­¦ä¹ è·¯ç”±ç­–ç•¥ï¼Œé¿å…è¿‡æ—©å‰ªæå†³ç­–ï¼Œå†é€šè¿‡LoRAå¾®è°ƒæ¢å¤æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒSkipGPTåœ¨å‡å°‘è¶…è¿‡40%æ¨¡å‹å‚æ•°çš„åŒæ—¶ï¼Œä¿æŒæˆ–æå‡æ€§èƒ½è¡¨ç°ã€‚å…¶åŠ¨æ€æ•ˆç‡ä¸è¡¨è¾¾èƒ½åŠ›çš„å¹³è¡¡æ¨åŠ¨äº†èµ„æºæ„ŸçŸ¥å‹LLMçš„å®é™…éƒ¨ç½²åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºå…¶æ·±åº¦å¤šå±‚æ¶æ„ã€‚</li>
<li>SkipGPTæ¡†æ¶æ—¨åœ¨é€šè¿‡åŠ¨æ€å±‚å‰ªææ¥ä¼˜åŒ–èµ„æºåˆ†é…ã€‚</li>
<li>SkipGPTé‡‡ç”¨å…¨å±€ä»¤ç‰Œæ„ŸçŸ¥è·¯ç”±å’Œç‹¬ç«‹å‰ªæç­–ç•¥è¿›è¡Œèµ„æºä¼˜åŒ–ã€‚</li>
<li>SkipGPTå¼•å…¥ä¸¤é˜¶æ®µä¼˜åŒ–èŒƒå¼ï¼Œå…ˆå­¦ä¹ è·¯ç”±ç­–ç•¥å†è¿›è¡Œå¾®è°ƒï¼Œé¿å…è¿‡æ—©çš„å‰ªæå†³ç­–å½±å“æ€§èƒ½ã€‚</li>
<li>SkipGPTèƒ½å¤Ÿåœ¨å‡å°‘è¶…è¿‡40%æ¨¡å‹å‚æ•°çš„åŒæ—¶ä¿æŒæˆ–æå‡æ€§èƒ½è¡¨ç°ã€‚</li>
<li>SkipGPTæ¡†æ¶æé«˜äº†LLMåœ¨å®é™…éƒ¨ç½²ä¸­çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42561de84e90518ee2e6d85a81b544b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05febfd1e40419f40495ae39c8176232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2b40fd5d0124af951aed5ff1d7673f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7282fec8de52af9e06e419a8e5a66663.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VISCA-Inferring-Component-Abstractions-for-Automated-End-to-End-Testing"><a href="#VISCA-Inferring-Component-Abstractions-for-Automated-End-to-End-Testing" class="headerlink" title="VISCA: Inferring Component Abstractions for Automated End-to-End Testing"></a>VISCA: Inferring Component Abstractions for Automated End-to-End Testing</h2><p><strong>Authors:Parsa Alian, Martin Tang, Ali Mesbah</strong></p>
<p>Providing optimal contextual input presents a significant challenge for automated end-to-end (E2E) test generation using large language models (LLMs), a limitation that current approaches inadequately address. This paper introduces Visual-Semantic Component Abstractor (VISCA), a novel method that transforms webpages into a hierarchical, semantically rich component abstraction. VISCA starts by partitioning webpages into candidate segments utilizing a novel heuristic-based segmentation method. These candidate segments subsequently undergo classification and contextual information extraction via multimodal LLM-driven analysis, facilitating their abstraction into a predefined vocabulary of user interface (UI) components. This component-centric abstraction offers a more effective contextual basis than prior approaches, enabling more accurate feature inference and robust E2E test case generation. Our evaluations demonstrate that the test cases generated by VISCA achieve an average feature coverage of 92%, exceeding the performance of the state-of-the-art LLM-based E2E test generation method by 16%. </p>
<blockquote>
<p>æä¾›æœ€ä½³ä¸Šä¸‹æ–‡è¾“å…¥å¯¹äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æµ‹è¯•ç”Ÿæˆæ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå½“å‰çš„æ–¹æ³•å¯¹æ­¤ç±»é—®é¢˜å¤„ç†ä¸è¶³ã€‚æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­ä¹‰ç»„ä»¶æŠ½è±¡ï¼ˆVISCAï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒå°†ç½‘é¡µè½¬åŒ–ä¸ºå±‚æ¬¡ä¸°å¯Œã€è¯­ä¹‰ä¸°å¯Œçš„ç»„ä»¶æŠ½è±¡ã€‚VISCAé¦–å…ˆåˆ©ç”¨åŸºäºå¯å‘å¼çš„æ–°æ–¹æ³•å°†ç½‘é¡µåˆ’åˆ†ä¸ºå€™é€‰æ®µã€‚è¿™äº›å€™é€‰æ®µéšåé€šè¿‡å¤šæ¨¡å¼LLMé©±åŠ¨çš„åˆ†æè¿›è¡Œåˆ†ç±»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æå–ï¼Œä¾¿äºå®ƒä»¬è¢«æŠ½è±¡ä¸ºé¢„å®šä¹‰çš„ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ç»„ä»¶è¯æ±‡ã€‚è¿™ç§ä»¥ç»„ä»¶ä¸ºä¸­å¿ƒçš„æŠ½è±¡æ–¹æ³•æä¾›äº†æ¯”ä»¥å‰çš„æ–¹æ³•æ›´æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡åŸºç¡€ï¼Œèƒ½å¤Ÿå®ç°æ›´å‡†ç¡®çš„åŠŸèƒ½æ¨æ–­å’Œç¨³å¥çš„ç«¯åˆ°ç«¯æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒVISCAç”Ÿæˆçš„æµ‹è¯•æ¡ˆä¾‹çš„å¹³å‡åŠŸèƒ½è¦†ç›–ç‡ä¸º92%ï¼Œè¶…è¿‡äº†æœ€æ–°LLMç«¯å¯¹ç«¯æµ‹è¯•ç”Ÿæˆæ–¹æ³•çš„æ€§èƒ½ï¼Œé«˜å‡º16%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04161v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨è§†è§‰è¯­ä¹‰ç»„ä»¶æŠ½è±¡å™¨ï¼ˆVISCAï¼‰å°†ç½‘é¡µè½¬åŒ–ä¸ºå±‚æ¬¡åŒ–ã€è¯­ä¹‰ä¸°å¯Œçš„ç»„ä»¶æŠ½è±¡çš„æ–¹æ³•ï¼Œè§£å†³äº†å½“å‰å¤§è¯­è¨€æ¨¡å‹åœ¨ç«¯åˆ°ç«¯æµ‹è¯•ç”Ÿæˆä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡åŸºäºå¯å‘å¼çš„åˆ†å‰²æ–¹æ³•å°†ç½‘é¡µåˆ’åˆ†ä¸ºå€™é€‰æ®µï¼Œå†é€šè¿‡å¤šæ¨¡æ€çš„è¯­è¨€æ¨¡å‹è¿›è¡Œåˆ†æå’Œåˆ†ç±»ï¼Œä»è€Œå®ç°ç²¾å‡†çš„ç‰¹å¾æ¨æ–­å’Œé²æ£’çš„ç«¯åˆ°ç«¯æµ‹è¯•æ¡ˆä¾‹ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒVISCAç”Ÿæˆçš„æµ‹è¯•æ¡ˆä¾‹å¹³å‡ç‰¹å¾è¦†ç›–ç‡è¾¾åˆ°92%ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VISCAé€šè¿‡å°†ç½‘é¡µè½¬åŒ–ä¸ºç»„ä»¶æŠ½è±¡æ¥è§£å†³å¤§è¯­è¨€æ¨¡å‹åœ¨ç«¯åˆ°ç«¯æµ‹è¯•ç”Ÿæˆä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºå¯å‘å¼çš„åˆ†å‰²æ–¹æ³•å°†ç½‘é¡µåˆ’åˆ†ä¸ºå€™é€‰æ®µã€‚</li>
<li>VISCAåˆ©ç”¨å¤šæ¨¡æ€çš„è¯­è¨€æ¨¡å‹å¯¹å€™é€‰æ®µè¿›è¡Œåˆ†ç±»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æå–ã€‚</li>
<li>ç»„ä»¶ä¸­å¿ƒçš„æŠ½è±¡æ–¹æ³•æä¾›äº†æ¯”å…ˆå‰æ–¹æ³•æ›´æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡åŸºç¡€ã€‚</li>
<li>VISCAç”Ÿæˆçš„æµ‹è¯•æ¡ˆä¾‹å®ç°äº†é«˜ç‰¹å¾è¦†ç›–ç‡ï¼Œå¹³å‡è¾¾åˆ°92%ã€‚</li>
<li>VISCAåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œæé«˜äº†ç«¯åˆ°ç«¯æµ‹è¯•ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aac14c3c318b522f1fb06736f332a5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc7fbe3b3574bf58b7e844802e24446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b975b4fc832be5b1f45659d794374ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ae4677787c642fdcb6a23c36e902983.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aaae30b3ad90644dd2a988fced4687a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6695769443305db9e6b95213c3d323bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f96c4428bdd149c7fa45296f581f659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd737c95ab75f953ed43961ced53136f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Dataset-for-Addressing-Patientâ€™s-Information-Needs-related-to-Clinical-Course-of-Hospitalization"><a href="#A-Dataset-for-Addressing-Patientâ€™s-Information-Needs-related-to-Clinical-Course-of-Hospitalization" class="headerlink" title="A Dataset for Addressing Patientâ€™s Information Needs related to Clinical   Course of Hospitalization"></a>A Dataset for Addressing Patientâ€™s Information Needs related to Clinical   Course of Hospitalization</h2><p><strong>Authors:Sarvesh Soni, Dina Demner-Fushman</strong></p>
<p>Patients have distinct information needs about their hospitalization that can be addressed using clinical evidence from electronic health records (EHRs). While artificial intelligence (AI) systems show promise in meeting these needs, robust datasets are needed to evaluate the factual accuracy and relevance of AI-generated responses. To our knowledge, no existing dataset captures patient information needs in the context of their EHRs. We introduce ArchEHR-QA, an expert-annotated dataset based on real-world patient cases from intensive care unit and emergency department settings. The cases comprise questions posed by patients to public health forums, clinician-interpreted counterparts, relevant clinical note excerpts with sentence-level relevance annotations, and clinician-authored answers. To establish benchmarks for grounded EHR question answering (QA), we evaluated three open-weight large language models (LLMs)â€“Llama 4, Llama 3, and Mixtralâ€“across three prompting strategies: generating (1) answers with citations to clinical note sentences, (2) answers before citations, and (3) answers from filtered citations. We assessed performance on two dimensions: Factuality (overlap between cited note sentences and ground truth) and Relevance (textual and semantic similarity between system and reference answers). The final dataset contains 134 patient cases. The answer-first prompting approach consistently performed best, with Llama 4 achieving the highest scores. Manual error analysis supported these findings and revealed common issues such as omitted key clinical evidence and contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong benchmark for developing and evaluating patient-centered EHR QA systems, underscoring the need for further progress toward generating factual and relevant responses in clinical contexts. </p>
<blockquote>
<p>æ‚£è€…å¯¹å…¶ä½é™¢è¿‡ç¨‹ä¸­çš„ä¿¡æ¯éœ€æ±‚ç‹¬ç‰¹ï¼Œå¯é€šè¿‡ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­çš„ä¸´åºŠè¯æ®æ¥æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚è™½ç„¶äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿåœ¨æ»¡è¶³è¿™äº›éœ€æ±‚æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†éœ€è¦å¯é çš„æ•°æ®é›†æ¥è¯„ä¼°AIç”Ÿæˆç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰æ²¡æœ‰æ•°æ®é›†èƒ½å¤Ÿæ•æ‰æ‚£è€…åœ¨ç”µå­å¥åº·è®°å½•èƒŒæ™¯ä¸‹çš„ä¿¡æ¯éœ€æ±‚ã€‚æˆ‘ä»¬ä»‹ç»äº†ArchEHR-QAæ•°æ®é›†ï¼Œå®ƒåŸºäºç°å®ä¸–ç•Œçš„æ‚£è€…æ¡ˆä¾‹ï¼Œè¿™äº›æ¡ˆä¾‹æ¥è‡ªé‡ç—‡ç›‘æŠ¤å®¤å’Œæ€¥è¯Šç§‘ã€‚è¿™äº›æ¡ˆä¾‹åŒ…æ‹¬æ‚£è€…å‘å…¬å…±å¥åº·è®ºå›æå‡ºçš„é—®é¢˜ã€åŒ»ç”Ÿè§£è¯»çš„ç›¸åº”é—®é¢˜ã€ç›¸å…³çš„ä¸´åºŠç¬”è®°æ‘˜å½•ä»¥åŠå¥å­çº§åˆ«çš„ç›¸å…³æ€§æ³¨é‡Šï¼Œä»¥åŠåŒ»ç”Ÿæ’°å†™çš„ç­”æ¡ˆã€‚ä¸ºäº†ä¸ºåŸºäºEHRçš„é—®ç­”ï¼ˆQAï¼‰å»ºç«‹åŸºå‡†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸‰ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰â€”â€”Llama 4ã€Llama 3å’ŒMixtralâ€”â€”è·¨è¶Šä¸‰ç§æç¤ºç­–ç•¥ï¼šç”Ÿæˆï¼ˆ1ï¼‰å¸¦æœ‰ä¸´åºŠç¬”è®°å¥å­çš„ç­”æ¡ˆå¼•ç”¨ã€ï¼ˆ2ï¼‰ç­”æ¡ˆå…ˆäºå¼•ç”¨å‡ºç°ï¼Œä»¥åŠï¼ˆ3ï¼‰ä»è¿‡æ»¤åçš„å¼•ç”¨ä¸­ç”Ÿæˆç­”æ¡ˆã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šäº‹å®æ€§ï¼ˆå¼•ç”¨çš„ç¬”è®°å¥å­ä¸çœŸå®ç­”æ¡ˆçš„é‡å ç¨‹åº¦ï¼‰å’Œç›¸å…³æ€§ï¼ˆç³»ç»Ÿä¸å‚è€ƒç­”æ¡ˆä¹‹é—´çš„æ–‡æœ¬å’Œè¯­ä¹‰ç›¸ä¼¼æ€§ï¼‰ã€‚æœ€ç»ˆæ•°æ®é›†åŒ…å«134ä¸ªæ‚£è€…æ¡ˆä¾‹ã€‚ç­”æ¡ˆä¼˜å…ˆçš„æç¤ºæ–¹æ³•è¡¨ç°æœ€ä¸ºç¨³å®šï¼ŒLlama 4å–å¾—æœ€é«˜åˆ†æ•°ã€‚æ‰‹åŠ¨è¯¯å·®åˆ†ææ”¯æŒäº†è¿™äº›å‘ç°ï¼Œå¹¶æ­ç¤ºäº†å¸¸è§çš„é—®é¢˜ï¼Œå¦‚é—æ¼çš„å…³é”®ä¸´åºŠè¯æ®å’ŒçŸ›ç›¾æˆ–è™šæ„çš„å†…å®¹ã€‚æ€»ä½“è€Œè¨€ï¼ŒArchEHR-QAä¸ºå¼€å‘å’Œè¯„ä¼°ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„EHR QAç³»ç»Ÿæä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œå¼ºè°ƒäº†éœ€è¦åœ¨ä¸´åºŠç¯å¢ƒä¸­ç”Ÿæˆäº‹å®å’Œç›¸å…³çš„ç­”æ¡ˆæ–¹é¢å–å¾—è¿›ä¸€æ­¥è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04156v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰çš„ä¸´åºŠè¯æ®ï¼Œæ‚£è€…å¯¹å…¶ä½é™¢è¿‡ç¨‹æœ‰ä¸åŒçš„ä¿¡æ¯éœ€æ±‚ã€‚è™½ç„¶äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿåœ¨æ»¡è¶³è¿™äº›éœ€æ±‚æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†éœ€è¦å¯é çš„æ•°æ®é›†æ¥è¯„ä¼°AIç”Ÿæˆå›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚ç›®å‰å°šæ— æ•°æ®é›†èƒ½å¤Ÿæ•æ‰æ‚£è€…ä¿¡æ¯éœ€æ±‚ä¸EHRsçš„å…³è”ã€‚æœ¬ç ”ç©¶æ¨å‡ºArchEHR-QAæ•°æ®é›†ï¼ŒåŸºäºçœŸå®æ‚£è€…ç—…ä¾‹ï¼Œæ¶µç›–é‡ç—‡ç›‘æŠ¤å®¤å’Œæ€¥è¯Šå®¤çš„åœºæ™¯ã€‚åŒ…æ‹¬æ‚£è€…å‘å…¬å…±å¥åº·è®ºå›æå‡ºçš„é—®é¢˜ã€åŒ»ç”Ÿè§£è¯»çš„é—®é¢˜ã€ç›¸å…³ä¸´åºŠç¬”è®°æ‘˜å½•ä»¥åŠåŒ»ç”Ÿæ’°å†™çš„ç­”æ¡ˆã€‚ä¸ºäº†å»ºç«‹åŸºäºEHRçš„é—®ç­”åŸºå‡†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå‘ç°ç­”æ¡ˆä¼˜å…ˆçš„æç¤ºç­–ç•¥æ•ˆæœæœ€ä½³ï¼Œå…¶ä¸­Llama 4è¡¨ç°æœ€ä¼˜ã€‚ArchEHR-QAä¸ºå¼€å‘è¯„ä¼°ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„EHRé—®ç­”ç³»ç»Ÿæä¾›äº†å¼ºæœ‰åŠ›çš„åŸºå‡†ï¼Œçªæ˜¾äº†éœ€è¦åœ¨ä¸´åºŠç¯å¢ƒä¸­ç”ŸæˆçœŸå®å’Œç›¸å…³çš„å›åº”çš„è¿›æ­¥éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‚£è€…å¯¹å…¶ä½é™¢è¿‡ç¨‹æœ‰ç‹¬ç‰¹çš„ä¿¡æ¯éœ€æ±‚ï¼Œå¯é€šè¿‡ç”µå­å¥åº·è®°å½•ä¸­çš„ä¸´åºŠè¯æ®æ¥æ»¡è¶³è¿™äº›éœ€æ±‚ã€‚</li>
<li>ArchEHR-QAæ˜¯ä¸€ä¸ªåŸºäºçœŸå®æ‚£è€…ç—…ä¾‹çš„ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ï¼Œæ¶µç›–äº†é‡ç—‡ç›‘æŠ¤å®¤å’Œæ€¥è¯Šå®¤çš„åœºæ™¯ã€‚</li>
<li>ArchEHR-QAåŒ…å«æ‚£è€…é—®é¢˜ã€åŒ»ç”Ÿè§£è¯»çš„é—®é¢˜ã€ä¸´åºŠç¬”è®°çš„æ‘˜å½•å’ŒåŒ»ç”Ÿç­”æ¡ˆã€‚</li>
<li>ä¸‰ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ArchEHR-QAæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå‘ç°ç­”æ¡ˆä¼˜å…ˆçš„æç¤ºç­–ç•¥è¡¨ç°æœ€ä½³ã€‚</li>
<li>Llama 4åœ¨è¯„ä¼°ä¸­è¡¨ç°æœ€ä¼˜ã€‚</li>
<li>æ‰‹åŠ¨è¯¯å·®åˆ†ææ”¯æŒäº†è¯„ä¼°ç»“æœï¼Œå¹¶æ­ç¤ºäº†å¸¸è§çš„é—®é¢˜ï¼Œå¦‚é—æ¼å…³é”®ä¸´åºŠè¯æ®å’Œå­˜åœ¨çŸ›ç›¾æˆ–è™šæ„çš„å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1231f45a9bf67f05b7e55ea34f905fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bad273e2400fc9542dabaff1a4d5544.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Establishing-Trustworthy-LLM-Evaluation-via-Shortcut-Neuron-Analysis"><a href="#Establishing-Trustworthy-LLM-Evaluation-via-Shortcut-Neuron-Analysis" class="headerlink" title="Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis"></a>Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis</h2><p><strong>Authors:Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao</strong></p>
<p>The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient ($\rho$) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: <a target="_blank" rel="noopener" href="https://github.com/GaryStack/Trustworthy-Evaluation">https://github.com/GaryStack/Trustworthy-Evaluation</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ä¾èµ–äºå¯ä¿¡çš„è¯„ä¼°ã€‚ç„¶è€Œï¼Œå½“å‰å¤§å¤šæ•°è¯„ä¼°éƒ½ä¾èµ–äºå…¬å…±åŸºå‡†æµ‹è¯•ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“é—®é¢˜çš„å½±å“ï¼Œä»è€Œä¸¥é‡æŸå®³è¯„ä¼°çš„å…¬æ­£æ€§ã€‚ä¹‹å‰çš„ç ”ç©¶è‡´åŠ›äºæ„å»ºåŠ¨æ€åŸºå‡†æµ‹è¯•æ¥è§£å†³æ±¡æŸ“é—®é¢˜ã€‚ç„¶è€Œï¼Œä¸æ–­æ„å»ºæ–°çš„åŸºå‡†æµ‹è¯•æˆæœ¬é«˜æ˜‚ä¸”å¾ªç¯å¾€å¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡åˆ†æå—æ±¡æŸ“æ¨¡å‹æœ¬èº«çš„æœºåˆ¶æ¥è§£å†³æ±¡æŸ“é—®é¢˜ã€‚é€šè¿‡æˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°å—æ±¡æŸ“æ¨¡å‹çš„è¿‡åº¦ä¼°è®¡å¯èƒ½æ˜¯ç”±äºæ¨¡å‹å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—äº†æ·å¾„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é€šè¿‡æ¯”è¾ƒå’Œå› æœåˆ†ææ¥è¯†åˆ«æ·å¾„ç¥ç»å…ƒçš„æ–°æ–¹æ³•ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºâ€œæ·å¾„ç¥ç»å…ƒä¿®è¡¥â€çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥æŠ‘åˆ¶æ·å¾„ç¥ç»å…ƒã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨å‡è½»æ±¡æŸ“æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è¯„ä¼°ç»“æœä¸æœ€è¿‘å‘å¸ƒçš„å¯ä¿¡åŸºå‡†æµ‹è¯•MixEvalè¡¨ç°å‡ºå¼ºçƒˆçš„çº¿æ€§ç›¸å…³æ€§ï¼Œæ–¯çš®å°”æ›¼ç³»æ•°ï¼ˆÏï¼‰è¶…è¿‡0.95ã€‚è¿™ç§é«˜ç›¸å…³æ€§è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç´§å¯†æ­ç¤ºæ¨¡å‹çš„çœŸå®èƒ½åŠ›ï¼Œæ˜¯å€¼å¾—ä¿¡èµ–çš„ã€‚æˆ‘ä»¬è¿›è¡Œäº†è¿›ä¸€æ­¥çš„å®éªŒï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•å’Œè¶…å‚æ•°è®¾ç½®ä¸­çš„é€šç”¨æ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/GaryStack/Trustworthy-Evaluation">https://github.com/GaryStack/Trustworthy-Evaluation</a></p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆ</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04142v1">PDF</a> Accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ä¾èµ–äºå¯é çš„è¯„ä¼°ã€‚å½“å‰å¤§å¤šæ•°è¯„ä¼°ä¾èµ–äºå…¬å…±åŸºå‡†æµ‹è¯•ï¼Œä½†å­˜åœ¨æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå½±å“å…¬å¹³æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡åˆ†æå—æ±¡æŸ“çš„æ¨¡å‹æœºåˆ¶æ¥è§£å†³æ±¡æŸ“é—®é¢˜ã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹æ±¡æŸ“çš„å¯èƒ½åŸå› æ˜¯å‚æ•°åœ¨è®­ç»ƒä¸­è·å¾—äº†æ·å¾„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡æ¯”è¾ƒå’Œå› æœåˆ†ææ¥è¯†åˆ«æ·å¾„ç¥ç»å…ƒçš„æ–°æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åä¸ºâ€œæ·å¾„ç¥ç»å…ƒä¿®è¡¥â€çš„è¯„ä¼°æ–¹æ³•æ¥æŠ‘åˆ¶æ·å¾„ç¥ç»å…ƒã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸æœ€æ–°å‘å¸ƒçš„å¯é åŸºå‡†æµ‹è¯•MixEvalå‘ˆå¼ºçº¿æ€§ç›¸å…³ï¼Œæ–¯çš®å°”æ›¼ç³»æ•°ï¼ˆÏï¼‰è¶…è¿‡0.95ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°éœ€è¦å¯é çš„æ–¹æ³•ã€‚</li>
<li>å…¬å…±åŸºå‡†æµ‹è¯•å­˜åœ¨æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå½±å“æ¨¡å‹è¯„ä¼°çš„å…¬å¹³æ€§ã€‚</li>
<li>å—æ±¡æŸ“çš„æ¨¡å‹å¯èƒ½å› å‚æ•°åœ¨è®­ç»ƒä¸­è·å–æ·å¾„è§£å†³æ–¹æ¡ˆè€Œå¯¼è‡´ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡æ¯”è¾ƒå’Œå› æœåˆ†æè¯†åˆ«æ·å¾„ç¥ç»å…ƒçš„æ–°æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºâ€œæ·å¾„ç¥ç»å…ƒä¿®è¡¥â€çš„è¯„ä¼°æ–¹æ³•æ¥æŠ‘åˆ¶æ·å¾„ç¥ç»å…ƒã€‚</li>
<li>å®éªŒéªŒè¯è¯¥æ–¹æ³•æœ‰æ•ˆï¼Œä¸MixEvalåŸºå‡†æµ‹è¯•å‘ˆå¼ºçº¿æ€§ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-daf88fefe69548e676a88c127aad6169.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2280c6915e427c4ebd5155151dcfa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c5ac3ec6550ccf929e9a6ecfd3bb04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e08be93dad3173221089712d620eb572.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MMR-V-Whatâ€™s-Left-Unsaid-A-Benchmark-for-Multimodal-Deep-Reasoning-in-Videos"><a href="#MMR-V-Whatâ€™s-Left-Unsaid-A-Benchmark-for-Multimodal-Deep-Reasoning-in-Videos" class="headerlink" title="MMR-V: Whatâ€™s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos"></a>MMR-V: Whatâ€™s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos</h2><p><strong>Authors:Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</strong></p>
<p>The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as â€œquestion frameâ€) and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities. </p>
<blockquote>
<p>è§†é¢‘çš„è¿ç»­ç»“æ„å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å®šä½å¤šå¸§è¯æ®å’Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºç†è§£ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡åªéœ€è¦æ¨¡å‹åŒ¹é…é—®é¢˜ä¸­æåˆ°çš„å¸§ï¼ˆä»¥ä¸‹ç®€ç§°â€œé—®é¢˜å¸§â€ï¼‰å¹¶æ„ŸçŸ¥å°‘æ•°ç›¸é‚»å¸§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MMR-Vï¼šè§†é¢‘å¤šæ¨¡æ€æ·±åº¦æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šï¼ˆ1ï¼‰é•¿ç¨‹å¤šå¸§æ¨ç†ï¼šæ¨¡å‹éœ€è¦æ¨æ–­å’Œåˆ†æå¯èƒ½è¿œç¦»é—®é¢˜å¸§çš„è¯æ®å¸§ã€‚ï¼ˆ2ï¼‰è¶…è¶Šæ„ŸçŸ¥ï¼šé—®é¢˜ä¸èƒ½ä»…é€šè¿‡ç›´æ¥æ„ŸçŸ¥æ¥å›ç­”ï¼Œè€Œæ˜¯éœ€è¦æ¨ç†éšè—çš„ä¿¡æ¯ã€‚ï¼ˆ3ï¼‰å¯é æ€§ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯æ‰‹åŠ¨æ³¨é‡Šçš„ï¼Œå‚è€ƒç°å®ä¸–ç•Œä¸­ç”¨æˆ·çš„å¹¿æ³›ç†è§£ï¼Œä»¥ç¬¦åˆæ™®éè®¤çŸ¥ã€‚ï¼ˆ4ï¼‰æ··æ·†æ€§ï¼šç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°æ³¨é‡Šç­–ç•¥ï¼Œä»¥å‡å°‘æ¨¡å‹çš„æ·å¾„ã€‚MMR-VåŒ…å«317ä¸ªè§†é¢‘å’Œ1,257ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼›å³ä½¿è¡¨ç°æœ€ä½³çš„o4-miniæ¨¡å‹ä¹Ÿä»…è¾¾åˆ°52.5%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ¨ç†å¢å¼ºç­–ç•¥ï¼ˆå¦‚æ€ç»´é“¾å’Œæµ‹è¯•æ—¶é—´è®¡ç®—ï¼‰å¸¦æ¥çš„æ”¶ç›Šæœ‰é™ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨ç†æ‰€éœ€çš„æ€ç»´é“¾ä¸æ–‡æœ¬æ¨ç†ä¸åŒï¼Œè¿™éƒ¨åˆ†è§£é‡Šäº†æ€§èƒ½æå‡æœ‰é™çš„åŸå› ã€‚æˆ‘ä»¬å¸Œæœ›MMR-Vèƒ½å¤Ÿæ¿€å‘å¯¹å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04141v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mmr-v.github.io/">https://mmr-v.github.io</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘åºåˆ—ç»“æ„å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®šä½å¤šå¸§è¯æ®å’Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢çš„èƒ½åŠ›æ„æˆæŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç†è§£ä»»åŠ¡ï¼Œä»…è¦æ±‚æ¨¡å‹åŒ¹é…é—®é¢˜ä¸­æåˆ°çš„å¸§ï¼ˆä»¥ä¸‹ç®€ç§°â€œé—®é¢˜å¸§â€ï¼‰å¹¶æ„ŸçŸ¥å°‘æ•°ç›¸é‚»å¸§ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºMMR-Vï¼šè§†é¢‘å¤šæ¨¡æ€æ·±åº¦æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é•¿ç¨‹å¤šå¸§æ¨ç†ï¼šæ¨¡å‹éœ€æ¨æ–­å’Œåˆ†æå¯èƒ½ä¸é—®é¢˜å¸§ç›¸è·è¾ƒè¿œçš„è¯æ®å¸§ã€‚ï¼ˆ2ï¼‰è¶…è¶Šæ„ŸçŸ¥ï¼šé—®é¢˜ä¸èƒ½ä»…é€šè¿‡ç›´æ¥æ„ŸçŸ¥æ¥å›ç­”ï¼Œè€Œéœ€è¦æ¨ç†éšè—ä¿¡æ¯ã€‚ï¼ˆ3ï¼‰å¯é æ€§ï¼šæ‰€æœ‰ä»»åŠ¡å‡ç»äººå·¥æ ‡æ³¨ï¼Œå‚ç…§å¹¿æ³›çš„å®é™…ç”¨æˆ·ç†è§£ï¼Œä»¥ç¬¦åˆé€šç”¨æ„ŸçŸ¥ã€‚ï¼ˆ4ï¼‰æ··æ·†æ€§ï¼šç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°è€…æ ‡æ³¨ç­–ç•¥ï¼Œä»¥å‡å°‘æ¨¡å‹æ·å¾„ã€‚MMR-VåŒ…å«317ä¸ªè§†é¢‘å’Œ1,257ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼›å³ä½¿è¡¨ç°æœ€ä½³çš„o4-miniæ¨¡å‹ï¼Œå‡†ç¡®ç‡ä¹Ÿä»…ä¸º52.5%ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ¨ç†å¢å¼ºç­–ç•¥ï¼ˆå¦‚æ€ç»´é“¾å’Œæµ‹è¯•æ—¶é—´è®¡ç®—ï¼‰å¸¦æ¥çš„æ”¶ç›Šæœ‰é™ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨ç†æ‰€éœ€çš„æ€ç»´é“¾ä¸æ–‡æœ¬æ¨ç†ä¸åŒï¼Œè¿™éƒ¨åˆ†è§£é‡Šäº†æ€§èƒ½æå‡æœ‰é™çš„åŸå› ã€‚æˆ‘ä»¬å¸Œæœ›MMR-Vèƒ½æ¿€å‘å¯¹å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘çš„åºåˆ—ç»“æ„å¯¹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ„æˆæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®šä½å¤šå¸§è¯æ®æ–¹é¢ã€‚</li>
<li>ç°æœ‰è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç†è§£ä»»åŠ¡ï¼Œå¿½ç•¥äº†é•¿ç¨‹å¤šå¸§æ¨ç†å’Œéšè—ä¿¡æ¯çš„æ¨ç†ã€‚</li>
<li>MMR-VåŸºå‡†æµ‹è¯•å¼¥è¡¥äº†è¿™ä¸€å·®è·ï¼Œè¦æ±‚æ¨¡å‹è¿›è¡Œé•¿ç¨‹å¤šå¸§æ¨ç†å’Œå¤æ‚çš„æ€ç»´é“¾ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨MMR-Vä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå³ä½¿æœ€å¥½çš„æ¨¡å‹å‡†ç¡®ç‡ä¹Ÿä»…ä¸º52.5%ã€‚</li>
<li>å½“å‰çš„æ¨ç†å¢å¼ºç­–ç•¥å¯¹å¤šæ¨¡æ€æ¨ç†çš„æ€§èƒ½æå‡æœ‰é™ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†ä¸æ–‡æœ¬æ¨ç†æ‰€éœ€çš„æ€ç»´é“¾æœ‰æ‰€ä¸åŒã€‚</li>
<li>MMR-VåŸºå‡†æµ‹è¯•æœ‰æœ›æ¿€å‘å¯¹å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9605454fe68ec6c42c13d1f54b4ad59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ac2b73c1ca05ab4310aeb96cdfc18c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cec1625179effab0b31a947c215ef67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-940181c5078ad798e878d4220edd6951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ce8dd5a60eadcd568f4fd39d77ad9c1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TRiSM-for-Agentic-AI-A-Review-of-Trust-Risk-and-Security-Management-in-LLM-based-Agentic-Multi-Agent-Systems"><a href="#TRiSM-for-Agentic-AI-A-Review-of-Trust-Risk-and-Security-Management-in-LLM-based-Agentic-Multi-Agent-Systems" class="headerlink" title="TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management   in LLM-based Agentic Multi-Agent Systems"></a>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management   in LLM-based Agentic Multi-Agent Systems</h2><p><strong>Authors:Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis</strong></p>
<p>Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy&#x2F;security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºçš„Agentic AIç³»ç»Ÿï¼Œåœ¨å¤šæ™ºèƒ½ä½“é…ç½®ä¸­éƒ¨ç½²ï¼Œæ­£åœ¨é‡æ–°å®šä¹‰ä¼ä¸šå’Œç¤¾ä¼šé¢†åŸŸä¸­çš„æ™ºèƒ½è‡ªä¸»æ€§ã€åä½œå’Œå†³ç­–ã€‚æœ¬æ–‡ä»‹ç»äº†åœ¨LLMåŸºäºçš„æ™ºèƒ½ä½“å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆAMASï¼‰èƒŒæ™¯ä¸‹å¯¹ä¿¡ä»»ã€é£é™©å’Œå®‰å…¨ç®¡ç†ï¼ˆTRiSMï¼‰çš„ç»“æ„æ€§åˆ†æã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶Agentic AIçš„æ¦‚å¿µåŸºç¡€ï¼Œä¸ä¼ ç»ŸAIä»£ç†çš„æ¶æ„å·®å¼‚ï¼Œä»¥åŠèƒ½å¤Ÿå®ç°å¯æ‰©å±•çš„å·¥å…·ä½¿ç”¨è‡ªä¸»æ€§çš„æ–°å…´ç³»ç»Ÿè®¾è®¡ã€‚ç„¶ååœ¨Agentic AIæ¡†æ¶ä¸­è¯¦ç»†è¯´æ˜äº†TRiSMçš„å››ä¸ªæ”¯æŸ±ï¼šæ²»ç†ã€å¯è§£é‡Šæ€§ã€ModelOpsä»¥åŠéšç§&#x2F;å®‰å…¨ï¼Œæ¯ä¸ªæ”¯æŸ±éƒ½é’ˆå¯¹Agentic LLMè¿›è¡Œäº†ä¸Šä¸‹æ–‡åŒ–ã€‚æˆ‘ä»¬ç¡®å®šäº†ç‹¬ç‰¹çš„å¨èƒå‘é‡ï¼Œå¹¶ä»‹ç»äº†ç”±æ¡ˆä¾‹ç ”ç©¶æ”¯æŒçš„agentic AIåº”ç”¨ç¨‹åºçš„ç»¼åˆé£é™©åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è°ƒæŸ¥äº†ä¿¡ä»»å»ºç«‹æœºåˆ¶ã€é€æ˜åº¦å’Œç›‘ç£æŠ€æœ¯ï¼Œä»¥åŠåˆ†å¸ƒå¼LLMä»£ç†ç³»ç»Ÿä¸­çš„æœ€æ–°å¯è§£é‡Šæ€§ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†è¯„ä¼°ä¿¡ä»»ã€è§£é‡Šèƒ½åŠ›å’Œä»¥äººä¸ºä¸­å¿ƒæ€§èƒ½çš„æŒ‡æ ‡ï¼Œä»¥åŠå…¬å¼€åŸºå‡†æµ‹è¯•æŒ‘æˆ˜ã€‚é€šè¿‡åŠ å¯†ã€å¯¹æŠ—æ€§é˜²å¾¡ä»¥åŠéµå®ˆä¸æ–­å‘å±•çš„AIæ³•è§„æ¥è§£å†³å®‰å…¨å’Œéšç§é—®é¢˜ã€‚æœ¬æ–‡æœ€åæå‡ºäº†è´Ÿè´£ä»»çš„Agentic AIè·¯çº¿å›¾ï¼Œæå‡ºäº†æ–°å…´çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸ç¨³å¥çš„TRiSMåŸåˆ™å¯¹é½çš„ç ”ç©¶æ–¹å‘ï¼Œä»¥å®ç°å®‰å…¨ã€è´Ÿè´£å’Œé€æ˜çš„éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04133v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºçš„Agentic AIç³»ç»Ÿï¼Œåœ¨å¤šæ™ºèƒ½ä½“é…ç½®ä¸­çš„éƒ¨ç½²æ­£åœ¨é‡æ–°å®šä¹‰ä¼ä¸šå’Œç¤¾ä¼šé¢†åŸŸçš„æ™ºèƒ½è‡ªä¸»æ€§ã€åä½œå’Œå†³ç­–ã€‚æœ¬æ–‡ä»‹ç»äº†åœ¨LLMåŸºç¡€ä¸Šæ„å»ºçš„æ™ºèƒ½å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆAMASï¼‰ä¸­çš„ä¿¡ä»»ã€é£é™©å’Œå®‰å…¨ç®¡ç†çš„ç»“æ„åŒ–åˆ†æã€‚æ–‡ç« é¦–å…ˆæ¢è®¨äº†Agentic AIçš„æ¦‚å¿µåŸºç¡€ï¼Œä¸ä¼ ç»ŸAIæ™ºèƒ½ä½“çš„æ¶æ„å·®å¼‚ï¼Œä»¥åŠæ–°å…´çš„ç³»ç»Ÿè®¾è®¡å¦‚ä½•èµ‹èƒ½å¯ä¼¸ç¼©çš„å·¥å…·ä½¿ç”¨è‡ªä¸»æ€§ã€‚æ¥ç€è¯¦ç»†é˜è¿°äº†Agentic AIæ¡†æ¶ä¸­çš„TRiSMï¼ŒåŒ…æ‹¬æ²»ç†ã€å¯è§£é‡Šæ€§ã€ModelOpsä»¥åŠéšç§&#x2F;å®‰å…¨ç­‰å››ä¸ªæ”¯æŸ±ï¼Œå¹¶é’ˆå¯¹agentic LLMè¿›è¡Œä¸Šä¸‹æ–‡åˆ†æã€‚æ–‡ç« è¯†åˆ«äº†ç‹¬ç‰¹çš„å¨èƒå‘é‡ï¼Œä¸ºagentic AIåº”ç”¨ç¨‹åºå¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„é£é™©åˆ†ç±»ï¼Œå¹¶é€šè¿‡æ¡ˆä¾‹ç ”ç©¶è¯´æ˜äº†ç°å®ä¸–ç•Œçš„æ¼æ´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è°ƒæŸ¥äº†æ„å»ºä¿¡ä»»æœºåˆ¶ã€é€æ˜åº¦å’Œç›‘ç£æŠ€æœ¯ï¼Œä»¥åŠåˆ†å¸ƒå¼LLMæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„æœ€æ–°å¯è§£é‡Šæ€§ç­–ç•¥ã€‚åŒæ—¶ï¼Œè¯„ä¼°ä¿¡ä»»ã€è§£é‡Šæ€§å’Œä»¥äººä¸ºä¸­å¿ƒæ€§èƒ½çš„æŒ‡æ ‡ä¸å…¬å¼€åŸºå‡†æµ‹è¯•æŒ‘æˆ˜ä¹Ÿè¢«å®¡æŸ¥ã€‚å®‰å…¨å’Œéšç§é€šè¿‡åŠ å¯†ã€å¯¹æŠ—æ€§é˜²å¾¡å’Œéµå®ˆä¸æ–­å‘å±•çš„AIæ³•è§„æ¥è§£å†³ã€‚æœ¬æ–‡æœ€åæå‡ºäº†è´Ÿè´£ä»»çš„Agentic AIè·¯çº¿å›¾ï¼Œä¸ºæ–°å…´çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸ç¨³å¥çš„TRiSMåŸåˆ™å¯¹é½ï¼Œä»¥å®ç°å®‰å…¨ã€è´Ÿè´£å’Œé€æ˜çš„éƒ¨ç½²æä¾›äº†ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Agentic AIç³»ç»Ÿæ­£æ”¹å˜ä¼ä¸šå’Œç¤¾ä¼šé¢†åŸŸçš„æ™ºèƒ½è‡ªä¸»æ€§ã€åä½œå’Œå†³ç­–æ–¹å¼ã€‚</li>
<li>TRiSMåœ¨LLMåŸºç¡€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆAMASï¼‰ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>Agentic AIå…·æœ‰ç‹¬ç‰¹çš„å¨èƒå‘é‡å’Œé£é™©åˆ†ç±»ï¼Œéœ€è¦é‡è§†ç°å®ä¸–ç•Œçš„æ¼æ´å’Œå®‰å…¨é—®é¢˜ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†æ„å»ºä¿¡ä»»æœºåˆ¶ã€é€æ˜åº¦å’Œç›‘ç£æŠ€æœ¯çš„æ–¹æ³•ã€‚</li>
<li>è¯„ä¼°å’Œè¡¡é‡ä¿¡ä»»ã€è§£é‡Šæ€§å’Œä»¥äººä¸ºä¸­å¿ƒæ€§èƒ½çš„æŒ‡æ ‡å—åˆ°å…³æ³¨ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†åŠ å¯†ã€å¯¹æŠ—æ€§é˜²å¾¡ä»¥åŠéµå®ˆAIæ³•è§„åœ¨è§£å†³éšç§å’Œå®‰å…¨é—®é¢˜ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7178f1471c5e5966789601f9f3024519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a75b4b184e72b7e02c601b3415e8725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2c071e7d7cf409fb5dd95114b364c72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7558c023dfbba1b26c85107c8e9bd6f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Guided-Speculative-Inference-for-Efficient-Test-Time-Alignment-of-LLMs"><a href="#Guided-Speculative-Inference-for-Efficient-Test-Time-Alignment-of-LLMs" class="headerlink" title="Guided Speculative Inference for Efficient Test-Time Alignment of LLMs"></a>Guided Speculative Inference for Efficient Test-Time Alignment of LLMs</h2><p><strong>Authors:Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis</strong></p>
<p>We propose Guided Speculative Inference (GSI), a novel algorithm for efficient reward-guided decoding in large language models. GSI combines soft best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We derive a theoretical bound on the KL divergence between our induced distribution and the optimal policy. In experiments on reasoning benchmarks (MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative decoding (Liao et al., 2025), and in certain settings even outperforms soft best-of-$n$ with $\pi_B$. The code is available at <a target="_blank" rel="noopener" href="https://github.com/j-geuter/GSI">https://github.com/j-geuter/GSI</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å¼•å¯¼å¼æ¨æµ‹æ¨ç†ï¼ˆGSIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é«˜æ•ˆå¥–åŠ±å¼•å¯¼è§£ç çš„æ–°å‹ç®—æ³•ã€‚GSIç»“åˆäº†è½¯æ€§æœ€ä½³næµ‹è¯•æ—¶é—´ç¼©æ”¾ã€å¥–åŠ±æ¨¡å‹r(xï¼Œy)ä»¥åŠæ¥è‡ªå°å‹è¾…åŠ©æ¨¡å‹Ï€s(y|x)çš„æ¨æµ‹æ ·æœ¬ã€‚æˆ‘ä»¬åœ¨ä¸»è¦æ¨¡å‹Ï€Bä¸‹ï¼Œé€šè¿‡è½¯æ€§æœ€ä½³nçš„å€¾æ–œç­–ç•¥Ï€Î²ï¼ŒB(y|x)â‰ˆÏ€B(y|x)exp(Î²r(xï¼Œy))è¿›è¡Œäº†è¯æ˜ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºäº†è¯±å¯¼åˆ†å¸ƒä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´KLæ•£åº¦çš„ç†è®ºç•Œé™ã€‚åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMATH500ã€OlympiadBenchã€Minerva Mathï¼‰çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ¯”æ ‡å‡†è½¯æ€§æœ€ä½³nä¸Ï€så’Œå¥–åŠ±å¼•å¯¼æ¨æµ‹è§£ç ï¼ˆLiaoç­‰äººï¼Œ2025ï¼‰æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†è½¯æ€§æœ€ä½³nä¸Ï€Bçš„è¡¨ç°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/j-geuter/GSI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/j-geuter/GSIæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04118v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºå¼•å¯¼å¼æ¨æµ‹æ¨æ–­ï¼ˆGSIï¼‰çš„æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±å¯¼å‘è§£ç è¿›è¡Œé«˜æ•ˆåŒ–å¤„ç†ã€‚é€šè¿‡è½¯æ€§æœ€ä¼˜è§£è¯•éªŒæ—¶é—´å’Œå¥–åŠ±æ¨¡å‹çš„å¼•å¯¼ç»“åˆä½¿ç”¨è¾…åŠ©æ¨¡å‹æ¥æ¨¡æ‹Ÿå¯¹æ¨¡å‹çš„å‡è®¾æ ·æœ¬ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿé€¼è¿‘æœ€ä¼˜å€¾æ–œç­–ç•¥ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†å¼•å¯¼å¼æ¨æµ‹æ¨æ–­ï¼ˆGSIï¼‰ç®—æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å¥–åŠ±å¯¼å‘è§£ç æ•ˆç‡ã€‚<br>*GSIç»“åˆäº†è½¯æ€§æœ€ä¼˜è§£è¯•éªŒæ—¶é—´ç¼©æ”¾ã€å¥–åŠ±æ¨¡å‹r(x,y)å’Œæ¥è‡ªå°å‹è¾…åŠ©æ¨¡å‹çš„æ¨æµ‹æ ·æœ¬Ï€S(yâˆ£x)ã€‚<br>*GSIèƒ½å¤Ÿé€¼è¿‘æœ€ä¼˜å€¾æ–œç­–ç•¥Ï€Î²,B(yâˆ£x)ï¼Œè¯¥ç­–ç•¥æ˜¯è½¯æ€§æœ€ä¼˜è§£åœ¨ä¸»å¯¼æ¨¡å‹Ï€Bä¸‹çš„äº§ç‰©ã€‚<br>*æä¾›äº†ä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦çš„ç†è®ºç•Œé™ã€‚<br>*åœ¨åŸºå‡†æµ‹è¯•MATH500ã€OlympiadBenchå’ŒMinerva Mathä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œä¸æ ‡å‡†è½¯æ€§æœ€ä¼˜è§£åŠå¥–åŠ±å¼•å¯¼æ¨æµ‹è§£ç ç›¸æ¯”ï¼ŒGSIæ–¹æ³•çš„å‡†ç¡®ç‡æ›´é«˜ã€‚<br>*åœ¨æŸäº›è®¾ç½®ä¸­ï¼ŒGSIç”šè‡³è¶…è¶Šäº†ä½¿ç”¨Ï€Bçš„è½¯æ€§æœ€ä¼˜è§£çš„è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c903355e0ef97ab170bc1cf8e0622b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1321812e159a8a4a109cbc818566cbd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c178d2bade050355024047b3464ba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7913d25280da196b467d9a3af3421035.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rectified-Sparse-Attention"><a href="#Rectified-Sparse-Attention" class="headerlink" title="Rectified Sparse Attention"></a>Rectified Sparse Attention</h2><p><strong>Authors:Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei</strong></p>
<p>Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at <a target="_blank" rel="noopener" href="https://aka.ms/ReSA-LM">https://aka.ms/ReSA-LM</a>. </p>
<blockquote>
<p>é«˜æ•ˆé•¿åºåˆ—ç”Ÿæˆå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘çš„ç¨€ç–è§£ç æ–¹æ³•æé«˜äº†æ•ˆç‡ï¼Œä½†å®ƒä»¬å­˜åœ¨é”®å€¼ç¼“å­˜ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå…¶ä¸­è¿‘ä¼¼è¯¯å·®ä¼šç´¯ç§¯å¹¶é™ä½ç”Ÿæˆè´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œä¿®æ­£ç¨€ç–æ³¨æ„åŠ›â€ï¼ˆReSAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®ƒå°†å—ç¨€ç–æ³¨æ„åŠ›ä¸å‘¨æœŸæ€§çš„å¯†é›†ä¿®æ­£ç›¸ç»“åˆã€‚é€šè¿‡å®šæœŸåœ¨å¯†é›†å‰å‘ä¼ é€’ä¸­åˆ·æ–°é”®å€¼ç¼“å­˜ï¼ŒReSAé™åˆ¶è¯¯å·®ç´¯ç§¯å¹¶ä¿æŒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„åŒ¹é…ã€‚åœ¨æ¶µç›–æ•°å­¦æ¨ç†ã€è¯­è¨€å»ºæ¨¡å’Œæ£€ç´¢ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒReSAå±•ç¤ºäº†åœ¨æ˜¾è‘—æé«˜æ•ˆç‡çš„åŒæ—¶å®ç°è¿‘ä¹æ— æŸçš„ç”Ÿæˆè´¨é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è§£ç 256Kåºåˆ—é•¿åº¦çš„æƒ…å†µä¸‹ï¼ŒReSAæä¾›é«˜è¾¾2.42å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿï¼Œä½¿å…¶æˆä¸ºå¯æ‰©å±•çš„é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„å®é™…è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://aka.ms/ReSA-LM%E8%8E%B7%E5%8F%96%E3%80%82">https://aka.ms/ReSA-LMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Rectified Sparse Attentionï¼ˆReSAï¼‰æ–¹æ³•ï¼Œç»“åˆäº†å—ç¨€ç–æ³¨æ„åŠ›ä¸å‘¨æœŸæ€§çš„å¯†é›†ä¿®æ­£ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚é€šè¿‡å›ºå®šé—´éš”ä½¿ç”¨å¯†é›†å‰å‘ä¼ æ’­åˆ·æ–°KVç¼“å­˜ï¼ŒReSAæ§åˆ¶è¯¯å·®ç´¯ç§¯å¹¶ä¿æŒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒReSAåœ¨ä¿æŒè¿‘ä¹æ— æŸçš„ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£ç é•¿è¾¾256Kåºåˆ—é•¿åº¦æ—¶ï¼Œå®ç°äº†é«˜è¾¾2.42å€ç«¯åˆ°ç«¯çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReSAæ–¹æ³•ç»“åˆäº†å—ç¨€ç–æ³¨æ„åŠ›ä¸å‘¨æœŸæ€§çš„å¯†é›†ä¿®æ­£ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>ReSAé€šè¿‡åˆ·æ–°KVç¼“å­˜æ¥æ§åˆ¶è¯¯å·®ç´¯ç§¯ï¼Œå¹¶ä¿æŒä¸é¢„è®­ç»ƒåˆ†å¸ƒçš„å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒReSAåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†è¿‘ä¹æ— æŸçš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ReSAæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿åºåˆ—æ—¶ã€‚</li>
<li>ReSAæ–¹æ³•å®ç°äº†ç«¯åˆ°ç«¯çš„åŠ é€Ÿï¼Œæœ€é«˜å¯è¾¾2.42å€ã€‚</li>
<li>ä»£ç å·²ç»å…¬å¼€å¯ç”¨ï¼Œæ–¹ä¾¿ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f14f9f544f438beb7014945975ebe25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444310ccb2c11c069bc95e23501ceaee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae63ec6753fc6a9fc1b03c3d2f8ce5a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-221c6edf0bb7eba7c687fba805adff58.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AmbiK-Dataset-of-Ambiguous-Tasks-in-Kitchen-Environment"><a href="#AmbiK-Dataset-of-Ambiguous-Tasks-in-Kitchen-Environment" class="headerlink" title="AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment"></a>AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment</h2><p><strong>Authors:Anastasiia Ivanova, Eva Bakaeva, Zoya Volovikova, Alexey K. Kovalev, Aleksandr I. Panov</strong></p>
<p>As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at <a target="_blank" rel="noopener" href="https://github.com/cog-model/AmbiK-dataset">https://github.com/cog-model/AmbiK-dataset</a>. </p>
<blockquote>
<p>ä½œä¸ºå®ä½“ä»£ç†çš„ä¸€éƒ¨åˆ†ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”¨äºè¡Œä¸ºè§„åˆ’ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®ç¯å¢ƒä¸­å¤„ç†æ¨¡ç³Šçš„æŒ‡ä»¤ä»ç„¶æ˜¯LLMé¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚å·²ç»æå‡ºäº†å„ç§ä»»åŠ¡æ¨¡ç³Šæ€§æ£€æµ‹æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œç”±äºå®ƒä»¬åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œä¸”æ²¡æœ‰é€šç”¨çš„åŸºå‡†ï¼Œå› æ­¤å¾ˆéš¾å¯¹å®ƒä»¬è¿›è¡Œæ¯”è¾ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AmbiKï¼ˆå¨æˆ¿ç¯å¢ƒä¸­æ¨¡ç³Šä»»åŠ¡ï¼‰ï¼Œè¿™æ˜¯é’ˆå¯¹å¨æˆ¿ç¯å¢ƒä¸­æœºå™¨äººæ‰€æ¥æ”¶çš„æ¨¡ç³ŠæŒ‡ä»¤çš„å®Œå…¨æ–‡æœ¬æ•°æ®é›†ã€‚AmbiKçš„æ”¶é›†æ˜¯åœ¨LLMçš„å¸®åŠ©ä¸‹å®Œæˆçš„ï¼Œå¹¶ä¸”ç»è¿‡äº†äººå·¥éªŒè¯ã€‚å®ƒåŒ…å«1000å¯¹æ¨¡ç³Šä»»åŠ¡åŠå…¶æ˜ç¡®çš„å¯¹åº”ä»»åŠ¡ï¼ŒæŒ‰æ¨¡ç³Šç±»å‹ï¼ˆäººç±»åå¥½ã€å¸¸è¯†çŸ¥è¯†ã€å®‰å…¨ï¼‰åˆ†ç±»ï¼ŒåŒ…æ‹¬ç¯å¢ƒæè¿°ã€æ¾„æ¸…é—®é¢˜å’Œç­”æ¡ˆã€ç”¨æˆ·æ„å›¾å’Œä»»åŠ¡è®¡åˆ’ï¼Œæ€»å…±2000ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›AmbiKèƒ½å¤Ÿè®©ç ”ç©¶äººå‘˜å¯¹æ¨¡ç³Šæ£€æµ‹æ–¹æ³•è¿›è¡Œç»Ÿä¸€çš„æ¯”è¾ƒã€‚AmbiKå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cog-model/AmbiK-dataset%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/cog-model/AmbiK-datasetè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04089v1">PDF</a> ACL 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç”¨æˆ·è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ—¶çš„è¡Œä¸ºè§„åˆ’éƒ¨åˆ†å‘æŒ¥é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œå¤„ç†çœŸå®ç¯å¢ƒä¸­çš„æ¨¡ç³ŠæŒ‡ä»¤ä»æ˜¯LLMé¢ä¸´çš„æŒ‘æˆ˜ã€‚å°½ç®¡å·²æå‡ºå¤šç§ä»»åŠ¡æ¨¡ç³Šæ€§æ£€æµ‹æ–¹æ³•ï¼Œä½†ç”±äºæµ‹è¯•æ•°æ®é›†ä¸åŒï¼Œç¼ºä¹é€šç”¨åŸºå‡†ï¼Œéš¾ä»¥è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºAmbiKæ•°æ®é›†â€”â€”å¨æˆ¿ç¯å¢ƒä¸­é’ˆå¯¹æœºå™¨äººçš„æ¨¡ç³ŠæŒ‡ä»¤çš„å®Œå…¨æ–‡æœ¬æ•°æ®é›†ã€‚AmbiKåˆ©ç”¨LLMååŠ©æ”¶é›†ï¼Œç»è¿‡äººç±»éªŒè¯ã€‚å®ƒåŒ…å«1000å¯¹æ¨¡ç³Šä»»åŠ¡åŠå…¶æ˜ç¡®çš„å¯¹åº”ä»»åŠ¡ï¼ŒæŒ‰æ¨¡ç³Šç±»å‹ï¼ˆäººç±»åå¥½ã€å¸¸è¯†çŸ¥è¯†ã€å®‰å…¨ï¼‰åˆ†ç±»ï¼ŒåŒ…å«ç¯å¢ƒæè¿°ã€æ¾„æ¸…é—®é¢˜åŠç­”æ¡ˆã€ç”¨æˆ·æ„å›¾å’Œä»»åŠ¡è®¡åˆ’ï¼Œæ€»å…±2000ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬æœŸæœ›AmbiKèƒ½å¤Ÿä¿ƒè¿›ç ”ç©¶è€…å¯¹æ¨¡ç³Šæ£€æµ‹æ–¹æ³•è¿›è¡Œç»Ÿä¸€æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç”¨æˆ·è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ—¶ç”¨äºè¡Œä¸ºè§„åˆ’ã€‚</li>
<li>å¤„ç†çœŸå®ç¯å¢ƒä¸­çš„æ¨¡ç³ŠæŒ‡ä»¤æ˜¯LLMé¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šç§ä»»åŠ¡æ¨¡ç³Šæ€§æ£€æµ‹æ–¹æ³•å·²è¢«æå‡ºï¼Œä½†ç¼ºä¹é€šç”¨åŸºå‡†ï¼Œéš¾ä»¥æ¯”è¾ƒã€‚</li>
<li>æ¨å‡ºAmbiKæ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹æœºå™¨äººçš„å¨æˆ¿ç¯å¢ƒä¸­çš„æ¨¡ç³ŠæŒ‡ä»¤çš„å®Œå…¨æ–‡æœ¬æ•°æ®ã€‚</li>
<li>AmbiKæ•°æ®é›†åˆ©ç”¨LLMååŠ©æ”¶é›†ï¼Œå¹¶ç»äººç±»éªŒè¯ã€‚</li>
<li>AmbiKåŒ…å«1000å¯¹æ¨¡ç³Šä»»åŠ¡åŠå…¶æ˜ç¡®çš„å¯¹åº”ä»»åŠ¡ï¼Œåˆ†ç±»åŒ…å«ç¯å¢ƒæè¿°ã€æ¾„æ¸…é—®é¢˜ç­”æ¡ˆç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a74200f453c7699db14558ea5a550de8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42e2996993027f7c8a6ef9db309a2100.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a31350f4c215f4b9c58b77565320e9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8398847576ef8856222fb4d3352968d5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VisCoder-Fine-Tuning-LLMs-for-Executable-Python-Visualization-Code-Generation"><a href="#VisCoder-Fine-Tuning-LLMs-for-Executable-Python-Visualization-Code-Generation" class="headerlink" title="VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation"></a>VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation</h2><p><strong>Authors:Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen</strong></p>
<p>Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç»˜å›¾ã€åˆ¶ä½œå›¾è¡¨ç­‰å¯è§†åŒ–ä»»åŠ¡æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™äº›ä»»åŠ¡çš„æˆåŠŸå–å†³äºä»£ç æ­£ç¡®æ€§å’Œè§†è§‰è¯­ä¹‰ä¸¤æ–¹é¢ã€‚ç°æœ‰çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç¼ºä¹æ‰§è¡ŒåŸºç¡€çš„ç›‘ç£ï¼Œå¯¹è¿­ä»£ä»£ç ä¿®æ­£çš„æ”¯æŒæœ‰é™ï¼Œå¯¼è‡´ç»˜å›¾ç”Ÿæˆè„†å¼±ä¸”ä¸å¯é ã€‚æˆ‘ä»¬æ¨å‡ºäº†VisCode-200Kï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºPythonå¯è§†åŒ–åŠè‡ªæˆ‘ä¿®æ­£çš„å¤§å‹æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚å®ƒåŒ…å«è¶…è¿‡20ä¸‡ä¸ªæ¥è‡ªä¸¤ä¸ªæ¥æºçš„ç¤ºä¾‹ï¼š1ï¼‰æ¥è‡ªå¼€æºå­˜å‚¨åº“çš„ç»è¿‡éªŒè¯çš„ç»˜å›¾ä»£ç ï¼Œä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œæ¸²æŸ“å›¾é…å¯¹ï¼›2ï¼‰æ¥è‡ªCode-Feedbackçš„4.5ä¸‡ä¸ªå¤šè½®ä¿®æ­£å¯¹è¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨è¿è¡Œæ—¶åé¦ˆä¿®æ­£é”™è¯¯ä»£ç ã€‚æˆ‘ä»¬å¯¹Qwencod v2.5è¿›è¡Œå¾®è°ƒä»¥åœ¨VisCode-200Kä¸Šåˆ›å»ºVisCoderï¼Œå¹¶åœ¨PandasPlotBenchä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚VisCoderæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å¼€æºåŸºçº¿ï¼Œå¹¶æ¥è¿‘ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4o miniï¼‰çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨è‡ªæˆ‘è°ƒè¯•è¯„ä¼°åè®®æ¥è¯„ä¼°è¿­ä»£ä¿®å¤ï¼Œè¯æ˜äº†åé¦ˆé©±åŠ¨å­¦ä¹ å¯¹äºå¯æ‰§è¡Œä¸”è§†è§‰å‡†ç¡®çš„ä»£ç ç”Ÿæˆçš„ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨å¤„ç†å¯è§†åŒ–ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ç»˜å›¾å’Œå›¾è¡¨ç”Ÿæˆç­‰ã€‚ç°æœ‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç¼ºä¹æ‰§è¡Œç›‘ç£ï¼Œå¯¹è¿­ä»£ä»£ç ä¿®æ­£çš„æ”¯æŒæœ‰é™ï¼Œå¯¼è‡´ç»˜å›¾ç”Ÿæˆè„†å¼±ä¸”ä¸å¯é ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VisCode-200Kæ•°æ®é›†ï¼Œç”¨äºPythonå¯è§†åŒ–æŒ‡ä»¤è°ƒæ•´å’Œè‡ªæˆ‘ä¿®æ­£ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªæ¥æºåŒ…å«è¶…è¿‡20ä¸‡ä¸ªç¤ºä¾‹ï¼šä¸€æ˜¯æ¥è‡ªå¼€æºä»“åº“çš„éªŒè¯ç»˜å›¾ä»£ç ï¼Œä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œæ¸²æŸ“çš„å›¾è¡¨é…å¯¹ï¼›äºŒæ˜¯æ¥è‡ªCode-Feedbackçš„4.5ä¸‡ä¸ªå¤šè½®ä¿®æ­£å¯¹è¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨è¿è¡Œæ—¶åé¦ˆä¿®æ­£é”™è¯¯ä»£ç ã€‚æˆ‘ä»¬å¯¹Qwen2.5-Coder-Instructè¿›è¡Œå¾®è°ƒï¼Œåˆ›å»ºVisCoderå¹¶å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚VisCoderæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å¼€æºåŸºå‡†æµ‹è¯•ï¼Œå¹¶æ¥è¿‘ä¸“æœ‰æ¨¡å‹å¦‚GPT-4o-miniçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨è‡ªæˆ‘è°ƒè¯•è¯„ä¼°åè®®æ¥è¯„ä¼°è¿­ä»£ä¿®å¤ï¼Œè¯æ˜åé¦ˆé©±åŠ¨å­¦ä¹ åœ¨å¯è§†åŒ–ç¼–ç¨‹ä¸­çš„é‡è¦æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†å¯è§†åŒ–ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡®ä¿ä»£ç æ­£ç¡®æ€§å’Œè§†è§‰è¯­ä¹‰æ–¹é¢ã€‚</li>
<li>ç°æœ‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ç¼ºä¹æ‰§è¡Œç›‘ç£å’Œå¯¹è¿­ä»£ä»£ç ä¿®æ­£çš„æ”¯æŒã€‚</li>
<li>VisCode-200Kæ•°æ®é›†åŒ…å«è¶…è¿‡20ä¸‡ä¸ªç”¨äºPythonå¯è§†åŒ–ä»»åŠ¡çš„ç¤ºä¾‹ã€‚</li>
<li>VisCode-200Kç”±éªŒè¯çš„ç»˜å›¾ä»£ç ã€è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œæ¸²æŸ“çš„å›¾è¡¨ç»„æˆï¼Œä»¥åŠæ¥è‡ªCode-Feedbackçš„å¤šè½®ä¿®æ­£å¯¹è¯ã€‚</li>
<li>VisCoderé€šè¿‡å¾®è°ƒQwen2.5-Coder-Instructåˆ›å»ºï¼Œæ˜¾è‘—ä¼˜äºå¼€æºåŸºå‡†æµ‹è¯•å¹¶æ¥è¿‘ä¸“æœ‰æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è‡ªæˆ‘è°ƒè¯•è¯„ä¼°åè®®è¯æ˜åé¦ˆé©±åŠ¨å­¦ä¹ åœ¨å¯è§†åŒ–ç¼–ç¨‹ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41df43db3f46645a371ed857d83f9c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-365c28382105a20af39bd9e319e37893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f871b0dcd8e40ab08fa83386ec85f318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7fa4cb14244cf882e1ff00903583ef5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="STELLA-Towards-Protein-Function-Prediction-with-Multimodal-LLMs-Integrating-Sequence-Structure-Representations"><a href="#STELLA-Towards-Protein-Function-Prediction-with-Multimodal-LLMs-Integrating-Sequence-Structure-Representations" class="headerlink" title="STELLA: Towards Protein Function Prediction with Multimodal LLMs   Integrating Sequence-Structure Representations"></a>STELLA: Towards Protein Function Prediction with Multimodal LLMs   Integrating Sequence-Structure Representations</h2><p><strong>Authors:Hongwang Xiao, Wenjun Lin, Xi Chen, Hui Wang, Kai Chen, Jiashan Li, Yuancheng Sun, Sicheng Dai, Boya Wu, Qiwei Ye</strong></p>
<p>Protein biology focuses on the intricate relationships among sequences, structures, and functions. Deciphering protein functions is crucial for understanding biological processes, advancing drug discovery, and enabling synthetic biology applications. Since protein sequences determine tertiary structures, which in turn govern functions, integrating sequence and structure information is essential for accurate prediction of protein functions. Traditional protein language models (pLMs) have advanced protein-related tasks by learning representations from large-scale sequence and structure data. However, pLMs are limited in integrating broader contextual knowledge, particularly regarding functional modalities that are fundamental to protein biology. In contrast, large language models (LLMs) have exhibited outstanding performance in contextual understanding, reasoning, and generation across diverse domains. Leveraging these capabilities, STELLA is proposed as a multimodal LLM integrating protein sequence-structure representations with general knowledge to address protein function prediction. Through multimodal instruction tuning (MMIT) using the proposed OPI-Struc dataset, STELLA achieves state-of-the-art performance in two function-related tasks-functional description prediction (FP) and enzyme-catalyzed reaction prediction (EP). This study highlights the potential of multimodal LLMs as an alternative paradigm to pLMs to advance protein biology research. </p>
<blockquote>
<p>è›‹ç™½è´¨ç”Ÿç‰©å­¦ä¸“æ³¨äºåºåˆ—ã€ç»“æ„å’ŒåŠŸèƒ½ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚è§£æè›‹ç™½è´¨åŠŸèƒ½æ˜¯ç†è§£ç”Ÿç‰©è¿‡ç¨‹ã€æ¨åŠ¨è¯ç‰©å‘ç°å’Œå®ç°åˆæˆç”Ÿç‰©å­¦åº”ç”¨çš„å…³é”®ã€‚ç”±äºè›‹ç™½è´¨åºåˆ—å†³å®šä¸‰çº§ç»“æ„ï¼Œè€Œä¸‰çº§ç»“æ„åˆæ§åˆ¶åŠŸèƒ½ï¼Œå› æ­¤æ•´åˆåºåˆ—å’Œç»“æ„ä¿¡æ¯å¯¹äºå‡†ç¡®é¢„æµ‹è›‹ç™½è´¨åŠŸèƒ½è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰å·²ç»é€šè¿‡ä»å¤§è§„æ¨¡åºåˆ—å’Œç»“æ„æ•°æ®ä¸­å­¦ä¹ è¡¨å¾æ¥æ¨è¿›ä¸è›‹ç™½è´¨ç›¸å…³çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒpLMsåœ¨æ•´åˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è›‹ç™½è´¨ç”Ÿç‰©å­¦ä¸­è‡³å…³é‡è¦çš„åŠŸèƒ½æ¨¡å¼æ–¹é¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ¨ç†å’Œè·¨åŸŸç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚åˆ©ç”¨è¿™äº›èƒ½åŠ›ï¼ŒSTELLAè¢«æå‡ºä¸ºä¸€ç§å¤šæ¨¡å¼LLMï¼Œå®ƒæ•´åˆè›‹ç™½è´¨åºåˆ—-ç»“æ„è¡¨å¾å’Œé€šç”¨çŸ¥è¯†æ¥è§£å†³è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æå‡ºçš„OPI-Strucæ•°æ®é›†è¿›è¡Œå¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ˆMMITï¼‰ï¼ŒSTELLAåœ¨ä¸¤ä¸ªä¸åŠŸèƒ½ç›¸å…³çš„ä»»åŠ¡â€”â€”åŠŸèƒ½æè¿°é¢„æµ‹ï¼ˆFPï¼‰å’Œé…¶å‚¬åŒ–ååº”é¢„æµ‹ï¼ˆEPï¼‰ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†å¤šæ¨¡å¼LLMä½œä¸ºæ¨è¿›è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶çš„æ›¿ä»£èŒƒå¼çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶åºåˆ—ã€ç»“æ„ä¸åŠŸèƒ½ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚è§£æè›‹ç™½è´¨åŠŸèƒ½å¯¹äºç†è§£ç”Ÿç‰©è¿‡ç¨‹ã€æ¨è¿›è¯ç‰©å‘ç°å’Œå®ç°åˆæˆç”Ÿç‰©å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰é€šè¿‡ä»å¤§è§„æ¨¡åºåˆ—å’Œç»“æ„æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤ºæ¥æ¨è¿›è›‹ç™½è´¨ç›¸å…³ä»»åŠ¡ï¼Œä½†éš¾ä»¥æ•´åˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯è›‹ç™½è´¨ç”Ÿç‰©å­¦ä¸­çš„åŸºæœ¬åŠŸèƒ½æ¨¡å¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨è¿™äº›èƒ½åŠ›çš„å¤šæ¨¡å¼LLM STELLAï¼Œé€šè¿‡æ•´åˆè›‹ç™½è´¨åºåˆ—-ç»“æ„è¡¨ç¤ºå’Œé€šç”¨çŸ¥è¯†æ¥é¢„æµ‹è›‹ç™½è´¨åŠŸèƒ½ã€‚ä½¿ç”¨æ‰€æå‡ºçš„OPI-Strucæ•°æ®é›†è¿›è¡Œå¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ˆMMITï¼‰ï¼ŒSTELLAåœ¨åŠŸèƒ½æè¿°é¢„æµ‹å’Œé…¶å‚¬åŒ–ååº”é¢„æµ‹ä¸¤ä¸ªä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶çªå‡ºäº†å¤šæ¨¡å¼LLMä½œä¸ºæ¨è¿›è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶çš„ä¸€ç§æ›¿ä»£æ–¹æ³•çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è›‹ç™½è´¨ç”Ÿç‰©å­¦å…³æ³¨åºåˆ—ã€ç»“æ„ä¸åŠŸèƒ½ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>è§£æè›‹ç™½è´¨åŠŸèƒ½å¯¹ç†è§£ç”Ÿç‰©è¿‡ç¨‹ã€è¯ç‰©å‘ç°å’Œåˆæˆç”Ÿç‰©å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰éš¾ä»¥æ•´åˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>STELLAæ˜¯ä¸€ä¸ªå¤šæ¨¡å¼LLMï¼Œé€šè¿‡æ•´åˆè›‹ç™½è´¨åºåˆ—-ç»“æ„è¡¨ç¤ºå’Œé€šç”¨çŸ¥è¯†æ¥é¢„æµ‹è›‹ç™½è´¨åŠŸèƒ½ã€‚</li>
<li>STELLAä½¿ç”¨OPI-Strucæ•°æ®é›†è¿›è¡Œå¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ˆMMITï¼‰ï¼Œåœ¨åŠŸèƒ½æè¿°é¢„æµ‹å’Œé…¶å‚¬åŒ–ååº”é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å…ˆè¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77339c99bc04ff317b52856b5a2ec630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f72ffc3f41650acbbfc68e50427f8668.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8b7dbf8f8bd9e135392c9cfe1310cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7d40f4ad4d4a8b7bc5673d05e01b8f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining"><a href="#Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining" class="headerlink" title="Scaling Transformers for Discriminative Recommendation via Generative   Pretraining"></a>Scaling Transformers for Discriminative Recommendation via Generative   Pretraining</h2><p><strong>Authors:Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen, Bing Wang, Xiaoyi Zeng</strong></p>
<p>Discriminative recommendation tasks, such as CTR (click-through rate) and CVR (conversion rate) prediction, play critical roles in the ranking stage of large-scale industrial recommender systems. However, training a discriminative model encounters a significant overfitting issue induced by data sparsity. Moreover, this overfitting issue worsens with larger models, causing them to underperform smaller ones. To address the overfitting issue and enhance model scalability, we propose a framework named GPSD (\textbf{G}enerative \textbf{P}retraining for \textbf{S}calable \textbf{D}iscriminative Recommendation), drawing inspiration from generative training, which exhibits no evident signs of overfitting. GPSD leverages the parameters learned from a pretrained generative model to initialize a discriminative model, and subsequently applies a sparse parameter freezing strategy. Extensive experiments conducted on both industrial-scale and publicly available datasets demonstrate the superior performance of GPSD. Moreover, it delivers remarkable improvements in online A&#x2F;B tests. GPSD offers two primary advantages: 1) it substantially narrows the generalization gap in model training, resulting in better test performance; and 2) it leverages the scalability of Transformers, delivering consistent performance gains as models are scaled up. Specifically, we observe consistent performance improvements as the model dense parameters scale from 13K to 0.3B, closely adhering to power laws. These findings pave the way for unifying the architectures of recommendation models and language models, enabling the direct application of techniques well-established in large language models to recommendation models. </p>
<blockquote>
<p>åˆ¤åˆ«å¼æ¨èä»»åŠ¡ï¼Œå¦‚ç‚¹å‡»ç‡ï¼ˆCTRï¼‰å’Œè½¬åŒ–ç‡ï¼ˆCVRï¼‰é¢„æµ‹ï¼Œåœ¨å¤§è§„æ¨¡å·¥ä¸šæ¨èç³»ç»Ÿçš„æ’åºé˜¶æ®µèµ·ç€å…³é”®ä½œç”¨ã€‚ç„¶è€Œï¼Œè®­ç»ƒåˆ¤åˆ«æ¨¡å‹ä¼šé‡åˆ°ç”±æ•°æ®ç¨€ç–å¼•èµ·çš„ä¸¥é‡è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è€Œä¸”ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œè¿‡æ‹Ÿåˆé—®é¢˜ä¼šæ¶åŒ–ï¼Œå¯¼è‡´å¤§å‹æ¨¡å‹æ€§èƒ½ä¸å¦‚å°å‹æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæ¨¡å‹çš„å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºGPSDï¼ˆä¸ºå¯æ‰©å±•åˆ¤åˆ«æ¨èè®¾è®¡çš„ç”Ÿæˆé¢„è®­ç»ƒæ¡†æ¶ï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´äº†ç”Ÿæˆè®­ç»ƒï¼Œç”Ÿæˆè®­ç»ƒæ²¡æœ‰æ˜æ˜¾çš„è¿‡æ‹Ÿåˆè¿¹è±¡ã€‚GPSDåˆ©ç”¨ä»é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ä¸­å­¦ä¹ çš„å‚æ•°æ¥åˆå§‹åŒ–åˆ¤åˆ«æ¨¡å‹ï¼Œéšååº”ç”¨ç¨€ç–å‚æ•°å†»ç»“ç­–ç•¥ã€‚åœ¨å·¥ä¸šçº§å’Œå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGPSDå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨åœ¨çº¿A&#x2F;Bæµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚GPSDå…·æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜ç‚¹ï¼š1ï¼‰å®ƒå¤§å¤§ç¼©å°äº†æ¨¡å‹è®­ç»ƒä¸­çš„æ³›åŒ–å·®è·ï¼Œä»è€Œæé«˜äº†æµ‹è¯•æ€§èƒ½ï¼›2ï¼‰å®ƒåˆ©ç”¨å˜å‹å™¨çš„å¯æ‰©å±•æ€§ï¼Œéšç€æ¨¡å‹çš„æ‰©å±•ï¼Œæ€§èƒ½ä¸æ–­æé«˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¨¡å‹å¯†é›†å‚æ•°ä»13Kåˆ°0.3Bçš„æ‰©å±•è¿‡ç¨‹ä¸­ï¼Œæ€§èƒ½æŒç»­æé«˜ï¼Œéµå¾ªå¹‚å¾‹ã€‚è¿™äº›å‘ç°ä¸ºç»Ÿä¸€æ¨èæ¨¡å‹å’Œè¯­è¨€æ¨¡å‹çš„æ¶æ„é“ºå¹³äº†é“è·¯ï¼Œä½¿å¾—åœ¨æ¨èæ¨¡å‹ä¸­ç›´æ¥åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æˆç†Ÿçš„æŠ€æœ¯æˆä¸ºå¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03699v1">PDF</a> KDDâ€™25</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºGPSDçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å°†ç”Ÿæˆå¼é¢„è®­ç»ƒèå…¥åˆ¤åˆ«å¼æ¨èä»»åŠ¡æ¥è§£å†³å¤§è§„æ¨¡å·¥ä¸šæ¨èç³»ç»Ÿä¸­çš„æ•°æ®ç¨€ç–æ€§é—®é¢˜åŠæ¨¡å‹è¿‡æ‹Ÿåˆé—®é¢˜ã€‚GPSDåˆ©ç”¨é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹çš„å‚æ•°åˆå§‹åŒ–åˆ¤åˆ«æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨ç¨€ç–å‚æ•°å†»ç»“ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼ŒGPSDåœ¨åœ¨çº¿A&#x2F;Bæµ‹è¯•å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·æœ‰ç¼©å°æ¨¡å‹è®­ç»ƒä¸­çš„æ³›åŒ–å·®è·å’Œæå‡æ¨¡å‹è§„æ¨¡æ‹“å±•æ€§ä¸¤å¤§ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒGPSDå®ç°äº†æ¨èæ¨¡å‹ä¸è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¶æ„èåˆï¼Œå¯ç›´æ¥åº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ¤åˆ«å¼æ¨èä»»åŠ¡å¦‚CTRå’ŒCVRé¢„æµ‹åœ¨å·¥ä¸šæ¨èç³»ç»Ÿä¸­æ’åé˜¶æ®µæ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†é¢ä¸´æ•°æ®ç¨€ç–å¯¼è‡´çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>è¿‡æ‹Ÿåˆé—®é¢˜éšæ¨¡å‹è§„æ¨¡çš„æ‰©å¤§è€ŒåŠ å‰§ï¼Œå¯¼è‡´å¤§å‹æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>GPSDæ¡†æ¶ç»“åˆç”Ÿæˆå¼é¢„è®­ç»ƒæ¥è§£å†³åˆ¤åˆ«å¼æ¨¡å‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„å‚æ•°åˆå§‹åŒ–åˆ¤åˆ«æ¨¡å‹ï¼Œå¹¶åº”ç”¨ç¨€ç–å‚æ•°å†»ç»“ç­–ç•¥ã€‚</li>
<li>å®éªŒè¯æ˜GPSDåœ¨åœ¨çº¿A&#x2F;Bæµ‹è¯•å’Œå…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>GPSDå…·æœ‰ç¼©å°æ¨¡å‹è®­ç»ƒä¸­çš„æ³›åŒ–å·®è·å’Œæå‡æ¨¡å‹è§„æ¨¡æ‹“å±•æ€§ä¸¤å¤§ä¼˜åŠ¿ã€‚</li>
<li>GPSDå®ç°äº†æ¨èæ¨¡å‹ä¸è¯­è¨€æ¨¡å‹çš„æ¶æ„ç»Ÿä¸€ï¼Œå¯ç›´æ¥åº”ç”¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æŠ€æœ¯æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a2e7db7c121cd55894d4525a1e8cfdfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93b25a6e587a5e4dcc308214bcbf235f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb62b7a0f425bc97d23a0f906f96c4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a83dc258dfa35a1e2d9f32f4127037b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07069a6dda775e41c701e5656437d9dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02e2203e613cc6d80b415ec0cffd2631.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping"><a href="#MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping" class="headerlink" title="MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping"></a>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</h2><p><strong>Authors:Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang</strong></p>
<p>Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„è¿›å±•å·²ç»åœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›çªç ´ä¸»è¦å¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼ï¼Œå®ƒåˆ©ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾å¤šæ¨¡æ€æ•°æ®ï¼Œç„¶ååœ¨ç²¾é€‰çš„æ ‡è®°æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¹¶ä½¿ç”¨é«˜è´¨é‡æç¤ºã€‚è™½ç„¶äººä»¬å¯¹æ‰©å¤§æŒ‡ä»¤å¾®è°ƒä»¥æ¶µç›–æ›´å¤šæ•°é‡å’Œè§„æ¨¡çš„æ•°æ®é›†è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»…ä»…å¢åŠ æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡çš„æ•°é‡å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ç›¸åï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡è·¨æ¨¡æ€çš„å¸¸è§äº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¦‚å‘ç°å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆé€‰æ‹©ä¸ç‹¬ç‰¹ä¿¡æ¯ç»“åˆçš„æ¨¡æ€ï¼Œæˆ–éœ€è¦ååŒèåˆä»¥ä»ä¸¤ç§æ¨¡æ€ä¸­å‘ç°æ–°ä¿¡æ¯ï¼Œè¿™é¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸€ç»„å†…çš„å¯è¿ç§»æŠ€èƒ½ï¼ŒåŒæ—¶æŠ‘åˆ¶äº†ä¸åŒ¹é…ä»»åŠ¡çš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MINTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ç®€å•è€Œæœ‰æ•ˆçš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯¹äºå¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¤§å¤§ä¼˜äºç°æœ‰çš„ä»»åŠ¡åˆ†ç»„åŸºçº¿ï¼Œåœ¨é€šç”¨æ€§å’Œä¸“ä¸šåŒ–ä¹‹é—´è¾¾åˆ°äº†æœ‰æ•ˆçš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02308v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ–°è¿›å±•åœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›çªç ´ä¸»è¦å¾—ç›Šäºæ–°çš„é¢„è®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å¼åˆ©ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾å¤šæ¨¡æ€æ•°æ®ï¼Œéšååœ¨ç²¾é€‰çš„æœ‰æ ‡ç­¾æ•°æ®é›†å’Œé«˜è´¨é‡æç¤ºä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚å°½ç®¡äººä»¬å¯¹æ‰©å¤§æŒ‡ä»¤å¾®è°ƒçš„æ•°æ®é›†è§„æ¨¡å’Œæ•°é‡è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç®€å•åœ°å¢åŠ æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡çš„æ•°é‡å¹¶ä¸ä¸€å®šèƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ç›¸åï¼Œæˆ‘ä»¬é€šè¿‡æŒ‰è·¨æ¨¡æ€çš„é€šç”¨äº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¦‚å‘ç°å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆé€‰æ‹©ä¸ç‹¬ç‰¹ä¿¡æ¯ç»“åˆçš„æ¨¡æ€æˆ–è¦æ±‚ä»ä¸¤ç§æ¨¡æ€ä¸­å‘ç°æ–°ä¿¡æ¯çš„ååŒèåˆç­‰æ–¹æ³•ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ ç»„å†…å¯è¿ç§»çš„æŠ€èƒ½ï¼ŒåŒæ—¶æŠ‘åˆ¶æ¥è‡ªä¸ç›¸å…³ä»»åŠ¡çš„å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ç®€å•è€Œæœ‰æ•ˆçš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥MINTã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´çš„ä»»åŠ¡åˆ†ç»„ä¸Šå¤§å¤§ä¼˜äºç°æœ‰åŸºçº¿ï¼Œåœ¨æ¨å¹¿å’Œä¸“ä¸šåŒ–ä¹‹é—´è¾¾åˆ°äº†æœ‰æ•ˆçš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹çš„æ–°è¿›å±•åœ¨å„ç§ä»»åŠ¡ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å¼ç»“åˆå¤§è§„æ¨¡æ— æ ‡ç­¾å¤šæ¨¡æ€æ•°æ®å’ŒæŒ‡ä»¤å¾®è°ƒæ˜¯æé«˜æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>å•çº¯å¢åŠ æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡çš„æ•°é‡å¹¶ä¸æ€»èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡æŒ‰è·¨æ¨¡æ€é€šç”¨äº¤äº’å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„ï¼Œå¯ä»¥é¼“åŠ±æ¨¡å‹å­¦ä¹ å¯è¿ç§»æŠ€èƒ½ã€‚</li>
<li>å‘ç°å†—ä½™çš„å…±äº«ä¿¡æ¯ã€ä¼˜å…ˆç»“åˆç‹¬ç‰¹ä¿¡æ¯çš„æ¨¡æ€å’ŒååŒèåˆæ˜¯æé«˜å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºå¤šæ¨¡æ€äº¤äº’ç±»å‹çš„ç®€å•è€Œæœ‰æ•ˆçš„ä»»åŠ¡åˆ†ç»„ç­–ç•¥MINTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78f0ea984b188fa8f886c4fabcb06cca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-320e29dcdf9de3a29ff9c850167e4ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbbfbcf76e25da5223c10c5b736a488.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e50ba509cddc17f4767d2be3e3ea694f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab60ebc7690d7c235db9dbfc46519a2b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  OWMM-Agent Open World Mobile Manipulation With Multi-modal Agentic Data   Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f72ffc3f41650acbbfc68e50427f8668.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
