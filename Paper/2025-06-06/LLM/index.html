<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-06-06  Language-Image Alignment with Fixed Text Encoders">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6695769443305db9e6b95213c3d323bb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-06-更新"><a href="#2025-06-06-更新" class="headerlink" title="2025-06-06 更新"></a>2025-06-06 更新</h1><h2 id="Language-Image-Alignment-with-Fixed-Text-Encoders"><a href="#Language-Image-Alignment-with-Fixed-Text-Encoders" class="headerlink" title="Language-Image Alignment with Fixed Text Encoders"></a>Language-Image Alignment with Fixed Text Encoders</h2><p><strong>Authors:Jingfeng Yang, Ziyang Wu, Yue Zhao, Yi Ma</strong></p>
<p>Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations. </p>
<blockquote>
<p>目前，建立语言-图像对齐的最主流方法是通过对比学习联合预训练文本和图像编码器，如CLIP及其变体。在这项工作中，我们质疑这种昂贵的联合训练是否有必要。我们特别调查预训练的固定大型语言模型（LLM）是否提供了足够好的文本编码器来指导视觉表示学习。也就是说，我们提出了只训练图像编码器，利用LLM的固定文本编码器来学习语言-图像对齐（LIFT）。令人惊讶的是，通过全面的基准测试和消融研究，我们发现这种简化的LIFT框架非常有效，在涉及组合理解和长字幕的多数场景中，它都优于CLIP，同时实现了计算效率上的显著收益。我们的工作朝着系统地探索LLM中的文本嵌入如何指导视觉学习迈出了第一步，并为学习语言对齐的视觉表示提供了另一种设计选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04209v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了使用预训练的固定大型语言模型（LLM）进行语言图像对齐的可行性，提出了一种仅训练图像编码器的方法（称为LIFT）。研究表明，这种方法在涉及组合理解和长文本描述的场景中，表现优于CLIP模型，并实现了计算效率的显著提高。本文首次系统地探索了LLM文本嵌入如何指导视觉学习，并为语言对齐的视觉表示学习提供了另一种设计选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目前主流的语言图像对齐方法是通过对比学习联合预训练文本和图像编码器，如CLIP及其变体。</li>
<li>本文质疑是否必须进行这种昂贵的联合训练。</li>
<li>提出仅通过预训练的固定大型语言模型（LLM）作为文本编码器来指导视觉表示学习的新方法，称为LIFT。</li>
<li>LIFT方法通过简化的框架实现了与CLIP相当或更好的性能，尤其在涉及组合理解和长文本描述的场景中表现更优秀。</li>
<li>LIFT方法在计算效率上实现了显著的提升。</li>
<li>本文首次系统地探索了LLM文本嵌入在指导视觉学习方面的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04209">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e35dd00dafa87d19ae5cdb2ef4db2c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08f71da0d9182562150788394cc22b7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7625d4d2b7bd22b0e72e62f64929559d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d83f766298684ab97e7007eeb17d817.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c64e4f027558ed0d577302978ed9d503.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8730eae0827117c25c0e102a735d373d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a57fc35e1379ffb11109a6822a1f03cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bdc57b842e28073c0ec8f285b5fc1c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Advancing-Multimodal-Reasoning-From-Optimized-Cold-Start-to-Staged-Reinforcement-Learning"><a href="#Advancing-Multimodal-Reasoning-From-Optimized-Cold-Start-to-Staged-Reinforcement-Learning" class="headerlink" title="Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning"></a>Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning</h2><p><strong>Authors:Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng</strong></p>
<p>Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025. </p>
<blockquote>
<p>受到Deepseek-R1在处理复杂文本任务中的出色推理能力的启发，许多工作尝试通过直接应用强化学习（RL）来激励类似的能力于多模态大型语言模型（MLLMs）。然而，他们在激活复杂推理方面仍然面临困难。在本文中，我们并没有孤立地研究多模态RL，而是深入研究了当前的训练管道，并发现了三个关键现象：1）有效的冷启动初始化对于增强MLLM推理至关重要。有趣的是，我们发现仅通过精心选择的文本数据进行初始化可以导致性能超越许多最新的多模态推理模型，甚至在多模态RL之前。2）应用于多模态RL的标准GRPO遭受梯度停滞的困扰，这降低了训练稳定性和性能。3）在多模态RL阶段之后进行的纯文本RL训练可进一步增强多模态推理。这种分阶段训练方法有效地平衡了感知定位与认知推理发展。通过融入上述见解并解决多模态RL的问题，我们推出了ReVisual-R1，在包括MathVerse、MathVision、WeMath、LogicVista、DynaMath以及具有挑战性的AIME2024和AIME2025等开放源代码的7B MLLMs上达到了最新的技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04207v1">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了如何激励多模态大型语言模型（MLLMs）的复杂推理能力。研究发现，有效的冷启动初始化对增强MLLM推理至关重要。通过精心选择的文本数据初始化可以超越许多最新的多模态推理模型。同时，发现标准GRPO在应用于多模态RL时存在梯度停滞问题，影响训练稳定性和性能。随后文本只有强化学习训练，在多层次强化学习阶段后，能进一步增强多模态推理。通过结合这些见解并解决多模态RL问题，推出了ReVisual-R1模型，在多个挑战基准测试中实现了开源7B MLLM的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>有效冷启动初始化对增强MLLM推理至关重要，通过精心选择的文本数据初始化性能可超越许多多模态推理模型。</li>
<li>标准GRPO在多模态RL中存在梯度停滞问题，影响训练稳定性和性能。</li>
<li>文本只有强化学习训练在多模态RL阶段之后能进一步增强多模态推理。</li>
<li>平衡感知基础和认知推理发展是关键，采用分阶段训练方法。</li>
<li>ReVisual-R1模型结合以上见解解决多模态RL问题，实现开源7B MLLM的最佳性能。</li>
<li>ReVisual-R1模型在多个挑战基准测试中表现优异，包括MathVerse, MathVision, WeMath, LogicVista, DynaMath及AIME2024和AIME2025。</li>
<li>此研究成果对于推动MLLMs的复杂推理能力有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c372b9949f00003cd9668401c8ba805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3df634d151ee7b5f86f0bb59474c69b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4d7104ecdbac515058be32687c3577.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation"><a href="#EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation" class="headerlink" title="EPiC: Towards Lossless Speedup for Reasoning Training through   Edge-Preserving CoT Condensation"></a>EPiC: Towards Lossless Speedup for Reasoning Training through   Edge-Preserving CoT Condensation</h2><p><strong>Authors:Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu</strong></p>
<p>Large language models (LLMs) have shown remarkable reasoning capabilities when trained with chain-of-thought (CoT) supervision. However, the long and verbose CoT traces, especially those distilled from large reasoning models (LRMs) such as DeepSeek-R1, significantly increase training costs during the distillation process, where a non-reasoning base model is taught to replicate the reasoning behavior of an LRM. In this work, we study the problem of CoT condensation for resource-efficient reasoning training, aimed at pruning intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling supervised model training on length-reduced CoT data while preserving both answer accuracy and the model’s ability to generate coherent reasoning. Our rationale is that CoT traces typically follow a three-stage structure: problem understanding, exploration, and solution convergence. Through empirical analysis, we find that retaining the structure of the reasoning trace, especially the early stage of problem understanding (rich in reflective cues) and the final stage of solution convergence, is sufficient to achieve lossless reasoning supervision. To this end, we propose an Edge-Preserving Condensation method, EPiC, which selectively retains only the initial and final segments of each CoT trace while discarding the middle portion. This design draws an analogy to preserving the “edge” of a reasoning trajectory, capturing both the initial problem framing and the final answer synthesis, to maintain logical continuity. Experiments across multiple model families (Qwen and LLaMA) and benchmarks show that EPiC reduces training time by over 34% while achieving lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To the best of our knowledge, this is the first study to explore thought-level CoT condensation for efficient reasoning model distillation. </p>
<blockquote>
<p>大型语言模型（LLM）在通过思维链（CoT）监督进行训练时，展现出了显著的推理能力。然而，思维链轨迹，尤其是从深度寻求R1等大型推理模型（LRM）中提炼出的轨迹，在蒸馏过程中显著增加了训练成本，其中非推理基础模型被教导复制LRM的推理行为。在这项工作中，我们研究了面向资源高效推理训练的思维链凝结问题，旨在删除思维链轨迹中的中间推理步骤（即思维），从而在保持答案准确性和模型生成连贯推理能力的同时，实现在缩减长度后的思维链数据上监督模型训练。我们的理念是，思维链轨迹通常遵循三阶段结构：理解问题、探索和解决方案收敛。通过实证分析，我们发现保留推理轨迹的结构，特别是理解问题的早期阶段（富含反思线索）和解决方案收敛的最后一个阶段，足以实现无损推理监督。为此，我们提出了一种边缘保留凝结方法（EPiC），该方法有选择地仅保留每个思维链轨迹的初始和最终部分，同时丢弃中间部分。这种设计类似于保留推理轨迹的“边缘”，捕捉初始问题框架和最终答案合成，以保持逻辑连续性。在多模型家族（Qwen和LLaMA）和基准测试上的实验表明，EPiC将训练时间减少了34%以上，同时在MATH500上实现了与完整思维链监督相当的推理准确性。据我们所知，这是第一项探索高效推理模型蒸馏中的思维水平思维链凝结的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04205v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLMs）通过链式思维（CoT）监督训练展现出出色的推理能力。然而，尤其是在从深度搜索R等大数据推理模型（LRMs）提炼出来的长且冗长的CoT轨迹中，显著增加了蒸馏过程中的训练成本。本文旨在研究减少资源消耗的推理训练中的思维链（CoT）凝聚问题，旨在修剪CoT轨迹中的中间推理步骤，从而在保持答案准确性和模型生成连贯推理能力的同时，对长度缩减的CoT数据进行监督模型训练。本文的推理轨迹通常遵循理解问题、探索、和解决方案收敛的三阶段结构。通过实验分析，我们发现保留理解问题的初步阶段以及解决方案收敛的最终阶段是确保无损失推理监督的关键。为此，我们提出了一种边缘保留凝聚方法（EPiC），该方法有选择地保留每个CoT轨迹的初始和最终部分，同时丢弃中间部分。这种方法类似于捕捉推理轨迹的“边缘”，既能保持问题的初始框架，又能维持最终的答案合成，从而维持逻辑连贯性。实验显示，EPiC在多模型家族和基准测试中减少了超过34%的训练时间，同时在MATH500上实现了无损推理准确性，与完整的CoT监督相当。据我们所知，这是第一项关于有效推理模型蒸馏中思维级CoT凝聚的研究。</p>
<p><strong>要点归纳</strong></p>
<ol>
<li>大型语言模型在链式思维监督下展现出卓越推理能力，但训练过程中存在高成本问题。</li>
<li>研究聚焦于思维链凝聚以减少资源消耗，旨在修剪中间推理步骤。</li>
<li>思维链轨迹遵循三阶段结构：理解问题、探索和解决方案收敛。</li>
<li>保留问题理解阶段和解决方案收敛阶段是确保无损推理监督的关键。</li>
<li>提出边缘保留凝聚方法（EPiC），选择性保留思维轨迹的初始和最终部分。</li>
<li>EPiC方法显著减少了训练时间，同时保持了MATH500上的无损推理准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04205">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-55aca2880c7d30e7d16c63af14f1b211.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9af991d4c026c1252da709b661ce928.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9bbc31fcad4fcc0d988bc4a1c71c5b6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86ae84eafc90ce41080ecab61e15d41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da85968f48f5e69fb97cfa1a3a40f70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aafeaa0f991135cc33b51381844ec1d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b74f08e0285f605dbb2e1f49223bac5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Cascadia-A-Cascade-Serving-System-for-Large-Language-Models"><a href="#Cascadia-A-Cascade-Serving-System-for-Large-Language-Models" class="headerlink" title="Cascadia: A Cascade Serving System for Large Language Models"></a>Cascadia: A Cascade Serving System for Large Language Models</h2><p><strong>Authors:Youhe Jiang, Fangcheng Fu, Wanru Zhao, Stephan Rabanser, Nicholas D. Lane, Binhang Yuan</strong></p>
<p>Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality answers. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet less capable. Recent work proposes balancing this latency-quality trade-off using model cascades, which route simpler queries to smaller models and more complex ones to larger models. However, enabling efficient cascade serving remains challenging. Current frameworks lack effective mechanisms for handling (i) the huge and varying resource demands of different LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the co-optimization of system deployment and routing strategy. Motivated by these observations, we introduce Cascadia, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. Cascadia employs a bi-level optimization method: at the inner level, it uses a mixed-integer linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the outer level, it applies a weighted Tchebycheff algorithm to iteratively co-optimize the routing strategy and the system deployment produced by the inner level. Our extensive evaluation on diverse workload traces and different model cascades (DeepSeek and the Llama series) demonstrates that Cascadia significantly outperforms both single-model deployments and the state-of-the-art cascade serving baseline, achieving up to 4x (2.3x on average) tighter latency SLOs and up to 5x (2.4x on average) higher throughput while maintaining target answer quality. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步加剧了对快速响应和高质答案的需求。更强大的模型虽然能产生更好的结果，但会导致更高的推理延迟，而较小的模型虽然速度更快，但能力较弱。最近的工作提出了使用模型级联来平衡这种延迟与质量的权衡，将简单的查询路由到较小的模型，将复杂的查询路由到较大的模型。然而，实现高效的级联服务仍然是一个挑战。当前框架缺乏有效机制来处理（i）不同LLM的巨大且多变的资源需求，（ii）LLM工作负载的固有异质性，以及（iii）系统部署和路由策略的协同优化。基于这些观察，我们引入了Cascadia，这是一个专门设计用于调度请求路由和部署模型级联的快速、保质的LLM服务的新型级联服务框架。Cascadia采用两级优化方法：在内层，它使用混合整数线性规划，基于LLM信息和工作负载特性选择资源分配和并行策略；在外层，它应用加权Tchebycheff算法，迭代地协同优化由内部级别产生的路由策略和系统部署。我们对不同的工作负载跟踪和模型级联（DeepSeek和Llama系列）进行了广泛评估，结果表明Cascadia显著优于单模型部署和最新的级联服务基准测试，实现了高达4倍（平均2.3倍）的更紧延迟SLO和高达5倍（平均2.4倍）的更高吞吐量，同时保持目标答案质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04203v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的最新进展加剧了对快速响应和高质回答的需求。更强大的模型虽然能产生更好的结果，但推理延迟较高，而较小的模型虽然速度更快，但能力有限。为平衡延迟与质量的权衡，提出了使用模型级联的方法，将简单的查询路由到较小的模型，复杂的查询路由到较大的模型。然而，实现高效的级联服务仍然存在挑战。当前框架缺乏处理LLM不同资源需求的巨大差异、LLM工作负载的内在异质性和系统部署与路由策略的协同优化的有效机制。本文介绍Cascadia，一种专门设计用于调度请求路由和部署模型级联的快速、高质量保留的LLM服务级联框架。Cascadia采用两级优化方法：内部使用混合整数线性规划根据LLM信息和工作负载特性选择资源分配和并行策略；外部采用加权切比雪夫算法迭代优化路由策略和内部产生的系统部署。在多种工作负载追踪和不同的模型级联（DeepSeek和Llama系列）上的广泛评估表明，Cascadia显著优于单模型部署和当前最先进的级联服务基线，在维持目标答案质量的同时，实现延迟SLO的4倍（平均提高2.3倍）和吞吐量提高5倍（平均提高2.4倍）。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）面临快速响应和高质回答的需求平衡问题。</li>
<li>模型级联是解决延迟与质量问题的一种有效方法。</li>
<li>当前框架在处理LLM的资源需求、工作负载异质性和系统部署与路由策略的协同优化方面存在挑战。</li>
<li>Cascadia框架通过两级优化方法解决这些问题，实现高效、高质量的LLM服务。</li>
<li>Cascadia采用混合整数线性规划和加权切比雪夫算法进行优化。</li>
<li>Cascadia在多种工作负载和模型级联上的表现显著优于单模型部署和现有基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04203">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d6209803dfb16b0c32700f7f605a6420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-374e38cb86bed8c62d13780d36e400cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6710574e883d22e435ac9ae3a8d2e7ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80b2ae9ee7a95c9dbc733bf335e34d73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-221d6563c6fffdb8e84b7fafee42aa05.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TracLLM-A-Generic-Framework-for-Attributing-Long-Context-LLMs"><a href="#TracLLM-A-Generic-Framework-for-Attributing-Long-Context-LLMs" class="headerlink" title="TracLLM: A Generic Framework for Attributing Long Context LLMs"></a>TracLLM: A Generic Framework for Attributing Long Context LLMs</h2><p><strong>Authors:Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia</strong></p>
<p>Long context large language models (LLMs) are deployed in many real-world applications such as RAG, agent, and broad LLM-integrated applications. Given an instruction and a long context (e.g., documents, PDF files, webpages), a long context LLM can generate an output grounded in the provided context, aiming to provide more accurate, up-to-date, and verifiable outputs while reducing hallucinations and unsupported claims. This raises a research question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs) in the context that contribute most to or are responsible for the generated output by an LLM? This process, which we call context traceback, has various real-world applications, such as 1) debugging LLM-based systems, 2) conducting post-attack forensic analysis for attacks (e.g., prompt injection attack, knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources to enhance the trust of users towards outputs generated by LLMs. When applied to context traceback for long context LLMs, existing feature attribution methods such as Shapley have sub-optimal performance and&#x2F;or incur a large computational cost. In this work, we develop TracLLM, the first generic context traceback framework tailored to long context LLMs. Our framework can improve the effectiveness and efficiency of existing feature attribution methods. To improve the efficiency, we develop an informed search based algorithm in TracLLM. We also develop contribution score ensemble&#x2F;denoising techniques to improve the accuracy of TracLLM. Our evaluation results show TracLLM can effectively identify texts in a long context that lead to the output of an LLM. Our code and data are at: <a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM">https://github.com/Wang-Yanting/TracLLM</a>. </p>
<blockquote>
<p>长语境大型语言模型（LLM）被部署在许多真实世界应用中，如RAG、智能代理和广泛的LLM集成应用。给定一个指令和长语境（例如文档、PDF文件、网页），长语境LLM可以基于提供的语境生成输出，旨在提供更准确、最新和可验证的输出，同时减少幻觉和未经证实的声明。这引发了一个研究问题：如何确定语境中的文本（例如句子、段落）对LLM生成的输出贡献最大或负责？我们称这个过程为上下文回溯，具有各种真实世界应用，例如1）调试基于LLM的系统，2）对LLM进行攻击后的法医分析（例如提示注入攻击、知识腐败攻击），以及3）突出显示知识来源，以增强用户对LLM生成输出的信任。当应用于长语境LLM的上下文回溯时，现有的特征归因方法（如Shapley）性能不佳且计算成本较高。在这项工作中，我们开发了TracLLM，这是第一个针对长语境LLM的通用上下文回溯框架。我们的框架可以提高现有特征归因方法的有效性和效率。为了提高效率，我们在TracLLM中开发了基于信息搜索的算法。我们还开发了贡献分数集成&#x2F;降噪技术，以提高TracLLM的准确性。我们的评估结果表明，TracLLM可以有效地识别长语境中导致LLM输出的文本。我们的代码和数据位于：<a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM%E3%80%82">https://github.com/Wang-Yanting/TracLLM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04202v1">PDF</a> To appear in USENIX Security Symposium 2025. The code and data are   at: <a target="_blank" rel="noopener" href="https://github.com/Wang-Yanting/TracLLM">https://github.com/Wang-Yanting/TracLLM</a></p>
<p><strong>Summary</strong></p>
<p>长语境大型语言模型（LLM）已广泛应用于RAG、智能代理和广泛的LLM集成应用等实际场景中。针对LLM如何定位生成输出中贡献最大的文本（如句子、段落等）的问题，我们称之为上下文回溯，具有重要的现实意义，如调试LLM系统、进行攻击后的法医学分析以及突出知识来源以增强用户对LLM生成输出的信任。现有特征归因方法如Shapley在应用于长语境LLM的上下文回溯时效果不理想且计算成本较高。本文开发了一个针对长语境LLM的通用上下文回溯框架TracLLM，改进了现有特征归因方法的有效性和效率。通过算法优化和技术改进，TracLLM能准确识别导致LLM输出的文本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs已被广泛应用于多个领域，具有强大的语言处理能力。</li>
<li>在提供长语境的情况下，LLMs能够提供更准确、实时和可验证的输出。</li>
<li>“上下文回溯”是一个重要的研究领域，旨在确定LLM生成输出所依赖的文本内容。</li>
<li>上下文回溯的应用包括调试LLM系统、攻击后的法医学分析和增强用户对LLM的信任等。</li>
<li>现有特征归因方法在长语境LLM的上下文回溯方面存在效率和准确性问题。</li>
<li>新开发的TracLLM框架旨在改进这些方法，并通过算法优化和技术改进提高效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-682ca0f3225d5bcb356e76b050c5b746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f013b6f939e53ca40d5a688f2bf64e69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68045b6106fec5bfb9494f37f3309866.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="R-Search-Empowering-LLM-Reasoning-with-Search-via-Multi-Reward-Reinforcement-Learning"><a href="#R-Search-Empowering-LLM-Reasoning-with-Search-via-Multi-Reward-Reinforcement-Learning" class="headerlink" title="R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning"></a>R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning</h2><p><strong>Authors:Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu</strong></p>
<p>Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/QingFei1/R-Search">https://github.com/QingFei1/R-Search</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在多步骤和长链推理方面取得了显著进展。然而，将其推理能力扩展以涵盖与搜索的深层次交互仍然是一个不小的挑战，因为模型通常无法识别出最佳的推理-搜索交互轨迹，从而导致响应不佳。我们提出了R-Search，这是一个用于推理-搜索集成的新型强化学习框架，旨在使LLM能够自主执行具有深层搜索交互的多步骤推理，并通过多奖励信号学习最佳的推理搜索交互轨迹，从而在复杂的逻辑和知识密集型任务中提高响应质量。R-Search引导LLM动态决定何时进行检索或推理，同时全局集成关键证据，增强推理和搜索之间的深层知识交互。在强化学习训练过程中，R-Search提供多阶段、多类型的奖励来共同优化推理-搜索轨迹。在7个数据集上的实验表明，R-Search较先进的RAG基准测试高出32.2%（领域内）和25.1%（跨领域）。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/QingFei1/R-Search%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/QingFei1/R-Search找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04185v1">PDF</a> 16 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多步和长链推理方面取得显著进展，但在与搜索的深度交互方面仍存在挑战。提出R-Search框架，通过强化学习实现推理与搜索的集成，使LLM能够自主执行多步推理并深度搜索交互，通过多奖励信号学习最优推理搜索交互轨迹，提高在复杂逻辑和知识密集型任务中的响应质量。R-Search框架可指导LLM动态决定何时检索或推理，同时全局整合关键证据，增强推理与搜索之间的深度知识交互。实验结果表明，R-Search在七个数据集上的表现优于先进的RAG基准测试，最高提升率达32.2%（领域内）和25.1%（跨领域）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多步和长链推理上有所进步，但在与搜索的深度交互方面仍面临挑战。</li>
<li>R-Search是一个强化学习框架，旨在实现推理与搜索的集成。</li>
<li>R-Search使LLM能够自主执行多步推理，并深度搜索交互。</li>
<li>通过多奖励信号学习最优推理搜索交互轨迹。</li>
<li>R-Search提高了在复杂逻辑和知识密集型任务中的响应质量。</li>
<li>R-Search框架可动态调整检索与推理的时机，并全局整合关键证据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04185">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cac3135fed19307e49f5a5ec1b29f3de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1494d44782cd1eec136119a2c80f523.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SuperWriter-Reflection-Driven-Long-Form-Generation-with-Large-Language-Models"><a href="#SuperWriter-Reflection-Driven-Long-Form-Generation-with-Large-Language-Models" class="headerlink" title="SuperWriter: Reflection-Driven Long-Form Generation with Large Language   Models"></a>SuperWriter: Reflection-Driven Long-Form Generation with Large Language   Models</h2><p><strong>Authors:Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee</strong></p>
<p>Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation. </p>
<blockquote>
<p>长文本生成对于大型语言模型（LLM）来说仍然是一个重大挑战，特别是在保持连贯性、确保逻辑一致性和随着序列长度增加而保持文本质量方面。为了解决这些局限性，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。SuperWriter-Agent将明确的结构化思考引入生成管道，通过规划和细化阶段，引导模型遵循一个更加深思熟虑和认知基础的过程，类似于专业作家的过程。基于此框架，我们构建了一个监督微调数据集，以训练一个7B的SuperWriter-LM。我们进一步开发了一种分层的直接偏好优化（DPO）程序，该程序使用蒙特卡洛树搜索（MCTS）来传播最终质量评估并相应地优化每个生成步骤。在多种基准测试上的实证结果表明，SuperWriter-LM达到了最先进的性能，不仅在自动评估中而且在人类评估中超越了更大规模的基准模型。此外，全面的消融研究证明了分层DPO的有效性，并强调了融入结构化思考步骤以提高长文本生成质量的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04180v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在长文本生成方面存在挑战，如保持连贯性、逻辑一致性和文本质量。为此，提出SuperWriter-Agent框架，通过引入结构化思考阶段，提高长文本生成的质量和一致性。基于该框架，训练了7B的SuperWriter-LM，并采用分层直接偏好优化（DPO）和蒙特卡洛树搜索（MCTS）进行优化。实证结果表明，SuperWriter-LM在多种基准测试中达到领先水平，并在自动评估和人工评估中超越了更大规模的基准模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）在长文本生成中面临挑战，包括连贯性、逻辑一致性和文本质量。</li>
<li>SuperWriter-Agent框架通过引入结构化思考阶段，增强长文本生成的质量和一致性。</li>
<li>构建了SuperWriter-LM，使用分层直接偏好优化（DPO）和蒙特卡洛树搜索（MCTS）进行训练和优化。</li>
<li>SuperWriter-LM在多种基准测试中表现优秀，超越大规模基准模型。</li>
<li>实证结果证明了SuperWriter-LM的有效性和优越性。</li>
<li>分层DPO方法对于提高文本生成质量具有重要意义。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04180">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-350699db7802966cd283e482f66fc7e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd7017a98e2a4136b192be26d92132c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b8f79ea31089a19e91dfa2a3c48b50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca497570e81c183cfeeb3ae59ca42905.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SkipGPT-Dynamic-Layer-Pruning-Reinvented-with-Token-Awareness-and-Module-Decoupling"><a href="#SkipGPT-Dynamic-Layer-Pruning-Reinvented-with-Token-Awareness-and-Module-Decoupling" class="headerlink" title="SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and   Module Decoupling"></a>SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and   Module Decoupling</h2><p><strong>Authors:Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen</strong></p>
<p>Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens, and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% of model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/SkipGPT">https://github.com/EIT-NLP/SkipGPT</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各项任务中取得了显著的性能，但由于其深层、多层架构，产生了巨大的计算成本。层剪枝作为一种缓解这些低效性的策略已经出现，但传统的静态剪枝方法忽略了LLM推理中固有的两个关键动态：（1）水平动态，其中令牌级别的异质性要求上下文感知的剪枝决策；（2）垂直动态，其中MLP和自我注意层的不同功能角色需要特定的组件剪枝策略。我们引入了SkipGPT，这是一个动态层剪枝框架，通过两个核心创新来优化计算资源分配：（1）全局令牌感知路由以优先处理关键令牌；（2）为MLP和自我注意组件提供解耦的剪枝策略。为了减轻训练不稳定的问题，我们提出了一个两阶段优化范式：首先是一个解耦的训练阶段，通过软参数化学习路由策略，以避免过早的剪枝决策，然后是参数高效的LoRA微调来恢复因层移除而影响性能。大量实验表明，SkipGPT在减少超过40%的模型参数的同时，在各项基准测试中实现了与原始密集模型相匹配或更高的性能。通过协调动态效率与保留的表现力，SkipGPT推动了可扩展、资源感知的LLM的实际部署。我们的代码公开在：<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/SkipGPT%E3%80%82">https://github.com/EIT-NLP/SkipGPT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04179v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在计算资源消耗方面存在显著不足，其深度多层架构导致高计算成本。为优化资源分配，提出SkipGPT动态层剪枝框架，通过全局令牌感知路由和独立剪枝策略，实现资源优化同时保持性能。引入两阶段优化范式，先进行非耦合训练学习路由策略，避免过早剪枝决策，再通过LoRA微调恢复性能。实验证明，SkipGPT在减少超过40%模型参数的同时，保持或提升性能表现。其动态效率与表达能力的平衡推动了资源感知型LLM的实际部署应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs面临计算成本高昂的问题，主要由于其深度多层架构。</li>
<li>SkipGPT框架旨在通过动态层剪枝来优化资源分配。</li>
<li>SkipGPT采用全局令牌感知路由和独立剪枝策略进行资源优化。</li>
<li>SkipGPT引入两阶段优化范式，先学习路由策略再进行微调，避免过早的剪枝决策影响性能。</li>
<li>SkipGPT能够在减少超过40%模型参数的同时保持或提升性能表现。</li>
<li>SkipGPT框架提高了LLM在实际部署中的实用性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04179">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-42561de84e90518ee2e6d85a81b544b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05febfd1e40419f40495ae39c8176232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2b40fd5d0124af951aed5ff1d7673f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7282fec8de52af9e06e419a8e5a66663.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VISCA-Inferring-Component-Abstractions-for-Automated-End-to-End-Testing"><a href="#VISCA-Inferring-Component-Abstractions-for-Automated-End-to-End-Testing" class="headerlink" title="VISCA: Inferring Component Abstractions for Automated End-to-End Testing"></a>VISCA: Inferring Component Abstractions for Automated End-to-End Testing</h2><p><strong>Authors:Parsa Alian, Martin Tang, Ali Mesbah</strong></p>
<p>Providing optimal contextual input presents a significant challenge for automated end-to-end (E2E) test generation using large language models (LLMs), a limitation that current approaches inadequately address. This paper introduces Visual-Semantic Component Abstractor (VISCA), a novel method that transforms webpages into a hierarchical, semantically rich component abstraction. VISCA starts by partitioning webpages into candidate segments utilizing a novel heuristic-based segmentation method. These candidate segments subsequently undergo classification and contextual information extraction via multimodal LLM-driven analysis, facilitating their abstraction into a predefined vocabulary of user interface (UI) components. This component-centric abstraction offers a more effective contextual basis than prior approaches, enabling more accurate feature inference and robust E2E test case generation. Our evaluations demonstrate that the test cases generated by VISCA achieve an average feature coverage of 92%, exceeding the performance of the state-of-the-art LLM-based E2E test generation method by 16%. </p>
<blockquote>
<p>提供最佳上下文输入对于使用大型语言模型（LLM）进行自动化端到端（E2E）测试生成来说是一个重大挑战，当前的方法对此类问题处理不足。本文介绍了视觉语义组件抽象（VISCA）这一新方法，它将网页转化为层次丰富、语义丰富的组件抽象。VISCA首先利用基于启发式的新方法将网页划分为候选段。这些候选段随后通过多模式LLM驱动的分析进行分类和上下文信息提取，便于它们被抽象为预定义的用户界面（UI）组件词汇。这种以组件为中心的抽象方法提供了比以前的方法更有效的上下文基础，能够实现更准确的功能推断和稳健的端到端测试案例生成。我们的评估表明，VISCA生成的测试案例的平均功能覆盖率为92%，超过了最新LLM端对端测试生成方法的性能，高出16%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04161v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了使用视觉语义组件抽象器（VISCA）将网页转化为层次化、语义丰富的组件抽象的方法，解决了当前大语言模型在端到端测试生成中面临的关键挑战。通过基于启发式的分割方法将网页划分为候选段，再通过多模态的语言模型进行分析和分类，从而实现精准的特征推断和鲁棒的端到端测试案例生成。实验表明，VISCA生成的测试案例平均特征覆盖率达到92%，优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VISCA通过将网页转化为组件抽象来解决大语言模型在端到端测试生成中的挑战。</li>
<li>该方法使用基于启发式的分割方法将网页划分为候选段。</li>
<li>VISCA利用多模态的语言模型对候选段进行分类和上下文信息提取。</li>
<li>组件中心的抽象方法提供了比先前方法更有效的上下文基础。</li>
<li>VISCA生成的测试案例实现了高特征覆盖率，平均达到92%。</li>
<li>VISCA在性能上超越了现有技术，提高了端到端测试生成的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04161">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aac14c3c318b522f1fb06736f332a5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dc7fbe3b3574bf58b7e844802e24446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b975b4fc832be5b1f45659d794374ec1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ae4677787c642fdcb6a23c36e902983.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aaae30b3ad90644dd2a988fced4687a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6695769443305db9e6b95213c3d323bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f96c4428bdd149c7fa45296f581f659.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd737c95ab75f953ed43961ced53136f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Dataset-for-Addressing-Patient’s-Information-Needs-related-to-Clinical-Course-of-Hospitalization"><a href="#A-Dataset-for-Addressing-Patient’s-Information-Needs-related-to-Clinical-Course-of-Hospitalization" class="headerlink" title="A Dataset for Addressing Patient’s Information Needs related to Clinical   Course of Hospitalization"></a>A Dataset for Addressing Patient’s Information Needs related to Clinical   Course of Hospitalization</h2><p><strong>Authors:Sarvesh Soni, Dina Demner-Fushman</strong></p>
<p>Patients have distinct information needs about their hospitalization that can be addressed using clinical evidence from electronic health records (EHRs). While artificial intelligence (AI) systems show promise in meeting these needs, robust datasets are needed to evaluate the factual accuracy and relevance of AI-generated responses. To our knowledge, no existing dataset captures patient information needs in the context of their EHRs. We introduce ArchEHR-QA, an expert-annotated dataset based on real-world patient cases from intensive care unit and emergency department settings. The cases comprise questions posed by patients to public health forums, clinician-interpreted counterparts, relevant clinical note excerpts with sentence-level relevance annotations, and clinician-authored answers. To establish benchmarks for grounded EHR question answering (QA), we evaluated three open-weight large language models (LLMs)–Llama 4, Llama 3, and Mixtral–across three prompting strategies: generating (1) answers with citations to clinical note sentences, (2) answers before citations, and (3) answers from filtered citations. We assessed performance on two dimensions: Factuality (overlap between cited note sentences and ground truth) and Relevance (textual and semantic similarity between system and reference answers). The final dataset contains 134 patient cases. The answer-first prompting approach consistently performed best, with Llama 4 achieving the highest scores. Manual error analysis supported these findings and revealed common issues such as omitted key clinical evidence and contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong benchmark for developing and evaluating patient-centered EHR QA systems, underscoring the need for further progress toward generating factual and relevant responses in clinical contexts. </p>
<blockquote>
<p>患者对其住院过程中的信息需求独特，可通过电子健康记录（EHRs）中的临床证据来满足这些需求。虽然人工智能（AI）系统在满足这些需求方面显示出潜力，但需要可靠的数据集来评估AI生成答案的事实准确性和相关性。据我们所知，目前没有数据集能够捕捉患者在电子健康记录背景下的信息需求。我们介绍了ArchEHR-QA数据集，它基于现实世界的患者案例，这些案例来自重症监护室和急诊科。这些案例包括患者向公共健康论坛提出的问题、医生解读的相应问题、相关的临床笔记摘录以及句子级别的相关性注释，以及医生撰写的答案。为了为基于EHR的问答（QA）建立基准，我们评估了三个开源的大型语言模型（LLMs）——Llama 4、Llama 3和Mixtral——跨越三种提示策略：生成（1）带有临床笔记句子的答案引用、（2）答案先于引用出现，以及（3）从过滤后的引用中生成答案。我们从两个维度进行评估：事实性（引用的笔记句子与真实答案的重叠程度）和相关性（系统与参考答案之间的文本和语义相似性）。最终数据集包含134个患者案例。答案优先的提示方法表现最为稳定，Llama 4取得最高分数。手动误差分析支持了这些发现，并揭示了常见的问题，如遗漏的关键临床证据和矛盾或虚构的内容。总体而言，ArchEHR-QA为开发和评估以患者为中心的EHR QA系统提供了强有力的基准，强调了需要在临床环境中生成事实和相关的答案方面取得进一步进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04156v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于电子健康记录（EHRs）的临床证据，患者对其住院过程有不同的信息需求。虽然人工智能（AI）系统在满足这些需求方面展现出潜力，但需要可靠的数据集来评估AI生成回答的准确性和相关性。目前尚无数据集能够捕捉患者信息需求与EHRs的关联。本研究推出ArchEHR-QA数据集，基于真实患者病例，涵盖重症监护室和急诊室的场景。包括患者向公共健康论坛提出的问题、医生解读的问题、相关临床笔记摘录以及医生撰写的答案。为了建立基于EHR的问答基准，我们评估了三种大型语言模型（LLMs），发现答案优先的提示策略效果最佳，其中Llama 4表现最优。ArchEHR-QA为开发评估以患者为中心的EHR问答系统提供了强有力的基准，突显了需要在临床环境中生成真实和相关的回应的进步需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>患者对其住院过程有独特的信息需求，可通过电子健康记录中的临床证据来满足这些需求。</li>
<li>ArchEHR-QA是一个基于真实患者病例的专家注释数据集，涵盖了重症监护室和急诊室的场景。</li>
<li>ArchEHR-QA包含患者问题、医生解读的问题、临床笔记的摘录和医生答案。</li>
<li>三种大型语言模型在ArchEHR-QA数据集上进行评估，发现答案优先的提示策略表现最佳。</li>
<li>Llama 4在评估中表现最优。</li>
<li>手动误差分析支持了评估结果，并揭示了常见的问题，如遗漏关键临床证据和存在矛盾或虚构的内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1231f45a9bf67f05b7e55ea34f905fdc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bad273e2400fc9542dabaff1a4d5544.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Establishing-Trustworthy-LLM-Evaluation-via-Shortcut-Neuron-Analysis"><a href="#Establishing-Trustworthy-LLM-Evaluation-via-Shortcut-Neuron-Analysis" class="headerlink" title="Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis"></a>Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis</h2><p><strong>Authors:Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao</strong></p>
<p>The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient ($\rho$) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: <a target="_blank" rel="noopener" href="https://github.com/GaryStack/Trustworthy-Evaluation">https://github.com/GaryStack/Trustworthy-Evaluation</a> </p>
<blockquote>
<p>大型语言模型（LLM）的发展依赖于可信的评估。然而，当前大多数评估都依赖于公共基准测试，这些基准测试容易受到数据污染问题的影响，从而严重损害评估的公正性。之前的研究致力于构建动态基准测试来解决污染问题。然而，不断构建新的基准测试成本高昂且循环往复。在这项工作中，我们的目标是通过分析受污染模型本身的机制来解决污染问题。通过我们的实验，我们发现受污染模型的过度估计可能是由于模型参数在训练过程中获得了捷径解决方案。我们进一步提出了一种通过比较和因果分析来识别捷径神经元的新方法。在此基础上，我们引入了一种名为“捷径神经元修补”的评估方法，以抑制捷径神经元。实验验证了我们方法在减轻污染方面的有效性。此外，我们的评估结果与最近发布的可信基准测试MixEval表现出强烈的线性相关性，斯皮尔曼系数（ρ）超过0.95。这种高相关性表明，我们的方法能够紧密揭示模型的真实能力，是值得信赖的。我们进行了进一步的实验，以证明我们的方法在各种基准测试和超参数设置中的通用性。代码地址：<a target="_blank" rel="noopener" href="https://github.com/GaryStack/Trustworthy-Evaluation">https://github.com/GaryStack/Trustworthy-Evaluation</a></p>
</blockquote>
<p><strong>简化版</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04142v1">PDF</a> Accepted to ACL 2025 Main Conference</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）的发展依赖于可靠的评估。当前大多数评估依赖于公共基准测试，但存在数据污染问题，影响公平性。本文旨在通过分析受污染的模型机制来解决污染问题。实验发现，模型污染的可能原因是参数在训练中获得了捷径解决方案。本文提出了一种通过比较和因果分析来识别捷径神经元的新方法，并引入了一种名为“捷径神经元修补”的评估方法来抑制捷径神经元。实验验证了我们方法的有效性，并与最新发布的可靠基准测试MixEval呈强线性相关，斯皮尔曼系数（ρ）超过0.95。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的评估需要可靠的方法。</li>
<li>公共基准测试存在数据污染问题，影响模型评估的公平性。</li>
<li>受污染的模型可能因参数在训练中获取捷径解决方案而导致。</li>
<li>提出了一种通过比较和因果分析识别捷径神经元的新方法。</li>
<li>引入了一种名为“捷径神经元修补”的评估方法来抑制捷径神经元。</li>
<li>实验验证该方法有效，与MixEval基准测试呈强线性相关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-daf88fefe69548e676a88c127aad6169.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2280c6915e427c4ebd5155151dcfa4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06c5ac3ec6550ccf929e9a6ecfd3bb04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e08be93dad3173221089712d620eb572.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MMR-V-What’s-Left-Unsaid-A-Benchmark-for-Multimodal-Deep-Reasoning-in-Videos"><a href="#MMR-V-What’s-Left-Unsaid-A-Benchmark-for-Multimodal-Deep-Reasoning-in-Videos" class="headerlink" title="MMR-V: What’s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos"></a>MMR-V: What’s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos</h2><p><strong>Authors:Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</strong></p>
<p>The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as “question frame”) and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities. </p>
<blockquote>
<p>视频的连续结构对多模态大型语言模型（MLLM）在定位多帧证据和进行多模态推理方面的能力提出了挑战。然而，现有的视频基准测试主要侧重于理解任务，这些任务只需要模型匹配问题中提到的帧（以下简称“问题帧”）并感知少数相邻帧。为了弥补这一空白，我们提出了MMR-V：视频多模态深度推理的基准测试。该基准测试具有以下特点：（1）长程多帧推理：模型需要推断和分析可能远离问题帧的证据帧。（2）超越感知：问题不能仅通过直接感知来回答，而是需要推理隐藏的信息。（3）可靠性：所有任务都是手动注释的，参考现实世界中用户的广泛理解，以符合普遍认知。（4）混淆性：精心设计的干扰注释策略，以减少模型的捷径。MMR-V包含317个视频和1,257个任务。我们的实验表明，当前模型在多模态推理方面仍然面临困难；即使表现最佳的o4-mini模型也仅达到52.5%的准确率。此外，当前的推理增强策略（如思维链和测试时间计算）带来的收益有限。进一步分析表明，多模态推理所需的思维链与文本推理不同，这部分解释了性能提升有限的原因。我们希望MMR-V能够激发对增强多模态推理能力的进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04141v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mmr-v.github.io/">https://mmr-v.github.io</a></p>
<p><strong>摘要</strong></p>
<p>视频序列结构对多模态大型语言模型（MLLMs）在定位多帧证据和进行多模态推理方面的能力构成挑战。然而，现有的视频基准测试主要关注理解任务，仅要求模型匹配问题中提到的帧（以下简称“问题帧”）并感知少数相邻帧。为解决这一差距，我们提出MMR-V：视频多模态深度推理的基准测试。该基准测试的特点包括：（1）长程多帧推理：模型需推断和分析可能与问题帧相距较远的证据帧。（2）超越感知：问题不能仅通过直接感知来回答，而需要推理隐藏信息。（3）可靠性：所有任务均经人工标注，参照广泛的实际用户理解，以符合通用感知。（4）混淆性：精心设计的干扰者标注策略，以减少模型捷径。MMR-V包含317个视频和1,257个任务。我们的实验表明，当前模型在多模态推理方面仍存在困难；即使表现最佳的o4-mini模型，准确率也仅为52.5%。此外，当前的推理增强策略（如思维链和测试时间计算）带来的收益有限。进一步分析表明，多模态推理所需的思维链与文本推理不同，这部分解释了性能提升有限的原因。我们希望MMR-V能激发对增强多模态推理能力的进一步研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频的序列结构对多模态语言模型的推理能力构成挑战，特别是在定位多帧证据方面。</li>
<li>现有视频基准测试主要关注理解任务，忽略了长程多帧推理和隐藏信息的推理。</li>
<li>MMR-V基准测试弥补了这一差距，要求模型进行长程多帧推理和复杂的思维链。</li>
<li>当前模型在MMR-V上的表现不佳，即使最好的模型准确率也仅为52.5%。</li>
<li>当前的推理增强策略对多模态推理的性能提升有限。</li>
<li>多模态推理与文本推理所需的思维链有所不同。</li>
<li>MMR-V基准测试有望激发对增强多模态推理能力的进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04141">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d9605454fe68ec6c42c13d1f54b4ad59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ac2b73c1ca05ab4310aeb96cdfc18c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cec1625179effab0b31a947c215ef67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-940181c5078ad798e878d4220edd6951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ce8dd5a60eadcd568f4fd39d77ad9c1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TRiSM-for-Agentic-AI-A-Review-of-Trust-Risk-and-Security-Management-in-LLM-based-Agentic-Multi-Agent-Systems"><a href="#TRiSM-for-Agentic-AI-A-Review-of-Trust-Risk-and-Security-Management-in-LLM-based-Agentic-Multi-Agent-Systems" class="headerlink" title="TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management   in LLM-based Agentic Multi-Agent Systems"></a>TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management   in LLM-based Agentic Multi-Agent Systems</h2><p><strong>Authors:Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis</strong></p>
<p>Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy&#x2F;security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment. </p>
<blockquote>
<p>基于大型语言模型（LLM）构建的Agentic AI系统，在多智能体配置中部署，正在重新定义企业和社会领域中的智能自主性、协作和决策。本文介绍了在LLM基于的智能体多智能体系统（AMAS）背景下对信任、风险和安全管理（TRiSM）的结构性分析。我们首先研究Agentic AI的概念基础，与传统AI代理的架构差异，以及能够实现可扩展的工具使用自主性的新兴系统设计。然后在Agentic AI框架中详细说明了TRiSM的四个支柱：治理、可解释性、ModelOps以及隐私&#x2F;安全，每个支柱都针对Agentic LLM进行了上下文化。我们确定了独特的威胁向量，并介绍了由案例研究支持的agentic AI应用程序的综合风险分类。此外，本文还调查了信任建立机制、透明度和监督技术，以及分布式LLM代理系统中的最新可解释性策略。此外，还介绍了评估信任、解释能力和以人为中心性能的指标，以及公开基准测试挑战。通过加密、对抗性防御以及遵守不断发展的AI法规来解决安全和隐私问题。本文最后提出了负责任的Agentic AI路线图，提出了新兴的多智能体系统与稳健的TRiSM原则对齐的研究方向，以实现安全、负责和透明的部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04133v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于大型语言模型（LLM）构建的Agentic AI系统，在多智能体配置中的部署正在重新定义企业和社会领域的智能自主性、协作和决策。本文介绍了在LLM基础上构建的智能多智能体系统（AMAS）中的信任、风险和安全管理的结构化分析。文章首先探讨了Agentic AI的概念基础，与传统AI智能体的架构差异，以及新兴的系统设计如何赋能可伸缩的工具使用自主性。接着详细阐述了Agentic AI框架中的TRiSM，包括治理、可解释性、ModelOps以及隐私&#x2F;安全等四个支柱，并针对agentic LLM进行上下文分析。文章识别了独特的威胁向量，为agentic AI应用程序引入了一个全面的风险分类，并通过案例研究说明了现实世界的漏洞。此外，本文还调查了构建信任机制、透明度和监督技术，以及分布式LLM智能体系统中的最新可解释性策略。同时，评估信任、解释性和以人为中心性能的指标与公开基准测试挑战也被审查。安全和隐私通过加密、对抗性防御和遵守不断发展的AI法规来解决。本文最后提出了负责任的Agentic AI路线图，为新兴的多智能体系统与稳健的TRiSM原则对齐，以实现安全、负责和透明的部署提供了研究方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Agentic AI系统正改变企业和社会领域的智能自主性、协作和决策方式。</li>
<li>TRiSM在LLM基础的多智能体系统（AMAS）中具有重要作用。</li>
<li>Agentic AI具有独特的威胁向量和风险分类，需要重视现实世界的漏洞和安全问题。</li>
<li>文章探讨了构建信任机制、透明度和监督技术的方法。</li>
<li>评估和衡量信任、解释性和以人为中心性能的指标受到关注。</li>
<li>文章强调了加密、对抗性防御以及遵守AI法规在解决隐私和安全问题中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04133">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7178f1471c5e5966789601f9f3024519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a75b4b184e72b7e02c601b3415e8725.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2c071e7d7cf409fb5dd95114b364c72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7558c023dfbba1b26c85107c8e9bd6f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Guided-Speculative-Inference-for-Efficient-Test-Time-Alignment-of-LLMs"><a href="#Guided-Speculative-Inference-for-Efficient-Test-Time-Alignment-of-LLMs" class="headerlink" title="Guided Speculative Inference for Efficient Test-Time Alignment of LLMs"></a>Guided Speculative Inference for Efficient Test-Time Alignment of LLMs</h2><p><strong>Authors:Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis</strong></p>
<p>We propose Guided Speculative Inference (GSI), a novel algorithm for efficient reward-guided decoding in large language models. GSI combines soft best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We derive a theoretical bound on the KL divergence between our induced distribution and the optimal policy. In experiments on reasoning benchmarks (MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative decoding (Liao et al., 2025), and in certain settings even outperforms soft best-of-$n$ with $\pi_B$. The code is available at <a target="_blank" rel="noopener" href="https://github.com/j-geuter/GSI">https://github.com/j-geuter/GSI</a> . </p>
<blockquote>
<p>我们提出了引导式推测推理（GSI），这是一种针对大型语言模型中高效奖励引导解码的新型算法。GSI结合了软性最佳n测试时间缩放、奖励模型r(x，y)以及来自小型辅助模型πs(y|x)的推测样本。我们在主要模型πB下，通过软性最佳n的倾斜策略πβ，B(y|x)≈πB(y|x)exp(βr(x，y))进行了证明。我们推导出了诱导分布与最优策略之间KL散度的理论界限。在推理基准测试（MATH500、OlympiadBench、Minerva Math）的实验中，我们的方法达到了比标准软性最佳n与πs和奖励引导推测解码（Liao等人，2025）更高的准确率，并且在某些情况下甚至超越了软性最佳n与πB的表现。代码可在<a target="_blank" rel="noopener" href="https://github.com/j-geuter/GSI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/j-geuter/GSI找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04118v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong><br>文本提出了一种名为引导式推测推断（GSI）的新算法，该算法针对大型语言模型的奖励导向解码进行高效化处理。通过软性最优解试验时间和奖励模型的引导结合使用辅助模型来模拟对模型的假设样本。该算法能够逼近最优倾斜策略，并在数学推理基准测试中表现出更高的准确性。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>提出了引导式推测推断（GSI）算法，旨在优化大型语言模型的奖励导向解码效率。<br>*GSI结合了软性最优解试验时间缩放、奖励模型r(x,y)和来自小型辅助模型的推测样本πS(y∣x)。<br>*GSI能够逼近最优倾斜策略πβ,B(y∣x)，该策略是软性最优解在主导模型πB下的产物。<br>*提供了与最优策略之间的KL散度的理论界限。<br>*在基准测试MATH500、OlympiadBench和Minerva Math上的实验显示，与标准软性最优解及奖励引导推测解码相比，GSI方法的准确率更高。<br>*在某些设置中，GSI甚至超越了使用πB的软性最优解的表现。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1c903355e0ef97ab170bc1cf8e0622b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1321812e159a8a4a109cbc818566cbd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c178d2bade050355024047b3464ba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7913d25280da196b467d9a3af3421035.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rectified-Sparse-Attention"><a href="#Rectified-Sparse-Attention" class="headerlink" title="Rectified Sparse Attention"></a>Rectified Sparse Attention</h2><p><strong>Authors:Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei</strong></p>
<p>Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at <a target="_blank" rel="noopener" href="https://aka.ms/ReSA-LM">https://aka.ms/ReSA-LM</a>. </p>
<blockquote>
<p>高效长序列生成对于大型语言模型来说是一个关键挑战。虽然最近的稀疏解码方法提高了效率，但它们存在键值缓存不匹配的问题，其中近似误差会累积并降低生成质量。在这项工作中，我们提出了“修正稀疏注意力”（ReSA）方法，这是一种简单有效的方法，它将块稀疏注意力与周期性的密集修正相结合。通过定期在密集前向传递中刷新键值缓存，ReSA限制误差累积并保持与预训练分布的匹配。在涵盖数学推理、语言建模和检索任务的实验中，ReSA展示了在显著提高效率的同时实现近乎无损的生成质量。值得注意的是，在解码256K序列长度的情况下，ReSA提供高达2.42倍的端到端加速，使其成为可扩展的长上下文推理的实际解决方案。代码可通过<a target="_blank" rel="noopener" href="https://aka.ms/ReSA-LM%E8%8E%B7%E5%8F%96%E3%80%82">https://aka.ms/ReSA-LM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了Rectified Sparse Attention（ReSA）方法，结合了块稀疏注意力与周期性的密集修正，以提高大型语言模型的效率。通过固定间隔使用密集前向传播刷新KV缓存，ReSA控制误差累积并保持与预训练分布的对齐。实验表明，ReSA在保持近乎无损的生成质量的同时，显著提高了效率，特别是在解码长达256K序列长度时，实现了高达2.42倍端到端的加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReSA方法结合了块稀疏注意力与周期性的密集修正，旨在提高大型语言模型的效率。</li>
<li>ReSA通过刷新KV缓存来控制误差累积，并保持与预训练分布的对齐。</li>
<li>实验结果显示，ReSA在多种任务上实现了近乎无损的生成质量。</li>
<li>ReSA显著提高了大型语言模型的效率，特别是在处理长序列时。</li>
<li>ReSA方法实现了端到端的加速，最高可达2.42倍。</li>
<li>代码已经公开可用，方便研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04108">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f14f9f544f438beb7014945975ebe25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444310ccb2c11c069bc95e23501ceaee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae63ec6753fc6a9fc1b03c3d2f8ce5a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-221c6edf0bb7eba7c687fba805adff58.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="AmbiK-Dataset-of-Ambiguous-Tasks-in-Kitchen-Environment"><a href="#AmbiK-Dataset-of-Ambiguous-Tasks-in-Kitchen-Environment" class="headerlink" title="AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment"></a>AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment</h2><p><strong>Authors:Anastasiia Ivanova, Eva Bakaeva, Zoya Volovikova, Alexey K. Kovalev, Aleksandr I. Panov</strong></p>
<p>As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at <a target="_blank" rel="noopener" href="https://github.com/cog-model/AmbiK-dataset">https://github.com/cog-model/AmbiK-dataset</a>. </p>
<blockquote>
<p>作为实体代理的一部分，大型语言模型（LLM）通常根据用户的自然语言指令用于行为规划。然而，在真实环境中处理模糊的指令仍然是LLM面临的一个挑战。已经提出了各种任务模糊性检测方法。但是，由于它们在不同的数据集上进行测试，且没有通用的基准，因此很难对它们进行比较。因此，我们提出了AmbiK（厨房环境中模糊任务），这是针对厨房环境中机器人所接收的模糊指令的完全文本数据集。AmbiK的收集是在LLM的帮助下完成的，并且经过了人工验证。它包含1000对模糊任务及其明确的对应任务，按模糊类型（人类偏好、常识知识、安全）分类，包括环境描述、澄清问题和答案、用户意图和任务计划，总共2000个任务。我们希望AmbiK能够让研究人员对模糊检测方法进行统一的比较。AmbiK可在<a target="_blank" rel="noopener" href="https://github.com/cog-model/AmbiK-dataset%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/cog-model/AmbiK-dataset获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04089v1">PDF</a> ACL 2025 (Main Conference)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在处理用户自然语言指令时的行为规划部分发挥重要作用。然而，处理真实环境中的模糊指令仍是LLM面临的挑战。尽管已提出多种任务模糊性检测方法，但由于测试数据集不同，缺乏通用基准，难以进行比较。为此，我们推出AmbiK数据集——厨房环境中针对机器人的模糊指令的完全文本数据集。AmbiK利用LLM协助收集，经过人类验证。它包含1000对模糊任务及其明确的对应任务，按模糊类型（人类偏好、常识知识、安全）分类，包含环境描述、澄清问题及答案、用户意图和任务计划，总共2000个任务。我们期望AmbiK能够促进研究者对模糊检测方法进行统一比较。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在处理用户自然语言指令时用于行为规划。</li>
<li>处理真实环境中的模糊指令是LLM面临的挑战。</li>
<li>多种任务模糊性检测方法已被提出，但缺乏通用基准，难以比较。</li>
<li>推出AmbiK数据集，包含针对机器人的厨房环境中的模糊指令的完全文本数据。</li>
<li>AmbiK数据集利用LLM协助收集，并经人类验证。</li>
<li>AmbiK包含1000对模糊任务及其明确的对应任务，分类包含环境描述、澄清问题答案等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04089">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a74200f453c7699db14558ea5a550de8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42e2996993027f7c8a6ef9db309a2100.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a31350f4c215f4b9c58b77565320e9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8398847576ef8856222fb4d3352968d5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="VisCoder-Fine-Tuning-LLMs-for-Executable-Python-Visualization-Code-Generation"><a href="#VisCoder-Fine-Tuning-LLMs-for-Executable-Python-Visualization-Code-Generation" class="headerlink" title="VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation"></a>VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code   Generation</h2><p><strong>Authors:Yuansheng Ni, Ping Nie, Kai Zou, Xiang Yue, Wenhu Chen</strong></p>
<p>Large language models (LLMs) often struggle with visualization tasks like plotting diagrams, charts, where success depends on both code correctness and visual semantics. Existing instruction-tuning datasets lack execution-grounded supervision and offer limited support for iterative code correction, resulting in fragile and unreliable plot generation. We present VisCode-200K, a large-scale instruction tuning dataset for Python-based visualization and self-correction. It contains over 200K examples from two sources: (1) validated plotting code from open-source repositories, paired with natural language instructions and rendered plots; and (2) 45K multi-turn correction dialogues from Code-Feedback, enabling models to revise faulty code using runtime feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation protocol to assess iterative repair, demonstrating the benefits of feedback-driven learning for executable, visually accurate code generation. </p>
<blockquote>
<p>大型语言模型（LLM）在绘图、制作图表等可视化任务方面常常遇到困难，这些任务的成功取决于代码正确性和视觉语义两方面。现有的指令调整数据集缺乏执行基础的监督，对迭代代码修正的支持有限，导致绘图生成脆弱且不可靠。我们推出了VisCode-200K，这是一个用于Python可视化及自我修正的大型指令调整数据集。它包含超过20万个来自两个来源的示例：1）来自开源存储库的经过验证的绘图代码，与自然语言指令和渲染图配对；2）来自Code-Feedback的4.5万个多轮修正对话，使模型能够使用运行时反馈修正错误代码。我们对Qwencod v2.5进行微调以在VisCode-200K上创建VisCoder，并在PandasPlotBench上对其进行评估。VisCoder显著优于强大的开源基线，并接近专有模型（如GPT-4o mini）的性能。我们还采用自我调试评估协议来评估迭代修复，证明了反馈驱动学习对于可执行且视觉准确的代码生成的益处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM在处理可视化任务时面临挑战，如绘图和图表生成等。现有指令调整数据集缺乏执行监督，对迭代代码修正的支持有限，导致绘图生成脆弱且不可靠。为此，我们推出了VisCode-200K数据集，用于Python可视化指令调整和自我修正。它通过两个来源包含超过20万个示例：一是来自开源仓库的验证绘图代码，与自然语言指令和渲染的图表配对；二是来自Code-Feedback的4.5万个多轮修正对话，使模型能够利用运行时反馈修正错误代码。我们对Qwen2.5-Coder-Instruct进行微调，创建VisCoder并对其进行评估。VisCoder显著优于强大的开源基准测试，并接近专有模型如GPT-4o-mini的性能。我们还采用自我调试评估协议来评估迭代修复，证明反馈驱动学习在可视化编程中的重要性。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在处理可视化任务时存在挑战，特别是在确保代码正确性和视觉语义方面。</li>
<li>现有指令调整数据集缺乏执行监督和对迭代代码修正的支持。</li>
<li>VisCode-200K数据集包含超过20万个用于Python可视化任务的示例。</li>
<li>VisCode-200K由验证的绘图代码、自然语言指令和渲染的图表组成，以及来自Code-Feedback的多轮修正对话。</li>
<li>VisCoder通过微调Qwen2.5-Coder-Instruct创建，显著优于开源基准测试并接近专有模型性能。</li>
<li>自我调试评估协议证明反馈驱动学习在可视化编程中的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-41df43db3f46645a371ed857d83f9c4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-365c28382105a20af39bd9e319e37893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f871b0dcd8e40ab08fa83386ec85f318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7fa4cb14244cf882e1ff00903583ef5.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="STELLA-Towards-Protein-Function-Prediction-with-Multimodal-LLMs-Integrating-Sequence-Structure-Representations"><a href="#STELLA-Towards-Protein-Function-Prediction-with-Multimodal-LLMs-Integrating-Sequence-Structure-Representations" class="headerlink" title="STELLA: Towards Protein Function Prediction with Multimodal LLMs   Integrating Sequence-Structure Representations"></a>STELLA: Towards Protein Function Prediction with Multimodal LLMs   Integrating Sequence-Structure Representations</h2><p><strong>Authors:Hongwang Xiao, Wenjun Lin, Xi Chen, Hui Wang, Kai Chen, Jiashan Li, Yuancheng Sun, Sicheng Dai, Boya Wu, Qiwei Ye</strong></p>
<p>Protein biology focuses on the intricate relationships among sequences, structures, and functions. Deciphering protein functions is crucial for understanding biological processes, advancing drug discovery, and enabling synthetic biology applications. Since protein sequences determine tertiary structures, which in turn govern functions, integrating sequence and structure information is essential for accurate prediction of protein functions. Traditional protein language models (pLMs) have advanced protein-related tasks by learning representations from large-scale sequence and structure data. However, pLMs are limited in integrating broader contextual knowledge, particularly regarding functional modalities that are fundamental to protein biology. In contrast, large language models (LLMs) have exhibited outstanding performance in contextual understanding, reasoning, and generation across diverse domains. Leveraging these capabilities, STELLA is proposed as a multimodal LLM integrating protein sequence-structure representations with general knowledge to address protein function prediction. Through multimodal instruction tuning (MMIT) using the proposed OPI-Struc dataset, STELLA achieves state-of-the-art performance in two function-related tasks-functional description prediction (FP) and enzyme-catalyzed reaction prediction (EP). This study highlights the potential of multimodal LLMs as an alternative paradigm to pLMs to advance protein biology research. </p>
<blockquote>
<p>蛋白质生物学专注于序列、结构和功能之间的复杂关系。解析蛋白质功能是理解生物过程、推动药物发现和实现合成生物学应用的关键。由于蛋白质序列决定三级结构，而三级结构又控制功能，因此整合序列和结构信息对于准确预测蛋白质功能至关重要。传统的蛋白质语言模型（pLMs）已经通过从大规模序列和结构数据中学习表征来推进与蛋白质相关的任务。然而，pLMs在整合更广泛的上下文知识方面存在局限性，特别是在蛋白质生物学中至关重要的功能模式方面。相比之下，大型语言模型（LLMs）在上下文理解、推理和跨域生成方面表现出卓越的性能。利用这些能力，STELLA被提出为一种多模式LLM，它整合蛋白质序列-结构表征和通用知识来解决蛋白质功能预测问题。通过使用提出的OPI-Struc数据集进行多模式指令调整（MMIT），STELLA在两个与功能相关的任务——功能描述预测（FP）和酶催化反应预测（EP）中达到了最先进的性能。这项研究突出了多模式LLM作为推进蛋白质生物学研究的替代范式的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>蛋白质生物学研究序列、结构与功能之间的复杂关系。解析蛋白质功能对于理解生物过程、推进药物发现和实现合成生物学应用至关重要。传统蛋白质语言模型（pLMs）通过从大规模序列和结构数据中学习表示来推进蛋白质相关任务，但难以整合更广泛的上下文知识，特别是蛋白质生物学中的基本功能模式。相比之下，大型语言模型（LLMs）在上下文理解、推理和生成方面表现出卓越性能。本研究提出利用这些能力的多模式LLM STELLA，通过整合蛋白质序列-结构表示和通用知识来预测蛋白质功能。使用所提出的OPI-Struc数据集进行多模式指令调整（MMIT），STELLA在功能描述预测和酶催化反应预测两个任务上实现了最先进的性能。本研究突出了多模式LLM作为推进蛋白质生物学研究的一种替代方法的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>蛋白质生物学关注序列、结构与功能之间的关系。</li>
<li>解析蛋白质功能对理解生物过程、药物发现和合成生物学应用至关重要。</li>
<li>传统蛋白质语言模型（pLMs）难以整合更广泛的上下文知识。</li>
<li>大型语言模型（LLMs）在上下文理解、推理和生成方面表现出卓越性能。</li>
<li>STELLA是一个多模式LLM，通过整合蛋白质序列-结构表示和通用知识来预测蛋白质功能。</li>
<li>STELLA使用OPI-Struc数据集进行多模式指令调整（MMIT），在功能描述预测和酶催化反应预测任务上表现先进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77339c99bc04ff317b52856b5a2ec630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f72ffc3f41650acbbfc68e50427f8668.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8b7dbf8f8bd9e135392c9cfe1310cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7d40f4ad4d4a8b7bc5673d05e01b8f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining"><a href="#Scaling-Transformers-for-Discriminative-Recommendation-via-Generative-Pretraining" class="headerlink" title="Scaling Transformers for Discriminative Recommendation via Generative   Pretraining"></a>Scaling Transformers for Discriminative Recommendation via Generative   Pretraining</h2><p><strong>Authors:Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen, Bing Wang, Xiaoyi Zeng</strong></p>
<p>Discriminative recommendation tasks, such as CTR (click-through rate) and CVR (conversion rate) prediction, play critical roles in the ranking stage of large-scale industrial recommender systems. However, training a discriminative model encounters a significant overfitting issue induced by data sparsity. Moreover, this overfitting issue worsens with larger models, causing them to underperform smaller ones. To address the overfitting issue and enhance model scalability, we propose a framework named GPSD (\textbf{G}enerative \textbf{P}retraining for \textbf{S}calable \textbf{D}iscriminative Recommendation), drawing inspiration from generative training, which exhibits no evident signs of overfitting. GPSD leverages the parameters learned from a pretrained generative model to initialize a discriminative model, and subsequently applies a sparse parameter freezing strategy. Extensive experiments conducted on both industrial-scale and publicly available datasets demonstrate the superior performance of GPSD. Moreover, it delivers remarkable improvements in online A&#x2F;B tests. GPSD offers two primary advantages: 1) it substantially narrows the generalization gap in model training, resulting in better test performance; and 2) it leverages the scalability of Transformers, delivering consistent performance gains as models are scaled up. Specifically, we observe consistent performance improvements as the model dense parameters scale from 13K to 0.3B, closely adhering to power laws. These findings pave the way for unifying the architectures of recommendation models and language models, enabling the direct application of techniques well-established in large language models to recommendation models. </p>
<blockquote>
<p>判别式推荐任务，如点击率（CTR）和转化率（CVR）预测，在大规模工业推荐系统的排序阶段起着关键作用。然而，训练判别模型会遇到由数据稀疏引起的严重过拟合问题。而且，随着模型规模的增大，过拟合问题会恶化，导致大型模型性能不如小型模型。为了解决过拟合问题并增强模型的可扩展性，我们提出了一个名为GPSD（为可扩展判别推荐设计的生成预训练框架）的框架，该框架借鉴了生成训练，生成训练没有明显的过拟合迹象。GPSD利用从预训练的生成模型中学习的参数来初始化判别模型，随后应用稀疏参数冻结策略。在工业级和公开数据集上进行的广泛实验表明，GPSD具有出色的性能。此外，它在在线A&#x2F;B测试中取得了显著的改进。GPSD具有两个主要优点：1）它大大缩小了模型训练中的泛化差距，从而提高了测试性能；2）它利用变压器的可扩展性，随着模型的扩展，性能不断提高。具体来说，我们观察到模型密集参数从13K到0.3B的扩展过程中，性能持续提高，遵循幂律。这些发现为统一推荐模型和语言模型的架构铺平了道路，使得在推荐模型中直接应用大型语言模型中成熟的技术成为可能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03699v1">PDF</a> KDD’25</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种名为GPSD的框架，它通过将生成式预训练融入判别式推荐任务来解决大规模工业推荐系统中的数据稀疏性问题及模型过拟合问题。GPSD利用预训练生成模型的参数初始化判别模型，并采用稀疏参数冻结策略。实验证明，GPSD在在线A&#x2F;B测试和公开数据集上的表现均优于传统方法，具有缩小模型训练中的泛化差距和提升模型规模拓展性两大优势。此外，GPSD实现了推荐模型与语言模型的统一架构融合，可直接应用自然语言处理领域的技术成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>判别式推荐任务如CTR和CVR预测在工业推荐系统中排名阶段扮演重要角色，但面临数据稀疏导致的过拟合问题。</li>
<li>过拟合问题随模型规模的扩大而加剧，导致大型模型性能下降。</li>
<li>GPSD框架结合生成式预训练来解决判别式模型的过拟合问题，利用生成模型的参数初始化判别模型，并应用稀疏参数冻结策略。</li>
<li>实验证明GPSD在在线A&#x2F;B测试和公开数据集上的表现优于传统方法。</li>
<li>GPSD具有缩小模型训练中的泛化差距和提升模型规模拓展性两大优势。</li>
<li>GPSD实现了推荐模型与语言模型的架构统一，可直接应用自然语言处理领域的技术成果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03699">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a2e7db7c121cd55894d4525a1e8cfdfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93b25a6e587a5e4dcc308214bcbf235f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb62b7a0f425bc97d23a0f906f96c4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a83dc258dfa35a1e2d9f32f4127037b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07069a6dda775e41c701e5656437d9dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02e2203e613cc6d80b415ec0cffd2631.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping"><a href="#MINT-Multimodal-Instruction-Tuning-with-Multimodal-Interaction-Grouping" class="headerlink" title="MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping"></a>MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping</h2><p><strong>Authors:Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang</strong></p>
<p>Recent advances in multimodal foundation models have achieved state-of-the-art performance across a range of tasks. These breakthroughs are largely driven by new pre-training paradigms that leverage large-scale, unlabeled multimodal data, followed by instruction fine-tuning on curated labeled datasets and high-quality prompts. While there is growing interest in scaling instruction fine-tuning to ever-larger datasets in both quantity and scale, our findings reveal that simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, we observe that grouping tasks by the common interactions across modalities, such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities, encourages the models to learn transferrable skills within a group while suppressing interference from mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly effective task-grouping strategy based on the type of multimodal interaction. We demonstrate that the proposed method greatly outperforms existing task grouping baselines for multimodal instruction tuning, striking an effective balance between generalization and specialization. </p>
<blockquote>
<p>最近的多模态基础模型的进展已经在一系列任务上达到了最先进的性能。这些突破主要得益于新的预训练模式，它利用大规模的无标签多模态数据，然后在精选的标记数据集上进行指令微调并使用高质量提示。虽然人们对扩大指令微调以涵盖更多数量和规模的数据集越来越感兴趣，但我们的研究结果表明，仅仅增加指令调整任务的数量并不总能带来更好的性能。相反，我们观察到，通过跨模态的常见交互对任务进行分组，如发现冗余的共享信息、优先选择与独特信息结合的模态，或需要协同融合以从两种模态中发现新信息，这鼓励模型学习一组内的可迁移技能，同时抑制了不匹配任务的干扰。为此，我们引入了MINT，这是一种基于多模态交互类型的简单而有效的任务分组策略。我们证明，对于多模态指令调整，所提出的方法大大优于现有的任务分组基线，在通用性和专业化之间达到了有效的平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02308v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模的多模态基础模型的新进展在各种任务上取得了最先进的性能。这些突破主要得益于新的预训练模式，该模式利用大规模的无标签多模态数据，随后在精选的有标签数据集和高质量提示上进行指令微调。尽管人们对扩大指令微调的数据集规模和数量越来越感兴趣，但我们的研究结果表明，简单地增加指令调整任务的数量并不一定能带来更好的性能。相反，我们通过按跨模态的通用交互对任务进行分组，如发现冗余的共享信息、优先选择与独特信息结合的模态或要求从两种模态中发现新信息的协同融合等方法，鼓励模型学习组内可迁移的技能，同时抑制来自不相关任务的干扰。为此，我们引入了基于多模态交互类型的简单而有效的任务分组策略MINT。我们证明，该方法在多模态指令调整的任务分组上大大优于现有基线，在推广和专业化之间达到了有效的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态基础模型的新进展在各种任务上取得了卓越性能。</li>
<li>预训练模式结合大规模无标签多模态数据和指令微调是提高性能的关键。</li>
<li>单纯增加指令调整任务的数量并不总能带来更好的性能。</li>
<li>通过按跨模态通用交互对任务进行分组，可以鼓励模型学习可迁移技能。</li>
<li>发现冗余的共享信息、优先结合独特信息的模态和协同融合是提高多模态模型性能的有效方法。</li>
<li>引入了一种基于多模态交互类型的简单而有效的任务分组策略MINT。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02308">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-78f0ea984b188fa8f886c4fabcb06cca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-320e29dcdf9de3a29ff9c850167e4ec7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbbfbcf76e25da5223c10c5b736a488.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e50ba509cddc17f4767d2be3e3ea694f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab60ebc7690d7c235db9dbfc46519a2b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-06-06  OWMM-Agent Open World Mobile Manipulation With Multi-modal Agentic Data   Synthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f72ffc3f41650acbbfc68e50427f8668.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-06  Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
