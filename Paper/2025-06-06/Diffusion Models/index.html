<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-06  Sounding that Object Interactive Object-Aware Image to Audio Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2bcb7ee303bd3489e19af35c3bdf82bd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-06-更新"><a href="#2025-06-06-更新" class="headerlink" title="2025-06-06 更新"></a>2025-06-06 更新</h1><h2 id="Sounding-that-Object-Interactive-Object-Aware-Image-to-Audio-Generation"><a href="#Sounding-that-Object-Interactive-Object-Aware-Image-to-Audio-Generation" class="headerlink" title="Sounding that Object: Interactive Object-Aware Image to Audio Generation"></a>Sounding that Object: Interactive Object-Aware Image to Audio Generation</h2><p><strong>Authors:Tingle Li, Baihe Huang, Xiaobin Zhuang, Dongya Jia, Jiawei Chen, Yuping Wang, Zhuo Chen, Gopala Anumanchipalli, Yuxuan Wang</strong></p>
<p>Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: <a target="_blank" rel="noopener" href="https://tinglok.netlify.app/files/avobject/">https://tinglok.netlify.app/files/avobject/</a> </p>
<blockquote>
<p>为复杂的视听场景生成准确的声音是一个挑战，特别是在存在多个对象和声音源的情况下。在本文中，我们提出了一种<strong>交互式对象感知音频生成</strong>模型，该模型以图像中用户选择的视觉对象为基础生成声音。我们的方法将对象中心学习集成到条件潜在扩散模型中，该模型通过多模式注意力学习将图像区域与相应的声音相关联。在测试阶段，我们的模型采用图像分割技术，允许用户以<strong>对象</strong>级别交互地生成声音。我们从理论上验证了我们注意力的机制，它在功能上近似于测试时的分割掩膜，确保生成的音频与所选对象对齐。定量和定性评估表明，我们的模型优于基线模型，在对象和与其相关的声音之间实现了更好的对齐。项目页面：<a target="_blank" rel="noopener" href="https://tinglok.netlify.app/files/avobject/">https://tinglok.netlify.app/files/avobject/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04214v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种交互式对象感知音频生成模型，该模型在用户选择的图像中的视觉对象基础上生成声音。通过集成对象中心学习于条件潜在扩散模型，该模型学会将图像区域与其相应的声音通过多模式注意力相关联。测试时，模型采用图像分割让用户能够互动地生成对象级别的声音。理论和实验验证显示，模型的注意力机制可近似测试时的分割掩模，确保生成的音频与所选对象相符。此模型优于基线模型，实现对象与其相关声音之间更好的对齐。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了交互式对象感知音频生成模型，将声音生成与图像中的视觉对象相关联。</li>
<li>集成对象中心学习于条件潜在扩散模型，学会关联图像区域与对应声音。</li>
<li>通过多模态注意力机制实现对象级别的声音生成。</li>
<li>注意力机制理论上可近似测试时的分割掩模，确保音频与所选对象对齐。</li>
<li>模型在定量和定性评估中均表现出优于基线模型的效果。</li>
<li>模型实现了对象与其相关声音之间的良好对齐。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04214">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d09ddcce20dd9e038e83aad8b92cebba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d3fd2af370ee0baa78bc9d2722bd8349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-587fad64eee5cc1ad7aef106fd32fb9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-248b247824ac7848d52b5c4c62cbd11c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Diffusion-Domain-Teacher-Diffusion-Guided-Domain-Adaptive-Object-Detector"><a href="#Diffusion-Domain-Teacher-Diffusion-Guided-Domain-Adaptive-Object-Detector" class="headerlink" title="Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object   Detector"></a>Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object   Detector</h2><p><strong>Authors:Boyong He, Yuxiang Ji, Zhuoyue Tan, Liaoni Wu</strong></p>
<p>Object detectors often suffer a decrease in performance due to the large domain gap between the training data (source domain) and real-world data (target domain). Diffusion-based generative models have shown remarkable abilities in generating high-quality and diverse images, suggesting their potential for extracting valuable feature from various domains. To effectively leverage the cross-domain feature representation of diffusion models, in this paper, we train a detector with frozen-weight diffusion model on the source domain, then employ it as a teacher model to generate pseudo labels on the unlabeled target domain, which are used to guide the supervised learning of the student model on the target domain. We refer to this approach as Diffusion Domain Teacher (DDT). By employing this straightforward yet potent framework, we significantly improve cross-domain object detection performance without compromising the inference speed. Our method achieves an average mAP improvement of 21.2% compared to the baseline on 6 datasets from three common cross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic}, surpassing the current state-of-the-art (SOTA) methods by an average of 5.7% mAP. Furthermore, extensive experiments demonstrate that our method consistently brings improvements even in more powerful and complex models, highlighting broadly applicable and effective domain adaptation capability of our DDT. The code is available at <a target="_blank" rel="noopener" href="https://github.com/heboyong/Diffusion-Domain-Teacher">https://github.com/heboyong/Diffusion-Domain-Teacher</a>. </p>
<blockquote>
<p>对象检测器通常由于训练数据（源域）和现实世界数据（目标域）之间的域差距较大而性能下降。基于扩散的生成模型在生成高质量和多样化图像方面表现出了显著的能力，这表明它们从各种域中提取有价值特征的潜力。为了有效利用扩散模型的跨域特征表示，本文在源域上训练了一个带有冻结权重扩散模型的检测器，然后将其作为教师模型在未经标记的目标域上生成伪标签，用于指导目标域上学生模型的监督学习。我们将这种方法称为扩散域教师（DDT）。通过采用这种简单而强大的框架，我们在不牺牲推理速度的情况下，显著提高了跨域目标检测性能。我们的方法在三个常见的跨域检测基准测试（跨摄像头、Syn2Real、Real2Artistic）的6个数据集上，与基线相比，平均mAP提高了21.2%，并且平均比当前最佳方法高出5.7%的mAP。此外，大量实验表明，即使在更强大和复杂的模型中，我们的方法也始终能带来改进，这突显了我们DDT方法广泛的适用性和有效的域适应能力。代码可用在<a target="_blank" rel="noopener" href="https://github.com/heboyong/Diffusion-Domain-Teacher%E3%80%82">https://github.com/heboyong/Diffusion-Domain-Teacher。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04211v1">PDF</a> MM2024 poster, with appendix and codes</p>
<p><strong>Summary</strong><br>     扩散模型在跨域目标检测中具有显著优势。通过训练源域上的冻结权重扩散模型作为教师模型，对目标域生成伪标签进行引导学习，提高了跨域目标检测的准确性，同时不损失推理速度。该方法在多个数据集上平均提高了21.2%的mAP性能，超过了现有技术水平平均提高了5.7%。这是一种强大而灵活的方法，能够广泛应用在各种领域适应的场景中。更多细节和代码可在GitHub仓库中找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型具有强大的跨域特征表示能力，能够在目标检测中发挥显著优势。</li>
<li>通过训练源域上的冻结权重扩散模型作为教师模型，生成伪标签用于目标域的学习，有效提高了跨域目标检测的准确性。</li>
<li>该方法在不损失推理速度的前提下实现了显著的性能提升。</li>
<li>在多个数据集上进行的实验表明，该方法平均提高了21.2%的mAP性能，显著优于现有技术。</li>
<li>该方法不仅适用于基础模型，而且在更强大和复杂的模型中也能带来一致的性能提升。</li>
<li>该方法具有广泛的应用性，能够应用于各种领域适应的场景中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04211">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0cfa696d379b4868fd3b72cf60c692a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-569fda5147ded2ee69c238c1e801d02a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a34f19d3b2aa15e79afd07812b25e15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a61d28d153473cf17b9c3cd7c27c951c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb26f85076d53dd0f4144e04be3108de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-235d1b9608d16d25d9996bcbcbff868f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c442d51b93f58f8a517188c0feea68cd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Image-Editing-As-Programs-with-Diffusion-Models"><a href="#Image-Editing-As-Programs-with-Diffusion-Models" class="headerlink" title="Image Editing As Programs with Diffusion Models"></a>Image Editing As Programs with Diffusion Models</h2><p><strong>Authors:Yujia Hu, Songhua Liu, Zhenxiong Tan, Xingyi Yang, Xinchao Wang</strong></p>
<p>While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/YujiaHu1109/IEAP">https://github.com/YujiaHu1109/IEAP</a>. </p>
<blockquote>
<p>虽然扩散模型在文本到图像生成方面取得了显著的成功，但在指令驱动图像编辑方面仍面临重大挑战。我们的研究突出了一个关键挑战：这些模型特别难以处理涉及大量布局变化的结构性不一致编辑。为了弥补这一差距，我们引入了“图像编辑作为程序（IEAP）”，这是一个基于扩散变压器（DiT）架构的统一图像编辑框架。IEAP的核心是通过还原论的方法来进行指令编辑，将复杂的编辑指令分解为原子操作序列。每个操作都是通过轻量级适配器实现的，该适配器共享相同的DiT主干，并针对特定类型的编辑进行专业化。这些操作通过视觉语言模型（VLM）驱动的代理进行编程，协同支持任意和结构上不一致的转换。通过这种方式模块化并排序编辑，IEAP在各种编辑任务上表现稳健，无论是简单的调整还是大幅的结构变化。大量实验表明，IEAP在各种编辑场景的基准测试中显著优于最新方法。在这些评估中，我们的框架在准确性和语义保真度方面表现出卓越性能，尤其是针对复杂的多步骤指令。代码可在<a target="_blank" rel="noopener" href="https://github.com/YujiaHu1109/IEAP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YujiaHu1109/IEAP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04158v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在文本到图像生成方面取得了显著的成功，但在指令驱动图像编辑方面面临重大挑战，特别是处理涉及重大布局变化的结构性不一致编辑。为缓解这一问题，我们提出了图像编辑作为程序（IEAP）的概念，这是一个基于Diffusion Transformer（DiT）架构的统一图像编辑框架。IEAP通过简约主义的视角进行指令编辑，将复杂的编辑指令分解为一系列原子操作序列。每个操作都通过轻量级适配器实现，该适配器共享相同的DiT主干并专用于特定类型的编辑。这些操作由视觉语言模型（VLM）驱动的代理进行编程，协同支持任意和结构性不一致的转换。通过这种方式模块化并编辑序列，IEAP能够广泛应用于各种编辑任务，从简单调整到重大结构性变化。在广泛的实验中，IEAP在多种编辑场景的基准测试中显著优于最新方法。特别是在复杂的多步骤指令下，我们的框架提供了更高的准确性和语义保真度。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/YujiaHu1109/IEAP">链接</a>中找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在文本到图像生成上表现出色，但在处理涉及重大布局变化的指令驱动图像编辑方面面临挑战。</li>
<li>提出的图像编辑作为程序（IEAP）框架基于Diffusion Transformer（DiT）架构构建。</li>
<li>IEAP通过分解为原子操作序列来处理复杂的编辑指令。</li>
<li>每个操作都是通过轻量级适配器实现的，该适配器共享相同的DiT主干并专为特定类型的编辑设计。</li>
<li>这些操作由视觉语言模型（VLM）驱动的代理进行编程，支持任意和结构性不一致的转换。</li>
<li>IEAP通过模块化并编辑序列的方式，可以广泛应用于各种编辑任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ad86c87163e278763ffa5576bf263fe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e0d2f7f8c55071cabb96627406d1122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb8be5176f81834f3c5ee6d4cff374b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1308ede2dd6e8477b20cf362e8c248fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffCAP-Diffusion-based-Cumulative-Adversarial-Purification-for-Vision-Language-Models"><a href="#DiffCAP-Diffusion-based-Cumulative-Adversarial-Purification-for-Vision-Language-Models" class="headerlink" title="DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision   Language Models"></a>DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision   Language Models</h2><p><strong>Authors:Jia Fu, Yongtao Wu, Yihang Chen, Kunyu Peng, Xiao Zhang, Volkan Cevher, Sepideh Pashami, Anders Holst</strong></p>
<p>Vision Language Models (VLMs) have shown remarkable capabilities in multimodal understanding, yet their susceptibility to perturbations poses a significant threat to their reliability in real-world applications. Despite often being imperceptible to humans, these perturbations can drastically alter model outputs, leading to erroneous interpretations and decisions. This paper introduces DiffCAP, a novel diffusion-based purification strategy that can effectively neutralize adversarial corruptions in VLMs. We observe that adding minimal noise to an adversarially corrupted image significantly alters its latent embedding with respect to VLMs. Building on this insight, DiffCAP cumulatively injects random Gaussian noise into adversarially perturbed input data. This process continues until the embeddings of two consecutive noisy images reach a predefined similarity threshold, indicating a potential approach to neutralize the adversarial effect. Subsequently, a pretrained diffusion model is employed to denoise the stabilized image, recovering a clean representation suitable for the VLMs to produce an output. Through extensive experiments across six datasets with three VLMs under varying attack strengths in three task scenarios, we show that DiffCAP consistently outperforms existing defense techniques by a substantial margin. Notably, DiffCAP significantly reduces both hyperparameter tuning complexity and the required diffusion time, thereby accelerating the denoising process. Equipped with strong theoretical and empirical support, DiffCAP provides a robust and practical solution for securely deploying VLMs in adversarial environments. </p>
<blockquote>
<p>视觉语言模型（VLMs）在多模态理解方面表现出了显著的能力，然而它们容易受到扰动的影响，这对它们在现实世界应用中的可靠性构成了重大威胁。尽管这些扰动通常对人类来说是不可察觉的，但它们会极大地改变模型的输出，导致错误的解释和决策。本文介绍了一种基于扩散的新净化策略DiffCAP，它可以有效地中和视觉语言模型中的对抗性腐败。我们观察到，向对抗性腐蚀的图像添加少量噪声会显著改变其相对于VLMs的潜在嵌入。基于这一见解，DiffCAP累积地向对抗性扰动输入数据注入随机高斯噪声。这个过程会继续进行，直到两个连续的带噪声图像的特征嵌入达到预定的相似性阈值，这表明了一种可能中和对抗效应的方法。随后，使用预训练的扩散模型对稳定的图像进行去噪，恢复一个适合视觉语言模型产生输出的清洁表示。通过六个数据集在三种任务场景下的广泛实验，使用三种视觉语言模型并在不同攻击强度下进行测试，我们表明DiffCAP在防御技术方面始终显著优于现有技术。值得注意的是，DiffCAP显著降低了超参数调整复杂性和所需的扩散时间，从而加速了去噪过程。凭借强大的理论和实证支持，DiffCAP为在敌对环境中安全部署视觉语言模型提供了稳健实用的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03933v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种名为DiffCAP的新颖扩散式净化策略，能有效中和视觉语言模型（VLMs）中的对抗性污染。通过向受攻击的图像添加最小噪声，改变其潜在嵌入，DiffCAP累积地向受干扰的输入数据注入随机高斯噪声，直到两个连续噪声图像的特征嵌入达到预定的相似性阈值。随后，利用预训练的扩散模型对稳定图像进行去噪处理，使其恢复干净状态以供VLMs处理。实验证明，DiffCAP在多种数据集和任务场景下均显著优于现有防御技术，大幅降低了超参数调整复杂性和所需的扩散时间，加速了去噪过程。这为在敌对环境中安全部署VLMs提供了稳健实用的解决方案。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>VLMs在多媒体理解方面表现出卓越的能力，但其对干扰的敏感性对其在现实世界应用中的可靠性构成威胁。</li>
<li>DiffCAP是一种基于扩散的净化策略，能有效中和VLMs中的对抗性污染。</li>
<li>DiffCAP通过向受攻击的图像添加噪声来改变其潜在嵌入，然后累积注入随机高斯噪声，直到图像稳定。</li>
<li>使用预训练的扩散模型对稳定图像进行去噪处理，恢复干净状态以供VLMs处理。</li>
<li>DiffCAP在多个数据集和任务场景下的实验表现优于现有防御技术。</li>
<li>DiffCAP降低了超参数调整复杂性和所需的扩散时间，加速了去噪过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03933">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab1e7295ff1fdd836a3f542eb1e54317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4af427612d52d2f49c52350f7d7194e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Facial-Appearance-Capture-at-Home-with-Patch-Level-Reflectance-Prior"><a href="#Facial-Appearance-Capture-at-Home-with-Patch-Level-Reflectance-Prior" class="headerlink" title="Facial Appearance Capture at Home with Patch-Level Reflectance Prior"></a>Facial Appearance Capture at Home with Patch-Level Reflectance Prior</h2><p><strong>Authors:Yuxuan Han, Junfeng Lyu, Kuan Sheng, Minghao Que, Qixuan Zhang, Lan Xu, Feng Xu</strong></p>
<p>Existing facial appearance capture methods can reconstruct plausible facial reflectance from smartphone-recorded videos. However, the reconstruction quality is still far behind the ones based on studio recordings. This paper fills the gap by developing a novel daily-used solution with a co-located smartphone and flashlight video capture setting in a dim room. To enhance the quality, our key observation is to solve facial reflectance maps within the data distribution of studio-scanned ones. Specifically, we first learn a diffusion prior over the Light Stage scans and then steer it to produce the reflectance map that best matches the captured images. We propose to train the diffusion prior at the patch level to improve generalization ability and training stability, as current Light Stage datasets are in ultra-high resolution but limited in data size. Tailored to this prior, we propose a patch-level posterior sampling technique to sample seamless full-resolution reflectance maps from this patch-level diffusion model. Experiments demonstrate our method closes the quality gap between low-cost and studio recordings by a large margin, opening the door for everyday users to clone themselves to the digital world. Our code will be released at <a target="_blank" rel="noopener" href="https://github.com/yxuhan/DoRA">https://github.com/yxuhan/DoRA</a>. </p>
<blockquote>
<p>现有的人脸捕捉方法可以从智能手机拍摄的视频中重建出合理的人脸反射。然而，重建质量仍然远远落后于基于工作室录制的方法。本文填补了这个空白，开发了一种新型的日常解决方案，采用定位智能手机和昏暗房间中的闪光灯视频捕捉设置。为了提高质量，我们的关键观察是，在工作室扫描的反射图的分布范围内解决面部反射图的问题。具体来说，我们首先学习Light Stage扫描的扩散先验知识，然后引导它生成与捕获的图像最匹配的反射图。我们建议在补丁级别训练扩散先验知识，以提高泛化能力和训练稳定性，因为当前的Light Stage数据集虽然具有超高的分辨率，但在数据大小上有限制。针对这种先验知识，我们提出了一种补丁级后采样技术，可以从这种补丁级扩散模型中无缝采样全分辨率反射图。实验证明，我们的方法大大提高了低成本和工作室录制之间的质量差距，为普通用户打开了克隆自己到数字世界的大门。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/yxuhan/DoRA%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/yxuhan/DoRA中发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03478v1">PDF</a> ACM Transactions on Graphics (Proc. of SIGGRAPH), 2025. Code:   <a target="_blank" rel="noopener" href="https://github.com/yxuhan/DoRA">https://github.com/yxuhan/DoRA</a>; Project Page: <a target="_blank" rel="noopener" href="https://yxuhan.github.io/DoRA">https://yxuhan.github.io/DoRA</a></p>
<p><strong>Summary</strong><br>     现有面部捕捉方法可从智能手机视频重建面部反射，但质量仍落后于基于工作室录制的方案。本文填补这一空白，提出一种日常使用的解决方案，采用智能手机与闪光灯视频捕捉设置，在暗室中进行。为提高质量，我们观察到解决面部反射映射需要在工作室扫描的数据分布内。具体而言，我们首先学习Light Stage扫描的光扩散先验知识，然后将其引导生成与捕获图像最匹配的反射映射图。我们提出在补丁级别训练扩散先验知识以提高通用性和训练稳定性，因为当前Light Stage数据集超高分辨率但数据量有限。针对此先验知识，我们提出了一种补丁级后采样技术，可从该补丁级扩散模型中无缝采样全分辨率反射映射图。实验证明，我们的方法大幅缩小了低成本与工作室录制之间的差距，为普通用户克隆自己进入数字世界打开了大门。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有面部捕捉方法虽能从智能手机视频重建面部反射，但质量低于工作室录制。</li>
<li>本文提出一种新型解决方案，采用智能手机与闪光灯视频捕捉在暗室中进行，旨在提高质量。</li>
<li>通过学习Light Stage扫描的光扩散先验知识来解决面部反射映射问题。</li>
<li>在补丁级别训练扩散模型以提高通用性和训练稳定性，应对当前数据集分辨率高但数据量有限的问题。</li>
<li>提出补丁级后采样技术，生成无缝全分辨率反射映射图。</li>
<li>实验证明该方法显著缩小了低成本与工作室录制之间的差距。</li>
<li>该方法为用户将其自身克隆到数字世界提供了可能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6e5dc102e5a43f68f09205f542bb3a18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f6bf205e7a3fb0824cc40c44bfc951f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44bf5e8623c7ff07e9bd0ac29f25540f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Generalized-Diffusion-Detector-Mining-Robust-Features-from-Diffusion-Models-for-Domain-Generalized-Detection"><a href="#Generalized-Diffusion-Detector-Mining-Robust-Features-from-Diffusion-Models-for-Domain-Generalized-Detection" class="headerlink" title="Generalized Diffusion Detector: Mining Robust Features from Diffusion   Models for Domain-Generalized Detection"></a>Generalized Diffusion Detector: Mining Robust Features from Diffusion   Models for Domain-Generalized Detection</h2><p><strong>Authors:Boyong He, Yuxiang Ji, Qianwen Ye, Zhuoyue Tan, Liaoni Wu</strong></p>
<p>Domain generalization (DG) for object detection aims to enhance detectors’ performance in unseen scenarios. This task remains challenging due to complex variations in real-world applications. Recently, diffusion models have demonstrated remarkable capabilities in diverse scene generation, which inspires us to explore their potential for improving DG tasks. Instead of generating images, our method extracts multi-step intermediate features during the diffusion process to obtain domain-invariant features for generalized detection. Furthermore, we propose an efficient knowledge transfer framework that enables detectors to inherit the generalization capabilities of diffusion models through feature and object-level alignment, without increasing inference time. We conduct extensive experiments on six challenging DG benchmarks. The results demonstrate that our method achieves substantial improvements of 14.0% mAP over existing DG approaches across different domains and corruption types. Notably, our method even outperforms most domain adaptation methods without accessing any target domain data. Moreover, the diffusion-guided detectors show consistent improvements of 15.9% mAP on average compared to the baseline. Our work aims to present an effective approach for domain-generalized detection and provide potential insights for robust visual recognition in real-world scenarios. The code is available at <a target="_blank" rel="noopener" href="https://github.com/heboyong/Generalized-Diffusion-Detector">https://github.com/heboyong/Generalized-Diffusion-Detector</a>. </p>
<blockquote>
<p>目标检测领域的域泛化（DG）旨在提高检测器在未见过场景中的性能。由于现实世界应用中的复杂变化，这一任务仍然具有挑战性。最近，扩散模型在场景生成方面表现出卓越的能力，这激发了我们在DG任务中探索其潜力的想法。我们的方法不是生成图像，而是在扩散过程中提取多步中间特征，以获得用于广义检测的域不变特征。此外，我们提出了一个有效的知识转移框架，使检测器能够通过特征和对象级的对齐，继承扩散模型的泛化能力，而无需增加推理时间。我们在六个具有挑战性的DG基准测试集上进行了广泛的实验。结果表明，我们的方法在跨不同领域和腐败类型的情况下，较现有的DG方法提高了显著的14.0%的mAP。值得注意的是，我们的方法甚至在没有访问任何目标域数据的情况下超过了大多数域自适应方法。此外，与基线相比，扩散引导的检测器的平均mAP提高了稳定的15.9%。我们的工作旨在提供一种有效的域泛化检测方法和为真实世界场景中的稳健视觉识别提供潜在见解。代码可在<a target="_blank" rel="noopener" href="https://github.com/heboyong/Generalized-Diffusion-Detector%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/heboyong/Generalized-Diffusion-Detector找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02101v2">PDF</a> CVPR2025 camera-ready version with supplementary material</p>
<p><strong>Summary</strong><br>     基于扩散模型的领域泛化（DG）方法被应用于目标检测，以提高在未见过场景中的检测性能。该方法通过提取扩散过程中的多步中间特征来获得领域不变特征，并提出一种有效的知识迁移框架，使检测器能够通过特征和对象级别的对齐，继承扩散模型的泛化能力，且不会增加推理时间。在六个具有挑战性的DG基准测试中，该方法较现有DG方法实现了显著的性能提升。即使不访问任何目标域数据，该方法也比大多数域自适应方法表现更出色。此外，与基线相比，扩散引导的检测器平均提高了15.9%的mAP。此工作旨在为领域泛化检测提供一种有效方法，并为真实场景中的稳健视觉识别提供潜在启示。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型被探索应用于领域泛化（DG）任务，旨在提高目标检测在未见场景中的性能。</li>
<li>通过提取扩散过程中的中间特征来获得领域不变特征。</li>
<li>提出一种知识迁移框架，使检测器能够继承扩散模型的泛化能力，且不影响推理时间。</li>
<li>在多个DG基准测试中实现了显著的性能提升，较现有方法提高14.0% mAP。</li>
<li>无需访问目标域数据即可表现出良好的性能，优于多数域自适应方法。</li>
<li>扩散引导的检测器与基线相比平均提高了15.9%的mAP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-28654c8baee6e66434c4b2dcf610eeb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-147f4172615ca3b78a423e27f9729e1e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18446421d66621980554478005971aac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-250b33af38d8b2448f5342a92bbb6ec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9fca7cdf09e8bcb308ac1dfbbfa5b85.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VCT-Training-Consistency-Models-with-Variational-Noise-Coupling"><a href="#VCT-Training-Consistency-Models-with-Variational-Noise-Coupling" class="headerlink" title="VCT: Training Consistency Models with Variational Noise Coupling"></a>VCT: Training Consistency Models with Variational Noise Coupling</h2><p><strong>Authors:Gianluigi Silvestri, Luca Ambrogioni, Chieh-Hsin Lai, Yuhta Takida, Yuki Mitsufuji</strong></p>
<p>Consistency Training (CT) has recently emerged as a strong alternative to diffusion models for image generation. However, non-distillation CT often suffers from high variance and instability, motivating ongoing research into its training dynamics. We propose Variational Consistency Training (VCT), a flexible and effective framework compatible with various forward kernels, including those in flow matching. Its key innovation is a learned noise-data coupling scheme inspired by Variational Autoencoders, where a data-dependent encoder models noise emission. This enables VCT to adaptively learn noise-todata pairings, reducing training variance relative to the fixed, unsorted pairings in classical CT. Experiments on multiple image datasets demonstrate significant improvements: our method surpasses baselines, achieves state-of-the-art FID among non-distillation CT approaches on CIFAR-10, and matches SoTA performance on ImageNet 64 x 64 with only two sampling steps. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sony/vct">https://github.com/sony/vct</a>. </p>
<blockquote>
<p>一致性训练（CT）最近作为图像生成的扩散模型的强大替代方案而出现。然而，非蒸馏CT经常遭受高方差和不稳定性的困扰，这促使人们对其训练动态进行持续研究。我们提出了变分一致性训练（VCT），这是一个灵活有效的框架，可与各种前向内核兼容，包括流匹配中的内核。其关键创新在于受到变分自编码器的启发而设计的一种学习噪声数据耦合方案，其中数据依赖的编码器对噪声排放进行建模。这使得VCT能够自适应地学习噪声到数据的配对，相对于经典CT中的固定且未排序的配对，降低了训练方差。在多个图像数据集上的实验证明了显著的改进：我们的方法超越了基线，在CIFAR-10上在非蒸馏CT方法中实现了最先进的FID，并在仅有两次采样步骤的ImageNet 64 x 64上达到了最先进的表现。代码可在<a target="_blank" rel="noopener" href="https://github.com/sony/vct%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sony/vct找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18197v2">PDF</a> 23 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为变分一致性训练（VCT）的新方法，它是一致性训练（CT）的改进版本，用于图像生成。VCT采用灵活的框架，兼容各种前向核，包括流匹配。其关键创新在于引入了基于变分自编码器的噪声数据耦合方案，使VCT能够自适应地学习噪声与数据的配对，从而降低了训练方差。实验表明，该方法在多个图像数据集上实现了显著改进，超过了基线方法，并在CIFAR-10上达到了非蒸馏一致性训练的最佳FID得分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>变分一致性训练（VCT）是一致性训练（CT）的改进版本，专门用于图像生成。</li>
<li>VCT引入了一种基于变分自编码器的噪声数据耦合方案，这是其关键创新。</li>
<li>该方案使VCT能够自适应地学习噪声与数据的配对，降低训练方差。</li>
<li>VCT兼容各种前向核，包括流匹配。</li>
<li>实验表明，VCT在多个图像数据集上实现了性能改进，超过了基线方法。</li>
<li>VCT在CIFAR-10数据集上达到了非蒸馏一致性训练的最佳FID得分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18197">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e54f70302875475ef264d8c22ad6a8a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bcb7ee303bd3489e19af35c3bdf82bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f24e7c19c5820715af2afb4c79b2f0b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diffusing-DeBias-Synthetic-Bias-Amplification-for-Model-Debiasing"><a href="#Diffusing-DeBias-Synthetic-Bias-Amplification-for-Model-Debiasing" class="headerlink" title="Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing"></a>Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing</h2><p><strong>Authors:Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino</strong></p>
<p>Deep learning model effectiveness in classification tasks is often challenged by the quality and quantity of training data whenever they are affected by strong spurious correlations between specific attributes and target labels. This results in a form of bias affecting training data, which typically leads to unrecoverable weak generalization in prediction. This paper aims at facing this problem by leveraging bias amplification with generated synthetic data: we introduce Diffusing DeBias (DDB), a novel approach acting as a plug-in for common methods of unsupervised model debiasing exploiting the inherent bias-learning tendency of diffusion models in data generation. Specifically, our approach adopts conditional diffusion models to generate synthetic bias-aligned images, which replace the original training set for learning an effective bias amplifier model that we subsequently incorporate into an end-to-end and a two-step unsupervised debiasing approach. By tackling the fundamental issue of bias-conflicting training samples memorization in learning auxiliary models, typical of this type of techniques, our proposed method beats current state-of-the-art in multiple benchmark datasets, demonstrating its potential as a versatile and effective tool for tackling bias in deep learning models. </p>
<blockquote>
<p>深度学习模型在分类任务中的有效性常常受到训练数据的质量和数量的挑战，尤其是在特定属性与目标标签之间存在强烈虚假关联时。这导致了一种影响训练数据的偏见，通常会导致预测中不可挽回的弱泛化。本文针对这一问题，利用生成合成数据的偏见放大来解决：我们引入了Diffusing DeBias（DDB），这是一种新型方法，作为对常见无监督模型去偏置方法的插件，利用数据生成中扩散模型固有的偏见学习倾向。具体来说，我们的方法采用条件扩散模型生成合成偏见对齐图像，这些图像替代原始训练集，用于学习有效的偏见放大模型，随后我们将其纳入端到端和两步无监督去偏置方法中。通过解决学习辅助模型中偏见冲突训练样本记忆的根本问题，这是此类技术的一个典型特征，我们提出的方法在多个基准数据集上超过了当前最佳水平，证明了其作为解决深度学习模型中偏见问题的通用和有效工具的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09564v4">PDF</a> 18 Pages, 9 Figures</p>
<p><strong>Summary</strong><br>深度学习方法在分类任务中的有效性常受训练数据的质量和数量的影响，特别是当这些数据中存在特定属性和目标标签之间的强烈偶然性关联时。这种情况会导致训练数据产生偏见，通常会导致预测中的不可恢复的弱泛化能力。本文旨在通过利用生成的合成数据放大偏见来应对这一问题：我们引入了Diffusing DeBias（DDB），这是一种新方法，可作为通用无监督模型去偏方法的插件。具体而言，我们的方法采用条件扩散模型生成合成偏见对齐图像，这些图像会替代原始训练集来学习有效的偏见放大模型，随后我们将其纳入端到端和两步无监督去偏方法。通过解决辅助模型学习中存在的偏见冲突训练样本记忆问题，我们的方法在多基准数据集上均优于当前的最佳方法，表明其在深度学习方法中作为通用和有效工具解决偏见问题的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习方法在分类任务中面临由训练数据的偏见导致的问题。</li>
<li>强烈的偶然性关联会影响训练数据的质量和数量。</li>
<li>现有技术中存在偏见冲突训练样本记忆问题。</li>
<li>本研究提出了一种新的方法Diffusing DeBias（DDB），旨在解决上述问题。</li>
<li>DDB利用条件扩散模型生成合成偏见对齐图像来替代原始训练集。</li>
<li>DDB可以纳入端到端和两步无监督去偏方法中使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3585947a03016df5900b771a56f00ee7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b15415704f30c46406a4917cc1dad33f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933c567d8dcfbdba004f7c3d209f2fb9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-67c51184135a005d25430a4dfa01f5e0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-06  A Novel Data Augmentation Approach for Automatic Speaking Assessment on   Opinion Expressions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d5d7858763222744160cf971cece3026.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-06-06  Pseudo-Simulation for Autonomous Driving
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
