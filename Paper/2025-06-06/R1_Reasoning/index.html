<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f72ffc3f41650acbbfc68e50427f8668.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    73 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-06-æ›´æ–°"><a href="#2025-06-06-æ›´æ–°" class="headerlink" title="2025-06-06 æ›´æ–°"></a>2025-06-06 æ›´æ–°</h1><h2 id="Does-Thinking-More-always-Help-Understanding-Test-Time-Scaling-in-Reasoning-Models"><a href="#Does-Thinking-More-always-Help-Understanding-Test-Time-Scaling-in-Reasoning-Models" class="headerlink" title="Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models"></a>Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models</h2><p><strong>Authors:Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi</strong></p>
<p>Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like â€œWaitâ€ or â€œLet me rethinkâ€ can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to â€œoverthinkingâ€. To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from â€œmore thinkingâ€ are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models. </p>
<blockquote>
<p>å…³äºæ¨ç†æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆä¾‹å¦‚OpenAI o1ã€DeepSeek R1ï¼‰çš„æœ€æ–°è¶‹åŠ¿å¼•å‘äº†ä¸€ç§æ™®éçš„è§‚å¿µï¼Œå³ä½¿ç”¨è¯¸å¦‚â€œç­‰ä¸€ä¸‹â€æˆ–â€œè®©æˆ‘å†æ€è€ƒä¸€ä¸‹â€ä¹‹ç±»çš„æç¤ºæ¥æ‰©å±•æ€ç»´è½¨è¿¹å¯ä»¥æé«˜æ€§èƒ½ã€‚è¿™å°±è‡ªç„¶åœ°æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šåœ¨æµ‹è¯•æ—¶æ€è€ƒæ›´å¤šæ˜¯å¦çœŸçš„ä¼šå¯¼è‡´æ›´å¥½çš„æ¨ç†ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹å’ŒåŸºå‡†è¿›è¡Œäº†è¯¦ç»†çš„å®è¯ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºåˆå§‹æ€§èƒ½ä¼šéšç€é¢å¤–çš„æ€è€ƒè€Œæœ‰æ‰€æé«˜ï¼Œä½†éšåä¼šç”±äºâ€œè¿‡åº¦æ€è€ƒâ€è€Œä¸‹é™ã€‚ä¸ºäº†ç†è§£è¿™ç§éå•è°ƒè¶‹åŠ¿ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªç®€å•çš„æ¦‚ç‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¡¨æ˜ï¼Œé¢å¤–çš„æ€è€ƒä¼šå¢åŠ è¾“å‡ºæ–¹å·®ï¼Œä»è€Œäº§ç”Ÿæ”¹è¿›çš„æ¨ç†é”™è§‰ï¼Œè€Œå®é™…ä¸Šä¼šç ´åç²¾åº¦ã€‚å› æ­¤ï¼Œâ€œæ›´å¤šæ€è€ƒâ€æ‰€è§‚å¯Ÿåˆ°çš„æ”¶ç›Šå¹¶ä¸æ˜¯æ¨ç†èƒ½åŠ›çœŸæ­£æé«˜çš„æŒ‡ç¤ºå™¨ï¼Œè€Œæ˜¯æºäºæ¨¡å‹ä¸ç¡®å®šæ€§å’Œè¯„ä¼°æŒ‡æ ‡ä¹‹é—´è”ç³»æ‰€äº§ç”Ÿçš„ä¼ªç°è±¡ã€‚è¿™è¡¨æ˜é€šè¿‡å»¶é•¿æ€è€ƒæ¥è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾å¹¶ä¸æ˜¯æœ‰æ•ˆåˆ©ç”¨æ¨ç†é¢„ç®—çš„æœ‰æ•ˆæ–¹æ³•ã€‚è®¤è¯†åˆ°è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å—åˆ°Best-of-Né‡‡æ ·çš„å¯å‘ï¼Œå¼•å…¥äº†ä¸€ç§æ›¿ä»£çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•â€”â€”å¹³è¡Œæ€è€ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸åŒçš„æ¨ç†é¢„ç®—å†…ç”Ÿæˆå¤šä¸ªç‹¬ç«‹çš„æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆï¼Œä¸å»¶é•¿æ€è€ƒç›¸æ¯”ï¼Œå‡†ç¡®åº¦é«˜å‡ºäº†é«˜è¾¾20%ã€‚è¿™ä¸ºæ¨ç†æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æµ‹è¯•æ—¶é—´å°ºåº¦ä¸Šçš„æœ€æ–°è¶‹åŠ¿è¡¨æ˜ï¼Œé€šè¿‡å¢åŠ æ€è€ƒè½¨è¿¹ï¼ˆå¦‚ä½¿ç”¨â€œç­‰ä¸€ä¸‹â€æˆ–â€œè®©æˆ‘å†æƒ³ä¸€æƒ³â€ç­‰æç¤ºï¼‰å¯ä»¥æé«˜æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚ä½†ç»è¿‡è¯¦ç»†å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°é¢å¤–çš„æ€è€ƒæœ€åˆä¼šæé«˜æ€§èƒ½ï¼Œéšåå´ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¿™æ˜¯ç”±äºâ€œè¿‡åº¦æ€è€ƒâ€æ‰€è‡´ã€‚ç®€å•æ¦‚ç‡æ¨¡å‹æ˜¾ç¤ºï¼Œé¢å¤–çš„æ€è€ƒå¢åŠ äº†è¾“å‡ºæ–¹å·®ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç§æ”¹è¿›çš„é”™è§‰æ¨ç†ï¼Œæœ€ç»ˆå´ç ´åäº†ç²¾åº¦ã€‚å› æ­¤ï¼Œâ€œæ›´å¤šæ€è€ƒâ€æ‰€è§‚å¯Ÿåˆ°çš„æ”¶ç›Šå¹¶éçœŸæ­£åæ˜ æ¨ç†èƒ½åŠ›çš„æé«˜ï¼Œè€Œæ˜¯æºäºæ¨¡å‹ä¸ç¡®å®šæ€§ä¸è¯„ä»·æŒ‡æ ‡ä¹‹é—´çš„è”ç³»æ‰€äº§ç”Ÿçš„ç»“æœã€‚è¿™æç¤ºæˆ‘ä»¬ï¼Œé€šè¿‡å»¶é•¿æ€è€ƒæ—¶é—´æ¥è¿›è¡Œæµ‹è¯•æ—¶é—´å°ºåº¦ç¼©æ”¾å¹¶ä¸æ˜¯æœ‰æ•ˆåˆ©ç”¨æ¨ç†é¢„ç®—çš„æœ‰æ•ˆæ–¹å¼ã€‚ä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†å—Best-of-Né‡‡æ ·å¯å‘çš„å¹¶è¡Œæ€è€ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨åŒä¸€æ¨ç†é¢„ç®—å†…ç”Ÿæˆå¤šä¸ªç‹¬ç«‹æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„å“åº”ï¼Œä¸å»¶é•¿æ€è€ƒæ—¶é—´ç›¸æ¯”ï¼Œæé«˜äº†é«˜è¾¾20%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸ºæµ‹è¯•æ—¶é—´å°ºåº¦çš„æ¨ç†æ¨¡å‹æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¢åŠ æ€è€ƒè½¨è¿¹ï¼ˆä½¿ç”¨æç¤ºï¼‰å¯ä»¥æé«˜æ¨ç†æ¨¡å‹çš„åˆå§‹æ€§èƒ½ã€‚</li>
<li>é¢å¤–çš„æ€è€ƒä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¿™æ˜¯å› ä¸ºâ€œè¿‡åº¦æ€è€ƒâ€ã€‚</li>
<li>ç®€å•æ¦‚ç‡æ¨¡å‹æ­ç¤ºé¢å¤–æ€è€ƒå¢åŠ è¾“å‡ºæ–¹å·®ï¼Œäº§ç”Ÿæ”¹è¿›é”™è§‰è€Œé™ä½ç²¾åº¦ã€‚</li>
<li>â€œæ›´å¤šæ€è€ƒâ€çš„æ”¶ç›Šå¹¶éçœŸæ­£åæ˜ æ¨ç†èƒ½åŠ›çš„æé«˜ï¼Œè€Œæ˜¯æºäºæ¨¡å‹ä¸ç¡®å®šæ€§ä¸è¯„ä»·æŒ‡æ ‡ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>é€šè¿‡å»¶é•¿æ€è€ƒæ—¶é—´æ¥è¿›è¡Œæµ‹è¯•æ—¶é—´å°ºåº¦ç¼©æ”¾å¹¶éæœ‰æ•ˆåˆ©ç”¨æ¨ç†é¢„ç®—ã€‚</li>
<li>æå‡ºäº†å¹¶è¡Œæ€è€ƒæ–¹æ³•ï¼Œåœ¨åŒä¸€æ¨ç†é¢„ç®—å†…ç”Ÿæˆå¤šä¸ªç‹¬ç«‹æ¨ç†è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-82138908979506732abf35d767c95ddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b068b61b24e9b295fbe2183fa8dfc044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-371c6c836c767dcc4bf7df499fb8722d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e1647cdb21ab7460ccbe682cc1e7ea0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Advancing-Multimodal-Reasoning-From-Optimized-Cold-Start-to-Staged-Reinforcement-Learning"><a href="#Advancing-Multimodal-Reasoning-From-Optimized-Cold-Start-to-Staged-Reinforcement-Learning" class="headerlink" title="Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning"></a>Advancing Multimodal Reasoning: From Optimized Cold Start to Staged   Reinforcement Learning</h2><p><strong>Authors:Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, Yu Cheng</strong></p>
<p>Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025. </p>
<blockquote>
<p>å—åˆ°Deepseek-R1åœ¨å¤æ‚æ–‡æœ¬ä»»åŠ¡ä¸­å‡ºè‰²æ¨ç†èƒ½åŠ›çš„å¯å‘ï¼Œè®¸å¤šå·¥ä½œè¯•å›¾é€šè¿‡ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æ¿€åŠ±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç±»ä¼¼èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»–ä»¬åœ¨æ¿€æ´»å¤æ‚æ¨ç†æ–¹é¢ä»é¢ä¸´å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å­¤ç«‹åœ°ç ”ç©¶å¤šæ¨¡æ€RLï¼Œè€Œæ˜¯æ·±å…¥ç ”ç©¶äº†å½“å‰çš„è®­ç»ƒç®¡é“ï¼Œå¹¶ç¡®å®šäº†ä¸‰ä¸ªå…³é”®ç°è±¡ï¼š1ï¼‰æœ‰æ•ˆçš„å†·å¯åŠ¨åˆå§‹åŒ–å¯¹äºå¢å¼ºMLLMæ¨ç†è‡³å…³é‡è¦ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°ä»…é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®è¿›è¡Œåˆå§‹åŒ–ï¼Œå°±å¯ä»¥åœ¨å°šæœªåº”ç”¨å¤šæ¨¡æ€RLä¹‹å‰ï¼Œå…¶æ€§èƒ½å°±è¶…è¶Šäº†è®¸å¤šæœ€æ–°çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚2ï¼‰åº”ç”¨äºå¤šæ¨¡æ€RLçš„æ ‡å‡†GRPOå­˜åœ¨æ¢¯åº¦åœæ»é—®é¢˜ï¼Œè¿™é™ä½äº†è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚3ï¼‰åœ¨å¤šæ¨¡æ€RLé˜¶æ®µä¹‹åè¿›è¡Œçš„ä»…æ–‡æœ¬RLè®­ç»ƒï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å¤šæ¨¡æ€æ¨ç†ã€‚è¿™ç§åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•æœ‰æ•ˆåœ°å¹³è¡¡äº†æ„ŸçŸ¥å®šä½å’Œè®¤çŸ¥æ¨ç†å‘å±•ã€‚é€šè¿‡èå…¥ä¸Šè¿°è§è§£å¹¶è§£å†³å¤šæ¨¡æ€RLé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ReVisual-R1ï¼Œåœ¨åŒ…æ‹¬MathVerseã€MathVisionã€WeMathã€LogicVistaã€DynaMathä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„AIME2024å’ŒAIME2025ç­‰å¼€æ”¾æºä»£ç çš„7B MLLMåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04207v1">PDF</a> 19 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å¼ºåŒ–é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„æ–‡æœ¬æ•°æ®åˆå§‹åŒ–å¯ä»¥æ˜¾è‘—æå‡MLLMsçš„æ¨ç†æ€§èƒ½ï¼Œä¸”æ ‡å‡†åŒ–GRPOåœ¨å¤šæ¨¡æ€RLä¸­ä¼šå¯¼è‡´æ¢¯åº¦åœæ»ã€‚é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼Œå…ˆå¤šæ¨¡æ€RLå†æ–‡æœ¬RLï¼Œèƒ½æœ‰æ•ˆå¹³è¡¡æ„ŸçŸ¥æ¥åœ°å’Œè®¤çŸ¥æ¨ç†å‘å±•ã€‚æ®æ­¤æ¨å‡ºReVisual-R1æ¨¡å‹ï¼Œåœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ•°æ®åˆå§‹åŒ–å¯¹æå‡MLLMsçš„æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æ ‡å‡†åŒ–GRPOåœ¨å¤šæ¨¡æ€RLä¸­ä¼šå¯¼è‡´æ¢¯åº¦åœæ»ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>åˆ†é˜¶æ®µè®­ç»ƒæ–¹å¼ï¼Œå…ˆå¤šæ¨¡æ€RLå†æ–‡æœ¬RLï¼Œèƒ½æœ‰æ•ˆå¢å¼ºå¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>ReVisual-R1æ¨¡å‹é€šè¿‡èå…¥ä¸Šè¿°è§è§£å¹¶è§£å†³å¤šæ¨¡æ€RLé—®é¢˜ï¼Œå®ç°äº†æ€§èƒ½æå‡ã€‚</li>
<li>ReVisual-R1æ¨¡å‹åœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MathVerse, MathVision, WeMathç­‰ï¼‰ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>å†·å¯åŠ¨åˆå§‹åŒ–è¿‡ç¨‹åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é‡è¦æ€§è¢«å¼ºè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c372b9949f00003cd9668401c8ba805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3df634d151ee7b5f86f0bb59474c69b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4d7104ecdbac515058be32687c3577.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation"><a href="#EPiC-Towards-Lossless-Speedup-for-Reasoning-Training-through-Edge-Preserving-CoT-Condensation" class="headerlink" title="EPiC: Towards Lossless Speedup for Reasoning Training through   Edge-Preserving CoT Condensation"></a>EPiC: Towards Lossless Speedup for Reasoning Training through   Edge-Preserving CoT Condensation</h2><p><strong>Authors:Jinghan Jia, Hadi Reisizadeh, Chongyu Fan, Nathalie Baracaldo, Mingyi Hong, Sijia Liu</strong></p>
<p>Large language models (LLMs) have shown remarkable reasoning capabilities when trained with chain-of-thought (CoT) supervision. However, the long and verbose CoT traces, especially those distilled from large reasoning models (LRMs) such as DeepSeek-R1, significantly increase training costs during the distillation process, where a non-reasoning base model is taught to replicate the reasoning behavior of an LRM. In this work, we study the problem of CoT condensation for resource-efficient reasoning training, aimed at pruning intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling supervised model training on length-reduced CoT data while preserving both answer accuracy and the modelâ€™s ability to generate coherent reasoning. Our rationale is that CoT traces typically follow a three-stage structure: problem understanding, exploration, and solution convergence. Through empirical analysis, we find that retaining the structure of the reasoning trace, especially the early stage of problem understanding (rich in reflective cues) and the final stage of solution convergence, is sufficient to achieve lossless reasoning supervision. To this end, we propose an Edge-Preserving Condensation method, EPiC, which selectively retains only the initial and final segments of each CoT trace while discarding the middle portion. This design draws an analogy to preserving the â€œedgeâ€ of a reasoning trajectory, capturing both the initial problem framing and the final answer synthesis, to maintain logical continuity. Experiments across multiple model families (Qwen and LLaMA) and benchmarks show that EPiC reduces training time by over 34% while achieving lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To the best of our knowledge, this is the first study to explore thought-level CoT condensation for efficient reasoning model distillation. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡‡ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå±•ç°å‡ºäº†æ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°¤å…¶æ˜¯ä»å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰ä¸­æç‚¼å‡ºçš„æ€ç»´é“¾è½¨è¿¹æ—¢é•¿åˆå†—é•¿ï¼Œåœ¨æç‚¼è¿‡ç¨‹ä¸­æ˜¾è‘—å¢åŠ äº†è®­ç»ƒæˆæœ¬ï¼Œè¿™é‡Œçš„éæ¨ç†åŸºç¡€æ¨¡å‹è¢«æ•™å¯¼å»å¤åˆ¶LRMçš„æ¨ç†è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†é¢å‘èµ„æºé«˜æ•ˆæ¨ç†è®­ç»ƒçš„CoTå‡ç»ƒé—®é¢˜ï¼Œæ—¨åœ¨ç¼©å‡æ€ç»´é“¾è½¨è¿¹ä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼ˆå³æ€ç»´ï¼‰ï¼Œåœ¨ç¼©å‡é•¿åº¦çš„CoTæ•°æ®ä¸Šè¿›è¡Œç›‘ç£æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä¿ç•™ç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨¡å‹ç”Ÿæˆè¿è´¯æ¨ç†çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç†å¿µæ˜¯ï¼ŒCoTè½¨è¿¹é€šå¸¸éµå¾ªä¸‰é˜¶æ®µç»“æ„ï¼šç†è§£é—®é¢˜ã€æ¢ç´¢å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¿ç•™æ¨ç†è½¨è¿¹çš„ç»“æ„ï¼Œç‰¹åˆ«æ˜¯ç†è§£é—®é¢˜çš„æ—©æœŸé˜¶æ®µï¼ˆå¯Œå«åæ€çº¿ç´¢ï¼‰å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›çš„æœ€åä¸€ä¸ªé˜¶æ®µï¼Œè¶³ä»¥å®ç°æ— æŸæ¨ç†ç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾¹ç¼˜ä¿ç•™å‡ç»ƒæ–¹æ³•EPiCï¼Œå®ƒé€‰æ‹©æ€§åœ°ä»…ä¿ç•™æ¯ä¸ªCoTè½¨è¿¹çš„åˆå§‹å’Œæœ€ç»ˆç‰‡æ®µï¼ŒåŒæ—¶ä¸¢å¼ƒä¸­é—´éƒ¨åˆ†ã€‚è¿™ç§è®¾è®¡ç±»ä¼¼äºä¿ç•™æ¨ç†è½¨è¿¹çš„â€œè¾¹ç¼˜â€ï¼Œæ•æ‰æœ€åˆçš„çš„é—®é¢˜æ¡†æ¶å’Œæœ€ç»ˆçš„ç­”æ¡ˆç»¼åˆï¼Œä»¥ä¿æŒé€»è¾‘è¿è´¯æ€§ã€‚åœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ï¼ˆQwenå’ŒLLaMAï¼‰å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEPiCé€šè¿‡å‡å°‘è¶…è¿‡34%çš„è®­ç»ƒæ—¶é—´ï¼Œåœ¨MATH500ä¸Šå®ç°äº†æ— æŸæ¨ç†å‡†ç¡®æ€§ï¼Œä¸å…¨CoTç›‘ç£ç›¸å½“ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹æ¢ç´¢é«˜æ•ˆæ¨ç†æ¨¡å‹è’¸é¦ä¸­çš„æ€ç»´å±‚é¢CoTå‡ç»ƒçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04205v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´ç›‘ç£å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§é•¿è€Œç¹ççš„æ€ç»´é“¾è¿½è¸ªï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä»æ·±åº¦å¯»çŸ¥ç­‰å¤§è§„æ¨¡æ¨ç†æ¨¡å‹è’¸é¦å¾—æ¥çš„ï¼Œæ˜¾è‘—å¢åŠ äº†è’¸é¦è¿‡ç¨‹ä¸­çš„è®­ç»ƒæˆæœ¬ã€‚åœ¨èµ„æºæ•ˆç‡æ¨ç†è®­ç»ƒä¸­ï¼Œæœ¬æ–‡ç ”ç©¶äº†æ€ç»´é“¾è¿½è¸ªå‡èšçš„é—®é¢˜ï¼Œç›®æ ‡æ˜¯åˆ å‡æ€ç»´é“¾è¿½è¸ªä¸­çš„ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œä½¿ç›‘ç£æ¨¡å‹èƒ½å¤Ÿåœ¨ç¼©çŸ­çš„æ€ç»´é“¾æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ä¿æŒç­”æ¡ˆå‡†ç¡®æ€§å’Œæ¨¡å‹çš„è¿è´¯æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç†å¿µæ˜¯ï¼Œæ€ç»´é“¾è¿½è¸ªé€šå¸¸éµå¾ªä¸‰é˜¶æ®µç»“æ„ï¼šç†è§£é—®é¢˜ã€æ¢ç´¢é˜¶æ®µå’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¿ç•™æ¨ç†è¿½è¸ªçš„ç»“æ„ï¼Œç‰¹åˆ«æ˜¯ç†è§£é—®é¢˜çš„æ—©æœŸé˜¶æ®µï¼ˆå¯Œå«åæ€çº¿ç´¢ï¼‰å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›çš„æœ€ç»ˆé˜¶æ®µï¼Œè¶³ä»¥å®ç°æ— æŸæ¨ç†ç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾¹ç¼˜ä¿ç•™å‡èšæ³•ï¼ˆEPiCï¼‰ï¼Œè¯¥æ–¹æ³•æœ‰é€‰æ‹©åœ°ä»…ä¿ç•™æ¯ä¸ªæ€ç»´é“¾è¿½è¸ªçš„åˆå§‹å’Œæœ€ç»ˆéƒ¨åˆ†ï¼ŒåŒæ—¶ä¸¢å¼ƒä¸­é—´éƒ¨åˆ†ã€‚è¿™ç§è®¾è®¡ç±»ä¼¼äºæ•æ‰æ¨ç†è½¨è¿¹çš„â€œè¾¹ç¼˜â€ï¼Œæ—¢ä¿æŒé—®é¢˜çš„åˆæ­¥æ¡†æ¶åˆä¿ç•™æœ€ç»ˆç­”æ¡ˆçš„åˆæˆï¼Œä»è€Œç»´æŒé€»è¾‘è¿è´¯æ€§ã€‚åœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ï¼ˆå¦‚Qwenå’ŒLLaMAï¼‰å’ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEPiCå°†è®­ç»ƒæ—¶é—´å‡å°‘äº†è¶…è¿‡34%ï¼ŒåŒæ—¶åœ¨MATH500ä¸Šçš„æ— æŸæ¨ç†å‡†ç¡®æ€§å¯ä¸å…¨æ€ç»´é“¾ç›‘ç£ç›¸åª²ç¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡æ¢ç´¢æ€ç»´å±‚é¢çš„æ€ç»´é“¾å‡èšä»¥æœ‰æ•ˆæé«˜æ¨ç†æ¨¡å‹è’¸é¦æ•ˆç‡çš„ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´ç›‘ç£å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨é«˜æˆæœ¬é—®é¢˜ã€‚</li>
<li>æå‡ºæ€ç»´é“¾å‡èšï¼ˆCoT condensationï¼‰ä»¥ä¼˜åŒ–èµ„æºæ•ˆç‡ï¼Œé€šè¿‡åˆ å‡ä¸­é—´æ¨ç†æ­¥éª¤é™ä½è®­ç»ƒæˆæœ¬ã€‚</li>
<li>æ€ç»´é“¾è¿½è¸ªé€šå¸¸éµå¾ªä¸‰é˜¶æ®µç»“æ„ï¼šç†è§£é—®é¢˜ã€æ¢ç´¢å’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›ã€‚</li>
<li>ä¿ç•™æ€ç»´é“¾è¿½è¸ªçš„ç»“æ„ï¼Œç‰¹åˆ«æ˜¯é—®é¢˜ç†è§£é˜¶æ®µå’Œè§£å†³æ–¹æ¡ˆæ”¶æ•›é˜¶æ®µï¼Œå¯¹äºå®ç°æ— æŸæ¨ç†ç›‘ç£è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥è¾¹ç¼˜ä¿ç•™å‡èšæ³•ï¼ˆEPiCï¼‰ï¼Œè¯¥æ–¹æ³•é€‰æ‹©ä¿ç•™æ€ç»´é“¾è¿½è¸ªçš„åˆå§‹å’Œæœ€ç»ˆéƒ¨åˆ†ï¼Œå®ç°é€»è¾‘è¿è´¯æ€§çš„ç»´æŒã€‚</li>
<li>EPiCæ–¹æ³•æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ï¼ŒåŒæ—¶åœ¨åŸºå‡†æµ‹è¯•ä¸Šä¿æŒæ— æŸæ¨ç†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55aca2880c7d30e7d16c63af14f1b211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9af991d4c026c1252da709b661ce928.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bbc31fcad4fcc0d988bc4a1c71c5b6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b86ae84eafc90ce41080ecab61e15d41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6da85968f48f5e69fb97cfa1a3a40f70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aafeaa0f991135cc33b51381844ec1d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b74f08e0285f605dbb2e1f49223bac5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="R-Search-Empowering-LLM-Reasoning-with-Search-via-Multi-Reward-Reinforcement-Learning"><a href="#R-Search-Empowering-LLM-Reasoning-with-Search-via-Multi-Reward-Reinforcement-Learning" class="headerlink" title="R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning"></a>R-Search: Empowering LLM Reasoning with Search via Multi-Reward   Reinforcement Learning</h2><p><strong>Authors:Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu</strong></p>
<p>Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/QingFei1/R-Search">https://github.com/QingFei1/R-Search</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šæ­¥å’Œé•¿é“¾æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå°†å…¶æ¨ç†èƒ½åŠ›æ‰©å±•åˆ°ä¸æœç´¢çš„æ·±åº¦äº¤äº’ä»æ˜¯ä¸€é¡¹éå¹³å‡¡çš„æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹å¾€å¾€æ— æ³•è¯†åˆ«æœ€ä½³çš„æ¨ç†-æœç´¢äº¤äº’è½¨è¿¹ï¼Œä»è€Œå¯¼è‡´æ¬¡ä¼˜å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†R-Searchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ¨ç†-æœç´¢èåˆçš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå…·æœ‰æ·±åº¦æœç´¢äº¤äº’çš„å¤šæ­¥æ¨ç†ï¼Œå¹¶é€šè¿‡å¤šå¥–åŠ±ä¿¡å·å­¦ä¹ æœ€ä½³çš„æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ï¼Œä»è€Œæé«˜åœ¨å¤æ‚é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å“åº”è´¨é‡ã€‚R-SearchæŒ‡å¯¼LLMåŠ¨æ€å†³å®šä½•æ—¶è¿›è¡Œæ£€ç´¢æˆ–æ¨ç†ï¼ŒåŒæ—¶å…¨å±€é›†æˆå…³é”®è¯æ®ï¼Œå¢å¼ºæ¨ç†å’Œæœç´¢ä¹‹é—´çš„æ·±åº¦çŸ¥è¯†äº¤äº’ã€‚åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒR-Searchæä¾›å¤šé˜¶æ®µã€å¤šç±»å‹çš„å¥–åŠ±æ¥å…±åŒä¼˜åŒ–æ¨ç†-æœç´¢è½¨è¿¹ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒR-Searchç›¸è¾ƒäºå…ˆè¿›çš„RAGåŸºå‡†æµ‹è¯•ï¼Œå…¶æ€§èƒ½æé«˜äº†é«˜è¾¾32.2%ï¼ˆé¢†åŸŸå†…ï¼‰å’Œ25.1%ï¼ˆè·¨é¢†åŸŸï¼‰ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/QingFei1/R-Search">https://github.com/QingFei1/R-Search</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04185v1">PDF</a> 16 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥å’Œé•¿é“¾æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ·±åº¦æœç´¢äº¤äº’æ–¹é¢çš„æ¨ç†èƒ½åŠ›ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†R-Searchï¼Œä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå…·æœ‰æ·±åº¦æœç´¢äº¤äº’çš„å¤šæ­¥æ¨ç†ï¼Œå¹¶é€šè¿‡å¤šç§å¥–åŠ±ä¿¡å·å­¦ä¹ æœ€ä½³çš„æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ï¼Œä»è€Œæé«˜åœ¨å¤æ‚é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„å“åº”è´¨é‡ã€‚R-Searchæ¡†æ¶å¯åŠ¨æ€å†³å®šä½•æ—¶è¿›è¡Œæ£€ç´¢å’Œæ¨ç†ï¼ŒåŒæ—¶å…¨å±€æ•´åˆå…³é”®è¯æ®ï¼Œå¢å¼ºæ¨ç†å’Œæœç´¢ä¹‹é—´çš„æ·±åº¦çŸ¥è¯†äº¤äº’ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR-Searchåœ¨ä¸ƒä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆè¿›çš„RAGåŸºå‡†æµ‹è¯•ï¼Œæœ€é«˜æå‡ç‡è¾¾32.2%ï¼ˆé¢†åŸŸå†…ï¼‰å’Œ25.1%ï¼ˆè·¨é¢†åŸŸï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥å’Œé•¿é“¾æ¨ç†ä¸Šæœ‰æ‰€çªç ´ï¼Œä½†åœ¨ä¸æœç´¢çš„æ·±åº¦äº¤äº’æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>R-Searchæ˜¯ä¸€ä¸ªæ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆæ¨ç†ä¸æœç´¢ï¼Œä½¿LLMèƒ½å¤Ÿè‡ªä¸»æ‰§è¡Œå¤šæ­¥æ¨ç†å¹¶æ·±åº¦å‚ä¸æœç´¢äº¤äº’ã€‚</li>
<li>R-Searché€šè¿‡å¤šé‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ¨ç†æœç´¢äº¤äº’è½¨è¿¹ï¼Œæ—¨åœ¨æé«˜å¤æ‚é€»è¾‘å’ŒçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å“åº”è´¨é‡ã€‚</li>
<li>R-Searchèƒ½åŠ¨æ€å¹³è¡¡æ£€ç´¢ä¸æ¨ç†çš„å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å…¨å±€æ•´åˆå…³é”®è¯æ®ã€‚</li>
<li>R-Searchæ¡†æ¶å¢å¼ºäº†æ¨ç†å’Œæœç´¢ä¹‹é—´çš„æ·±åº¦çŸ¥è¯†äº¤äº’ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR-Searchåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„æå‡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cac3135fed19307e49f5a5ec1b29f3de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1494d44782cd1eec136119a2c80f523.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OpenThoughts-Data-Recipes-for-Reasoning-Models"><a href="#OpenThoughts-Data-Recipes-for-Reasoning-Models" class="headerlink" title="OpenThoughts: Data Recipes for Reasoning Models"></a>OpenThoughts: Data Recipes for Reasoning Models</h2><p><strong>Authors:Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, Ludwig Schmidt</strong></p>
<p>Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06&#x2F;24-01&#x2F;25, and 54% on GPQA Diamond. All of our datasets and models are available on <a target="_blank" rel="noopener" href="https://openthoughts.ai/">https://openthoughts.ai</a>. </p>
<blockquote>
<p>æ¨ç†æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç å’Œç§‘å­¦ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å¿«é€Ÿè¿›å±•ã€‚ç„¶è€Œï¼Œå…³äºæœ€ä½³æ¨ç†è®­ç»ƒæ–¹æ¡ˆä»ç„¶å­˜åœ¨è®¸å¤šæœªè§£çš„é—®é¢˜ï¼Œå› ä¸ºæœ€å…ˆè¿›çš„æ¨¡å‹é€šå¸¸ä¾èµ–äºä¸“æœ‰æ•°æ®é›†ï¼Œè€Œå…¬ä¼—èƒ½æ¥è§¦åˆ°çš„ä¿¡æ¯å¾ˆå°‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒOpenThoughtsé¡¹ç›®çš„ç›®æ ‡æ˜¯åˆ›å»ºç”¨äºè®­ç»ƒæ¨ç†æ¨¡å‹çš„å¼€æºæ•°æ®é›†ã€‚ç»è¿‡åˆæ­¥æ¢ç´¢ï¼Œæˆ‘ä»¬çš„OpenThoughts2-1Mæ•°æ®é›†å‚¬ç”Ÿäº†OpenThinker2-32Bæ¨¡å‹ï¼Œå®ƒæ˜¯é¦–ä¸ªåœ¨å…¬å¼€æ¨ç†æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨AIMEå’ŒLiveCodeBenchç­‰æ ‡å‡†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šä¸DeepSeek-R1-Distill-32Bç›¸åŒ¹é…ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°è°ƒæŸ¥æ•°æ®ç”Ÿæˆç®¡é“çš„æ¯ä¸€æ­¥è¿›è¡Œäº†1000å¤šæ¬¡å—æ§å®éªŒï¼Œè¿›ä¸€æ­¥æ”¹è¿›äº†æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä»è€Œäº§ç”Ÿäº†OpenThoughts3ã€‚å°†ç®¡é“æ‰©å±•åˆ°120ä¸‡ä¸ªä¾‹å­ï¼Œå¹¶ä½¿ç”¨QwQ-32Bä½œä¸ºæ•™å¸ˆï¼Œæˆ‘ä»¬å¾—åˆ°äº†OpenThinker3-7Bæ¨¡å‹ï¼Œå–å¾—äº†æœ€æ–°ç»“æœï¼šAIME 2025è¾¾æˆç‡ä¸º53%ï¼ŒLiveCodeBench 06&#x2F;24-01&#x2F;25è¾¾æˆç‡ä¸º51%ï¼ŒGPQA Diamondè¾¾æˆç‡ä¸º54%ã€‚æˆ‘ä»¬çš„æ‰€æœ‰æ•°æ®é›†å’Œæ¨¡å‹éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://openthoughts.ai/">https://openthoughts.ai</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04178v1">PDF</a> <a target="_blank" rel="noopener" href="https://www.openthoughts.ai/blog/ot3">https://www.openthoughts.ai/blog/ot3</a></p>
<p><strong>Summary</strong></p>
<p>éšç€æ¨ç†æ¨¡å‹åœ¨è®¸å¤šæ¶‰åŠæ•°å­¦ã€ä»£ç å’Œç§‘å­¦çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—å¿«é€Ÿè¿›å±•ï¼Œå…³äºæœ€ä½³è®­ç»ƒé…æ–¹çš„é—®é¢˜ä»ç„¶å­˜åœ¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒOpenThoughtsé¡¹ç›®çš„ç›®æ ‡æ˜¯åˆ›å»ºç”¨äºè®­ç»ƒæ¨ç†æ¨¡å‹çš„å¼€æºæ•°æ®é›†ã€‚ç»è¿‡åˆæ­¥æ¢ç´¢å’Œå¯¹æ•°æ®é›†ç³»ç»Ÿçš„æ”¹è¿›ï¼Œä»–ä»¬çš„OpenThoughtsæ•°æ®é›†åŸ¹è‚²å‡ºèƒ½åœ¨å…¬å…±æ¨ç†æ•°æ®ä¸Šè®­ç»ƒçš„OpenThinkeræ¨¡å‹ï¼ŒåŒ¹é…DeepSeek-R1-Distillåœ¨AIMEå’ŒLiveCodeBenchç­‰æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚å…¶æœ€æ–°çš„OpenThoughtsæ•°æ®é›†åŸ¹è‚²å‡ºOpenThinker3-7Bæ¨¡å‹ï¼Œè¾¾åˆ°ä¸šç•Œé¡¶å°–æ°´å¹³ï¼Œç›¸å…³æ•°æ®å’Œæ¨¡å‹å‡å¯ä»<a target="_blank" rel="noopener" href="https://openthoughts.aiè·å–./">https://openthoughts.aiè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—å¿«é€Ÿè¿›å±•ï¼Œä½†ä»å­˜åœ¨å…³äºæœ€ä½³è®­ç»ƒæ–¹æ³•çš„å¼€æ”¾é—®é¢˜ã€‚</li>
<li>OpenThoughtsé¡¹ç›®çš„ç›®æ ‡æ˜¯åˆ›å»ºå¼€æºæ•°æ®é›†ä»¥è®­ç»ƒæ¨ç†æ¨¡å‹ã€‚</li>
<li>OpenThoughtsæ•°æ®é›†æˆåŠŸåŸ¹è‚²å‡ºOpenThinkeræ¨¡å‹ç³»åˆ—ï¼Œèƒ½åœ¨å…¬å…±æ¨ç†æ•°æ®ä¸Šè®­ç»ƒå¹¶ä¸DeepSeek-R1-Distillç›¸å½“ã€‚</li>
<li>OpenThinkerç³»åˆ—æ¨¡å‹åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¾‹å¦‚AIMEå’ŒLiveCodeBenchç­‰ã€‚</li>
<li>OpenThoughtsé¡¹ç›®é€šè¿‡ç³»ç»Ÿåœ°æ”¹è¿›æ•°æ®é›†ç”Ÿæˆæµç¨‹ï¼Œè¿›è¡Œäº†è¶…è¿‡ä¸€åƒæ¬¡çš„å—æ§å®éªŒã€‚</li>
<li>OpenThoughtsé¡¹ç›®æœ€æ–°çš„æ•°æ®é›†æ˜¯OpenThoughts3ï¼ŒåŸ¹è‚²å‡ºäº†å…·æœ‰ä¸šç•Œé¡¶å°–æ°´å¹³çš„OpenThinker3-7Bæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1b67e956740237d932e8344ab5aa7e83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99cdd32681bb33847a3059e6e3672d77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea99bd99e8f513ebea30ba316225138a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c632e581ad1cab232efebdc022e3e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d886903ef8238d4762723dbbf5ba4b25.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MMR-V-Whatâ€™s-Left-Unsaid-A-Benchmark-for-Multimodal-Deep-Reasoning-in-Videos"><a href="#MMR-V-Whatâ€™s-Left-Unsaid-A-Benchmark-for-Multimodal-Deep-Reasoning-in-Videos" class="headerlink" title="MMR-V: Whatâ€™s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos"></a>MMR-V: Whatâ€™s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in   Videos</h2><p><strong>Authors:Kejian Zhu, Zhuoran Jin, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao</strong></p>
<p>The sequential structure of videos poses a challenge to the ability of multimodal large language models (MLLMs) to locate multi-frame evidence and conduct multimodal reasoning. However, existing video benchmarks mainly focus on understanding tasks, which only require models to match frames mentioned in the question (hereafter referred to as â€œquestion frameâ€) and perceive a few adjacent frames. To address this gap, we propose MMR-V: A Benchmark for Multimodal Deep Reasoning in Videos. The benchmark is characterized by the following features. (1) Long-range, multi-frame reasoning: Models are required to infer and analyze evidence frames that may be far from the question frame. (2) Beyond perception: Questions cannot be answered through direct perception alone but require reasoning over hidden information. (3) Reliability: All tasks are manually annotated, referencing extensive real-world user understanding to align with common perceptions. (4) Confusability: Carefully designed distractor annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos and 1,257 tasks. Our experiments reveal that current models still struggle with multi-modal reasoning; even the best-performing model, o4-mini, achieves only 52.5% accuracy. Additionally, current reasoning enhancement strategies (Chain-of-Thought and scaling test-time compute) bring limited gains. Further analysis indicates that the CoT demanded for multi-modal reasoning differs from it in textual reasoning, which partly explains the limited performance gains. We hope that MMR-V can inspire further research into enhancing multi-modal reasoning capabilities. </p>
<blockquote>
<p>è§†é¢‘çš„è¿ç»­ç»“æ„å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®šä½å¤šå¸§è¯æ®å’Œè¿›è¡Œå¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ä¸Šæå‡ºäº†æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç†è§£ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡åªéœ€è¦æ¨¡å‹åŒ¹é…é—®é¢˜ä¸­æåˆ°çš„å¸§ï¼ˆä»¥ä¸‹ç®€ç§°â€œé—®é¢˜å¸§â€ï¼‰å¹¶æ„ŸçŸ¥å°‘æ•°ç›¸é‚»å¸§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†MMR-Vï¼šè§†é¢‘å¤šæ¨¡æ€æ·±åº¦æ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•çš„ç‰¹ç‚¹å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰é•¿ç¨‹å¤šå¸§æ¨ç†ï¼šæ¨¡å‹éœ€è¦æ¨æ–­å’Œåˆ†æå¯èƒ½è¿œç¦»é—®é¢˜å¸§çš„è¯æ®å¸§ã€‚ï¼ˆ2ï¼‰è¶…è¶Šæ„ŸçŸ¥ï¼šé—®é¢˜ä¸èƒ½ä»…é€šè¿‡ç›´æ¥æ„ŸçŸ¥æ¥å›ç­”ï¼Œè€Œæ˜¯éœ€è¦æ¨ç†éšè—çš„ä¿¡æ¯ã€‚ï¼ˆ3ï¼‰å¯é æ€§ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½æ˜¯æ‰‹åŠ¨æ³¨é‡Šçš„ï¼Œå‚è€ƒå¹¿æ³›çš„å®é™…ç”¨æˆ·ç†è§£ï¼Œä»¥ç¬¦åˆæ™®éè®¤çŸ¥ã€‚ï¼ˆ4ï¼‰æ··æ·†æ€§ï¼šç²¾å¿ƒè®¾è®¡çš„å¹²æ‰°æ³¨é‡Šç­–ç•¥ï¼Œä»¥å‡å°‘æ¨¡å‹çš„æ·å¾„ã€‚MMR-VåŒ…å«317ä¸ªè§†é¢‘å’Œ1257ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼›å³ä½¿è¡¨ç°æœ€ä½³çš„æ¨¡å‹o4-miniä¹Ÿä»…è¾¾åˆ°52.5%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œå½“å‰æ¨ç†å¢å¼ºç­–ç•¥ï¼ˆæ€ç»´é“¾å’Œæ‰©å±•æµ‹è¯•æ—¶é—´è®¡ç®—ï¼‰å¸¦æ¥çš„æ”¶ç›Šæœ‰é™ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ¨ç†æ‰€éœ€çš„æ€ç»´é“¾ä¸æ–‡æœ¬æ¨ç†æœ‰æ‰€ä¸åŒï¼Œè¿™éƒ¨åˆ†è§£é‡Šäº†æ€§èƒ½æå‡æœ‰é™çš„åŸå› ã€‚æˆ‘ä»¬å¸Œæœ›MMR-Vèƒ½å¤Ÿæ¿€å‘å¯¹å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04141v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://mmr-v.github.io/">https://mmr-v.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘ä¸­çš„å¤šæ¨¡æ€æ·±åº¦æ¨ç†åŸºå‡†æµ‹è¯•MMR-Vã€‚è¯¥åŸºå‡†æµ‹è¯•çš„ç‰¹ç‚¹åŒ…æ‹¬é•¿ç¨‹å¤šå¸§æ¨ç†ã€è¶…è¶Šæ„ŸçŸ¥çš„éšè—ä¿¡æ¯æ¨ç†ã€ä»»åŠ¡æ‰‹åŠ¨æ ‡æ³¨ä¸çœŸå®ä¸–ç•Œç”¨æˆ·ç†è§£å¯¹é½ä»¥åŠè®¾è®¡æ··æ·†æ ‡æ³¨ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»æœ‰å›°éš¾ï¼Œæœ€ä½³æ¨¡å‹o4-miniçš„å‡†ç¡®ç‡ä»…ä¸º52.5%ï¼Œç°æœ‰çš„æ¨ç†å¢å¼ºç­–ç•¥å¸¦æ¥æœ‰é™æ”¶ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ä¸­çš„åºè´¯ç»“æ„å®šä½å¤šå¸§è¯æ®å’Œå¤šæ¨¡æ€æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§†é¢‘åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨ç†è§£ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åŒ¹é…é—®é¢˜ä¸­çš„å¸§å¹¶æ„ŸçŸ¥å°‘é‡ç›¸é‚»å¸§ï¼Œå­˜åœ¨å±€é™ã€‚</li>
<li>MMR-VåŸºå‡†æµ‹è¯•å…·æœ‰é•¿ç¨‹å¤šå¸§æ¨ç†ã€è¶…è¶Šæ„ŸçŸ¥çš„éšè—ä¿¡æ¯æ¨ç†ç­‰ç‰¹å¾ã€‚</li>
<li>MMR-VåŒ…å«317ä¸ªè§†é¢‘å’Œ1,257ä¸ªä»»åŠ¡ï¼Œé‡‡ç”¨æ‰‹åŠ¨æ ‡æ³¨ï¼Œä¸çœŸå®ä¸–ç•Œç”¨æˆ·ç†è§£å¯¹é½ã€‚</li>
<li>å½“å‰æ¨¡å‹åœ¨MMR-Vä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæœ€ä½³æ¨¡å‹o4-miniçš„å‡†ç¡®ç‡ä»…ä¸º52.5%ã€‚</li>
<li>ç°æœ‰çš„æ¨ç†å¢å¼ºç­–ç•¥ï¼ˆå¦‚Chain-of-Thoughtå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æ‰©å±•ï¼‰å¯¹å¤šæ¨¡æ€æ¨ç†çš„å¢ç›Šæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d9605454fe68ec6c42c13d1f54b4ad59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ac2b73c1ca05ab4310aeb96cdfc18c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cec1625179effab0b31a947c215ef67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-940181c5078ad798e878d4220edd6951.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2ce8dd5a60eadcd568f4fd39d77ad9c1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Guided-Speculative-Inference-for-Efficient-Test-Time-Alignment-of-LLMs"><a href="#Guided-Speculative-Inference-for-Efficient-Test-Time-Alignment-of-LLMs" class="headerlink" title="Guided Speculative Inference for Efficient Test-Time Alignment of LLMs"></a>Guided Speculative Inference for Efficient Test-Time Alignment of LLMs</h2><p><strong>Authors:Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis</strong></p>
<p>We propose Guided Speculative Inference (GSI), a novel algorithm for efficient reward-guided decoding in large language models. GSI combines soft best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We derive a theoretical bound on the KL divergence between our induced distribution and the optimal policy. In experiments on reasoning benchmarks (MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative decoding (Liao et al., 2025), and in certain settings even outperforms soft best-of-$n$ with $\pi_B$. The code is available at <a target="_blank" rel="noopener" href="https://github.com/j-geuter/GSI">https://github.com/j-geuter/GSI</a> . </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†å¼•å¯¼å¼æ¨æµ‹æ¨ç†ï¼ˆGSIï¼‰è¿™ä¸€æ–°é¢–ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­é«˜æ•ˆçš„å¥–åŠ±å¼•å¯¼è§£ç ã€‚GSIå°†è½¯æœ€å¥½çš„nä¸ªæµ‹è¯•æ—¶é—´ç¼©æ”¾ä¸å¥–åŠ±æ¨¡å‹r(xï¼Œy)ä»¥åŠä»å°å‹è¾…åŠ©æ¨¡å‹Ï€S(yâˆ£x)ç”Ÿæˆçš„æ¨æµ‹æ ·æœ¬ç›¸ç»“åˆã€‚æˆ‘ä»¬å¯ä»¥è¯æ˜ï¼Œåœ¨ä¸»è¦æ¨¡å‹Ï€Bä¸‹ï¼Œæˆ‘ä»¬å¯¹è½¯æœ€å¥½çš„nä¸ªæœ€ä¼˜å€¾æ–œç­–ç•¥Ï€Î²ï¼ŒB(yâˆ£x)â‰ˆÏ€B(yâˆ£x)exp(Î²r(xï¼Œy))è¿›è¡Œäº†è¿‘ä¼¼ã€‚æˆ‘ä»¬æ¨å¯¼å‡ºäº†æˆ‘ä»¬æ‰€äº§ç”Ÿçš„åˆ†å¸ƒä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦çš„ç†è®ºç•Œé™ã€‚åœ¨MATH500ã€OlympiadBenchå’ŒMinerva Mathç­‰æ¨ç†åŸºå‡†æµ‹è¯•å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æ¯”ä½¿ç”¨Ï€Så’Œå¥–åŠ±å¼•å¯¼æ¨æµ‹è§£ç çš„æ ‡å‡†è½¯æœ€å¥½çš„nä¸ªæ›´é«˜çš„ç²¾åº¦ï¼ˆLiaoç­‰äººï¼Œ2025ï¼‰ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¿‡äº†ä½¿ç”¨Ï€Bçš„è½¯æœ€å¥½çš„nä¸ªã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/j-geuter/GSI%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/j-geuter/GSIæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04118v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºå¼•å¯¼å¼æ¨æµ‹æ¨æ–­ï¼ˆGSIï¼‰çš„æ–°ç®—æ³•ï¼Œè¯¥ç®—æ³•ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¥–åŠ±å¼•å¯¼è§£ç ï¼Œå®ç°é«˜æ•ˆæ¨ç†ã€‚GSIç»“åˆäº†è½¯é€‰æ‹©Nä¸ªæœ€ä½³ç­”æ¡ˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ã€å¥–åŠ±æ¨¡å‹r(x,y)ä»¥åŠæ¥è‡ªå°å‹è¾…åŠ©æ¨¡å‹Ï€S(yâˆ£x)çš„æ¨æµ‹æ ·æœ¬ã€‚è¯¥ç®—æ³•å¯è¿‘ä¼¼æ¨¡æ‹Ÿè½¯é€‰æ‹©Nä¸ªæœ€ä½³ç­”æ¡ˆä¸‹çš„æœ€ä¼˜å€¾æ–œç­–ç•¥Ï€Î²,B(yâˆ£x)ï¼Œå¹¶åœ¨ä¸»è¦æ¨¡å‹Ï€Bçš„ç†è®ºæ¡†æ¶ä¸‹æ¨å¯¼å‡ºä¸€ä¸ªå…³äºè¯±å¯¼åˆ†å¸ƒä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´KLæ•£åº¦çš„ç†è®ºç•Œé™ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMATH500ã€OlympiadBenchã€Minerva Mathï¼‰ä¸­ï¼Œè¯¥æ–¹æ³•è¾ƒæ ‡å‡†è½¯é€‰æ‹©Nä¸ªæœ€ä½³ç­”æ¡ˆæ–¹æ³•ä¸å¥–åŠ±å¼•å¯¼æ¨æµ‹è§£ç è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚ä»£ç å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ç®—æ³•GSIï¼Œç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¥–åŠ±å¼•å¯¼è§£ç ã€‚</li>
<li>GSIç»“åˆäº†è½¯é€‰æ‹©Nä¸ªæœ€ä½³ç­”æ¡ˆçš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ã€å¥–åŠ±æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹çš„æ¨æµ‹æ ·æœ¬ã€‚<br>3.GSIèƒ½å¤Ÿæ¨¡æ‹Ÿè½¯é€‰æ‹©Nä¸ªæœ€ä½³ç­”æ¡ˆçš„æœ€ä¼˜å€¾æ–œç­–ç•¥ã€‚<br>4.æ¨å¯¼å‡ºäº†è¯±å¯¼åˆ†å¸ƒä¸æœ€ä¼˜ç­–ç•¥ä¹‹é—´KLæ•£åº¦çš„ç†è®ºç•Œé™ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒGSIè¾ƒæ ‡å‡†æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä»£ç å·²å…¬å¼€äºGitHubä¸Šï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c903355e0ef97ab170bc1cf8e0622b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1321812e159a8a4a109cbc818566cbd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0c178d2bade050355024047b3464ba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7913d25280da196b467d9a3af3421035.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multimodal-Tabular-Reasoning-with-Privileged-Structured-Information"><a href="#Multimodal-Tabular-Reasoning-with-Privileged-Structured-Information" class="headerlink" title="Multimodal Tabular Reasoning with Privileged Structured Information"></a>Multimodal Tabular Reasoning with Privileged Structured Information</h2><p><strong>Authors:Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye</strong></p>
<p>Tabular reasoning involves multi-step information extraction and logical inference over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning from table images, leveraging privileged structured information available during training to enhance multimodal large language models (MLLMs). The key challenges lie in the complexity of accurately aligning structured information with visual representations, and in effectively transferring structured reasoning skills to MLLMs despite the input modality gap. To address these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a new framework for multimodal tabular reasoning with privileged structured tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, contributing to high-quality modality-bridged data. On this basis, {\sc Turbo} repeatedly generates and selects the advantageous reasoning paths, further enhancing the modelâ€™s tabular reasoning ability. Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo} achieves state-of-the-art performance ($+7.2%$ vs. previous SOTA) across multiple datasets. </p>
<blockquote>
<p>è¡¨æ ¼æ¨ç†æ¶‰åŠå¤šæ­¥éª¤çš„ä¿¡æ¯æå–å’Œè¡¨æ ¼æ•°æ®çš„é€»è¾‘æ¨ç†ã€‚å°½ç®¡æœ€è¿‘çš„è¿›å±•å·²ç»åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç»“æ„åŒ–è¡¨æ ¼è¿›è¡Œæ¨ç†ï¼Œä½†åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œé«˜è´¨é‡æ–‡æœ¬è¡¨ç¤ºé€šå¸¸ä¸å¯ç”¨ï¼Œè¡¨æ ¼é€šå¸¸ä»¥å›¾åƒçš„å½¢å¼å‡ºç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†ä»è¡¨æ ¼å›¾åƒä¸­è¿›è¡Œè¡¨æ ¼æ¨ç†çš„ä»»åŠ¡ï¼Œåˆ©ç”¨è®­ç»ƒæœŸé—´å¯ç”¨çš„ç‰¹æƒç»“æ„åŒ–ä¿¡æ¯æ¥å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºå‡†ç¡®åœ°å°†ç»“æ„åŒ–ä¿¡æ¯ä¸è§†è§‰è¡¨ç¤ºå¯¹é½çš„å¤æ‚æ€§ï¼Œä»¥åŠå°½ç®¡å­˜åœ¨è¾“å…¥æ¨¡å¼å·®è·ï¼Œä»ç„¶æœ‰æ•ˆåœ°å°†ç»“æ„åŒ–æ¨ç†æŠ€èƒ½è½¬ç§»åˆ°MLLMä¸Šã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å€ŸåŠ©ç‰¹æƒç»“æ„åŒ–è¡¨æ ¼çš„å¤šæ¨¡æ€è¡¨æ ¼æ¨ç†çš„æ–°æ¡†æ¶â€”â€”æ¡¥æ¥ä¿¡æ¯è¡¨æ¨ç†ï¼ˆTurboï¼‰ã€‚Turboå—ç›ŠäºåŸºäºDeepSeek-R1çš„ç»“æ„æ„ŸçŸ¥æ¨ç†è½¨è¿¹ç”Ÿæˆå™¨ï¼Œæœ‰åŠ©äºäº§ç”Ÿé«˜è´¨é‡çš„æ¨¡å¼æ¡¥æ¥æ•°æ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒTurboåå¤ç”Ÿæˆå¹¶é€‰æ‹©æœ‰åˆ©çš„æ¨ç†è·¯å¾„ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™ï¼ˆ9kï¼‰æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒTurboåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆè¾ƒä¹‹å‰çš„æœ€ä¼˜æ–¹æ¡ˆæé«˜7.2%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04088v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨çš„æ˜¯è¡¨æ ¼æ¨ç†ä»»åŠ¡ï¼Œå³ä»è¡¨æ ¼å›¾åƒä¸­è¿›è¡Œä¿¡æ¯æå–å’Œé€»è¾‘æ¨ç†ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬å½¢å¼çš„è¡¨æ ¼æ¨ç†ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­ï¼Œè¡¨æ ¼é€šå¸¸ä»¥å›¾åƒå½¢å¼å‡ºç°ï¼Œç¼ºä¹é«˜è´¨é‡æ–‡æœ¬è¡¨ç¤ºã€‚æ–‡ç« æå‡ºä¸€ä¸ªåˆ©ç”¨è®­ç»ƒæ—¶å¯ç”¨ç»“æ„ä¿¡æ¯å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¡†æ¶ï¼Œåä¸ºTabUlar Reasoning with Bridged infOrmationï¼ˆTurboï¼‰ã€‚Turboé€šè¿‡åŸºäºDeepSeek-R1çš„ç»“æ„æ„ŸçŸ¥æ¨ç†è½¨è¿¹ç”Ÿæˆå™¨ï¼Œäº§ç”Ÿé«˜è´¨é‡æ¨¡æ€æ¡¥æ¥æ•°æ®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒTurboåå¤ç”Ÿæˆå¹¶é€‰æ‹©æœ‰åˆ©çš„æ¨ç†è·¯å¾„ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æœ‰é™çš„æ•°æ®ä¸‹ï¼ŒTurboåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼æ¨ç†æ¶‰åŠä»è¡¨æ ¼å›¾åƒä¸­è¿›è¡Œå¤šæ­¥éª¤ä¿¡æ¯æå–å’Œé€»è¾‘æ¨ç†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬å½¢å¼çš„è¡¨æ ¼æ¨ç†ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­çš„è¡¨æ ¼å›¾åƒä¸Šç¼ºä¹é«˜è´¨é‡æ–‡æœ¬è¡¨ç¤ºã€‚</li>
<li>æå‡ºçš„Turboæ¡†æ¶åˆ©ç”¨è®­ç»ƒæ—¶çš„ç»“æ„ä¿¡æ¯å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>Turboé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å‡†ç¡®å¯¹é½ç»“æ„ä¿¡æ¯ä¸è§†è§‰è¡¨ç¤ºï¼Œä»¥åŠè§£å†³è¾“å…¥æ¨¡æ€å·®å¼‚å¯¼è‡´çš„æ¨ç†æŠ€èƒ½è½¬ç§»é—®é¢˜ã€‚</li>
<li>Turboé‡‡ç”¨åŸºäºDeepSeek-R1çš„ç»“æ„æ„ŸçŸ¥æ¨ç†è½¨è¿¹ç”Ÿæˆå™¨ï¼Œç”Ÿæˆé«˜è´¨é‡æ¨¡æ€æ¡¥æ¥æ•°æ®ã€‚</li>
<li>Turboé€šè¿‡åå¤ç”Ÿæˆå¹¶é€‰æ‹©æœ‰åˆ©çš„æ¨ç†è·¯å¾„ï¼Œå¢å¼ºäº†æ¨¡å‹çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e24de5c2e7d18a00092edebe5191db8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-625204b35bf13cd3d064859f98d40a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84138ad949b99cf89829a97ba2fe8da6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Generative-Adaptive-Replay-Continual-Learning-Model-for-Temporal-Knowledge-Graph-Reasoning"><a href="#A-Generative-Adaptive-Replay-Continual-Learning-Model-for-Temporal-Knowledge-Graph-Reasoning" class="headerlink" title="A Generative Adaptive Replay Continual Learning Model for Temporal   Knowledge Graph Reasoning"></a>A Generative Adaptive Replay Continual Learning Model for Temporal   Knowledge Graph Reasoning</h2><p><strong>Authors:Zhiyu Zhang, Wei Chen, Youfang Lin, Huaiyu Wan</strong></p>
<p>Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning (TKGR) methods focus on significantly reducing computational cost and mitigating catastrophic forgetting caused by fine-tuning models with new data. However, existing CL-based TKGR methods still face two key limitations: (1) They usually one-sidedly reorganize individual historical facts, while overlooking the historical context essential for accurately understanding the historical semantics of these facts; (2) They preserve historical knowledge by simply replaying historical facts, while ignoring the potential conflicts between historical and emerging facts. In this paper, we propose a Deep Generative Adaptive Replay (DGAR) method, which can generate and adaptively replay historical entity distribution representations from the whole historical context. To address the first challenge, historical context prompts as sampling units are built to preserve the whole historical context information. To overcome the second challenge, a pre-trained diffusion model is adopted to generate the historical distribution. During the generation process, the common features between the historical and current distributions are enhanced under the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay mechanism is designed to effectively integrate historical and current distributions. Experimental results demonstrate that DGAR significantly outperforms baselines in reasoning and mitigating forgetting. </p>
<blockquote>
<p>æœ€è¿‘çš„åŸºäºæŒç»­å­¦ä¹ ï¼ˆCLï¼‰çš„æ—¶é—´çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆTKGRï¼‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬å¹¶å‡è½»ç”±äºä½¿ç”¨æ–°æ•°æ®å¾®è°ƒæ¨¡å‹è€Œäº§ç”Ÿçš„ç¾éš¾æ€§é—å¿˜ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºCLçš„TKGRæ–¹æ³•ä»é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šï¼ˆ1ï¼‰å®ƒä»¬é€šå¸¸å•æ–¹é¢åœ°é‡æ–°ç»„ç»‡å•ä¸ªå†å²äº‹å®ï¼Œè€Œå¿½ç•¥äº†å¯¹äºå‡†ç¡®ç†è§£è¿™äº›äº‹å®çš„å†å²è¯­ä¹‰è‡³å…³é‡è¦çš„å†å²ä¸Šä¸‹æ–‡ï¼›ï¼ˆ2ï¼‰å®ƒä»¬é€šè¿‡ç®€å•åœ°é‡æ”¾å†å²äº‹å®æ¥ä¿ç•™å†å²çŸ¥è¯†ï¼Œè€Œå¿½ç•¥äº†å†å²äº‹å®ä¸æ–°å…´äº‹å®ä¹‹é—´çš„æ½œåœ¨å†²çªã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦ç”Ÿæˆè‡ªé€‚åº”é‡æ”¾ï¼ˆDGARï¼‰æ–¹æ³•ï¼Œå®ƒå¯ä»¥ç”Ÿæˆå¹¶è‡ªé€‚åº”åœ°é‡æ”¾æ•´ä¸ªå†å²ä¸Šä¸‹æ–‡çš„å®ä½“åˆ†å¸ƒè¡¨ç¤ºã€‚ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œæ„å»ºäº†å†å²ä¸Šä¸‹æ–‡æç¤ºä½œä¸ºé‡‡æ ·å•å…ƒæ¥ä¿ç•™æ•´ä¸ªå†å²ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†å…‹æœç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œé‡‡ç”¨äº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå†å²åˆ†å¸ƒã€‚åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œåœ¨TKGRæ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œå†å²å’Œå½“å‰åˆ†å¸ƒä¹‹é—´çš„å…±åŒç‰¹å¾å¾—åˆ°äº†å¢å¼ºã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§é€å±‚è‡ªé€‚åº”é‡æ”¾æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆå†å²å’Œå½“å‰åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGARåœ¨æ¨ç†å’Œå‡è½»é—å¿˜æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæŒç»­å­¦ä¹ ï¼ˆCLï¼‰çš„æ—¶é—´çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆTKGRï¼‰æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œè¿™äº›æ–¹æ³•æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶å‡è½»ç”±äºæ–°æ•°æ®å¾®è°ƒæ¨¡å‹å¯¼è‡´çš„ç¾éš¾æ€§é—å¿˜ã€‚é’ˆå¯¹ç°æœ‰CL-based TKGRæ–¹æ³•çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDeep Generative Adaptive Replayï¼ˆDGARï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¹¶è‡ªé€‚åº”åœ°é‡æ–°æ’­æ”¾æ•´ä¸ªå†å²è¯­å¢ƒä¸­çš„å®ä½“åˆ†å¸ƒè¡¨ç¤ºæ¥è§£å†³æŒ‘æˆ˜ã€‚DGARé€šè¿‡æ„å»ºå†å²è¯­å¢ƒæç¤ºä½œä¸ºé‡‡æ ·å•å…ƒæ¥ä¿ç•™æ•´ä¸ªå†å²è¯­å¢ƒä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå†å²åˆ†å¸ƒã€‚åŒæ—¶ï¼Œè®¾è®¡äº†ä¸€ç§é€å±‚è‡ªé€‚åº”å›æ”¾æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆå†å²å’Œå½“å‰åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDGARåœ¨æ¨ç†å’Œé˜²æ­¢é—å¿˜æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºæŒç»­å­¦ä¹ ï¼ˆCLï¼‰çš„æ—¶é—´çŸ¥è¯†å›¾è°±æ¨ç†ï¼ˆTKGRï¼‰æ–¹æ³•æ—¨åœ¨é™ä½è®¡ç®—æˆæœ¬å¹¶å‡å°‘ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>ç°æœ‰CL-based TKGRæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šå¿½è§†å†å²ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œå†å²ä¸æ–°å…´äº‹å®ä¹‹é—´çš„æ½œåœ¨å†²çªã€‚</li>
<li>Deep Generative Adaptive Replay (DGAR) æ–¹æ³•é€šè¿‡ç”Ÿæˆå¹¶è‡ªé€‚åº”åœ°é‡æ–°æ’­æ”¾å®ä½“åˆ†å¸ƒè¡¨ç¤ºæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DGARæ„å»ºå†å²è¯­å¢ƒæç¤ºä½œä¸ºé‡‡æ ·å•å…ƒï¼Œä»¥ä¿ç•™æ•´ä¸ªå†å²è¯­å¢ƒä¿¡æ¯ã€‚</li>
<li>DGARé‡‡ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆå†å²åˆ†å¸ƒï¼Œå¢å¼ºå†å²å’Œå½“å‰åˆ†å¸ƒä¹‹é—´çš„å…±åŒç‰¹å¾ã€‚</li>
<li>DGARè®¾è®¡äº†ä¸€ç§é€å±‚è‡ªé€‚åº”å›æ”¾æœºåˆ¶ï¼Œä»¥æ•´åˆå†å²å’Œå½“å‰åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd6b8c4ba6bf4e22b5a1e71c673dd766.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2c749179a72802656137832c7e03b40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d9f069a0eeded8b88724b462b5b2299.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LaF-GRPO-In-Situ-Navigation-Instruction-Generation-for-the-Visually-Impaired-via-GRPO-with-LLM-as-Follower-Reward"><a href="#LaF-GRPO-In-Situ-Navigation-Instruction-Generation-for-the-Visually-Impaired-via-GRPO-with-LLM-as-Follower-Reward" class="headerlink" title="LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually   Impaired via GRPO with LLM-as-Follower Reward"></a>LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually   Impaired via GRPO with LLM-as-Follower Reward</h2><p><strong>Authors:Yi Zhao, Siqi Wang, Jing Li</strong></p>
<p>Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study, hence, focuses on producing precise, in-situ, step-by-step navigation instructions that are practically usable by VI users. Concretely, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate rewards guiding the Vision-Language Model (VLM) post-training. This enhances instruction usability while reducing costly real-world data needs. To facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced benchmark. It provides diverse navigation scenarios with accurate spatial coordinates, supporting detailed, open-ended in-situ instruction generation. Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4oâ€™s 0.323) and yields more intuitive, safer instructions. Code and benchmark are available at \href{<a target="_blank" rel="noopener" href="https://github.com/YiyiyiZhao/NIG4VI%7D%7Bhttps://github.com/YiyiyiZhao/NIG4VI%7D">https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}</a>. </p>
<blockquote>
<p>ä¸ºè§†éšœï¼ˆVIï¼‰ä¸ªä½“ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤ï¼ˆNIG-VIï¼‰è‡³å…³é‡è¦ï¼Œä½†ç›¸å¯¹ç ”ç©¶ä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶ä¸“æ³¨äºç”Ÿæˆç²¾ç¡®ã€ç°åœºã€åˆ†æ­¥çš„å¯¼èˆªæŒ‡ä»¤ï¼Œä½¿VIç”¨æˆ·èƒ½å¤Ÿå®é™…ä½¿ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºLaF-GRPOï¼ˆLLMä½œä¸ºè·Ÿéšè€…çš„GRPOï¼‰ï¼Œå…¶ä¸­LLMæ¨¡æ‹ŸVIç”¨æˆ·çš„å“åº”æ¥ç”Ÿæˆå¥–åŠ±ï¼ŒæŒ‡å¯¼è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åæœŸè®­ç»ƒã€‚è¿™æé«˜äº†æŒ‡ä»¤çš„å®ç”¨æ€§ï¼ŒåŒæ—¶é™ä½äº†å¯¹æ˜‚è´µç°å®ä¸–ç•Œæ•°æ®çš„éœ€æ±‚ã€‚ä¸ºäº†ä¿ƒè¿›è®­ç»ƒå’Œæµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†NIG4VIï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰27kæ ·æœ¬çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚å®ƒæä¾›äº†å…·æœ‰å‡†ç¡®ç©ºé—´åæ ‡çš„å¤šæ ·åŒ–å¯¼èˆªåœºæ™¯ï¼Œæ”¯æŒè¯¦ç»†ã€å¼€æ”¾å¼çš„ç°åœºæŒ‡ä»¤ç”Ÿæˆã€‚åœ¨NIG4VIä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLaF-GRPOé€šè¿‡å®šé‡æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œé›¶ï¼ˆLaF-GRPOï¼‰æå‡BLEU 14%ï¼›SFT+ï¼ˆLaF-GRPOï¼‰METEOR 0.542 å¯¹æ¯” GPT-4oçš„0.323ï¼‰æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶äº§ç”Ÿæ›´ç›´è§‚ã€æ›´å®‰å…¨çš„æŒ‡ä»¤ã€‚ä»£ç å’ŒåŸºå‡†æµ‹è¯•å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YiyiyiZhao/NIG4VI%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YiyiyiZhao/NIG4VIè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04070v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥ç ”ç©¶å…³æ³¨ä¸ºè§†éšœäººå£«ç”Ÿæˆç²¾ç¡®ã€å®æ—¶ã€åˆ†æ­¥éª¤çš„å¯¼èˆªæŒ‡ä»¤ã€‚æå‡ºLaF-GRPOæ–¹æ³•ï¼Œé€šè¿‡æ¨¡æ‹Ÿè§†éšœç”¨æˆ·åé¦ˆæ¥æŒ‡å¯¼è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åç»­è®­ç»ƒï¼Œæå‡æŒ‡ä»¤å®ç”¨æ€§å¹¶å‡å°‘æ˜‚è´µçœŸå®ä¸–ç•Œæ•°æ®éœ€æ±‚ã€‚å¼•å…¥NIG4VIå¼€æ”¾æºä»£ç åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ”¯æŒå¤šæ ·åŒ–å¯¼èˆªåœºæ™¯å’Œç²¾ç¡®ç©ºé—´åæ ‡çš„å¼€æ”¾ç«¯å®æ—¶æŒ‡ä»¤ç”Ÿæˆã€‚å®éªŒè¯æ˜LaF-GRPOæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶å…³æ³¨ä¸ºè§†éšœäººå£«ç”Ÿæˆå¯¼èˆªæŒ‡ä»¤çš„é‡è¦æ€§åŠå…¶ç°æœ‰ç ”ç©¶çš„ä¸è¶³ã€‚</li>
<li>æå‡ºLaF-GRPOæ–¹æ³•ï¼Œæ¨¡æ‹Ÿè§†éšœç”¨æˆ·åé¦ˆä»¥ä¼˜åŒ–å¯¼èˆªæŒ‡ä»¤ç”Ÿæˆã€‚</li>
<li>LaF-GRPOæ–¹æ³•é€šè¿‡æå‡æŒ‡ä»¤å®ç”¨æ€§ï¼ŒåŒæ—¶é™ä½å¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„ä¾èµ–ã€‚</li>
<li>å¼•å…¥NIG4VIå¼€æ”¾æºä»£ç åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ”¯æŒå¤šæ ·åŒ–å¯¼èˆªåœºæ™¯çš„å®æ—¶æŒ‡ä»¤ç”Ÿæˆã€‚</li>
<li>NIG4VIå¹³å°æä¾›å‡†ç¡®ç©ºé—´åæ ‡ï¼Œæœ‰åŠ©äºç”Ÿæˆæ›´ç²¾ç¡®çš„å¯¼èˆªæŒ‡ä»¤ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºLaF-GRPOæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å®šé‡æŒ‡æ ‡çš„æå‡å’Œæ›´ç›´è§‚ã€å®‰å…¨çš„æŒ‡ä»¤ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0829676b1d7c72f820a63428ef0633a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18e156a10a93725cd49022209bb610a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c5569e6b0915a3e90b15c93b50cc19a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e029c603744919f25d08ea9acea4475.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb14e2ac82ee2e5ddd0e2ba16bf0a36f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Progressive-Mastery-Customized-Curriculum-Learning-with-Guided-Prompting-for-Mathematical-Reasoning"><a href="#Progressive-Mastery-Customized-Curriculum-Learning-with-Guided-Prompting-for-Mathematical-Reasoning" class="headerlink" title="Progressive Mastery: Customized Curriculum Learning with Guided   Prompting for Mathematical Reasoning"></a>Progressive Mastery: Customized Curriculum Learning with Guided   Prompting for Mathematical Reasoning</h2><p><strong>Authors:Muling Wu, Qi Qian, Wenhao Liu, Xiaohua Wang, Zisu Huang, Di Liang, LI Miao, Shihan Dou, Changze Lv, Zhenghua Wang, Zhibo Xu, Lina Chen, Tianlong Li, Xiaoqing Zheng, Xuanjing Huang</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable performance across various reasoning tasks, yet post-training is constrained by inefficient sample utilization and inflexible difficulty samples processing. To address these limitations, we propose Customized Curriculum Learning (CCL), a novel framework with two key innovations. First, we introduce model-adaptive difficulty definition that customizes curriculum datasets based on each modelâ€™s individual capabilities rather than using predefined difficulty metrics. Second, we develop â€œGuided Prompting,â€ which dynamically reduces sample difficulty through strategic hints, enabling effective utilization of challenging samples that would otherwise degrade performance. Comprehensive experiments on supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks, confirming its effectiveness across both paradigms in enhancing sample utilization and model performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä½†æ˜¯åœ¨è®­ç»ƒåå—åˆ°äº†æ ·æœ¬åˆ©ç”¨æ•ˆç‡ä½ä¸‹å’Œéš¾åº¦æ ·æœ¬å¤„ç†ä¸çµæ´»çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å®šåˆ¶è¯¾ç¨‹å­¦ä¹ ï¼ˆCCLï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨¡å‹è‡ªé€‚åº”éš¾åº¦å®šä¹‰ï¼Œæ ¹æ®æ¯ä¸ªæ¨¡å‹çš„èƒ½åŠ›å®šåˆ¶è¯¾ç¨‹æ•°æ®é›†ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é¢„å…ˆå®šä¹‰çš„éš¾åº¦æŒ‡æ ‡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†â€œå¼•å¯¼æç¤ºâ€ï¼Œé€šè¿‡ç­–ç•¥æ€§æç¤ºåŠ¨æ€é™ä½æ ·æœ¬éš¾åº¦ï¼Œæœ‰æ•ˆåˆ©ç”¨åŸæœ¬ä¼šé™ä½æ€§èƒ½çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ã€‚åœ¨ç›‘ç£å¾®è°ƒä»»åŠ¡å’Œå¼ºåŒ–å­¦ä¹ çš„å…¨é¢å®éªŒè¡¨æ˜ï¼ŒCCLåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå‡åŒ€è®­ç»ƒæ–¹æ³•ï¼Œè¯å®å…¶åœ¨æé«˜æ ·æœ¬åˆ©ç”¨å’Œæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ— è®ºåœ¨ä½•ç§æ¨¡å¼ä¸‹ï¼ŒCCLéƒ½èƒ½å¤Ÿå¸®åŠ©å¢å¼ºæ¨¡å‹è¡¨ç°å¹¶æ˜¾è‘—æé«˜æ ·æœ¬åˆ©ç”¨ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04065v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨è®­ç»ƒåå—åˆ°æ ·æœ¬åˆ©ç”¨ä¸è¶³å’Œéš¾åº¦æ ·æœ¬å¤„ç†ä¸çµæ´»çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å®šåˆ¶è¯¾ç¨‹å­¦ä¹ ï¼ˆCCLï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå…¶ä¸­åŒ…å«ä¸¤å¤§åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯é‡‡ç”¨æ¨¡å‹è‡ªé€‚åº”éš¾åº¦å®šä¹‰ï¼Œæ ¹æ®æ¯ä¸ªæ¨¡å‹çš„èƒ½åŠ›å®šåˆ¶è¯¾ç¨‹æ•°æ®é›†ï¼Œè€Œéä½¿ç”¨é¢„å…ˆå®šä¹‰çš„éš¾åº¦æŒ‡æ ‡ï¼›äºŒæ˜¯å¼€å‘â€œå¼•å¯¼æç¤ºâ€ï¼Œé€šè¿‡ç­–ç•¥æ€§æç¤ºåŠ¨æ€é™ä½æ ·æœ¬éš¾åº¦ï¼Œæœ‰æ•ˆåˆ©ç”¨åŸæœ¬ä¼šé™ä½æ€§èƒ½çš„æ ·æœ¬ã€‚åœ¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ–¹é¢çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒCCLåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºç»Ÿä¸€è®­ç»ƒæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æ ·æœ¬åˆ©ç”¨ç‡å’Œæ¨¡å‹æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å­˜åœ¨æ ·æœ¬åˆ©ç”¨ä¸è¶³å’Œéš¾åº¦æ ·æœ¬å¤„ç†ä¸çµæ´»çš„é—®é¢˜ã€‚</li>
<li>å®šåˆ¶è¯¾ç¨‹å­¦ä¹ ï¼ˆCCLï¼‰æ¡†æ¶é€šè¿‡æ¨¡å‹è‡ªé€‚åº”éš¾åº¦å®šä¹‰å’Œâ€œå¼•å¯¼æç¤ºâ€ä¸¤å¤§åˆ›æ–°æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¨¡å‹è‡ªé€‚åº”éš¾åº¦å®šä¹‰æ˜¯æ ¹æ®æ¯ä¸ªæ¨¡å‹çš„èƒ½åŠ›æ¥å®šåˆ¶è¯¾ç¨‹æ•°æ®é›†ï¼Œè€Œéä¾èµ–é¢„è®¾çš„éš¾åº¦æŒ‡æ ‡ã€‚</li>
<li>â€œå¼•å¯¼æç¤ºâ€èƒ½åŠ¨æ€é™ä½æ ·æœ¬éš¾åº¦ï¼Œæœ‰æ•ˆè¿ç”¨åŸæœ¬å¯èƒ½é™ä½æ¨¡å‹æ€§èƒ½çš„æ ·æœ¬ã€‚</li>
<li>ç»¼åˆå®éªŒæ˜¾ç¤ºï¼ŒCCLåœ¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿè®­ç»ƒæ–¹æ³•ã€‚</li>
<li>CCLåœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cf3614541980f73de76f9b47b393cace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d280093da566e41b94331e92c3ca9cf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-548ef6dac6c189d1cd26c8257c0dea55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f8e0af764611bd8e6628e5126dacdb8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rex-Thinker-Grounded-Object-Referring-via-Chain-of-Thought-Reasoning"><a href="#Rex-Thinker-Grounded-Object-Referring-via-Chain-of-Thought-Reasoning" class="headerlink" title="Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning"></a>Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning</h2><p><strong>Authors:Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, Lei Zhang</strong></p>
<p>Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings. </p>
<blockquote>
<p>å¯¹è±¡å¼•ç”¨æ—¨åœ¨æ£€æµ‹å›¾åƒä¸­æ‰€æœ‰ä¸ç»™å®šè‡ªç„¶è¯­è¨€æè¿°åŒ¹é…çš„å¯¹è±¡ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œä¸€ä¸ªç¨³å¥çš„å¯¹è±¡å¼•ç”¨æ¨¡å‹åº”è¯¥æ˜¯æœ‰æ ¹æ®çš„ï¼Œè¿™æ„å‘³ç€å®ƒçš„é¢„æµ‹åº”è¯¥æ˜¯å¯è§£é‡Šçš„ï¼Œå¹¶ä¸”å¯¹è§†è§‰å†…å®¹ä¿æŒå¿ è¯šã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåº”è¯¥æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªå…³é”®å±æ€§ï¼š1ï¼‰å¯éªŒè¯æ€§ï¼Œé€šè¿‡äº§ç”Ÿå¯è§£é‡Šçš„ç†ç”±æ¥è¯æ˜å…¶é¢„æµ‹ï¼Œå¹¶å°†å…¶ä¸è§†è§‰è¯æ®æ˜ç¡®è”ç³»èµ·æ¥ï¼›2ï¼‰å¯é æ€§ï¼Œé€šè¿‡å­¦ä¹ åœ¨å›¾åƒä¸­æ²¡æœ‰å¯¹è±¡æ»¡è¶³ç»™å®šè¡¨è¾¾å¼æ—¶é€‰æ‹©æ”¾å¼ƒã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•å°†å¼•ç”¨ä½œä¸ºç›´æ¥çš„è¾¹ç•Œæ¡†é¢„æµ‹ä»»åŠ¡ï¼Œè¿™æä¾›äº†æœ‰é™çš„è§£é‡Šæ€§ï¼Œå¹¶ä¸”åœ¨æ‹’ç»æ²¡æœ‰åŒ¹é…å¯¹è±¡çš„è¡¨è¾¾å¼æ—¶é‡åˆ°äº†å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Rex-Thinkeræ¨¡å‹ï¼Œå®ƒå°†å¯¹è±¡å¼•ç”¨åˆ¶å®šä¸ºä¸€ä¸ªæ˜ç¡®çš„è®¤çŸ¥æ¨ç†ä»»åŠ¡ã€‚ç»™å®šä¸€ä¸ªå¼•ç”¨è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«æ‰€æœ‰å¯¹åº”äºæ‰€å¼•ç”¨å¯¹è±¡ç±»åˆ«çš„å€™é€‰å¯¹è±¡å®ä¾‹ã€‚Rex-Thinkeréšåå¯¹æ¯ä¸€ä¸ªå€™é€‰å¯¹è±¡è¿›è¡Œé€æ­¥æ¨ç†ï¼Œä»¥è¯„ä¼°å…¶æ˜¯å¦ä¸ç»™å®šçš„è¡¨è¾¾å¼åŒ¹é…ï¼Œç„¶ååšå‡ºæœ€ç»ˆé¢„æµ‹ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€èŒƒå¼ï¼Œæˆ‘ä»¬ä½¿ç”¨GPT-4oåœ¨HumanRefæ•°æ®é›†ä¸Šæ„å»ºäº†å¤§è§„æ¨¡çš„è®¤çŸ¥é£æ ¼å¼•ç”¨æ•°æ®é›†HumanRef-CoTã€‚æ¯ä¸ªæ¨ç†è½¨è¿¹éƒ½éµå¾ªç»“æ„åŒ–è§„åˆ’ã€è¡ŒåŠ¨å’Œæ€»ç»“æ ¼å¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¯¹è±¡å€™é€‰è€…ä¸Šè¿›è¡Œåˆ†è§£å’Œå¯è§£é‡Šæ¨ç†ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†Rex-Thinkeråˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆæ˜¯å†·å¯åŠ¨ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œæ•™ä¼šæ¨¡å‹å¦‚ä½•è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œå…¶æ¬¡æ˜¯åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ æ¥æé«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»åŸŸè¯„ä¼°ä¸­çš„ç²¾åº¦å’Œè§£é‡Šæ€§æ–¹é¢ä¼˜äºæ ‡å‡†åŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºæ‹’ç»å¹»è§‰è¾“å‡ºå’Œæé«˜ç¦»åŸŸè®¾ç½®ä¸­çš„æ³›åŒ–èƒ½åŠ›çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04034v1">PDF</a> homepage: <a target="_blank" rel="noopener" href="https://rexthinker.github.io/">https://rexthinker.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRex-Thinkerçš„å¯¹è±¡å¼•ç”¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†å¯¹è±¡å¼•ç”¨è¡¨è¾¾ä¸ºæ˜ç¡®çš„è®¤çŸ¥æ¨ç†ä»»åŠ¡ã€‚ç»™å®šä¸€ä¸ªå¼•ç”¨è¡¨è¾¾å¼ï¼ŒRex-Thinkeré¦–å…ˆè¯†åˆ«æ‰€æœ‰ä¸å¼•ç”¨å¯¹è±¡ç±»åˆ«å¯¹åº”çš„å€™é€‰å¯¹è±¡å®ä¾‹ã€‚ç„¶åï¼Œå®ƒä¼šå¯¹æ¯ä¸ªå€™é€‰å¯¹è±¡è¿›è¡Œé€æ­¥æ¨ç†ï¼Œè¯„ä¼°å…¶æ˜¯å¦ä¸ç»™å®šçš„è¡¨è¾¾å¼åŒ¹é…ï¼Œæœ€ååšå‡ºé¢„æµ‹ã€‚ä¸ºæ­¤ï¼Œæ„å»ºäº†å¤§è§„æ¨¡çš„è®¤çŸ¥è½¨è¿¹é£æ ¼å¼•ç”¨æ•°æ®é›†HumanRef-CoTï¼Œå¹¶é€šè¿‡GPT-4oåœ¨HumanRefæ•°æ®é›†ä¸Šè¿›è¡Œæç¤ºã€‚è®­ç»ƒRex-Thinkeråˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯å†·å¯åŠ¨ç›‘ç£å¾®è°ƒé˜¶æ®µï¼Œæ•™ä¼šæ¨¡å‹å¦‚ä½•è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼›ç„¶åæ˜¯åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œæé«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸŸå†…è¯„ä¼°çš„ç²¾åº¦å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºæ ‡å‡†åŸºçº¿ï¼ŒåŒæ—¶è¡¨ç°å‡ºæ›´å¼ºçš„æ‹’ç»è™šæ„è¾“å‡ºçš„èƒ½åŠ›å’Œè‰¯å¥½çš„åŸŸå¤–æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rex-Thinkeræ¨¡å‹å°†å¯¹è±¡å¼•ç”¨è¡¨è¾¾ä¸ºæ˜ç¡®çš„è®¤çŸ¥æ¨ç†ä»»åŠ¡ï¼Œä»¥æé«˜æ¨¡å‹çš„è§£é‡Šæ€§å’Œå¯¹è§†è§‰å†…å®¹çš„å¿ å®æ€§ã€‚</li>
<li>Rex-Thinkeré€šè¿‡é€æ­¥æ¨ç†è¯„ä¼°å€™é€‰å¯¹è±¡ä¸ç»™å®šè¡¨è¾¾å¼çš„åŒ¹é…ç¨‹åº¦ï¼Œå†è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºHumanRef-CoTçš„å¤§è§„æ¨¡è®¤çŸ¥è½¨è¿¹é£æ ¼å¼•ç”¨æ•°æ®é›†ï¼Œç”¨äºæ”¯æŒRex-Thinkeræ¨¡å‹ã€‚</li>
<li>HumanRef-CoTæ•°æ®é›†é€šè¿‡GPT-4oåœ¨HumanRefæ•°æ®é›†ä¸Šçš„æç¤ºè¿›è¡Œæ„å»ºï¼Œéµå¾ªç»“æ„åŒ–è§„åˆ’ã€è¡ŒåŠ¨å’Œæ€»ç»“æ ¼å¼ã€‚</li>
<li>Rex-Thinkerçš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç›‘ç£å¾®è°ƒé˜¶æ®µå’ŒåŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRex-Thinkeråœ¨ç²¾åº¦å’Œå¯è§£é‡Šæ€§æ–¹é¢ä¼˜äºæ ‡å‡†åŸºçº¿ï¼Œå¹¶èƒ½æ›´å¥½åœ°æ‹’ç»è™šæ„è¾“å‡ºå’Œæ³›åŒ–åˆ°æœªçŸ¥é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05e6cd5742cd6da5ef040e2c6213d1d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a031084980bb87a338f4816dc5395f8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d0c8ceb259a086b22ccdb91a407131.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d76c16be64b52b857d17ead25577788a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Structured-Pruning-for-Diverse-Best-of-N-Reasoning-Optimization"><a href="#Structured-Pruning-for-Diverse-Best-of-N-Reasoning-Optimization" class="headerlink" title="Structured Pruning for Diverse Best-of-N Reasoning Optimization"></a>Structured Pruning for Diverse Best-of-N Reasoning Optimization</h2><p><strong>Authors:Hieu Trung Nguyen, Bao Nguyen, Viet Anh Nguyen</strong></p>
<p>Model pruning in transformer-based language models, traditionally viewed as a means of achieving computational savings, can enhance the modelâ€™s reasoning capabilities. In this work, we uncover a surprising phenomenon: the selective pruning of certain attention heads leads to improvements in reasoning performance, particularly on challenging tasks. Motivated by this observation, we propose SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference. By aligning question embeddings with head embeddings, SPRINT identifies those pruned-head configurations that result in more accurate reasoning. Extensive experiments demonstrate that our method significantly outperforms traditional best-of-$N$ and random head selection strategies on the MATH500 and GSM8K datasets. </p>
<blockquote>
<p>åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹ä¸­çš„æ¨¡å‹å‰ªæï¼Œä¼ ç»Ÿä¸Šè¢«è§†ä¸ºå®ç°è®¡ç®—èŠ‚çœçš„æ‰‹æ®µï¼Œä½†èƒ½å¤Ÿå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªä»¤äººæƒŠè®¶çš„ç°è±¡ï¼šé€‰æ‹©æ€§å‰ªææŸäº›æ³¨æ„åŠ›å¤´æœ‰åŠ©äºæé«˜æ¨ç†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šã€‚å—æ­¤è§‚å¯Ÿç»“æœçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶SPRINTï¼Œå®ƒåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©æœ€ä½³çš„å¤´éƒ¨å’Œå±‚è¿›è¡Œå‰ªæã€‚é€šè¿‡é—®é¢˜åµŒå…¥ä¸å¤´éƒ¨åµŒå…¥çš„å¯¹é½ï¼ŒSPRINTå¯ä»¥è¯†åˆ«é‚£äº›å‰ªæå¤´éƒ¨é…ç½®èƒ½å¤Ÿå¯¼è‡´æ›´å‡†ç¡®çš„æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MATH500å’ŒGSM8Kæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æœ€ä½³Né€‰å’Œéšæœºå¤´éƒ¨é€‰æ‹©ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03978v1">PDF</a> Accepted to ACL 2025</p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹ä¿®å‰ªå¯ä»¥å¢å¼ºåŸºäºTransformerçš„è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å‘ç°é€‰æ‹©æ€§ä¿®å‰ªæŸäº›æ³¨æ„åŠ›å¤´èƒ½æå‡æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šçš„æ¨ç†æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€é€‰æ‹©æœ€ä½³å¤´å±‚è¿›è¡Œä¿®å‰ªçš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶â€”â€”SPRINTã€‚é€šè¿‡é—®é¢˜åµŒå…¥ä¸å¤´åµŒå…¥çš„å¯¹é½ï¼ŒSPRINTèƒ½å¤Ÿè¯†åˆ«å‡ºèƒ½å¤Ÿæå‡æ¨ç†å‡†ç¡®ç‡çš„ä¿®å‰ªå¤´é…ç½®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MATH500å’ŒGSM8Kæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„æœ€ä½³-Né€‰æ‹©å’Œéšæœºå¤´é€‰æ‹©ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹ä¿®å‰ªä¸ä»…èƒ½èŠ‚çœè®¡ç®—æˆæœ¬ï¼Œè¿˜èƒ½æå‡åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ³¨æ„åŠ›å¤´çš„é€‰æ‹©æ€§ä¿®å‰ªèƒ½æ˜¾è‘—æé«˜æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>SPRINTæ˜¯ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‰æ‹©æœ€ä½³çš„å¤´éƒ¨å’Œå±‚è¿›è¡Œä¿®å‰ªã€‚</li>
<li>SPRINTé€šè¿‡é—®é¢˜åµŒå…¥ä¸å¤´åµŒå…¥çš„å¯¹é½ï¼Œæœ‰æ•ˆè¯†åˆ«æœ‰åŠ©äºæå‡æ¨ç†å‡†ç¡®ç‡çš„ä¿®å‰ªé…ç½®ã€‚</li>
<li>ä¸ä¼ ç»Ÿé€‰æ‹©ç­–ç•¥ç›¸æ¯”ï¼ŒSPRINTåœ¨MATH500å’ŒGSM8Kæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>SPRINTæ¡†æ¶èƒ½åº”å¯¹å¤§è§„æ¨¡æ¨¡å‹ä¸­çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0491165cf281f56f8cf4d4ee7f1cc4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61a53a3af4acdbc7b6d8d74d5a51c5e3" align="middle">
<img src="https://pic1.zhimg.com/v2-e5175bb7c42bddbfa730b5373bd8e837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3a1b9f3618223f79a4ead785f2ef119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e47fe5e8b1d7844c182563e13febd74c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="TableEval-A-Real-World-Benchmark-for-Complex-Multilingual-and-Multi-Structured-Table-Question-Answering"><a href="#TableEval-A-Real-World-Benchmark-for-Complex-Multilingual-and-Multi-Structured-Table-Question-Answering" class="headerlink" title="TableEval: A Real-World Benchmark for Complex, Multilingual, and   Multi-Structured Table Question Answering"></a>TableEval: A Real-World Benchmark for Complex, Multilingual, and   Multi-Structured Table Question Answering</h2><p><strong>Authors:Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu</strong></p>
<p>LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: <a target="_blank" rel="noopener" href="https://github.com/wenge-research/TableEval">https://github.com/wenge-research/TableEval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ã€‚ç„¶è€Œï¼Œåœ¨è¡¨æ ¼é—®ç­”ï¼ˆTableQAï¼‰é¢†åŸŸï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¡¨æ ¼é—®ç­”ä¸­ï¼Œç°å®ä¸–ç•Œä¸­çš„å¤æ‚æ€§è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å„ç§è¡¨æ ¼ç»“æ„ã€å¤šè¯­è¨€æ•°æ®å’Œç‰¹å®šé¢†åŸŸçš„æ¨ç†ã€‚ç°æœ‰çš„TableQAåŸºå‡†æµ‹è¯•é€šå¸¸å±€é™äºç®€å•çš„å¹³é¢è¡¨æ ¼ï¼Œå¹¶å­˜åœ¨æ•°æ®æ³„éœ²çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°åŸºå‡†æµ‹è¯•éƒ½æ˜¯å•è¯­è¨€çš„ï¼Œæ— æ³•æ•è·å®é™…åº”ç”¨ä¸­çš„è·¨è¯­è¨€å’Œè·¨é¢†åŸŸå˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TableEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®çš„è¡¨æ ¼é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼ŒTableEvalåŒ…å«äº†ä»å››ä¸ªé¢†åŸŸï¼ˆåŒ…æ‹¬æ”¿åºœã€é‡‘èã€å­¦æœ¯å’Œå·¥ä¸šæŠ¥å‘Šï¼‰æ”¶é›†çš„å…·æœ‰å„ç§ç»“æ„ï¼ˆå¦‚ç®€æ´ã€åˆ†å±‚å’ŒåµŒå¥—è¡¨æ ¼ï¼‰çš„è¡¨æ ¼ã€‚æ­¤å¤–ï¼ŒTableEvalè¿˜å…·æœ‰ç®€ä½“ä¸­æ–‡ã€ç¹ä½“ä¸­æ–‡å’Œè‹±æ–‡çš„è·¨è¯­è¨€åœºæ™¯ã€‚ä¸ºäº†æœ€å°åŒ–æ•°æ®æ³„éœ²çš„é£é™©ï¼Œæˆ‘ä»¬ä»æœ€æ–°çš„ç°å®æ–‡æ¡£ä¸­æ”¶é›†æ‰€æœ‰æ•°æ®ã€‚è€ƒè™‘åˆ°ç°æœ‰çš„TableQAæŒ‡æ ‡æ— æ³•æ•æ‰è¯­ä¹‰å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†SEATï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨å­é—®é¢˜çº§åˆ«è¯„ä¼°æ¨¡å‹å“åº”å’Œå‚è€ƒç­”æ¡ˆçš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSEATä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚åœ¨TableEvalä¸Šçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›å¤æ‚ã€ç°å®çš„è¡¨æ ¼é—®ç­”ä»»åŠ¡æ—¶çš„å…³é”®å·®è·ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†è§è§£ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥æä¾›äº†æˆ‘ä»¬çš„æ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/wenge-research/TableEval">https://github.com/wenge-research/TableEval</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03949v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨TableQAé¢†åŸŸä»é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ç°æœ‰TableQAåŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç®€å•è¡¨æ ¼ä¸Šï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é—®é¢˜ï¼Œä¸”å¤šä¸ºå•è¯­ç§ï¼Œæ— æ³•æ•æ‰å®é™…åº”ç”¨çš„è·¨è¯­ç§å’Œè·¨åŸŸå˜åŒ–ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºTableEvalåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨çœŸå®TableQAä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚TableEvalåŒ…å«æ¥è‡ªæ”¿åºœã€é‡‘èã€å­¦æœ¯å’Œå·¥ä¸šæŠ¥å‘Šç­‰å››ä¸ªé¢†åŸŸçš„è¡¨æ ¼ï¼Œæ¶µç›–ç®€æ´ã€å±‚æ¬¡å’ŒåµŒå¥—ç­‰å„å¼ç»“æ„ï¼Œå¹¶è®¾æœ‰ç®€ä½“ä¸­ã€ç¹ä½“ä¸­å’Œè‹±æ–‡çš„è·¨è¯­è¨€åœºæ™¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºSEATè¯„ä¼°æ¡†æ¶ï¼Œä»¥å­é—®é¢˜çº§åˆ«è¯„ä¼°æ¨¡å‹å›ç­”ä¸å‚è€ƒç­”æ¡ˆçš„å¥‘åˆåº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºSEATä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œå¯¹TableEvalçš„å¹¿æ³›å®éªŒæ­ç¤ºäº†æœ€å…ˆè¿›LLMså¤„ç†è¿™äº›å¤æ‚ã€çœŸå®TableQAä»»åŠ¡æ—¶çš„å…³é”®å·®è·ï¼Œä¸ºæœªæ¥çš„æ”¹è¿›æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨TableQAé¢†åŸŸä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰TableQAåŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç®€å•è¡¨æ ¼ä¸Šï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é—®é¢˜ã€‚</li>
<li>TableEvalåŸºå‡†æµ‹è¯•åŒ…å«å„ç§ç»“æ„çš„è¡¨æ ¼ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œå¹¶è®¾æœ‰è·¨è¯­è¨€åœºæ™¯ã€‚</li>
<li>TableEvalæ³¨é‡çœŸå®ä¸–ç•Œæ•°æ®çš„æ”¶é›†ï¼Œä»¥å‡å°æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>ç°æœ‰TableQAè¯„ä¼°æŒ‡æ ‡æ— æ³•å‡†ç¡®æ•æ‰è¯­ä¹‰å‡†ç¡®æ€§ï¼Œå› æ­¤æå‡ºSEATè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>SEATè¯„ä¼°æ¡†æ¶èƒ½å¤Ÿä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-917860c874a9b51a28abf0622cad7691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36b9a94e876595dd7efe3df07e865994.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9382373db947e8050f13553613aeea6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db475d1d0d0523a7db40612c780bebb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7685ae984299ec8c4fe41c0d93e33737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c9b58fa512a037535ad9579a005826.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Graph-Counselor-Adaptive-Graph-Exploration-via-Multi-Agent-Synergy-to-Enhance-LLM-Reasoning"><a href="#Graph-Counselor-Adaptive-Graph-Exploration-via-Multi-Agent-Synergy-to-Enhance-LLM-Reasoning" class="headerlink" title="Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to   Enhance LLM Reasoning"></a>Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to   Enhance LLM Reasoning</h2><p><strong>Authors:Junqi Gao, Xiang Zou, YIng Ai, Dong Li, Yichen Niu, Biqing Qi, Jianxing Liu</strong></p>
<p>Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gjq100/Graph-Counselor.git">https://github.com/gjq100/Graph-Counselor.git</a>. </p>
<blockquote>
<p>å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraphRAGï¼‰é€šè¿‡æ˜¾å¼å»ºæ¨¡çŸ¥è¯†å…³ç³»ï¼Œæœ‰æ•ˆå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šé¢†åŸŸçš„äº‹å®å‡†ç¡®æ€§å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä»è€Œæé«˜äº†å¤–éƒ¨çŸ¥è¯†çš„æ•´åˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå›ºæœ‰å±€é™æ€§ï¼š1ï¼‰ä¿¡æ¯èšåˆæ•ˆç‡ä½ä¸‹ï¼šå®ƒä»¬ä¾èµ–äºå•ä¸ªä»£ç†å’Œå›ºå®šçš„è¿­ä»£æ¨¡å¼ï¼Œéš¾ä»¥è‡ªé€‚åº”æ•è·å›¾æ•°æ®ä¸­çš„å¤šçº§æ–‡æœ¬ã€ç»“æ„å’Œç¨‹åº¦ä¿¡æ¯ã€‚2ï¼‰æ¨ç†æœºåˆ¶åƒµåŒ–ï¼šå®ƒä»¬é‡‡ç”¨é¢„è®¾çš„æ¨ç†æ–¹æ¡ˆï¼Œä¸èƒ½åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œä¹Ÿæ— æ³•å®ç°ç²¾ç¡®çš„è¯­ä¹‰æ ¡æ­£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03939v1">PDF</a> Accepted by ACL 2025</p>
<p><strong>Summary</strong>ï¼š<br>Graph Counseloré€šè¿‡å¤šä¸»ä½“åä½œçš„Graph Retrieval Augmented Generationï¼ˆGraphRAGï¼‰æ–¹æ³•ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„äº‹å®å‡†ç¡®æ€§å’Œç”Ÿæˆè´¨é‡ã€‚é€šè¿‡è‡ªé€‚åº”å›¾ä¿¡æ¯æå–æ¨¡å—ï¼ˆAGIEMï¼‰å’Œå¤šè§’åº¦è‡ªæˆ‘åæ€ï¼ˆSRï¼‰æ¨¡å—ï¼Œå®ç°äº†å¯¹å¤æ‚å›¾ç»“æ„çš„ç²¾ç¡®å»ºæ¨¡ã€ä¿¡æ¯æå–ç­–ç•¥çš„åŠ¨æ€è°ƒæ•´ï¼Œä»¥åŠæ¨ç†ç»“æœçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒGraph Counseloråœ¨å¤šä¸ªå›¾æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>GraphRAGæ–¹æ³•é€šè¿‡æ˜¾å¼å»ºæ¨¡çŸ¥è¯†å…³ç³»å¢å¼ºäº†å¤–éƒ¨çŸ¥è¯†çš„æ•´åˆèƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ä¿¡æ¯èšåˆæ•ˆç‡ä½ä¸‹å’Œæ¨ç†æœºåˆ¶åƒµåŒ–çš„é—®é¢˜ã€‚</li>
<li>Graph Counseloré€šè¿‡å¤šä¸»ä½“åä½œå…‹æœè¿™äº›é—®é¢˜ï¼ŒåŒ…æ‹¬è§„åˆ’ã€æ€è€ƒå’Œæ‰§è¡Œä¸»ä½“å…±åŒå·¥ä½œã€‚</li>
<li>AGIEMæ¨¡å—å®ç°äº†å¯¹å¤æ‚å›¾ç»“æ„çš„ç²¾ç¡®å»ºæ¨¡å’Œä¿¡æ¯æå–ç­–ç•¥çš„åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>SRæ¨¡å—é€šè¿‡è‡ªæˆ‘åæ€å’Œé€†å‘æ¨ç†æœºåˆ¶æé«˜äº†æ¨ç†ç»“æœçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>Graph Counseloråœ¨å¤šå›¾æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´é«˜çš„æ¨ç†å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53dac6352e1dbde104408bac9753b2f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-acaa938837dab6c7ad8dc2e63fb4d34d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b46f78854be6081617cd0c7d791d20cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c616605e9b64a793f125ded16c4e8d06.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Boosting-Open-Source-LLMs-for-Program-Repair-via-Reasoning-Transfer-and-LLM-Guided-Reinforcement-Learning"><a href="#Boosting-Open-Source-LLMs-for-Program-Repair-via-Reasoning-Transfer-and-LLM-Guided-Reinforcement-Learning" class="headerlink" title="Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and   LLM-Guided Reinforcement Learning"></a>Boosting Open-Source LLMs for Program Repair via Reasoning Transfer and   LLM-Guided Reinforcement Learning</h2><p><strong>Authors:Xunzhu Tang, Jacques Klein, TegawendÃ© F. BissyandÃ©</strong></p>
<p>Several closed-source LLMs have consistently outperformed open-source alternatives in program repair tasks, primarily due to their superior reasoning capabilities and extensive pre-training. This paper introduces Repairity, a novel three-stage methodology that significantly narrows this performance gap through reasoning extraction and reinforcement learning. Our approach: (1) systematically filters high-quality reasoning traces from closed-source models using correctness verification, (2) transfers this reasoning knowledge to open-source models via supervised fine-tuning, and (3) develops reinforcement learning with LLM-based feedback to further optimize performance. Empirical evaluation across multiple program repair benchmarks demonstrates that Repairity improves the performance of Qwen2.5-Coder-32B-Instruct, a base open source LLM, by 8.68% on average, reducing the capability gap with Claude-Sonnet3.7, a state-of-the-art closed-source model, from 10.05% to 1.35%. Ablation studies confirm that both reasoning extraction and LLM-guided reinforcement learning contribute significantly to these improvements. Our methodology generalizes effectively to additional code-related tasks, enabling organizations to leverage high-quality program repair capabilities while maintaining the customizability, transparency, and deployment flexibility inherent to open-source models. </p>
<blockquote>
<p>åœ¨å¤šç¨‹åºä¿®å¤ä»»åŠ¡ä¸­ï¼Œä¸€äº›é—­æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å§‹ç»ˆè¡¨ç°å‡ºä¼˜äºå¼€æºæ›¿ä»£å“çš„æ€§èƒ½ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå®ƒä»¬å‡ºè‰²çš„æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›çš„é¢„è®­ç»ƒã€‚æœ¬æ–‡ä»‹ç»äº†Repairityï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼Œé€šè¿‡æ¨ç†æå–å’Œå¼ºåŒ–å­¦ä¹ æ˜¾è‘—ç¼©å°äº†æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼šï¼ˆ1ï¼‰é€šè¿‡æ­£ç¡®æ€§éªŒè¯ç³»ç»Ÿåœ°ä»é—­æºæ¨¡å‹ä¸­è¿‡æ»¤å‡ºé«˜è´¨é‡çš„æ¨ç†è½¨è¿¹ï¼Œï¼ˆ2ï¼‰é€šè¿‡ç›‘ç£å¾®è°ƒå°†è¿™äº›æ¨ç†çŸ¥è¯†è½¬ç§»åˆ°å¼€æºæ¨¡å‹ä¸­ï¼Œï¼ˆ3ï¼‰åˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åé¦ˆå¼€å‘å¼ºåŒ–å­¦ä¹ ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚åœ¨å¤šä¸ªç¨‹åºä¿®å¤åŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒRepairityæé«˜äº†åŸºç¡€å¼€æºLLM Qwen2.5-Coder-32B-Instructçš„æ€§èƒ½ï¼Œå¹³å‡æé«˜äº†8.68%ï¼Œå¹¶ç¼©å°äº†å…¶ä¸æœ€å…ˆè¿›çš„é—­æºæ¨¡å‹Claude-Sonnet3.7çš„èƒ½åŠ›å·®è·ï¼Œä»10.05%é™è‡³1.35%ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œæ¨ç†æå–å’ŒLLMå¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ éƒ½å¯¹è¿™äº›æ”¹è¿›åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°å…¶ä»–çš„ä»£ç ç›¸å…³ä»»åŠ¡ï¼Œä½¿ç»„ç»‡èƒ½å¤Ÿåœ¨ä¿æŒå¼€æºæ¨¡å‹çš„å®šåˆ¶æ€§ã€é€æ˜åº¦å’Œéƒ¨ç½²çµæ´»æ€§çš„åŒæ—¶ï¼Œåˆ©ç”¨é«˜è´¨é‡çš„ç¨‹åºä¿®å¤èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03921v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹ç¨‹åºä¿®å¤ä»»åŠ¡ï¼Œå°é—­å¼æºä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ä¼˜äºå¼€æºæ¨¡å‹ï¼Œä¸»è¦ä½“ç°åœ¨å…¶å‡ºè‰²çš„æ¨ç†èƒ½åŠ›å’Œé¢„è®­ç»ƒå¹¿åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µæ–¹æ³•Repairityï¼Œé€šè¿‡æ¨ç†æå–å’Œå¼ºåŒ–å­¦ä¹ æ˜¾è‘—ç¼©å°äº†è¿™ä¸€æ€§èƒ½å·®è·ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æ­£ç¡®æ€§éªŒè¯ç³»ç»Ÿåœ°è¿‡æ»¤å°é—­å¼æ¨¡å‹ä¸­çš„é«˜è´¨é‡æ¨ç†è½¨è¿¹ï¼Œç„¶åå°†è¿™äº›æ¨ç†çŸ¥è¯†é€šè¿‡ç›‘ç£å¾®è°ƒè½¬ç§»åˆ°å¼€æºæ¨¡å‹ä¸­ï¼Œå¹¶å‘å±•å‡ºåŸºäºLLMåé¦ˆçš„å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒRepairityèƒ½å¤Ÿæå‡åŸºç¡€å¼€æºLLM Qwen2.5-Coder-3.2B-Instructçš„æ€§èƒ½ï¼Œå¹³å‡æå‡å¹…åº¦è¾¾åˆ°8.68%ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†å…¶ä¸å…ˆè¿›å°é—­å¼æ¨¡å‹Claude-Sonnet3.7çš„èƒ½åŠ›å·®è·ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•çš„æ¨ç†æå–å’ŒLLMæŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ å¯¹äºæ€§èƒ½æå‡å‡æœ‰é‡è¦è´¡çŒ®ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯æœ‰æ•ˆæ¨å¹¿åˆ°å…¶ä»–ä»£ç ç›¸å…³ä»»åŠ¡ï¼Œä½¿å¾—ç»„ç»‡èƒ½å¤Ÿåœ¨åˆ©ç”¨é«˜è´¨é‡ç¨‹åºä¿®å¤èƒ½åŠ›çš„åŒæ—¶ä¿æŒå¼€æºæ¨¡å‹çš„è‡ªå®šä¹‰æ€§ã€é€æ˜åº¦å’Œéƒ¨ç½²çµæ´»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å°é—­å¼LLMåœ¨ç¨‹åºä¿®å¤ä»»åŠ¡ä¸Šè¾ƒå¼€æºLLMæœ‰ä¼˜åŠ¿ï¼Œä¸»è¦ç”±äºå®ƒä»¬çš„æ¨ç†èƒ½åŠ›å’Œé¢„è®­ç»ƒæ•ˆæœæ›´ä½³ã€‚</li>
<li>Repairityæ–¹æ³•é€šè¿‡ä¸‰ä¸ªæ­¥éª¤æ˜¾è‘—ç¼©å°äº†å¼€æºå’Œå°é—­å¼LLMåœ¨ç¨‹åºä¿®å¤ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®è·ã€‚</li>
<li>Repairityé€šè¿‡æ­£ç¡®æ€§éªŒè¯è¿‡æ»¤é«˜è´¨é‡æ¨ç†è½¨è¿¹ï¼Œç„¶åå°†å…¶ä»å°é—­å¼LLMè½¬ç§»åˆ°å¼€æºLLMã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è¢«ç”¨äºè¿›ä¸€æ­¥ä¼˜åŒ–å¼€æºLLMçš„æ€§èƒ½ï¼ŒåŸºäºLLMçš„åé¦ˆã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒRepairityæå‡äº†åŸºç¡€å¼€æºLLMçš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†ä¸å…ˆè¿›å°é—­å¼LLMçš„å·®è·ã€‚</li>
<li>æ¨ç†æå–å’ŒLLMæŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ å¯¹Repairityçš„æ€§èƒ½æå‡éƒ½æœ‰é‡è¦è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fea4b4cbd532829c055ec13f8ccd7ce3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b238c7b08e721be4bef6f39e3241a57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a72f1a7043d7af94c40a1cedab602d0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="STELLA-Towards-Protein-Function-Prediction-with-Multimodal-LLMs-Integrating-Sequence-Structure-Representations"><a href="#STELLA-Towards-Protein-Function-Prediction-with-Multimodal-LLMs-Integrating-Sequence-Structure-Representations" class="headerlink" title="STELLA: Towards Protein Function Prediction with Multimodal LLMs   Integrating Sequence-Structure Representations"></a>STELLA: Towards Protein Function Prediction with Multimodal LLMs   Integrating Sequence-Structure Representations</h2><p><strong>Authors:Hongwang Xiao, Wenjun Lin, Xi Chen, Hui Wang, Kai Chen, Jiashan Li, Yuancheng Sun, Sicheng Dai, Boya Wu, Qiwei Ye</strong></p>
<p>Protein biology focuses on the intricate relationships among sequences, structures, and functions. Deciphering protein functions is crucial for understanding biological processes, advancing drug discovery, and enabling synthetic biology applications. Since protein sequences determine tertiary structures, which in turn govern functions, integrating sequence and structure information is essential for accurate prediction of protein functions. Traditional protein language models (pLMs) have advanced protein-related tasks by learning representations from large-scale sequence and structure data. However, pLMs are limited in integrating broader contextual knowledge, particularly regarding functional modalities that are fundamental to protein biology. In contrast, large language models (LLMs) have exhibited outstanding performance in contextual understanding, reasoning, and generation across diverse domains. Leveraging these capabilities, STELLA is proposed as a multimodal LLM integrating protein sequence-structure representations with general knowledge to address protein function prediction. Through multimodal instruction tuning (MMIT) using the proposed OPI-Struc dataset, STELLA achieves state-of-the-art performance in two function-related tasks-functional description prediction (FP) and enzyme-catalyzed reaction prediction (EP). This study highlights the potential of multimodal LLMs as an alternative paradigm to pLMs to advance protein biology research. </p>
<blockquote>
<p>è›‹ç™½è´¨ç”Ÿç‰©å­¦ä¸»è¦ç ”ç©¶åºåˆ—ã€ç»“æ„å’ŒåŠŸèƒ½ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚è§£è¯»è›‹ç™½è´¨åŠŸèƒ½å¯¹äºç†è§£ç”Ÿç‰©è¿‡ç¨‹ã€æ¨è¿›è¯ç‰©å‘ç°å’Œå®ç°åˆæˆç”Ÿç‰©å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚ç”±äºè›‹ç™½è´¨åºåˆ—å†³å®šä¸‰çº§ç»“æ„ï¼Œè€Œä¸‰çº§ç»“æ„åˆæ§åˆ¶åŠŸèƒ½ï¼Œå› æ­¤æ•´åˆåºåˆ—å’Œç»“æ„ä¿¡æ¯å¯¹äºå‡†ç¡®é¢„æµ‹è›‹ç™½è´¨åŠŸèƒ½è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰é€šè¿‡ä»å¤§è§„æ¨¡åºåˆ—å’Œç»“æ„æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤ºæ¥æ¨è¿›ä¸è›‹ç™½è´¨ç›¸å…³çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒpLMsåœ¨æ•´åˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è›‹ç™½è´¨ç”Ÿç‰©å­¦ä¸­è‡³å…³é‡è¦çš„åŠŸèƒ½æ¨¡å¼æ–¹é¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚åˆ©ç”¨è¿™äº›èƒ½åŠ›ï¼Œæå‡ºäº†å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹STELLAï¼Œå®ƒæ•´åˆäº†è›‹ç™½è´¨åºåˆ—ç»“æ„è¡¨ç¤ºå’Œé€šç”¨çŸ¥è¯†æ¥è§£å†³è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æå‡ºçš„OPI-Strucæ•°æ®é›†è¿›è¡Œå¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ˆMMITï¼‰ï¼ŒSTELLAåœ¨ä¸¤ä¸ªä¸åŠŸèƒ½ç›¸å…³çš„ä»»åŠ¡â€”â€”åŠŸèƒ½æè¿°é¢„æµ‹ï¼ˆFPï¼‰å’Œé…¶å‚¬åŒ–ååº”é¢„æµ‹ï¼ˆEPï¼‰ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†å¤šæ¨¡å¼LLMsä½œä¸ºpLMsçš„æ›¿ä»£èŒƒå¼åœ¨æ¨è¿›è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶åºåˆ—ã€ç»“æ„ä¸åŠŸèƒ½ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚è§£æè›‹ç™½è´¨åŠŸèƒ½å¯¹äºç†è§£ç”Ÿç‰©è¿‡ç¨‹ã€æ¨è¿›è¯ç‰©å‘ç°å’Œå®ç°åˆæˆç”Ÿç‰©å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰é€šè¿‡ä»å¤§è§„æ¨¡åºåˆ—å’Œç»“æ„æ•°æ®ä¸­å­¦ä¹ è¡¨å¾æ¥æ¨è¿›è›‹ç™½è´¨ç›¸å…³ä»»åŠ¡ï¼Œä½†åœ¨æ•´åˆæ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›å¯¹è›‹ç™½è´¨ç”Ÿç‰©å­¦è‡³å…³é‡è¦çš„åŠŸèƒ½æ¨¡å¼ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åˆ©ç”¨è¿™äº›èƒ½åŠ›ï¼ŒSTELLAè¢«æå‡ºä¸ºä¸€ç§å¤šæ¨¡å¼LLMï¼Œå®ƒæ•´åˆè›‹ç™½è´¨åºåˆ—-ç»“æ„è¡¨å¾å’Œä¸€èˆ¬çŸ¥è¯†æ¥è§£å†³è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æå‡ºçš„OPI-Strucæ•°æ®é›†è¿›è¡Œå¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ˆMMITï¼‰ï¼ŒSTELLAåœ¨åŠŸèƒ½æè¿°é¢„æµ‹å’Œé…¶å‚¬åŒ–ååº”é¢„æµ‹ä¸¤ä¸ªä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†å¤šæ¨¡å¼LLMä½œä¸ºæ¨è¿›è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶çš„æ›¿ä»£æ–¹æ³•çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è›‹ç™½è´¨ç”Ÿç‰©å­¦å…³æ³¨åºåˆ—ã€ç»“æ„ä¸åŠŸèƒ½ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>è§£æè›‹ç™½è´¨åŠŸèƒ½å¯¹ç†è§£ç”Ÿç‰©è¿‡ç¨‹ã€è¯ç‰©å‘ç°å’Œåˆæˆç”Ÿç‰©å­¦åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿè›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰åœ¨æ•´åˆä¸Šä¸‹æ–‡çŸ¥è¯†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä¸Šä¸‹æ–‡ç†è§£ã€æ¨ç†å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>STELLAæ˜¯ä¸€ç§å¤šæ¨¡å¼LLMï¼Œèƒ½æ•´åˆè›‹ç™½è´¨åºåˆ—-ç»“æ„è¡¨å¾å’Œä¸€èˆ¬çŸ¥è¯†æ¥è§£å†³è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹é—®é¢˜ã€‚</li>
<li>é€šè¿‡å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´ï¼ˆMMITï¼‰å’ŒOPI-Strucæ•°æ®é›†ï¼ŒSTELLAåœ¨è›‹ç™½è´¨åŠŸèƒ½ç›¸å…³ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡å¼LLMåœ¨è›‹ç™½è´¨ç”Ÿç‰©å­¦ç ”ç©¶ä¸­çš„æ½œåŠ›ï¼Œå¯ä½œä¸ºpLMsçš„æ›¿ä»£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-77339c99bc04ff317b52856b5a2ec630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f72ffc3f41650acbbfc68e50427f8668.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8b7dbf8f8bd9e135392c9cfe1310cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7d40f4ad4d4a8b7bc5673d05e01b8f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reason-from-Future-Reverse-Thought-Chain-Enhances-LLM-Reasoning"><a href="#Reason-from-Future-Reverse-Thought-Chain-Enhances-LLM-Reasoning" class="headerlink" title="Reason from Future: Reverse Thought Chain Enhances LLM Reasoning"></a>Reason from Future: Reverse Thought Chain Enhances LLM Reasoning</h2><p><strong>Authors:Yinlong Xu, Yanzhao Zheng, Shuoshuo Sun, Shuaihan Huang, Baohua Dong, Hangcheng Zhu, Ruohui Huang, Gang Yu, Hongxia Xu, Jian Wu</strong></p>
<p>It has been demonstrated that carefully designed reasoning paradigms, like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning capabilities of small language models by detailed thinking and extensive thought searching, unbounded branching factors in the searching space create prohibitive reasoning consumption. However these methods fall into the trap of local optimum reasoning, which means the model lacks a global perspective while solving problems. We propose a novel reasoning paradigm called Reason from Future (RFF), which generates reasoning paths by bidirectional reasoning that combines top-down planning with bottom-up reasoning accumulation. The essence of RFF lies in its reverse reasoning mechanism, which prioritizes core logical relationships and imposes goal-oriented constraints on intermediate steps, thereby reducing the searching space and mitigating error accumulation inherent in sequential forward reasoning. Empirical evaluations across diverse experiments demonstrate that RFF outperforms conventional paradigms with higher accuracy and less searching space to solve complex tasks. </p>
<blockquote>
<p>ç»è¿‡ç²¾å¿ƒè®¾è®¡æ¨ç†èŒƒå¼ï¼Œå¦‚æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰ï¼Œèƒ½å¤Ÿé€šè¿‡è¯¦ç»†æ€è€ƒå’Œå¹¿æ³›çš„æ€æƒ³æœç´¢å¢å¼ºå°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•çš„æœç´¢ç©ºé—´ä¸­å­˜åœ¨ä¸å—é™åˆ¶çš„åˆ†æ”¯å› ç´ ï¼Œä¼šäº§ç”Ÿå¤§é‡çš„æ¨ç†æ¶ˆè€—ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜æ¨ç†çš„é™·é˜±ï¼Œè¿™æ„å‘³ç€æ¨¡å‹åœ¨è§£å†³é—®é¢˜æ—¶ç¼ºä¹å…¨å±€è§†è§’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ¨ç†èŒƒå¼ï¼Œç§°ä¸ºæœªæ¥æ¨ç†ï¼ˆRFFï¼‰ï¼Œå®ƒé€šè¿‡ç»“åˆè‡ªä¸Šè€Œä¸‹è§„åˆ’ä¸è‡ªä¸‹è€Œä¸Šæ¨ç†ç§¯ç´¯æ¥è¿›è¡ŒåŒå‘æ¨ç†ï¼Œç”Ÿæˆæ¨ç†è·¯å¾„ã€‚RFFçš„æœ¬è´¨åœ¨äºå…¶é€†å‘æ¨ç†æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¼˜å…ˆå¤„ç†æ ¸å¿ƒé€»è¾‘å…³ç³»ï¼Œå¹¶å¯¹ä¸­é—´æ­¥éª¤æ–½åŠ ç›®æ ‡å¯¼å‘çš„çº¦æŸï¼Œä»è€Œå‡å°‘æœç´¢ç©ºé—´å¹¶å‡è½»é¡ºåºæ­£å‘æ¨ç†ä¸­å›ºæœ‰çš„é”™è¯¯ç´¯ç§¯é—®é¢˜ã€‚è·¨å¤šç§å®éªŒçš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ä¼ ç»ŸèŒƒå¼ç›¸æ¯”ï¼ŒRFFä»¥æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´å°çš„æœç´¢ç©ºé—´è§£å†³å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›æ›´ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03673v1">PDF</a> Accepted by ACL 2025 findings</p>
<p><strong>Summary</strong>ï¼šæ–°å‹æ¨ç†æ¨¡å¼å¦‚é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰ä¸æ ‘çŠ¶æ€ç»´ï¼ˆTree-of-Thoughtï¼ŒToTï¼‰èƒ½å¤Ÿæå‡å°å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬é¢ä¸´é™·å…¥å±€éƒ¨æœ€ä¼˜æ¨ç†çš„å›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¨¡å¼â€”â€”æœªæ¥å¯¼å‘æ¨ç†ï¼ˆReason from Futureï¼ŒRFFï¼‰ã€‚å®ƒé€šè¿‡åŒå‘æ¨ç†ç”Ÿæˆæ¨ç†è·¯å¾„ï¼Œå°†è‡ªä¸Šè€Œä¸‹è§„åˆ’ä¸è‡ªä¸‹è€Œä¸Šçš„æ¨ç†ç§¯ç´¯ç›¸ç»“åˆã€‚å…¶æ ¸å¿ƒåœ¨äºåå‘æ¨ç†æœºåˆ¶ï¼Œä¼˜å…ˆæ ¸å¿ƒé€»è¾‘å…³ç³»å¹¶å¯¹ä¸­é—´æ­¥éª¤æ–½åŠ ç›®æ ‡å¯¼å‘çº¦æŸï¼Œå‡å°‘æœç´¢ç©ºé—´å¹¶ç¼“è§£åºåˆ—æ­£å‘æ¨ç†ä¸­çš„é”™è¯¯ç´¯ç§¯ã€‚å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒRFFåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´å°çš„æœç´¢ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰æ¨ç†æ¨¡å¼å¦‚CoTå’ŒToTèƒ½å¤Ÿå¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨é™·å…¥å±€éƒ¨æœ€ä¼˜æ¨ç†çš„é—®é¢˜ã€‚</li>
<li>RFFé€šè¿‡åŒå‘æ¨ç†ç»“åˆé¡¶å‘ä¸‹è§„åˆ’ä¸åº•å‘ä¸Šæ¨ç†ç§¯ç´¯ã€‚</li>
<li>RFFçš„æ ¸å¿ƒåœ¨äºå…¶åå‘æ¨ç†æœºåˆ¶ï¼Œä¼˜å…ˆæ ¸å¿ƒé€»è¾‘å…³ç³»ã€‚</li>
<li>RFFå¯¹ä¸­é—´æ­¥éª¤æ–½åŠ ç›®æ ‡å¯¼å‘çº¦æŸï¼Œæœ‰åŠ©äºå‡å°‘æœç´¢ç©ºé—´ã€‚</li>
<li>RFFèƒ½å¤Ÿç¼“è§£åºåˆ—æ­£å‘æ¨ç†ä¸­çš„é”™è¯¯ç´¯ç§¯ã€‚</li>
<li>å®è¯è¯„ä¼°æ˜¾ç¤ºRFFåœ¨è§£å†³å¤æ‚ä»»åŠ¡æ—¶å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>RFFç›¸æ¯”ä¼ ç»Ÿæ¨¡å¼å…·æœ‰æ›´å°çš„æœç´¢ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53853210edb078b5c06a0d1f1574a0df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5103e7fc82246c22843d2a5d8ede05ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dccbde00148478f51c10da13fe9c9b29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffe4bf8f1b8c54f1f50cf8c85f04996e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-504bb365b3ce7520a06454beedad7a43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b800c869e38e33b09fec2c9a65ad3587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a7daad95a88f477023693c561de3b8c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-06/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6695769443305db9e6b95213c3d323bb.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-06  Language-Image Alignment with Fixed Text Encoders
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-05/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cb8907fe237c7478db2c8123ab464095.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-05  ANT Adaptive Neural Temporal-Aware Text-to-Motion Model
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-05
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
