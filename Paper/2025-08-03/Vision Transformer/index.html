<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  UPRE Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8577c124f2f2b957f9eb7ed8c1121359.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    65 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement"><a href="#UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement" class="headerlink" title="UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement"></a>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement</h2><p><strong>Authors:Xiao Zhang, Fei Wei, Yong Wang, Wenda Zhao, Feiyi Li, Xiangxiang Chu</strong></p>
<p>Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE">https://github.com/AMAP-ML/UPRE</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”ï¼ˆZSDAï¼‰ç”±äºç›®æ ‡åŸŸä¸­ç¼ºä¹å›¾åƒè€Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå‘æŒ¥å®ƒä»¬çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦è§£å†³åŸŸåˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œè€Œå¿½ç•¥äº†æ£€æµ‹ä»»åŠ¡ä¸VLMä¹‹é—´çš„ä¸åŒ¹é…ï¼ŒVLMä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€æç¤ºå’Œè¡¨ç¤ºå¢å¼ºï¼ˆUPREï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¤šè§†è§’åŸŸæç¤ºï¼Œå®ƒå°†è¯­è¨€åŸŸå…ˆéªŒçŸ¥è¯†ä¸æ£€æµ‹ç‰¹å®šçŸ¥è¯†ç›¸ç»“åˆï¼Œä»¥åŠä¸€ä¸ªè§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼Œç”¨äºç”ŸæˆåŸŸé£æ ¼å˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šçº§å¢å¼ºç­–ç•¥ï¼ŒåŒ…æ‹¬ç›¸å¯¹åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»ï¼Œå®ƒä»¬åˆ†åˆ«åœ¨å›¾åƒçº§åˆ«å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºï¼Œå¹¶åœ¨å®ä¾‹çº§åˆ«æ•è·å¤šæ ·åŒ–çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ZSDAæ£€æµ‹åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/UPREæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00721v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†é›¶æ ·æœ¬é¢†åŸŸè‡ªé€‚åº”æ£€æµ‹çš„æ–°æ–¹æ³•ï¼Œå³ç»Ÿä¸€æç¤ºå’Œè¡¨ç¤ºå¢å¼ºï¼ˆUPREï¼‰æ¡†æ¶ã€‚å®ƒæ—¨åœ¨è§£å†³ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œé›¶æ ·æœ¬å­¦ä¹ æ—¶å‡ºç°çš„æ–‡æœ¬æç¤ºä¸æ£€æµ‹ä»»åŠ¡ä¸åŒ¹é…ä»¥åŠé¢†åŸŸåˆ†å¸ƒåç§»é—®é¢˜ã€‚UPREæ¡†æ¶é€šè¿‡ä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºï¼Œå¼•å…¥å¤šè§†è§’é¢†åŸŸæç¤ºå’Œè§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼Œå¹¶é‡‡ç”¨å¤šå±‚æ¬¡å¢å¼ºç­–ç•¥ï¼Œå®ç°è·¨é¢†åŸŸå›¾åƒæ£€æµ‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZSDAé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ç›®æ ‡é¢†åŸŸçš„å›¾åƒç¼ºå¤±ã€‚</li>
<li>ä»¥å¾€çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿›è¡Œé›¶æ ·æœ¬å­¦ä¹ æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>UPREæ¡†æ¶è§£å†³äº†VLMsä¸æ£€æµ‹ä»»åŠ¡ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼ŒåŒæ—¶ä¼˜åŒ–äº†æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºã€‚</li>
<li>UPREå¼•å…¥äº†å¤šè§†è§’é¢†åŸŸæç¤ºï¼Œç»“åˆäº†è¯­è¨€é¢†åŸŸå…ˆéªŒçŸ¥è¯†å’Œæ£€æµ‹ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>è§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ç”¨äºç”Ÿæˆé¢†åŸŸé£æ ¼å˜åŒ–ã€‚</li>
<li>å¤šå±‚æ¬¡å¢å¼ºç­–ç•¥åŒ…æ‹¬ç›¸å¯¹é¢†åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»ï¼Œç”¨äºå¯¹é½å¤šæ¨¡æ€è¡¨ç¤ºå’Œæ•æ‰å®ä¾‹çº§åˆ«çš„å¤šç§è§†è§‰è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00721">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76420c74a7f52503f7ce343aecb151f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bbdd914ef07f98a2ab2538d3c00b878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7870f451037050b4697fea48b540040f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7831142b23aeab239215ec5295dd6afb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CLIP-HandID-Vision-Language-Model-for-Hand-Based-Person-Identification"><a href="#CLIP-HandID-Vision-Language-Model-for-Hand-Based-Person-Identification" class="headerlink" title="CLIP-HandID: Vision-Language Model for Hand-Based Person Identification"></a>CLIP-HandID: Vision-Language Model for Hand-Based Person Identification</h2><p><strong>Authors:Nathanael L. Baisa, Babu Pallam, Amudhavel Jayavel</strong></p>
<p>This paper introduces a novel approach to person identification using hand images, designed specifically for criminal investigations. The method is particularly valuable in serious crimes such as sexual abuse, where hand images are often the only identifiable evidence available. Our proposed method, CLIP-HandID, leverages a pre-trained foundational vision-language model - CLIP - to efficiently learn discriminative deep feature representations from hand images (input to CLIPâ€™s image encoder) using textual prompts as semantic guidance. Since hand images are labeled with indexes rather than text descriptions, we employ a textual inversion network to learn pseudo-tokens that encode specific visual contexts or appearance attributes. These learned pseudo-tokens are then incorporated into textual prompts, which are fed into CLIPâ€™s text encoder to leverage its multi-modal reasoning and enhance generalization for identification. Through extensive evaluations on two large, publicly available hand datasets with multi-ethnic representation, we demonstrate that our method significantly outperforms existing approaches. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ‰‹éƒ¨å›¾åƒè¿›è¡Œäººå‘˜è¯†åˆ«çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸“ä¸ºåˆ‘äº‹ä¾¦æŸ¥è®¾è®¡ã€‚è¯¥æ–¹æ³•åœ¨æ€§è™å¾…ç­‰ä¸¥é‡çŠ¯ç½ªä¸­å°¤å…¶æœ‰ä»·å€¼ï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ‰‹éƒ¨å›¾åƒé€šå¸¸æ˜¯å”¯ä¸€å¯ç”¨çš„å¯è¯†åˆ«è¯æ®ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•CLIP-HandIDï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹CLIPï¼Œé€šè¿‡æ–‡æœ¬æç¤ºä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°ä»æ‰‹éƒ¨å›¾åƒï¼ˆè¾“å…¥CLIPå›¾åƒç¼–ç å™¨ï¼‰ä¸­å­¦ä¹ åˆ¤åˆ«æ·±åº¦ç‰¹å¾è¡¨ç¤ºã€‚ç”±äºæ‰‹éƒ¨å›¾åƒæ˜¯ç”¨ç´¢å¼•è€Œä¸æ˜¯æ–‡æœ¬æè¿°æ¥æ ‡è®°çš„ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬åè½¬ç½‘ç»œæ¥å­¦ä¹ ç¼–ç ç‰¹å®šè§†è§‰ä¸Šä¸‹æ–‡æˆ–å¤–è§‚å±æ€§çš„ä¼ªä»¤ç‰Œã€‚ç„¶åï¼Œè¿™äº›å­¦ä¹ åˆ°çš„ä¼ªä»¤ç‰Œè¢«çº³å…¥æ–‡æœ¬æç¤ºä¸­ï¼Œè¿™äº›æç¤ºè¢«è¾“å…¥åˆ°CLIPçš„æ–‡æœ¬ç¼–ç å™¨ä¸­ï¼Œä»¥åˆ©ç”¨å…¶å¤šæ¨¡å¼æ¨ç†å¹¶å¢å¼ºè¯†åˆ«æ¨å¹¿ã€‚é€šè¿‡å¯¹ä¸¤ä¸ªåŒ…å«å¤šç§æ—ä»£è¡¨çš„å¤§å‹å…¬å¼€æ‰‹éƒ¨æ•°æ®é›†è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12447v3">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºCLIPæ¨¡å‹çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨æ‰‹éƒ¨å›¾åƒè¿›è¡Œäººå‘˜è¯†åˆ«çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ€§è™å¾…ç­‰ä¸¥é‡çŠ¯ç½ªä¸­ï¼Œæ‰‹éƒ¨å›¾åƒæ˜¯å”¯ä¸€å¯è¯†åˆ«çš„è¯æ®ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬æç¤ºä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ï¼ŒCLIP-HandIDèƒ½å¤Ÿä»æ‰‹éƒ¨å›¾åƒä¸­å­¦ä¹ å…·æœ‰åŒºåˆ†æ€§çš„æ·±åº¦ç‰¹å¾è¡¨ç¤ºã€‚ä½¿ç”¨ç´¢å¼•è€Œä¸æ˜¯æ–‡æœ¬æè¿°æ¥æ ‡è®°æ‰‹éƒ¨å›¾åƒï¼Œå¹¶é€šè¿‡æ–‡æœ¬åè½¬ç½‘ç»œå­¦ä¹ ç‰¹å®šè§†è§‰ä¸Šä¸‹æ–‡æˆ–å¤–è§‚å±æ€§çš„ä¼ªæ ‡è®°ã€‚å°†è¿™äº›ä¼ªæ ‡è®°çº³å…¥æ–‡æœ¬æç¤ºä¸­ï¼Œä»¥å¢å¼ºCLIPæ–‡æœ¬ç¼–ç å™¨çš„å¤šæ¨¡æ€æ¨ç†å’Œè¯†åˆ«æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…·æœ‰å¤šæ°‘æ—ä»£è¡¨æ€§çš„ä¸¤ä¸ªå¤§å‹å…¬å¼€æ‰‹éƒ¨æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºCLIPæ¨¡å‹çš„æ–°å‹æ‰‹éƒ¨å›¾åƒäººå‘˜è¯†åˆ«æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºåˆ‘äº‹è°ƒæŸ¥ä¸­çš„æ€§è™å¾…ç­‰ä¸¥é‡çŠ¯ç½ªã€‚</li>
<li>CLIP-HandIDåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æ–‡æœ¬æç¤ºæé«˜ä»æ‰‹éƒ¨å›¾åƒä¸­å­¦ä¹ ç‰¹å¾è¡¨ç¤ºçš„æ•ˆç‡ã€‚</li>
<li>ç”±äºæ‰‹éƒ¨å›¾åƒä½¿ç”¨ç´¢å¼•è€Œéæ–‡æœ¬æè¿°è¿›è¡Œæ ‡è®°ï¼Œå› æ­¤é‡‡ç”¨æ–‡æœ¬åè½¬ç½‘ç»œå­¦ä¹ ç‰¹å®šè§†è§‰ä¸Šä¸‹æ–‡æˆ–å¤–è§‚å±æ€§çš„ä¼ªæ ‡è®°ã€‚</li>
<li>ä¼ªæ ‡è®°è¢«çº³å…¥æ–‡æœ¬æç¤ºä¸­ï¼Œå¢å¼ºäº†CLIPçš„å¤šæ¨¡æ€æ¨ç†å’Œè¯†åˆ«æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…·æœ‰å¤šæ°‘æ—ä»£è¡¨æ€§çš„ä¸¤ä¸ªå¤§å‹å…¬å¼€æ‰‹éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCLIP-HandIDåœ¨äººå‘˜è¯†åˆ«æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d54dac5228be8fc95b56c950d59d1084.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef06b5e3ecbed6b2d20383e008ef3119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-933d8bd637d9332ee6482b13b7422f5a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Beyond-Single-Channel-Multichannel-Signal-Imaging-for-PPG-to-ECG-Reconstruction-with-Vision-Transformers"><a href="#Beyond-Single-Channel-Multichannel-Signal-Imaging-for-PPG-to-ECG-Reconstruction-with-Vision-Transformers" class="headerlink" title="Beyond Single-Channel: Multichannel Signal Imaging for PPG-to-ECG   Reconstruction with Vision Transformers"></a>Beyond Single-Channel: Multichannel Signal Imaging for PPG-to-ECG   Reconstruction with Vision Transformers</h2><p><strong>Authors:Xiaoyan Li, Shixin Xu, Faisal Habib, Arvind Gupta, Huaxiong Huang</strong></p>
<p>Reconstructing ECG from PPG is a promising yet challenging task. While recent advancements in generative models have significantly improved ECG reconstruction, accurately capturing fine-grained waveform features remains a key challenge. To address this, we propose a novel PPG-to-ECG reconstruction method that leverages a Vision Transformer (ViT) as the core network. Unlike conventional approaches that rely on single-channel PPG, our method employs a four-channel signal image representation, incorporating the original PPG, its first-order difference, second-order difference, and area under the curve. This multi-channel design enriches feature extraction by preserving both temporal and physiological variations within the PPG. By leveraging the self-attention mechanism in ViT, our approach effectively captures both inter-beat and intra-beat dependencies, leading to more robust and accurate ECG reconstruction. Experimental results demonstrate that our method consistently outperforms existing 1D convolution-based approaches, achieving up to 29% reduction in PRD and 15% reduction in RMSE. The proposed approach also produces improvements in other evaluation metrics, highlighting its robustness and effectiveness in reconstructing ECG signals. Furthermore, to ensure a clinically relevant evaluation, we introduce new performance metrics, including QRS area error, PR interval error, RT interval error, and RT amplitude difference error. Our findings suggest that integrating a four-channel signal image representation with the self-attention mechanism of ViT enables more effective extraction of informative PPG features and improved modeling of beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the potential of PPG as a viable alternative for heart activity monitoring, our approach opens new avenues for cyclic signal analysis and prediction. </p>
<blockquote>
<p>ä»PPGé‡å»ºECGæ˜¯ä¸€é¡¹æœ‰å‰é€”ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ˜¾ç€æ”¹å–„äº†ECGé‡å»ºï¼Œä½†å‡†ç¡®æ•è·ç²¾ç»†æ³¢å½¢ç‰¹å¾ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„PPG-to-ECGé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸ºæ ¸å¿ƒç½‘ç»œã€‚ä¸ä¾èµ–å•é€šé“PPGçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å››é€šé“ä¿¡å·å›¾åƒè¡¨ç¤ºå½¢å¼ï¼Œç»“åˆäº†åŸå§‹PPGã€å…¶ä¸€é˜¶å·®åˆ†ã€äºŒé˜¶å·®åˆ†å’Œæ›²çº¿ä¸‹çš„é¢ç§¯ã€‚è¿™ç§å¤šé€šé“è®¾è®¡é€šè¿‡ä¿ç•™PPGä¸­çš„æ—¶é—´å’Œç”Ÿç†å˜åŒ–æ¥ä¸°å¯Œç‰¹å¾æå–ã€‚é€šè¿‡åˆ©ç”¨ViTä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æ•è·äº†å¿ƒè·³é—´å’Œå¿ƒè·³å†…çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œå®ç°äº†æ›´ç¨³å¥å’Œå‡†ç¡®çš„ECGé‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰çš„åŸºäºä¸€ç»´å·ç§¯çš„æ–¹æ³•ï¼Œåœ¨PRDä¸Šå‡å°‘äº†é«˜è¾¾29%ï¼Œåœ¨RMSEä¸Šå‡å°‘äº†15%ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨å…¶ä»–è¯„ä¼°æŒ‡æ ‡ä¸Šä¹Ÿå–å¾—äº†æ”¹è¿›ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨é‡å»ºECGä¿¡å·æ–¹é¢çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿ä¸´åºŠä¸Šç›¸å…³çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬QRSé¢ç§¯è¯¯å·®ã€PRé—´éš”è¯¯å·®ã€RTé—´éš”è¯¯å·®å’ŒRTæŒ¯å¹…å·®å¼‚è¯¯å·®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°†å››é€šé“ä¿¡å·å›¾åƒè¡¨ç¤ºä¸ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æå–æœ‰ç”¨çš„PPGç‰¹å¾ï¼Œå¹¶æé«˜å¯¹PPG-to-ECGæ˜ å°„çš„å¿ƒè·³é—´å˜åŒ–çš„å»ºæ¨¡èƒ½åŠ›ã€‚é™¤äº†è¯æ˜PPGä½œä¸ºå¿ƒè„æ´»åŠ¨ç›‘æµ‹çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆå…·æœ‰æ½œåŠ›å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºå¾ªç¯ä¿¡å·åˆ†æå’Œé¢„æµ‹å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21767v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºè„‰å†²æ³¢ï¼ˆPPGï¼‰çš„å¿ƒç”µå›¾ï¼ˆECGï¼‰é‡å»ºæ˜¯ä¸€ä¸ªå‰æ™¯å¹¿é˜”ä½†å……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚å°½ç®¡ç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æœ€æ–°è¿›å±•æ˜¾è‘—æ”¹è¿›äº†ECGé‡å»ºï¼Œä½†å‡†ç¡®æ•æ‰ç²¾ç»†æ³¢å½¢ç‰¹å¾ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„PPGåˆ°ECGé‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸ºæ ¸å¿ƒç½‘ç»œã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºå•é€šé“PPGçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å››é€šé“ä¿¡å·å›¾åƒè¡¨ç¤ºï¼ŒåŒ…æ‹¬åŸå§‹PPGã€å…¶ä¸€é˜¶å·®åˆ†ã€äºŒé˜¶å·®åˆ†å’Œæ›²çº¿ä¸‹é¢ç§¯ï¼Œé€šè¿‡åˆ©ç”¨ViTä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æ•æ‰åˆ°äº†å¿ƒè·³é—´å’Œå¿ƒè·³å†…çš„ä¾èµ–å…³ç³»ï¼Œå®ç°äº†æ›´ç¨³å¥å’Œå‡†ç¡®çš„ECGé‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ç»´å·ç§¯åŸºæ–¹æ³•ä¸ŠæŒç»­è¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒPRDé™ä½äº†29%ï¼ŒRMSEé™ä½äº†15%ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿ä¸´åºŠç›¸å…³çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬QRSé¢ç§¯è¯¯å·®ã€PRé—´éš”è¯¯å·®ã€RTé—´éš”è¯¯å·®å’ŒRTæŒ¯å¹…å·®å¼‚è¯¯å·®ã€‚ç ”ç©¶å‘ç°ï¼Œå°†å››é€šé“ä¿¡å·å›¾åƒè¡¨ç¤ºä¸ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ç›¸ç»“åˆï¼Œèƒ½æœ‰æ•ˆæå–PPGçš„ä¿¡æ¯ç‰¹å¾ï¼Œæé«˜å¿ƒè·³é—´å˜åŒ–çš„å»ºæ¨¡èƒ½åŠ›ï¼Œä¸ºPPGåˆ°ECGçš„æ˜ å°„æä¾›äº†æ–°æ€è·¯ã€‚é™¤äº†è¯æ˜PPGä½œä¸ºå¿ƒè„æ´»åŠ¨ç›‘æµ‹çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆå…·æœ‰æ½œåŠ›å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜ä¸ºå‘¨æœŸæ€§ä¿¡å·åˆ†æå’Œé¢„æµ‹å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PPG-to-ECGé‡å»ºæ˜¯ä¸€é¡¹æœ‰å‰é€”ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡ç”Ÿæˆæ¨¡å‹çš„æœ€æ–°è¿›å±•æœ‰æ‰€æ”¹è¿›ï¼Œä½†æ•æ‰ç²¾ç»†æ³¢å½¢ç‰¹å¾ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„PPG-to-ECGé‡å»ºæ–¹æ³•ï¼Œä½¿ç”¨å››é€šé“ä¿¡å·å›¾åƒè¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼ŒåŒ…æ‹¬åŸå§‹PPGåŠå…¶ä¸€é˜¶ã€äºŒé˜¶å·®å€¼å’Œæ›²çº¿ä¸‹é¢ç§¯ã€‚</li>
<li>åˆ©ç”¨ViTä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æœ‰æ•ˆæ•æ‰å¿ƒè·³é—´å’Œå¿ƒè·³å†…çš„ä¾èµ–å…³ç³»ï¼Œå®ç°æ›´ç¨³å¥å’Œå‡†ç¡®çš„ECGé‡å»ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸€ç»´å·ç§¯åŸºæ–¹æ³•ä¸Šè¡¨ç°ä¼˜è¶Šï¼ŒPRDå’ŒRMSEç­‰æŒ‡æ ‡æœ‰æ‰€é™ä½ã€‚</li>
<li>ä¸ºäº†è¿›è¡Œä¸´åºŠç›¸å…³è¯„ä¼°ï¼Œå¼•å…¥äº†æ–°çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚QRSé¢ç§¯è¯¯å·®ã€PRé—´éš”è¯¯å·®ç­‰ã€‚</li>
<li>ç»“åˆå››é€šé“ä¿¡å·å›¾åƒè¡¨ç¤ºå’ŒViTè‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½æœ‰æ•ˆæå–PPGä¿¡æ¯ç‰¹å¾ï¼Œæé«˜å¿ƒè·³é—´å˜åŒ–çš„å»ºæ¨¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-864c7ffd7e363c748d7523533d39430a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition"><a href="#Texture-or-Semantics-Vision-Language-Models-Get-Lost-in-Font-Recognition" class="headerlink" title="Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition"></a>Texture or Semantics? Vision-Language Models Get Lost in Font   Recognition</h2><p><strong>Authors:Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang</strong></p>
<p>Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic capabilities, achieving impressive performance in various tasks such as image recognition and object localization. However, their effectiveness in fine-grained tasks remains an open question. In everyday scenarios, individuals encountering design materials, such as magazines, typography tutorials, research papers, or branding content, may wish to identify aesthetically pleasing fonts used in the text. Given their multimodal capabilities and free accessibility, many VLMs are often considered potential tools for font recognition. This raises a fundamental question: Do VLMs truly possess the capability to recognize fonts? To investigate this, we introduce the Font Recognition Benchmark (FRB), a compact and well-structured dataset comprising 15 commonly used fonts. FRB includes two versions: (i) an easy version, where 10 sentences are rendered in different fonts, and (ii) a hard version, where each text sample consists of the names of the 15 fonts themselves, introducing a stroop effect that challenges model perception. Through extensive evaluation of various VLMs on font recognition tasks, we arrive at the following key findings: (i) Current VLMs exhibit limited font recognition capabilities, with many state-of-the-art models failing to achieve satisfactory performance and being easily affected by the stroop effect introduced by textual information. (ii) Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits in improving font recognition accuracy across different VLMs. (iii) Attention analysis sheds light on the inherent limitations of VLMs in capturing semantic features. </p>
<blockquote>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¡¨ç°å‡ºæ˜¾è‘—çš„è§†é¢‘å’Œè¯­è¨€èƒ½åŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ï¼ˆå¦‚å›¾åƒè¯†åˆ«å’Œå¯¹è±¡å®šä½ï¼‰ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç²¾ç»†ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æ—¥å¸¸åœºæ™¯ä¸­ï¼Œä¸ªäººé‡åˆ°è®¾è®¡ææ–™ï¼Œå¦‚æ‚å¿—ã€æ’ç‰ˆæ•™ç¨‹ã€ç ”ç©¶è®ºæ–‡æˆ–å“ç‰Œå†…å®¹ï¼Œå¯èƒ½ä¼šå¸Œæœ›è¯†åˆ«æ–‡æœ¬ä¸­è§†è§‰ä¸Šä»¤äººæ„‰æ‚¦çš„å­—ä½“ã€‚è€ƒè™‘åˆ°å®ƒä»¬çš„å¤šæ¨¡å¼èƒ½åŠ›å’Œè‡ªç”±è®¿é—®æ€§ï¼Œè®¸å¤šVLMsé€šå¸¸è¢«è®¤ä¸ºæ˜¯å­—ä½“è¯†åˆ«çš„æ½œåœ¨å·¥å…·ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šVLMsæ˜¯å¦çœŸçš„å…·å¤‡è¯†åˆ«å­—ä½“çš„èƒ½åŠ›ï¼Ÿä¸ºäº†è°ƒæŸ¥è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±15ç§å¸¸ç”¨å­—ä½“ç»„æˆçš„ç´§å‡‘ä¸”ç»“æ„è‰¯å¥½çš„æ•°æ®é›†ã€‚FRBåŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šï¼ˆiï¼‰ç®€æ˜“ç‰ˆï¼Œå…¶ä¸­10å¥è¯ä»¥ä¸åŒçš„å­—ä½“å‘ˆç°ï¼›ï¼ˆiiï¼‰å›°éš¾ç‰ˆï¼Œå…¶ä¸­æ¯ä¸ªæ–‡æœ¬æ ·æœ¬ç”±ä¸Šè¿°çš„15ç§å­—ä½“çš„åç§°ç»„æˆï¼Œå¼•å…¥ä¸€ç§æ–¯ç‰¹é²æ™®æ•ˆåº”ï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡å¯¹å„ç§VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ä»¥ä¸‹å…³é”®å‘ç°ï¼šï¼ˆiï¼‰å½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ¨¡å‹æœªèƒ½å–å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“å—åˆ°æ–‡æœ¬ä¿¡æ¯å¼•å…¥çš„æ–¯ç‰¹é²æ™®æ•ˆåº”çš„å½±å“ã€‚ï¼ˆiiï¼‰åœ¨VLMsä¸­ï¼Œå°‘æ ·æœ¬å­¦ä¹ å’Œæ€ç»´é“¾æç¤ºå¯¹æ”¹å–„å­—ä½“è¯†åˆ«ç²¾åº¦çš„å¥½å¤„å¾®ä¹å…¶å¾®ã€‚ï¼ˆiiiï¼‰æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•è·è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23768v2">PDF</a> Accepted to COLM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒè¯†åˆ«ã€ç‰©ä½“å®šä½ç­‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„è§†è§‰å’Œè¯­è¨€èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†ä»»åŠ¡ä¸Šçš„æ•ˆæœä»æœ‰å¾…æ¢è®¨ã€‚é’ˆå¯¹è®¾è®¡ææ–™ä¸­çš„å­—ä½“è¯†åˆ«é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å­—ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆFRBï¼‰ï¼ŒåŒ…å«15ç§å¸¸ç”¨å­—ä½“ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œé¡¶å°–æ¨¡å‹éš¾ä»¥è¾¾åˆ°æ»¡æ„æ€§èƒ½ï¼Œä¸”æ˜“å—æ–‡æœ¬ä¿¡æ¯å¹²æ‰°ã€‚å°‘é‡å­¦ä¹ åŠChain-of-Thoughtæç¤ºå¯¹æå‡æ€§èƒ½ä½œç”¨æœ‰é™ã€‚æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsæ•æ‰è¯­ä¹‰ç‰¹å¾çš„å†…åœ¨å±€é™ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰VLMsåœ¨å­—ä½“è¯†åˆ«æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œéš¾ä»¥æ»¡è¶³ç²¾ç»†ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>FRBåŸºå‡†æµ‹è¯•æœ‰æ•ˆæ­ç¤ºäº†VLMsåœ¨å­—ä½“è¯†åˆ«ä¸Šçš„æ€§èƒ½çŸ­æ¿ã€‚</li>
<li>é¡¶å°–æ¨¡å‹åœ¨FRBæµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œæ˜“å—æ–‡æœ¬ä¿¡æ¯å¹²æ‰°ã€‚</li>
<li>å°‘é‡å­¦ä¹ å¯¹æå‡VLMsåœ¨å­—ä½“è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä½œç”¨æœ‰é™ã€‚</li>
<li>Chain-of-Thoughtæç¤ºç­–ç•¥åœ¨æ”¹å–„å­—ä½“è¯†åˆ«å‡†ç¡®ç‡æ–¹é¢æ•ˆæœä¸æ˜¾è‘—ã€‚</li>
<li>æ³¨æ„åŠ›åˆ†ææ­ç¤ºäº†VLMsåœ¨æ•æ‰è¯­ä¹‰ç‰¹å¾æ–¹é¢çš„å†…åœ¨å±€é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b5b435e0364fe8539723631a7fc08e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09797cf823fa1543e9098f0df2c1abe6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-689f255139b611ef5d349dcc73baf099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73c08be47c36f087923de6cb182eac21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b02f4b80fa36aeba1763c3c0d79e2562.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EEG-CLIP-Learning-EEG-representations-from-natural-language-descriptions"><a href="#EEG-CLIP-Learning-EEG-representations-from-natural-language-descriptions" class="headerlink" title="EEG-CLIP : Learning EEG representations from natural language   descriptions"></a>EEG-CLIP : Learning EEG representations from natural language   descriptions</h2><p><strong>Authors:Tidiane Camaret Ndir, Robin Tibor Schirrmeister, Tonio Ball</strong></p>
<p>Deep networks for electroencephalogram (EEG) decoding are often only trained to solve one specific task, such as pathology or age decoding. A more general task-agnostic approach is to train deep networks to match a (clinical) EEG recording to its corresponding textual medical report and vice versa. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework, EEG-CLIP, that aligns the EEG time series and the descriptions of the corresponding clinical text in a shared embedding space. We investigated its potential for versatile EEG decoding, evaluating performance in a range of few-shot and zero-shot settings. Overall, we show that EEG-CLIP manages to non-trivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero-shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at <a target="_blank" rel="noopener" href="https://github.com/tidiane-camaret/EEGClip">https://github.com/tidiane-camaret/EEGClip</a> </p>
<blockquote>
<p>è„‘ç”µå›¾ï¼ˆEEGï¼‰è§£ç çš„æ·±åº¦ç½‘ç»œé€šå¸¸åªè¢«è®­ç»ƒç”¨äºè§£å†³ä¸€ä¸ªç‰¹å®šä»»åŠ¡ï¼Œå¦‚ç—…ç†å­¦æˆ–å¹´é¾„è§£ç ã€‚ä¸€ç§æ›´é€šç”¨çš„ä»»åŠ¡æ— å…³æ–¹æ³•æ˜¯å¯¹æ·±åº¦ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œä½¿å…¶èƒ½å¤ŸåŒ¹é…ä¸´åºŠè„‘ç”µå›¾è®°å½•åŠå…¶ç›¸åº”çš„æ–‡æœ¬åŒ»å­¦æŠ¥å‘Šï¼Œåä¹‹äº¦ç„¶ã€‚è¿™ç§æ–¹æ³•åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­å›¾åƒåŠå…¶æ–‡æœ¬æ ‡é¢˜çš„åŒ¹é…ä¸Šå¼€åˆ›äº†å…ˆæ²³ï¼Œéšåé€šè¿‡ä½¿ç”¨æ–‡æœ¬ç±»åˆ«æç¤ºå®ç°äº†æˆåŠŸçš„é›¶æ ·æœ¬è§£ç ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬éµå¾ªè¿™ç§æ–¹æ³•ï¼Œå¼€å‘äº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶EEG-CLIPï¼Œè¯¥æ¡†æ¶å°†è„‘ç”µå›¾æ—¶é—´åºåˆ—ä¸ç›¸åº”ä¸´åºŠæ–‡æœ¬çš„æè¿°å¯¹é½åˆ°ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´ä¸­ã€‚æˆ‘ä»¬ç ”ç©¶äº†å…¶åœ¨å¤šç§è„‘ç”µå›¾è§£ç ä¸­çš„æ½œåŠ›ï¼Œè¯„ä¼°äº†åœ¨å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†EEG-CLIPèƒ½å¤Ÿéå¹³å‡¡åœ°å¯¹é½æ–‡æœ¬å’Œè„‘ç”µå›¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å·¥ä½œå±•ç¤ºäº†ä¸€ç§å­¦ä¹ é€šç”¨è„‘ç”µå›¾è¡¨ç¤ºçš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œè¿™å¯èƒ½é€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–ä½¿ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹æ¥æ›´å®¹æ˜“åœ°åˆ†æå„ç§è§£ç é—®é¢˜ã€‚é‡ç°æˆ‘ä»¬ç»“æœçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tidiane-camaret/EEGClip%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/tidiane-camaret/EEGClipæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16531v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–°æ–¹æ³•EEG-CLIPï¼Œç”¨äºå°†è„‘ç”µå›¾ï¼ˆEEGï¼‰æ—¶é—´åºåˆ—ä¸ç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°å¯¹é½ï¼Œåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­è¿›è¡ŒåŒ¹é…ã€‚è¯¥æ–¹æ³•é‡‡ç”¨é€šç”¨ä»»åŠ¡æ— å…³çš„å­¦ä¹ æ–¹å¼ï¼Œèƒ½å¤Ÿåº”å¯¹å¤šç§è„‘ç”µå›¾è§£ç ä»»åŠ¡ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒEEG-CLIPèƒ½å¤Ÿå®ç°å¯¹æ–‡æœ¬å’Œè„‘ç”µå›¾è¡¨ç¤ºçš„æœ‰æ•ˆå¯¹é½ï¼Œä¸ºå­¦ä¹ é€šç”¨çš„è„‘ç”µå›¾è¡¨ç¤ºæä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ï¼Œæœ‰æœ›é€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–åˆ©ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è¿›è¡Œç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œç®€åŒ–å¯¹å¤šç§è§£ç é—®é¢˜çš„åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EEG-CLIPæ˜¯ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œç”¨äºåŒ¹é…EEGè®°å½•ä¸ç›¸åº”çš„æ–‡æœ¬åŒ»ç–—æŠ¥å‘Šã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä»»åŠ¡æ— å…³çš„å­¦ä¹ æ–¹å¼ï¼Œå¯åº”ç”¨äºå¤šç§EEGè§£ç ä»»åŠ¡ã€‚</li>
<li>EEG-CLIPåœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­å¯¹EEGæ—¶é—´åºåˆ—å’Œä¸´åºŠæ–‡æœ¬æè¿°è¿›è¡Œå¯¹é½ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜EEG-CLIPèƒ½æœ‰æ•ˆå¯¹é½æ–‡æœ¬å’ŒEEGè¡¨ç¤ºã€‚</li>
<li>EEG-CLIPæ–¹æ³•å…·æœ‰æ½œåŠ›é€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–åˆ©ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è¿›è¡Œç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œç®€åŒ–å¯¹å¤šç§è§£ç é—®é¢˜çš„åˆ†æã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œæ–¹ä¾¿å¤ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa644fcfa34eed2b0b16c76e27f8d8bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576a1016958861ef530e146394d78e19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6be7f6f1506263a5657bc1eafcdff857.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2348d77a854c27f128a0c809f5603e85.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion"><a href="#PRISM-High-Resolution-Precise-Counterfactual-Medical-Image-Generation-using-Language-guided-Stable-Diffusion" class="headerlink" title="PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion"></a>PRISM: High-Resolution &amp; Precise Counterfactual Medical Image Generation   using Language-guided Stable Diffusion</h2><p><strong>Authors:Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel</strong></p>
<p>Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures that are robust to the unique complexities posed by medical imaging data. Rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at <a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM">https://github.com/Amarkr1/PRISM</a>. </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å½±åƒå¼€å‘å¯é ä¸”å¯æ¨å¹¿çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæ•°æ®ä¸­çš„è™šå‡å…³è”ã€æ•°æ®ä¸å¹³è¡¡ä»¥åŠæ–‡æœ¬æ³¨é‡Šæœ‰é™ç­‰é—®é¢˜ã€‚è¦è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œéœ€è¦èƒ½å¤Ÿé€‚åº”åŒ»å­¦å½±åƒæ•°æ®ç‹¬ç‰¹å¤æ‚æ€§çš„æ¶æ„ã€‚è‡ªç„¶å›¾åƒé¢†åŸŸçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¼•å‘äº†ä¸€ä¸ªé—®é¢˜ï¼Œå³å¦‚ä½•å°†å…¶é€‚åº”äºåŒ»å­¦å½±åƒä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PRISMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹é€šè¿‡Stable Diffusionç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€è¯­è¨€å¼•å¯¼çš„åŒ»ç–—å½±åƒåäº‹å®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥å‰æ‰€æœªæœ‰çš„ç²¾åº¦æœ‰é€‰æ‹©åœ°ä¿®æ”¹è™šå‡å…³è”ï¼ˆåŒ»ç–—è®¾å¤‡å’Œç–¾ç—…ç‰¹å¾ï¼‰ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™å…¶ä»–å›¾åƒç‰¹æ€§çš„åŒæ—¶ç§»é™¤å’Œæ·»åŠ ç‰¹å®šå±æ€§ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†PRISMåœ¨åäº‹å®ç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶ä¿ƒè¿›äº†åœ¨ä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆä¸­å¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ã€‚ä¸ºäº†æ›´å¹¿æ³›çš„é‡‡ç”¨å’Œç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Amarkr1/PRISM%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/Amarkr1/PRISMä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00196v2">PDF</a> MIDL 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†PRISMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹é€šè¿‡Stable Diffusionç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€è¯­è¨€å¼•å¯¼çš„åŒ»ç–—å›¾åƒåäº‹å®æ•°æ®ã€‚æ­¤æ–¹æ³•åœ¨é€‰æ‹©æ€§ä¿®æ”¹åŒ»ç–—è®¾å¤‡å’Œç–¾ç—…ç‰¹å¾æ–¹é¢çš„è™šå‡å…³è”æ–¹é¢å±•ç°äº†å‰æ‰€æœªæœ‰çš„ç²¾åº¦ï¼Œèƒ½å¤Ÿåœ¨ä¿ç•™å…¶ä»–å›¾åƒç‰¹å¾çš„åŒæ—¶ç§»é™¤å’Œæ·»åŠ ç‰¹å®šå±æ€§ã€‚é€šè¿‡å¹¿æ³›è¯„ä¼°ï¼Œè¡¨æ˜PRISMæ¨åŠ¨äº†åäº‹å®ç”Ÿæˆçš„å‘å±•ï¼Œå¹¶ä¸ºä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆçš„ä¸‹æ¸¸åˆ†ç±»å™¨å¼€å‘æä¾›äº†æ›´ç¨³å¥çš„åŸºç¡€ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŒ»ç–—æˆåƒé¢†åŸŸåœ¨å¼€å‘å¯é ä¸”å¯æ¨å¹¿çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è™šå‡å…³è”ã€æ•°æ®ä¸å¹³è¡¡å’Œæœ‰é™æ–‡æœ¬æ³¨é‡Šç­‰é—®é¢˜ã€‚</li>
<li>éœ€è¦é’ˆå¯¹åŒ»ç–—æˆåƒæ•°æ®çš„ç‹¬ç‰¹å¤æ‚æ€§æ„å»ºç¨³å¥çš„æ¶æ„æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>PRISMæ¡†æ¶åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€è¯­è¨€å¼•å¯¼çš„åŒ»ç–—å›¾åƒåäº‹å®æ•°æ®ï¼Œä»¥åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ã€‚</li>
<li>PRISMèƒ½å¤Ÿåœ¨é€‰æ‹©æ€§ä¿®æ”¹åŒ»ç–—è®¾å¤‡å’Œç–¾ç—…ç‰¹å¾çš„è™šå‡å…³è”æ–¹é¢å®ç°é«˜ç²¾åº¦ã€‚</li>
<li>PRISMæ¨åŠ¨äº†åäº‹å®ç”Ÿæˆçš„å‘å±•ï¼Œæœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥çš„ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œä¸ºä¸´åºŠéƒ¨ç½²è§£å†³æ–¹æ¡ˆæä¾›æ”¯æŒã€‚</li>
<li>PRISMæ¡†æ¶çš„ä»£ç å·²å…¬å¼€å¯ç”¨ï¼Œä¾¿äºæ›´å¹¿æ³›çš„é‡‡ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbb82f5361ed12de49365e0003c43ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7da124f74e6350e07548626ca0e7182.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89a2ff88eede7968fbc857e3cdb7cd3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7520b5b0e75705022c85eab786ec2336.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning"><a href="#Visual-Adaptive-Prompting-for-Compositional-Zero-Shot-Learning" class="headerlink" title="Visual Adaptive Prompting for Compositional Zero-Shot Learning"></a>Visual Adaptive Prompting for Compositional Zero-Shot Learning</h2><p><strong>Authors:Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh</strong></p>
<p>Vision-Language Models (VLMs) have demonstrated impressive multimodal capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitivesâ€“such as attributes and objectsâ€“that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose a Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è”åˆè¡¨ç¤ºè§†è§‰å’Œæ–‡æœ¬æ•°æ®æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è·¨æ¨¡æ€èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå®Œæˆç»„åˆé›¶å°„å‡»å­¦ä¹ ï¼ˆCZSLï¼‰ç­‰ä»»åŠ¡çš„æœ‰åŠ›å·¥å…·ã€‚CZSLè¦æ±‚æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°è®­ç»ƒæœŸé—´æœªæ˜ç¡®é‡åˆ°çš„æ–°ç»„åˆçš„è§†è§‰å…ƒç´ ï¼Œå¦‚å±æ€§å’Œå¯¹è±¡ã€‚æœ€è¿‘çš„å…³äºCZSLæç¤ºçš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿®æ”¹æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥ï¼Œé€šå¸¸ä½¿ç”¨é™æ€æç¤ºï¼Œè¿™äº›æç¤ºåœ¨ä¸åŒçš„è§†è§‰ä¸Šä¸‹æ–‡ä¸­ä¸ä¼šå‘ç”Ÿå˜åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ•æ‰ä¸æ–­å˜åŒ–çš„è§†è§‰ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¾§é‡äºæ–‡æœ¬é€‚åº”ï¼Œè€Œä¸æ˜¯åˆ©ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œç»„åˆæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºä»“åº“å’Œè§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶å†…çš„åŸºäºç›¸ä¼¼æ€§çš„æ£€ç´¢æœºåˆ¶æ¥å¼¥è¡¥è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€çš„è§†è§‰æç¤ºä»“åº“æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®å›¾åƒä¸­çš„è§†è§‰ç‰¹å¾é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å’Œå¯¹è±¡æç¤ºã€‚æˆ‘ä»¬æå‡ºçš„ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œå®ƒé¼“åŠ±æ¨¡å‹å­¦ä¹ ä¸€ä¸ªæ›´å…·æ³›åŒ–èƒ½åŠ›çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ä¸‰ä¸ªCZSLåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œæ— è®ºæ˜¯å°é—­è¿˜æ˜¯å¼€æ”¾ä¸–ç•Œåœºæ™¯ï¼Œéƒ½è¯æ˜äº†å…¶å¤„äºé¢†å…ˆåœ°ä½çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20292v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè§†è§‰è‡ªé€‚åº”æç¤ºç³»ç»Ÿï¼ˆVAPSï¼‰ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºåº“å’ŒåŸºäºç›¸ä¼¼åº¦çš„æ£€ç´¢æœºåˆ¶ï¼Œåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¡†æ¶ä¸‹ï¼Œæ¡¥æ¥è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„é¸¿æ²Ÿã€‚VAPSé€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å¯¹è±¡æç¤ºï¼Œä»¥å›¾åƒè§†è§‰ç‰¹å¾ä¸ºåŸºç¡€è¿›è¡Œæç¤ºï¼Œå¹¶åŒ…æ‹¬ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´é€šç”¨çš„åµŒå…¥ç©ºé—´ã€‚åœ¨ä¸‰ä¸ªCZSLåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬å°é—­å’Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨Compositional Zero-Shot Learning (CZSL)ä»»åŠ¡ä¸­ã€‚</li>
<li>ç°æœ‰çš„æç¤ºæ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬é€‚åº”ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨è§†è§‰ç‰¹å¾è¿›è¡Œç»„åˆæ¨ç†ã€‚</li>
<li>æå‡ºçš„VAPSç³»ç»Ÿé€šè¿‡åˆ©ç”¨å¯å­¦ä¹ çš„è§†è§‰æç¤ºåº“å’ŒåŸºäºç›¸ä¼¼åº¦çš„æ£€ç´¢æœºåˆ¶ï¼Œæ¡¥æ¥è¯­ä¹‰å’Œè§†è§‰ç‰¹å¾ä¹‹é—´çš„é¸¿æ²Ÿã€‚</li>
<li>VAPSåŒ…æ‹¬ä¸€ä¸ªåŠ¨æ€è§†è§‰æç¤ºé€‰æ‹©æœºåˆ¶ï¼Œæ ¹æ®å›¾åƒè§†è§‰ç‰¹å¾é€‰æ‹©æœ€ç›¸å…³çš„å±æ€§å¯¹è±¡æç¤ºã€‚</li>
<li>VAPSç³»ç»ŸåŒ…å«ä¸€ä¸ªè§†è§‰æç¤ºé€‚é…å™¨ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´é€šç”¨çš„åµŒå…¥ç©ºé—´ã€‚</li>
<li>åœ¨ä¸‰ä¸ªCZSLåŸºå‡†æµ‹è¯•ä¸Šï¼ŒVAPSç³»ç»Ÿå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e23c1f66bac94afa827f3375f13931e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-923fab0be9f10852e4f71791c8342aa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8577c124f2f2b957f9eb7ed8c1121359.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Prompt-driven-Transferable-Adversarial-Attack-on-Person-Re-Identification-with-Attribute-aware-Textual-Inversion"><a href="#Prompt-driven-Transferable-Adversarial-Attack-on-Person-Re-Identification-with-Attribute-aware-Textual-Inversion" class="headerlink" title="Prompt-driven Transferable Adversarial Attack on Person   Re-Identification with Attribute-aware Textual Inversion"></a>Prompt-driven Transferable Adversarial Attack on Person   Re-Identification with Attribute-aware Textual Inversion</h2><p><strong>Authors:Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang</strong></p>
<p>Person re-identification (re-id) models are vital in security surveillance systems, requiring transferable adversarial attacks to explore the vulnerabilities of them. Recently, vision-language models (VLM) based attacks have shown superior transferability by attacking generalized image and textual features of VLM, but they lack comprehensive feature disruption due to the overemphasis on discriminative semantics in integral representation. In this paper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novel method that leverages VLMâ€™s image-text alignment capability to explicitly disrupt fine-grained semantic features of pedestrian images by destroying attribute-specific textual embeddings. To obtain personalized textual descriptions for individual attributes, textual inversion networks are designed to map pedestrian images to pseudo tokens that represent semantic embeddings, trained in the contrastive learning manner with images and a predefined prompt template that explicitly describes the pedestrian attributes. Inverted benign and adversarial fine-grained textual semantics facilitate attacker in effectively conducting thorough disruptions, enhancing the transferability of adversarial examples. Extensive experiments show that AP-Attack achieves state-of-the-art transferability, significantly outperforming previous methods by 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios. </p>
<blockquote>
<p>è¡Œäººå†è¯†åˆ«ï¼ˆRe-IDï¼‰æ¨¡å‹åœ¨å®‰é˜²ç›‘æ§ç³»ç»Ÿä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œéœ€è¦å¯è¿ç§»çš„å¯¹æŠ—æ€§æ”»å‡»æ¥æ¢ç´¢å…¶è„†å¼±æ€§ã€‚æœ€è¿‘ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ”»å‡»ï¼ˆVLMï¼‰é€šè¿‡æ”»å‡»VLMçš„é€šç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾è¡¨ç°å‡ºä¼˜è¶Šçš„è¿ç§»æ€§ï¼Œä½†ç”±äºè¿‡åˆ†å¼ºè°ƒæ•´ä½“è¡¨ç¤ºä¸­çš„åˆ¤åˆ«è¯­ä¹‰ï¼Œå®ƒä»¬ç¼ºä¹å…¨é¢çš„ç‰¹å¾ç ´åã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§æ„ŸçŸ¥æç¤ºæ”»å‡»ï¼ˆAP-Attackï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨VLMçš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡ç ´åç‰¹å®šå±æ€§çš„æ–‡æœ¬åµŒå…¥æ¥æ˜ç¡®ç ´åè¡Œäººå›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ç‰¹å¾ã€‚ä¸ºäº†è·å¾—ä¸ªåˆ«å±æ€§çš„ä¸ªæ€§åŒ–æ–‡æœ¬æè¿°ï¼Œè®¾è®¡äº†æ–‡æœ¬åè½¬ç½‘ç»œï¼Œå°†è¡Œäººå›¾åƒæ˜ å°„åˆ°ä»£è¡¨è¯­ä¹‰åµŒå…¥çš„ä¼ªä»¤ç‰Œä¸Šï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ–¹å¼ä¸å›¾åƒå’Œé¢„å…ˆå®šä¹‰çš„æç¤ºæ¨¡æ¿è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¨¡æ¿æ˜ç¡®æè¿°äº†è¡Œäººå±æ€§ã€‚æ­£å‘å’Œåå‘çš„ç»†ç²’åº¦æ–‡æœ¬è¯­ä¹‰æœ‰åŠ©äºæ”»å‡»è€…è¿›è¡Œæœ‰æ•ˆçš„å…¨é¢ç ´åï¼Œæé«˜å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAP-Attackåœ¨è¿ç§»æ€§æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œåœ¨è·¨æ¨¡å‹å’Œæ•°æ®é›†æ”»å‡»åœºæ™¯ä¸­ï¼Œå¹³å‡ä¸‹é™ç‡æé«˜äº†22.9%ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19697v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–°å‹æ”»å‡»æ–¹æ³•â€”â€”å±æ€§æ„ŸçŸ¥æç¤ºæ”»å‡»ï¼ˆAP-Attackï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨VLMçš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡ç ´åè¡Œäººå›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ç‰¹å¾æ¥æ˜ç¡®å¹²æ‰°æ¨¡å‹ã€‚è®¾è®¡æ–‡æœ¬åæ¼”ç½‘ç»œå°†è¡Œäººå›¾åƒæ˜ å°„åˆ°ä»£è¡¨è¯­ä¹‰åµŒå…¥çš„ä¼ªä»¤ç‰Œä¸Šï¼Œå€ŸåŠ©å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ä¸å›¾åƒå’Œé¢„å…ˆå®šä¹‰çš„æè¿°è¡Œäººå±æ€§çš„æç¤ºæ¨¡æ¿è¿›è¡Œè®­ç»ƒã€‚è¿™ç§æ–°å‹çš„æ”»å‡»æ–¹å¼æ˜¾è‘—æé«˜äº†å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ï¼Œåœ¨è·¨æ¨¡å‹å’Œè·¨æ•°æ®é›†æ”»å‡»åœºæ™¯ä¸­ï¼Œå¹³å‡ä¸‹é™ç‡æé«˜äº†22.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AP-Attackæ˜¯ä¸€ç§æ–°å‹çš„æ”»å‡»æ–¹æ³•ï¼ŒåŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å›¾åƒæ–‡æœ¬å¯¹é½èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç ´åè¡Œäººå›¾åƒçš„ç»†ç²’åº¦è¯­ä¹‰ç‰¹å¾æ¥å¹²æ‰°æ¨¡å‹ã€‚</li>
<li>æ–‡æœ¬åæ¼”ç½‘ç»œè¢«è®¾è®¡ç”¨äºå°†è¡Œäººå›¾åƒæ˜ å°„åˆ°ä»£è¡¨è¯­ä¹‰åµŒå…¥çš„ä¼ªä»¤ç‰Œä¸Šã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œç»“åˆå›¾åƒå’Œé¢„å®šä¹‰çš„æè¿°è¡Œäººå±æ€§çš„æç¤ºæ¨¡æ¿è¿›è¡Œè®­ç»ƒã€‚</li>
<li>AP-Attackæ˜¾è‘—æé«˜äº†å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ã€‚</li>
<li>åœ¨è·¨æ¨¡å‹å’Œè·¨æ•°æ®é›†æ”»å‡»åœºæ™¯ä¸­ï¼ŒAP-Attackçš„å¹³å‡ä¸‹é™ç‡æé«˜äº†22.9%ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f8c4a57ffad1e2395bcedafb3398d6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a88b7adb0f1df6c1e9aaac4117302f6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84b0a52681812334b36c46d74f07b231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c33e817683ea3976042918d9c78dadd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Advancing-Textual-Prompt-Learning-with-Anchored-Attributes"><a href="#Advancing-Textual-Prompt-Learning-with-Anchored-Attributes" class="headerlink" title="Advancing Textual Prompt Learning with Anchored Attributes"></a>Advancing Textual Prompt Learning with Anchored Attributes</h2><p><strong>Authors:Zheng Li, Yibing Song, Ming-Ming Cheng, Xiang Li, Jian Yang</strong></p>
<p>Textual-based prompt learning methods primarily employ multiple learnable soft prompts and hard class tokens in a cascading manner as text inputs, aiming to align image and text (category) spaces for downstream tasks. However, current training is restricted to aligning images with predefined known categories and cannot be associated with unknown categories. In this work, we propose utilizing universal attributes as a bridge to enhance the alignment between images and unknown categories. Specifically, we introduce an Attribute-anchored Textual Prompt learning method for vision-language models, named ATPrompt. This approach expands the learning space of soft prompts from the original one-dimensional category level into the multi-dimensional attribute level by incorporating multiple attribute tokens into the learnable soft prompts. Through this modification, we transform the text prompt from a category-centric form to an attribute-category hybrid form. Additionally, we introduce a straightforward differentiable attribute search method to identify representative and suitable attributes for downstream tasks. As an easy-to-use plug-in technique, ATPrompt can seamlessly replace the existing basic prompt format in textual-based methods, providing general improvements at a negligible computational cost. Extensive experiments across 11 datasets validate the effectiveness of our method. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt">https://github.com/zhengli97/ATPrompt</a>. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦ä½¿ç”¨å¤šä¸ªå¯å­¦ä¹ çš„è½¯æç¤ºå’Œç¡¬ç±»åˆ«ä»¤ç‰Œä»¥çº§è”æ–¹å¼ä½œä¸ºæ–‡æœ¬è¾“å…¥ï¼Œæ—¨åœ¨å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼ˆç±»åˆ«ï¼‰ç©ºé—´ä»¥ä¾›ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰è®­ç»ƒä»…é™äºå¯¹é½å›¾åƒä¸é¢„å®šä¹‰çš„å·²çŸ¥ç±»åˆ«ï¼Œæ— æ³•ä¸æœªçŸ¥ç±»åˆ«ç›¸å…³è”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨é€šç”¨å±æ€§ä½œä¸ºæ¡¥æ¢ï¼Œå¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹å¼•å…¥äº†ä¸€ç§åä¸ºATPromptçš„å±æ€§é”šå®šæ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¤šä¸ªå±æ€§ä»¤ç‰Œçº³å…¥å¯å­¦ä¹ çš„è½¯æç¤ºä¸­ï¼Œå°†è½¯æç¤ºçš„å­¦ä¹ ç©ºé—´ä»åŸå§‹çš„ä¸€ç»´ç±»åˆ«å±‚é¢æ‰©å±•åˆ°å¤šç»´å±æ€§å±‚é¢ã€‚é€šè¿‡è¿™ä¸€æ”¹è¿›ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æç¤ºä»ä»¥ç±»åˆ«ä¸ºä¸­å¿ƒçš„å½¢å¼è½¬å˜ä¸ºå±æ€§-ç±»åˆ«æ··åˆå½¢å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç®€å•çš„å¯åŒºåˆ†å±æ€§æœç´¢æ–¹æ³•ï¼Œä»¥è¯†åˆ«é€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å±æ€§ã€‚ä½œä¸ºä¸€ç§æ˜“äºä½¿ç”¨çš„æ’ä»¶æŠ€æœ¯ï¼ŒATPromptå¯ä»¥æ— ç¼æ›¿æ¢åŸºäºæ–‡æœ¬æ–¹æ³•ä¸­çš„ç°æœ‰åŸºæœ¬æç¤ºæ ¼å¼ï¼Œä»¥å¾®è–„çš„è®¡ç®—æˆæœ¬æä¾›ä¸€èˆ¬æ€§çš„æ”¹è¿›ã€‚åœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt%E3%80%82">https://github.com/zhengli97/ATPromptã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09442v4">PDF</a> ICCV 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt">https://github.com/zhengli97/ATPrompt</a>. Project Page:   <a target="_blank" rel="noopener" href="https://zhengli97.github.io/ATPrompt/">https://zhengli97.github.io/ATPrompt/</a></p>
<p><strong>Summary</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŸºäºå±æ€§çš„æ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆATPromptï¼‰ï¼Œé€šè¿‡ç»“åˆé€šç”¨å±æ€§å¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚å®ƒå°†å­¦ä¹ ç©ºé—´ä»å•ä¸€ç±»åˆ«çº§åˆ«æ‰©å±•åˆ°å¤šå±æ€§çº§åˆ«ï¼Œå®ç°äº†ç±»åˆ«ä¸å±æ€§çš„æ··åˆæ–‡æœ¬æç¤ºå½¢å¼ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¼•å…¥äº†ä¸€ç§ç®€æ´çš„å¯åŒºåˆ†å±æ€§æœç´¢æ–¹æ³•ï¼Œä»¥è¯†åˆ«ä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å±æ€§ã€‚ATPromptä½œä¸ºæ˜“äºä½¿ç”¨çš„æ’ä»¶æŠ€æœ¯ï¼Œå¯åœ¨æ–‡æœ¬æç¤ºæ–¹æ³•ä¸­æ— ç¼æ›¿æ¢ç°æœ‰åŸºæœ¬æ ¼å¼ï¼Œæä¾›é€šç”¨çš„æ”¹è¿›ä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºäºå±æ€§çš„æ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆATPromptï¼‰ï¼Œæ—¨åœ¨å¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>ATPrompté€šè¿‡å°†å­¦ä¹ ç©ºé—´æ‰©å±•åˆ°å¤šå±æ€§çº§åˆ«ï¼Œå®ç°äº†æ–‡æœ¬æç¤ºå½¢å¼çš„ç±»åˆ«ä¸å±æ€§çš„æ··åˆã€‚</li>
<li>ATPromptå¼•å…¥äº†å¯åŒºåˆ†å±æ€§æœç´¢æ–¹æ³•ï¼Œä»¥è¯†åˆ«ä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å±æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§æ•°æ®é›†ï¼Œå…·æœ‰å¹¿æ³›çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ATPromptæŠ€æœ¯èƒ½å¤Ÿæ— ç¼æ›¿æ¢ç°æœ‰æ–‡æœ¬æç¤ºæ–¹æ³•ä¸­çš„åŸºæœ¬æ ¼å¼ã€‚</li>
<li>ATPromptæä¾›é€šç”¨çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c671303a09d449cab55b104bda291534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00c7920d0a2b6ceea27a1f0c234f1931.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41647804142d238406e8d963b18df0c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8d03e7a646a1d75a5597edc755d19ac0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-374deb0af5ddf78d2382db575a1d9bd3.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Cascaded-Multi-Scale-Attention-for-Enhanced-Multi-Scale-Feature-Extraction-and-Interaction-with-Low-Resolution-Images"><a href="#Cascaded-Multi-Scale-Attention-for-Enhanced-Multi-Scale-Feature-Extraction-and-Interaction-with-Low-Resolution-Images" class="headerlink" title="Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature   Extraction and Interaction with Low-Resolution Images"></a>Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature   Extraction and Interaction with Low-Resolution Images</h2><p><strong>Authors:Xiangyong Lu, Masanori Suganuma, Takayuki Okatani</strong></p>
<p>In real-world applications of image recognition tasks, such as human pose estimation, cameras often capture objects, like human bodies, at low resolutions. This scenario poses a challenge in extracting and leveraging multi-scale features, which is often essential for precise inference. To address this challenge, we propose a new attention mechanism, named cascaded multi-scale attention (CMSA), tailored for use in CNN-ViT hybrid architectures, to handle low-resolution inputs effectively. The design of CMSA enables the extraction and seamless integration of features across various scales without necessitating the downsampling of the input image or feature maps. This is achieved through a novel combination of grouped multi-head self-attention mechanisms with window-based local attention and cascaded fusion of multi-scale features over different scales. This architecture allows for the effective handling of features across different scales, enhancing the modelâ€™s ability to perform tasks such as human pose estimation, head pose estimation, and more with low-resolution images. Our experimental results show that the proposed method outperforms existing state-of-the-art methods in these areas with fewer parameters, showcasing its potential for broad application in real-world scenarios where capturing high-resolution images is not feasible. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xyongLu/CMSA">https://github.com/xyongLu/CMSA</a>. </p>
<blockquote>
<p>åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡çš„ç°å®åº”ç”¨ï¼ˆå¦‚äººä½“å§¿æ€ä¼°è®¡ï¼‰ä¸­ï¼Œç›¸æœºé€šå¸¸ä¼šä»¥ä½åˆ†è¾¨ç‡æ•è·ç‰©ä½“ï¼ˆå¦‚äººä½“ï¼‰ã€‚è¿™ç§æƒ…å†µåœ¨æå–å’Œåˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾æ–¹é¢æ„æˆäº†æŒ‘æˆ˜ï¼Œè€Œå¯¹äºç²¾ç¡®æ¨æ–­ï¼Œå¤šå°ºåº¦ç‰¹å¾é€šå¸¸æ˜¯å¿…ä¸å¯å°‘çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåä¸ºçº§è”å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆCMSAï¼‰ï¼Œå®ƒé€‚ç”¨äºCNN-ViTæ··åˆæ¶æ„ï¼Œå¯æœ‰æ•ˆå¤„ç†ä½åˆ†è¾¨ç‡è¾“å…¥ã€‚CMSAçš„è®¾è®¡å®ç°äº†è·¨ä¸åŒå°ºåº¦çš„ç‰¹å¾æå–å’Œæ— ç¼é›†æˆï¼Œæ— éœ€å¯¹è¾“å…¥å›¾åƒæˆ–ç‰¹å¾å›¾è¿›è¡Œé™é‡‡æ ·ã€‚è¿™æ˜¯é€šè¿‡åˆ†ç»„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸åŸºäºçª—å£çš„å±€éƒ¨æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œä»¥åŠåœ¨ä¸åŒå°ºåº¦ä¸Šå®ç°å¤šå°ºåº¦ç‰¹å¾çš„çº§è”èåˆæ¥å®ç°çš„ã€‚è¯¥æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œæé«˜æ¨¡å‹åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸‹æ‰§è¡Œè¯¸å¦‚äººä½“å§¿æ€ä¼°è®¡ã€å¤´éƒ¨å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¾ƒå°‘çš„å‚æ•°ä¸‹è¶…è¿‡äº†è¿™äº›é¢†åŸŸçš„æœ€æ–°æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæ•è·ä¸å¯è¡Œçš„ç°å®åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xyongLu/CMSA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xyongLu/CMSAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02197v2">PDF</a> 9 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­å¸¸è§çš„ä½åˆ†è¾¨ç‡è¾“å…¥é—®é¢˜ï¼Œå¦‚äººä½“å§¿æ€ä¼°è®¡ç­‰å®é™…åº”ç”¨åœºæ™¯ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºçº§è”å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆCMSAï¼‰çš„æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚è¯¥æœºåˆ¶é€‚ç”¨äºCNN-ViTæ··åˆæ¶æ„ï¼Œå¯æœ‰æ•ˆåœ°å¤„ç†ä½åˆ†è¾¨ç‡è¾“å…¥ã€‚CMSAçš„è®¾è®¡å®ç°äº†ä¸åŒå°ºåº¦ç‰¹å¾çš„æå–å’Œæ— ç¼é›†æˆï¼Œæ— éœ€å¯¹è¾“å…¥å›¾åƒæˆ–ç‰¹å¾å›¾è¿›è¡Œé™é‡‡æ ·ã€‚é€šè¿‡åˆ†ç»„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸åŸºäºçª—å£çš„å±€éƒ¨æ³¨æ„åŠ›çš„æ–°é¢–ç»“åˆï¼Œä»¥åŠä¸åŒå°ºåº¦ä¸Šå¤šå°ºåº¦ç‰¹å¾çš„çº§è”èåˆï¼Œè¯¥æ¶æ„å®ç°äº†è·¨å°ºåº¦ç‰¹å¾çš„æœ‰æ•ˆå¤„ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººä½“å§¿æ€ä¼°è®¡ã€å¤´éƒ¨å§¿æ€ä¼°è®¡ç­‰é¢†åŸŸä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”å‚æ•°æ›´å°‘ï¼Œå±•ç¤ºå‡ºäº†åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæ•è·ä¸å¯è¡Œçš„ç°å®åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºçº§è”å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆCMSAï¼‰çš„æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå¤„ç†å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­çš„ä½åˆ†è¾¨ç‡è¾“å…¥é—®é¢˜ã€‚</li>
<li>CMSAé€‚ç”¨äºCNN-ViTæ··åˆæ¶æ„ï¼Œå¯å®ç°åœ¨ä¸åŒå°ºåº¦ä¸Šç‰¹å¾çš„æå–å’Œæ— ç¼é›†æˆã€‚</li>
<li>CMSAè®¾è®¡é€šè¿‡ç»“åˆåˆ†ç»„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒåŸºäºçª—å£çš„å±€éƒ¨æ³¨æ„åŠ›ï¼Œå®ç°è·¨å°ºåº¦ç‰¹å¾çš„æœ‰æ•ˆå¤„ç†ã€‚</li>
<li>æå‡ºçš„CMSAæ¶æ„åœ¨äººä½“å§¿æ€ä¼°è®¡ã€å¤´éƒ¨å§¿æ€ä¼°è®¡ç­‰é¢†åŸŸä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”å‚æ•°æ›´å°‘ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæ•è·ä¸å¯è¡Œçš„ç°å®åœºæ™¯ä¸­ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„ä»£ç èµ„æºä¸ºç ”ç©¶è€…æä¾›äº†ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d44f3b4b9451d2c31a92e30d3a6eff9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5baea04ee868e730896e50ec808ba3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63d7eccebf11ea8c28683e6aafcb1a40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f120cf3bfb6c1d84961c4fa86b085f95.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Tackling-the-Abstraction-and-Reasoning-Corpus-with-Vision-Transformers-the-Importance-of-2D-Representation-Positions-and-Objects"><a href="#Tackling-the-Abstraction-and-Reasoning-Corpus-with-Vision-Transformers-the-Importance-of-2D-Representation-Positions-and-Objects" class="headerlink" title="Tackling the Abstraction and Reasoning Corpus with Vision Transformers:   the Importance of 2D Representation, Positions, and Objects"></a>Tackling the Abstraction and Reasoning Corpus with Vision Transformers:   the Importance of 2D Representation, Positions, and Objects</h2><p><strong>Authors:Wenhao Li, Yudong Xu, Scott Sanner, Elias Boutros Khalil</strong></p>
<p>The Abstraction and Reasoning Corpus (ARC) is a popular benchmark focused on visual reasoning in the evaluation of Artificial Intelligence systems. In its original framing, an ARC task requires solving a program synthesis problem over small 2D images using a few input-output training pairs. In this work, we adopt the recently popular data-driven approach to the ARC and ask whether a Vision Transformer (ViT) can learn the implicit mapping, from input image to output image, that underlies the task. We show that a ViT â€“ otherwise a state-of-the-art model for images â€“ fails dramatically on most ARC tasks even when trained on one million examples per task. This points to an inherent representational deficiency of the ViT architecture that makes it incapable of uncovering the simple structured mappings underlying the ARC tasks. Building on these insights, we propose ViTARC, a ViT-style architecture that unlocks some of the visual reasoning capabilities required by the ARC. Specifically, we use a pixel-level input representation, design a spatially-aware tokenization scheme, and introduce a novel object-based positional encoding that leverages automatic segmentation, among other enhancements. Our task-specific ViTARC models achieve a test solve rate close to 100% on more than half of the 400 public ARC tasks strictly through supervised learning from input-output grids. This calls attention to the importance of imbuing the powerful (Vision) Transformer with the correct inductive biases for abstract visual reasoning that are critical even when the training data is plentiful and the mapping is noise-free. Hence, ViTARC provides a strong foundation for future research in visual reasoning using transformer-based architectures. </p>
<blockquote>
<p>æŠ½è±¡ä¸æ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰æ˜¯äººå·¥æ™ºèƒ½ç³»ç»Ÿè¯„ä¼°ä¸­ä¸“æ³¨äºè§†è§‰æ¨ç†çš„æµè¡ŒåŸºå‡†æµ‹è¯•ã€‚åœ¨åŸå§‹è®¾å®šä¸­ï¼ŒARCä»»åŠ¡éœ€è¦åœ¨å°å‹2Då›¾åƒä¸Šè§£å†³ä¸€ä¸ªç¨‹åºåˆæˆé—®é¢˜ï¼Œä½¿ç”¨å°‘é‡çš„è¾“å…¥-è¾“å‡ºè®­ç»ƒå¯¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€è¿‘æµè¡Œçš„æ•°æ®é©±åŠ¨æ–¹æ³•æ¥å¤„ç†ARCï¼Œå¹¶æ¢è®¨Vision Transformerï¼ˆViTï¼‰æ˜¯å¦èƒ½å¤Ÿå­¦ä¹ ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºå›¾åƒä¹‹é—´çš„éšå«æ˜ å°„ï¼Œè¿™æ˜¯ä»»åŠ¡çš„åŸºç¡€ã€‚æˆ‘ä»¬å±•ç¤ºäº†ViTâ€”â€”ä¸€ç§ç”¨äºå›¾åƒå¤„ç†çš„æœ€æ–°æ¨¡å‹â€”â€”å³ä½¿åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šæ¥å—ä¸€ç™¾ä¸‡ä¸ªæ ·æœ¬çš„è®­ç»ƒï¼Œä¹Ÿå¤§å¤šä¼šåœ¨ARCä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚è¿™è¡¨æ˜ViTæ¶æ„ä¸­å­˜åœ¨å›ºæœ‰çš„ä»£è¡¨æ€§ç¼ºé™·ï¼Œä½¿å…¶æ— æ³•å‘ç°ARCä»»åŠ¡èƒŒåç®€å•çš„ç»“æ„åŒ–æ˜ å°„ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ViTARCï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ViTé£æ ¼çš„æ¶æ„ï¼Œè§£é”äº†ARCæ‰€éœ€çš„ä¸€äº›è§†è§‰æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åƒç´ çº§çš„è¾“å…¥è¡¨ç¤ºï¼Œè®¾è®¡äº†ä¸€ç§ç©ºé—´æ„ŸçŸ¥çš„æ ‡è®°åŒ–æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºå¯¹è±¡çš„ä½ç½®ç¼–ç ï¼Œåˆ©ç”¨è‡ªåŠ¨åˆ†å‰²ç­‰æŠ€æœ¯è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬çš„é’ˆå¯¹ä»»åŠ¡çš„ViTARCæ¨¡å‹é€šè¿‡ç›‘ç£å­¦ä¹ ä»è¾“å…¥-è¾“å‡ºç½‘æ ¼ä¸­è¿›è¡Œè®­ç»ƒï¼Œåœ¨è¶…è¿‡ä¸€åŠçš„400ä¸ªå…¬å…±ARCä»»åŠ¡ä¸Šçš„æµ‹è¯•è§£å†³ç‡æ¥è¿‘ç™¾åˆ†ä¹‹ç™¾ã€‚è¿™å†æ¬¡å¼ºè°ƒäº†èµ‹äºˆå¼ºå¤§çš„ï¼ˆè§†è§‰ï¼‰Transformeræ­£ç¡®çš„å½’çº³åè§è¿›è¡ŒæŠ½è±¡è§†è§‰æ¨ç†çš„é‡è¦æ€§ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®å……è¶³ä¸”æ˜ å°„æ— å™ªå£°çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼ŒViTARCä¸ºæœªæ¥åŸºäºTransformeræ¶æ„çš„è§†è§‰æ¨ç†ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06405v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨Vision Transformerï¼ˆViTï¼‰åœ¨æŠ½è±¡å’Œæ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è™½ç„¶ViTåœ¨å›¾åƒé¢†åŸŸæ˜¯å…ˆè¿›æ¨¡å‹ï¼Œä½†åœ¨ARCä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶å†…åœ¨è¡¨å¾çš„ç¼ºé™·ã€‚å› æ­¤ï¼Œæå‡ºäº†ViTARCæ¶æ„ï¼Œé€šè¿‡åƒç´ çº§è¾“å…¥è¡¨ç¤ºã€ç©ºé—´æ„ŸçŸ¥çš„ä»¤ç‰ŒåŒ–æ–¹æ¡ˆã€åŸºäºå¯¹è±¡çš„å®šä½ç¼–ç ç­‰å¢å¼ºåŠŸèƒ½ï¼Œè§£é”äº†ViTæ‰€éœ€çš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚åœ¨è¶…è¿‡ä¸€åŠçš„å…¬å…±ARCä»»åŠ¡ä¸Šï¼ŒViTARCæ¨¡å‹é€šè¿‡ç›‘ç£å­¦ä¹ å®ç°äº†è¿‘100%çš„æµ‹è¯•è§£å†³ç‡ã€‚è¿™å¼ºè°ƒäº†ä¸ºå¼ºå¤§çš„Vision Transformeræ³¨å…¥æ­£ç¡®å½’çº³åç½®çš„é‡è¦æ€§ï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®å……è¶³ã€æ˜ å°„æ— å™ªå£°æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼ŒViTARCä¸ºä½¿ç”¨åŸºäºè½¬æ¢å™¨çš„æ¶æ„è¿›è¡Œè§†è§‰æ¨ç†çš„æœªæ¥ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARCæ˜¯ä¸“æ³¨äºè§†è§‰æ¨ç†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿè¯„ä¼°åŸºå‡†ã€‚</li>
<li>åŸå§‹çš„ARCä»»åŠ¡éœ€è¦è§£å†³åŸºäºå°2Då›¾åƒçš„ç¨‹åºåˆæˆé—®é¢˜ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰åœ¨ARCä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œå³ä½¿å¯¹æ¯ä¸ªä»»åŠ¡è®­ç»ƒäº†ä¸€ç™¾ä¸‡ä¸ªä¾‹å­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™è¡¨æ˜ViTæ¶æ„å­˜åœ¨å†…åœ¨è¡¨å¾ç¼ºé™·ã€‚</li>
<li>ViTARCæ¶æ„æ˜¯ä¸ºäº†è§£å†³ViTåœ¨ARCä»»åŠ¡ä¸Šçš„ç¼ºé™·è€Œæå‡ºçš„ï¼ŒåŒ…æ‹¬åƒç´ çº§è¾“å…¥è¡¨ç¤ºã€ç©ºé—´æ„ŸçŸ¥çš„ä»¤ç‰ŒåŒ–æ–¹æ¡ˆå’ŒåŸºäºå¯¹è±¡çš„å®šä½ç¼–ç ç­‰å¢å¼ºåŠŸèƒ½ã€‚</li>
<li>ViTARCæ¨¡å‹åœ¨è¶…è¿‡ä¸€åŠçš„å…¬å…±ARCä»»åŠ¡ä¸Šé€šè¿‡ç›‘ç£å­¦ä¹ å®ç°äº†è¿‘100%çš„æµ‹è¯•è§£å†³ç‡ã€‚</li>
<li>æ­£ç¡®çš„å½’çº³åç½®å¯¹äºæŠ½è±¡è§†è§‰æ¨ç†è‡³å…³é‡è¦ï¼Œå³ä½¿è®­ç»ƒæ•°æ®å……è¶³ä¸”æ˜ å°„æ— å™ªå£°ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f03070e0a0210fea73083302953f2083.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f16ea01deda983de110a6414a0d1a2ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b64d6d4d3498badac1ab72f731069daa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2eb68235aaecf980813a3a52af250da3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DARE-Diverse-Visual-Question-Answering-with-Robustness-Evaluation"><a href="#DARE-Diverse-Visual-Question-Answering-with-Robustness-Evaluation" class="headerlink" title="DARE: Diverse Visual Question Answering with Robustness Evaluation"></a>DARE: Diverse Visual Question Answering with Robustness Evaluation</h2><p><strong>Authors:Hannah Sterz, Jonas Pfeiffer, Ivan VuliÄ‡</strong></p>
<p>Vision Language Models (VLMs) extend remarkable capabilities of text-only large language models and vision-only models, and are able to learn from and process multi-modal vision-text input. While modern VLMs perform well on a number of standard image classification and image-text matching tasks, they still struggle with a number of crucial vision-language (VL) reasoning abilities such as counting and spatial reasoning. Moreover, while they might be very brittle to small variations in instructions and&#x2F;or evaluation protocols, existing benchmarks fail to evaluate their robustness (or rather the lack of it). In order to couple challenging VL scenarios with comprehensive robustness evaluation, we introduce DARE, Diverse Visual Question Answering with Robustness Evaluation, a carefully created and curated multiple-choice VQA benchmark. DARE evaluates VLM performance on five diverse categories and includes four robustness-oriented evaluations based on the variations of: prompts, the subsets of answer options, the output format and the number of correct answers. Among a spectrum of other findings, we report that state-of-the-art VLMs still struggle with questions in most categories and are unable to consistently deliver their peak performance across the tested robustness evaluations. The worst case performance across the subsets of options is up to 34% below the performance in the standard case. The robustness of the open-source VLMs such as LLaVA 1.6 and Idefics2 cannot match the closed-source models such as GPT-4 and Gemini, but even the latter remain very brittle to different variations. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ‰©å±•äº†çº¯æ–‡æœ¬å¤§å‹è¯­è¨€æ¨¡å‹å’Œçº¯è§†è§‰æ¨¡å‹çš„æ˜¾è‘—èƒ½åŠ›ï¼Œå¹¶èƒ½å¤Ÿä»å¹¶å¤„ç†å¤šæ¨¡æ€è§†è§‰æ–‡æœ¬è¾“å…¥ä¸­å­¦ä¹ ã€‚è™½ç„¶ç°ä»£VLMsåœ¨è®¸å¤šæ ‡å‡†å›¾åƒåˆ†ç±»å’Œå›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥æŒæ¡è®¸å¤šå…³é”®è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚è®¡æ•°å’Œç©ºé—´æ¨ç†ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯èƒ½å¯¹æŒ‡ä»¤å’Œ&#x2F;æˆ–è¯„ä¼°åè®®çš„å¾®å°å˜åŒ–éå¸¸æ•æ„Ÿï¼Œè€Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½è¯„ä¼°å®ƒä»¬çš„ç¨³å¥æ€§ï¼ˆæˆ–ç¼ºä¹ç¨³å¥æ€§ï¼‰ã€‚ä¸ºäº†å°†å…·æœ‰æŒ‘æˆ˜æ€§çš„VLåœºæ™¯ä¸å…¨é¢çš„ç¨³å¥æ€§è¯„ä¼°ç›¸ç»“åˆï¼Œæˆ‘ä»¬å¼•å…¥äº†DAREï¼ˆå…¼å…·ç¨³å¥æ€§è¯„ä»·çš„å¤šæ ·è§†è§‰é—®ç­”ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒåˆ›å»ºå’Œç­–åˆ’çš„å¤šé¡¹é€‰æ‹©é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ã€‚DAREè¯„ä¼°äº†VLMåœ¨äº”ä¸ªä¸åŒç±»åˆ«ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬åŸºäºä»¥ä¸‹å››ä¸ªæ–¹é¢çš„ç¨³å¥æ€§è¯„ä»·ï¼šæç¤ºã€ç­”æ¡ˆé€‰é¡¹çš„å­é›†ã€è¾“å‡ºæ ¼å¼å’Œæ­£ç¡®ç­”æ¡ˆçš„æ•°é‡ã€‚åœ¨å…¶ä»–ä¸€ç³»åˆ—å‘ç°ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šè¯´ï¼Œæœ€å…ˆè¿›çš„VLMåœ¨å¤§å¤šæ•°ç±»åˆ«çš„é—®é¢˜ä¸­ä»ç„¶é¢ä¸´å›°éš¾ï¼Œå¹¶ä¸”æ— æ³•åœ¨æµ‹è¯•è¿‡çš„ç¨³å¥æ€§è¯„ä¼°ä¸­æŒç»­å‘æŒ¥æœ€ä½³æ€§èƒ½ã€‚é€‰é¡¹å­é›†ä¸­çš„æœ€åæƒ…å†µæ€§èƒ½æ¯”æ ‡å‡†æƒ…å†µä¸‹çš„æ€§èƒ½ä½è¾¾34%ã€‚å¼€æºVLMï¼ˆå¦‚LLaVA 1.6å’ŒIdefics2ï¼‰çš„ç¨³å¥æ€§æ— æ³•ä¸é—­æºæ¨¡å‹ï¼ˆå¦‚GPT-4å’ŒGeminiï¼‰ç›¸åŒ¹æ•Œï¼Œä½†å³ä½¿æ˜¯åè€…ä»å¯¹ä¸åŒå˜åŒ–éå¸¸æ•æ„Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18023v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç»“åˆäº†æ–‡æœ¬æ¨¡å‹å’Œè§†è§‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œèƒ½å¤„ç†å¤šæ¨¡æ€çš„å›¾æ–‡è¾“å…¥ã€‚è™½ç„¶ç°ä»£VLMsåœ¨è®¸å¤šæ ‡å‡†çš„å›¾åƒåˆ†ç±»å’Œå›¾æ–‡åŒ¹é…ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸€äº›å…³é”®çš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›å¦‚è®¡æ•°å’Œç©ºé—´æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è¯„ä¼°å…¶åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DAREåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šå…ƒåŒ–çš„è§†è§‰é—®ç­”è¯„ä¼°ä½“ç³»ï¼Œæ—¨åœ¨è¯„ä¼°VLMåœ¨äº”ä¸ªä¸åŒç±»åˆ«çš„æ€§èƒ½ï¼Œå¹¶åŸºäºå››ç§ç¨³å¥æ€§è¯„ä»·æ–¹æ³•å¯¹å…¶è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨å¤šå…ƒåŒ–çš„ç±»åˆ«ä¸­ï¼Œæœ€å…ˆè¿›çš„VLMä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶ä¸”åœ¨æµ‹è¯•ç¨³å¥æ€§è¯„ä»·ä¸­çš„è¡¨ç°ä¸ä¸€è‡´ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ€§èƒ½ç”šè‡³ä½äºæ ‡å‡†æƒ…å†µä¸‹é«˜è¾¾34%ã€‚å¼€æºVLMçš„ç¨³å¥æ€§ä¸å¦‚å°é—­æºæ¨¡å‹ï¼Œä½†å³ä½¿æ˜¯åè€…ï¼Œåœ¨ä¸åŒçš„å˜å¼‚é¢å‰ä¾ç„¶å¾ˆè„†å¼±ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VLMså±•ç°äº†å¤„ç†å¤šæ¨¡æ€å›¾æ–‡è¾“å…¥çš„èƒ½åŠ›ï¼Œä½†åœ¨è§†è§‰è¯­è¨€æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°ä»£VLMsåœ¨æ ‡å‡†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è®¡æ•°å’Œç©ºé—´æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†è¯„ä¼°VLMsåœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†DAREåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°VLMåœ¨äº”ä¸ªä¸åŒç±»åˆ«çš„æ€§èƒ½ä»¥åŠå››ç§ç¨³å¥æ€§è¯„ä»·æ–¹æ³•ä¸‹çš„è¡¨ç°ã€‚</li>
<li>æœ€å…ˆè¿›çš„VLMåœ¨DAREåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ç¨³å®šï¼Œåœ¨æŸäº›ç±»åˆ«å’Œç¨³å¥æ€§è¯„ä»·ä¸­çš„æ€§èƒ½ä¸‹é™æ˜¾è‘—ã€‚</li>
<li>å¼€æºVLMçš„ç¨³å¥æ€§ä½äºå°é—­æºæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c45b8d18945e3e71ff9d05c4e517b15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9433ad0973078d34698626e530b0681f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4081a7ac72ab80fc7cc7ac331dcc4762.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="V-RoAst-Visual-Road-Assessment-Can-VLM-be-a-Road-Safety-Assessor-Using-the-iRAP-Standard"><a href="#V-RoAst-Visual-Road-Assessment-Can-VLM-be-a-Road-Safety-Assessor-Using-the-iRAP-Standard" class="headerlink" title="V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using   the iRAP Standard?"></a>V-RoAst: Visual Road Assessment. Can VLM be a Road Safety Assessor Using   the iRAP Standard?</h2><p><strong>Authors:Natchapon Jongwiriyanurak, Zichao Zeng, June Moh Goo, Xinglei Wang, Ilya Ilyankou, Kerkritt Sriroongvikrai, Nicola Christie, Meihui Wang, Huanfa Chen, James Haworth</strong></p>
<p>Road safety assessments are critical yet costly, especially in Low- and Middle-Income Countries (LMICs), where most roads remain unrated. Traditional methods require expert annotation and training data, while supervised learning-based approaches struggle to generalise across regions. In this paper, we introduce \textit{V-RoAst}, a zero-shot Visual Question Answering (VQA) framework using Vision-Language Models (VLMs) to classify road safety attributes defined by the iRAP standard. We introduce the first open-source dataset from ThaiRAP, consisting of over 2,000 curated street-level images from Thailand annotated for this task. We evaluate Gemini-1.5-flash and GPT-4o-mini on this dataset and benchmark their performance against VGGNet and ResNet baselines. While VLMs underperform on spatial awareness, they generalise well to unseen classes and offer flexible prompt-based reasoning without retraining. Our results show that VLMs can serve as automatic road assessment tools when integrated with complementary data. This work is the first to explore VLMs for zero-shot infrastructure risk assessment and opens new directions for automatic, low-cost road safety mapping. Code and dataset: <a target="_blank" rel="noopener" href="https://github.com/PongNJ/V-RoAst">https://github.com/PongNJ/V-RoAst</a>. </p>
<blockquote>
<p>é“è·¯å®‰å…¨è¯„ä¼°è‡³å…³é‡è¦ï¼Œä½†æˆæœ¬é«˜æ˜‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­ä½æ”¶å…¥å›½å®¶ï¼ˆLMICsï¼‰ï¼Œè¿™äº›å›½å®¶çš„å¤šæ•°é“è·¯å°šæœªè¿›è¡Œè¯„ä¼°ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦ä¸“å®¶æ ‡æ³¨å’Œè®­ç»ƒæ•°æ®ï¼Œè€ŒåŸºäºç›‘ç£å­¦ä¹ çš„æ–¹æ³•åœ¨è·¨åŒºåŸŸæ¨å¹¿æ—¶é¢ä¸´å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é›¶æ ·æœ¬è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¡†æ¶V-RoAstï¼Œç”¨äºæ ¹æ®iRAPæ ‡å‡†å¯¹é“è·¯å®‰å…¨å±æ€§è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªå¼€æºæ•°æ®é›†ThaiRAPï¼Œè¯¥æ•°æ®é›†åŒ…å«æ³°å›½è¶…è¿‡2000å¼ ä¸ºæ­¤ä»»åŠ¡æ ‡æ³¨çš„è¡—é“çº§åˆ«å›¾åƒã€‚æˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šè¯„ä¼°äº†Gemini-1.5-flashå’ŒGPT-4o-miniçš„æ€§èƒ½ï¼Œå¹¶ä¸VGGNetå’ŒResNetåŸºçº¿è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚è™½ç„¶VLMåœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†å®ƒä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„ç±»åˆ«ï¼Œå¹¶æä¾›çµæ´»çš„åŸºäºæç¤ºçš„æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“ä¸è¡¥å……æ•°æ®ç»“åˆæ—¶ï¼ŒVLMå¯ä»¥ä½œä¸ºè‡ªåŠ¨é“è·¯è¯„ä¼°å·¥å…·ã€‚æœ¬æ–‡é¦–æ¬¡æ¢ç´¢äº†ç”¨äºé›¶æ ·æœ¬åŸºç¡€è®¾æ–½é£é™©è¯„ä¼°çš„VLMï¼Œä¸ºè‡ªåŠ¨ã€ä½æˆæœ¬çš„é“è·¯å®‰å…¨åœ°å›¾ç»˜åˆ¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚ä»£ç å’Œæ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/PongNJ/V-RoAst%E3%80%82">https://github.com/PongNJ/V-RoAstã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10872v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé›¶æ ·æœ¬è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¡†æ¶çš„V-RoAstæ¨¡å‹ï¼Œç”¨äºæ ¹æ®iRAPæ ‡å‡†å¯¹é“è·¯å®‰å…¨å±æ€§è¿›è¡Œåˆ†ç±»ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†æ¥è‡ªThaiRAPçš„é¦–ä¸ªå¼€æºæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2000å¼ é’ˆå¯¹æ­¤ä»»åŠ¡æ ‡æ³¨çš„æ³°å›½è¡—é“çº§åˆ«å›¾åƒã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶VLMåœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†å®ƒä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§ç±»åˆ«ï¼Œå¹¶æä¾›çµæ´»çš„åŸºäºæç¤ºçš„æ¨ç†è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨é“è·¯è¯„ä¼°å·¥å…·æä¾›äº†æ½œåŠ›ï¼Œå½“ä¸è¡¥å……æ•°æ®é›†æˆæ—¶æ•ˆæœæ›´ä½³ã€‚è¿™æ˜¯é¦–æ¬¡æ¢ç´¢å°†VLMç”¨äºé›¶æ ·æœ¬åŸºç¡€è®¾æ–½é£é™©è¯„ä¼°ï¼Œä¸ºè‡ªåŠ¨ã€ä½æˆæœ¬çš„é“è·¯å®‰å…¨æ˜ å°„å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>V-RoAstæ¨¡å‹åˆ©ç”¨é›¶æ ·æœ¬è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ¡†æ¶è¿›è¡Œé“è·¯å®‰å…¨è¯„ä¼°ï¼Œè¯¥æ¡†æ¶åŸºäºVision-Language Modelsï¼ˆVLMsï¼‰ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå¼€æºæ•°æ®é›†ï¼ŒåŒ…å«é’ˆå¯¹é“è·¯å®‰å…¨å±æ€§åˆ†ç±»ä»»åŠ¡æ ‡æ³¨çš„è¡—é“çº§åˆ«å›¾åƒã€‚</li>
<li>è¯„ä¼°äº†å¤šç§æ¨¡å‹ï¼ˆåŒ…æ‹¬Gemini-1.5-flashã€GPT-4o-miniã€VGGNetå’ŒResNetï¼‰åœ¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>VLMsè™½ç„¶åœ¨ç©ºé—´æ„ŸçŸ¥æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†åœ¨æœªè§ç±»åˆ«ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¼ºï¼Œå¹¶èƒ½æä¾›çµæ´»çš„åŸºäºæç¤ºçš„æ¨ç†ã€‚</li>
<li>VLMsä¸è¡¥å……æ•°æ®é›†æˆåå¯ä½œä¸ºè‡ªåŠ¨é“è·¯è¯„ä¼°å·¥å…·ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†VLMåº”ç”¨äºé›¶æ ·æœ¬åŸºç¡€è®¾æ–½é£é™©è¯„ä¼°çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8a86b5c3601de139e6492ac5d8cfc2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e523e29548cec2d34ad153b9a3dcf35c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9916132d73e23b4bd69cfe26b95f9514.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d3a7a962346421015d3c74d100bffb1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VLM-CPL-Consensus-Pseudo-Labels-from-Vision-Language-Models-for-Annotation-Free-Pathological-Image-Classification"><a href="#VLM-CPL-Consensus-Pseudo-Labels-from-Vision-Language-Models-for-Annotation-Free-Pathological-Image-Classification" class="headerlink" title="VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for   Annotation-Free Pathological Image Classification"></a>VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for   Annotation-Free Pathological Image Classification</h2><p><strong>Authors:Lanfeng Zhong, Zongyao Huang, Yang Liu, Wenjun Liao, Shichuan Zhang, Guotai Wang, Shaoting Zhang</strong></p>
<p>Classification of pathological images is the basis for automatic cancer diagnosis. Despite that deep learning methods have achieved remarkable performance, they heavily rely on labeled data, demanding extensive human annotation efforts. In this study, we present a novel human annotation-free method by leveraging pre-trained Vision-Language Models (VLMs). Without human annotation, pseudo-labels of the training set are obtained by utilizing the zero-shot inference capabilities of VLM, which may contain a lot of noise due to the domain gap between the pre-training and target datasets. To address this issue, we introduce VLM-CPL, a novel approach that contains two noisy label filtering techniques with a semi-supervised learning strategy. Specifically, we first obtain prompt-based pseudo-labels with uncertainty estimation by zero-shot inference with the VLM using multiple augmented views of an input. Then, by leveraging the feature representation ability of VLM, we obtain feature-based pseudo-labels via sample clustering in the feature space. Prompt-feature consensus is introduced to select reliable samples based on the consensus between the two types of pseudo-labels. We further propose High-confidence Cross Supervision by to learn from samples with reliable pseudo-labels and the remaining unlabeled samples. Additionally, we present an innovative open-set prompting strategy that filters irrelevant patches from whole slides to enhance the quality of selected patches. Experimental results on five public pathological image datasets for patch-level and slide-level classification showed that our method substantially outperformed zero-shot classification by VLMs, and was superior to existing noisy label learning methods. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HiLab-git/VLM-CPL">https://github.com/HiLab-git/VLM-CPL</a>. </p>
<blockquote>
<p>ç—…ç†å›¾åƒçš„è‡ªåŠ¨åˆ†ç±»æ˜¯ç™Œç—‡è¯Šæ–­çš„åŸºç¡€ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºæ ‡æ³¨æ•°æ®ï¼Œéœ€è¦å¤§é‡çš„äººåŠ›æ ‡æ³¨å·¥ä½œã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— æ ‡æ³¨äººåŠ›å‚ä¸çš„æ£€æµ‹æ–¹æ³•ï¼Œå®ƒè¿ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚æœ¬ç ”ç©¶é‡‡ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬æ¨ç†ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯è·å¾—è®­ç»ƒé›†çš„ä¼ªæ ‡ç­¾ï¼Œä½†ç”±äºé¢„è®­ç»ƒå’Œç›®æ ‡æ•°æ®é›†ä¹‹é—´çš„é¢†åŸŸå·®å¼‚ï¼Œè¿™äº›ä¼ªæ ‡ç­¾å¯èƒ½åŒ…å«å¤§é‡å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VLM-CPLè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒåŒ…å«ä¸¤ç§å¸¦æœ‰åŠç›‘ç£å­¦ä¹ ç­–ç•¥çš„å™ªå£°æ ‡ç­¾è¿‡æ»¤æŠ€æœ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨è¾“å…¥å›¾åƒå¤šä¸ªå¢å¼ºè§†å›¾åœ¨VLMä¸­è¿›è¡Œé›¶æ ·æœ¬æ¨ç†å¹¶åŸºäºä¸ç¡®å®šæ€§ä¼°è®¡æ¥è·å¾—åŸºäºæç¤ºçš„ä¼ªæ ‡ç­¾ã€‚ç„¶ååˆ©ç”¨VLMçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œé€šè¿‡æ ·æœ¬èšç±»åœ¨ç‰¹å¾ç©ºé—´ä¸­è·å¾—åŸºäºç‰¹å¾çš„ä¼ªæ ‡ç­¾ã€‚é€šè¿‡è¿™ä¸¤ç§ç±»å‹ä¼ªæ ‡ç­¾ä¹‹é—´çš„å…±è¯†å¼•å…¥æç¤ºç‰¹å¾å…±è¯†æ¥é€‰æ‹©å¯é çš„æ ·æœ¬ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é«˜ç½®ä¿¡åº¦äº¤å‰ç›‘ç£æ³•æ¥å­¦ä¹ å¯é ä¼ªæ ‡ç­¾æ ·æœ¬å’Œå‰©ä½™æœªæ ‡è®°æ ·æœ¬çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å¼€æ”¾é›†æç¤ºç­–ç•¥ï¼Œç”¨äºä»å…¨åˆ‡ç‰‡ä¸­è¿‡æ»¤æ‰æ— å…³çš„è¡¥ä¸ï¼Œä»¥æé«˜æ‰€é€‰è¡¥ä¸çš„è´¨é‡ã€‚åœ¨äº”ä¸ªå…¬å…±ç—…ç†å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè¡¥ä¸çº§åˆ«å’Œåˆ‡ç‰‡çº§åˆ«çš„åˆ†ç±»å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬åˆ†ç±»æ–¹æ³•ï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰çš„å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/VLM-CPL%E4%BE%9B%E6%9F%A5%E8%AF%BB">https://github.com/HiLab-git/VLM-CPLä¾›æŸ¥é˜…</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.15836v3">PDF</a> Accepted at TMI</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€äººå·¥æ ‡æ³¨çš„æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œç—…ç†å­¦å›¾åƒåˆ†ç±»ã€‚æ–¹æ³•åˆ©ç”¨VLMçš„é›¶æ ·æœ¬æ¨ç†èƒ½åŠ›è·å–è®­ç»ƒé›†çš„ä¼ªæ ‡ç­¾ï¼Œé€šè¿‡å¼•å…¥VLM-CPLæ–¹æ³•å’Œä¸¤ç§å™ªå£°æ ‡ç­¾è¿‡æ»¤æŠ€æœ¯ï¼Œç»“åˆåŠç›‘ç£å­¦ä¹ ç­–ç•¥è§£å†³ä¼ªæ ‡ç­¾å¯èƒ½å­˜åœ¨çš„å™ªå£°é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªå…¬å¼€ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨VLMçš„é›¶æ ·æœ¬åˆ†ç±»ä»¥åŠç°æœ‰å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å­¦å›¾åƒåˆ†ç±»æ˜¯è‡ªåŠ¨ç™Œç—‡è¯Šæ–­çš„åŸºç¡€ï¼Œä½†æ·±åº¦å­¦ä¹ æ–¹æ³•éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œéœ€è¦ç¹é‡çš„äººå·¥æ ‡æ³¨å·¥ä½œã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œäººç±»æ ‡æ³¨ä¹‹å¤–çš„æ–¹æ³•ï¼Œé€šè¿‡é›¶æ ·æœ¬æ¨ç†è·å–è®­ç»ƒé›†çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>ç”±äºé¢†åŸŸå·®å¼‚ï¼Œä¼ªæ ‡ç­¾å¯èƒ½åŒ…å«å™ªå£°ï¼Œå› æ­¤å¼•å…¥äº†VLM-CPLæ–¹æ³•å’Œä¸¤ç§å™ªå£°æ ‡ç­¾è¿‡æ»¤æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡å¤šç§å¢å¼ºè§†å›¾è·å¾—åŸºäºæç¤ºçš„ä¼ªæ ‡ç­¾ï¼Œå¹¶åˆ©ç”¨VLMçš„ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›åœ¨ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæ ·æœ¬èšç±»ï¼Œè·å¾—åŸºäºç‰¹å¾çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>é€šè¿‡æç¤ºç‰¹å¾å…±è¯†é€‰æ‹©å¯é çš„æ ·æœ¬ï¼Œå¹¶å¼•å…¥é«˜ç½®ä¿¡åº¦äº¤å‰ç›‘ç£æ¥å­¦ä¹ å…·æœ‰å¯é ä¼ªæ ‡ç­¾çš„æ ·æœ¬å’Œå‰©ä½™çš„æ— æ ‡ç­¾æ ·æœ¬ã€‚</li>
<li>æå‡ºä¸€ç§åˆ›æ–°çš„å¼€æ”¾é›†æç¤ºç­–ç•¥ï¼Œä»å…¨ç‰‡ä¸­è¿‡æ»¤å‡ºæ— å…³è¡¥ä¸ï¼Œæé«˜æ‰€é€‰è¡¥ä¸çš„è´¨é‡ã€‚</li>
<li>åœ¨äº”ä¸ªå…¬å¼€ç—…ç†å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬åˆ†ç±»çš„VLMsä»¥åŠç°æœ‰çš„å™ªå£°æ ‡ç­¾å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.15836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-48c77b6db988cda33d43a5df89faf61c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba5d6461215369ec929565648da81bd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5e4a84bef58a631b56285e014c70271.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5566be4d1e9b594cc6cfa31d767fe23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc419908ba5f5576643aa7c2a31f4ded.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Utility-of-the-Virtual-Imaging-Trials-Methodology-for-Objective-Characterization-of-AI-Systems-and-Training-Data"><a href="#The-Utility-of-the-Virtual-Imaging-Trials-Methodology-for-Objective-Characterization-of-AI-Systems-and-Training-Data" class="headerlink" title="The Utility of the Virtual Imaging Trials Methodology for Objective   Characterization of AI Systems and Training Data"></a>The Utility of the Virtual Imaging Trials Methodology for Objective   Characterization of AI Systems and Training Data</h2><p><strong>Authors:Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo</strong></p>
<p>Purpose: The credibility of Artificial Intelligence (AI) models for medical imaging continues to be a challenge, affected by the diversity of models, the data used to train the models, and applicability of their combination to produce reproducible results for new data. Approach: In this work we aimed to explore if the emerging Virtual Imaging Trials (VIT) methodologies can provide an objective resource to approach this challenge. The study was conducted for the case example of COVID-19 diagnosis using clinical and virtual computed tomography (CT) and chest radiography (CXR) processed with convolutional neural networks. Multiple AI models were developed and tested using 3D ResNet-like and 2D EfficientNetv2 architectures across diverse datasets. Results: The performance differences were evaluated in terms of the area under the curve (AUC) and the DeLong method for AUC confidence intervals. The models trained on the most diverse datasets showed the highest external testing performance, with AUC values ranging from 0.73-0.76 for CT and 0.70-0.73 for CXR. Internal testing yielded higher AUC values (0.77 -0.85 for CT and 0.77-1.0 for CXR), highlighting a substantial drop in performance during external validation, which underscores the importance of diverse and comprehensive training and testing data. Most notably, VIT approach provided objective assessment of the utility of diverse models and datasets while further providing insight into the influence of dataset characteristics, patient factors, and imaging physics on AI efficacy. Conclusions: The VIT approach can be used to enhance model transparency and reliability, offering nuanced insights into the factors driving AI performance and bridging the gap between experimental and clinical settings. </p>
<blockquote>
<p>ç›®çš„ï¼šåŒ»å­¦å½±åƒçš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼ˆAIæ¨¡å‹ï¼‰çš„å¯ä¿¡åº¦ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå—åˆ°æ¨¡å‹å¤šæ ·æ€§ã€ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®ä»¥åŠå®ƒä»¬ç»„åˆå¯¹æ–°æ•°æ®äº§ç”Ÿå¯é‡å¤ç»“æœçš„åº”ç”¨èƒ½åŠ›çš„å½±å“ã€‚æ–¹æ³•ï¼šåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¢ç´¢æ–°å…´çš„è™šæ‹Ÿæˆåƒè¯•éªŒï¼ˆVITï¼‰æ–¹æ³•æ˜¯å¦èƒ½ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜æä¾›å®¢è§‚èµ„æºã€‚ç ”ç©¶æ˜¯é’ˆå¯¹æ–°å† è‚ºç‚è¯Šæ–­çš„æ¡ˆä¾‹åˆ†æï¼Œä½¿ç”¨ä¸´åºŠå’Œè™šæ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä»¥åŠç»è¿‡å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰ã€‚ä½¿ç”¨3D ResNetå’Œ2D EfficientNetv2æ¶æ„å¼€å‘å¹¶æµ‹è¯•äº†å¤šä¸ªAIæ¨¡å‹ï¼Œè·¨è¶Šå¤šä¸ªæ•°æ®é›†ã€‚ç»“æœï¼šæ ¹æ®æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰å’Œç”¨äºAUCç½®ä¿¡åŒºé—´çš„DeLongæ–¹æ³•è¯„ä¼°æ€§èƒ½å·®å¼‚ã€‚åœ¨æœ€å…·å¤šæ ·æ€§çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹è¡¨ç°å‡ºæœ€é«˜çš„å¤–éƒ¨æµ‹è¯•æ€§èƒ½ï¼ŒCTçš„AUCå€¼èŒƒå›´ä¸º0.73-0.76ï¼ŒCXRçš„AUCå€¼ä¸º0.70-0.73ã€‚å†…éƒ¨æµ‹è¯•å¾—å‡ºçš„AUCå€¼è¾ƒé«˜ï¼ˆCTä¸º0.77-0.85ï¼ŒCXRä¸º0.77-1.0ï¼‰ï¼Œè¿™çªæ˜¾äº†åœ¨å¤–éƒ¨éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°æ€§èƒ½å¤§å¹…ä¸‹é™çš„æƒ…å†µï¼Œä»è€Œå¼ºè°ƒå¤šæ ·åŒ–å’Œå…¨é¢çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„é‡è¦æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVITæ–¹æ³•å®¢è§‚åœ°è¯„ä¼°äº†ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†çš„ä½œç”¨ï¼ŒåŒæ—¶æä¾›äº†å…³äºæ•°æ®é›†ç‰¹å¾ã€æ‚£è€…å› ç´ å’Œæˆåƒç‰©ç†å¯¹AIæ•ˆæœå½±å“çš„ä¿¡æ¯ã€‚ç»“è®ºï¼šVITæ–¹æ³•å¯ç”¨äºæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ï¼Œæä¾›å…³äºé©±åŠ¨AIæ€§èƒ½çš„å› ç´ çš„å¾®å¦™è§è§£ï¼Œå¹¶å¼¥åˆäº†å®éªŒç¯å¢ƒå’Œä¸´åºŠç¯å¢ƒä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09730v5">PDF</a> 8 figures, 4 Tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>AIæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„å¯ä¿¡åº¦ä¾ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜æ¥æºäºæ¨¡å‹çš„å¤šæ ·æ€§ã€ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®ä»¥åŠè¿™äº›æ¨¡å‹ç»„åˆå¯¹æ–°æ•°æ®äº§ç”Ÿå¯é‡å¤ç»“æœçš„åº”ç”¨æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢æ–°å…´çš„è™šæ‹Ÿæˆåƒè¯•éªŒï¼ˆVITï¼‰æ–¹æ³•æ˜¯å¦èƒ½ä¸ºè¿™ä¸€æŒ‘æˆ˜æä¾›å®¢è§‚èµ„æºã€‚ä»¥COVID-19è¯Šæ–­ä¸ºä¾‹ï¼Œç ”ç©¶ä½¿ç”¨äº†ä¸´åºŠå’Œè™šæ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä»¥åŠèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å¤„ç†çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚ä½¿ç”¨3D ResNet-likeå’Œ2D EfficientNetv2æ¶æ„å¼€å‘å¹¶æµ‹è¯•äº†å¤šä¸ªAIæ¨¡å‹ï¼Œåº”ç”¨äºä¸åŒçš„æ•°æ®é›†ã€‚åœ¨æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰å’ŒAUCç½®ä¿¡åŒºé—´çš„DeLongæ–¹æ³•æ–¹é¢è¯„ä¼°äº†æ€§èƒ½å·®å¼‚ã€‚åœ¨æœ€å…·å¤šæ ·æ€§çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¤–éƒ¨æµ‹è¯•ä¸­çš„è¡¨ç°æœ€ä½³ï¼ŒCTçš„AUCå€¼ä¸º0.73-0.76ï¼ŒCXRçš„AUCå€¼ä¸º0.70-0.73ã€‚å†…éƒ¨æµ‹è¯•çš„AUCå€¼è¾ƒé«˜ï¼ˆCTä¸º0.77-0.85ï¼ŒCXRä¸º0.77-1.0ï¼‰ï¼Œçªæ˜¾äº†åœ¨å¤–éƒ¨éªŒè¯æ—¶æ€§èƒ½å¤§å¹…ä¸‹é™çš„ç°è±¡ï¼Œè¿™çªæ˜¾äº†å¤šæ ·æ€§å’Œç»¼åˆæ€§è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„é‡è¦æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVITæ–¹æ³•ä¸ºä¸åŒæ¨¡å‹çš„å®ç”¨æ€§å’Œæ•°æ®é›†æä¾›äº†å®¢è§‚è¯„ä¼°ï¼ŒåŒæ—¶è¿›ä¸€æ­¥æ·±å…¥äº†è§£äº†æ•°æ®é›†ç‰¹æ€§ã€æ‚£è€…å› ç´ å’Œæˆåƒç‰©ç†å­¦å¯¹AIæ•ˆç‡çš„å½±å“ã€‚ç»“è®ºï¼šVITæ–¹æ³•å¯ç”¨äºæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ï¼Œæä¾›å¯¹é©±åŠ¨AIæ€§èƒ½å› ç´ çš„æ·±åˆ»è§è§£ï¼Œå¹¶å¼¥åˆäº†å®éªŒå’Œä¸´åºŠç¯å¢ƒä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AIæ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„å¯ä¿¡åº¦å—æ¨¡å‹å¤šæ ·æ€§ã€è®­ç»ƒæ•°æ®å’Œæ¨¡å‹ç»„åˆåº”ç”¨çš„å½±å“ã€‚</li>
<li>Virtual Imaging Trials (VIT)æ–¹æ³•ä¸ºè¯„ä¼°AIæ¨¡å‹æä¾›äº†å®¢è§‚èµ„æºã€‚</li>
<li>åœ¨COVID-19è¯Šæ–­çš„æ¡ˆä¾‹ä¸­ï¼Œä½¿ç”¨äº†ä¸´åºŠå’Œè™šæ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä»¥åŠèƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰å¤„ç†çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚</li>
<li>å¤šæ ·æ€§å’Œç»¼åˆæ€§çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®å¯¹AIæ¨¡å‹çš„è¡¨ç°è‡³å…³é‡è¦ã€‚</li>
<li>VITæ–¹æ³•æä¾›äº†å¯¹ä¸åŒæ¨¡å‹å’Œå®ç”¨æ€§çš„å®¢è§‚è¯„ä¼°ï¼ŒåŒæ—¶æ·±å…¥äº†è§£æ•°æ®é›†ç‰¹æ€§ã€æ‚£è€…å› ç´ å’Œæˆåƒç‰©ç†å­¦å¯¹AIæ•ˆç‡çš„å½±å“ã€‚</li>
<li>VITæ–¹æ³•èƒ½æé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5962c8fda6149724fb2e59eed99cdbe7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dcb1c87dbde6833faf9ce04298537eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1461d6f4313960f19a7873d57d8c4dec.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-433052a0433b464385ce31e5b34be465.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  MyGO Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9288ed48417716f6b40a6abd40c993b6.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  VideoMind An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31373.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
