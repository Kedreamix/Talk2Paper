<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-08-03  MyGO Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-433052a0433b464385ce31e5b34be465.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    81 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-03-更新"><a href="#2025-08-03-更新" class="headerlink" title="2025-08-03 更新"></a>2025-08-03 更新</h1><h2 id="MyGO-Make-your-Goals-Obvious-Avoiding-Semantic-Confusion-in-Prostate-Cancer-Lesion-Region-Segmentation"><a href="#MyGO-Make-your-Goals-Obvious-Avoiding-Semantic-Confusion-in-Prostate-Cancer-Lesion-Region-Segmentation" class="headerlink" title="MyGO: Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation"></a>MyGO: Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation</h2><p><strong>Authors:Zhengcheng Lin, Zuobin Ying, Zhenyu Li, Zhenyu Liu, Jian Lu, Weiping Ding</strong></p>
<p>Early diagnosis and accurate identification of lesion location and progression in prostate cancer (PCa) are critical for assisting clinicians in formulating effective treatment strategies. However, due to the high semantic homogeneity between lesion and non-lesion areas, existing medical image segmentation methods often struggle to accurately comprehend lesion semantics, resulting in the problem of semantic confusion. To address this challenge, we propose a novel Pixel Anchor Module, which guides the model to discover a sparse set of feature anchors that serve to capture and interpret global contextual information. This mechanism enhances the model’s nonlinear representation capacity and improves segmentation accuracy within lesion regions. Moreover, we design a self-attention-based Top_k selection strategy to further refine the identification of these feature anchors, and incorporate a focal loss function to mitigate class imbalance, thereby facilitating more precise semantic interpretation across diverse regions. Our method achieves state-of-the-art performance on the PI-CAI dataset, demonstrating 69.73% IoU and 74.32% Dice scores, and significantly improving prostate cancer lesion detection. </p>
<blockquote>
<p>早期诊断以及准确识别前列腺癌（PCa）的病变位置和进展对于帮助临床医生制定有效的治疗策略至关重要。然而，由于病变区域与非病变区域之间存在高度的语义同质性，现有的医学图像分割方法往往难以准确理解病变语义，从而导致语义混淆的问题。为了应对这一挑战，我们提出了一种新型的Pixel Anchor Module，该模块引导模型发现一组稀疏的特征锚点，用于捕获和解释全局上下文信息。这种机制增强了模型的非线性表示能力，提高了病变区域内的分割精度。此外，我们还设计了一种基于自注意力的Top_k选择策略，以进一步改进这些特征锚点的识别，并引入了一种焦点损失函数来缓解类别不平衡问题，从而在不同区域实现更精确的语义解释。我们的方法在PI-CAI数据集上实现了最佳性能，达到了69.73%的IoU和74.32%的Dice系数，显著提高了前列腺癌病变的检测效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17269v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Pixel Anchor Module的新方法，用于提高前列腺癌病变检测准确性。该方法通过发现稀疏的特征锚点来捕捉全局上下文信息，增强模型的非线性表示能力，从而提高病变区域的分割精度。此外，还设计了基于自注意力的Top_k选择策略来优化特征锚点的识别，并结合focal loss函数缓解类别不平衡问题，从而实现更精确的区域语义解释。该方法在PI-CAI数据集上取得了领先水平，达到69.73%的IoU和74.32%的Dice得分。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pixel Anchor Module被提出用于解决前列腺癌病变检测中的语义混淆问题。</li>
<li>通过发现稀疏的特征锚点来捕捉全局上下文信息，增强模型的表示能力。</li>
<li>设计的Top_k选择策略能优化特征锚点的识别。</li>
<li>结合focal loss函数来缓解类别不平衡问题。</li>
<li>方法在PI-CAI数据集上实现了先进的性能。</li>
<li>达到69.73%的IoU和74.32%的Dice得分。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e56733667d851890e8c75d544fb75d9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba402605984cb8d11d92f35395e64c07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2da6d44bf828dc8d4128ee8006a4ebbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70708e59a78ba0e78512ea81056260de.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ScSAM-Debiasing-Morphology-and-Distributional-Variability-in-Subcellular-Semantic-Segmentation"><a href="#ScSAM-Debiasing-Morphology-and-Distributional-Variability-in-Subcellular-Semantic-Segmentation" class="headerlink" title="ScSAM: Debiasing Morphology and Distributional Variability in   Subcellular Semantic Segmentation"></a>ScSAM: Debiasing Morphology and Distributional Variability in   Subcellular Semantic Segmentation</h2><p><strong>Authors:Bo Fang, Jianan Fan, Dongnan Liu, Hang Chang, Gerald J. Shami, Filip Braet, Weidong Cai</strong></p>
<p>The significant morphological and distributional variability among subcellular components poses a long-standing challenge for learning-based organelle segmentation models, significantly increasing the risk of biased feature learning. Existing methods often rely on single mapping relationships, overlooking feature diversity and thereby inducing biased training. Although the Segment Anything Model (SAM) provides rich feature representations, its application to subcellular scenarios is hindered by two key challenges: (1) The variability in subcellular morphology and distribution creates gaps in the label space, leading the model to learn spurious or biased features. (2) SAM focuses on global contextual understanding and often ignores fine-grained spatial details, making it challenging to capture subtle structural alterations and cope with skewed data distributions. To address these challenges, we introduce ScSAM, a method that enhances feature robustness by fusing pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge to alleviate training bias from data imbalance. Specifically, we design a feature alignment and fusion module to align pre-trained embeddings to the same feature space and efficiently combine different representations. Moreover, we present a cosine similarity matrix-based class prompt encoder to activate class-specific features to recognize subcellular categories. Extensive experiments on diverse subcellular image datasets demonstrate that ScSAM outperforms state-of-the-art methods. </p>
<blockquote>
<p>细胞亚组分之间显著的形态和分布变化给基于学习的细胞器分割模型带来了长期挑战，显著增加了特征学习偏向的风险。现有方法往往依赖于单一的映射关系，忽视了特征的多样性，从而导致训练偏向。尽管Segment Anything Model（SAM）提供了丰富的特征表示，但其在亚细胞场景中的应用受到两个关键挑战的制约：（1）亚细胞形态和分布的多样性导致标签空间中存在间隙，使模型学习特征时出现错误或偏向。（2）SAM侧重于全局上下文理解，往往忽视精细的空间细节，这使得捕捉微妙的结构变化和应对数据分布不均的情况具有挑战性。为了解决这些挑战，我们引入了ScSAM方法，它通过融合预训练的SAM和由Masked Autoencoder（MAE）引导的细胞先验知识，增强特征稳健性，以减轻因数据不平衡导致的训练偏差。具体来说，我们设计了一个特征对齐和融合模块，将预训练嵌入对齐到同一特征空间，并有效地结合不同的表示。此外，我们提出了一种基于余弦相似度矩阵的类提示编码器，以激活特定类别的特征来识别亚细胞类别。在多种亚细胞图像数据集上的广泛实验表明，ScSAM优于现有最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17149v1">PDF</a> Accepted by 28th European Conference on Artificial Intelligence   (ECAI)</p>
<p><strong>Summary</strong></p>
<p>本文指出了亚细胞结构形态和分布多样性的挑战，对基于学习的细胞器分割模型造成了影响。现有方法常常依赖单一映射关系，忽略了特征多样性，导致训练偏差。为解决这些问题，本文提出了ScSAM方法，通过融合预训练的Segment Anything Model（SAM）和Masked Autoencoder（MAE）引导的细胞先验知识，增强特征稳健性，以减轻因数据不平衡导致的训练偏差。实验证明，ScSAM在多种亚细胞图像数据集上的表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>亚细胞结构的形态和分布多样性对基于学习的分割模型构成挑战。</li>
<li>现有方法因依赖单一映射关系而忽视特征多样性，易导致训练偏差。</li>
<li>Segment Anything Model（SAM）虽提供丰富的特征表示，但在亚细胞场景下应用受限。</li>
<li>ScSAM方法通过融合预训练的SAM和MAE引导的细胞先验知识，增强特征稳健性。</li>
<li>ScSAM设计特征对齐和融合模块，将预训练嵌入对齐到同一特征空间并有效结合不同表示。</li>
<li>ScSAM采用余弦相似度矩阵的类提示编码器，激活类特定特征以识别亚细胞类别。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6e133e37635680fcd1dcfded9c9a19d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ddaa4dfae9860e61717fdaf1974fd7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8895a281ceb9b97d3f48452752907ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-356911f312d8922c7a8397fec588221c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c898ed0ab011bcd75326217d6d05cc26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a75e0879f5d14495c19b729f67c70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8154aa09912cba45209da51d0bf15ad3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Zero-shot-Quantization-Aware-Training-for-Object-Detection"><a href="#Task-Specific-Zero-shot-Quantization-Aware-Training-for-Object-Detection" class="headerlink" title="Task-Specific Zero-shot Quantization-Aware Training for Object Detection"></a>Task-Specific Zero-shot Quantization-Aware Training for Object Detection</h2><p><strong>Authors:Changhao Li, Xinrui Chen, Ji Wang, Kang Zhao, Jianfei Chen</strong></p>
<p>Quantization is a key technique to reduce network size and computational complexity by representing the network parameters with a lower precision. Traditional quantization methods rely on access to original training data, which is often restricted due to privacy concerns or security challenges. Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated from pre-trained models, eliminating the need for real training data. Recently, ZSQ has been extended to object detection. However, existing methods use unlabeled task-agnostic synthetic images that lack the specific information required for object detection, leading to suboptimal performance. In this paper, we propose a novel task-specific ZSQ framework for object detection networks, which consists of two main stages. First, we introduce a bounding box and category sampling strategy to synthesize a task-specific calibration set from the pre-trained network, reconstructing object locations, sizes, and category distributions without any prior knowledge. Second, we integrate task-specific training into the knowledge distillation process to restore the performance of quantized detection networks. Extensive experiments conducted on the MS-COCO and Pascal VOC datasets demonstrate the efficiency and state-of-the-art performance of our method. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/DFQ-Dojo/dfq-toolkit">https://github.com/DFQ-Dojo/dfq-toolkit</a> . </p>
<blockquote>
<p>量化是一种通过用较低的精度表示网络参数来减小网络规模和计算复杂性的关键技术。传统量化方法依赖于原始训练数据的访问，这通常由于隐私担忧或安全挑战而受到限制。零射量化（ZSQ）通过使用预先训练模型生成合成数据来解决这个问题，无需使用真实训练数据。最近，ZSQ已扩展到目标检测领域。然而，现有方法使用未标记的任务无关合成图像，缺乏目标检测所需的具体信息，导致性能不佳。本文提出了一种用于目标检测网络的新型任务特定ZSQ框架，该框架由两个阶段组成。首先，我们引入了一种边界框和类别采样策略，以从预训练网络合成任务特定校准集，重建对象位置、大小和类别分布，无需任何先验知识。其次，我们将任务特定训练集成到知识蒸馏过程中，以恢复量化检测网络的性能。在MS-COCO和Pascal VOC数据集上进行的广泛实验证明了我们的方法的效率和最先进的性能。我们的代码公开在：[<a target="_blank" rel="noopener" href="https://github.com/DFQ-Dojo/dfq-toolkit%E3%80%82]">https://github.com/DFQ-Dojo/dfq-toolkit。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16782v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>量化是网络缩减和降低计算复杂度的重要技术，它通过低精度表示网络参数来实现。传统量化方法依赖原始训练数据，但出于隐私或安全考虑，这些数据往往受到限制。零样本量化（ZSQ）通过使用预训练模型生成合成数据来解决这一问题，无需真实训练数据。最新研究将ZSQ扩展到目标检测领域，但现有方法使用无标签的任务无关合成图像，缺乏目标检测所需的具体信息，导致性能不佳。本文提出了一种用于目标检测网络的新型任务特定ZSQ框架，该框架分为两个阶段：首先，通过预训练网络合成任务特定校准集，重构目标位置、大小和类别分布，无需任何先验知识；其次，将任务特定训练集成到知识蒸馏过程中，以恢复量化检测网络的性能。在MS-COCO和Pascal VOC数据集上的大量实验证明了该方法的效率和最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>量化技术用于减少网络大小和计算复杂度，通过低精度表示网络参数实现。</li>
<li>传统量化方法依赖原始训练数据，但存在隐私或安全限制。</li>
<li>零样本量化（ZSQ）通过预训练模型生成的合成数据解决此问题，无需真实训练数据。</li>
<li>最新ZSQ研究已扩展到目标检测领域，但现有方法使用无标签的任务无关合成图像，性能不佳。</li>
<li>本文提出新型任务特定ZSQ框架，分为合成任务特定校准集和集成任务特定训练两个阶段。</li>
<li>框架能够重构目标位置、大小和类别分布，无需任何先验知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4cab5b833569039d618fc13450425da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-442ee2b0776dcff4acc3bdba1908922b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa9640a1519bb6259a4b4caaae53ca2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-246f2f03180a3c413dfd28bb056b22fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-for-Preoperative-Planning-in-Transcatheter-Aortic-Valve-Replacement"><a href="#Semantic-Segmentation-for-Preoperative-Planning-in-Transcatheter-Aortic-Valve-Replacement" class="headerlink" title="Semantic Segmentation for Preoperative Planning in Transcatheter Aortic   Valve Replacement"></a>Semantic Segmentation for Preoperative Planning in Transcatheter Aortic   Valve Replacement</h2><p><strong>Authors:Cedric Zöllner, Simon Reiß, Alexander Jaus, Amroalalaa Sholi, Ralf Sodian, Rainer Stiefelhagen</strong></p>
<p>When preoperative planning for surgeries is conducted on the basis of medical images, artificial intelligence methods can support medical doctors during assessment. In this work, we consider medical guidelines for preoperative planning of the transcatheter aortic valve replacement (TAVR) and identify tasks, that may be supported via semantic segmentation models by making relevant anatomical structures measurable in computed tomography scans. We first derive fine-grained TAVR-relevant pseudo-labels from coarse-grained anatomical information, in order to train segmentation models and quantify how well they are able to find these structures in the scans. Furthermore, we propose an adaptation to the loss function in training these segmentation models and through this achieve a +1.27% Dice increase in performance. Our fine-grained TAVR-relevant pseudo-labels and the computed tomography scans we build upon are available at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.16274176">https://doi.org/10.5281/zenodo.16274176</a>. </p>
<blockquote>
<p>在基于医学图像进行手术术前规划时，人工智能方法可以在评估过程中为医生提供支持。在这项工作中，我们考虑了经导管主动脉瓣置换术（TAVR）的术前规划医学指南，并确定了可通过语义分割模型支持的任务，通过在计算机断层扫描中测量相关解剖结构来实现。我们首先从粗粒度的解剖信息中推导出精细粒度的TAVR相关伪标签，以训练分割模型并量化它们在扫描中发现这些结构的能力。此外，我们对训练这些分割模型的损失函数进行了调整，并因此实现了性能上1.27%的Dice增长。我们构建的精细粒度TAVR相关伪标签和计算机断层扫描数据可在<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.16274176%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://doi.org/10.5281/zenodo.16274176上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16573v1">PDF</a> Accepted at 16th MICCAI Workshop on Statistical Atlases and   Computational Modeling of the Heart (STACOM)</p>
<p><strong>Summary</strong><br>     本研究探讨在术前规划中运用人工智能辅助医学医生进行经导管主动脉瓣置换术（TAVR）的评估。通过从粗颗粒度的解剖信息中衍生出精细颗粒度的TAVR相关伪标签，训练分割模型以量化其在扫描中定位相关结构的能力。同时提出调整损失函数来训练这些分割模型，从而实现性能上的改进。数据集可在此处获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能在术前规划中对医学图像的分析支持医生进行手术评估。</li>
<li>通过精细颗粒度的TAVR相关伪标签训练分割模型。</li>
<li>利用这些标签在计算机断层扫描中找到相关解剖结构。</li>
<li>调整损失函数以增强分割模型的性能，并实现更高的Dice系数得分。</li>
<li>该研究的数据集可供公开访问和使用。</li>
<li>此方法有助于提高术前规划的准确性和效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16573">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0362a2708b24a3196fd2ed0d562773e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5632015d038fda2ff8a7e5643ce5aecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35ea67fb0f6c2ab4507a2884f38e7c79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd3d353fe1c0b9b8e6f42b3b1a21c293.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PlantSAM-An-Object-Detection-Driven-Segmentation-Pipeline-for-Herbarium-Specimens"><a href="#PlantSAM-An-Object-Detection-Driven-Segmentation-Pipeline-for-Herbarium-Specimens" class="headerlink" title="PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium   Specimens"></a>PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium   Specimens</h2><p><strong>Authors:Youcef Sklab, Florian Castanet, Hanane Ariouat, Souhila Arib, Jean-Daniel Zucker, Eric Chenin, Edi Prifti</strong></p>
<p>Deep learning-based classification of herbarium images is hampered by background heterogeneity, which introduces noise and artifacts that can potentially mislead models and reduce classification accuracy. Addressing these background-related challenges is critical to improving model performance. We introduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10 for plant region detection and the Segment Anything Model (SAM2) for segmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing segmentation accuracy. Both models were fine-tuned on herbarium images and evaluated using Intersection over Union (IoU) and Dice coefficient metrics. PlantSAM achieved state-of-the-art segmentation performance, with an IoU of 0.94 and a Dice coefficient of 0.97. Incorporating segmented images into classification models led to consistent performance improvements across five tested botanical traits, with accuracy gains of up to 4.36% and F1-score improvements of 4.15%. Our findings highlight the importance of background removal in herbarium image analysis, as it significantly enhances classification accuracy by allowing models to focus more effectively on the foreground plant structures. </p>
<blockquote>
<p>基于深度学习的植物标本图像分类受到背景异质性的阻碍，背景异质性引入了噪声和伪影，这些可能会误导模型并降低分类准确性。解决这些与背景相关的挑战对于提高模型性能至关重要。我们引入了PlantSAM，这是一个集成了YOLOv10用于植物区域检测和Segment Anything Model（SAM2）用于分割的自动化分割管道。YOLOv10生成边界框提示来引导SAM2，提高分割精度。两个模型都在植物标本图像上进行了微调，并使用交集比（IoU）和Dice系数指标进行了评估。PlantSAM达到了最先进的分割性能，IoU为0.94，Dice系数为0.97。将分割图像纳入分类模型导致了五种测试的植物特征性能的一致性提高，准确率提高了高达4.36%，F1分数提高了4.15%。我们的研究结果表明，在植物标本图像分析中去除背景的重要性，因为它可以显著提高分类模型的准确性，使模型更有效地专注于前景植物结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16506v1">PDF</a> 19 pages, 11 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>基于深度学习的植物标本图像分类受到背景异质性的干扰，这引入了噪声和伪影，可能导致模型误导并降低分类准确性。为解决背景相关的挑战，我们推出了PlantSAM自动化分割管道，集成了YOLOv10用于植物区域检测和Segment Anything Model（SAM2）进行分割。YOLOv10生成边界框提示来引导SAM2，提高分割精度。这两个模型都在植物标本图像上进行微调，并使用交集比（IoU）和Dice系数指标进行评估。PlantSAM达到了先进的分割性能，IoU为0.94，Dice系数为0.97。将分割图像纳入分类模型导致在五个测试的植物学特征上性能一致提高，准确率提高高达4.36%，F1分数提高4.15%。我们的研究结果表明，背景去除在植物标本图像分析中非常重要，因为它可以显著提高分类准确性，使模型更有效地关注前景植物结构。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>植物标本图像分类面临背景异质性的挑战。</li>
<li>背景异质性可能导致模型误导和降低分类准确性。</li>
<li>PlantSAM是一个集成了YOLOv10和SAM2模型的自动化分割管道，用于解决背景干扰问题。</li>
<li>PlantSAM在植物标本图像上实现了高分割性能，IoU达到0.94，Dice系数达到0.97。</li>
<li>将分割图像纳入分类模型可以提高分类准确性。</li>
<li>背景去除在植物标本图像分析中至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f03b09543b88a9c949f2d42ae0e2f79d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8272c95ffafc6f5a57380b1b1e32c6eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16964d6732adfe406a8f017033e5cb37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8599bfad8e79d05e2224e0d4fbd713e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66df0f3b49d27b5b41ae2606fa070c70.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improved-Semantic-Segmentation-from-Ultra-Low-Resolution-RGB-Images-Applied-to-Privacy-Preserving-Object-Goal-Navigation"><a href="#Improved-Semantic-Segmentation-from-Ultra-Low-Resolution-RGB-Images-Applied-to-Privacy-Preserving-Object-Goal-Navigation" class="headerlink" title="Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images   Applied to Privacy-Preserving Object-Goal Navigation"></a>Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images   Applied to Privacy-Preserving Object-Goal Navigation</h2><p><strong>Authors:Xuying Huang, Sicong Pan, Olga Zatsarynna, Juergen Gall, Maren Bennewitz</strong></p>
<p>User privacy in mobile robotics has become a critical concern. Existing methods typically prioritize either the performance of downstream robotic tasks or privacy protection, with the latter often constraining the effectiveness of task execution. To jointly address both objectives, we study semantic-based robot navigation in an ultra-low-resolution setting to preserve visual privacy. A key challenge in such scenarios is recovering semantic segmentation from ultra-low-resolution RGB images. In this work, we introduce a novel fully joint-learning method that integrates an agglomerative feature extractor and a segmentation-aware discriminator to solve ultra-low-resolution semantic segmentation, thereby enabling privacy-preserving, semantic object-goal navigation. Our method outperforms different baselines on ultra-low-resolution semantic segmentation and our improved segmentation results increase the success rate of the semantic object-goal navigation in a real-world privacy-constrained scenario. </p>
<blockquote>
<p>移动机器人的用户隐私已成为人们关注的重点。现有的方法通常侧重于下游机器人任务的性能或隐私保护，而后者往往会限制任务执行的有效性。为了同时解决这两个目标，我们研究在超低分辨率环境下基于语义的机器人导航以保护视觉隐私。此类场景中的一个关键挑战是从超低分辨率的RGB图像中恢复语义分割。在这项工作中，我们引入了一种新颖的完全联合学习方法，该方法结合了聚合特征提取器和分割感知鉴别器来解决超低分辨率语义分割问题，从而实现可保护隐私的、语义目标导航。我们的方法在超低分辨率语义分割方面超越了不同的基线，并且我们改进的分割结果提高了在现实世界的隐私受限场景中语义目标导航的成功率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16034v1">PDF</a> Submitted to RA-L</p>
<p><strong>Summary</strong></p>
<p>本摘要针对移动机器人用户隐私保护问题展开研究。针对现有方法侧重于机器人任务性能而忽视隐私保护的问题，提出在超低分辨率环境下进行语义基础机器人导航的方法，以保护视觉隐私。挑战在于从超低分辨率RGB图像中恢复语义分割。本研究引入了一种全新的联合学习方法，结合特征提取器和分割感知鉴别器，解决超低分辨率语义分割问题，实现隐私保护语义目标导航。该方法在超低分辨率语义分割方面优于不同基线，并且提高了隐私约束场景中语义目标导航的成功率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>移动机器人用户隐私保护成为重要关注点。</li>
<li>现有方法通常侧重机器人任务性能或隐私保护，二者兼顾存在挑战。</li>
<li>在超低分辨率环境下研究语义基础机器人导航以保护视觉隐私。</li>
<li>从超低分辨率RGB图像中恢复语义分割是此类场景的关键挑战。</li>
<li>引入全新的联合学习方法，结合特征提取器和分割感知鉴别器，解决超低分辨率语义分割问题。</li>
<li>所提方法优于其他基线方法，在超低分辨率语义分割方面表现出色。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a32a2469c81b7c3913e5fb927b1d1c20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28068cbd4ea67f450aeca334914b4da9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e575d0bfeb9e19e55bca69b2f20707a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3fd80ad55813a6a1ec9010e41a21e64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58520823a696fe3c90be7c4c310ac6ac.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConformalSAM-Unlocking-the-Potential-of-Foundational-Segmentation-Models-in-Semi-Supervised-Semantic-Segmentation-with-Conformal-Prediction"><a href="#ConformalSAM-Unlocking-the-Potential-of-Foundational-Segmentation-Models-in-Semi-Supervised-Semantic-Segmentation-with-Conformal-Prediction" class="headerlink" title="ConformalSAM: Unlocking the Potential of Foundational Segmentation   Models in Semi-Supervised Semantic Segmentation with Conformal Prediction"></a>ConformalSAM: Unlocking the Potential of Foundational Segmentation   Models in Semi-Supervised Semantic Segmentation with Conformal Prediction</h2><p><strong>Authors:Danhui Chen, Ziquan Liu, Chuxi Yang, Dan Wang, Yan Yan, Yi Xu, Xiangyang Ji</strong></p>
<p>Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domain’s labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in. </p>
<blockquote>
<p>像素级视觉任务（如语义分割）需要大量的高质量标注数据，而这些数据的获取成本很高。半监督语义分割（SSSS）的出现作为一种解决方案，通过自训练技术利用有标签和无标签的数据来缓解标注负担。同时，在大量数据上预训练的基础分割模型的出现，显示出在不同领域进行有效推广的潜力。这项工作探讨了基础分割模型作为未标注图像的注释器，在像素级视觉任务中解决标签稀缺问题的能力。具体来说，我们研究了使用SEEM（一种为文本输入微调过的Segment Anything Model（SAM）变种）来为未标注数据生成预测掩膜的有效性。为了解决使用SEEM生成的掩膜作为监督的缺点，我们提出了ConformalSAM，这是一种新的SSSS框架。它首先使用目标领域的标注数据校准基础模型，然后过滤出未标注数据的不可靠像素标签，以便只有高置信度的标签被用作监督。通过利用合数预测（CP）来通过不确定性校准适应基础模型到目标数据，ConformalSAM可靠地利用基础分割模型的强大功能，这有利于早期学习，而随后的自我依赖训练策略则减轻了后期训练中过度依赖SEEM生成的掩膜的问题。我们的实验表明，在三个标准的SSSS基准测试中，ConformalSAM与最新的SSSS方法相比取得了优越的性能，并且作为插件有助于提高这些方法的效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15803v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>基于像素级别的视觉任务，如语义分割，需要大量高质量标注数据，但获取这些数据成本高昂。为解决标注负担问题，半监督语义分割（SSSS）应运而生，它通过利用标注和无标签数据通过自训练技术来减轻负担。同时，预训练在大量数据上的基础分割模型显示出跨域有效泛化的潜力。本研究探讨基础分割模型作为未标注图像注释器来解决像素级视觉任务中的标签稀缺问题的可行性。特别是，我们研究了使用SEEM（一种针对文本输入的Segment Anything Model（SAM）变体）来为无标签数据生成预测掩膜的效果。为解决使用SEEM生成的掩膜作为监督的缺点，我们提出了ConformalSAM——一种新型SSSS框架。它首先使用目标域的标注数据校准基础模型，然后过滤出无标签数据中不可靠的像素标签，仅使用高置信度的标签作为监督。通过利用顺应性预测（CP）来适应目标数据的不确定性校准，ConformalSAM能够可靠地利用基础分割模型的强大能力，有益于早期学习阶段；随后采用自我依赖训练策略，缓解了对SEEM生成掩膜在后期训练阶段的过度拟合问题。实验表明，在三个标准的SSSS基准测试中，ConformalSAM相较于近期SSSS方法实现了优越的性能，并且作为插件有助于提升这些方法的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>像素级视觉任务如语义分割需要高质量的大量标注数据，但获取这些数据成本高昂。</li>
<li>半监督语义分割（SSSS）方法旨在通过利用标注和无标签数据来缓解标注负担。</li>
<li>基础分割模型具有跨域有效泛化的潜力。</li>
<li>使用SEEM模型为无标签数据生成预测掩膜是一种解决方案。</li>
<li>ConformalSAM是一种新型SSSS框架，通过校准基础模型和过滤不可靠像素标签来提高性能。</li>
<li>ConformalSAM利用顺应性预测（CP）以适应目标数据的不确定性校准，结合早期学习和自我依赖训练策略，提高模型的可靠性并缓解过度拟合问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-071a378bad372c54ac088adf9bf819c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96a89f4eae2b9b450fe23b0cc69dbb16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c29c45e230b8ad2a4eabce780c88f299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd7e0d9382264fe4ec192f1daea5ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1fe67c88d96da8e9c82eef80d78ed5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Label-tree-semantic-losses-for-rich-multi-class-medical-image-segmentation"><a href="#Label-tree-semantic-losses-for-rich-multi-class-medical-image-segmentation" class="headerlink" title="Label tree semantic losses for rich multi-class medical image   segmentation"></a>Label tree semantic losses for rich multi-class medical image   segmentation</h2><p><strong>Authors:Junwen Wang, Oscar MacCormac, William Rochford, Aaron Kujawa, Jonathan Shapey, Tom Vercauteren</strong></p>
<p>Rich and accurate medical image segmentation is poised to underpin the next generation of AI-defined clinical practice by delineating critical anatomy for pre-operative planning, guiding real-time intra-operative navigation, and supporting precise post-operative assessment. However, commonly used learning methods for medical and surgical imaging segmentation tasks penalise all errors equivalently and thus fail to exploit any inter-class semantics in the labels space. This becomes particularly problematic as the cardinality and richness of labels increases to include subtly different classes. In this work, we propose two tree-based semantic loss functions which take advantage of a hierarchical organisation of the labels. We further incorporate our losses in a recently proposed approach for training with sparse, background-free annotations to extend the applicability of our proposed losses. Extensive experiments are reported on two medical and surgical image segmentation tasks, namely head MRI for whole brain parcellation (WBP) with full supervision and neurosurgical hyperspectral imaging (HSI) for scene understanding with sparse annotations. Results demonstrate that our proposed method reaches state-of-the-art performance in both cases. </p>
<blockquote>
<p>丰富而准确的医学图像分割技术将为下一代人工智能定义的临床实践提供支持，通过描绘关键解剖结构来进行术前规划，指导实时术中导航，并支持精确的术后评估。然而，医学和手术图像分割任务中常用的学习方法平等地惩罚所有错误，因此未能利用标签空间中的类间语义。当标签的基数和丰富性增加以包括细微不同的类别时，这变得特别成问题。在这项工作中，我们提出了两种基于树的语义损失函数，它们利用了标签的层次组织。我们进一步将我们的损失纳入最近提出的具有稀疏、无背景注释的训练方法中，以扩展我们提出的损失适用性。在两项医学和手术图像分割任务上报告了广泛实验的结果，分别是全监督的头MRI全脑分区（WBP）和具有稀疏注释的神经外科高光谱成像（HSI）的场景理解。结果表明，我们的方法在这两种情况下均达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15777v1">PDF</a> arXiv admin note: text overlap with arXiv:2506.21150</p>
<p><strong>Summary</strong></p>
<p>本文强调医疗图像分割技术在新一代AI定义的临床实践中的重要性，包括在术前规划、术中实时导航和术后精确评估中的应用。然而，常用的学习方法和标签空间的语义之间缺乏关联，对于复杂的医疗和手术图像分割任务存在问题。本研究提出了两种基于树的语义损失函数，能够利用标签的层次结构。此外，将损失函数应用于稀疏、无背景注释的训练方法，扩大了其应用范围。在医疗和手术图像分割任务上的实验表明，该方法达到了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗图像分割技术在新一代AI临床实践中具有重要作用。</li>
<li>常用的医学和手术图像分割学习方法未充分利用标签空间中的语义关系。</li>
<li>提出两种基于树的语义损失函数，能够利用标签的层次结构提高分割性能。</li>
<li>损失函数应用于稀疏、无背景注释的训练方法，增强了其适用性。</li>
<li>实验结果表明该方法在医疗和手术图像分割任务上达到了最先进的性能。</li>
<li>在头部MRI全监督下进行全脑分块（WBP）和神经外科超光谱成像（HSI）场景理解的任务中验证了该方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-534ffca011f1f130d01ec12b61d90605.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Depthwise-Dilated-Convolutional-Adapters-for-Medical-Object-Tracking-and-Segmentation-Using-the-Segment-Anything-Model-2"><a href="#Depthwise-Dilated-Convolutional-Adapters-for-Medical-Object-Tracking-and-Segmentation-Using-the-Segment-Anything-Model-2" class="headerlink" title="Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and   Segmentation Using the Segment Anything Model 2"></a>Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and   Segmentation Using the Segment Anything Model 2</h2><p><strong>Authors:Guoping Xu, Christopher Kabat, You Zhang</strong></p>
<p>Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2’s streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/apple1986/DD-SAM2">https://github.com/apple1986/DD-SAM2</a>. </p>
<blockquote>
<p>最近，医学图像分割领域的进展主要得益于深度学习。然而，大多数现有方法仍然受到特定模态设计的限制，在动态医学影像场景中适应性较差。Segment Anything Model 2（SAM2）及其相关变体引入了实时视频分割的流式内存机制，为基于提示的通用解决方案提供了新的机会。然而，将这些模型适应到医学视频场景通常需要大规模数据集进行重新训练或迁移学习，这导致了较高的计算成本以及灾难性遗忘的风险。为了解决这些挑战，我们提出了DD-SAM2，这是一个SAM2的有效适配框架，它结合了Depthwise-Dilated Adapter（DD-Adapter）以增强多尺度特征提取能力，同时尽量减少参数开销。这种设计使得在有限的训练数据上对医学视频进行SAM2精细调整成为可能。与现有仅专注于静态图像的适配器方法不同，DD-SAM2充分利用SAM2的流式内存进行医学视频目标跟踪和分割。在TrackRad2025（肿瘤分割）和EchoNet-Dynamic（左心室跟踪）数据集上的综合评估表明，其性能卓越，分别实现了Dice得分0.93和0.97。据我们所知，这项工作初步尝试系统地探索基于适配器的SAM2精细调整在医学视频分割和跟踪中的应用。代码、数据集和模型将在<a target="_blank" rel="noopener" href="https://github.com/apple1986/DD-SAM">https://github.com/apple1986/DD-SAM</a> 公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14613v1">PDF</a> 24 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>基于深度学习的医学图像分割技术取得新进展，但仍存在针对特定模态的设计限制以及适应动态医学影像场景的适应性差的问题。SAM2模型及其变体通过引入实时视频分割的流式内存机制，为基于提示的通用解决方案提供了新的机会。然而，将这些模型适应到医学视频场景通常需要大规模数据集进行重新训练或迁移学习，导致高计算成本和灾难性遗忘的风险。为解决这些挑战，我们提出了DD-SAM2，一个高效的SAM2适应框架，它采用Depthwise-Dilated Adapter（DD-Adapter）以增强多尺度特征提取，同时参数开销最小化。该设计可在有限的训练数据上实现对医学视频的SAM2精细调整。与现有仅专注于静态图像的适配器方法不同，DD-SAM2充分利用SAM2的流式内存进行医学视频目标跟踪和分割。在TrackRad2025（肿瘤分割）和EchoNet-Dynamic（左心室跟踪）数据集上的全面评估表明其卓越性能，分别实现了Dice得分0.93和0.97。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在医学图像分割领域取得进展，但仍存在模态特定设计和动态医学影像场景适应性问题。</li>
<li>SAM2模型及其变体为基于提示的通用解决方案提供新机会，尤其适用于实时视频分割。</li>
<li>适应医学视频场景通常需要大规模数据集，导致高计算成本和灾难性遗忘风险。</li>
<li>提出DD-SAM2框架，通过Depthwise-Dilated Adapter增强多尺度特征提取，实现SAM2在医学视频上的精细调整。</li>
<li>DD-SAM2不同于现有专注于静态图像的适配器方法，充分利用SAM2的流式内存进行医学视频目标跟踪和分割。</li>
<li>在TrackRad2025和EchoNet-Dynamic数据集上评估，DD-SAM2表现出卓越性能。</li>
<li>研究者公开了代码、数据集和模型，便于他人访问和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14613">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dc53c57b32f8c8971efb1c2a5235890d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2634f32af68d0603b25dae869fab1b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SCORE-Scene-Context-Matters-in-Open-Vocabulary-Remote-Sensing-Instance-Segmentation"><a href="#SCORE-Scene-Context-Matters-in-Open-Vocabulary-Remote-Sensing-Instance-Segmentation" class="headerlink" title="SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance   Segmentation"></a>SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance   Segmentation</h2><p><strong>Authors:Shiqi Huang, Shuting He, Huaiyuan Qin, Bihan Wen</strong></p>
<p>Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose $\textbf{SCORE}$ ($\textbf{S}$cene $\textbf{C}$ontext matters in $\textbf{O}$pen-vocabulary $\textbf{RE}$mote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/SCORE">https://github.com/HuangShiqi128/SCORE</a>. </p>
<blockquote>
<p>当前大多数遥感实例分割方法都是针对封闭词汇预测设计的，这限制了它们对新型类别的识别能力，以及在数据集间的泛化能力。这限制了它们在多种地球观测场景中的应用。为了解决这个问题，我们为遥感实例分割引入了开放词汇（OV）学习。虽然当前的OV分割模型在自然图像数据集上表现良好，但它们直接应用于遥感却面临着挑战，例如景观多样性、季节变化和空中影像中小目标或模糊物体的存在。为了克服这些挑战，我们提出了$\textbf{SCORE}$（$\textbf{S}$cene $\textbf{C}$ontext matters in $\textbf{O}$pen-vocabulary $\textbf{RE}$mote sensing instance segmentation），一个整合多粒度场景上下文的框架，即区域上下文和全局上下文，以增强视觉和文本表示。具体来说，我们引入了区域感知集成，通过区域上下文细化类别嵌入，以提高目标可区分性。此外，我们提出了全局上下文适应，用遥感全局上下文丰富原始文本嵌入，为分类器创建一个更具适应性和表达能力的语言潜在空间。我们在多个数据集上建立了OV遥感实例分割的新基准。实验结果表明，我们提出的方法达到了最先进的性能，为大规模、真实世界的地理空间分析提供了稳健的解决方案。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/SCORE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HuangShiqi128/SCORE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12857v2">PDF</a> ICCV 2025 (Highlight), code see   <a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/SCORE">https://github.com/HuangShiqi128/SCORE</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了遥感实例分割领域中的开放词汇表学习方法。现有方法主要面向封闭词汇预测，难以识别新类别或跨数据集进行推广，限制了其在多样地球观测场景中的应用。为此，本文提出了一个名为SCORE的框架，该框架融合了多粒度场景上下文信息，包括区域上下文和全局上下文，以增强视觉和文本表示。实验结果表明，该方法实现了先进性能，为大规模地理空间分析提供了稳健解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前遥感实例分割方法主要面向封闭词汇预测，限制了其在多样地球观测场景中的应用。</li>
<li>开放词汇（OV）学习是解决此问题的一种方法，但直接应用于遥感面临挑战。</li>
<li>SCORE框架融合了多粒度场景上下文信息，包括区域上下文和全局上下文。</li>
<li>Region-Aware Integration通过结合区域上下文信息优化类别嵌入，提高目标区分度。</li>
<li>Global Context Adaptation丰富了文本嵌入的遥感全局上下文信息，使分类器更具适应性和表现力。</li>
<li>实验结果表明，该方法在开放词汇遥感实例分割方面达到了先进性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e4f6be50adfe021fbca4cd5f6ec85396.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e546c4dda49d2959f3f4e6e6a13a422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41ae00d913e6e00cdf37416578065762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-492ce0d6f40513bcc7efd539fb9bc595.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OD-VIRAT-A-Large-Scale-Benchmark-for-Object-Detection-in-Realistic-Surveillance-Environments"><a href="#OD-VIRAT-A-Large-Scale-Benchmark-for-Object-Detection-in-Realistic-Surveillance-Environments" class="headerlink" title="OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic   Surveillance Environments"></a>OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic   Surveillance Environments</h2><p><strong>Authors:Hayat Ullah, Abbas Khan, Arslan Munir, Hari Kalva</strong></p>
<p>Realistic human surveillance datasets are crucial for training and evaluating computer vision models under real-world conditions, facilitating the development of robust algorithms for human and human-interacting object detection in complex environments. These datasets need to offer diverse and challenging data to enable a comprehensive assessment of model performance and the creation of more reliable surveillance systems for public safety. To this end, we present two visual object detection benchmarks named OD-VIRAT Large and OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance imagery. The video sequences in both benchmarks cover 10 different scenes of human surveillance recorded from significant height and distance. The proposed benchmarks offer rich annotations of bounding boxes and categories, where OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also focuses on benchmarking state-of-the-art object detection architectures, including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object detection-specific variant of VIRAT dataset. To the best of our knowledge, it is the first work to examine the performance of these recently published state-of-the-art object detection architectures on realistic surveillance imagery under challenging conditions such as complex backgrounds, occluded objects, and small-scale objects. The proposed benchmarking and experimental settings will help in providing insights concerning the performance of selected object detection models and set the base for developing more efficient and robust object detection architectures. </p>
<blockquote>
<p>现实的人类监控数据集对于在现实条件下训练和评估计算机视觉模型至关重要，有助于在复杂环境中开发用于人类和人机交互对象检测的稳健算法。这些数据集需要提供多样化和具有挑战性的数据，以实现对模型性能的全面评估，并创建更可靠的公共安全监控系统。为此，我们提出了两个名为OD-VIRAT Large和OD-VIRAT Tiny的视觉目标检测基准测试，旨在推进监控图像中的视觉理解任务。这两个基准测试中的视频序列涵盖了从重大高度和距离记录的10个不同的人类监控场景。所提出基准测试提供了丰富的边界框和类别注释，其中OD-VIRAT Large有599,996张图像中的870万个注释实例，而OD-VIRAT Tiny有19,860张图像中的288,901个注释实例。这项工作还重点关注基于最新技术水平的对象检测架构的基准测试，包括在VIRAT数据集的这一特定对象检测变体上的RETMDET、YOLOX、RetinaNet、DETR和可变形DETR。据我们所知，这是第一项工作在复杂的现实监控图像上检验这些最近发布的最新目标检测架构的性能，如在复杂背景、遮挡物体和小规模物体等具有挑战性的条件下。所提出的基准测试和实验设置将有助于深入了解所选目标检测模型的性能，并为开发更高效和稳健的目标检测架构奠定基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12396v2">PDF</a> 14 pages</p>
<p><strong>摘要</strong><br>    提出两个用于监控图像视觉理解任务的大型和微型视觉对象检测基准数据集OD-VIRAT Large和OD-VIRAT Tiny。这些数据集涵盖从不同高度和距离录制的真实人类监控视频序列，包含丰富的边界框和类别注释。该工作重点评估先进的对象检测架构在具有挑战性的现实监控图像上的性能，包括复杂背景、遮挡物体和小物体等条件。这些基准测试和实验设置有助于了解所选对象检测模型性能并为开发更高效和稳健的对象检测架构奠定基础。</p>
<p><strong>关键见解</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12396">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0b63697a15eb9d18a1e33fef6b62135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2a5f265c6f462489d5637de35be7aaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ca1931928cb46d8fc09d3990e92ecc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2369bae62093ae8271fc472cc564af9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f636cd881583602194dcd47abe264f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e09b78b916781a52c3a73c08e5bc14e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Out-of-distribution-data-supervision-towards-biomedical-semantic-segmentation"><a href="#Out-of-distribution-data-supervision-towards-biomedical-semantic-segmentation" class="headerlink" title="Out-of-distribution data supervision towards biomedical semantic   segmentation"></a>Out-of-distribution data supervision towards biomedical semantic   segmentation</h2><p><strong>Authors:Yiquan Gao, Duohui Xu</strong></p>
<p>Biomedical segmentation networks easily suffer from the unexpected misclassification between foreground and background objects when learning on limited and imperfect medical datasets. Inspired by the strong power of Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric framework, Med-OoD to address this issue by introducing OoD data supervision into fully-supervised biomedical segmentation with none of the following needs: (i) external data sources, (ii) feature regularization objectives, (iii) additional annotations. Our method can be seamlessly integrated into segmentation networks without any modification on the architectures. Extensive experiments show that Med-OoD largely prevents various segmentation networks from the pixel misclassification on medical images and achieves considerable performance improvements on Lizard dataset. We also present an emerging learning paradigm of training a medical segmentation network completely using OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU as test result. We hope this learning paradigm will attract people to rethink the roles of OoD data. Code is made available at <a target="_blank" rel="noopener" href="https://github.com/StudioYG/Med-OoD">https://github.com/StudioYG/Med-OoD</a>. </p>
<blockquote>
<p>生物医学分割网络在有限且存在缺陷的医疗数据集上进行学习时，容易受到前景和背景对象之间意外误分类的影响。受其他视觉任务中异常数据（Out-of-Distribution，OoD）强大能力的启发，我们提出了一个以数据为中心的框架Med-OoD，通过引入OoD数据监督来解决这个问题，完全监督生物医学分割，无需以下需求：（i）外部数据源，（ii）特征正则化目标，（iii）额外的注释。我们的方法可以无缝集成到分割网络中，无需对架构进行任何修改。大量实验表明，Med-OoD在很大程度上防止了各种分割网络在医学图像上的像素误分类，并在蜥蜴数据集上实现了显著的性能改进。我们还提出了一种全新的学习范式，即完全使用不含前景类别标签的OoD数据来训练医学分割网络，令人惊讶的是，测试结果为mIoU达到76.1%。我们希望这种学习范式能吸引人们重新思考OoD数据的作用。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/StudioYG/Med-OoD%E3%80%82">https://github.com/StudioYG/Med-OoD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12105v1">PDF</a> This paper was published in Proceedings of SPIE Volume 13442 and is   reprinted with permission. The official version is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1117/12.3052988">https://doi.org/10.1117/12.3052988</a>. One personal copy is allowed.   Reproduction, distribution, or commercial use is prohibited</p>
<p><strong>Summary</strong><br>生物医学分割网络在有限且非完美的医疗数据集上进行学习时，容易出现前景和背景对象之间的意外误分类。受其他视觉任务中异常分布（Out-of-Distribution，OoD）数据强大能力的启发，我们提出了一个以数据为中心的框架Med-OoD，通过引入OoD数据监督来解决这个问题，完全监督生物医学分割，无需以下需求：（i）外部数据源，（ii）特征正则化目标，（iii）额外注释。我们的方法可以无缝集成到分割网络中，无需对架构进行任何修改。大量实验表明，Med-OoD在很大程度上防止了各种分割网络在医学图像上的像素误分类，并在蜥蜴数据集上实现了显著的性能改进。我们还提出了一种全新的学习范式，即完全使用OoD数据训练医学分割网络，无需前景类别标签，令人惊讶的是，测试结果为mIoU 76.1%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生物医学分割网络在有限且非完美的医疗数据集上易遭受前景和背景对象误分类的问题。</li>
<li>引入Out-of-Distribution (OoD) 数据监督能够提升生物医学分割网络的性能。</li>
<li>提出的Med-OoD框架无需外部数据源、特征正则化目标和额外注释。</li>
<li>Med-OoD框架可无缝集成到分割网络中，无需对架构进行任何修改。</li>
<li>Med-OoD在医学图像分割中显著减少了像素误分类。</li>
<li>在Lizard数据集上，Med-OoD实现了显著的性能改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12105">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-74eef02fc50d33592f55dfe9db594be8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe21d1301a9e549ebf0e77a38b569f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6dd7c3a585d92b4d89517c256b99216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d8d20d44260c0de06ce091e50c88c1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="YOLOv8-SMOT-An-Efficient-and-Robust-Framework-for-Real-Time-Small-Object-Tracking-via-Slice-Assisted-Training-and-Adaptive-Association"><a href="#YOLOv8-SMOT-An-Efficient-and-Robust-Framework-for-Real-Time-Small-Object-Tracking-via-Slice-Assisted-Training-and-Adaptive-Association" class="headerlink" title="YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small   Object Tracking via Slice-Assisted Training and Adaptive Association"></a>YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small   Object Tracking via Slice-Assisted Training and Adaptive Association</h2><p><strong>Authors:Xiang Yu, Xinyao Liu, Guang Liang</strong></p>
<p>Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 “Finding Birds” Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \textbf{SliceTrain}. This framework, through the synergy of ‘deterministic full-coverage slicing’ and ‘slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \textbf{motion direction maintenance (EMA)} mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Salvatore-Love/YOLOv8-SMOT">https://github.com/Salvatore-Love/YOLOv8-SMOT</a>. </p>
<blockquote>
<p>从无人机（UAV）的角度跟踪小型敏捷多目标（SMOT），如鸟类，是一项极具挑战性的计算机视觉任务。难度主要来源于三个方面：目标外观特征的极度稀缺，由相机和目标本身动力学结合导致的复杂运动纠缠，以及由密集集群行为引起的频繁遮挡和身份模糊。本文详细介绍了我们在MVA 2025“寻找鸟类”小型多目标跟踪挑战赛（SMOT4SB）中夺冠的解决方案，该方案采用检测跟踪范式，在检测和关联层面都有针对性创新。在检测方面，我们提出了一个系统的训练增强框架，名为SliceTrain。该框架通过“确定性全覆盖切片”和“切片级随机增强”的协同作用，有效解决了高分辨率图像训练中目标过小导致的训练不足问题。在跟踪方面，我们设计了一个完全独立于外观信息的稳健跟踪器。通过将运动方向维护（EMA）机制和结合边界框扩展和距离惩罚的自适应相似度量融入OC-SORT框架，我们的跟踪器可以稳定处理不规则运动并保持目标身份。我们的方法在SMOT4SB公开测试集上达到了业界领先性能，SO-HOTA得分达到55.205，充分验证了我们的框架在解决复杂现实世界SMOT问题上的有效性和先进性。源代码将发布在<a target="_blank" rel="noopener" href="https://github.com/Salvatore-Love/YOLOv8-SMOT%E3%80%82">https://github.com/Salvatore-Love/YOLOv8-SMOT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12087v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在无人飞行器视角中对小型敏捷多目标（如鸟类）进行跟踪的计算机视觉任务的高挑战性和其解决方案。文章详细描述了该团队在MVA 2025“寻找鸟类”小型多目标跟踪挑战赛（SMOT4SB）中的冠军解决方案，该方案采用检测跟踪范式，在检测和关联层面都有针对性创新。针对检测方面的问题，提出了名为SliceTrain的系统性训练增强框架，有效解决了高分辨率图像训练中目标学习不足的问题。在跟踪方面，设计了一个独立于外观信息的稳健跟踪器，通过融入运动方向维护机制和自适应相似性度量标准，实现了对不规则运动的稳定处理和目标身份的维持。该方法在SMOT4SB公开测试集上达到了业界领先水平，SO-HOTA得分为55.205，验证了该框架在解决复杂现实SMOT问题中的有效性和先进性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无人飞行器视角对小型敏捷多目标进行跟踪是计算机视觉领域的一大挑战。</li>
<li>文章详述了在MVA 2025“寻找鸟类”小型多目标跟踪挑战赛中的冠军解决方案。</li>
<li>该方案采用检测跟踪范式，针对检测与关联层面进行创新。</li>
<li>SliceTrain框架解决了高分辨率图像训练中目标学习不足的问题。</li>
<li>设计了一个独立于外观信息的稳健跟踪器，实现对不规则运动的稳定处理和目标身份的维持。</li>
<li>跟踪器融合了运动方向维护机制和自适应相似性度量标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12087">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-55720a96bee800d6d37bf6922dee2c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62cfafe2bc0968b73a863bd5a8a7826d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-106ec6544516fed88166fa0d37b7838e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Spatial-Frequency-Modulation-for-Semantic-Segmentation"><a href="#Spatial-Frequency-Modulation-for-Semantic-Segmentation" class="headerlink" title="Spatial Frequency Modulation for Semantic Segmentation"></a>Spatial Frequency Modulation for Semantic Segmentation</h2><p><strong>Authors:Linwei Chen, Ying Fu, Lin Gu, Dezhi Zheng, Jifeng Dai</strong></p>
<p>High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Linwei-Chen/SFM">https://github.com/Linwei-Chen/SFM</a>. </p>
<blockquote>
<p>高频空间信息，包括纹理等细节，对语义分割的准确性有很大贡献。然而，根据Nyquist-Shannon采样定理，高频分量在通过步幅卷积等降采样层传播时容易发生混叠或失真。在这里，我们提出了一种新的空间频率调制（SFM）方法，该方法在降采样之前将高频特征调制到较低频率，然后在上采样过程中进行解调。具体来说，我们通过自适应重采样（ARS）实现调制，并设计了一个轻量级的附加组件，可以密集地对高频区域进行采样以扩大信号，从而根据频率缩放属性降低其频率。我们还提出了多尺度自适应上采样（MSAU）来解调调制特征，并通过非均匀上采样恢复高频信息。该模块通过显式利用多个尺度上密集重采样和稀疏重采样区域之间的信息交互，进一步提高了分割效果。这两个模块可以无缝地集成到各种架构中，从卷积神经网络到转换器。特征可视化和分析证实，我们的方法有效地减轻了混叠现象，同时在解调后成功保留了细节。最后，我们通过将其扩展到图像分类、对抗稳健性、实例分割和全视分割任务来验证SFM的广泛适用性和有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/Linwei-Chen/SFM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Linwei-Chen/SFM找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11893v2">PDF</a> Accept by TPAMI 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了空间频率对语义分割准确度的影响，提出了空间频率调制（SFM）技术。该技术包括自适应重采样（ARS）和多尺度自适应上采样（MSAU），用于处理高频特征信息的失真问题，并通过实验验证了在多种任务中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高频信息如纹理对语义分割的准确度有重要贡献。</li>
<li>Nyquist-Shannon采样定理指出高频成分在传播过程中易受到混叠或失真影响。</li>
<li>提出空间频率调制（SFM）技术，通过自适应重采样（ARS）实现高频特征到低频的调制，再通过多尺度自适应上采样（MSAU）进行解调并恢复高频信息。</li>
<li>该方法能无缝集成到各种架构中，包括卷积神经网络和转换器。</li>
<li>特征可视化和分析证实该方法能有效减轻混叠现象，并在解调后成功保留细节。</li>
<li>实验验证了SFM在图像分类、对抗鲁棒性、实例分割和全景分割任务中的广泛应用和有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11893">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0a1d554775d6787f6ed968bf79a5fbb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b62a23ad0c33432c909c9e9b442a616b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9649a243203d964257f55f2e967fa31e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-567a0800f607802e2939076d5a661cee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation"><a href="#Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation" class="headerlink" title="Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation"></a>Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation</h2><p><strong>Authors:Shuchang Ye, Usman Naseem, Mingyuan Meng, Jinman Kim</strong></p>
<p>Medical language-guided segmentation, integrating textual clinical reports as auxiliary guidance to enhance image segmentation, has demonstrated significant improvements over unimodal approaches. However, its inherent reliance on paired image-text input, which we refer to as &#96;&#96;textual reliance”, presents two fundamental limitations: 1) many medical segmentation datasets lack paired reports, leaving a substantial portion of image-only data underutilized for training; and 2) inference is limited to retrospective analysis of cases with paired reports, limiting its applicability in most clinical scenarios where segmentation typically precedes reporting. To address these limitations, we propose ProLearn, the first Prototype-driven Learning framework for language-guided segmentation that fundamentally alleviates textual reliance. At its core, we introduce a novel Prototype-driven Semantic Approximation (PSA) module to enable approximation of semantic guidance from textual input. PSA initializes a discrete and compact prototype space by distilling segmentation-relevant semantics from textual reports. Once initialized, it supports a query-and-respond mechanism which approximates semantic guidance for images without textual input, thereby alleviating textual reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn outperforms state-of-the-art language-guided methods when limited text is available. </p>
<blockquote>
<p>医疗语言引导分割技术通过将文本临床报告作为辅助指导来增强图像分割，已经证明其在单模态方法上取得了显著改进。然而，其对配对图像文本输入的固有依赖，我们称之为“文本依赖”，存在两个基本局限：1）许多医疗分割数据集缺乏配对报告，导致大量仅包含图像的数据未能得到充分利用；2）推理仅限于具有配对报告的病例的回顾性分析，限制了其在大多数临床场景中的应用，因为在大多数情况下，分割是在报告之前进行的。为了解决这些局限，我们提出了ProLearn，即首个用于语言引导分割的原型驱动学习框架，从根本上缓解了文本依赖问题。其核心在于，我们引入了一种新型原型驱动语义逼近（PSA）模块，以实现从文本输入中进行语义指导的逼近。PSA通过从文本报告中提炼出与分割相关的语义，初始化一个离散且紧凑的原型空间。初始化完成后，它支持查询和响应机制，能够在没有文本输入的情况下对图像进行语义指导逼近，从而缓解了对文本的依赖。在QaTa-COV19、MosMedData+和Kvasir-SEG上的大量实验表明，当文本有限时，ProLearn在现有语言引导方法中表现最佳。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11055v3">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>医学语言引导分割技术中，整合文本临床报告作为辅助指导来提升图像分割的效果已显现。然而，其固有依赖于配对图文输入（“文本依赖”）存在两大局限：一是许多医学分割数据集缺乏配对报告，导致大量仅图像数据未被充分利用于训练；二是推理仅限于有配对报告的回顾性案例分析，限制了其在大多数临床场景中的应用。为解决这个问题，我们提出ProLearn——首个原型驱动的学习框架用于语言引导分割，从根本上减轻文本依赖。其核心引入新颖的原型驱动语义近似模块，实现文本输入语义指导的近似。PSA通过蒸馏报告中的分割相关语义，初始化一个离散且紧凑的原型空间。初始化后，它支持查询和响应机制，为无文本输入的图像近似语义指导，从而减轻文本依赖。在QaTa-COV19、MosMedData+和Kvasir-SEG上的广泛实验证明，当文本有限时，ProLearn表现优于先进的语言引导方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>医学语言引导分割技术结合文本临床报告来提升图像分割效果。</li>
<li>该技术存在对配对图文输入的依赖，导致数据利用和实际应用场景受限。</li>
<li>提出ProLearn框架，通过原型驱动学习减轻对文本依赖。</li>
<li>ProLearn引入原型驱动语义近似模块，实现文本语义指导的近似。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ebba7c9d0958ff69f7000f5f917ee09d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-581a2ac34e56e6d9c43a0fe382a3f143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfb49aef80910b9762fe28fb9303d90d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b6727b70a9303b94d9b35023ccabaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be8317ce099f7cc5319b5eef5332a5a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffae2a1d29c91fb90ad2aa4271470703.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Diffusion-Guided-Knowledge-Distillation-for-Weakly-Supervised-Low-Light-Semantic-Segmentation"><a href="#Diffusion-Guided-Knowledge-Distillation-for-Weakly-Supervised-Low-Light-Semantic-Segmentation" class="headerlink" title="Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light   Semantic Segmentation"></a>Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light   Semantic Segmentation</h2><p><strong>Authors:Chunyan Wang, Dong Zhang, Jinhui Tang</strong></p>
<p>Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the model’s ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:<a target="_blank" rel="noopener" href="https://github.com/ChunyanWang1/DGKD-WLSS">https://github.com/ChunyanWang1/DGKD-WLSS</a>. </p>
<blockquote>
<p>弱监督语义分割旨在利用弱标注为每个像素分配类别标签，从而显著降低手动标注成本。尽管现有方法在全光照场景下取得了显著的进步，但在低光环境中，由于两个基本局限性，它们的性能会显著下降：严重的图像质量下降（例如，低对比度、噪声和颜色失真）和弱监督的内在约束。这些因素共同作用，导致不可靠的类激活图和语义模糊的伪标签，最终损害模型学习判别特征表示的能力。为了解决这些问题，我们提出了用于弱监督低光语义分割的扩散引导知识蒸馏（DGKD-WLSS）这一新型框架，它协同结合了扩散引导知识蒸馏（DGKD）和深度引导特征融合（DGF2）。DGKD通过基于扩散的去噪和知识蒸馏对齐正常光和低光特征，而DGF2将深度图作为光照不变的几何先验进行集成，以增强结构特征学习。大量实验表明DGKD-WLSS的有效性，它在低光条件下的弱监督语义分割任务中实现了最佳性能。源代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/ChunyanWang1/DGKD-WLSS%E3%80%82">https://github.com/ChunyanWang1/DGKD-WLSS。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07578v2">PDF</a> Accepted by ACM Multimedia</p>
<p><strong>Summary</strong><br>    弱监督语义分割利用弱标注对每像素分配类别标签，显著降低手动标注成本。现有方法在光照充足场景中取得显著进展，但在低光环境下性能下降，主要由于图像质量严重退化和弱监督的内在约束。为解决这些问题，提出结合扩散引导知识蒸馏和深度引导特征融合的框架（DGKD-WLSS），通过扩散去噪和知识蒸馏对齐正常光和低光特征，同时利用深度图增强结构特征学习。DGKD-WLSS在弱监督语义分割任务中实现最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>弱监督语义分割旨在利用弱标注降低手动标注成本。</li>
<li>在低光环境下，现有方法性能下降，主要由于图像质量退化和弱监督约束。</li>
<li>DGKD-WLSS框架结合扩散引导知识蒸馏和深度引导特征融合。</li>
<li>扩散去噪和知识蒸馏用于对齐正常光和低光特征。</li>
<li>深度图作为照明不变几何先验增强结构特征学习。</li>
<li>DGKD-WLSS在弱监督语义分割任务中实现最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07578">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9c3ecb61b77a59681a9a0d5879d7d27f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85c216e5089f300274a5b86edcc59f41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d3ac68ddb94c6f497ad2161a33c0777.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9ad9c3f3cbe52a2440aeb93a68f1055.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257b646c4d9b4a0c9cbc1aed98551af2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Accurate-and-Efficient-3D-Object-Detection-for-Autonomous-Driving-A-Mixture-of-Experts-Computing-System-on-Edge"><a href="#Towards-Accurate-and-Efficient-3D-Object-Detection-for-Autonomous-Driving-A-Mixture-of-Experts-Computing-System-on-Edge" class="headerlink" title="Towards Accurate and Efficient 3D Object Detection for Autonomous   Driving: A Mixture of Experts Computing System on Edge"></a>Towards Accurate and Efficient 3D Object Detection for Autonomous   Driving: A Mixture of Experts Computing System on Edge</h2><p><strong>Authors:Linshen Liu, Boyan Su, Junyue Jiang, Guanlin Wu, Cong Guo, Ceyu Xu, Hao Frank Yang</strong></p>
<p>This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as an end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/LinshenLiu622/EMC2">https://github.com/LinshenLiu622/EMC2</a>. </p>
<blockquote>
<p>本文提出了基于边缘计算的混合专家（MoE）协同计算（EMC2）系统，这是一种专为自动驾驶车辆设计的优化计算系统，可同时进行低延迟和高精度的3D目标检测。与传统的不同，EMC2采用了针对边缘平台的场景感知MoE架构。通过有效地融合激光雷达和相机数据，系统利用稀疏的3D点云和密集的2D图像的优势，生成稳健的多模态表示。为此，EMC2采用自适应多模态数据桥进行多尺度预处理传感器输入，然后通过场景感知路由机制根据目标可见性和距离动态地将特征调度到专用的专家模型。此外，EMC2集成了联合硬件软件优化，包括硬件资源利用优化和计算图简化，以确保在资源受限的边缘设备上实现高效实时的推理。在开源基准测试上的实验清楚地表明了EMC2作为端到端系统的优势。在KITTI数据集上，与Jetson平台上的15种基线方法相比，其平均精度提高了3.58%，推理速度提高了159.06%，在nuscenes数据集上也取得了类似的性能提升，这突显了其在推动可靠、实时的自动驾驶车辆3D目标检测任务方面的能力。官方实现可在<a target="_blank" rel="noopener" href="https://github.com/LinshenLiu622/EMC2">https://github.com/LinshenLiu622/EMC2</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04123v2">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong>：</p>
<p>本文提出了基于边缘计算的混合专家协作计算系统（EMC2），专为自动驾驶车辆设计，可实现低延迟、高准确度的3D目标检测。EMC2采用情景感知的混合专家架构，优化边缘平台，结合激光雷达和相机数据，利用稀疏的3D点云和密集的2D图像生成稳健的多模式表示。通过自适应多模式数据桥和多尺度预处理，以及基于目标可见性和距离的情景感知路由机制，将特性动态分派给专用专家模型。此外，EMC2还集成了联合软硬件优化，包括硬件资源利用优化和计算图简化，以确保在资源受限的边缘设备上实现高效、实时的推理。在KITTI和nuscenes数据集上的实验表明，与基线方法相比，EMC2在准确性上有所提高，推理速度加快。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>EMC2是一个为自动驾驶车辆设计的计算系统，实现低延迟、高准确度的3D目标检测。</li>
<li>采用情景感知的混合专家架构，优化边缘平台。</li>
<li>结合激光雷达和相机数据，生成稳健的多模式表示。</li>
<li>通过自适应多模式数据桥和多尺度预处理，以及情景感知路由机制，实现特性动态分配。</li>
<li>EMC2集成了联合软硬件优化，包括硬件资源利用优化和计算图简化。</li>
<li>在KITTI和nuscenes数据集上，EMC2在准确性和推理速度上均有所改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04123">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a7a9c43d98aad6d8dc886736faa834a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23dcdeba355fec3313a321fab300fcc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6856d88879f84ad660b99f7ae37ba4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e862d54b099ec2b96471efcb536040.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="2-5D-Object-Detection-for-Intelligent-Roadside-Infrastructure"><a href="#2-5D-Object-Detection-for-Intelligent-Roadside-Infrastructure" class="headerlink" title="2.5D Object Detection for Intelligent Roadside Infrastructure"></a>2.5D Object Detection for Intelligent Roadside Infrastructure</h2><p><strong>Authors:Nikolai Polley, Yacin Boualili, Ferdinand Mütsch, Maximilian Zipfl, Tobias Fleck, J. Marius Zöllner</strong></p>
<p>On-board sensors of autonomous vehicles can be obstructed, occluded, or limited by restricted fields of view, complicating downstream driving decisions. Intelligent roadside infrastructure perception systems, installed at elevated vantage points, can provide wide, unobstructed intersection coverage, supplying a complementary information stream to autonomous vehicles via vehicle-to-everything (V2X) communication. However, conventional 3D object-detection algorithms struggle to generalize under the domain shift introduced by top-down perspectives and steep camera angles. We introduce a 2.5D object detection framework, tailored specifically for infrastructure roadside-mounted cameras. Unlike conventional 2D or 3D object detection, we employ a prediction approach to detect ground planes of vehicles as parallelograms in the image frame. The parallelogram preserves the planar position, size, and orientation of objects while omitting their height, which is unnecessary for most downstream applications. For training, a mix of real-world and synthetically generated scenes is leveraged. We evaluate generalizability on a held-out camera viewpoint and in adverse-weather scenarios absent from the training set. Our results show high detection accuracy, strong cross-viewpoint generalization, and robustness to diverse lighting and weather conditions. Model weights and inference code are provided at: <a target="_blank" rel="noopener" href="https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection">https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection</a> </p>
<blockquote>
<p>自动驾驶车辆的车载传感器可能会受到遮挡、视线受限或其他障碍物的干扰，从而增加下游驾驶决策的难度。安装在有利位置的智能路边基础设施感知系统可以提供宽阔、无遮挡的交叉路口覆盖范围，并通过车对万物（V2X）通信为自动驾驶车辆提供补充信息流。然而，传统的三维物体检测算法在自上而下视角和陡峭相机角度引入的域偏移下难以进行泛化。我们引入了一个针对基础设施路边安装的相机专门设计的2.5D物体检测框架。与传统的二维或三维物体检测不同，我们采用预测方法来检测图像帧中的车辆地面平面作为平行四边形。平行四边形保留了物体的平面位置、大小和方位，同时省略了高度信息，这在大多数下游应用中是不必要的。我们利用现实场景和合成场景的结合进行训练。我们在不包括训练集的另一相机视角和恶劣天气场景下评估泛化能力。我们的结果展示了较高的检测精度、较强的跨视角泛化能力以及在不同光线和天气条件下的鲁棒性。模型权重和推理代码可以在以下网址找到：<a target="_blank" rel="noopener" href="https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d%E7%8B%AC%E4%BD%BF%E7%9B%AE%E6%A0%BC%E7%BB%AA%E6%BA%90%E6%A3%A2">https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03564v2">PDF</a> Accepted at 2025 IEEE 28th International Conference on Intelligent   Transportation Systems (ITSC)</p>
<p><strong>Summary</strong>：</p>
<p>自主驾驶车辆的车载传感器可能会受到遮挡、视野受限等影响，导致驾驶决策困难。智能路边感知系统可以安装在高位观察点，提供无遮挡的交叉口覆盖，并通过车对万物（V2X）通信为自主驾驶车辆提供补充信息。然而，传统的3D目标检测算法在应对由顶部向下视角和陡峭相机角度引起的领域变化时难以实现通用化。为此，我们提出了一种2.5D目标检测框架，专门用于路边基础设施安装的相机。我们采用预测方法检测车辆的地面平面，将其表示为图像帧中的平行四边形。平行四边形保留了物体的平面位置、大小和方向，同时省略了高度信息，这对于大多数下游应用来说是不必要的。我们使用真实场景和合成场景的组合数据进行训练，并在未参与训练的相机视角和恶劣天气场景下评估其通用性和鲁棒性。结果显示，该方法的检测精度高、跨视角通用性强，对不同的光照和天气条件具有稳健性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自主驾驶车辆的车载传感器可能受到遮挡或视野限制，导致驾驶决策复杂化。</li>
<li>智能路边基础设施感知系统可提供无遮挡的交叉口视图，通过V2X通信为自主驾驶车辆提供补充信息。</li>
<li>传统3D目标检测算法在应对领域变化时难以实现通用化，需要特定的2.5D目标检测框架。</li>
<li>提出的2.5D目标检测框架采用预测方法检测车辆的地面平面，表示为图像帧中的平行四边形。</li>
<li>该框架具有高度的检测精度和跨视角的通用性。</li>
<li>框架对不同的光照和天气条件具有稳健性，能够在恶劣天气场景下进行有效检测。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03564">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62b91743d5080691b80e91c2df2c14a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67223e5f193dda2e1c8a72024e0d1799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b91c92af46ddf4c3e6e55cea0a1c0dd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-433052a0433b464385ce31e5b34be465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3e861f79fb08e85880808a093ecae95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2808eff91b338e87af50eac6510d3aed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e573c1c54c0d4f73e5b2beb04816251.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement"><a href="#UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement" class="headerlink" title="UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement"></a>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement</h2><p><strong>Authors:Xiao Zhang, Fei Wei, Yong Wang, Wenda Zhao, Feiyi Li, Xiangxiang Chu</strong></p>
<p>Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE">https://github.com/AMAP-ML/UPRE</a>. </p>
<blockquote>
<p>零样本域自适应（ZSDA）由于缺乏目标域的图像而面临巨大挑战。之前的方法利用视觉语言模型（VLMs）来解决这一挑战，利用其零样本学习能力。然而，这些方法主要解决域分布转移问题，而忽视了检测任务与VLMs之间的不匹配，后者依赖于手工制作的提示。为了克服这些局限性，我们提出了统一提示和表示增强（UPRE）框架，该框架联合优化文本提示和视觉表示。具体来说，我们的方法引入了一种多视角域提示，它将语言域先验知识与检测特定知识相结合，以及一种视觉表示增强模块，用于生成域风格变化。此外，我们引入了多层次增强策略，包括相对域距离和正负分离，分别在图像级别对齐多模式表示，并在实例级别捕获多样化的视觉表示。在九个基准数据集上进行的广泛实验表明，我们的框架在ZSDA检测场景中具有卓越的性能。代码可访问于 <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE%E3%80%82">https://github.com/AMAP-ML/UPRE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00721v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>文本介绍了零样本领域自适应（ZSDA）面临的挑战，尤其是目标领域缺乏图像的问题。为了解决这一问题，文章提出了一个统一的提示和表示增强（UPRE）框架，该框架联合优化了文本提示和视觉表示。UPRE框架通过引入多视角领域提示和视觉表示增强模块来解决领域分布变化和检测任务与视觉语言模型之间的不匹配问题。此外，还引入了多层次增强策略，包括相对领域距离和正负分离，以在图像级别对齐多模式表示并在实例级别捕获多样化的视觉表示。在九个基准数据集上的实验表明，UPRE框架在ZSDA检测场景中表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZSDA面临目标领域缺乏图像的挑战。</li>
<li>UPRE框架通过联合优化文本提示和视觉表示来解决这个问题。</li>
<li>UPRE引入多视角领域提示，结合语言领域先验知识和检测特定知识。</li>
<li>UPRE框架包含视觉表示增强模块，用于生成领域风格变化。</li>
<li>多层次增强策略包括相对领域距离和正负分离，用于对齐多模式表示并捕获多样化的视觉表示。</li>
<li>在九个基准数据集上的实验表明UPRE框架在ZSDA检测场景中性能卓越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00721">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-76420c74a7f52503f7ce343aecb151f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bbdd914ef07f98a2ab2538d3c00b878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7870f451037050b4697fea48b540040f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7831142b23aeab239215ec5295dd6afb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Position-Prediction-Self-Supervised-Learning-for-Multimodal-Satellite-Imagery-Semantic-Segmentation"><a href="#Position-Prediction-Self-Supervised-Learning-for-Multimodal-Satellite-Imagery-Semantic-Segmentation" class="headerlink" title="Position Prediction Self-Supervised Learning for Multimodal Satellite   Imagery Semantic Segmentation"></a>Position Prediction Self-Supervised Learning for Multimodal Satellite   Imagery Semantic Segmentation</h2><p><strong>Authors:John Waithaka, Moise Busogi</strong></p>
<p>Semantic segmentation of satellite imagery is crucial for Earth observation applications, but remains constrained by limited labelled training data. While self-supervised pretraining methods like Masked Autoencoders (MAE) have shown promise, they focus on reconstruction rather than localisation-a fundamental aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a position prediction self-supervised learning method, for multimodal satellite imagery semantic segmentation. Our approach addresses the unique challenges of satellite data by extending SatMAE’s channel grouping from multispectral to multimodal data, enabling effective handling of multiple modalities, and introducing same-group attention masking to encourage cross-modal interaction during pretraining. The method uses relative patch position prediction, encouraging spatial reasoning for localisation rather than reconstruction. We evaluate our approach on the Sen1Floods11 flood mapping dataset, where it significantly outperforms existing reconstruction-based self-supervised learning methods for satellite imagery. Our results demonstrate that position prediction tasks, when properly adapted for multimodal satellite imagery, learn representations more effective for satellite image semantic segmentation than reconstruction-based approaches. </p>
<blockquote>
<p>卫星图像的语义分割对于地球观测应用至关重要，但仍受限于标记训练数据的有限性。虽然像Masked Autoencoders（MAE）这样的自监督预训练方法在重建方面显示出潜力，但它们主要关注重建而非定位，这是分割任务的一个基本方面。我们提出适应LOCA（位置感知）方法，这是一种位置预测自监督学习方法，用于多模态卫星图像语义分割。我们的方法通过扩展SatMAE的渠道分组，从多光谱到多模态数据，解决了卫星数据的独特挑战，实现了多种模式的有效处理，并引入了同组注意力遮蔽，以鼓励在预训练期间进行跨模态交互。该方法使用相对斑块位置预测，鼓励定位的空间推理而非重建。我们在Sen1Floods11洪水测绘数据集上评估了我们的方法，在洪水测绘数据集上，我们的方法显著优于现有的基于重建的自监督学习方法。我们的结果表明，当适当适应多模态卫星图像时，位置预测任务对于卫星图像语义分割的代表性学习比基于重建的方法更有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06852v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>卫星图像语义分割对地球观测应用至关重要，但受限于标记训练数据的不足。虽然自监督预训练方法（如Mask Autoencoders）已显示出潜力，但它们主要关注重建而非定位——分割任务的基本方面。本文提出将位置感知的自监督学习方法LOCA应用于多模态卫星图像语义分割。该方法通过扩展SatMAE的渠道组合能力以处理多模态数据，引入了同一群体注意力屏蔽机制来鼓励在预训练期间的跨模态交互。此方法使用相对补丁位置预测，鼓励定位的空间推理而非重建。在Sen1Floods11洪水测绘数据集上的评估显示，该方法显著优于基于重建的现有自监督学习方法。结果表明，适当适应多模态卫星图像的定位预测任务，对于卫星图像语义分割而言，比基于重建的方法更有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>卫星图像语义分割是地球观测应用的关键挑战，主要受限于标记训练数据的缺乏。</li>
<li>自监督预训练方法如Mask Autoencoders虽然展现出潜力，但在解决定位问题上效果有限。</li>
<li>本文提出的LOCA方法是一种位置感知自监督学习方法，针对多模态卫星图像语义分割进行改进。</li>
<li>方法通过扩展SatMAE的渠道组合能力处理多模态数据，并引入跨模态交互机制。</li>
<li>采用相对补丁位置预测，强调空间推理和定位而非单纯的图像重建。</li>
<li>在Sen1Floods11数据集上的实验显示，LOCA方法显著优于传统的重建型自监督学习方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06852">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d18905a481d3fe347e608bb4538c2609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09b00547dd833be894dad6a2fad4543e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9a478c5aaf9107355e7ae07416b39236.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-08-03  The Impact of Image Resolution on Face Detection A Comparative Analysis   of MTCNN, YOLOv XI and YOLOv XII models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8577c124f2f2b957f9eb7ed8c1121359.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-03  UPRE Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
