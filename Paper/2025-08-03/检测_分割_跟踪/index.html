<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  MyGO Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-433052a0433b464385ce31e5b34be465.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="MyGO-Make-your-Goals-Obvious-Avoiding-Semantic-Confusion-in-Prostate-Cancer-Lesion-Region-Segmentation"><a href="#MyGO-Make-your-Goals-Obvious-Avoiding-Semantic-Confusion-in-Prostate-Cancer-Lesion-Region-Segmentation" class="headerlink" title="MyGO: Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation"></a>MyGO: Make your Goals Obvious, Avoiding Semantic Confusion in Prostate   Cancer Lesion Region Segmentation</h2><p><strong>Authors:Zhengcheng Lin, Zuobin Ying, Zhenyu Li, Zhenyu Liu, Jian Lu, Weiping Ding</strong></p>
<p>Early diagnosis and accurate identification of lesion location and progression in prostate cancer (PCa) are critical for assisting clinicians in formulating effective treatment strategies. However, due to the high semantic homogeneity between lesion and non-lesion areas, existing medical image segmentation methods often struggle to accurately comprehend lesion semantics, resulting in the problem of semantic confusion. To address this challenge, we propose a novel Pixel Anchor Module, which guides the model to discover a sparse set of feature anchors that serve to capture and interpret global contextual information. This mechanism enhances the modelâ€™s nonlinear representation capacity and improves segmentation accuracy within lesion regions. Moreover, we design a self-attention-based Top_k selection strategy to further refine the identification of these feature anchors, and incorporate a focal loss function to mitigate class imbalance, thereby facilitating more precise semantic interpretation across diverse regions. Our method achieves state-of-the-art performance on the PI-CAI dataset, demonstrating 69.73% IoU and 74.32% Dice scores, and significantly improving prostate cancer lesion detection. </p>
<blockquote>
<p>æ—©æœŸè¯Šæ–­ä»¥åŠå‡†ç¡®è¯†åˆ«å‰åˆ—è…ºç™Œï¼ˆPCaï¼‰çš„ç—…å˜ä½ç½®å’Œè¿›å±•å¯¹äºå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿåˆ¶å®šæœ‰æ•ˆçš„æ²»ç–—ç­–ç•¥è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç—…å˜åŒºåŸŸä¸éç—…å˜åŒºåŸŸä¹‹é—´å­˜åœ¨é«˜åº¦çš„è¯­ä¹‰åŒè´¨æ€§ï¼Œç°æœ‰çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•å¾€å¾€éš¾ä»¥å‡†ç¡®ç†è§£ç—…å˜è¯­ä¹‰ï¼Œä»è€Œå¯¼è‡´è¯­ä¹‰æ··æ·†çš„é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Pixel Anchor Moduleï¼Œè¯¥æ¨¡å—å¼•å¯¼æ¨¡å‹å‘ç°ä¸€ç»„ç¨€ç–çš„ç‰¹å¾é”šç‚¹ï¼Œç”¨äºæ•è·å’Œè§£é‡Šå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ç§æœºåˆ¶å¢å¼ºäº†æ¨¡å‹çš„éçº¿æ€§è¡¨ç¤ºèƒ½åŠ›ï¼Œæé«˜äº†ç—…å˜åŒºåŸŸå†…çš„åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›çš„Top_ké€‰æ‹©ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æ”¹è¿›è¿™äº›ç‰¹å¾é”šç‚¹çš„è¯†åˆ«ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç„¦ç‚¹æŸå¤±å‡½æ•°æ¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä»è€Œåœ¨ä¸åŒåŒºåŸŸå®ç°æ›´ç²¾ç¡®çš„è¯­ä¹‰è§£é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨PI-CAIæ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°äº†69.73%çš„IoUå’Œ74.32%çš„Diceç³»æ•°ï¼Œæ˜¾è‘—æé«˜äº†å‰åˆ—è…ºç™Œç—…å˜çš„æ£€æµ‹æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17269v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPixel Anchor Moduleçš„æ–°æ–¹æ³•ï¼Œç”¨äºæé«˜å‰åˆ—è…ºç™Œç—…å˜æ£€æµ‹å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡å‘ç°ç¨€ç–çš„ç‰¹å¾é”šç‚¹æ¥æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºæ¨¡å‹çš„éçº¿æ€§è¡¨ç¤ºèƒ½åŠ›ï¼Œä»è€Œæé«˜ç—…å˜åŒºåŸŸçš„åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†åŸºäºè‡ªæ³¨æ„åŠ›çš„Top_ké€‰æ‹©ç­–ç•¥æ¥ä¼˜åŒ–ç‰¹å¾é”šç‚¹çš„è¯†åˆ«ï¼Œå¹¶ç»“åˆfocal losså‡½æ•°ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„åŒºåŸŸè¯­ä¹‰è§£é‡Šã€‚è¯¥æ–¹æ³•åœ¨PI-CAIæ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆæ°´å¹³ï¼Œè¾¾åˆ°69.73%çš„IoUå’Œ74.32%çš„Diceå¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pixel Anchor Moduleè¢«æå‡ºç”¨äºè§£å†³å‰åˆ—è…ºç™Œç—…å˜æ£€æµ‹ä¸­çš„è¯­ä¹‰æ··æ·†é—®é¢˜ã€‚</li>
<li>é€šè¿‡å‘ç°ç¨€ç–çš„ç‰¹å¾é”šç‚¹æ¥æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>è®¾è®¡çš„Top_ké€‰æ‹©ç­–ç•¥èƒ½ä¼˜åŒ–ç‰¹å¾é”šç‚¹çš„è¯†åˆ«ã€‚</li>
<li>ç»“åˆfocal losså‡½æ•°æ¥ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>æ–¹æ³•åœ¨PI-CAIæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>è¾¾åˆ°69.73%çš„IoUå’Œ74.32%çš„Diceå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e56733667d851890e8c75d544fb75d9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba402605984cb8d11d92f35395e64c07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2da6d44bf828dc8d4128ee8006a4ebbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70708e59a78ba0e78512ea81056260de.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ScSAM-Debiasing-Morphology-and-Distributional-Variability-in-Subcellular-Semantic-Segmentation"><a href="#ScSAM-Debiasing-Morphology-and-Distributional-Variability-in-Subcellular-Semantic-Segmentation" class="headerlink" title="ScSAM: Debiasing Morphology and Distributional Variability in   Subcellular Semantic Segmentation"></a>ScSAM: Debiasing Morphology and Distributional Variability in   Subcellular Semantic Segmentation</h2><p><strong>Authors:Bo Fang, Jianan Fan, Dongnan Liu, Hang Chang, Gerald J. Shami, Filip Braet, Weidong Cai</strong></p>
<p>The significant morphological and distributional variability among subcellular components poses a long-standing challenge for learning-based organelle segmentation models, significantly increasing the risk of biased feature learning. Existing methods often rely on single mapping relationships, overlooking feature diversity and thereby inducing biased training. Although the Segment Anything Model (SAM) provides rich feature representations, its application to subcellular scenarios is hindered by two key challenges: (1) The variability in subcellular morphology and distribution creates gaps in the label space, leading the model to learn spurious or biased features. (2) SAM focuses on global contextual understanding and often ignores fine-grained spatial details, making it challenging to capture subtle structural alterations and cope with skewed data distributions. To address these challenges, we introduce ScSAM, a method that enhances feature robustness by fusing pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge to alleviate training bias from data imbalance. Specifically, we design a feature alignment and fusion module to align pre-trained embeddings to the same feature space and efficiently combine different representations. Moreover, we present a cosine similarity matrix-based class prompt encoder to activate class-specific features to recognize subcellular categories. Extensive experiments on diverse subcellular image datasets demonstrate that ScSAM outperforms state-of-the-art methods. </p>
<blockquote>
<p>ç»†èƒäºšç»„åˆ†ä¹‹é—´æ˜¾è‘—çš„å½¢æ€å’Œåˆ†å¸ƒå˜åŒ–ç»™åŸºäºå­¦ä¹ çš„ç»†èƒå™¨åˆ†å‰²æ¨¡å‹å¸¦æ¥äº†é•¿æœŸæŒ‘æˆ˜ï¼Œæ˜¾è‘—å¢åŠ äº†ç‰¹å¾å­¦ä¹ åå‘çš„é£é™©ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå•ä¸€çš„æ˜ å°„å…³ç³»ï¼Œå¿½è§†äº†ç‰¹å¾çš„å¤šæ ·æ€§ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒåå‘ã€‚å°½ç®¡Segment Anything Modelï¼ˆSAMï¼‰æä¾›äº†ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºï¼Œä½†å…¶åœ¨äºšç»†èƒåœºæ™¯ä¸­çš„åº”ç”¨å—åˆ°ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜çš„åˆ¶çº¦ï¼šï¼ˆ1ï¼‰äºšç»†èƒå½¢æ€å’Œåˆ†å¸ƒçš„å¤šæ ·æ€§å¯¼è‡´æ ‡ç­¾ç©ºé—´ä¸­å­˜åœ¨é—´éš™ï¼Œä½¿æ¨¡å‹å­¦ä¹ ç‰¹å¾æ—¶å‡ºç°é”™è¯¯æˆ–åå‘ã€‚ï¼ˆ2ï¼‰SAMä¾§é‡äºå…¨å±€ä¸Šä¸‹æ–‡ç†è§£ï¼Œå¾€å¾€å¿½è§†ç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œè¿™ä½¿å¾—æ•æ‰å¾®å¦™çš„ç»“æ„å˜åŒ–å’Œåº”å¯¹æ•°æ®åˆ†å¸ƒä¸å‡çš„æƒ…å†µå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ScSAMæ–¹æ³•ï¼Œå®ƒé€šè¿‡èåˆé¢„è®­ç»ƒçš„SAMå’Œç”±Masked Autoencoderï¼ˆMAEï¼‰å¼•å¯¼çš„ç»†èƒå…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºç‰¹å¾ç¨³å¥æ€§ï¼Œä»¥å‡è½»å› æ•°æ®ä¸å¹³è¡¡å¯¼è‡´çš„è®­ç»ƒåå·®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰¹å¾å¯¹é½å’Œèåˆæ¨¡å—ï¼Œå°†é¢„è®­ç»ƒåµŒå…¥å¯¹é½åˆ°åŒä¸€ç‰¹å¾ç©ºé—´ï¼Œå¹¶æœ‰æ•ˆåœ°ç»“åˆä¸åŒçš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µçš„ç±»æç¤ºç¼–ç å™¨ï¼Œä»¥æ¿€æ´»ç‰¹å®šç±»åˆ«çš„ç‰¹å¾æ¥è¯†åˆ«äºšç»†èƒç±»åˆ«ã€‚åœ¨å¤šç§äºšç»†èƒå›¾åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒScSAMä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17149v1">PDF</a> Accepted by 28th European Conference on Artificial Intelligence   (ECAI)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºäº†äºšç»†èƒç»“æ„å½¢æ€å’Œåˆ†å¸ƒå¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œå¯¹åŸºäºå­¦ä¹ çš„ç»†èƒå™¨åˆ†å‰²æ¨¡å‹é€ æˆäº†å½±å“ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸ä¾èµ–å•ä¸€æ˜ å°„å…³ç³»ï¼Œå¿½ç•¥äº†ç‰¹å¾å¤šæ ·æ€§ï¼Œå¯¼è‡´è®­ç»ƒåå·®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ScSAMæ–¹æ³•ï¼Œé€šè¿‡èåˆé¢„è®­ç»ƒçš„Segment Anything Modelï¼ˆSAMï¼‰å’ŒMasked Autoencoderï¼ˆMAEï¼‰å¼•å¯¼çš„ç»†èƒå…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºç‰¹å¾ç¨³å¥æ€§ï¼Œä»¥å‡è½»å› æ•°æ®ä¸å¹³è¡¡å¯¼è‡´çš„è®­ç»ƒåå·®ã€‚å®éªŒè¯æ˜ï¼ŒScSAMåœ¨å¤šç§äºšç»†èƒå›¾åƒæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äºšç»†èƒç»“æ„çš„å½¢æ€å’Œåˆ†å¸ƒå¤šæ ·æ€§å¯¹åŸºäºå­¦ä¹ çš„åˆ†å‰²æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å› ä¾èµ–å•ä¸€æ˜ å°„å…³ç³»è€Œå¿½è§†ç‰¹å¾å¤šæ ·æ€§ï¼Œæ˜“å¯¼è‡´è®­ç»ƒåå·®ã€‚</li>
<li>Segment Anything Modelï¼ˆSAMï¼‰è™½æä¾›ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºï¼Œä½†åœ¨äºšç»†èƒåœºæ™¯ä¸‹åº”ç”¨å—é™ã€‚</li>
<li>ScSAMæ–¹æ³•é€šè¿‡èåˆé¢„è®­ç»ƒçš„SAMå’ŒMAEå¼•å¯¼çš„ç»†èƒå…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºç‰¹å¾ç¨³å¥æ€§ã€‚</li>
<li>ScSAMè®¾è®¡ç‰¹å¾å¯¹é½å’Œèåˆæ¨¡å—ï¼Œå°†é¢„è®­ç»ƒåµŒå…¥å¯¹é½åˆ°åŒä¸€ç‰¹å¾ç©ºé—´å¹¶æœ‰æ•ˆç»“åˆä¸åŒè¡¨ç¤ºã€‚</li>
<li>ScSAMé‡‡ç”¨ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µçš„ç±»æç¤ºç¼–ç å™¨ï¼Œæ¿€æ´»ç±»ç‰¹å®šç‰¹å¾ä»¥è¯†åˆ«äºšç»†èƒç±»åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6e133e37635680fcd1dcfded9c9a19d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ddaa4dfae9860e61717fdaf1974fd7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8895a281ceb9b97d3f48452752907ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-356911f312d8922c7a8397fec588221c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c898ed0ab011bcd75326217d6d05cc26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a75e0879f5d14495c19b729f67c70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8154aa09912cba45209da51d0bf15ad3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Zero-shot-Quantization-Aware-Training-for-Object-Detection"><a href="#Task-Specific-Zero-shot-Quantization-Aware-Training-for-Object-Detection" class="headerlink" title="Task-Specific Zero-shot Quantization-Aware Training for Object Detection"></a>Task-Specific Zero-shot Quantization-Aware Training for Object Detection</h2><p><strong>Authors:Changhao Li, Xinrui Chen, Ji Wang, Kang Zhao, Jianfei Chen</strong></p>
<p>Quantization is a key technique to reduce network size and computational complexity by representing the network parameters with a lower precision. Traditional quantization methods rely on access to original training data, which is often restricted due to privacy concerns or security challenges. Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated from pre-trained models, eliminating the need for real training data. Recently, ZSQ has been extended to object detection. However, existing methods use unlabeled task-agnostic synthetic images that lack the specific information required for object detection, leading to suboptimal performance. In this paper, we propose a novel task-specific ZSQ framework for object detection networks, which consists of two main stages. First, we introduce a bounding box and category sampling strategy to synthesize a task-specific calibration set from the pre-trained network, reconstructing object locations, sizes, and category distributions without any prior knowledge. Second, we integrate task-specific training into the knowledge distillation process to restore the performance of quantized detection networks. Extensive experiments conducted on the MS-COCO and Pascal VOC datasets demonstrate the efficiency and state-of-the-art performance of our method. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/DFQ-Dojo/dfq-toolkit">https://github.com/DFQ-Dojo/dfq-toolkit</a> . </p>
<blockquote>
<p>é‡åŒ–æ˜¯ä¸€ç§é€šè¿‡ç”¨è¾ƒä½çš„ç²¾åº¦è¡¨ç¤ºç½‘ç»œå‚æ•°æ¥å‡å°ç½‘ç»œè§„æ¨¡å’Œè®¡ç®—å¤æ‚æ€§çš„å…³é”®æŠ€æœ¯ã€‚ä¼ ç»Ÿé‡åŒ–æ–¹æ³•ä¾èµ–äºåŸå§‹è®­ç»ƒæ•°æ®çš„è®¿é—®ï¼Œè¿™é€šå¸¸ç”±äºéšç§æ‹…å¿§æˆ–å®‰å…¨æŒ‘æˆ˜è€Œå—åˆ°é™åˆ¶ã€‚é›¶å°„é‡åŒ–ï¼ˆZSQï¼‰é€šè¿‡ä½¿ç”¨é¢„å…ˆè®­ç»ƒæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ— éœ€ä½¿ç”¨çœŸå®è®­ç»ƒæ•°æ®ã€‚æœ€è¿‘ï¼ŒZSQå·²æ‰©å±•åˆ°ç›®æ ‡æ£€æµ‹é¢†åŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä½¿ç”¨æœªæ ‡è®°çš„ä»»åŠ¡æ— å…³åˆæˆå›¾åƒï¼Œç¼ºä¹ç›®æ ‡æ£€æµ‹æ‰€éœ€çš„å…·ä½“ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç›®æ ‡æ£€æµ‹ç½‘ç»œçš„æ–°å‹ä»»åŠ¡ç‰¹å®šZSQæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾¹ç•Œæ¡†å’Œç±»åˆ«é‡‡æ ·ç­–ç•¥ï¼Œä»¥ä»é¢„è®­ç»ƒç½‘ç»œåˆæˆä»»åŠ¡ç‰¹å®šæ ¡å‡†é›†ï¼Œé‡å»ºå¯¹è±¡ä½ç½®ã€å¤§å°å’Œç±»åˆ«åˆ†å¸ƒï¼Œæ— éœ€ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡ç‰¹å®šè®­ç»ƒé›†æˆåˆ°çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­ï¼Œä»¥æ¢å¤é‡åŒ–æ£€æµ‹ç½‘ç»œçš„æ€§èƒ½ã€‚åœ¨MS-COCOå’ŒPascal VOCæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ•ˆç‡å’Œæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/DFQ-Dojo/dfq-toolkit%E3%80%82]">https://github.com/DFQ-Dojo/dfq-toolkitã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16782v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>é‡åŒ–æ˜¯ç½‘ç»œç¼©å‡å’Œé™ä½è®¡ç®—å¤æ‚åº¦çš„é‡è¦æŠ€æœ¯ï¼Œå®ƒé€šè¿‡ä½ç²¾åº¦è¡¨ç¤ºç½‘ç»œå‚æ•°æ¥å®ç°ã€‚ä¼ ç»Ÿé‡åŒ–æ–¹æ³•ä¾èµ–åŸå§‹è®­ç»ƒæ•°æ®ï¼Œä½†å‡ºäºéšç§æˆ–å®‰å…¨è€ƒè™‘ï¼Œè¿™äº›æ•°æ®å¾€å¾€å—åˆ°é™åˆ¶ã€‚é›¶æ ·æœ¬é‡åŒ–ï¼ˆZSQï¼‰é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ— éœ€çœŸå®è®­ç»ƒæ•°æ®ã€‚æœ€æ–°ç ”ç©¶å°†ZSQæ‰©å±•åˆ°ç›®æ ‡æ£€æµ‹é¢†åŸŸï¼Œä½†ç°æœ‰æ–¹æ³•ä½¿ç”¨æ— æ ‡ç­¾çš„ä»»åŠ¡æ— å…³åˆæˆå›¾åƒï¼Œç¼ºä¹ç›®æ ‡æ£€æµ‹æ‰€éœ€çš„å…·ä½“ä¿¡æ¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç›®æ ‡æ£€æµ‹ç½‘ç»œçš„æ–°å‹ä»»åŠ¡ç‰¹å®šZSQæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œé€šè¿‡é¢„è®­ç»ƒç½‘ç»œåˆæˆä»»åŠ¡ç‰¹å®šæ ¡å‡†é›†ï¼Œé‡æ„ç›®æ ‡ä½ç½®ã€å¤§å°å’Œç±»åˆ«åˆ†å¸ƒï¼Œæ— éœ€ä»»ä½•å…ˆéªŒçŸ¥è¯†ï¼›å…¶æ¬¡ï¼Œå°†ä»»åŠ¡ç‰¹å®šè®­ç»ƒé›†æˆåˆ°çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­ï¼Œä»¥æ¢å¤é‡åŒ–æ£€æµ‹ç½‘ç»œçš„æ€§èƒ½ã€‚åœ¨MS-COCOå’ŒPascal VOCæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æ•ˆç‡å’Œæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–æŠ€æœ¯ç”¨äºå‡å°‘ç½‘ç»œå¤§å°å’Œè®¡ç®—å¤æ‚åº¦ï¼Œé€šè¿‡ä½ç²¾åº¦è¡¨ç¤ºç½‘ç»œå‚æ•°å®ç°ã€‚</li>
<li>ä¼ ç»Ÿé‡åŒ–æ–¹æ³•ä¾èµ–åŸå§‹è®­ç»ƒæ•°æ®ï¼Œä½†å­˜åœ¨éšç§æˆ–å®‰å…¨é™åˆ¶ã€‚</li>
<li>é›¶æ ·æœ¬é‡åŒ–ï¼ˆZSQï¼‰é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®è§£å†³æ­¤é—®é¢˜ï¼Œæ— éœ€çœŸå®è®­ç»ƒæ•°æ®ã€‚</li>
<li>æœ€æ–°ZSQç ”ç©¶å·²æ‰©å±•åˆ°ç›®æ ‡æ£€æµ‹é¢†åŸŸï¼Œä½†ç°æœ‰æ–¹æ³•ä½¿ç”¨æ— æ ‡ç­¾çš„ä»»åŠ¡æ— å…³åˆæˆå›¾åƒï¼Œæ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºæ–°å‹ä»»åŠ¡ç‰¹å®šZSQæ¡†æ¶ï¼Œåˆ†ä¸ºåˆæˆä»»åŠ¡ç‰¹å®šæ ¡å‡†é›†å’Œé›†æˆä»»åŠ¡ç‰¹å®šè®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿé‡æ„ç›®æ ‡ä½ç½®ã€å¤§å°å’Œç±»åˆ«åˆ†å¸ƒï¼Œæ— éœ€ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4cab5b833569039d618fc13450425da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-442ee2b0776dcff4acc3bdba1908922b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa9640a1519bb6259a4b4caaae53ca2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-246f2f03180a3c413dfd28bb056b22fb.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-for-Preoperative-Planning-in-Transcatheter-Aortic-Valve-Replacement"><a href="#Semantic-Segmentation-for-Preoperative-Planning-in-Transcatheter-Aortic-Valve-Replacement" class="headerlink" title="Semantic Segmentation for Preoperative Planning in Transcatheter Aortic   Valve Replacement"></a>Semantic Segmentation for Preoperative Planning in Transcatheter Aortic   Valve Replacement</h2><p><strong>Authors:Cedric ZÃ¶llner, Simon ReiÃŸ, Alexander Jaus, Amroalalaa Sholi, Ralf Sodian, Rainer Stiefelhagen</strong></p>
<p>When preoperative planning for surgeries is conducted on the basis of medical images, artificial intelligence methods can support medical doctors during assessment. In this work, we consider medical guidelines for preoperative planning of the transcatheter aortic valve replacement (TAVR) and identify tasks, that may be supported via semantic segmentation models by making relevant anatomical structures measurable in computed tomography scans. We first derive fine-grained TAVR-relevant pseudo-labels from coarse-grained anatomical information, in order to train segmentation models and quantify how well they are able to find these structures in the scans. Furthermore, we propose an adaptation to the loss function in training these segmentation models and through this achieve a +1.27% Dice increase in performance. Our fine-grained TAVR-relevant pseudo-labels and the computed tomography scans we build upon are available at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.16274176">https://doi.org/10.5281/zenodo.16274176</a>. </p>
<blockquote>
<p>åœ¨åŸºäºåŒ»å­¦å›¾åƒè¿›è¡Œæ‰‹æœ¯æœ¯å‰è§„åˆ’æ—¶ï¼Œäººå·¥æ™ºèƒ½æ–¹æ³•å¯ä»¥åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä¸ºåŒ»ç”Ÿæä¾›æ”¯æŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ç»å¯¼ç®¡ä¸»åŠ¨è„‰ç“£ç½®æ¢æœ¯ï¼ˆTAVRï¼‰çš„æœ¯å‰è§„åˆ’åŒ»å­¦æŒ‡å—ï¼Œå¹¶ç¡®å®šäº†å¯é€šè¿‡è¯­ä¹‰åˆ†å‰²æ¨¡å‹æ”¯æŒçš„ä»»åŠ¡ï¼Œé€šè¿‡åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æä¸­æµ‹é‡ç›¸å…³è§£å‰–ç»“æ„æ¥å®ç°ã€‚æˆ‘ä»¬é¦–å…ˆä»ç²—ç²’åº¦çš„è§£å‰–ä¿¡æ¯ä¸­æ¨å¯¼å‡ºç²¾ç»†ç²’åº¦çš„TAVRç›¸å…³ä¼ªæ ‡ç­¾ï¼Œä»¥è®­ç»ƒåˆ†å‰²æ¨¡å‹å¹¶é‡åŒ–å®ƒä»¬åœ¨æ‰«æä¸­å‘ç°è¿™äº›ç»“æ„çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒè¿™äº›åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°è¿›è¡Œäº†è°ƒæ•´ï¼Œå¹¶å› æ­¤å®ç°äº†æ€§èƒ½ä¸Š1.27%çš„Diceå¢é•¿ã€‚æˆ‘ä»¬æ„å»ºçš„ç²¾ç»†ç²’åº¦TAVRç›¸å…³ä¼ªæ ‡ç­¾å’Œè®¡ç®—æœºæ–­å±‚æ‰«ææ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.16274176%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://doi.org/10.5281/zenodo.16274176ä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16573v1">PDF</a> Accepted at 16th MICCAI Workshop on Statistical Atlases and   Computational Modeling of the Heart (STACOM)</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨åœ¨æœ¯å‰è§„åˆ’ä¸­è¿ç”¨äººå·¥æ™ºèƒ½è¾…åŠ©åŒ»å­¦åŒ»ç”Ÿè¿›è¡Œç»å¯¼ç®¡ä¸»åŠ¨è„‰ç“£ç½®æ¢æœ¯ï¼ˆTAVRï¼‰çš„è¯„ä¼°ã€‚é€šè¿‡ä»ç²—é¢—ç²’åº¦çš„è§£å‰–ä¿¡æ¯ä¸­è¡ç”Ÿå‡ºç²¾ç»†é¢—ç²’åº¦çš„TAVRç›¸å…³ä¼ªæ ‡ç­¾ï¼Œè®­ç»ƒåˆ†å‰²æ¨¡å‹ä»¥é‡åŒ–å…¶åœ¨æ‰«æä¸­å®šä½ç›¸å…³ç»“æ„çš„èƒ½åŠ›ã€‚åŒæ—¶æå‡ºè°ƒæ•´æŸå¤±å‡½æ•°æ¥è®­ç»ƒè¿™äº›åˆ†å‰²æ¨¡å‹ï¼Œä»è€Œå®ç°æ€§èƒ½ä¸Šçš„æ”¹è¿›ã€‚æ•°æ®é›†å¯åœ¨æ­¤å¤„è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨æœ¯å‰è§„åˆ’ä¸­å¯¹åŒ»å­¦å›¾åƒçš„åˆ†ææ”¯æŒåŒ»ç”Ÿè¿›è¡Œæ‰‹æœ¯è¯„ä¼°ã€‚</li>
<li>é€šè¿‡ç²¾ç»†é¢—ç²’åº¦çš„TAVRç›¸å…³ä¼ªæ ‡ç­¾è®­ç»ƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨è¿™äº›æ ‡ç­¾åœ¨è®¡ç®—æœºæ–­å±‚æ‰«æä¸­æ‰¾åˆ°ç›¸å…³è§£å‰–ç»“æ„ã€‚</li>
<li>è°ƒæ•´æŸå¤±å‡½æ•°ä»¥å¢å¼ºåˆ†å‰²æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å®ç°æ›´é«˜çš„Diceç³»æ•°å¾—åˆ†ã€‚</li>
<li>è¯¥ç ”ç©¶çš„æ•°æ®é›†å¯ä¾›å…¬å¼€è®¿é—®å’Œä½¿ç”¨ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰åŠ©äºæé«˜æœ¯å‰è§„åˆ’çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0362a2708b24a3196fd2ed0d562773e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5632015d038fda2ff8a7e5643ce5aecb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35ea67fb0f6c2ab4507a2884f38e7c79.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd3d353fe1c0b9b8e6f42b3b1a21c293.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PlantSAM-An-Object-Detection-Driven-Segmentation-Pipeline-for-Herbarium-Specimens"><a href="#PlantSAM-An-Object-Detection-Driven-Segmentation-Pipeline-for-Herbarium-Specimens" class="headerlink" title="PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium   Specimens"></a>PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium   Specimens</h2><p><strong>Authors:Youcef Sklab, Florian Castanet, Hanane Ariouat, Souhila Arib, Jean-Daniel Zucker, Eric Chenin, Edi Prifti</strong></p>
<p>Deep learning-based classification of herbarium images is hampered by background heterogeneity, which introduces noise and artifacts that can potentially mislead models and reduce classification accuracy. Addressing these background-related challenges is critical to improving model performance. We introduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10 for plant region detection and the Segment Anything Model (SAM2) for segmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing segmentation accuracy. Both models were fine-tuned on herbarium images and evaluated using Intersection over Union (IoU) and Dice coefficient metrics. PlantSAM achieved state-of-the-art segmentation performance, with an IoU of 0.94 and a Dice coefficient of 0.97. Incorporating segmented images into classification models led to consistent performance improvements across five tested botanical traits, with accuracy gains of up to 4.36% and F1-score improvements of 4.15%. Our findings highlight the importance of background removal in herbarium image analysis, as it significantly enhances classification accuracy by allowing models to focus more effectively on the foreground plant structures. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ¤ç‰©æ ‡æœ¬å›¾åƒåˆ†ç±»å—åˆ°èƒŒæ™¯å¼‚è´¨æ€§çš„é˜»ç¢ï¼ŒèƒŒæ™¯å¼‚è´¨æ€§å¼•å…¥äº†å™ªå£°å’Œä¼ªå½±ï¼Œè¿™äº›å¯èƒ½ä¼šè¯¯å¯¼æ¨¡å‹å¹¶é™ä½åˆ†ç±»å‡†ç¡®æ€§ã€‚è§£å†³è¿™äº›ä¸èƒŒæ™¯ç›¸å…³çš„æŒ‘æˆ˜å¯¹äºæé«˜æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†PlantSAMï¼Œè¿™æ˜¯ä¸€ä¸ªé›†æˆäº†YOLOv10ç”¨äºæ¤ç‰©åŒºåŸŸæ£€æµ‹å’ŒSegment Anything Modelï¼ˆSAM2ï¼‰ç”¨äºåˆ†å‰²çš„è‡ªåŠ¨åŒ–åˆ†å‰²ç®¡é“ã€‚YOLOv10ç”Ÿæˆè¾¹ç•Œæ¡†æç¤ºæ¥å¼•å¯¼SAM2ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚ä¸¤ä¸ªæ¨¡å‹éƒ½åœ¨æ¤ç‰©æ ‡æœ¬å›¾åƒä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶ä½¿ç”¨äº¤é›†æ¯”ï¼ˆIoUï¼‰å’ŒDiceç³»æ•°æŒ‡æ ‡è¿›è¡Œäº†è¯„ä¼°ã€‚PlantSAMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼ŒIoUä¸º0.94ï¼ŒDiceç³»æ•°ä¸º0.97ã€‚å°†åˆ†å‰²å›¾åƒçº³å…¥åˆ†ç±»æ¨¡å‹å¯¼è‡´äº†äº”ç§æµ‹è¯•çš„æ¤ç‰©ç‰¹å¾æ€§èƒ½çš„ä¸€è‡´æ€§æé«˜ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾4.36%ï¼ŒF1åˆ†æ•°æé«˜äº†4.15%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ¤ç‰©æ ‡æœ¬å›¾åƒåˆ†æä¸­å»é™¤èƒŒæ™¯çš„é‡è¦æ€§ï¼Œå› ä¸ºå®ƒå¯ä»¥æ˜¾è‘—æé«˜åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°ä¸“æ³¨äºå‰æ™¯æ¤ç‰©ç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16506v1">PDF</a> 19 pages, 11 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„æ¤ç‰©æ ‡æœ¬å›¾åƒåˆ†ç±»å—åˆ°èƒŒæ™¯å¼‚è´¨æ€§çš„å¹²æ‰°ï¼Œè¿™å¼•å…¥äº†å™ªå£°å’Œä¼ªå½±ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹è¯¯å¯¼å¹¶é™ä½åˆ†ç±»å‡†ç¡®æ€§ã€‚ä¸ºè§£å†³èƒŒæ™¯ç›¸å…³çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PlantSAMè‡ªåŠ¨åŒ–åˆ†å‰²ç®¡é“ï¼Œé›†æˆäº†YOLOv10ç”¨äºæ¤ç‰©åŒºåŸŸæ£€æµ‹å’ŒSegment Anything Modelï¼ˆSAM2ï¼‰è¿›è¡Œåˆ†å‰²ã€‚YOLOv10ç”Ÿæˆè¾¹ç•Œæ¡†æç¤ºæ¥å¼•å¯¼SAM2ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹éƒ½åœ¨æ¤ç‰©æ ‡æœ¬å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ä½¿ç”¨äº¤é›†æ¯”ï¼ˆIoUï¼‰å’ŒDiceç³»æ•°æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚PlantSAMè¾¾åˆ°äº†å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼ŒIoUä¸º0.94ï¼ŒDiceç³»æ•°ä¸º0.97ã€‚å°†åˆ†å‰²å›¾åƒçº³å…¥åˆ†ç±»æ¨¡å‹å¯¼è‡´åœ¨äº”ä¸ªæµ‹è¯•çš„æ¤ç‰©å­¦ç‰¹å¾ä¸Šæ€§èƒ½ä¸€è‡´æé«˜ï¼Œå‡†ç¡®ç‡æé«˜é«˜è¾¾4.36%ï¼ŒF1åˆ†æ•°æé«˜4.15%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒèƒŒæ™¯å»é™¤åœ¨æ¤ç‰©æ ‡æœ¬å›¾åƒåˆ†æä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥æ˜¾è‘—æé«˜åˆ†ç±»å‡†ç¡®æ€§ï¼Œä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°å…³æ³¨å‰æ™¯æ¤ç‰©ç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¤ç‰©æ ‡æœ¬å›¾åƒåˆ†ç±»é¢ä¸´èƒŒæ™¯å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>èƒŒæ™¯å¼‚è´¨æ€§å¯èƒ½å¯¼è‡´æ¨¡å‹è¯¯å¯¼å’Œé™ä½åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>PlantSAMæ˜¯ä¸€ä¸ªé›†æˆäº†YOLOv10å’ŒSAM2æ¨¡å‹çš„è‡ªåŠ¨åŒ–åˆ†å‰²ç®¡é“ï¼Œç”¨äºè§£å†³èƒŒæ™¯å¹²æ‰°é—®é¢˜ã€‚</li>
<li>PlantSAMåœ¨æ¤ç‰©æ ‡æœ¬å›¾åƒä¸Šå®ç°äº†é«˜åˆ†å‰²æ€§èƒ½ï¼ŒIoUè¾¾åˆ°0.94ï¼ŒDiceç³»æ•°è¾¾åˆ°0.97ã€‚</li>
<li>å°†åˆ†å‰²å›¾åƒçº³å…¥åˆ†ç±»æ¨¡å‹å¯ä»¥æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>èƒŒæ™¯å»é™¤åœ¨æ¤ç‰©æ ‡æœ¬å›¾åƒåˆ†æä¸­è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f03b09543b88a9c949f2d42ae0e2f79d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8272c95ffafc6f5a57380b1b1e32c6eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16964d6732adfe406a8f017033e5cb37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8599bfad8e79d05e2224e0d4fbd713e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66df0f3b49d27b5b41ae2606fa070c70.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improved-Semantic-Segmentation-from-Ultra-Low-Resolution-RGB-Images-Applied-to-Privacy-Preserving-Object-Goal-Navigation"><a href="#Improved-Semantic-Segmentation-from-Ultra-Low-Resolution-RGB-Images-Applied-to-Privacy-Preserving-Object-Goal-Navigation" class="headerlink" title="Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images   Applied to Privacy-Preserving Object-Goal Navigation"></a>Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images   Applied to Privacy-Preserving Object-Goal Navigation</h2><p><strong>Authors:Xuying Huang, Sicong Pan, Olga Zatsarynna, Juergen Gall, Maren Bennewitz</strong></p>
<p>User privacy in mobile robotics has become a critical concern. Existing methods typically prioritize either the performance of downstream robotic tasks or privacy protection, with the latter often constraining the effectiveness of task execution. To jointly address both objectives, we study semantic-based robot navigation in an ultra-low-resolution setting to preserve visual privacy. A key challenge in such scenarios is recovering semantic segmentation from ultra-low-resolution RGB images. In this work, we introduce a novel fully joint-learning method that integrates an agglomerative feature extractor and a segmentation-aware discriminator to solve ultra-low-resolution semantic segmentation, thereby enabling privacy-preserving, semantic object-goal navigation. Our method outperforms different baselines on ultra-low-resolution semantic segmentation and our improved segmentation results increase the success rate of the semantic object-goal navigation in a real-world privacy-constrained scenario. </p>
<blockquote>
<p>ç§»åŠ¨æœºå™¨äººçš„ç”¨æˆ·éšç§å·²æˆä¸ºäººä»¬å…³æ³¨çš„é‡ç‚¹ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾§é‡äºä¸‹æ¸¸æœºå™¨äººä»»åŠ¡çš„æ€§èƒ½æˆ–éšç§ä¿æŠ¤ï¼Œè€Œåè€…å¾€å¾€ä¼šé™åˆ¶ä»»åŠ¡æ‰§è¡Œçš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬ç ”ç©¶åœ¨è¶…ä½åˆ†è¾¨ç‡ç¯å¢ƒä¸‹åŸºäºè¯­ä¹‰çš„æœºå™¨äººå¯¼èˆªä»¥ä¿æŠ¤è§†è§‰éšç§ã€‚æ­¤ç±»åœºæ™¯ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä»è¶…ä½åˆ†è¾¨ç‡çš„RGBå›¾åƒä¸­æ¢å¤è¯­ä¹‰åˆ†å‰²ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å®Œå…¨è”åˆå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†èšåˆç‰¹å¾æå–å™¨å’Œåˆ†å‰²æ„ŸçŸ¥é‰´åˆ«å™¨æ¥è§£å†³è¶…ä½åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œä»è€Œå®ç°å¯ä¿æŠ¤éšç§çš„ã€è¯­ä¹‰ç›®æ ‡å¯¼èˆªã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è¶…ä½åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²æ–¹é¢è¶…è¶Šäº†ä¸åŒçš„åŸºçº¿ï¼Œå¹¶ä¸”æˆ‘ä»¬æ”¹è¿›çš„åˆ†å‰²ç»“æœæé«˜äº†åœ¨ç°å®ä¸–ç•Œçš„éšç§å—é™åœºæ™¯ä¸­è¯­ä¹‰ç›®æ ‡å¯¼èˆªçš„æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16034v1">PDF</a> Submitted to RA-L</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹ç§»åŠ¨æœºå™¨äººç”¨æˆ·éšç§ä¿æŠ¤é—®é¢˜å±•å¼€ç ”ç©¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾§é‡äºæœºå™¨äººä»»åŠ¡æ€§èƒ½è€Œå¿½è§†éšç§ä¿æŠ¤çš„é—®é¢˜ï¼Œæå‡ºåœ¨è¶…ä½åˆ†è¾¨ç‡ç¯å¢ƒä¸‹è¿›è¡Œè¯­ä¹‰åŸºç¡€æœºå™¨äººå¯¼èˆªçš„æ–¹æ³•ï¼Œä»¥ä¿æŠ¤è§†è§‰éšç§ã€‚æŒ‘æˆ˜åœ¨äºä»è¶…ä½åˆ†è¾¨ç‡RGBå›¾åƒä¸­æ¢å¤è¯­ä¹‰åˆ†å‰²ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„è”åˆå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆç‰¹å¾æå–å™¨å’Œåˆ†å‰²æ„ŸçŸ¥é‰´åˆ«å™¨ï¼Œè§£å†³è¶…ä½åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œå®ç°éšç§ä¿æŠ¤è¯­ä¹‰ç›®æ ‡å¯¼èˆªã€‚è¯¥æ–¹æ³•åœ¨è¶…ä½åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²æ–¹é¢ä¼˜äºä¸åŒåŸºçº¿ï¼Œå¹¶ä¸”æé«˜äº†éšç§çº¦æŸåœºæ™¯ä¸­è¯­ä¹‰ç›®æ ‡å¯¼èˆªçš„æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æœºå™¨äººç”¨æˆ·éšç§ä¿æŠ¤æˆä¸ºé‡è¦å…³æ³¨ç‚¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸ä¾§é‡æœºå™¨äººä»»åŠ¡æ€§èƒ½æˆ–éšç§ä¿æŠ¤ï¼ŒäºŒè€…å…¼é¡¾å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åœ¨è¶…ä½åˆ†è¾¨ç‡ç¯å¢ƒä¸‹ç ”ç©¶è¯­ä¹‰åŸºç¡€æœºå™¨äººå¯¼èˆªä»¥ä¿æŠ¤è§†è§‰éšç§ã€‚</li>
<li>ä»è¶…ä½åˆ†è¾¨ç‡RGBå›¾åƒä¸­æ¢å¤è¯­ä¹‰åˆ†å‰²æ˜¯æ­¤ç±»åœºæ™¯çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å…¨æ–°çš„è”åˆå­¦ä¹ æ–¹æ³•ï¼Œç»“åˆç‰¹å¾æå–å™¨å’Œåˆ†å‰²æ„ŸçŸ¥é‰´åˆ«å™¨ï¼Œè§£å†³è¶…ä½åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ï¼Œåœ¨è¶…ä½åˆ†è¾¨ç‡è¯­ä¹‰åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a32a2469c81b7c3913e5fb927b1d1c20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28068cbd4ea67f450aeca334914b4da9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e575d0bfeb9e19e55bca69b2f20707a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3fd80ad55813a6a1ec9010e41a21e64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58520823a696fe3c90be7c4c310ac6ac.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ConformalSAM-Unlocking-the-Potential-of-Foundational-Segmentation-Models-in-Semi-Supervised-Semantic-Segmentation-with-Conformal-Prediction"><a href="#ConformalSAM-Unlocking-the-Potential-of-Foundational-Segmentation-Models-in-Semi-Supervised-Semantic-Segmentation-with-Conformal-Prediction" class="headerlink" title="ConformalSAM: Unlocking the Potential of Foundational Segmentation   Models in Semi-Supervised Semantic Segmentation with Conformal Prediction"></a>ConformalSAM: Unlocking the Potential of Foundational Segmentation   Models in Semi-Supervised Semantic Segmentation with Conformal Prediction</h2><p><strong>Authors:Danhui Chen, Ziquan Liu, Chuxi Yang, Dan Wang, Yan Yan, Yi Xu, Xiangyang Ji</strong></p>
<p>Pixel-level vision tasks, such as semantic segmentation, require extensive and high-quality annotated data, which is costly to obtain. Semi-supervised semantic segmentation (SSSS) has emerged as a solution to alleviate the labeling burden by leveraging both labeled and unlabeled data through self-training techniques. Meanwhile, the advent of foundational segmentation models pre-trained on massive data, has shown the potential to generalize across domains effectively. This work explores whether a foundational segmentation model can address label scarcity in the pixel-level vision task as an annotator for unlabeled images. Specifically, we investigate the efficacy of using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual input, to generate predictive masks for unlabeled data. To address the shortcomings of using SEEM-generated masks as supervision, we propose ConformalSAM, a novel SSSS framework which first calibrates the foundation model using the target domainâ€™s labeled data and then filters out unreliable pixel labels of unlabeled data so that only high-confidence labels are used as supervision. By leveraging conformal prediction (CP) to adapt foundation models to target data through uncertainty calibration, ConformalSAM exploits the strong capability of the foundational segmentation model reliably which benefits the early-stage learning, while a subsequent self-reliance training strategy mitigates overfitting to SEEM-generated masks in the later training stage. Our experiment demonstrates that, on three standard benchmarks of SSSS, ConformalSAM achieves superior performance compared to recent SSSS methods and helps boost the performance of those methods as a plug-in. </p>
<blockquote>
<p>åƒç´ çº§è§†è§‰ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ï¼‰éœ€è¦å¤§é‡çš„é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®çš„è·å–æˆæœ¬å¾ˆé«˜ã€‚åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSSSSï¼‰çš„å‡ºç°ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡è‡ªè®­ç»ƒæŠ€æœ¯åˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„æ•°æ®æ¥ç¼“è§£æ ‡æ³¨è´Ÿæ‹…ã€‚åŒæ—¶ï¼Œåœ¨å¤§é‡æ•°æ®ä¸Šé¢„è®­ç»ƒçš„åŸºç¡€åˆ†å‰²æ¨¡å‹çš„å‡ºç°ï¼Œæ˜¾ç¤ºå‡ºåœ¨ä¸åŒé¢†åŸŸè¿›è¡Œæœ‰æ•ˆæ¨å¹¿çš„æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œæ¢è®¨äº†åŸºç¡€åˆ†å‰²æ¨¡å‹ä½œä¸ºæœªæ ‡æ³¨å›¾åƒçš„æ³¨é‡Šå™¨ï¼Œåœ¨åƒç´ çº§è§†è§‰ä»»åŠ¡ä¸­è§£å†³æ ‡ç­¾ç¨€ç¼ºé—®é¢˜çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨SEEMï¼ˆä¸€ç§ä¸ºæ–‡æœ¬è¾“å…¥å¾®è°ƒè¿‡çš„Segment Anything Modelï¼ˆSAMï¼‰å˜ç§ï¼‰æ¥ä¸ºæœªæ ‡æ³¨æ•°æ®ç”Ÿæˆé¢„æµ‹æ©è†œçš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³ä½¿ç”¨SEEMç”Ÿæˆçš„æ©è†œä½œä¸ºç›‘ç£çš„ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ConformalSAMï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„SSSSæ¡†æ¶ã€‚å®ƒé¦–å…ˆä½¿ç”¨ç›®æ ‡é¢†åŸŸçš„æ ‡æ³¨æ•°æ®æ ¡å‡†åŸºç¡€æ¨¡å‹ï¼Œç„¶åè¿‡æ»¤å‡ºæœªæ ‡æ³¨æ•°æ®çš„ä¸å¯é åƒç´ æ ‡ç­¾ï¼Œä»¥ä¾¿åªæœ‰é«˜ç½®ä¿¡åº¦çš„æ ‡ç­¾è¢«ç”¨ä½œç›‘ç£ã€‚é€šè¿‡åˆ©ç”¨åˆæ•°é¢„æµ‹ï¼ˆCPï¼‰æ¥é€šè¿‡ä¸ç¡®å®šæ€§æ ¡å‡†é€‚åº”åŸºç¡€æ¨¡å‹åˆ°ç›®æ ‡æ•°æ®ï¼ŒConformalSAMå¯é åœ°åˆ©ç”¨åŸºç¡€åˆ†å‰²æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½ï¼Œè¿™æœ‰åˆ©äºæ—©æœŸå­¦ä¹ ï¼Œè€Œéšåçš„è‡ªæˆ‘ä¾èµ–è®­ç»ƒç­–ç•¥åˆ™å‡è½»äº†åæœŸè®­ç»ƒä¸­è¿‡åº¦ä¾èµ–SEEMç”Ÿæˆçš„æ©è†œçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸‰ä¸ªæ ‡å‡†çš„SSSSåŸºå‡†æµ‹è¯•ä¸­ï¼ŒConformalSAMä¸æœ€æ–°çš„SSSSæ–¹æ³•ç›¸æ¯”å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”ä½œä¸ºæ’ä»¶æœ‰åŠ©äºæé«˜è¿™äº›æ–¹æ³•çš„æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15803v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåƒç´ çº§åˆ«çš„è§†è§‰ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²ï¼Œéœ€è¦å¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œä½†è·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³æ ‡æ³¨è´Ÿæ‹…é—®é¢˜ï¼ŒåŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSSSSï¼‰åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ ‡æ³¨å’Œæ— æ ‡ç­¾æ•°æ®é€šè¿‡è‡ªè®­ç»ƒæŠ€æœ¯æ¥å‡è½»è´Ÿæ‹…ã€‚åŒæ—¶ï¼Œé¢„è®­ç»ƒåœ¨å¤§é‡æ•°æ®ä¸Šçš„åŸºç¡€åˆ†å‰²æ¨¡å‹æ˜¾ç¤ºå‡ºè·¨åŸŸæœ‰æ•ˆæ³›åŒ–çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶æ¢è®¨åŸºç¡€åˆ†å‰²æ¨¡å‹ä½œä¸ºæœªæ ‡æ³¨å›¾åƒæ³¨é‡Šå™¨æ¥è§£å†³åƒç´ çº§è§†è§‰ä»»åŠ¡ä¸­çš„æ ‡ç­¾ç¨€ç¼ºé—®é¢˜çš„å¯è¡Œæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨SEEMï¼ˆä¸€ç§é’ˆå¯¹æ–‡æœ¬è¾“å…¥çš„Segment Anything Modelï¼ˆSAMï¼‰å˜ä½“ï¼‰æ¥ä¸ºæ— æ ‡ç­¾æ•°æ®ç”Ÿæˆé¢„æµ‹æ©è†œçš„æ•ˆæœã€‚ä¸ºè§£å†³ä½¿ç”¨SEEMç”Ÿæˆçš„æ©è†œä½œä¸ºç›‘ç£çš„ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ConformalSAMâ€”â€”ä¸€ç§æ–°å‹SSSSæ¡†æ¶ã€‚å®ƒé¦–å…ˆä½¿ç”¨ç›®æ ‡åŸŸçš„æ ‡æ³¨æ•°æ®æ ¡å‡†åŸºç¡€æ¨¡å‹ï¼Œç„¶åè¿‡æ»¤å‡ºæ— æ ‡ç­¾æ•°æ®ä¸­ä¸å¯é çš„åƒç´ æ ‡ç­¾ï¼Œä»…ä½¿ç”¨é«˜ç½®ä¿¡åº¦çš„æ ‡ç­¾ä½œä¸ºç›‘ç£ã€‚é€šè¿‡åˆ©ç”¨é¡ºåº”æ€§é¢„æµ‹ï¼ˆCPï¼‰æ¥é€‚åº”ç›®æ ‡æ•°æ®çš„ä¸ç¡®å®šæ€§æ ¡å‡†ï¼ŒConformalSAMèƒ½å¤Ÿå¯é åœ°åˆ©ç”¨åŸºç¡€åˆ†å‰²æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ï¼Œæœ‰ç›Šäºæ—©æœŸå­¦ä¹ é˜¶æ®µï¼›éšåé‡‡ç”¨è‡ªæˆ‘ä¾èµ–è®­ç»ƒç­–ç•¥ï¼Œç¼“è§£äº†å¯¹SEEMç”Ÿæˆæ©è†œåœ¨åæœŸè®­ç»ƒé˜¶æ®µçš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸‰ä¸ªæ ‡å‡†çš„SSSSåŸºå‡†æµ‹è¯•ä¸­ï¼ŒConformalSAMç›¸è¾ƒäºè¿‘æœŸSSSSæ–¹æ³•å®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸”ä½œä¸ºæ’ä»¶æœ‰åŠ©äºæå‡è¿™äº›æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åƒç´ çº§è§†è§‰ä»»åŠ¡å¦‚è¯­ä¹‰åˆ†å‰²éœ€è¦é«˜è´¨é‡çš„å¤§é‡æ ‡æ³¨æ•°æ®ï¼Œä½†è·å–è¿™äº›æ•°æ®æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆSSSSï¼‰æ–¹æ³•æ—¨åœ¨é€šè¿‡åˆ©ç”¨æ ‡æ³¨å’Œæ— æ ‡ç­¾æ•°æ®æ¥ç¼“è§£æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>åŸºç¡€åˆ†å‰²æ¨¡å‹å…·æœ‰è·¨åŸŸæœ‰æ•ˆæ³›åŒ–çš„æ½œåŠ›ã€‚</li>
<li>ä½¿ç”¨SEEMæ¨¡å‹ä¸ºæ— æ ‡ç­¾æ•°æ®ç”Ÿæˆé¢„æµ‹æ©è†œæ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ConformalSAMæ˜¯ä¸€ç§æ–°å‹SSSSæ¡†æ¶ï¼Œé€šè¿‡æ ¡å‡†åŸºç¡€æ¨¡å‹å’Œè¿‡æ»¤ä¸å¯é åƒç´ æ ‡ç­¾æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>ConformalSAMåˆ©ç”¨é¡ºåº”æ€§é¢„æµ‹ï¼ˆCPï¼‰ä»¥é€‚åº”ç›®æ ‡æ•°æ®çš„ä¸ç¡®å®šæ€§æ ¡å‡†ï¼Œç»“åˆæ—©æœŸå­¦ä¹ å’Œè‡ªæˆ‘ä¾èµ–è®­ç»ƒç­–ç•¥ï¼Œæé«˜æ¨¡å‹çš„å¯é æ€§å¹¶ç¼“è§£è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-071a378bad372c54ac088adf9bf819c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96a89f4eae2b9b450fe23b0cc69dbb16.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c29c45e230b8ad2a4eabce780c88f299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd7e0d9382264fe4ec192f1daea5ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1fe67c88d96da8e9c82eef80d78ed5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Label-tree-semantic-losses-for-rich-multi-class-medical-image-segmentation"><a href="#Label-tree-semantic-losses-for-rich-multi-class-medical-image-segmentation" class="headerlink" title="Label tree semantic losses for rich multi-class medical image   segmentation"></a>Label tree semantic losses for rich multi-class medical image   segmentation</h2><p><strong>Authors:Junwen Wang, Oscar MacCormac, William Rochford, Aaron Kujawa, Jonathan Shapey, Tom Vercauteren</strong></p>
<p>Rich and accurate medical image segmentation is poised to underpin the next generation of AI-defined clinical practice by delineating critical anatomy for pre-operative planning, guiding real-time intra-operative navigation, and supporting precise post-operative assessment. However, commonly used learning methods for medical and surgical imaging segmentation tasks penalise all errors equivalently and thus fail to exploit any inter-class semantics in the labels space. This becomes particularly problematic as the cardinality and richness of labels increases to include subtly different classes. In this work, we propose two tree-based semantic loss functions which take advantage of a hierarchical organisation of the labels. We further incorporate our losses in a recently proposed approach for training with sparse, background-free annotations to extend the applicability of our proposed losses. Extensive experiments are reported on two medical and surgical image segmentation tasks, namely head MRI for whole brain parcellation (WBP) with full supervision and neurosurgical hyperspectral imaging (HSI) for scene understanding with sparse annotations. Results demonstrate that our proposed method reaches state-of-the-art performance in both cases. </p>
<blockquote>
<p>ä¸°å¯Œè€Œå‡†ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯å°†ä¸ºä¸‹ä¸€ä»£äººå·¥æ™ºèƒ½å®šä¹‰çš„ä¸´åºŠå®è·µæä¾›æ”¯æŒï¼Œé€šè¿‡æç»˜å…³é”®è§£å‰–ç»“æ„æ¥è¿›è¡Œæœ¯å‰è§„åˆ’ï¼ŒæŒ‡å¯¼å®æ—¶æœ¯ä¸­å¯¼èˆªï¼Œå¹¶æ”¯æŒç²¾ç¡®çš„æœ¯åè¯„ä¼°ã€‚ç„¶è€Œï¼ŒåŒ»å­¦å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å¸¸ç”¨çš„å­¦ä¹ æ–¹æ³•å¹³ç­‰åœ°æƒ©ç½šæ‰€æœ‰é”™è¯¯ï¼Œå› æ­¤æœªèƒ½åˆ©ç”¨æ ‡ç­¾ç©ºé—´ä¸­çš„ç±»é—´è¯­ä¹‰ã€‚å½“æ ‡ç­¾çš„åŸºæ•°å’Œä¸°å¯Œæ€§å¢åŠ ä»¥åŒ…æ‹¬ç»†å¾®ä¸åŒçš„ç±»åˆ«æ—¶ï¼Œè¿™å˜å¾—ç‰¹åˆ«æˆé—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºæ ‘çš„è¯­ä¹‰æŸå¤±å‡½æ•°ï¼Œå®ƒä»¬åˆ©ç”¨äº†æ ‡ç­¾çš„å±‚æ¬¡ç»„ç»‡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†æˆ‘ä»¬çš„æŸå¤±çº³å…¥æœ€è¿‘æå‡ºçš„å…·æœ‰ç¨€ç–ã€æ— èƒŒæ™¯æ³¨é‡Šçš„è®­ç»ƒæ–¹æ³•ä¸­ï¼Œä»¥æ‰©å±•æˆ‘ä»¬æå‡ºçš„æŸå¤±é€‚ç”¨æ€§ã€‚åœ¨ä¸¤é¡¹åŒ»å­¦å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸ŠæŠ¥å‘Šäº†å¹¿æ³›å®éªŒçš„ç»“æœï¼Œåˆ†åˆ«æ˜¯å…¨ç›‘ç£çš„å¤´MRIå…¨è„‘åˆ†åŒºï¼ˆWBPï¼‰å’Œå…·æœ‰ç¨€ç–æ³¨é‡Šçš„ç¥ç»å¤–ç§‘é«˜å…‰è°±æˆåƒï¼ˆHSIï¼‰çš„åœºæ™¯ç†è§£ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15777v1">PDF</a> arXiv admin note: text overlap with arXiv:2506.21150</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒåŒ»ç–—å›¾åƒåˆ†å‰²æŠ€æœ¯åœ¨æ–°ä¸€ä»£AIå®šä¹‰çš„ä¸´åºŠå®è·µä¸­çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬åœ¨æœ¯å‰è§„åˆ’ã€æœ¯ä¸­å®æ—¶å¯¼èˆªå’Œæœ¯åç²¾ç¡®è¯„ä¼°ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå¸¸ç”¨çš„å­¦ä¹ æ–¹æ³•å’Œæ ‡ç­¾ç©ºé—´çš„è¯­ä¹‰ä¹‹é—´ç¼ºä¹å…³è”ï¼Œå¯¹äºå¤æ‚çš„åŒ»ç–—å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡å­˜åœ¨é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§åŸºäºæ ‘çš„è¯­ä¹‰æŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿåˆ©ç”¨æ ‡ç­¾çš„å±‚æ¬¡ç»“æ„ã€‚æ­¤å¤–ï¼Œå°†æŸå¤±å‡½æ•°åº”ç”¨äºç¨€ç–ã€æ— èƒŒæ™¯æ³¨é‡Šçš„è®­ç»ƒæ–¹æ³•ï¼Œæ‰©å¤§äº†å…¶åº”ç”¨èŒƒå›´ã€‚åœ¨åŒ»ç–—å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒåˆ†å‰²æŠ€æœ¯åœ¨æ–°ä¸€ä»£AIä¸´åºŠå®è·µä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>å¸¸ç”¨çš„åŒ»å­¦å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²å­¦ä¹ æ–¹æ³•æœªå……åˆ†åˆ©ç”¨æ ‡ç­¾ç©ºé—´ä¸­çš„è¯­ä¹‰å…³ç³»ã€‚</li>
<li>æå‡ºä¸¤ç§åŸºäºæ ‘çš„è¯­ä¹‰æŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿåˆ©ç”¨æ ‡ç­¾çš„å±‚æ¬¡ç»“æ„æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>æŸå¤±å‡½æ•°åº”ç”¨äºç¨€ç–ã€æ— èƒŒæ™¯æ³¨é‡Šçš„è®­ç»ƒæ–¹æ³•ï¼Œå¢å¼ºäº†å…¶é€‚ç”¨æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨åŒ»ç–—å’Œæ‰‹æœ¯å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤´éƒ¨MRIå…¨ç›‘ç£ä¸‹è¿›è¡Œå…¨è„‘åˆ†å—ï¼ˆWBPï¼‰å’Œç¥ç»å¤–ç§‘è¶…å…‰è°±æˆåƒï¼ˆHSIï¼‰åœºæ™¯ç†è§£çš„ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-534ffca011f1f130d01ec12b61d90605.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Depthwise-Dilated-Convolutional-Adapters-for-Medical-Object-Tracking-and-Segmentation-Using-the-Segment-Anything-Model-2"><a href="#Depthwise-Dilated-Convolutional-Adapters-for-Medical-Object-Tracking-and-Segmentation-Using-the-Segment-Anything-Model-2" class="headerlink" title="Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and   Segmentation Using the Segment Anything Model 2"></a>Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and   Segmentation Using the Segment Anything Model 2</h2><p><strong>Authors:Guoping Xu, Christopher Kabat, You Zhang</strong></p>
<p>Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2â€™s streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/apple1986/DD-SAM2">https://github.com/apple1986/DD-SAM2</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„è¿›å±•ä¸»è¦å¾—ç›Šäºæ·±åº¦å­¦ä¹ ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä»ç„¶å—åˆ°ç‰¹å®šæ¨¡æ€è®¾è®¡çš„é™åˆ¶ï¼Œåœ¨åŠ¨æ€åŒ»å­¦å½±åƒåœºæ™¯ä¸­é€‚åº”æ€§è¾ƒå·®ã€‚Segment Anything Model 2ï¼ˆSAM2ï¼‰åŠå…¶ç›¸å…³å˜ä½“å¼•å…¥äº†å®æ—¶è§†é¢‘åˆ†å‰²çš„æµå¼å†…å­˜æœºåˆ¶ï¼Œä¸ºåŸºäºæç¤ºçš„é€šç”¨è§£å†³æ–¹æ¡ˆæä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹é€‚åº”åˆ°åŒ»å­¦è§†é¢‘åœºæ™¯é€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé‡æ–°è®­ç»ƒæˆ–è¿ç§»å­¦ä¹ ï¼Œè¿™å¯¼è‡´äº†è¾ƒé«˜çš„è®¡ç®—æˆæœ¬ä»¥åŠç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DD-SAM2ï¼Œè¿™æ˜¯ä¸€ä¸ªSAM2çš„æœ‰æ•ˆé€‚é…æ¡†æ¶ï¼Œå®ƒç»“åˆäº†Depthwise-Dilated Adapterï¼ˆDD-Adapterï¼‰ä»¥å¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–èƒ½åŠ›ï¼ŒåŒæ—¶å°½é‡å‡å°‘å‚æ•°å¼€é”€ã€‚è¿™ç§è®¾è®¡ä½¿å¾—åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šå¯¹åŒ»å­¦è§†é¢‘è¿›è¡ŒSAM2ç²¾ç»†è°ƒæ•´æˆä¸ºå¯èƒ½ã€‚ä¸ç°æœ‰ä»…ä¸“æ³¨äºé™æ€å›¾åƒçš„é€‚é…å™¨æ–¹æ³•ä¸åŒï¼ŒDD-SAM2å……åˆ†åˆ©ç”¨SAM2çš„æµå¼å†…å­˜è¿›è¡ŒåŒ»å­¦è§†é¢‘ç›®æ ‡è·Ÿè¸ªå’Œåˆ†å‰²ã€‚åœ¨TrackRad2025ï¼ˆè‚¿ç˜¤åˆ†å‰²ï¼‰å’ŒEchoNet-Dynamicï¼ˆå·¦å¿ƒå®¤è·Ÿè¸ªï¼‰æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œå…¶æ€§èƒ½å“è¶Šï¼Œåˆ†åˆ«å®ç°äº†Diceå¾—åˆ†0.93å’Œ0.97ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œåˆæ­¥å°è¯•ç³»ç»Ÿåœ°æ¢ç´¢åŸºäºé€‚é…å™¨çš„SAM2ç²¾ç»†è°ƒæ•´åœ¨åŒ»å­¦è§†é¢‘åˆ†å‰²å’Œè·Ÿè¸ªä¸­çš„åº”ç”¨ã€‚ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/apple1986/DD-SAM">https://github.com/apple1986/DD-SAM</a> å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14613v1">PDF</a> 24 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯å–å¾—æ–°è¿›å±•ï¼Œä½†ä»å­˜åœ¨é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„è®¾è®¡é™åˆ¶ä»¥åŠé€‚åº”åŠ¨æ€åŒ»å­¦å½±åƒåœºæ™¯çš„é€‚åº”æ€§å·®çš„é—®é¢˜ã€‚SAM2æ¨¡å‹åŠå…¶å˜ä½“é€šè¿‡å¼•å…¥å®æ—¶è§†é¢‘åˆ†å‰²çš„æµå¼å†…å­˜æœºåˆ¶ï¼Œä¸ºåŸºäºæç¤ºçš„é€šç”¨è§£å†³æ–¹æ¡ˆæä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹é€‚åº”åˆ°åŒ»å­¦è§†é¢‘åœºæ™¯é€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†è¿›è¡Œé‡æ–°è®­ç»ƒæˆ–è¿ç§»å­¦ä¹ ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œç¾éš¾æ€§é—å¿˜çš„é£é™©ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DD-SAM2ï¼Œä¸€ä¸ªé«˜æ•ˆçš„SAM2é€‚åº”æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨Depthwise-Dilated Adapterï¼ˆDD-Adapterï¼‰ä»¥å¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–ï¼ŒåŒæ—¶å‚æ•°å¼€é”€æœ€å°åŒ–ã€‚è¯¥è®¾è®¡å¯åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šå®ç°å¯¹åŒ»å­¦è§†é¢‘çš„SAM2ç²¾ç»†è°ƒæ•´ã€‚ä¸ç°æœ‰ä»…ä¸“æ³¨äºé™æ€å›¾åƒçš„é€‚é…å™¨æ–¹æ³•ä¸åŒï¼ŒDD-SAM2å……åˆ†åˆ©ç”¨SAM2çš„æµå¼å†…å­˜è¿›è¡ŒåŒ»å­¦è§†é¢‘ç›®æ ‡è·Ÿè¸ªå’Œåˆ†å‰²ã€‚åœ¨TrackRad2025ï¼ˆè‚¿ç˜¤åˆ†å‰²ï¼‰å’ŒEchoNet-Dynamicï¼ˆå·¦å¿ƒå®¤è·Ÿè¸ªï¼‰æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜å…¶å“è¶Šæ€§èƒ½ï¼Œåˆ†åˆ«å®ç°äº†Diceå¾—åˆ†0.93å’Œ0.97ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå–å¾—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ¨¡æ€ç‰¹å®šè®¾è®¡å’ŒåŠ¨æ€åŒ»å­¦å½±åƒåœºæ™¯é€‚åº”æ€§é—®é¢˜ã€‚</li>
<li>SAM2æ¨¡å‹åŠå…¶å˜ä½“ä¸ºåŸºäºæç¤ºçš„é€šç”¨è§£å†³æ–¹æ¡ˆæä¾›æ–°æœºä¼šï¼Œå°¤å…¶é€‚ç”¨äºå®æ—¶è§†é¢‘åˆ†å‰²ã€‚</li>
<li>é€‚åº”åŒ»å­¦è§†é¢‘åœºæ™¯é€šå¸¸éœ€è¦å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œç¾éš¾æ€§é—å¿˜é£é™©ã€‚</li>
<li>æå‡ºDD-SAM2æ¡†æ¶ï¼Œé€šè¿‡Depthwise-Dilated Adapterå¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œå®ç°SAM2åœ¨åŒ»å­¦è§†é¢‘ä¸Šçš„ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>DD-SAM2ä¸åŒäºç°æœ‰ä¸“æ³¨äºé™æ€å›¾åƒçš„é€‚é…å™¨æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨SAM2çš„æµå¼å†…å­˜è¿›è¡ŒåŒ»å­¦è§†é¢‘ç›®æ ‡è·Ÿè¸ªå’Œåˆ†å‰²ã€‚</li>
<li>åœ¨TrackRad2025å’ŒEchoNet-Dynamicæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDD-SAM2è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è€…å…¬å¼€äº†ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä¾¿äºä»–äººè®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14613">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc53c57b32f8c8971efb1c2a5235890d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2634f32af68d0603b25dae869fab1b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SCORE-Scene-Context-Matters-in-Open-Vocabulary-Remote-Sensing-Instance-Segmentation"><a href="#SCORE-Scene-Context-Matters-in-Open-Vocabulary-Remote-Sensing-Instance-Segmentation" class="headerlink" title="SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance   Segmentation"></a>SCORE: Scene Context Matters in Open-Vocabulary Remote Sensing Instance   Segmentation</h2><p><strong>Authors:Shiqi Huang, Shuting He, Huaiyuan Qin, Bihan Wen</strong></p>
<p>Most existing remote sensing instance segmentation approaches are designed for close-vocabulary prediction, limiting their ability to recognize novel categories or generalize across datasets. This restricts their applicability in diverse Earth observation scenarios. To address this, we introduce open-vocabulary (OV) learning for remote sensing instance segmentation. While current OV segmentation models perform well on natural image datasets, their direct application to remote sensing faces challenges such as diverse landscapes, seasonal variations, and the presence of small or ambiguous objects in aerial imagery. To overcome these challenges, we propose $\textbf{SCORE}$ ($\textbf{S}$cene $\textbf{C}$ontext matters in $\textbf{O}$pen-vocabulary $\textbf{RE}$mote sensing instance segmentation), a framework that integrates multi-granularity scene context, i.e., regional context and global context, to enhance both visual and textual representations. Specifically, we introduce Region-Aware Integration, which refines class embeddings with regional context to improve object distinguishability. Additionally, we propose Global Context Adaptation, which enriches naive text embeddings with remote sensing global context, creating a more adaptable and expressive linguistic latent space for the classifier. We establish new benchmarks for OV remote sensing instance segmentation across diverse datasets. Experimental results demonstrate that, our proposed method achieves SOTA performance, which provides a robust solution for large-scale, real-world geospatial analysis. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/SCORE">https://github.com/HuangShiqi128/SCORE</a>. </p>
<blockquote>
<p>å½“å‰å¤§å¤šæ•°é¥æ„Ÿå®ä¾‹åˆ†å‰²æ–¹æ³•éƒ½æ˜¯é’ˆå¯¹å°é—­è¯æ±‡é¢„æµ‹è®¾è®¡çš„ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æ–°å‹ç±»åˆ«çš„è¯†åˆ«èƒ½åŠ›ï¼Œä»¥åŠåœ¨æ•°æ®é›†é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤šç§åœ°çƒè§‚æµ‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºé¥æ„Ÿå®ä¾‹åˆ†å‰²å¼•å…¥äº†å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰å­¦ä¹ ã€‚è™½ç„¶å½“å‰çš„OVåˆ†å‰²æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒæ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬ç›´æ¥åº”ç”¨äºé¥æ„Ÿå´é¢ä¸´ç€æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ™¯è§‚å¤šæ ·æ€§ã€å­£èŠ‚å˜åŒ–å’Œç©ºä¸­å½±åƒä¸­å°ç›®æ ‡æˆ–æ¨¡ç³Šç‰©ä½“çš„å­˜åœ¨ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†$\textbf{SCORE}$ï¼ˆ$\textbf{S}$cene $\textbf{C}$ontext matters in $\textbf{O}$pen-vocabulary $\textbf{RE}$mote sensing instance segmentationï¼‰ï¼Œä¸€ä¸ªæ•´åˆå¤šç²’åº¦åœºæ™¯ä¸Šä¸‹æ–‡çš„æ¡†æ¶ï¼Œå³åŒºåŸŸä¸Šä¸‹æ–‡å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œä»¥å¢å¼ºè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒºåŸŸæ„ŸçŸ¥é›†æˆï¼Œé€šè¿‡åŒºåŸŸä¸Šä¸‹æ–‡ç»†åŒ–ç±»åˆ«åµŒå…¥ï¼Œä»¥æé«˜ç›®æ ‡å¯åŒºåˆ†æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨å±€ä¸Šä¸‹æ–‡é€‚åº”ï¼Œç”¨é¥æ„Ÿå…¨å±€ä¸Šä¸‹æ–‡ä¸°å¯ŒåŸå§‹æ–‡æœ¬åµŒå…¥ï¼Œä¸ºåˆ†ç±»å™¨åˆ›å»ºä¸€ä¸ªæ›´å…·é€‚åº”æ€§å’Œè¡¨è¾¾èƒ½åŠ›çš„è¯­è¨€æ½œåœ¨ç©ºé—´ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå»ºç«‹äº†OVé¥æ„Ÿå®ä¾‹åˆ†å‰²çš„æ–°åŸºå‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡ã€çœŸå®ä¸–ç•Œçš„åœ°ç†ç©ºé—´åˆ†ææä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/SCORE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HuangShiqi128/SCOREæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12857v2">PDF</a> ICCV 2025 (Highlight), code see   <a target="_blank" rel="noopener" href="https://github.com/HuangShiqi128/SCORE">https://github.com/HuangShiqi128/SCORE</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¥æ„Ÿå®ä¾‹åˆ†å‰²é¢†åŸŸä¸­çš„å¼€æ”¾è¯æ±‡è¡¨å­¦ä¹ æ–¹æ³•ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é¢å‘å°é—­è¯æ±‡é¢„æµ‹ï¼Œéš¾ä»¥è¯†åˆ«æ–°ç±»åˆ«æˆ–è·¨æ•°æ®é›†è¿›è¡Œæ¨å¹¿ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ ·åœ°çƒè§‚æµ‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSCOREçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†å¤šç²’åº¦åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŒºåŸŸä¸Šä¸‹æ–‡å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œä»¥å¢å¼ºè§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œä¸ºå¤§è§„æ¨¡åœ°ç†ç©ºé—´åˆ†ææä¾›äº†ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é¥æ„Ÿå®ä¾‹åˆ†å‰²æ–¹æ³•ä¸»è¦é¢å‘å°é—­è¯æ±‡é¢„æµ‹ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ ·åœ°çƒè§‚æµ‹åœºæ™¯ä¸­çš„åº”ç”¨ã€‚</li>
<li>å¼€æ”¾è¯æ±‡ï¼ˆOVï¼‰å­¦ä¹ æ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œä½†ç›´æ¥åº”ç”¨äºé¥æ„Ÿé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>SCOREæ¡†æ¶èåˆäº†å¤šç²’åº¦åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŒºåŸŸä¸Šä¸‹æ–‡å’Œå…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>Region-Aware Integrationé€šè¿‡ç»“åˆåŒºåŸŸä¸Šä¸‹æ–‡ä¿¡æ¯ä¼˜åŒ–ç±»åˆ«åµŒå…¥ï¼Œæé«˜ç›®æ ‡åŒºåˆ†åº¦ã€‚</li>
<li>Global Context Adaptationä¸°å¯Œäº†æ–‡æœ¬åµŒå…¥çš„é¥æ„Ÿå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½¿åˆ†ç±»å™¨æ›´å…·é€‚åº”æ€§å’Œè¡¨ç°åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æ”¾è¯æ±‡é¥æ„Ÿå®ä¾‹åˆ†å‰²æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4f6be50adfe021fbca4cd5f6ec85396.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e546c4dda49d2959f3f4e6e6a13a422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41ae00d913e6e00cdf37416578065762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-492ce0d6f40513bcc7efd539fb9bc595.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="OD-VIRAT-A-Large-Scale-Benchmark-for-Object-Detection-in-Realistic-Surveillance-Environments"><a href="#OD-VIRAT-A-Large-Scale-Benchmark-for-Object-Detection-in-Realistic-Surveillance-Environments" class="headerlink" title="OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic   Surveillance Environments"></a>OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic   Surveillance Environments</h2><p><strong>Authors:Hayat Ullah, Abbas Khan, Arslan Munir, Hari Kalva</strong></p>
<p>Realistic human surveillance datasets are crucial for training and evaluating computer vision models under real-world conditions, facilitating the development of robust algorithms for human and human-interacting object detection in complex environments. These datasets need to offer diverse and challenging data to enable a comprehensive assessment of model performance and the creation of more reliable surveillance systems for public safety. To this end, we present two visual object detection benchmarks named OD-VIRAT Large and OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance imagery. The video sequences in both benchmarks cover 10 different scenes of human surveillance recorded from significant height and distance. The proposed benchmarks offer rich annotations of bounding boxes and categories, where OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also focuses on benchmarking state-of-the-art object detection architectures, including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object detection-specific variant of VIRAT dataset. To the best of our knowledge, it is the first work to examine the performance of these recently published state-of-the-art object detection architectures on realistic surveillance imagery under challenging conditions such as complex backgrounds, occluded objects, and small-scale objects. The proposed benchmarking and experimental settings will help in providing insights concerning the performance of selected object detection models and set the base for developing more efficient and robust object detection architectures. </p>
<blockquote>
<p>ç°å®çš„äººç±»ç›‘æ§æ•°æ®é›†å¯¹äºåœ¨ç°å®æ¡ä»¶ä¸‹è®­ç»ƒå’Œè¯„ä¼°è®¡ç®—æœºè§†è§‰æ¨¡å‹è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºåœ¨å¤æ‚ç¯å¢ƒä¸­å¼€å‘ç”¨äºäººç±»å’Œäººæœºäº¤äº’å¯¹è±¡æ£€æµ‹çš„ç¨³å¥ç®—æ³•ã€‚è¿™äº›æ•°æ®é›†éœ€è¦æä¾›å¤šæ ·åŒ–å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®ï¼Œä»¥å®ç°å¯¹æ¨¡å‹æ€§èƒ½çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶åˆ›å»ºæ›´å¯é çš„å…¬å…±å®‰å…¨ç›‘æ§ç³»ç»Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåä¸ºOD-VIRAT Largeå’ŒOD-VIRAT Tinyçš„è§†è§‰ç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨è¿›ç›‘æ§å›¾åƒä¸­çš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚è¿™ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„è§†é¢‘åºåˆ—æ¶µç›–äº†ä»é‡å¤§é«˜åº¦å’Œè·ç¦»è®°å½•çš„10ä¸ªä¸åŒçš„äººç±»ç›‘æ§åœºæ™¯ã€‚æ‰€æå‡ºåŸºå‡†æµ‹è¯•æä¾›äº†ä¸°å¯Œçš„è¾¹ç•Œæ¡†å’Œç±»åˆ«æ³¨é‡Šï¼Œå…¶ä¸­OD-VIRAT Largeæœ‰599,996å¼ å›¾åƒä¸­çš„870ä¸‡ä¸ªæ³¨é‡Šå®ä¾‹ï¼Œè€ŒOD-VIRAT Tinyæœ‰19,860å¼ å›¾åƒä¸­çš„288,901ä¸ªæ³¨é‡Šå®ä¾‹ã€‚è¿™é¡¹å·¥ä½œè¿˜é‡ç‚¹å…³æ³¨åŸºäºæœ€æ–°æŠ€æœ¯æ°´å¹³çš„å¯¹è±¡æ£€æµ‹æ¶æ„çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬åœ¨VIRATæ•°æ®é›†çš„è¿™ä¸€ç‰¹å®šå¯¹è±¡æ£€æµ‹å˜ä½“ä¸Šçš„RETMDETã€YOLOXã€RetinaNetã€DETRå’Œå¯å˜å½¢DETRã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹å·¥ä½œåœ¨å¤æ‚çš„ç°å®ç›‘æ§å›¾åƒä¸Šæ£€éªŒè¿™äº›æœ€è¿‘å‘å¸ƒçš„æœ€æ–°ç›®æ ‡æ£€æµ‹æ¶æ„çš„æ€§èƒ½ï¼Œå¦‚åœ¨å¤æ‚èƒŒæ™¯ã€é®æŒ¡ç‰©ä½“å’Œå°è§„æ¨¡ç‰©ä½“ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ã€‚æ‰€æå‡ºçš„åŸºå‡†æµ‹è¯•å’Œå®éªŒè®¾ç½®å°†æœ‰åŠ©äºæ·±å…¥äº†è§£æ‰€é€‰ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸ºå¼€å‘æ›´é«˜æ•ˆå’Œç¨³å¥çš„ç›®æ ‡æ£€æµ‹æ¶æ„å¥ å®šåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12396v2">PDF</a> 14 pages</p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸¤ä¸ªç”¨äºç›‘æ§å›¾åƒè§†è§‰ç†è§£ä»»åŠ¡çš„å¤§å‹å’Œå¾®å‹è§†è§‰å¯¹è±¡æ£€æµ‹åŸºå‡†æ•°æ®é›†OD-VIRAT Largeå’ŒOD-VIRAT Tinyã€‚è¿™äº›æ•°æ®é›†æ¶µç›–ä»ä¸åŒé«˜åº¦å’Œè·ç¦»å½•åˆ¶çš„çœŸå®äººç±»ç›‘æ§è§†é¢‘åºåˆ—ï¼ŒåŒ…å«ä¸°å¯Œçš„è¾¹ç•Œæ¡†å’Œç±»åˆ«æ³¨é‡Šã€‚è¯¥å·¥ä½œé‡ç‚¹è¯„ä¼°å…ˆè¿›çš„å¯¹è±¡æ£€æµ‹æ¶æ„åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ç›‘æ§å›¾åƒä¸Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤æ‚èƒŒæ™¯ã€é®æŒ¡ç‰©ä½“å’Œå°ç‰©ä½“ç­‰æ¡ä»¶ã€‚è¿™äº›åŸºå‡†æµ‹è¯•å’Œå®éªŒè®¾ç½®æœ‰åŠ©äºäº†è§£æ‰€é€‰å¯¹è±¡æ£€æµ‹æ¨¡å‹æ€§èƒ½å¹¶ä¸ºå¼€å‘æ›´é«˜æ•ˆå’Œç¨³å¥çš„å¯¹è±¡æ£€æµ‹æ¶æ„å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0b63697a15eb9d18a1e33fef6b62135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2a5f265c6f462489d5637de35be7aaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ca1931928cb46d8fc09d3990e92ecc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2369bae62093ae8271fc472cc564af9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f636cd881583602194dcd47abe264f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e09b78b916781a52c3a73c08e5bc14e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Out-of-distribution-data-supervision-towards-biomedical-semantic-segmentation"><a href="#Out-of-distribution-data-supervision-towards-biomedical-semantic-segmentation" class="headerlink" title="Out-of-distribution data supervision towards biomedical semantic   segmentation"></a>Out-of-distribution data supervision towards biomedical semantic   segmentation</h2><p><strong>Authors:Yiquan Gao, Duohui Xu</strong></p>
<p>Biomedical segmentation networks easily suffer from the unexpected misclassification between foreground and background objects when learning on limited and imperfect medical datasets. Inspired by the strong power of Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric framework, Med-OoD to address this issue by introducing OoD data supervision into fully-supervised biomedical segmentation with none of the following needs: (i) external data sources, (ii) feature regularization objectives, (iii) additional annotations. Our method can be seamlessly integrated into segmentation networks without any modification on the architectures. Extensive experiments show that Med-OoD largely prevents various segmentation networks from the pixel misclassification on medical images and achieves considerable performance improvements on Lizard dataset. We also present an emerging learning paradigm of training a medical segmentation network completely using OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU as test result. We hope this learning paradigm will attract people to rethink the roles of OoD data. Code is made available at <a target="_blank" rel="noopener" href="https://github.com/StudioYG/Med-OoD">https://github.com/StudioYG/Med-OoD</a>. </p>
<blockquote>
<p>ç”Ÿç‰©åŒ»å­¦åˆ†å‰²ç½‘ç»œåœ¨æœ‰é™ä¸”å­˜åœ¨ç¼ºé™·çš„åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œå­¦ä¹ æ—¶ï¼Œå®¹æ˜“å—åˆ°å‰æ™¯å’ŒèƒŒæ™¯å¯¹è±¡ä¹‹é—´æ„å¤–è¯¯åˆ†ç±»çš„å½±å“ã€‚å—å…¶ä»–è§†è§‰ä»»åŠ¡ä¸­å¼‚å¸¸æ•°æ®ï¼ˆOut-of-Distributionï¼ŒOoDï¼‰å¼ºå¤§èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ¡†æ¶Med-OoDï¼Œé€šè¿‡å¼•å…¥OoDæ•°æ®ç›‘ç£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®Œå…¨ç›‘ç£ç”Ÿç‰©åŒ»å­¦åˆ†å‰²ï¼Œæ— éœ€ä»¥ä¸‹éœ€æ±‚ï¼šï¼ˆiï¼‰å¤–éƒ¨æ•°æ®æºï¼Œï¼ˆiiï¼‰ç‰¹å¾æ­£åˆ™åŒ–ç›®æ ‡ï¼Œï¼ˆiiiï¼‰é¢å¤–çš„æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°åˆ†å‰²ç½‘ç»œä¸­ï¼Œæ— éœ€å¯¹æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMed-OoDåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šé˜²æ­¢äº†å„ç§åˆ†å‰²ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒä¸Šçš„åƒç´ è¯¯åˆ†ç±»ï¼Œå¹¶åœ¨èœ¥èœ´æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å…¨æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œå³å®Œå…¨ä½¿ç”¨ä¸å«å‰æ™¯ç±»åˆ«æ ‡ç­¾çš„OoDæ•°æ®æ¥è®­ç»ƒåŒ»å­¦åˆ†å‰²ç½‘ç»œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæµ‹è¯•ç»“æœä¸ºmIoUè¾¾åˆ°76.1%ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç§å­¦ä¹ èŒƒå¼èƒ½å¸å¼•äººä»¬é‡æ–°æ€è€ƒOoDæ•°æ®çš„ä½œç”¨ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/StudioYG/Med-OoD%E3%80%82">https://github.com/StudioYG/Med-OoDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12105v1">PDF</a> This paper was published in Proceedings of SPIE Volume 13442 and is   reprinted with permission. The official version is available at   <a target="_blank" rel="noopener" href="https://doi.org/10.1117/12.3052988">https://doi.org/10.1117/12.3052988</a>. One personal copy is allowed.   Reproduction, distribution, or commercial use is prohibited</p>
<p><strong>Summary</strong><br>ç”Ÿç‰©åŒ»å­¦åˆ†å‰²ç½‘ç»œåœ¨æœ‰é™ä¸”éå®Œç¾çš„åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œå­¦ä¹ æ—¶ï¼Œå®¹æ˜“å‡ºç°å‰æ™¯å’ŒèƒŒæ™¯å¯¹è±¡ä¹‹é—´çš„æ„å¤–è¯¯åˆ†ç±»ã€‚å—å…¶ä»–è§†è§‰ä»»åŠ¡ä¸­å¼‚å¸¸åˆ†å¸ƒï¼ˆOut-of-Distributionï¼ŒOoDï¼‰æ•°æ®å¼ºå¤§èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ¡†æ¶Med-OoDï¼Œé€šè¿‡å¼•å…¥OoDæ•°æ®ç›‘ç£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®Œå…¨ç›‘ç£ç”Ÿç‰©åŒ»å­¦åˆ†å‰²ï¼Œæ— éœ€ä»¥ä¸‹éœ€æ±‚ï¼šï¼ˆiï¼‰å¤–éƒ¨æ•°æ®æºï¼Œï¼ˆiiï¼‰ç‰¹å¾æ­£åˆ™åŒ–ç›®æ ‡ï¼Œï¼ˆiiiï¼‰é¢å¤–æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°åˆ†å‰²ç½‘ç»œä¸­ï¼Œæ— éœ€å¯¹æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMed-OoDåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šé˜²æ­¢äº†å„ç§åˆ†å‰²ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒä¸Šçš„åƒç´ è¯¯åˆ†ç±»ï¼Œå¹¶åœ¨èœ¥èœ´æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å…¨æ–°çš„å­¦ä¹ èŒƒå¼ï¼Œå³å®Œå…¨ä½¿ç”¨OoDæ•°æ®è®­ç»ƒåŒ»å­¦åˆ†å‰²ç½‘ç»œï¼Œæ— éœ€å‰æ™¯ç±»åˆ«æ ‡ç­¾ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæµ‹è¯•ç»“æœä¸ºmIoU 76.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿç‰©åŒ»å­¦åˆ†å‰²ç½‘ç»œåœ¨æœ‰é™ä¸”éå®Œç¾çš„åŒ»ç–—æ•°æ®é›†ä¸Šæ˜“é­å—å‰æ™¯å’ŒèƒŒæ™¯å¯¹è±¡è¯¯åˆ†ç±»çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥Out-of-Distribution (OoD) æ•°æ®ç›‘ç£èƒ½å¤Ÿæå‡ç”Ÿç‰©åŒ»å­¦åˆ†å‰²ç½‘ç»œçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„Med-OoDæ¡†æ¶æ— éœ€å¤–éƒ¨æ•°æ®æºã€ç‰¹å¾æ­£åˆ™åŒ–ç›®æ ‡å’Œé¢å¤–æ³¨é‡Šã€‚</li>
<li>Med-OoDæ¡†æ¶å¯æ— ç¼é›†æˆåˆ°åˆ†å‰²ç½‘ç»œä¸­ï¼Œæ— éœ€å¯¹æ¶æ„è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚</li>
<li>Med-OoDåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ˜¾è‘—å‡å°‘äº†åƒç´ è¯¯åˆ†ç±»ã€‚</li>
<li>åœ¨Lizardæ•°æ®é›†ä¸Šï¼ŒMed-OoDå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-74eef02fc50d33592f55dfe9db594be8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fe21d1301a9e549ebf0e77a38b569f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6dd7c3a585d92b4d89517c256b99216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d8d20d44260c0de06ce091e50c88c1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="YOLOv8-SMOT-An-Efficient-and-Robust-Framework-for-Real-Time-Small-Object-Tracking-via-Slice-Assisted-Training-and-Adaptive-Association"><a href="#YOLOv8-SMOT-An-Efficient-and-Robust-Framework-for-Real-Time-Small-Object-Tracking-via-Slice-Assisted-Training-and-Adaptive-Association" class="headerlink" title="YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small   Object Tracking via Slice-Assisted Training and Adaptive Association"></a>YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small   Object Tracking via Slice-Assisted Training and Adaptive Association</h2><p><strong>Authors:Xiang Yu, Xinyao Liu, Guang Liang</strong></p>
<p>Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 â€œFinding Birdsâ€ Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \textbf{SliceTrain}. This framework, through the synergy of â€˜deterministic full-coverage slicingâ€™ and â€˜slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \textbf{motion direction maintenance (EMA)} mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Salvatore-Love/YOLOv8-SMOT">https://github.com/Salvatore-Love/YOLOv8-SMOT</a>. </p>
<blockquote>
<p>ä»æ— äººæœºï¼ˆUAVï¼‰çš„è§’åº¦è·Ÿè¸ªå°å‹æ•æ·å¤šç›®æ ‡ï¼ˆSMOTï¼‰ï¼Œå¦‚é¸Ÿç±»ï¼Œæ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚éš¾åº¦ä¸»è¦æ¥æºäºä¸‰ä¸ªæ–¹é¢ï¼šç›®æ ‡å¤–è§‚ç‰¹å¾çš„æåº¦ç¨€ç¼ºï¼Œç”±ç›¸æœºå’Œç›®æ ‡æœ¬èº«åŠ¨åŠ›å­¦ç»“åˆå¯¼è‡´çš„å¤æ‚è¿åŠ¨çº ç¼ ï¼Œä»¥åŠç”±å¯†é›†é›†ç¾¤è¡Œä¸ºå¼•èµ·çš„é¢‘ç¹é®æŒ¡å’Œèº«ä»½æ¨¡ç³Šã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬åœ¨MVA 2025â€œå¯»æ‰¾é¸Ÿç±»â€å°å‹å¤šç›®æ ‡è·Ÿè¸ªæŒ‘æˆ˜èµ›ï¼ˆSMOT4SBï¼‰ä¸­å¤ºå† çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨æ£€æµ‹è·Ÿè¸ªèŒƒå¼ï¼Œåœ¨æ£€æµ‹å’Œå…³è”å±‚é¢éƒ½æœ‰é’ˆå¯¹æ€§åˆ›æ–°ã€‚åœ¨æ£€æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„è®­ç»ƒå¢å¼ºæ¡†æ¶ï¼Œåä¸ºSliceTrainã€‚è¯¥æ¡†æ¶é€šè¿‡â€œç¡®å®šæ€§å…¨è¦†ç›–åˆ‡ç‰‡â€å’Œâ€œåˆ‡ç‰‡çº§éšæœºå¢å¼ºâ€çš„ååŒä½œç”¨ï¼Œæœ‰æ•ˆè§£å†³äº†é«˜åˆ†è¾¨ç‡å›¾åƒè®­ç»ƒä¸­ç›®æ ‡è¿‡å°å¯¼è‡´çš„è®­ç»ƒä¸è¶³é—®é¢˜ã€‚åœ¨è·Ÿè¸ªæ–¹é¢ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®Œå…¨ç‹¬ç«‹äºå¤–è§‚ä¿¡æ¯çš„ç¨³å¥è·Ÿè¸ªå™¨ã€‚é€šè¿‡å°†è¿åŠ¨æ–¹å‘ç»´æŠ¤ï¼ˆEMAï¼‰æœºåˆ¶å’Œç»“åˆè¾¹ç•Œæ¡†æ‰©å±•å’Œè·ç¦»æƒ©ç½šçš„è‡ªé€‚åº”ç›¸ä¼¼åº¦é‡èå…¥OC-SORTæ¡†æ¶ï¼Œæˆ‘ä»¬çš„è·Ÿè¸ªå™¨å¯ä»¥ç¨³å®šå¤„ç†ä¸è§„åˆ™è¿åŠ¨å¹¶ä¿æŒç›®æ ‡èº«ä»½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨SMOT4SBå…¬å¼€æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ€§èƒ½ï¼ŒSO-HOTAå¾—åˆ†è¾¾åˆ°55.205ï¼Œå……åˆ†éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨è§£å†³å¤æ‚ç°å®ä¸–ç•ŒSMOTé—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§å’Œå…ˆè¿›æ€§ã€‚æºä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Salvatore-Love/YOLOv8-SMOT%E3%80%82">https://github.com/Salvatore-Love/YOLOv8-SMOTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12087v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ— äººé£è¡Œå™¨è§†è§’ä¸­å¯¹å°å‹æ•æ·å¤šç›®æ ‡ï¼ˆå¦‚é¸Ÿç±»ï¼‰è¿›è¡Œè·Ÿè¸ªçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„é«˜æŒ‘æˆ˜æ€§å’Œå…¶è§£å†³æ–¹æ¡ˆã€‚æ–‡ç« è¯¦ç»†æè¿°äº†è¯¥å›¢é˜Ÿåœ¨MVA 2025â€œå¯»æ‰¾é¸Ÿç±»â€å°å‹å¤šç›®æ ‡è·Ÿè¸ªæŒ‘æˆ˜èµ›ï¼ˆSMOT4SBï¼‰ä¸­çš„å† å†›è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨æ£€æµ‹è·Ÿè¸ªèŒƒå¼ï¼Œåœ¨æ£€æµ‹å’Œå…³è”å±‚é¢éƒ½æœ‰é’ˆå¯¹æ€§åˆ›æ–°ã€‚é’ˆå¯¹æ£€æµ‹æ–¹é¢çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºSliceTrainçš„ç³»ç»Ÿæ€§è®­ç»ƒå¢å¼ºæ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†é«˜åˆ†è¾¨ç‡å›¾åƒè®­ç»ƒä¸­ç›®æ ‡å­¦ä¹ ä¸è¶³çš„é—®é¢˜ã€‚åœ¨è·Ÿè¸ªæ–¹é¢ï¼Œè®¾è®¡äº†ä¸€ä¸ªç‹¬ç«‹äºå¤–è§‚ä¿¡æ¯çš„ç¨³å¥è·Ÿè¸ªå™¨ï¼Œé€šè¿‡èå…¥è¿åŠ¨æ–¹å‘ç»´æŠ¤æœºåˆ¶å’Œè‡ªé€‚åº”ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†ï¼Œå®ç°äº†å¯¹ä¸è§„åˆ™è¿åŠ¨çš„ç¨³å®šå¤„ç†å’Œç›®æ ‡èº«ä»½çš„ç»´æŒã€‚è¯¥æ–¹æ³•åœ¨SMOT4SBå…¬å¼€æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼ŒSO-HOTAå¾—åˆ†ä¸º55.205ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶åœ¨è§£å†³å¤æ‚ç°å®SMOTé—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§å’Œå…ˆè¿›æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— äººé£è¡Œå™¨è§†è§’å¯¹å°å‹æ•æ·å¤šç›®æ ‡è¿›è¡Œè·Ÿè¸ªæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>æ–‡ç« è¯¦è¿°äº†åœ¨MVA 2025â€œå¯»æ‰¾é¸Ÿç±»â€å°å‹å¤šç›®æ ‡è·Ÿè¸ªæŒ‘æˆ˜èµ›ä¸­çš„å† å†›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ¡ˆé‡‡ç”¨æ£€æµ‹è·Ÿè¸ªèŒƒå¼ï¼Œé’ˆå¯¹æ£€æµ‹ä¸å…³è”å±‚é¢è¿›è¡Œåˆ›æ–°ã€‚</li>
<li>SliceTrainæ¡†æ¶è§£å†³äº†é«˜åˆ†è¾¨ç‡å›¾åƒè®­ç»ƒä¸­ç›®æ ‡å­¦ä¹ ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªç‹¬ç«‹äºå¤–è§‚ä¿¡æ¯çš„ç¨³å¥è·Ÿè¸ªå™¨ï¼Œå®ç°å¯¹ä¸è§„åˆ™è¿åŠ¨çš„ç¨³å®šå¤„ç†å’Œç›®æ ‡èº«ä»½çš„ç»´æŒã€‚</li>
<li>è·Ÿè¸ªå™¨èåˆäº†è¿åŠ¨æ–¹å‘ç»´æŠ¤æœºåˆ¶å’Œè‡ªé€‚åº”ç›¸ä¼¼æ€§åº¦é‡æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12087">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55720a96bee800d6d37bf6922dee2c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62cfafe2bc0968b73a863bd5a8a7826d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-106ec6544516fed88166fa0d37b7838e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Spatial-Frequency-Modulation-for-Semantic-Segmentation"><a href="#Spatial-Frequency-Modulation-for-Semantic-Segmentation" class="headerlink" title="Spatial Frequency Modulation for Semantic Segmentation"></a>Spatial Frequency Modulation for Semantic Segmentation</h2><p><strong>Authors:Linwei Chen, Ying Fu, Lin Gu, Dezhi Zheng, Jifeng Dai</strong></p>
<p>High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Linwei-Chen/SFM">https://github.com/Linwei-Chen/SFM</a>. </p>
<blockquote>
<p>é«˜é¢‘ç©ºé—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬çº¹ç†ç­‰ç»†èŠ‚ï¼Œå¯¹è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®æ€§æœ‰å¾ˆå¤§è´¡çŒ®ã€‚ç„¶è€Œï¼Œæ ¹æ®Nyquist-Shannoné‡‡æ ·å®šç†ï¼Œé«˜é¢‘åˆ†é‡åœ¨é€šè¿‡æ­¥å¹…å·ç§¯ç­‰é™é‡‡æ ·å±‚ä¼ æ’­æ—¶å®¹æ˜“å‘ç”Ÿæ··å æˆ–å¤±çœŸã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç©ºé—´é¢‘ç‡è°ƒåˆ¶ï¼ˆSFMï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨é™é‡‡æ ·ä¹‹å‰å°†é«˜é¢‘ç‰¹å¾è°ƒåˆ¶åˆ°è¾ƒä½é¢‘ç‡ï¼Œç„¶ååœ¨ä¸Šé‡‡æ ·è¿‡ç¨‹ä¸­è¿›è¡Œè§£è°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªé€‚åº”é‡é‡‡æ ·ï¼ˆARSï¼‰å®ç°è°ƒåˆ¶ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„é™„åŠ ç»„ä»¶ï¼Œå¯ä»¥å¯†é›†åœ°å¯¹é«˜é¢‘åŒºåŸŸè¿›è¡Œé‡‡æ ·ä»¥æ‰©å¤§ä¿¡å·ï¼Œä»è€Œæ ¹æ®é¢‘ç‡ç¼©æ”¾å±æ€§é™ä½å…¶é¢‘ç‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼ˆMSAUï¼‰æ¥è§£è°ƒè°ƒåˆ¶ç‰¹å¾ï¼Œå¹¶é€šè¿‡éå‡åŒ€ä¸Šé‡‡æ ·æ¢å¤é«˜é¢‘ä¿¡æ¯ã€‚è¯¥æ¨¡å—é€šè¿‡æ˜¾å¼åˆ©ç”¨å¤šä¸ªå°ºåº¦ä¸Šå¯†é›†é‡é‡‡æ ·å’Œç¨€ç–é‡é‡‡æ ·åŒºåŸŸä¹‹é—´çš„ä¿¡æ¯äº¤äº’ï¼Œè¿›ä¸€æ­¥æé«˜äº†åˆ†å‰²æ•ˆæœã€‚è¿™ä¸¤ä¸ªæ¨¡å—å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°å„ç§æ¶æ„ä¸­ï¼Œä»å·ç§¯ç¥ç»ç½‘ç»œåˆ°è½¬æ¢å™¨ã€‚ç‰¹å¾å¯è§†åŒ–å’Œåˆ†æè¯å®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°å‡è½»äº†æ··å ç°è±¡ï¼ŒåŒæ—¶åœ¨è§£è°ƒåæˆåŠŸä¿ç•™äº†ç»†èŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶æ‰©å±•åˆ°å›¾åƒåˆ†ç±»ã€å¯¹æŠ—ç¨³å¥æ€§ã€å®ä¾‹åˆ†å‰²å’Œå…¨è§†åˆ†å‰²ä»»åŠ¡æ¥éªŒè¯SFMçš„å¹¿æ³›é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Linwei-Chen/SFM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Linwei-Chen/SFMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11893v2">PDF</a> Accept by TPAMI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ç©ºé—´é¢‘ç‡å¯¹è¯­ä¹‰åˆ†å‰²å‡†ç¡®åº¦çš„å½±å“ï¼Œæå‡ºäº†ç©ºé—´é¢‘ç‡è°ƒåˆ¶ï¼ˆSFMï¼‰æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯åŒ…æ‹¬è‡ªé€‚åº”é‡é‡‡æ ·ï¼ˆARSï¼‰å’Œå¤šå°ºåº¦è‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼ˆMSAUï¼‰ï¼Œç”¨äºå¤„ç†é«˜é¢‘ç‰¹å¾ä¿¡æ¯çš„å¤±çœŸé—®é¢˜ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†åœ¨å¤šç§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜é¢‘ä¿¡æ¯å¦‚çº¹ç†å¯¹è¯­ä¹‰åˆ†å‰²çš„å‡†ç¡®åº¦æœ‰é‡è¦è´¡çŒ®ã€‚</li>
<li>Nyquist-Shannoné‡‡æ ·å®šç†æŒ‡å‡ºé«˜é¢‘æˆåˆ†åœ¨ä¼ æ’­è¿‡ç¨‹ä¸­æ˜“å—åˆ°æ··å æˆ–å¤±çœŸå½±å“ã€‚</li>
<li>æå‡ºç©ºé—´é¢‘ç‡è°ƒåˆ¶ï¼ˆSFMï¼‰æŠ€æœ¯ï¼Œé€šè¿‡è‡ªé€‚åº”é‡é‡‡æ ·ï¼ˆARSï¼‰å®ç°é«˜é¢‘ç‰¹å¾åˆ°ä½é¢‘çš„è°ƒåˆ¶ï¼Œå†é€šè¿‡å¤šå°ºåº¦è‡ªé€‚åº”ä¸Šé‡‡æ ·ï¼ˆMSAUï¼‰è¿›è¡Œè§£è°ƒå¹¶æ¢å¤é«˜é¢‘ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æ— ç¼é›†æˆåˆ°å„ç§æ¶æ„ä¸­ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨ã€‚</li>
<li>ç‰¹å¾å¯è§†åŒ–å’Œåˆ†æè¯å®è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå‡è½»æ··å ç°è±¡ï¼Œå¹¶åœ¨è§£è°ƒåæˆåŠŸä¿ç•™ç»†èŠ‚ã€‚</li>
<li>å®éªŒéªŒè¯äº†SFMåœ¨å›¾åƒåˆ†ç±»ã€å¯¹æŠ—é²æ£’æ€§ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a1d554775d6787f6ed968bf79a5fbb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b62a23ad0c33432c909c9e9b442a616b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9649a243203d964257f55f2e967fa31e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-567a0800f607802e2939076d5a661cee.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation"><a href="#Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation" class="headerlink" title="Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation"></a>Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation</h2><p><strong>Authors:Shuchang Ye, Usman Naseem, Mingyuan Meng, Jinman Kim</strong></p>
<p>Medical language-guided segmentation, integrating textual clinical reports as auxiliary guidance to enhance image segmentation, has demonstrated significant improvements over unimodal approaches. However, its inherent reliance on paired image-text input, which we refer to as &#96;&#96;textual relianceâ€, presents two fundamental limitations: 1) many medical segmentation datasets lack paired reports, leaving a substantial portion of image-only data underutilized for training; and 2) inference is limited to retrospective analysis of cases with paired reports, limiting its applicability in most clinical scenarios where segmentation typically precedes reporting. To address these limitations, we propose ProLearn, the first Prototype-driven Learning framework for language-guided segmentation that fundamentally alleviates textual reliance. At its core, we introduce a novel Prototype-driven Semantic Approximation (PSA) module to enable approximation of semantic guidance from textual input. PSA initializes a discrete and compact prototype space by distilling segmentation-relevant semantics from textual reports. Once initialized, it supports a query-and-respond mechanism which approximates semantic guidance for images without textual input, thereby alleviating textual reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn outperforms state-of-the-art language-guided methods when limited text is available. </p>
<blockquote>
<p>åŒ»ç–—è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯é€šè¿‡å°†æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼æ¥å¢å¼ºå›¾åƒåˆ†å‰²ï¼Œå·²ç»è¯æ˜å…¶åœ¨å•æ¨¡æ€æ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œå…¶å¯¹é…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥çš„å›ºæœ‰ä¾èµ–ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ–‡æœ¬ä¾èµ–â€ï¼Œå­˜åœ¨ä¸¤ä¸ªåŸºæœ¬å±€é™ï¼š1ï¼‰è®¸å¤šåŒ»ç–—åˆ†å‰²æ•°æ®é›†ç¼ºä¹é…å¯¹æŠ¥å‘Šï¼Œå¯¼è‡´å¤§é‡ä»…åŒ…å«å›¾åƒçš„æ•°æ®æœªèƒ½å¾—åˆ°å……åˆ†åˆ©ç”¨ï¼›2ï¼‰æ¨ç†ä»…é™äºå…·æœ‰é…å¯¹æŠ¥å‘Šçš„ç—…ä¾‹çš„å›é¡¾æ€§åˆ†æï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å¤šæ•°ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå› ä¸ºåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆ†å‰²æ˜¯åœ¨æŠ¥å‘Šä¹‹å‰è¿›è¡Œçš„ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†ProLearnï¼Œå³é¦–ä¸ªç”¨äºè¯­è¨€å¼•å¯¼åˆ†å‰²çš„åŸå‹é©±åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œä»æ ¹æœ¬ä¸Šç¼“è§£äº†æ–‡æœ¬ä¾èµ–é—®é¢˜ã€‚å…¶æ ¸å¿ƒåœ¨äºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹åŸå‹é©±åŠ¨è¯­ä¹‰é€¼è¿‘ï¼ˆPSAï¼‰æ¨¡å—ï¼Œä»¥å®ç°ä»æ–‡æœ¬è¾“å…¥ä¸­è¿›è¡Œè¯­ä¹‰æŒ‡å¯¼çš„é€¼è¿‘ã€‚PSAé€šè¿‡ä»æ–‡æœ¬æŠ¥å‘Šä¸­æç‚¼å‡ºä¸åˆ†å‰²ç›¸å…³çš„è¯­ä¹‰ï¼Œåˆå§‹åŒ–ä¸€ä¸ªç¦»æ•£ä¸”ç´§å‡‘çš„åŸå‹ç©ºé—´ã€‚åˆå§‹åŒ–å®Œæˆåï¼Œå®ƒæ”¯æŒæŸ¥è¯¢å’Œå“åº”æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹å¯¹å›¾åƒè¿›è¡Œè¯­ä¹‰æŒ‡å¯¼é€¼è¿‘ï¼Œä»è€Œç¼“è§£äº†å¯¹æ–‡æœ¬çš„ä¾èµ–ã€‚åœ¨QaTa-COV19ã€MosMedData+å’ŒKvasir-SEGä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“æ–‡æœ¬æœ‰é™æ—¶ï¼ŒProLearnåœ¨ç°æœ‰è¯­è¨€å¼•å¯¼æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11055v3">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯ä¸­ï¼Œæ•´åˆæ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼æ¥æå‡å›¾åƒåˆ†å‰²çš„æ•ˆæœå·²æ˜¾ç°ã€‚ç„¶è€Œï¼Œå…¶å›ºæœ‰ä¾èµ–äºé…å¯¹å›¾æ–‡è¾“å…¥ï¼ˆâ€œæ–‡æœ¬ä¾èµ–â€ï¼‰å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯è®¸å¤šåŒ»å­¦åˆ†å‰²æ•°æ®é›†ç¼ºä¹é…å¯¹æŠ¥å‘Šï¼Œå¯¼è‡´å¤§é‡ä»…å›¾åƒæ•°æ®æœªè¢«å……åˆ†åˆ©ç”¨äºè®­ç»ƒï¼›äºŒæ˜¯æ¨ç†ä»…é™äºæœ‰é…å¯¹æŠ¥å‘Šçš„å›é¡¾æ€§æ¡ˆä¾‹åˆ†æï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å¤šæ•°ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºProLearnâ€”â€”é¦–ä¸ªåŸå‹é©±åŠ¨çš„å­¦ä¹ æ¡†æ¶ç”¨äºè¯­è¨€å¼•å¯¼åˆ†å‰²ï¼Œä»æ ¹æœ¬ä¸Šå‡è½»æ–‡æœ¬ä¾èµ–ã€‚å…¶æ ¸å¿ƒå¼•å…¥æ–°é¢–çš„åŸå‹é©±åŠ¨è¯­ä¹‰è¿‘ä¼¼æ¨¡å—ï¼Œå®ç°æ–‡æœ¬è¾“å…¥è¯­ä¹‰æŒ‡å¯¼çš„è¿‘ä¼¼ã€‚PSAé€šè¿‡è’¸é¦æŠ¥å‘Šä¸­çš„åˆ†å‰²ç›¸å…³è¯­ä¹‰ï¼Œåˆå§‹åŒ–ä¸€ä¸ªç¦»æ•£ä¸”ç´§å‡‘çš„åŸå‹ç©ºé—´ã€‚åˆå§‹åŒ–åï¼Œå®ƒæ”¯æŒæŸ¥è¯¢å’Œå“åº”æœºåˆ¶ï¼Œä¸ºæ— æ–‡æœ¬è¾“å…¥çš„å›¾åƒè¿‘ä¼¼è¯­ä¹‰æŒ‡å¯¼ï¼Œä»è€Œå‡è½»æ–‡æœ¬ä¾èµ–ã€‚åœ¨QaTa-COV19ã€MosMedData+å’ŒKvasir-SEGä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œå½“æ–‡æœ¬æœ‰é™æ—¶ï¼ŒProLearnè¡¨ç°ä¼˜äºå…ˆè¿›çš„è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯ç»“åˆæ–‡æœ¬ä¸´åºŠæŠ¥å‘Šæ¥æå‡å›¾åƒåˆ†å‰²æ•ˆæœã€‚</li>
<li>è¯¥æŠ€æœ¯å­˜åœ¨å¯¹é…å¯¹å›¾æ–‡è¾“å…¥çš„ä¾èµ–ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨å’Œå®é™…åº”ç”¨åœºæ™¯å—é™ã€‚</li>
<li>æå‡ºProLearnæ¡†æ¶ï¼Œé€šè¿‡åŸå‹é©±åŠ¨å­¦ä¹ å‡è½»å¯¹æ–‡æœ¬ä¾èµ–ã€‚</li>
<li>ProLearnå¼•å…¥åŸå‹é©±åŠ¨è¯­ä¹‰è¿‘ä¼¼æ¨¡å—ï¼Œå®ç°æ–‡æœ¬è¯­ä¹‰æŒ‡å¯¼çš„è¿‘ä¼¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebba7c9d0958ff69f7000f5f917ee09d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-581a2ac34e56e6d9c43a0fe382a3f143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfb49aef80910b9762fe28fb9303d90d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b6727b70a9303b94d9b35023ccabaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be8317ce099f7cc5319b5eef5332a5a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffae2a1d29c91fb90ad2aa4271470703.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Diffusion-Guided-Knowledge-Distillation-for-Weakly-Supervised-Low-Light-Semantic-Segmentation"><a href="#Diffusion-Guided-Knowledge-Distillation-for-Weakly-Supervised-Low-Light-Semantic-Segmentation" class="headerlink" title="Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light   Semantic Segmentation"></a>Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-Light   Semantic Segmentation</h2><p><strong>Authors:Chunyan Wang, Dong Zhang, Jinhui Tang</strong></p>
<p>Weakly-supervised semantic segmentation aims to assign category labels to each pixel using weak annotations, significantly reducing manual annotation costs. Although existing methods have achieved remarkable progress in well-lit scenarios, their performance significantly degrades in low-light environments due to two fundamental limitations: severe image quality degradation (e.g., low contrast, noise, and color distortion) and the inherent constraints of weak supervision. These factors collectively lead to unreliable class activation maps and semantically ambiguous pseudo-labels, ultimately compromising the modelâ€™s ability to learn discriminative feature representations. To address these problems, we propose Diffusion-Guided Knowledge Distillation for Weakly-Supervised Low-light Semantic Segmentation (DGKD-WLSS), a novel framework that synergistically combines Diffusion-Guided Knowledge Distillation (DGKD) with Depth-Guided Feature Fusion (DGF2). DGKD aligns normal-light and low-light features via diffusion-based denoising and knowledge distillation, while DGF2 integrates depth maps as illumination-invariant geometric priors to enhance structural feature learning. Extensive experiments demonstrate the effectiveness of DGKD-WLSS, which achieves state-of-the-art performance in weakly supervised semantic segmentation tasks under low-light conditions. The source codes have been released at:<a target="_blank" rel="noopener" href="https://github.com/ChunyanWang1/DGKD-WLSS">https://github.com/ChunyanWang1/DGKD-WLSS</a>. </p>
<blockquote>
<p>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ—¨åœ¨åˆ©ç”¨å¼±æ ‡æ³¨ä¸ºæ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾ï¼Œä»è€Œæ˜¾è‘—é™ä½æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨å…¨å…‰ç…§åœºæ™¯ä¸‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨ä½å…‰ç¯å¢ƒä¸­ï¼Œç”±äºä¸¤ä¸ªåŸºæœ¬å±€é™æ€§ï¼Œå®ƒä»¬çš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼šä¸¥é‡çš„å›¾åƒè´¨é‡ä¸‹é™ï¼ˆä¾‹å¦‚ï¼Œä½å¯¹æ¯”åº¦ã€å™ªå£°å’Œé¢œè‰²å¤±çœŸï¼‰å’Œå¼±ç›‘ç£çš„å†…åœ¨çº¦æŸã€‚è¿™äº›å› ç´ å…±åŒä½œç”¨ï¼Œå¯¼è‡´ä¸å¯é çš„ç±»æ¿€æ´»å›¾å’Œè¯­ä¹‰æ¨¡ç³Šçš„ä¼ªæ ‡ç­¾ï¼Œæœ€ç»ˆæŸå®³æ¨¡å‹å­¦ä¹ åˆ¤åˆ«ç‰¹å¾è¡¨ç¤ºçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºå¼±ç›‘ç£ä½å…‰è¯­ä¹‰åˆ†å‰²çš„æ‰©æ•£å¼•å¯¼çŸ¥è¯†è’¸é¦ï¼ˆDGKD-WLSSï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒååŒç»“åˆäº†æ‰©æ•£å¼•å¯¼çŸ¥è¯†è’¸é¦ï¼ˆDGKDï¼‰å’Œæ·±åº¦å¼•å¯¼ç‰¹å¾èåˆï¼ˆDGF2ï¼‰ã€‚DGKDé€šè¿‡åŸºäºæ‰©æ•£çš„å»å™ªå’ŒçŸ¥è¯†è’¸é¦å¯¹é½æ­£å¸¸å…‰å’Œä½å…‰ç‰¹å¾ï¼Œè€ŒDGF2å°†æ·±åº¦å›¾ä½œä¸ºå…‰ç…§ä¸å˜çš„å‡ ä½•å…ˆéªŒè¿›è¡Œé›†æˆï¼Œä»¥å¢å¼ºç»“æ„ç‰¹å¾å­¦ä¹ ã€‚å¤§é‡å®éªŒè¡¨æ˜DGKD-WLSSçš„æœ‰æ•ˆæ€§ï¼Œå®ƒåœ¨ä½å…‰æ¡ä»¶ä¸‹çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚æºä»£ç å·²å‘å¸ƒåœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ChunyanWang1/DGKD-WLSS%E3%80%82">https://github.com/ChunyanWang1/DGKD-WLSSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07578v2">PDF</a> Accepted by ACM Multimedia</p>
<p><strong>Summary</strong><br>    å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²åˆ©ç”¨å¼±æ ‡æ³¨å¯¹æ¯åƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾ï¼Œæ˜¾è‘—é™ä½æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•åœ¨å…‰ç…§å……è¶³åœºæ™¯ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä½å…‰ç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦ç”±äºå›¾åƒè´¨é‡ä¸¥é‡é€€åŒ–å’Œå¼±ç›‘ç£çš„å†…åœ¨çº¦æŸã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºç»“åˆæ‰©æ•£å¼•å¯¼çŸ¥è¯†è’¸é¦å’Œæ·±åº¦å¼•å¯¼ç‰¹å¾èåˆçš„æ¡†æ¶ï¼ˆDGKD-WLSSï¼‰ï¼Œé€šè¿‡æ‰©æ•£å»å™ªå’ŒçŸ¥è¯†è’¸é¦å¯¹é½æ­£å¸¸å…‰å’Œä½å…‰ç‰¹å¾ï¼ŒåŒæ—¶åˆ©ç”¨æ·±åº¦å›¾å¢å¼ºç»“æ„ç‰¹å¾å­¦ä¹ ã€‚DGKD-WLSSåœ¨å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²æ—¨åœ¨åˆ©ç”¨å¼±æ ‡æ³¨é™ä½æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>åœ¨ä½å…‰ç¯å¢ƒä¸‹ï¼Œç°æœ‰æ–¹æ³•æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦ç”±äºå›¾åƒè´¨é‡é€€åŒ–å’Œå¼±ç›‘ç£çº¦æŸã€‚</li>
<li>DGKD-WLSSæ¡†æ¶ç»“åˆæ‰©æ•£å¼•å¯¼çŸ¥è¯†è’¸é¦å’Œæ·±åº¦å¼•å¯¼ç‰¹å¾èåˆã€‚</li>
<li>æ‰©æ•£å»å™ªå’ŒçŸ¥è¯†è’¸é¦ç”¨äºå¯¹é½æ­£å¸¸å…‰å’Œä½å…‰ç‰¹å¾ã€‚</li>
<li>æ·±åº¦å›¾ä½œä¸ºç…§æ˜ä¸å˜å‡ ä½•å…ˆéªŒå¢å¼ºç»“æ„ç‰¹å¾å­¦ä¹ ã€‚</li>
<li>DGKD-WLSSåœ¨å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­å®ç°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c3ecb61b77a59681a9a0d5879d7d27f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85c216e5089f300274a5b86edcc59f41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d3ac68ddb94c6f497ad2161a33c0777.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9ad9c3f3cbe52a2440aeb93a68f1055.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257b646c4d9b4a0c9cbc1aed98551af2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Towards-Accurate-and-Efficient-3D-Object-Detection-for-Autonomous-Driving-A-Mixture-of-Experts-Computing-System-on-Edge"><a href="#Towards-Accurate-and-Efficient-3D-Object-Detection-for-Autonomous-Driving-A-Mixture-of-Experts-Computing-System-on-Edge" class="headerlink" title="Towards Accurate and Efficient 3D Object Detection for Autonomous   Driving: A Mixture of Experts Computing System on Edge"></a>Towards Accurate and Efficient 3D Object Detection for Autonomous   Driving: A Mixture of Experts Computing System on Edge</h2><p><strong>Authors:Linshen Liu, Boyan Su, Junyue Jiang, Guanlin Wu, Cong Guo, Ceyu Xu, Hao Frank Yang</strong></p>
<p>This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as an end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/LinshenLiu622/EMC2">https://github.com/LinshenLiu622/EMC2</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºè¾¹ç¼˜è®¡ç®—çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ååŒè®¡ç®—ï¼ˆEMC2ï¼‰ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†è®¾è®¡çš„ä¼˜åŒ–è®¡ç®—ç³»ç»Ÿï¼Œå¯åŒæ—¶è¿›è¡Œä½å»¶è¿Ÿå’Œé«˜ç²¾åº¦çš„3Dç›®æ ‡æ£€æµ‹ã€‚ä¸ä¼ ç»Ÿçš„ä¸åŒï¼ŒEMC2é‡‡ç”¨äº†é’ˆå¯¹è¾¹ç¼˜å¹³å°çš„åœºæ™¯æ„ŸçŸ¥MoEæ¶æ„ã€‚é€šè¿‡æœ‰æ•ˆåœ°èåˆæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®ï¼Œç³»ç»Ÿåˆ©ç”¨ç¨€ç–çš„3Dç‚¹äº‘å’Œå¯†é›†çš„2Då›¾åƒçš„ä¼˜åŠ¿ï¼Œç”Ÿæˆç¨³å¥çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼ŒEMC2é‡‡ç”¨è‡ªé€‚åº”å¤šæ¨¡æ€æ•°æ®æ¡¥è¿›è¡Œå¤šå°ºåº¦é¢„å¤„ç†ä¼ æ„Ÿå™¨è¾“å…¥ï¼Œç„¶åé€šè¿‡åœºæ™¯æ„ŸçŸ¥è·¯ç”±æœºåˆ¶æ ¹æ®ç›®æ ‡å¯è§æ€§å’Œè·ç¦»åŠ¨æ€åœ°å°†ç‰¹å¾è°ƒåº¦åˆ°ä¸“ç”¨çš„ä¸“å®¶æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒEMC2é›†æˆäº†è”åˆç¡¬ä»¶è½¯ä»¶ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ç¡¬ä»¶èµ„æºåˆ©ç”¨ä¼˜åŒ–å’Œè®¡ç®—å›¾ç®€åŒ–ï¼Œä»¥ç¡®ä¿åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆå®æ—¶çš„æ¨ç†ã€‚åœ¨å¼€æºåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ¸…æ¥šåœ°è¡¨æ˜äº†EMC2ä½œä¸ºç«¯åˆ°ç«¯ç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚åœ¨KITTIæ•°æ®é›†ä¸Šï¼Œä¸Jetsonå¹³å°ä¸Šçš„15ç§åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå…¶å¹³å‡ç²¾åº¦æé«˜äº†3.58%ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†159.06%ï¼Œåœ¨nuscenesæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨æ¨åŠ¨å¯é ã€å®æ—¶çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†3Dç›®æ ‡æ£€æµ‹ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚å®˜æ–¹å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LinshenLiu622/EMC2">https://github.com/LinshenLiu622/EMC2</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04123v2">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºè¾¹ç¼˜è®¡ç®—çš„æ··åˆä¸“å®¶åä½œè®¡ç®—ç³»ç»Ÿï¼ˆEMC2ï¼‰ï¼Œä¸“ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†è®¾è®¡ï¼Œå¯å®ç°ä½å»¶è¿Ÿã€é«˜å‡†ç¡®åº¦çš„3Dç›®æ ‡æ£€æµ‹ã€‚EMC2é‡‡ç”¨æƒ…æ™¯æ„ŸçŸ¥çš„æ··åˆä¸“å®¶æ¶æ„ï¼Œä¼˜åŒ–è¾¹ç¼˜å¹³å°ï¼Œç»“åˆæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®ï¼Œåˆ©ç”¨ç¨€ç–çš„3Dç‚¹äº‘å’Œå¯†é›†çš„2Då›¾åƒç”Ÿæˆç¨³å¥çš„å¤šæ¨¡å¼è¡¨ç¤ºã€‚é€šè¿‡è‡ªé€‚åº”å¤šæ¨¡å¼æ•°æ®æ¡¥å’Œå¤šå°ºåº¦é¢„å¤„ç†ï¼Œä»¥åŠåŸºäºç›®æ ‡å¯è§æ€§å’Œè·ç¦»çš„æƒ…æ™¯æ„ŸçŸ¥è·¯ç”±æœºåˆ¶ï¼Œå°†ç‰¹æ€§åŠ¨æ€åˆ†æ´¾ç»™ä¸“ç”¨ä¸“å®¶æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒEMC2è¿˜é›†æˆäº†è”åˆè½¯ç¡¬ä»¶ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ç¡¬ä»¶èµ„æºåˆ©ç”¨ä¼˜åŒ–å’Œè®¡ç®—å›¾ç®€åŒ–ï¼Œä»¥ç¡®ä¿åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆã€å®æ—¶çš„æ¨ç†ã€‚åœ¨KITTIå’Œnuscenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒEMC2åœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æé«˜ï¼Œæ¨ç†é€Ÿåº¦åŠ å¿«ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>EMC2æ˜¯ä¸€ä¸ªä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†è®¾è®¡çš„è®¡ç®—ç³»ç»Ÿï¼Œå®ç°ä½å»¶è¿Ÿã€é«˜å‡†ç¡®åº¦çš„3Dç›®æ ‡æ£€æµ‹ã€‚</li>
<li>é‡‡ç”¨æƒ…æ™¯æ„ŸçŸ¥çš„æ··åˆä¸“å®¶æ¶æ„ï¼Œä¼˜åŒ–è¾¹ç¼˜å¹³å°ã€‚</li>
<li>ç»“åˆæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®ï¼Œç”Ÿæˆç¨³å¥çš„å¤šæ¨¡å¼è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”å¤šæ¨¡å¼æ•°æ®æ¡¥å’Œå¤šå°ºåº¦é¢„å¤„ç†ï¼Œä»¥åŠæƒ…æ™¯æ„ŸçŸ¥è·¯ç”±æœºåˆ¶ï¼Œå®ç°ç‰¹æ€§åŠ¨æ€åˆ†é…ã€‚</li>
<li>EMC2é›†æˆäº†è”åˆè½¯ç¡¬ä»¶ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ç¡¬ä»¶èµ„æºåˆ©ç”¨ä¼˜åŒ–å’Œè®¡ç®—å›¾ç®€åŒ–ã€‚</li>
<li>åœ¨KITTIå’Œnuscenesæ•°æ®é›†ä¸Šï¼ŒEMC2åœ¨å‡†ç¡®æ€§å’Œæ¨ç†é€Ÿåº¦ä¸Šå‡æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a7a9c43d98aad6d8dc886736faa834a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23dcdeba355fec3313a321fab300fcc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6856d88879f84ad660b99f7ae37ba4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e862d54b099ec2b96471efcb536040.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="2-5D-Object-Detection-for-Intelligent-Roadside-Infrastructure"><a href="#2-5D-Object-Detection-for-Intelligent-Roadside-Infrastructure" class="headerlink" title="2.5D Object Detection for Intelligent Roadside Infrastructure"></a>2.5D Object Detection for Intelligent Roadside Infrastructure</h2><p><strong>Authors:Nikolai Polley, Yacin Boualili, Ferdinand MÃ¼tsch, Maximilian Zipfl, Tobias Fleck, J. Marius ZÃ¶llner</strong></p>
<p>On-board sensors of autonomous vehicles can be obstructed, occluded, or limited by restricted fields of view, complicating downstream driving decisions. Intelligent roadside infrastructure perception systems, installed at elevated vantage points, can provide wide, unobstructed intersection coverage, supplying a complementary information stream to autonomous vehicles via vehicle-to-everything (V2X) communication. However, conventional 3D object-detection algorithms struggle to generalize under the domain shift introduced by top-down perspectives and steep camera angles. We introduce a 2.5D object detection framework, tailored specifically for infrastructure roadside-mounted cameras. Unlike conventional 2D or 3D object detection, we employ a prediction approach to detect ground planes of vehicles as parallelograms in the image frame. The parallelogram preserves the planar position, size, and orientation of objects while omitting their height, which is unnecessary for most downstream applications. For training, a mix of real-world and synthetically generated scenes is leveraged. We evaluate generalizability on a held-out camera viewpoint and in adverse-weather scenarios absent from the training set. Our results show high detection accuracy, strong cross-viewpoint generalization, and robustness to diverse lighting and weather conditions. Model weights and inference code are provided at: <a target="_blank" rel="noopener" href="https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection">https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection</a> </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è½¦è½½ä¼ æ„Ÿå™¨å¯èƒ½ä¼šå—åˆ°é®æŒ¡ã€è§†çº¿å—é™æˆ–å…¶ä»–éšœç¢ç‰©çš„å¹²æ‰°ï¼Œä»è€Œå¢åŠ ä¸‹æ¸¸é©¾é©¶å†³ç­–çš„éš¾åº¦ã€‚å®‰è£…åœ¨æœ‰åˆ©ä½ç½®çš„æ™ºèƒ½è·¯è¾¹åŸºç¡€è®¾æ–½æ„ŸçŸ¥ç³»ç»Ÿå¯ä»¥æä¾›å®½é˜”ã€æ— é®æŒ¡çš„äº¤å‰è·¯å£è¦†ç›–èŒƒå›´ï¼Œå¹¶é€šè¿‡è½¦å¯¹ä¸‡ç‰©ï¼ˆV2Xï¼‰é€šä¿¡ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†æä¾›è¡¥å……ä¿¡æ¯æµã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ä¸‰ç»´ç‰©ä½“æ£€æµ‹ç®—æ³•åœ¨è‡ªä¸Šè€Œä¸‹è§†è§’å’Œé™¡å³­ç›¸æœºè§’åº¦å¼•å…¥çš„åŸŸåç§»ä¸‹éš¾ä»¥è¿›è¡Œæ³›åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹åŸºç¡€è®¾æ–½è·¯è¾¹å®‰è£…çš„ç›¸æœºä¸“é—¨è®¾è®¡çš„2.5Dç‰©ä½“æ£€æµ‹æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„äºŒç»´æˆ–ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„æµ‹æ–¹æ³•æ¥æ£€æµ‹å›¾åƒå¸§ä¸­çš„è½¦è¾†åœ°é¢å¹³é¢ä½œä¸ºå¹³è¡Œå››è¾¹å½¢ã€‚å¹³è¡Œå››è¾¹å½¢ä¿ç•™äº†ç‰©ä½“çš„å¹³é¢ä½ç½®ã€å¤§å°å’Œæ–¹ä½ï¼ŒåŒæ—¶çœç•¥äº†é«˜åº¦ä¿¡æ¯ï¼Œè¿™åœ¨å¤§å¤šæ•°ä¸‹æ¸¸åº”ç”¨ä¸­æ˜¯ä¸å¿…è¦çš„ã€‚æˆ‘ä»¬åˆ©ç”¨ç°å®åœºæ™¯å’Œåˆæˆåœºæ™¯çš„ç»“åˆè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬åœ¨ä¸åŒ…æ‹¬è®­ç»ƒé›†çš„å¦ä¸€ç›¸æœºè§†è§’å’Œæ¶åŠ£å¤©æ°”åœºæ™¯ä¸‹è¯„ä¼°æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†è¾ƒé«˜çš„æ£€æµ‹ç²¾åº¦ã€è¾ƒå¼ºçš„è·¨è§†è§’æ³›åŒ–èƒ½åŠ›ä»¥åŠåœ¨ä¸åŒå…‰çº¿å’Œå¤©æ°”æ¡ä»¶ä¸‹çš„é²æ£’æ€§ã€‚æ¨¡å‹æƒé‡å’Œæ¨ç†ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d%E7%8B%AC%E4%BD%BF%E7%9B%AE%E6%A0%BC%E7%BB%AA%E6%BA%90%E6%A3%A2">https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03564v2">PDF</a> Accepted at 2025 IEEE 28th International Conference on Intelligent   Transportation Systems (ITSC)</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è‡ªä¸»é©¾é©¶è½¦è¾†çš„è½¦è½½ä¼ æ„Ÿå™¨å¯èƒ½ä¼šå—åˆ°é®æŒ¡ã€è§†é‡å—é™ç­‰å½±å“ï¼Œå¯¼è‡´é©¾é©¶å†³ç­–å›°éš¾ã€‚æ™ºèƒ½è·¯è¾¹æ„ŸçŸ¥ç³»ç»Ÿå¯ä»¥å®‰è£…åœ¨é«˜ä½è§‚å¯Ÿç‚¹ï¼Œæä¾›æ— é®æŒ¡çš„äº¤å‰å£è¦†ç›–ï¼Œå¹¶é€šè¿‡è½¦å¯¹ä¸‡ç‰©ï¼ˆV2Xï¼‰é€šä¿¡ä¸ºè‡ªä¸»é©¾é©¶è½¦è¾†æä¾›è¡¥å……ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„3Dç›®æ ‡æ£€æµ‹ç®—æ³•åœ¨åº”å¯¹ç”±é¡¶éƒ¨å‘ä¸‹è§†è§’å’Œé™¡å³­ç›¸æœºè§’åº¦å¼•èµ·çš„é¢†åŸŸå˜åŒ–æ—¶éš¾ä»¥å®ç°é€šç”¨åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§2.5Dç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè·¯è¾¹åŸºç¡€è®¾æ–½å®‰è£…çš„ç›¸æœºã€‚æˆ‘ä»¬é‡‡ç”¨é¢„æµ‹æ–¹æ³•æ£€æµ‹è½¦è¾†çš„åœ°é¢å¹³é¢ï¼Œå°†å…¶è¡¨ç¤ºä¸ºå›¾åƒå¸§ä¸­çš„å¹³è¡Œå››è¾¹å½¢ã€‚å¹³è¡Œå››è¾¹å½¢ä¿ç•™äº†ç‰©ä½“çš„å¹³é¢ä½ç½®ã€å¤§å°å’Œæ–¹å‘ï¼ŒåŒæ—¶çœç•¥äº†é«˜åº¦ä¿¡æ¯ï¼Œè¿™å¯¹äºå¤§å¤šæ•°ä¸‹æ¸¸åº”ç”¨æ¥è¯´æ˜¯ä¸å¿…è¦çš„ã€‚æˆ‘ä»¬ä½¿ç”¨çœŸå®åœºæ™¯å’Œåˆæˆåœºæ™¯çš„ç»„åˆæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æœªå‚ä¸è®­ç»ƒçš„ç›¸æœºè§†è§’å’Œæ¶åŠ£å¤©æ°”åœºæ™¯ä¸‹è¯„ä¼°å…¶é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•çš„æ£€æµ‹ç²¾åº¦é«˜ã€è·¨è§†è§’é€šç”¨æ€§å¼ºï¼Œå¯¹ä¸åŒçš„å…‰ç…§å’Œå¤©æ°”æ¡ä»¶å…·æœ‰ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªä¸»é©¾é©¶è½¦è¾†çš„è½¦è½½ä¼ æ„Ÿå™¨å¯èƒ½å—åˆ°é®æŒ¡æˆ–è§†é‡é™åˆ¶ï¼Œå¯¼è‡´é©¾é©¶å†³ç­–å¤æ‚åŒ–ã€‚</li>
<li>æ™ºèƒ½è·¯è¾¹åŸºç¡€è®¾æ–½æ„ŸçŸ¥ç³»ç»Ÿå¯æä¾›æ— é®æŒ¡çš„äº¤å‰å£è§†å›¾ï¼Œé€šè¿‡V2Xé€šä¿¡ä¸ºè‡ªä¸»é©¾é©¶è½¦è¾†æä¾›è¡¥å……ä¿¡æ¯ã€‚</li>
<li>ä¼ ç»Ÿ3Dç›®æ ‡æ£€æµ‹ç®—æ³•åœ¨åº”å¯¹é¢†åŸŸå˜åŒ–æ—¶éš¾ä»¥å®ç°é€šç”¨åŒ–ï¼Œéœ€è¦ç‰¹å®šçš„2.5Dç›®æ ‡æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>æå‡ºçš„2.5Dç›®æ ‡æ£€æµ‹æ¡†æ¶é‡‡ç”¨é¢„æµ‹æ–¹æ³•æ£€æµ‹è½¦è¾†çš„åœ°é¢å¹³é¢ï¼Œè¡¨ç¤ºä¸ºå›¾åƒå¸§ä¸­çš„å¹³è¡Œå››è¾¹å½¢ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰é«˜åº¦çš„æ£€æµ‹ç²¾åº¦å’Œè·¨è§†è§’çš„é€šç”¨æ€§ã€‚</li>
<li>æ¡†æ¶å¯¹ä¸åŒçš„å…‰ç…§å’Œå¤©æ°”æ¡ä»¶å…·æœ‰ç¨³å¥æ€§ï¼Œèƒ½å¤Ÿåœ¨æ¶åŠ£å¤©æ°”åœºæ™¯ä¸‹è¿›è¡Œæœ‰æ•ˆæ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62b91743d5080691b80e91c2df2c14a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67223e5f193dda2e1c8a72024e0d1799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b91c92af46ddf4c3e6e55cea0a1c0dd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-433052a0433b464385ce31e5b34be465.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3e861f79fb08e85880808a093ecae95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2808eff91b338e87af50eac6510d3aed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e573c1c54c0d4f73e5b2beb04816251.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement"><a href="#UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement" class="headerlink" title="UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement"></a>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement</h2><p><strong>Authors:Xiao Zhang, Fei Wei, Yong Wang, Wenda Zhao, Feiyi Li, Xiangxiang Chu</strong></p>
<p>Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE">https://github.com/AMAP-ML/UPRE</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”ï¼ˆZSDAï¼‰ç”±äºç¼ºä¹ç›®æ ‡åŸŸçš„å›¾åƒè€Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œåˆ©ç”¨å…¶é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦è§£å†³åŸŸåˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œè€Œå¿½è§†äº†æ£€æµ‹ä»»åŠ¡ä¸VLMsä¹‹é—´çš„ä¸åŒ¹é…ï¼Œåè€…ä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€æç¤ºå’Œè¡¨ç¤ºå¢å¼ºï¼ˆUPREï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¤šè§†è§’åŸŸæç¤ºï¼Œå®ƒå°†è¯­è¨€åŸŸå…ˆéªŒçŸ¥è¯†ä¸æ£€æµ‹ç‰¹å®šçŸ¥è¯†ç›¸ç»“åˆï¼Œä»¥åŠä¸€ç§è§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼Œç”¨äºç”ŸæˆåŸŸé£æ ¼å˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå±‚æ¬¡å¢å¼ºç­–ç•¥ï¼ŒåŒ…æ‹¬ç›¸å¯¹åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»ï¼Œåˆ†åˆ«åœ¨å›¾åƒçº§åˆ«å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºï¼Œå¹¶åœ¨å®ä¾‹çº§åˆ«æ•è·å¤šæ ·åŒ–çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ZSDAæ£€æµ‹åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE%E3%80%82">https://github.com/AMAP-ML/UPREã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00721v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é›¶æ ·æœ¬é¢†åŸŸè‡ªé€‚åº”ï¼ˆZSDAï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç›®æ ‡é¢†åŸŸç¼ºä¹å›¾åƒçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æç¤ºå’Œè¡¨ç¤ºå¢å¼ºï¼ˆUPREï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆä¼˜åŒ–äº†æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºã€‚UPREæ¡†æ¶é€šè¿‡å¼•å…¥å¤šè§†è§’é¢†åŸŸæç¤ºå’Œè§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—æ¥è§£å†³é¢†åŸŸåˆ†å¸ƒå˜åŒ–å’Œæ£€æµ‹ä»»åŠ¡ä¸è§†è§‰è¯­è¨€æ¨¡å‹ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¤šå±‚æ¬¡å¢å¼ºç­–ç•¥ï¼ŒåŒ…æ‹¬ç›¸å¯¹é¢†åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»ï¼Œä»¥åœ¨å›¾åƒçº§åˆ«å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºå¹¶åœ¨å®ä¾‹çº§åˆ«æ•è·å¤šæ ·åŒ–çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒUPREæ¡†æ¶åœ¨ZSDAæ£€æµ‹åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZSDAé¢ä¸´ç›®æ ‡é¢†åŸŸç¼ºä¹å›¾åƒçš„æŒ‘æˆ˜ã€‚</li>
<li>UPREæ¡†æ¶é€šè¿‡è”åˆä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>UPREå¼•å…¥å¤šè§†è§’é¢†åŸŸæç¤ºï¼Œç»“åˆè¯­è¨€é¢†åŸŸå…ˆéªŒçŸ¥è¯†å’Œæ£€æµ‹ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>UPREæ¡†æ¶åŒ…å«è§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼Œç”¨äºç”Ÿæˆé¢†åŸŸé£æ ¼å˜åŒ–ã€‚</li>
<li>å¤šå±‚æ¬¡å¢å¼ºç­–ç•¥åŒ…æ‹¬ç›¸å¯¹é¢†åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»ï¼Œç”¨äºå¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºå¹¶æ•è·å¤šæ ·åŒ–çš„è§†è§‰è¡¨ç¤ºã€‚</li>
<li>åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜UPREæ¡†æ¶åœ¨ZSDAæ£€æµ‹åœºæ™¯ä¸­æ€§èƒ½å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00721">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76420c74a7f52503f7ce343aecb151f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bbdd914ef07f98a2ab2538d3c00b878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7870f451037050b4697fea48b540040f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7831142b23aeab239215ec5295dd6afb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Position-Prediction-Self-Supervised-Learning-for-Multimodal-Satellite-Imagery-Semantic-Segmentation"><a href="#Position-Prediction-Self-Supervised-Learning-for-Multimodal-Satellite-Imagery-Semantic-Segmentation" class="headerlink" title="Position Prediction Self-Supervised Learning for Multimodal Satellite   Imagery Semantic Segmentation"></a>Position Prediction Self-Supervised Learning for Multimodal Satellite   Imagery Semantic Segmentation</h2><p><strong>Authors:John Waithaka, Moise Busogi</strong></p>
<p>Semantic segmentation of satellite imagery is crucial for Earth observation applications, but remains constrained by limited labelled training data. While self-supervised pretraining methods like Masked Autoencoders (MAE) have shown promise, they focus on reconstruction rather than localisation-a fundamental aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a position prediction self-supervised learning method, for multimodal satellite imagery semantic segmentation. Our approach addresses the unique challenges of satellite data by extending SatMAEâ€™s channel grouping from multispectral to multimodal data, enabling effective handling of multiple modalities, and introducing same-group attention masking to encourage cross-modal interaction during pretraining. The method uses relative patch position prediction, encouraging spatial reasoning for localisation rather than reconstruction. We evaluate our approach on the Sen1Floods11 flood mapping dataset, where it significantly outperforms existing reconstruction-based self-supervised learning methods for satellite imagery. Our results demonstrate that position prediction tasks, when properly adapted for multimodal satellite imagery, learn representations more effective for satellite image semantic segmentation than reconstruction-based approaches. </p>
<blockquote>
<p>å«æ˜Ÿå›¾åƒçš„è¯­ä¹‰åˆ†å‰²å¯¹äºåœ°çƒè§‚æµ‹åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†ä»å—é™äºæ ‡è®°è®­ç»ƒæ•°æ®çš„æœ‰é™æ€§ã€‚è™½ç„¶åƒMasked Autoencodersï¼ˆMAEï¼‰è¿™æ ·çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•åœ¨é‡å»ºæ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨é‡å»ºè€Œéå®šä½ï¼Œè¿™æ˜¯åˆ†å‰²ä»»åŠ¡çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ã€‚æˆ‘ä»¬æå‡ºé€‚åº”LOCAï¼ˆä½ç½®æ„ŸçŸ¥ï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä½ç½®é¢„æµ‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºå¤šæ¨¡æ€å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ‰©å±•SatMAEçš„æ¸ é“åˆ†ç»„ï¼Œä»å¤šå…‰è°±åˆ°å¤šæ¨¡æ€æ•°æ®ï¼Œè§£å†³äº†å«æ˜Ÿæ•°æ®çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå®ç°äº†å¤šç§æ¨¡å¼çš„æœ‰æ•ˆå¤„ç†ï¼Œå¹¶å¼•å…¥äº†åŒç»„æ³¨æ„åŠ›é®è”½ï¼Œä»¥é¼“åŠ±åœ¨é¢„è®­ç»ƒæœŸé—´è¿›è¡Œè·¨æ¨¡æ€äº¤äº’ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ç›¸å¯¹æ–‘å—ä½ç½®é¢„æµ‹ï¼Œé¼“åŠ±å®šä½çš„ç©ºé—´æ¨ç†è€Œéé‡å»ºã€‚æˆ‘ä»¬åœ¨Sen1Floods11æ´ªæ°´æµ‹ç»˜æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨æ´ªæ°´æµ‹ç»˜æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºé‡å»ºçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“é€‚å½“é€‚åº”å¤šæ¨¡æ€å«æ˜Ÿå›¾åƒæ—¶ï¼Œä½ç½®é¢„æµ‹ä»»åŠ¡å¯¹äºå«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²çš„ä»£è¡¨æ€§å­¦ä¹ æ¯”åŸºäºé‡å»ºçš„æ–¹æ³•æ›´æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06852v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²å¯¹åœ°çƒè§‚æµ‹åº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å—é™äºæ ‡è®°è®­ç»ƒæ•°æ®çš„ä¸è¶³ã€‚è™½ç„¶è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚Mask Autoencodersï¼‰å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨é‡å»ºè€Œéå®šä½â€”â€”åˆ†å‰²ä»»åŠ¡çš„åŸºæœ¬æ–¹é¢ã€‚æœ¬æ–‡æå‡ºå°†ä½ç½®æ„ŸçŸ¥çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•LOCAåº”ç”¨äºå¤šæ¨¡æ€å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰©å±•SatMAEçš„æ¸ é“ç»„åˆèƒ½åŠ›ä»¥å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œå¼•å…¥äº†åŒä¸€ç¾¤ä½“æ³¨æ„åŠ›å±è”½æœºåˆ¶æ¥é¼“åŠ±åœ¨é¢„è®­ç»ƒæœŸé—´çš„è·¨æ¨¡æ€äº¤äº’ã€‚æ­¤æ–¹æ³•ä½¿ç”¨ç›¸å¯¹è¡¥ä¸ä½ç½®é¢„æµ‹ï¼Œé¼“åŠ±å®šä½çš„ç©ºé—´æ¨ç†è€Œéé‡å»ºã€‚åœ¨Sen1Floods11æ´ªæ°´æµ‹ç»˜æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºäºé‡å»ºçš„ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œé€‚å½“é€‚åº”å¤šæ¨¡æ€å«æ˜Ÿå›¾åƒçš„å®šä½é¢„æµ‹ä»»åŠ¡ï¼Œå¯¹äºå«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²è€Œè¨€ï¼Œæ¯”åŸºäºé‡å»ºçš„æ–¹æ³•æ›´æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯åœ°çƒè§‚æµ‹åº”ç”¨çš„å…³é”®æŒ‘æˆ˜ï¼Œä¸»è¦å—é™äºæ ‡è®°è®­ç»ƒæ•°æ®çš„ç¼ºä¹ã€‚</li>
<li>è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•å¦‚Mask Autoencodersè™½ç„¶å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨è§£å†³å®šä½é—®é¢˜ä¸Šæ•ˆæœæœ‰é™ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„LOCAæ–¹æ³•æ˜¯ä¸€ç§ä½ç½®æ„ŸçŸ¥è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé’ˆå¯¹å¤šæ¨¡æ€å«æ˜Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ‰©å±•SatMAEçš„æ¸ é“ç»„åˆèƒ½åŠ›å¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œå¹¶å¼•å…¥è·¨æ¨¡æ€äº¤äº’æœºåˆ¶ã€‚</li>
<li>é‡‡ç”¨ç›¸å¯¹è¡¥ä¸ä½ç½®é¢„æµ‹ï¼Œå¼ºè°ƒç©ºé—´æ¨ç†å’Œå®šä½è€Œéå•çº¯çš„å›¾åƒé‡å»ºã€‚</li>
<li>åœ¨Sen1Floods11æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒLOCAæ–¹æ³•æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é‡å»ºå‹è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d18905a481d3fe347e608bb4538c2609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09b00547dd833be894dad6a2fad4543e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9a478c5aaf9107355e7ae07416b39236.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  The Impact of Image Resolution on Face Detection A Comparative Analysis   of MTCNN, YOLOv XI and YOLOv XII models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8577c124f2f2b957f9eb7ed8c1121359.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  UPRE Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
