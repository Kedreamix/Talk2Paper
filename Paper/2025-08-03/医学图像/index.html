<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  OptiGradTrust Byzantine-Robust Federated Learning with Multi-Feature   Gradient Analysis and Reinforcement Learning-Based Trust Weighting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b1b89f0316d4305b5d9eeadc5744ba88.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="OptiGradTrust-Byzantine-Robust-Federated-Learning-with-Multi-Feature-Gradient-Analysis-and-Reinforcement-Learning-Based-Trust-Weighting"><a href="#OptiGradTrust-Byzantine-Robust-Federated-Learning-with-Multi-Feature-Gradient-Analysis-and-Reinforcement-Learning-Based-Trust-Weighting" class="headerlink" title="OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature   Gradient Analysis and Reinforcement Learning-Based Trust Weighting"></a>OptiGradTrust: Byzantine-Robust Federated Learning with Multi-Feature   Gradient Analysis and Reinforcement Learning-Based Trust Weighting</h2><p><strong>Authors:Mohammad Karami, Fatemeh Ghassemi, Hamed Kebriaei, Hamid Azadegan</strong></p>
<p>Federated Learning (FL) enables collaborative model training across distributed medical institutions while preserving patient privacy, but remains vulnerable to Byzantine attacks and statistical heterogeneity. We present OptiGradTrust, a comprehensive defense framework that evaluates gradient updates through a novel six-dimensional fingerprint including VAE reconstruction error, cosine similarity metrics, $L_2$ norm, sign-consistency ratio, and Monte Carlo Shapley value, which drive a hybrid RL-attention module for adaptive trust scoring. To address convergence challenges under data heterogeneity, we develop FedBN-Prox (FedBN-P), combining Federated Batch Normalization with proximal regularization for optimal accuracy-convergence trade-offs. Extensive evaluation across MNIST, CIFAR-10, and Alzheimerâ€™s MRI datasets under various Byzantine attack scenarios demonstrates significant improvements over state-of-the-art defenses, achieving up to +1.6 percentage points over FLGuard under non-IID conditions while maintaining robust performance against diverse attack patterns through our adaptive learning approach. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰èƒ½å¤Ÿåœ¨åˆ†å¸ƒå¼åŒ»ç–—æœºæ„ä¹‹é—´è¿›è¡Œåä½œæ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä¿æŠ¤æ‚£è€…éšç§ï¼Œä½†ä»æ˜“å—åˆ°æ‹œå åº­æ”»å‡»å’Œç»Ÿè®¡å¼‚è´¨æ€§çš„å¨èƒã€‚æˆ‘ä»¬æå‡ºäº†OptiGradTrustï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„é˜²å¾¡æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸€ç§æ–°å‹å…­ç»´æŒ‡çº¹æ¥è¯„ä¼°æ¢¯åº¦æ›´æ–°ï¼ŒåŒ…æ‹¬VAEé‡å»ºè¯¯å·®ã€ä½™å¼¦ç›¸ä¼¼åº¦æŒ‡æ ‡ã€L2èŒƒæ•°ã€ç¬¦å·ä¸€è‡´æ€§æ¯”ç‡ä»¥åŠè’™ç‰¹å¡æ´›æ²™æ™®åˆ©å€¼ï¼Œè¿™äº›æŒ‡æ ‡é©±åŠ¨æ··åˆRL-æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œè‡ªé€‚åº”ä¿¡ä»»è¯„åˆ†ã€‚ä¸ºäº†è§£å†³æ•°æ®å¼‚è´¨æ€§ä¸‹çš„æ”¶æ•›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†FedBN-Proxï¼ˆFedBN-Pï¼‰ï¼Œå®ƒå°†è”é‚¦æ‰¹æ ‡å‡†åŒ–ä¸è¿‘ç«¯æ­£åˆ™åŒ–ç›¸ç»“åˆï¼Œä»¥å®ç°æœ€ä¼˜çš„ç²¾åº¦-æ”¶æ•›æƒè¡¡ã€‚åœ¨MNISTã€CIFAR-10å’Œé˜¿å°”èŒ¨æµ·é»˜ç—‡MRIæ•°æ®é›†ä¸Šçš„å„ç§æ‹œå åº­æ”»å‡»åœºæ™¯ä¸‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„é˜²å¾¡æ‰‹æ®µç›¸æ¯”ï¼Œæˆ‘ä»¬çš„é˜²å¾¡ç­–ç•¥æœ‰ç€æ˜¾è‘—çš„æå‡ã€‚åœ¨éIIDæ¡ä»¶ä¸‹ï¼Œä¸FLGuardç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é«˜è¾¾1.6ä¸ªç™¾åˆ†ç‚¹ï¼›é€šè¿‡æˆ‘ä»¬çš„è‡ªé€‚åº”å­¦ä¹ æ–¹æ³•ï¼Œå¯¹å¤šç§æ”»å‡»æ¨¡å¼ä¿æŒäº†ç¨³å¥çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23638v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰é¢ä¸´æ‹œå åº­æ”»å‡»å’Œç»Ÿè®¡å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚æå‡ºOptiGradTrustæ¡†æ¶ï¼Œé€šè¿‡å…­ç»´æŒ‡çº¹è¯„ä¼°æ¢¯åº¦æ›´æ–°ï¼ŒåŒ…æ‹¬VAEé‡å»ºè¯¯å·®ã€ä½™å¼¦ç›¸ä¼¼åº¦åº¦é‡ã€L2èŒƒæ•°ç­‰ï¼Œé©±åŠ¨æ··åˆRL-æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œè‡ªé€‚åº”ä¿¡ä»»è¯„åˆ†ã€‚ä¸ºè§£å†³æ•°æ®å¼‚è´¨æ€§ä¸‹çš„æ”¶æ•›æŒ‘æˆ˜ï¼Œå¼€å‘FedBN-Proxç»“åˆè”é‚¦æ‰¹å½’ä¸€åŒ–ä¸è¿‘ç«¯æ­£åˆ™åŒ–ï¼Œå®ç°ç²¾åº¦ä¸æ”¶æ•›çš„å¹³è¡¡ã€‚åœ¨MNISTã€CIFAR-10å’Œé˜¿å°”èŒ¨æµ·é»˜ç—‡MRIæ•°æ®é›†ä¸‹çš„æ‹œå åº­æ”»å‡»åœºæ™¯ä¸­è¯„ä¼°è¡¨ç°å“è¶Šï¼Œç›¸è¾ƒäºç°æœ‰é˜²å¾¡æ‰‹æ®µæœ‰æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨éIIDæ¡ä»¶ä¸‹è¾ƒFLGuardæå‡æœ€å¤šè¾¾+1.6ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶èƒ½åœ¨å„ç§æ”»å‡»æ¨¡å¼ä¸‹ä¿æŒç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è”é‚¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸé¢ä¸´æ‹œå åº­æ”»å‡»å’Œç»Ÿè®¡å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>OptiGradTrustæ¡†æ¶é€šè¿‡å…­ç»´æŒ‡çº¹è¯„ä¼°æ¢¯åº¦æ›´æ–°ï¼ŒåŒ…æ‹¬å¤šç§æŒ‡æ ‡å¦‚VAEé‡å»ºè¯¯å·®ç­‰ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨æ··åˆRL-æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œè‡ªé€‚åº”ä¿¡ä»»è¯„åˆ†ã€‚</li>
<li>FedBN-Proxç»“åˆè”é‚¦æ‰¹å½’ä¸€åŒ–ä¸è¿‘ç«¯æ­£åˆ™åŒ–ä»¥è§£å†³æ•°æ®å¼‚è´¨æ€§ä¸‹çš„æ”¶æ•›æŒ‘æˆ˜ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ”»å‡»åœºæ™¯ä¸‹çš„è¯„ä¼°æ˜¾ç¤ºOptiGradTrustè¾ƒç°æœ‰é˜²å¾¡æ‰‹æ®µæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>åœ¨éIIDæ¡ä»¶ä¸‹è¾ƒFLGuardæå‡æœ€å¤šè¾¾+1.6ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ab6ef903a6ec79775b5223df33cfa46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b1bc9d165d90422ff026b134a2bda4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6495deb09bc47036c4d07e8c4e04a027.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b7b2511713689f6de15be07f012a146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-760d300c7a3c9b3badf6ceaa3a855be2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-605a61d25ec3b6b568e7ba1a5cb96649.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fd5963958f80cd8f7b4c60f02dfd4e9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Consistent-Point-Matching"><a href="#Consistent-Point-Matching" class="headerlink" title="Consistent Point Matching"></a>Consistent Point Matching</h2><p><strong>Authors:Halid Ziya Yerebakan, Gerardo Hermosillo Valadez</strong></p>
<p>This study demonstrates that incorporating a consistency heuristic into the point-matching algorithm \cite{yerebakan2023hierarchical} improves robustness in matching anatomical locations across pairs of medical images. We validated our approach on diverse longitudinal internal and public datasets spanning CT and MRI modalities. Notably, it surpasses state-of-the-art results on the Deep Lesion Tracking dataset. Additionally, we show that the method effectively addresses landmark localization. The algorithm operates efficiently on standard CPU hardware and allows configurable trade-offs between speed and robustness. The method enables high-precision navigation between medical images without requiring a machine learning model or training data. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å±•ç¤ºäº†å°†ä¸€è‡´æ€§å¯å‘å¼ç­–ç•¥èå…¥ç‚¹åŒ¹é…ç®—æ³•ï¼ˆå¼•ç”¨è‡ªyerebakan2023hierarchicalï¼‰ä¸­ï¼Œå¯ä»¥æé«˜è·¨å¯¹åŒ»å­¦å›¾åƒåŒ¹é…è§£å‰–ä½ç½®çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨æ¶µç›–CTå’ŒMRIæ¨¡æ€çš„å¤šæ ·çºµå‘å†…éƒ¨å’Œå…¬å¼€æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨Deep Lesion Trackingæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³åœ°æ ‡å®šä½é—®é¢˜ã€‚è¯¥ç®—æ³•åœ¨æ ‡å‡†CPUç¡¬ä»¶ä¸Šè¿è¡Œé«˜æ•ˆï¼Œå…è®¸åœ¨é€Ÿåº¦å’Œç¨³å¥æ€§ä¹‹é—´è¿›è¡Œå¯é…ç½®çš„æƒè¡¡ã€‚è¯¥æ–¹æ³•æ— éœ€æœºå™¨å­¦ä¹ æ¨¡å‹æˆ–è®­ç»ƒæ•°æ®ï¼Œå³å¯å®ç°åŒ»å­¦å›¾åƒä¹‹é—´çš„é«˜ç²¾åº¦å¯¼èˆªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23609v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡å±•ç¤ºäº†åœ¨ç‚¹åŒ¹é…ç®—æ³•ä¸­å¼•å…¥ä¸€è‡´æ€§å¯å‘å¼æ–¹æ³•ï¼Œæé«˜äº†åœ¨ä¸åŒåŒ»å­¦å›¾åƒå¯¹ä¹‹é—´åŒ¹é…è§£å‰–ä½ç½®çš„ç¨³å¥æ€§ã€‚è¯¥ç®—æ³•åœ¨å¤šç§çºµå‘å†…éƒ¨å’Œå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬CTå’ŒMRIæ¨¡æ€ã€‚è¯¥æ–¹æ³•åœ¨Deep Lesion Trackingæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶æœ‰æ•ˆåœ°è§£å†³äº†åœ°æ ‡å®šä½é—®é¢˜ã€‚ç®—æ³•åœ¨æ ‡å‡†CPUç¡¬ä»¶ä¸Šè¿è¡Œé«˜æ•ˆï¼Œå¯å®ç°é€Ÿåº¦å’Œç¨³å¥æ€§ä¹‹é—´çš„å¯é…ç½®æƒè¡¡ã€‚è¯¥æ–¹æ³•æ— éœ€æœºå™¨å­¦ä¹ æ¨¡å‹æˆ–è®­ç»ƒæ•°æ®å³å¯å®ç°åŒ»å­¦å›¾åƒä¹‹é—´çš„é«˜ç²¾åº¦å¯¼èˆªã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ä¸€è‡´æ€§å¯å‘å¼æ–¹æ³•æ”¹è¿›äº†ç‚¹åŒ¹é…ç®—æ³•çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å¤šç§çºµå‘å†…éƒ¨å’Œå…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬CTå’ŒMRIæ¨¡æ€ï¼‰ä¸ŠéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨Deep Lesion Trackingæ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ç®—æ³•è§£å†³äº†åœ°æ ‡å®šä½é—®é¢˜ã€‚</li>
<li>ç®—æ³•åœ¨æ ‡å‡†CPUç¡¬ä»¶ä¸Šè¿è¡Œé«˜æ•ˆã€‚</li>
<li>ç®—æ³•å¯å®ç°é€Ÿåº¦å’Œç¨³å¥æ€§ä¹‹é—´çš„å¯é…ç½®æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-807e2fa9537d5edebf12afb11443cd75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c63c8cd8113694d4a21fbc9888627847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b6df602e52618c959733f401aba9c38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-818b307ac9f8eba8483a760192d47cd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f391d9c9fd0c14a7c9225ef88518a58f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89112110a993bd69930cecd561f010c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73d2826b3794f19bfa663bc6b26be8a9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CADS-A-Comprehensive-Anatomical-Dataset-and-Segmentation-for-Whole-Body-Anatomy-in-Computed-Tomography"><a href="#CADS-A-Comprehensive-Anatomical-Dataset-and-Segmentation-for-Whole-Body-Anatomy-in-Computed-Tomography" class="headerlink" title="CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body   Anatomy in Computed Tomography"></a>CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body   Anatomy in Computed Tomography</h2><p><strong>Authors:Murong Xu, Tamaz Amiranashvili, Fernando Navarro, Maksym Fritsak, Ibrahim Ethem Hamamci, Suprosanna Shit, Bastian Wittmann, Sezgin Er, Sebastian M. Christ, Ezequiel de la Rosa, Julian Deseoe, Robert Graf, Hendrik MÃ¶ller, Anjany Sekuboyina, Jan C. Peeken, Sven Becker, Giulia Baldini, Johannes Haubold, Felix Nensa, RenÃ© Hosch, Nikhil Mirajkar, Saad Khalid, Stefan Zachow, Marc-AndrÃ© Weber, Georg Langs, Jakob Wasserthal, Mehmet Kemal Ozdemir, Andrey Fedorov, Ron Kikinis, Stephanie Tanadini-Lang, Jan S. Kirschke, Stephanie E. Combs, Bjoern Menze</strong></p>
<p>Accurate delineation of anatomical structures in volumetric CT scans is crucial for diagnosis and treatment planning. While AI has advanced automated segmentation, current approaches typically target individual structures, creating a fragmented landscape of incompatible models with varying performance and disparate evaluation protocols. Foundational segmentation models address these limitations by providing a holistic anatomical view through a single model. Yet, robust clinical deployment demands comprehensive training data, which is lacking in existing whole-body approaches, both in terms of data heterogeneity and, more importantly, anatomical coverage. In this work, rather than pursuing incremental optimizations in model architecture, we present CADS, an open-source framework that prioritizes the systematic integration, standardization, and labeling of heterogeneous data sources for whole-body CT segmentation. At its core is a large-scale dataset of 22,022 CT volumes with complete annotations for 167 anatomical structures, representing a significant advancement in both scale and coverage, with 18 times more scans than existing collections and 60% more distinct anatomical targets. Building on this diverse dataset, we develop the CADS-model using established architectures for accessible and automated full-body CT segmentation. Through comprehensive evaluation across 18 public datasets and an independent real-world hospital cohort, we demonstrate advantages over SoTA approaches. Notably, thorough testing of the modelâ€™s performance in segmentation tasks from radiation oncology validates its direct utility for clinical interventions. By making our large-scale dataset, our segmentation models, and our clinical software tool publicly available, we aim to advance robust AI solutions in radiology and make comprehensive anatomical analysis accessible to clinicians and researchers alike. </p>
<blockquote>
<p>åœ¨ä¸‰ç»´CTæ‰«æä¸­å¯¹è§£å‰–ç»“æ„çš„ç²¾ç¡®æç»˜å¯¹è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šè‡³å…³é‡è¦ã€‚è™½ç„¶äººå·¥æ™ºèƒ½å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„å‘å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•é€šå¸¸é’ˆå¯¹å•ä¸ªç»“æ„ï¼Œå¯¼è‡´å‡ºç°äº†åˆ†å‰²æ¨¡å‹è¡¨ç°ä¸ä¸€ã€è¯„ä»·åè®®å„å¼‚çš„ä¸ç»Ÿä¸€å±€é¢ã€‚åŸºç¡€åˆ†å‰²æ¨¡å‹é€šè¿‡å•ä¸€æ¨¡å‹æä¾›æ•´ä½“è§£å‰–è§†å›¾æ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚ç„¶è€Œï¼Œåœ¨ä¸´åºŠä¸Šçš„ç¨³å¥éƒ¨ç½²éœ€è¦å…¨é¢çš„è®­ç»ƒæ•°æ®ï¼Œè€Œç°æœ‰çš„å…¨èº«æ–¹æ³•åœ¨è¿™æ–¹é¢ç¼ºä¹æ•°æ®ï¼Œæ—¢ä½“ç°åœ¨æ•°æ®å¼‚è´¨æ€§ä¸Šï¼Œæ›´é‡è¦çš„æ˜¯ä½“ç°åœ¨è§£å‰–ç»“æ„è¦†ç›–æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¿½æ±‚æ¨¡å‹æ¶æ„çš„å¢é‡ä¼˜åŒ–ï¼Œè€Œæ˜¯æ¨å‡ºäº†CADSï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜å…ˆæ•´åˆã€æ ‡å‡†åŒ–å’Œæ ‡æ³¨å¼‚è´¨æ•°æ®æºçš„å…¨èº«CTåˆ†å‰²å¼€æºæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒ…å«22,022ä¸ªCTä½“ç§¯çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯¹167ä¸ªè§£å‰–ç»“æ„è¿›è¡Œäº†å®Œæ•´æ ‡æ³¨ï¼Œæ— è®ºæ˜¯åœ¨è§„æ¨¡è¿˜æ˜¯è¦†ç›–é¢ä¸Šï¼Œè¿™ä»£è¡¨äº†ä¸€æ¬¡é‡å¤§è¿›æ­¥ï¼Œå…¶åŒ…å«çš„æ‰«ææ¬¡æ•°æ˜¯ç°æœ‰é›†åˆçš„18å€ï¼Œç‹¬ç‰¹çš„è§£å‰–ç›®æ ‡å¢åŠ äº†60%ã€‚åŸºäºè¿™ä¸ªå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å»ºç«‹äº†CADSæ¨¡å‹ï¼Œé‡‡ç”¨å·²å»ºç«‹çš„æ¶æ„è¿›è¡Œå¯è®¿é—®å’Œè‡ªåŠ¨åŒ–çš„å…¨èº«CTåˆ†å‰²ã€‚é€šè¿‡å¯¹18ä¸ªå…¬å…±æ•°æ®é›†å’Œä¸€ä¸ªç‹¬ç«‹çš„çœŸå®ä¸–ç•ŒåŒ»é™¢é˜Ÿåˆ—çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç›¸è¾ƒäºæœ€æ–°æ–¹æ³•çš„å¥½å¤„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨æ”¾å°„è‚¿ç˜¤å­¦ä¸­çš„åˆ†å‰²ä»»åŠ¡æ–¹é¢çš„å‡ºè‰²è¡¨ç°éªŒè¯äº†å…¶åœ¨ä¸´åºŠå¹²é¢„ä¸­çš„ç›´æ¥æ•ˆç”¨ã€‚æˆ‘ä»¬è‡´åŠ›äºå°†å¤§è§„æ¨¡æ•°æ®é›†ã€æˆ‘ä»¬çš„åˆ†å‰²æ¨¡å‹å’Œä¸´åºŠè½¯ä»¶å·¥å…·å…¬å¼€æä¾›ï¼Œä»¥æ¨åŠ¨æ”¾å°„å­¦çš„ç¨³å¥äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆçš„å‘å±•ï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜éƒ½èƒ½è·å¾—å…¨é¢çš„è§£å‰–åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22953v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å®¹ç§¯CTæ‰«æä¸­å¯¹è§£å‰–ç»“æ„è¿›è¡Œç²¾ç¡®æç»˜çš„é‡è¦æ€§ï¼Œå¯¹äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚è™½ç„¶äººå·¥æ™ºèƒ½å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•é€šå¸¸é’ˆå¯¹å•ä¸ªç»“æ„ï¼Œå­˜åœ¨æ¨¡å‹ä¸å…¼å®¹ã€æ€§èƒ½å·®å¼‚å¤§ä»¥åŠè¯„ä¼°åè®®ä¸ç»Ÿä¸€ç­‰é—®é¢˜ã€‚åŸºç¡€åˆ†å‰²æ¨¡å‹é€šè¿‡å•ä¸€æ¨¡å‹æä¾›æ•´ä½“è§£å‰–è§†å›¾æ¥è§£å†³è¿™äº›é™åˆ¶ã€‚ç„¶è€Œï¼Œè¦åœ¨ä¸´åºŠå®è·µä¸­ç¨³å¥åº”ç”¨ä»éœ€è¦å…¨é¢çš„è®­ç»ƒæ•°æ®ï¼Œè€Œç°æœ‰çš„å…¨èº«æ–¹æ³•ç¼ºä¹æ•°æ®å¼‚è´¨æ€§å’Œæ›´é‡è¦çš„è§£å‰–è¦†ç›–ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCADSçš„å¼€æºæ¡†æ¶ï¼Œå®ƒä¼˜å…ˆæ•´åˆã€æ ‡å‡†åŒ–å’Œæ ‡è®°å¼‚è´¨æ•°æ®æºï¼Œç”¨äºå…¨èº«CTåˆ†å‰²ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«22,022ä¸ªCTä½“ç§¯å’Œ167ä¸ªè§£å‰–ç»“æ„çš„å®Œæ•´æ³¨é‡Šï¼Œæ— è®ºåœ¨è§„æ¨¡è¿˜æ˜¯è¦†ç›–èŒƒå›´ä¸Šéƒ½ä»£è¡¨äº†é‡å¤§è¿›å±•ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæœ¬æ–‡å»ºç«‹äº†CADSæ¨¡å‹ï¼Œé‡‡ç”¨ç°æœ‰æ¶æ„å®ç°å¯è®¿é—®å’Œè‡ªåŠ¨åŒ–çš„å…¨èº«CTåˆ†å‰²ã€‚é€šè¿‡è·¨18ä¸ªå…¬å…±æ•°æ®é›†å’Œç‹¬ç«‹åŒ»é™¢é˜Ÿåˆ—çš„å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”çš„ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨è¾å°„è‚¿ç˜¤å­¦ä¸­çš„åˆ†å‰²ä»»åŠ¡ä¸­éªŒè¯äº†å…¶ä¸´åºŠå¹²é¢„çš„ç›´æ¥æ•ˆç”¨ã€‚æœ¬æ–‡å…¬å¼€æä¾›äº†å¤§è§„æ¨¡æ•°æ®é›†ã€åˆ†å‰²æ¨¡å‹å’Œä¸´åºŠè½¯ä»¶å·¥å…·ï¼Œæ—¨åœ¨æ¨åŠ¨æ”¾å°„å­¦ä¸­ç¨³å¥çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼Œä½¿ä¸´åºŠåŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜éƒ½èƒ½è¿›è¡Œç»¼åˆåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨å¯¹äºè¯Šæ–­ä¸æ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰AIæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå¤šé’ˆå¯¹å•ä¸€ç»“æ„ï¼Œå­˜åœ¨æ¨¡å‹ä¸å…¼å®¹ã€æ€§èƒ½å·®å¼‚å¤§ç­‰é—®é¢˜ã€‚</li>
<li>åŸºç¡€åˆ†å‰²æ¨¡å‹æä¾›æ•´ä½“è§£å‰–è§†å›¾æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ä¸´åºŠå®è·µä¸­çš„ç¨³å¥åº”ç”¨éœ€è¦å…¨é¢çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>CADSæ¡†æ¶æ•´åˆã€æ ‡å‡†åŒ–å’Œæ ‡è®°å¼‚è´¨æ•°æ®æºç”¨äºå…¨èº«CTåˆ†å‰²ã€‚</li>
<li>CADSæ¡†æ¶åŒ…å«å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¦†ç›–æ›´å¤šè§£å‰–ç»“æ„å’Œæ›´å¤šæ‰«ææ¬¡æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f270de7566ee66088c4ca71c30df1aa6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4202a3d64b6fb3ce77645dea1f6250bd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LAMA-Net-A-Convergent-Network-Architecture-for-Dual-Domain-Reconstruction"><a href="#LAMA-Net-A-Convergent-Network-Architecture-for-Dual-Domain-Reconstruction" class="headerlink" title="LAMA-Net: A Convergent Network Architecture for Dual-Domain   Reconstruction"></a>LAMA-Net: A Convergent Network Architecture for Dual-Domain   Reconstruction</h2><p><strong>Authors:Chi Ding, Qingchao Zhang, Ge Wang, Xiaojing Ye, Yunmei Chen</strong></p>
<p>We propose a learnable variational model that learns the features and leverages complementary information from both image and measurement domains for image reconstruction. In particular, we introduce a learned alternating minimization algorithm (LAMA) from our prior work, which tackles two-block nonconvex and nonsmooth optimization problems by incorporating a residual learning architecture in a proximal alternating framework. In this work, our goal is to provide a complete and rigorous convergence proof of LAMA and show that all accumulation points of a specified subsequence of LAMA must be Clarke stationary points of the problem. LAMA directly yields a highly interpretable neural network architecture called LAMA-Net. Notably, in addition to the results shown in our prior work, we demonstrate that the convergence property of LAMA yields outstanding stability and robustness of LAMA-Net in this work. We also show that the performance of LAMA-Net can be further improved by integrating a properly designed network that generates suitable initials, which we call iLAMA-Net. To evaluate LAMA-Net&#x2F;iLAMA-Net, we conduct several experiments and compare them with several state-of-the-art methods on popular benchmark datasets for Sparse-View Computed Tomography. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„å˜åˆ†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å­¦ä¹ å›¾åƒå’Œæµ‹é‡åŸŸçš„ç‰¹å¾å¹¶åˆ©ç”¨ä¸¤è€…ä¹‹é—´çš„äº’è¡¥ä¿¡æ¯è¿›è¡Œå›¾åƒé‡å»ºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…ˆå‰å·¥ä½œä¸­æå‡ºçš„å¯å­¦ä¹ äº¤æ›¿æœ€å°åŒ–ç®—æ³•ï¼ˆLAMAï¼‰ï¼Œè¯¥ç®—æ³•é€šè¿‡åœ¨æœ‰è¿‘ç«¯äº¤æ›¿æ¡†æ¶ä¸­èå…¥æ®‹å·®å­¦ä¹ æ¶æ„ï¼Œè§£å†³äº†ä¸¤å—éå‡¸å’Œéå¹³æ»‘ä¼˜åŒ–é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æä¾›LAMAçš„å®Œæ•´ä¸”ä¸¥æ ¼çš„æ”¶æ•›æ€§è¯æ˜ï¼Œå¹¶è¡¨æ˜LAMAæŒ‡å®šå­åºåˆ—çš„æ‰€æœ‰ç´¯ç§¯ç‚¹éƒ½å¿…é¡»æ˜¯è¯¥é—®é¢˜çš„Clarkeç¨³å®šç‚¹ã€‚LAMAç›´æ¥äº§ç”Ÿäº†ä¸€ç§é«˜åº¦å¯è§£é‡Šçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºLAMA-Netã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé™¤äº†å…ˆå‰å·¥ä½œä¸­çš„ç»“æœä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜LAMAçš„æ”¶æ•›æ€§åœ¨æœ¬ç ”ç©¶ä¸­ä¸ºLAMA-Netå¸¦æ¥äº†å‡ºè‰²çš„ç¨³å®šæ€§å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œé€šè¿‡æ•´åˆé€‚å½“è®¾è®¡çš„ç½‘ç»œæ¥ç”Ÿæˆåˆé€‚çš„åˆå§‹å€¼ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜LAMA-Netçš„æ€§èƒ½ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºiLAMA-Netã€‚ä¸ºäº†è¯„ä¼°LAMA-Net&#x2F;iLAMA-Netï¼Œæˆ‘ä»¬è¿›è¡Œäº†å‡ é¡¹å®éªŒï¼Œå¹¶å°†å®ƒä»¬ä¸ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«ææµè¡ŒåŸºå‡†æ•°æ®é›†ä¸Šçš„å‡ ç§æœ€æ–°æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22316v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2410.21111</p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§å¯å­¦ä¹ çš„å˜åˆ†æ¨¡å‹ï¼Œç»“åˆå›¾åƒå’Œæµ‹é‡é¢†åŸŸçš„ç‰¹å¾ï¼Œå¹¶å€ŸåŠ©äº’è¡¥ä¿¡æ¯è¿›è¡Œå›¾åƒé‡å»ºã€‚å¼•å…¥å…ˆå‰å·¥ä½œä¸­çš„å¯å­¦ä¹ äº¤æ›¿æœ€å°åŒ–ç®—æ³•ï¼ˆLAMAï¼‰ï¼Œè§£å†³ä¸¤å—éå‡¸å’Œéå¹³æ»‘ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶åœ¨è¿‘ç«¯äº¤æ›¿æ¡†æ¶ä¸­èå…¥æ®‹å·®å­¦ä¹ æ¶æ„ã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯æä¾›LAMAçš„å®Œæ•´æ”¶æ•›æ€§è¯æ˜ï¼Œå¹¶å±•ç¤ºLAMAæ‰€æœ‰ç´¯ç§¯ç‚¹éƒ½æ˜¯é—®é¢˜çš„Clarkeç¨³å®šç‚¹ã€‚LAMAç›´æ¥äº§ç”Ÿé«˜åº¦å¯è§£é‡Šçš„ç¥ç»ç½‘ç»œæ¶æ„â€”â€”LAMA-Netã€‚é™¤å…ˆå‰çš„å·¥ä½œæˆæœå¤–ï¼Œæœ¬å·¥ä½œè¿˜å±•ç¤ºäº†LAMAçš„æ”¶æ•›æ€§ä¸ºLAMA-Netçš„ç¨³å®šæ€§å’Œç¨³å¥æ€§æä¾›äº†æœ‰åŠ›ä¿éšœã€‚é€šè¿‡æ•´åˆé€‚å½“è®¾è®¡çš„ç½‘ç»œæ¥ç”Ÿæˆåˆå§‹å€¼ï¼Œè¿›ä¸€æ­¥æé«˜LAMA-Netçš„æ€§èƒ½ï¼Œç§°ä¹‹ä¸ºiLAMA-Netã€‚é€šè¿‡åœ¨æµè¡Œçš„ç¨€ç–è§†å›¾è®¡ç®—æœºæ–­å±‚æ‰«æåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œè¯„ä¼°LAMA-Net&#x2F;iLAMA-Netä¸æœ€æ–°æŠ€æœ¯æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¯å­¦ä¹ çš„å˜åˆ†æ¨¡å‹ï¼Œç»“åˆå›¾åƒå’Œæµ‹é‡é¢†åŸŸçš„ç‰¹å¾è¿›è¡Œå›¾åƒé‡å»ºã€‚</li>
<li>é‡‡ç”¨å…ˆå‰å·¥ä½œä¸­çš„å¯å­¦ä¹ äº¤æ›¿æœ€å°åŒ–ç®—æ³•ï¼ˆLAMAï¼‰è§£å†³éå‡¸å’Œéå¹³æ»‘ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LAMAç®—æ³•æä¾›å®Œæ•´æ”¶æ•›æ€§è¯æ˜ï¼Œç´¯ç§¯ç‚¹æ˜¯Clarkeç¨³å®šç‚¹ã€‚</li>
<li>LAMAäº§ç”Ÿé«˜åº¦å¯è§£é‡Šçš„ç¥ç»ç½‘ç»œæ¶æ„â€”â€”LAMA-Netã€‚</li>
<li>é™¤å…ˆå‰å·¥ä½œå¤–ï¼Œå±•ç¤ºäº†LAMAçš„æ”¶æ•›æ€§å¢å¼ºäº†LAMA-Netçš„ç¨³å®šæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡æ•´åˆé€‚å½“è®¾è®¡çš„ç½‘ç»œç”Ÿæˆåˆå§‹å€¼ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œå½¢æˆiLAMA-Netã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-24eb1abad25233b4a0f3495dd56d6d02.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multivariate-Spatio-temporal-Modelling-for-Completing-Cancer-Registries-and-Forecasting-Incidence"><a href="#Multivariate-Spatio-temporal-Modelling-for-Completing-Cancer-Registries-and-Forecasting-Incidence" class="headerlink" title="Multivariate Spatio-temporal Modelling for Completing Cancer Registries   and Forecasting Incidence"></a>Multivariate Spatio-temporal Modelling for Completing Cancer Registries   and Forecasting Incidence</h2><p><strong>Authors:Garazi Retegui, Jaione Etxeberria, MarÃ­a Dolores Ugarte</strong></p>
<p>Cancer data, particularly cancer incidence and mortality, are fundamental to understand the cancer burden, to set targets for cancer control and to evaluate the evolution of the implementation of a cancer control policy. However, the complexity of data collection, classification, validation and processing result in cancer incidence figures often lagging two to three years behind the calendar year. In response, national or regional population-based cancer registries (PBCRs) are increasingly interested in methods for forecasting cancer incidence. However, in many countries there is an additional difficulty in projecting cancer incidence as regional registries are usually not established in the same year and therefore cancer incidence data series between different regions of a country are not harmonised over time. This study addresses the challenge of forecasting cancer incidence with incomplete data at both regional and national levels. To achieve our objective, we propose the use of multivariate spatio-temporal shared component models that jointly model mortality data and available cancer incidence data. The performance of these multivariate models are analyzed using lung cancer incidence data, together with the number of deaths reported in England in the period 2001-2019. Different model predictive measures have been calculated to select the best model. </p>
<blockquote>
<p>ç™Œç—‡æ•°æ®ï¼Œç‰¹åˆ«æ˜¯ç™Œç—‡å‘ç—…ç‡å’Œæ­»äº¡ç‡æ•°æ®ï¼Œå¯¹äºäº†è§£ç™Œç—‡è´Ÿæ‹…ã€è®¾å®šç™Œç—‡æ§åˆ¶ç›®æ ‡ä»¥åŠè¯„ä¼°ç™Œç—‡æ§åˆ¶æ”¿ç­–çš„å®æ–½è¿›å±•è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ•°æ®æ”¶é›†ã€åˆ†ç±»ã€éªŒè¯å’Œå¤„ç†çš„å¤æ‚æ€§å¯¼è‡´ç™Œç—‡å‘ç—…ç‡æ•°æ®é€šå¸¸æ»åäºæ—¥å†å¹´ä»½ä¸¤åˆ°ä¸‰å¹´ã€‚å› æ­¤ï¼Œå›½å®¶æˆ–åŒºåŸŸæ€§äººå£ä¸ºåŸºç¡€çš„ç™Œç—‡ç™»è®°å¤„ï¼ˆPBCRsï¼‰è¶Šæ¥è¶Šå…³æ³¨ç™Œç—‡å‘ç—…ç‡çš„é¢„æµ‹æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šå›½å®¶ï¼Œé¢„æµ‹ç™Œç—‡å‘ç—…ç‡è¿˜å­˜åœ¨é¢å¤–å›°éš¾ï¼Œå› ä¸ºåŒºåŸŸæ€§ç™»è®°é€šå¸¸ä¸æ˜¯åœ¨åŒä¸€å¹´ä»½å»ºç«‹çš„ï¼Œå› æ­¤ï¼Œä¸€ä¸ªå›½å®¶ä¸åŒåŒºåŸŸä¹‹é—´çš„ç™Œç—‡å‘ç—…ç‡æ•°æ®ç³»åˆ—åœ¨æ—¶é—´ä¸Šå¹¶æ²¡æœ‰ç»Ÿä¸€ã€‚æœ¬ç ”ç©¶è§£å†³äº†åœ¨åŒºåŸŸå’Œå›½å®¶å±‚é¢æ•°æ®ä¸å®Œæ•´çš„æƒ…å†µä¸‹é¢„æµ‹ç™Œç—‡å‘ç—…ç‡çš„æŒ‘æˆ˜ã€‚ä¸ºäº†å®ç°æˆ‘ä»¬çš„ç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¤šå…ƒæ—¶ç©ºå…±äº«æˆåˆ†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒæ—¶å¯¹æ­»äº¡æ•°æ®å’Œå¯ç”¨çš„ç™Œç—‡å‘ç—…ç‡æ•°æ®è¿›è¡Œå»ºæ¨¡ã€‚è¿™äº›å¤šå…ƒæ¨¡å‹çš„è¡¨ç°é€šè¿‡åˆ†æè‚ºç™Œå‘ç—…ç‡æ•°æ®å’Œ2001-2019å¹´è‹±æ ¼å…°çš„æ­»äº¡æŠ¥å‘Šæ•°æ®æ¥è¯„ä¼°ã€‚å·²ç»è®¡ç®—äº†ä¸åŒçš„æ¨¡å‹é¢„æµ‹æŒ‡æ ‡æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21714v1">PDF</a> 36 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ç™Œç—‡æ•°æ®çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬ç™Œç—‡å‘ç—…ç‡å’Œæ­»äº¡ç‡æ•°æ®å¯¹äºäº†è§£ç™Œç—‡è´Ÿæ‹…ã€è®¾å®šç™Œç—‡æ§åˆ¶ç›®æ ‡ä»¥åŠè¯„ä¼°ç™Œç—‡æ§åˆ¶æ”¿ç­–å®æ–½æƒ…å†µçš„ä½œç”¨ã€‚ç”±äºæ•°æ®æ”¶é›†ã€åˆ†ç±»ã€éªŒè¯å’Œå¤„ç†çš„å¤æ‚æ€§ï¼Œç™Œç—‡å‘ç—…ç‡æ•°æ®å¾€å¾€æ»åäºæ—¥å†å¹´ä»½ä¸¤åˆ°ä¸‰å¹´ã€‚å› æ­¤ï¼Œå›½å®¶æˆ–åŒºåŸŸæ€§çš„äººå£åŸºç¡€ç™Œç—‡ç™»è®°å¤„ï¼ˆPBCRsï¼‰è¶Šæ¥è¶Šå…³æ³¨é¢„æµ‹ç™Œç—‡å‘ç—…ç‡çš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³åŒºåŸŸå’Œå›½å®¶å±‚é¢æ•°æ®ä¸å®Œæ•´æƒ…å†µä¸‹é¢„æµ‹ç™Œç—‡å‘ç—…ç‡çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å¤šå…ƒæ—¶ç©ºå…±äº«æˆåˆ†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è”åˆå»ºæ¨¡æ­»äº¡æ•°æ®å’Œå¯ç”¨çš„ç™Œç—‡å‘ç—…ç‡æ•°æ®ã€‚åˆ©ç”¨è‹±æ ¼å…°2001-2019å¹´çš„è‚ºç™Œå‘ç—…ç‡æ•°æ®å’Œæ­»äº¡äººæ•°æ•°æ®ï¼Œå¯¹å¤šå…ƒæ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†åˆ†æï¼Œé€šè¿‡è®¡ç®—ä¸åŒçš„æ¨¡å‹é¢„æµ‹æŒ‡æ ‡æ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç™Œç—‡æ•°æ®ï¼Œç‰¹åˆ«æ˜¯å‘ç—…ç‡å’Œæ­»äº¡ç‡æ•°æ®ï¼Œå¯¹äºäº†è§£ç™Œç—‡è´Ÿæ‹…ã€è®¾å®šæ§åˆ¶ç›®æ ‡å’Œè¯„ä¼°æ”¿ç­–å®æ–½è‡³å…³é‡è¦ã€‚</li>
<li>ç™Œç—‡æ•°æ®å­˜åœ¨æ”¶é›†ã€åˆ†ç±»ã€éªŒè¯å’Œå¤„ç†çš„å¤æ‚æ€§ï¼Œå¯¼è‡´ç™Œç—‡å‘ç—…ç‡æ•°æ®ç»å¸¸æ»åã€‚</li>
<li>äººå£åŸºç¡€ç™Œç—‡ç™»è®°å¤„ï¼ˆPBCRsï¼‰æ­£è¶Šæ¥è¶Šå¤šåœ°å…³æ³¨é¢„æµ‹ç™Œç—‡å‘ç—…ç‡çš„æ–¹æ³•ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†å¤šå…ƒæ—¶ç©ºå…±äº«æˆåˆ†æ¨¡å‹ä»¥é¢„æµ‹ç™Œç—‡å‘ç—…ç‡ï¼Œè¯¥æ¨¡å‹å¯è”åˆå»ºæ¨¡æ­»äº¡æ•°æ®å’Œç™Œç—‡å‘ç—…ç‡æ•°æ®ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨çš„æ•°æ®é›†ä¸ºè‹±æ ¼å…°çš„è‚ºç™Œå‘ç—…ç‡æ•°æ®å’Œ2001-2019å¹´çš„æ­»äº¡äººæ•°æ•°æ®ã€‚</li>
<li>é€šè¿‡åˆ†æä¸åŒæ¨¡å‹é¢„æµ‹æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½å¹¶é€‰æ‹©æœ€ä½³æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2835071d0416d4fbbaf3cea836335e47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34c7cb08a516563a7168ac3306d467a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb9c0eea665002ec82cd469ab74f32f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-059aef726fd7cc341dbd873f30f4a770.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a393feb7806be2dcb5c10abc074ce20f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e1989fc6ad1ce08bcd82a5880951eb9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging"><a href="#Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging" class="headerlink" title="Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging"></a>Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging</h2><p><strong>Authors:Maoquan Zhang, Bisser Raytchev, Xiujuan Sun</strong></p>
<p>Medical image segmentation requires not only accuracy but also robustness under challenging imaging conditions. In this study, we show that a carefully configured DeepLabv3 model can achieve high performance in segmenting induced pluripotent stem (iPS) cell colonies, and, under our experimental conditions, outperforms large-scale foundation models such as SAM2 and its medical variant MedSAM2 without structural modifications. These results suggest that, for specialized tasks characterized by subtle, low-contrast boundaries, increased model complexity does not necessarily translate to better performance. Our work revisits the assumption that ever-larger and more generalized architectures are always preferable, and provides evidence that appropriately adapted, simpler models may offer strong accuracy and practical reliability in domain-specific biomedical applications. We also offer an open-source implementation that includes strategies for small datasets and domain-specific encoding, with the aim of supporting further advances in semantic segmentation for regenerative medicine and related fields. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸ä»…éœ€è¦å‡†ç¡®æ€§ï¼Œè¿˜éœ€è¦åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æˆåƒæ¡ä»¶ä¸‹ä¿æŒç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰èŒè½æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬çš„å®éªŒæ¡ä»¶ä¸‹ï¼Œæ— éœ€ç»“æ„ä¿®æ”¹å³å¯è¶…è¶Šå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ï¼Œå¦‚SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºç‰¹å¾ä¸ºç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œçš„ä¸“é¡¹ä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šè½¬åŒ–ä¸ºæ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œé‡æ–°è€ƒè™‘äº†å‡è®¾æ›´å¤§ã€æ›´é€šç”¨çš„æ¶æ„å§‹ç»ˆæ˜¯å¯å–çš„ï¼Œå¹¶æä¾›äº†è¯æ®æ”¯æŒï¼Œé€‚å½“ç®€åŒ–å¹¶é€‚åº”çš„æ¨¡å‹å¯èƒ½åœ¨ç‰¹å®šé¢†åŸŸçš„ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­æä¾›å¼ºå¤§çš„å‡†ç¡®æ€§å’Œå®é™…å¯é æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºå®ç°ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸçš„ç¼–ç ç­–ç•¥ï¼Œæ—¨åœ¨æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21608v1">PDF</a> 19th International Conference on Machine Vision Applications MVA2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸ä»…éœ€è¦å‡†ç¡®æ€§ï¼Œè¿˜éœ€åœ¨å¤æ‚çš„æˆåƒæ¡ä»¶ä¸‹ä¿æŒç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰èŒè½æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œå¹¶åœ¨å®éªŒæ¡ä»¶ä¸‹ï¼Œæœªç»ç»“æ„ä¿®æ”¹å³è¶…è¶Šäº†å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ï¼Œå¦‚SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ã€‚å¯¹äºå…·æœ‰ç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œç‰¹å¾çš„ä¸“ä¸šä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šèƒ½æé«˜æ€§èƒ½ã€‚ç ”ç©¶é‡æ–°å®¡è§†äº†æ›´å¤§çš„ã€æ›´é€šç”¨çš„æ¶æ„æ€»æ˜¯æ›´å¯å–è¿™ä¸€å‡è®¾ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜é€‚å½“ç®€åŒ–æ¨¡å‹å¯èƒ½åœ¨ç‰¹å®šé¢†åŸŸçš„ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­æä¾›å¼ºå¤§çš„å‡†ç¡®æ€§å’Œå®é™…å¯é æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å¼€æºå®ç°ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸç¼–ç çš„ç­–ç•¥ï¼Œæ—¨åœ¨æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²éœ€è¦å…¼é¡¾å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æˆåƒæ¡ä»¶ä¸‹ã€‚</li>
<li>ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²iPSç»†èƒèŒè½æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨å®éªŒæ¡ä»¶ä¸‹ï¼ŒDeepLabv3æ¨¡å‹æ€§èƒ½è¶…è¶Šäº†å¤§è§„æ¨¡çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚SAM2å’ŒMedSAM2ã€‚</li>
<li>å¯¹äºå…·æœ‰ç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œç‰¹å¾çš„ä¸“ä¸šä»»åŠ¡ï¼Œæ¨¡å‹å¤æ‚æ€§å¹¶éæ€§èƒ½æå‡çš„å…³é”®ã€‚</li>
<li>ç ”ç©¶é‡æ–°è¯„ä¼°äº†å¤§è§„æ¨¡ã€é€šç”¨æ¶æ„æ¨¡å‹æ˜¯å¦æ€»æ˜¯æ›´ä¼˜çš„é—®é¢˜ï¼Œæå‡ºé€‚å½“ç®€åŒ–çš„æ¨¡å‹å¯èƒ½åœ¨ç‰¹å®šç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½å’Œå¯é æ€§ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å¼€æºå®ç°ï¼ŒåŒ…å«é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸç¼–ç çš„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e87f14ec6885586249392bd244b7b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d9642ce16b584c3532f561fa244da6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f4451222ff3b4febb53467c771cdf53e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Cross-image-Semantic-Consistency-with-Self-aware-Pseudo-Labeling-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Dual-Cross-image-Semantic-Consistency-with-Self-aware-Pseudo-Labeling-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling   for Semi-supervised Medical Image Segmentation"></a>Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling   for Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Han Wu, Chong Wang, Zhiming Cui</strong></p>
<p>Semi-supervised learning has proven highly effective in tackling the challenge of limited labeled training data in medical image segmentation. In general, current approaches, which rely on intra-image pixel-wise consistency training via pseudo-labeling, overlook the consistency at more comprehensive semantic levels (e.g., object region) and suffer from severe discrepancy of extracted features resulting from an imbalanced number of labeled and unlabeled data. To overcome these limitations, we present a new \underline{Du}al \underline{C}ross-\underline{i}mage \underline{S}emantic \underline{C}onsistency (DuCiSC) learning framework, for semi-supervised medical image segmentation. Concretely, beyond enforcing pixel-wise semantic consistency, DuCiSC proposes dual paradigms to encourage region-level semantic consistency across: 1) labeled and unlabeled images; and 2) labeled and fused images, by explicitly aligning their prototypes. Relying on the dual paradigms, DuCiSC can effectively establish consistent cross-image semantics via prototype representations, thereby addressing the feature discrepancy issue. Moreover, we devise a novel self-aware confidence estimation strategy to accurately select reliable pseudo labels, allowing for exploiting the training dynamics of unlabeled data. Our DuCiSC method is extensively validated on four datasets, including two popular binary benchmarks in segmenting the left atrium and pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a challenging scenario of segmenting the inferior alveolar nerve that features complicated anatomical structures, showing superior segmentation results over previous state-of-the-art approaches. Our code is publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/ShanghaiTech-IMPACT/DuCiSC%7D%7Bhttps://github.com/ShanghaiTech-IMPACT/DuCiSC%7D">https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}</a>. </p>
<blockquote>
<p>åŠç›‘ç£å­¦ä¹ å·²ç»è¯æ˜åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ ‡ç­¾è®­ç»ƒæ•°æ®æœ‰é™çš„é—®é¢˜æ—¶éå¸¸æœ‰æ•ˆã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºé€šè¿‡ä¼ªæ ‡ç­¾è¿›è¡Œå›¾åƒå†…åƒç´ çº§çš„ä¸€è‡´æ€§è®­ç»ƒï¼Œè€Œå¿½ç•¥äº†æ›´å…¨é¢çš„è¯­ä¹‰çº§åˆ«çš„ä¸€è‡´æ€§ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡åŒºåŸŸï¼‰ï¼Œå¹¶å› æ ‡ç­¾å’Œæ— æ ‡ç­¾æ•°æ®æ•°é‡ä¸å¹³è¡¡è€Œé­å—æ‰€æå–ç‰¹å¾çš„ä¸¥é‡å·®å¼‚å½±å“ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Dual Cross-Image Semantic Consistencyï¼ˆDuCiSCï¼‰åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé™¤äº†å¼ºåˆ¶åƒç´ çº§è¯­ä¹‰ä¸€è‡´æ€§å¤–ï¼ŒDuCiSCè¿˜æå‡ºäº†ä¸¤ç§èŒƒå¼æ¥é¼“åŠ±åŒºåŸŸçº§è¯­ä¹‰ä¸€è‡´æ€§ï¼šï¼ˆ1ï¼‰è·¨æ ‡ç­¾å’Œæ— æ ‡ç­¾å›¾åƒï¼›ï¼ˆ2ï¼‰è·¨æ ‡ç­¾å’Œèåˆå›¾åƒï¼Œé€šè¿‡æ˜ç¡®åœ°å¯¹å…¶åŸå‹è¿›è¡Œå¯¹é½ã€‚ä¾èµ–äºåŒé‡èŒƒå¼ï¼ŒDuCiSCå¯ä»¥é€šè¿‡åŸå‹è¡¨ç¤ºæœ‰æ•ˆåœ°å»ºç«‹ä¸€è‡´çš„è·¨å›¾åƒè¯­ä¹‰ï¼Œä»è€Œè§£å†³ç‰¹å¾å·®å¼‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„è‡ªæ„ŸçŸ¥ç½®ä¿¡åº¦ä¼°è®¡ç­–ç•¥ï¼Œä»¥å‡†ç¡®é€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„è®­ç»ƒåŠ¨æ€ã€‚æˆ‘ä»¬çš„DuCiSCæ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæµè¡Œçš„å·¦å¿ƒæˆ¿å’Œèƒ°è…ºåˆ†å‰²äºŒå…ƒåŸºå‡†æµ‹è¯•ã€ä¸€ä¸ªç”¨äºè‡ªåŠ¨å¿ƒè„è¯Šæ–­çš„å¤šç±»æŒ‘æˆ˜æ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªå…·æœ‰å¤æ‚è§£å‰–ç»“æ„çš„ä¸‹ç‰™æ§½ç¥ç»åˆ†å‰²æŒ‘æˆ˜åœºæ™¯ã€‚å®ƒè¡¨ç°å‡ºä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ShanghaiTech-IMPACT/DuCiSC%E3%80%82">https://github.com/ShanghaiTech-IMPACT/DuCiSCã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21440v1">PDF</a> IEEE TMI</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£å­¦ä¹ åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰é™çš„æ ‡è®°è®­ç»ƒæ•°æ®é—®é¢˜æ—¶è¡¨ç°é«˜æ•ˆã€‚ç°æœ‰æ–¹æ³•å¤šä¾èµ–äºä¼ªæ ‡ç­¾æŠ€æœ¯çš„åƒç´ çº§ä¸€è‡´æ€§è®­ç»ƒï¼Œä½†å¿½è§†äº†æ›´å…¨é¢çš„è¯­ä¹‰çº§åˆ«ä¸€è‡´æ€§ï¼Œä¸”åœ¨å¤„ç†æ ‡è®°ä¸æœªæ ‡è®°æ•°æ®ç‰¹å¾ä¸å¹³è¡¡æ—¶ä¼šå‡ºç°ä¸¥é‡åå·®ã€‚æœ¬ç ”ç©¶æå‡ºäº†æ–°å‹çš„DuCiSCï¼ˆåŒè·¨å›¾åƒè¯­ä¹‰ä¸€è‡´æ€§ï¼‰åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²å­¦ä¹ æ¡†æ¶ã€‚é™¤äº†åƒç´ çº§è¯­ä¹‰ä¸€è‡´æ€§å¤–ï¼ŒDuCiSCè¿˜é€šè¿‡åŸå‹å¯¹é½æå‡ºäº†è·¨å›¾åƒçš„åŒºåŸŸçº§è¯­ä¹‰ä¸€è‡´æ€§ï¼Œè§£å†³äº†ç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ã€‚æ­¤å¤–ï¼ŒDuCiSCè¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„è‡ªæˆ‘æ„ŸçŸ¥ç½®ä¿¡åº¦ä¼°è®¡ç­–ç•¥ï¼Œä»¥å‡†ç¡®é€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾æ¥åˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„è®­ç»ƒåŠ¨æ€ã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDuCiSCçš„åˆ†å‰²ç»“æœä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ‰é™æ ‡è®°æ•°æ®æŒ‘æˆ˜æ—¶æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¾èµ–ä¼ªæ ‡ç­¾æŠ€æœ¯çš„åƒç´ çº§ä¸€è‡´æ€§è®­ç»ƒï¼Œä½†å¿½è§†äº†æ›´å…¨é¢çš„è¯­ä¹‰çº§åˆ«ä¸€è‡´æ€§ã€‚</li>
<li>DuCiSCæ¡†æ¶é€šè¿‡åŸå‹å¯¹é½æå‡ºè·¨å›¾åƒçš„åŒºåŸŸçº§è¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»¥è§£å†³ç‰¹å¾ä¸ä¸€è‡´é—®é¢˜ã€‚</li>
<li>DuCiSCè®¾è®¡äº†ä¸€ç§è‡ªæˆ‘æ„ŸçŸ¥ç½®ä¿¡åº¦ä¼°è®¡ç­–ç•¥ï¼Œå‡†ç¡®é€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>DuCiSCåœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>DuCiSCæ¡†æ¶å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œé€‚ç”¨äºä¸åŒç±»å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e9689c44e1f8c735590b033e21f41b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4f9813195811217391b06e01f3e5847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e8e57878a606add81b6bea8581d3376.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-and-Accelerating-Brain-MRI-through-Deep-Learning-Reconstruction-Using-Prior-Subject-Specific-Imaging"><a href="#Enhancing-and-Accelerating-Brain-MRI-through-Deep-Learning-Reconstruction-Using-Prior-Subject-Specific-Imaging" class="headerlink" title="Enhancing and Accelerating Brain MRI through Deep Learning   Reconstruction Using Prior Subject-Specific Imaging"></a>Enhancing and Accelerating Brain MRI through Deep Learning   Reconstruction Using Prior Subject-Specific Imaging</h2><p><strong>Authors:Amirmohammad Shamaei, Alexander Stebner,  Salome,  Bosshart, Johanna Ospel, Gouri Ginde, Mariana Bento, Roberto Souza</strong></p>
<p>Magnetic resonance imaging (MRI) is a crucial medical imaging modality. However, long acquisition times remain a significant challenge, leading to increased costs, and reduced patient comfort. Recent studies have shown the potential of using deep learning models that incorporate information from prior subject-specific MRI scans to improve reconstruction quality of present scans. Integrating this prior information requires registration of the previous scan to the current image reconstruction, which can be time-consuming. We propose a novel deep-learning-based MRI reconstruction framework which consists of an initial reconstruction network, a deep registration model, and a transformer-based enhancement network. We validated our method on a longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18 subjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics confirmed our approachâ€™s superiority over existing methods (p &lt; 0.05, Wilcoxon signed-rank test). Furthermore, we analyzed the impact of our MRI reconstruction method on the downstream task of brain segmentation and observed improved accuracy and volumetric agreement with reference segmentations. Our approach also achieved a substantial reduction in total reconstruction time compared to methods that use traditional registration algorithms, making it more suitable for real-time clinical applications. The code associated with this work is publicly available at <a target="_blank" rel="noopener" href="https://github.com/amirshamaei/longitudinal-mri-deep-recon">https://github.com/amirshamaei/longitudinal-mri-deep-recon</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸€ç§å…³é”®çš„åŒ»å­¦æˆåƒæ–¹å¼ã€‚ç„¶è€Œï¼Œé•¿æ—¶é—´çš„é‡‡é›†ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´äº†æˆæœ¬çš„å¢åŠ å’Œæ‚£è€…èˆ’é€‚åº¦çš„é™ä½ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ç»“åˆæ¥è‡ªå…ˆå‰ç‰¹å®šä¸»é¢˜çš„MRIæ‰«æä¿¡æ¯ï¼Œå¯ä»¥æé«˜å½“å‰æ‰«æçš„é‡å»ºè´¨é‡ã€‚é›†æˆæ­¤å…ˆå‰ä¿¡æ¯éœ€è¦è¿›è¡Œå…ˆå‰æ‰«æä¸å½“å‰å›¾åƒé‡å»ºä¹‹é—´çš„é…å‡†ï¼Œè¿™å¯èƒ½ä¼šå¾ˆè€—æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„MRIé‡å»ºæ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬åˆå§‹é‡å»ºç½‘ç»œã€æ·±åº¦é…å‡†æ¨¡å‹å’ŒåŸºäºå˜å‹å™¨çš„å¢å¼ºç½‘ç»œã€‚æˆ‘ä»¬åœ¨çºµå‘æ•°æ®é›†ä¸Šå¯¹T1åŠ æƒMRIæ‰«æçš„2808å¼ å›¾åƒè¿›è¡Œäº†éªŒè¯ï¼Œè¿™äº›å›¾åƒæ¥è‡ªå››ä¸ªåŠ é€Ÿå› å­ï¼ˆR5ã€R10ã€R15ã€R20ï¼‰çš„18ä¸ªå—è¯•è€…ã€‚å®šé‡æŒ‡æ ‡è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆp &lt; 0.05ï¼ŒWilcoxonç¬¦å·ç§©æ£€éªŒï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†æˆ‘ä»¬çš„MRIé‡å»ºæ–¹æ³•å¯¹ä¸‹æ¸¸è„‘åˆ†å‰²ä»»åŠ¡çš„å½±å“ï¼Œå¹¶è§‚å¯Ÿåˆ°ä¸å‚è€ƒåˆ†å‰²ç›¸æ¯”ï¼Œå‡†ç¡®æ€§å’Œä½“ç§¯ä¸€è‡´æ€§éƒ½æœ‰æ‰€æé«˜ã€‚ä¸é‡‡ç”¨ä¼ ç»Ÿé…å‡†ç®—æ³•çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€»é‡å»ºæ—¶é—´ä¸Šå®ç°äº†æ˜¾è‘—å‡å°‘ï¼Œä½¿å…¶æ›´é€‚åˆå®æ—¶ä¸´åºŠåº”ç”¨ã€‚ä¸æ­¤å·¥ä½œç›¸å…³çš„ä»£ç å¯åœ¨å…¬å¼€ç½‘ç«™<a target="_blank" rel="noopener" href="https://github.com/amirshamaei/longitudinal-mri-deep-recon%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/amirshamaei/longitudinal-mri-deep-reconä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„MRIé‡å»ºæ¡†æ¶ï¼ŒåŒ…å«åˆå§‹é‡å»ºç½‘ç»œã€æ·±åº¦æ³¨å†Œæ¨¡å‹åŠåŸºäºè½¬æ¢å™¨çš„å¢å¼ºç½‘ç»œã€‚åœ¨çºµå‘æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œå¹¶æ˜¾è‘—æé«˜é‡å»ºé€Ÿåº¦ï¼Œé€‚ç”¨äºå®æ—¶ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è§£å†³äº†MRIæˆåƒä¸­é‡‡é›†æ—¶é—´é•¿çš„é—®é¢˜ï¼Œæé«˜äº†æˆåƒæ•ˆç‡ï¼Œé™ä½äº†æˆæœ¬ï¼Œå¹¶æå‡äº†æ‚£è€…èˆ’é€‚åº¦ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œåˆ©ç”¨å…ˆå‰ç‰¹å®šå¯¹è±¡çš„MRIæ‰«æä¿¡æ¯æ”¹å–„å½“å‰æ‰«æçš„é‡å»ºè´¨é‡ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬åˆå§‹é‡å»ºç½‘ç»œã€æ·±åº¦æ³¨å†Œæ¨¡å‹å’ŒåŸºäºè½¬æ¢å™¨çš„å¢å¼ºç½‘ç»œï¼Œèƒ½æœ‰æ•ˆæé«˜MRIå›¾åƒçš„è´¨é‡ã€‚</li>
<li>åœ¨åŒ…å«18ä¸ªä¸»ä½“ã€å…±2808å¼ å›¾åƒçš„çºµå‘æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŠ é€Ÿå› å­è¾¾åˆ°R5è‡³R20ã€‚</li>
<li>å¯¹æ¯”å®šé‡æŒ‡æ ‡æ˜¾ç¤ºï¼Œæ–°æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆp &lt; 0.05ï¼ŒWilcoxonç¬¦å·ç§©æ£€éªŒï¼‰ã€‚</li>
<li>åˆ†æè¡¨æ˜ï¼Œæ–°MRIé‡å»ºæ–¹æ³•å¯¹ä¸‹æ¸¸çš„å¤§è„‘åˆ†å‰²ä»»åŠ¡æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œä½“ç§¯ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4529cc09723ad2978895c8c861f315af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df28603f9938d81c5c3b064c835b2d83.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RIS-LAD-A-Benchmark-and-Model-for-Referring-Low-Altitude-Drone-Image-Segmentation"><a href="#RIS-LAD-A-Benchmark-and-Model-for-Referring-Low-Altitude-Drone-Image-Segmentation" class="headerlink" title="RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image   Segmentation"></a>RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image   Segmentation</h2><p><strong>Authors:Kai Ye, YingShi Luan, Zhudi Chen, Guangyue Meng, Pingyang Dai, Liujuan Cao</strong></p>
<p>Referring Image Segmentation (RIS), which aims to segment specific objects based on natural language descriptions, plays an essential role in vision-language understanding. Despite its progress in remote sensing applications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored. Existing datasets and methods are typically designed for high-altitude and static-view imagery. They struggle to handle the unique characteristics of LAD views, such as diverse viewpoints and high object density. To fill this gap, we present RIS-LAD, the first fine-grained RIS benchmark tailored for LAD scenarios. This dataset comprises 13,871 carefully annotated image-text-mask triplets collected from realistic drone footage, with a focus on small, cluttered, and multi-viewpoint scenes. It highlights new challenges absent in previous benchmarks, such as category drift caused by tiny objects and object drift under crowded same-class objects. To tackle these issues, we propose the Semantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly injecting all linguistic features, SAARN decomposes and routes semantic information to different stages of the network. Specifically, the Category-Dominated Linguistic Enhancement (CDLE) aligns visual features with object categories during early encoding, while the Adaptive Reasoning Fusion Module (ARFM) dynamically selects semantic cues across scales to improve reasoning in complex scenes. The experimental evaluation reveals that RIS-LAD presents substantial challenges to state-of-the-art RIS algorithms, and also demonstrates the effectiveness of our proposed model in addressing these challenges. The dataset and code will be publicly released soon at: <a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/">https://github.com/AHideoKuzeA/RIS-LAD/</a>. </p>
<blockquote>
<p>å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å¯¹ç‰¹å®šå¯¹è±¡è¿›è¡Œåˆ†å‰²ï¼Œåœ¨è§†è§‰è¯­è¨€ç†è§£ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å°½ç®¡å…¶åœ¨é¥æ„Ÿåº”ç”¨æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä½ç©ºæ— äººæœºï¼ˆLADï¼‰åœºæ™¯ä¸­çš„RISä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç°æœ‰çš„æ•°æ®é›†å’Œæ–¹æ³•é€šå¸¸é’ˆå¯¹é«˜ç©ºå’Œé™æ€è§†å›¾å›¾åƒè®¾è®¡ï¼Œå®ƒä»¬éš¾ä»¥å¤„ç†LADè§†å›¾çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œä¾‹å¦‚å¤šæ ·åŒ–çš„è§†è§’å’Œé«˜å¯¹è±¡å¯†åº¦ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RIS-LADï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºLADåœºæ™¯å®šåˆ¶çš„é¦–ä¸ªç²¾ç»†ç²’åº¦RISåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»ç°å®æ— äººæœºå½±åƒä¸­æ”¶é›†çš„13871ä¸ªç»è¿‡ä»”ç»†æ³¨é‡Šçš„å›¾åƒ-æ–‡æœ¬-é®ç½©ä¸‰å…ƒç»„ï¼Œé‡ç‚¹æ˜¯å°è§„æ¨¡ã€æ‚ä¹±å’Œå¤šç‚¹è§†å›¾åœºæ™¯ã€‚å®ƒçªå‡ºäº†ä»¥å‰åŸºå‡†æµ‹è¯•ä¸­ä¸å­˜åœ¨çš„æ–°çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ç”±äºå¾®å°ç‰©ä½“å’Œæ‹¥æŒ¤çš„åŒç±»ç‰©ä½“å¼•èµ·çš„ç±»åˆ«æ¼‚ç§»å’Œç‰©ä½“æ¼‚ç§»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†ç½‘ç»œï¼ˆSAARNï¼‰ã€‚SAARNä¸æ˜¯ç»Ÿä¸€æ³¨å…¥æ‰€æœ‰è¯­è¨€ç‰¹å¾ï¼Œè€Œæ˜¯åˆ†è§£å’Œå°†è¯­ä¹‰ä¿¡æ¯è·¯ç”±åˆ°ç½‘ç»œçš„ä¸åŒé˜¶æ®µã€‚å…·ä½“æ¥è¯´ï¼Œç±»åˆ«ä¸»å¯¼çš„è¯­è¨€å¢å¼ºï¼ˆCDLEï¼‰åœ¨æ—©æœŸç¼–ç æ—¶å°†è§†è§‰ç‰¹å¾ä¸å¯¹è±¡ç±»åˆ«å¯¹é½ï¼Œè€Œè‡ªé€‚åº”æ¨ç†èåˆæ¨¡å—ï¼ˆARFMï¼‰åˆ™åŠ¨æ€é€‰æ‹©è·¨å°ºåº¦çš„è¯­ä¹‰çº¿ç´¢ï¼Œä»¥æ”¹è¿›å¤æ‚åœºæ™¯ä¸­çš„æ¨ç†ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒRIS-LADç»™æœ€å…ˆè¿›çš„RISç®—æ³•å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶ä¹Ÿè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å°†å¾ˆå¿«åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/AHideoKuzeA/RIS-LAD/">https://github.com/AHideoKuzeA/RIS-LAD/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20920v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä½ç©ºæ— äººæœºï¼ˆLADï¼‰åœºæ™¯çš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ç ”ç©¶å­˜åœ¨ç©ºç™½ã€‚ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦é¢å‘é«˜ç©ºå’Œé™æ€å›¾åƒï¼Œéš¾ä»¥å¤„ç†LADè§†è§’çš„ç‹¬ç‰¹ç‰¹ç‚¹ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†RIS-LADï¼Œé¦–ä¸ªé’ˆå¯¹LADåœºæ™¯çš„ç²¾ç»†RISåŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å«ä»çœŸå®æ— äººæœºå½±åƒä¸­æ”¶é›†çš„13,871ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å›¾åƒ-æ–‡æœ¬-é®ç½©ä¸‰å…ƒç»„ï¼Œé‡ç‚¹å…³æ³¨å°ã€æ‚ä¹±å’Œå¤šè§†è§’åœºæ™¯ã€‚ä¸ºè§£å†³æ–°å‡ºç°çš„æŒ‘æˆ˜ï¼Œå¦‚ç±»åˆ«æ¼‚ç§»å’Œå¯¹è±¡æ¼‚ç§»ï¼Œæå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†ç½‘ç»œï¼ˆSAARNï¼‰ã€‚è¯¥ç½‘ç»œåˆ†è§£å¹¶è·¯ç”±è¯­ä¹‰ä¿¡æ¯ï¼Œé€šè¿‡ç±»åˆ«ä¸»å¯¼çš„è¯­è¨€å¢å¼ºï¼ˆCDLEï¼‰å’Œè‡ªé€‚åº”æ¨ç†èåˆæ¨¡å—ï¼ˆARFMï¼‰æ”¹å–„å¤æ‚åœºæ™¯çš„æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç©ºæ— äººæœºï¼ˆLADï¼‰åœºæ™¯çš„å‚ç…§å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰ç ”ç©¶å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä¸»è¦é¢å‘é«˜ç©ºå’Œé™æ€å›¾åƒï¼Œéš¾ä»¥æ»¡è¶³LADè§†è§’çš„ç‹¬ç‰¹éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªé’ˆå¯¹LADåœºæ™¯çš„ç²¾ç»†RISåŸºå‡†æ•°æ®é›†â€”â€”RIS-LADã€‚</li>
<li>RIS-LADåŒ…å«ä»ç°å®æ— äººæœºå½±åƒä¸­æ”¶é›†çš„13,871ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å›¾åƒ-æ–‡æœ¬-é®ç½©ä¸‰å…ƒç»„ã€‚</li>
<li>RIS-LADé‡ç‚¹å…³æ³¨å°ã€æ‚ä¹±å’Œå¤šè§†è§’åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå¦‚ç±»åˆ«æ¼‚ç§»å’Œå¯¹è±¡æ¼‚ç§»ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†è¯­ä¹‰æ„ŸçŸ¥è‡ªé€‚åº”æ¨ç†ç½‘ç»œï¼ˆSAARNï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd284a58922866c09ffd2bd355392060.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-651c78e4ae326ef37a24f66a5032b152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07549e85781419bb7e397d9e7c1584b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c254f6c909501e8ebced4add67c8209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff6ecc17d867c748424c1c0fc2a74041.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9295e85d80edd850987e6794f783b822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c2de2054456b4cc0cb919d21dd7d7bb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ATR-UMMIM-A-Benchmark-Dataset-for-UAV-Based-Multimodal-Image-Registration-under-Complex-Imaging-Conditions"><a href="#ATR-UMMIM-A-Benchmark-Dataset-for-UAV-Based-Multimodal-Image-Registration-under-Complex-Imaging-Conditions" class="headerlink" title="ATR-UMMIM: A Benchmark Dataset for UAV-Based Multimodal Image   Registration under Complex Imaging Conditions"></a>ATR-UMMIM: A Benchmark Dataset for UAV-Based Multimodal Image   Registration under Complex Imaging Conditions</h2><p><strong>Authors:Kangcheng Bin, Chen Chen, Ting Hu, Jiahao Qi, Ping Zhong</strong></p>
<p>Multimodal fusion has become a key enabler for UAV-based object detection, as each modality provides complementary cues for robust feature extraction. However, due to significant differences in resolution, field of view, and sensing characteristics across modalities, accurate registration is a prerequisite before fusion. Despite its importance, there is currently no publicly available benchmark specifically designed for multimodal registration in UAV-based aerial scenarios, which severely limits the development and evaluation of advanced registration methods under real-world conditions. To bridge this gap, we present ATR-UMMIM, the first benchmark dataset specifically tailored for multimodal image registration in UAV-based applications. This dataset includes 7,969 triplets of raw visible, infrared, and precisely registered visible images captured covers diverse scenarios including flight altitudes from 80m to 300m, camera angles from 0{\deg} to 75{\deg}, and all-day, all-year temporal variations under rich weather and illumination conditions. To ensure high registration quality, we design a semi-automated annotation pipeline to introduce reliable pixel-level ground truth to each triplet. In addition, each triplet is annotated with six imaging condition attributes, enabling benchmarking of registration robustness under real-world deployment settings. To further support downstream tasks, we provide object-level annotations on all registered images, covering 11 object categories with 77,753 visible and 78,409 infrared bounding boxes. We believe ATR-UMMIM will serve as a foundational benchmark for advancing multimodal registration, fusion, and perception in real-world UAV scenarios. The datatset can be download from <a target="_blank" rel="noopener" href="https://github.com/supercpy/ATR-UMMIM">https://github.com/supercpy/ATR-UMMIM</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€èåˆå·²æˆä¸ºæ— äººæœºç›®æ ‡æ£€æµ‹çš„å…³é”®æŠ€æœ¯ï¼Œå› ä¸ºæ¯ä¸ªæ¨¡æ€éƒ½èƒ½ä¸ºç¨³å¥çš„ç‰¹å¾æå–æä¾›äº’è¡¥çº¿ç´¢ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒæ¨¡æ€çš„åˆ†è¾¨ç‡ã€è§†é‡å’Œæ„ŸçŸ¥ç‰¹æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå› æ­¤åœ¨èåˆä¹‹å‰ï¼Œå‡†ç¡®çš„é…å‡†æ˜¯å¿…å¤‡æ¡ä»¶ã€‚å°½ç®¡å…¶é‡è¦æ€§ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰é’ˆå¯¹æ— äººæœºç©ºä¸­åœºæ™¯çš„å¤šæ¨¡æ€é…å‡†çš„å…¬å¼€åŸºå‡†æµ‹è¯•ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…ˆè¿›é…å‡†æ–¹æ³•åœ¨çœŸå®æ¡ä»¶ä¸‹çš„å¼€å‘å’Œè¯„ä¼°ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ATR-UMMIMï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºæ— äººæœºåº”ç”¨ä¸­çš„å¤šæ¨¡æ€å›¾åƒé…å‡†è®¾è®¡çš„é¦–ä¸ªåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«7969å¼ åŸå§‹å¯è§å…‰ã€çº¢å¤–å’Œç²¾ç¡®é…å‡†çš„å¯è§å›¾åƒä¸‰å…ƒç»„ï¼Œè¦†ç›–äº†å¤šç§åœºæ™¯ï¼ŒåŒ…æ‹¬é£è¡Œé«˜åº¦ä»80ç±³è‡³300ç±³ã€ç›¸æœºè§’åº¦ä»0Â°è‡³75Â°ï¼Œä»¥åŠå…¨å¹´å…¨å¤©å€™çš„ä¸°å¯Œå¤©æ°”å’Œç…§æ˜æ¡ä»¶ä¸‹çš„æ—¶é—´å˜åŒ–ã€‚ä¸ºäº†ç¡®ä¿é«˜æ³¨å†Œè´¨é‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠè‡ªåŠ¨æ³¨é‡Šç®¡é“ï¼Œä¸ºæ¯ä¸ªä¸‰å…ƒç»„å¼•å…¥å¯é çš„åƒç´ çº§çœŸå®æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªä¸‰å…ƒç»„éƒ½æ ‡æ³¨äº†å…­ä¸ªæˆåƒæ¡ä»¶å±æ€§ï¼Œèƒ½å¤Ÿåœ¨çœŸå®éƒ¨ç½²ç¯å¢ƒä¸­å¯¹é…å‡†çš„ç¨³å¥æ€§è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰å·²æ³¨å†Œçš„å›¾åƒä¸Šæä¾›äº†å¯¹è±¡çº§åˆ«çš„æ³¨é‡Šï¼Œè¦†ç›–11ä¸ªå¯¹è±¡ç±»åˆ«ï¼ŒåŒ…å«77753å¼ å¯è§å…‰å’Œ78409å¼ çº¢å¤–è¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬ç›¸ä¿¡ATR-UMMIMå°†ä¸ºæ¨è¿›å¤šæ¨¡æ€é…å‡†ã€èåˆå’ŒçœŸå®ä¸–ç•Œæ— äººæœºåœºæ™¯ä¸­çš„æ„ŸçŸ¥å¥ å®šåŸºçŸ³ã€‚æ•°æ®é›†å¯ä»¥ä»<a target="_blank" rel="noopener" href="https://github.com/supercpy/ATR-UMMIM%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/supercpy/ATR-UMMIMä¸‹è½½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20764v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªåä¸ºATR-UMMIMçš„åŸºå‡†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸“é—¨ä¸ºæ— äººæœºåº”ç”¨ä¸­å¤šæ¨¡æ€å›¾åƒæ³¨å†Œè€Œè®¾è®¡ã€‚æ•°æ®é›†åŒ…å«å¤šç§åœºæ™¯çš„å›¾åƒï¼Œå¦‚é£è¡Œé«˜åº¦ã€ç›¸æœºè§’åº¦ã€æ—¶é—´å˜åŒ–ç­‰ï¼Œå…·æœ‰å¯é çš„åƒç´ çº§åœ°é¢çœŸå®æ€§å’Œæˆåƒæ¡ä»¶å±æ€§æ ‡æ³¨ã€‚æ•°æ®é›†æ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ³¨å†Œã€èåˆå’Œæ„ŸçŸ¥ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ATR-UMMIMæ˜¯é¦–ä¸ªä¸ºæ— äººæœºåº”ç”¨ä¸­å¤šæ¨¡æ€å›¾åƒæ³¨å†Œè€Œè®¾è®¡çš„åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«7,969å¼ å›¾åƒï¼ŒåŒ…æ‹¬å¯è§å…‰ã€çº¢å¤–å’Œç²¾ç¡®æ³¨å†Œçš„å¯è§å…‰å›¾åƒã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤šç§åœºæ™¯ï¼Œå¦‚ä¸åŒçš„é£è¡Œé«˜åº¦ã€ç›¸æœºè§’åº¦ã€æ—¶é—´å’Œå¤©æ°”æ¡ä»¶ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨åŠè‡ªåŠ¨æ ‡æ³¨ç®¡é“ï¼Œç¡®ä¿é«˜è´¨é‡çš„æ³¨å†Œå’Œå¯é çš„åƒç´ çº§åœ°é¢çœŸå®æ€§ã€‚</li>
<li>æ¯ä¸ªå›¾åƒä¸‰å…ƒç»„éƒ½æ ‡æ³¨äº†å…­ä¸ªæˆåƒæ¡ä»¶å±æ€§ï¼Œå¯åœ¨å®é™…éƒ¨ç½²ç¯å¢ƒä¸­è¯„ä¼°æ³¨å†Œçš„ç¨³å¥æ€§ã€‚</li>
<li>æ•°æ®é›†è¿˜æä¾›å¯¹è±¡çº§åˆ«çš„æ³¨é‡Šå’Œ11ä¸ªå¯¹è±¡ç±»åˆ«çš„è¾¹ç•Œæ¡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25c2d068122a0be9f75897e28991b653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d291d6e9246914eaed817e7fb65b0df1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation"></a>Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Chaowei Chen, Xiang Zhang, Honglie Guo, Shunfang Wang</strong></p>
<p>Weak-strong consistency learning strategies are widely employed in semi-supervised medical image segmentation to train models by leveraging limited labeled data and enforcing weak-to-strong consistency. However, existing methods primarily focus on designing and combining various perturbation schemes, overlooking the inherent potential and limitations within the framework itself. In this paper, we first identify two critical deficiencies: (1) separated training data streams, which lead to confirmation bias dominated by the labeled stream; and (2) incomplete utilization of supervisory information, which limits exploration of strong-to-weak consistency. To tackle these challenges, we propose a style-aware blending and prototype-based cross-contrast consistency learning framework. Specifically, inspired by the empirical observation that the distribution mismatch between labeled and unlabeled data can be characterized by statistical moments, we design a style-guided distribution blending module to break the independent training data streams. Meanwhile, considering the potential noise in strong pseudo-labels, we introduce a prototype-based cross-contrast strategy to encourage the model to learn informative supervisory signals from both weak-to-strong and strong-to-weak predictions, while mitigating the adverse effects of noise. Experimental results demonstrate the effectiveness and superiority of our framework across multiple medical segmentation benchmarks under various semi-supervised settings. </p>
<blockquote>
<p>å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡åˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å¹¶å¼ºåˆ¶æ‰§è¡Œå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡å’Œç»„åˆå„ç§æ‰°åŠ¨æ–¹æ¡ˆä¸Šï¼Œå¿½è§†äº†æ¡†æ¶æœ¬èº«çš„å†…åœ¨æ½œåŠ›å’Œå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼šä¸€æ˜¯è®­ç»ƒæ•°æ®æµåˆ†ç¦»ï¼Œè¿™å¯¼è‡´ä»¥æ ‡è®°æµä¸ºä¸»çš„ç¡®è®¤åè§ï¼›äºŒæ˜¯ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œå…¨ï¼Œè¿™é™åˆ¶äº†ä»å¼ºåˆ°å¼±çš„ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé£æ ¼æ„ŸçŸ¥çš„æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼Œå—å®è¯è§‚å¯Ÿå¯å‘ï¼Œå³æ ‡è®°å’Œæ— æ ‡è®°æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…å¯ä»¥é€šè¿‡ç»Ÿè®¡çŸ©æ¥è¡¨å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ¥æ‰“ç ´ç‹¬ç«‹çš„è®­ç»ƒæ•°æ®æµã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥é¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šä¸ªåŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸‹å…·æœ‰ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20729v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²ä¸­å¸¸é‡‡ç”¨å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œåˆ©ç”¨æœ‰é™æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒå¹¶å¼ºåˆ¶å®æ–½å¼±åˆ°å¼ºçš„ä¸€è‡´æ€§ã€‚ä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡å„ç§æ‰°åŠ¨æ–¹æ¡ˆå¹¶è¿›è¡Œç»„åˆï¼Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„æ½œåœ¨ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æœ¬æ–‡é¦–å…ˆæŒ‡å‡ºä¸¤å¤§ç¼ºé™·ï¼šä¸€æ˜¯ç‹¬ç«‹è®­ç»ƒæ•°æ®æµå¯¼è‡´çš„ç¡®è®¤åè§ï¼›äºŒæ˜¯ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸è¶³ï¼Œé™åˆ¶äº†å¼ºåˆ°å¼±ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é£æ ¼æ„ŸçŸ¥æ··åˆä¸åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚é€šè¿‡è®¾è®¡é£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ‰“ç ´ç‹¬ç«‹è®­ç»ƒæ•°æ®æµï¼ŒåŒæ—¶è€ƒè™‘å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œå¼•å…¥åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡å°‘å™ªå£°çš„ä¸åˆ©å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåŠç›‘ç£åˆ†å‰²ä¸­å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥çš„åº”ç”¨å¹¿æ³›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ‰°åŠ¨æ–¹æ¡ˆçš„è®¾è®¡ï¼Œå¿½è§†äº†æ¡†æ¶çš„æ½œåœ¨ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>æŒ‡å‡ºç‹¬ç«‹è®­ç»ƒæ•°æ®æµå¯¼è‡´çš„ç¡®è®¤åè§å’Œç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºé£æ ¼æ„ŸçŸ¥æ··åˆä¸åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>é€šè¿‡è®¾è®¡é£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ¥è§£å†³ç‹¬ç«‹è®­ç»ƒæ•°æ®æµé—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥å‡è½»å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4eebe85ab8275c597819ed310962620d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9a6f90eaf7eb8f231767407015c37b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c53c1fbfcd1b96d44e8a8acd6dcb224.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0fea33e009705b924bec10648494aa3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Prostate-Cancer-Classification-Using-Multimodal-Feature-Fusion-and-Explainable-AI"><a href="#Prostate-Cancer-Classification-Using-Multimodal-Feature-Fusion-and-Explainable-AI" class="headerlink" title="Prostate Cancer Classification Using Multimodal Feature Fusion and   Explainable AI"></a>Prostate Cancer Classification Using Multimodal Feature Fusion and   Explainable AI</h2><p><strong>Authors:Asma Sadia Khan, Fariba Tasnia Khan, Tanjim Mahmud, Salman Karim Khan, Rishita Chakma, Nahed Sharmen, Mohammad Shahadat Hossain, Karl Andersson</strong></p>
<p>Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2&#x2F;3 recall: 0.900 combined vs 0.824 numerical&#x2F;0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual featuresâ€™ complementary value. This accessible approach offers hospitals a balance of high performance (F1&#x3D;89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics. </p>
<blockquote>
<p>å‰åˆ—è…ºç™Œæ˜¯ç”·æ€§ç¬¬äºŒå¸¸è§çš„æ¶æ€§è‚¿ç˜¤ï¼Œéœ€è¦å…ˆè¿›çš„è¯Šæ–­å·¥å…·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œé€šè¿‡æ–°å‹çš„å¤šæ¨¡å¼èåˆç­–ç•¥ç»“åˆäº†BERTï¼ˆç”¨äºæ–‡æœ¬ä¸´åºŠç¬”è®°ï¼‰å’Œéšæœºæ£®æ—ï¼ˆç”¨äºæ•°å€¼å®éªŒå®¤æ•°æ®ï¼‰ï¼Œåœ¨PLCO-NIHæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„åˆ†ç±»æ€§èƒ½ï¼ˆå‡†ç¡®åº¦98%ï¼ŒAUC 99%ï¼‰ã€‚è™½ç„¶å¤šæ¨¡å¼èåˆå·²ç»å»ºç«‹ï¼Œä½†æˆ‘ä»¬çš„å·¥ä½œè¯æ˜ï¼Œç®€å•ä½†å¯è§£é‡Šçš„BERT+RFç®¡é“å¸¦æ¥äº†ä¸´åºŠä¸Šæ˜¾è‘—çš„æ”¹è¿›â€”â€”ç‰¹åˆ«æ˜¯å¯¹ä¸­é—´é˜¶æ®µçš„ç™Œç—‡ï¼ˆç¬¬2&#x2F;3ç±»å¬å›ç‡ï¼šç»“åˆä¸º0.900 vsæ•°å€¼ä¸º0.824&#x2F;æ–‡æœ¬ä¸º0.725ï¼‰ã€‚SHAPåˆ†ææä¾›äº†é€æ˜çš„ç‰¹å¾é‡è¦æ€§æ’åï¼Œè€Œæ¶ˆèç ”ç©¶è¯æ˜äº†æ–‡æœ¬ç‰¹å¾çš„è¡¥å……ä»·å€¼ã€‚è¿™ç§å¯è¡Œçš„æ–¹æ³•ä¸ºåŒ»é™¢æä¾›äº†é«˜æ€§èƒ½ï¼ˆF1&#x3D;89%ï¼‰ã€è®¡ç®—æ•ˆç‡å’Œä¸´åºŠå¯è§£é‡Šæ€§çš„å¹³è¡¡ï¼Œæ»¡è¶³äº†å‰åˆ—è…ºç™Œè¯Šæ–­ä¸­çš„å…³é”®éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20714v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»“åˆBERTï¼ˆç”¨äºæ–‡æœ¬ä¸´åºŠç¬”è®°ï¼‰å’Œéšæœºæ£®æ—ï¼ˆç”¨äºæ•°å€¼å®éªŒå®¤æ•°æ®ï¼‰çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œé€šè¿‡æ–°å‹å¤šæ¨¡å¼èåˆç­–ç•¥ï¼Œåœ¨PLCO-NIHæ•°æ®é›†ä¸Šå®ç°ä¼˜å¼‚çš„åˆ†ç±»æ€§èƒ½ï¼ˆå‡†ç¡®åº¦98%ï¼ŒAUC 99%ï¼‰ã€‚ç ”ç©¶è¯æ˜ï¼Œè¯¥ç®€å•ä½†å¯è§£é‡Šçš„BERT+RFæµç¨‹ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸­é—´ç™Œç—‡é˜¶æ®µï¼ˆç¬¬äºŒç±»å’Œç¬¬ä¸‰ç±»å¬å›ç‡ï¼šç»“åˆä¸º0.900ï¼Œæ•°å€¼&#x2F;æ–‡æœ¬åˆ†åˆ«ä¸º0.824å’Œ0.725ï¼‰ï¼Œå¯å®ç°ä¸´åºŠä¸Šçš„æ˜¾è‘—æ”¹å–„ã€‚SHAPåˆ†ææä¾›äº†é€æ˜çš„ç‰¹å¾é‡è¦æ€§æ’åï¼Œè€Œæ¶ˆèç ”ç©¶è¯æ˜äº†æ–‡æœ¬ç‰¹å¾çš„è¡¥å……ä»·å€¼ã€‚è¿™ç§æ˜“äºå®æ–½çš„æ–¹æ³•å®ç°äº†é«˜æ€§èƒ½ï¼ˆF1&#x3D;89%ï¼‰ã€è®¡ç®—æ•ˆç‡å’Œä¸´åºŠå¯è§£é‡Šæ€§çš„å¹³è¡¡ï¼Œæ»¡è¶³äº†å‰åˆ—è…ºç™Œè¯Šæ–­çš„å…³é”®éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§ç»“åˆBERTå’Œéšæœºæ£®æ—çš„æ–°å‹å‰åˆ—è…ºç™Œè¯Šæ–­æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ–‡æœ¬ä¸´åºŠç¬”è®°å’Œæ•°å€¼å®éªŒå®¤æ•°æ®ï¼Œå®ç°å¤šæ¨¡å¼èåˆç­–ç•¥ã€‚</li>
<li>åœ¨PLCO-NIHæ•°æ®é›†ä¸Šå–å¾—è¾ƒé«˜åˆ†ç±»æ€§èƒ½ï¼ˆå‡†ç¡®åº¦98%ï¼ŒAUC 99%ï¼‰ã€‚</li>
<li>ç ”ç©¶é’ˆå¯¹ä¸­é—´ç™Œç—‡é˜¶æ®µè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯ç¬¬äºŒç±»å’Œç¬¬ä¸‰ç±»å¬å›ç‡çš„æå‡æ˜¾è‘—ã€‚</li>
<li>SHAPåˆ†ææä¾›äº†ç‰¹å¾é‡è¦æ€§çš„é€æ˜æ’åã€‚</li>
<li>æ¶ˆèç ”ç©¶è¯æ˜äº†æ–‡æœ¬ç‰¹å¾åœ¨è¯Šæ–­ä¸­çš„è¡¥å……ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76a343ddc162d9dc49b1325221b17934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b494fa8a4227b69737ac6abff5012f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f131cbf06fcd69f68455972b10b856d3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="M-Net-MRI-Brain-Tumor-Sequential-Segmentation-Network-via-Mesh-Cast"><a href="#M-Net-MRI-Brain-Tumor-Sequential-Segmentation-Network-via-Mesh-Cast" class="headerlink" title="M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast"></a>M-Net: MRI Brain Tumor Sequential Segmentation Network via Mesh-Cast</h2><p><strong>Authors:Jiacheng Lu, Hui Ding, Shiyu Zhang, Guoping Huo</strong></p>
<p>MRI tumor segmentation remains a critical challenge in medical imaging, where volumetric analysis faces unique computational demands due to the complexity of 3D data. The spatially sequential arrangement of adjacent MRI slices provides valuable information that enhances segmentation continuity and accuracy, yet this characteristic remains underutilized in many existing models. The spatial correlations between adjacent MRI slices can be regarded as â€œtemporal-likeâ€ data, similar to frame sequences in video segmentation tasks. To bridge this gap, we propose M-Net, a flexible framework specifically designed for sequential image segmentation. M-Net introduces the novel Mesh-Cast mechanism, which seamlessly integrates arbitrary sequential models into the processing of both channel and temporal information, thereby systematically capturing the inherent â€œtemporal-likeâ€ spatial correlations between MRI slices. Additionally, we define an MRI sequential input pattern and design a Two-Phase Sequential (TPS) training strategy, which first focuses on learning common patterns across sequences before refining slice-specific feature extraction. This approach leverages temporal modeling techniques to preserve volumetric contextual information while avoiding the high computational cost of full 3D convolutions, thereby enhancing the generalizability and robustness of M-Net in sequential segmentation tasks. Experiments on the BraTS2019 and BraTS2023 datasets demonstrate that M-Net outperforms existing methods across all key metrics, establishing itself as a robust solution for temporally-aware MRI tumor segmentation. </p>
<blockquote>
<p>MRIè‚¿ç˜¤åˆ†å‰²åœ¨åŒ»å­¦æˆåƒä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„æŒ‘æˆ˜ã€‚ç”±äº3Dæ•°æ®çš„å¤æ‚æ€§ï¼Œä½“ç§¯åˆ†æé¢ä¸´ç€ç‹¬ç‰¹çš„è®¡ç®—éœ€æ±‚ã€‚ç›¸é‚»MRIåˆ‡ç‰‡çš„ç©ºé—´åºåˆ—æ’åˆ—æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œå¯ä»¥æé«˜åˆ†å‰²çš„è¿ç»­æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†åœ¨è®¸å¤šç°æœ‰æ¨¡å‹ä¸­ï¼Œè¿™ä¸€ç‰¹å¾çš„ä½¿ç”¨å¹¶ä¸å……åˆ†ã€‚ç›¸é‚»MRIåˆ‡ç‰‡ä¹‹é—´çš„ç©ºé—´ç›¸å…³æ€§å¯ä»¥è¢«è§†ä¸ºç±»ä¼¼äºè§†é¢‘åˆ†å‰²ä»»åŠ¡ä¸­çš„å¸§åºåˆ—çš„â€œæ—¶é—´æ€§â€æ•°æ®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†M-Netï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºé¡ºåºå›¾åƒåˆ†å‰²è®¾è®¡çš„çµæ´»æ¡†æ¶ã€‚M-Netå¼•å…¥äº†æ–°é¢–çš„Mesh-Castæœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ— ç¼é›†æˆäº†ä»»æ„é¡ºåºæ¨¡å‹ï¼Œä»¥å¤„ç†é€šé“å’Œæ—¶é—´ä¿¡æ¯ï¼Œä»è€Œç³»ç»Ÿåœ°æ•è·MRIåˆ‡ç‰‡ä¹‹é—´å›ºæœ‰çš„â€œæ—¶é—´æ€§â€ç©ºé—´ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®šä¹‰äº†MRIåºåˆ—è¾“å…¥æ¨¡å¼ï¼Œå¹¶è®¾è®¡äº†ä¸¤é˜¶æ®µé¡ºåºï¼ˆTPSï¼‰è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¦–å…ˆä¸“æ³¨äºå­¦ä¹ åºåˆ—ä¸­çš„é€šç”¨æ¨¡å¼ï¼Œç„¶åå†ç»†åŒ–åˆ‡ç‰‡ç‰¹å®šçš„ç‰¹å¾æå–ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨æ—¶é—´åºåˆ—å»ºæ¨¡æŠ€æœ¯æ¥ä¿ç•™ä½“ç§¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶é¿å…äº†å…¨3Då·ç§¯çš„é«˜è®¡ç®—æˆæœ¬ï¼Œä»è€Œæé«˜äº†M-Netåœ¨é¡ºåºåˆ†å‰²ä»»åŠ¡ä¸­çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚åœ¨BraTS2019å’ŒBraTS2023æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒM-Netåœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæˆä¸ºäº†ä¸€ç§ç”¨äºæ—¶é—´æ„ŸçŸ¥MRIè‚¿ç˜¤åˆ†å‰²çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20582v1">PDF</a> ICCV 2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºMRIè‚¿ç˜¤åˆ†å‰²æ˜¯åŒ»å­¦æˆåƒä¸­çš„ä¸€é¡¹é‡è¦æŒ‘æˆ˜ï¼Œå› ä¸‰ç»´æ•°æ®çš„å¤æ‚æ€§è€Œé¢ä¸´ç‹¬ç‰¹çš„è®¡ç®—éœ€æ±‚ã€‚æ–‡ç« åˆ©ç”¨MRIåˆ‡ç‰‡çš„ç©ºé—´åºåˆ—æ’åˆ—ä¿¡æ¯ï¼Œå¢å¼ºåˆ†å‰²çš„è¿ç»­æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†è¿™ä¸€ç‰¹æ€§åœ¨è®¸å¤šç°æœ‰æ¨¡å‹ä¸­æœªè¢«å……åˆ†åˆ©ç”¨ã€‚ä¸ºæ­¤ï¼Œæå‡ºM-Netæ¡†æ¶ï¼Œé€šè¿‡Mesh-Castæœºåˆ¶å’Œä¸¤é˜¶æ®µåºè´¯è®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆæ•æ‰MRIåˆ‡ç‰‡é—´çš„â€œæ—¶é—´æ€§â€ç©ºé—´å…³è”ï¼Œå®ç°åºåˆ—å›¾åƒåˆ†å‰²ã€‚å®éªŒè¯æ˜ï¼ŒM-Netåœ¨BraTS2019å’ŒBraTS2023æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæˆä¸ºäº†ä¸€ç§åœ¨æ—¶åºæ„ŸçŸ¥MRIè‚¿ç˜¤åˆ†å‰²ä¸­çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIè‚¿ç˜¤åˆ†å‰²æ˜¯åŒ»å­¦æˆåƒä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œæ¶‰åŠå¤æ‚çš„ä¸‰ç»´æ•°æ®å¤„ç†å’Œç‹¬ç‰¹çš„è®¡ç®—éœ€æ±‚ã€‚</li>
<li>MRIåˆ‡ç‰‡çš„ç©ºé—´åºåˆ—æ’åˆ—ä¿¡æ¯å¯¹äºæé«˜åˆ†å‰²çš„è¿ç»­æ€§å’Œå‡†ç¡®æ€§å…·æœ‰é‡è¦ä»·å€¼ï¼Œä½†åœ¨è®¸å¤šæ¨¡å‹ä¸­æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚</li>
<li>M-Netæ¡†æ¶é€šè¿‡Mesh-Castæœºåˆ¶æ•´åˆä»»æ„åºåˆ—æ¨¡å‹ï¼Œå¤„ç†é€šé“å’Œæ—¶åºä¿¡æ¯ï¼Œæ•æ‰MRIåˆ‡ç‰‡é—´çš„â€œæ—¶é—´æ€§â€ç©ºé—´å…³è”ã€‚</li>
<li>å®šä¹‰äº†MRIåºåˆ—è¾“å…¥æ¨¡å¼ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µåºè´¯è®­ç»ƒç­–ç•¥ï¼Œå…ˆå­¦ä¹ åºåˆ—é—´çš„é€šç”¨æ¨¡å¼ï¼Œå†ç»†åŒ–åˆ‡ç‰‡ç‰¹å¾æå–ã€‚</li>
<li>M-Neté€šè¿‡æ—¶åºå»ºæ¨¡æŠ€æœ¯ä¿ç•™ä½“ç§¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé¿å…å…¨3Då·ç§¯çš„é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>M-Netåœ¨BraTS2019å’ŒBraTS2023æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºå…¶ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7bfc9611f0640fb5b13febfb124ab3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2e0e1ecacc5ec041272f34b57462a3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e7af99204e6bb7828b6daddd339deb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e6149e05ad2e2f03d9df8c2f64b78d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d4b0b9fdebd91134661ab1a99e074fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b519b16e2cef9580ac6193554bf8d3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16e453c4a242f29d075c50bd0c562f59.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Multi-Attention-Stacked-Ensemble-for-Lung-Cancer-Detection-in-CT-Scans"><a href="#Multi-Attention-Stacked-Ensemble-for-Lung-Cancer-Detection-in-CT-Scans" class="headerlink" title="Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans"></a>Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans</h2><p><strong>Authors:Uzzal Saha, Surya Prakash</strong></p>
<p>In this work, we address the challenge of binary lung nodule classification (benign vs malignant) using CT images by proposing a multi-level attention stacked ensemble of deep neural networks. Three pretrained backbones â€“ EfficientNet V2 S, MobileViT XXS, and DenseNet201 â€“ are each adapted with a custom classification head tailored to 96 x 96 pixel inputs. A two-stage attention mechanism learns both model-wise and class-wise importance scores from concatenated logits, and a lightweight meta-learner refines the final prediction. To mitigate class imbalance and improve generalization, we employ dynamic focal loss with empirically calculated class weights, MixUp augmentation during training, and test-time augmentation at inference. Experiments on the LIDC-IDRI dataset demonstrate exceptional performance, achieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in error rate compared to state-of-the-art methods. The model exhibits balanced performance across sensitivity (98.73) and specificity (98.96), with particularly strong results on challenging cases where radiologist disagreement was high. Statistical significance testing confirms the robustness of these improvements across multiple experimental runs. Our approach can serve as a robust, automated aid for radiologists in lung cancer screening. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šå±‚æ³¨æ„åŠ›å †å æ·±åº¦ç¥ç»ç½‘ç»œé›†åˆæ¥è§£å†³ä½¿ç”¨CTå›¾åƒè¿›è¡ŒäºŒåˆ†ç±»è‚ºç»“èŠ‚ï¼ˆè‰¯æ€§ä¸æ¶æ€§ï¼‰çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹â€”â€”EfficientNet V2 Sã€MobileViT XXSå’ŒDenseNet201ï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å‹é€‚é…äº†ä¸€ä¸ªé’ˆå¯¹96x96åƒç´ è¾“å…¥çš„å®šåˆ¶åˆ†ç±»å¤´ã€‚ä¸¤é˜¶æ®µæ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ æ¨¡å‹é—´å’Œç±»åˆ«é—´çš„é‡è¦æ€§å¾—åˆ†ï¼Œå¹¶é€šè¿‡å¯¹æ•°å‡ ç‡è¿›è¡Œç»„åˆï¼ŒåŒæ—¶ä¸€ä¸ªè½»é‡çº§å…ƒå­¦ä¹ è€…å¯¹æœ€ç»ˆé¢„æµ‹è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡å’Œæé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨åŠ¨æ€ç„¦ç‚¹æŸå¤±ä¸ç»éªŒè®¡ç®—å¾—åˆ°çš„ç±»åˆ«æƒé‡ã€è®­ç»ƒè¿‡ç¨‹ä¸­çš„MixUpæ•°æ®å¢å¼ºä»¥åŠåœ¨æ¨ç†æ—¶çš„æµ‹è¯•æ—¶é—´æ•°æ®å¢å¼ºã€‚åœ¨LIDC-IDRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¾¾åˆ°äº†98.09%çš„å‡†ç¡®ç‡å’Œ0.9961çš„AUCå€¼ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œè¯¯å·®ç‡é™ä½äº†35%ã€‚è¯¥æ¨¡å‹åœ¨çµæ•åº¦å’Œç‰¹å¼‚åº¦æ–¹é¢è¡¨ç°å‡ºå¹³è¡¡çš„æ€§èƒ½ï¼ˆåˆ†åˆ«ä¸º98.73å’Œ98.96ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾å°„ç§‘åŒ»ç”Ÿæ„è§åˆ†æ­§è¾ƒå¤§çš„æŒ‘æˆ˜æ€§ç—…ä¾‹ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•è¯å®äº†è¿™äº›æ”¹è¿›åœ¨å¤šæ¬¡å®éªŒä¸­çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½œä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨è‚ºç™Œç­›æŸ¥ä¸­çš„ç¨³å¥è‡ªåŠ¨åŒ–è¾…åŠ©å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20221v1">PDF</a> 26 pages, 14 figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶é€šè¿‡æå‡ºå¤šå±‚æ¬¡çš„æ³¨æ„åŠ›æ·±åº¦ç¥ç»ç½‘ç»œé›†æˆæ–¹æ³•ï¼Œè§£å†³åŸºäºCTå›¾åƒçš„è‚ºéƒ¨ç»“èŠ‚è‰¯æ¶æ€§åˆ†ç±»æŒ‘æˆ˜ã€‚é€šè¿‡é‡‡ç”¨ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹ï¼ˆEfficientNet V2 Sã€MobileViT XXSå’ŒDenseNet201ï¼‰ï¼Œç»“åˆå®šåˆ¶çš„96x96åƒç´ è¾“å…¥åˆ†ç±»å¤´ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µæ³¨æ„åŠ›æœºåˆ¶ï¼Œå­¦ä¹ æ¨¡å‹é—´å’Œç±»åˆ«é—´çš„æƒé‡ã€‚ä¸ºç¼“è§£ç±»åˆ«ä¸å¹³è¡¡å’Œæé«˜æ³›åŒ–èƒ½åŠ›ï¼Œç ”ç©¶é‡‡ç”¨åŠ¨æ€ç„¦ç‚¹æŸå¤±ä¸ç»éªŒè®¡ç®—ç±»åˆ«æƒé‡ã€è®­ç»ƒæœŸé—´çš„MixUpå¢å¼ºå’Œæ¨ç†æ—¶çš„æµ‹è¯•å¢å¼ºã€‚åœ¨LIDC-IDRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º98.09%ï¼ŒAUCä¸º0.9961ï¼Œç›¸æ¯”æœ€æ–°æ–¹æ³•è¯¯å·®ç‡é™ä½äº†35%ã€‚è¯¥æ¨¡å‹åœ¨çµæ•åº¦å’Œç‰¹å¼‚åº¦æ–¹é¢è¡¨ç°å‡è¡¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾å°„ç§‘åŒ»ç”Ÿæ„è§ä¸ä¸€è‡´çš„ç—…ä¾‹ä¸­è¡¨ç°å‡ºè‰²ã€‚ç»Ÿè®¡æµ‹è¯•è¯æ˜è¿™äº›æ”¹è¿›åœ¨å¤šæ¬¡å®éªŒä¸­å‡ç¨³å¥ã€‚æœ¬ç ”ç©¶å¯ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿè‚ºç™Œç­›æŸ¥æä¾›ç¨³å¥çš„è‡ªåŠ¨åŒ–è¾…åŠ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶æå‡ºä¸€ç§å¤šå±‚æ¬¡çš„æ³¨æ„åŠ›æ·±åº¦ç¥ç»ç½‘ç»œé›†æˆæ–¹æ³•ç”¨äºè§£å†³åŸºäºCTå›¾åƒçš„è‚ºéƒ¨ç»“èŠ‚è‰¯æ¶æ€§åˆ†ç±»é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨ä¸‰ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ç»“åˆå®šåˆ¶çš„è¾“å…¥åˆ†ç±»å¤´å¤„ç†å›¾åƒæ•°æ®ã€‚</li>
<li>ä½¿ç”¨ä¸¤é˜¶æ®µæ³¨æ„åŠ›æœºåˆ¶ä»¥å­¦ä¹ æ¨¡å‹é—´å’Œç±»åˆ«é—´çš„æƒé‡åˆ†é…ã€‚</li>
<li>é€šè¿‡åŠ¨æ€ç„¦ç‚¹æŸå¤±ã€ç»éªŒè®¡ç®—ç±»åˆ«æƒé‡ã€è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼ºç­‰æ–¹æ³•æ”¹å–„ç±»åˆ«ä¸å¹³è¡¡å’Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨LIDC-IDRIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡å’ŒAUCå‡è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åœ¨çµæ•åº¦å’Œç‰¹å¼‚åº¦æ–¹é¢è¡¨ç°å‡è¡¡ï¼Œå°¤å…¶é€‚ç”¨äºæ”¾å°„ç§‘åŒ»ç”Ÿè¯Šæ–­æ„è§ä¸ä¸€è‡´çš„ç—…ä¾‹ã€‚</li>
<li>ç»Ÿè®¡æµ‹è¯•è¯æ˜äº†æ¨¡å‹æ”¹è¿›çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b9a01c0ecd23e71d33b918544ab2da1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b77a6f62828afa0158a42c478e69d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a59295d67576cc5cbb36e58248195ad2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e34e3361c97aea989fa877045698da2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MambaVesselNet-A-Hybrid-CNN-Mamba-Architecture-for-Medical-Image-Segmentation"><a href="#MambaVesselNet-A-Hybrid-CNN-Mamba-Architecture-for-Medical-Image-Segmentation" class="headerlink" title="MambaVesselNet++: A Hybrid CNN-Mamba Architecture for Medical Image   Segmentation"></a>MambaVesselNet++: A Hybrid CNN-Mamba Architecture for Medical Image   Segmentation</h2><p><strong>Authors:Qing Xu, Yanming Chen, Yue Li, Ziyu Liu, Zhenye Lou, Yixuan Zhang, Xiangjian He</strong></p>
<p>Medical image segmentation plays an important role in computer-aided diagnosis. Traditional convolution-based U-shape segmentation architectures are usually limited by the local receptive field. Existing vision transformers have been widely applied to diverse medical segmentation frameworks due to their superior capabilities of capturing global contexts. Despite the advantage, the real-world application of vision transformers is challenged by their non-linear self-attention mechanism, requiring huge computational costs. To address this issue, the selective state space model (SSM) Mamba has gained recognition for its adeptness in modeling long-range dependencies in sequential data, particularly noted for its efficient memory costs. In this paper, we propose MambaVesselNet++, a Hybrid CNN-Mamba framework for medical image segmentation. Our MambaVesselNet++ is comprised of a hybrid image encoder (Hi-Encoder) and a bifocal fusion decoder (BF-Decoder). In Hi-Encoder, we first devise the texture-aware layer to capture low-level semantic features by leveraging convolutions. Then, we utilize Mamba to effectively model long-range dependencies with linear complexity. The Bi-Decoder adopts skip connections to combine local and global information of the Hi-Encoder for the accurate generation of segmentation masks. Extensive experiments demonstrate that MambaVesselNet++ outperforms current convolution-based, transformer-based, and Mamba-based state-of-the-arts across diverse medical 2D, 3D, and instance segmentation tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/CC0117/MambaVesselNet">https://github.com/CC0117/MambaVesselNet</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ä¼ ç»Ÿçš„åŸºäºå·ç§¯çš„Uå‹åˆ†å‰²æ¶æ„é€šå¸¸å—åˆ°å±€éƒ¨æ„Ÿå—é‡çš„é™åˆ¶ã€‚ç”±äºæ•æ‰å…¨å±€ä¸Šä¸‹æ–‡çš„ä¼˜åŠ¿ï¼Œç°æœ‰çš„è§†è§‰è½¬æ¢å™¨å·²è¢«å¹¿æ³›åº”ç”¨äºå„ç§åŒ»å­¦åˆ†å‰²æ¡†æ¶ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè§†è§‰è½¬æ¢å™¨åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­é¢ä¸´ç€å…¶éçº¿æ€§è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æŒ‘æˆ˜ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰Mambaå› å…¶å¯¹åºåˆ—æ•°æ®ä¸­é•¿è·ç¦»ä¾èµ–å…³ç³»çš„å»ºæ¨¡èƒ½åŠ›è€Œå—åˆ°è®¤å¯ï¼Œå°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯å…¶é«˜æ•ˆçš„å†…å­˜æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MambaVesselNet++ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ··åˆCNN-Mambaæ¡†æ¶ã€‚æˆ‘ä»¬çš„MambaVesselNet++ç”±æ··åˆå›¾åƒç¼–ç å™¨ï¼ˆHi-Encoderï¼‰å’ŒåŒç„¦ç‚¹èåˆè§£ç å™¨ï¼ˆBF-Decoderï¼‰ç»„æˆã€‚åœ¨Hi-Encoderä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªçº¹ç†æ„ŸçŸ¥å±‚ï¼Œåˆ©ç”¨å·ç§¯æ¥æ•æ‰ä½çº§åˆ«è¯­ä¹‰ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Mambaæœ‰æ•ˆåœ°å¯¹é•¿è·ç¦»ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œå…·æœ‰çº¿æ€§å¤æ‚æ€§ã€‚Bi-Decoderé‡‡ç”¨è·³è·ƒè¿æ¥ï¼Œç»“åˆHi-Encoderçš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œä»¥å‡†ç¡®ç”Ÿæˆåˆ†å‰²æ©è†œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMambaVesselNet++åœ¨å¤šç§åŒ»å­¦äºŒç»´ã€ä¸‰ç»´å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰çš„åŸºäºå·ç§¯ã€åŸºäºè½¬æ¢å™¨å’ŒåŸºäºMambaçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/CC0117/MambaVesselNet%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/CC0117/MambaVesselNetæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19931v1">PDF</a> Accepted by TOMM</p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚æœ¬æ–‡æå‡ºMambaVesselNet++ï¼Œä¸€ä¸ªæ··åˆCNN-Mambaæ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¡†æ¶åŒ…å«æ··åˆå›¾åƒç¼–ç å™¨ï¼ˆHi-Encoderï¼‰å’ŒåŒç„¦ç‚¹èåˆè§£ç å™¨ï¼ˆBF-Decoderï¼‰ã€‚Hi-Encoderåˆ©ç”¨çº¹ç†æ„ŸçŸ¥å±‚æ•æ‰ä½çº§åˆ«è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨Mambaæœ‰æ•ˆå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚BF-Decoderé€šè¿‡è·³è¿‡è¿æ¥ç»“åˆHi-Encoderçš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œä»¥ç”Ÿæˆç²¾ç¡®çš„åˆ†å‰²æ©è†œã€‚å®éªŒè¡¨æ˜ï¼ŒMambaVesselNet++åœ¨å¤šç§åŒ»å­¦äºŒç»´ã€ä¸‰ç»´å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¶…è¶Šäº†å½“å‰çš„å·ç§¯åŸºã€è½¬æ¢å™¨åŸºå’ŒMambaåŸºå…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä¼ ç»ŸåŸºäºå·ç§¯çš„Uå‹åˆ†å‰²æ¶æ„å—é™äºå±€éƒ¨æ„Ÿå—é‡ã€‚</li>
<li>è§†è§‰è½¬æ¢å™¨å› æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›è€Œå¹¿æ³›åº”ç”¨äºåŒ»å­¦åˆ†å‰²æ¡†æ¶ã€‚</li>
<li>è§†è§‰è½¬æ¢å™¨åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æŒ‘æˆ˜æ˜¯å…¶éçº¿æ€§è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰Mambaæ“…é•¿å¯¹åºåˆ—æ•°æ®çš„é•¿è·ç¦»ä¾èµ–è¿›è¡Œå»ºæ¨¡ï¼Œå…·æœ‰é«˜æ•ˆçš„å†…å­˜æˆæœ¬ã€‚</li>
<li>æå‡ºçš„MambaVesselNet++æ˜¯ä¸€ä¸ªæ··åˆCNN-Mambaæ¡†æ¶ï¼ŒåŒ…å«Hi-Encoderå’ŒBF-Decoderã€‚</li>
<li>MambaVesselNet++åœ¨å¤šç§åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19931">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8e8d1cd13ed45e77feb96df12b366f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d69c6cf7fe45f7021b055b62238553d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-611b105b6b3765b15be91be819dfcf6c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="All-in-One-Medical-Image-Restoration-with-Latent-Diffusion-Enhanced-Vector-Quantized-Codebook-Prior"><a href="#All-in-One-Medical-Image-Restoration-with-Latent-Diffusion-Enhanced-Vector-Quantized-Codebook-Prior" class="headerlink" title="All-in-One Medical Image Restoration with Latent Diffusion-Enhanced   Vector-Quantized Codebook Prior"></a>All-in-One Medical Image Restoration with Latent Diffusion-Enhanced   Vector-Quantized Codebook Prior</h2><p><strong>Authors:Haowei Chen, Zhiwen Yang, Haotian Hou, Hui Zhang, Bingzheng Wei, Gang Zhou, Yan Xu</strong></p>
<p>All-in-one medical image restoration (MedIR) aims to address multiple MedIR tasks using a unified model, concurrently recovering various high-quality (HQ) medical images (e.g., MRI, CT, and PET) from low-quality (LQ) counterparts. However, all-in-one MedIR presents significant challenges due to the heterogeneity across different tasks. Each task involves distinct degradations, leading to diverse information losses in LQ images. Existing methods struggle to handle these diverse information losses associated with different tasks. To address these challenges, we propose a latent diffusion-enhanced vector-quantized codebook prior and develop \textbf{DiffCode}, a novel framework leveraging this prior for all-in-one MedIR. Specifically, to compensate for diverse information losses associated with different tasks, DiffCode constructs a task-adaptive codebook bank to integrate task-specific HQ prior features across tasks, capturing a comprehensive prior. Furthermore, to enhance prior retrieval from the codebook bank, DiffCode introduces a latent diffusion strategy that utilizes the diffusion modelâ€™s powerful mapping capabilities to iteratively refine the latent feature distribution, estimating more accurate HQ prior features during restoration. With the help of the task-adaptive codebook bank and latent diffusion strategy, DiffCode achieves superior performance in both quantitative metrics and visual quality across three MedIR tasks: MRI super-resolution, CT denoising, and PET synthesis. </p>
<blockquote>
<p>å…¨åŠŸèƒ½åŒ»å­¦å½±åƒä¿®å¤ï¼ˆMedIRï¼‰æ—¨åœ¨ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è§£å†³å¤šä¸ªMedIRä»»åŠ¡ï¼ŒåŒæ—¶ä»ä½è´¨é‡ï¼ˆLQï¼‰åŒ»å­¦å½±åƒä¸­æ¢å¤å¤šç§é«˜è´¨é‡ï¼ˆHQï¼‰åŒ»å­¦å½±åƒï¼ˆä¾‹å¦‚MRIã€CTå’ŒPETï¼‰ã€‚ç„¶è€Œï¼Œå…¨åŠŸèƒ½MedIRç”±äºä¸åŒä»»åŠ¡ä¹‹é—´çš„å¼‚è´¨æ€§è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æ¯ä¸ªä»»åŠ¡æ¶‰åŠä¸åŒçš„é€€åŒ–ï¼Œå¯¼è‡´ä½è´¨é‡å›¾åƒä¸­ä¿¡æ¯æŸå¤±å¤šæ ·ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ä¸ä¸åŒä»»åŠ¡ç›¸å…³çš„å¤šæ ·ä¿¡æ¯æŸå¤±ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ½œåœ¨æ‰©æ•£å¢å¼ºå‘é‡é‡åŒ–ç æœ¬å…ˆéªŒï¼Œå¹¶å¼€å‘äº†æ–°å‹æ¡†æ¶DiffCodeï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ­¤å…ˆéªŒè¿›è¡Œå…¨åŠŸèƒ½MedIRã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è¡¥å¿ä¸ä¸åŒä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯æŸå¤±å¤šæ ·æ€§ï¼ŒDiffCodeæ„å»ºäº†ä¸€ä¸ªä»»åŠ¡è‡ªé€‚åº”ç æœ¬é“¶è¡Œï¼Œä»¥æ•´åˆä»»åŠ¡ç‰¹å®šçš„é«˜è´¨é‡å…ˆéªŒç‰¹å¾ï¼Œä»è€Œæ•æ‰å…¨é¢çš„å…ˆéªŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä»ç æœ¬é“¶è¡Œä¸­å¢å¼ºå…ˆéªŒæ£€ç´¢ï¼ŒDiffCodeå¼•å…¥äº†ä¸€ç§æ½œåœ¨æ‰©æ•£ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§æ˜ å°„èƒ½åŠ›æ¥è¿­ä»£ä¼˜åŒ–æ½œåœ¨ç‰¹å¾åˆ†å¸ƒï¼Œåœ¨ä¿®å¤è¿‡ç¨‹ä¸­ä¼°è®¡æ›´å‡†ç¡®çš„é«˜è´¨é‡å…ˆéªŒç‰¹å¾ã€‚å€ŸåŠ©ä»»åŠ¡è‡ªé€‚åº”ç æœ¬é“¶è¡Œå’Œæ½œåœ¨æ‰©æ•£ç­–ç•¥ï¼ŒDiffCodeåœ¨ä¸‰ä¸ªMedIRä»»åŠ¡ï¼šMRIè¶…åˆ†è¾¨ç‡ã€CTå»å™ªå’ŒPETåˆæˆä¸­ï¼Œæ— è®ºæ˜¯åœ¨å®šé‡æŒ‡æ ‡è¿˜æ˜¯è§†è§‰è´¨é‡æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19874v1">PDF</a> 11pages, 3figures, MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨åŠŸèƒ½åŒ»ç–—å›¾åƒä¿®å¤ï¼ˆMedIRï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†DiffCodeæ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ½œåœ¨æ‰©æ•£å¢å¼ºå‘é‡é‡åŒ–ç æœ¬å…ˆéªŒï¼Œé€šè¿‡æ„å»ºä»»åŠ¡é€‚åº”æ€§ç æœ¬é“¶è¡Œå’Œå¼•å…¥æ½œåœ¨æ‰©æ•£ç­–ç•¥ï¼Œæé«˜äº†å¯¹ä¸åŒä»»åŠ¡çš„é€‚åº”æ€§ï¼Œä»è€Œåœ¨ä¸åŒåŒ»ç–—å›¾åƒä¿®å¤ä»»åŠ¡ä¸­å–å¾—äº†ä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedIRçš„ç›®æ ‡æ˜¯ä½¿ç”¨ç»Ÿä¸€æ¨¡å‹è§£å†³å¤šä¸ªä»»åŠ¡ï¼Œä»ä½è´¨é‡å›¾åƒä¸­æ¢å¤é«˜è´¨é‡åŒ»ç–—å›¾åƒã€‚</li>
<li>å…¨åŠŸèƒ½MedIRé¢ä¸´çš„ä»»åŠ¡é—´å¼‚è´¨æ€§æ˜¯å…¶ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æ¯ä¸ªä»»åŠ¡æ¶‰åŠä¸åŒçš„é™è§£ï¼Œå¯¼è‡´ä½è´¨é‡å›¾åƒä¸­çš„ä¿¡æ¯æŸå¤±å¤šæ ·åŒ–ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†ä¸ä¸åŒä»»åŠ¡ç›¸å…³çš„å¤šæ ·åŒ–ä¿¡æ¯æŸå¤±ã€‚</li>
<li>DiffCodeæ¡†æ¶é€šè¿‡é‡‡ç”¨æ½œåœ¨æ‰©æ•£å¢å¼ºå‘é‡é‡åŒ–ç æœ¬å…ˆéªŒæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>DiffCodeæ„å»ºäº†ä»»åŠ¡é€‚åº”æ€§ç æœ¬é“¶è¡Œæ¥é›†æˆè·¨ä»»åŠ¡çš„ç‰¹å®šé«˜è´¨é‡å…ˆéªŒç‰¹å¾ï¼Œå¹¶å¼•å…¥äº†æ½œåœ¨æ‰©æ•£ç­–ç•¥æ¥å¢å¼ºç æœ¬é“¶è¡Œä¸­çš„å…ˆéªŒæ£€ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3965cb368f36dfabd22a933f07f20d7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3abc90bb9a29a0882c0a28855d5a1b0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83629997a8473382a4af0780df89e5ec.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Hybrid-Deep-Learning-and-Handcrafted-Feature-Fusion-for-Mammographic-Breast-Cancer-Classification"><a href="#Hybrid-Deep-Learning-and-Handcrafted-Feature-Fusion-for-Mammographic-Breast-Cancer-Classification" class="headerlink" title="Hybrid Deep Learning and Handcrafted Feature Fusion for Mammographic   Breast Cancer Classification"></a>Hybrid Deep Learning and Handcrafted Feature Fusion for Mammographic   Breast Cancer Classification</h2><p><strong>Authors:Maximilian Tschuchnig, Michael Gadermayr, Khalifa Djemal</strong></p>
<p>Automated breast cancer classification from mammography remains a significant challenge due to subtle distinctions between benign and malignant tissue. In this work, we present a hybrid framework combining deep convolutional features from a ResNet-50 backbone with handcrafted descriptors and transformer-based embeddings. Using the CBIS-DDSM dataset, we benchmark our ResNet-50 baseline (AUC: 78.1%) and demonstrate that fusing handcrafted features with deep ResNet-50 and DINOv2 features improves AUC to 79.6% (setup d1), with a peak recall of 80.5% (setup d1) and highest F1 score of 67.4% (setup d1). Our experiments show that handcrafted features not only complement deep representations but also enhance performance beyond transformer-based embeddings. This hybrid fusion approach achieves results comparable to state-of-the-art methods while maintaining architectural simplicity and computational efficiency, making it a practical and effective solution for clinical decision support. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ä¹³è…ºç™Œåˆ†ç±»ä»ä¹³è…ºé’¼é¶å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºè‰¯æ€§å’Œæ¶æ€§ç»„ç»‡ä¹‹é—´çš„åŒºåˆ«å¾ˆå¾®å¦™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œç»“åˆäº†ResNet-50ä¸»å¹²çš„æ·±åº¦å·ç§¯ç‰¹å¾ã€æ‰‹å·¥æè¿°ç¬¦å’ŒåŸºäºå˜å‹å™¨çš„åµŒå…¥ã€‚æˆ‘ä»¬ä½¿ç”¨CBIS-DDSMæ•°æ®é›†å¯¹ResNet-50åŸºçº¿è¿›è¡Œäº†è¯„ä¼°ï¼ˆAUCï¼š78.1%ï¼‰ï¼Œå¹¶è¯æ˜å°†æ‰‹å·¥ç‰¹å¾ä¸æ·±åº¦ResNet-50å’ŒDINOv2ç‰¹å¾èåˆï¼Œå¯ä»¥å°†AUCæé«˜åˆ°79.6%ï¼ˆè®¾ç½®d1ï¼‰ï¼Œæœ€é«˜å¬å›ç‡ä¸º80.5%ï¼ˆè®¾ç½®d1ï¼‰ï¼Œæœ€é«˜F1åˆ†æ•°ä¸º67.4%ï¼ˆè®¾ç½®d1ï¼‰ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰‹å·¥ç‰¹å¾ä¸ä»…è¡¥å……äº†æ·±åº¦è¡¨ç¤ºï¼Œè¿˜æé«˜äº†åŸºäºå˜å‹å™¨çš„åµŒå…¥çš„æ€§èƒ½ã€‚è¿™ç§æ··åˆèåˆæ–¹æ³•çš„ç»“æœä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒäº†æ¶æ„çš„ç®€æ´æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä½¿å…¶æˆä¸ºä¸´åºŠå†³ç­–æ”¯æŒçš„ä¸€ç§å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19843v1">PDF</a> Accepted at IPTA2025</p>
<p><strong>Summary</strong><br>ä¹³è…ºç™Œè‡ªåŠ¨åˆ†ç±»ä»ç„¶æ˜¯åŒ»å­¦å½±åƒå­¦ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå› è‰¯æ¶æ€§ç»„ç»‡é—´çš„å·®å¼‚ç»†å¾®ã€‚æœ¬ç ”ç©¶ç»“åˆResNet-50éª¨å¹²ç½‘ç»œçš„æ·±åº¦å·ç§¯ç‰¹å¾ã€æ‰‹å·¥ç‰¹å¾ä»¥åŠåŸºäºè½¬æ¢å™¨çš„åµŒå…¥ï¼Œæå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ã€‚åˆ©ç”¨CBIS-DDSMæ•°æ®é›†ï¼ŒåŸºå‡†ResNet-50æ¨¡å‹çš„AUCä¸º78.1%ï¼Œèåˆæ‰‹å·¥ç‰¹å¾ä¸æ·±åº¦ResNet-50å’ŒDINOv2ç‰¹å¾åï¼ŒAUCæé«˜åˆ°79.6%ï¼ˆè®¾ç½®d1ï¼‰ï¼Œæœ€é«˜å¬å›ç‡ä¸º80.5%ï¼ˆè®¾ç½®d1ï¼‰ï¼Œæœ€é«˜F1åˆ†æ•°ä¸º67.4%ï¼ˆè®¾ç½®d1ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæ‰‹å·¥ç‰¹å¾ä¸ä»…è¡¥å……äº†æ·±åº¦è¡¨ç¤ºï¼Œè¿˜æé«˜äº†åŸºäºè½¬æ¢å™¨çš„åµŒå…¥çš„æ€§èƒ½ã€‚è¿™ç§æ··åˆèåˆæ–¹æ³•çš„ç»“æœä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼ŒåŒæ—¶ä¿æŒäº†æ¶æ„çš„ç®€æ´æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œä½¿å…¶æˆä¸ºä¸´åºŠå†³ç­–æ”¯æŒçš„å®é™…æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œè‡ªåŠ¨åˆ†ç±»é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè‰¯æ¶æ€§ç»„ç»‡ä¹‹é—´çš„å·®å¼‚å¾ˆç»†å¾®ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œç»“åˆäº†æ·±åº¦å·ç§¯ç‰¹å¾ã€æ‰‹å·¥ç‰¹å¾ä»¥åŠåŸºäºè½¬æ¢å™¨çš„åµŒå…¥ã€‚</li>
<li>ä½¿ç”¨CBIS-DDSMæ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒResNet-50æ¨¡å‹çš„AUCä¸º78.1%ã€‚</li>
<li>èåˆæ‰‹å·¥ç‰¹å¾ä¸æ·±åº¦ResNet-50å’ŒDINOv2ç‰¹å¾åï¼ŒAUCæœ‰æ‰€æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æé«˜å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚</li>
<li>æ‰‹å·¥ç‰¹å¾ä¸ä»…è¡¥å……äº†æ·±åº¦è¡¨ç¤ºï¼Œä¹Ÿå¢å¼ºäº†åŸºäºè½¬æ¢å™¨çš„åµŒå…¥çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0df5926bed62e6a2cb9715797bdb4d4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b640f354c84595720256fc22fc7b4e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c019ab32baf6396bfc6190b707428d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-970365dd83041c4830515f198d8e8682.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Debunking-Optimization-Myths-in-Federated-Learning-for-Medical-Image-Classification"><a href="#Debunking-Optimization-Myths-in-Federated-Learning-for-Medical-Image-Classification" class="headerlink" title="Debunking Optimization Myths in Federated Learning for Medical Image   Classification"></a>Debunking Optimization Myths in Federated Learning for Medical Image   Classification</h2><p><strong>Authors:Youngjoon Lee, Hyukjoon Lee, Jinu Gong, Yang Cao, Joonhyuk Kang</strong></p>
<p>Federated Learning (FL) is a collaborative learning method that enables decentralized model training while preserving data privacy. Despite its promise in medical imaging, recent FL methods are often sensitive to local factors such as optimizers and learning rates, limiting their robustness in practical deployments. In this work, we revisit vanilla FL to clarify the impact of edge device configurations, benchmarking recent FL methods on colorectal pathology and blood cell classification task. We numerically show that the choice of local optimizer and learning rate has a greater effect on performance than the specific FL method. Moreover, we find that increasing local training epochs can either enhance or impair convergence, depending on the FL method. These findings indicate that appropriate edge-specific configuration is more crucial than algorithmic complexity for achieving effective FL. </p>
<blockquote>
<p>è”åˆå­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§åä½œå­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒæ•°æ®éšç§çš„åŒæ—¶å®ç°åˆ†æ•£å¼æ¨¡å‹è®­ç»ƒã€‚å°½ç®¡å…¶åœ¨åŒ»å­¦æˆåƒæ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ï¼Œä½†æœ€è¿‘çš„è”åˆå­¦ä¹ æ–¹æ³•é€šå¸¸å¯¹æœ¬åœ°å› ç´ ï¼ˆå¦‚ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ï¼‰å¾ˆæ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„ç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†åŸºæœ¬çš„è”åˆå­¦ä¹ æ–¹æ³•ï¼Œä»¥é˜æ˜è¾¹ç¼˜è®¾å¤‡é…ç½®çš„å½±å“ï¼Œå¹¶åœ¨ç»“ç›´è‚ ç™Œç—…ç†å’Œè¡€æ¶²ç»†èƒåˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†æœ€è¿‘çš„è”åˆå­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡æ•°å€¼åˆ†æè¡¨æ˜ï¼Œæœ¬åœ°ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡çš„é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“å¤§äºç‰¹å®šçš„è”åˆå­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ æœ¬åœ°è®­ç»ƒå‘¨æœŸå¯èƒ½ä¼šå¢å¼ºæˆ–æŸå®³æ”¶æ•›æ€§ï¼Œè¿™å–å†³äºè”åˆå­¦ä¹ æ–¹æ³•ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºå®ç°æœ‰æ•ˆçš„è”åˆå­¦ä¹ è€Œè¨€ï¼Œé€‚å½“çš„è¾¹ç¼˜ç‰¹å®šé…ç½®æ¯”ç®—æ³•å¤æ‚æ€§æ›´é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19822v1">PDF</a> Accepted to Efficient Medical AI Workshop - MICCAI 2025</p>
<p><strong>Summary</strong><br>     è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§åä½œå­¦ä¹ æ–¹æ³•ï¼Œå¯å®ç°åˆ†æ•£å¼æ¨¡å‹è®­ç»ƒï¼ŒåŒæ—¶ä¿æŠ¤æ•°æ®éšç§ã€‚å°½ç®¡å…¶åœ¨åŒ»å­¦æˆåƒé¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œä½†ç°æœ‰çš„è”é‚¦å­¦ä¹ æ–¹æ³•å¯¹æœ¬åœ°å› ç´ ï¼ˆå¦‚ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ï¼‰å¾ˆæ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ç¨³å¥æ€§ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†æ™®é€šè”é‚¦å­¦ä¹ ï¼Œä»¥æ˜ç¡®è¾¹ç¼˜è®¾å¤‡é…ç½®çš„å½±å“ï¼Œå¹¶åœ¨ç»“è‚ ç—…ç†å’Œè¡€æ¶²ç»†èƒåˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†æœ€è¿‘çš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæœ¬åœ°ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡çš„é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“å¤§äºç‰¹å®šçš„è”é‚¦å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ æœ¬åœ°è®­ç»ƒå‘¨æœŸå¯èƒ½ä¼šå¢å¼ºæˆ–æŸå®³æ”¶æ•›æ€§ï¼Œè¿™å–å†³äºè”é‚¦å­¦ä¹ æ–¹æ³•ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå¯¹äºå®ç°æœ‰æ•ˆçš„è”é‚¦å­¦ä¹ è€Œè¨€ï¼Œé€‚å½“çš„è¾¹ç¼˜ç‰¹å®šé…ç½®æ¯”ç®—æ³•å¤æ‚æ€§æ›´é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ˜¯ä¸€ç§èƒ½åœ¨ä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶å®ç°æ¨¡å‹åˆ†æ•£è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œè”é‚¦å­¦ä¹ å…·æœ‰æ½œåŠ›ä½†å®é™…åº”ç”¨ä¸­å­˜åœ¨ç¨³å¥æ€§é—®é¢˜ã€‚</li>
<li>æœ¬åœ°å› ç´ ï¼ˆå¦‚ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡ï¼‰å¯¹è”é‚¦å­¦ä¹ æ€§èƒ½å½±å“è¾ƒå¤§ã€‚</li>
<li>ä¸åŒè”é‚¦å­¦ä¹ æ–¹æ³•å¯¹äºå¢åŠ æœ¬åœ°è®­ç»ƒå‘¨æœŸçš„ååº”ä¸åŒï¼Œå¯èƒ½å¢å¼ºæˆ–æŸå®³æ”¶æ•›æ€§ã€‚</li>
<li>é€‰æ‹©é€‚å½“çš„è¾¹ç¼˜è®¾å¤‡é…ç½®å¯¹äºå®ç°æœ‰æ•ˆçš„è”é‚¦å­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>ç›¸è¾ƒäºç®—æ³•å¤æ‚æ€§ï¼Œè¾¹ç¼˜ç‰¹å®šé…ç½®å¯¹è”é‚¦å­¦ä¹ æ•ˆæœå½±å“æ›´å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19822">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1b89f0316d4305b5d9eeadc5744ba88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba061a2b9afec89540fd1d9bd308e2aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6db762b5db955960047f90f75d2ef130.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-888552f70b833ae199a7057fa555ff82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bc798a369044281af121d100c78ccbe.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Is-Exchangeability-better-than-I-I-D-to-handle-Data-Distribution-Shifts-while-Pooling-Data-for-Data-scarce-Medical-image-segmentation"><a href="#Is-Exchangeability-better-than-I-I-D-to-handle-Data-Distribution-Shifts-while-Pooling-Data-for-Data-scarce-Medical-image-segmentation" class="headerlink" title="Is Exchangeability better than I.I.D to handle Data Distribution Shifts   while Pooling Data for Data-scarce Medical image segmentation?"></a>Is Exchangeability better than I.I.D to handle Data Distribution Shifts   while Pooling Data for Data-scarce Medical image segmentation?</h2><p><strong>Authors:Ayush Roy, Samin Enam, Jun Xia, Vishnu Suresh Lokhande, Won Hwa Kim</strong></p>
<p>Data scarcity is a major challenge in medical imaging, particularly for deep learning models. While data pooling (combining datasets from multiple sources) and data addition (adding more data from a new dataset) have been shown to enhance model performance, they are not without complications. Specifically, increasing the size of the training dataset through pooling or addition can induce distributional shifts, negatively affecting downstream model performance, a phenomenon known as the â€œData Addition Dilemmaâ€. While the traditional i.i.d. assumption may not hold in multi-source contexts, assuming exchangeability across datasets provides a more practical framework for data pooling. In this work, we investigate medical image segmentation under these conditions, drawing insights from causal frameworks to propose a method for controlling foreground-background feature discrepancies across all layers of deep networks. This approach improves feature representations, which are crucial in data-addition scenarios. Our method achieves state-of-the-art segmentation performance on histopathology and ultrasound images across five datasets, including a novel ultrasound dataset that we have curated and contributed. Qualitative results demonstrate more refined and accurate segmentation maps compared to prominent baselines across three model architectures. The code will be available on Github. </p>
<blockquote>
<p>æ•°æ®ç¨€ç¼ºæ˜¯åŒ»å­¦å½±åƒé¢†åŸŸçš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹è€Œè¨€ã€‚è™½ç„¶æ•°æ®æ•´åˆï¼ˆå°†å¤šä¸ªæ¥æºçš„æ•°æ®é›†åˆå¹¶ï¼‰å’Œæ•°æ®å¢åŠ ï¼ˆæ·»åŠ æ–°çš„æ•°æ®é›†ä¸­çš„æ•°æ®ï¼‰å·²è¢«è¯æ˜å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å®ƒä»¬å¹¶éæ²¡æœ‰å¤æ‚æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ•´åˆæˆ–å¢åŠ æ•°æ®æ¥å¢åŠ è®­ç»ƒæ•°æ®é›†çš„å¤§å°å¯èƒ½ä¼šå¯¼è‡´åˆ†å¸ƒåç§»ï¼Œä»è€Œè´Ÿé¢å½±å“ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œæ•°æ®å¢åŠ å›°å¢ƒâ€ã€‚è™½ç„¶åœ¨å¤šæºç¯å¢ƒä¸­ä¼ ç»Ÿçš„ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾å¯èƒ½ä¸æˆç«‹ï¼Œä½†å‡è®¾ä¸åŒæ•°æ®é›†ä¹‹é—´çš„å¯äº¤æ¢æ€§ä¸ºæ•°æ®æ•´åˆæä¾›äº†ä¸€ä¸ªæ›´å®ç”¨çš„æ¡†æ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨è¿™äº›æ¡ä»¶ä¸‹ç ”ç©¶åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œä»å› æœæ¡†æ¶ä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§æ§åˆ¶æ·±å±‚ç½‘ç»œä¸­æ‰€æœ‰å±‚çš„å‰æ™¯èƒŒæ™¯ç‰¹å¾å·®å¼‚çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•æ”¹è¿›äº†ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨æ•°æ®å¢åŠ åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„ç—…ç†å­¦å’Œè¶…å£°å›¾åƒåˆ†å‰²æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬æˆ‘ä»¬æ•´ç†å¹¶è´¡çŒ®çš„ä¸€ä¸ªæ–°å‹è¶…å£°æ•°æ®é›†ã€‚å®šæ€§ç»“æœè¡¨æ˜ï¼Œä¸ä¸‰ç§æ¨¡å‹æ¶æ„çš„ä¸»è¦åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†æ›´ç²¾ç»†å’Œå‡†ç¡®çš„åˆ†å‰²å›¾ã€‚ä»£ç å°†åœ¨Githubä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19575v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ•°æ®ç¨€ç¼ºæ˜¯åŒ»å­¦å½±åƒé¢†åŸŸæ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚æ•°æ®æ± åŒ–ï¼ˆæ•´åˆå¤šæºæ•°æ®é›†ï¼‰å’Œæ•°æ®å¢åŠ ï¼ˆæ·»åŠ æ–°æ•°æ®é›†ï¼‰è™½èƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†ä¹Ÿä¼šå¼•å‘åˆ†å¸ƒåç§»é—®é¢˜ï¼Œå½±å“æ¨¡å‹çš„åç»­è¡¨ç°ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œæ•°æ®å¢åŠ å›°å¢ƒâ€ã€‚æœ¬æ–‡ç ”ç©¶å¤šæºæ•°æ®ç¯å¢ƒä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ï¼Œåˆ©ç”¨å› æœæ¡†æ¶æå‡ºä¸€ç§æ§åˆ¶æ·±å±‚ç½‘ç»œä¸­å‰æ™¯èƒŒæ™¯ç‰¹å¾å·®å¼‚çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•æé«˜äº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œåœ¨æ•°æ®å¢åŠ åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„ç—…ç†å­¦å’Œè¶…å£°å›¾åƒåˆ†å‰²æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼ŒåŒ…æ‹¬æˆ‘ä»¬æ•´ç†å¹¶è´¡çŒ®çš„ä¸€ä¸ªæ–°å‹è¶…å£°æ•°æ®é›†ã€‚å®šæ€§ç»“æœå±•ç¤ºäº†ä¸‰ç§æ¨¡å‹æ¶æ„çš„ç²¾ç»†å’Œå‡†ç¡®çš„åˆ†å‰²å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ç¨€ç¼ºæ˜¯åŒ»å­¦å½±åƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æ•°æ®æ± åŒ–å’Œæ•°æ®å¢åŠ èƒ½æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å¯èƒ½å¼•å‘åˆ†å¸ƒåç§»é—®é¢˜ã€‚</li>
<li>åˆ†å¸ƒåç§»é—®é¢˜è¢«ç§°ä¸ºâ€œæ•°æ®å¢åŠ å›°å¢ƒâ€ï¼Œä¼šå½±å“æ¨¡å‹çš„åç»­è¡¨ç°ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ç§åœ¨å¤šæºæ•°æ®ç¯å¢ƒä¸‹æ§åˆ¶åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å‰æ™¯èƒŒæ™¯ç‰¹å¾å·®å¼‚çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å› æœæ¡†æ¶ï¼Œæé«˜äº†ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œå°¤å…¶åœ¨æ•°æ®å¢åŠ åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„ç—…ç†å­¦å’Œè¶…å£°å›¾åƒåˆ†å‰²æ€§èƒ½è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b48e7a02e5fd9fa789c66ae01ed5a536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1be9295a309c3137bab79422ca924ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e29f2dcd6e8eadf3ce4d937acdc4e2c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f056edd0e6ffc1ee8eba40f1cdbe42f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68297bc2792513359823ea9f4464c75e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4595afb735918d66fdc455e60e3348e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-538226c76c0c1f6a08a265f0256ef003.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="MedSymmFlow-Bridging-Generative-Modeling-and-Classification-in-Medical-Imaging-through-Symmetrical-Flow-Matching"><a href="#MedSymmFlow-Bridging-Generative-Modeling-and-Classification-in-Medical-Imaging-through-Symmetrical-Flow-Matching" class="headerlink" title="MedSymmFlow: Bridging Generative Modeling and Classification in Medical   Imaging through Symmetrical Flow Matching"></a>MedSymmFlow: Bridging Generative Modeling and Classification in Medical   Imaging through Symmetrical Flow Matching</h2><p><strong>Authors:Francisco Caetano, Lemar Abdi, Christiaan Viviers, Amaan Valiuddin, Fons van der Sommen</strong></p>
<p>Reliable medical image classification requires accurate predictions and well-calibrated uncertainty estimates, especially in high-stakes clinical settings. This work presents MedSymmFlow, a generative-discriminative hybrid model built on Symmetrical Flow Matching, designed to unify classification, generation, and uncertainty quantification in medical imaging. MedSymmFlow leverages a latent-space formulation that scales to high-resolution inputs and introduces a semantic mask conditioning mechanism to enhance diagnostic relevance. Unlike standard discriminative models, it naturally estimates uncertainty through its generative sampling process. The model is evaluated on four MedMNIST datasets, covering a range of modalities and pathologies. The results show that MedSymmFlow matches or exceeds the performance of established baselines in classification accuracy and AUC, while also delivering reliable uncertainty estimates validated by performance improvements under selective prediction. </p>
<blockquote>
<p>å¯é çš„åŒ»å­¦å›¾åƒåˆ†ç±»éœ€è¦å‡†ç¡®çš„é¢„æµ‹å’Œæ ¡å‡†è‰¯å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„ä¸´åºŠç¯å¢ƒä¸­ã€‚æœ¬ç ”ç©¶æå‡ºäº†MedSymmFlowï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯¹ç§°æµåŒ¹é…çš„ç”Ÿæˆ-åˆ¤åˆ«æ··åˆæ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€åŒ»å­¦æˆåƒä¸­çš„åˆ†ç±»ã€ç”Ÿæˆå’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚MedSymmFlowåˆ©ç”¨æ½œåœ¨ç©ºé—´å…¬å¼ï¼Œå¯æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼Œå¹¶å¼•å…¥è¯­ä¹‰æ©æ¨¡è°ƒèŠ‚æœºåˆ¶ä»¥æé«˜è¯Šæ–­ç›¸å…³æ€§ã€‚ä¸æ ‡å‡†åˆ¤åˆ«æ¨¡å‹ä¸åŒï¼Œå®ƒå¯ä»¥é€šè¿‡ç”Ÿæˆé‡‡æ ·è¿‡ç¨‹è‡ªç„¶åœ°ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚è¯¥æ¨¡å‹åœ¨å››ä¸ªMedMNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†å¤šç§æ¨¡æ€å’Œç—…ç†ã€‚ç»“æœè¡¨æ˜ï¼ŒMedSymmFlowåœ¨åˆ†ç±»ç²¾åº¦å’ŒAUCæ–¹é¢çš„æ€§èƒ½ä¸æ—¢å®šåŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¿‡ï¼ŒåŒæ—¶æä¾›äº†å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§é¢„æµ‹ä¸‹çš„æ€§èƒ½æ”¹è¿›å¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19098v1">PDF</a> DGM4MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MedSymmFlowæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¯¹ç§°æµåŒ¹é…çš„ç”Ÿæˆ-åˆ¤åˆ«æ··åˆæ¨¡å‹ï¼Œæ—¨åœ¨ç»Ÿä¸€åŒ»å­¦æˆåƒä¸­çš„åˆ†ç±»ã€ç”Ÿæˆå’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ½œåœ¨ç©ºé—´å…¬å¼ï¼Œå¯æ‰©å±•åˆ°é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼Œå¹¶å¼•å…¥è¯­ä¹‰æ©æ¨¡æ¡ä»¶æœºåˆ¶ä»¥æé«˜è¯Šæ–­ç›¸å…³æ€§ã€‚ä¸ä¼ ç»Ÿçš„åˆ¤åˆ«æ¨¡å‹ä¸åŒï¼Œå®ƒå¯ä»¥é€šè¿‡ç”Ÿæˆé‡‡æ ·è¿‡ç¨‹è‡ªç„¶åœ°ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚åœ¨å››ä¸ªMedMNISTæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒMedSymmFlowåœ¨åˆ†ç±»å‡†ç¡®æ€§å’ŒAUCæ–¹é¢ä¸ç°æœ‰åŸºçº¿ç›¸å½“æˆ–è¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶æä¾›å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§é¢„æµ‹çš„æ€§èƒ½æ”¹è¿›å¾—åˆ°éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedSymmFlowæ˜¯ä¸€ä¸ªç”Ÿæˆ-åˆ¤åˆ«æ··åˆæ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€ç”Ÿæˆå’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨æ½œåœ¨ç©ºé—´å…¬å¼ï¼Œå¯å¤„ç†é«˜åˆ†è¾¨ç‡è¾“å…¥ï¼Œå¹¶é€šè¿‡è¯­ä¹‰æ©æ¨¡æ¡ä»¶æœºåˆ¶æé«˜è¯Šæ–­ç›¸å…³æ€§ã€‚</li>
<li>MedSymmFlowèƒ½å¤Ÿè‡ªç„¶åœ°ä¼°è®¡ä¸ç¡®å®šæ€§ï¼Œè¿™æ˜¯ä¼ ç»Ÿåˆ¤åˆ«æ¨¡å‹æ‰€ç¼ºä¹çš„ã€‚</li>
<li>MedSymmFlowåœ¨åˆ†ç±»æ€§èƒ½å’ŒAUCæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä¸ç°æœ‰åŸºçº¿ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å››ä¸ªä¸åŒçš„MedMNISTæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†å¤šç§æ¨¡æ€å’Œç—…ç†ã€‚</li>
<li>MedSymmFlowæä¾›å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œè¿™æœ‰åŠ©äºåœ¨é«˜é£é™©çš„ä¸´åºŠç¯å¢ƒä¸­åšå‡ºæ›´å‡†ç¡®çš„é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eecaf21d457d771cd5a8cc624bfba3cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69d4f8c26a57770186cc028ae3c57322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bcad245c37e5ff308c2156d650e9b4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-464497f9d5caad11fdd3a5f8ec0842e6.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-40549d220dcc4048b5af741c3260e1b6.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  SpeechFake A Large-Scale Multilingual Speech Deepfake Dataset   Incorporating Cutting-Edge Generation Methods
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-677de526bfccdc43e4d4d0d55ba4ae1a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  LesionGen A Concept-Guided Diffusion Model for Dermatology Image   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
