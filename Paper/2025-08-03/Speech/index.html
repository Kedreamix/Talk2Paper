<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Conan A Chunkwise Online Network for Zero-Shot Adaptive Voice   Conversion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-bf249282f924a7e4a548b1b4183ac0d7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="Conan-A-Chunkwise-Online-Network-for-Zero-Shot-Adaptive-Voice-Conversion"><a href="#Conan-A-Chunkwise-Online-Network-for-Zero-Shot-Adaptive-Voice-Conversion" class="headerlink" title="Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice   Conversion"></a>Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice   Conversion</h2><p><strong>Authors:Yu Zhang, Baotong Tian, Zhiyao Duan</strong></p>
<p>Zero-shot online voice conversion (VC) holds significant promise for real-time communications and entertainment. However, current VC models struggle to preserve semantic fidelity under real-time constraints, deliver natural-sounding conversions, and adapt effectively to unseen speaker characteristics. To address these challenges, we introduce Conan, a chunkwise online zero-shot voice conversion model that preserves the content of the source while matching the voice timbre and styles of reference speech. Conan comprises three core components: 1) a Stream Content Extractor that leverages Emformer for low-latency streaming content encoding; 2) an Adaptive Style Encoder that extracts fine-grained stylistic features from reference speech for enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations demonstrate that Conan outperforms baseline models in subjective and objective metrics. Audio samples can be found at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/ConanDemo">https://aaronz345.github.io/ConanDemo</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬åœ¨çº¿è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰åœ¨å®æ—¶é€šä¿¡å’Œå¨±ä¹æ–¹é¢æœ‰ç€å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„VCæ¨¡å‹åœ¨å®æ—¶çº¦æŸä¸‹å¾ˆéš¾ä¿æŒè¯­ä¹‰ä¿çœŸï¼Œå®ç°è‡ªç„¶è½¬æ¢ï¼Œå¹¶æœ‰æ•ˆåœ°é€‚åº”æœªè§è¿‡çš„è¯´è¯äººç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Conanï¼Œè¿™æ˜¯ä¸€ç§åˆ†å—åœ¨çº¿é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œèƒ½å¤Ÿä¿ç•™æºå†…å®¹çš„åŒæ—¶åŒ¹é…å‚è€ƒè¯­éŸ³çš„éŸ³è‰²å’Œé£æ ¼ã€‚ConanåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š1) æµå†…å®¹æå–å™¨ï¼Œå®ƒåˆ©ç”¨Emformerè¿›è¡Œä½å»¶è¿Ÿæµå†…å®¹ç¼–ç ï¼›2) é€‚åº”æ€§é£æ ¼ç¼–ç å™¨ï¼Œä»å‚è€ƒè¯­éŸ³ä¸­æå–ç²¾ç»†çš„é£æ ¼ç‰¹å¾ï¼Œä»¥å¢å¼ºé£æ ¼é€‚åº”ï¼›3) å› æœæ´—ç‰Œç¼–è§£ç å™¨ï¼Œä½¿ç”¨åƒç´ æ´—ç‰Œæœºåˆ¶å®ç°äº†å®Œå…¨å› æœçš„HiFiGANã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒConanåœ¨ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://aaronz345.github.io/ConanDemo%E6%89%BE%E5%88%B0%E3%80%82">https://aaronz345.github.io/ConanDemoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14534v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é›¶é•œå¤´åœ¨çº¿è¯­éŸ³è½¬æ¢æŠ€æœ¯å¯¹äºå®æ—¶é€šä¿¡å’Œå¨±ä¹é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ¨¡å‹åœ¨å®æ—¶çº¦æŸä¸‹éš¾ä»¥ä¿æŒè¯­ä¹‰ä¿çœŸã€å®ç°è‡ªç„¶çš„å£°éŸ³è½¬æ¢ä»¥åŠæœ‰æ•ˆé€‚åº”æœªè§è¿‡çš„è¯´è¯äººç‰¹å¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºConanæ¨¡å‹ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°åˆ†å—åœ¨çº¿é›¶é•œå¤´è¯­éŸ³è½¬æ¢ï¼š1ï¼‰åˆ©ç”¨Emformerè¿›è¡Œä½å»¶è¿Ÿæµå¼å†…å®¹ç¼–ç çš„Stream Content Extractorï¼›2ï¼‰æå–å‚è€ƒè¯­éŸ³ä¸­çš„ç²¾ç»†é£æ ¼ç‰¹å¾çš„Adaptive Style Encoderï¼Œä»¥å¢å¼ºé£æ ¼é€‚åº”æ€§ï¼›3ï¼‰ä½¿ç”¨åƒç´ æ··æ´—æœºåˆ¶çš„å®Œå…¨å› æœHiFiGANå®ç°çš„Causal Shuffle Vocoderã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒConanåœ¨ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é›¶é•œå¤´åœ¨çº¿è¯­éŸ³è½¬æ¢æŠ€æœ¯åœ¨å®æ—¶é€šä¿¡å’Œå¨±ä¹ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰è¯­éŸ³è½¬æ¢æ¨¡å‹é¢ä¸´ä¿æŒè¯­ä¹‰ä¿çœŸã€è‡ªç„¶å£°éŸ³è½¬æ¢å’Œé€‚åº”æœªè§è¯´è¯äººç‰¹å¾çš„æŒ‘æˆ˜ã€‚</li>
<li>Conanæ¨¡å‹é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶å®ç°åˆ†å—åœ¨çº¿é›¶é•œå¤´è¯­éŸ³è½¬æ¢ã€‚</li>
<li>Stream Content Extractoråˆ©ç”¨Emformerè¿›è¡Œä½å»¶è¿Ÿæµå¼å†…å®¹ç¼–ç ã€‚</li>
<li>Adaptive Style Encoderæå–å‚è€ƒè¯­éŸ³ä¸­çš„ç²¾ç»†é£æ ¼ç‰¹å¾ï¼Œå¢å¼ºé£æ ¼é€‚åº”æ€§ã€‚</li>
<li>Causal Shuffle Vocoderä½¿ç”¨åƒç´ æ··æ´—æœºåˆ¶å®ç°å®Œå…¨å› æœHiFiGANã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºConanåœ¨ä¸»è§‚å’Œå®¢è§‚æŒ‡æ ‡ä¸Šä¼˜äºåŸºå‡†æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d4ba19dd61208bb6bf1ef3083e1a38d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c6ea524c0363526c663c11de2c1c6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ea1f6055aaaa890447ad33645c82b70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3606bfdab144a0dc1cfdcbcac3fc097f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8417860e31830eb9c01bc4800e779a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf249282f924a7e4a548b1b4183ac0d7.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AVFSNet-Audio-Visual-Speech-Separation-for-Flexible-Number-of-Speakers-with-Multi-Scale-and-Multi-Task-Learning"><a href="#AVFSNet-Audio-Visual-Speech-Separation-for-Flexible-Number-of-Speakers-with-Multi-Scale-and-Multi-Task-Learning" class="headerlink" title="AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers   with Multi-Scale and Multi-Task Learning"></a>AVFSNet: Audio-Visual Speech Separation for Flexible Number of Speakers   with Multi-Scale and Multi-Task Learning</h2><p><strong>Authors:Daning Zhang, Ying Wei</strong></p>
<p>Separating target speech from mixed signals containing flexible speaker quantities presents a challenging task. While existing methods demonstrate strong separation performance and noise robustness, they predominantly assume prior knowledge of speaker counts in mixtures. The limited research addressing unknown speaker quantity scenarios exhibits significantly constrained generalization capabilities in real acoustic environments. To overcome these challenges, this paper proposes AVFSNet â€“ an audio-visual speech separation model integrating multi-scale encoding and parallel architecture â€“ jointly optimized for speaker counting and multi-speaker separation tasks. The model independently separates each speaker in parallel while enhancing environmental noise adaptability through visual information integration. Comprehensive experimental evaluations demonstrate that AVFSNet achieves state-of-the-art results across multiple evaluation metrics and delivers outstanding performance on diverse datasets. </p>
<blockquote>
<p>ä»åŒ…å«çµæ´»è¯´è¯äººæ•°é‡çš„æ··åˆä¿¡å·ä¸­åˆ†ç¦»ç›®æ ‡è¯­éŸ³æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„åˆ†ç¦»æ€§èƒ½å’Œå™ªå£°é²æ£’æ€§ï¼Œä½†å®ƒä»¬ä¸»è¦å‡è®¾æ··åˆç‰©ä¸­çš„è¯´è¯äººæ•°é‡æ˜¯å·²çŸ¥çš„ã€‚è§£å†³æœªçŸ¥è¯´è¯äººæ•°é‡åœºæ™¯çš„ç ”ç©¶æœ‰é™ï¼Œåœ¨çœŸå®å£°å­¦ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å—åˆ°æ˜¾è‘—é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†AVFSNetâ€”â€”ä¸€ç§è§†å¬è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œå®ƒèåˆäº†å¤šå°ºåº¦ç¼–ç å’Œå¹¶è¡Œæ¶æ„ï¼Œå¹¶è”åˆä¼˜åŒ–è¯´è¯äººè®¡æ•°å’Œå¤šè¯´è¯äººåˆ†ç¦»ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å¹¶è¡Œç‹¬ç«‹åœ°åˆ†ç¦»æ¯ä¸ªè¯´è¯äººï¼ŒåŒæ—¶é€šè¿‡æ•´åˆè§†è§‰ä¿¡æ¯æé«˜ç¯å¢ƒå™ªå£°é€‚åº”æ€§ã€‚ç»¼åˆå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒAVFSNetåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12972v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä»åŒ…å«å¯å˜è¯´è¯äººæ•°é‡çš„æ··åˆä¿¡å·ä¸­åˆ†ç¦»ç›®æ ‡è¯­éŸ³çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å‡è®¾æ··åˆä¿¡å·ä¸­çš„è¯´è¯äººæ•°é‡æ˜¯å·²çŸ¥çš„ï¼Œè€Œåœ¨é¢å¯¹æœªçŸ¥è¯´è¯äººæ•°é‡çš„åœºæ™¯æ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å—é™ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†AVFSNetâ€”â€”ä¸€ç§é›†å¤šå°ºåº¦ç¼–ç å’Œå¹¶è¡Œæ¶æ„äºä¸€ä½“çš„è§†å¬è¯­éŸ³åˆ†ç¦»æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨ä¼˜åŒ–è¯´è¯äººè®¡æ•°å’Œå¤šè¯´è¯äººåˆ†ç¦»ä»»åŠ¡æ—¶ï¼Œèƒ½å¹¶è¡Œç‹¬ç«‹åœ°åˆ†ç¦»æ¯ä¸ªè¯´è¯äººï¼Œå¹¶é€šè¿‡æ•´åˆè§†è§‰ä¿¡æ¯æé«˜å¯¹ç¯å¢ƒå™ªå£°çš„é€‚åº”æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒAVFSNetåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯­éŸ³åˆ†ç¦»æ–¹æ³•åœ¨åº”å¯¹åŒ…å«å¯å˜è¯´è¯äººæ•°çš„æ··åˆä¿¡å·æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤§éƒ¨åˆ†ç°æœ‰æ–¹æ³•å‡è®¾çŸ¥é“æ··åˆä¿¡å·ä¸­çš„è¯´è¯äººæ•°é‡ï¼Œä½†åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œè¿™ä¸€ä¿¡æ¯å¾€å¾€æ˜¯æœªçŸ¥çš„ã€‚</li>
<li>AVFSNetæ˜¯ä¸€ç§è§†å¬è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>AVFSNeté€šè¿‡å¤šå°ºåº¦ç¼–ç å’Œå¹¶è¡Œæ¶æ„æ¥ç‹¬ç«‹åˆ†ç¦»æ¯ä¸ªè¯´è¯äººã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ•´åˆè§†è§‰ä¿¡æ¯æ¥æé«˜å¯¹ç¯å¢ƒå™ªå£°çš„é€‚åº”æ€§ã€‚</li>
<li>å®éªŒè¯æ˜AVFSNetåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12972">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6e593692e7e03593d6a69fbabc74f13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48b1de2cc26e897afd725a0795886b1c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Knowing-When-to-Quit-Probabilistic-Early-Exits-for-Speech-Separation"><a href="#Knowing-When-to-Quit-Probabilistic-Early-Exits-for-Speech-Separation" class="headerlink" title="Knowing When to Quit: Probabilistic Early Exits for Speech Separation"></a>Knowing When to Quit: Probabilistic Early Exits for Speech Separation</h2><p><strong>Authors:Kenny FalkÃ¦r Olsen, Mads Ã˜stergaard, Karl UlbÃ¦k, SÃ¸ren FÃ¸ns Nielsen, Rasmus Malik HÃ¸egh Lindrup, BjÃ¸rn Sand Jensen, Morten MÃ¸rup</strong></p>
<p>In recent years, deep learning-based single-channel speech separation has improved considerably, in large part driven by increasingly compute- and parameter-efficient neural network architectures. Most such architectures are, however, designed with a fixed compute and parameter budget, and consequently cannot scale to varying compute demands or resources, which limits their use in embedded and heterogeneous devices such as mobile phones and hearables. To enable such use-cases we design a neural network architecture for speech separation capable of early-exit, and we propose an uncertainty-aware probabilistic framework to jointly model the clean speech signal and error variance which we use to derive probabilistic early-exit conditions in terms of desired signal-to-noise ratios. We evaluate our methods on both speech separation and enhancement tasks, and we show that a single early-exit model can be competitive with state-of-the-art models trained at many compute and parameter budgets. Our framework enables fine-grained dynamic compute-scaling of speech separation networks while achieving state-of-the-art performance and interpretable exit conditions. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å•é€šé“è¯­éŸ³åˆ†ç¦»æŠ€æœ¯å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºç¥ç»ç½‘ç»œæ¶æ„çš„è®¡ç®—å’Œå‚æ•°æ•ˆç‡ä¸æ–­æé«˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™æ ·çš„æ¶æ„éƒ½æ˜¯ä¸ºå›ºå®šçš„è®¡ç®—å’Œå‚æ•°é¢„ç®—è®¾è®¡çš„ï¼Œå› æ­¤æ— æ³•æ‰©å±•åˆ°ä¸åŒçš„è®¡ç®—éœ€æ±‚æˆ–èµ„æºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨åµŒå…¥å¼å’Œå¼‚æ„è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºå’Œå¯ç©¿æˆ´è®¾å¤‡ï¼‰ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†æ”¯æŒè¿™äº›ç”¨ä¾‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”¨äºè¯­éŸ³åˆ†ç¦»çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿå®ç°æå‰é€€å‡ºï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ¦‚ç‡æ¡†æ¶ï¼Œè”åˆå¯¹æ¸…æ´è¯­éŸ³ä¿¡å·å’Œè¯¯å·®æ–¹å·®è¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬ç”¨å…¶æ¥æ¨å¯¼åŸºäºæ‰€éœ€ä¿¡å™ªæ¯”çš„æ¦‚ç‡æå‰é€€å‡ºæ¡ä»¶ã€‚æˆ‘ä»¬åœ¨è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œå•ä¸ªæå‰é€€å‡ºæ¨¡å‹å¯ä»¥ä¸åœ¨è®¸å¤šè®¡ç®—å’Œå‚æ•°é¢„ç®—ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹ç›¸ç«äº‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°è¯­éŸ³åˆ†ç¦»ç½‘ç»œçš„ç²¾ç»†åŠ¨æ€è®¡ç®—ç¼©æ”¾ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„é€€å‡ºæ¡ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09768v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å•é€šé“è¯­éŸ³åˆ†ç¦»æŠ€æœ¯æœ‰äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œè¿™ä¸»è¦å¾—ç›Šäºè®¡ç®—å’Œå‚æ•°æ•ˆç‡è¶Šæ¥è¶Šé«˜çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™æ ·çš„æ¶æ„éƒ½æ˜¯ä¸ºå›ºå®šçš„è®¡ç®—å’Œå‚æ•°é¢„ç®—è®¾è®¡çš„ï¼Œå› æ­¤æ— æ³•é€‚åº”ä¸åŒçš„è®¡ç®—éœ€æ±‚æˆ–èµ„æºï¼Œè¿™åœ¨åµŒå…¥å¼å’Œå¼‚æ„è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºå’Œå¯ç©¿æˆ´è®¾å¤‡ï¼‰çš„ä½¿ç”¨ä¸­é€ æˆäº†é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¯ç”¨äºè¯­éŸ³åˆ†ç¦»çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œæ”¯æŒæå‰é€€å‡ºæœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ¦‚ç‡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè”åˆå»ºæ¨¡æ¸…æ´è¯­éŸ³ä¿¡å·å’Œè¯¯å·®æ–¹å·®ï¼Œç”¨äºæ¨å¯¼ç¬¦åˆæ‰€éœ€ä¿¡å™ªæ¯”çš„æ¦‚ç‡æå‰é€€å‡ºæ¡ä»¶ã€‚æˆ‘ä»¬åœ¨è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜å•ä¸€æå‰é€€å‡ºæ¨¡å‹å¯ä»¥ä¸éè®­ç»ƒæ—¶çš„å¤šç§è®¡ç®—å’Œå‚æ•°é¢„ç®—çš„æœ€ä½³æ¨¡å‹ç›¸ç«äº‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨è¾¾åˆ°æœ€æ–°æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†è¯­éŸ³åˆ†ç¦»ç½‘ç»œçš„ç²¾ç»†åŠ¨æ€è®¡ç®—ç¼©æ”¾ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šæ€§çš„é€€å‡ºæ¡ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œæ¶æ„çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å•é€šé“è¯­éŸ³åˆ†ç¦»æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°ç¥ç»ç½‘ç»œæ¶æ„å›ºå®šäºç‰¹å®šçš„è®¡ç®—å’Œå‚æ•°é¢„ç®—ï¼Œæ— æ³•é€‚åº”ä¸åŒèµ„æºç¯å¢ƒã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ”¯æŒæå‰é€€å‡ºçš„ç¥ç»ç½‘ç»œæ¶æ„ç”¨äºè¯­éŸ³åˆ†ç¦»ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåŸºäºä¸ç¡®å®šæ€§çš„æ¦‚ç‡æ¡†æ¶ï¼Œè”åˆå»ºæ¨¡æ¸…æ´è¯­éŸ³ä¿¡å·å’Œè¯¯å·®æ–¹å·®ã€‚</li>
<li>é€šè¿‡æ¦‚ç‡æ¨¡å‹æ¨å¯¼å‡ºäº†ç¬¦åˆæ‰€éœ€ä¿¡å™ªæ¯”çš„æå‰é€€å‡ºæ¡ä»¶ã€‚</li>
<li>åœ¨è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œæå‰é€€å‡ºæ¨¡å‹å…·æœ‰ä¸éè®­ç»ƒæ—¶çš„å¤šç§æ¨¡å‹ç›¸ç«äº‰çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e691ff47690e070f56f8544f8072c13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdffe98bfa712205d7f498d59a17a101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd95f6e0475220abb4ba7f61f6035dd2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition"><a href="#Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition" class="headerlink" title="Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition"></a>Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition</h2><p><strong>Authors:Bingshen Mu, Kun Wei, Pengcheng Guo, Lei Xie</strong></p>
<p>Despite improvements in automatic speech recognition, performance drops with accented speech. Generative error correction (GER) leverages the linguistic knowledge of large language models (LLMs), outperforming typical language model methods. However, it lacks specificity in accented speech scenarios. Accents represent deviations from standard pronunciation, making multi-granularity pronunciation and semantic information essential for accented speech recognition. Moreover, accents exhibit considerable diversity, with each accent possessing distinct characteristics. In this study, we leverage GER to improve transcription accuracy by addressing the two primary features. We propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level pronunciation information. These methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through low-rank adaptation (LoRA) fine-tuning. We employ a three-stage strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge mono-accent LoRA experts within a single multi-modal GER to overcome accent diversity challenges. Furthermore, multi-granularity GER leverages N-best word-level and phoneme-level hypotheses from the HDMoLE model to predict final transcriptions. Experiments on a multi-accent English dataset show that our methods reduce word error rate by 67.35% compared to the baseline vanilla Whisper-large-v3 model. </p>
<blockquote>
<p>å°½ç®¡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯å·²ç»å¾—åˆ°äº†æ”¹è¿›ï¼Œä½†åœ¨å¸¦æœ‰å£éŸ³çš„è¯­éŸ³æ–¹é¢ï¼Œå…¶æ€§èƒ½ä»ç„¶ä¼šä¸‹é™ã€‚ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­è¨€çŸ¥è¯†ï¼Œåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢ä¼˜äºå…¸å‹çš„è¯­è¨€æ¨¡å‹æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å£éŸ³è¯­éŸ³åœºæ™¯æ–¹é¢çš„å…·ä½“æ€§è¾ƒå·®ã€‚å£éŸ³ä»£è¡¨äº†ä¸æ ‡å‡†å‘éŸ³çš„åå·®ï¼Œä½¿å¾—å¤šç²’åº¦å‘éŸ³å’Œè¯­ä¹‰ä¿¡æ¯å¯¹äºå¸¦å£éŸ³çš„è¯­éŸ³è¯†åˆ«è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå£éŸ³è¡¨ç°å‡ºç›¸å½“å¤§çš„å¤šæ ·æ€§ï¼Œæ¯ç§å£éŸ³éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹ç‚¹ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è§£å†³ä¸¤ä¸ªä¸»è¦ç‰¹ç‚¹ï¼Œåˆ©ç”¨GERæ¥æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€GERï¼Œå®ƒç»“åˆäº†è¯­éŸ³æ¨¡æ€çš„å‘éŸ³ä¿¡æ¯ï¼Œä»¥åŠå¤šç²’åº¦GERï¼Œå®ƒç»“åˆäº†ç²¾ç»†çš„éŸ³ç´ çº§å‘éŸ³ä¿¡æ¯ã€‚è¿™äº›æ–¹æ³•ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¸¦å£éŸ³è¯­éŸ³çš„å‘éŸ³ä¿¡æ¯å’Œè¯çº§å‡è®¾çš„è¯­ä¹‰ä¿¡æ¯ï¼Œé€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒè¿›è¡Œå‡†ç¡®çš„è½¬å½•é¢„æµ‹ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µç­–ç•¥ï¼Œé’ˆå¯¹æ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œä»¥è·å¾—å•å£éŸ³LoRAä¸“å®¶ã€‚é€šè¿‡é‡‡ç”¨æˆ‘ä»¬æå‡ºçš„HDMoLEæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨LoRAä¸“å®¶çš„æ··åˆä½“ä¸­åŠ å…¥åˆ†å±‚è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å°†å•å£éŸ³LoRAä¸“å®¶åˆå¹¶åˆ°ä¸€ä¸ªå•ä¸€çš„å¤šæ¨¡æ€GERä¸­ï¼Œä»¥å…‹æœå£éŸ³å¤šæ ·æ€§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå¤šç²’åº¦GERåˆ©ç”¨HDMoLEæ¨¡å‹çš„Nä¸ªæœ€ä½³è¯çº§å’ŒéŸ³ç´ çº§å‡è®¾æ¥é¢„æµ‹æœ€ç»ˆè½¬å½•ã€‚åœ¨å¤šå£éŸ³è‹±è¯­æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†è¯é”™è¯¯ç‡é™ä½äº†67.35%ï¼Œä¸åŸºçº¿Whisper-large-v3æ¨¡å‹ç›¸æ¯”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09116v3">PDF</a> IEEE Transactions on Audio, Speech and Language Processing</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¸¦å£éŸ³çš„è¯­éŸ³è¯†åˆ«é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºå¤šæ¨¡æ€ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰æ–¹æ³•ï¼Œç»“åˆå£éŸ³çš„è¯­éŸ³ä¿¡æ¯ä»¥åŠè¯­ä¹‰ä¿¡æ¯è¿›è¡Œè½¬å½•é¢„æµ‹ã€‚é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œè®­ç»ƒé’ˆå¯¹æ¯ç§å£éŸ³çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨HDMoLEæ–¹æ³•åˆå¹¶è¿™äº›æ¨¡å‹ä»¥åº”å¯¹å£éŸ³å¤šæ ·æ€§æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒåŸºçº¿æ¨¡å‹é™ä½äº†67.35%çš„è¯é”™è¯¯ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­è¨€çŸ¥è¯†ï¼Œç”¨äºæ”¹å–„å¸¦å£éŸ³è¯­éŸ³çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€GERæ–¹æ³•è¢«æå‡ºï¼Œä»¥ç»“åˆè¯­éŸ³æ¨¡æ€çš„å‘éŸ³ä¿¡æ¯ã€‚</li>
<li>å¤šç²’åº¦GERæ–¹æ³•å¼•å…¥ç»†ç²’åº¦éŸ³ç´ çº§å‘éŸ³ä¿¡æ¯ï¼Œä»¥æé«˜è½¬å½•å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¾®è°ƒæŠ€æœ¯ï¼Œé’ˆå¯¹æ¯ç§å£éŸ³è®­ç»ƒå¤šæ¨¡æ€GERæ¨¡å‹ã€‚</li>
<li>HDMoLEæ–¹æ³•é€šè¿‡å±‚æ¬¡è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼åœ¨å£éŸ³ä¸“å®¶æ¨¡å‹çš„æ··åˆä¸­æœ‰æ•ˆåˆå¹¶å•å£éŸ³LoRAä¸“å®¶ï¼Œä»¥åº”å¯¹å£éŸ³å¤šæ ·æ€§ã€‚</li>
<li>å¤šç²’åº¦GERåˆ©ç”¨N-bestçš„å•è¯çº§å’ŒéŸ³ç´ çº§å‡è®¾è¿›è¡Œæœ€ç»ˆè½¬å½•é¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ee1e1c9d0c31dead4cba61fa84fe602f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df860c51bb3a9de47b8cfc6172963cd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28738aaa599edfa6bce756212b3d0e98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ebc3577017306e8b5c9ca6c87395839.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DMF2Mel-A-Dynamic-Multiscale-Fusion-Network-for-EEG-Driven-Mel-Spectrogram-Reconstruction"><a href="#DMF2Mel-A-Dynamic-Multiscale-Fusion-Network-for-EEG-Driven-Mel-Spectrogram-Reconstruction" class="headerlink" title="DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel   Spectrogram Reconstruction"></a>DMF2Mel: A Dynamic Multiscale Fusion Network for EEG-Driven Mel   Spectrogram Reconstruction</h2><p><strong>Authors:Cunhang Fan, Sheng Zhang, Jingjing Zhang, Enrui Liu, Xinhui Li, Minggang Zhao, Zhao Lv</strong></p>
<p>Decoding speech from brain signals is a challenging research problem. Although existing technologies have made progress in reconstructing the mel spectrograms of auditory stimuli at the word or letter level, there remain core challenges in the precise reconstruction of minute-level continuous imagined speech: traditional models struggle to balance the efficiency of temporal dependency modeling and information retention in long-sequence decoding. To address this issue, this paper proposes the Dynamic Multiscale Fusion Network (DMF2Mel), which consists of four core components: the Dynamic Contrastive Feature Aggregation Module (DC-FAM), the Hierarchical Attention-Guided Multi-Scale Network (HAMS-Net), the SplineMap attention mechanism, and the bidirectional state space module (convMamba). Specifically, the DC-FAM separates speech-related â€œforeground featuresâ€ from noisy â€œbackground featuresâ€ through local convolution and global attention mechanisms, effectively suppressing interference and enhancing the representation of transient signals. HAMS-Net, based on the U-Net framework,achieves cross-scale fusion of high-level semantics and low-level details. The SplineMap attention mechanism integrates the Adaptive Gated Kolmogorov-Arnold Network (AGKAN) to combine global context modeling with spline-based local fitting. The convMamba captures long-range temporal dependencies with linear complexity and enhances nonlinear dynamic modeling capabilities. Results on the SparrKULee dataset show that DMF2Mel achieves a Pearson correlation coefficient of 0.074 in mel spectrogram reconstruction for known subjects (a 48% improvement over the baseline) and 0.048 for unknown subjects (a 35% improvement over the baseline).Code is available at: <a target="_blank" rel="noopener" href="https://github.com/fchest/DMF2Mel">https://github.com/fchest/DMF2Mel</a>. </p>
<blockquote>
<p>ä»è„‘ç”µæ³¢è§£ç è¯­éŸ³æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ã€‚å°½ç®¡ç°æœ‰æŠ€æœ¯å·²åœ¨é‡å»ºå¬è§‰åˆºæ¿€çš„æ¢…å°”é¢‘è°±å›¾ï¼ˆå¦‚å•è¯æˆ–å­—æ¯çº§åˆ«ï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨ç²¾ç¡®é‡å»ºåˆ†é’Ÿçº§åˆ«çš„è¿ç»­æƒ³è±¡ä¸­çš„è¯­éŸ³æ–¹é¢ä»å­˜åœ¨æ ¸å¿ƒæŒ‘æˆ˜ï¼šä¼ ç»Ÿæ¨¡å‹åœ¨å¹³è¡¡æ—¶é—´åºåˆ—ä¾èµ–å»ºæ¨¡çš„æ•ˆç‡å’Œé•¿åºåˆ—è§£ç ä¸­çš„ä¿¡æ¯ä¿ç•™æ–¹é¢æ„Ÿåˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŠ¨æ€å¤šå°ºåº¦èåˆç½‘ç»œï¼ˆDMF2Melï¼‰ï¼Œå®ƒåŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å¯¹æ¯”ç‰¹å¾èšåˆæ¨¡å—ï¼ˆDC-FAMï¼‰ã€åˆ†å±‚æ³¨æ„åŠ›å¼•å¯¼å¤šå°ºåº¦ç½‘ç»œï¼ˆHAMS-Netï¼‰ã€SplineMapæ³¨æ„åŠ›æœºåˆ¶å’ŒåŒå‘çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆconvMambaï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒDC-FAMé€šè¿‡å±€éƒ¨å·ç§¯å’Œå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¯­éŸ³ç›¸å…³çš„â€œå‰æ™¯ç‰¹å¾â€ä»å˜ˆæ‚çš„â€œèƒŒæ™¯ç‰¹å¾â€ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†å¹²æ‰°å¹¶å¢å¼ºäº†ç¬æ€ä¿¡å·çš„è¡¨ç¤ºã€‚HAMS-NetåŸºäºU-Netæ¡†æ¶ï¼Œå®ç°äº†é«˜çº§è¯­ä¹‰å’Œä½çº§ç»†èŠ‚çš„è·¨å°ºåº¦èåˆã€‚SplineMapæ³¨æ„åŠ›æœºåˆ¶ç»“åˆäº†è‡ªé€‚åº”é—¨æ§Kolmogorov-Arnoldç½‘ç»œï¼ˆAGKANï¼‰ï¼Œå°†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡ä¸åŸºäºæ ·æ¡çš„å±€éƒ¨æ‹Ÿåˆç›¸ç»“åˆã€‚convMambaä»¥çº¿æ€§å¤æ‚åº¦æ•è·é•¿æœŸæ—¶é—´ä¾èµ–å…³ç³»ï¼Œå¹¶å¢å¼ºäº†éçº¿æ€§åŠ¨æ€å»ºæ¨¡èƒ½åŠ›ã€‚åœ¨SparrKULeeæ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒDMF2Melåœ¨å·²çŸ¥ä¸»é¢˜çš„æ¢…å°”é¢‘è°±å›¾é‡å»ºä¸­è¾¾åˆ°äº†0.074çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆæ¯”åŸºçº¿æé«˜äº†48%ï¼‰ï¼Œåœ¨æœªçŸ¥ä¸»é¢˜ä¸Šè¾¾åˆ°äº†0.048ï¼ˆæ¯”åŸºçº¿æé«˜äº†35%ï¼‰ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/fchest/DMF2Mel%E3%80%82">https://github.com/fchest/DMF2Melã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07526v2">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€å¤šå°ºåº¦èåˆç½‘ç»œï¼ˆDMF2Melï¼‰ï¼ŒåŒ…æ‹¬å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œæ—¨åœ¨è§£å†³ä»è„‘ä¿¡å·è§£ç è¯­éŸ³ä¸­çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚è¯¥ç½‘ç»œèƒ½å¤Ÿåœ¨ä¸åŒå°ºåº¦ä¸Šèåˆé«˜ä½å±‚æ¬¡çš„è¯­ä¹‰å’Œç»†èŠ‚ï¼Œæ›´æœ‰æ•ˆåœ°é‡å»ºè¿ç»­çš„å¾®å°æƒ³è±¡è¯­éŸ³ã€‚åœ¨SparrKULeeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMF2Melåœ¨å·²çŸ¥å’ŒæœªçŸ¥ä¸»ä½“ä¸Šçš„æ¢…å°”é¢‘è°±å›¾é‡å»ºæ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€å¤šå°ºåº¦èåˆç½‘ç»œï¼ˆDMF2Melï¼‰è§£å†³äº†ä»è„‘ä¿¡å·è§£ç è¯­éŸ³çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚</li>
<li>DMF2MelåŒ…å«å››ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŠ¨æ€å¯¹æ¯”ç‰¹å¾èšåˆæ¨¡å—ï¼ˆDC-FAMï¼‰ã€åˆ†å±‚æ³¨æ„åŠ›å¼•å¯¼å¤šå°ºåº¦ç½‘ç»œï¼ˆHAMS-Netï¼‰ã€SplineMapæ³¨æ„åŠ›æœºåˆ¶å’ŒåŒå‘çŠ¶æ€ç©ºé—´æ¨¡å—ï¼ˆconvMambaï¼‰ã€‚</li>
<li>DC-FAMé€šè¿‡å±€éƒ¨å·ç§¯å’Œå…¨å±€æ³¨æ„åŠ›æœºåˆ¶åˆ†ç¦»è¯­éŸ³ç›¸å…³çš„â€œå‰æ™¯ç‰¹å¾â€å’Œå™ªå£°â€œèƒŒæ™¯ç‰¹å¾â€ï¼Œå¢å¼ºç¬æ€ä¿¡å·çš„è¡¨ç¤ºã€‚</li>
<li>HAMS-NetåŸºäºU-Netæ¡†æ¶ï¼Œå®ç°é«˜ä½å±‚æ¬¡è¯­ä¹‰å’Œè·¨å°ºåº¦ç»†èŠ‚èåˆã€‚</li>
<li>SplineMapæ³¨æ„åŠ›æœºåˆ¶ç»“åˆäº†å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’ŒåŸºäºæ ·æ¡çš„å±€éƒ¨æ‹Ÿåˆã€‚</li>
<li>convMambaèƒ½å¤Ÿæ•æ‰é•¿æœŸæ—¶é—´ä¾èµ–å…³ç³»ï¼Œå¢å¼ºéçº¿æ€§åŠ¨æ€å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>åœ¨SparrKULeeæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDMF2Melåœ¨æ¢…å°”é¢‘è°±å›¾é‡å»ºæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œç›¸å¯¹äºåŸºçº¿æ–¹æ³•æœ‰å¤§å¹…åº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9414ff52be23ded9679072510339205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f0e79f97d9502168453674b2e693d68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cda5a2b51211f1faf9948e972ee09eaf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rectifying-Magnitude-Neglect-in-Linear-Attention"><a href="#Rectifying-Magnitude-Neglect-in-Linear-Attention" class="headerlink" title="Rectifying Magnitude Neglect in Linear Attention"></a>Rectifying Magnitude Neglect in Linear Attention</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Yuang Ai, ran He</strong></p>
<p>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Queryâ€™s magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a> </p>
<blockquote>
<p>Transformerçš„æ ¸å¿ƒæ“ä½œå™¨Softmax Attentionå±•ç°å‡ºä¼˜ç§€çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLinear Attentionä¸Softmax Attentionæœ‰ç›¸ä¼¼çš„å…¬å¼ï¼Œä½†å®ç°äº†çº¿æ€§å¤æ‚åº¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¿›è¡Œå…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚ç„¶è€Œï¼Œä¸æ ‡å‡†çš„Softmax Attentionç›¸æ¯”ï¼ŒLinear Attentionçš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åŸºäºLinear Attentionçš„å…¬å¼åˆ†ææ­¤é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸Softmax Attentionä¸åŒï¼ŒLinear Attentionå®Œå…¨å¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ã€‚è¿™é˜»æ­¢äº†æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒåœ¨Queryç¼©æ”¾æ—¶åŠ¨æ€é€‚åº”ã€‚å› æ­¤ï¼Œå°½ç®¡å…¶ä¸Softmax Attentionçš„ç»“æ„ç›¸ä¼¼ï¼Œä½†Linear Attentionçš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒå´å¤§ä¸ç›¸åŒã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†å¹…åº¦æ„ŸçŸ¥çš„Linear Attentionï¼ˆMALAï¼‰ï¼Œå®ƒä¿®æ”¹äº†Linear Attentionçš„è®¡ç®—ä»¥å……åˆ†èå…¥Queryçš„å¹…åº¦ã€‚è¿™ä¸€è°ƒæ•´ä½¿å¾—MALAèƒ½å¤Ÿç”Ÿæˆä¸Softmax Attentionç›¸ä¼¼çš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒï¼ŒåŒæ—¶å±•ç°å‡ºæ›´å¹³è¡¡çš„ç»“æ„ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†MALAçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²ã€è¯­ä¹‰åˆ†å‰²ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„MALAåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å¼ºå¤§çš„ç»“æœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00698v2">PDF</a> Accepted by ICCV2025, highlight paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformerä¸­çš„æ ¸å¿ƒæ“ä½œï¼ŒåŒ…æ‹¬Softmax Attentionå’ŒLinear Attentionã€‚Softmax Attentionå±•ç°å‡ºå¼ºå¤§çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä½†ç”±äºå…¶äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚Linear Attentionè™½ç„¶åœ¨å¤æ‚åº¦ä¸Šè¾¾åˆ°äº†çº¿æ€§çº§åˆ«ï¼Œä½†ä¸æ ‡å‡†çš„Softmax Attentionç›¸æ¯”æ€§èƒ½æ˜æ˜¾ä¸‹é™ã€‚åŸå› åœ¨äºLinear Attentionå¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒæ— æ³•éšQueryçš„ç¼©æ”¾åŠ¨æ€è°ƒæ•´ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†Magnitude-Aware Linear Attentionï¼ˆMALAï¼‰ï¼Œé€šè¿‡ä¿®æ”¹Linear Attentionçš„è®¡ç®—æ–¹æ³•ä½¿å…¶èƒ½å……åˆ†åˆ©ç”¨Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œäº§ç”Ÿç±»ä¼¼äºSoftmax Attentionçš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒã€‚MALAåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½éå¸¸å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Softmax Attentionå…·æœ‰å¼ºå¤§çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä½†å…¶äºŒæ¬¡å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>Linear Attentionå…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œä½†ä¸Softmax Attentionç›¸æ¯”æ€§èƒ½è¾ƒå·®ã€‚åŸå› åœ¨äºå…¶å¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ã€‚</li>
<li>MALAæ—¨åœ¨è§£å†³Linear Attentionçš„é—®é¢˜ï¼Œé€šè¿‡ä¿®æ”¹è®¡ç®—æ–¹æ³•ä»¥å……åˆ†åˆ©ç”¨Queryçš„å¹…åº¦ä¿¡æ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77b1d304544d55fa735b5b10735c288d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9ae945a2ce34db84f97b69f636a7ad2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c67263aea0a851211e4b263070b438e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d0fbfade3a2fae4267e44252804f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9855ff5403bcef39fb12ca47885498.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SALM-Duplex-Efficient-and-Direct-Duplex-Modeling-for-Speech-to-Speech-Language-Model"><a href="#SALM-Duplex-Efficient-and-Direct-Duplex-Modeling-for-Speech-to-Speech-Language-Model" class="headerlink" title="SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech   Language Model"></a>SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech   Language Model</h2><p><strong>Authors:Ke Hu, Ehsan Hosseini-Asl, Chen Chen, Edresson Casanova, Subhankar Ghosh, Piotr Å»elasko, Zhehuai Chen, Jason Li, Jagadeesh Balam, Boris Ginsburg</strong></p>
<p>Spoken dialogue is an intuitive form of human-computer interaction, yet current speech language models often remain constrained to turn-based exchanges, lacking real-time adaptability such as user barge-in. We propose a novel duplex speech to speech (S2S) architecture featuring continuous user inputs and codec agent outputs with channel fusion that directly models simultaneous user and agent streams. Using a pretrained streaming encoder for user input enables the first duplex S2S model without requiring speech pretrain. Separate architectures for agent and user modeling facilitate codec fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared to previous works. Experimental results show that the proposed model outperforms previous duplex models in reasoning, turn-taking, and barge-in abilities. The model requires significantly less speech data, as speech pretrain is skipped, which markedly simplifies the process of building a duplex S2S model from any LLMs. Finally, it is the first openly available duplex S2S model with training and inference code to foster reproducibility. </p>
<blockquote>
<p>å£è¯­å¯¹è¯æ˜¯äººç±»ä¸è®¡ç®—æœºäº¤äº’çš„ä¸€ç§ç›´è§‚å½¢å¼ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯­éŸ³è¯­è¨€æ¨¡å‹é€šå¸¸ä»…é™äºåŸºäºå›åˆçš„äº¤äº’ï¼Œç¼ºä¹å®æ—¶é€‚åº”æ€§ï¼Œå¦‚ç”¨æˆ·æŠ¢è¯ç­‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒè¯­è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰æ¶æ„ï¼Œå…·æœ‰è¿ç»­ç”¨æˆ·è¾“å…¥å’Œç¼–è§£ç å™¨ä»£ç†è¾“å‡ºï¼Œé€šè¿‡ä¿¡é“èåˆç›´æ¥å»ºæ¨¡ç”¨æˆ·å’Œä»£ç†çš„å¹¶è¡Œæµã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„æµå¼ç¼–ç å™¨è¿›è¡Œç”¨æˆ·è¾“å…¥ï¼Œä½¿ç¬¬ä¸€ä¸ªåŒè¯­S2Sæ¨¡å‹æ— éœ€è¯­éŸ³é¢„è®­ç»ƒå³å¯å®ç°ã€‚å¯¹ä»£ç†å’Œç”¨æˆ·å»ºæ¨¡çš„å•ç‹¬æ¶æ„æœ‰åŠ©äºå¯¹ç¼–è§£ç å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥äº§ç”Ÿæ›´å¥½çš„ä»£ç†å£°éŸ³ï¼Œå¹¶å°†æ¯”ç‰¹ç‡ä¸ä»¥å‰çš„å·¥ä½œç›¸æ¯”å‡åŠï¼ˆ0.6kbpsï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºæ¨¡å‹åœ¨æ¨ç†ã€è¯è½®è½¬æ¢å’ŒæŠ¢è¯èƒ½åŠ›æ–¹é¢ä¼˜äºä¹‹å‰çš„åŒè¯­æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¤§å¤§å‡å°‘äº†æ‰€éœ€çš„è¯­éŸ³æ•°æ®ï¼Œå› ä¸ºè·³è¿‡äº†è¯­éŸ³é¢„è®­ç»ƒï¼Œä»è€Œå¤§å¤§ç®€åŒ–äº†ä»ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºåŒè¯­S2Sæ¨¡å‹çš„è¿‡ç¨‹ã€‚æœ€åï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„å¸¦æœ‰è®­ç»ƒå’Œæ¨ç†ä»£ç çš„åŒè¯­S2Sæ¨¡å‹ï¼Œæœ‰åˆ©äºä¿ƒè¿›å¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15670v4">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒå‘è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆDuplex Speech to Speechï¼Œç®€ç§°S2Sï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„æ”¯æŒè¿ç»­ç”¨æˆ·è¾“å…¥å’Œç¼–ç è§£ç å™¨è¾“å‡ºï¼Œé€šè¿‡èåˆé€šé“ç›´æ¥æ¨¡æ‹Ÿç”¨æˆ·å’Œä»£ç†çš„å®æ—¶äº¤äº’æµã€‚è¯¥æ¶æ„é‡‡ç”¨é¢„è®­ç»ƒçš„æµå¼ç¼–ç å™¨å¯¹ç”¨æˆ·è¾“å…¥è¿›è¡Œå»ºæ¨¡ï¼Œæ— éœ€è¯­éŸ³é¢„è®­ç»ƒå³å¯å®ç°é¦–ä¸ªDuplex S2Sæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†ã€è½®æ›¿å’Œæ‰“æ–­èƒ½åŠ›ä¸Šä¼˜äºå…ˆå‰çš„åŒå‘æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å¯¹è¯­éŸ³æ•°æ®çš„éœ€æ±‚æ›´å°‘ï¼Œæå¤§åœ°ç®€åŒ–äº†ä»ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºDuplex S2Sæ¨¡å‹çš„è¿‡ç¨‹ã€‚æœ€åï¼Œå®ƒæ˜¯é¦–ä¸ªå…¬å¼€æä¾›è®­ç»ƒå’Œæ¨ç†ä»£ç çš„Duplex S2Sæ¨¡å‹ï¼Œä¿ƒè¿›äº†å¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†æ–°å‹çš„Duplex S2Sæ¶æ„ï¼Œæ”¯æŒè¿ç»­ç”¨æˆ·è¾“å…¥å’Œç¼–ç è§£ç å™¨è¾“å‡ºã€‚</li>
<li>é€šè¿‡èåˆé€šé“ç›´æ¥æ¨¡æ‹Ÿç”¨æˆ·å’Œä»£ç†çš„å®æ—¶äº¤äº’æµã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„æµå¼ç¼–ç å™¨ï¼Œæ— éœ€è¯­éŸ³é¢„è®­ç»ƒå³å¯å®ç°Duplex S2Sæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨ç†ã€è½®æ›¿å’Œæ‰“æ–­èƒ½åŠ›ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¨¡å‹å¯¹è¯­éŸ³æ•°æ®çš„éœ€æ±‚å‡å°‘ï¼Œç®€åŒ–äº†æ„å»ºDuplex S2Sæ¨¡å‹çš„è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¯é¦–ä¸ªå…¬å¼€æä¾›è®­ç»ƒå’Œæ¨ç†ä»£ç çš„Duplex S2Sæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6171f8fe9e4fb74ca669bb97b2e20175.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef932e4ff1bfedf59bd69d407f6dadcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43286475d61216d96e04f61afa875a08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a52190db9196b96b93e7d321d1c4652d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-629ecaad18edc32c35eeb2da1902c380.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6c6e92e9ff79b76304ec9810d98274.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49cbc6f2fa0a1e7855e7296f97163e36.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages"><a href="#Towards-Inclusive-ASR-Investigating-Voice-Conversion-for-Dysarthric-Speech-Recognition-in-Low-Resource-Languages" class="headerlink" title="Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages"></a>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric   Speech Recognition in Low-Resource Languages</h2><p><strong>Authors:Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea PÃ©rez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar NÃ¶th, David R. Mortensen</strong></p>
<p>Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics. </p>
<blockquote>
<p>é’ˆå¯¹å‘éŸ³å›°éš¾ï¼ˆå‘éŸ³éšœç¢ï¼‰è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™æ˜¯ç”±äºæ•°æ®ç¨€ç¼ºé€ æˆçš„ï¼Œç‰¹åˆ«æ˜¯åœ¨éè‹±è¯­é¢†åŸŸå°¤ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹è‹±æ–‡å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¾¿ç¼–ç å‘è¨€äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚ç„¶åå°†å…¶åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºéè‹±è¯­å‘éŸ³éšœç¢ç±»ä¼¼è¯­éŸ³ã€‚ç”Ÿæˆçš„æ•°æ®éšåç”¨äºå¾®è°ƒå¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨PC-GITAï¼ˆè¥¿ç­ç‰™è¯­ï¼‰ã€EasyCallï¼ˆæ„å¤§åˆ©è¯­ï¼‰å’ŒSSNCEï¼ˆæ³°ç±³å°”è¯­ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŒæ—¶å®ç°å‘è¨€äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚å¯¹ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14874v4">PDF</a> 5 pages, 1 figure, Accepted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éè‹±è¯­è¯­è¨€çš„å‘éŸ³éšœç¢è¯­éŸ³è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿä½¿ç”¨è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰å¯¹è¯­éŸ³è½¬æ¢æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚éšåï¼Œè¯¥æ¨¡å‹è¢«åº”ç”¨äºå°†å¥åº·çš„éè‹±è¯­è¯­éŸ³ï¼ˆFLEURSï¼‰è½¬æ¢ä¸ºç±»ä¼¼å‘éŸ³éšœç¢çš„è¯­éŸ³ã€‚ç”Ÿæˆçš„è¯­éŸ³æ•°æ®è¢«ç”¨äºå¾®è°ƒå¤šè¯­è¨€ASRæ¨¡å‹Massively Multilingual Speechï¼ˆMMSï¼‰ï¼Œä»¥æé«˜å¯¹å‘éŸ³éšœç¢è¯­éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚åœ¨è¥¿ç­ç‰™è¯­PC-GITAã€æ„å¤§åˆ©è¯­EasyCallå’Œæ³°ç±³å°”è¯­SSNCEä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒåŒæ—¶å®ç°è¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯æ˜¾è‘—ä¼˜äºç°æˆçš„MMSæ€§èƒ½å’Œä¼ ç»Ÿçš„å¢å¼ºæŠ€æœ¯ï¼Œå¦‚é€Ÿåº¦å’ŒèŠ‚å¥æ‰°åŠ¨ã€‚ç”Ÿæˆæ•°æ®çš„å®¢è§‚å’Œä¸»è§‚åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œç”Ÿæˆçš„è¯­éŸ³æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å› æ•°æ®ç¨€ç¼ºè€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>é€šè¿‡è‹±è¯­å‘éŸ³éšœç¢è¯­éŸ³ï¼ˆUASpeechï¼‰å¾®è°ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ä»¥ç¼–ç è¯´è¯äººçš„ç‰¹å¾å’ŒéŸµå¾‹æ‰­æ›²ã€‚</li>
<li>ä½¿ç”¨è¯¥æ¨¡å‹å°†å¥åº·çš„éè‹±è¯­è¯­éŸ³è½¬æ¢ä¸ºç±»ä¼¼å‘éŸ³éšœç¢çš„è¯­éŸ³ï¼Œå¹¶ç”¨äºå¾®è°ƒå¤šè¯­è¨€ASRæ¨¡å‹ã€‚</li>
<li>åœ¨ä¸åŒè¯­è¨€çš„è¯„ä¼°ä¸­ï¼ŒåŒæ—¶å®ç°è¯´è¯äººå’ŒéŸµå¾‹è½¬æ¢çš„è¯­éŸ³è½¬æ¢æŠ€æœ¯æ˜¾è‘—æé«˜äº†ASRæ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„ASRå¢å¼ºæŠ€æœ¯ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•çš„æ€§èƒ½æ›´ä½³ã€‚</li>
<li>ç”Ÿæˆçš„è¯­éŸ³æ•°æ®åœ¨å®¢è§‚å’Œä¸»è§‚åˆ†æä¸Šå‡æ¨¡æ‹Ÿäº†å‘éŸ³éšœç¢çš„ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47c40e2c19271ea63344baaa1cdcaf90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-851cefed71dd03055dee202aa3b93374.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bff05fd4865b97d0af86cd3b7475e92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-569360d203bc2186acb89f7d6ccd725e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8274cfe195b12f29332b0779ceb8c6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ISDrama-Immersive-Spatial-Drama-Generation-through-Multimodal-Prompting"><a href="#ISDrama-Immersive-Spatial-Drama-Generation-through-Multimodal-Prompting" class="headerlink" title="ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting"></a>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</h2><p><strong>Authors:Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao</strong></p>
<p>Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos are available at <a target="_blank" rel="noopener" href="https://aaronz345.github.io/ISDramaDemo">https://aaronz345.github.io/ISDramaDemo</a>. We provide the dataset and the evaluation code at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/AaronZ345/MRSDrama">https://huggingface.co/datasets/AaronZ345/MRSDrama</a> and <a target="_blank" rel="noopener" href="https://github.com/AaronZ345/ISDrama">https://github.com/AaronZ345/ISDrama</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”ŸæˆæŠ€æœ¯ä¸“æ³¨äºåŸºäºå¤šæ¨¡æ€æç¤ºåˆ›å»ºå…·æœ‰æˆå‰§è¯­è°ƒçš„è¿ç»­å¤šå£°é“åŒè€³è¯­éŸ³ï¼Œåœ¨ARã€VRç­‰é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦åŒæ—¶æ¨¡æ‹Ÿç©ºé—´ä¿¡æ¯å’Œæˆå‰§è¯­è°ƒï¼ŒåŸºäºå¤šæ¨¡æ€è¾“å…¥ï¼Œå¹¶ä¸”æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡å°è¯•åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ„å»ºäº†MRSDramaï¼Œå³é¦–ä¸ªå¤šæ¨¡æ€è®°å½•çš„ç©ºé—´æˆå‰§æ•°æ®é›†ï¼ŒåŒ…å«åŒè€³æˆå‰§éŸ³é¢‘ã€å‰§æœ¬ã€è§†é¢‘ã€å‡ ä½•å§¿åŠ¿å’Œæ–‡æœ¬æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ISDramaï¼Œå³é¦–ä¸ªé€šè¿‡å¤šæ¨¡æ€æç¤ºè¿›è¡Œæ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆæ¨¡å‹ã€‚ISDramaä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸»è¦ç»„ä»¶ï¼š1ï¼‰åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€å§¿åŠ¿ç¼–ç å™¨ï¼Œè€ƒè™‘ç§»åŠ¨æ‰¬å£°å™¨äº§ç”Ÿçš„å¤šæ™®å‹’æ•ˆåº”ï¼Œä»å¤šæ¨¡æ€æç¤ºä¸­æå–ç»Ÿä¸€å§¿åŠ¿ä¿¡æ¯ã€‚2ï¼‰æ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨ï¼Œä¸€ä¸ªåŸºäºæµçš„mamba-transformeræ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡æˆå‰§ï¼Œç»“åˆæˆå‰§MOEé€‰æ‹©é€‚å½“çš„ä¸“å®¶ä»¥å¢å¼ºè¯­è°ƒå’Œå§¿åŠ¿æ§åˆ¶ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ä¸Šä¸‹æ–‡ä¸€è‡´çš„æ— åˆ†ç±»å™¨å¼•å¯¼ç­–ç•¥ï¼Œä»¥è¿è´¯åœ°ç”Ÿæˆå®Œæ•´æˆå‰§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒISDramaåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ¼”ç¤ºåœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://aaronz345.github.io/ISDramaDemo%E3%80%82%E6%88%91%E4%BB%AC%E5%9C%A8https://huggingface.co/datasets/AaronZ345/MRSDrama%E5%92%8Chttps://github.com/AaronZ345/ISDrama%E6%8F%9B%E4%BA%86%E6%95%B0%E%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AF%84%E4%BC%B0%E4%BB%A3%E7%A0%81%E3%80%82">https://aaronz345.github.io/ISDramaDemoã€‚æˆ‘ä»¬åœ¨https://huggingface.co/datasets/AaronZ345/MRSDramaå’Œhttps://github.com/AaronZ345/ISDramaæä¾›äº†æ•°æ®é›†å’Œè¯„ä¼°ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.20630v6">PDF</a> Accepted by ACM Multimedia 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€æç¤ºçš„æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆï¼Œæ—¨åœ¨åˆ›å»ºè¿ç»­çš„å¤šæ‰¬å£°å™¨åŒè€³æˆå‰§è¯­éŸ³ï¼Œå…·æœ‰æˆå‰§æ€§çš„éŸµå¾‹ï¼Œå¹¶åº”ç”¨äºARã€VRç­‰é¢†åŸŸã€‚æ­¤ä»»åŠ¡éœ€è¦åŒæ—¶å»ºæ¨¡ç©ºé—´ä¿¡æ¯å’Œæˆå‰§éŸµå¾‹ï¼Œä¸”æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æ˜¯é¦–æ¬¡å°è¯•è§£å†³è¿™äº›æŒ‘æˆ˜çš„å·¥ä½œï¼Œæ„å»ºäº†é¦–ä¸ªå¤šæ¨¡æ€è®°å½•çš„ç©ºé—´æˆå‰§æ•°æ®é›†MRSDramaï¼Œå¹¶æå‡ºé¦–ä¸ªæ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆæ¨¡å‹ISDramaã€‚è¯¥æ¨¡å‹åŒ…æ‹¬å¤šæ¨¡æ€å§¿æ€ç¼–ç å™¨å’Œæ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡æˆå‰§å¹¶æ§åˆ¶é€‚å½“çš„ä¸“å®¶è¿›è¡ŒéŸµå¾‹å’Œå§¿æ€é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒISDramaåœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆé›†ä¸­äºåˆ›å»ºåŸºäºå¤šæ¨¡æ€æç¤ºçš„è¿ç»­å¤šæ‰¬å£°å™¨åŒè€³æˆå‰§è¯­éŸ³ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œå¦‚ARå’ŒVRã€‚</li>
<li>è¯¥ä»»åŠ¡éœ€è¦åŒæ—¶å»ºæ¨¡ç©ºé—´ä¿¡æ¯å’Œæˆå‰§éŸµå¾‹ï¼Œå¯¹æ•°æ®é‡‡é›†çš„è¦æ±‚è¾ƒé«˜ï¼Œæˆæœ¬è¾ƒå¤§ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†é¦–ä¸ªå¤šæ¨¡æ€è®°å½•çš„ç©ºé—´æˆå‰§æ•°æ®é›†MRSDramaï¼ŒåŒ…å«åŒè€³æˆå‰§éŸ³é¢‘ã€å‰§æœ¬ã€è§†é¢‘ã€å‡ ä½•å§¿æ€å’Œæ–‡æœ¬æç¤ºã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªæ²‰æµ¸å¼ç©ºé—´æˆå‰§ç”Ÿæˆæ¨¡å‹ISDramaï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å§¿æ€ç¼–ç å™¨å’Œæ²‰æµ¸å¼æˆå‰§è½¬æ¢å™¨ã€‚</li>
<li>ISDramaæ¨¡å‹è€ƒè™‘äº†å¤šæ™®å‹’æ•ˆåº”ï¼Œèƒ½æå–ç»Ÿä¸€å§¿æ€ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨åŸºäºæµçš„mamba-transformeræ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æˆå‰§ã€‚</li>
<li>å‰§ä¸­å¼•å…¥Drama-MOEä»¥é€‰æ‹©åˆé€‚çš„ä¸“å®¶å¢å¼ºéŸµå¾‹å’Œå§¿æ€æ§åˆ¶ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä¸Šä¸‹æ–‡ä¸€è‡´çš„æ— ç›‘ç£æŒ‡å¯¼ç­–ç•¥ï¼Œä»¥è¿è´¯åœ°ç”Ÿæˆå®Œæ•´æˆå‰§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.20630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0b79da2c04f799ef024fb37d2ffaa823.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-091d1f1b2d9619e5c292e077db66c27f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1876cbd475930ae447167ec2ce3c4c62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55de04137b7ad8dde779236e2045b3d4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MAVFlow-Preserving-Paralinguistic-Elements-with-Conditional-Flow-Matching-for-Zero-Shot-AV2AV-Multilingual-Translation"><a href="#MAVFlow-Preserving-Paralinguistic-Elements-with-Conditional-Flow-Matching-for-Zero-Shot-AV2AV-Multilingual-Translation" class="headerlink" title="MAVFlow: Preserving Paralinguistic Elements with Conditional Flow   Matching for Zero-Shot AV2AV Multilingual Translation"></a>MAVFlow: Preserving Paralinguistic Elements with Conditional Flow   Matching for Zero-Shot AV2AV Multilingual Translation</h2><p><strong>Authors:Sungwoo Cho, Jeongsoo Choi, Sungnyun Kim, Se-Young Yun</strong></p>
<p>Despite recent advances in text-to-speech (TTS) models, audio-visual-to-audio-visual (AV2AV) translation still faces a critical challenge: maintaining speaker consistency between the original and translated vocal and facial features. To address this issue, we propose a conditional flow matching (CFM) zero-shot audio-visual renderer that utilizes strong dual guidance from both audio and visual modalities. By leveraging multimodal guidance with CFM, our model robustly preserves speaker-specific characteristics and enhances zero-shot AV2AV translation abilities. For the audio modality, we enhance the CFM process by integrating robust speaker embeddings with x-vectors, which serve to bolster speaker consistency. Additionally, we convey emotional nuances to the face rendering module. The guidance provided by both audio and visual cues remains independent of semantic or linguistic content, allowing our renderer to effectively handle zero-shot translation tasks for monolingual speakers in different languages. We empirically demonstrate that the inclusion of high-quality mel-spectrograms conditioned on facial information not only enhances the quality of the synthesized speech but also positively influences facial generation, leading to overall performance improvements in LSE and FID score. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Peter-SungwooCho/MAVFlow">https://github.com/Peter-SungwooCho/MAVFlow</a>. </p>
<blockquote>
<p>å°½ç®¡è¿‘æœŸæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œè§†å¬è½¬è§†å¬ï¼ˆAV2AVï¼‰ç¿»è¯‘ä»ç„¶é¢ä¸´ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä¿æŒåŸå§‹å’Œç¿»è¯‘åçš„è¯­éŸ³å’Œé¢éƒ¨ç‰¹å¾ä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰çš„é›¶æ ·æœ¬è§†å¬æ¸²æŸ“å™¨ï¼Œè¯¥æ¸²æŸ“å™¨åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰ä¸¤ç§æ¨¡æ€çš„å¼ºå¤§åŒé‡æŒ‡å¯¼ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€æŒ‡å¯¼å’ŒCFMï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°ä¿ç•™ç‰¹å®šäºè¯´è¯äººçš„ç‰¹å¾ï¼Œå¹¶å¢å¼ºé›¶æ ·æœ¬AV2AVç¿»è¯‘èƒ½åŠ›ã€‚å¯¹äºéŸ³é¢‘æ¨¡æ€ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç¨³å¥çš„è¯´è¯äººåµŒå…¥ä¸xå‘é‡é›†æˆï¼Œå¢å¼ºäº†CFMè¿‡ç¨‹ï¼Œè¿™æœ‰åŠ©äºåŠ å¼ºè¯´è¯äººä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘é¢éƒ¨æ¸²æŸ“æ¨¡å—ä¼ è¾¾äº†æƒ…æ„Ÿç»†å¾®å·®åˆ«ã€‚ç”±éŸ³é¢‘å’Œè§†è§‰çº¿ç´¢æä¾›çš„æŒ‡å¯¼ç‹¬ç«‹äºè¯­ä¹‰æˆ–è¯­è¨€å†…å®¹ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„æ¸²æŸ“å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ä¸åŒè¯­è¨€çš„å•è¯­è¯´è¯äººçš„é›¶æ ·æœ¬ç¿»è¯‘ä»»åŠ¡ã€‚æˆ‘ä»¬å®è¯è¡¨æ˜ï¼Œé¢éƒ¨ä¿¡æ¯æ¡ä»¶ä¸‹é«˜è´¨é‡æ¢…å°”é¢‘è°±å›¾çš„ä½¿ç”¨ä¸ä»…æé«˜äº†åˆæˆè¯­éŸ³çš„è´¨é‡ï¼Œè€Œä¸”å¯¹é¢éƒ¨ç”Ÿæˆäº§ç”Ÿäº†ç§¯æå½±å“ï¼Œä»è€Œæé«˜äº†æ•´ä½“æ€§èƒ½è¡¨ç°åœ¨LSEå’ŒFIDåˆ†æ•°ä¸Šã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Peter-SungwooCho/MAVFlow%E3%80%82">https://github.com/Peter-SungwooCho/MAVFlowã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11026v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰çš„é›¶æ ·æœ¬éŸ³è§†é¢‘æ¸²æŸ“å™¨ï¼Œåˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰ä¸¤ç§æ¨¡æ€çš„å¼ºåŒé‡æŒ‡å¯¼æ¥è§£å†³è§†å¬è‡³éŸ³è§†é¢‘ï¼ˆAV2AVï¼‰ç¿»è¯‘ä¸­ä¿æŒå‘è¨€è€…ä¸€è‡´æ€§çš„å…³é”®æŒ‘æˆ˜ã€‚é€šè¿‡æ•´åˆè¯´è¯äººåµŒå…¥å’Œxå‘é‡å¢å¼ºCFMè¿‡ç¨‹ï¼ŒåŒæ—¶å‘é¢éƒ¨æ¸²æŸ“æ¨¡å—ä¼ è¾¾æƒ…æ„Ÿç»†å¾®å·®åˆ«ï¼Œè¯¥æ–¹æ³•ç¨³å¥åœ°ä¿ç•™äº†å‘è¨€è€…ç‰¹å®šç‰¹å¾å¹¶å¢å¼ºäº†é›¶æ ·æœ¬AV2AVç¿»è¯‘èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç‹¬ç«‹äºè¯­ä¹‰æˆ–è¯­è¨€å†…å®¹ï¼Œèƒ½æœ‰æ•ˆå¤„ç†ä¸åŒè¯­è¨€çš„å•è¯­å‘è¨€è€…é›¶æ ·æœ¬ç¿»è¯‘ä»»åŠ¡ã€‚å®éªŒè¯æ˜ï¼Œèåˆé«˜è´¨é‡æ¢…å°”é¢‘è°±å›¾æ¡ä»¶é¢éƒ¨ä¿¡æ¯ä¸ä»…æé«˜äº†åˆæˆè¯­éŸ³çš„è´¨é‡ï¼Œä¹Ÿç§¯æå½±å“äº†é¢éƒ¨ç”Ÿæˆï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢ä¸´AV2AVç¿»è¯‘çš„æŒ‘æˆ˜ï¼šä¿æŒåŸå§‹å’Œç¿»è¯‘åçš„éŸ³è§†é¢‘ä¸­çš„å‘è¨€è€…ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºåŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰çš„é›¶æ ·æœ¬éŸ³è§†é¢‘æ¸²æŸ“å™¨æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰ä¸¤ç§æ¨¡æ€çš„å¼ºåŒé‡æŒ‡å¯¼ã€‚</li>
<li>é€šè¿‡æ•´åˆè¯´è¯äººåµŒå…¥å’Œxå‘é‡æ¥å¢å¼ºCFMè¿‡ç¨‹ï¼Œä¿ç•™å‘è¨€è€…ç‰¹å®šç‰¹å¾ã€‚</li>
<li>é¢å‘é¢éƒ¨æ¸²æŸ“æ¨¡å—ä¼ è¾¾æƒ…æ„Ÿç»†å¾®å·®åˆ«ã€‚</li>
<li>æ–¹æ³•ç‹¬ç«‹äºè¯­ä¹‰æˆ–è¯­è¨€å†…å®¹ï¼Œèƒ½å¤„ç†ä¸åŒè¯­è¨€çš„å•è¯­å‘è¨€è€…é›¶æ ·æœ¬ç¿»è¯‘ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1988e148823b5113d6883be0e2fc7063.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e9fc4cd8655248d683780e81e930be0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-399ae241ed83218d3d29da6334264cba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2df157f15ebb5686624ec13d4d95b14.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Expressive Talking Human Animation"></a>Versatile Multimodal Controls for Expressive Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Sanping Zhou, Ming Yang, Le Wang</strong></p>
<p>In filmmaking, directors typically allow actors to perform freely based on the script before providing specific guidance on how to present key actions. AI-generated content faces similar requirements, where users not only need automatic generation of lip synchronization and basic gestures from audio input but also desire semantically accurate and expressive body movement that can be &#96;&#96;directly guidedâ€™â€™ through text descriptions. Therefore, we present VersaAnimator, a versatile framework that synthesizes expressive talking human videos from arbitrary portrait images. Specifically, we design a motion generator that produces basic rhythmic movements from audio input and supports text-prompt control for specific actions. The generated whole-body 3D motion tokens can animate portraits of various scales, producing talking heads, half-body gestures and even leg movements for whole-body images. Besides, we introduce a multi-modal controlled video diffusion that generates photorealistic videos, where speech signals govern lip synchronization, facial expressions, and head motions while body movements are guided by the 2D poses. Furthermore, we introduce a token2pose translator to smoothly map 3D motion tokens to 2D pose sequences. This design mitigates the stiffness resulting from direct 3D to 2D conversion and enhances the details of the generated body movements. Extensive experiments shows that VersaAnimator synthesizes lip-synced and identity-preserving videos while generating expressive and semantically meaningful whole-body motions. </p>
<blockquote>
<p>åœ¨å½±è§†åˆ¶ä½œä¸­ï¼Œå¯¼æ¼”é€šå¸¸ä¼šè®©æ¼”å‘˜æ ¹æ®å‰§æœ¬è‡ªç”±å‘æŒ¥ï¼Œç„¶åå†æä¾›å…³äºå¦‚ä½•å‘ˆç°å…³é”®åŠ¨ä½œçš„å…·ä½“æŒ‡å¯¼ã€‚äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹é¢ä¸´ç€ç±»ä¼¼çš„è¦æ±‚ï¼Œç”¨æˆ·ä¸ä»…éœ€è¦è‡ªåŠ¨ç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥åŒæ­¥çš„å˜´å”‡å’ŒåŸºæœ¬æ‰‹åŠ¿ï¼Œè¿˜å¸Œæœ›è·å¾—è¯­ä¹‰å‡†ç¡®ã€å¯Œæœ‰è¡¨ç°åŠ›çš„èº«ä½“åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œå¯ä»¥é€šè¿‡æ–‡æœ¬æè¿°æ¥â€œç›´æ¥æŒ‡å¯¼â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VersaAnimatorï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥ä»ä»»æ„è‚–åƒå›¾åƒä¸­åˆæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è°ˆè¯äººç±»è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¿åŠ¨ç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥ä»éŸ³é¢‘è¾“å…¥ä¸­äº§ç”ŸåŸºæœ¬çš„æœ‰èŠ‚å¥çš„è¿åŠ¨ï¼Œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æç¤ºæ¥æ§åˆ¶ç‰¹å®šåŠ¨ä½œã€‚ç”Ÿæˆçš„å…¨èº«ä¸‰ç»´è¿åŠ¨ä»¤ç‰Œå¯ä»¥åŠ¨ç”»åŒ–å„ç§è§„æ¨¡çš„è‚–åƒï¼Œäº§ç”Ÿè°ˆè¯å¤´éƒ¨ã€åŠèº«æ‰‹åŠ¿ç”šè‡³å…¨èº«å›¾åƒçš„ä¸‹è‚¢åŠ¨ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡å¼æ§åˆ¶çš„è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œå…¶ä¸­è¯­éŸ³ä¿¡å·æ§åˆ¶å˜´å”‡åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨è¿åŠ¨ï¼Œè€Œèº«ä½“è¿åŠ¨åˆ™ç”±äºŒç»´å§¿åŠ¿å¼•å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†token2poseè½¬æ¢å™¨ï¼Œå°†ä¸‰ç»´è¿åŠ¨ä»¤ç‰Œå¹³æ»‘åœ°æ˜ å°„åˆ°äºŒç»´å§¿åŠ¿åºåˆ—ã€‚è¿™ç§è®¾è®¡ç¼“è§£äº†ç›´æ¥ä¸‰ç»´åˆ°äºŒç»´è½¬æ¢å¯¼è‡´çš„åƒµç¡¬é—®é¢˜ï¼Œå¹¶å¢å¼ºäº†ç”Ÿæˆçš„èº«ä½“åŠ¨ä½œçš„ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVersaAnimatoråˆæˆçš„è§†é¢‘å…·æœ‰å”‡åŒæ­¥å’Œèº«ä»½ä¿ç•™ç‰¹æ€§ï¼ŒåŒæ—¶äº§ç”Ÿå¯Œæœ‰è¡¨ç°åŠ›å’Œè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å…¨èº«åŠ¨ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v4">PDF</a> Accepted by ACM MM2025</p>
<p><strong>Summary</strong></p>
<p>VersaAnimatoræ¡†æ¶èƒ½å¤ŸåŸºäºä»»æ„è‚–åƒå›¾åƒç”Ÿæˆå…·æœ‰è¡¨è¾¾èƒ½åŠ›çš„è°ˆè¯è§†é¢‘ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªåŠ¨ä½œç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿä»éŸ³é¢‘è¾“å…¥ä¸­äº§ç”ŸåŸºæœ¬èŠ‚å¥åŠ¨ä½œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æè¿°è¿›è¡Œç‰¹å®šåŠ¨ä½œçš„ç›´æ¥æŒ‡å¯¼ã€‚å®ƒèƒ½ç”Ÿæˆå…¨èº«ä¸‰ç»´åŠ¨ä½œä»¤ç‰Œï¼Œèƒ½å¤Ÿé©±åŠ¨ä¸åŒè§„æ¨¡çš„è‚–åƒå›¾åƒï¼Œäº§ç”Ÿè¯´è¯å¤´éƒ¨ã€åŠèº«å§¿æ€å’Œå…¨èº«å›¾åƒè…¿éƒ¨åŠ¨ä½œç­‰ã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤šæ¨¡æ€æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œé€šè¿‡è¯­éŸ³ä¿¡å·æ§åˆ¶å”‡åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œï¼Œé€šè¿‡äºŒç»´å§¿æ€å¼•å¯¼èº«ä½“åŠ¨ä½œã€‚åŒæ—¶ï¼Œå¼•å…¥token2poseç¿»è¯‘å™¨ï¼Œå°†ä¸‰ç»´åŠ¨ä½œä»¤ç‰Œå¹³æ»‘æ˜ å°„åˆ°äºŒç»´å§¿æ€åºåˆ—ï¼Œå¢å¼ºç”Ÿæˆè§†é¢‘çš„ç»†èŠ‚å’Œæµç•…æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VersaAnimatoræ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥ä»ä»»æ„è‚–åƒå›¾åƒåˆæˆå…·æœ‰è¡¨è¾¾èƒ½åŠ›çš„è°ˆè¯è§†é¢‘ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªåŠ¨ä½œç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿä»éŸ³é¢‘è¾“å…¥äº§ç”ŸåŸºæœ¬èŠ‚å¥åŠ¨ä½œå¹¶æ”¯æŒæ–‡æœ¬æè¿°å¯¹ç‰¹å®šåŠ¨ä½œçš„ç›´æ¥æŒ‡å¯¼ã€‚</li>
<li>ç”Ÿæˆçš„ä¸‰ç»´åŠ¨ä½œä»¤ç‰Œå¯ä»¥åº”ç”¨äºä¸åŒè§„æ¨¡çš„è‚–åƒå›¾åƒï¼ŒåŒ…æ‹¬å¤´éƒ¨ã€åŠèº«å’Œå…¨èº«åŠ¨ä½œã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œä½¿è¯­éŸ³ä¿¡å·èƒ½æ§åˆ¶å”‡åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œï¼Œè€Œèº«ä½“åŠ¨ä½œåˆ™ç”±äºŒç»´å§¿æ€å¼•å¯¼ã€‚</li>
<li>token2poseç¿»è¯‘å™¨çš„è®¾è®¡å¢å¼ºäº†ä»ä¸‰ç»´åˆ°äºŒç»´è½¬æ¢çš„æµç•…æ€§ï¼Œæé«˜äº†ç”Ÿæˆè§†é¢‘çš„ç»†èŠ‚è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶åˆæˆçš„è§†é¢‘å…·æœ‰å”‡åŒæ­¥å’Œèº«ä»½ä¿æŒçš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f12638b9cf1c0f25da3f50bc91e174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edeef7feb631e64561559cfeaf972c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7864a8c243ae7b482377f2235a595d5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1841831cdf57ca48d05cf99b17e27349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245c7d0f1c0b208f404ad96895c9a136.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multi-Microphone-and-Multi-Modal-Emotion-Recognition-in-Reverberant-Environment"><a href="#Multi-Microphone-and-Multi-Modal-Emotion-Recognition-in-Reverberant-Environment" class="headerlink" title="Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant   Environment"></a>Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant   Environment</h2><p><strong>Authors:Ohad Cohen, Gershon Hazan, Sharon Gannot</strong></p>
<p>This paper presents a Multi-modal Emotion Recognition (MER) system designed to enhance emotion recognition accuracy in challenging acoustic conditions. Our approach combines a modified and extended Hierarchical Token-semantic Audio Transformer (HTS-AT) for multi-channel audio processing with an R(2+1)D Convolutional Neural Networks (CNN) model for video analysis. We evaluate our proposed method on a reverberated version of the Ryerson audio-visual database of emotional speech and song (RAVDESS) dataset using synthetic and real-world Room Impulse Responsess (RIRs). Our results demonstrate that integrating audio and video modalities yields superior performance compared to uni-modal approaches, especially in challenging acoustic conditions. Moreover, we show that the multimodal (audiovisual) approach that utilizes multiple microphones outperforms its single-microphone counterpart. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦æ¡ä»¶ä¸‹çš„æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ä¿®æ”¹å’Œæ‰©å±•çš„åˆ†å±‚ä»¤ç‰Œè¯­ä¹‰éŸ³é¢‘è½¬æ¢å™¨ï¼ˆHTS-ATï¼‰è¿›è¡Œå¤šé€šé“éŸ³é¢‘å¤„ç†ï¼Œä»¥åŠç”¨äºè§†é¢‘åˆ†æçš„R(2+1)Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ç‘å°”æ£®è§†å¬æƒ…æ„Ÿè¯­éŸ³å’Œæ­Œæ›²æ•°æ®åº“ï¼ˆRAVDESSï¼‰çš„æ··å“ç‰ˆæœ¬æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨åˆæˆå’Œç°å®ä¸–ç•Œæˆ¿é—´å†²å‡»å“åº”ï¼ˆRIRsï¼‰è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œä¸å•æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼Œæ•´åˆéŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦æ¡ä»¶ä¸‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œåˆ©ç”¨å¤šä¸ªéº¦å…‹é£çš„è§†å¬å¤šåª’ä½“æ–¹æ³•ä¼˜äºå…¶å•éº¦å…‹é£å¯¹åº”æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.09545v3">PDF</a> 5 pages, 4 figures, 2 tables. Accepted to EUSIPCO 2025</p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æŒ‘æˆ˜ç¯å¢ƒä¸‹çš„æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶é‡‡ç”¨æ”¹è‰¯å’Œæ‰©å±•çš„åˆ†å±‚ä»¤ç‰Œè¯­ä¹‰éŸ³é¢‘è½¬æ¢å™¨ï¼ˆHTS-ATï¼‰è¿›è¡Œå¤šé€šé“éŸ³é¢‘å¤„ç†ï¼Œå¹¶ç»“åˆRï¼ˆ2+1ï¼‰Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹è¿›è¡Œè§†é¢‘åˆ†æã€‚åœ¨ç‘å°”æ£®éŸ³é¢‘è§†è§‰æƒ…æ„Ÿè¯­éŸ³å’Œæ­Œæ›²æ•°æ®åº“ï¼ˆRAVDESSï¼‰æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæˆ¿é—´è„‰å†²å“åº”ï¼ˆRIRsï¼‰å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒèåˆéŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€çš„æ–¹æ³•ç›¸è¾ƒäºå•æ¨¡æ€æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜ç¯å¢ƒä¸‹ã€‚åŒæ—¶ï¼Œä½¿ç”¨å¤šä¸ªéº¦å…‹é£çš„å¤šæ¨¡æ€ï¼ˆè§†å¬ï¼‰æ–¹æ³•ä¼˜äºå•éº¦å…‹é£æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰ç³»ç»Ÿï¼Œç”¨äºæé«˜æŒ‘æˆ˜ç¯å¢ƒä¸‹çš„æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†åˆ†å±‚ä»¤ç‰Œè¯­ä¹‰éŸ³é¢‘è½¬æ¢å™¨ï¼ˆHTS-ATï¼‰å’ŒRï¼ˆ2+1ï¼‰Då·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ï¼Œåˆ†åˆ«å¤„ç†å¤šé€šé“éŸ³é¢‘å’Œè§†é¢‘ä¿¡æ¯ã€‚</li>
<li>è®ºæ–‡åœ¨ç‘å°”æ£®éŸ³é¢‘è§†è§‰æƒ…æ„Ÿæ•°æ®åº“ä¸Šè¿›è¡Œäº†å®éªŒï¼Œä½¿ç”¨äº†åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„æˆ¿é—´è„‰å†²å“åº”ã€‚</li>
<li>èåˆéŸ³é¢‘å’Œè§†é¢‘æ¨¡æ€çš„æ–¹æ³•è¡¨ç°ä¼˜äºå•æ¨¡æ€æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜ç¯å¢ƒä¸‹ã€‚</li>
<li>ä½¿ç”¨å¤šä¸ªéº¦å…‹é£çš„å¤šæ¨¡æ€æ–¹æ³•ç›¸è¾ƒäºå•éº¦å…‹é£æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å’Œå¤šå˜çš„ç¯å¢ƒå™ªå£°æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.09545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ebc9c4bd848821e2074680dfd1d7b2e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b50ad2ac40cf9a956c82cba017d8f52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-463eee280244fad0da1d20b82dbc217d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-554144ad407247709119f3b9887ef017.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0bb2ab8b147bbcef9116cb0e293cca95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aec2a07c0c03ed11acd948aa3b87520.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Multi-Input-Multi-Output-Target-Speaker-Voice-Activity-Detection-For-Unified-Flexible-and-Robust-Audio-Visual-Speaker-Diarization"><a href="#Multi-Input-Multi-Output-Target-Speaker-Voice-Activity-Detection-For-Unified-Flexible-and-Robust-Audio-Visual-Speaker-Diarization" class="headerlink" title="Multi-Input Multi-Output Target-Speaker Voice Activity Detection For   Unified, Flexible, and Robust Audio-Visual Speaker Diarization"></a>Multi-Input Multi-Output Target-Speaker Voice Activity Detection For   Unified, Flexible, and Robust Audio-Visual Speaker Diarization</h2><p><strong>Authors:Ming Cheng, Ming Li</strong></p>
<p>Audio-visual learning has demonstrated promising results in many classical speech tasks (e.g., speech separation, automatic speech recognition, wake-word spotting). We believe that introducing visual modality will also benefit speaker diarization. To date, Target-Speaker Voice Activity Detection (TS-VAD) plays an important role in highly accurate speaker diarization. However, previous TS-VAD models take audio features and utilize the speakerâ€™s acoustic footprint to distinguish his or her personal speech activities, which is easily affected by overlapped speech in multi-speaker scenarios. Although visual information naturally tolerates overlapped speech, it suffers from spatial occlusion, low resolution, etc. The potential modality-missing problem blocks TS-VAD towards an audio-visual approach. This paper proposes a novel Multi-Input Multi-Output Target-Speaker Voice Activity Detection (MIMO-TSVAD) framework for speaker diarization. The proposed method can take audio-visual input and leverage the speakerâ€™s acoustic footprint or lip track to flexibly conduct audio-based, video-based, and audio-visual speaker diarization in a unified sequence-to-sequence framework. Experimental results show that the MIMO-TSVAD framework demonstrates state-of-the-art performance on the VoxConverse, DIHARD-III, and MISP 2022 datasets under corresponding evaluation metrics, obtaining the Diarization Error Rates (DERs) of 4.18%, 10.10%, and 8.15%, respectively. In addition, it can perform robustly in heavy lip-missing scenarios. </p>
<blockquote>
<p>è§†å¬å­¦ä¹ åœ¨å¤šä¸ªç»å…¸è¯­éŸ³ä»»åŠ¡ä¸­å±•ç°å‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä¾‹å¦‚è¯­éŸ³åˆ†ç¦»ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å”¤é†’è¯è¯†åˆ«ç­‰ã€‚æˆ‘ä»¬ç›¸ä¿¡å¼•å…¥è§†è§‰æ¨¡å¼ä¹Ÿå°†å¯¹è¯´è¯äººå£°éŸ³æ—¥è®°æœ‰ç›Šã€‚è¿„ä»Šä¸ºæ­¢ï¼Œç›®æ ‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰åœ¨é«˜åº¦å‡†ç¡®çš„è¯´è¯äººå£°éŸ³æ—¥è®°ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„TS-VADæ¨¡å‹é‡‡ç”¨éŸ³é¢‘ç‰¹å¾å¹¶åˆ©ç”¨è¯´è¯è€…çš„å£°éŸ³ç‰¹å¾æ¥åŒºåˆ†å…¶ä¸ªäººè¯­éŸ³æ´»åŠ¨ï¼Œè¿™åœ¨å¤šè¯´è¯äººåœºæ™¯ä¸­çš„é‡å è¯­éŸ³ä¸‹å¾ˆå®¹æ˜“å—åˆ°å½±å“ã€‚è™½ç„¶è§†è§‰ä¿¡æ¯è‡ªç„¶åœ°å®¹å¿é‡å è¯­éŸ³ï¼Œä½†å®ƒå—åˆ°ç©ºé—´é®æŒ¡ã€åˆ†è¾¨ç‡ä½ç­‰é—®é¢˜çš„å›°æ‰°ã€‚æ½œåœ¨çš„æ¨¡æ€ç¼ºå¤±é—®é¢˜é˜»ç¢äº†TS-VADå‘è§†å¬æ–¹æ³•çš„è½¬å˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”¨äºè¯´è¯äººå£°éŸ³æ—¥è®°çš„å¤šè¾“å…¥å¤šè¾“å‡ºç›®æ ‡è¯´è¯äººè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆMIMO-TSVADï¼‰æ¡†æ¶ã€‚è¯¥æ–¹æ³•å¯ä»¥æ¥æ”¶è§†å¬è¾“å…¥ï¼Œå¹¶åˆ©ç”¨è¯´è¯è€…çš„å£°éŸ³ç‰¹å¾æˆ–å”‡éƒ¨è½¨è¿¹åœ¨ç»Ÿä¸€çš„åºåˆ—åˆ°åºåˆ—æ¡†æ¶ä¸­è¿›è¡ŒåŸºäºéŸ³é¢‘ã€åŸºäºè§†é¢‘å’Œè§†å¬ç»“åˆçš„è¯´è¯äººå£°éŸ³æ—¥è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIMO-TSVADæ¡†æ¶åœ¨VoxConverseã€DIHARD-IIIå’ŒMISP 2022æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨ç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ä¸‹ï¼Œåˆ†åˆ«è·å¾—äº†4.18%ã€10.10%å’Œ8.15%çš„æ—¥è®°åŒ–é”™è¯¯ç‡ï¼ˆDERsï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å”‡éƒ¨ä¸¥é‡ç¼ºå¤±çš„åœºæ™¯ä¸­ä¹Ÿèƒ½ç¨³å¥åœ°è¿è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.08052v3">PDF</a> Accepted by IEEE Transactions on Audio, Speech, and Language   Processing</p>
<p><strong>æ‘˜è¦</strong><br>è§†å¬å­¦ä¹ åœ¨å¤šç§ç»å…¸è¯­éŸ³ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æˆæœï¼Œå¦‚è¯­éŸ³åˆ†ç¦»ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œå”¤é†’è¯è¯†åˆ«ç­‰ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°é¢–çš„é’ˆå¯¹æ¼”è®²è€…åˆ†ç±»çš„å¤šè¾“å…¥å¤šè¾“å‡ºç›®æ ‡è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆMIMO-TSVADï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¯èåˆè§†å¬è¾“å…¥ï¼Œåˆ©ç”¨æ¼”è®²è€…çš„å£°å­¦è¶³è¿¹æˆ–å”‡çº¹è¿›è¡Œçµæ´»éŸ³é¢‘ã€è§†é¢‘å’Œè§†å¬æ¼”è®²è€…åˆ†ç±»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMIMO-TSVADæ¡†æ¶åœ¨VoxConverseã€DIHARD-IIIå’ŒMISP 2022æ•°æ®é›†ä¸Šå®ç°å‡ºè‰²æ€§èƒ½ï¼Œå¯¹åº”è¯„ä»·æŒ‡æ ‡ä¸‹æ‰€å¾—èšç±»é”™è¯¯ç‡åˆ†åˆ«ä¸º4.18%ã€10.10%å’Œ8.15%ï¼Œä¸”åœ¨ä¸¥é‡å”‡ç¼ºå¤±åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰å­¦ä¹ åœ¨å¤šä¸ªè¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¯­éŸ³åˆ†ç¦»ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰ã€‚</li>
<li>ç›®æ ‡è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆTS-VADï¼‰åœ¨ç²¾ç¡®æ¼”è®²è€…åˆ†ç±»ä¸­èµ·åˆ°é‡è¦ä½œç”¨ã€‚</li>
<li>ä¼ ç»ŸTS-VADæ¨¡å‹ä¾èµ–äºéŸ³é¢‘ç‰¹å¾ï¼Œæ˜“å—å¤šè¯´è¯äººåœºæ™¯ä¸­çš„è¯­éŸ³é‡å å½±å“ã€‚</li>
<li>è§†è§‰ä¿¡æ¯è™½ç„¶èƒ½è‡ªç„¶å®¹å¿è¯­éŸ³é‡å ï¼Œä½†å—åˆ°ç©ºé—´é®æŒ¡ã€åˆ†è¾¨ç‡ä½ç­‰é—®é¢˜çš„å½±å“ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºMIMO-TSVADæ¡†æ¶ï¼Œèåˆè§†å¬è¾“å…¥ï¼Œè¿›è¡Œçµæ´»éŸ³é¢‘ã€è§†é¢‘å’Œè§†å¬æ¼”è®²è€…åˆ†ç±»ã€‚</li>
<li>MIMO-TSVADæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬VoxConverseã€DIHARD-IIIå’ŒMISP 2022ã€‚</li>
<li>MIMO-TSVADæ¡†æ¶åœ¨ä¸¥é‡å”‡ç¼ºå¤±åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.08052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f75a8974bdfdd72a92c33ed1fff81bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71a36ae249391160a00f6fa99b3c88ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eafe4324b63bb9c046b346fb511d631.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87003699b6e545859c78efb2ff81ab3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9856b1f091e15a649fb63ec83c642b47.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d1ebfb707a977432f368bd71b86ffedd.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  X-NeMo Expressive Neural Motion Reenactment via Disentangled Latent   Attention
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-915fbd0455e5a59e4a1b7e840d9d098f.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Pathology Foundation Models are Scanner Sensitive Benchmark and   Mitigation with Contrastive ScanGen Loss
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
