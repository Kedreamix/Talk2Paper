<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Efficient Masked Attention Transformer for Few-Shot Classification and   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1a63c44975d3d4faa55d2ab02950c7f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="Efficient-Masked-Attention-Transformer-for-Few-Shot-Classification-and-Segmentation"><a href="#Efficient-Masked-Attention-Transformer-for-Few-Shot-Classification-and-Segmentation" class="headerlink" title="Efficient Masked Attention Transformer for Few-Shot Classification and   Segmentation"></a>Efficient Masked Attention Transformer for Few-Shot Classification and   Segmentation</h2><p><strong>Authors:Dustin CarriÃ³n-Ojeda, Stefan Roth, Simone Schaub-Meyer</strong></p>
<p>Few-shot classification and segmentation (FS-CS) focuses on jointly performing multi-label classification and multi-class segmentation using few annotated examples. Although the current state of the art (SOTA) achieves high accuracy in both tasks, it struggles with small objects. To overcome this, we propose the Efficient Masked Attention Transformer (EMAT), which improves classification and segmentation accuracy, especially for small objects. EMAT introduces three modifications: a novel memory-efficient masked attention mechanism, a learnable downscaling strategy, and parameter-efficiency enhancements. EMAT outperforms all FS-CS methods on the PASCAL-5$^i$ and COCO-20$^i$ datasets, using at least four times fewer trainable parameters. Moreover, as the current FS-CS evaluation setting discards available annotations, despite their costly collection, we introduce two novel evaluation settings that consider these annotations to better reflect practical scenarios. </p>
<blockquote>
<p>å°‘æ ·æœ¬åˆ†ç±»ä¸åˆ†å‰²ï¼ˆFS-CSï¼‰ä¸»è¦å…³æ³¨ä½¿ç”¨å°‘é‡æ ‡æ³¨æ ·æœ¬è”åˆæ‰§è¡Œå¤šæ ‡ç­¾åˆ†ç±»å’Œå¤šç±»åˆ†å‰²ã€‚å°½ç®¡å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­éƒ½å–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨å°ç›®æ ‡ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆæ©è”½æ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆEMATï¼‰ï¼Œå®ƒæé«˜äº†åˆ†ç±»å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯å¯¹å°ç›®æ ‡ã€‚EMATå¼•å…¥äº†ä¸‰ç§æ”¹è¿›ï¼šä¸€ç§æ–°å‹çš„å†…å­˜é«˜æ•ˆæ©è”½æ³¨æ„åŠ›æœºåˆ¶ã€ä¸€ç§å¯å­¦ä¹ çš„é™å°ºåº¦ç­–ç•¥ä»¥åŠå‚æ•°æ•ˆç‡æå‡ã€‚EMATåœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæ‰€æœ‰FS-CSæ–¹æ³•ï¼Œä½¿ç”¨è‡³å°‘å°‘å››å€çš„å¯è®­ç»ƒå‚æ•°ã€‚æ­¤å¤–ï¼Œç”±äºå½“å‰çš„FS-CSè¯„ä¼°è®¾ç½®æ”¾å¼ƒäº†å¯ç”¨çš„æ³¨é‡Šï¼Œå°½ç®¡è¿™äº›æ³¨é‡Šçš„æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§è€ƒè™‘è¿™äº›æ³¨é‡Šçš„æ–°å‹è¯„ä¼°è®¾ç½®ï¼Œä»¥æ›´å¥½åœ°åæ˜ å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23642v1">PDF</a> Accepted for GCPR 2025. Project page: <a target="_blank" rel="noopener" href="https://visinf.github.io/emat">https://visinf.github.io/emat</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„è§†è§‰è½¬æ¢æ¨¡å‹Efficient Masked Attention Transformerï¼ˆEMATï¼‰ï¼Œæ—¨åœ¨æé«˜åŸºäºå°‘æ•°æ ‡æ³¨æ ·æœ¬çš„å›¾åƒåˆ†ç±»å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«å…³æ³¨å¯¹å°ç‰©ä½“çš„è¯†åˆ«ã€‚ç›¸è¾ƒäºå½“å‰å…ˆè¿›çš„æ¨¡å‹ï¼ŒEMATæ›´ä¸ºé«˜æ•ˆä¸”å…·æœ‰ä¼˜åŠ¿ï¼Œå¯¹PASCAL-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šçš„å°‘æ ·æœ¬åˆ†ç±»å’Œåˆ†å‰²æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œé‰´äºç°æœ‰è¯„ä¼°è®¾ç½®å¿½ç•¥æˆæœ¬é«˜æ˜‚çš„æ ‡æ³¨ä¿¡æ¯ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸¤ç§æ–°çš„è¯„ä¼°è®¾ç½®ä»¥æ›´æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Efficient Masked Attention Transformer (EMAT)æ—¨åœ¨æ”¹è¿›åŸºäºå°‘é‡æ ‡æ³¨æ ·æœ¬çš„åˆ†ç±»å’Œåˆ†å‰²æ•ˆæœã€‚</li>
<li>EMATé’ˆå¯¹å°ç‰©ä½“è¯†åˆ«è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>EMATé€šè¿‡å¼•å…¥ä¸‰ç§æ”¹è¿›â€”â€”é«˜æ•ˆæ©ç æ³¨æ„åŠ›æœºåˆ¶ã€å¯å­¦ä¹ ç¼©æ”¾ç­–ç•¥å’Œå‚æ•°æ•ˆç‡å¢å¼ºâ€”â€”æé«˜äº†åˆ†ç±»å’Œåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>EMATåœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºæ‰€æœ‰ç°æœ‰çš„å°‘æ ·æœ¬åˆ†ç±»å’Œåˆ†å‰²æ–¹æ³•ã€‚</li>
<li>å½“å‰FS-CSè¯„ä¼°è®¾ç½®å¿½ç•¥äº†æ˜‚è´µçš„æ ‡æ³¨ä¿¡æ¯ï¼Œå› æ­¤æå‡ºäº†ä¸¤ç§æ–°çš„è¯„ä¼°è®¾ç½®ä»¥æ›´æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5729b46a8eeae87c1078265abaea01c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f5a5df8fe57be4a9ea188e0bdf488f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31b7c9355797d8d379c23b0c8fb34f80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6646afaa8ecf22f2d5cd6e80a3898969.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rethinking-Few-Shot-CLIP-Benchmarks-A-Critical-Analysis-in-the-Inductive-Setting"><a href="#Rethinking-Few-Shot-CLIP-Benchmarks-A-Critical-Analysis-in-the-Inductive-Setting" class="headerlink" title="Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the   Inductive Setting"></a>Rethinking Few Shot CLIP Benchmarks: A Critical Analysis in the   Inductive Setting</h2><p><strong>Authors:Alexey Kravets, Da Chen, Vinay P. Namboodiri</strong></p>
<p>CLIP is a foundational model with transferable classification performance in the few-shot setting. Several methods have shown improved performance of CLIP using few-shot examples. However, so far, all these techniques have been benchmarked using standard few-shot datasets. We argue that this mode of evaluation does not provide a true indication of the inductive generalization ability using few-shot examples. As most datasets have been seen by the CLIP model, the resultant setting can be termed as partially transductive. To solve this, we propose a pipeline that uses an unlearning technique to obtain true inductive baselines. In this new inductive setting, the methods show a significant drop in performance (-55% on average among 13 baselines with multiple datasets). We validate the unlearning technique using oracle baselines. An improved few-shot classification technique is proposed that consistently obtains state-of-the-art performance over 13 other recent baseline methods on a comprehensive analysis with 5880 experiments - varying the datasets, differing number of few-shot examples, unlearning setting, and with different seeds. Thus, we identify the issue with the evaluation of CLIP-based few-shot classification, provide a solution using unlearning, propose new benchmarks, and provide an improved method. </p>
<blockquote>
<p>CLIPæ˜¯ä¸€ä¸ªé€‚ç”¨äºå°æ ·æœ¬è®¾ç½®çš„å…·æœ‰å¯è¿ç§»åˆ†ç±»æ€§èƒ½çš„åŸºç¡€æ¨¡å‹ã€‚å‡ ç§æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºä½¿ç”¨å°æ ·æœ¬ä¾‹å­æ”¹è¿›CLIPçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰è¿™äº›æŠ€æœ¯éƒ½æ˜¯ç”¨æ ‡å‡†çš„å°æ ·æœ¬æ•°æ®é›†è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§è¯„ä¼°æ–¹å¼å¹¶ä¸èƒ½çœŸæ­£åæ˜ å‡ºä½¿ç”¨å°æ ·æœ¬ä¾‹å­è¿›è¡Œå½’çº³æ¦‚æ‹¬çš„èƒ½åŠ›ã€‚ç”±äºCLIPæ¨¡å‹å·²ç»çœ‹åˆ°è¿‡å¤§å¤šæ•°æ•°æ®é›†ï¼Œå› æ­¤æ‰€å¾—çš„è®¾ç½®å¯ä»¥è¢«ç§°ä¸ºéƒ¨åˆ†å½’çº³æ€§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨é—å¿˜æŠ€æœ¯æ¥è·å¾—çœŸæ­£çš„å½’çº³åŸºå‡†çš„ç®¡é“ã€‚åœ¨æ–°çš„å½’çº³è®¾ç½®ä¸­ï¼Œè¿™äº›æ–¹æ³•è¡¨ç°å‡ºæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆåœ¨å¤šä¸ªæ•°æ®é›†çš„13ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡ä¸‹é™55%ï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨OracleåŸºå‡†éªŒè¯äº†é—å¿˜æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å°æ ·æœ¬åˆ†ç±»æŠ€æœ¯ï¼Œåœ¨ç»¼åˆåˆ†æçš„13ä¸ªå…¶ä»–æœ€æ–°åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆè·å¾—æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…±è¿›è¡Œäº†5880æ¬¡å®éªŒâ€”â€”æ”¹å˜äº†æ•°æ®é›†ã€ä¸åŒæ•°é‡çš„å°æ ·æœ¬ä¾‹å­ã€é—å¿˜è®¾ç½®å’Œä¸åŒç§å­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‘ç°äº†åŸºäºCLIPçš„å°æ ·æœ¬åˆ†ç±»è¯„ä¼°çš„é—®é¢˜ï¼Œæä¾›äº†ä½¿ç”¨é—å¿˜çš„è§£å†³æ–¹æ³•ï¼Œæå‡ºäº†æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æä¾›äº†ä¸€ç§æ”¹è¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20834v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰å¯è¿ç§»çš„åˆ†ç±»æ€§èƒ½ã€‚è™½ç„¶å·²æœ‰æ–¹æ³•è¯æ˜ä½¿ç”¨å°‘é‡æ ·æœ¬å¯ä»¥æé«˜CLIPçš„æ€§èƒ½ï¼Œä½†ç›®å‰çš„è¯„ä¼°æ–¹å¼ä¸»è¦åŸºäºæ ‡å‡†å°‘æ ·æœ¬æ•°æ®é›†ï¼Œå¹¶ä¸èƒ½çœŸæ­£åæ˜ æ¨¡å‹ä½¿ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œå½’çº³æ³›åŒ–çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨â€œé—å¿˜æŠ€æœ¯â€æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶åœ¨æ–°çš„å½’çº³è®¾ç½®ä¸‹å‘ç°ç°æœ‰æ–¹æ³•æ€§èƒ½å¹³å‡ä¸‹é™55%ã€‚æˆ‘ä»¬éªŒè¯äº†é—å¿˜æŠ€æœ¯çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„å°‘æ ·æœ¬åˆ†ç±»æŠ€æœ¯ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤§é‡å®éªŒåˆ†æä¸­ï¼Œç›¸è¾ƒäºå…¶ä»–åŸºçº¿æ–¹æ³•è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¯†åˆ«äº†CLIPå°‘æ ·æœ¬åˆ†ç±»è¯„ä¼°çš„é—®é¢˜ï¼Œæä¾›äº†åŸºäºé—å¿˜æŠ€æœ¯çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†æ–°çš„åŸºå‡†æ–¹æ³•å’Œæ”¹è¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹å¼ä¸èƒ½çœŸæ­£åæ˜ æ¨¡å‹ä½¿ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œå½’çº³æ³›åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨çœŸæ­£çš„å½’çº³è®¾ç½®ä¸‹æ€§èƒ½å¹³å‡ä¸‹é™55%ã€‚</li>
<li>ä½¿ç”¨é—å¿˜æŠ€æœ¯è§£å†³è¯„ä¼°ä¸­çš„é—®é¢˜ã€‚</li>
<li>æ”¹è¿›åçš„å°‘æ ·æœ¬åˆ†ç±»æŠ€æœ¯åœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤§é‡å®éªŒåˆ†æä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>æˆ‘ä»¬æä¾›äº†åŸºäºé—å¿˜æŠ€æœ¯çš„è§£å†³æ–¹æ¡ˆå’Œæ–°çš„åŸºå‡†æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20834">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09a833b7b289bbc48792262abeb9c1ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e698e63b994abd113af6ec129d19259.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29f53e8e653653e24d247ade9aa1e089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-197e97e49b054abf191add87018ec104.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Audio-Classification-by-Transitioning-from-Zero-to-Few-Shot"><a href="#Improving-Audio-Classification-by-Transitioning-from-Zero-to-Few-Shot" class="headerlink" title="Improving Audio Classification by Transitioning from Zero- to Few-Shot"></a>Improving Audio Classification by Transitioning from Zero- to Few-Shot</h2><p><strong>Authors:James Taylor, Wolfgang Mack</strong></p>
<p>State-of-the-art audio classification often employs a zero-shot approach, which involves comparing audio embeddings with embeddings from text describing the respective audio class. These embeddings are usually generated by neural networks trained through contrastive learning to align audio and text representations. Identifying the optimal text description for an audio class is challenging, particularly when the class comprises a wide variety of sounds. This paper examines few-shot methods designed to improve classification accuracy beyond the zero-shot approach. Specifically, audio embeddings are grouped by class and processed to replace the inherently noisy text embeddings. Our results demonstrate that few-shot classification typically outperforms the zero-shot baseline. </p>
<blockquote>
<p>ç›®å‰å…ˆè¿›çš„éŸ³é¢‘åˆ†ç±»é€šå¸¸é‡‡ç”¨é›¶æ ·æœ¬æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¶‰åŠå°†éŸ³é¢‘åµŒå…¥ä¸æè¿°ç›¸åº”éŸ³é¢‘ç±»åˆ«çš„æ–‡æœ¬åµŒå…¥è¿›è¡Œæ¯”è¾ƒã€‚è¿™äº›åµŒå…¥é€šå¸¸ç”±é€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„ç¥ç²¾ç½‘ç»œç”Ÿæˆï¼Œä»¥å¯¹é½éŸ³é¢‘å’Œæ–‡æœ¬è¡¨ç¤ºã€‚ä¸ºéŸ³é¢‘ç±»åˆ«ç¡®å®šæœ€ä½³æ–‡æœ¬æè¿°å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å½“ç±»åˆ«åŒ…å«å¤šç§å£°éŸ³æ—¶ã€‚æœ¬æ–‡ç ”ç©¶äº†æ—¨åœ¨æé«˜é›¶æ ·æœ¬æ–¹æ³•ä¹‹å¤–åˆ†ç±»ç²¾åº¦çš„å°‘æ ·æœ¬æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒéŸ³é¢‘åµŒå…¥æŒ‰ç±»åˆ«åˆ†ç»„å¹¶ç»è¿‡å¤„ç†ä»¥æ›¿æ¢å›ºæœ‰çš„å™ªå£°æ–‡æœ¬åµŒå…¥ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°‘æ ·æœ¬åˆ†ç±»é€šå¸¸ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20036v1">PDF</a> Submitted to Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†éŸ³é¢‘åˆ†ç±»ä¸­çš„å°‘æ ·æœ¬æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•æ—¨åœ¨æ”¹è¿›é›¶æ ·æœ¬æ–¹æ³•çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚è®ºæ–‡é‡‡ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„ç¥ç»ç½‘ç»œç”ŸæˆéŸ³é¢‘åµŒå…¥å’Œæ–‡æœ¬åµŒå…¥ï¼Œé€šè¿‡å¯¹éŸ³é¢‘åµŒå…¥è¿›è¡Œåˆ†ç»„å¤„ç†ä»¥æ›¿ä»£æœ¬è´¨ä¸Šå­˜åœ¨å™ªå£°çš„æ–‡æœ¬åµŒå…¥ï¼Œä»¥å®ç°æ›´ä½³çš„åˆ†ç±»æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°‘æ ·æœ¬åˆ†ç±»é€šå¸¸ä¼˜äºé›¶æ ·æœ¬åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ¢è®¨äº†éŸ³é¢‘åˆ†ç±»ä¸­çš„å°‘æ ·æœ¬æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹è¿›é›¶æ ·æœ¬æ–¹æ³•çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>éŸ³é¢‘åµŒå…¥æ˜¯é€šè¿‡ç¥ç»ç½‘ç»œç”Ÿæˆçš„ï¼Œè¿™äº›ç½‘ç»œé€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä»¥å¯¹é½éŸ³é¢‘å’Œæ–‡æœ¬è¡¨ç¤ºã€‚</li>
<li>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä¸ºéŸ³é¢‘ç±»åˆ«æ‰¾åˆ°æœ€ä½³æ–‡æœ¬æè¿°å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç±»åˆ«åŒ…å«å¤šç§å£°éŸ³æ—¶ã€‚</li>
<li>å°‘æ ·æœ¬åˆ†ç±»é€šè¿‡åˆ©ç”¨æœ‰é™çš„å¸¦æ ‡ç­¾æ•°æ®æ¥æ”¹å–„åˆ†ç±»æ€§èƒ½ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­å°¤å…¶é‡è¦ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡å¯¹éŸ³é¢‘åµŒå…¥è¿›è¡Œåˆ†ç»„å¤„ç†æ¥æ›¿ä»£å­˜åœ¨å™ªå£°çš„æ–‡æœ¬åµŒå…¥ï¼Œä»è€Œæé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå°‘æ ·æœ¬åˆ†ç±»æ–¹æ³•é€šå¸¸ä¼˜äºé›¶æ ·æœ¬åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20036">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-973564b3e9081e35ceb949b0d4807732.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1588344f1bb309eb33065399b9e815aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bc678024f5207ace92b19c1aa30698a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbd9df91ba00416fac3606b05b33d630.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Anomaly-Detection-in-Human-Language-via-Meta-Learning-A-Few-Shot-Approach"><a href="#Anomaly-Detection-in-Human-Language-via-Meta-Learning-A-Few-Shot-Approach" class="headerlink" title="Anomaly Detection in Human Language via Meta-Learning: A Few-Shot   Approach"></a>Anomaly Detection in Human Language via Meta-Learning: A Few-Shot   Approach</h2><p><strong>Authors:Saurav Singla, Aarav Singla, Advik Gupta, Parnika Gupta</strong></p>
<p>We propose a meta learning framework for detecting anomalies in human language across diverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to hate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection as a few shot binary classification problem and leverage meta-learning to train models that generalize across tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we evaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines episodic training with prototypical networks and domain resampling to adapt quickly to new anomaly detection tasks. Empirical results show that our method outperforms strong baselines in F1 and AUC scores. We also release the code and benchmarks to facilitate further research in few-shot text anomaly detection. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ä¸ªå…ƒå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨å¤šæ ·é¢†åŸŸå’Œæœ‰é™æ ‡æ³¨æ•°æ®ä¸‹æ£€æµ‹äººç±»è¯­è¨€çš„å¼‚å¸¸å€¼ã€‚ä»åƒåœ¾é‚®ä»¶å’Œå‡æ–°é—»åˆ°ä»‡æ¨è¨€è®ºçš„å¼‚å¸¸è¯­è¨€è¡¨ç°å‡ºå¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶ç¨€ç¼ºæ€§å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬æŠŠå¼‚å¸¸æ£€æµ‹è§†ä¸ºå°æ ·æœ¬äºŒåˆ†ç±»é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å…ƒå­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä»»åŠ¡ä¹‹é—´æ³›åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªçŸ­ä¿¡åƒåœ¾é‚®ä»¶ã€COVID-19å‡æ–°é—»å’Œä»‡æ¨è¨€è®ºç­‰é¢†åŸŸçš„æ•°æ®é›†ï¼Œåœ¨æœªè§è¿‡çš„ä»»åŠ¡ä¸Šè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå…¶ä¸­æ ‡æ³¨çš„å¼‚å¸¸å€¼å¾ˆå°‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æƒ…å¢ƒè®­ç»ƒã€åŸå‹ç½‘ç»œå’Œé¢†åŸŸé‡é‡‡æ ·ï¼Œå¯ä»¥å¿«é€Ÿé€‚åº”æ–°çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨F1å’ŒAUCå¾—åˆ†ä¸Šä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä»£ç å’ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥ä¿ƒè¿›åœ¨å°‘æ ·æœ¬æ–‡æœ¬å¼‚å¸¸æ£€æµ‹æ–¹é¢çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20019v1">PDF</a> 15 pages. PyTorch code for few-shot anomaly detection using   meta-learning is available upon request or can be shared via GitHub</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ƒå­¦ä¹ æ¡†æ¶è¿›è¡Œè·¨é¢†åŸŸäººç±»è¯­è¨€å¼‚å¸¸æ£€æµ‹çš„æ–¹æ³•ï¼Œå°¤å…¶é€‚ç”¨äºæœ‰é™æ ‡æ³¨æ•°æ®ä¸‹çš„åœºæ™¯ã€‚é’ˆå¯¹ä»åƒåœ¾é‚®ä»¶ã€å‡æ–°é—»åˆ°ä»‡æ¨è¨€è®ºç­‰å¤šæ ·åŒ–çš„è¯­è¨€å¼‚å¸¸é—®é¢˜ï¼Œæœ¬æ–‡å°†å¼‚å¸¸æ£€æµ‹è§†ä¸ºå°æ ·æœ¬äºŒåˆ†ç±»é—®é¢˜ï¼Œå¹¶å€ŸåŠ©å…ƒå­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œå®ç°è·¨ä»»åŠ¡æ³›åŒ–ã€‚é€šè¿‡æ¥è‡ªçŸ­ä¿¡åƒåœ¾é‚®ä»¶ã€COVID-19å‡æ–°é—»å’Œä»‡æ¨è¨€è®ºç­‰é¢†åŸŸçš„æ•°æ®é›†ï¼Œå¯¹æ¨¡å‹åœ¨æ–°å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›è¿›è¡Œè¯„ä¼°ã€‚æœ¬æ–‡ç»“åˆäº†æƒ…å¢ƒè®­ç»ƒã€åŸå‹ç½‘ç»œå’Œé¢†åŸŸé‡é‡‡æ ·ç­‰æ–¹æ³•ï¼Œå®ç°äº†å¿«é€Ÿé€‚åº”æ–°å¼‚å¸¸æ£€æµ‹ä»»åŠ¡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨F1å’ŒAUCåˆ†æ•°ä¸Šä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå…ƒå­¦ä¹ æ¡†æ¶çš„è·¨é¢†åŸŸäººç±»è¯­è¨€å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>å°†å¼‚å¸¸æ£€æµ‹è§†ä¸ºå°æ ·æœ¬äºŒåˆ†ç±»é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆæƒ…å¢ƒè®­ç»ƒã€åŸå‹ç½‘ç»œå’Œé¢†åŸŸé‡é‡‡æ ·ï¼Œæ¨¡å‹èƒ½è¿…é€Ÿé€‚åº”æ–°å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨æ¥è‡ªå¤šæ ·åŒ–é¢†åŸŸï¼ˆå¦‚çŸ­ä¿¡åƒåœ¾é‚®ä»¶ã€COVID-19å‡æ–°é—»å’Œä»‡æ¨è¨€è®ºï¼‰çš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨F1å’ŒAUCåˆ†æ•°ä¸Šè¶…è¶Šäº†ä¸€äº›åŸºçº¿æ–¹æ³•ã€‚</li>
<li>å…¬å¼€äº†ä»£ç å’ŒåŸºå‡†æµ‹è¯•ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å°æ ·æœ¬æ–‡æœ¬å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>å¯¹äºæœ‰é™æ ‡æ³¨æ•°æ®çš„åœºæ™¯ï¼Œè¯¥æ–¹æ³•å±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de709291f298f36b288c1b5e0a56f81e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Knowledge-Regularized-Negative-Feature-Tuning-of-Vision-Language-Models-for-Out-of-Distribution-Detection"><a href="#Knowledge-Regularized-Negative-Feature-Tuning-of-Vision-Language-Models-for-Out-of-Distribution-Detection" class="headerlink" title="Knowledge Regularized Negative Feature Tuning of Vision-Language Models   for Out-of-Distribution Detection"></a>Knowledge Regularized Negative Feature Tuning of Vision-Language Models   for Out-of-Distribution Detection</h2><p><strong>Authors:Wenjie Zhu, Yabin Zhang, Xin Jin, Wenjun Zeng, Lei Zhang</strong></p>
<p>Out-of-distribution (OOD) detection is crucial for building reliable machine learning models. Although negative prompt tuning has enhanced the OOD detection capabilities of vision-language models, these tuned models often suffer from reduced generalization performance on unseen classes and styles. To address this challenge, we propose a novel method called Knowledge Regularized Negative Feature Tuning (KR-NFT), which integrates an innovative adaptation architecture termed Negative Feature Tuning (NFT) and a corresponding knowledge-regularization (KR) optimization strategy. Specifically, NFT applies distribution-aware transformations to pre-trained text features, effectively separating positive and negative features into distinct spaces. This separation maximizes the distinction between in-distribution (ID) and OOD images. Additionally, we introduce image-conditional learnable factors through a lightweight meta-network, enabling dynamic adaptation to individual images and mitigating sensitivity to class and style shifts. Compared to traditional negative prompt tuning, NFT demonstrates superior efficiency and scalability. To optimize this adaptation architecture, the KR optimization strategy is designed to enhance the discrimination between ID and OOD sets while mitigating pre-trained knowledge forgetting. This enhances OOD detection performance on trained ID classes while simultaneously improving OOD detection on unseen ID datasets. Notably, when trained with few-shot samples from ImageNet dataset, KR-NFT not only improves ID classification accuracy and OOD detection but also significantly reduces the FPR95 by 5.44% under an unexplored generalization setting with unseen ID categories. Codes can be found at \href{<a target="_blank" rel="noopener" href="https://github.com/ZhuWenjie98/KRNFT%7D">https://github.com/ZhuWenjie98/KRNFT}</a>. </p>
<blockquote>
<p>åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹å¯¹äºæ„å»ºå¯é çš„æœºå™¨å­¦ä¹ æ¨¡å‹è‡³å…³é‡è¦ã€‚å°½ç®¡è´Ÿæç¤ºè°ƒæ•´æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„OODæ£€æµ‹èƒ½åŠ›ï¼Œä½†è¿™äº›è°ƒæ•´åçš„æ¨¡å‹å¾€å¾€åœ¨é¢å¯¹æœªè§è¿‡çš„ç±»åˆ«å’Œé£æ ¼æ—¶è¡¨ç°å‡ºé™ä½çš„æ³›åŒ–æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºçŸ¥è¯†æ­£åˆ™åŒ–è´Ÿç‰¹å¾è°ƒæ•´ï¼ˆKR-NFTï¼‰ï¼Œå®ƒç»“åˆäº†åä¸ºè´Ÿç‰¹å¾è°ƒæ•´ï¼ˆNFTï¼‰çš„åˆ›æ–°é€‚åº”æ¶æ„å’Œç›¸åº”çš„çŸ¥è¯†æ­£åˆ™åŒ–ï¼ˆKRï¼‰ä¼˜åŒ–ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼ŒNFTå¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬ç‰¹å¾è¿›è¡Œåˆ†å¸ƒæ„ŸçŸ¥è½¬æ¢ï¼Œæœ‰æ•ˆåœ°å°†æ­£è´Ÿç‰¹å¾åˆ†ç¦»åˆ°ä¸åŒçš„ç©ºé—´ã€‚è¿™ç§åˆ†ç¦»æœ€å¤§é™åº¦åœ°æé«˜äº†å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰å’ŒOODå›¾åƒä¹‹é—´çš„åŒºåˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡çº§å…ƒç½‘ç»œå¼•å…¥å›¾åƒæ¡ä»¶å¯å­¦ä¹ å› å­ï¼Œå®ç°åŠ¨æ€é€‚åº”ä¸ªåˆ«å›¾åƒå¹¶å‡è½»å¯¹ç±»åˆ«å’Œé£æ ¼å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚ä¸ä¼ ç»Ÿçš„è´Ÿæç¤ºè°ƒæ•´ç›¸æ¯”ï¼ŒNFTè¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†ä¼˜åŒ–æ­¤é€‚åº”æ¶æ„ï¼ŒKRä¼˜åŒ–ç­–ç•¥æ—¨åœ¨å¢å¼ºIDå’ŒOODé›†ä¹‹é—´çš„è¾¨åˆ«åŠ›ï¼ŒåŒæ—¶å‡è½»é¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜ã€‚è¿™æé«˜äº†åœ¨è®­ç»ƒçš„IDç±»åˆ«ä¸Šçš„OODæ£€æµ‹æ€§èƒ½ï¼ŒåŒæ—¶æ”¹å–„äº†åœ¨æœªè§è¿‡çš„IDæ•°æ®é›†ä¸Šçš„OODæ£€æµ‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ä½¿ç”¨æ¥è‡ªImageNetæ•°æ®é›†çš„å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒKR-NFTä¸ä»…æé«˜äº†IDåˆ†ç±»ç²¾åº¦å’ŒOODæ£€æµ‹ï¼Œè€Œä¸”åœ¨æœªæ¢ç´¢çš„å…·æœ‰æœªè§è¿‡çš„IDç±»åˆ«çš„æ³›åŒ–è®¾ç½®ä¸‹ï¼Œå°†FPR95é™ä½äº†5.44ï¼…ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhuWenjie98/KRNFT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhuWenjie98/KRNFTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19847v2">PDF</a> accepted by ACMMM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºKR-NFTçš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­æ¨¡å‹çš„åˆ†å¸ƒå¤–æ£€æµ‹èƒ½åŠ›ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è´Ÿç‰¹å¾è°ƒä¼˜ï¼ˆNFTï¼‰å’ŒçŸ¥è¯†æ­£åˆ™åŒ–ï¼ˆKRï¼‰ç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè´Ÿæç¤ºè°ƒä¼˜åœ¨åº”å¯¹æœªè§ç±»åˆ«å’Œé£æ ¼æ—¶çš„æ³›åŒ–æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚NFTé€šè¿‡åˆ†å¸ƒæ„ŸçŸ¥è½¬æ¢æ¥æœ‰æ•ˆåˆ†ç¦»æ­£è´Ÿç‰¹å¾ï¼Œæé«˜æ¨¡å‹å¯¹åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¼•å…¥çš„å›¾åƒæ¡ä»¶å¯å­¦ä¹ å› ç´ é€šè¿‡è½»é‡çº§å…ƒç½‘ç»œå®ç°åŠ¨æ€é€‚åº”ä¸ªä½“å›¾åƒï¼Œå‡è½»ç±»åˆ«å’Œé£æ ¼å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒKR-NFTå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½åœ¨ä¿æŒå¯¹è®­ç»ƒé›†çš„åˆ†ç±»ç²¾åº¦å’Œæé«˜æœªè§æ•°æ®é›†çš„OODæ£€æµ‹èƒ½åŠ›çš„åŒæ—¶ï¼Œé™ä½FPR95ã€‚æ­¤æ–¹æ³•åœ¨ImageNetæ•°æ®é›†å°‘é‡æ ·æœ¬è®­ç»ƒä¸‹è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KR-NFTæ–¹æ³•ç»“åˆäº†è´Ÿç‰¹å¾è°ƒä¼˜ï¼ˆNFTï¼‰å’ŒçŸ¥è¯†æ­£åˆ™åŒ–ï¼ˆKRï¼‰ç­–ç•¥ï¼Œæ—¨åœ¨å¢å¼ºæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­æ¨¡å‹çš„åˆ†å¸ƒå¤–æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>NFTé€šè¿‡åˆ†å¸ƒæ„ŸçŸ¥è½¬æ¢æœ‰æ•ˆåˆ†ç¦»æ­£è´Ÿç‰¹å¾ï¼Œæé«˜æ¨¡å‹å¯¹åˆ†å¸ƒå†…ï¼ˆIDï¼‰å’Œåˆ†å¸ƒå¤–ï¼ˆOODï¼‰å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>è½»é‡çº§å…ƒç½‘ç»œå¼•å…¥å›¾åƒæ¡ä»¶å¯å­¦ä¹ å› ç´ ï¼Œå®ç°æ¨¡å‹å¯¹ä¸ªä½“å›¾åƒçš„åŠ¨æ€é€‚åº”ï¼Œå‡å°‘ç±»åˆ«å’Œé£æ ¼å˜åŒ–çš„å½±å“ã€‚</li>
<li>KRä¼˜åŒ–ç­–ç•¥æ—¨åœ¨å¢å¼ºIDå’ŒOODé›†ä¹‹é—´çš„åŒºåˆ†åº¦ï¼ŒåŒæ—¶å‡å°‘é¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜ã€‚</li>
<li>KR-NFTæ–¹æ³•èƒ½æé«˜æ¨¡å‹çš„åˆ†ç±»ç²¾åº¦å’ŒOODæ£€æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒæ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨ImageNetæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒKR-NFTèƒ½æœ‰æ•ˆé™ä½FPR95è¾¾5.44%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eed7cd1d1f1404cd94072d2944bcc9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eef513fd2fc62d5a82d5717faa0271a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14d9aead208d46a34db6826b31e4bf56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1990559244bd346ad5493df0d003d307.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f591d0bf706ed0cf76f68b162811f37b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Balancing-Conservatism-and-Aggressiveness-Prototype-Affinity-Hybrid-Network-for-Few-Shot-Segmentation"><a href="#Balancing-Conservatism-and-Aggressiveness-Prototype-Affinity-Hybrid-Network-for-Few-Shot-Segmentation" class="headerlink" title="Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid   Network for Few-Shot Segmentation"></a>Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid   Network for Few-Shot Segmentation</h2><p><strong>Authors:Tianyu Zou, Shengwu Xiong, Ruilin Yao, Yi Rong</strong></p>
<p>This paper studies the few-shot segmentation (FSS) task, which aims to segment objects belonging to unseen categories in a query image by learning a model on a small number of well-annotated support samples. Our analysis of two mainstream FSS paradigms reveals that the predictions made by prototype learning methods are usually conservative, while those of affinity learning methods tend to be more aggressive. This observation motivates us to balance the conservative and aggressive information captured by these two types of FSS frameworks so as to improve the segmentation performance. To achieve this, we propose a <strong>P</strong>rototype-<strong>A</strong>ffinity <strong>H</strong>ybrid <strong>Net</strong>work (PAHNet), which introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention Score Calibration (ASC) module in each attention block of an affinity learning model (called affinity learner). These two modules utilize the predictions generated by a pre-trained prototype learning model (called prototype predictor) to enhance the foreground information in support and query image representations and suppress the mismatched foreground-background (FG-BG) relationships between them, respectively. In this way, the aggressiveness of the affinity learner can be effectively mitigated, thereby eventually increasing the segmentation accuracy of our PAHNet method. Experimental results show that PAHNet outperforms most recently proposed methods across 1-shot and 5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its effectiveness. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/tianyu-zou/PAHNet">GitHub - tianyu-zou&#x2F;PAHNet: Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation (ICCVâ€™25)</a> </p>
<blockquote>
<p>æœ¬æ–‡ç ”ç©¶äº†å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯é€šè¿‡åœ¨å°‘é‡æ ‡æ³¨è‰¯å¥½çš„æ”¯æ’‘æ ·æœ¬ä¸Šå­¦ä¹ æ¨¡å‹ï¼Œå¯¹æŸ¥è¯¢å›¾åƒä¸­å±äºæœªè§ç±»åˆ«çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚æˆ‘ä»¬å¯¹ä¸¤ç§ä¸»æµçš„FSSèŒƒå¼çš„åˆ†æè¡¨æ˜ï¼ŒåŸå‹å­¦ä¹ æ–¹æ³•åšå‡ºçš„é¢„æµ‹é€šå¸¸è¾ƒä¸ºä¿å®ˆï¼Œè€Œäº²å’ŒåŠ›å­¦ä¹ æ–¹æ³•åšå‡ºçš„é¢„æµ‹åˆ™æ›´å€¾å‘äºæ¿€è¿›ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœä¿ƒä½¿æˆ‘ä»¬å¹³è¡¡è¿™ä¸¤ç§FSSæ¡†æ¶æ‰€æ•è·çš„ä¿å®ˆå’Œæ¿€è¿›ä¿¡æ¯ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§<strong>P</strong>rototype-<strong>A</strong>ffinity <strong>H</strong>ybrid <strong>Net</strong>workï¼ˆPAHNetï¼‰ï¼Œå®ƒåœ¨äº²å’ŒåŠ›å­¦ä¹ æ¨¡å‹ï¼ˆç§°ä¸ºäº²å’ŒåŠ›å­¦ä¹ è€…ï¼‰çš„æ¯ä¸ªæ³¨æ„åŠ›å—ä¸­å¼•å…¥äº†Prototype-guided Feature Enhancementï¼ˆPFEï¼‰æ¨¡å—å’ŒAttention Score Calibrationï¼ˆASCï¼‰æ¨¡å—ã€‚è¿™ä¸¤ä¸ªæ¨¡å—åˆ©ç”¨ç”±é¢„è®­ç»ƒçš„åŸå‹å­¦ä¹ æ¨¡å‹ï¼ˆç§°ä¸ºåŸå‹é¢„æµ‹å™¨ï¼‰ç”Ÿæˆçš„é¢„æµ‹ç»“æœï¼Œå¢å¼ºæ”¯æ’‘å›¾åƒå’ŒæŸ¥è¯¢å›¾åƒè¡¨ç¤ºä¸­çš„å‰æ™¯ä¿¡æ¯ï¼Œå¹¶åˆ†åˆ«æŠ‘åˆ¶å®ƒä»¬ä¹‹é—´ä¸åŒ¹é…çš„å‰æ™¯-èƒŒæ™¯ï¼ˆFG-BGï¼‰å…³ç³»ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å‡è½»äº²å’ŒåŠ›å­¦ä¹ è€…çš„æ¿€è¿›æ€§ï¼Œä»è€Œæœ€ç»ˆæé«˜æˆ‘ä»¬çš„PAHNetæ–¹æ³•çš„åˆ†å‰²ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šï¼ŒPAHNetåœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹å‡ä¼˜äºæœ€è¿‘æå‡ºçš„å¤§å¤šæ•°æ–¹æ³•ï¼Œè¿™è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/tianyu-zou/PAHNet">GitHub - tianyu-zou&#x2F;PAHNet: å¹³è¡¡ä¿å®ˆä¸æ¿€è¿›ï¼šç”¨äºå°æ ·æœ¬åˆ†å‰²çš„åŸå‹-äº²å’ŒåŠ›æ··åˆç½‘ç»œï¼ˆICCVâ€™25ï¼‰</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19140v1">PDF</a> 8 pages, 7 figures</p>
<p><strong>Summary</strong><br>è¯¥è®ºæ–‡ç ”ç©¶äº†å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡æ ‡æ³¨è‰¯å¥½çš„æ ·æœ¬å­¦ä¹ æ¨¡å‹ï¼Œå¯¹æŸ¥è¯¢å›¾åƒä¸­æœªè§ç±»åˆ«è¿›è¡Œå¯¹è±¡åˆ†å‰²ã€‚åˆ†æä¸¤ç§ä¸»æµFSSèŒƒå¼åï¼Œå‘ç°åŸå‹å­¦ä¹ æ–¹æ³•é¢„æµ‹é€šå¸¸ä¿å®ˆï¼Œè€Œäº²å’ŒåŠ›å­¦ä¹ æ–¹æ³•é¢„æµ‹æ›´ä¸ºæ¿€è¿›ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸå‹äº²å’ŒåŠ›æ··åˆç½‘ç»œï¼ˆPAHNetï¼‰ï¼Œé€šè¿‡å¼•å…¥åŸå‹å¼•å¯¼ç‰¹å¾å¢å¼ºï¼ˆPFEï¼‰æ¨¡å—å’Œæ³¨æ„åŠ›å¾—åˆ†æ ¡å‡†ï¼ˆASCï¼‰æ¨¡å—ï¼Œå¹³è¡¡ä¸¤ç§æ–¹æ³•çš„é¢„æµ‹ä¿¡æ¯ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPAHNetåœ¨PASCAL-5$^i$å’ŒCOCO-20$^i$æ•°æ®é›†ä¸Šçš„1-shotå’Œ5-shotè®¾ç½®ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ç ”ç©¶äº†å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡æ ‡æ³¨è‰¯å¥½çš„æ ·æœ¬å¯¹æœªè§ç±»åˆ«è¿›è¡Œå¯¹è±¡åˆ†å‰²ã€‚</li>
<li>åˆ†æå‘ç°ï¼ŒåŸå‹å­¦ä¹ æ–¹æ³•é¢„æµ‹é€šå¸¸ä¿å®ˆï¼Œè€Œäº²å’ŒåŠ›å­¦ä¹ æ–¹æ³•é¢„æµ‹è¾ƒä¸ºæ¿€è¿›ã€‚</li>
<li>ä¸ºäº†å¹³è¡¡è¿™ä¸¤ç§æ–¹æ³•çš„é¢„æµ‹ä¿¡æ¯ï¼Œè®ºæ–‡æå‡ºäº†åŸå‹äº²å’ŒåŠ›æ··åˆç½‘ç»œï¼ˆPAHNetï¼‰ã€‚</li>
<li>PAHNeté€šè¿‡å¼•å…¥PFEå’ŒASCæ¨¡å—ï¼Œæé«˜äº†åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPAHNetåœ¨å¤šç§æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>PAHNetæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£äº²å’ŒåŠ›å­¦ä¹ è€…çš„æ¿€è¿›æ€§ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f427f5bf8065ef1a4841afa8146b78f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1281956f0c479f49a5ede250954b2afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df33feee0cb8404bd288e9ef91affc1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-404c6a726e33539ef7fb3dfd34d2dfa3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d0f0c2957d81fcb6ba983397c48ccb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Performance-of-AI-Text-Detectors-Few-Shot-and-Chain-of-Thought-Prompting-Using-DeepSeek-Generated-Text"><a href="#Evaluating-the-Performance-of-AI-Text-Detectors-Few-Shot-and-Chain-of-Thought-Prompting-Using-DeepSeek-Generated-Text" class="headerlink" title="Evaluating the Performance of AI Text Detectors, Few-Shot and   Chain-of-Thought Prompting Using DeepSeek Generated Text"></a>Evaluating the Performance of AI Text Detectors, Few-Shot and   Chain-of-Thought Prompting Using DeepSeek Generated Text</h2><p><strong>Authors:Hulayyil Alshammari, Praveen Rao</strong></p>
<p>Large language models (LLMs) have rapidly transformed the creation of written materials. LLMs have led to questions about writing integrity, thereby driving the creation of artificial intelligence (AI) detection technologies. Adversarial attacks, such as standard and humanized paraphrasing, inhibit detectorsâ€™ ability to detect machine-generated text. Previous studies have mainly focused on ChatGPT and other well-known LLMs and have shown varying accuracy across detectors. However, there is a clear gap in the literature about DeepSeek, a recently published LLM. Therefore, in this work, we investigate whether six generally accessible AI detection tools â€“ AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero â€“ can consistently recognize text generated by DeepSeek. The detectors were exposed to the aforementioned adversarial attacks. We also considered DeepSeek as a detector by performing few-shot prompting and chain-of-thought reasoning (CoT) for classifying AI and human-written text. We collected 49 human-authored question-answer pairs from before the LLM era and generated matching responses using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied adversarial techniques such as paraphrasing and humanizing to add 196 more samples. These were used to challenge detector robustness and assess accuracy impact. While QuillBot and Copyleaks showed near-perfect performance on original and paraphrased DeepSeek text, others â€“ particularly AI Text Classifier and GPT-2 â€“ showed inconsistent results. The most effective attack was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and 52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best five-shot result misclassifying only one of 49 samples (AI recall 96%, human recall 100%). </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿…é€Ÿæ”¹å˜äº†ä¹¦é¢ææ–™çš„åˆ›ä½œæ–¹å¼ã€‚å®ƒä»¬å¼•å‘äº†å…³äºå†™ä½œå®Œæ•´æ€§çš„é—®é¢˜ï¼Œä»è€Œæ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚å¯¹æŠ—æ€§æ”»å‡»ï¼Œå¦‚æ ‡å‡†å¤è¿°å’Œäººæ€§åŒ–å¤è¿°ï¼Œé™åˆ¶äº†æ£€æµ‹å™¨æ£€æµ‹æœºå™¨ç”Ÿæˆæ–‡æœ¬çš„èƒ½åŠ›ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ChatGPTå’Œå…¶ä»–çŸ¥åçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼Œå¹¶ä¸”åœ¨æ£€æµ‹å™¨ä¹‹é—´æ˜¾ç¤ºäº†ä¸åŒçš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå…³äºæœ€è¿‘å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹DeepSeekçš„æ–‡çŒ®ä¸­å­˜åœ¨æ˜æ˜¾çš„ç©ºç™½ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…­ç§æ™®éå¯ç”¨çš„AIæ£€æµ‹å·¥å…·â€”â€”AIæ–‡æœ¬åˆ†ç±»å™¨ã€å†…å®¹æ£€æµ‹AIã€Copyleaksã€QuillBotã€GPT-2å’ŒGPTZeroï¼Œæ˜¯å¦èƒ½ä¸€è‡´åœ°è¯†åˆ«DeepSeekç”Ÿæˆçš„æ–‡æœ¬ã€‚è¿™äº›æ£€æµ‹å™¨å—åˆ°äº†ä¸Šè¿°å¯¹æŠ—æ€§æ”»å‡»çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¿˜é€šè¿‡è¿›è¡Œå°‘é‡æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰å°†DeepSeekä½œä¸ºæ£€æµ‹å™¨æ¥åˆ†ç±»AIå’Œäººç±»æ’°å†™çš„æ–‡æœ¬ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£ä¹‹å‰æ”¶é›†äº†49ä¸ªäººç±»æé—®å’Œå›ç­”çš„å¯¹ï¼Œå¹¶ä½¿ç”¨DeepSeek-v3ç”Ÿæˆäº†åŒ¹é…çš„å›åº”ï¼Œäº§ç”Ÿäº†49ä¸ªAIç”Ÿæˆçš„æ ·æœ¬ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿ç”¨è¯¸å¦‚å¤è¿°å’Œäººæ€§åŒ–ç­‰å¯¹æŠ—æŠ€æœ¯ï¼Œå¢åŠ äº†196ä¸ªæ ·æœ¬ã€‚è¿™äº›è¢«ç”¨æ¥æŒ‘æˆ˜æ£€æµ‹å™¨çš„ç¨³å¥æ€§å¹¶è¯„ä¼°å‡†ç¡®æ€§çš„å½±å“ã€‚è™½ç„¶QuillBotå’ŒCopyleaksåœ¨åŸå§‹å’Œå¤è¿°çš„DeepSeekæ–‡æœ¬ä¸Šè¡¨ç°å‡ºè¿‘ä¹å®Œç¾çš„æ€§èƒ½ï¼Œä½†å…¶ä»–äººâ€”â€”å°¤å…¶æ˜¯AIæ–‡æœ¬åˆ†ç±»å™¨å’ŒGPT-2â€”â€”çš„ç»“æœå´ä¸å°½ä¸€è‡´ã€‚æœ€æœ‰æ•ˆçš„æ”»å‡»æ˜¯äººæ€§åŒ–æ”»å‡»ï¼Œå°†Copyleaksçš„å‡†ç¡®ç‡é™ä½åˆ°71%ï¼ŒQuillBotçš„å‡†ç¡®ç‡é™ä½åˆ°58%ï¼ŒGPTZeroçš„å‡†ç¡®ç‡é™ä½åˆ°52%ã€‚å°‘é‡æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œå…¶ä¸­æœ€å¥½çš„äº”é•œå¤´ç»“æœä»…è¯¯åˆ¤äº†49ä¸ªæ ·æœ¬ä¸­çš„ä¸€ä¸ªï¼ˆAIå¬å›ç‡ä¸º96%ï¼Œäººç±»å¬å›ç‡ä¸º100%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17944v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•å¼•å‘äº†å…³äºå†™ä½œå®Œæ•´æ€§çš„é—®é¢˜ï¼Œå¹¶æ¨åŠ¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ£€æµ‹æŠ€æœ¯çš„è¿›æ­¥ã€‚å½“å‰å­˜åœ¨å…­ç§å¸¸ç”¨çš„AIæ£€æµ‹å·¥å…·ï¼Œå®ƒä»¬åœ¨å¯¹æŠ—æ”»å‡»ä¸‹è¯†åˆ«DeepSeekç”Ÿæˆçš„æ–‡æœ¬çš„èƒ½åŠ›æœ‰æ‰€å·®å¼‚ã€‚é€šè¿‡é‡‡ç”¨å°‘æ ·æœ¬æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰æ–¹æ³•ï¼Œç ”ç©¶è¯„ä¼°äº†è¿™äº›å·¥å…·çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDeepSeekåœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°å‡ºè¾ƒé«˜çš„ç”Ÿæˆæ–‡æœ¬èƒ½åŠ›ï¼Œä½†å¯¹æŠ—æ€§æŠ€æœ¯å¦‚äººæ€§åŒ–å¤„ç†ä»èƒ½æœ‰æ•ˆé™ä½æ£€æµ‹å·¥å…·çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ™®åŠå¼•å‘å¯¹å†™ä½œå®Œæ•´æ€§å’ŒAIæ£€æµ‹æŠ€æœ¯çš„å…³æ³¨ã€‚</li>
<li>ç›®å‰å¸¸ç”¨çš„AIæ£€æµ‹å·¥å…·åœ¨é¢å¯¹DeepSeekç”Ÿæˆçš„æ–‡æœ¬æ—¶ï¼Œå…¶æ£€æµ‹èƒ½åŠ›å­˜åœ¨å·®å¼‚ã€‚</li>
<li>å¯¹æŠ—æ”»å‡»ï¼Œç‰¹åˆ«æ˜¯äººæ€§åŒ–å¤„ç†ï¼Œèƒ½æœ‰æ•ˆé™ä½æ£€æµ‹å·¥å…·çš„å‡†ç¡®æ€§ã€‚</li>
<li>AIæ£€æµ‹å·¥å…·åœ¨é¢å¯¹DeepSeekç”Ÿæˆçš„æ–‡æœ¬æ—¶ï¼ŒQuillBotå’ŒCopyleaksè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>å°‘æ ·æœ¬æç¤ºå’Œé“¾å¼æ€ç»´æ¨ç†ï¼ˆCoTï¼‰æ–¹æ³•èƒ½æ˜¾è‘—æé«˜DeepSeekçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>DeepSeekåœ¨æŸäº›æƒ…å†µä¸‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹å¯¹æŠ—æ€§æŠ€æœ¯æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17944">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35bf13cba6903e56ff3638bb88ef0bfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cf7941de4cabe740787e573d78936a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e2e12c686a2adaadfa1b4c2a873dcaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-634aa6588f69da1ed7d8aeca20dd35ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58dfac29b0a7282a1cf82bd64ee3e3e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60370fd9f6aed50c1907e5a19d724b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee69a69b1dda142f303ac9787ea6bee1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c50d3d9b3fdceff5931f0fbf74682b5d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Masked-Language-Models-are-Good-Heterogeneous-Graph-Generalizers"><a href="#Masked-Language-Models-are-Good-Heterogeneous-Graph-Generalizers" class="headerlink" title="Masked Language Models are Good Heterogeneous Graph Generalizers"></a>Masked Language Models are Good Heterogeneous Graph Generalizers</h2><p><strong>Authors:Jinyu Yang, Cheng Yang, Shanyuan Cui, Zeyuan Guo, Liangwei Yang, Muhan Zhang, Zhiqiang Zhang, Chuan Shi</strong></p>
<p>Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. With the rapid advancement of large language models (LLMs), a recent study explored the integration of HGNNs with LLMs for generalizable heterogeneous graph learning. However, this approach typically encodes structural information as HG tokens using HGNNs, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLMâ€™s comprehension of HGs. Moreover, since these HG tokens are often derived from node-level tasks, the modelâ€™s ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style â€˜maskâ€™ token prediction paradigm. Specifically,MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/MLM4HG">https://github.com/BUPT-GAMMA/MLM4HG</a>. </p>
<blockquote>
<p>å¼‚è´¨å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰æ“…é•¿æ•è·å¼‚è´¨å›¾ï¼ˆHGï¼‰ä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä½†åœ¨è·¨åŸŸå’Œä»»åŠ¡ä¹‹é—´çš„æ³›åŒ–æ–¹é¢é‡åˆ°å›°éš¾ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæœ€è¿‘çš„ä¸€é¡¹ç ”ç©¶æ¢ç´¢äº†å°†HGNNsä¸LLMsç»“åˆè¿›è¡Œå¯æ³›åŒ–çš„å¼‚è´¨å›¾å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é€šå¸¸ä½¿ç”¨HGNNså°†ç»“æ„ä¿¡æ¯ç¼–ç ä¸ºHGä»¤ç‰Œï¼ŒHGNNså’ŒLLMsä¹‹é—´çš„åµŒå…¥ç©ºé—´å·®å¼‚å·²è¯æ˜ä¼šåå‘LLMå¯¹HGçš„ç†è§£ã€‚æ­¤å¤–ï¼Œç”±äºè¿™äº›HGä»¤ç‰Œé€šå¸¸æ¥æºäºèŠ‚ç‚¹çº§ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºMasked Language Modelingçš„æ–¹æ³•ï¼Œç§°ä¸ºMLM4HGã€‚MLM4HGå¼•å…¥åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—ï¼Œè€Œä¸æ˜¯HGä»¤ç‰Œï¼Œä»¥æå–HGå›ºæœ‰çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶è®¾è®¡å®šåˆ¶çš„æ–‡æœ¬æ¨¡æ¿å°†ä¸åŒçš„å›¾ä»»åŠ¡ç»Ÿä¸€ä¸ºä¸€ä¸ªè¿è´¯çš„å¡«å……å¼â€œæ©ç â€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼ŒMLM4HGé¦–å…ˆæ ¹æ®å…ƒè·¯å¾„å°†æ¥è‡ªä¸åŒé¢†åŸŸçš„HGè½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åå°†å…¶ä¸ç»Ÿä¸€çš„ä»»åŠ¡æ–‡æœ¬ç»“åˆå½¢æˆåŸºäºHGçš„è¯­æ–™åº“ã€‚æ­¤å¤–ï¼Œè¯¥è¯­æ–™åº“è¢«è¾“å…¥åˆ°é¢„è®­ç»ƒçš„LMä¸­è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨å—é™çš„ç›®æ ‡è¯æ±‡è¡¨ï¼Œä½¿å¾®è°ƒåçš„LMèƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„ç›®æ ‡HGã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è·¨åŸŸå’Œå¤šä»»åŠ¡å®éªŒå¹¿æ³›è¡¨æ˜ï¼ŒMLM4HGåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­çš„æ³›åŒ–æ€§èƒ½å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/MLM4HG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BUPT-GAMMA/MLM4HGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06157v2">PDF</a> </p>
<p><strong>Summary</strong><br>    HGNNä¸LLMç»“åˆè¿›è¡Œå¼‚è´¨å›¾å­¦ä¹ çš„æ–¹æ³•è™½èƒ½æ•æ‰ç»“æ„è¯­ä¹‰ä¿¡æ¯ï¼Œä½†åœ¨è·¨åŸŸè·¨ä»»åŠ¡æ³›åŒ–æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†MLM4HGæ–¹æ³•ï¼Œé€šè¿‡å…ƒè·¯å¾„æ–‡æœ¬åºåˆ—æ›¿ä»£HGä»¤ç‰Œï¼Œè®¾è®¡ç»Ÿä¸€ä»»åŠ¡æ–‡æœ¬ï¼Œå½¢æˆæ©ç›–å¼â€œæ©ç›–â€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ã€‚MLM4HGèƒ½è½¬åŒ–ä¸åŒé¢†åŸŸçš„å¼‚è´¨å›¾ä¸ºæ–‡æœ¬ï¼Œç»“åˆä»»åŠ¡æ–‡æœ¬å½¢æˆåŸºäºå¼‚è´¨å›¾çš„è¯­æ–™åº“ï¼Œå¹¶å¾®è°ƒé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä»¥å®ç°æ³›åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒMLM4HGåœ¨è·¨åŸŸå¤šä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HGNNåœ¨æ•æ‰å¼‚è´¨å›¾çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·¨åŸŸå’Œä»»åŠ¡é—´çš„æ³›åŒ–èƒ½åŠ›ä¸Šé‡åˆ°å›°éš¾ã€‚</li>
<li>ç»“åˆLLMsçš„æ–¹æ³•é€šå¸¸å°†ç»“æ„ä¿¡æ¯ç¼–ç ä¸ºHGä»¤ç‰Œï¼Œä½†å­˜åœ¨åµŒå…¥ç©ºé—´å·®å¼‚é—®é¢˜ï¼Œå½±å“LLMå¯¹HGçš„ç†è§£ã€‚</li>
<li>MLM4HGé‡‡ç”¨åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—æ›¿ä»£HGä»¤ç‰Œï¼Œæ›´æœ‰æ•ˆåœ°æå–å¼‚è´¨å›¾ä¸­çš„ç»“æ„è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>MLM4HGé€šè¿‡è®¾è®¡ç»Ÿä¸€çš„ä»»åŠ¡æ–‡æœ¬ï¼Œå°†ä¸åŒçš„å›¾ä»»åŠ¡æ•´åˆåˆ°ä¸€è‡´çš„æ©ç›–å¼â€œæ©ç›–â€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ä¸­ã€‚</li>
<li>MLM4HGèƒ½å°†å¼‚è´¨å›¾ä»å„ç§é¢†åŸŸè½¬æ¢ä¸ºæ–‡æœ¬ï¼Œä¸ä»»åŠ¡æ–‡æœ¬ç»“åˆå½¢æˆåŸºäºå¼‚è´¨å›¾çš„è¯­æ–™åº“ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ŒMLM4HGæé«˜äº†å¯¹æœªè§è¿‡çš„å¼‚è´¨å›¾çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-633d8ef0d37b9093b73b3f40450f7602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc267be8fa6f6e9d1c7730eef403859b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6fb6b0271b53fdc00917ea42aa447a2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Continual-MEGA-A-Large-scale-Benchmark-for-Generalizable-Continual-Anomaly-Detection"><a href="#Continual-MEGA-A-Large-scale-Benchmark-for-Generalizable-Continual-Anomaly-Detection" class="headerlink" title="Continual-MEGA: A Large-scale Benchmark for Generalizable Continual   Anomaly Detection"></a>Continual-MEGA: A Large-scale Benchmark for Generalizable Continual   Anomaly Detection</h2><p><strong>Authors:Geonu Lee, Yujeong Oh, Geonhui Jang, Soyoung Lee, Jeonghyo Song, Sungmin Cha, YoungJoon Yoo</strong></p>
<p>In this paper, we introduce a new benchmark for continual learning in anomaly detection, aimed at better reflecting real-world deployment scenarios. Our benchmark, Continual-MEGA, includes a large and diverse dataset that significantly expands existing evaluation settings by combining carefully curated existing datasets with our newly proposed dataset, ContinualAD. In addition to standard continual learning with expanded quantity, we propose a novel scenario that measures zero-shot generalization to unseen classes, those not observed during continual adaptation. This setting poses a new problem setting that continual adaptation also enhances zero-shot performance. We also present a unified baseline algorithm that improves robustness in few-shot detection and maintains strong generalization. Through extensive evaluations, we report three key findings: (1) existing methods show substantial room for improvement, particularly in pixel-level defect localization; (2) our proposed method consistently outperforms prior approaches; and (3) the newly introduced ContinualAD dataset enhances the performance of strong anomaly detection models. We release the benchmark and code in <a target="_blank" rel="noopener" href="https://github.com/Continual-Mega/Continual-Mega">https://github.com/Continual-Mega/Continual-Mega</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹å¼‚å¸¸æ£€æµ‹ä¸­çš„æŒç»­å­¦ä¹ çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ›´å¥½åœ°åæ˜ çœŸå®ä¸–ç•Œçš„éƒ¨ç½²åœºæ™¯ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•â€œContinual-MEGAâ€åŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œå®ƒé€šè¿‡ç»“åˆç²¾å¿ƒç­–åˆ’çš„ç°æœ‰æ•°æ®é›†å’Œæˆ‘ä»¬æ–°æå‡ºçš„æ•°æ®é›†ContinualADï¼Œæ˜¾è‘—æ‰©å±•äº†ç°æœ‰çš„è¯„ä¼°è®¾ç½®ã€‚é™¤äº†æ ‡å‡†æŒç»­å­¦ä¹ æ‰©å……æ•°é‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯ï¼Œå³æµ‹é‡å¯¹æœªè§ç±»åˆ«çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¿™äº›ç±»åˆ«åœ¨æŒç»­é€‚åº”æœŸé—´æœªè¢«è§‚å¯Ÿåˆ°ã€‚è¿™ä¸€è®¾ç½®æå‡ºäº†ä¸€ä¸ªæ–°çš„éš¾é¢˜ï¼Œå³æŒç»­é€‚åº”ä¹Ÿèƒ½æé«˜é›¶æ ·æœ¬æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åŸºçº¿ç®—æ³•ï¼Œè¯¥ç®—æ³•æé«˜äº†å°‘æ ·æœ¬æ£€æµ‹ä¸­çš„ç¨³å¥æ€§ï¼Œå¹¶ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰ç°æœ‰æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§ç¼ºé™·å®šä½æ–¹é¢ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æŒç»­é€‚åº”æœŸé—´å§‹ç»ˆä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼›ï¼ˆ3ï¼‰æ–°æ¨å‡ºçš„ContinualADæ•°æ®é›†å¢å¼ºäº†å¼ºå¤§çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Continual-Mega/Continual-Mega%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/Continual-Mega/Continual-Megaä¸Šå‘å¸ƒäº†åŸºå‡†æµ‹è¯•å’Œä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00956v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé’ˆå¯¹å¼‚å¸¸æ£€æµ‹ä¸­çš„æŒç»­å­¦ä¹ çš„æ–°åŸºå‡†æµ‹è¯•é›†ï¼Œå³Continual-MEGAã€‚è¯¥åŸºå‡†æµ‹è¯•é›†åŒ…æ‹¬å¤§è§„æ¨¡å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œé€šè¿‡ç»“åˆç°æœ‰ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†å’Œæ–°æå‡ºçš„ContinualADæ•°æ®é›†ï¼Œæ‰©å±•äº†ç°æœ‰çš„è¯„ä¼°è®¾ç½®ã€‚é™¤äº†æ ‡å‡†æŒç»­å­¦ä¹ çš„å¤§è§„æ¨¡æ‰©å±•å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„åœºæ™¯æ¥è¡¡é‡å¯¹æœªè§ç±»åˆ«çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„åŸºçº¿ç®—æ³•ï¼Œæé«˜äº†å°‘æ ·æœ¬æ£€æµ‹çš„é²æ£’æ€§å¹¶ä¿æŒäº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæœ¬æ–‡æŠ¥å‘Šäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šç°æœ‰æ–¹æ³•å­˜åœ¨æ”¹è¿›ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨åƒç´ çº§ç¼ºé™·å®šä½æ–¹é¢ï¼›æ‰€ææ–¹æ³•å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼›æ–°å¼•å…¥çš„ContinualADæ•°æ®é›†å¢å¼ºäº†å¼ºå¤§çš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹å¼‚å¸¸æ£€æµ‹ä¸­çš„æŒç»­å­¦ä¹ çš„æ–°åŸºå‡†æµ‹è¯•é›†â€”â€”Continual-MEGAã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•é›†åŒ…æ‹¬å¤§è§„æ¨¡å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæ‰©å±•äº†ç°æœ‰è¯„ä¼°è®¾ç½®ã€‚</li>
<li>é™¤äº†æ ‡å‡†æŒç»­å­¦ä¹ å¤–ï¼Œè¿˜è€ƒè™‘äº†é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„è¡¡é‡ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åŸºçº¿ç®—æ³•ï¼Œç”¨äºæé«˜å°‘æ ·æœ¬æ£€æµ‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åƒç´ çº§ç¼ºé™·å®šä½æ–¹é¢å­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚</li>
<li>æ‰€ææ–¹æ³•åœ¨æŸäº›å…³é”®æ–¹é¢å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00956">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8eb1d6cf420efa97b7a90a1614a6b43f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b384f7e7535d75a3c1428b42ee83a06d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd0513e70160279833fcaa2e76bd5765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ae320d170991ee9773116f302f5343b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bb35465123734622e14c0b275dcb212.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning"><a href="#Leveraging-Large-Language-Models-for-Bengali-Math-Word-Problem-Solving-with-Chain-of-Thought-Reasoning" class="headerlink" title="Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning"></a>Leveraging Large Language Models for Bengali Math Word Problem Solving   with Chain of Thought Reasoning</h2><p><strong>Authors:Bidyarthi Paul, Jalisha Jashim Era, Mirazur Rahman Zim, Tahmid Sattar Aothoi, Faisal Muhammad Shah</strong></p>
<p>Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the languageâ€™s low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies. </p>
<blockquote>
<p>è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰ä»ç„¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå­ŸåŠ æ‹‰è¯­çš„èµ„æºç›¸å¯¹è¾ƒå°‘ä»¥åŠæ‰€éœ€çš„å¤šæ­¥éª¤æ¨ç†é€ æˆçš„ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦é—®é¢˜æ—¶é¢ä¸´å›°éš¾ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å› ä¸ºä¹‹å‰æ²¡æœ‰äººå¯¹å­ŸåŠ æ‹‰æ•°æ®é›†è¿›è¡Œäººå·¥æ ‡æ³¨æ¥è§£å†³è¿™ä¸€ä»»åŠ¡ã€‚è¿™ä¸€å·®è·é™åˆ¶äº†å­ŸåŠ æ‹‰æ•°å­¦æ¨ç†çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼ŒåŒ…å«8792ä¸ªå¤æ‚çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜å’Œæ‰‹åŠ¨ç¼–å†™çš„åˆ†æ­¥è§£ç­”ã€‚æˆ‘ä»¬è®¾è®¡è¿™ä¸ªæ•°æ®é›†æ˜¯ä¸ºäº†æ”¯æŒåœ¨ç¼ºä¹ä»£è¡¨æ€§è¯­å¢ƒä¸­çš„æ¨ç†è¯„ä¼°æ¨¡å‹å‘å±•ã€‚ä½¿ç”¨SOMADHANæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒåŒ…æ‹¬GPT-4oã€GPT-3.5 Turboã€LLaMAç³»åˆ—æ¨¡å‹ã€Deepseekå’ŒQwenç­‰ï¼Œé€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºä»¥åŠæœ‰æ— æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿›è¡Œã€‚æ€ç»´é“¾æç¤ºå§‹ç»ˆæé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„ä»»åŠ¡ä¸­ã€‚LLaMA-3.3 70Båœ¨å°‘æ ·æœ¬æ€ç»´é“¾æç¤ºä¸‹å–å¾—äº†88%çš„æœ€é«˜å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¥æœ‰æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼Œå¹¶å°½é‡å‡å°‘è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æä¾›é«˜è´¨é‡çš„æ¨ç†æ•°æ®é›†å’Œè§£å†³å¤æ‚æ•°å­¦æ–‡å­—é¢˜çš„å¯æ‰©å±•æ¡†æ¶ï¼Œå¡«è¡¥äº†å­ŸåŠ æ‹‰NLPé¢†åŸŸçš„ä¸€ä¸ªå…³é”®ç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨ä½èµ„æºè¯­è¨€çš„å…¬å¹³ç ”ç©¶ï¼Œå¹¶æå‡æ•™è‚²å’Œè¯­è¨€æŠ€æœ¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21354v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§£å†³å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„ä¸»è¦æŒ‘æˆ˜ï¼Œå› å­ŸåŠ æ‹‰è¯­çš„ä½èµ„æºçŠ¶æ€å’Œå¤šæ­¥éª¤æ¨ç†çš„éœ€æ±‚ã€‚å…ˆå‰æ¨¡å‹éš¾ä»¥å¤„ç†å¤æ‚çš„å­ŸåŠ æ‹‰MWPsï¼Œå› ä¸ºæ²¡æœ‰é’ˆå¯¹æ­¤ä»»åŠ¡çš„å­ŸåŠ æ‹‰è¯­äººå·¥æ ‡æ³¨æ•°æ®é›†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†SOMADHANæ•°æ®é›†ï¼ŒåŒ…å«8792é“å¤æ‚çš„å­ŸåŠ æ‹‰MWPså’Œæ‰‹åŠ¨ç¼–å†™çš„é€æ­¥è§£ç­”ã€‚æ­¤æ•°æ®é›†æ—¨åœ¨æ”¯æŒè¯­è¨€ä¸Šè¢«å¿½è§†çš„æƒ…å¢ƒä¸­çš„æ¨ç†è¯„ä¼°æ¨¡å‹å‘å±•ã€‚ä½¿ç”¨SOMADHANæ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4oã€GPT-3.5 Turboã€LLaMAç³»åˆ—æ¨¡å‹ç­‰ï¼Œé€šè¿‡é›¶é•œå¤´å’Œå°‘é•œå¤´æç¤ºï¼Œä»¥åŠæœ‰æ— æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿›è¡Œè¯„ä»·ã€‚æ€ç»´é“¾æç¤ºå§‹ç»ˆæé«˜äº†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„ä»»åŠ¡ä¸­ã€‚LLaMA-3.3 70Båœ¨å°‘é•œå¤´CoTæç¤ºä¸‹è¾¾åˆ°88%çš„æœ€é«˜å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¥æœ‰æ•ˆåœ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”å­ŸåŠ æ‹‰MWPsï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡æä¾›é«˜è´¨é‡æ¨ç†æ•°æ®é›†å’Œå¤æ‚MWPsçš„å¯æ‰©å±•æ¡†æ¶ï¼Œå¡«è¡¥äº†å­ŸåŠ æ‹‰NLPé¢†åŸŸçš„ä¸€ä¸ªå…³é”®ç©ºç™½ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨åŠ¨ä½èµ„æºè¯­è¨€çš„å…¬å¹³ç ”ç©¶ï¼Œæé«˜æ•™è‚²å’Œè¯­è¨€æŠ€æœ¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰è§£å†³æ˜¯NLPé¢†åŸŸçš„æŒ‘æˆ˜ï¼Œå› è¯­è¨€çš„ä½èµ„æºçŠ¶æ€å’Œå¤šæ­¥éª¤æ¨ç†éœ€æ±‚ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å­ŸåŠ æ‹‰MWPsæ—¶é‡åˆ°å›°éš¾ï¼Œå› ä¸ºæ²¡æœ‰ä¸“é—¨çš„æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥äº†SOMADHANæ•°æ®é›†ï¼ŒåŒ…å«8792é“å¤æ‚çš„å­ŸåŠ æ‹‰MWPsåŠå…¶é€æ­¥è§£ç­”ï¼Œä»¥æ”¯æŒæ¨ç†è¯„ä¼°æ¨¡å‹å‘å±•ã€‚</li>
<li>å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨SOMADHANæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬GPTç³»åˆ—å’ŒLLaMAç³»åˆ—æ¨¡å‹ã€‚</li>
<li>æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºåœ¨è§£å†³éœ€è¦å¤šæ­¥éª¤é€»è¾‘çš„é—®é¢˜æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>LLaMA-3.3 70Båœ¨å°‘é•œå¤´CoTæç¤ºä¸‹å–å¾—äº†æœ€é«˜å‡†ç¡®ç‡ã€‚</li>
<li>åº”ç”¨äº†ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•å¾®è°ƒæ¨¡å‹ï¼Œä»¥åœ¨ä¿æŒè¾ƒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶é€‚åº”å­ŸåŠ æ‹‰MWPsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21354">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f42ef39c2e2fd9b35ea29978bb4cacf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee0512f5ac535ccfc71236625e198cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7c8ae6685c99851bac1eb5263ac78be.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation"></a>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-&quot;U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality. </p>
<blockquote>
<p>è—è¯­æ˜¯ä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œå…¶ä¸‰å¤§æ–¹è¨€åŒºâ€”â€”ä¹Œå…¹è—ã€å®‰å¤šå’Œåº·åŒºçš„å¹³è¡Œè¯­éŸ³è¯­æ–™åº“æä¸ºæœ‰é™ï¼Œé™åˆ¶äº†è¯­éŸ³å»ºæ¨¡çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FMSD-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªå°‘æ ·æœ¬ã€å¤šå‘è¨€äººã€å¤šæ–¹è¨€çš„æ–‡æœ¬-è¯­éŸ³åˆæˆæ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»æœ‰é™çš„å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾ä¸­åˆæˆå¹³è¡Œçš„æ–¹è¨€è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ–°é¢–çš„å‘å£°äºº-æ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ä¸šåŒ–åŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒæ–¹è¨€ä¹‹é—´ç²¾ç»†çš„å£°å­¦å’Œè¯­è¨€å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™å‘å£°äººçš„èº«ä»½ã€‚å¤§é‡çš„å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨ç°åŠ›å’Œå‘å£°äººç›¸ä¼¼æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³çš„æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹è—è¯­å¤šæ–¹è¨€è¯­éŸ³åˆæˆçš„å°‘æ ·æœ¬TTSç³»ç»Ÿï¼Œï¼ˆ2ï¼‰å…¬å¼€å‘å¸ƒç”±FMSD-TTSç”Ÿæˆçš„å¤§è§„æ¨¡è—è¯­åˆæˆè¯­éŸ³è¯­æ–™åº“ï¼Œï¼ˆ3ï¼‰å¼€æ”¾æºä»£ç è¯„ä¼°å·¥å…·åŒ…ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°å‘å£°äººç›¸ä¼¼æ€§ã€æ–¹è¨€ä¸€è‡´æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14351v2">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è—è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€ï¼Œå¦‚ä½•åˆ©ç”¨æœ‰é™å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾ï¼Œé€šè¿‡ä¸€ç§åä¸ºFMSD-TTSçš„å°‘æ ·æœ¬å¤šè¯´è¯è€…å¤šæ–¹è¨€æ–‡æœ¬è½¬è¯­éŸ³æ¡†æ¶ï¼Œåˆæˆå¹¶è¡Œæ–¹è¨€è¯­éŸ³ã€‚è¯¥æ¡†æ¶å…·æœ‰è¯´è¯è€…æ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ç‰¹å®šåŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰æ–¹è¨€é—´çš„ç²¾ç»†å£°å­¦å·®å¼‚å’Œè¯­è¨€å­¦å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒè¯´è¯è€…èº«ä»½ã€‚ç»è¿‡å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œè¯´è¯è€…ç›¸ä¼¼æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†å…¶åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FMSD-TTSæ˜¯ä¸€ç§é’ˆå¯¹è—è¯­å¤šæ–¹è¨€è¯­éŸ³åˆæˆçš„å°‘æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡èåˆè¯´è¯è€…å’Œæ–¹è¨€æ¨¡å—ï¼Œä»¥åŠé‡‡ç”¨æ–¹è¨€ç‰¹å®šåŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰æ–¹è¨€é—´çš„å£°å­¦å·®å¼‚å’Œè¯­è¨€å­¦å·®å¼‚ã€‚</li>
<li>FMSD-TTSåœ¨å®¢è§‚å’Œä¸»è§‚è¯„ä¼°ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†æ–¹è¨€è¡¨è¾¾åŠ›å’Œè¯´è¯è€…ç›¸ä¼¼æ€§ã€‚</li>
<li>é€šè¿‡æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†å…¶åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚</li>
<li>FMSD-TTSå…¬å¼€å‘å¸ƒäº†ä¸€ä¸ªç”±è¯¥æ¡†æ¶ç”Ÿæˆçš„å¤§è§„æ¨¡è—è¯­åˆæˆè¯­éŸ³è¯­æ–™åº“ã€‚</li>
<li>ç ”ç©¶è¿˜è´¡çŒ®äº†ä¸€ä¸ªå¼€æºè¯„ä¼°å·¥å…·åŒ…ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°è¯´è¯è€…ç›¸ä¼¼æ€§ã€æ–¹è¨€ä¸€è‡´æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºè—è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€çš„è¯­éŸ³å»ºæ¨¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆå’Œæ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7888cb105d27db2b93f79b4c300e04dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-998248bb074f7ffbbd5f513847beb602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c69a1a206da3abcfa0b83c47e41a7bfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f13991fe356ad14a63fe258a28770e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f865a8154f8484ffe94e704f6a5e803.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2b28cca5e2b8d5f438db483aed09b3f7.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="KAN-or-MLP-Point-Cloud-Shows-the-Way-Forward"><a href="#KAN-or-MLP-Point-Cloud-Shows-the-Way-Forward" class="headerlink" title="KAN or MLP? Point Cloud Shows the Way Forward"></a>KAN or MLP? Point Cloud Shows the Way Forward</h2><p><strong>Authors:Yan Shi, Qingdong He, Yijun Liu, Xiaoyu Liu, Jingyong Su</strong></p>
<p>Multi-Layer Perceptrons (MLPs) have become one of the fundamental architectural component in point cloud analysis due to its effective feature learning mechanism. However, when processing complex geometric structures in point clouds, MLPsâ€™ fixed activation functions struggle to efficiently capture local geometric features, while suffering from poor parameter efficiency and high model redundancy. In this paper, we propose PointKAN, which applies Kolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate their efficacy in hierarchical feature representation. First, we introduce a Geometric Affine Module (GAM) to transform local features, improving the modelâ€™s robustness to geometric variations. Next, in the Local Feature Processing (LFP), a parallel structure extracts both group-level features and global context, providing a rich representation of both fine details and overall structure. Finally, these features are combined and processed in the Global Feature Processing (GFP). By repeating these operations, the receptive field gradually expands, enabling the model to capture complete geometric information of the point cloud. To overcome the high parameter counts and computational inefficiency of standard KANs, we develop Efficient-KANs in the PointKAN-elite variant, which significantly reduces parameters while maintaining accuracy. Experimental results demonstrate that PointKAN outperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN, and ShapeNetPart, with particularly strong performance in Few-shot Learning task. Additionally, PointKAN achieves substantial reductions in parameter counts and computational complexity (FLOPs). This work highlights the potential of KANs-based architectures in 3D vision and opens new avenues for research in point cloud understanding. </p>
<blockquote>
<p>å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰ç”±äºå…¶æœ‰æ•ˆçš„ç‰¹å¾å­¦ä¹ æœºåˆ¶ï¼Œå·²æˆä¸ºç‚¹äº‘åˆ†æä¸­çš„åŸºæœ¬æ¶æ„ç»„ä»¶ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†ç‚¹äº‘ä¸­çš„å¤æ‚å‡ ä½•ç»“æ„æ—¶ï¼ŒMLPsçš„å›ºå®šæ¿€æ´»å‡½æ•°åœ¨æœ‰æ•ˆåœ°æ•è·å±€éƒ¨å‡ ä½•ç‰¹å¾æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼ŒåŒæ—¶è¿˜å­˜åœ¨å‚æ•°æ•ˆç‡ä½ä¸‹å’Œæ¨¡å‹å†—ä½™çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PointKANï¼Œå®ƒå°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰åº”ç”¨äºç‚¹äº‘åˆ†æä»»åŠ¡ï¼Œä»¥ç ”ç©¶å…¶åœ¨åˆ†å±‚ç‰¹å¾è¡¨ç¤ºä¸­çš„æœ‰æ•ˆæ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•ä»¿å°„æ¨¡å—ï¼ˆGAMï¼‰æ¥å˜æ¢å±€éƒ¨ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹å¯¹å‡ ä½•å˜åŒ–çš„é²æ£’æ€§ã€‚å…¶æ¬¡ï¼Œåœ¨å±€éƒ¨ç‰¹å¾å¤„ç†ï¼ˆLFPï¼‰ä¸­ï¼Œå¹¶è¡Œç»“æ„æå–äº†ç»„çº§ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œæä¾›äº†å¯¹ç²¾ç»†ç»†èŠ‚å’Œæ•´ä½“ç»“æ„çš„ä¸°å¯Œè¡¨ç¤ºã€‚æœ€åï¼Œè¿™äº›ç‰¹å¾åœ¨å…¨å±€ç‰¹å¾å¤„ç†ï¼ˆGFPï¼‰ä¸­è¿›è¡Œç»„åˆå’Œå¤„ç†ã€‚é€šè¿‡é‡å¤è¿™äº›æ“ä½œï¼Œæ„Ÿå—é‡é€æ¸æ‰©å¤§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·ç‚¹äº‘çš„å®Œæ•´å‡ ä½•ä¿¡æ¯ã€‚ä¸ºäº†å…‹æœæ ‡å‡†KANså‚æ•°å¤šã€è®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨PointKAN-eliteå˜ç§ä¸­å¼€å‘äº†Efficient-KANsï¼Œå®ƒåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘äº†å‚æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPointKANåœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºPointMLPï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼ŒPointKANå®ç°äº†å‚æ•°æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼ˆFLOPsï¼‰çš„å¤§å¹…å‡å°‘ã€‚è¿™é¡¹å·¥ä½œçªå‡ºäº†åŸºäºKANsçš„æ¶æ„åœ¨3Dè§†è§‰ä¸­çš„æ½œåŠ›ï¼Œä¸ºç‚¹äº‘ç†è§£ç ”ç©¶å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13593v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºPointKANæ¨¡å‹ï¼Œå°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰åº”ç”¨äºç‚¹äº‘åˆ†æä»»åŠ¡ï¼Œä»¥ç ”ç©¶å…¶åœ¨åˆ†å±‚ç‰¹å¾è¡¨ç¤ºä¸­çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¼•å…¥å‡ ä½•ä»¿å°„æ¨¡å—ï¼ˆGAMï¼‰å’Œå±€éƒ¨ç‰¹å¾å¤„ç†ï¼ˆLFPï¼‰æ¥æ”¹è¿›æ¨¡å‹å¯¹å‡ ä½•å˜åŒ–çš„ç¨³å¥æ€§å¹¶ä¸°å¯Œç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ç‚¹KANç²¾è‹±ç‰ˆä¸­çš„é«˜æ•ˆKANsï¼Œä»¥åœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘å‚æ•°ã€‚PointKANåœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºPointMLPï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PointKANæ¨¡å‹ç»“åˆäº†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANsï¼‰è¿›è¡Œç‚¹äº‘åˆ†æï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥å‡ ä½•ä»¿å°„æ¨¡å—ï¼ˆGAMï¼‰ä»¥æ”¹å–„æ¨¡å‹å¯¹å‡ ä½•å˜åŒ–çš„ç¨³å¥æ€§ã€‚</li>
<li>é€šè¿‡å±€éƒ¨ç‰¹å¾å¤„ç†ï¼ˆLFPï¼‰ç»“åˆå¹¶è¡Œç»“æ„æå–ç»„çº§ç‰¹å¾å’Œå…¨å±€ä¸Šä¸‹æ–‡ï¼Œå®ç°ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>å…¨çƒç‰¹å¾å¤„ç†ï¼ˆGFPï¼‰ç»“åˆå¹¶å¤„ç†è¿™äº›ç‰¹å¾ï¼Œé€šè¿‡é‡å¤æ“ä½œé€æ¸æ‰©å¤§æ„Ÿå—é‡ï¼Œæ•æ‰ç‚¹äº‘çš„å®Œæ•´å‡ ä½•ä¿¡æ¯ã€‚</li>
<li>PointKAN-eliteä¸­çš„Efficient-KANsæ˜¾è‘—å‡å°‘å‚æ•°å’Œè®¡ç®—å¤æ‚æ€§ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ModelNet40ã€ScanObjectNNå’ŒShapeNetPartç­‰åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒPointKANæ¨¡å‹è¡¨ç°ä¼˜äºPointMLPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13593">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce45d5db1fabc52bf89f9c1e26d86031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5f4d6deb2eae82ddd8c1e23714ce74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc35a0c67d1b9e3940ba8a47f1442636.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddb489054b707668a09f4cbdf80426ef.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="â€œWhose-Side-Are-You-On-â€-Estimating-Ideology-of-Political-and-News-Content-Using-Large-Language-Models-and-Few-shot-Demonstration-Selection"><a href="#â€œWhose-Side-Are-You-On-â€-Estimating-Ideology-of-Political-and-News-Content-Using-Large-Language-Models-and-Few-shot-Demonstration-Selection" class="headerlink" title="â€œWhose Side Are You On?â€ Estimating Ideology of Political and News   Content Using Large Language Models and Few-shot Demonstration Selection"></a>â€œWhose Side Are You On?â€ Estimating Ideology of Political and News   Content Using Large Language Models and Few-shot Demonstration Selection</h2><p><strong>Authors:Muhammad Haroon, Magdalena Wojcieszak, Anshuman Chhabra</strong></p>
<p>The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLMâ€™s classification. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹æç«¯åŒ–ã€ä¿¡æ¯èŒ§æˆ¿å’Œå†…å®¹åè§é—®é¢˜çš„æ‹…å¿§ã€‚ç°æœ‰çš„æ„è¯†å½¢æ€åˆ†ç±»æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦å¤§é‡äººå·¥å‚ä¸ã€æ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„æ„è¯†å½¢æ€ç¯å¢ƒã€‚æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºç¾å›½ä¸¤å…šæ”¿æ²»è°±ç³»çš„åœ¨çº¿å†…å®¹æ”¿æ²»æ„è¯†å½¢æ€åˆ†ç±»ä¸­çš„æ½œåŠ›ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰è¿›è¡Œæ„è¯†å½¢æ€åˆ†ç±»ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŒ…å«æ–°é—»æ–‡ç« å’ŒYouTubeè§†é¢‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œé‡‡ç”¨æ ‡ç­¾å¹³è¡¡çš„æ–¹å¼è¿›è¡Œæ¼”ç¤ºé€‰æ‹©ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å…ƒæ•°æ®ï¼ˆå¦‚å†…å®¹æ¥æºå’Œæè¿°ï¼‰å¯¹æ„è¯†å½¢æ€åˆ†ç±»çš„å½±å“ï¼Œå¹¶è®¨è®ºäº†å…¶å«ä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ”¿æ²»å’Œéæ”¿æ²»å†…å®¹æ¥æºå¯¹LLMåˆ†ç±»çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20797v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å…³äºæç«¯åŒ–ã€ä¿¡æ¯èŒ§æˆ¿å’Œå†…å®¹åå‘çš„æ‹…å¿§ã€‚ç°æœ‰åˆ†ç±»æ„è¯†å½¢æ€çš„æ–¹æ³•éœ€è¦å¤§é‡äººå·¥æ“ä½œã€æ ‡æ³¨å¤§é‡æ•°æ®é›†ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„æ„è¯†å½¢æ€ç¯å¢ƒã€‚æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å¯¹ç¾å›½ä¸¤å…šæ”¿æ²»å…‰è°±èƒŒæ™¯ä¸‹åœ¨çº¿å†…å®¹çš„æ”¿æ²»æ„è¯†å½¢æ€è¿›è¡Œåˆ†ç±»çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŒ…å«æ–°é—»æ–‡ç« å’ŒYouTubeè§†é¢‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ ‡ç­¾å¹³è¡¡æ–¹å¼ä¸‹çš„æ¼”ç¤ºé€‰æ‹©å®éªŒï¼Œå‘ç°æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å…ƒæ•°æ®ï¼ˆå¦‚å†…å®¹æ¥æºå’Œæè¿°ï¼‰å¯¹æ„è¯†å½¢æ€åˆ†ç±»çš„å½±å“ï¼Œå¹¶è®¨è®ºäº†å…¶æ„ä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æä¾›æ”¿æ²»å’Œéæ”¿æ²»å†…å®¹çš„æ¥æºå¦‚ä½•å½±å“LLMçš„åˆ†ç±»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å…³äºæç«¯åŒ–ã€ä¿¡æ¯èŒ§æˆ¿å’Œå†…å®¹åå‘çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰åˆ†ç±»æ„è¯†å½¢æ€çš„æ–¹æ³•éœ€è¦å¤§é‡èµ„æºå’Œæ— æ³•é€‚åº”å˜åŒ–çš„ç¯å¢ƒã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åœ¨åˆ†ç±»æ”¿æ²»æ„è¯†å½¢æ€æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œç›¸è¾ƒäºé›¶æ ·æœ¬å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•ï¼ŒLLMçš„è¡¨ç°æ›´ä½³ã€‚</li>
<li>å…ƒæ•°æ®å¯¹æ„è¯†å½¢æ€åˆ†ç±»æœ‰å½±å“ã€‚</li>
<li>å†…å®¹æ¥æºå¯¹LLMåˆ†ç±»ç»“æœæœ‰å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ad7b42005811073c36ab2c014abd37c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0149d950ac5406cc0f1de01676fe70c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49c2dadbfefc2f90c902150a79d01626.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d22025643cfa703f7ff4214a0666910.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="EEG-CLIP-Learning-EEG-representations-from-natural-language-descriptions"><a href="#EEG-CLIP-Learning-EEG-representations-from-natural-language-descriptions" class="headerlink" title="EEG-CLIP : Learning EEG representations from natural language   descriptions"></a>EEG-CLIP : Learning EEG representations from natural language   descriptions</h2><p><strong>Authors:Tidiane Camaret Ndir, Robin Tibor Schirrmeister, Tonio Ball</strong></p>
<p>Deep networks for electroencephalogram (EEG) decoding are often only trained to solve one specific task, such as pathology or age decoding. A more general task-agnostic approach is to train deep networks to match a (clinical) EEG recording to its corresponding textual medical report and vice versa. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework, EEG-CLIP, that aligns the EEG time series and the descriptions of the corresponding clinical text in a shared embedding space. We investigated its potential for versatile EEG decoding, evaluating performance in a range of few-shot and zero-shot settings. Overall, we show that EEG-CLIP manages to non-trivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero-shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at <a target="_blank" rel="noopener" href="https://github.com/tidiane-camaret/EEGClip">https://github.com/tidiane-camaret/EEGClip</a> </p>
<blockquote>
<p>æ·±åº¦ç½‘ç»œé€šå¸¸åªç”¨äºè§£å†³è„‘ç”µå›¾ï¼ˆEEGï¼‰è§£ç çš„å•ä¸€ç‰¹å®šä»»åŠ¡ï¼Œå¦‚ç—…ç†å­¦æˆ–å¹´é¾„è§£ç ã€‚ä¸€ç§æ›´é€šç”¨çš„ä»»åŠ¡æ— å…³æ–¹æ³•æ˜¯é€šè¿‡è®­ç»ƒæ·±åº¦ç½‘ç»œæ¥åŒ¹é…ï¼ˆä¸´åºŠï¼‰è„‘ç”µå›¾è®°å½•ä¸å…¶ç›¸åº”çš„æ–‡æœ¬åŒ»å­¦æŠ¥å‘Šï¼Œåä¹‹äº¦ç„¶ã€‚è¿™ç§æ–¹æ³•æœ€åˆåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸç”¨äºåŒ¹é…å›¾åƒå’Œæ–‡æœ¬æ ‡é¢˜ï¼Œéšåé€šè¿‡ä½¿ç”¨æ–‡æœ¬ç±»åˆ«æç¤ºå®ç°äº†é›¶æ ·æœ¬è§£ç çš„æˆåŠŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬éµå¾ªè¿™ç§æ–¹æ³•ï¼Œå¼€å‘äº†ä¸€ä¸ªå¯¹æ¯”å­¦ä¹ æ¡†æ¶EEG-CLIPï¼Œå®ƒå°†è„‘ç”µå›¾æ—¶é—´åºåˆ—ä¸å¯¹åº”ä¸´åºŠæ–‡æœ¬çš„è¯´æ˜å¯¹é½åˆ°ä¸€ä¸ªå…±äº«åµŒå…¥ç©ºé—´ä¸­ã€‚æˆ‘ä»¬å¯¹å…¶åœ¨å¤šç§å°æ ·æœ¬æ¬¡æ•°å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹çš„é€šç”¨è„‘ç”µå›¾è§£ç æ½œåŠ›è¿›è¡Œäº†è°ƒæŸ¥ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†EEG-CLIPèƒ½å¤Ÿå®ç°å¯¹æ–‡æœ¬å’Œè„‘ç”µå›¾è¡¨ç¤ºçš„éå¹³å‡¡å¯¹é½ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ç§å­¦ä¹ é€šç”¨è„‘ç”µå›¾è¡¨ç¤ºçš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œè¿™å¯èƒ½é€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–ä½¿ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è®­ç»ƒç‰¹å®šä»»åŠ¡æ¨¡å‹æ¥æ›´å®¹æ˜“åœ°åˆ†æå„ç§è§£ç é—®é¢˜ã€‚é‡ç°æˆ‘ä»¬ç»“æœçš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/tidiane-camaret/EEGClip">https://github.com/tidiane-camaret/EEGClip</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16531v2">PDF</a> </p>
<p><strong>Summary</strong><br>EEG-CLIPæ˜¯ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ¡†æ¶ï¼Œå®ƒå°†è„‘ç”µå›¾ï¼ˆEEGï¼‰è®°å½•å’Œç›¸åº”çš„åŒ»å­¦æŠ¥å‘Šæ–‡æœ¬åŒ¹é…èµ·æ¥ï¼Œå¹¶å¼€å‘äº†ä¸€ç§é€šç”¨çš„è„‘ç”µå›¾è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§å°æ ·æœ¬å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°é›¶æ ·æœ¬è§£ç æˆ–åˆ©ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è¿›è¡Œä»»åŠ¡ç‰¹å®šæ¨¡å‹çš„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶é‡‡ç”¨ä¸€ç§é€šç”¨çš„ä»»åŠ¡æ— å…³æ–¹æ³•ï¼Œé€šè¿‡åŒ¹é…è„‘ç”µå›¾ï¼ˆEEGï¼‰è®°å½•å’Œç›¸åº”çš„åŒ»å­¦æŠ¥å‘Šæ–‡æœ¬ï¼Œè®­ç»ƒæ·±åº¦ç½‘ç»œã€‚</li>
<li>EEG-CLIPæ¡†æ¶é‡‡ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå°†è„‘ç”µå›¾æ—¶é—´åºåˆ—ä¸ç›¸åº”çš„ä¸´åºŠæ–‡æœ¬æè¿°å¯¹é½åœ¨å…±äº«åµŒå…¥ç©ºé—´ä¸­ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§å°æ ·æœ¬æ¬¡æ•°å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>EEG-CLIPæˆåŠŸå®ç°äº†æ–‡æœ¬å’Œè„‘ç”µå›¾è¡¨ç¤ºçš„éå¹³å‡¡å¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æœ›ç®€åŒ–å¯¹å¤šç§è§£ç é—®é¢˜çš„åˆ†æï¼Œé€šè¿‡é›¶æ ·æœ¬è§£ç æˆ–ä½¿ç”¨æ›´å°‘è®­ç»ƒæ ·æœ¬è¿›è¡Œä»»åŠ¡ç‰¹å®šæ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›ä»–äººå¤ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa644fcfa34eed2b0b16c76e27f8d8bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576a1016958861ef530e146394d78e19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be7f6f1506263a5657bc1eafcdff857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2348d77a854c27f128a0c809f5603e85.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GneissWeb-Preparing-High-Quality-Data-for-LLMs-at-Scale"><a href="#GneissWeb-Preparing-High-Quality-Data-for-LLMs-at-Scale" class="headerlink" title="GneissWeb: Preparing High Quality Data for LLMs at Scale"></a>GneissWeb: Preparing High Quality Data for LLMs at Scale</h2><p><strong>Authors:Hajar Emami Gohari, Swanand Ravindra Kadhe, Syed Yousaf Shah, Constantin Adam, Abdulhamid Adebayo, Praneet Adusumilli, Farhan Ahmed, Nathalie Baracaldo Angel, Santosh Subhashrao Borse, Yuan-Chi Chang, Xuan-Hong Dang, Nirmit Desai, Revital Eres, Ran Iwamoto, Alexei Karve, Yan Koyfman, Wei-Han Lee, Changchang Liu, Boris Lublinsky, Takuyo Ohko, Pablo Pesce, Maroun Touma, Shiqiang Wang, Shalisha Witherspoon, Herbert WoisetschlÃ¤ger, David Wood, Kun-Lung Wu, Issei Yoshida, Syed Zawad, Petros Zerfos, Yi Zhou, Bishwaranjan Bhattacharjee</strong></p>
<p>Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLMâ€™s ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.   In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens).   We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0. </p>
<blockquote>
<p>æ•°æ®æ•°é‡å’Œå“è´¨åœ¨å†³å®šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ä¸Šæ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚é«˜å“è´¨çš„æ•°æ®ï¼Œå°¤å…¶èƒ½æ˜¾è‘—æå‡LLMåœ¨å¹¿æ³›ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ‰€ä½¿ç”¨çš„å¤§å‹é¢„è®­ç»ƒæ•°æ®é›†ä»ç„¶ä¸å¯¹å…¬ä¼—å¼€æ”¾ï¼Œè€Œè®¸å¤šå…¬å¼€æ•°æ®é›†è§„æ¨¡è¾ƒå°ï¼ˆå°‘äº5ä¸‡äº¿æ ‡è®°ï¼‰ï¼Œé™åˆ¶äº†å®ƒä»¬è®­ç»ƒå¤§å‹æ¨¡å‹çš„é€‚ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GneissWebï¼Œè¿™æ˜¯ä¸€ä¸ªäº§ç”Ÿå¤§çº¦10ä¸‡äº¿æ ‡è®°çš„å¤§å‹æ•°æ®é›†ï¼Œæ»¡è¶³äº†è®­ç»ƒLLMå¯¹æ•°æ®è´¨é‡å’Œæ•°é‡çš„è¦æ±‚ã€‚æˆ‘ä»¬äº§ç”ŸGneissWebæ•°æ®é›†çš„æ–¹æ¡ˆåŒ…æ‹¬åˆ†ç‰‡ç²¾ç¡®çš„å­å­—ç¬¦ä¸²å»é‡å’Œç²¾å¿ƒæ„å»ºçš„è´¨é‡è¿‡æ»¤å™¨ç»„åˆã€‚GneissWebåœ¨æ•°æ®è´¨é‡å’Œæ•°é‡ä¹‹é—´å®ç°äº†æœ‰åˆ©çš„æƒè¡¡ï¼Œäº§ç”Ÿçš„æ¨¡å‹åœ¨æœ€æ–°å…¬å¼€å¤§å‹æ•°æ®é›†ï¼ˆè¶…è¿‡5ä¸‡äº¿æ ‡è®°ï¼‰è®­ç»ƒçš„æ¨¡å‹ä¹‹ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨GneissWebæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨ç”±å¸¸ç”¨çš„é¢„è®­ç»ƒæ•°æ®é›†è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ä¸­å¹³å‡å¾—åˆ†ä¸Šæ¯”ä½¿ç”¨FineWeb-V1.1.0è®­ç»ƒçš„æ¨¡å‹é«˜å‡º2.73ä¸ªç™¾åˆ†ç‚¹ã€‚å½“è¯„ä¼°é›†æ‰©å±•åˆ°åŒ…å«20ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆé›¶æ ·æœ¬å’Œå°‘é‡æ ·æœ¬ï¼‰æ—¶ï¼Œä½¿ç”¨GneissWebè®­ç»ƒçš„æ¨¡å‹ä»æ¯”ä½¿ç”¨FineWeb-V1.1.0è®­ç»ƒçš„æ¨¡å‹å…·æœ‰é«˜è¾¾é«˜è¾¾è¿‘ æ›´å¤§çš„ä¼˜åŠ¿ï¼ˆä¼˜åŠ¿è¾¾é«˜è¾¾è¿‘é«˜å‡º è¶…å‡ºæ ‡å‡†ä»¥ä¸Šæé«˜å°†è¿‘ç™¾åˆ†ä¹‹å‡ ç‚¹ï¼‰ã€‚ç®€è€Œè¨€ä¹‹ï¼Œé€šè¿‡æˆ‘ä»¬åˆ›æ–°çš„æ–¹æ³•è®ºåˆ›å»ºçš„æ•°æ®é›†å®ç°äº†ä¼˜ç§€çš„æ€§èƒ½å’Œå‰æ²¿æŠ€æœ¯ä¹‹é—´çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14907v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ•°æ®è´¨é‡ä¸æ•°é‡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½ä¸Šçš„é‡è¦ä½œç”¨ã€‚æ–‡ç« æå‡ºGneissWebæ•°æ®é›†ï¼ŒåŒ…å«çº¦10ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œè§£å†³äº†è®­ç»ƒLLMæ‰€éœ€çš„æ•°æ®è´¨é‡å’Œæ•°é‡è¦æ±‚é—®é¢˜ã€‚GneissWebçš„é…æ–¹åŒ…æ‹¬åˆ†ç‰‡ç²¾ç¡®çš„å­å­—ç¬¦ä¸²é‡å¤æ¶ˆé™¤å’Œç²¾å¿ƒæ„å»ºçš„è´¨é‡è¿‡æ»¤å™¨é›†åˆï¼Œå¯å®ç°æ•°æ®è´¨é‡ä¸æ•°é‡ä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨GneissWebæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä½¿ç”¨ç°æœ‰å¤§å‹å¼€æ”¾æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®è´¨é‡å’Œæ•°é‡åœ¨LLMæ€§èƒ½ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>GneissWebæ•°æ®é›†å«æœ‰çº¦10ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œæ»¡è¶³LLMè®­ç»ƒçš„æ•°æ®éœ€æ±‚ã€‚</li>
<li>GneissWebé‡‡ç”¨åˆ†ç‰‡ç²¾ç¡®çš„å­å­—ç¬¦ä¸²é‡å¤æ¶ˆé™¤å’Œç²¾å¿ƒæ„å»ºçš„è´¨é‡è¿‡æ»¤å™¨ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€‚</li>
<li>GneissWebæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºä½¿ç”¨å…¶ä»–å¤§å‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>GneissWebè®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿åœ¨æ‰©å±•è‡³æ›´å¤šåŸºå‡†æµ‹è¯•æ—¶ä¾ç„¶æ˜¾è‘—ã€‚</li>
<li>GneissWebæœ‰åŠ©äºæé«˜LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21834de35f256778ef6c1a481dbe47d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc9b186161fe681ba261d4a1b5849430.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-448edd5039ed765dedd5da485fd63cf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-164546a1b7f8660be41fe12f0ed99885.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Verbalized-Representation-Learning-for-Interpretable-Few-Shot-Generalization"><a href="#Verbalized-Representation-Learning-for-Interpretable-Few-Shot-Generalization" class="headerlink" title="Verbalized Representation Learning for Interpretable Few-Shot   Generalization"></a>Verbalized Representation Learning for Interpretable Few-Shot   Generalization</h2><p><strong>Authors:Cheng-Fu Yang, Da Yin, Wenbo Hu, Nanyun Peng, Bolei Zhou, Kai-Wei Chang</strong></p>
<p>Humans recognize objects after observing only a few examples, a remarkable capability enabled by their inherent language understanding of the real-world environment. Developing verbalized and interpretable representation can significantly improve model generalization in low-data settings. In this work, we propose Verbalized Representation Learning (VRL), a novel approach for automatically extracting human-interpretable features for object recognition using few-shot data. Our method uniquely captures inter-class differences and intra-class commonalities in the form of natural language by employing a Vision-Language Model (VLM) to identify key discriminative features between different classes and shared characteristics within the same class. These verbalized features are then mapped to numeric vectors through the VLM. The resulting feature vectors can be further utilized to train and infer with downstream classifiers. Experimental results show that, at the same model scale, VRL achieves a 24% absolute improvement over prior state-of-the-art methods while using 95% less data and a smaller mode. Furthermore, compared to human-labeled attributes, the features learned by VRL exhibit a 20% absolute gain when used for downstream classification tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/joeyy5588/VRL/tree/main">https://github.com/joeyy5588/VRL/tree/main</a>. </p>
<blockquote>
<p>äººç±»åªéœ€è§‚å¯Ÿå‡ ä¸ªä¾‹å­å°±èƒ½è¯†åˆ«ç‰©ä½“ï¼Œè¿™æ˜¯ç”±ä»–ä»¬å¯¹ç°å®ç¯å¢ƒå›ºæœ‰ç†è§£æ‰€èµ‹äºˆçš„æƒŠäººèƒ½åŠ›ã€‚åœ¨æ•°æ®ç¨€ç¼ºçš„ç¯å¢ƒä¸­ï¼Œå¼€å‘è¯­è¨€åŒ–çš„å¯è§£é‡Šè¡¨ç¤ºå¯ä»¥æå¤§åœ°æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€åŒ–è¡¨ç¤ºå­¦ä¹ ï¼ˆVRLï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥é€šè¿‡å°‘é‡æ•°æ®è‡ªåŠ¨æå–äººç±»å¯è§£é‡Šçš„ç‰¹å¾æ¥è¿›è¡Œå¯¹è±¡è¯†åˆ«ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼ç‹¬ç‰¹åœ°æ•æ‰äº†ç±»é—´å·®å¼‚å’Œç±»å†…å…±æ€§ï¼Œé€šè¿‡é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è¯†åˆ«ä¸åŒç±»ä¹‹é—´çš„å…³é”®åˆ¤åˆ«ç‰¹å¾ä»¥åŠåŒä¸€ç±»å†…çš„å…±äº«ç‰¹å¾ã€‚è¿™äº›è¯­è¨€åŒ–çš„ç‰¹å¾ç„¶åé€šè¿‡VLMæ˜ å°„åˆ°æ•°å€¼å‘é‡ä¸Šã€‚æ‰€å¾—çš„ç‰¹å¾å‘é‡å¯è¿›ä¸€æ­¥ç”¨äºè®­ç»ƒå¹¶ç”¨äºä¸‹æ¸¸åˆ†ç±»å™¨çš„æ¨æ–­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒVRLåœ¨ä»…ä½¿ç”¨95%æ›´å°‘çš„æ•°æ®å’Œè¾ƒå°æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•å®ç°äº†24%çš„ç»å¯¹æ”¹è¿›ã€‚æ­¤å¤–ï¼Œä¸äººå·¥æ ‡æ³¨çš„å±æ€§ç›¸æ¯”ï¼Œä½¿ç”¨VRLå­¦ä¹ çš„ç‰¹å¾è¿›è¡Œä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†20%çš„ç»å¯¹ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/joeyy5588/VRL/tree/main">https://github.com/joeyy5588/VRL/tree/main</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18651v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºVerbalized Representation Learningï¼ˆVRLï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å°‘é‡æ•°æ®è‡ªåŠ¨æå–äººç±»å¯è§£é‡Šçš„ç‰¹å¾è¿›è¡Œå¯¹è±¡è¯†åˆ«ã€‚é€šè¿‡é‡‡ç”¨Vision-Language Modelï¼ˆVLMï¼‰ï¼ŒVRLèƒ½å¤Ÿè¯†åˆ«ä¸åŒç±»åˆ«ä¹‹é—´çš„å…³é”®åˆ¤åˆ«ç‰¹å¾ä»¥åŠåŒä¸€ç±»åˆ«å†…çš„å…±äº«ç‰¹å¾ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€å½¢å¼ã€‚è¿™äº›è¯­è¨€åŒ–çš„ç‰¹å¾è¿›ä¸€æ­¥è¢«æ˜ å°„ä¸ºæ•°å€¼å‘é‡ï¼Œå¯ç”¨äºè®­ç»ƒå’Œæ¨æ–­ä¸‹æ¸¸åˆ†ç±»å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒVRLç›¸è¾ƒäºç°æœ‰å…ˆè¿›æŠ€æœ¯å®ç°äº†24%çš„ç»å¯¹æ”¹è¿›ï¼Œå¹¶åœ¨ä½¿ç”¨95%æ›´å°‘æ•°æ®å’Œå°å‹æ¨¡å‹çš„æƒ…å†µä¸‹å–å¾—äº†æ˜¾è‘—æˆæœã€‚æ­¤å¤–ï¼Œä¸äººå·¥æ ‡æ³¨çš„å±æ€§ç›¸æ¯”ï¼Œé€šè¿‡VRLå­¦ä¹ çš„ç‰¹å¾ç”¨äºä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†20%çš„ç»å¯¹ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»ä»…éœ€è§‚å¯Ÿå°‘é‡ç¤ºä¾‹å³å¯è¯†åˆ«ç‰©ä½“ï¼Œè¿™æ˜¯ç”±ä»–ä»¬å†…åœ¨çš„å¯¹ç°å®ä¸–ç•Œç¯å¢ƒçš„ç†è§£æ‰€é©±åŠ¨çš„ã€‚</li>
<li>è¨€è¯­åŒ–è¡¨ç¤ºå­¦ä¹ ï¼ˆVRLï¼‰æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å°‘é‡æ•°æ®ä¸­è‡ªåŠ¨æå–äººç±»å¯è§£é‡Šçš„ç‰¹å¾è¿›è¡Œå¯¹è±¡è¯†åˆ«ã€‚</li>
<li>VRLé‡‡ç”¨Vision-Language Modelï¼ˆVLMï¼‰è¯†åˆ«ä¸åŒç±»åˆ«é—´çš„å…³é”®åˆ¤åˆ«ç‰¹å¾å’ŒåŒä¸€ç±»åˆ«å†…çš„å…±äº«ç‰¹å¾ã€‚</li>
<li>VRLå°†è¯­è¨€åŒ–çš„ç‰¹å¾æ˜ å°„ä¸ºæ•°å€¼å‘é‡ï¼Œè¿™äº›å‘é‡å¯ç”¨äºè®­ç»ƒå’Œæ¨æ–­ä¸‹æ¸¸åˆ†ç±»å™¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒVRLç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>VRLåœ¨å‡å°‘æ•°æ®éœ€æ±‚å’Œä½¿ç”¨å°å‹æ¨¡å‹æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd595607a296a813885a111319f7e317.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a63c44975d3d4faa55d2ab02950c7f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a4759ad61111ef5c4138271594c835.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation"><a href="#Can-LLMs-assist-with-Ambiguity-A-Quantitative-Evaluation-of-various-Large-Language-Models-on-Word-Sense-Disambiguation" class="headerlink" title="Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation"></a>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various   Large Language Models on Word Sense Disambiguation</h2><p><strong>Authors:T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough</strong></p>
<p>Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication. </p>
<blockquote>
<p>åœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­ç»å¸¸å¯ä»¥å‘ç°è¯ä¹‰æ¨¡ç³Šçš„è¯è¯­ã€‚ç”±äºæ•°æ®æœ‰é™ï¼Œè¯æ±‡çš„æ¨¡ç³Šæ€§ç»™ä¼ ç»Ÿçš„è¯ä¹‰æ¶ˆæ­§ï¼ˆWord Sense Disambiguation, WSDï¼‰æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œç¿»è¯‘ã€ä¿¡æ¯æ£€ç´¢å’Œé—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å—åˆ°äº†è¿™äº›é™åˆ¶çš„é˜»ç¢ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆä¸€ç§ç³»ç»Ÿçš„æç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒè¯ä¹‰è§£é‡Šçš„çŸ¥è¯†åº“ï¼ˆKBï¼‰æ¥æ”¹å–„è¯ä¹‰æ¶ˆæ­§çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§äººç±»å¾ªç¯æç¤ºå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”±è¯æ€§æ ‡æ³¨ã€æ¨¡ç³Šè¯çš„åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„è¯ä¹‰è¿‡æ»¤å’Œå°‘é‡æç¤ºç»„æˆï¼Œä»¥æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäºå°‘é‡æç¤ºé“¾æ€ç»´ï¼ˆCOTï¼‰çš„æç¤ºæ–¹æ³•ï¼Œè¿™é¡¹å·¥ä½œåœ¨æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚è¯„ä¼°å·¥ä½œæ˜¯é€šè¿‡ä½¿ç”¨FEWSæµ‹è¯•æ•°æ®å’Œè¯ä¹‰æ ‡ç­¾è¿›è¡Œçš„ã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†ç¤¾äº¤åª’ä½“å’Œæ•°å­—é€šä¿¡ä¸­çš„å‡†ç¡®è¯ä¹‰è§£è¯»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18337v4">PDF</a> 12 pages,6 tables, 1 figure, Proceedings of the 1st International   Conference on NLP &amp; AI for Cyber Security</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç°ä»£æ•°å­—é€šä¿¡ä¸­å¸¸è§çš„è¯æ±‡æ­§ä¹‰é—®é¢˜ã€‚é’ˆå¯¹ä¼ ç»Ÿè¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰æ–¹æ³•å› æ•°æ®æœ‰é™è€Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œç ”ç©¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ”¹è¿›ã€‚é€šè¿‡ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒè¯ä¹‰è§£é‡Šçš„ knowledge baseï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨äººå·¥å‚ä¸å¾ªç¯çš„æç¤ºå¢å¼ºæ–¹å¼ï¼Œå€ŸåŠ©è¯æ€§æ ‡æ³¨ã€æ¨¡ç³Šè¯åŒä¹‰è¯ã€åŸºäºæ–¹é¢çš„è¯ä¹‰è¿‡æ»¤å’Œå°‘é‡æç¤ºæ¥å¼•å¯¼LLMã€‚é€šè¿‡åŸºäºå°‘é‡æç¤ºçš„Chain of Thoughtï¼ˆCOTï¼‰æ–¹æ³•ï¼Œè¯¥ç ”ç©¶å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨FEWSæµ‹è¯•æ•°æ®å’Œè¯ä¹‰æ ‡ç­¾ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä¸ºç¤¾äº¤åª’ä½“å’Œæ•°å­—é€šä¿¡ä¸­çš„å‡†ç¡®è¯æ±‡è§£é‡Šæä¾›äº†è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯æ±‡æ­§ä¹‰åœ¨ç°ä»£æ•°å­—é€šä¿¡ä¸­æ˜¯å¸¸è§é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿè¯ä¹‰æ¶ˆæ­§æ–¹æ³•å› æ•°æ®æœ‰é™è€Œé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è¢«ç”¨äºæ”¹è¿›è¯ä¹‰æ¶ˆæ­§ã€‚</li>
<li>ç»“åˆç³»ç»Ÿæç¤ºå¢å¼ºæœºåˆ¶å’ŒåŒ…å«ä¸åŒè¯ä¹‰è§£é‡Šçš„ knowledge base æå‡ºæ–°æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨äººå·¥å‚ä¸å¾ªç¯çš„æç¤ºå¢å¼ºæ–¹å¼ï¼ŒåŒ…æ‹¬è¯æ€§æ ‡æ³¨ã€æ¨¡ç³Šè¯åŒä¹‰è¯ç­‰ã€‚</li>
<li>ä½¿ç”¨åŸºäºå°‘é‡æç¤ºçš„Chain of Thoughtæ–¹æ³•å®ç°æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad3886a2ed9dcbd7c66b7dcd1a190cc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-770565f58942563b94aaa03ad6dd5e2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b2d1dd25c8c65295f0bcf388c3611d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2076c8f95119ae476ffbefe14cd878a4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Label-Anything-Multi-Class-Few-Shot-Semantic-Segmentation-with-Visual-Prompts"><a href="#Label-Anything-Multi-Class-Few-Shot-Semantic-Segmentation-with-Visual-Prompts" class="headerlink" title="Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual   Prompts"></a>Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual   Prompts</h2><p><strong>Authors:Pasquale De Marinis, Nicola Fanelli, Raffaele Scaringi, Emanuele Colonna, Giuseppe Fiameni, Gennaro Vessio, Giovanna Castellano</strong></p>
<p>Few-shot semantic segmentation aims to segment objects from previously unseen classes using only a limited number of labeled examples. In this paper, we introduce Label Anything, a novel transformer-based architecture designed for multi-prompt, multi-way few-shot semantic segmentation. Our approach leverages diverse visual prompts â€“ points, bounding boxes, and masks â€“ to create a highly flexible and generalizable framework that significantly reduces annotation burden while maintaining high accuracy. Label Anything makes three key contributions: ($\textit{i}$) we introduce a new task formulation that relaxes conventional few-shot segmentation constraints by supporting various types of prompts, multi-class classification, and enabling multiple prompts within a single image; ($\textit{ii}$) we propose a novel architecture based on transformers and attention mechanisms; and ($\textit{iii}$) we design a versatile training procedure allowing our model to operate seamlessly across different $N$-way $K$-shot and prompt-type configurations with a single trained model. Our extensive experimental evaluation on the widely used COCO-$20^i$ benchmark demonstrates that Label Anything achieves state-of-the-art performance among existing multi-way few-shot segmentation methods, while significantly outperforming leading single-class models when evaluated in multi-class settings. Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/pasqualedem/LabelAnything">https://github.com/pasqualedem/LabelAnything</a>. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ä½¿ç”¨æœ‰é™çš„æ ‡æ³¨æ ·æœ¬å¯¹ä¹‹å‰æœªè§è¿‡çš„ç±»åˆ«è¿›è¡Œå¯¹è±¡åˆ†å‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Label Anythingï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„æ–°å‹æ¶æ„ï¼Œä¸“ä¸ºå¤šæç¤ºã€å¤šç±»åˆ«å°‘é‡æ ·æœ¬è¯­ä¹‰åˆ†å‰²è®¾è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šæ ·åŒ–çš„è§†è§‰æç¤ºâ€”â€”ç‚¹ã€è¾¹ç•Œæ¡†å’Œè’™ç‰ˆï¼Œåˆ›å»ºä¸€ä¸ªé«˜åº¦çµæ´»å’Œå¯æ¨å¹¿çš„æ¡†æ¶ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚Label Anythingæœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šï¼ˆiï¼‰æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä»»åŠ¡å½¢å¼åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ”¯æŒå„ç§æç¤ºç±»å‹ã€å¤šç±»åˆ†ç±»ï¼Œå¹¶åœ¨å•ä¸ªå›¾åƒå†…æ”¯æŒå¤šä¸ªæç¤ºï¼Œæ”¾å®½äº†ä¼ ç»Ÿçš„å°‘é‡æ ·æœ¬åˆ†å‰²çº¦æŸï¼›ï¼ˆiiï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerå’Œæ³¨æ„åŠ›æœºåˆ¶çš„å…¨æ–°æ¶æ„ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§é€šç”¨è®­ç»ƒç¨‹åºï¼Œä½¿æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„Nè·¯Kæ ·æœ¬å’Œæç¤ºç±»å‹é…ç½®ä¸­ä½¿ç”¨å•ä¸ªè®­ç»ƒæ¨¡å‹æ— ç¼è¿è¡Œã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„COCO-20iåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLabel Anythingåœ¨ç°æœ‰çš„å¤šç±»åˆ«å°‘é‡æ ·æœ¬åˆ†å‰²æ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¤šç±»åˆ«è®¾ç½®ä¸­æ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å•ç±»åˆ«æ¨¡å‹ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/pasqualedem/LabelAnything%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/pasqualedem/LabelAnythingè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02075v3">PDF</a> ECAI 2025 - 28th European Conference on Artificial Intelligence</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šæç¤ºã€å¤šç±»åˆ«å°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„æ–°å‹è½¬æ¢å™¨æ¶æ„Label Anythingã€‚å®ƒåˆ©ç”¨å¤šæ ·åŒ–çš„è§†è§‰æç¤ºï¼ˆç‚¹ã€è¾¹ç•Œæ¡†å’Œæ©è†œï¼‰ï¼Œåˆ›å»ºäº†ä¸€ä¸ªçµæ´»ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ ‡æ³¨çš„è´Ÿæ‹…ã€‚Label Anythingçš„è´¡çŒ®åŒ…æ‹¬ï¼šå¼•å…¥æ”¯æŒå¤šç§æç¤ºã€å¤šç±»åˆ«åˆ†ç±»å’Œå•å›¾åƒå†…å¤šä¸ªæç¤ºçš„æ–°ä»»åŠ¡å½¢å¼ï¼›æå‡ºåŸºäºè½¬æ¢å™¨å’Œæ³¨æ„åŠ›æœºåˆ¶çš„å…¨æ–°æ¶æ„ï¼›è®¾è®¡äº†ä¸€ç§é€šç”¨è®­ç»ƒç¨‹åºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„Nè·¯Kæ ·æœ¬å’Œæç¤ºç±»å‹é…ç½®ä¸­æ— ç¼è¿è¡Œã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„COCO-20iåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLabel Anythingå®ç°äº†ç°æœ‰å¤šç±»åˆ«å°‘æ ·æœ¬åˆ†å‰²æ–¹æ³•ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Label Anythingæ˜¯ä¸€ä¸ªä¸ºå°‘æ ·æœ¬è¯­ä¹‰åˆ†å‰²ä»»åŠ¡è®¾è®¡çš„åŸºäºè½¬æ¢å™¨çš„æ–°å‹æ¶æ„ã€‚</li>
<li>å®ƒæ”¯æŒå¤šç§ç±»å‹çš„è§†è§‰æç¤ºï¼Œå¦‚ç‚¹ã€è¾¹ç•Œæ¡†å’Œæ©è†œã€‚</li>
<li>Label Anythingæ˜¾è‘—å‡å°‘äº†æ ‡æ³¨è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚</li>
<li>å®ƒå¼•å…¥äº†æ–°çš„ä»»åŠ¡å½¢å¼ï¼Œæ”¯æŒå¤šç§æç¤ºã€å¤šç±»åˆ«åˆ†ç±»å’Œå•å›¾åƒå†…çš„å¤šä¸ªæç¤ºã€‚</li>
<li>Label Anythingé€šè¿‡åŸºäºè½¬æ¢å™¨å’Œæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„å®ç°ã€‚</li>
<li>å®ƒè®¾è®¡äº†ä¸€ç§é€šç”¨è®­ç»ƒç¨‹åºï¼Œå…è®¸æ¨¡å‹åœ¨ä¸åŒé…ç½®ä¸­æ— ç¼è¿è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02075">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dcb33a86312a231d0372e7eeb4ec65a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb44e3cf1868f68e99b6e10dc46ec244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de0a205bdd88658ebc95bfca2df6d6fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-550edadb12b0ffc69c01fcd52099f0f5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Iterative-Repair-with-Weak-Verifiers-for-Few-shot-Transfer-in-KBQA-with-Unanswerability"><a href="#Iterative-Repair-with-Weak-Verifiers-for-Few-shot-Transfer-in-KBQA-with-Unanswerability" class="headerlink" title="Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with   Unanswerability"></a>Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with   Unanswerability</h2><p><strong>Authors:Riya Sawhney, Samrat Yadav, Indrajit Bhattacharya,  Mausam</strong></p>
<p>Real-world applications of KBQA require models to handle unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions and contribute two new datasets for performance evaluation. We present FUn-FuSIC - a novel solution for our task that extends FuSIC KBQA, the state-of-the-art few-shot transfer model for answerable-only KBQA. We first note that FuSIC-KBQAâ€™s iterative repair makes a strong assumption that all questions are unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which uses iterative repair using feedback from a suite of strong and weak verifiers, and an adaptation of self consistency for unanswerabilty to better assess the answerability of a question. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM based and supervised SoTA models on our task, while establishing a new SoTA for answerable few-shot transfer as well. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„KBQAåº”ç”¨è¦æ±‚æ¨¡å‹èƒ½å¤Ÿå¤„ç†æ— æ³•å›ç­”çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨é¢†åŸŸå†…éƒ¨åªæœ‰æœ‰é™é‡çš„æ ‡è®°è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬é’ˆå¯¹å¸¦æœ‰æ— æ³•å›ç­”é—®é¢˜çš„KBQAæå‡ºäº†æ–°é¢–çš„å°‘é‡è½¬ç§»ä»»åŠ¡ï¼Œå¹¶ä¸ºæ€§èƒ½è¯„ä¼°è´¡çŒ®äº†ä¸¤ä¸ªæ–°æ•°æ®é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†FUN-FuSICæ˜¯æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆçš„æ–°é¢–ä¹‹å¤„ï¼Œå®ƒæ‰©å±•äº†FuSIC KBQAæ¨¡å‹ï¼Œå³å½“å‰æœ€å…ˆè¿›çš„æœ‰ç­”æ¡ˆKBQAçš„å°‘é‡è½¬ç§»æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆæ³¨æ„åˆ°ï¼ŒFuSIC-KBQAçš„è¿­ä»£ä¿®å¤å‡è®¾æ‰€æœ‰é—®é¢˜éƒ½æ˜¯æ— æ³•å›ç­”çš„ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä¸å¯å›ç­”æ€§çš„åé¦ˆï¼ˆFUNï¼‰ï¼Œå®ƒé€šè¿‡ä¸€ç³»åˆ—å¼ºéªŒè¯å™¨å’Œå¼±éªŒè¯å™¨çš„åé¦ˆè¿›è¡Œè¿­ä»£ä¿®å¤ï¼Œå¹¶é€‚åº”è‡ªæˆ‘ä¸€è‡´æ€§ä»¥æ›´å¥½åœ°è¯„ä¼°é—®é¢˜çš„å¯å›ç­”æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFUN-FuSICåœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå¤šä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é€‚å½“é€‚åº”æ¨¡å‹å’Œæœ€æ–°çš„ç›‘ç£æ¨¡å‹ï¼ŒåŒæ—¶ä¸ºæˆ‘ä»¬å¯å›ç­”çš„ç­”æ¡ˆå‹å°‘é‡è½¬ç§»è®¾å®šäº†æ–°çš„æœ€æ–°æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14313v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>KBQAåœ¨ç°å®ä¸–ç•Œåº”ç”¨æ—¶é¢ä¸´å¤„ç†ä¸å¯å›ç­”é—®é¢˜çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™é¢†åŸŸå†…æ ‡ç­¾è®­ç»ƒæ•°æ®é‡è¾ƒå°çš„æƒ…å†µä¸‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹KBQAä¸å¯å›ç­”é—®é¢˜çš„æ–°å‹few-shotè¿ç§»ä»»åŠ¡ï¼Œå¹¶è´¡çŒ®äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºæ€§èƒ½è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†FUn-FuSICè§£å†³æ–¹æ¡ˆï¼Œå®ƒæ˜¯FuSIC KBQAçš„æ‰©å±•ï¼Œç”¨äºå¤„ç†å¯å›ç­”KBQAçš„few-shotè¿ç§»æ¨¡å‹ã€‚æˆ‘ä»¬æ³¨æ„åˆ°FuSIC-KBQAçš„è¿­ä»£ä¿®å¤å‡è®¾æ‰€æœ‰é—®é¢˜éƒ½æ˜¯ä¸å¯å›ç­”çš„ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†Feedback for Unanswerabilityï¼ˆFUnï¼‰æ–¹æ³•ï¼Œåˆ©ç”¨ä¸€ç³»åˆ—å¼ºå¼±éªŒè¯å™¨çš„åé¦ˆå’Œæœªå›ç­”é—®é¢˜è‡ªæˆ‘ä¸€è‡´æ€§è¯„ä¼°çš„é€‚åº”æ€§æ”¹è¿›ï¼Œæ¥è¯„ä¼°é—®é¢˜çš„å¯å›ç­”æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFUn-FuSICåœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç°æœ‰ç›‘ç£æ¨¡å‹çš„é€‚åº”ç‰ˆæœ¬ï¼ŒåŒæ—¶ä¸ºå¯å›ç­”é—®é¢˜çš„few-shotè¿ç§»ä»»åŠ¡å»ºç«‹äº†æ–°çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KBQAåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­é¢ä¸´å¤„ç†ä¸å¯å›ç­”é—®é¢˜çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ•°æ®é‡æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºäº†é’ˆå¯¹KBQAä¸å¯å›ç­”é—®é¢˜çš„æ–°å‹few-shotè¿ç§»ä»»åŠ¡ï¼Œå¹¶è´¡çŒ®äº†ä¸¤ä¸ªæ–°çš„æ•°æ®é›†ç”¨äºè¯„ä¼°ã€‚</li>
<li>FUn-FuSICæ˜¯FuSIC KBQAçš„æ‰©å±•ï¼Œç”¨äºå¤„ç†å¯å›ç­”KBQAçš„few-shotè¿ç§»ã€‚</li>
<li>FuSIC-KBQAçš„è¿­ä»£ä¿®å¤å‡è®¾å­˜åœ¨é—®é¢˜ï¼Œæ‰€æœ‰é—®é¢˜éƒ½å‡è®¾ä¸ºä¸å¯å›ç­”çš„ã€‚</li>
<li>æå‡ºFeedback for Unanswerabilityï¼ˆFUnï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆ©ç”¨å¼ºå¼±éªŒè¯å™¨çš„åé¦ˆå’Œæœªå›ç­”é—®é¢˜çš„è‡ªæˆ‘ä¸€è‡´æ€§è¯„ä¼°ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºFUn-FuSICåœ¨ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œç°æœ‰ç›‘ç£æ¨¡å‹çš„é€‚åº”ç‰ˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b9e8af428a3bf20165f9706f5b648a23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda8773a2bee374aee9743222e69b05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aceba7e4d29324f8a89be45ce3e8343.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PLOT-TAL-Prompt-Learning-with-Optimal-Transport-for-Few-Shot-Temporal-Action-Localization"><a href="#PLOT-TAL-Prompt-Learning-with-Optimal-Transport-for-Few-Shot-Temporal-Action-Localization" class="headerlink" title="PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal   Action Localization"></a>PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal   Action Localization</h2><p><strong>Authors:Edward Fish, Andrew Gilbert</strong></p>
<p>Few-shot temporal action localization (TAL) methods that adapt large models via single-prompt tuning often fail to produce precise temporal boundaries. This stems from the model learning a non-discriminative mean representation of an action from sparse data, which compromises generalization. We address this by proposing a new paradigm based on multi-prompt ensembles, where a set of diverse, learnable prompts for each action is encouraged to specialize on compositional sub-events. To enforce this specialization, we introduce PLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally optimal alignment between the prompt ensemble and the videoâ€™s temporal features. Our method establishes a new state-of-the-art on the challenging few-shot benchmarks of THUMOSâ€™14 and EPIC-Kitchens, without requiring complex meta-learning. The significant performance gains, particularly at high IoU thresholds, validate our hypothesis and demonstrate the superiority of learning distributed, compositional representations for precise temporal localization. </p>
<blockquote>
<p>å°‘æ•°æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½ï¼ˆTALï¼‰æ–¹æ³•é€šè¿‡å•æ¬¡æç¤ºè°ƒæ•´å¤§å‹æ¨¡å‹ï¼Œä½†å¾€å¾€æ— æ³•äº§ç”Ÿç²¾ç¡®çš„æ—¶åºè¾¹ç•Œã€‚è¿™æºäºæ¨¡å‹ä»ç¨€ç–æ•°æ®ä¸­å­¦ä¹ åŠ¨ä½œçš„éåˆ¤åˆ«æ€§å¹³å‡è¡¨ç¤ºï¼Œä»è€ŒæŸå®³äº†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§åŸºäºå¤šæç¤ºé›†æˆçš„æ–°èŒƒå¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥èŒƒå¼é¼“åŠ±é’ˆå¯¹æ¯ä¸ªåŠ¨ä½œçš„ä¸€ç»„å¤šæ ·ã€å¯å­¦ä¹ çš„æç¤ºä¸“æ³¨äºç»„åˆå­äº‹ä»¶ã€‚ä¸ºäº†å®æ–½ä¸“ä¸šåŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†PLOT-TALæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰æŠ€æœ¯æ‰¾åˆ°æç¤ºé›†æˆå’Œè§†é¢‘æ—¶åºç‰¹å¾ä¹‹é—´çš„å…¨å±€æœ€ä¼˜å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„THUMOSâ€™14å’ŒEPIC-Kitchenså°‘é‡æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ— éœ€å¤æ‚çš„å…ƒå­¦ä¹ ã€‚æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜IoUé˜ˆå€¼ä¸Šï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„å‡è®¾ï¼Œå¹¶è¯æ˜äº†å­¦ä¹ åˆ†å¸ƒå¼ç»„åˆè¡¨ç¤ºè¿›è¡Œç²¾ç¡®æ—¶åºå®šä½çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18915v2">PDF</a> Accepted to ICCVWS</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨¡å‹é€šè¿‡å•æç¤ºè°ƒæ•´è¿›è¡Œå°æ ·æœ¬æ—¶åºåŠ¨ä½œå®šä½ï¼ˆTALï¼‰æ—¶ï¼Œå¾€å¾€éš¾ä»¥äº§ç”Ÿç²¾ç¡®çš„æ—¶åºè¾¹ç•Œã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¤šæç¤ºé›†æˆçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡é¼“åŠ±æ¯ä¸ªåŠ¨ä½œçš„ä¸€ç»„å¯å­¦ä¹ ã€å¤šæ ·åŒ–çš„æç¤ºæ¥ä¸“æ³¨äºç»„åˆå­äº‹ä»¶ï¼Œä»¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ä¸ºäº†å®æ–½ä¸“ä¸šåŒ–ï¼Œæœ¬æ–‡å¼•å…¥äº†PLOT-TALæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰æ¥æ‰¾åˆ°æç¤ºé›†åˆå’Œè§†é¢‘æ—¶åºç‰¹å¾ä¹‹é—´çš„å…¨å±€æœ€ä¼˜å¯¹é½ã€‚è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„THUMOSâ€™14å’ŒEPIC-Kitchenså°‘é‡æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æ ‡å‡†ï¼Œæ— éœ€å¤æ‚çš„å…ƒå­¦ä¹ ã€‚æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜IoUé˜ˆå€¼ä¸Šï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„å‡è®¾å¹¶è¯æ˜äº†å­¦ä¹ åˆ†å¸ƒå¼ã€ç»„åˆè¡¨ç¤ºè¿›è¡Œç²¾ç¡®æ—¶åºå®šä½çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åŸºäºå•æç¤ºè°ƒæ•´çš„å°æ ·æœ¬TALæ–¹æ³•å­˜åœ¨ç²¾ç¡®æ—¶åºè¾¹ç•Œé—®é¢˜ã€‚</li>
<li>æå‡ºåŸºäºå¤šæç¤ºé›†æˆçš„æ–°æ–¹æ³•æ¥è§£å†³æ­¤é—®é¢˜ï¼Œé¼“åŠ±æ¯ä¸ªåŠ¨ä½œçš„æç¤ºä¸“æ³¨äºç»„åˆå­äº‹ä»¶ã€‚</li>
<li>å¼•å…¥PLOT-TALæ¡†æ¶ï¼Œåˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰æ‰¾åˆ°æç¤ºé›†åˆå’Œè§†é¢‘æ—¶åºç‰¹å¾ä¹‹é—´çš„å…¨å±€æœ€ä¼˜å¯¹é½ã€‚</li>
<li>åœ¨THUMOSâ€™14å’ŒEPIC-Kitchensçš„å°‘é‡æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚</li>
<li>æ–¹æ³•æ— éœ€å¤æ‚çš„å…ƒå­¦ä¹ ã€‚</li>
<li>æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜IoUé˜ˆå€¼ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.18915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-446ac2bbcb7678d5469c307018790bdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66e44bad6e3fa1c6e945c9ebbc8b764b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b919c3fe1fc9d5f64922ebe5407a4f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58ba75629af8b2bb99247708d8cfcb3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a055ed6299e20393ab384807017659a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5769ed135493ffd68acfacce012b0425.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-012d72b4584744f8f53bf706d28d9e81.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-9288ed48417716f6b40a6abd40c993b6.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  VideoMind An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f473f53deeea858c036aa8a1ad4b3a97.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Falcon-H1 A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
