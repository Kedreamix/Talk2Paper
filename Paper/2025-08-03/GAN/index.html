<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN 方向最新论文已更新，请持续关注 Update in 2025-08-03  X-NeMo Expressive Neural Motion Reenactment via Disentangled Latent   Attention">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d1ebfb707a977432f368bd71b86ffedd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-03-更新"><a href="#2025-08-03-更新" class="headerlink" title="2025-08-03 更新"></a>2025-08-03 更新</h1><h2 id="X-NeMo-Expressive-Neural-Motion-Reenactment-via-Disentangled-Latent-Attention"><a href="#X-NeMo-Expressive-Neural-Motion-Reenactment-via-Disentangled-Latent-Attention" class="headerlink" title="X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent   Attention"></a>X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent   Attention</h2><p><strong>Authors:Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, Yebin Liu</strong></p>
<p>We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. We further enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance, our design eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models are available for research. </p>
<blockquote>
<p>我们提出了X-NeMo，这是一种新型零样本扩散式肖像动画流水线，它使用来自不同个体驱动视频的面部动作来动画静态肖像。我们的工作首先确定了先前方法中的主要问题的根本原因，例如身份泄露和捕捉微妙及极端表情的困难。为了解决这些挑战，我们引入了一个全新的端到端训练框架，从驱动图像中提炼出1D身份无关潜在运动描述符，通过交叉注意有效控制在图像生成过程中的运动。我们的隐性运动描述符以精细的方式捕捉表情丰富的面部运动，从多样化的视频数据集中进行端到端学习，无需依赖预训练的运动检测器。我们进一步通过双GAN解码器进行空间和时间增强来增强表现能力，并将运动潜在性与身份线索分开。通过将驱动运动嵌入到1D潜在向量中，并通过交叉注意而不是附加的空间指导来控制运动，我们的设计消除了从驱动条件到扩散主干的空间对齐结构线索的传输，极大地减轻了身份泄露问题。大量实验表明，X-NeMo超越了最先进的基线，产生了具有高度表现力且与身份高度相似的动画。我们的代码和模型可供研究使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23143v1">PDF</a> ICLR 2025, code is available at   <a target="_blank" rel="noopener" href="https://github.com/bytedance/x-nemo-inference">https://github.com/bytedance/x-nemo-inference</a></p>
<p><strong>Summary</strong><br>基于上述技术文本的描述，该研究提出了一种新型零样本扩散式肖像动画管道X-NeMo，该管道能够以另一人的面部动作驱动静态肖像动画。该研究解决了先前方法中的身份泄露和捕捉微妙及极端表情困难等核心问题。通过引入端到端的训练框架和双重GAN解码器等技术手段，该研究实现了精细的面部表情捕捉和身份信息的解耦，有效提升了动画的表达力和质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新型的零样本扩散式肖像动画方法X-NeMo，能够使用不同个体的面部动作驱动静态肖像动画。</li>
<li>解决了身份泄露和捕捉微妙及极端表情困难等核心问题。</li>
<li>通过端到端的训练框架，从驱动图像中提炼出身份无关的一维潜在运动描述符，有效控制动画过程中的运动。</li>
<li>引入了一种隐式运动描述符，能够精细捕捉面部表情。</li>
<li>通过双重GAN解码器、空间和时间增强技术，提高了动画的表达力和质量，实现了运动潜力和身份线索的解耦。</li>
<li>通过将驱动动作嵌入到一维潜在向量中，并通过交叉注意力控制运动，避免了空间对齐的结构线索从驱动条件传递到扩散主干，有效减轻了身份泄露问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23143">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f1015e97019a4fe7c3d5fd3cbbf6ce2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78f27db2da7cb510edfb8c75163485b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1589808bb0c06255e685f2f4473e068.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Harnessing-Diffusion-Yielded-Score-Priors-for-Image-Restoration"><a href="#Harnessing-Diffusion-Yielded-Score-Priors-for-Image-Restoration" class="headerlink" title="Harnessing Diffusion-Yielded Score Priors for Image Restoration"></a>Harnessing Diffusion-Yielded Score Priors for Image Restoration</h2><p><strong>Authors:Xinqi Lin, Fanghua Yu, Jinfan Hu, Zhiyuan You, Wu Shi, Jimmy S. Ren, Jinjin Gu, Chao Dong</strong></p>
<p>Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration. </p>
<blockquote>
<p>深度图像恢复模型旨在从退化图像空间学习映射到自然图像空间。然而，它们面临几个关键挑战：去除退化因素、生成逼真的细节，以及确保像素级别的一致性。随着时间的推移，出现了三大类方法，包括基于MSE的方法、基于GAN的方法和基于扩散的方法。然而，它们在恢复质量、保真度和速度之间并未取得良好的平衡。我们提出了一种新方法HYPIR来解决这些挑战。我们的解决方案流程很简单：它涉及使用预训练的扩散模型初始化图像恢复模型，然后使用对抗性训练进行微调。这种方法不需要依赖扩散损失、迭代采样或额外的适配器。从理论上讲，我们从预训练的扩散模型开始对抗性训练，使初始恢复模型非常接近自然图像分布。因此，这种初始化提高了数值稳定性，避免了模式崩溃，并大大加速了对抗性训练的收敛。此外，HYPIR继承了扩散模型的丰富用户控制功能，可实现文本引导的恢复和可调整纹理丰富度。仅需一次前向传递，它的收敛和推理速度就超过了基于扩散的方法。大量实验表明，HYPIR优于以前的最先进方法，实现了高效和高质量的图像恢复。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20590v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文介绍了一种新型的图像修复方法HYPIR，旨在解决现有方法面临的关键挑战，包括去除退化、生成逼真的细节以及确保像素级别的连贯性。通过初始化图像修复模型并使用预训练的扩散模型进行对抗训练，该方法无需依赖扩散损失、迭代采样或额外的适配器。初始化对抗训练的策略将修复模型更接近自然图像分布，从而提高了数值稳定性，避免了模式崩溃，并大大加快了对抗训练的收敛速度。同时，HYPIR继承了扩散模型的丰富用户控制功能，支持文本引导修复和调整纹理丰富度。只需单次前向传递，它的收敛速度和推理速度都比基于扩散的方法更快，且实验表明HYPIR的性能优于现有最佳方法，可实现高效且高质量的图像修复。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HYPIR是一种新型的图像修复方法，旨在解决现有方法面临的挑战，包括去除图像退化、生成逼真细节和确保像素级一致性。</li>
<li>HYPIR通过初始化图像修复模型并使用预训练的扩散模型进行对抗训练，简化了解决方案流程。</li>
<li>初始化对抗训练使修复模型更接近自然图像分布，提高了数值稳定性和收敛速度。</li>
<li>HYPIR无需依赖扩散损失、迭代采样或额外的适配器，实现了快速收敛和推理速度。</li>
<li>HYPIR继承了扩散模型的丰富用户控制功能，支持文本引导修复和调整纹理丰富度。</li>
<li>HYPIR的性能优于现有最佳方法，可实现高效且高质量的图像修复。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20590">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fd6fd6498b961104dd88dbaa0f92aa81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4c65969439942ba2cf96eafbc3f9511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d6b4634a057433db6d84ef84eb06aa2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unsupervised-anomaly-detection-using-Bayesian-flow-networks-application-to-brain-FDG-PET-in-the-context-of-Alzheimer’s-disease"><a href="#Unsupervised-anomaly-detection-using-Bayesian-flow-networks-application-to-brain-FDG-PET-in-the-context-of-Alzheimer’s-disease" class="headerlink" title="Unsupervised anomaly detection using Bayesian flow networks: application   to brain FDG PET in the context of Alzheimer’s disease"></a>Unsupervised anomaly detection using Bayesian flow networks: application   to brain FDG PET in the context of Alzheimer’s disease</h2><p><strong>Authors:Hugues Roy, Reuben Dorent, Ninon Burgos</strong></p>
<p>Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for identifying deviations from healthy subject data and thus facilitating the diagnosis of neurological disorders. In this work, we focus on Bayesian flow networks (BFNs), a novel class of generative models, which have not yet been applied to medical imaging or anomaly detection. BFNs combine the strength of diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension of BFNs for UAD, designed to: i) perform conditional image generation under high levels of spatially correlated noise, and ii) preserve subject specificity by incorporating a recursive feedback from the input image throughout the generative process. We evaluate AnoBFN on the challenging task of Alzheimer’s disease-related anomaly detection in FDG PET images. Our approach outperforms other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and diffusion models (AnoDDPM), demonstrating its effectiveness at detecting anomalies while reducing false positive rates. </p>
<blockquote>
<p>无监督异常检测（UAD）在神经成像中发挥着关键作用，能够识别健康受试者数据的偏差，从而有助于神经紊乱疾病的诊断。在这项工作中，我们专注于贝叶斯流网络（BFNs）这一新型生成模型，尚未应用于医学成像或异常检测。BFNs结合了扩散框架和贝叶斯推断的优势。我们介绍了AnoBFN，这是BFNs在UAD方面的扩展，旨在：i）在高水平空间相关噪声下进行条件图像生成，以及ii）通过在整个生成过程中融入来自输入图像的递归反馈来保持受试者特异性。我们在阿尔茨海默病相关的异常检测这一具有挑战性的任务上评估了AnoBFN在FDG PET图像中的应用。我们的方法优于其他基于VAEs（beta-VAE）、GANs（f-AnoGAN）和扩散模型（AnoDDPM）的最先进方法，证明其在检测异常时降低误报率的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17486v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于贝叶斯流网络（BFNs）的UAD技术在医学影像中扮演重要角色，用于识别偏离健康主体的数据偏差，进而辅助诊断神经性疾病。本研究推出了一种针对医学影像的无监督异常检测新方法——AnoBFN，它结合了扩散框架和贝叶斯推断的优势，能在高空间相关噪声下进行条件图像生成，并能在生成过程中通过递归反馈保留主题特异性。在阿尔茨海默病相关的异常检测任务中，与beta-VAEs、f-AnoGAN和扩散模型等先进方法相比，其表现更为出色。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>UAD在医学影像中扮演重要角色，用于识别偏离健康主体的数据偏差。</li>
<li>本研究引入了一种针对医学影像的无监督异常检测新方法——AnoBFN。</li>
<li>AnoBFN结合了扩散框架和贝叶斯推断的优势。</li>
<li>AnoBFN能在高空间相关噪声下进行条件图像生成。</li>
<li>AnoBFN通过递归反馈保留主题特异性。</li>
<li>在阿尔茨海默病相关的异常检测任务中，AnoBFN表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17486">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-37e58469f69d8fdfd827692f0750a601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1ebfb707a977432f368bd71b86ffedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dd5f55b5389a9892f96401a9bd071d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Collaborative-Advantage-of-Low-level-Information-on-Generalizable-AI-Generated-Image-Detection"><a href="#Exploring-the-Collaborative-Advantage-of-Low-level-Information-on-Generalizable-AI-Generated-Image-Detection" class="headerlink" title="Exploring the Collaborative Advantage of Low-level Information on   Generalizable AI-Generated Image Detection"></a>Exploring the Collaborative Advantage of Low-level Information on   Generalizable AI-Generated Image Detection</h2><p><strong>Authors:Ziyin Zhou, Ke Sun, Zhongxi Chen, Xianming Lin, Yunpeng Luo, Ke Yan, Shouhong Ding, Xiaoshuai Sun</strong></p>
<p>Existing state-of-the-art AI-Generated image detection methods mostly consider extracting low-level information from RGB images to help improve the generalization of AI-Generated image detection, such as noise patterns. However, these methods often consider only a single type of low-level information, which may lead to suboptimal generalization. Through empirical analysis, we have discovered a key insight: different low-level information often exhibits generalization capabilities for different types of forgeries. Furthermore, we found that simple fusion strategies are insufficient to leverage the detection advantages of each low-level and high-level information for various forgery types. Therefore, we propose the Adaptive Low-level Experts Injection (ALEI) framework. Our approach introduces Lora Experts, enabling the backbone network, which is trained with high-level semantic RGB images, to accept and learn knowledge from different low-level information. We utilize a cross-attention method to adaptively fuse these features at intermediate layers. To prevent the backbone network from losing the modeling capabilities of different low-level features during the later stages of modeling, we developed a Low-level Information Adapter that interacts with the features extracted by the backbone network. Finally, we propose Dynamic Feature Selection, which dynamically selects the most suitable features for detecting the current image to maximize generalization detection capability. Extensive experiments demonstrate that our method, finetuned on only four categories of mainstream ProGAN data, performs excellently and achieves state-of-the-art results on multiple datasets containing unseen GAN and Diffusion methods. </p>
<blockquote>
<p>现有最先进的AI生成图像检测方法大多考虑从RGB图像中提取低级信息，以帮助提高AI生成图像检测的泛化能力，例如噪声模式。然而，这些方法通常只考虑一种低级信息，可能导致泛化效果不佳。通过实证分析，我们发现了关键见解：不同的低级信息通常对不同类型的伪造表现出不同的泛化能力。此外，我们发现简单的融合策略不足以利用每种低级和高级信息在不同类型伪造检测中的优势。因此，我们提出了自适应低级专家注入（ALEI）框架。我们的方法引入了Lora专家，使主干网络（用高级语义RGB图像进行训练）能够接受和学习来自不同低级信息的知识。我们使用交叉注意力方法来自适应地融合中间层中的这些特征。为了防止主干网络在建模后期失去对不同低级特征的建模能力，我们开发了一个低级信息适配器，与主干网络提取的特征进行交互。最后，我们提出了动态特征选择，它动态选择最适合检测当前图像的特征，以最大限度地提高泛化检测能力。大量实验表明，我们的方法仅在四个主流ProGAN数据类别上进行微调，就表现出卓越性能，并在包含未见过的GAN和Diffusion方法的多个数据集上实现了最新技术成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00463v2">PDF</a> </p>
<p><strong>Summary</strong><br>     现有主流AI图像检测方法多依赖从RGB图像中提取低层次信息来提高图像检测的泛化能力，但存在信息单一和融合策略简单的问题。本研究发现不同低层次信息对不同类型伪造图像的泛化能力不同，因此提出自适应低层次专家注入（ALEI）框架。该框架引入Lora专家，使训练有高级语义RGB图像的主干网络能够接受和学习不同低层次信息的知识。通过跨注意力方法自适应融合这些特征，并开发了低层次信息适配器来与主干网络提取的特征进行交互。同时提出动态特征选择，可动态选择最适于当前图像检测的特征以实现最佳的泛化检测能力。本研究通过实验验证了该方法的优秀性能和前沿成果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI图像检测方法多依赖低层次信息提高泛化能力。</li>
<li>现有方法存在信息单一和融合策略不足的问题。</li>
<li>不同低层次信息对不同类型伪造图像的泛化能力不同。</li>
<li>提出自适应低层次专家注入（ALEI）框架来解决上述问题。</li>
<li>ALEI框架引入Lora专家，结合跨注意力方法自适应融合特征。</li>
<li>开发低层次信息适配器来增强主干网络的特征交互能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00463">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4c042374980cbc641ca2982a1545d833.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f5186b30e80346598418f6afa89ae5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1efa71886031e529de5d78070607556f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder"><a href="#Att-Adapter-A-Robust-and-Precise-Domain-Specific-Multi-Attributes-T2I-Diffusion-Adapter-via-Conditional-Variational-Autoencoder" class="headerlink" title="Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder"></a>Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I   Diffusion Adapter via Conditional Variational Autoencoder</h2><p><strong>Authors:Wonwoong Cho, Yan-Ying Chen, Matthew Klenk, David I. Inouye, Yanxia Zhang</strong></p>
<p>Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in generating high quality images. However, enabling precise control of continuous attributes, especially multiple attributes simultaneously, in a new domain (e.g., numeric values like eye openness or car width) with text-only guidance remains a significant challenge. To address this, we introduce the Attribute (Att) Adapter, a novel plug-and-play module designed to enable fine-grained, multi-attributes control in pretrained diffusion models. Our approach learns a single control adapter from a set of sample images that can be unpaired and contain multiple visual attributes. The Att-Adapter leverages the decoupled cross attention module to naturally harmonize the multiple domain attributes with text conditioning. We further introduce Conditional Variational Autoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the diverse nature of the visual world. Evaluations on two public datasets show that Att-Adapter outperforms all LoRA-based baselines in controlling continuous attributes. Additionally, our method enables a broader control range and also improves disentanglement across multiple attributes, surpassing StyleGAN-based techniques. Notably, Att-Adapter is flexible, requiring no paired synthetic data for training, and is easily scalable to multiple attributes within a single model. </p>
<blockquote>
<p>文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著的成绩。然而，在新的领域（如眼睛睁开程度或汽车宽度等数值）实现连续属性的精确控制，尤其是同时控制多个属性，仅凭文本指导仍是一个巨大的挑战。为了解决这个问题，我们引入了属性（Att）适配器，这是一种新型即插即用模块，旨在在预训练的扩散模型中实现精细的多属性控制。我们的方法从一组样本图像中学习单个控制适配器，这些图像可以是未配对的，并包含多个视觉属性。Att-Adapter利用解耦交叉注意模块，自然地协调多个域属性与文本条件。为了进一步缓解过拟合问题，并匹配视觉世界的多样性，我们在Att-Adapter中引入了条件变分自编码器（CVAE）。在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的方法。此外，我们的方法扩大了控制范围，并改善了多个属性之间的解纠缠，超越了基于StyleGAN的技术。值得注意的是，Att-Adapter非常灵活，无需配对合成数据进行训练，并且很容易在单个模型中扩展到多个属性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11937v4">PDF</a> ICCV’25 (Highlight), The project page is available at   <a target="_blank" rel="noopener" href="https://tri-mac.github.io/att-adapter/">https://tri-mac.github.io/att-adapter/</a></p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型已生成高质量图像方面取得了显著成效。然而，使用纯文本指导在新领域（如眼睛睁开程度或汽车宽度等数值）实现连续属性的精确控制，尤其是同时控制多个属性，仍然是一个重大挑战。为解决此问题，我们引入了属性适配器（Att-Adapter），这是一种新型即插即用模块，旨在实现在预训练扩散模型中的精细粒度多属性控制。我们的方法从一组非配对包含多个视觉属性的样本图像中学习单个控制适配器。Att-Adapter利用解耦交叉注意力模块，自然地协调多个域属性与文本条件。为进一步缓解过拟合问题并匹配视觉世界的多样性，我们还在Att-Adapter中引入了条件变分自编码器（CVAE）。在两个公共数据集上的评估显示，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线。此外，我们的方法具有更广泛的控制范围，并在多个属性之间改善了分离性，超越了StyleGAN技术。值得一提的是，Att-Adapter非常灵活，无需配对合成数据进行训练，并且很容易在单个模型中扩展到多个属性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I扩散模型在生成高质量图像方面表现出色，但在新领域实现多属性连续控制仍存在挑战。</li>
<li>引入属性适配器（Att-Adapter）模块，旨在解决预训练扩散模型中的精细粒度多属性控制问题。</li>
<li>Att-Adapter通过利用解耦交叉注意力模块和条件变分自编码器（CVAE）实现多域属性与文本条件的自然协调。</li>
<li>Att-Adapter的优势在于其灵活性，无需配对合成数据即可训练，并且容易扩展到单个模型中的多个属性。</li>
<li>在两个公共数据集上的评估显示，Att-Adapter在控制连续属性和提高多个属性的分离性方面优于其他技术。</li>
<li>Att-Adapter具有更广泛的控制范围，并且可以缓解过拟合问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11937">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-501c7ad8cd52c75d872df827366e41a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76195c815f831cbd14c3668d282aeaa8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34079aa19aff64e07c130e59d119f2ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00bc2bbcb59dab963c32b4d31100b867.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-940707c6cb8e1cd0b55b1b0ae9fb4ea7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dual-Frequency-Branch-Framework-with-Reconstructed-Sliding-Windows-Attention-for-AI-Generated-Image-Detection"><a href="#Dual-Frequency-Branch-Framework-with-Reconstructed-Sliding-Windows-Attention-for-AI-Generated-Image-Detection" class="headerlink" title="Dual Frequency Branch Framework with Reconstructed Sliding Windows   Attention for AI-Generated Image Detection"></a>Dual Frequency Branch Framework with Reconstructed Sliding Windows   Attention for AI-Generated Image Detection</h2><p><strong>Authors:Jiazhen Yan, Ziqiang Li, Fan Wang, Ziwen He, Zhangjie Fu</strong></p>
<p>The rapid advancement of Generative Adversarial Networks (GANs) and diffusion models has enabled the creation of highly realistic synthetic images, presenting significant societal risks, such as misinformation and deception. As a result, detecting AI-generated images has emerged as a critical challenge. Existing researches emphasize extracting fine-grained features to enhance detector generalization, yet they often lack consideration for the importance and interdependencies of internal elements within local regions and are limited to a single frequency domain, hindering the capture of general forgery traces. To overcome the aforementioned limitations, we first utilize a sliding window to restrict the attention mechanism to a local window, and reconstruct the features within the window to model the relationships between neighboring internal elements within the local region. Then, we design a dual frequency domain branch framework consisting of four frequency domain subbands of DWT and the phase part of FFT to enrich the extraction of local forgery features from different perspectives. Through feature enrichment of dual frequency domain branches and fine-grained feature extraction of reconstruction sliding window attention, our method achieves superior generalization detection capabilities on both GAN and diffusion model-based generative images. Evaluated on diverse datasets comprising images from 65 distinct generative models, our approach achieves a 2.13% improvement in detection accuracy over state-of-the-art methods. </p>
<blockquote>
<p>生成对抗网络（GANs）和扩散模型的快速发展使得能够创建高度逼真的合成图像，这带来了诸如虚假信息和欺骗等重大社会风险。因此，检测AI生成的图像已成为一项重大挑战。现有研究强调提取精细特征以增强检测器的泛化能力，但它们往往忽略了局部内部元素的重要性和相互依赖性，并且仅限于单一频率域，这阻碍了通用伪造痕迹的捕获。为了克服上述局限性，我们首先使用滑动窗口将注意力机制限制在局部窗口内，并重建窗口内的特征以建模局部区域内相邻内部元素之间的关系。然后，我们设计了一个由四个DWT频率域子带和FFT相位部分组成的双频域分支框架，以丰富从不同角度提取的局部伪造特征。通过双频域分支的特征丰富和重建滑动窗口注意力的精细特征提取，我们的方法在基于GAN和扩散模型的生成图像上实现了卓越的检测泛化能力。在包含来自65个不同生成模型的图像的多样化数据集上评估，我们的方法在检测精度上比现有最新方法提高了2.13%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15253v2">PDF</a> under review</p>
<p><strong>Summary</strong><br>     生成对抗网络（GANs）和扩散模型的快速发展能够生成高度逼真的合成图像，带来了诸如虚假信息和欺骗等重大社会风险。因此，检测AI生成的图像成为了一项关键挑战。为了克服现有研究的局限性，本文采用局部窗口注意力机制重构局部区域内的特征关系，并设计包含小波变换四个频率子带和快速傅里叶变换相位部分的双频域分支框架，从不同角度丰富局部伪造特征的提取。此方法在GAN和扩散模型生成的图像上实现了出色的检测能力，并在包含来自65种不同生成模型的图像的数据集上实现了相较于现有方法2.13%的检测准确性提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成对抗网络（GANs）和扩散模型能快速生成逼真合成图像，引发社会风险。</li>
<li>检测AI生成图像成为重要挑战，现有研究侧重于提取精细特征以增强检测器的泛化能力。</li>
<li>现有研究忽略了局部内部元素的重要性和互依赖性，且仅限于单一频域。</li>
<li>本文采用局部窗口注意力机制重构局部区域特征关系，并设计双频域分支框架丰富局部伪造特征的提取。</li>
<li>方法实现了在GAN和扩散模型生成的图像上的出色检测能力。</li>
<li>在包含多种生成模型的图像数据集上实现了较高的检测准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ab20c97e31a909f13a57eb3de84b9f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dc61fa8faa598386ad3fcc271ed9593.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50eed127c6862053e3c69bc1aac72793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03a34166b330857598528f62dc7ee93a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9df34e90548be7b603c22c486bab3d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1c7cbd20adaacb856b41c36914bfa58.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a8268415a1d63573090cab143031dee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3aa150e25b9b48f9b4a444d63495437.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="On-the-Statistical-Properties-of-Generative-Adversarial-Models-for-Low-Intrinsic-Data-Dimension"><a href="#On-the-Statistical-Properties-of-Generative-Adversarial-Models-for-Low-Intrinsic-Data-Dimension" class="headerlink" title="On the Statistical Properties of Generative Adversarial Models for Low   Intrinsic Data Dimension"></a>On the Statistical Properties of Generative Adversarial Models for Low   Intrinsic Data Dimension</h2><p><strong>Authors:Saptarshi Chakraborty, Peter L. Bartlett</strong></p>
<p>Despite the remarkable empirical successes of Generative Adversarial Networks (GANs), the theoretical guarantees for their statistical accuracy remain rather pessimistic. In particular, the data distributions on which GANs are applied, such as natural images, are often hypothesized to have an intrinsic low-dimensional structure in a typically high-dimensional feature space, but this is often not reflected in the derived rates in the state-of-the-art analyses. In this paper, we attempt to bridge the gap between the theory and practice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs), by deriving statistical guarantees on the estimated densities in terms of the intrinsic dimension of the data and the latent space. We analytically show that if one has access to $n$ samples from the unknown target distribution and the network architectures are properly chosen, the expected Wasserstein-1 distance of the estimates from the target scales as $O\left( n^{-1&#x2F;d_\mu } \right)$ for GANs and $\tilde{O}\left( n^{-1&#x2F;(d_\mu+\ell)} \right)$ for BiGANs, where $d_\mu$ and $\ell$ are the upper Wasserstein-1 dimension of the data-distribution and latent-space dimension, respectively. The theoretical analyses not only suggest that these methods successfully avoid the curse of dimensionality, in the sense that the exponent of $n$ in the error rates does not depend on the data dimension but also serve to bridge the gap between the theoretical analyses of GANs and the known sharp rates from optimal transport literature. Additionally, we demonstrate that GANs can effectively achieve the minimax optimal rate even for non-smooth underlying distributions, with the use of interpolating generator networks. </p>
<blockquote>
<p>尽管生成对抗网络（GANs）在经验上取得了显著的成果，但其统计精度的理论保证仍然相当悲观。特别是，GANs所应用的数据分布，如自然图像，通常假设在通常的高维特征空间中具有内在的低维结构，但这在最新分析中并未得到反映。在本文中，我们试图通过推导关于数据固有维度和潜在空间的估计密度的统计保证，来填补GANs及其双向变体Bi-directional GANs（BiGANs）理论与实践之间的鸿沟。我们分析表明，如果我们有来自未知目标分布的n个样本，并且网络架构选择得当，那么估计值与目标的Wasserstein-1距离期望值对于GANs来说会按照O(n^-1&#x2F;d_μ)的比例进行缩放，而对于BiGANs来说则按照O~(n^-1&#x2F;(d_μ+\ell))的比例进行缩放，其中d_μ和l分别是数据分布和潜在空间维度的Wasserstein-1维数的上限。理论分析不仅表明这些方法成功地避免了维数诅咒，即误差率中的n的指数不依赖于数据维度，而且弥合了GANs的理论分析与最优传输文献中已知尖锐速率之间的鸿沟。此外，我们还证明了即使对于非平滑的基础分布，通过使用插值生成网络，GANs也可以有效地实现最小最大最优率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.15801v2">PDF</a> Journal of Machine Learning Research (2025), volume 26</p>
<p><strong>Summary</strong></p>
<p>本文探讨了生成对抗网络（GANs）的理论与实践之间的鸿沟，特别是在统计数据分布方面的理论保证。文章通过推导统计数据保证，尝试弥合了GANs及其双向变体BiGANs之间的这一差距。研究表明，在合理选择网络架构的情况下，GANs和BiGANs能够从目标分布中获得有效的密度估计，并且在特定的维度条件下实现了优化。理论分析显示，这些方法成功避免了维度诅咒，并且与最优传输文献中的已知尖锐率建立了联系。此外，使用插值生成器网络，GANs可以有效实现最小最大最优率，即使对于非平滑的基础分布也是如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANs的理论保证对其统计准确性相对悲观，特别是在实际应用中的数据分布上。</li>
<li>GANs和BiGANs的理论与实践之间存在差距，本文尝试通过推导统计数据保证来弥合这一差距。</li>
<li>在合理选择网络架构的情况下，GANs和BiGANs能够从目标分布中获得有效的密度估计。</li>
<li>理论分析显示，这些方法成功避免了维度诅咒，即误差率中的指数不依赖于数据维度。</li>
<li>GANs的理论分析与最优传输文献中的已知尖锐率建立了联系。</li>
<li>使用插值生成器网络，GANs可以有效处理非平滑的基础分布，并实现最小最大最优率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.15801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc7f13ed8ecc5f7d053915f2878f58f3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4d37b04335d7a85c6e8cb20a5e5e3a09.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-08-03  EPSilon Efficient Point Sampling for Lightening of Hybrid-based 3D   Avatar Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-bf249282f924a7e4a548b1b4183ac0d7.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech 方向最新论文已更新，请持续关注 Update in 2025-08-03  Conan A Chunkwise Online Network for Zero-Shot Adaptive Voice   Conversion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
