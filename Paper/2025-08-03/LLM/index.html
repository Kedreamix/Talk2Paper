<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Falcon-H1 A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f473f53deeea858c036aa8a1ad4b3a97.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="Falcon-H1-A-Family-of-Hybrid-Head-Language-Models-Redefining-Efficiency-and-Performance"><a href="#Falcon-H1-A-Family-of-Hybrid-Head-Language-Models-Redefining-Efficiency-and-Performance" class="headerlink" title="Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance"></a>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance</h2><p><strong>Authors:Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha</strong></p>
<p>In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research. </p>
<blockquote>
<p>åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Falcon-H1ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å®ƒé‡‡ç”¨äº†æ··åˆæ¶æ„è®¾è®¡ï¼Œé’ˆå¯¹å„ç§ä¸åŒçš„ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥å®ç°é«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚ä¸åŒäºæ—©æœŸä»…åŸºäºTransformeræˆ–Mambaæ¶æ„çš„Falconæ¨¡å‹ï¼ŒFalcon-H1é‡‡ç”¨äº†å¹¶è¡Œæ··åˆæ–¹æ³•ï¼Œå°†åŸºäºTransformerçš„æ³¨æ„åŠ›æœºåˆ¶ä¸ä»¥çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ç›¸ç»“åˆã€‚SSMä»¥å‡ºè‰²çš„é•¿ä¸Šä¸‹æ–‡è®°å¿†å’Œè®¡ç®—æ•ˆç‡è€Œé—»åã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†æ¨¡å‹è®¾è®¡ã€æ•°æ®ç­–ç•¥å’Œè®­ç»ƒåŠ¨æ€ï¼ŒæŒ‘æˆ˜äº†è¯¥é¢†åŸŸçš„ä¼ ç»Ÿå®è·µã€‚Falcon-H1ä»¥å¤šç§é…ç½®å‘å¸ƒï¼ŒåŒ…æ‹¬åŸºæœ¬å’ŒæŒ‡ä»¤è°ƒæ•´å‹å˜ä½“ï¼Œå‚æ•°ä»0.5Bã€1.5Bã€1.5B-deepã€3Bã€7Båˆ°34Bä¸ç­‰ã€‚è¿˜æä¾›é‡åŒ–æŒ‡ä»¤è°ƒæ•´å‹æ¨¡å‹ï¼Œæ€»è®¡è¶…è¿‡30ä¸ªæ£€æŸ¥ç‚¹åœ¨Hugging Face Hubä¸Šæä¾›ã€‚Falcon-H1æ¨¡å‹å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå‡ºè‰²çš„å‚æ•°åŠè®­ç»ƒæ•ˆç‡ã€‚æ——èˆ°äº§å“Falcon-H1-34Bçš„å‚æ•°å’Œæ€§èƒ½ä¸è§„æ¨¡é«˜è¾¾70Bçš„æ¨¡å‹ï¼ˆå¦‚Qwen3-32Bã€Qwen2.5-72Bå’ŒLlama3.3-70Bï¼‰ç›¸åŒ¹é…æˆ–æ›´ä¼˜ç§€ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°å’Œæ•°æ®æ›´å°‘ã€‚è¾ƒå°çš„æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºç›¸ä¼¼çš„è¶‹åŠ¿ï¼šFalcon-H1-1.5B-Deepä¸å½“å‰çš„7B-10Bé¢†å…ˆæ¨¡å‹ç›¸åŒ¹æ•Œï¼Œè€ŒFalcon-H1-0.5Bçš„æ€§èƒ½ç›¸å½“äºå…¸å‹çš„2024å¹´7Bæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åœ¨æ¨ç†ã€æ•°å­¦ã€å¤šè¯­ç§ä»»åŠ¡ã€æŒ‡ä»¤éµå¾ªå’Œç§‘å­¦çŸ¥è¯†ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ”¯æŒå¤šè¾¾256Kä¸ªä¸Šä¸‹æ–‡æ ‡è®°å’Œ18ç§è¯­è¨€ï¼ŒFalcon-H1é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚æ‰€æœ‰æ¨¡å‹éƒ½åœ¨è®¸å¯çš„å¼€æºè®¸å¯è¯ä¸‹å‘å¸ƒï¼Œè¿™ä½“ç°äº†æˆ‘ä»¬å¯¹å¯è®¿é—®å’Œæœ‰å½±å“åŠ›çš„AIç ”ç©¶çš„æ‰¿è¯ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22448v1">PDF</a> Technical report of Falcon-H1 model series</p>
<p><strong>Summary</strong></p>
<p>è¯¥æŠ¥å‘Šä»‹ç»äº†æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—â€”â€”Falcon-H1ã€‚å®ƒé‡‡ç”¨æ··åˆæ¶æ„è®¾è®¡ï¼Œç»“åˆäº†Transformerçš„æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œä¼˜åŒ–äº†é«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚ç»è¿‡å¯¹æ¨¡å‹è®¾è®¡ã€æ•°æ®ç­–ç•¥å’Œè®­ç»ƒåŠ¨åŠ›å­¦çš„ç³»ç»Ÿå›é¡¾ï¼ŒFalcon-H1å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ã€‚è¯¥æ¨¡å‹æœ‰å¤šä¸ªé…ç½®ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒæ•´å‹ï¼Œå‚æ•°ä»0.5Båˆ°34Bä¸ç­‰ã€‚å…¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ”¯æŒå¤šè¾¾256Kçš„ä¸Šä¸‹æ–‡æ ‡è®°å’Œ18ç§è¯­è¨€ã€‚æ‰€æœ‰æ¨¡å‹å‡é‡‡ç”¨å®½æ¾çš„å¼€æºè®¸å¯åè®®å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Falcon-H1æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—çš„æ–°å‹äº§å“ï¼Œå…·æœ‰æ··åˆæ¶æ„è®¾è®¡ã€‚</li>
<li>ç»“åˆäº†Transformerçš„æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œä¼˜åŒ–äº†é«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>ç³»ç»Ÿæ€§åœ°å›é¡¾äº†æ¨¡å‹è®¾è®¡ã€æ•°æ®ç­–ç•¥å’Œè®­ç»ƒåŠ¨åŠ›å­¦ã€‚</li>
<li>Falcon-H1å…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ï¼Œå±•ç°å‡ºä¸šç•Œé¢†å…ˆçš„è¡¨ç°ã€‚</li>
<li>æä¾›äº†å¤šä¸ªé…ç½®ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒæ•´å‹ï¼Œå‚æ•°èŒƒå›´å¹¿æ³›ã€‚</li>
<li>æ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚æ¨ç†ã€æ•°å­¦ã€å¤šè¯­ç§ä»»åŠ¡ã€æŒ‡ä»¤éµå¾ªå’Œç§‘å­¦çŸ¥è¯†å‚¨å¤‡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce7b68b599b015742eef6f9e2a5c39db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7516e8853b9e9f40b97ae8f240bee4c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14c7153eaac3869e09ac279621d74ce3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ChatGPT-Reads-Your-Tone-and-Responds-Accordingly-â€“-Until-It-Does-Not-â€“-Emotional-Framing-Induces-Bias-in-LLM-Outputs"><a href="#ChatGPT-Reads-Your-Tone-and-Responds-Accordingly-â€“-Until-It-Does-Not-â€“-Emotional-Framing-Induces-Bias-in-LLM-Outputs" class="headerlink" title="ChatGPT Reads Your Tone and Responds Accordingly â€“ Until It Does Not â€“   Emotional Framing Induces Bias in LLM Outputs"></a>ChatGPT Reads Your Tone and Responds Accordingly â€“ Until It Does Not â€“   Emotional Framing Induces Bias in LLM Outputs</h2><p><strong>Authors:Franck Bardol</strong></p>
<p>Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a â€œreboundâ€ bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the â€œtone floorâ€ - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/bardolfranck/llm-responses-viewer">https://github.com/bardolfranck/llm-responses-viewer</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ä¸ä»…ä¼šæ ¹æ®æ‰€é—®çš„é—®é¢˜è°ƒæ•´å…¶å›åº”ï¼Œè¿˜ä¼šæ ¹æ®é—®é¢˜çš„æƒ…ç»ªè¡¨è¾¾æ¥è°ƒæ•´ã€‚æˆ‘ä»¬å¯¹æ¶µç›–äº‰è®®å’Œæ—¥å¸¸è¯é¢˜çš„156ä¸ªæç¤ºè¿›è¡Œäº†ç³»ç»Ÿçš„æƒ…æ„Ÿè¯­æ°”å˜åŒ–ï¼Œå¹¶åˆ†æäº†å…¶å¯¹æ¨¡å‹ååº”çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸ä¸­æ€§é—®é¢˜ç›¸æ¯”ï¼ŒGPT-4å¯¹å¸¦æœ‰è´Ÿé¢è¯­æ°”çš„é—®é¢˜å›åº”è´Ÿé¢çš„å¯èƒ½æ€§é™ä½äº†ä¸‰å€ã€‚è¿™è¡¨æ˜äº†ä¸€ç§â€œåå¼¹â€åè§ï¼Œå³æ¨¡å‹ä¼šè¿‡åº¦çº æ­£ï¼Œå¾€å¾€è½¬å‘ä¸­æ€§æˆ–æ­£é¢ã€‚åœ¨æ•æ„Ÿè¯é¢˜ï¼ˆå¦‚æ­£ä¹‰æˆ–æ”¿æ²»ï¼‰ä¸Šï¼Œè¿™ç§æ•ˆæœæ›´ä¸ºæ˜¾è‘—ï¼šåŸºäºè¯­æ°”çš„å˜åŒ–è¢«æŠ‘åˆ¶ï¼Œè¿™è¡¨æ˜äº†å¯¹é½è¦†ç›–ã€‚æˆ‘ä»¬å¼•å…¥äº†è¯¸å¦‚â€œè¯­æ°”ä¸‹é™â€ï¼ˆå›åº”è´Ÿé¢æ€§çš„ä¸‹é™ï¼‰ç­‰æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨è¯­æ°”æ•ˆä»·è¿‡æ¸¡çŸ©é˜µæ¥é‡åŒ–è¡Œä¸ºã€‚åŸºäº1536ç»´åµŒå…¥çš„è§†è§‰å±•ç¤ºè¯å®äº†åŸºäºè¯­æ°”çš„è¯­ä¹‰æ¼‚ç§»ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†ç”±æç¤ºä¸­çš„æƒ…ç»ªæ¡†æ¶é©±åŠ¨çš„ä¸€ç±»è¢«å¿½è§†çš„åè§ï¼Œè¿™å¯¹äººå·¥æ™ºèƒ½çš„å¯¹é½å’Œä¿¡ä»»æœ‰é‡è¦æ„ä¹‰ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bardolfranck/llm-responses-viewer%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bardolfranck/llm-responses-vieweræ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPT-4ä¸ä»…ä¼šæ ¹æ®æ‰€æçš„é—®é¢˜è°ƒæ•´å›åº”ï¼Œè¿˜ä¼šæ ¹æ®é—®é¢˜çš„æƒ…æ„Ÿè¡¨è¾¾æ¥è°ƒæ•´ã€‚ç ”ç©¶é€šè¿‡ç³»ç»Ÿåœ°æ”¹å˜156ä¸ªæç¤ºçš„æƒ…æ„ŸåŸºè°ƒï¼Œå‘ç°GPT-4å¯¹è´Ÿé¢é—®é¢˜çš„å›åº”æ¯”ä¸­æ€§é—®é¢˜æ›´å°‘å‡ºç°è´Ÿé¢ååº”ï¼Œå­˜åœ¨ä¸€ç§â€œåå¼¹â€åè§ã€‚åœ¨æ•æ„Ÿè¯é¢˜ä¸Šï¼Œè¿™ç§æ•ˆåº”æ›´åŠ æ˜æ˜¾ï¼Œæƒ…æ„ŸåŸºè°ƒçš„å˜åŒ–è¢«æŠ‘åˆ¶ã€‚ç ”ç©¶å¼•å…¥äº†â€œæƒ…æ„ŸåŸºè°ƒä¸‹é™â€ç­‰æ¦‚å¿µï¼Œå¹¶ä½¿ç”¨æƒ…æ„Ÿä»·å€¼è¿‡æ¸¡çŸ©é˜µæ¥é‡åŒ–æ¨¡å‹è¡Œä¸ºã€‚å¯è§†åŒ–ç»“æœè¯å®äº†åŸºäºæƒ…æ„ŸåŸºè°ƒçš„è¯­ä¹‰æ¼‚ç§»ç°è±¡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†æƒ…æ„Ÿæ¡†æ¶é©±åŠ¨çš„ä¸€ç±»åè§ï¼Œå¯¹äººå·¥æ™ºèƒ½çš„å¯¹é½å’Œä¿¡ä»»æœ‰é‡è¦å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPT-4ä¼šæ ¹æ®é—®é¢˜çš„æƒ…æ„Ÿè¡¨è¾¾è°ƒæ•´å›åº”ã€‚</li>
<li>GPT-4å¯¹è´Ÿé¢é—®é¢˜çš„å›åº”æ¯”ä¸­æ€§é—®é¢˜æ›´å°‘å‡ºç°è´Ÿé¢ååº”ï¼Œå­˜åœ¨â€œåå¼¹â€åè§ã€‚</li>
<li>åœ¨æ•æ„Ÿè¯é¢˜ä¸Šï¼Œæƒ…æ„ŸåŸºè°ƒçš„å˜åŒ–è¢«æŠ‘åˆ¶ï¼Œæ˜¾ç¤ºæ¨¡å‹çš„å¯¹é½ä¼˜å…ˆçº§ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†â€œæƒ…æ„ŸåŸºè°ƒä¸‹é™â€æ¦‚å¿µï¼Œæè¿°æ¨¡å‹å›åº”çš„è´Ÿé¢ç¨‹åº¦æœ€ä½ç•Œé™ã€‚</li>
<li>ä½¿ç”¨æƒ…æ„Ÿä»·å€¼è¿‡æ¸¡çŸ©é˜µæ¥é‡åŒ–æ¨¡å‹è¡Œä¸ºã€‚</li>
<li>å¯è§†åŒ–ç»“æœè¯å®äº†åŸºäºæƒ…æ„ŸåŸºè°ƒçš„è¯­ä¹‰æ¼‚ç§»ç°è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a40716b38f8be280cc8d9c17450863a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f887fc0072bc70f460d19a1c45edc87.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-NLP-Assessing-Compressed-Multilingual-Transformers-across-Diverse-Language-Benchmarks"><a href="#Towards-Inclusive-NLP-Assessing-Compressed-Multilingual-Transformers-across-Diverse-Language-Benchmarks" class="headerlink" title="Towards Inclusive NLP: Assessing Compressed Multilingual Transformers   across Diverse Language Benchmarks"></a>Towards Inclusive NLP: Assessing Compressed Multilingual Transformers   across Diverse Language Benchmarks</h2><p><strong>Authors:Maitha Alshehhi, Ahmed Sharshar, Mohsen Guizani</strong></p>
<p>Although LLMs have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models (LLMs) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as pruning and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive pruning significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨è¯¸å¦‚åçº³è¾¾è¯­å’Œé˜¿æ‹‰ä¼¯è¯­ç­‰ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„èƒ½åŠ›å°šæœªå®Œå…¨äº†è§£ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°é˜¿æ‹‰ä¼¯è¯­ã€è‹±è¯­å’Œå°åº¦è¯­è¨€ä¸­çš„å¤šè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå•è¯­è¨€æ¨¡å‹åœ¨å„æ–¹é¢çš„è¡¨ç°ï¼Œç‰¹åˆ«å¼ºè°ƒæ¨¡å‹å‹ç¼©ç­–ç•¥çš„å½±å“ï¼Œå¦‚ä¿®å‰ªå’Œé‡åŒ–å¤„ç†ã€‚æˆ‘ä»¬å‘ç°æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚æºäºè¯­è¨€å­¦å¤šæ ·æ€§å’Œèµ„æºå¯ç”¨æ€§å¯¹å½“å‰å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BLOOMZã€AceGPTã€Jaisã€LLaMA-2ã€XGLMå’ŒAraGPT2ï¼‰çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°å¤šè¯­è¨€ç‰ˆæœ¬çš„æ¨¡å‹åœ¨æ‰€æœ‰æ–¹é¢éƒ½è¶…è¿‡äº†ç‰¹å®šè¯­è¨€çš„åŒç±»æ¨¡å‹ï¼Œè¿™æ˜¾ç¤ºå‡ºè·¨è¯­è¨€è¿ç§»çš„å·¨å¤§ä¼˜åŠ¿ã€‚é‡åŒ–å¤„ç†ï¼ˆ4ä½å’Œ8ä½ï¼‰åœ¨å¤„ç†æ¨¡å‹ç²¾åº¦å’Œæé«˜æ•ˆç‡æ–¹é¢æœ‰æ•ˆï¼Œä½†è¿‡äºæ¿€çƒˆçš„ä¿®å‰ªä¼šä¸¥é‡å½±å“æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶æŒ‡å‡ºäº†æ„å»ºå¯æ‰©å±•å’Œå…¬å¹³çš„å¤šè¯­è¨€è‡ªç„¶è¯­è¨€å¤„ç†è§£å†³æ–¹æ¡ˆçš„å…³é”®ç­–ç•¥ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦åœ¨ä½èµ„æºç¯å¢ƒä¸­é‡‡å–å¹²é¢„æªæ–½æ¥è§£å†³è™šæ„å’Œè‡ªæˆ‘æ³›åŒ–é”™è¯¯çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19699v1">PDF</a> Published in the 3rd International Workshop on Generalizing from   Limited Resources in the Open World. Workshop at International Joint   Conference on Artificial Intelligence (IJCAI) 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é˜¿æ‹‰ä¼¯è¯­ã€è‹±è¯­å’Œå°åº¦è¯­ç­‰è¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°å·®å¼‚ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹å‹ç¼©ç­–ç•¥ï¼ˆå¦‚ä¿®å‰ªå’Œé‡åŒ–ï¼‰çš„å½±å“ã€‚ç ”ç©¶å‘ç°åœ¨å½“å‰çš„æœ€ä¼˜LLMæ¨¡å‹ä¸­ï¼Œç”±äºè¯­è¨€å¤šæ ·æ€§å’Œèµ„æºå¯ç”¨æ€§çš„å·®å¼‚ï¼Œè¡¨ç°å­˜åœ¨æ˜¾è‘—ä¸åŒã€‚å¤šè¯­ç§æ¨¡å‹é€šå¸¸è¡¨ç°ä¼˜äºå•è¯­ç§æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºäº†æ˜¾è‘—çš„è·¨è¯­è¨€è½¬ç§»ä¼˜åŠ¿ã€‚é‡åŒ–æŠ€æœ¯å¯ä»¥æœ‰æ•ˆä¿æŒæ¨¡å‹ç²¾åº¦å¹¶æé«˜æ•ˆç‡ï¼Œä½†è¿‡åº¦ä¿®å‰ªä¼šå¯¹æ¨¡å‹æ€§èƒ½é€ æˆæ˜¾è‘—å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹æ¨¡å‹ä¸­ã€‚ç ”ç©¶æå‡ºäº†æ„å»ºå¯æ‰©å±•å’Œå…¬å¹³çš„å¤šè¯­ç§è‡ªç„¶è¯­è¨€å¤„ç†è§£å†³æ–¹æ¡ˆçš„å…³é”®ç­–ç•¥ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦åœ¨ä½èµ„æºç¯å¢ƒä¸­è§£å†³å¹»è§‰å’Œæ³›åŒ–é”™è¯¯çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨é«˜èµ„æºè¯­è¨€ä¸­çš„æˆåŠŸå¹¶æœªå®Œå…¨æ‰©å±•åˆ°ä½èµ„æºè¯­è¨€ç¯å¢ƒå¦‚åçº³è¾¾è¯­å’Œé˜¿æ‹‰ä¼¯è¯­ã€‚</li>
<li>å¤šè¯­ç§LLMæ¨¡å‹åœ¨è·¨è¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°é€šå¸¸ä¼˜äºå•è¯­ç§æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å‹ç¼©ç­–ç•¥ï¼ˆå¦‚ä¿®å‰ªå’Œé‡åŒ–ï¼‰å¯¹LLMæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>é‡åŒ–æŠ€æœ¯å¯ä»¥æé«˜æ¨¡å‹æ•ˆç‡å¹¶ä¿æŒç²¾åº¦ï¼Œä½†è¿‡åº¦ä¿®å‰ªå¯èƒ½å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>è¯­è¨€å¤šæ ·æ€§å’Œèµ„æºå¯ç”¨æ€§å¯¹LLMæ€§èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚</li>
<li>åœ¨ä½èµ„æºç¯å¢ƒä¸­ï¼Œéœ€è¦è§£å†³å¹»è§‰å’Œæ³›åŒ–é”™è¯¯çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87c8cc97bd5f9e8ead62bb0523ca61ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c0c88b8f50c95ffd27c3ebd8f8475122.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Detection-of-Adverse-Drug-Events-in-Dutch-clinical-free-text-documents-using-Transformer-Models-benchmark-study"><a href="#Detection-of-Adverse-Drug-Events-in-Dutch-clinical-free-text-documents-using-Transformer-Models-benchmark-study" class="headerlink" title="Detection of Adverse Drug Events in Dutch clinical free text documents   using Transformer Models: benchmark study"></a>Detection of Adverse Drug Events in Dutch clinical free text documents   using Transformer Models: benchmark study</h2><p><strong>Authors:Rachel M. Murphy, Nishant Mishra, Nicolette F. de Keizer, Dave A. Dongelmans, Kitty J. Jager, Ameen Abu-Hanna, Joanna E. Klopotowska, Iacer Calixto</strong></p>
<p>In this study, we establish a benchmark for adverse drug event (ADE) detection in Dutch clinical free-text documents using several transformer models, clinical scenarios, and fit-for-purpose performance measures. We trained a Bidirectional Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and&#x2F;or multilingual encoder models (BERTje, RobBERT, MedRoBERTa(.)nl, and NuNER) for the tasks of named entity recognition (NER) and relation classification (RC) using 102 richly annotated Dutch ICU clinical progress notes. Anonymized free-text clinical progress notes of patients admitted to the intensive care unit (ICU) of one academic hospital and discharge letters of patients admitted to Internal Medicine wards of two non-academic hospitals were reused. We evaluated our ADE RC models internally using the gold standard (two-step task) and predicted entities (end-to-end task). In addition, all models were externally validated for detecting ADEs at the document level. We report both micro- and macro-averaged F1 scores, given the dataset imbalance in ADEs. Although differences for the ADE RC task between the models were small, MedRoBERTa(.)nl was the best performing model with a macro-averaged F1 score of 0.63 using the gold standard and 0.62 using predicted entities. The MedRoBERTa(.)nl models also performed the best in our external validation and achieved a recall of between 0.67 to 0.74 using predicted entities, meaning between 67 to 74% of discharge letters with ADEs were detected. Our benchmark study presents a robust and clinically meaningful approach for evaluating language models for ADE detection in clinical free-text documents. Our study highlights the need to use appropriate performance measures fit for the task of ADE detection in clinical free-text documents and envisioned future clinical use. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šç§transformeræ¨¡å‹ã€ä¸´åºŠåœºæ™¯ä»¥åŠé€‚åˆç›®çš„çš„ç»©æ•ˆè¡¡é‡æ ‡å‡†ï¼Œä¸ºåœ¨è·å…°ä¸´åºŠè‡ªç”±æ–‡æœ¬æ–‡ä»¶ä¸­æ£€æµ‹è¯ç‰©ä¸è‰¯äº‹ä»¶ï¼ˆADEï¼‰å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŒå‘é•¿çŸ­æ—¶è®°å¿†ï¼ˆBi-LSTMï¼‰æ¨¡å‹å’Œå››ä¸ªåŸºäºtransformerçš„è·å…°è¯­å’Œ&#x2F;æˆ–å¤šè¯­è¨€ç¼–ç å™¨æ¨¡å‹ï¼ˆBERTjeã€RobBERTã€MedRoBERTa(.)nlå’ŒNuNERï¼‰ï¼Œç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå…³ç³»åˆ†ç±»ï¼ˆRCï¼‰ä»»åŠ¡ã€‚è®­ç»ƒæ‰€ç”¨çš„æ•°æ®æ˜¯102ä»½ç»è¿‡ä¸°å¯Œæ³¨é‡Šçš„è·å…°é‡ç—‡ç›‘æŠ¤å®¤ä¸´åºŠç—…ç¨‹è®°å½•ã€‚æˆ‘ä»¬é‡æ–°ä½¿ç”¨äº†æŸå¤§å­¦åŒ»é™¢é‡ç—‡ç›‘æŠ¤å®¤æ‚£è€…çš„åŒ¿åè‡ªç”±æ–‡æœ¬ä¸´åºŠç—…ç¨‹è®°å½•å’Œä¸¤å®¶éå¤§å­¦åŒ»é™¢å†…ç§‘ç—…æˆ¿æ‚£è€…çš„å‡ºé™¢é€šçŸ¥ä¹¦ã€‚æˆ‘ä»¬ä½¿ç”¨é‡‘æ ‡å‡†ï¼ˆä¸¤æ­¥ä»»åŠ¡ï¼‰å’Œé¢„æµ‹å®ä½“ï¼ˆç«¯åˆ°ç«¯ä»»åŠ¡ï¼‰å¯¹å†…è¿›è¡Œäº†ADE RCæ¨¡å‹çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨æ–‡æ¡£çº§åˆ«ä¸Šå¯¹æ£€æµ‹ADEè¿›è¡Œäº†å¤–éƒ¨éªŒè¯ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å¾®å¹³å‡å’Œå®å¹³å‡F1åˆ†æ•°ï¼Œè€ƒè™‘åˆ°ADEæ•°æ®é›†çš„ä¸å¹³è¡¡æ€§ã€‚å°½ç®¡å„æ¨¡å‹ä¹‹é—´åœ¨ADE RCä»»åŠ¡ä¸Šçš„å·®å¼‚å¾ˆå°ï¼Œä½†MedRoBERTa(.)nlè¡¨ç°æœ€ä½³ï¼Œä½¿ç”¨é‡‘æ ‡å‡†çš„å®å¹³å‡F1åˆ†æ•°ä¸º0.63ï¼Œä½¿ç”¨é¢„æµ‹å®ä½“çš„å®å¹³å‡F1åˆ†æ•°ä¸º0.62ã€‚MedRoBERTa(.)nlæ¨¡å‹åœ¨å¤–éƒ¨éªŒè¯ä¸­ä¹Ÿè¡¨ç°æœ€ä½³ï¼Œä½¿ç”¨é¢„æµ‹å®ä½“çš„å¬å›ç‡ä¸º0.67è‡³0.74ï¼Œè¿™æ„å‘³ç€æœ‰67%è‡³74%çš„å«æœ‰ADEçš„å‡ºé™¢é€šçŸ¥ä¹¦è¢«æ£€æµ‹å‡ºæ¥ã€‚æˆ‘ä»¬çš„åŸºå‡†ç ”ç©¶ä¸ºè¯„ä¼°ç”¨äºä¸´åºŠè‡ªç”±æ–‡æœ¬æ–‡ä»¶ADEæ£€æµ‹çš„è¯­è¨€æ¨¡å‹æä¾›äº†ç¨³å¥å’Œä¸´åºŠä¸Šæœ‰æ„ä¹‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†ä½¿ç”¨é€‚åˆä¸´åºŠè‡ªç”±æ–‡æœ¬æ–‡ä»¶ADEæ£€æµ‹ä»»åŠ¡çš„é€‚å½“æ€§èƒ½è¡¡é‡æ ‡å‡†çš„å¿…è¦æ€§ï¼Œå¹¶è®¾æƒ³å°†æ¥ä¸´åºŠä½¿ç”¨çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19396v2">PDF</a> 30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).   Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.   Klopotowska and Iacer Calixto are shared last authors</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶å»ºç«‹äº†åŸºäºè·å…°è¯­ä¸´åºŠè‡ªç”±æ–‡æœ¬æ–‡æ¡£çš„ä¸è‰¯è¯ç‰©äº‹ä»¶ï¼ˆADEï¼‰æ£€æµ‹çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶è®­ç»ƒäº†åŒå‘é•¿çŸ­æ—¶è®°å¿†ï¼ˆBi-LSTMï¼‰æ¨¡å‹å’Œå››ç§åŸºäºè·å…°è¯­å’Œå¤šè¯­è¨€çš„transformerç¼–ç å™¨æ¨¡å‹ï¼ˆBERTjeã€RobBERTã€MedRoBERTa(.)nlå’ŒNuNERï¼‰ï¼Œç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å’Œå…³ç³»åˆ†ç±»ï¼ˆRCï¼‰ã€‚ä½¿ç”¨ä¸°å¯Œçš„è·å…°è¯­é‡ç—‡ç›‘æŠ¤ä¸´åºŠè®°å½•ç¬”è®°æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å„æ¨¡å‹ä¹‹é—´çš„å·®å¼‚è¾ƒå°ï¼Œä½†MedRoBERTa(.)nlåœ¨ADEå…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œä½¿ç”¨é‡‘æ ‡å‡†å’Œé¢„æµ‹å®ä½“çš„å®è§‚å¹³å‡F1åˆ†æ•°åˆ†åˆ«ä¸º0.63å’Œ0.62ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤–éƒ¨éªŒè¯ä¸­è¡¨ç°æœ€ä½³ï¼Œä½¿ç”¨é¢„æµ‹å®ä½“çš„å¬å›ç‡ä»‹äº0.67è‡³0.74ä¹‹é—´ã€‚æœ¬ç ”ç©¶ä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨è·å…°è¯­ä¸´åºŠè‡ªç”±æ–‡æœ¬æ–‡æ¡£ä¸­çš„ADEæ£€æµ‹èƒ½åŠ›æä¾›äº†ç¨³å¥ä¸”å®ç”¨çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶å»ºç«‹äº†åŸºäºè·å…°è¯­çš„ä¸´åºŠè‡ªç”±æ–‡æœ¬æ–‡æ¡£ADEæ£€æµ‹çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ä½¿ç”¨äº†Bi-LSTMæ¨¡å‹å’Œå››ç§transformerç¼–ç å™¨æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ç»è¿‡è®­ç»ƒå’Œè¯„ä¼°ï¼Œç”¨äºå‘½åå®ä½“è¯†åˆ«å’Œå…³ç³»åˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>MedRoBERTa(.)nlæ¨¡å‹åœ¨ADEå…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>é‡‘æ ‡å‡†å’Œé¢„æµ‹å®ä½“çš„å®è§‚å¹³å‡F1åˆ†æ•°åˆ†åˆ«ä¸º0.63å’Œ0.62ã€‚</li>
<li>å¤–éƒ¨éªŒè¯ä¸­ï¼Œä½¿ç”¨é¢„æµ‹å®ä½“çš„å¬å›ç‡ä»‹äº0.67è‡³0.74ä¹‹é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c71ad3466fd20217455b609f0a67be29.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Identifying-Fine-grained-Forms-of-Populism-in-Political-Discourse-A-Case-Study-on-Donald-Trumpâ€™s-Presidential-Campaigns"><a href="#Identifying-Fine-grained-Forms-of-Populism-in-Political-Discourse-A-Case-Study-on-Donald-Trumpâ€™s-Presidential-Campaigns" class="headerlink" title="Identifying Fine-grained Forms of Populism in Political Discourse: A   Case Study on Donald Trumpâ€™s Presidential Campaigns"></a>Identifying Fine-grained Forms of Populism in Political Discourse: A   Case Study on Donald Trumpâ€™s Presidential Campaigns</h2><p><strong>Authors:Ilias Chalkidis, Stephanie Brandl, Paris Aslanidis</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of instruction-following tasks, yet their grasp of nuanced social science concepts remains underexplored. This paper examines whether LLMs can identify and classify fine-grained forms of populism, a complex and contested concept in both academic and media debates. To this end, we curate and release novel datasets specifically designed to capture populist discourse. We evaluate a range of pre-trained (large) language models, both open-weight and proprietary, across multiple prompting paradigms. Our analysis reveals notable variation in performance, highlighting the limitations of LLMs in detecting populist discourse. We find that a fine-tuned RoBERTa classifier vastly outperforms all new-era instruction-tuned LLMs, unless fine-tuned. Additionally, we apply our best-performing model to analyze campaign speeches by Donald Trump, extracting valuable insights into his strategic use of populist rhetoric. Finally, we assess the generalizability of these models by benchmarking them on campaign speeches by European politicians, offering a lens into cross-context transferability in political discourse analysis. In this setting, we find that instruction-tuned LLMs exhibit greater robustness on out-of-domain data. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¾®å¦™çš„ç¤¾ä¼šç§‘å­¦æ¦‚å¿µä¸Šçš„æŒæ¡ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨LLMæ˜¯å¦èƒ½è¯†åˆ«å’Œåˆ†ç±»ç»†å¾®å½¢å¼çš„æ°‘ç²¹ä¸»ä¹‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å­¦æœ¯å’Œåª’ä½“è¾©è®ºä¸­å¤æ‚ä¸”æœ‰äº‰è®®çš„æ¦‚å¿µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç­–åˆ’å¹¶å‘å¸ƒäº†ä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰æ°‘ç²¹ä¸»ä¹‰è¯è¯­çš„æ–°æ•°æ®é›†ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—é¢„è®­ç»ƒçš„ï¼ˆå¤§å‹ï¼‰è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æ”¾æƒé‡å’Œä¸“æœ‰æ¨¡å‹ï¼Œä»¥åŠå¤šç§æç¤ºèŒƒå¼ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†æ€§èƒ½çš„æ˜¾è‘—å·®å¼‚ï¼Œçªå‡ºäº†LLMåœ¨æ£€æµ‹æ°‘ç²¹ä¸»ä¹‰è¯è¯­æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬å‘ç°ç»è¿‡å¾®è°ƒåçš„RoBERTaåˆ†ç±»å™¨å¤§å¤§ä¼˜äºæ‰€æœ‰æ–°æ—¶ä»£æŒ‡ä»¤è°ƒæ•´LLMï¼Œé™¤éä¹Ÿè¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åº”ç”¨è¡¨ç°æœ€ä½³çš„æ¨¡å‹åˆ†æå”çº³å¾·Â·ç‰¹æœ—æ™®çš„ç«é€‰æ¼”è®²ï¼Œæ·±å…¥æ´å¯Ÿä»–æˆ˜ç•¥æ€§åœ°ä½¿ç”¨æ°‘ç²¹ä¸»ä¹‰ä¿®è¾çš„æ‰‹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†æ¨¡å‹åŸºå‡†æµ‹è¯•ç½®äºæ¬§æ´²æ”¿æ²»å€™é€‰äººçš„æ¼”è®²ä¸Šï¼Œè¯„ä¼°è¿™äº›æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ”¿æ²»è¯è¯­åˆ†æçš„è·¨è¯­å¢ƒå¯è¿ç§»æ€§æä¾›è§†è§’ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‘ç°æŒ‡ä»¤è°ƒæ•´çš„LLMåœ¨è·¨åŸŸæ•°æ®ä¸Šè¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19303v1">PDF</a> Pre-print</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æŠŠæ¡å¾®å¦™çš„ç¤¾ä¼šç§‘å­¦æ¦‚å¿µæ–¹é¢ä»å¾…æ¢ç´¢ã€‚æœ¬æ–‡ç ”ç©¶LLMæ˜¯å¦èƒ½è¯†åˆ«å’Œåˆ†ç±»ç²¾ç»†ç²’åº¦çš„æ°‘ç²¹ä¸»ä¹‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å­¦æœ¯å’Œåª’ä½“è¾©è®ºä¸­å¤æ‚ä¸”å…·äº‰è®®çš„æ¦‚å¿µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç­–åˆ’å¹¶å‘å¸ƒäº†ä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰æ°‘ç²¹ä¸»ä¹‰è¯è¯­çš„æ–°æ•°æ®é›†ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—é¢„è®­ç»ƒçš„ï¼ˆå¤§å‹ï¼‰è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œä»¥åŠå¤šç§æç¤ºèŒƒå¼ã€‚åˆ†æè¡¨æ˜æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œçªæ˜¾äº†LLMåœ¨æ£€æµ‹æ°‘ç²¹ä¸»ä¹‰è¯è¯­æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬å‘ç°ç»è¿‡ç²¾ç»†è°ƒæ•´çš„RoBERTaåˆ†ç±»å™¨å¤§å¤§ä¼˜äºæ‰€æœ‰æ–°æ—¶ä»£æŒ‡ä»¤è°ƒæ•´LLMï¼Œé™¤éå®ƒä»¬ä¹Ÿç»è¿‡ç²¾ç»†è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†è¡¨ç°æœ€ä½³çš„æ¨¡å‹åº”ç”¨äºåˆ†æå”çº³å¾·Â·ç‰¹æœ—æ™®çš„ç«é€‰æ¼”è®²ï¼Œæ·±å…¥äº†è§£ä»–æˆ˜ç•¥æ€§ä½¿ç”¨æ°‘ç²¹ä¸»ä¹‰ä¿®è¾çš„æ‰‹æ³•ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å°†è¿™äº›æ¨¡å‹åŸºå‡†æµ‹è¯•äºæ¬§æ´²æ”¿æ²»å®¶çš„ç«é€‰æ¼”è®²ï¼Œè¯„ä¼°äº†è¿™äº›æ¨¡å‹çš„è·¨æƒ…å¢ƒå¯è¿ç§»æ€§ï¼Œå‘ç°åœ¨è·¨åŸŸæ•°æ®ä¸Šï¼ŒæŒ‡ä»¤è°ƒæ•´çš„LLMè¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¹¿æ³›çš„æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨ç†è§£å’Œå¤„ç†ç¤¾ä¼šç§‘å­¦çš„ç»†å¾®æ¦‚å¿µï¼ˆå¦‚æ°‘ç²¹ä¸»ä¹‰ï¼‰æ–¹é¢ä»éœ€è¦æå‡ã€‚</li>
<li>æ–°æ•°æ®é›†è¢«å¼€å‘å¹¶ç”¨äºæ•æ‰æ°‘ç²¹ä¸»ä¹‰è¯è¯­çš„ç‰¹ç‚¹ï¼Œä»¥æ”¯æŒå¯¹LLMçš„ç ”ç©¶ã€‚</li>
<li>ä¸åŒLLMæ¨¡å‹åœ¨è¯†åˆ«å’Œåˆ†ç±»æ°‘ç²¹ä¸»ä¹‰æ–¹é¢çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>ç»è¿‡ç²¾ç»†è°ƒæ•´çš„RoBERTaåˆ†ç±»å™¨åœ¨æ°‘ç²¹ä¸»ä¹‰è¯è¯­æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å¯¹ç‰¹æœ—æ™®ç«é€‰æ¼”è®²çš„åˆ†ææ­ç¤ºäº†æ°‘ç²¹ä¸»ä¹‰ä¿®è¾çš„æˆ˜ç•¥ä½¿ç”¨ã€‚</li>
<li>LLMåœ¨è·¨æƒ…å¢ƒè¿ç§»èƒ½åŠ›æ–¹é¢æœ‰æ‰€è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ææ¬§æ´²æ”¿æ²»å®¶ç«é€‰æ¼”è®²æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1fea62d6e4cb88972e972bb6cfd350a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-107494a89170ae1177b695971275c385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e935140a37f74babf05482a917cc3f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ed41ff5c70cf2faf1278f6bf1f1572e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d7495d3ec56678d4f7c7ae79d90912b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c8d6d8b7d560aee753e72851342cf9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MGHFT-Multi-Granularity-Hierarchical-Fusion-Transformer-for-Cross-Modal-Sticker-Emotion-Recognition"><a href="#MGHFT-Multi-Granularity-Hierarchical-Fusion-Transformer-for-Cross-Modal-Sticker-Emotion-Recognition" class="headerlink" title="MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal   Sticker Emotion Recognition"></a>MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal   Sticker Emotion Recognition</h2><p><strong>Authors:Jian Chen, Yuxuan Hu, Haifeng Lu, Wei Wang, Min Yang, Chengming Li, Xiping Hu</strong></p>
<p>Although pre-trained visual models with text have demonstrated strong capabilities in visual feature extraction, sticker emotion understanding remains challenging due to its reliance on multi-view information, such as background knowledge and stylistic cues. To address this, we propose a novel multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view sticker interpreter based on Multimodal Large Language Models. Specifically, inspired by the human ability to interpret sticker emotions from multiple views, we first use Multimodal Large Language Models to interpret stickers by providing rich textual context via multi-view descriptions. Then, we design a hierarchical fusion strategy to fuse the textual context into visual understanding, which builds upon a pyramid visual transformer to extract both global and local sticker features at multiple stages. Through contrastive learning and attention mechanisms, textual features are injected at different stages of the visual backbone, enhancing the fusion of global- and local-granularity visual semantics with textual guidance. Finally, we introduce a text-guided fusion attention mechanism to effectively integrate the overall multimodal features, enhancing semantic understanding. Extensive experiments on 2 public sticker emotion datasets demonstrate that MGHFT significantly outperforms existing sticker emotion recognition approaches, achieving higher accuracy and more fine-grained emotion recognition. Compared to the best pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4% on F1 and 4.0% on accuracy. The code is released at <a target="_blank" rel="noopener" href="https://github.com/cccccj-03/MGHFT_ACMMM2025">https://github.com/cccccj-03/MGHFT_ACMMM2025</a>. </p>
<blockquote>
<p>å°½ç®¡é¢„è®­ç»ƒçš„å¸¦æœ‰æ–‡æœ¬è§†è§‰æ¨¡å‹åœ¨è§†è§‰ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶ä¾èµ–äºå¤šè§†å›¾ä¿¡æ¯ï¼ˆå¦‚èƒŒæ™¯çŸ¥è¯†å’Œé£æ ¼çº¿ç´¢ï¼‰ï¼Œè´´çº¸æƒ…æ„Ÿç†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šç²’åº¦å±‚æ¬¡èåˆå˜å‹å™¨ï¼ˆMGHFTï¼‰ï¼Œå¹¶åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº†ä¸€ä¸ªå¤šè§†å›¾è´´çº¸è§£é‡Šå™¨ã€‚å…·ä½“è€Œè¨€ï¼Œå—åˆ°äººç±»ä»å¤šä¸ªè§†è§’è§£è¯»è´´çº¸æƒ…æ„Ÿçš„èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¤šè§†å›¾æè¿°æ¥ä¸°å¯Œæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œä»è€Œè§£é‡Šè´´çº¸ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å±‚æ¬¡èåˆç­–ç•¥ï¼Œå°†æ–‡æœ¬ä¸Šä¸‹æ–‡èåˆåˆ°è§†è§‰ç†è§£ä¸­ï¼Œè¯¥ç­–ç•¥å»ºç«‹åœ¨é‡‘å­—å¡”è§†è§‰å˜å‹å™¨ä¹‹ä¸Šï¼Œåœ¨å¤šé˜¶æ®µæå–è´´çº¸çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ–‡æœ¬ç‰¹å¾åœ¨ä¸åŒçš„è§†è§‰ä¸»å¹²é˜¶æ®µè¢«æ³¨å…¥ï¼Œå¢å¼ºäº†å…¨å±€å’Œå±€éƒ¨ç²’åº¦çš„è§†è§‰è¯­ä¹‰ä¸æ–‡æœ¬æŒ‡å¯¼çš„èåˆã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆæ•´ä½“çš„å¤šæ¨¡æ€ç‰¹å¾ï¼Œå¢å¼ºè¯­ä¹‰ç†è§£ã€‚åœ¨2ä¸ªå…¬å…±è´´çº¸æƒ…ç»ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMGHFTæ˜¾è‘—ä¼˜äºç°æœ‰çš„è´´çº¸æƒ…ç»ªè¯†åˆ«æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ç²¾ç»†çš„æƒ…ç»ªè¯†åˆ«ã€‚ä¸æœ€ä½³é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„MGHFTåœ¨F1ä¸Šæé«˜äº†5.4%ï¼Œåœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†4.0%ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/cccccj-03/MGHFT_ACMMM2025%E3%80%82">https://github.com/cccccj-03/MGHFT_ACMMM2025ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18929v1">PDF</a> Accepted by ACMMM2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šç²’åº¦å±‚æ¬¡èåˆå˜æ¢å™¨ï¼ˆMGHFTï¼‰çš„è´´çº¸æƒ…ç»ªè¯†åˆ«æ–¹æ³•ï¼Œç»“åˆåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è·¨è§†å›¾è´´çº¸è§£æå™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»ä»å¤šä¸ªè§’åº¦è§£è¯»è´´çº¸æƒ…ç»ªçš„èƒ½åŠ›ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¤šè§†å›¾æè¿°æä¾›ä¸°å¯Œçš„æ–‡æœ¬ä¸Šä¸‹æ–‡æ¥è§£æè´´çº¸ã€‚è®¾è®¡äº†ä¸€ç§å±‚æ¬¡èåˆç­–ç•¥ï¼Œå°†æ–‡æœ¬ä¸Šä¸‹æ–‡èå…¥è§†è§‰ç†è§£ä¸­ï¼Œå»ºç«‹äºé‡‘å­—å¡”è§†è§‰å˜æ¢å™¨ä¸Šï¼Œåœ¨å¤šä¸ªé˜¶æ®µæå–è´´çº¸çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ä¸åŒé˜¶æ®µæ³¨å…¥æ–‡æœ¬ç‰¹å¾ï¼Œå¢å¼ºå…¨å±€å’Œå±€éƒ¨ç²’åº¦è§†è§‰è¯­ä¹‰ä¸æ–‡æœ¬æŒ‡å¯¼çš„èåˆã€‚å¼•å…¥æ–‡æœ¬å¼•å¯¼èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°æ•´åˆæ•´ä½“å¤šæ¨¡æ€ç‰¹å¾ï¼Œå¢å¼ºè¯­ä¹‰ç†è§£ã€‚åœ¨å…¬å…±è´´çº¸æƒ…ç»ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMGHFTæ˜¾è‘—ä¼˜äºç°æœ‰è´´çº¸æƒ…ç»ªè¯†åˆ«æ–¹æ³•ï¼Œå®ç°æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ç²¾ç»†çš„æƒ…ç»ªè¯†åˆ«ã€‚ä¸æœ€ä½³é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ç›¸æ¯”ï¼ŒMGHFTåœ¨F1å¾—åˆ†ä¸Šæé«˜äº†5.4%ï¼Œå‡†ç¡®ç‡æé«˜äº†4.0%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è´´çº¸æƒ…ç»ªç†è§£ä¾èµ–äºå¤šè§†å›¾ä¿¡æ¯ï¼Œå¦‚èƒŒæ™¯çŸ¥è¯†å’Œé£æ ¼çº¿ç´¢ï¼Œä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šç²’åº¦å±‚æ¬¡èåˆå˜æ¢å™¨ï¼ˆMGHFTï¼‰çš„æ–°æ–¹æ³•ï¼Œç»“åˆå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè´´çº¸æƒ…ç»ªè¯†åˆ«ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿäººç±»èƒ½åŠ›ï¼Œä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡å¤šè§†å›¾æè¿°æä¾›ä¸°å¯Œçš„æ–‡æœ¬ä¸Šä¸‹æ–‡æ¥è§£æè´´çº¸æƒ…ç»ªã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§å±‚æ¬¡èåˆç­–ç•¥ï¼Œå°†æ–‡æœ¬ä¸Šä¸‹æ–‡èå…¥è§†è§‰ç†è§£ä¸­ï¼Œå¹¶åœ¨å¤šä¸ªé˜¶æ®µæå–è´´çº¸çš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶å¢å¼ºå…¨å±€å’Œå±€éƒ¨ç²’åº¦è§†è§‰è¯­ä¹‰ä¸æ–‡æœ¬æŒ‡å¯¼çš„èåˆã€‚</li>
<li>å¼•å…¥æ–‡æœ¬å¼•å¯¼èåˆæ³¨æ„åŠ›æœºåˆ¶æ¥æœ‰æ•ˆåœ°æ•´åˆæ•´ä½“å¤šæ¨¡æ€ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4bd3a6f052d4b4c3856da6baa909cd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d50b6f5972a9ac5002de5a5a4b09de2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-001d7a747c71b93f060f121ea9b3eac0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd912c19645d697aaf4de103e5414094.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a10d8767c80b567f500733c994bc6723.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79e29d1a47dce61323bf664bf38ff020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7bacbb60db0e0853b072082fb789688.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AQuilt-Weaving-Logic-and-Self-Inspection-into-Low-Cost-High-Relevance-Data-Synthesis-for-Specialist-LLMs"><a href="#AQuilt-Weaving-Logic-and-Self-Inspection-into-Low-Cost-High-Relevance-Data-Synthesis-for-Specialist-LLMs" class="headerlink" title="AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance   Data Synthesis for Specialist LLMs"></a>AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance   Data Synthesis for Specialist LLMs</h2><p><strong>Authors:Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang</strong></p>
<p>Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at <a target="_blank" rel="noopener" href="https://github.com/Krueske/AQuilt">https://github.com/Krueske/AQuilt</a>. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ•°æ®åˆæˆæ–¹æ³•ï¼Œå¹¶ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®æ•è·ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ï¼Œä»è€Œäº§ç”Ÿäº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè¦ä¹ˆå—åˆ°æ€§èƒ½é™åˆ¶çš„å½±å“ï¼Œå¹¶ä¸”åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºä¸è¶³çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AQuiltæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ä»ç›¸åº”çš„æ— æ ‡ç­¾æ•°æ®ä¸­æ„å»ºé€‚ç”¨äºä»»ä½•ä¸“ä¸šé¢†åŸŸçš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ŒåŒ…æ‹¬ç­”æ¡ˆã€é—®é¢˜ã€æ— æ ‡ç­¾æ•°æ®ã€æ£€æŸ¥ã€é€»è¾‘å’Œä»»åŠ¡ç±»å‹ã€‚é€šè¿‡ç»“åˆé€»è¾‘å’Œæ£€æŸ¥ï¼Œæˆ‘ä»¬é¼“åŠ±æ¨ç†è¿‡ç¨‹å’Œè‡ªæˆ‘æ£€æŸ¥ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯å®šåˆ¶çš„ä»»åŠ¡æŒ‡ä»¤èƒ½å¤Ÿå®ç°ä»»ä½•ä»»åŠ¡çš„é«˜è´¨é‡æ•°æ®ç”Ÿæˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«703kç¤ºä¾‹çš„æ•°æ®é›†æ¥è®­ç»ƒå¼ºå¤§çš„æ•°æ®åˆæˆæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒAQuiltä¸DeepSeek-V3ç›¸å½“ï¼Œä½†ä»…ä½¿ç”¨äº†å…¶ç”Ÿäº§æˆæœ¬çš„17%ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„æ•°æ®ä¸ä¸‹æ¸¸ä»»åŠ¡çš„ç›¸å…³æ€§æ›´é«˜ã€‚æºä»£ç ã€æ¨¡å‹å’Œè„šæœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Krueske/AQuilt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Krueske/AQuiltæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18584v1">PDF</a> 32 pages, 4 figures</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸè¡¨ç°ä¸ä½³ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–æ•°æ®åˆæˆæ–¹æ³•ï¼Œä½¿ç”¨æœªæ ‡è®°æ•°æ®æ•è·ç‰¹å®šé¢†åŸŸç‰¹å¾ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æˆ–æ€§èƒ½æœ‰é™ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AQuiltæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½ä»å¯¹åº”çš„æœªæ ‡è®°æ•°æ®ä¸­ä¸ºä»»ä½•ä¸“ä¸šé¢†åŸŸæ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼ŒåŒ…æ‹¬ç­”æ¡ˆã€é—®é¢˜ã€æœªæ ‡è®°æ•°æ®ã€æ£€æŸ¥ã€é€»è¾‘å’Œä»»åŠ¡ç±»å‹ã€‚é€šè¿‡ç»“åˆé€»è¾‘å’Œæ£€æŸ¥ï¼Œæˆ‘ä»¬é¼“åŠ±æ¨ç†è¿‡ç¨‹å’Œè‡ªæˆ‘æ£€æŸ¥ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯å®šåˆ¶çš„ä»»åŠ¡æŒ‡ä»¤èƒ½å¤Ÿå®ç°ä»»ä½•ä»»åŠ¡çš„é«˜è´¨é‡æ•°æ®ç”Ÿæˆã€‚ä½¿ç”¨AQuiltæ„å»ºçš„æ•°æ®é›†è®­ç»ƒå¼ºå¤§æ•°æ®åˆæˆæ¨¡å‹ï¼Œå®éªŒè¡¨æ˜å…¶æ€§èƒ½ä¸DeepSeek-V3ç›¸å½“ï¼Œä½†ç”Ÿäº§æˆæœ¬ä»…ä¸ºå…¶17%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¸“ä¸šé¢†åŸŸå­˜åœ¨æ€§èƒ½çŸ­æ¿ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ•°æ®åˆæˆï¼Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®æ•è·ç‰¹å®šé¢†åŸŸç‰¹å¾ã€‚</li>
<li>AQuiltæ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„æŒ‘æˆ˜ï¼Œé€šè¿‡æ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>AQuiltç»“åˆé€»è¾‘å’Œæ£€æŸ¥ï¼Œé¼“åŠ±æ¨¡å‹è¿›è¡Œæ¨ç†å’Œè‡ªæˆ‘æ£€æŸ¥ã€‚</li>
<li>å®šåˆ¶åŒ–ä»»åŠ¡æŒ‡ä»¤èƒ½ç”Ÿæˆé«˜è´¨é‡æ•°æ®ï¼Œé€‚ç”¨äºå„ç§ä»»åŠ¡ã€‚</li>
<li>ä½¿ç”¨AQuiltæ„å»ºçš„æ•°æ®é›†è®­ç»ƒå‡ºçš„æ¨¡å‹æ€§èƒ½ä¸DeepSeek-V3ç›¸å½“ï¼Œä½†ç”Ÿäº§æˆæœ¬è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78806b5f0bf56d9f12b5642810a41601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90fe1a92dad6b6e2a32f2367d5e6d0ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4467f67d9d91fc74408c5334e8749783.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a38dcb6ab9dc0f9a5282ab15fa686e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea1f28de9cb3cae330639840cf7ab7cc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Restoring-Rhythm-Punctuation-Restoration-Using-Transformer-Models-for-Bangla-a-Low-Resource-Language"><a href="#Restoring-Rhythm-Punctuation-Restoration-Using-Transformer-Models-for-Bangla-a-Low-Resource-Language" class="headerlink" title="Restoring Rhythm: Punctuation Restoration Using Transformer Models for   Bangla, a Low-Resource Language"></a>Restoring Rhythm: Punctuation Restoration Using Transformer Models for   Bangla, a Low-Resource Language</h2><p><strong>Authors:Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu</strong></p>
<p>Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha &#x3D; 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.   Results show strong generalization to reference and ASR transcripts, demonstrating the modelâ€™s effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP. </p>
<blockquote>
<p>æ ‡ç‚¹ç¬¦å·ä¿®å¤æé«˜äº†æ–‡æœ¬çš„å¯è¯»æ€§ï¼Œå¯¹äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åå¤„ç†ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯å¯¹åƒå­ŸåŠ æ‹‰è¯­è¿™æ ·çš„ä½èµ„æºè¯­è¨€è€Œè¨€ï¼Œè‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºè½¬æ¢æ¨¡å‹çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯XLM-RoBERTa-largeï¼Œæ¥è‡ªåŠ¨ä¿®å¤æœªåŠ æ ‡ç‚¹ç¬¦å·çš„å­ŸåŠ æ‹‰è¯­æ–‡æœ¬ã€‚æˆ‘ä»¬ä¸“æ³¨äºé¢„æµ‹å››ä¸ªæ ‡ç‚¹ç¬¦å·ï¼šå¥å·ã€é€—å·ã€é—®å·å’Œæ„Ÿå¹å·ï¼Œæ¶µç›–ä¸åŒçš„æ–‡æœ¬é¢†åŸŸã€‚ä¸ºäº†è§£å†³æ ‡æ³¨èµ„æºç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹ã€å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶åº”ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä»¥alpha&#x3D;0.2%çš„å¢å¼ºç³»æ•°è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ–°é—»æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º97.1%ï¼Œåœ¨å‚è€ƒé›†ä¸Šä¸º91.2%ï¼Œåœ¨ASRé›†ä¸Šä¸º90.2%ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å‚è€ƒå’ŒASRè½¬å½•æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›å¼ºï¼Œè¯æ˜å…¶åœ¨ç°å®ä¸–ç•Œçš„å˜ˆæ‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºå­ŸåŠ æ‹‰è¯­æ ‡ç‚¹ç¬¦å·ä¿®å¤å»ºç«‹äº†åšå®çš„åŸºå‡†ï¼Œå¹¶æä¾›äº†å…¬å¼€å¯ç”¨çš„æ•°æ®é›†å’Œä»£ç ï¼Œä»¥æ”¯æŒæœªæ¥ä½èµ„æºNLPçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶åº”ç”¨åŸºäºå˜å‹å™¨çš„æ¨¡å‹XLM-RoBERTa-largeå¯¹æœªåŠ æ ‡ç‚¹çš„å­ŸåŠ æ‹‰è¯­æ–‡æœ¬è¿›è¡Œè‡ªåŠ¨æ ‡ç‚¹æ¢å¤ï¼Œä»¥æé«˜æ–‡æœ¬çš„å¯è¯»æ€§ï¼Œå¹¶æ”¹å–„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åå¤„ç†ä»»åŠ¡ã€‚ç ”ç©¶æ„å»ºäº†å¤§å‹å¤šå…ƒè®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶åº”ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯è§£å†³æ ‡æ³¨èµ„æºç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€ä½³æ¨¡å‹åœ¨æ–°é—»æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®åº¦è¾¾åˆ°97.1%ï¼Œå‚è€ƒé›†ä¸Šä¸º91.2%ï¼Œè¯­éŸ³è¯†åˆ«é›†ä¸Šä¸º90.2%ã€‚ç»“æœè¯æ˜è¯¥æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„å˜ˆæ‚åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå­ŸåŠ æ‹‰è¯­æ ‡ç‚¹æ¢å¤å»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶ä¸ºä½èµ„æºè‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶æä¾›äº†å…¬å¼€å¯ç”¨çš„æ•°æ®é›†å’Œä»£ç æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨äºæ ‡ç‚¹æ¢å¤åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€å¦‚å­ŸåŠ æ‹‰è¯­ä¸­ã€‚</li>
<li>é‡‡ç”¨åŸºäºXLM-RoBERTa-largeæ¨¡å‹çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œæ ‡ç‚¹æ¢å¤ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†å¤§å‹å¤šå…ƒè®­ç»ƒè¯­æ–™åº“ä»¥è§£å†³æ ‡æ³¨èµ„æºç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æœ€ä½³æ¨¡å‹åœ¨æ–°é—»ã€å‚è€ƒå’Œè¯­éŸ³è¯†åˆ«ç­‰ä¸åŒæµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„å˜ˆæ‚åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81a2a3f456b4b4fbd23f974e6403590c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b73936c17d8f66ec2ffe670eef11bd11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f473f53deeea858c036aa8a1ad4b3a97.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Seq2Seq-Transformer-for-Predicting-Brain-Responses-to-Naturalistic-Stimuli"><a href="#A-Multimodal-Seq2Seq-Transformer-for-Predicting-Brain-Responses-to-Naturalistic-Stimuli" class="headerlink" title="A Multimodal Seq2Seq Transformer for Predicting Brain Responses to   Naturalistic Stimuli"></a>A Multimodal Seq2Seq Transformer for Predicting Brain Responses to   Naturalistic Stimuli</h2><p><strong>Authors:Qianyi He, Yuan Chang Leong</strong></p>
<p>The Algonauts 2025 Challenge called on the community to develop encoding models that predict whole-brain fMRI responses to naturalistic multimodal movies. In this submission, we propose a sequence-to-sequence Transformer that autoregressively predicts fMRI activity from visual, auditory, and language inputs. Stimulus features were extracted using pretrained models including VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information from prior brain states and current stimuli via dual cross-attention mechanisms that attend to both perceptual information extracted from the stimulus as well as narrative information provided by high-level summaries of the content. One core innovation of our approach is the use of sequences of multimodal context to predict sequences of brain activity, enabling the model to capture long-range temporal structure in both stimuli and neural responses. Another is the combination of a shared encoder with partial subject-specific decoder, which leverages common representational structure across subjects while accounting for individual variability. Our model achieves strong performance on both in-distribution and out-of-distribution data, demonstrating the effectiveness of temporally-aware, multimodal sequence modeling for brain activity prediction. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Angelneer926/Algonauts_challenge">https://github.com/Angelneer926/Algonauts_challenge</a>. </p>
<blockquote>
<p>é˜¿å°”è´¡çº³èŒ¨2025æŒ‘æˆ˜èµ›å·å¬ç¤¾åŒºå¼€å‘ç¼–ç æ¨¡å‹ï¼Œé¢„æµ‹å¤§è„‘å¯¹è‡ªç„¶ä¸»ä¹‰å¤šåª’ä½“ç”µå½±çš„fMRIå“åº”ã€‚åœ¨æœ¬æ¬¡æäº¤ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åºåˆ—åˆ°åºåˆ—çš„è½¬æ¢å™¨ï¼Œè¯¥è½¬æ¢å™¨èƒ½å¤Ÿæ ¹æ®è§†è§‰ã€å¬è§‰å’Œè¯­è¨€è¾“å…¥è¿›è¡Œè‡ªåŠ¨å›å½’é¢„æµ‹fMRIæ´»åŠ¨ã€‚åˆºæ¿€ç‰¹å¾æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–çš„ï¼ŒåŒ…æ‹¬VideoMAEã€HuBERTã€Qwenå’ŒBridgeTowerã€‚è§£ç å™¨é€šè¿‡åŒé‡äº¤å‰æ³¨æ„æœºåˆ¶æ•´åˆæ¥è‡ªå…ˆå‰å¤§è„‘çŠ¶æ€å’Œå½“å‰åˆºæ¿€çš„ä¿¡æ¯ï¼Œè¿™äº›æœºåˆ¶æ—¢å…³æ³¨ä»åˆºæ¿€ä¸­æå–çš„æ„ŸçŸ¥ä¿¡æ¯ï¼Œä¹Ÿå…³æ³¨ç”±å†…å®¹é«˜çº§æ‘˜è¦æä¾›çš„å™äº‹ä¿¡æ¯ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€æ˜¯ä½¿ç”¨å¤šæ¨¡å¼ä¸Šä¸‹æ–‡åºåˆ—æ¥é¢„æµ‹å¤§è„‘æ´»åŠ¨åºåˆ—ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆºæ¿€å’Œç¥ç»ååº”ä¸­çš„é•¿æœŸæ—¶é—´ç»“æ„ã€‚å¦ä¸€é¡¹åˆ›æ–°æ˜¯ç»“åˆä½¿ç”¨å…±äº«ç¼–ç å™¨å’Œéƒ¨åˆ†ç‰¹å®šä¸»é¢˜çš„è§£ç å™¨ï¼Œè¿™æ—¢å¯ä»¥åˆ©ç”¨è·¨ä¸»é¢˜çš„å¸¸è§ä»£è¡¨æ€§ç»“æ„ï¼Œåˆå¯ä»¥è€ƒè™‘ä¸ªä½“å·®å¼‚ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®ä¸Šéƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¯æ˜äº†æ—¶é—´æ„ŸçŸ¥ã€å¤šæ¨¡å¼åºåˆ—å»ºæ¨¡åœ¨é¢„æµ‹å¤§è„‘æ´»åŠ¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Angelneer926/Algonauts_challenge%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Angelneer926/Algonauts_challengeæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18104v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Algonauts 2025æŒ‘æˆ˜èµ›ç¤¾åŒºå¼€å‘ç¼–ç æ¨¡å‹çš„æƒ…å†µã€‚è¯¥ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åºåˆ—åˆ°åºåˆ—çš„Transformeræ¨¡å‹ï¼Œå¯ä»¥è‡ªåŠ¨é¢„æµ‹è‡ªç„¶ä¸»ä¹‰å¤šæ¨¡æ€ç”µå½±å¼•å‘çš„å…¨è„‘fMRIå“åº”ã€‚è¯¥ç ”ç©¶é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æå–åˆºæ¿€ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è§£ç å™¨æ•´åˆå…ˆå‰è„‘çŠ¶æ€å’Œå½“å‰åˆºæ¿€çš„ä¿¡æ¯ã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºä½¿ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åºåˆ—é¢„æµ‹è„‘æ´»åŠ¨åºåˆ—ï¼Œèƒ½å¤Ÿæ•æ‰åˆºæ¿€å’Œç¥ç»ååº”çš„é•¿æœŸæ—¶é—´ç»“æ„ã€‚è¯¥æ¨¡å‹åœ¨å†…éƒ¨æ•°æ®å’Œå¤–éƒ¨æ•°æ®ä¸Šéƒ½è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ï¼Œè¯æ˜äº†æ—¶åºæ„ŸçŸ¥ã€å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡åœ¨é¢„æµ‹è„‘æ´»åŠ¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Algonauts 2025æŒ‘æˆ˜èµ›æ—¨åœ¨å¼€å‘èƒ½å¤Ÿé¢„æµ‹è‡ªç„¶ä¸»ä¹‰å¤šæ¨¡æ€ç”µå½±å¼•å‘çš„å…¨è„‘fMRIå“åº”çš„ç¼–ç æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åºåˆ—åˆ°åºåˆ—çš„Transformeræ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹æå–åˆºæ¿€ç‰¹å¾ï¼ŒåŒ…æ‹¬è§†é¢‘ã€éŸ³é¢‘å’Œè¯­è¨€è¾“å…¥ã€‚</li>
<li>è§£ç å™¨æ•´åˆäº†å…ˆå‰è„‘çŠ¶æ€å’Œå½“å‰åˆºæ¿€çš„ä¿¡æ¯ï¼Œé€šè¿‡åŒé‡äº¤å‰æ³¨æ„æœºåˆ¶å…³æ³¨åˆºæ¿€ä¸­çš„æ„ŸçŸ¥ä¿¡æ¯ä»¥åŠå†…å®¹çš„é«˜çº§æ‘˜è¦ä¸­çš„å™äº‹ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€æ˜¯ä½¿ç”¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡åºåˆ—é¢„æµ‹è„‘æ´»åŠ¨åºåˆ—ï¼Œèƒ½å¤Ÿæ•æ‰åˆºæ¿€å’Œç¥ç»ååº”çš„é•¿æœŸæ—¶é—´ç»“æ„ã€‚</li>
<li>æ¨¡å‹ç»“åˆäº†å…±äº«ç¼–ç å™¨ä¸éƒ¨åˆ†ç‰¹å®šä¸»é¢˜çš„è§£ç å™¨ï¼Œåˆ©ç”¨è·¨ä¸»é¢˜çš„ä»£è¡¨æ€§ç»“æ„ï¼ŒåŒæ—¶è€ƒè™‘ä¸ªä½“å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-095e5b6482718735b8006c888af96e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aece3cf84e39655bacec3b77d50673f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24367b253274d174d46b2e3f07d46612.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-236d0f90d740c43c66b8f7d2efc1221b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Datasets-and-Recipes-for-Video-Temporal-Grounding-via-Reinforcement-Learning"><a href="#Datasets-and-Recipes-for-Video-Temporal-Grounding-via-Reinforcement-Learning" class="headerlink" title="Datasets and Recipes for Video Temporal Grounding via Reinforcement   Learning"></a>Datasets and Recipes for Video Temporal Grounding via Reinforcement   Learning</h2><p><strong>Authors:Ruizhe Chen, Zhiting Fan, Tianze Luo, Heqing Zou, Zhaopeng Feng, Guiyang Xie, Hansheng Zhang, Zhuochen Wang, Zuozhu Liu, Huaijian Zhang</strong></p>
<p>Video Temporal Grounding (VTG) aims to localize relevant temporal segments in videos given natural language queries. Despite recent progress with large vision-language models (LVLMs) and instruction-tuning, existing approaches often suffer from limited temporal awareness and poor generalization. In this work, we introduce a two-stage training framework that integrates supervised fine-tuning with reinforcement learning (RL) to improve both the accuracy and robustness of VTG models. Our approach first leverages high-quality curated cold start data for SFT initialization, followed by difficulty-controlled RL to further enhance temporal localization and reasoning abilities. Comprehensive experiments on multiple VTG benchmarks demonstrate that our method consistently outperforms existing models, particularly in challenging and open-domain scenarios. We conduct an in-depth analysis of training strategies and dataset curation, highlighting the importance of both high-quality cold start data and difficulty-controlled RL. To facilitate further research and industrial adoption, we release all intermediate datasets, models, and code to the community. </p>
<blockquote>
<p>è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„ç›¸å…³æ—¶åºç‰‡æ®µã€‚å°½ç®¡æœ€è¿‘ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å’ŒæŒ‡ä»¤å¾®è°ƒå–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨æ—¶åºæ„ŸçŸ¥èƒ½åŠ›æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œä»¥æé«˜VTGæ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨é«˜è´¨é‡çš„ç²¾é€‰å†·å¯åŠ¨æ•°æ®è¿›è¡ŒSFTåˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡éš¾åº¦æ§åˆ¶çš„RLæ¥è¿›ä¸€æ­¥å¢å¼ºæ—¶åºå®šä½å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªVTGåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€ç›´ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¼€æ”¾é¢†åŸŸçš„åœºæ™¯ä¸­ã€‚æˆ‘ä»¬å¯¹è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†ç¼–åˆ¶è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¼ºè°ƒäº†é«˜è´¨é‡å†·å¯åŠ¨æ•°æ®å’Œéš¾åº¦æ§åˆ¶RLçš„é‡è¦æ€§ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ï¼Œæˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒæ‰€æœ‰ä¸­é—´æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18100v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘æ—¶åºå®šä½ï¼ˆVTGï¼‰æ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„ç›¸å…³æ—¶åºç‰‡æ®µã€‚å°½ç®¡è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å’ŒæŒ‡ä»¤å¾®è°ƒå–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€å­˜åœ¨æ—¶åºæ„ŸçŸ¥èƒ½åŠ›æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå°†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸ç»“åˆï¼Œæ—¨åœ¨æé«˜VTGæ¨¡å‹çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨é«˜è´¨é‡é¢„è®¾å†·å¯åŠ¨æ•°æ®è¿›è¡ŒSFTåˆå§‹åŒ–ï¼Œç„¶åé€šè¿‡éš¾åº¦æ§åˆ¶çš„RLè¿›ä¸€æ­¥æ”¹å–„æ—¶åºå®šä½å’Œæ¨ç†èƒ½åŠ›ã€‚åœ¨å¤šä¸ªVTGåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æŒç»­ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå°¤å…¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¼€æ”¾é¢†åŸŸçš„åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚æˆ‘ä»¬å¯¹è®­ç»ƒç­–ç•¥å’Œæ•°æ®é›†æ„å»ºè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¼ºè°ƒäº†é«˜è´¨é‡å†·å¯åŠ¨æ•°æ®å’Œéš¾åº¦æ§åˆ¶RLçš„é‡è¦æ€§ã€‚ä¸ºä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ï¼Œæˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒäº†æ‰€æœ‰ä¸­é—´æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VTGæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢å®šä½è§†é¢‘ä¸­çš„ç›¸å…³æ—¶åºç‰‡æ®µã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ—¶åºæ„ŸçŸ¥èƒ½åŠ›æœ‰é™å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå·®çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œç»“åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨é«˜è´¨é‡é¢„è®¾å†·å¯åŠ¨æ•°æ®è¿›è¡ŒSFTåˆå§‹åŒ–ã€‚</li>
<li>é€šè¿‡éš¾åº¦æ§åˆ¶çš„RLè¿›ä¸€æ­¥æ”¹å–„æ—¶åºå®šä½å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13ffb93c299197274e3bcf6e6c5ce32e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e1d41472feccaaefcf4d72b652623f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbec2c7fc47ac250444f7b1e948b503f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa697dc5b06c1ab6c61ba375bede2fef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d4a7165d5fcfffcac1f9b30f633d70e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89ad8fc466917d0139356833e8e0529.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="InstructFLIP-Exploring-Unified-Vision-Language-Model-for-Face-Anti-spoofing"><a href="#InstructFLIP-Exploring-Unified-Vision-Language-Model-for-Face-Anti-spoofing" class="headerlink" title="InstructFLIP: Exploring Unified Vision-Language Model for Face   Anti-spoofing"></a>InstructFLIP: Exploring Unified Vision-Language Model for Face   Anti-spoofing</h2><p><strong>Authors:Kun-Hsiang Lin, Yu-Wen Tseng, Kang-Yang Huang, Jhih-Ciang Wu, Wen-Huang Cheng</strong></p>
<p>Face anti-spoofing (FAS) aims to construct a robust system that can withstand diverse attacks. While recent efforts have concentrated mainly on cross-domain generalization, two significant challenges persist: limited semantic understanding of attack types and training redundancy across domains. We address the first by integrating vision-language models (VLMs) to enhance the perception of visual input. For the second challenge, we employ a meta-domain strategy to learn a unified model that generalizes well across multiple domains. Our proposed InstructFLIP is a novel instruction-tuned framework that leverages VLMs to enhance generalization via textual guidance trained solely on a single domain. At its core, InstructFLIP explicitly decouples instructions into content and style components, where content-based instructions focus on the essential semantics of spoofing, and style-based instructions consider variations related to the environment and camera characteristics. Extensive experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA models in accuracy and substantially reducing training redundancy across diverse domains in FAS. Project website is available at <a target="_blank" rel="noopener" href="https://kunkunlin1221.github.io/InstructFLIP">https://kunkunlin1221.github.io/InstructFLIP</a>. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«é˜²ä¼ªï¼ˆFASï¼‰æ—¨åœ¨æ„å»ºä¸€ä¸ªç¨³å¥çš„ç³»ç»Ÿï¼Œèƒ½å¤ŸæŠµå¾¡å„ç§æ”»å‡»ã€‚è™½ç„¶æœ€è¿‘çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨è·¨åŸŸæ³›åŒ–ä¸Šï¼Œä½†ä»ç„¶å­˜åœ¨ä¸¤ä¸ªé‡å¤§æŒ‘æˆ˜ï¼šå¯¹æ”»å‡»ç±»å‹çš„è¯­ä¹‰ç†è§£æœ‰é™ä»¥åŠè·¨åŸŸçš„è®­ç»ƒå†—ä½™ã€‚æˆ‘ä»¬é€šè¿‡æ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ¥è§£å†³ç¬¬ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä»¥å¢å¼ºå¯¹è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥ã€‚å¯¹äºç¬¬äºŒä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å…ƒåŸŸç­–ç•¥æ¥å­¦ä¹ ä¸€ä¸ªèƒ½åœ¨å¤šä¸ªé¢†åŸŸé—´è‰¯å¥½æ³›åŒ–çš„ç»Ÿä¸€æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºçš„InstructFLIPæ˜¯ä¸€ä¸ªæ–°å‹æŒ‡ä»¤è°ƒä¼˜æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨VLMsé€šè¿‡æ–‡æœ¬æŒ‡å¯¼å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä»…åœ¨å•ä¸ªé¢†åŸŸè¿›è¡Œè®­ç»ƒã€‚InstructFLIPçš„æ ¸å¿ƒæ˜¾å¼åœ°å°†æŒ‡ä»¤è§£è€¦ä¸ºå†…å®¹å’Œé£æ ¼ç»„ä»¶ï¼Œå…¶ä¸­åŸºäºå†…å®¹çš„æŒ‡ä»¤ä¾§é‡äºæ¬ºéª—çš„æœ¬è´¨è¯­ä¹‰ï¼Œè€ŒåŸºäºé£æ ¼çš„æŒ‡ä»¤åˆ™è€ƒè™‘ä¸ç¯å¢ƒç‰¹æ€§å’Œç›¸æœºç‰¹æ€§ç›¸å…³çš„å˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒInstructFLIPåœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºSOTAæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨äººè„¸è¯†åˆ«é˜²ä¼ªçš„å¤šä¸ªé¢†åŸŸä¸­æ˜¾è‘—å‡å°‘äº†è·¨åŸŸè®­ç»ƒå†—ä½™ã€‚é¡¹ç›®ç½‘ç«™ä½äº<a target="_blank" rel="noopener" href="https://kunkunlin1221.github.io/InstructFLIP%E3%80%82">https://kunkunlin1221.github.io/InstructFLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12060v2">PDF</a> Accepted by MMâ€™25</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½äººè„¸è¯†åˆ«é˜²ä¼ªç³»ç»Ÿåœ¨è§†è§‰å’Œè¯­è¨€çš„ç»“åˆä¸‹å®ç°äº†æ›´å¼ºçš„æ”»å‡»ç±»å‹è¯­ä¹‰ç†è§£ï¼Œå¹¶è§£å†³äº†è·¨åŸŸè®­ç»ƒå†—ä½™çš„é—®é¢˜ã€‚InstructFLIPæ¡†æ¶é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶é€šè¿‡ç»Ÿä¸€æ¨¡å‹å®ç°è·¨å¤šä¸ªé¢†åŸŸçš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®åŒºåˆ†æŒ‡ä»¤çš„å†…å®¹å’Œé£æ ¼ï¼Œå®ç°åŸºäºå†…å®¹çš„æŒ‡ä»¤ä¸“æ³¨äºé˜²ä¼ªçš„æœ¬è´¨è¯­ä¹‰ï¼Œè€ŒåŸºäºé£æ ¼çš„æŒ‡ä»¤åˆ™è€ƒè™‘ä¸ç¯å¢ƒã€ç›¸æœºç‰¹æ€§ç›¸å…³çš„å˜åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒInstructFLIPåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ï¼Œå¹¶åœ¨äººè„¸è¯†åˆ«é˜²ä¼ªç³»ç»Ÿçš„ä¸åŒé¢†åŸŸæœ‰æ•ˆé™ä½äº†è®­ç»ƒå†—ä½™ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒç›¸å…³ç½‘ç«™ <a target="_blank" rel="noopener" href="https://kunkunlin1221.github.io/InstructFLIP%E3%80%82">https://kunkunlin1221.github.io/InstructFLIPã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºæ„ŸçŸ¥èƒ½åŠ›ä»¥è§£å†³è¯­ä¹‰ç†è§£æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºæŒ‡ä»¤è°ƒè°çš„æ–¹æ³•æ¥å¤„ç†è·¨é¢†åŸŸå†—ä½™è®­ç»ƒé—®é¢˜ã€‚</li>
<li>é€šè¿‡å†…å®¹é£æ ¼å’ŒæŒ‡ä»¤çš„æ˜ç¡®åŒºåˆ†æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>InstructFLIPæ¡†æ¶å®ç°æ”»å‡»ç±»å‹è¯­ä¹‰çš„æ ¸å¿ƒç†è§£å¹¶å¢å¼ºé˜²ä¼ªç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºInstructFLIPåœ¨å‡†ç¡®æ€§ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰æ•ˆé™ä½äº†äººè„¸è¯†åˆ«é˜²ä¼ªç³»ç»Ÿåœ¨ä¸åŒé¢†åŸŸçš„è®­ç»ƒå†—ä½™ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bce06075db82436ae90961ec50fededc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad21775752e7ead05c49d597e065aba6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d68d26da11f5c39d428077cb86713ef2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c29ba580059011fb90b18044e44207fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e81c3c39053f625a99dfc9c8e1f29d4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-GPT-4o-mini-and-Gemini-2-0-Flash-Predict-Fine-Grained-Fashion-Product-Attributes-A-Zero-Shot-Analysis"><a href="#Can-GPT-4o-mini-and-Gemini-2-0-Flash-Predict-Fine-Grained-Fashion-Product-Attributes-A-Zero-Shot-Analysis" class="headerlink" title="Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion   Product Attributes? A Zero-Shot Analysis"></a>Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion   Product Attributes? A Zero-Shot Analysis</h2><p><strong>Authors:Shubham Shukla, Kunal Sonalkar</strong></p>
<p>The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the â€˜discovery experienceâ€™ of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (<a target="_blank" rel="noopener" href="https://github.com/yumingj/DeepFashion-MultiModal">https://github.com/yumingj/DeepFashion-MultiModal</a>) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction. </p>
<blockquote>
<p>æ—¶å°šé›¶å”®è¡Œä¸šçš„æ ¸å¿ƒåœ¨äºç†è§£äº§å“çš„èƒ½åŠ›ã€‚äº§å“å±æ€§æœ‰åŠ©äºæ ¹æ®ä¸šåŠ¡æµç¨‹ç†è§£äº§å“ã€‚è´¨é‡å±æ€§æé«˜äº†å®¢æˆ·åœ¨æµè§ˆé›¶å”®ç½‘ç«™æä¾›çš„æ•°ç™¾ä¸‡äº§å“æ—¶çš„ä½“éªŒï¼Œè¿™å¯¼è‡´äº†ç»„ç»‡è‰¯å¥½çš„äº§å“ç›®å½•ã€‚æœ€ç»ˆï¼Œäº§å“å±æ€§ç›´æ¥å½±å“å®¢æˆ·çš„â€œå‘ç°ä½“éªŒâ€ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å¤šæ¨¡å¼æ•°æ®æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†çš„æ—¶å°šå±æ€§è¯†åˆ«æ–¹é¢ï¼Œå®ƒä»¬çš„æ€§èƒ½ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡å¯¹æ‰€é‡‡ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†é›¶æ ·æœ¬è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹åœ¨æ€§èƒ½å’Œé€Ÿåº¦ä»¥åŠæˆæœ¬æ•ˆç›Šä¹‹é—´å–å¾—äº†å¹³è¡¡ï¼Œä¸»è¦åŒ…æ‹¬GPT-4o-miniå’ŒGemini 2.0 Flashã€‚æˆ‘ä»¬ä½¿ç”¨DeepFashion-MultiModalæ•°æ®é›†ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/yumingj/DeepFashion-MultiModal%EF%BC%89%E6%9D%A5%E8%AF%84%E4%BC%B0%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%97%B6%E5%B0%9A%E4%BA%A7%E5%93%81%E5%B1%9E%E6%80%A7%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E8%A1%A8%E7%8E%B0%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E7%A0%94%E7%A9%B6%E5%9C%A8%E6%97%B6%E5%B0%9A%E5%B1%9E%E6%80%A7%E7%9A%8418%E4%B8%AA%E7%B1%BB%E5%88%AB%E4%B8%AD%E8%AF%84%E4%BC%B0%E4%BA%86%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%EF%BC%8C%E6%B4%9E%E5%AF%9F%E4%BA%86%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E5%8A%BF%E6%89%80%E5%9C%A8%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AA%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%83%8F%E4%BD%9C%E4%B8%BA%E4%BA%A7%E5%93%81%E4%BF%A1%E6%81%AF%E7%9A%84%E5%94%AF%E4%B8%80%E8%BE%93%E5%85%A5%EF%BC%8C%E4%BB%A5%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8F%97%E9%99%90%E5%88%B6%E7%9A%84%E7%8E%AF%E5%A2%83%E3%80%82%E6%88%91%E4%BB%AC%E7%9A%84%E5%88%86%E6%9E%90%E8%A1%A8%E6%98%8E%EF%BC%8CGemini">https://github.com/yumingj/DeepFashion-MultiModalï¼‰æ¥è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨æ—¶å°šäº§å“å±æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨æ—¶å°šå±æ€§çš„18ä¸ªç±»åˆ«ä¸­è¯„ä¼°äº†è¿™äº›æ¨¡å‹ï¼Œæ´å¯Ÿäº†è¿™äº›æ¨¡å‹çš„ä¼˜åŠ¿æ‰€åœ¨ã€‚æˆ‘ä»¬åªä½¿ç”¨å›¾åƒä½œä¸ºäº§å“ä¿¡æ¯çš„å”¯ä¸€è¾“å…¥ï¼Œä»¥åˆ›å»ºä¸€ä¸ªå—é™åˆ¶çš„ç¯å¢ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒGemini</a> 2.0 Flashåœ¨æ‰€æœ‰å±æ€§ä¸Šçš„å®è§‚F1åˆ†æ•°è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°56.79%ï¼Œè€ŒGPT-4o-miniçš„å®è§‚F1åˆ†æ•°ä¸º43.28%ã€‚é€šè¿‡è¯¦ç»†çš„è¯¯å·®åˆ†æï¼Œæˆ‘ä»¬çš„å‘ç°æä¾›äº†å°†è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç”µå­å•†åŠ¡äº§å“å±æ€§ç›¸å…³ä»»åŠ¡çš„å®ç”¨è§è§£ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦è¿›è¡Œç‰¹å®šé¢†åŸŸçš„å¾®è°ƒæ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œä¹Ÿä¸ºæœªæ¥çš„æ—¶å°šäººå·¥æ™ºèƒ½å’Œå¤šæ¨¡å¼å±æ€§æå–ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09950v2">PDF</a> Version 2: Added a missing citation</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶å°šäº§å“å±æ€§è¯†åˆ«æ–¹é¢çš„åº”ç”¨ã€‚ç ”ç©¶é‡‡ç”¨äº†DeepFashion-MultiModalæ•°æ®é›†ï¼Œå¯¹GPT-4o-miniå’ŒGemini 2.0 Flashæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠ18ç±»æ—¶å°šå±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGemini 2.0 Flashåœ¨æ•´ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œè€ŒGPT-4o-miniåˆ™ç›¸å¯¹è¾ƒå¼±ã€‚ç ”ç©¶è¿˜é€šè¿‡é”™è¯¯åˆ†ææä¾›äº†å®é™…è§è§£ï¼Œä¸ºè¿™äº›LLMsåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç”µå­å•†åŠ¡äº§å“å±æ€§è¯†åˆ«ä»»åŠ¡çš„åº”ç”¨æä¾›äº†æŒ‡å¯¼ï¼Œå¹¶å¼ºè°ƒäº†é¢†åŸŸç‰¹å®šå¾®è°ƒæ–¹æ³•çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ—¶å°šäº§å“å±æ€§è¯†åˆ«æ–¹é¢å­˜åœ¨åº”ç”¨æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†DeepFashion-MultiModalæ•°æ®é›†è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚</li>
<li>GPT-4o-miniå’ŒGemini 2.0 Flashæ¨¡å‹å‚ä¸äº†è¯„ä¼°ï¼Œæ¶‰åŠ18ç±»æ—¶å°šå±æ€§ã€‚</li>
<li>Gemini 2.0 Flashåœ¨æ•´ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>é€šè¿‡é”™è¯¯åˆ†æï¼Œç ”ç©¶æä¾›äº†å®é™…è§è§£ï¼ŒæŒ‡å¯¼äº†è¿™äº›LLMsåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç”µå­å•†åŠ¡äº§å“å±æ€§è¯†åˆ«ä»»åŠ¡çš„åº”ç”¨ã€‚</li>
<li>é¢†åŸŸç‰¹å®šçš„å¾®è°ƒæ–¹æ³•å¯¹äºæé«˜LLMsåœ¨æ—¶å°šäº§å“å±æ€§è¯†åˆ«æ–¹é¢çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc06e465f3124b1d1603b819f7e14f96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716ae70c11d1da2f858848749b5b08ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52b13a3e15104cef5ae1174dc5aed043.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1"><a href="#A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1" class="headerlink" title="A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1"></a>A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1</h2><p><strong>Authors:Marcin PietroÅ„, RafaÅ‚ Olszowski, Jakub GomuÅ‚ka, Filip Gampel, Andrzej Tomski</strong></p>
<p>Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLMâ€™s, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings. </p>
<blockquote>
<p>è®ºè¯æŒ–æ˜ï¼ˆAMï¼‰æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼Œå®ƒèåˆäº†é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ã€ä¿®è¾å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰é¢†åŸŸçš„è§è§£ã€‚å®ƒæ¶‰åŠè‡ªåŠ¨è¯†åˆ«å’Œæå–è®ºè¯æ€§æˆåˆ†ï¼Œå¦‚å‰æå’Œè®ºæ–­ï¼Œä»¥åŠæ£€æµ‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚æ”¯æŒã€æ”»å‡»æˆ–ä¸­ç«‹ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ä¼ ç»Ÿçš„æ–¹æ³•å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒLLMæé«˜äº†åˆ†ææå–è®ºè¯è¯­ä¹‰çš„æ•ˆç‡ã€‚è™½ç„¶æœ‰å¾ˆå¤šåŸºå‡†æµ‹è¯•ç”¨äºæµ‹è¯•å’ŒéªŒè¯LLMçš„è´¨é‡ï¼Œä½†åœ¨å…¬å¼€å¯ç”¨çš„è®ºè¯åˆ†ç±»æ•°æ®åº“ä¸­å…³äºè¿™äº›æ¨¡å‹æ“ä½œçš„ç ”ç©¶å’Œç»“æœä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡ä½¿ç”¨Args.meå’ŒUKPç­‰å¤šæ ·åŒ–æ•°æ®é›†å¯¹éƒ¨åˆ†LLMè¿›è¡Œäº†ç ”ç©¶ã€‚æµ‹è¯•çš„æ¨¡å‹åŒ…æ‹¬GPTã€Llamaå’ŒDeepSeekçš„ç‰ˆæœ¬ï¼Œä»¥åŠé‡‡ç”¨Chain-of-Thoughtsç®—æ³•çš„æ¨ç†å¢å¼ºå˜ä½“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒChatGPT-4oçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚åœ¨èå…¥æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¸­ï¼ŒDeepseek-R1è¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡GPT-4oå’ŒDeepseek-R1å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬ä»ç„¶ä¼šå‡ºé”™ã€‚æœ¬æ–‡è®¨è®ºäº†æ‰€æœ‰æ¨¡å‹æœ€å¸¸è§çš„é”™è¯¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ‰€å‘ˆç°çš„å·¥ä½œæ˜¯ä½¿ç”¨LLMå’Œæç¤ºç®—æ³•å¯¹æ‰€è¿°æ•°æ®é›†è¿›è¡Œçš„é¦–æ¬¡æ›´å¹¿æ³›çš„åˆ†æã€‚è¯¥å·¥ä½œè¿˜å±•ç¤ºäº†å·²çŸ¥æç¤ºç®—æ³•åœ¨è®ºè¯åˆ†æä¸­çš„ä¸€äº›å¼±ç‚¹ï¼Œå¹¶æŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘ã€‚è¯¥å·¥ä½œçš„é™„åŠ å€¼æ˜¯å¯¹å¯ç”¨è®ºè¯æ•°æ®é›†è¿›è¡Œæ·±å…¥åˆ†æå’Œå±•ç¤ºå…¶ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08621v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†è®ºè¯æŒ–æ˜ï¼ˆAMï¼‰è¿™ä¸€è·¨å­¦ç§‘ç ”ç©¶é¢†åŸŸï¼Œç‰¹åˆ«å…³æ³¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®ºè¯è¯­ä¹‰åˆ†ææå–ä¸­çš„åº”ç”¨ã€‚æ–‡ç« å¯¹å¤šä¸ªLLMè¿›è¡Œäº†æµ‹è¯•ä¸æ¯”è¾ƒï¼ŒåŒ…æ‹¬GPTã€Llamaå’ŒDeepSeekç­‰æ¨¡å‹åŠå…¶ç»“åˆæ€ç»´é“¾ç®—æ³•çš„å˜ä½“ã€‚ç»“æœæ˜¾ç¤ºï¼ŒChatGPT-4oåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œç»“åˆæ¨ç†èƒ½åŠ›çš„Deepseek-R1ä¹Ÿè¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚ä½†å³ä¾¿æ˜¯è¿™äº›ä¼˜ç§€æ¨¡å‹ä¹Ÿå­˜åœ¨é”™è¯¯ï¼Œæ–‡ç« å¯¹æœ€å¸¸è§çš„é”™è¯¯è¿›è¡Œäº†è®¨è®ºã€‚æ­¤é¡¹ç ”ç©¶æ˜¯é¦–æ¬¡ä½¿ç”¨LLMå’Œæç¤ºç®—æ³•å¯¹æåˆ°çš„æ•°æ®é›†è¿›è¡Œå¹¿æ³›åˆ†æï¼Œå±•ç¤ºäº†ç°æœ‰æç¤ºç®—æ³•åœ¨è®ºè¯åˆ†æä¸­çš„å¼±ç‚¹ï¼Œå¹¶ä¸ºæ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºè¯æŒ–æ˜æ˜¯ä¸€ä¸ªæ¶‰åŠå¤šä¸ªå­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ã€ä¿®è¾å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç”¨äºå¢å¼ºè®ºè¯è¯­ä¹‰åˆ†æå’Œæå–çš„æ•ˆç‡ã€‚</li>
<li>åœ¨ä½¿ç”¨çš„æ¨¡å‹æµ‹è¯•ä¸­ï¼ŒChatGPT-4oåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç»“åˆæ¨ç†èƒ½åŠ›çš„Deepseek-R1æ¨¡å‹ä¹Ÿæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§ã€‚</li>
<li>å°½ç®¡å­˜åœ¨ä¼˜è¶Šæ€§ï¼Œä½†GPT-4oå’ŒDeepseek-R1ç­‰æ¨¡å‹ä»ç„¶ä¼šçŠ¯é”™è¯¯ï¼Œæ–‡ç« æŒ‡å‡ºäº†è¿™äº›é”™è¯¯ä¸­æœ€å¸¸è§çš„é—®é¢˜ã€‚</li>
<li>è¯¥ç ”ç©¶é¦–æ¬¡ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæç¤ºç®—æ³•å¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdd5c028a173a052700b45926a8b7474.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38837f646b5db2769cef3d1544aebc1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-554497861f6c0d11059799e2e0db3fed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c3d9bcbd47fc0f0f79df92b3663d52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa7415114b13d001e419215dcce9c0c7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="White-Basilisk-A-Hybrid-Model-for-Code-Vulnerability-Detection"><a href="#White-Basilisk-A-Hybrid-Model-for-Code-Vulnerability-Detection" class="headerlink" title="White-Basilisk: A Hybrid Model for Code Vulnerability Detection"></a>White-Basilisk: A Hybrid Model for Code Vulnerability Detection</h2><p><strong>Authors:Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis</strong></p>
<p>The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The modelâ€™s capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications. </p>
<blockquote>
<p>è½¯ä»¶æ¼æ´çš„æ¿€å¢å¯¹ç½‘ç»œå®‰å…¨æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†White-Basiliskï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¼æ´æ£€æµ‹æ–¹æ³•ï¼Œå®ƒåœ¨æŒ‘æˆ˜äººå·¥æ™ºèƒ½æ¨¡å‹æ‰©å±•çš„æ™®éå‡è®¾çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚White-Basiliskåˆ©ç”¨äº†ä¸€ç§åˆ›æ–°æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†Mambaå±‚ã€çº¿æ€§è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä¸“å®¶æ··åˆæ¡†æ¶ï¼Œä»…åœ¨å‚æ•°è®¡æ•°ä¸º200Mçš„æƒ…å†µä¸‹ï¼Œå°±åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€æ–°ç»“æœã€‚è¯¥æ¨¡å‹å¤„ç†å‰æ‰€æœªæœ‰çš„åºåˆ—é•¿åº¦çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡ä¼ é€’ä¸­å¯¹å¹¿æ³›çš„ä»£ç åº“è¿›è¡Œå…¨é¢åˆ†æï¼Œè¶…è¶Šäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚White-Basiliskåœ¨ä¸å¹³è¡¡ã€ç°å®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œä¾¿äºåœ¨å„ç±»ç»„ç»‡è§„æ¨¡ä¸Šéƒ¨ç½²ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…ä¸ºä»£ç å®‰å…¨å»ºç«‹äº†æ–°çš„åŸºå‡†çº¿ï¼Œè€Œä¸”è¿˜æä¾›äº†å®è¯è¯æ®ï¼Œè¯æ˜åœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè®¾è®¡ç´§å‡‘ã€é«˜æ•ˆçš„æ¨¡å‹å¯ä»¥è¶…è¶Šè¾ƒå¤§çš„æ¨¡å‹ï¼Œè¿™æœ‰å¯èƒ½é‡æ–°å®šä¹‰äººå·¥æ™ºèƒ½å¼€å‘ä¸­çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä¸ºé¢†åŸŸç‰¹å®šåº”ç”¨æä¾›æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08540v2">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>ç™½å·´è¥¿å°”æ˜¯å…¨çƒé¦–æ¬¡é‡‡ç”¨èåˆç‹¬ç‰¹æŠ€æœ¯æ‰“é€ çš„å°–ç«¯è½¯ä»¶æ¼æ´æ£€æµ‹æ¨¡å‹ï¼Œç»“åˆæŠ€æœ¯ç²¾æ¹›çš„å¤šé¡¹æˆæœæ¨å‡ºé©å‘½æ€§è¡¨ç°ã€‚ä½¿ç”¨æ¶µç›–èåˆéªŠç‹¼å±‚çº§ï¼ˆMamba layersï¼‰ã€çº¿æ€§è‡ªæ³¨æ„ï¼ˆlinear self-attentionï¼‰åŠä¸“ä¸šé¡¾é—®ç»„èåˆï¼ˆMixture of Experts frameworkï¼‰çš„ç»¼åˆæŠ€æœ¯ä½“ç³»ï¼Œâ€œç™½å·´è¥¿å°”â€æˆåŠŸå®ç°æ¼æ´æ£€æµ‹é¢†åŸŸçš„é¡¶å°–æˆæœï¼Œä»…é€šè¿‡2äº¿å‚æ•°å³è¾¾åˆ°å“è¶Šæ€§èƒ½ã€‚å…¶å¼ºå¤§çš„åºåˆ—å¤„ç†èƒ½åŠ›å¯ä¸€æ¬¡æ€§å…¨é¢åˆ†æå¤§å‹ä»£ç åº“ï¼Œçªç ´å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¯­å¢ƒçš„é™åˆ¶ã€‚â€œç™½å·´è¥¿å°”â€é¢å¯¹ç°å®ä¸–ç•Œæ•°æ®é›†ä¹Ÿè¡¨ç°ç¨³å¥ï¼Œå°¤å…¶å±•ç°å‡ºå…¶åœ¨ä¸å¹³è¡¡æ•°æ®å¤„ç†èƒ½åŠ›ä¸Šçš„é«˜è¶…è¡¨ç°ï¼Œå¹¶é€šè¿‡é«˜åº¦ç²¾ç¡®è®¡ç®—è¿›ä¸€æ­¥æ¨åŠ¨äº†å„ç§ç»„ç»‡çš„å¹¿æ³›ä½¿ç”¨éƒ¨ç½²ã€‚æœ¬ç ”ç©¶ä¸ä»…æ ‘ç«‹äº†ä»£ç å®‰å…¨çš„æ–°æ ‡æ†ï¼Œè€Œä¸”å®è¯è¡¨æ˜ç´§å‡‘ã€é«˜æ•ˆè®¾è®¡çš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¯è¶…è¶Šå¤§å‹æ¨¡å‹ï¼Œä¸ºé¢†åŸŸç‰¹å®šåº”ç”¨çš„AIå¼€å‘ä¼˜åŒ–ç­–ç•¥æä¾›äº†å…¨æ–°è§†è§’ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<p>ä¸€ã€ç™½å·´è¥¿å°”æ¨¡å‹é€šè¿‡èåˆå¤šç§æŠ€æœ¯å®ç°é«˜æ•ˆæ¼æ´æ£€æµ‹ï¼ŒåŒ…æ‹¬åˆ›æ–°çš„æ¨¡å‹æ¶æ„ä»¥åŠç‰¹æ®Šçš„çº¿æ€§è‡ªæ³¨æ„æœºåˆ¶å’Œä¸“å®¶ç³»ç»Ÿã€‚æ¨¡å‹å‚æ•°é‡ä»…éœ€è¾ƒå°‘çš„æ•°å€¼ä¾¿å…·æœ‰ä¼˜ç§€è¡¨ç°ï¼Œè¿™ç§å“è¶Šçš„æ€§èƒ½æä¾›äº†æœºä¼šçª—å£è¿›è¡Œæ›´å¹¿æ³›çš„éƒ¨ç½²å’Œåº”ç”¨ã€‚</p>
<p>äºŒã€ç™½å·´è¥¿å°”æ¨¡å‹å…·æœ‰å¤„ç†è¶…é•¿åºåˆ—çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿå…¨é¢åˆ†æå¤§å‹ä»£ç åº“ï¼Œçªç ´äº†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­å¢ƒé™åˆ¶ã€‚è¿™ç§çªç ´å¯¹äºåº”å¯¹å¤æ‚çš„è½¯ä»¶ä»£ç å®‰å…¨æŒ‘æˆ˜è‡³å…³é‡è¦ã€‚</p>
<p>ä¸‰ã€ç™½å·´è¥¿å°”æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚è¿™æ„å‘³ç€è¯¥æ¨¡å‹èƒ½å¤Ÿåº”å¯¹å„ç§å®é™…åœºæ™¯ä¸‹çš„æ•°æ®æŒ‘æˆ˜ï¼Œå…·æœ‰æé«˜çš„å®ç”¨ä»·å€¼ã€‚</p>
<p>å››ã€ç™½å·´è¥¿å°”çš„è®¡ç®—æ•ˆç‡æé«˜ï¼Œèƒ½å¤Ÿåœ¨å„ç§ç»„ç»‡è§„æ¨¡ä¸Šå®ç°å¿«é€Ÿéƒ¨ç½²å’Œåº”ç”¨ã€‚è¿™ç§é«˜æ•ˆçš„è®¡ç®—æ€§èƒ½å¯¹äºå¤§è§„æ¨¡çš„è½¯ä»¶å®‰å…¨æ£€æµ‹ä»»åŠ¡è‡³å…³é‡è¦ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91c3ee40f85cd9766e9880234b50cbfc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning"><a href="#Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning" class="headerlink" title="Perception-Aware Policy Optimization for Multimodal Reasoning"></a>Perception-Aware Policy Optimization for Multimodal Reasoning</h2><p><strong>Authors:Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, a novel policy gradient algorithm that encourages the model to learn to perceive while learning to reason. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term, which can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely on additional data curation, reward models, or stronger teacher models. To further enhance the training stability of PAPO, we introduce the Double Entropy Loss, which effectively regularizes the new KL objective without compromising performance. Despite its simplicity, PAPO yields significant overall improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%-19.1%, on tasks with high vision dependency. We also observe a substantial reduction of 30.5% in perception errors, indicating improved perceptual capabilities with PAPO. Overall, our work introduces a deeper integration of perception-aware supervision into core learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Code and data will be made publicly available for research purposes. Project page: <a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">https://mikewangwzhl.github.io/PAPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²è¢«è¯æ˜æ˜¯èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›çš„ä¸€ç§é«˜åº¦æœ‰æ•ˆçš„ç­–ç•¥ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡å’Œä¼˜åŒ–ä»ç„¶é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯¯å·®ä¸»è¦æ¥æºäºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†PAPOï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­å­¦ä¼šæ„ŸçŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»¥KLæ•£åº¦é¡¹çš„å½¢å¼å¼•å…¥éšæ„ŸçŸ¥æŸå¤±ï¼Œå®ƒå¯ä»¥æ— ç¼åœ°æ’å…¥ä¸»æµçš„RLVRç®—æ³•ï¼Œå¦‚GRPOå’ŒDAPOã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPAPOä¸ä¾èµ–é¢å¤–çš„æ•°æ®æ•´ç†ã€å¥–åŠ±æ¨¡å‹æˆ–æ›´å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜PAPOçš„è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡ç†µæŸå¤±ï¼Œå®ƒæœ‰æ•ˆåœ°è§„èŒƒäº†æ–°çš„KLç›®æ ‡ï¼Œè€Œä¸ä¼šå½±å“æ€§èƒ½ã€‚å°½ç®¡PAPOç®€å•ï¼Œä½†åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒå®ç°äº†4.4%~17.5%çš„æ˜¾è‘—æ€»ä½“æ”¹è¿›ã€‚åœ¨é«˜è§†è§‰ä¾èµ–çš„ä»»åŠ¡ä¸Šï¼Œæ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œè¾¾åˆ°äº†8.0%~19.1%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯ç‡é™ä½äº†30.5%ï¼Œè¿™è¡¨æ˜PAPOæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°é›†æˆåˆ°æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰åŸºç¡€æ¨ç†çš„æ–°å‹RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚ä»£ç å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒä»¥ä¾›ç ”ç©¶ä¹‹ç”¨ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO%E3%80%82">https://mikewangwzhl.github.io/PAPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06448v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹äºˆç¨³å¥çš„å¤šæ­¥æ¨ç†èƒ½åŠ›æä¾›äº†æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡ä¸ä¼˜åŒ–ä»…é™äºçº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚é’ˆå¯¹å½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­è¯¯å·®çš„ä¸»è¦æ¥æºâ€”â€”è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PAPOè¿™ä¸€æ–°å‹ç­–ç•¥ä¼˜åŒ–æ¢¯åº¦ç®—æ³•ã€‚å®ƒé€šè¿‡å¼•å…¥éšå¼æ„ŸçŸ¥æŸå¤±ï¼ˆä»¥KLæ•£åº¦å½¢å¼ï¼‰æ¥é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†ä¸­å­¦ä¹ æ„ŸçŸ¥ï¼Œå¯æ— ç¼é›†æˆåˆ°ä¸»æµRLVRç®—æ³•å¦‚GRPOå’ŒDAPOä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPAPOä¸ä¾èµ–é¢å¤–æ•°æ®æ•´ç†ã€å¥–åŠ±æ¨¡å‹æˆ–æ›´å¼ºæ•™å¸ˆæ¨¡å‹ã€‚ä¸ºè¿›ä¸€æ­¥å¢å¼ºPAPOçš„è®­ç»ƒç¨³å®šæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒé‡ç†µæŸå¤±ï¼Œæœ‰æ•ˆæ­£åˆ™åŒ–æ–°çš„KLç›®æ ‡è€Œä¸æŸå®³æ€§èƒ½ã€‚å°½ç®¡ç®€å•ï¼ŒPAPOåœ¨å¤šæ ·åŒ–å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­æ•´ä½“æ”¹è¿›äº†4.4%-17.5%çš„æˆç»©ï¼Œåœ¨é«˜è§†è§‰ä¾èµ–ä»»åŠ¡ä¸Šçš„æ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œè¾¾åˆ°8.0%-19.1%ã€‚åŒæ—¶è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯ç‡æ˜¾è‘—ä¸‹é™30.5%ï¼Œè¡¨æ˜PAPOæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°é›†æˆåˆ°æ ¸å¿ƒå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰æ„ŸçŸ¥æ¨ç†çš„æ–°å‹RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLVRåœ¨èµ‹äºˆLLMå¤šæ­¥æ¨ç†èƒ½åŠ›æ–¹é¢é«˜åº¦æœ‰æ•ˆï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€æ¨ç†çš„ä¸»è¦è¯¯å·®æ¥æºäºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥é—®é¢˜ã€‚</li>
<li>PAPOæ˜¯ä¸€ç§æ–°å‹ç­–ç•¥ä¼˜åŒ–æ¢¯åº¦ç®—æ³•ï¼Œé€šè¿‡å¼•å…¥éšå¼æ„ŸçŸ¥æŸå¤±è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>PAPOèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ä¸»æµRLVRç®—æ³•ä¸­ï¼Œä¸”ä¸ä¾èµ–é¢å¤–æ•°æ®ã€å¥–åŠ±æ¨¡å‹æˆ–æ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŒé‡ç†µæŸå¤±ï¼Œå¢å¼ºäº†PAPOçš„è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>PAPOåœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜è§†è§‰ä¾èµ–ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3fbfe26e7812ec9e6b648e06af4dff0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf44c02baa83bd65ec827384ea00ac9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc805aaee1789b774c13bebe5c62db41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ae77696b96e7c175701b51476deca66.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks"><a href="#How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks" class="headerlink" title="How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks"></a>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks</h2><p><strong>Authors:Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, OÄŸuzhan Fatih Kar, Amir Zamir</strong></p>
<p>Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåƒGPT-4oè¿™æ ·çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨è§†è§‰ç†è§£æ–¹é¢ï¼Œè¿™äº›æ¨¡å‹çš„ç¡®åˆ‡è¡¨ç°å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€æ·±åº¦å’Œè¡¨é¢æ³•çº¿é¢„æµ‹ï¼‰ä¸Šï¼Œä½¿ç”¨å·²å»ºç«‹çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚COCOã€ImageNetåŠå…¶å˜ä½“ç­‰ï¼‰å¯¹æµè¡Œçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆGPT-4oã€o4-miniã€Gemini 1.5 Proå’ŒGemini 2.0 Flashã€Claude 3.5 Sonnetã€Qwen2-VLã€Llama 3.2ï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æ‰§è¡Œæ­¤æ“ä½œçš„ä¸»è¦æŒ‘æˆ˜æ˜¯ï¼š1ï¼‰å¤§å¤šæ•°æ¨¡å‹è¢«è®­ç»ƒç”¨äºè¾“å‡ºæ–‡æœ¬ï¼Œæ— æ³•åŸç”Ÿè¡¨è¾¾è¯¸å¦‚ç‰‡æ®µæˆ–3Då‡ ä½•ç­‰å¤šæ ·é¢†åŸŸï¼›2ï¼‰è®¸å¤šé¢†å…ˆçš„æ¨¡å‹æ˜¯ä¸“æœ‰æ¨¡å‹ï¼Œåªèƒ½é€šè¿‡APIçº§åˆ«è®¿é—®ï¼Œå³æ— æ³•è·å–å…¶æƒé‡ä»¥è¿›è¡Œé€‚åº”ã€‚æˆ‘ä»¬é€šè¿‡å°†æ ‡å‡†è§†è§‰ä»»åŠ¡é€šè¿‡æç¤ºé“¾è½¬åŒ–ä¸ºç­‰æ•ˆçš„æ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹çš„ä»»åŠ¡ï¼Œæ¥åˆ›å»ºæ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„è§‚å¯Ÿæ˜¯ï¼š1ï¼‰è¿™äº›æ¨¡å‹åœ¨ä»»ä½•ä»»åŠ¡ä¸Šéƒ½æœªæ¥è¿‘æœ€å…ˆè¿›çš„ä¸“å®¶æ¨¡å‹ï¼›ç„¶è€Œï¼Œ2ï¼‰å®ƒä»¬æ˜¯ç›¸å½“ä¸é”™çš„é€šç”¨æ¨¡å‹ï¼›è¿™å°¤ä¸ºæ˜¾è‘—ï¼Œå› ä¸ºå®ƒä»¬çš„è®­ç»ƒä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒæ–‡æœ¬çš„ä»»åŠ¡ä¸Šã€‚3ï¼‰å®ƒä»¬æ‰§è¡Œè¯­ä¹‰ä»»åŠ¡æ˜æ˜¾æ¯”å‡ ä½•ä»»åŠ¡æ›´å¥½ã€‚4ï¼‰è™½ç„¶æç¤ºé“¾æŠ€æœ¯ä¼šå½±å“æ€§èƒ½ï¼Œä½†æ›´å¥½çš„æ¨¡å‹å¯¹æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§è¾ƒä½ã€‚5ï¼‰GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨6ä¸ªä»»åŠ¡ä¸­çš„4ä¸ªä¸­ä½å±…æ¦œé¦–ã€‚6ï¼‰æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚o3ï¼‰åœ¨å‡ ä½•ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œ7ï¼‰å¯¹å…·æœ‰åŸç”Ÿå›¾åƒç”Ÿæˆçš„æ¨¡å‹ï¼ˆå¦‚æœ€æ–°çš„GPT-4oï¼‰çš„åˆæ­¥åˆ†ææ˜¾ç¤ºï¼Œå®ƒä»¬å…·æœ‰è¯¸å¦‚å¹»è§‰å’Œç©ºé—´é”™ä½ç­‰ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01955v2">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://fm-vision-evals.epfl.ch/">https://fm-vision-evals.epfl.ch/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è¯„ä¼°äº†å¤šæ¬¾æµè¡Œçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT-4oã€o4-miniã€Geminiç³»åˆ—ç­‰ï¼‰åœ¨æ ‡å‡†è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚é€šè¿‡é‡‡ç”¨ç‰¹å®šçš„æç¤ºé“¾æŠ€æœ¯ï¼Œå°†æ ‡å‡†çš„è§†è§‰ä»»åŠ¡è½¬åŒ–ä¸ºæ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹çš„ä»»åŠ¡ï¼Œä»è€Œå»ºç«‹äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹è™½åœ¨å„é¡¹ä»»åŠ¡ä¸Šå‡æœªè¾¾åˆ°æœ€ä½³çŠ¶æ€ï¼Œä½†åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„é€šç”¨æ€§èƒ½ã€‚å…¶ä¸­GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨å››é¡¹ä»»åŠ¡ä¸­ä½åˆ—ç¬¬ä¸€ã€‚åŒæ—¶ï¼Œå…·æœ‰åŸç”Ÿå›¾åƒç”ŸæˆåŠŸèƒ½çš„æ¨¡å‹å­˜åœ¨è¯¸å¦‚å¹»è§‰å’Œç©ºé—´é”™ä½ç­‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°è¢«è¯„ä¼°ï¼Œæ¶‰åŠè¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€æ·±åº¦é¢„æµ‹å’Œè¡¨é¢æ³•çº¿é¢„æµ‹ç­‰ä»»åŠ¡ã€‚</li>
<li>é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ¨¡å‹åŸç”Ÿè¡¨è¾¾èƒ½åŠ›çš„å±€é™æ€§å’Œä¸“æœ‰æ¨¡å‹çš„æƒé‡ä¸å¯è®¿é—®æ€§ã€‚</li>
<li>é€šè¿‡æç¤ºé“¾æŠ€æœ¯ï¼Œå°†æ ‡å‡†è§†è§‰ä»»åŠ¡è½¬åŒ–ä¸ºæ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹çš„ä»»åŠ¡ï¼Œå»ºç«‹äº†æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¿™äº›æ¨¡å‹è™½æœªæ¥è¿‘ä¸“ä¸šæ¨¡å‹çš„è¡¨ç°ï¼Œä½†åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šå±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§èƒ½ã€‚</li>
<li>GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å››é¡¹ä»»åŠ¡ä¸­ä½åˆ—ç¬¬ä¸€ã€‚</li>
<li>æ¨ç†æ¨¡å‹åœ¨å‡ ä½•ä»»åŠ¡ä¸Šæœ‰æ‰€æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d88c4d0d1c59d5281a6b0b13366df18e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b8abef8fa4c24f2f1c3edf3591b0cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8665bb9a66c6900ff01051abba32ea93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86a19d55133674d93a00588735b57752.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-983cfb67114d5796d8d0919e170c0b8f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Unable-to-Forget-Proactive-Interference-Reveals-Working-Memory-Limits-in-LLMs-Beyond-Context-Length"><a href="#Unable-to-Forget-Proactive-Interference-Reveals-Working-Memory-Limits-in-LLMs-Beyond-Context-Length" class="headerlink" title="Unable to Forget: Proactive Interference Reveals Working Memory Limits   in LLMs Beyond Context Length"></a>Unable to Forget: Proactive Interference Reveals Working Memory Limits   in LLMs Beyond Context Length</h2><p><strong>Authors:Chupei Wang, Jiaqiu Vince Sun</strong></p>
<p>Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMsâ€™ ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen modelsâ€™ ability to suppress irrelevant content during retrieval. </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œä¿¡æ¯æ£€ç´¢ä¸ç”Ÿæˆèƒ½åŠ›çš„ç›¸äº’äº¤ç»‡è¶Šæ¥è¶Šè¢«äººä»¬æ‰€è®¤è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„æŸ¥æ‰¾ã€‚è™½ç„¶å¸¸å¸¸å‡è®¾è¾ƒé•¿çš„ä¸Šä¸‹æ–‡ç¯å¢ƒå¯ä»¥æé«˜æ£€ç´¢æ•ˆæœï¼Œä½†å…³äºè¯­å¢ƒå†…éƒ¨å¹²æ‰°çš„å½±å“ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»è®¤çŸ¥ç§‘å­¦ä¸­é‡‡ç”¨äº†ä¸»åŠ¨å¹²æ‰°ï¼ˆPIï¼‰èŒƒå¼ï¼Œå…¶ä¸­æ—©æœŸä¿¡æ¯ä¼šå¹²æ‰°å¯¹æ›´æ–°çš„å›å¿†ã€‚åœ¨äººç±»ä¸­ï¼Œå¯¹è¿™ç§å¹²æ‰°çš„æ•æ„Ÿæ€§æ˜¯ä¸å·¥ä½œè®°å¿†å®¹é‡æˆåæ¯”çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†PI-LLMè¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æŒ‰é¡ºåºæµå¼ä¼ è¾“è¯­ä¹‰ç›¸å…³çš„é”®å€¼æ›´æ–°ï¼Œå¹¶ä»…æŸ¥è¯¢æœ€ç»ˆå€¼ã€‚å°½ç®¡è¿™äº›æœ€ç»ˆå€¼æ˜ç¡®åœ°å®šä½åœ¨æŸ¥è¯¢ä¹‹å‰ï¼Œä½†éšç€å¹²æ‰°çš„ç´¯ç§¯ï¼ŒLLMçš„æ£€ç´¢å‡†ç¡®ç‡ä»¥å¯¹æ•°çº¿æ€§æ–¹å¼ä¸‹é™åˆ°é›¶ï¼›é”™è¯¯æºè‡ªäºæ£€ç´¢å…ˆå‰è¦†ç›–çš„å€¼ã€‚å°è¯•é€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆä¾‹å¦‚æŒ‡å¯¼æ¨¡å‹å¿½ç•¥æ—©æœŸè¾“å…¥ï¼‰æ¥å‡è½»å¹²æ‰°å–å¾—äº†æœ‰é™çš„æˆåŠŸã€‚è¿™äº›å‘ç°æ­ç¤ºäº†LLMåœ¨è§£å¼€å¹²æ‰°å’Œçµæ´»æ“ä½œä¿¡æ¯æ–¹é¢çš„åŸºæœ¬çº¦æŸï¼Œè¡¨æ˜å­˜åœ¨ä¸€ä¸ªå·¥ä½œè®°å¿†ç“¶é¢ˆï¼Œè¶…å‡ºäº†ç®€å•çš„ä¸Šä¸‹æ–‡è®¿é—®ã€‚è¿™è¦æ±‚é‡‡ç”¨åŠ å¼ºæ¨¡å‹åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­æŠ‘åˆ¶æ— å…³å†…å®¹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08184v3">PDF</a> Accepted at ICML 2025 Workshop on Long Context Foundation Models   (ICFM). Code: <a target="_blank" rel="noopener" href="https://github.com/zhuangziGiantfish/Unable-to-Forget">https://github.com/zhuangziGiantfish/Unable-to-Forget</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>LLMä¸­çš„ä¿¡æ¯æ£€ç´¢ä¸å†ä»…ä»…æ˜¯æŸ¥æ‰¾ï¼Œè€Œæ˜¯ä¸ç”Ÿæˆèƒ½åŠ›ç´§å¯†ç›¸è¿ã€‚å°½ç®¡é•¿æ–‡æœ¬èƒŒæ™¯é€šå¸¸è¢«è®¤ä¸ºèƒ½æé«˜æ£€ç´¢æ•ˆæœï¼Œä½†å…³äºå†…éƒ¨ä¸Šä¸‹æ–‡å¹²æ‰°çš„å½±å“å°šç¼ºä¹ç ”ç©¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»è®¤çŸ¥ç§‘å­¦ä¸­å¼•å…¥äº†ä¸»åŠ¨å¹²æ‰°ï¼ˆPIï¼‰çš„æ¦‚å¿µï¼Œå³æ—©æœŸä¿¡æ¯ä¼šå¹²æ‰°å¯¹æœ€æ–°æ›´æ–°çš„å›å¿†ã€‚åœ¨äººç±»ä¸­ï¼Œå¯¹è¿™ç±»å¹²æ‰°çš„æ•æ„Ÿæ€§æ˜¯ä¸å·¥ä½œè®°å¿†å®¹é‡æˆåæ¯”çš„ã€‚æˆ‘ä»¬æå‡ºäº†PI-LLMè¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æŒ‰é¡ºåºæµå¼ä¼ è¾“è¯­ä¹‰ç›¸å…³çš„é”®å€¼æ›´æ–°ï¼Œå¹¶ä»…æŸ¥è¯¢æœ€ç»ˆå€¼ã€‚å°½ç®¡è¿™äº›æœ€ç»ˆå€¼æ˜ç¡®åœ°ä½äºæŸ¥è¯¢ä¹‹å‰ï¼Œä½†éšç€å¹²æ‰°çš„ç´¯ç§¯ï¼ŒLLMçš„æ£€ç´¢å‡†ç¡®ç‡ä¼šä»¥å¯¹æ•°çº¿æ€§æ–¹å¼é™ä½åˆ°é›¶ï¼›é”™è¯¯æ¥è‡ªäºæ£€ç´¢åˆ°å…ˆå‰è¢«è¦†ç›–çš„å€¼ã€‚å°è¯•é€šè¿‡æç¤ºå·¥ç¨‹ï¼ˆä¾‹å¦‚æŒ‡å¯¼æ¨¡å‹å¿½ç•¥æ—©æœŸè¾“å…¥ï¼‰æ¥å‡è½»å¹²æ‰°çš„æ•ˆæœæœ‰é™ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ä¸€ä¸ªåŸºæœ¬é™åˆ¶ï¼šLLMåœ¨åˆ†ç¦»å¹²æ‰°å’Œçµæ´»æ“çºµä¿¡æ¯æ–¹é¢å­˜åœ¨å·¥ä½œè®°å¿†ç“¶é¢ˆï¼Œè¿™ä¸ä»…ä»…æ˜¯è®¿é—®ä¸Šä¸‹æ–‡çš„é—®é¢˜ã€‚è¿™è¦æ±‚é‡‡ç”¨åŠ å¼ºæ¨¡å‹åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­æŠ‘åˆ¶æ— å…³å†…å®¹çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢åœ¨LLMä¸­ä¸å†ä»…æ˜¯æŸ¥æ‰¾ï¼Œè€Œæ˜¯ä¸ç”Ÿæˆèƒ½åŠ›ç´§å¯†ç›¸è¿ã€‚</li>
<li>é•¿æ–‡æœ¬èƒŒæ™¯è™½æœ‰åŠ©äºæé«˜æ£€ç´¢æ•ˆæœï¼Œä½†å†…éƒ¨ä¸Šä¸‹æ–‡å¹²æ‰°çš„å½±å“å°šå¾…ç ”ç©¶ã€‚</li>
<li>ä¸»åŠ¨å¹²æ‰°ï¼ˆPIï¼‰æ¦‚å¿µå¯¹äºç†è§£LLMçš„æ£€ç´¢èƒ½åŠ›å¾ˆé‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ–°ä¿¡æ¯å’Œæ—§ä¿¡æ¯ä¹‹é—´çš„å¹²æ‰°æ—¶ã€‚</li>
<li>äººç±»å¯¹ä¸»åŠ¨å¹²æ‰°çš„æ•æ„Ÿæ€§ä¸å·¥ä½œè®°å¿†å®¹é‡æˆåæ¯”ã€‚</li>
<li>LLMåœ¨åº”å¯¹å¹²æ‰°ä¿¡æ¯æ—¶å­˜åœ¨å·¥ä½œè®°å¿†ç“¶é¢ˆï¼Œè¿™å½±å“äº†å…¶æ£€ç´¢å‡†ç¡®æ€§ã€‚</li>
<li>å°è¯•é€šè¿‡æç¤ºå·¥ç¨‹æ¥å‡è½»å¹²æ‰°çš„æ•ˆæœæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb1f05090f862ac72a42429799278618.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb255a86bd8a6818da7bcdaea8538ea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e180b33de23aa69660913191e01eca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d65442de385c1964559f69fb52c05d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1f9bace7ce1d895ce25a082af816c98.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FLAT-LLM-Fine-grained-Low-rank-Activation-Space-Transformation-for-Large-Language-Model-Compression"><a href="#FLAT-LLM-Fine-grained-Low-rank-Activation-Space-Transformation-for-Large-Language-Model-Compression" class="headerlink" title="FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression"></a>FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for   Large Language Model Compression</h2><p><strong>Authors:Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang</strong></p>
<p>Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis, and employ a greedy budget redistribution strategy to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 5 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç„¶è€Œå®ƒä»¬å¯¹è®¡ç®—å’Œå†…å­˜çš„é«˜éœ€æ±‚ç»™èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘çš„ä½ç§©åˆ†è§£æ–¹æ³•ä¸ºç»“æ„å‹ç¼©æä¾›äº†ä¸€æ¡æœ‰å‰æ™¯çš„é“è·¯ï¼Œä½†å®ƒä»¬å¾€å¾€å­˜åœ¨ç²¾åº¦ä¸‹é™ã€æ ¡å‡†ç¨‹åºæ˜‚è´µçš„é—®é¢˜ï¼Œå¹¶å¯¼è‡´æ¨¡å‹æ¶æ„æ•ˆç‡ä½ä¸‹ï¼Œé˜»ç¢äº†ç°å®ä¸–ç•Œä¸­çš„æ¨ç†é€Ÿåº¦æå‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FLAT-LLMï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿä¸”å‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•ï¼ŒåŸºäºæ¿€æ´»ç©ºé—´ä¸­çš„ç²¾ç»†ä½ç§©è½¬æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¤´ä¸»æˆåˆ†åˆ†æè®¡ç®—å¾—åˆ°çš„æˆªæ–­ç‰¹å¾å‘é‡æ¥è½¬æ¢æƒé‡ï¼Œé™ä½éšè—ç»´åº¦ï¼Œå¹¶é‡‡ç”¨è´ªå©ªé¢„ç®—å†åˆ†é…ç­–ç•¥æ¥è‡ªé€‚åº”åœ°åœ¨è§£ç å™¨ä¹‹é—´åˆ†é…ç­‰çº§ã€‚FLAT-LLMå®ç°äº†é«˜æ•ˆæœ‰æ•ˆçš„æƒé‡å‹ç¼©ï¼Œæ— éœ€æ¢å¤å¾®è°ƒï¼Œå¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆæ ¡å‡†ã€‚åœ¨5ä¸ªæ¨¡å‹å’Œ11ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFLAT-LLMåœ¨é€šç”¨æ€§å’Œä¸‹æ¸¸æ€§èƒ½æ–¹é¢çš„è¡¨ç°ä¼˜äºç»“æ„å‰ªæåŸºçº¿ï¼ŒåŒæ—¶åœ¨åŸºäºåˆ†è§£çš„æ–¹æ³•ä¸Šå®ç°äº†æ¨ç†é€Ÿåº¦çš„æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23966v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶é«˜è®¡ç®—éœ€æ±‚å’Œå†…å­˜éœ€æ±‚ä½¿å…¶åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­éƒ¨ç½²é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç²¾ç»†ç²’åº¦ä½ç§©è½¬æ¢çš„æ¿€æ´»ç©ºé—´å¿«é€Ÿå‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•FLAT-LLMã€‚é€šè¿‡åˆ©ç”¨å¤´ä¸»æˆåˆ†åˆ†æè®¡ç®—æˆªæ–­ç‰¹å¾å‘é‡æ¥è½¬æ¢æƒé‡ï¼Œå¹¶é‡‡ç”¨è´ªå©ªé¢„ç®—é‡æ–°åˆ†é…ç­–ç•¥è‡ªé€‚åº”åœ°è°ƒæ•´è§£ç å™¨ä¹‹é—´çš„ç­‰çº§ï¼Œå®ç°äº†æ— éœ€æ¢å¤å¾®è°ƒçš„æœ‰æ•ˆæƒé‡å‹ç¼©ã€‚åœ¨5ä¸ªæ¨¡å‹å’Œ11ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFLAT-LLMåœ¨æ³›åŒ–å’Œä¸‹æ¸¸æ€§èƒ½ä¸Šä¼˜äºç»“æ„å‰ªæåŸºçº¿ï¼Œå¹¶ä¸”åœ¨åŸºäºåˆ†è§£çš„æ–¹æ³•ä¸Šå®ç°äº†æ¨ç†é€Ÿåº¦çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†èµ„æºå—é™ç¯å¢ƒçš„éƒ¨ç½²å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>FLAT-LLMæ˜¯ä¸€ç§åŸºäºæ¿€æ´»ç©ºé—´çš„å¿«é€Ÿå‡†ç¡®çš„æ— è®­ç»ƒç»“æ„å‹ç¼©æ–¹æ³•ã€‚</li>
<li>FLAT-LLMåˆ©ç”¨å¤´ä¸»æˆåˆ†åˆ†æè¿›è¡Œæƒé‡è½¬æ¢ï¼Œå¹¶é‡‡ç”¨è´ªå©ªé¢„ç®—é‡æ–°åˆ†é…ç­–ç•¥ã€‚</li>
<li>FLAT-LLMå®ç°äº†æ— éœ€æ¢å¤ç»†è°ƒçš„æƒé‡å‹ç¼©ã€‚</li>
<li>FLAT-LLMåœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç»“æ„å‰ªæåŸºçº¿ã€‚</li>
<li>FLAT-LLMæå‡äº†æ¨ç†é€Ÿåº¦ï¼Œç›¸æ¯”åŸºäºåˆ†è§£çš„æ–¹æ³•æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e3c33cb43dc3cc89b92bd60efe0092a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8238603467e58c73ee04a89483d793b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a562cb131db96f9a0559a116c850aa35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cd4d8c864ce86703ae532bff90e5d91.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="How-Can-I-Publish-My-LLM-Benchmark-Without-Giving-the-True-Answers-Away"><a href="#How-Can-I-Publish-My-LLM-Benchmark-Without-Giving-the-True-Answers-Away" class="headerlink" title="How Can I Publish My LLM Benchmark Without Giving the True Answers Away?"></a>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</h2><p><strong>Authors:Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</strong></p>
<p>Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies. </p>
<blockquote>
<p>åœ¨äº’è”ç½‘ä¸Šå‘å¸ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨ä¸€å®šçš„é£é™©ï¼Œå¯èƒ½ä¼šæ±¡æŸ“æœªæ¥çš„LLMï¼šè¯¥åŸºå‡†æµ‹è¯•å¯èƒ½ä¼šè¢«æ— æ„ä¸­ï¼ˆæˆ–æ•…æ„ï¼‰ç”¨äºè®­ç»ƒæˆ–é€‰æ‹©æ¨¡å‹ã€‚ä¸€ç§å¸¸è§çš„ç¼“è§£æ–¹æ³•æ˜¯ä¿æŒåŸºå‡†æµ‹è¯•ç§å¯†ï¼Œè®©å‚ä¸è€…å‘ç»„ç»‡è€…æäº¤ä»–ä»¬çš„æ¨¡å‹æˆ–é¢„æµ‹ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥éœ€è¦ä¿¡ä»»å•ä¸€ç»„ç»‡ï¼Œå¹¶ä¸”ä»ç„¶å…è®¸é€šè¿‡é‡å¤æŸ¥è¯¢æ¥è¿›è¡Œæµ‹è¯•é›†è¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‘å¸ƒåŸºå‡†æµ‹è¯•çš„æ–¹æ³•ï¼Œè€Œæ— éœ€å®Œå…¨æŠ«éœ²é—®é¢˜çš„çœŸå®ç­”æ¡ˆï¼ŒåŒæ—¶ä»èƒ½å¤Ÿå…¬å¼€è¯„ä¼°LLMã€‚æˆ‘ä»¬çš„ä¸»è¦æƒ³æ³•æ˜¯é€šè¿‡å‡†å¤‡å‡ ä¸ªé€»è¾‘ä¸Šæ­£ç¡®çš„ç­”æ¡ˆå¹¶å¼•å…¥éšæœºæ€§ï¼Œç„¶åå°†å…¶ä¸­åªæœ‰ä¸€ä¸ªä½œä¸ºåŸºå‡†æµ‹è¯•ä¸­çš„è§£å†³æ–¹æ¡ˆã€‚è¿™é™ä½äº†åŸºå‡†æµ‹è¯•çš„æœ€ä½³å¯èƒ½ç²¾åº¦ï¼Œå³è´å¶æ–¯ç²¾åº¦ã€‚è¿™ä¸ä»…æœ‰åŠ©äºæˆ‘ä»¬é¿å…é€éœ²çœŸå®ç­”æ¡ˆï¼Œè€Œä¸”è¿™ç§æ–¹æ³•è¿˜æä¾›äº†ä¸€ç§æ£€æµ‹æ•°æ®æ±¡æŸ“çš„æ–¹æ³•ã€‚åŸåˆ™ä¸Šï¼Œå³ä½¿æ˜¯éå¸¸å®Œå–„çš„æ¨¡å‹ä¹Ÿä¸åº”è¶…è¿‡è´å¶æ–¯ç²¾åº¦ã€‚å¦‚æœä¸€ä¸ªæ¨¡å‹è¶…å‡ºäº†è¿™ä¸ªä¸Šé™ï¼Œå°½ç®¡æœ‰è¿™æ ·çš„é¢„æœŸï¼Œè¿™ä¹Ÿæ˜¯æ•°æ®æ±¡æŸ“çš„ä¸€ä¸ªå¼ºçƒˆä¿¡å·ã€‚æˆ‘ä»¬æä¾›çš„å®éªŒè¯æ®è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•è®ºä¸­å‡†ç¡®åœ°æ£€æµ‹æ•°æ®æ±¡æŸ“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18102v2">PDF</a> Extended version of the paper presented as an Oral at the ICML 2025   Workshop on the Impact of Memorization on Trustworthy Foundation Models</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è®¨è®ºäº†åœ¨äº’è”ç½‘ä¸Šå‘å¸ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºå‡†æµ‹è¯•çš„é£é™©ï¼Œå¯èƒ½ä¼šæ±¡æŸ“æœªæ¥çš„LLMã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å‘å¸ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å‡†å¤‡å¤šä¸ªé€»è¾‘æ­£ç¡®çš„ç­”æ¡ˆå¹¶éšæœºé€‰æ‹©å…¶ä¸­ä¹‹ä¸€ä½œä¸ºè§£å†³æ–¹æ¡ˆæ¥æ³¨å…¥éšæœºæ€§ï¼Œä»¥ç»´æŒå…¬å¼€è¯„ä¼°LLMçš„èƒ½åŠ›çš„åŒæ—¶ä¸é€éœ²é—®é¢˜çš„çœŸå®ç­”æ¡ˆã€‚è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºä¸é€éœ²çœŸå®ç­”æ¡ˆï¼Œè€Œä¸”è¿˜ä¸ºæ£€æµ‹æ•°æ®æ±¡æŸ“æä¾›äº†ä¸€ä¸ªæµ‹è¯•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨å„ç§åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•è®ºä¸­å‡†ç¡®æ£€æµ‹æ•°æ®æ±¡æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘å¸ƒLLMåŸºå‡†æµ‹è¯•å¯èƒ½å­˜åœ¨æ±¡æŸ“æœªæ¥LLMçš„é£é™©ã€‚</li>
<li>å¸¸ç”¨çš„ç¼“è§£ç­–ç•¥æ˜¯ä¿æŒåŸºå‡†æµ‹è¯•ç§æœ‰ï¼Œä½†è¿™ç§æ–¹æ³•éœ€è¦ä¿¡ä»»å•ä¸€ç»„ç»‡å¹¶å¯èƒ½å…è®¸é€šè¿‡é‡å¤æŸ¥è¯¢è¿›è¡Œè¿‡åº¦æ‹Ÿåˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•å‘å¸ƒæ–¹æ³•ï¼Œé€šè¿‡æ³¨å…¥éšæœºæ€§æ¥å…¬å¼€è¯„ä¼°LLMï¼ŒåŒæ—¶ä¸é€éœ²é—®é¢˜çš„çœŸå®ç­”æ¡ˆã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å‡†å¤‡å¤šä¸ªé€»è¾‘æ­£ç¡®çš„ç­”æ¡ˆï¼Œåªé€‰æ‹©å…¶ä¸­ä¹‹ä¸€ä½œä¸ºè§£å†³æ–¹æ¡ˆæ¥å®ç°éšæœºæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æœ‰åŠ©äºé˜²æ­¢é€éœ²çœŸå®ç­”æ¡ˆï¼Œè€Œä¸”æä¾›äº†ä¸€ä¸ªæ£€æµ‹æ•°æ®æ±¡æŸ“çš„ä¿¡å·ï¼šæ¨¡å‹çš„è¡¨ç°å¦‚æœè¶…è¶Šäº†æ ¹æ®è´å¶æ–¯å‡†ç¡®ç‡é¢„æœŸçš„æ°´å¹³ï¼Œåˆ™å¯èƒ½æ˜¯æ•°æ®æ±¡æŸ“çš„ä¿¡å·ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•å¯åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•è®ºä¸­å‡†ç¡®æ£€æµ‹æ•°æ®æ±¡æŸ“ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºä¿æŒåŸºå‡†æµ‹è¯•çš„å…¬æ­£æ€§å’Œé€æ˜åº¦ï¼ŒåŒæ—¶å‡å°‘æ•°æ®æ±¡æŸ“çš„é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d1eabd83e77080c1fa1c90452568ff12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0416b25abb011038b6d2dc07b5ff79af.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Colombian-Waitresses-y-Jueces-canadienses-Gender-and-Country-Biases-in-Occupation-Recommendations-from-LLMs"><a href="#Colombian-Waitresses-y-Jueces-canadienses-Gender-and-Country-Biases-in-Occupation-Recommendations-from-LLMs" class="headerlink" title="Colombian Waitresses y Jueces canadienses: Gender and Country Biases in   Occupation Recommendations from LLMs"></a>Colombian Waitresses y Jueces canadienses: Gender and Country Biases in   Occupation Recommendations from LLMs</h2><p><strong>Authors:Elisa Forcada RodrÃ­guez, Olatz Perez-de-ViÃ±aspre, Jon Ander Campos, Dietrich Klakow, Vagrant Gautam</strong></p>
<p>One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„å…¬å¹³æ€§ç ”ç©¶çš„ç›®æ ‡ä¹‹ä¸€æ˜¯è¡¡é‡å’Œå‡è½»ç”±NLPç³»ç»Ÿä¼ æ’­çš„åˆ»æ¿åè§ã€‚ç„¶è€Œï¼Œè¿™ç±»å·¥ä½œå¾€å¾€ä¾§é‡äºå•ä¸€åè§è½´ï¼ˆé€šå¸¸æ˜¯æ€§åˆ«ï¼‰å’Œè‹±è¯­è¯­è¨€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹å¤šè¯­è¨€äº¤å‰å›½å®¶ä¸æ€§åˆ«åè§è¿›è¡Œäº†é¦–æ¬¡ç ”ç©¶ï¼Œé‡ç‚¹ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„èŒä¸šæ¨èã€‚æˆ‘ä»¬åœ¨è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­ä¸­æ„å»ºäº†åŸºå‡†æç¤ºï¼Œç³»ç»Ÿåœ°æ”¹å˜å›½å®¶å’Œæ€§åˆ«ï¼Œä½¿ç”¨25ä¸ªå›½å®¶å’Œå››ç»„ä»£è¯ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨è¿™ä¸€åŸºå‡†ä¸Šå¯¹äº”ä¸ªåŸºäºLlamaçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åŒ…å«äº†é‡è¦çš„æ€§åˆ«å’Œå›½å®¶åè§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ¨¡å‹åœ¨å•ç‹¬è€ƒè™‘æ€§åˆ«æˆ–å›½å®¶æ—¶è¡¨ç°å‡ºå…¬å¹³æ€§ï¼ŒåŸºäºå›½å®¶å’Œæ€§åˆ«çš„äº¤å‰èŒä¸šåè§ä»ç„¶å­˜åœ¨ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæç¤ºè¯­è¨€å¯¹åè§æœ‰å¾ˆå¤§å½±å“ï¼Œè€ŒæŒ‡ä»¤è°ƒæ•´æ¨¡å‹åˆ™å§‹ç»ˆè¡¨ç°å‡ºæœ€ä½ä¸”æœ€ç¨³å®šçš„åè§æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒï¼Œå…¬å¹³ç ”ç©¶äººå‘˜éœ€è¦åœ¨å·¥ä½œä¸­ä½¿ç”¨äº¤å‰å’Œå¤šè¯­è¨€çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02456v2">PDF</a> Workshop on Gender Bias in Natural Language Processing at ACL 2025</p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œä½œè€…è¡¡é‡å¹¶å‡è½»äº†ç”±è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿä¼ æ’­çš„èŒä¸šåè§é—®é¢˜ã€‚ä»–ä»¬é¦–æ¬¡è¿›è¡Œäº†å¤šè¯­è¨€äº¤å‰å›½å®¶ä¸æ€§åˆ«åè§çš„ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„èŒä¸šæ¨èåè§ã€‚ç ”ç©¶é€šè¿‡è‹±è¯­ã€è¥¿ç­ç‰™è¯­å’Œå¾·è¯­çš„ç³»ç»ŸåŒ–æµ‹è¯•ï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„æ€§åˆ«å’Œå›½ç±åè§ï¼Œå³ä½¿æ¨¡å‹åœ¨å•ä¸€ç»´åº¦ä¸Šè¡¨ç°å‡ºå…¬å¹³æ€§ï¼ŒåŸºäºå›½å®¶å’Œæ€§åˆ«çš„äº¤å‰åè§ä»ç„¶å­˜åœ¨ã€‚ç ”ç©¶è¿˜æ˜¾ç¤ºæç¤ºè¯­è¨€æ˜¾è‘—å½±å“åè§ï¼Œè€ŒæŒ‡ä»¤è®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºçš„åè§ç¨‹åº¦æœ€ä½ä¸”æœ€ç¨³å®šã€‚è¿™é¡¹ç ”ç©¶å¼ºè°ƒå…¬å¹³æ€§ç ”ç©¶äººå‘˜éœ€è¦åœ¨å·¥ä½œä¸­ä½¿ç”¨äº¤å‰å’Œå¤šè¯­è¨€è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶è¡¡é‡å¹¶å‡è½»äº†è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿä¸­çš„èŒä¸šåè§é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡è¿›è¡Œäº†å¤šè¯­è¨€äº¤å‰å›½å®¶ä¸æ€§åˆ«åè§çš„ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨æ˜¾è‘—çš„æ€§åˆ«å’Œå›½ç±åè§ã€‚</li>
<li>å³ä½¿æ¨¡å‹åœ¨å•ä¸€ç»´åº¦ä¸Šè¡¨ç°å‡ºå…¬å¹³æ€§ï¼ŒåŸºäºå›½å®¶å’Œæ€§åˆ«çš„äº¤å‰åè§ä»ç„¶å­˜åœ¨ã€‚</li>
<li>æç¤ºè¯­è¨€æ˜¾è‘—å½±å“åè§ã€‚</li>
<li>æŒ‡ä»¤è®­ç»ƒæ¨¡å‹åœ¨èŒä¸šæ¨èåè§é—®é¢˜ä¸Šè¡¨ç°æœ€ä½³ï¼Œåè§ç¨‹åº¦æœ€ä½ä¸”æœ€ç¨³å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9439acb12944cf386fae297699ae9bf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0278cc84a4b0b3bab25d874e14e8d420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a09fbba1d53aa7eecdc55a7a6cab1a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ec97b191e010d80be4ea8b1eac48abd.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a63c44975d3d4faa55d2ab02950c7f5.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Efficient Masked Attention Transformer for Few-Shot Classification and   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c4bb5b5ae33416bcfcb29963833e00a1.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  LLMs Between the Nodes Community Discovery Beyond Vectors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
