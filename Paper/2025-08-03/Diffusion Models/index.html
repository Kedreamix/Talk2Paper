<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-03  LesionGen A Concept-Guided Diffusion Model for Dermatology Image   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-677de526bfccdc43e4d4d0d55ba4ae1a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-03-更新"><a href="#2025-08-03-更新" class="headerlink" title="2025-08-03 更新"></a>2025-08-03 更新</h1><h2 id="LesionGen-A-Concept-Guided-Diffusion-Model-for-Dermatology-Image-Synthesis"><a href="#LesionGen-A-Concept-Guided-Diffusion-Model-for-Dermatology-Image-Synthesis" class="headerlink" title="LesionGen: A Concept-Guided Diffusion Model for Dermatology Image   Synthesis"></a>LesionGen: A Concept-Guided Diffusion Model for Dermatology Image   Synthesis</h2><p><strong>Authors:Jamil Fayyad, Nourhan Bayasi, Ziyang Yu, Homayoun Najjaran</strong></p>
<p>Deep learning models for skin disease classification require large, diverse, and well-annotated datasets. However, such resources are often limited due to privacy concerns, high annotation costs, and insufficient demographic representation. While text-to-image diffusion probabilistic models (T2I-DPMs) offer promise for medical data synthesis, their use in dermatology remains underexplored, largely due to the scarcity of rich textual descriptions in existing skin image datasets. In this work, we introduce LesionGen, a clinically informed T2I-DPM framework for dermatology image synthesis. Unlike prior methods that rely on simplistic disease labels, LesionGen is trained on structured, concept-rich dermatological captions derived from expert annotations and pseudo-generated, concept-guided reports. By fine-tuning a pretrained diffusion model on these high-quality image-caption pairs, we enable the generation of realistic and diverse skin lesion images conditioned on meaningful dermatological descriptions. Our results demonstrate that models trained solely on our synthetic dataset achieve classification accuracy comparable to those trained on real images, with notable gains in worst-case subgroup performance. Code and data are available here. </p>
<blockquote>
<p>深度学习模型在皮肤疾病分类方面需要大规模、多样化和标注良好的数据集。然而，由于隐私担忧、标注成本高昂以及人口代表性不足等问题，此类资源往往有限。虽然文本到图像扩散概率模型（T2I-DPMs）在医学数据合成方面显示出潜力，但它们在皮肤科的应用仍被较少探索，这主要是因为现有皮肤图像数据集中缺乏丰富的文本描述。在这项工作中，我们引入了LesionGen，这是一个基于临床信息的皮肤科图像合成T2I-DPM框架。不同于依赖简单疾病标签的先前方法，LesionGen以专家标注生成的丰富概念的皮肤科标题以及伪造的基于概念的报告进行训练。通过对这些高质量图像与标题配对进行微调扩散模型，我们能够根据具有意义的皮肤科描述生成逼真且多样化的皮肤病变图像。我们的结果显示，仅在我们合成数据集上训练的模型达到了与真实图像训练相当的分类精度，并且在最差情况下的小组表现中取得了显著的提升。代码和数据集可在此处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23001v1">PDF</a> Accepted at the MICCAI 2025 ISIC Workshop</p>
<p><strong>Summary</strong></p>
<p>基于深度学习模型的皮肤疾病分类需要大规模、多样化和标注良好的数据集。然而，由于隐私担忧、高标注成本和人口代表性不足，此类资源往往有限。文本到图像扩散概率模型（T2I-DPMs）在医学数据合成方面显示出潜力，但在皮肤科的应用仍然探索不足，这主要是由于现有皮肤图像数据集中丰富的文本描述稀缺。在此研究中，我们引入了LesionGen，这是一个基于临床信息的皮肤科图像合成T2I-DPM框架。不同于依赖简单疾病标签的先前方法，LesionGen的训练数据是结构化的、概念丰富的皮肤科注释以及伪生成的、概念引导的报告。通过在这些高质量图像-字幕对上对预训练的扩散模型进行微调，我们能够根据有意义的皮肤科描述生成真实且多样的皮肤病变图像。实验结果表明，仅在我们的合成数据集上训练的模型，其分类准确率可与在真实图像上训练的模型相媲美，并且在最不利的情况下取得了明显的性能提升。相关代码和数据可在XX获得。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在皮肤疾病分类中受限于大规模、多样化和标注良好的数据集缺乏的问题。</li>
<li>文本到图像扩散概率模型（T2I-DPMs）在医学数据合成中有潜力，但在皮肤科应用尚未得到充分探索。</li>
<li>LesionGen是一个基于临床信息的皮肤科图像合成框架，使用结构化的皮肤科注释和概念引导的报告进行训练。</li>
<li>LesionGen通过高质量图像-字幕对预训练的扩散模型进行微调，生成真实且多样的皮肤病变图像。</li>
<li>仅使用合成数据集训练的模型可以达到与真实图像训练的模型相当的分类准确率。</li>
<li>在最不利的情况下，模型性能有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23001">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bfe700b8c55a3f3b29bb7ea017e53fcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d851bc9b071132ba74cdbc600cf07d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-132f23f1ad0ae87f60e12af69e032572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b84000cf5a37d72bdb530d94493b058c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DepR-Depth-Guided-Single-view-Scene-Reconstruction-with-Instance-level-Diffusion"><a href="#DepR-Depth-Guided-Single-view-Scene-Reconstruction-with-Instance-level-Diffusion" class="headerlink" title="DepR: Depth Guided Single-view Scene Reconstruction with Instance-level   Diffusion"></a>DepR: Depth Guided Single-view Scene Reconstruction with Instance-level   Diffusion</h2><p><strong>Authors:Qingcheng Zhao, Xiang Zhang, Haiyang Xu, Zeyuan Chen, Jianwen Xie, Yuan Gao, Zhuowen Tu</strong></p>
<p>We propose DepR, a depth-guided single-view scene reconstruction framework that integrates instance-level diffusion within a compositional paradigm. Instead of reconstructing the entire scene holistically, DepR generates individual objects and subsequently composes them into a coherent 3D layout. Unlike previous methods that use depth solely for object layout estimation during inference and therefore fail to fully exploit its rich geometric information, DepR leverages depth throughout both training and inference. Specifically, we introduce depth-guided conditioning to effectively encode shape priors into diffusion models. During inference, depth further guides DDIM sampling and layout optimization, enhancing alignment between the reconstruction and the input image. Despite being trained on limited synthetic data, DepR achieves state-of-the-art performance and demonstrates strong generalization in single-view scene reconstruction, as shown through evaluations on both synthetic and real-world datasets. </p>
<blockquote>
<p>我们提出了DepR，这是一个深度引导的单视图场景重建框架，它在一个组合范式中集成了实例级别的扩散。DepR不同于整体重建整个场景的方法，而是生成单个物体，然后将其组合成一个连贯的3D布局。与之前的方法不同，这些方法仅在推理过程中使用深度进行对象布局估计，因此未能充分利用其丰富的几何信息，DepR在训练和推理过程中都利用了深度信息。具体来说，我们引入了深度引导条件，以有效地将形状先验知识编码到扩散模型中。在推理过程中，深度进一步引导DDIM采样和布局优化，提高了重建与输入图像之间的对齐性。尽管DepR是在有限的合成数据上训练的，但在单视图场景重建方面取得了最先进的性能，并在合成和真实世界数据集上的评估中展示了强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22825v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了DepR框架，这是一个深度引导的单视图场景重建框架，它在一组成分的模式中集成了实例级别的扩散。DepR不同于整体重建整个场景的方法，而是生成单个物体，然后将其组合成一个连贯的3D布局。不同于之前仅在推理阶段使用深度进行物体布局估计的方法，未能充分利用深度的丰富几何信息，DepR在训练和推理阶段都使用深度信息。具体来说，我们引入了深度引导条件，以有效地将形状先验知识编码到扩散模型中。在推理阶段，深度进一步引导DDIM采样和布局优化，提高了重建与输入图像的契合度。即使在有限的合成数据上进行训练，DepR仍取得了最先进的性能，并在单视图场景重建中展示了强大的泛化能力，通过合成和真实世界数据集的评价得到了验证。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DepR是一个深度引导的单视图场景重建框架，采用成分式方法生成并组合个体物体。</li>
<li>不同于整体重建方法，DepR生成单个物体并组合成连贯的3D布局。</li>
<li>DepR在训练和推理阶段都利用深度信息，而之前的方法主要只在推理阶段使用深度进行物体布局估计。</li>
<li>引入深度引导条件，有效编码形状先验知识到扩散模型中。</li>
<li>深度信息在推理阶段指导DDIM采样和布局优化，提高重建与输入图像的契合度。</li>
<li>DepR在有限的合成数据上训练，但取得了先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-677de526bfccdc43e4d4d0d55ba4ae1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa21c3b00e54c2a913b99818589cc6b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4995cfff3e18a8cb4127b8046313b611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e6ad73fc20296731c72abc36e775e42.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LOTS-of-Fashion-Multi-Conditioning-for-Image-Generation-via-Sketch-Text-Pairing"><a href="#LOTS-of-Fashion-Multi-Conditioning-for-Image-Generation-via-Sketch-Text-Pairing" class="headerlink" title="LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text   Pairing"></a>LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text   Pairing</h2><p><strong>Authors:Federico Girella, Davide Talon, Ziyue Liu, Zanxi Ruan, Yiming Wang, Marco Cristani</strong></p>
<p>Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model’s multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization. </p>
<blockquote>
<p>服装设计是一个复杂的创意过程，融合了视觉和文本表达。设计师通过草图传达想法，这些草图定义了空间结构和设计元素，以及文本描述，捕捉材料、纹理和风格细节。在本文中，我们介绍了用于服装图像生成的局部文本和草图（LOTS），这是一种基于组合草图文本生成完整时尚外观的方法。LOTS利用全局描述与配对局部草图+文本信息进行条件设置，并引入了一种基于步骤的合并策略来进行扩散适应。首先，模块化配对中心表示法将草图和文本编码到共享潜在空间中，同时保留独立的局部特征；然后，扩散对指导阶段通过扩散模型的多步去噪过程中的注意力导向整合了局部和全局条件。为了验证我们的方法，我们在Fashionpedia的基础上构建了Sketchy数据集，这是第一个每张图像都提供多个文本草图对的数据集。定量结果表明，LOTS在全球和局部指标上都达到了最先进的图像生成性能，而定性示例和人类评估研究则突出了其前所未有的设计定制水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22627v1">PDF</a> Accepted at ICCV25 (Oral). Project page:   <a target="_blank" rel="noopener" href="https://intelligolabs.github.io/lots/">https://intelligolabs.github.io/lots/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于组合草图与文本信息的时尚图像生成方法——LOTS。该方法通过利用全局描述与配对局部草图及文本信息来适应扩散模型，引入了一种新型的基于步骤的合并策略。首先，采用模块化配对中心表示法将草图和文本编码到共享潜在空间中，同时保留独立的局部特征；接着，在扩散模型的去噪过程中的多步骤中，通过注意力导向机制整合局部和全局条件，形成扩散配对导向阶段。为验证方法的有效性，研究团队在Fashionpedia的基础上构建了Sketchy数据集，该数据集为每个图像提供了多个文本草图对。定量结果表明，LOTS在全局和局部指标上均实现了图像生成性能的最佳状态，而定性示例和人为评估研究突出了其前所未有的设计定制水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时尚设计是一个复杂的创意过程，结合视觉和文本表达。</li>
<li>LOTS方法利用全局描述与配对局部草图及文本信息，为适应扩散模型提供条件。</li>
<li>提出了一种新颖的基于步骤的合并策略，整合局部和全局条件。</li>
<li>通过模块化配对中心表示法，将草图和文本编码到共享潜在空间并保留独立局部特征。</li>
<li>扩散配对导向阶段通过注意力机制实现局部和全局信息的结合。</li>
<li>为验证方法的有效性，建立了Sketchy数据集，每个图像包含多个文本草图对。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1dd310705d3aff45f64d8e5cc12176d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b029cade73249cf1f07a0b5dec169137.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c833ecb79f4d9c730af2e9f2f8516ed1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Transformed-Low-rank-Adaptation-via-Tensor-Decomposition-and-Its-Applications-to-Text-to-image-Models"><a href="#Transformed-Low-rank-Adaptation-via-Tensor-Decomposition-and-Its-Applications-to-Text-to-image-Models" class="headerlink" title="Transformed Low-rank Adaptation via Tensor Decomposition and Its   Applications to Text-to-image Models"></a>Transformed Low-rank Adaptation via Tensor Decomposition and Its   Applications to Text-to-image Models</h2><p><strong>Authors:Zerui Tao, Yuhta Takida, Naoki Murata, Qibin Zhao, Yuki Mitsufuji</strong></p>
<p>Parameter-Efficient Fine-Tuning (PEFT) of text-to-image models has become an increasingly popular technique with many applications. Among the various PEFT methods, Low-Rank Adaptation (LoRA) and its variants have gained significant attention due to their effectiveness, enabling users to fine-tune models with limited computational resources. However, the approximation gap between the low-rank assumption and desired fine-tuning weights prevents the simultaneous acquisition of ultra-parameter-efficiency and better performance. To reduce this gap and further improve the power of LoRA, we propose a new PEFT method that combines two classes of adaptations, namely, transform and residual adaptations. In specific, we first apply a full-rank and dense transform to the pre-trained weight. This learnable transform is expected to align the pre-trained weight as closely as possible to the desired weight, thereby reducing the rank of the residual weight. Then, the residual part can be effectively approximated by more compact and parameter-efficient structures, with a smaller approximation error. To achieve ultra-parameter-efficiency in practice, we design highly flexible and effective tensor decompositions for both the transform and residual adaptations. Additionally, popular PEFT methods such as DoRA can be summarized under this transform plus residual adaptation scheme. Experiments are conducted on fine-tuning Stable Diffusion models in subject-driven and controllable generation. The results manifest that our method can achieve better performances and parameter efficiency compared to LoRA and several baselines. </p>
<blockquote>
<p>文本到图像模型的参数高效微调（PEFT）技术越来越受到欢迎，并有许多应用。在各种PEFT方法中，低秩适应（LoRA）及其变体因其有效性而备受关注，使用户能够在有限的计算资源下微调模型。然而，低秩假设与所需的微调权重之间的近似差距阻碍了超参数效率和性能的同时提升。为了减少这个差距并进一步提升LoRA的威力，我们提出了一种新的PEFT方法，该方法结合了两种适应类别，即变换适应和残差适应。具体来说，我们首先应用全秩和密集变换到预训练权重上。这个可学习的变换期望尽可能紧密地对齐预训练权重到所需权重，从而减少残差权重的秩。然后，残差部分可以通过更紧凑和参数高效的结构进行有效的近似，以减小近似误差。为了实现实践中的超参数效率，我们为变换和残差适应都设计了高度灵活和有效的张量分解。此外，流行的PEFT方法如DoRA可以归纳在这种变换加残差适应方案下。实验在微调Stable Diffusion模型的主题驱动和可控生成上进行了实施。结果表明，我们的方法相较于LoRA和一些基线方法，在性能和参数效率方面都能取得更好的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08727v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的参数高效微调（PEFT）方法，结合了转换和残差适应两类适应方式，用于优化文本到图像模型的微调。该方法首先应用全秩密集转换来尽可能地接近预训练权重和目标权重，然后通过更紧凑和参数高效的结构有效地近似剩余部分，以减小近似误差。实验表明，该方法在主题驱动和可控生成方面对Stable Diffusion模型的微调具有更好的性能和参数效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PEFT方法结合了转换和残差适应，旨在缩小低秩假设与所需微调权重之间的近似差距。</li>
<li>通过全秩密集转换来接近预训练权重和目标权重，减少剩余权重的秩。</li>
<li>剩余部分通过更紧凑和参数高效的结构进行有效近似，减小近似误差。</li>
<li>提出高度灵活和有效的张量分解来实现实际中的超参数效率。</li>
<li>现有的PEFT方法如DoRA可归纳为此转换加残差适应方案。</li>
<li>实验结果表明，该方法在主题驱动和可控生成方面对Stable Diffusion模型的微调具有更好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08727">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-df58bbf2ebf814aad45ef4b85224ee2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d1780dfb75c527a2941d93aef0601fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef534cf6e7aac0e62848879c6967bb46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-544f82290b0c099fd550c8b1a377eccc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="An-Inversion-based-Measure-of-Memorization-for-Diffusion-Models"><a href="#An-Inversion-based-Measure-of-Memorization-for-Diffusion-Models" class="headerlink" title="An Inversion-based Measure of Memorization for Diffusion Models"></a>An Inversion-based Measure of Memorization for Diffusion Models</h2><p><strong>Authors:Zhe Ma, Qingming Li, Xuhong Zhang, Tianyu Du, Ruixiao Lin, Zonghui Wang, Shouling Ji, Wenzhi Chen</strong></p>
<p>The past few years have witnessed substantial advances in image generation powered by diffusion models. However, it was shown that diffusion models are susceptible to training data memorization, raising significant concerns regarding copyright infringement and privacy invasion. This study delves into a rigorous analysis of memorization in diffusion models. We introduce InvMM, an inversion-based measure of memorization, which is based on inverting a sensitive latent noise distribution accounting for the replication of an image. For accurate estimation of the measure, we propose an adaptive algorithm that balances the normality and sensitivity of the noise distribution. Comprehensive experiments across four datasets, conducted on both unconditional and text-guided diffusion models, demonstrate that InvMM provides a reliable and complete quantification of memorization. Notably, InvMM is commensurable between samples, reveals the true extent of memorization from an adversarial standpoint and implies how memorization differs from membership. In practice, it serves as an auditing tool for developers to reliably assess the risk of memorization, thereby contributing to the enhancement of trustworthiness and privacy-preserving capabilities of diffusion models. </p>
<blockquote>
<p>过去几年，扩散模型在图像生成领域取得了重大进展。然而，研究表明扩散模型容易记忆训练数据，这引发了关于版权侵犯和隐私侵犯的担忧。本研究深入分析了扩散模型中的记忆问题。我们介绍了InvMM，这是一种基于反演的记忆度量方法，它基于敏感潜在噪声分布的反演来计量图像的复制程度。为了准确估计该度量标准，我们提出了一种平衡噪声分布的正常性和敏感性的自适应算法。在无条件扩散模型和文本引导扩散模型上进行的四个数据集的综合实验表明，InvMM提供了可靠的记忆量化方法。值得注意的是，InvMM在样本之间具有可比较性，从对抗角度揭示了记忆的真实程度，并显示了记忆与成员资格之间的差异。在实践中，它作为开发人员可靠的评估记忆风险的审计工具，有助于提高扩散模型的可靠性和隐私保护能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.05846v3">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>     扩散模型在图像生成领域取得了显著进展，但其易受到训练数据记忆能力的影响，引发版权和隐私侵权问题。本研究深入探讨了扩散模型的记忆能力问题，提出了基于反演的测量手段InvMM，能有效评估图像复制中的敏感潜在噪声分布。通过自适应算法准确估计测量值，并在四个数据集上进行了实验验证。InvMM可靠且全面地量化记忆能力，与样本之间具有可比性，揭示了记忆能力的真实程度，并强调了其与成员身份的区别。它可作为开发人员评估记忆风险的可信工具，有助于提高扩散模型的可靠性和隐私保护能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成领域取得显著进展，但存在训练数据记忆能力问题。</li>
<li>训练数据记忆能力问题可能引发版权和隐私侵权问题。</li>
<li>提出了基于反演的测量手段InvMM来评估图像复制中的敏感潜在噪声分布。</li>
<li>通过自适应算法准确估计测量值，进行实验研究验证。</li>
<li>InvMM可靠且全面地量化记忆能力，具有样本间可比性。</li>
<li>InvMM能揭示记忆能力的真实程度，与成员身份有所区别。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.05846">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d267c7c196cc9a26f288de6b1f37cbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5713eb059f9fc14ca8bb82edf9dfbf0a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41a4b597c8a50cf656f95279373067ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a64470776f353da294231f08a9248058.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompt-Based-Exemplar-Super-Compression-and-Regeneration-for-Class-Incremental-Learning"><a href="#Prompt-Based-Exemplar-Super-Compression-and-Regeneration-for-Class-Incremental-Learning" class="headerlink" title="Prompt-Based Exemplar Super-Compression and Regeneration for   Class-Incremental Learning"></a>Prompt-Based Exemplar Super-Compression and Regeneration for   Class-Incremental Learning</h2><p><strong>Authors:Ruxiao Duan, Jieneng Chen, Adam Kortylewski, Alan Yuille, Yaoyao Liu</strong></p>
<p>Replay-based methods in class-incremental learning (CIL) have attained remarkable success. Despite their effectiveness, the inherent memory restriction results in saving a limited number of exemplars with poor diversity. In this paper, we introduce PESCR, a novel approach that substantially increases the quantity and enhances the diversity of exemplars based on a pre-trained general-purpose diffusion model, without fine-tuning it on target datasets or storing it in the memory buffer. Images are compressed into visual and textual prompts, which are saved instead of the original images, decreasing memory consumption by a factor of 24. In subsequent phases, diverse exemplars are regenerated by the diffusion model. We further propose partial compression and diffusion-based data augmentation to minimize the domain gap between generated exemplars and real images. PESCR significantly improves CIL performance across multiple benchmarks, e.g., 3.2% above the previous state-of-the-art on ImageNet-100. </p>
<blockquote>
<p>在类增量学习（CIL）中，基于重放的方法已经取得了显著的成果。尽管它们有效，但固有的内存限制导致保存的样本数量有限且多样性较差。在本文中，我们介绍了PESCR，这是一种基于预训练的通用扩散模型的新方法，可以在不微调目标数据集或将其存储在内存缓冲区中的情况下，大幅增加样本数量并提高样本的多样性。图像被压缩成视觉和文本提示，代替原始图像进行保存，使内存消耗减少了24倍。在后续阶段，扩散模型会重新生成多样化的样本。我们还提出了部分压缩和基于扩散的数据增强，以最小化生成样本和真实图像之间的域差距。PESCR在多个基准测试上显著提高了CIL的性能，例如在ImageNet-100上的性能超过了之前的最先进水平3.2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.18266v3">PDF</a> BMVC 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/KerryDRX/PESCR">https://github.com/KerryDRX/PESCR</a></p>
<p><strong>摘要</strong></p>
<p>回放方法在班级增量学习（CIL）中取得了显著的成功。然而，由于内存限制，这些方法只能保存有限数量的样本，且多样性较差。本文介绍了一种新方法PESCR，它基于预训练的通用扩散模型，在不对目标数据集进行微调或占用内存缓冲区的情况下，大幅增加样本数量并提高样本多样性。图像被压缩成视觉和文本提示，替代原始图像进行保存，降低了24倍的内存消耗。随后，扩散模型再生各种样本。为进一步缩小生成样本与真实图像之间的域差距，我们还提出了部分压缩和基于扩散的数据增强方法。PESCR在多个基准测试上显著提高了CIL性能，例如在ImageNet-100上的表现优于最新技术状态3.2%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>回放方法在班级增量学习中表现优秀，但受内存限制影响，保存样本数量有限且多样性不足。</li>
<li>PESCR方法基于预训练的通用扩散模型，能提高样本数量和多样性。</li>
<li>图像被压缩成视觉和文本提示来保存，大幅度降低了内存消耗。</li>
<li>扩散模型在后续阶段再生各种样本。</li>
<li>部分压缩技术用于缩小生成样本和真实图像之间的域差距。</li>
<li>PESCR在多个基准测试上显著提高了班级增量学习的性能。</li>
<li>在ImageNet-100上，PESCR的表现优于其他方法，表现出较大的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.18266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-08541d35f29518abcf652eac4fd067dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3116f9aa83e8339f5a606b2288f5ba89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b5de280d7bd87bf3307f5c5bf8dd46f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b1b89f0316d4305b5d9eeadc5744ba88.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-03  OptiGradTrust Byzantine-Robust Federated Learning with Multi-Feature   Gradient Analysis and Reinforcement Learning-Based Trust Weighting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-eae8a16bbe80a86db4bee31efd988011.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-03  NVS-SQA Exploring Self-Supervised Quality Representation Learning for   Neurally Synthesized Scenes without References
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
