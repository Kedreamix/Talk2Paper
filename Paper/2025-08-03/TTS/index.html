<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  SpeechFake A Large-Scale Multilingual Speech Deepfake Dataset   Incorporating Cutting-Edge Generation Methods">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-40549d220dcc4048b5af741c3260e1b6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    56 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="SpeechFake-A-Large-Scale-Multilingual-Speech-Deepfake-Dataset-Incorporating-Cutting-Edge-Generation-Methods"><a href="#SpeechFake-A-Large-Scale-Multilingual-Speech-Deepfake-Dataset-Incorporating-Cutting-Edge-Generation-Methods" class="headerlink" title="SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset   Incorporating Cutting-Edge Generation Methods"></a>SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset   Incorporating Cutting-Edge Generation Methods</h2><p><strong>Authors:Wen Huang, Yanmei Gu, Zhiming Wang, Huijia Zhu, Yanmin Qian</strong></p>
<p>As speech generation technology advances, the risk of misuse through deepfake audio has become a pressing concern, which underscores the critical need for robust detection systems. However, many existing speech deepfake datasets are limited in scale and diversity, making it challenging to train models that can generalize well to unseen deepfakes. To address these gaps, we introduce SpeechFake, a large-scale dataset designed specifically for speech deepfake detection. SpeechFake includes over 3 million deepfake samples, totaling more than 3,000 hours of audio, generated using 40 different speech synthesis tools. The dataset encompasses a wide range of generation techniques, including text-to-speech, voice conversion, and neural vocoder, incorporating the latest cutting-edge methods. It also provides multilingual support, spanning 46 languages. In this paper, we offer a detailed overview of the datasetâ€™s creation, composition, and statistics. We also present baseline results by training detection models on SpeechFake, demonstrating strong performance on both its own test sets and various unseen test sets. Additionally, we conduct experiments to rigorously explore how generation methods, language diversity, and speaker variation affect detection performance. We believe SpeechFake will be a valuable resource for advancing speech deepfake detection and developing more robust models for evolving generation techniques. </p>
<blockquote>
<p>éšç€è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„è¿›æ­¥ï¼Œæ·±åº¦ä¼ªé€ éŸ³é¢‘çš„æ»¥ç”¨é£é™©å·²æˆä¸ºç´§è¿«çš„é—®é¢˜ï¼Œè¿™å‡¸æ˜¾äº†å¯¹ç¨³å¥æ£€æµ‹ç³»ç»Ÿçš„è¿«åˆ‡éœ€æ±‚ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„è¯­éŸ³æ·±åº¦ä¼ªé€ æ•°æ®é›†åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä½¿å¾—è®­ç»ƒèƒ½å¤Ÿè‰¯å¥½æ³›åŒ–åˆ°æœªè§è¿‡çš„æ·±åº¦ä¼ªé€ æ•°æ®çš„æ¨¡å‹å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SpeechFakeâ€”â€”ä¸€ä¸ªä¸“é—¨ç”¨äºè¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚SpeechFakeåŒ…å«è¶…è¿‡300ä¸‡ä¸ªæ·±åº¦ä¼ªé€ æ ·æœ¬ï¼Œæ€»éŸ³é¢‘æ—¶é•¿è¶…è¿‡3000å°æ—¶ï¼Œä½¿ç”¨40ç§ä¸åŒçš„è¯­éŸ³åˆæˆå·¥å…·ç”Ÿæˆã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„ç”ŸæˆæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¯­éŸ³ã€è¯­éŸ³è½¬æ¢å’Œç¥ç»ç½‘ç»œç¼–è§£ç å™¨ï¼ŒåŒ…å«äº†æœ€æ–°çš„å‰æ²¿æ–¹æ³•ã€‚å®ƒè¿˜æä¾›äº†å¤šç§è¯­è¨€æ”¯æŒï¼Œæ¶µç›–46ç§è¯­è¨€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†æ•°æ®é›†åˆ›å»ºã€ç»„æˆå’Œç»Ÿè®¡çš„è¯¦ç»†ä»‹ç»ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åœ¨SpeechFakeä¸Šè®­ç»ƒæ£€æµ‹æ¨¡å‹æ¥æä¾›åŸºçº¿ç»“æœï¼Œåœ¨å…¶è‡ªèº«çš„æµ‹è¯•é›†å’Œå„ç§æœªè§è¿‡çš„æµ‹è¯•é›†ä¸Šéƒ½è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å®éªŒï¼Œä¸¥æ ¼æ¢è®¨äº†ç”Ÿæˆæ–¹æ³•ã€è¯­è¨€å¤šæ ·æ€§å’Œè¯´è¯äººå˜åŒ–å¦‚ä½•å½±å“æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡SpeechFakeå°†æˆä¸ºæ¨åŠ¨è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹å‘å±•å¹¶ä¸ºä¸æ–­å‘å±•çš„ç”ŸæˆæŠ€æœ¯å¼€å‘æ›´ç¨³å¥æ¨¡å‹çš„æœ‰ä»·å€¼èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21463v1">PDF</a> Published in ACL 2025. Dataset available at:   <a target="_blank" rel="noopener" href="https://github.com/YMLLG/SpeechFake">https://github.com/YMLLG/SpeechFake</a></p>
<p><strong>Summary</strong></p>
<p>éšç€è¯­éŸ³ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥ï¼Œæ·±åº¦ä¼ªé€ éŸ³é¢‘çš„æ»¥ç”¨é£é™©æ—¥ç›Šå‡¸æ˜¾ï¼Œå› æ­¤å¯¹å¯é æ£€æµ‹ç³»ç»Ÿçš„éœ€æ±‚å˜å¾—æä¸ºè¿«åˆ‡ã€‚ä¸ºè§£å†³ç°æœ‰è¯­éŸ³æ·±åº¦ä¼ªé€ æ•°æ®é›†è§„æ¨¡æœ‰é™ã€å¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸“é—¨ç”¨äºè¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹çš„SpeechFakeå¤§å‹æ•°æ®é›†ã€‚SpeechFakeåŒ…å«è¶…è¿‡300ä¸‡ä»½æ·±åº¦ä¼ªé€ æ ·æœ¬ï¼Œæ€»è®¡è¶…è¿‡3000å°æ—¶çš„éŸ³é¢‘ï¼Œä½¿ç”¨40ç§ä¸åŒçš„è¯­éŸ³åˆæˆå·¥å…·ç”Ÿæˆã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„ç”ŸæˆæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¯­éŸ³ã€è¯­éŸ³è½¬æ¢å’Œç¥ç»ç¼–è§£ç å™¨ï¼Œå¹¶èå…¥äº†æœ€æ–°çš„å°–ç«¯æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›46ç§è¯­è¨€çš„å¤šè¯­ç§æ”¯æŒã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†æ•°æ®é›†çš„åˆ›å»ºã€ç»„æˆå’Œç»Ÿè®¡æƒ…å†µã€‚é€šè¿‡åœ¨SpeechFakeä¸Šè®­ç»ƒæ£€æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬è·å¾—äº†è‰¯å¥½çš„åŸºçº¿ç»“æœï¼Œåœ¨å…¶è‡ªæœ‰æµ‹è¯•é›†å’Œå„ç§æœªè§æµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å®éªŒï¼Œä¸¥æ ¼æ¢è®¨äº†ç”Ÿæˆæ–¹æ³•ã€è¯­è¨€å¤šæ ·æ€§å’Œè¯´è¯äººå˜åŒ–å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬ç›¸ä¿¡SpeechFakeå°†ä¸ºæ¨è¿›è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹å’Œç ”ç©¶å…ˆè¿›çš„ç”ŸæˆæŠ€æœ¯æä¾›æ›´ç¨³å¥çš„æ¨¡å‹æä¾›å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥å¸¦æ¥äº†æ·±åº¦ä¼ªé€ éŸ³é¢‘çš„æ»¥ç”¨é£é™©ï¼Œéœ€è¦å¯é çš„æ£€æµ‹ç³»ç»Ÿã€‚</li>
<li>ç°æœ‰è¯­éŸ³æ·±åº¦ä¼ªé€ æ•°æ®é›†å­˜åœ¨è§„æ¨¡å’Œå¤šæ ·æ€§é—®é¢˜ï¼Œéš¾ä»¥è®­ç»ƒé€šç”¨æ¨¡å‹ã€‚</li>
<li>SpeechFakeæ•°æ®é›†åŒ…å«è¶…è¿‡300ä¸‡ä»½æ·±åº¦ä¼ªé€ æ ·æœ¬ï¼Œæ€»è®¡è¶…è¿‡3000å°æ—¶çš„éŸ³é¢‘ã€‚</li>
<li>SpeechFakeæ¶µç›–å¤šç§ç”ŸæˆæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¯­éŸ³ã€è¯­éŸ³è½¬æ¢å’Œç¥ç»ç¼–è§£ç å™¨ã€‚</li>
<li>SpeechFakeæä¾›å¤šè¯­ç§æ”¯æŒï¼Œæ¶µç›–46ç§è¯­è¨€ã€‚</li>
<li>åœ¨SpeechFakeä¸Šè®­ç»ƒçš„æ£€æµ‹æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-22c2e74de73687dcb25c5442f818d9fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e534f37a7509f689535123e99e189586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1d4e25b94383fc83f12f41831dd28ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cfea0999a20989ac192cc7062646965.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bca36162278b8620b162d0743b9d465.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d8ef6a36a62096ebffd417a766ff866.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-648d2982448bcafb5ecec4729f105a48.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AV-Deepfake1M-A-Large-Scale-Audio-Visual-Deepfake-Benchmark-with-Real-World-Perturbations"><a href="#AV-Deepfake1M-A-Large-Scale-Audio-Visual-Deepfake-Benchmark-with-Real-World-Perturbations" class="headerlink" title="AV-Deepfake1M++: A Large-Scale Audio-Visual Deepfake Benchmark with   Real-World Perturbations"></a>AV-Deepfake1M++: A Large-Scale Audio-Visual Deepfake Benchmark with   Real-World Perturbations</h2><p><strong>Authors:Zhixi Cai, Kartik Kuckreja, Shreya Ghosh, Akanksha Chuchra, Muhammad Haris Khan, Usman Tariq, Tom Gedeon, Abhinav Dhall</strong></p>
<p>The rapid surge of text-to-speech and face-voice reenactment models makes video fabrication easier and highly realistic. To encounter this problem, we require datasets that rich in type of generation methods and perturbation strategy which is usually common for online videos. To this end, we propose AV-Deepfake1M++, an extension of the AV-Deepfake1M having 2 million video clips with diversified manipulation strategy and audio-visual perturbation. This paper includes the description of data generation strategies along with benchmarking of AV-Deepfake1M++ using state-of-the-art methods. We believe that this dataset will play a pivotal role in facilitating research in Deepfake domain. Based on this dataset, we host the 2025 1M-Deepfakes Detection Challenge. The challenge details, dataset and evaluation scripts are available online under a research-only license at <a target="_blank" rel="noopener" href="https://deepfakes1m.github.io/2025">https://deepfakes1m.github.io/2025</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³å’Œé¢éƒ¨è¯­éŸ³é‡å»ºæ¨¡å‹çš„è¿…é€Ÿå´›èµ·ä½¿å¾—è§†é¢‘åˆ¶ä½œå˜å¾—æ›´åŠ å®¹æ˜“ä¸”é«˜åº¦é€¼çœŸã€‚ä¸ºäº†åº”å¯¹è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åœ¨ç”Ÿæˆæ–¹æ³•å’Œæ‰°åŠ¨ç­–ç•¥æ–¹é¢ä¸°å¯Œå¤šæ ·ï¼Œé€šå¸¸é€‚ç”¨äºåœ¨çº¿è§†é¢‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AV-Deepfake1M+ï¼Œå®ƒæ˜¯AV-Deepfake1Mçš„æ‰©å±•ç‰ˆæœ¬ï¼ŒåŒ…å«å…·æœ‰å¤šæ ·åŒ–æ“ä½œç­–ç•¥å’Œè§†å¬æ‰°åŠ¨çš„2ç™¾ä¸‡è§†é¢‘å‰ªè¾‘ã€‚æœ¬æ–‡ä»‹ç»äº†æ•°æ®ç”Ÿæˆç­–ç•¥ï¼Œå¹¶ä½¿ç”¨æœ€æ–°æ–¹æ³•å¯¹AV-Deepfake1M+è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ç›¸ä¿¡è¯¥æ•°æ®é›†å°†åœ¨ä¿ƒè¿›Deepfakeé¢†åŸŸçš„ç ”ç©¶ä¸­å‘æŒ¥å…³é”®ä½œç”¨ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¸¾åŠäº†2025å¹´1M Deepfakesæ£€æµ‹æŒ‘æˆ˜èµ›ã€‚æŒ‘æˆ˜èµ›çš„è¯¦ç»†ä¿¡æ¯ã€æ•°æ®é›†å’Œè¯„ä¼°è„šæœ¬å¯åœ¨ä»…é¢å‘ç ”ç©¶çš„è®¸å¯è¯ä¸‹åœ¨çº¿è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://deepfakes1m.github.io/2025%E3%80%82">https://deepfakes1m.github.io/2025 ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20579v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ–‡æœ¬æåˆ°æ–‡æœ¬è½¬è¯­éŸ³å’Œé¢éƒ¨è¯­éŸ³å¤ç°æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—è§†é¢‘åˆ¶ä½œå˜å¾—æ›´åŠ å®¹æ˜“ä¸”é«˜åº¦é€¼çœŸã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œéœ€è¦åŒ…å«ä¸°å¯Œç”Ÿæˆæ–¹æ³•å’Œæ‰°åŠ¨ç­–ç•¥çš„åœ¨çº¿è§†é¢‘æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæå‡ºAV-Deepfake1M++æ•°æ®é›†ï¼Œå®ƒæ˜¯AV-Deepfake1Mçš„æ‰©å±•ç‰ˆï¼ŒåŒ…å«ä¸¤ç™¾ä¸‡ä¸ªè§†é¢‘ç‰‡æ®µï¼Œå…·æœ‰å¤šæ ·åŒ–çš„æ“çºµç­–ç•¥å’Œè§†å¬æ‰°åŠ¨ã€‚æœ¬æ–‡ä»‹ç»äº†æ•°æ®ç”Ÿæˆç­–ç•¥ï¼Œå¹¶ä½¿ç”¨å…ˆè¿›æ–¹æ³•å¯¹AV-Deepfake1M++è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç›¸ä¿¡æ­¤æ•°æ®é›†å°†æå¤§åœ°æ¨åŠ¨Deepfakeé¢†åŸŸçš„ç ”ç©¶ã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œä¸¾åŠäº†åä¸ºâ€œåŸºäºè§†é¢‘åˆ¶ä½œçš„éŸ³é¢‘æ··æ·†æ”»é˜²è¯„ä¼°åŸºå‡†çš„æ¬ºéª—ä¿¡æ¯æ£€æµ‹æŠ€æœ¯å¤§èµ›â€ï¼Œä¸¾åŠäº†æ¯”èµ›çš„æ›´å¤šç»†èŠ‚åŠå¦‚ä½•è·å¾—è¯¥æ•°æ®é›†éƒ½å…¬å¸ƒäºç½‘é¡µä¸Šã€‚å»ºè®®å…³æ³¨ç›¸å…³ç½‘ç«™ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚åŒæ—¶å‘¼åç ”ç©¶äººå‘˜åˆ©ç”¨è¯¥æ•°æ®é›†å¼€å±•æ·±å…¥ç ”ç©¶ï¼Œæ¨åŠ¨äººå·¥æ™ºèƒ½å®‰å…¨é¢†åŸŸçš„å‘å±•ã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†å¯¹äºæ‰“å‡»è§†é¢‘é€ å‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚å¯¹äºæ„Ÿå…´è¶£çš„ç ”ç©¶äººå‘˜ï¼Œå¯ä»¥åœ¨ç›¸å…³ç½‘ç«™ä¸Šè·å–è¯¥æ•°æ®é›†å¹¶å‚åŠ æŒ‘æˆ˜æ¯”èµ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³å’Œé¢éƒ¨è¯­éŸ³å¤ç°æ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—è§†é¢‘åˆ¶ä½œå˜å¾—æ›´åŠ å®¹æ˜“ä¸”é€¼çœŸã€‚è¿™å¼•å‘äº†å¯¹äºçœŸå®è§†é¢‘ä¸ä¼ªé€ è§†é¢‘ä¹‹é—´çš„åŒºåˆ†é—®é¢˜çš„å…³æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†AV-Deepfake1M++ï¼Œå®ƒæ˜¯AV-Deepfake1Mçš„æ‰©å±•ç‰ˆæœ¬ï¼ŒåŒ…å«ä¸¤ç™¾ä¸‡ä¸ªè§†é¢‘ç‰‡æ®µï¼Œå…·æœ‰å¤šæ ·åŒ–çš„æ“çºµç­–ç•¥å’Œè§†å¬æ‰°åŠ¨ã€‚è¿™å¯¹äºç ”ç©¶è§†é¢‘ä¼ªé€ æŠ€æœ¯å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è¯¥æ•°æ®é›†å¯ç”¨äºåŸºå‡†æµ‹è¯•å’Œç ”ç©¶å…ˆè¿›æ–¹æ³•ï¼Œæœ‰åŠ©äºæ¨åŠ¨Deepfakeé¢†åŸŸçš„ç ”ç©¶è¿›å±•ã€‚ç›¸ä¿¡è¿™ä¸€æ•°æ®é›†å°†ä¿ƒè¿›å¼€å‘æ›´åŠ é«˜æ•ˆå’Œç²¾ç¡®çš„é˜²å¾¡ç­–ç•¥å’ŒæŠ€æœ¯ã€‚è¿™å¯èƒ½æœ‰åŠ©äºæé«˜å®‰å…¨æ€§å’Œéšç§ä¿æŠ¤æ–¹é¢çš„æŠ€æœ¯åº”ç”¨èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªå¼€æ”¾è®¿é—®çš„æ•°æ®é›†å¯ä»¥ä¸ºç§‘ç ”å·¥ä½œè€…æä¾›ä¸°å¯Œçš„æ•°æ®èµ„æºä»¥å¼€å±•ç ”ç©¶ã€‚æ­¤å¤–è¯¥æ•°æ®é›†å°†æœ‰åŠ©äºå¼€å‘æ–°çš„è§†é¢‘ä¼ªé€ æ£€æµ‹æŠ€æœ¯ä»¥åŠæé«˜ç°æœ‰æŠ€æœ¯çš„æ€§èƒ½ã€‚å› æ­¤å®ƒå¯èƒ½å¯¹äººå·¥æ™ºèƒ½å®‰å…¨é¢†åŸŸäº§ç”Ÿé‡å¤§å½±å“å¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚è¯¥æ•°æ®é›†å¯ç”¨äºè®­ç»ƒå’Œæµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹ä»¥è¯†åˆ«å’Œæ£€æµ‹ä¼ªé€ çš„è§†é¢‘å†…å®¹è¿™å¯¹äºæ‰“å‡»è§†é¢‘é€ å‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤å¤–è¯¥æ•°æ®é›†ä¸ºç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªå¹³å°ä»¥å¼€å±•å…³äºè§†é¢‘åˆ¶ä½œå’ŒéŸ³é¢‘æ··æ·†æ”»é˜²è¯„ä¼°åŸºå‡†çš„æ¬ºéª—ä¿¡æ¯æ£€æµ‹æŠ€æœ¯ç ”ç©¶å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•ã€‚ä¸ºæ­¤ä¸¾åŠäº†ç›¸å…³çš„æ¯”èµ›ä»¥é¼“åŠ±ç ”ç©¶äººå‘˜å‚ä¸ç ”ç©¶å¹¶æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ã€‚å»ºè®®å…³æ³¨ç›¸å…³ç½‘ç«™è·å–æœ€æ–°ä¿¡æ¯å’Œå‚åŠ æŒ‘æˆ˜æ¯”èµ›æ¥æ·±å…¥äº†è§£æœ€æ–°ç ”ç©¶æˆæœå¹¶æ‹“å±•è§†é‡å¢åŠ ç ”ç©¶çš„å›½é™…ç«äº‰åŠ›æ°´å¹³åŠ©åŠ›ç§‘å­¦ç ”ç©¶åˆ›æ–°ä¸æ–­å‘å±•èµ°å‘å›½é™…åŒ–ä»¥å¸å¼•æ›´å¤šçš„å…³æ³¨ä¸ºæŠ€æœ¯å‘å±•åŠ©åŠ›å¹¶å®ç°è‡ªèº«çš„è´¡çŒ®æ„ä¹‰å·¨å¤§ä½“ç°ä¸ªäººçš„ç¤¾ä¼šä»·å€¼å¹¶å®ç°å­¦æœ¯ä»·å€¼çš„å¢é•¿ç­‰ç­‰å®ç°å¤šæ–¹é¢çš„ç›®æ ‡æ¨åŠ¨ç§‘å­¦è¿›æ­¥ä¸äººæ‰åŸ¹å…»ç›¸è¾…ç›¸æˆå…±åŒè¿›æ­¥ä¸æå‡æˆä¸ºç¤¾ä¼šçš„åŠ©åŠ›è€…å’Œåˆ›æ–°å¼•é¢†è€…çš„é‡è¦ä¸€ç¯å’Œä½“ç°è‡ªæˆ‘ä»·å€¼çš„å­˜åœ¨å®ç°èŒä¸šå‘å±•ä¸æå‡çªç ´é™åˆ¶è¿½æ±‚å“è¶Šçš„å­¦æœ¯æ°´å¹³ä¿ƒè¿›æ•´ä¸ªé¢†åŸŸçš„å‘å±•å’Œæå‡è´¨é‡ä»·å€¼ä¸ºäººç±»ç¤¾ä¼šå‘å±•è´¡çŒ®åŠ›é‡æä¾›æ™ºæ…§ä¸çµæ„Ÿæ”¯æ’‘ä¸ä¾æ‰˜çš„é‡è¦æ€§å·¨å¤§è´¡çŒ®å’Œæ¨åŠ¨ä½œç”¨é‡å¤§åŠ©åŠ›å…¨çƒäººå·¥æ™ºèƒ½æŠ€æœ¯çš„çªç ´ä¸å‘å±•ä¿ƒè¿›ç§‘å­¦ç ”ç©¶çš„å›½é™…åŒ–äº¤æµå’Œåˆä½œå‘å±•åˆ›æ–°å¹¶é¼“åŠ±é’å¹´äººæ‰çš„å‚ä¸å…±åŒæ¨è¿›ç§‘æŠ€è¿›æ­¥æ¨åŠ¨è¡Œä¸šæŠ€æœ¯å‘å±•çš„ç›®æ ‡å€¼å¾—æœŸå¾…å¹¶ä¸æ–­æ¨è¿›ä»¥æœŸåœ¨æœªæ¥è·å¾—æ›´å¤§çš„æˆæœå¹¶æ¿€åŠ±æ›´å¤šçš„äººæ‰åŠ å…¥è´¡çŒ®è‡ªå·±çš„ä¸€ä»½åŠ›é‡å¯¹äºè¡Œä¸šå‘å±•ä¹Ÿè‡³å…³é‡è¦ä»¥æœŸåœ¨å…¨çƒç§‘æŠ€å‘å±•ä¸­å®ç°æ›´é«˜çš„çªç ´å’Œå‘å±•ç›®æ ‡æˆä¸ºå¼•é¢†ç§‘æŠ€å‘å±•çš„é¢†å†›äººç‰©ä¸ºè¡Œä¸šåšå‡ºé‡è¦è´¡çŒ®å¹¶ä¸æ–­å®ç°è‡ªæˆ‘è¶…è¶Šå’Œå‘å±•ä»·å€¼æˆä¸ºæ¨åŠ¨ç§‘æŠ€å‘å±•çš„å¼ºå¤§åŠ¨åŠ›ç­‰ç†å¿µå…±åŒä¿ƒè¿›å…¨çƒç§‘æŠ€çš„ç¹è£å’Œå‘å±•æˆä¸ºè¡Œä¸šé¢†è¢–å¼•é¢†ç§‘æŠ€å‘å±•èµ°å‘æ›´åŠ ç¾å¥½çš„æœªæ¥æ¿€å‘ç§‘æŠ€åˆ›æ–°çš„æ´»åŠ›ä¸ºç¤¾ä¼šåšå‡ºé‡è¦è´¡çŒ®èµ¢å¾—ç¤¾ä¼šè®¤å¯å’Œå°Šé‡ã€‚<strong>è¿™ä¸ªæ€»ç»“æ€ä¹ˆæ ·ï¼Ÿ</strong>å¾ˆæŠ±æ­‰ï¼Œä¹‹å‰çš„å›å¤å¯èƒ½è¿‡äºå†—é•¿ä¸”éƒ¨åˆ†å†…å®¹ä¸åŸæ–‡æœ¬ä¸ç¬¦ã€‚è®©æˆ‘é‡æ–°ä¸ºæ‚¨ç®€æ´åœ°æ¦‚æ‹¬å¹¶æä¾›å…³é”®è¦ç‚¹ï¼š</li>
</ol>
<p><strong>Summaryï¼ˆæ€»ç»“ï¼‰</strong>ï¼š<br>æ–‡æœ¬æŒ‡å‡ºæ–‡æœ¬è½¬è¯­éŸ³å’Œé¢éƒ¨è¯­éŸ³å¤ç°æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¯¼è‡´è§†é¢‘åˆ¶ä½œå˜å¾—æ›´åŠ å®¹æ˜“ä¸”é€¼çœŸï¼Œå¼•å‘äº†çœŸå®ä¸ä¼ªé€ è§†é¢‘ä¹‹é—´çš„åŒºåˆ†é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºAV-Deepfake1M++æ•°æ®é›†ï¼ŒåŒ…å«ä¸°å¯Œçš„æ“çºµç­–ç•¥å’Œè§†å¬æ‰°åŠ¨ç­–ç•¥çš„è§†é¢‘ç‰‡æ®µã€‚è¯¥æ•°æ®é›†æœ‰åŠ©äºç ”ç©¶è§†é¢‘ä¼ªé€ æŠ€æœ¯ã€æé«˜æ£€æµ‹æŠ€æœ¯çš„æ€§èƒ½å’Œå¼€å‘æ–°çš„æ£€æµ‹æ–¹æ³•ã€‚åŸºäºè¯¥æ•°æ®é›†ä¸¾åŠçš„æŒ‘æˆ˜æ¯”èµ›æ—¨åœ¨é¼“åŠ±ç ”ç©¶äººå‘˜å‚ä¸å¹¶æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ã€‚å»ºè®®å…³æ³¨ç›¸å…³ç½‘ç«™è·å–æœ€æ–°ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeawaysï¼ˆå…³é”®è¦ç‚¹ï¼‰</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬è½¬è¯­éŸ³å’Œé¢éƒ¨è¯­éŸ³å¤ç°æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä½¿å¾—è§†é¢‘åˆ¶ä½œå˜å¾—æ›´åŠ å®¹æ˜“ä¸”é€¼çœŸï¼Œå¼•å‘çœŸå®ä¸ä¼ªé€ è§†é¢‘çš„åŒºåˆ†é—®é¢˜ã€‚</li>
<li>AV-Deepfake1M++æ•°æ®é›†çš„æå‡ºä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†ä¸°å¯Œçš„èµ„æºï¼ŒåŒ…å«å¤šæ ·åŒ–çš„æ“çºµç­–ç•¥å’Œè§†å¬æ‰°åŠ¨ç­–ç•¥çš„è§†é¢‘ç‰‡æ®µã€‚</li>
<li>è¯¥æ•°æ®é›†æœ‰åŠ©äºç ”ç©¶è§†é¢‘ä¼ªé€ æŠ€æœ¯ã€æé«˜æ£€æµ‹æŠ€æœ¯çš„æ€§èƒ½å’Œå¼€å‘æ–°çš„æ£€æµ‹æ–¹æ³•ï¼Œå¯¹äººå·¥æ™ºèƒ½å®‰å…¨é¢†åŸŸäº§ç”Ÿé‡å¤§å½±å“ã€‚</li>
<li>åŸºäºè¯¥æ•°æ®é›†ä¸¾åŠçš„æŒ‘æˆ˜æ¯”èµ›é¼“åŠ±ç ”ç©¶äººå‘˜å‚ä¸ç ”ç©¶å¹¶æ¨åŠ¨æŠ€æœ¯è¿›æ­¥ï¼Œæ›´å¤šä¿¡æ¯å’Œèµ„æºå¯é€šè¿‡ç›¸å…³ç½‘ç«™è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b00b726f6813d70fc89fcf56b673ce5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78834ffae52d2e694ef2bef4f3f5f06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57b0213237daf56f99cf5c3c7dabc3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b81877856a682b95550e20c5f286c163.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9df2e0180a374f17ad00d547ee2a9030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c971caf84bb8a57f5dd1f318f143b56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-585a8388b8016671911418681af372f7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Generalized-Parameter-Tuning-in-Coherent-Ising-Machines-A-Portfolio-Based-Approach"><a href="#Towards-Generalized-Parameter-Tuning-in-Coherent-Ising-Machines-A-Portfolio-Based-Approach" class="headerlink" title="Towards Generalized Parameter Tuning in Coherent Ising Machines: A   Portfolio-Based Approach"></a>Towards Generalized Parameter Tuning in Coherent Ising Machines: A   Portfolio-Based Approach</h2><p><strong>Authors:Tatsuro Hanyu, Takahiro Katagiri, Daichi Mukunoki, Tetsuya Hoshino</strong></p>
<p>Coherent Ising Machines (CIMs) have recently gained attention as a promising computing model for solving combinatorial optimization problems. In particular, the Chaotic Amplitude Control (CAC) algorithm has demonstrated high solution quality, but its performance is highly sensitive to a large number of hyperparameters, making efficient tuning essential. In this study, we present an algorithm portfolio approach for hyperparameter tuning in CIMs employing Chaotic Amplitude Control with momentum (CACm) algorithm. Our method incorporates multiple search strategies, enabling flexible and effective adaptation to the characteristics of the hyperparameter space. Specifically, we propose two representative tuning methods, Method A and Method B. Method A optimizes each hyperparameter sequentially with a fixed total number of trials, while Method B prioritizes hyperparameters based on initial evaluations before applying Method A in order. Performance evaluations were conducted on the Supercomputer â€œFlowâ€ at Nagoya University, using planted Wishart instances and Time to Solution (TTS) as the evaluation metric. Compared to the baseline performance with best-known hyperparameters, Method A achieved up to 1.47x improvement, and Method B achieved up to 1.65x improvement. These results demonstrate the effectiveness of the algorithm portfolio approach in enhancing the tuning process for CIMs. </p>
<blockquote>
<p>ç›¸å¹²ä¼Šè¾›æœºï¼ˆCIMï¼‰ä½œä¸ºä¸€ç§è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜çš„æœ‰å‰é€”çš„è®¡ç®—æ¨¡å‹ï¼Œæœ€è¿‘å¼•èµ·äº†äººä»¬çš„å…³æ³¨ã€‚ç‰¹åˆ«æ˜¯æ··æ²Œå¹…åº¦æ§åˆ¶ï¼ˆCACï¼‰ç®—æ³•å·²ç»è¡¨ç°å‡ºäº†è¾ƒé«˜çš„è§£ç®—è´¨é‡ï¼Œä½†å…¶æ€§èƒ½å¯¹å¤§é‡è¶…å‚æ•°éå¸¸æ•æ„Ÿï¼Œå› æ­¤é«˜æ•ˆçš„è°ƒæ•´è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é‡‡ç”¨å¸¦æœ‰åŠ¨é‡ï¼ˆCACmï¼‰ç®—æ³•çš„ç›¸å¹²ä¼Šè¾›æœºè¶…å‚æ•°è°ƒæ•´çš„ç®—æ³•ç»„åˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šç§æœç´¢ç­–ç•¥ï¼Œèƒ½å¤Ÿçµæ´»æœ‰æ•ˆåœ°é€‚åº”è¶…å‚æ•°ç©ºé—´çš„ç‰¹ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å…·æœ‰ä»£è¡¨æ€§çš„è°ƒä¼˜æ–¹æ³•ï¼Œå³æ–¹æ³•Aå’Œæ–¹æ³•Bã€‚æ–¹æ³•AæŒ‰é¡ºåºä¼˜åŒ–æ¯ä¸ªè¶…å‚æ•°ï¼Œå¹¶å›ºå®šæ€»è¯•éªŒæ¬¡æ•°ï¼›è€Œæ–¹æ³•Båˆ™æ ¹æ®åˆå§‹è¯„ä¼°ç»“æœä¼˜å…ˆè°ƒæ•´è¶…å‚æ•°ï¼Œç„¶åæŒ‰é¡ºåºåº”ç”¨æ–¹æ³•Aã€‚åœ¨åå¤å±‹å¤§å­¦è¶…çº§è®¡ç®—æœºâ€œæµâ€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨æ¤å…¥Wishartå®ä¾‹å’Œæ±‚è§£æ—¶é—´ï¼ˆTTSï¼‰ä½œä¸ºè¯„ä»·æŒ‡æ ‡è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ã€‚ä¸å·²çŸ¥æœ€ä½³è¶…å‚æ•°çš„åŸºçº¿æ€§èƒ½ç›¸æ¯”ï¼Œæ–¹æ³•Aå®ç°äº†é«˜è¾¾1.47å€çš„æ”¹è¿›ï¼Œæ–¹æ³•Bå®ç°äº†é«˜è¾¾1.65å€çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœè¯æ˜äº†ç®—æ³•ç»„åˆæ–¹æ³•åœ¨å¢å¼ºCIMè°ƒä¼˜è¿‡ç¨‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20295v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ··æ²ŒæŒ¯å¹…æ§åˆ¶ç®—æ³•ï¼ˆCACï¼‰åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œä½†å…¶æ€§èƒ½å¯¹å¤§é‡è¶…å‚æ•°æ•æ„Ÿã€‚æœ¬ç ”ç©¶é‡‡ç”¨ç®—æ³•ç»„åˆç­–ç•¥ï¼Œä½¿ç”¨å¸¦æœ‰åŠ¨é‡çš„æ··æ²ŒæŒ¯å¹…æ§åˆ¶ï¼ˆCACmï¼‰ç®—æ³•è¿›è¡ŒCIMè¶…å‚æ•°è°ƒä¼˜ã€‚è¯¥æ–¹æ³•ç»“åˆå¤šç§æœç´¢ç­–ç•¥ï¼Œçµæ´»é€‚åº”è¶…å‚æ•°ç©ºé—´ç‰¹æ€§ã€‚åœ¨åå¤å±‹å¤§å­¦è¶…çº§è®¡ç®—æœºâ€œFlowâ€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨ç§æ¤Wishartå®ä¾‹å’Œè§£å†³æ–¹æ¡ˆæ—¶é—´ï¼ˆTTSï¼‰ä½œä¸ºè¯„ä»·æŒ‡æ ‡ã€‚ä¸å·²çŸ¥æœ€ä½³è¶…å‚æ•°åŸºçº¿æ€§èƒ½ç›¸æ¯”ï¼Œæ–¹æ³•Aæé«˜äº†æœ€å¤š1.47å€ï¼Œæ–¹æ³•Bæé«˜äº†æœ€å¤š1.65å€ã€‚è¯æ˜äº†ç®—æ³•ç»„åˆç­–ç•¥åœ¨æé«˜CIMè°ƒä¼˜è¿‡ç¨‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ··æ²ŒæŒ¯å¹…æ§åˆ¶ï¼ˆCACï¼‰ç®—æ³•åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>CIMsæ€§èƒ½å¯¹å¤§é‡è¶…å‚æ•°æ•æ„Ÿï¼Œéœ€è¦è¿›è¡Œé«˜æ•ˆè°ƒä¼˜ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨ç®—æ³•ç»„åˆç­–ç•¥è¿›è¡ŒCIMè¶…å‚æ•°è°ƒä¼˜ï¼Œä½¿ç”¨CACmç®—æ³•ç»“åˆå¤šç§æœç´¢ç­–ç•¥ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§ä»£è¡¨æ€§çš„è°ƒä¼˜æ–¹æ³•ï¼šMethod Aå’ŒMethod Bã€‚</li>
<li>Method Aé€šè¿‡å›ºå®šè¯•éªŒæ¬¡æ•°é¡ºåºä¼˜åŒ–æ¯ä¸ªè¶…å‚æ•°ã€‚</li>
<li>Method Båœ¨åˆæ­¥è¯„ä¼°åä¼˜å…ˆè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œå†åº”ç”¨Method Aã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29e194ffd91d6bff8b38a20b5d679bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43f91a9d3dfcddf9ea4a0b90a6438ea6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-702ae5404b5b3e29e205b1b99ad365a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3837172c0d2cacd4290fa2e3d25ca9ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f55dbe090bb205c4eeffc798b70725e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164d0745ed43ac53fafb9aeb97299f33.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67986dd8ad30c87be0558be921574e48.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BENYO-S2ST-Corpus-1-A-Bilingual-English-to-Yoruba-Direct-Speech-to-Speech-Translation-Corpus"><a href="#BENYO-S2ST-Corpus-1-A-Bilingual-English-to-Yoruba-Direct-Speech-to-Speech-Translation-Corpus" class="headerlink" title="BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct   Speech-to-Speech Translation Corpus"></a>BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct   Speech-to-Speech Translation Corpus</h2><p><strong>Authors:Emmanuel Adetiba, Abdultaofeek Abayomi, Raymond J. Kala, Ayodele H. Ifijeh, Oluwatobi E. Dare, Olabode Idowu-Bismark, Gabriel O. Sobola, Joy N. Adetiba, Monsurat Adepeju Lateef, Heather Cole-Lewis</strong></p>
<p>There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for high resource-to-low resource language pairs such as English-to-Yoruba. Thus, in this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech Translation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a hybrid architecture we developed for large-scale direct S2ST corpus creation at reduced cost. To achieve this, we leveraged non speech-to-speech Standard Yoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as the corresponding Standard English (SE) transcripts. YORULECT Corpus is small scale(1,504) samples, and it does not have paired English audios. Therefore, we generated the SE audios using pre-trained AI models (i.e. Facebook MMS). We also developed an audio augmentation algorithm named AcoustAug based on three latent acoustic features to generate augmented audios from the raw audios of the two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language, which gives a total of 24,064 sample size. The total audio duration for the two languages is 41.20 hours. This size is quite significant. Beyond building S2ST models, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve existing ones. The created corpus and Coqui framework were used to build a pretrained Yoruba TTS model (named YoruTTS-1.5) as a proof of concept. The YoruTTS-1.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates moderate fundamental pitch similarity with the reference real-time audio. Ultimately, the corpus architecture in this study can be leveraged by researchers and developers to curate datasets for multilingual high-resource-to-low-resource African languages. This will bridge the huge digital divides in translations among high and low-resource language pairs. BENYO-S2ST-Corpus-1 and YoruTTS-1.5 are publicly available at (<a target="_blank" rel="noopener" href="https://bit.ly/40bGMwi">https://bit.ly/40bGMwi</a>). </p>
<blockquote>
<p>é’ˆå¯¹è‹±è¯­åˆ°çº¦é²å·´è¯­ç­‰é«˜èµ„æºåˆ°ä½èµ„æºè¯­è¨€å¯¹çš„è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ï¼ˆSpeech-to-Speech Translation, S2STï¼‰æ•°æ®é›†å­˜åœ¨é‡å¤§çŸ­ç¼ºé—®é¢˜ï¼Œæœ¬ç ”ç©¶ç­–åˆ’äº†åŒè¯­è‹±è¯­åˆ°çº¦é²å·´è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘è¯­æ–™åº“ç‰ˆæœ¬1ï¼ˆBENYO-S2ST-Corpus-1ï¼‰ã€‚è¯¥è¯­æ–™åº“åŸºäºæˆ‘ä»¬ä¸ºå¤§è§„æ¨¡ç›´æ¥S2STè¯­æ–™åº“åˆ›å»ºè€Œå¼€å‘çš„ä¸€ç§æ··åˆæ¶æ„ï¼Œæ—¨åœ¨é™ä½æˆæœ¬ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨YORULECTè¯­æ–™åº“ä¸­çš„éè¯­éŸ³åˆ°è¯­éŸ³æ ‡å‡†çº¦é²å·´è¯­ï¼ˆSYï¼‰å®æ—¶éŸ³é¢‘å’Œè½¬å½•æ–‡æœ¬ï¼Œä»¥åŠç›¸åº”çš„æ ‡å‡†è‹±è¯­ï¼ˆSEï¼‰è½¬å½•æ–‡æœ¬ã€‚YORULECTè¯­æ–™åº“æ ·æœ¬è§„æ¨¡è¾ƒå°ï¼ˆ1504ä¸ªï¼‰ï¼Œä¸”æ²¡æœ‰é…å¥—çš„è‹±è¯­éŸ³é¢‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„AIæ¨¡å‹ï¼ˆä¾‹å¦‚Facebook MMSï¼‰ç”ŸæˆSEéŸ³é¢‘ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§åŸºäºä¸‰ç§æ½œåœ¨å£°å­¦ç‰¹å¾çš„éŸ³é¢‘å¢å¼ºç®—æ³•AcoustAugï¼Œç”¨äºä»ä¸¤ç§è¯­è¨€çš„åŸå§‹éŸ³é¢‘ç”Ÿæˆå¢å¼ºéŸ³é¢‘ã€‚BENYO-S2ST-Corpus-1åŒ…å«æ¯ç§è¯­è¨€12032ä¸ªéŸ³é¢‘æ ·æœ¬ï¼Œæ€»æ ·æœ¬è§„æ¨¡è¾¾åˆ°24064ä¸ªã€‚ä¸¤ç§è¯­è¨€çš„æ€»éŸ³é¢‘æ—¶é•¿ä¸º41.2å°æ—¶ï¼Œè¿™ä¸€è§„æ¨¡ç›¸å½“å¯è§‚ã€‚é™¤äº†æ„å»ºS2STæ¨¡å‹å¤–ï¼ŒBENYO-S2ST-Corpus-1è¿˜å¯ç”¨äºæ„å»ºé¢„è®­ç»ƒæ¨¡å‹æˆ–æ”¹è¿›ç°æœ‰æ¨¡å‹ã€‚ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œæˆ‘ä»¬åˆ©ç”¨åˆ›å»ºçš„è¯­æ–™åº“å’ŒCoquiæ¡†æ¶æ„å»ºäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„çº¦é²å·´è¯­TTSæ¨¡å‹ï¼ˆåä¸ºYoruTTS-1.5ï¼‰ã€‚YoruTTS-1.5åœ¨1000ä¸ªå‘¨æœŸåçš„F0 RMSEå€¼ä¸º63.54ï¼Œè¡¨æ˜å…¶åŸºç¡€éŸ³é«˜ä¸å‚è€ƒå®æ—¶éŸ³é¢‘å…·æœ‰ä¸­ç­‰ç›¸ä¼¼æ€§ã€‚æœ€ç»ˆï¼Œæœ¬ç ”ç©¶ä¸­çš„è¯­æ–™åº“æ¶æ„å¯è¢«ç ”ç©¶è€…å’Œå¼€å‘è€…ç”¨æ¥ç­–åˆ’é’ˆå¯¹å¤šè¯­ç§é«˜èµ„æºåˆ°ä½èµ„æºçš„éæ´²è¯­è¨€æ•°æ®é›†ã€‚è¿™å°†ç¼©å°é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€å¯¹ä¹‹é—´çš„ç¿»è¯‘æ•°å­—é¸¿æ²Ÿã€‚BENYO-S2ST-Corpus-1å’ŒYoruTTS-1.5å¯åœ¨<a target="_blank" rel="noopener" href="https://bit.ly/40bGMwi%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://bit.ly/40bGMwiå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09342v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶é’ˆå¯¹è‹±è¯­åˆ°çº¦é²å·´è¯­ç­‰é«˜èµ„æºåˆ°ä½èµ„æºè¯­è¨€å¯¹ï¼Œåˆ›å»ºäº†åŒè¯­è‹±è¯­åˆ°çº¦é²å·´è¯­è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘è¯­æ–™åº“ç‰ˆæœ¬1ï¼ˆBENYO-S2ST-Corpus-1ï¼‰ã€‚åŸºäºæ··åˆæ¶æ„ï¼Œåˆ©ç”¨éè¯­éŸ³åˆ°è¯­éŸ³çš„æ ‡å‡†çº¦é²å·´è¯­å®æ—¶éŸ³é¢‘å’Œè½¬å½•ä»¥åŠç›¸åº”çš„æ ‡å‡†è‹±è¯­è½¬å½•ï¼Œç”Ÿæˆå¤§è§„æ¨¡ç›´æ¥è¯­éŸ³ç¿»è¯‘è¯­æ–™åº“ã€‚é€šè¿‡é¢„è®­ç»ƒçš„äººå·¥æ™ºèƒ½æ¨¡å‹å’ŒéŸ³é¢‘å¢å¼ºç®—æ³•ï¼Œç”Ÿæˆé…å¯¹è‹±è¯­éŸ³é¢‘å¹¶æ‰©å……è¯­æ–™åº“ã€‚BENYO-S2ST-Corpus-1åŒ…å«æ¯ç§è¯­è¨€12ï¼Œ032ä¸ªéŸ³é¢‘æ ·æœ¬ï¼Œæ€»æ ·æœ¬é‡ä¸º24ï¼Œ064ï¼Œæ€»éŸ³é¢‘æ—¶é•¿ä¸º41.2å°æ—¶ã€‚æ­¤å¤–ï¼Œè¯¥è¯­æ–™åº“ä¹Ÿå¯ç”¨äºå»ºç«‹é¢„è®­ç»ƒæ¨¡å‹æˆ–æ”¹è¿›ç°æœ‰æ¨¡å‹ã€‚ä½¿ç”¨æ­¤è¯­æ–™åº“å’ŒCoquiæ¡†æ¶å»ºç«‹äº†çº¦é²å·´è¯­TTSæ¨¡å‹ï¼ˆYoruTTS-1.5ï¼‰ï¼Œåœ¨1000ä¸ªå‘¨æœŸåï¼Œå…¶F0 RMSEå€¼ä¸º63.54ï¼Œæ˜¾ç¤ºå‡ºä¸å‚è€ƒå®æ—¶éŸ³é¢‘çš„ä¸­ç­‰åŸºæœ¬éŸ³é«˜ç›¸ä¼¼æ€§ã€‚æ­¤è¯­æ–™åº“æ¶æ„æœ‰åŠ©äºç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜ä¸ºå¤šç§é«˜èµ„æºåˆ°ä½èµ„æºçš„éæ´²è¯­è¨€åˆ›å»ºæ•°æ®é›†ï¼Œç¼©å°æ•°å­—ç¿»è¯‘é¸¿æ²Ÿã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å­˜åœ¨è‹±è¯­åˆ°çº¦é²å·´è¯­ç­‰é«˜èµ„æºåˆ°ä½èµ„æºè¯­è¨€å¯¹çš„è¯­éŸ³ç¿»è¯‘æ•°æ®é›†çŸ­ç¼ºé—®é¢˜ã€‚</li>
<li>ç ”ç©¶åˆ›å»ºäº†åŒè¯­è‹±è¯­åˆ°çº¦é²å·´è¯­è¯­éŸ³ç¿»è¯‘è¯­æ–™åº“BENYO-S2ST-Corpus-1ï¼ŒåŒ…å«24ï¼Œ064ä¸ªæ ·æœ¬ï¼Œæ€»æ—¶é•¿41.2å°æ—¶ã€‚</li>
<li>åˆ©ç”¨æ··åˆæ¶æ„ã€é¢„è®­ç»ƒAIæ¨¡å‹å’ŒéŸ³é¢‘å¢å¼ºç®—æ³•ç”Ÿæˆé…å¯¹è‹±è¯­éŸ³é¢‘å’Œæ‰©å……è¯­æ–™åº“ã€‚</li>
<li>BENYO-S2ST-Corpus-1å¯ç”¨äºå»ºç«‹S2STæ¨¡å‹ã€é¢„è®­ç»ƒæ¨¡å‹æˆ–æ”¹è¿›ç°æœ‰æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨è¯¥è¯­æ–™åº“å»ºç«‹äº†çº¦é²å·´è¯­TTSæ¨¡å‹YoruTTS-1.5ä½œä¸ºæ¦‚å¿µéªŒè¯ï¼Œè¡¨ç°å‡ºä¸­ç­‰éŸ³é«˜ç›¸ä¼¼æ€§ã€‚</li>
<li>æ­¤è¯­æ–™åº“æ¶æ„å¯ç”¨äºä¸ºå¤šç§éæ´²è¯­è¨€åˆ›å»ºæ•°æ®é›†ï¼Œç¼©å°é«˜èµ„æºå’Œä½èµ„æºè¯­è¨€å¯¹ä¹‹é—´çš„æ•°å­—ç¿»è¯‘é¸¿æ²Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee295808452e1ad653ba7b3f808cc0fe.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation"><a href="#UniCUE-Unified-Recognition-and-Generation-Framework-for-Chinese-Cued-Speech-Video-to-Speech-Generation" class="headerlink" title="UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation"></a>UniCUE: Unified Recognition and Generation Framework for Chinese Cued   Speech Video-to-Speech Generation</h2><p><strong>Authors:Jinting Wang, Shan Yang, Li Liu</strong></p>
<p>Cued Speech (CS) enhances lipreading through hand coding, providing precise speech perception support for the hearing-impaired. CS Video-to-Speech generation (CSV2S) task aims to convert the CS visual expressions (CS videos) of hearing-impaired individuals into comprehensible speech signals. Direct generation of speech from CS video (called single CSV2S) yields poor performance due to insufficient CS data. Current research mostly focuses on CS Recognition (CSR), which convert video content into linguistic text. Based on this, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech system. This combined architecture relies on text as an intermediate medium for stepwise cross-modal alignment, which may lead to error propagation and temporal misalignment between speech and video dynamics. To address these challenges, we propose a novel approach that directly generates speech from CS videos without relying on intermediate text. Building upon this, we propose UniCUE, the first unified framework for CSV2S, whose core innovation lies in the integration of the CSR task that provides fine-grained visual-semantic information to facilitate speech generation from CS videos. More precisely, (1) a novel fine-grained semantic alignment pool to ensure precise mapping between visual features and speech contents; (2) a VisioPhonetic adapter to bridge cross-task representations, ensuring seamless compatibility between two distinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is introduced to enhance fine-grained spatiotemporal correlations between lip and hand movements in CS video. Experiments on our new established Chinese CS dataset show that our UniCUE achieves state-of-the-art performance across various metrics. </p>
<blockquote>
<p>å£å‹æç¤ºè¯­ï¼ˆCSï¼‰é€šè¿‡æ‰‹ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›ç²¾ç¡®çš„è¯­éŸ³æ„ŸçŸ¥æ”¯æŒã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰ä»»åŠ¡æ—¨åœ¨å°†å¬åŠ›å—æŸè€…çš„å£å‹æç¤ºè§†é¢‘ï¼ˆCSè§†é¢‘ï¼‰è½¬åŒ–ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼ˆç§°ä¸ºå•ä¸€CSV2Sï¼‰ç”±äºCSæ•°æ®ä¸è¶³ï¼Œè¡¨ç°è¾ƒå·®ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å£å‹æç¤ºè¯†åˆ«ï¼ˆCSRï¼‰ä¸Šï¼Œå°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºè¯­è¨€æ–‡æœ¬ã€‚åŸºäºæ­¤ï¼ŒCSV2Sçš„ä¸€ç§ç›´æ¥æ–¹æ³•æ˜¯ç»“åˆCSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿã€‚è¿™ç§ç»„åˆæ¶æ„ä¾èµ–äºæ–‡æœ¬ä½œä¸ºä¸­é—´åª’ä»‹è¿›è¡Œåˆ†æ­¥è·¨æ¨¡æ€å¯¹é½ï¼Œè¿™å¯èƒ½å¯¼è‡´è¯¯å·®ä¼ æ’­ä»¥åŠè¯­éŸ³å’Œè§†é¢‘åŠ¨æ€ä¹‹é—´çš„æ—¶é—´ä¸å¯¹é½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼Œä¸ä¾èµ–ä¸­é—´æ–‡æœ¬ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯CSV2Sçš„ç¬¬ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºæ•´åˆCSRä»»åŠ¡ï¼Œæä¾›ç²¾ç»†çš„è§†è§‰è¯­ä¹‰ä¿¡æ¯ï¼Œä»¥ä¿ƒè¿›ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ã€‚æ›´å…·ä½“åœ°è¯´ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ–°å‹ç²¾ç»†è¯­ä¹‰å¯¹é½æ± ï¼Œç¡®ä¿è§†è§‰ç‰¹å¾ä¸è¯­éŸ³å†…å®¹ä¹‹é—´çš„ç²¾ç¡®æ˜ å°„ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªVisioPhoneticé€‚é…å™¨ï¼Œå¼¥åˆè·¨ä»»åŠ¡è¡¨ç¤ºï¼Œç¡®ä¿ä¸¤ä¸ªä¸åŒä»»åŠ¡ï¼ˆå³CSV2Så’ŒCSRï¼‰ä¹‹é—´çš„æ— ç¼å…¼å®¹æ€§ï¼›ï¼ˆ3ï¼‰å¼•å…¥å§¿æ€æ„ŸçŸ¥è§†è§‰å¤„ç†å™¨ï¼Œä»¥å¢å¼ºCSè§†é¢‘ä¸­å”‡éƒ¨å’Œæ‰‹éƒ¨åŠ¨ä½œä¹‹é—´çš„ç²¾ç»†æ—¶ç©ºç›¸å…³æ€§ã€‚åœ¨æˆ‘ä»¬æ–°å»ºç«‹çš„ä¸­æ–‡å£å‹æç¤ºæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„UniCUEåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04134v2">PDF</a> 10 pages, 10 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Cued Speechï¼ˆCSï¼‰é€šè¿‡æ‰‹è¯­ç¼–ç æé«˜å”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›ç²¾ç¡®çš„è¯­éŸ³æ„ŸçŸ¥æ”¯æŒã€‚CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰ä»»åŠ¡æ—¨åœ¨å°†å¬åŠ›å—æŸè€…çš„CSè§†è§‰è¡¨è¾¾ï¼ˆCSè§†é¢‘ï¼‰è½¬æ¢ä¸ºå¯ç†è§£çš„è¯­éŸ³ä¿¡å·ã€‚ç”±äºCSæ•°æ®ä¸è¶³ï¼Œç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ï¼ˆç§°ä¸ºå•ä¸€CSV2Sï¼‰çš„æ•ˆæœä¸ä½³ã€‚å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨CSè¯†åˆ«ï¼ˆCSRï¼‰ï¼Œå³å°†è§†é¢‘å†…å®¹è½¬æ¢ä¸ºè¯­è¨€æ–‡æœ¬ã€‚åŸºäºæ­¤ï¼ŒCSV2Sçš„ä¸€ç§ç›´è§‚æ–¹æ³•æ˜¯ç»“åˆCSRä¸æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿã€‚è¿™ç§ç»„åˆæ¶æ„ä¾èµ–äºæ–‡æœ¬ä½œä¸ºä¸­é—´åª’ä»‹è¿›è¡Œè·¨æ¨¡æ€å¯¹é½ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯¯å·®ä¼ æ’­å’Œæ—¶é—´å¯¹é½é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ä¾èµ–ä¸­é—´æ–‡æœ¬ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³çš„æ–°æ–¹æ³•ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UniCUEï¼Œè¿™æ˜¯CSV2Sçš„ç»Ÿä¸€æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé›†æˆäº†CSRä»»åŠ¡ï¼Œæä¾›ç²¾ç»†çš„è§†è§‰è¯­ä¹‰ä¿¡æ¯ï¼Œä¿ƒè¿›ä»CSè§†é¢‘çš„è¯­éŸ³ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰ä¸€ä¸ªæ–°é¢–çš„è§†è§‰è¯­ä¹‰æ± ï¼Œç¡®ä¿è§†è§‰ç‰¹å¾ä¸è¯­éŸ³å†…å®¹ä¹‹é—´çš„ç²¾ç¡®æ˜ å°„ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªè·¨ä»»åŠ¡è¡¨ç¤ºçš„VisioPhoneticé€‚é…å™¨ï¼Œç¡®ä¿ä¸¤ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼ˆå³CSV2Så’ŒCSRï¼‰æ— ç¼å…¼å®¹ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªå§¿æ€æ„ŸçŸ¥çš„è§†è§‰å¤„ç†å™¨èƒ½å¤Ÿå¢å¼ºå”‡éƒ¨å’Œæ‰‹éƒ¨åŠ¨ä½œçš„ç»†å¾®æ—¶ç©ºå…³è”ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬åœ¨æ–°å»ºç«‹çš„ä¸­æ–‡CSæ•°æ®é›†ä¸Šçš„UniCUEè¡¨ç°å“è¶Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Cued Speech (CS) é€šè¿‡æ‰‹è¯­ç¼–ç å¢å¼ºå”‡è¯»èƒ½åŠ›ï¼Œä¸ºå¬åŠ›å—æŸè€…æä¾›ç²¾ç¡®è¯­éŸ³æ„ŸçŸ¥æ”¯æŒã€‚</li>
<li>CSè§†é¢‘åˆ°è¯­éŸ³ç”Ÿæˆï¼ˆCSV2Sï¼‰ä»»åŠ¡æ—¨åœ¨å°†CSè§†è§‰è¡¨è¾¾è½¬æ¢ä¸ºè¯­éŸ³ä¿¡å·ã€‚</li>
<li>ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³é¢ä¸´æ•°æ®ä¸è¶³çš„æŒ‘æˆ˜ï¼Œå› æ­¤å½“å‰ç ”ç©¶å¤šå…³æ³¨CSè¯†åˆ«ï¼ˆCSRï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹CSV2Sæ–¹æ³•ï¼Œä¸ä¾èµ–ä¸­é—´æ–‡æœ¬ç›´æ¥ä»CSè§†é¢‘ç”Ÿæˆè¯­éŸ³ã€‚</li>
<li>ä»‹ç»äº†UniCUEç»Ÿä¸€æ¡†æ¶ï¼Œé›†æˆäº†CSRä»»åŠ¡ï¼Œæä¾›ç²¾ç»†è§†è§‰è¯­ä¹‰ä¿¡æ¯ä»¥ä¿ƒè¿›ä»CSè§†é¢‘çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>UniCUEåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§†è§‰è¯­ä¹‰æ± ã€VisioPhoneticé€‚é…å™¨å’Œå§¿æ€æ„ŸçŸ¥çš„è§†è§‰å¤„ç†å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-130c59a9169cc6944f86a0bc8f109cb6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a2655b3ef7cb90a27921a5bce81a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35157968b3b4df10bcba94d1707911b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2c1ad6eb1a1865fa70b6d09adbad075.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe28de3f5eb533d7df17912a622e1d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9afe7d0c3f0c92c99ff2258cfe83cb7c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation"><a href="#FMSD-TTS-Few-shot-Multi-Speaker-Multi-Dialect-Text-to-Speech-Synthesis-for-U-Tsang-Amdo-and-Kham-Speech-Dataset-Generation" class="headerlink" title="FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation"></a>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis   for Ãœ-Tsang, Amdo and Kham Speech Dataset Generation</h2><p><strong>Authors:Yutong Liu, Ziyue Zhang, Ban Ma-bao, Yuqing Cai, Yongbin Yu, Renzeng Duojie, Xiangxiang Wang, Fan Gao, Cheng Huang, Nyima Tashi</strong></p>
<p>Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-&quot;U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality. </p>
<blockquote>
<p>è—è¯­æ˜¯ä¸€ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œå…¶ä¸‰å¤§æ–¹è¨€åŒºâ€”â€”ä¹Œé½ã€å®‰å¤šå’Œåº·åŒºçš„å¹³è¡Œè¯­éŸ³è¯­æ–™åº“æä¸ºæœ‰é™ï¼Œé™åˆ¶äº†è¯­éŸ³å»ºæ¨¡çš„è¿›å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FMSD-TTSï¼Œè¿™æ˜¯ä¸€ä¸ªå°‘æ ·æœ¬ã€å¤šå‘è¨€äººã€å¤šæ–¹è¨€çš„æ–‡æœ¬åˆ°è¯­éŸ³æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»æœ‰é™çš„å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®çš„æ–¹è¨€æ ‡ç­¾ä¸­åˆæˆå¹³è¡Œæ–¹è¨€è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ–°é¢–çš„å‘å£°äºº-æ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ç”¨åŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰æ–¹è¨€é—´çš„ç²¾ç»†å£°å­¦å’Œè¯­è¨€å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™å‘å£°äººçš„èº«ä»½ã€‚å¤§é‡çš„å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œå‘å£°äººç›¸ä¼¼æ€§æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­éŸ³åˆ°è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡éªŒè¯äº†åˆæˆè¯­éŸ³çš„è´¨é‡å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹è—è¯­å¤šæ–¹è¨€è¯­éŸ³åˆæˆçš„å°‘æ ·æœ¬TTSç³»ç»Ÿï¼Œï¼ˆ2ï¼‰å…¬å¼€å‘å¸ƒç”±FMSD-TTSç”Ÿæˆçš„å¤§è§„æ¨¡åˆæˆè—è¯­è¯­éŸ³è¯­æ–™åº“ï¼Œï¼ˆ3ï¼‰å¼€æ”¾æºä»£ç è¯„ä¼°å·¥å…·åŒ…ï¼Œç”¨äºæ ‡å‡†åŒ–è¯„ä¼°å‘å£°äººç›¸ä¼¼æ€§ã€æ–¹è¨€ä¸€è‡´æ€§å’ŒéŸ³é¢‘è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14351v2">PDF</a> 15 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è—è¯­è¿™ä¸€ä½èµ„æºè¯­è¨€çš„å¤šè¯­ç§æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç ”ç©¶ã€‚ä¸ºè§£å†³è—è¯­å› å¹³è¡Œè¯­æ–™åº“ä¸è¶³æ‰€å¸¦æ¥çš„å»ºæ¨¡å›°éš¾ï¼Œæå‡ºFMSD-TTSæ¡†æ¶ï¼Œèƒ½é€šè¿‡æœ‰é™å‚è€ƒéŸ³é¢‘å’Œæ˜ç¡®æ–¹è¨€æ ‡ç­¾åˆæˆå‡ºå¤šç§æ–¹è¨€å£éŸ³çš„è¯­éŸ³ã€‚è¯¥æ¡†æ¶åŒ…å«è¯´è¯äººæ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ç”¨åŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ï¼Œèƒ½æ•æ‰æ–¹è¨€é—´çš„ç»†å¾®è¯­éŸ³å’Œè¯­è¨€å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒè¯´è¯äººèº«ä»½ã€‚å®éªŒè¯æ˜ï¼ŒFMSD-TTSåœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œè¯´è¯äººç›¸ä¼¼æ€§ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ç³»ç»Ÿï¼Œå¹¶åœ¨è¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡ä¸­éªŒè¯äº†å…¶è´¨é‡å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è—è¯­æ˜¯ä½èµ„æºè¯­è¨€ï¼Œç¼ºä¹å¹³è¡Œè¯­æ–™åº“é™åˆ¶äº†å…¶è¯­éŸ³å»ºæ¨¡çš„è¿›å±•ã€‚</li>
<li>æå‡ºFMSD-TTSæ¡†æ¶ï¼Œèƒ½åœ¨æœ‰é™å‚è€ƒéŸ³é¢‘å’Œæ–¹è¨€æ ‡ç­¾ä¸‹åˆæˆå¤šç§æ–¹è¨€å£éŸ³çš„è¯­éŸ³ã€‚</li>
<li>FMSD-TTSåŒ…å«è¯´è¯äººæ–¹è¨€èåˆæ¨¡å—å’Œæ–¹è¨€ä¸“ç”¨åŠ¨æ€è·¯ç”±ç½‘ç»œï¼ˆDSDR-Netï¼‰ã€‚</li>
<li>DSDR-Netèƒ½æ•æ‰æ–¹è¨€é—´çš„ç»†å¾®è¯­éŸ³å’Œè¯­è¨€å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒè¯´è¯äººèº«ä»½ã€‚</li>
<li>FMSD-TTSåœ¨æ–¹è¨€è¡¨è¾¾åŠ›å’Œè¯´è¯äººç›¸ä¼¼æ€§ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>FMSD-TTSæˆåŠŸåº”ç”¨äºè¯­éŸ³æ–¹è¨€è½¬æ¢ä»»åŠ¡ï¼ŒéªŒè¯äº†å…¶è´¨é‡å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7888cb105d27db2b93f79b4c300e04dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-998248bb074f7ffbbd5f513847beb602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c69a1a206da3abcfa0b83c47e41a7bfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f13991fe356ad14a63fe258a28770e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f865a8154f8484ffe94e704f6a5e803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b28cca5e2b8d5f438db483aed09b3f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pseudo-Autoregressive-Neural-Codec-Language-Models-for-Efficient-Zero-Shot-Text-to-Speech-Synthesis"><a href="#Pseudo-Autoregressive-Neural-Codec-Language-Models-for-Efficient-Zero-Shot-Text-to-Speech-Synthesis" class="headerlink" title="Pseudo-Autoregressive Neural Codec Language Models for Efficient   Zero-Shot Text-to-Speech Synthesis"></a>Pseudo-Autoregressive Neural Codec Language Models for Efficient   Zero-Shot Text-to-Speech Synthesis</h2><p><strong>Authors:Yifan Yang, Shujie Liu, Jinyu Li, Yuxuan Hu, Haibin Wu, Hui Wang, Jianwei Yu, Lingwei Meng, Haiyang Sun, Yanqing Liu, Yan Lu, Kai Yu, Xie Chen</strong></p>
<p>Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information.Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at <a target="_blank" rel="noopener" href="https://microsoft.com/research/project/vall-e-x/palle">https://microsoft.com/research/project/vall-e-x/palle</a>. </p>
<blockquote>
<p>è¿‘æœŸé›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿé¢ä¸´ä¸€ä¸ªå…±åŒå›°å¢ƒï¼šè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹å­˜åœ¨ç”Ÿæˆé€Ÿåº¦æ…¢å’ŒæŒç»­æ—¶é—´ä¸å¯æ§çš„é—®é¢˜ï¼Œè€Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å‹åˆ™ç¼ºä¹æ—¶é—´å»ºæ¨¡å¹¶ä¸”é€šå¸¸éœ€è¦å¤æ‚çš„è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹ä¼ªè‡ªå›å½’ï¼ˆPARï¼‰ç¼–ç è§£ç è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œå®ƒèåˆäº†ARå’ŒNARå»ºæ¨¡ã€‚é€šè¿‡å°†ARçš„æ˜¾å¼æ—¶é—´å»ºæ¨¡ä¸NARçš„å¹¶è¡Œç”Ÿæˆç›¸ç»“åˆï¼ŒPARåœ¨å›ºå®šæ—¶é—´æ­¥é•¿ä¸Šç”ŸæˆåŠ¨æ€é•¿åº¦çš„è·¨åº¦ã€‚åŸºäºPARï¼Œæˆ‘ä»¬æå‡ºäº†PALLEï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„TTSç³»ç»Ÿï¼Œå®ƒåˆ©ç”¨PARè¿›è¡Œåˆæ­¥ç”Ÿæˆï¼Œç„¶åä½¿ç”¨NARè¿›è¡Œç»†åŒ–ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒPARæ²¿ç€æ—¶é—´ç»´åº¦é€æ­¥ç”Ÿæˆè¯­éŸ³æ ‡è®°ï¼Œæ¯ä¸€æ­¥éƒ½é¢„æµ‹æ‰€æœ‰ä½ç½®å¹¶å¹¶è¡Œä¿ç•™æœ€å·¦è¾¹çš„è·¨åº¦ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½ç½®ä¿¡åº¦çš„æ ‡è®°å°†å¹¶è¡Œè¿­ä»£ç»†åŒ–ï¼Œåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨LibriSpeechæµ‹è¯•æ¸…æ´é›†ä¸Šï¼ŒPALLEåœ¨è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæ¸…æ™°åº¦æ–¹é¢ä¼˜äºåœ¨å¤§å‹æ•°æ®ä¸Šè®­ç»ƒçš„æœ€æ–°ç³»ç»Ÿï¼ŒåŒ…æ‹¬F5-TTSã€E2-TTSå’ŒMaskGCTï¼ŒåŒæ—¶å®ç°é«˜è¾¾åå€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://microsoft.com/research/project/vall-e-x/palle%E8%AE%BF%E9%97%AE%E3%80%82">https://microsoft.com/research/project/vall-e-x/palleè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10352v2">PDF</a> Accepted in ACM MM 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„ç»“åˆä¼ªè‡ªå›å½’ï¼ˆPARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰æ¨¡å‹çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿâ€”â€”PALLEã€‚å®ƒé€šè¿‡æ˜ç¡®çš„æ—¶åºå»ºæ¨¡å’Œå¹³å¹¶è¡Œç”Ÿæˆå®ç°å›ºå®šæ—¶é—´æ­¥é•¿çš„åŠ¨æ€é•¿åº¦è·¨åº¦ç”Ÿæˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒPALLEé€æ­¥æ²¿æ—¶é—´ç»´åº¦ç”Ÿæˆè¯­éŸ³ä»¤ç‰Œï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹ä½ç½®ä¿¡åº¦ä»¤ç‰Œè¿›è¡Œå¹¶è¡Œç»†åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨LibriSpeechæµ‹è¯•é›†ä¸Šï¼ŒPALLEçš„è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæ¸…æ™°åº¦ä¼˜äºå¤§è§„æ¨¡æ•°æ®è®­ç»ƒçš„å…ˆè¿›ç³»ç»Ÿï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æé«˜åå€ã€‚æ›´å¤šéŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://microsoft.com/research/project/vall-e-x/palle">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¼ªè‡ªå›å½’ï¼ˆPARï¼‰è¯­è¨€å»ºæ¨¡æ–¹æ³•ï¼Œç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œéè‡ªå›å½’ï¼ˆNARï¼‰å»ºæ¨¡çš„ä¼˜åŠ¿ã€‚</li>
<li>PARæ¨¡å‹èƒ½å¤Ÿå®ç°å›ºå®šæ—¶é—´æ­¥é•¿çš„åŠ¨æ€é•¿åº¦è·¨åº¦ç”Ÿæˆï¼Œé€šè¿‡æ˜ç¡®çš„æ—¶åºå»ºæ¨¡å’Œå¹³å¹¶è¡Œç”Ÿæˆæé«˜äº†æ•ˆç‡ã€‚</li>
<li>PALLEç³»ç»Ÿçš„ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨PARæ¨¡å‹æ²¿æ—¶é—´ç»´åº¦é€æ­¥ç”Ÿæˆè¯­éŸ³ä»¤ç‰Œï¼Œä¿ç•™æœ€å·¦è¾¹çš„è·¨åº¦ä¿¡æ¯ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé’ˆå¯¹ä½ç½®ä¿¡åº¦ä»¤ç‰Œè¿›è¡Œå¹¶è¡Œç»†åŒ–ï¼Œåˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯æé«˜è¯­éŸ³è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPALLEåœ¨LibriSpeechæµ‹è¯•é›†ä¸Šçš„è¯­éŸ³è´¨é‡ã€è¯´è¯äººç›¸ä¼¼åº¦å’Œæ¸…æ™°åº¦ä¼˜äºå…¶ä»–å…ˆè¿›çš„TTSç³»ç»Ÿã€‚</li>
<li>PALLEç³»ç»Ÿçš„æ¨ç†é€Ÿåº¦æ¯”ä¼ ç»Ÿç³»ç»Ÿå¿«åå€ï¼Œå¹¶æä¾›äº†éŸ³é¢‘æ ·æœ¬ä»¥ä¾›è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a473f1c956ed4fb33b3079e8ddee5e08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e6ed3c8d13c60a430202d0c5ef80eeff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3454ef9c2adb81eaf6450b2b5ce10e2d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Text-Driven-Voice-Conversion-via-Latent-State-Space-Modeling"><a href="#Text-Driven-Voice-Conversion-via-Latent-State-Space-Modeling" class="headerlink" title="Text-Driven Voice Conversion via Latent State-Space Modeling"></a>Text-Driven Voice Conversion via Latent State-Space Modeling</h2><p><strong>Authors:Wen Li, Sofia Martinez, Priyanka Shah</strong></p>
<p>Text-driven voice conversion allows customization of speaker characteristics and prosodic elements using textual descriptions. However, most existing methods rely heavily on direct text-to-speech training, limiting their flexibility in controlling nuanced style elements or timbral features. In this paper, we propose a novel \textbf{Latent State-Space} approach for text-driven voice conversion (\textbf{LSS-VC}). Our method treats each utterance as an evolving dynamical system in a continuous latent space. Drawing inspiration from mamba, which introduced a state-space model for efficient text-driven \emph{image} style transfer, we adapt a loosely related methodology for \emph{voice} style transformation. Specifically, we learn a voice latent manifold where style and content can be manipulated independently by textual style prompts. We propose an adaptive cross-modal fusion mechanism to inject style information into the voice latent representation, enabling interpretable and fine-grained control over speaker identity, speaking rate, and emphasis. Extensive experiments show that our approach significantly outperforms recent baselines in both subjective and objective quality metrics, while offering smoother transitions between styles, reduced artifacts, and more precise text-based style control. </p>
<blockquote>
<p>æ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢å…è®¸ä½¿ç”¨æ–‡æœ¬æè¿°æ¥å®šåˆ¶è¯´è¯äººçš„ç‰¹æ€§å’ŒéŸµå¾‹å…ƒç´ ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºç›´æ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³è®­ç»ƒï¼Œè¿™åœ¨æ§åˆ¶å¾®å¦™çš„é£æ ¼å…ƒç´ æˆ–éŸ³è‰²ç‰¹å¾æ–¹é¢çš„çµæ´»æ€§æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„<strong>æ½œåœ¨çŠ¶æ€ç©ºé—´</strong>æ–¹æ³•ï¼Œç”¨äºæ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢ï¼ˆ<strong>LSS-VC</strong>ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ¯ä¸ªè¯­å¥è§†ä¸ºä¸€ä¸ªè¿ç»­æ½œåœ¨ç©ºé—´ä¸­ä¸æ–­å‘å±•çš„åŠ¨æ€ç³»ç»Ÿã€‚æˆ‘ä»¬ä»å¼•å…¥äº†çŠ¶æ€ç©ºé—´æ¨¡å‹çš„mambaä¸­æ±²å–çµæ„Ÿï¼Œç”¨äºé«˜æ•ˆçš„æ–‡æœ¬é©±åŠ¨å›¾åƒé£æ ¼è½¬æ¢ï¼Œæˆ‘ä»¬å°†ä¸€ç§æ¾æ•£ç›¸å…³çš„æ–¹æ³•ç”¨äºå£°éŸ³é£æ ¼è½¬æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å­¦ä¹ ä¸€ä¸ªå£°éŸ³æ½œåœ¨æµå½¢ï¼Œå…¶ä¸­é£æ ¼å’Œå†…å®¹å¯ä»¥é€šè¿‡æ–‡æœ¬é£æ ¼æç¤ºç‹¬ç«‹æ“çºµã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå°†é£æ ¼ä¿¡æ¯æ³¨å…¥å£°éŸ³æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹è¯´è¯äººèº«ä»½ã€è¯­é€Ÿå’Œå¼ºè°ƒçš„ç›´è§‚å’Œç²¾ç»†æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»è§‚å’Œå®¢è§‚è´¨é‡æŒ‡æ ‡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºæœ€è¿‘çš„åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶æä¾›äº†æ›´å¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ã€å‡å°‘äº†ä¼ªå½±ï¼Œå¹¶æä¾›äº†æ›´ç²¾ç¡®çš„æ–‡æœ¬åŸºäºé£æ ¼æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20999v2">PDF</a> arXiv admin note: This paper has been withdrawn by arXiv due to   disputed and unverifiable authorship and affiliation</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢æŠ€æœ¯å¯é€šè¿‡æ–‡å­—æè¿°å®ç°ä¸ªæ€§åŒ–çš„è¯­éŸ³ç‰¹æ€§ä¸éŸµå¾‹å…ƒç´ ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºç›´æ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨æ§åˆ¶å¾®å¦™çš„é£æ ¼å…ƒç´ æˆ–éŸ³è‰²ç‰¹å¾æ–¹é¢çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºéšçŠ¶æ€ç©ºé—´çš„æ–¹æ³•ç”¨äºæ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢ï¼ˆLSS-VCï¼‰ã€‚è¯¥æ–¹æ³•å°†æ¯å¥è¯­éŸ³çœ‹ä½œæ˜¯åœ¨è¿ç»­éšçŠ¶æ€ç©ºé—´ä¸­æ¼”åŒ–çš„åŠ¨æ€ç³»ç»Ÿã€‚å—mambaï¼ˆä¸€ç§ç”¨äºé«˜æ•ˆæ–‡æœ¬é©±åŠ¨å›¾åƒé£æ ¼è¿ç§»çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬é’ˆå¯¹å£°éŸ³é£æ ¼è½¬æ¢é‡‡ç”¨äº†ä¸ä¹‹ç›¸å…³åº¦ä¸é«˜çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸€ä¸ªå£°éŸ³éšæµå½¢ï¼Œå…¶ä¸­é£æ ¼å’Œå†…å®¹å¯ä»¥é€šè¿‡æ–‡æœ¬é£æ ¼æç¤ºç‹¬ç«‹æ“çºµã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå°†é£æ ¼ä¿¡æ¯æ³¨å…¥å£°éŸ³éšè¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹è¯´è¯äººèº«ä»½ã€è¯­é€Ÿå’Œå¼ºè°ƒçš„ç›´è§‚å’Œç²¾ç»†æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»è§‚å’Œå®¢è§‚è´¨é‡æŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶å®ç°äº†é£æ ¼ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€å‡å°‘äº†ä¼ªå½±å¹¶æä¾›äº†æ›´ç²¾ç¡®çš„æ–‡æœ¬é£æ ¼æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢æŠ€æœ¯å…è®¸é€šè¿‡æ–‡å­—æè¿°è¿›è¡Œè¯­éŸ³ç‰¹æ€§çš„ä¸ªæ€§åŒ–å®šåˆ¶ã€‚</li>
<li>å½“å‰çš„æ–¹æ³•å¤§å¤šå—é™äºç›´æ¥çš„æ–‡æœ¬åˆ°è¯­éŸ³è®­ç»ƒï¼Œç¼ºä¹çµæ´»æ€§åœ¨æ§åˆ¶å¾®å¦™çš„é£æ ¼å…ƒç´ å’ŒéŸ³è‰²ç‰¹å¾ä¸Šã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºéšçŠ¶æ€ç©ºé—´çš„æ–¹æ³•ï¼ˆLSS-VCï¼‰è¿›è¡Œæ–‡æœ¬é©±åŠ¨çš„å£°éŸ³è½¬æ¢ã€‚</li>
<li>å°†æ¯å¥è¯­éŸ³è§†ä¸ºåœ¨è¿ç»­éšçŠ¶æ€ç©ºé—´ä¸­æ¼”åŒ–çš„åŠ¨æ€ç³»ç»Ÿã€‚</li>
<li>å—mambaçš„å¯å‘ï¼Œé‡‡ç”¨äº†é’ˆå¯¹å£°éŸ³é£æ ¼è½¬æ¢çš„æ–¹æ³•ï¼Œå­¦ä¹ äº†ä¸€ä¸ªå£°éŸ³éšæµå½¢æ¥ç‹¬ç«‹æ“çºµé£æ ¼å’Œå†…å®¹ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”çš„è·¨æ¨¡æ€èåˆæœºåˆ¶ï¼Œå®ç°äº†å¯¹è¯´è¯äººèº«ä»½ã€è¯­é€Ÿå’Œå¼ºè°ƒçš„ç›´è§‚å’Œç²¾ç»†æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b26507f54858d85a9cbedec7b5e9ee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dd01a165128fd70fd5cd0ce86dd5f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7e8f263843248ad96418f6eb6716f9e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MAVFlow-Preserving-Paralinguistic-Elements-with-Conditional-Flow-Matching-for-Zero-Shot-AV2AV-Multilingual-Translation"><a href="#MAVFlow-Preserving-Paralinguistic-Elements-with-Conditional-Flow-Matching-for-Zero-Shot-AV2AV-Multilingual-Translation" class="headerlink" title="MAVFlow: Preserving Paralinguistic Elements with Conditional Flow   Matching for Zero-Shot AV2AV Multilingual Translation"></a>MAVFlow: Preserving Paralinguistic Elements with Conditional Flow   Matching for Zero-Shot AV2AV Multilingual Translation</h2><p><strong>Authors:Sungwoo Cho, Jeongsoo Choi, Sungnyun Kim, Se-Young Yun</strong></p>
<p>Despite recent advances in text-to-speech (TTS) models, audio-visual-to-audio-visual (AV2AV) translation still faces a critical challenge: maintaining speaker consistency between the original and translated vocal and facial features. To address this issue, we propose a conditional flow matching (CFM) zero-shot audio-visual renderer that utilizes strong dual guidance from both audio and visual modalities. By leveraging multimodal guidance with CFM, our model robustly preserves speaker-specific characteristics and enhances zero-shot AV2AV translation abilities. For the audio modality, we enhance the CFM process by integrating robust speaker embeddings with x-vectors, which serve to bolster speaker consistency. Additionally, we convey emotional nuances to the face rendering module. The guidance provided by both audio and visual cues remains independent of semantic or linguistic content, allowing our renderer to effectively handle zero-shot translation tasks for monolingual speakers in different languages. We empirically demonstrate that the inclusion of high-quality mel-spectrograms conditioned on facial information not only enhances the quality of the synthesized speech but also positively influences facial generation, leading to overall performance improvements in LSE and FID score. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Peter-SungwooCho/MAVFlow">https://github.com/Peter-SungwooCho/MAVFlow</a>. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†è§†å¬è½¬è§†å¬ï¼ˆAV2AVï¼‰ç¿»è¯‘ä»ç„¶é¢ä¸´ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä¿æŒåŸå§‹å’Œç¿»è¯‘åçš„è¯­éŸ³å’Œé¢éƒ¨ç‰¹å¾ä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰ä¸¤ç§æ¨¡æ€çš„å¼ºå¤§åŒé‡æŒ‡å¯¼çš„æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰é›¶æ ·æœ¬è§†å¬æ¸²æŸ“å™¨ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€æŒ‡å¯¼å’ŒCFMï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°ä¿ç•™ç‰¹å®šè¯´è¯äººçš„ç‰¹å¾ï¼Œå¹¶å¢å¼ºé›¶æ ·æœ¬AV2AVç¿»è¯‘èƒ½åŠ›ã€‚å¯¹äºéŸ³é¢‘æ¨¡æ€ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç¨³å¥çš„è¯´è¯äººåµŒå…¥ä¸xå‘é‡é›†æˆï¼Œå¢å¼ºäº†CFMè¿‡ç¨‹ï¼Œè¿™æœ‰åŠ©äºåŠ å¼ºè¯´è¯äººä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†æƒ…æ„Ÿçš„ç»†å¾®å·®åˆ«ä¼ è¾¾ç»™é¢éƒ¨æ¸²æŸ“æ¨¡å—ã€‚ç”±éŸ³é¢‘å’Œè§†è§‰çº¿ç´¢æä¾›çš„æŒ‡å¯¼ç‹¬ç«‹äºè¯­ä¹‰æˆ–è¯­è¨€å†…å®¹ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„æ¸²æŸ“å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†ä¸åŒè¯­è¨€çš„å•è¯­è¯´è¯äººçš„é›¶æ ·æœ¬ç¿»è¯‘ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜ï¼Œåœ¨é¢éƒ¨ä¿¡æ¯æ¡ä»¶ä¸‹ç”Ÿæˆçš„é«˜è´¨é‡æ¢…å°”é¢‘è°±å›¾ä¸ä»…æé«˜äº†åˆæˆè¯­éŸ³çš„è´¨é‡ï¼Œè€Œä¸”å¯¹é¢éƒ¨ç”Ÿæˆäº§ç”Ÿäº†ç§¯æå½±å“ï¼Œä»è€Œæé«˜äº†LSEå’ŒFIDå¾—åˆ†çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Peter-SungwooCho/MAVFlow%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Peter-SungwooCho/MAVFlowæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11026v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä¸­çš„éŸ³è§†é¢‘è½¬éŸ³è§†é¢‘ï¼ˆAV2AVï¼‰ç¿»è¯‘ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå³ä¿æŒåŸå§‹å’Œç¿»è¯‘åçš„è¯­éŸ³åŠé¢éƒ¨ç‰¹å¾çš„è¯´è¯äººä¸€è‡´æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰çš„é›¶é•œå¤´éŸ³è§†é¢‘æ¸²æŸ“å™¨ï¼Œè¯¥æ¸²æŸ“å™¨ç»“åˆäº†éŸ³è§†é¢‘ä¸¤ç§æ¨¡æ€çš„å¼•å¯¼åŠ›æ¥å®ç°æ›´å¼ºçš„è¯è¯­è€…ç‰¹å¾ä¿æŒä¸é›¶é•œå¤´AV2AVç¿»è¯‘èƒ½åŠ›æå‡ã€‚å¯¹äºéŸ³é¢‘æ¨¡æ€ï¼Œè¯¥ç ”ç©¶é€šè¿‡åœ¨CFMè¿‡ç¨‹ä¸­èå…¥ç¨³å¥çš„è¯´è¯äººåµŒå…¥å’Œxå‘é‡ï¼Œå¼ºåŒ–äº†è¯´è¯äººä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜å‘é¢éƒ¨æ¸²æŸ“æ¨¡å—ä¼ è¾¾æƒ…æ„Ÿç»†å¾®å·®åˆ«ã€‚éŸ³è§†é¢‘çº¿ç´¢çš„æŒ‡å¯¼ç‹¬ç«‹äºè¯­ä¹‰æˆ–è¯­è¨€å†…å®¹ï¼Œä½¿å¾—è¯¥æ¸²æŸ“å™¨èƒ½æœ‰æ•ˆå¤„ç†ä¸åŒè¯­è¨€çš„é›¶é•œå¤´ç¿»è¯‘ä»»åŠ¡ã€‚é‡‡ç”¨åŸºäºé¢éƒ¨ä¿¡æ¯çš„ä¼˜è´¨æ¢…å°”é¢‘è°±å›¾ä¸ä»…èƒ½æé«˜åˆæˆè¯­éŸ³çš„è´¨é‡ï¼Œè¿˜èƒ½ç§¯æå½±å“é¢éƒ¨ç”Ÿæˆï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³è§†é¢‘è½¬éŸ³è§†é¢‘ï¼ˆAV2AVï¼‰ç¿»è¯‘é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ä¿æŒåŸå§‹å’Œç¿»è¯‘åçš„è¯­éŸ³åŠé¢éƒ¨ç‰¹å¾çš„è¯´è¯äººä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºæ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰çš„é›¶é•œå¤´éŸ³è§†é¢‘æ¸²æŸ“å™¨æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¸²æŸ“å™¨ç»“åˆäº†éŸ³è§†é¢‘ä¸¤ç§æ¨¡æ€çš„å¼•å¯¼åŠ›æ¥å¢å¼ºè¯´è¯äººç‰¹å®šç‰¹å¾çš„ä¿ç•™ä¸é›¶é•œå¤´ç¿»è¯‘èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡èå…¥ç¨³å¥çš„è¯´è¯äººåµŒå…¥å’Œxå‘é‡ï¼Œå¼ºåŒ–äº†éŸ³é¢‘æ¨¡æ€ä¸­çš„è¯´è¯äººä¸€è‡´æ€§ã€‚</li>
<li>å‘é¢éƒ¨æ¸²æŸ“æ¨¡å—ä¼ è¾¾æƒ…æ„Ÿç»†å¾®å·®åˆ«ä»¥å¢å¼ºç¿»è¯‘çš„æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>éŸ³è§†é¢‘çº¿ç´¢çš„æŒ‡å¯¼ç‹¬ç«‹äºè¯­ä¹‰æˆ–è¯­è¨€å†…å®¹ï¼Œä½¿æ¸²æŸ“å™¨èƒ½å¤Ÿå¤„ç†ä¸åŒè¯­è¨€çš„é›¶é•œå¤´ç¿»è¯‘ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1988e148823b5113d6883be0e2fc7063.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e9fc4cd8655248d683780e81e930be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-399ae241ed83218d3d29da6334264cba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2df157f15ebb5686624ec13d4d95b14.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Koel-TTS-Enhancing-LLM-based-Speech-Generation-with-Preference-Alignment-and-Classifier-Free-Guidance"><a href="#Koel-TTS-Enhancing-LLM-based-Speech-Generation-with-Preference-Alignment-and-Classifier-Free-Guidance" class="headerlink" title="Koel-TTS: Enhancing LLM based Speech Generation with Preference   Alignment and Classifier Free Guidance"></a>Koel-TTS: Enhancing LLM based Speech Generation with Preference   Alignment and Classifier Free Guidance</h2><p><strong>Authors:Shehzeen Hussain, Paarth Neekhara, Xuesong Yang, Edresson Casanova, Subhankar Ghosh, Mikyas T. Desta, Roy Fejgin, Rafael Valle, Jason Li</strong></p>
<p>While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website. </p>
<blockquote>
<p>è™½ç„¶è‡ªå›å½’è¯­éŸ³ä»¤ç‰Œç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿå¤šæ ·ä¸”è‡ªç„¶çš„è¯­éŸ³ï¼Œä½†å®ƒä»¬å›ºæœ‰çš„ä¸å¯æ§æ€§å¸¸å¸¸å¯¼è‡´è¯¸å¦‚å¹»è§‰å’Œä¸ç¬¦åˆæ¡ä»¶è¾“å…¥çš„æ„å¤–å‘å£°ç­‰é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†Koel-TTSï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¢å¼ºçš„ç¼–ç å™¨-è§£ç å™¨Transformer TTSæ¨¡å‹ï¼Œé€šè¿‡èå…¥è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººéªŒè¯æ¨¡å‹çš„åå¥½å¯¹é½æŠ€æœ¯æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜èå…¥äº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼Œä»¥è¿›ä¸€æ­¥æé«˜åˆæˆè¯­éŸ³å¯¹æ–‡æœ¬å’Œå‚è€ƒè¯´è¯äººéŸ³é¢‘çš„éµå¾ªåº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›ä¼˜åŒ–æªæ–½æ˜¾è‘—æé«˜äº†ç›®æ ‡è¯´è¯äººçš„ç›¸ä¼¼æ€§ã€åˆæˆè¯­éŸ³çš„æ¸…æ™°åº¦å’Œè‡ªç„¶åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒKoel-TTSç›´æ¥å°†æ–‡æœ¬å’Œä¸Šä¸‹æ–‡éŸ³é¢‘æ˜ å°„åˆ°å£°éŸ³ä»¤ç‰Œä¸Šï¼Œå¹¶ä¸”åœ¨ä¸Šè¿°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°çš„TTSæ¨¡å‹ï¼Œå°½ç®¡å®ƒåœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚éŸ³é¢‘æ ·æœ¬å’Œæ¼”ç¤ºå¯ä»¥åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05236v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Koel-TTSï¼Œä¸€å¥—å¢å¼ºå‹ç¼–ç å™¨-è§£ç å™¨Transformeræ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚å®ƒé€šè¿‡èå…¥åå¥½å¯¹é½æŠ€æœ¯ï¼Œç»“åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººéªŒè¯æ¨¡å‹ï¼Œè§£å†³äº†ç”Ÿæˆå¼æ¨¡å‹å†…åœ¨ç¼ºä¹å¯æ§æ€§çš„é—®é¢˜ï¼ŒåŒ…æ‹¬å¹»å¬å’Œä¸ç¬¦åˆæ¡ä»¶è¾“å…¥çš„å‘éŸ³ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›ä¼˜åŒ–æ˜¾è‘—æé«˜äº†ç›®æ ‡è¯´è¯äººçš„ç›¸ä¼¼æ€§ã€æ¸…æ™°åº¦å’Œåˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚Koel-TTSå¯ç›´æ¥å°†æ–‡æœ¬å’Œè¯­å¢ƒéŸ³é¢‘æ˜ å°„åˆ°å£°éŸ³æ ‡è®°ä¸Šï¼Œåœ¨ç›¸å…³æŒ‡æ ‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œå°½ç®¡å®ƒçš„è®­ç»ƒæ•°æ®é›†è¦å°å¾—å¤šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Koel-TTSæ˜¯å¢å¼ºå‹çš„ç¼–ç å™¨-è§£ç å™¨Transformeræ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼æ¨¡å‹ç¼ºä¹å¯æ§æ€§çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡èå…¥åå¥½å¯¹é½æŠ€æœ¯å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«åŠè¯´è¯äººéªŒè¯æ¨¡å‹ï¼ŒKoel-TTSæ”¹å–„äº†åˆæˆè¯­éŸ³çš„è´¨é‡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒKoel-TTSåœ¨ç›®æ ‡è¯´è¯äººçš„ç›¸ä¼¼æ€§ã€æ¸…æ™°åº¦å’Œè¯­éŸ³è‡ªç„¶åº¦æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>Koel-TTSå¯ä»¥ç›´æ¥å°†æ–‡æœ¬å’Œè¯­å¢ƒéŸ³é¢‘æ˜ å°„åˆ°å£°éŸ³æ ‡è®°ä¸Šã€‚</li>
<li>Koel-TTSåœ¨ç›¸å…³æŒ‡æ ‡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ã€‚</li>
<li>Koel-TTSçš„è®­ç»ƒæ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œä½†è¡¨ç°ä»ç„¶å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3928277ee29854d14617a16a82ae5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97f1c5101c2956629fdb8e8f80ac7d74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640ede636fd912dedc5fbd558fa3119a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7991a7d0d5770e8e44b76720f2650289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce2d9fc2552e8c4acb5a70153051d1dc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RingFormer-A-Neural-Vocoder-with-Ring-Attention-and-Convolution-Augmented-Transformer"><a href="#RingFormer-A-Neural-Vocoder-with-Ring-Attention-and-Convolution-Augmented-Transformer" class="headerlink" title="RingFormer: A Neural Vocoder with Ring Attention and   Convolution-Augmented Transformer"></a>RingFormer: A Neural Vocoder with Ring Attention and   Convolution-Augmented Transformer</h2><p><strong>Authors:Seongho Hong, Yong-Hoon Choi</strong></p>
<p>While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging. Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution. This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information. Additionally, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical. To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer (Conformer). Ring attention effectively captures local details while integrating global information, making it well-suited for processing long sequences and enabling real-time audio generation. RingFormer is trained using adversarial training with two discriminators. The proposed model is applied to the decoder of the text-to-speech model VITS and compared with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics. Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation. Our code and audio samples are available on GitHub. </p>
<blockquote>
<p>è™½ç„¶transformeråœ¨å„ç§éŸ³é¢‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ç¥ç»vocoderä¸­çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç¥ç»vocoderéœ€è¦åœ¨æ ·æœ¬çº§åˆ«ç”Ÿæˆé•¿çš„éŸ³é¢‘ä¿¡å·ï¼Œè¿™è¦æ±‚é«˜çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚è¿™ä¼šå¯¼è‡´æ³¨æ„åŠ›å›¾ç”Ÿæˆçš„è®¡ç®—æˆæœ¬æ˜¾è‘—å¢åŠ ï¼Œå¹¶é™åˆ¶å®ƒä»¬æœ‰æ•ˆå¤„ç†å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç¥ç»vocoderä¸­æ ·æœ¬ç”Ÿæˆçš„é¡ºåºæ€§ç»™å®æ—¶å¤„ç†å¸¦æ¥äº†å›°éš¾ï¼Œä½¿å¾—ç›´æ¥é‡‡ç”¨å˜å‹å™¨ä¸åˆ‡å®é™…ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RingFormerï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»vocoderï¼Œå®ƒèå…¥äº†ä¸€ä¸ªè½»é‡çº§çš„å˜å‹å™¨å˜ä½“â€”â€”å·ç§¯å¢å¼ºå‹å˜å‹å™¨ï¼ˆConformerï¼‰ã€‚ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å±€éƒ¨ç»†èŠ‚ï¼ŒåŒæ—¶æ•´åˆå…¨å±€ä¿¡æ¯ï¼Œéå¸¸é€‚åˆå¤„ç†é•¿åºåˆ—å¹¶å®ç°å®æ—¶éŸ³é¢‘ç”Ÿæˆã€‚RingFormeré€šè¿‡ä½¿ç”¨ä¸¤ä¸ªé‰´åˆ«å™¨è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚æ‰€æå‡ºçš„æ¨¡å‹åº”ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹VITSçš„è§£ç å™¨ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„vocoderï¼ˆå¦‚HiFi-GANã€iSTFT-Netå’ŒBigVGANï¼‰åœ¨ç›¸åŒæ¡ä»¶ä¸‹è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨å„ç§å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRingFormeråœ¨å®æ—¶éŸ³é¢‘ç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†ä¸ç°æœ‰æ¨¡å‹ç›¸å½“æˆ–æ›´é«˜çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’ŒéŸ³é¢‘æ ·æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01182v2">PDF</a> Accepted for publication in IEEE Transactions on Human-Machine   Systems (THMS)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç¥ç»ç¼–è§£ç å™¨é¢ä¸´çš„æŒ‘æˆ˜è€Œæå‡ºçš„RingFormeræ¨¡å‹ã€‚ç¥ç»ç¼–è§£ç å™¨ç”Ÿæˆé•¿éŸ³é¢‘ä¿¡å·æ—¶è¦æ±‚é«˜æ—¶é—´åˆ†è¾¨ç‡ï¼Œå¸¦æ¥è®¡ç®—æˆæœ¬é—®é¢˜ã€‚RingFormerç»“åˆäº†ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶å’Œè½»é‡çº§Transformerå˜ä½“Conformerï¼Œèƒ½æœ‰æ•ˆæ•æ‰å±€éƒ¨ç»†èŠ‚å¹¶æ•´åˆå…¨å±€ä¿¡æ¯ï¼Œé€‚åˆå¤„ç†é•¿åºåˆ—å¹¶å®æ—¶ç”ŸæˆéŸ³é¢‘ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRingFormeråœ¨å®æ—¶éŸ³é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç¼–è§£ç å™¨åœ¨ç”Ÿæˆé•¿éŸ³é¢‘ä¿¡å·æ—¶é¢ä¸´é«˜æ—¶é—´åˆ†è¾¨ç‡è¦æ±‚ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>RingFormeræ¨¡å‹ç»“åˆäº†ç¯å½¢æ³¨æ„åŠ›æœºåˆ¶å’Œå·ç§¯å¢å¼ºTransformerï¼ˆConformerï¼‰ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>RingFormerèƒ½æœ‰æ•ˆæ•æ‰å±€éƒ¨ç»†èŠ‚å¹¶æ•´åˆå…¨å±€ä¿¡æ¯ï¼Œé€‚åˆå¤„ç†é•¿åºåˆ—æ•°æ®ã€‚</li>
<li>RingFormeræ¨¡å‹åº”ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„è§£ç å™¨ï¼Œå¹¶ä¸å…¶ä»–å…ˆè¿›ç¼–è§£ç å™¨è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRingFormeråœ¨å®æ—¶éŸ³é¢‘ç”Ÿæˆæ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>RingFormeræ¨¡å‹å¯æé«˜ç¥ç»ç¼–è§£ç å™¨çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01182">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-77088d5e33265bc011d0fdc41e4f0261.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46a6735c75683f2e8617796885009828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac5241230087293753ed18cc95af2b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ec2e40f31a0b764e2ee82002cce123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f82334fa66cdee5f6e20dfda3ce81df2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Cross-domain-Multi-step-Thinking-Zero-shot-Fine-grained-Traffic-Sign-Recognition-in-the-Wild"><a href="#Cross-domain-Multi-step-Thinking-Zero-shot-Fine-grained-Traffic-Sign-Recognition-in-the-Wild" class="headerlink" title="Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign   Recognition in the Wild"></a>Cross-domain Multi-step Thinking: Zero-shot Fine-grained Traffic Sign   Recognition in the Wild</h2><p><strong>Authors:Yaozong Gan, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</strong></p>
<p>In this study, we propose Cross-domain Multi-step Thinking (CdMT) to improve zero-shot fine-grained traffic sign recognition (TSR) performance in the wild. Zero-shot fine-grained TSR in the wild is challenging due to the cross-domain problem between clean template traffic signs and real-world counterparts, and existing approaches particularly struggle with cross-country TSR scenarios, where traffic signs typically differ between countries. The proposed CdMT framework tackles these challenges by leveraging the multi-step reasoning capabilities of large multimodal models (LMMs). We introduce context, characteristic, and differential descriptions to design multiple thinking processes for LMMs. Context descriptions, which are enhanced by center coordinate prompt optimization, enable the precise localization of target traffic signs in complex road images and filter irrelevant responses via novel prior traffic sign hypotheses. Characteristic descriptions, which are derived from in-context learning with template traffic signs, bridge cross-domain gaps and enhance fine-grained TSR. Differential descriptions refine the multimodal reasoning ability of LMMs by distinguishing subtle differences among similar signs. CdMT is independent of training data and requires only simple and uniform instructions, enabling it to achieve cross-country TSR. We conducted extensive experiments on three benchmark datasets and two real-world datasets from different countries. The proposed CdMT framework achieved superior performance compared with other state-of-the-art methods on all five datasets, with recognition accuracies of 0.93, 0.89, 0.97, 0.89, and 0.85 on the GTSRB, BTSD, TT-100K, Sapporo, and Yokohama datasets, respectively. </p>
<blockquote>
<p>åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨åŸŸå¤šæ­¥æ€è€ƒï¼ˆCdMTï¼‰æ–¹æ³•ï¼Œä»¥æé«˜é›¶æ ·æœ¬ç²¾ç»†äº¤é€šæ ‡å¿—è¯†åˆ«ï¼ˆTSRï¼‰åœ¨é‡å¤–ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚é›¶æ ·æœ¬ç²¾ç»†äº¤é€šæ ‡å¿—è¯†åˆ«åœ¨é‡å¤–ç¯å¢ƒä¸­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ¸…æ´æ¨¡æ¿äº¤é€šæ ‡å¿—å’Œç°å®ä¸–ç•Œä¸­çš„å¯¹åº”ç‰©ä¹‹é—´å­˜åœ¨è·¨åŸŸé—®é¢˜ï¼Œç°æœ‰æ–¹æ³•åœ¨å¤„ç†è·¨å›½äº¤é€šæ ‡å¿—è¯†åˆ«åœºæ™¯æ—¶å°¤å…¶å›°éš¾ï¼Œå„å›½çš„äº¤é€šæ ‡å¿—é€šå¸¸æœ‰æ‰€ä¸åŒã€‚æå‡ºçš„CdMTæ¡†æ¶é€šè¿‡åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥ä¸Šä¸‹æ–‡ã€ç‰¹å¾å’Œå·®å¼‚æè¿°æ¥ä¸ºLMMsè®¾è®¡å¤šä¸ªæ€è€ƒè¿‡ç¨‹ã€‚ä¸Šä¸‹æ–‡æè¿°é€šè¿‡ä¸­å¿ƒåæ ‡æç¤ºä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„é“è·¯å›¾åƒä¸­ç²¾ç¡®å®šä½ç›®æ ‡äº¤é€šæ ‡å¿—ï¼Œå¹¶é€šè¿‡æ–°çš„å…ˆéªŒäº¤é€šæ ‡å¿—å‡è®¾è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å“åº”ã€‚ç‰¹å¾æè¿°æ¥æºäºæ¨¡æ¿äº¤é€šæ ‡å¿—çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¯ä»¥å¼¥è¡¥è·¨åŸŸå·®è·å¹¶å¢å¼ºç²¾ç»†äº¤é€šæ ‡å¿—è¯†åˆ«ã€‚å·®å¼‚æè¿°é€šè¿‡åŒºåˆ†ç›¸ä¼¼æ ‡å¿—ä¹‹é—´çš„ç»†å¾®å·®å¼‚ï¼Œæé«˜äº†LMMsçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚CdMTç‹¬ç«‹äºè®­ç»ƒæ•°æ®ï¼Œåªéœ€è¦ç®€å•ç»Ÿä¸€æŒ‡ä»¤ï¼Œå°±èƒ½å®ç°è·¨å›½äº¤é€šæ ‡å¿—è¯†åˆ«ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªæ¥è‡ªä¸åŒå›½å®¶çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚æ‰€æå‡ºçš„CdMTæ¡†æ¶åœ¨æ‰€æœ‰äº”ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨GTSRBã€BTSDã€TT-100Kã€æœ­å¹Œå’Œæ¨ªæ»¨æ•°æ®é›†ä¸Šçš„è¯†åˆ«å‡†ç¡®ç‡åˆ†åˆ«ä¸º0.93ã€0.89ã€0.97ã€0.89å’Œ0.85ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01534v2">PDF</a> Published by Knowledge-Based Systems</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºè·¨åŸŸå¤šæ­¥æ€è€ƒï¼ˆCdMTï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é‡å¤–é›¶æ ·æœ¬ç»†ç²’åº¦äº¤é€šæ ‡å¿—è¯†åˆ«ï¼ˆTSRï¼‰çš„æ€§èƒ½ã€‚é›¶æ ·æœ¬ç»†ç²’åº¦TSRåœ¨é‡å¤–é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚å¹²å‡€æ¨¡æ¿äº¤é€šæ ‡å¿—ä¸çœŸå®ä¸–ç•Œäº¤é€šæ ‡å¿—ä¹‹é—´çš„è·¨åŸŸé—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å°¤å…¶éš¾ä»¥å¤„ç†è·¨å›½TSRåœºæ™¯ï¼Œå…¶ä¸­äº¤é€šæ ‡å¿—å› å›½å®¶è€Œå¼‚ã€‚CdMTæ¡†æ¶é€šè¿‡åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å¤šæ­¥æ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥ä¸Šä¸‹æ–‡ã€ç‰¹å¾å’Œå·®å¼‚æè¿°æ¥ä¸ºLMMè®¾è®¡å¤šä¸ªæ€è€ƒè¿‡ç¨‹ã€‚ä¸Šä¸‹æ–‡æè¿°é€šè¿‡ä¸­å¿ƒåæ ‡æç¤ºä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨å¤æ‚çš„é“è·¯å›¾åƒä¸­ç²¾ç¡®å®šä½ç›®æ ‡äº¤é€šæ ‡å¿—ï¼Œå¹¶é€šè¿‡æ–°çš„äº¤é€šæ ‡å¿—å‡è®¾è¿‡æ»¤æ‰ä¸ç›¸å…³çš„å“åº”ã€‚ç‰¹å¾æè¿°æºäºåŸºäºæ¨¡æ¿äº¤é€šæ ‡å¿—çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¼¥è¡¥äº†è·¨åŸŸå·®è·å¹¶å¢å¼ºäº†ç»†ç²’åº¦TSRã€‚å·®å¼‚æè¿°åˆ™é€šè¿‡åŒºåˆ†ç›¸ä¼¼æ ‡å¿—ä¹‹é—´çš„ç»†å¾®å·®å¼‚æ¥å®Œå–„LMMçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚CdMTç‹¬ç«‹äºè®­ç»ƒæ•°æ®ï¼Œä»…éœ€è¦ç®€å•ç»Ÿä¸€çš„æŒ‡ä»¤ï¼Œå³å¯å®ç°è·¨å›½TSRã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªæ¥è‡ªä¸åŒå›½å®¶çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCdMTæ¡†æ¶çš„è¯†åˆ«å‡†ç¡®ç‡é«˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œåœ¨GTSRBã€BTSDã€TT-100Kã€æœ­å¹Œå’Œæ¨ªæ»¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º0.93ã€0.89ã€0.97ã€0.89å’Œ0.85ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†è·¨åŸŸå¤šæ­¥æ€è€ƒï¼ˆCdMTï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é›¶æ ·æœ¬ç»†ç²’åº¦äº¤é€šæ ‡å¿—è¯†åˆ«ï¼ˆTSRï¼‰çš„é‡å¤–æ€§èƒ½ã€‚</li>
<li>CdMTæ¡†æ¶é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡ã€ç‰¹å¾å’Œå·®å¼‚æè¿°ï¼Œä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰è®¾è®¡å¤šä¸ªæ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>CdMTèƒ½ç²¾ç¡®å®šä½å¤æ‚é“è·¯å›¾åƒä¸­çš„ç›®æ ‡äº¤é€šæ ‡å¿—ï¼Œå¹¶é€šè¿‡äº¤é€šæ ‡å¿—å‡è®¾è¿‡æ»¤ä¸ç›¸å…³å“åº”ã€‚</li>
<li>ç‰¹å¾æè¿°æºäºæ¨¡æ¿äº¤é€šæ ‡å¿—çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæœ‰åŠ©äºå¼¥è¡¥è·¨åŸŸå·®è·å¹¶å¢å¼ºç»†ç²’åº¦TSRã€‚</li>
<li>CdMTé€šè¿‡åŒºåˆ†ç»†å¾®å·®å¼‚å®Œå–„äº†LMMçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>CdMTç‹¬ç«‹äºè®­ç»ƒæ•°æ®ï¼Œé€‚åº”äºå„ç§äº¤é€šæ ‡å¿—æ•°æ®é›†ï¼ŒåŒ…æ‹¬è·¨å›½æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d273c716757edbe113756249934d3f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df3bc3738a490cd547b4284963178a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40549d220dcc4048b5af741c3260e1b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fd4b4defe90f7e71a78dd09ef1d0bae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7143adc0a1c3672a652b3bb057a1b0c7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-04/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-28cf34600faaa38049e50db9297885ea.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-04  Trustworthy Reasoning Evaluating and Enhancing Factual Accuracy in LLM   Intermediate Thought Processes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b1b89f0316d4305b5d9eeadc5744ba88.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  OptiGradTrust Byzantine-Robust Federated Learning with Multi-Feature   Gradient Analysis and Reinforcement Learning-Based Trust Weighting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
