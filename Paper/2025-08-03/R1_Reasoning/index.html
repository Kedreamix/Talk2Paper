<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  LLMs Between the Nodes Community Discovery Beyond Vectors">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c4bb5b5ae33416bcfcb29963833e00a1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="LLMs-Between-the-Nodes-Community-Discovery-Beyond-Vectors"><a href="#LLMs-Between-the-Nodes-Community-Discovery-Beyond-Vectors" class="headerlink" title="LLMs Between the Nodes: Community Discovery Beyond Vectors"></a>LLMs Between the Nodes: Community Discovery Beyond Vectors</h2><p><strong>Authors:Ekta Gujral, Apurva Sinha</strong></p>
<p>Community detection in social network graphs plays a vital role in uncovering group dynamics, influence pathways, and the spread of information. Traditional methods focus primarily on graph structural properties, but recent advancements in Large Language Models (LLMs) open up new avenues for integrating semantic and contextual information into this task. In this paper, we present a detailed investigation into how various LLM-based approaches perform in identifying communities within social graphs. We introduce a two-step framework called CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to fuse language model outputs with graph structure. Evaluations are conducted on six real-world social network datasets, measuring performance using key metrics such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Variation of Information (VOI), and cluster purity. Our findings reveal that LLMs, particularly when guided by graph-aware strategies, can be successfully applied to community detection tasks in small to medium-sized graphs. We observe that the integration of instruction-tuned models and carefully engineered prompts significantly improves the accuracy and coherence of detected communities. These insights not only highlight the potential of LLMs in graph-based research but also underscore the importance of tailoring model interactions to the specific structure of graph data. </p>
<blockquote>
<p>ç¤¾äº¤ç½‘ç»œå›¾ä¸­çš„ç¤¾åŒºæ£€æµ‹åœ¨æ­ç¤ºç¾¤ä½“åŠ¨æ€ã€å½±å“è·¯å¾„å’Œä¿¡æ¯ä¼ æ’­æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¸»è¦å…³æ³¨å›¾çš„ç»“æ„å±æ€§ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå°†æ­¤ä»»åŠ¡ä¸è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯é›†æˆæä¾›äº†æ–°çš„é€”å¾„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºLLMçš„ä¸åŒæ–¹æ³•åœ¨è¯†åˆ«ç¤¾äº¤ç½‘ç»œå›¾ä¸­ç¤¾åŒºçš„è¡¨ç°è¿›è¡Œäº†è¯¦ç»†è°ƒæŸ¥ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªä¸¤æ­¥æ¡†æ¶CommLLMï¼Œå®ƒåˆ©ç”¨GPT-4oæ¨¡å‹ä»¥åŠåŸºäºæç¤ºçš„æ¨ç†ï¼Œå°†è¯­è¨€æ¨¡å‹è¾“å‡ºä¸å›¾ç»“æ„ç›¸èåˆã€‚æˆ‘ä»¬åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œçš„ç¤¾äº¤ç½‘ç»œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨å½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰ã€è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆARIï¼‰ã€ä¿¡æ¯å˜å¼‚ï¼ˆVOIï¼‰å’Œé›†ç¾¤çº¯åº¦ç­‰å…³é”®æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œç‰¹åˆ«æ˜¯å½“å—åˆ°å›¾æ„ŸçŸ¥ç­–ç•¥å¼•å¯¼æ—¶ï¼ŒLLMå¯ä»¥æˆåŠŸåº”ç”¨äºå°åˆ°ä¸­å‹å›¾çš„ç¤¾åŒºæ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°æŒ‡ä»¤è°ƒæ•´æ¨¡å‹çš„é›†æˆå’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºæ˜¾è‘—æé«˜äº†æ£€æµ‹åˆ°çš„ç¤¾åŒºçš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚è¿™äº›è§è§£ä¸ä»…çªå‡ºäº†LLMåœ¨å›¾åŸºç ”ç©¶ä¸­çš„æ½œåŠ›ï¼Œè€Œä¸”å¼ºè°ƒäº†æ ¹æ®å›¾å½¢æ•°æ®çš„ç‰¹å®šç»“æ„å®šåˆ¶æ¨¡å‹äº¤äº’çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22955v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç¤¾åŒºæ£€æµ‹åœ¨ç¤¾ä¼šç½‘ç»œå›¾ä¸­å‘æŒ¥ç€æ­ç¤ºç¾¤ä½“åŠ¨æ€ã€ä¿¡æ¯ä¼ æ’­è·¯å¾„çš„é‡è¦ä½œç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å›¾çš„ç»“æ„å±æ€§ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºé›†æˆè¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›äº†æ–°çš„é€”å¾„ã€‚æœ¬æ–‡è¯¦ç»†è°ƒæŸ¥äº†åŸºäºLLMçš„æ–¹æ³•åœ¨è¯†åˆ«ç¤¾ä¼šå›¾å†…ç¤¾åŒºæ—¶çš„è¡¨ç°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCommLLMçš„ä¸¤æ­¥æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨GPT-4oæ¨¡å‹å’ŒåŸºäºæç¤ºçš„æ¨ç†æ¥èåˆè¯­è¨€æ¨¡å‹è¾“å‡ºå’Œå›¾ç»“æ„ã€‚åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œçš„ç¤¾ä¼šç½‘ç»œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨å½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰ã€è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆARIï¼‰ã€å˜å¼‚ä¿¡æ¯ï¼ˆVOIï¼‰å’Œé›†ç¾¤çº¯åº¦ç­‰å…³é”®æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°å‹è‡³ä¸­å‹å›¾ä¸­çš„ç¤¾åŒºæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾å½¢æ„ŸçŸ¥ç­–ç•¥çš„å¼•å¯¼ä¸‹ï¼Œå‡†ç¡®ç‡æœ‰æ‰€æé«˜ã€‚è¿™äº›å‘ç°ä¸ä»…çªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾ç ”ç©¶ä¸­çš„æ½œåŠ›ï¼Œä¹Ÿå¼ºè°ƒäº†é’ˆå¯¹å›¾å½¢æ•°æ®ç‰¹å®šç»“æ„å®šåˆ¶æ¨¡å‹äº¤äº’çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºæ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>ç¤¾åŒºæ£€æµ‹åœ¨ç¤¾ä¼šç½‘ç»œå›¾ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œæœ‰åŠ©äºæ­ç¤ºç¾¤ä½“åŠ¨æ€å’Œä¿¡æ¯ä¼ æ’­è·¯å¾„ã€‚</li>
<li>ä¼ ç»Ÿç¤¾åŒºæ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨å›¾çš„ç»“æ„å±æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä¸ºç¤¾åŒºæ£€æµ‹æä¾›äº†æ–°çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé›†æˆè¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æå‡ºçš„CommLLMæ¡†æ¶ç»“åˆäº†è¯­è¨€æ¨¡å‹å’Œå›¾ç»“æ„ï¼Œé€šè¿‡ä¸¤æ­¥å®ç°ç¤¾åŒºæ£€æµ‹ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLLMåœ¨å°å‹è‡³ä¸­å‹å›¾çš„ç¤¾åŒºæ£€æµ‹ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>å›¾å½¢æ„ŸçŸ¥ç­–ç•¥å’ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„ç»“åˆæé«˜äº†ç¤¾åŒºæ£€æµ‹çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5f3b13a1c2100e572cd85e913aeb892b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4bb5b5ae33416bcfcb29963833e00a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b223dbfc5765d0a85c751b8b8624a0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c64b44063c688a4e02a506166d7de7a9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Sufficiency-to-Reflection-Reinforcement-Guided-Thinking-Quality-in-Retrieval-Augmented-Reasoning-for-LLMs"><a href="#From-Sufficiency-to-Reflection-Reinforcement-Guided-Thinking-Quality-in-Retrieval-Augmented-Reasoning-for-LLMs" class="headerlink" title="From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in   Retrieval-Augmented Reasoning for LLMs"></a>From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in   Retrieval-Augmented Reasoning for LLMs</h2><p><strong>Authors:Jie He, Victor Gutierrez Basulto, Jeff Z. Pan</strong></p>
<p>Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/probe2/TIRESRAG-R1">https://github.com/probe2/TIRESRAG-R1</a>. </p>
<blockquote>
<p>åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ–¹æ³•ä»…ä¾èµ–äºæœ€ç»ˆç­”æ¡ˆçš„å¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è´¨é‡ã€‚æœ¬æ–‡åˆ†æäº†ç°æœ‰çš„RAGæ¨ç†æ¨¡å‹ï¼Œå¹¶è¯†åˆ«å‡ºäº†ä¸‰ç§ä¸»è¦çš„å¤±è´¥æ¨¡å¼ï¼šï¼ˆ1ï¼‰ä¿¡æ¯ä¸è¶³ï¼Œå³æ¨¡å‹æœªèƒ½æ£€ç´¢åˆ°è¶³å¤Ÿçš„æ”¯æŒä¿¡æ¯ï¼›ï¼ˆ2ï¼‰æ¨ç†é”™è¯¯ï¼Œå³ä½¿ä¿¡æ¯å……è¶³ï¼Œé€»è¾‘æˆ–å†…å®¹å±‚é¢ä»ç„¶ä¼šå‡ºç°ç¼ºé™·ï¼›ï¼ˆ3ï¼‰ç­”æ¡ˆä¸æ¨ç†ä¸ä¸€è‡´ï¼Œæœ‰æ•ˆçš„æ¨ç†é“¾å¯¼è‡´æœ€ç»ˆçš„ç­”æ¡ˆä¸åŒ¹é…ã€‚æˆ‘ä»¬æå‡ºäº†TIRESRAG-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æ€è€ƒ-æ£€ç´¢-åæ€è¿‡ç¨‹å’Œå¤šç»´å¥–åŠ±ç³»ç»Ÿçš„æ–°å‹æ¡†æ¶ï¼Œä»¥æé«˜æ¨ç†å’Œç¨³å®šæ€§ã€‚TIRESRAG-R1å¼•å…¥äº†ï¼šï¼ˆ1ï¼‰å……è¶³æ€§å¥–åŠ±ï¼Œä»¥é¼“åŠ±å…¨é¢æ£€ç´¢ï¼›ï¼ˆ2ï¼‰æ¨ç†è´¨é‡å¥–åŠ±ï¼Œä»¥è¯„ä¼°æ¨ç†é“¾çš„åˆç†æ€§å’Œå‡†ç¡®æ€§ï¼›ï¼ˆ3ï¼‰åæ€å¥–åŠ±ï¼Œä»¥æ£€æµ‹å’Œä¿®æ­£é”™è¯¯ã€‚å®ƒè¿˜é‡‡ç”¨äº†éš¾åº¦æ„ŸçŸ¥çš„é‡æ–°åŠ æƒç­–ç•¥å’Œè®­ç»ƒæ ·æœ¬è¿‡æ»¤ï¼Œä»¥æé«˜åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åœ¨å››ä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTIRESRAG-R1ä¼˜äºå…ˆå‰çš„RAGæ–¹æ³•ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å•è·³ä»»åŠ¡ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/probe2/TIRESRAG-R1%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/probe2/TIRESRAG-R1è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22716v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„æ£€ç´¢è¾…åŠ©ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•èƒ½å¤Ÿæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¤šæ•°æ–¹æ³•ä»…ä¾èµ–æœ€ç»ˆç­”æ¡ˆå¥–åŠ±ï¼Œå¿½è§†äº†æ¨ç†è¿‡ç¨‹çš„å“è´¨ã€‚æœ¬æ–‡åˆ†æäº†ç°æœ‰RAGæ¨ç†æ¨¡å‹ï¼Œå¹¶æŒ‡å‡ºäº†ä¸‰ç§ä¸»è¦å¤±è´¥æ¨¡å¼ã€‚ä¸ºæ”¹è¿›æ­¤çŠ¶å†µï¼Œæå‡ºTIRESRAG-R1æ¡†æ¶ï¼Œé‡‡ç”¨æ€è€ƒ-æ£€ç´¢-åæ€è¿‡ç¨‹å’Œå¤šç§å¥–åŠ±ç³»ç»Ÿæ¥æå‡æ¨ç†èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚å®éªŒè¯æ˜ï¼ŒTIRESRAG-R1åœ¨å¤šä¸ªå¤šè·³é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„RAGæ–¹æ³•ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°å•è·³ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„æ£€ç´¢è¾…åŠ©ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•èƒ½æå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰RAGæ–¹æ³•ä¸»è¦ä¾èµ–æœ€ç»ˆç­”æ¡ˆå¥–åŠ±ï¼Œå¿½è§†ä¸­é—´æ¨ç†å“è´¨ã€‚</li>
<li>ç°æœ‰RAGæ¨ç†æ¨¡å‹å­˜åœ¨ä¸‰å¤§å¤±è´¥æ¨¡å¼ï¼šä¿¡æ¯ä¸è¶³ã€æ¨ç†é”™è¯¯å’Œç­”æ¡ˆä¸æ¨ç†ä¸ä¸€è‡´ã€‚</li>
<li>TIRESRAG-R1æ¡†æ¶é€šè¿‡æ€è€ƒ-æ£€ç´¢-åæ€è¿‡ç¨‹å’Œå¤šç§å¥–åŠ±ç³»ç»Ÿæ”¹è¿›äº†RAGçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>TIRESRAG-R1å¼•å…¥å……åˆ†æ€§å¥–åŠ±æ¥é¼“åŠ±å…¨é¢æ£€ç´¢ã€‚</li>
<li>TIRESRAG-R1é€šè¿‡éš¾åº¦æ„ŸçŸ¥é‡æƒç­–ç•¥å’Œè®­ç»ƒæ ·æœ¬è¿‡æ»¤æ¥æå‡å¤æ‚ä»»åŠ¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de1e156cab282320fc2231ba9eba0dac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-503c68755568ecbeb0936865a73c475b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24122a811f0760edf3df7a2c06810fcd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9f425e044a6ab34db12f1295f538ac4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RePaCA-Leveraging-Reasoning-Large-Language-Models-for-Static-Automated-Patch-Correctness-Assessment"><a href="#RePaCA-Leveraging-Reasoning-Large-Language-Models-for-Static-Automated-Patch-Correctness-Assessment" class="headerlink" title="RePaCA: Leveraging Reasoning Large Language Models for Static Automated   Patch Correctness Assessment"></a>RePaCA: Leveraging Reasoning Large Language Models for Static Automated   Patch Correctness Assessment</h2><p><strong>Authors:Marcos Fuster-Pena, David de-Fitero-Dominguez, Antonio Garcia-Cabot, Eva Garcia-Lopez</strong></p>
<p>Automated Program Repair (APR) seeks to automatically correct software bugs without requiring human intervention. However, existing tools tend to generate patches that satisfy test cases without fixing the underlying bug, those are known as overfitting patches. To address this issue, Automated Patch Correctness Assessment (APCA) attempts to identify overfitting patches generated by APR tools. It can be solved as a static approach, meaning that no additional information is needed beyond the original and fixed code snippets. Current static techniques often struggle with reliability, flexibility and transparency. To address these issues, we introduce RePaCA, a novel static APCA technique that leverages Large Language Models (LLMs) specialized in thinking tasks. Our model is prompted with both buggy and fixed code snippets and guided to generate a Chain of Thought that analyses code differences, reasons about how the patch addresses the root cause, and ultimately provides a binary classification: correct or overfitting. To enhance these reasoning capabilities for the APCA task specifically, the LLM is finetuned using Reinforcement Learning with the Group Relative Policy Optimization algorithm. When evaluated on a standard Defects4J-derived test, our approach achieves state-of-the-art performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model demonstrates superior generalization capabilities when trained on different datasets, outperforming the leading technique. This reasoning capability also provides enhanced explainability for the patch assessment. These findings underscore the considerable promise of finetuned, reasoning LLMs to advance static APCA by enhancing accuracy, generalization, and explainability. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨æ— éœ€äººå·¥å¹²é¢„å³å¯è‡ªåŠ¨ä¿®å¤è½¯ä»¶ä¸­çš„æ¼æ´ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥å…·å¾€å¾€ç”Ÿæˆæ»¡è¶³æµ‹è¯•ç”¨ä¾‹çš„è¡¥ä¸ï¼Œè€Œæ²¡æœ‰ä¿®å¤åº•å±‚æ¼æ´ï¼Œè¿™äº›è¢«ç§°ä¸ºè¿‡åº¦æ‹Ÿåˆè¡¥ä¸ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè‡ªåŠ¨åŒ–è¡¥ä¸æ­£ç¡®æ€§è¯„ä¼°ï¼ˆAPCAï¼‰è¯•å›¾è¯†åˆ«APRå·¥å…·ç”Ÿæˆçš„è¿‡åº¦æ‹Ÿåˆè¡¥ä¸ã€‚å®ƒå¯ä»¥ä½œä¸ºä¸€ç§é™æ€æ–¹æ³•æ¥è§£å†³ï¼Œè¿™æ„å‘³ç€é™¤äº†åŸå§‹å’Œå›ºå®šçš„ä»£ç ç‰‡æ®µä¹‹å¤–ï¼Œä¸éœ€è¦é¢å¤–çš„ä¿¡æ¯ã€‚å½“å‰çš„é™æ€æŠ€æœ¯åœ¨å¯é æ€§ã€çµæ´»æ€§å’Œé€æ˜åº¦æ–¹é¢ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RePaCAï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é™æ€APCAæŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨ä¸“é—¨ç”¨äºæ€è€ƒä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»¥å¸¦æœ‰é”™è¯¯å’Œå›ºå®šä»£ç ç‰‡æ®µçš„æç¤ºè¿›è¡Œå¼•å¯¼ï¼Œç”Ÿæˆåˆ†æä»£ç å·®å¼‚çš„æ€è€ƒé“¾ï¼Œæ¨ç†è¡¥ä¸å¦‚ä½•è§£å†³æ ¹æœ¬åŸå› ï¼Œå¹¶æœ€ç»ˆæä¾›äºŒå…ƒåˆ†ç±»ï¼šæ­£ç¡®æˆ–è¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†æé«˜è¿™äº›ä¸“é—¨é’ˆå¯¹APCAä»»åŠ¡çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ Group Relative Policy Optimizationç®—æ³•å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚åœ¨åŸºäºDefects4Jçš„æ ‡å‡†æµ‹è¯•ä¸Šè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡83.1%ï¼ŒF1åˆ†æ•°84.8%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒä¸åŒçš„æ•°æ®é›†æ—¶è¡¨ç°å‡ºå“è¶Šçš„ç»¼åˆèƒ½åŠ›ï¼Œè¶…è¿‡äº†é¢†å…ˆçš„æŠ€æœ¯ã€‚è¿™ç§æ¨ç†èƒ½åŠ›è¿˜ä¸ºè¡¥ä¸è¯„ä¼°æä¾›äº†å¢å¼ºçš„å¯è§£é‡Šæ€§ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å¾®è°ƒåçš„æ¨ç†LLMåœ¨å¢å¼ºå‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§æ–¹é¢å¯¹æ¨è¿›é™æ€APCAçš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22580v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨æ— éœ€äººå·¥å¹²é¢„å³å¯è‡ªåŠ¨ä¿®å¤è½¯ä»¶ä¸­çš„æ¼æ´ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥å…·äº§ç”Ÿçš„è¡¥ä¸å¸¸å¸¸ä»…æ»¡è¶³æµ‹è¯•ç”¨ä¾‹çš„è¦æ±‚è€Œå¹¶æœªçœŸæ­£ä¿®å¤åŸæœ‰æ¼æ´ï¼Œè¿™è¢«ç§°ä¸ºè¿‡æ‹Ÿåˆè¡¥ä¸ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè‡ªåŠ¨åŒ–è¡¥ä¸æ­£ç¡®æ€§è¯„ä¼°ï¼ˆAPCAï¼‰å°è¯•è¯†åˆ«APRå·¥å…·äº§ç”Ÿçš„è¿‡æ‹Ÿåˆè¡¥ä¸ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„é™æ€APCAæŠ€æœ¯â€”â€”RePaCAï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ€ç»´ä»»åŠ¡åˆ†æã€‚RePaCAæ¨¡å‹æ¥æ”¶æœ‰ç¼ºé™·çš„ä»£ç ç‰‡æ®µå’Œä¿®å¤åçš„ä»£ç ç‰‡æ®µä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡ç”Ÿæˆåˆ†æä»£ç å·®å¼‚çš„æ€è€ƒé“¾ï¼Œç†è§£è¡¥ä¸å¦‚ä½•ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜ï¼Œå¹¶ç»™å‡ºæ˜¯å¦ä¸ºæ­£ç¡®è¡¥ä¸æˆ–è¿‡åº¦æ‹Ÿåˆçš„äºŒå…ƒåˆ†ç±»ç»“æœã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»¥æé«˜æ¨¡å‹åœ¨APCAä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨åŸºäºDefects4Jçš„æ ‡å‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼ŒRePaCAè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†83.1%ï¼ŒF1åˆ†æ•°ä¸º84.8%ï¼Œå¹¶ä¸”åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è®­ç»ƒè¡¨ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å¾®è°ƒåçš„æ¨ç†å‹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æå‡é™æ€APCAçš„å‡†ç¡®æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰èƒ½è‡ªåŠ¨ä¿®å¤è½¯ä»¶æ¼æ´ä½†å­˜åœ¨ç”Ÿæˆè¿‡æ‹Ÿåˆè¡¥ä¸çš„é—®é¢˜ã€‚</li>
<li>è‡ªåŠ¨åŒ–è¡¥ä¸æ­£ç¡®æ€§è¯„ä¼°ï¼ˆAPCAï¼‰ç”¨äºè¯†åˆ«APRå·¥å…·äº§ç”Ÿçš„è¿‡æ‹Ÿåˆè¡¥ä¸ã€‚</li>
<li>RePaCAæ˜¯ä¸€ç§æ–°å‹çš„é™æ€APCAæŠ€æœ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ€ç»´ä»»åŠ¡åˆ†æã€‚</li>
<li>RePaCAé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼Œé‡‡ç”¨ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•æé«˜åœ¨APCAä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>RePaCAåœ¨æ ‡å‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡å’ŒF1åˆ†æ•°é«˜ã€‚</li>
<li>RePaCAåœ¨ä¸åŒæ•°æ®é›†ä¸Šå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40dcd4889c63fc44ce074f11a02e5150.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a9a7396bf813ab1c0965be50015f324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b94795eb262bfafdcbcd0f6ca7be87.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ControlMed-Adding-Reasoning-Control-to-Medical-Language-Model"><a href="#ControlMed-Adding-Reasoning-Control-to-Medical-Language-Model" class="headerlink" title="ControlMed: Adding Reasoning Control to Medical Language Model"></a>ControlMed: Adding Reasoning Control to Medical Language Model</h2><p><strong>Authors:Sung-Min Lee, Siyoon Lee, Juyeon Kim, Kyungmin Roh</strong></p>
<p>Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis. </p>
<blockquote>
<p>éšç€ä¸´åºŠå†³ç­–çš„ç”Ÿå‘½å…³é”®æ€§è´¨å¯¹å¯é æ”¯æŒçš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œå…·æœ‰å¢å¼ºå‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä¹Ÿè¶Šæ¥è¶Šå¹¿æ³›ã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œç°æœ‰çš„æ¨ç†LLMså¸¸å¸¸äº§ç”Ÿä¸å¿…è¦çš„å†—é•¿æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…å’Œå“åº”å»¶è¿Ÿã€‚è¿™äº›é™åˆ¶é˜»ç¢äº†å®ƒä»¬åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>ControlMed</strong>åŒ»ç–—è¯­è¨€æ¨¡å‹ï¼Œå®ƒå…è®¸ç”¨æˆ·åœ¨æ¨ç†æ—¶é—´é€šè¿‡ç²¾ç»†çš„ç²’åº¦æ§åˆ¶æ ‡è®°æ¥ä¸»åŠ¨æ§åˆ¶æ¨ç†è¿‡ç¨‹çš„é•¿åº¦ã€‚ControlMedé€šè¿‡ä¸‰é˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒï¼š1ï¼‰åœ¨å¤§è§„æ¨¡åˆæˆåŒ»ç–—æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–<em>ç›´æ¥</em>å’Œ<em>æ¨ç†å“åº”</em>ï¼›2ï¼‰ä½¿ç”¨å¤šé•¿åº¦æ¨ç†æ•°æ®å’Œæ˜ç¡®çš„é•¿åº¦æ§åˆ¶æ ‡è®°è¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒï¼›3ï¼‰ä½¿ç”¨åŸºäºæ¨¡å‹çš„å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜äº‹å®å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚åœ¨å¤šç§è‹±è¯­å’ŒéŸ©è¯­åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¸æœ€å…ˆè¿›æ¨¡å‹çš„æ€§èƒ½ç›¸å½“æˆ–æ›´å¥½ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦æ§åˆ¶æ¨ç†é•¿åº¦ï¼Œçµæ´»åœ°å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼ŒControlMedæ˜¯ä¸´åºŠé—®ç­”å’ŒåŒ»ç–—ä¿¡æ¯åˆ†æçš„å®ç”¨ä¸”å¯é€‚åº”çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22545v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong><br>åœ¨åŒ»ç–—é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†åº”ç”¨æ—¥ç›Šæ™®åŠï¼Œå› ä¸ºå®ƒä»¬å…·å¤‡æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ï¼Œèƒ½ä¸ºä¸´åºŠå†³ç­–æä¾›å¯é æ”¯æŒã€‚ç„¶è€Œï¼Œç°æœ‰æ¨ç†LLMså¸¸å¸¸ç”Ÿæˆå†—é•¿çš„æ¨ç†è¿‡ç¨‹ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—è¿‡å¤§å’Œå“åº”å»¶è¿Ÿã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºControlMedåŒ»å­¦è¯­è¨€æ¨¡å‹ï¼Œç”¨æˆ·å¯é€šè¿‡ç²¾ç»†çš„æ§åˆ¶æ ‡è®°åœ¨æ¨ç†æ—¶ä¸»åŠ¨æ§åˆ¶æ¨ç†é•¿åº¦ã€‚ControlMedé€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“å®ç°ï¼š1ï¼‰åœ¨å¤§è§„æ¨¡åˆæˆåŒ»å­¦æŒ‡ä»¤æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–ç›´æ¥å’Œæ¨ç†å“åº”ï¼›2ï¼‰ä½¿ç”¨å¤šé•¿åº¦æ¨ç†æ•°æ®å’Œæ˜ç¡®çš„é•¿åº¦æ§åˆ¶æ ‡è®°è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼›3ï¼‰ä½¿ç”¨æ¨¡å‹åŸºç¡€å¥–åŠ±ä¿¡å·è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº‹å®å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è‹±è¯­å’ŒéŸ©è¯­åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸æœ€æ–°æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€è¦çµæ´»å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ï¼Œæ§åˆ¶æ¨ç†é•¿åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨é€æ¸æ™®åŠï¼Œå› å…¶é«˜å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ä¸ºä¸´åºŠå†³ç­–æä¾›æ”¯æŒã€‚</li>
<li>ç°æœ‰LLMså­˜åœ¨å†—é•¿æ¨ç†é—®é¢˜ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæ¶ˆè€—å¤§åŠå“åº”å»¶è¿Ÿã€‚</li>
<li>ControlMedåŒ»å­¦è¯­è¨€æ¨¡å‹é€šè¿‡ç²¾ç»†æ§åˆ¶æ ‡è®°å®ç°æ¨ç†é•¿åº¦çš„ä¸»åŠ¨æ§åˆ¶ã€‚</li>
<li>ControlMedé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€æœ‰ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>ControlMedåœ¨è‹±è¯­å’ŒéŸ©è¯­åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç”¨æˆ·å¯çµæ´»è°ƒæ•´æ¨ç†é•¿åº¦ä»¥å¹³è¡¡æ¨ç†å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d30b8535b99d065a22f95216cf9729e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5e9c5ce514c320e99de9d6c49f2d7e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1812f1cd2be8cc272305a69f116e35e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Benchmark-Dataset-and-Evaluation-Framework-for-Vietnamese-Large-Language-Models-in-Customer-Support"><a href="#A-Benchmark-Dataset-and-Evaluation-Framework-for-Vietnamese-Large-Language-Models-in-Customer-Support" class="headerlink" title="A Benchmark Dataset and Evaluation Framework for Vietnamese Large   Language Models in Customer Support"></a>A Benchmark Dataset and Evaluation Framework for Vietnamese Large   Language Models in Customer Support</h2><p><strong>Authors:Long S. T. Nguyen, Truong P. Hua, Thanh M. Nguyen, Toan Q. Pham, Nam K. Ngo, An X. Nguyen, Nghi D. M. Pham, Nghia H. Nguyen, Tho T. Quan</strong></p>
<p>With the rapid growth of Artificial Intelligence, Large Language Models (LLMs) have become essential for Question Answering (QA) systems, improving efficiency and reducing human workload in customer service. The emergence of Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a practical choice for their accuracy, efficiency, and privacy benefits. However, domain-specific evaluations remain limited, and the absence of benchmark datasets reflecting real customer interactions makes it difficult for enterprises to select suitable models for support applications. To address this gap, we introduce the Customer Support Conversations Dataset (CSConDa), a curated benchmark of over 9,000 QA pairs drawn from real interactions with human advisors at a large Vietnamese software company. Covering diverse topics such as pricing, product availability, and technical troubleshooting, CSConDa provides a representative basis for evaluating ViLLMs in practical scenarios. We further present a comprehensive evaluation framework, benchmarking 11 lightweight open-source ViLLMs on CSConDa with both automatic metrics and syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns. This study offers insights into model behavior, explains performance differences, and identifies key areas for improvement, supporting the development of next-generation ViLLMs. By establishing a robust benchmark and systematic evaluation, our work enables informed model selection for customer service QA and advances research on Vietnamese LLMs. The dataset is publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA">https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA</a>. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½çš„é£é€Ÿå‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹äºé—®ç­”ï¼ˆQAï¼‰ç³»ç»Ÿæ¥è¯´å·²ç»å˜å¾—è‡³å…³é‡è¦ï¼Œæé«˜äº†æ•ˆç‡ï¼Œå¹¶å‡å°‘äº†å®¢æˆ·æœåŠ¡ä¸­çš„äººåŠ›å·¥ä½œé‡ã€‚è¶Šå—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆViLLMï¼‰çš„å‡ºç°ï¼Œå‡¸æ˜¾å‡ºè½»é‡åŒ–å¼€æºæ¨¡å‹å…·æœ‰å‡†ç¡®åº¦é«˜ã€æ•ˆç‡é«˜å’Œéšç§ä¼˜åŠ¿çš„ç‰¹ç‚¹ï¼Œæ˜¯ä¸€ä¸ªå®é™…å¯è¡Œçš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°ä»ç„¶æœ‰é™ï¼Œç¼ºä¹åæ˜ çœŸå®å®¢æˆ·äº’åŠ¨çš„åŸºå‡†æ•°æ®é›†ï¼Œä½¿å¾—ä¼ä¸šåœ¨é€‰æ‹©æ”¯æŒåº”ç”¨ç¨‹åºçš„åˆé€‚æ¨¡å‹æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®¢æˆ·æ”¯æŒå¯¹è¯æ•°æ®é›†ï¼ˆCSConDaï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„ç»è¿‡ç­›é€‰çš„åŸºå‡†æµ‹è¯•æ•°æ®ï¼ŒåŒ…å«è¶…è¿‡9000å¯¹é—®ç­”å¯¹ï¼Œè¿™äº›æ•°æ®æ¥è‡ªä¸€å®¶å¤§å‹è¶Šå—è½¯ä»¶å…¬å¸ä¸­äººç±»é¡¾é—®çš„çœŸå®äº’åŠ¨ã€‚CSConDaæ¶µç›–äº†å®šä»·ã€äº§å“å¯ç”¨æ€§ã€æŠ€æœ¯æ•…éšœæ’é™¤ç­‰å¤šæ ·åŒ–ä¸»é¢˜ï¼Œä¸ºè¯„ä¼°ViLLMåœ¨å®é™…åœºæ™¯ä¸­çš„è¡¨ç°æä¾›äº†ä»£è¡¨æ€§çš„åŸºç¡€ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œåœ¨CSConDaä¸Šå¯¹11ä¸ªè½»é‡åŒ–å¼€æºViLLMè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨è‡ªåŠ¨æŒ‡æ ‡å’Œå¥æ³•åˆ†ææ¥æ­ç¤ºæ¨¡å‹çš„ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œè¯­è¨€æ¨¡å¼ã€‚æœ¬ç ”ç©¶æ·±å…¥äº†è§£äº†æ¨¡å‹çš„è¡Œä¸ºï¼Œè§£é‡Šäº†æ€§èƒ½å·®å¼‚ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®æ”¹è¿›é¢†åŸŸï¼Œæœ‰åŠ©äºä¸‹ä¸€ä»£ViLLMçš„å‘å±•ã€‚é€šè¿‡å»ºç«‹ç¨³å¥çš„åŸºå‡†æµ‹è¯•å’Œç³»ç»Ÿçš„è¯„ä¼°ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¯ä»¥ä¸ºå®¢æˆ·æœåŠ¡QAæä¾›çŸ¥æƒ…çš„æ¨¡å‹é€‰æ‹©ï¼Œå¹¶æ¨åŠ¨è¶Šå—LLMçš„ç ”ç©¶ã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QAå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22542v1">PDF</a> Under review at ICCCI 2025</p>
<p><strong>Summary</strong>ï¼šéšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ç³»ç»Ÿä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œæé«˜äº†æ•ˆç‡å¹¶å‡è½»äº†å®¢æœå·¥ä½œçš„è´Ÿæ‹…ã€‚è¶Šå—è¯­LLMï¼ˆViLLMï¼‰çš„å‡ºç°å‡¸æ˜¾äº†è½»é‡çº§å¼€æºæ¨¡å‹åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œéšç§æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è¯„ä¼°ä»ç„¶æœ‰é™ï¼Œç¼ºä¹åæ˜ çœŸå®å®¢æˆ·äº’åŠ¨çš„åŸºå‡†æ•°æ®é›†ï¼Œä½¿å¾—ä¼ä¸šéš¾ä»¥é€‰æ‹©é€‚åˆçš„æ”¯æŒåº”ç”¨ç¨‹åºçš„æ¨¡å‹ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å®¢æˆ·æ”¯æŒå¯¹è¯æ•°æ®é›†ï¼ˆCSConDaï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„åŒ…å«è¶…è¿‡9000ä¸ªé—®ç­”å¯¹çš„åŸºå‡†æµ‹è¯•ï¼Œå–è‡ªè¶Šå—ä¸€å®¶å¤§å‹è½¯ä»¶å…¬å¸çœŸå®çš„äººæœºäº’åŠ¨åœºæ™¯ã€‚æˆ‘ä»¬å¯¹11ä¸ªè½»é‡çº§å¼€æºViLLMè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨è‡ªåŠ¨æŒ‡æ ‡å’Œå¥æ³•åˆ†æåœ¨CSConDaä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œæ­ç¤ºäº†æ¨¡å‹çš„ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œè¯­è¨€æ¨¡å¼ã€‚æ­¤ç ”ç©¶ä¸ºæ¨¡å‹è¡Œä¸ºæä¾›äº†è§è§£ï¼Œè§£é‡Šäº†æ€§èƒ½å·®å¼‚å¹¶ç¡®å®šäº†å…³é”®æ”¹è¿›é¢†åŸŸï¼Œæ”¯æŒä¸‹ä¸€ä»£ViLLMçš„å¼€å‘ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å»ºç«‹ç¨³å¥çš„åŸºå‡†æµ‹è¯•å’Œç³»ç»Ÿçš„è¯„ä¼°ï¼Œä¸ºå®¢æœé—®ç­”çš„æ¨¡å‹é€‰æ‹©æä¾›äº†ä¾æ®ï¼Œå¹¶æ¨åŠ¨äº†è¶Šå—è¯­LLMçš„ç ”ç©¶è¿›å±•ã€‚æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA">é“¾æ¥</a>å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ç³»ç»Ÿä¸­çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜æ•ˆç‡å’Œå‡å°‘å®¢æœå·¥ä½œé‡æ–¹é¢ã€‚</li>
<li>è¶Šå—è¯­LLMï¼ˆViLLMï¼‰å› å…¶å‡†ç¡®æ€§ã€æ•ˆç‡å’Œéšç§ä¼˜åŠ¿è€Œå—åˆ°å…³æ³¨ã€‚</li>
<li>é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹è¯„ä¼°ä»ç„¶æœ‰é™ï¼Œç¼ºä¹åæ˜ çœŸå®å®¢æˆ·äº’åŠ¨çš„åŸºå‡†æ•°æ®é›†ï¼Œç»™ä¼ä¸šé€‰æ‹©é€‚åˆçš„æ¨¡å‹å¸¦æ¥å›°éš¾ã€‚</li>
<li>å¼•å…¥å®¢æˆ·æ”¯æŒå¯¹è¯æ•°æ®é›†ï¼ˆCSConDaï¼‰ï¼ŒåŒ…å«çœŸå®äººæœºäº’åŠ¨åœºæ™¯ä¸­çš„é—®ç­”å¯¹ï¼Œä¸ºè¯„ä¼°ViLLMæä¾›å®é™…åŸºç¡€ã€‚</li>
<li>å¯¹è½»é‡çº§å¼€æºViLLMè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæ­ç¤ºå…¶ä¼˜åŠ¿ã€åŠ£åŠ¿å’Œè¯­è¨€æ¨¡å¼ã€‚</li>
<li>ç ”ç©¶ä¸ºæ¨¡å‹çš„è¡Œä¸ºå’Œæ€§èƒ½å·®å¼‚æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶æŒ‡å‡ºäº†å…³é”®æ”¹è¿›é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc6110f29275f96b84113b8d6d0f7edc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3da98cc88cdb5c3ed82947f74a5c369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff29d0dd757d698a8b89f01a29e44206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11bff28ccce2c59f4e1df3ba9257e79b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SLM-SQL-An-Exploration-of-Small-Language-Models-for-Text-to-SQL"><a href="#SLM-SQL-An-Exploration-of-Small-Language-Models-for-Text-to-SQL" class="headerlink" title="SLM-SQL: An Exploration of Small Language Models for Text-to-SQL"></a>SLM-SQL: An Exploration of Small Language Models for Text-to-SQL</h2><p><strong>Authors:Lei Sheng, Shuai-Shuai Xu</strong></p>
<p>Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87% execution accuracy (EX), while the 1.5B model achieved 67.08% EX. We will release our dataset, model, and code to github: <a target="_blank" rel="noopener" href="https://github.com/CycloneBoy/slm_sql">https://github.com/CycloneBoy/slm_sql</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆSQLæŸ¥è¯¢ï¼ˆæ–‡æœ¬åˆ°SQLï¼‰æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„å‚æ•°èŒƒå›´åœ¨0.5Båˆ°1.5Bä¹‹é—´ï¼Œç›®å‰åœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ï¼Œä¸»è¦æ˜¯ç”±äºå…¶é€»è¾‘æ¨ç†èƒ½åŠ›æœ‰é™ã€‚ç„¶è€Œï¼ŒSLMåœ¨æ¨ç†é€Ÿåº¦å’Œé€‚ç”¨äºè¾¹ç¼˜éƒ¨ç½²æ–¹é¢æœ‰ç€å›ºæœ‰ä¼˜åŠ¿ã€‚ä¸ºäº†æ¢ç´¢å…¶åœ¨æ–‡æœ¬åˆ°SQLåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€è¿‘çš„åè®­ç»ƒæŠ€æœ¯è¿›å±•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼€æºçš„SynSQL-2.5Mæ•°æ®é›†æ„å»ºäº†ä¸¤ä¸ªæ´¾ç”Ÿæ•°æ®é›†ï¼šç”¨äºSQLç”Ÿæˆçš„SynSQL-Think-916Kå’Œç”¨äºSQLåˆå¹¶ä¿®è®¢çš„SynSQL-Merge-Think-310Kã€‚ç„¶åæˆ‘ä»¬å¯¹SLMåº”ç”¨äº†åŸºäºç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒï¼Œéšåä½¿ç”¨çº æ­£è‡ªæˆ‘ä¸€è‡´æ€§æ–¹æ³•è¿›è¡Œæ¨ç†ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•SLM-SQLçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚åœ¨BIRDå¼€å‘é›†ä¸Šï¼Œäº”ä¸ªè¯„ä¼°æ¨¡å‹çš„å¹³å‡æ”¹è¿›äº†31.4ä¸ªç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ0.5Bæ¨¡å‹çš„æ‰§è¡Œå‡†ç¡®ç‡ï¼ˆEXï¼‰è¾¾åˆ°56.87%ï¼Œè€Œ1.5Bæ¨¡å‹çš„EXè¾¾åˆ°67.08%ã€‚æˆ‘ä»¬å°†åœ¨GitHubä¸Šå‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/CycloneBoy/slm_sql%E3%80%82">https://github.com/CycloneBoy/slm_sqlã€‚</a> </p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22478v1">PDF</a> 16 pages, 2 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬è½¬SQLï¼ˆText-to-SQLï¼‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè€Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å› é€»è¾‘æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œåœ¨æ­¤ç±»ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ã€‚ç„¶è€Œï¼ŒSLMsåœ¨æ¨ç†é€Ÿåº¦å’Œè¾¹ç¼˜éƒ¨ç½²æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æœ€æ–°çš„åè®­ç»ƒæŠ€æœ¯ï¼Œæ„å»ºäº†ä¸¤ä¸ªè¡ç”Ÿæ•°æ®é›†ï¼Œå¹¶é€šè¿‡ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€ææ–¹æ³•SLM-SQLæ•ˆæœæ˜¾è‘—ï¼Œå¹³å‡æå‡31.4ä¸ªç™¾åˆ†ç‚¹ã€‚ç›¸å…³æ¨¡å‹å°†åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬SQLä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹å› é€»è¾‘æ¨ç†èƒ½åŠ›æœ‰é™ï¼Œåœ¨æ­¤ç±»ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå·®ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦å’Œè¾¹ç¼˜éƒ¨ç½²æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶è€…æ„å»ºäº†ä¸¤ä¸ªç”¨äºæ–‡æœ¬è½¬SQLä»»åŠ¡çš„è¡ç”Ÿæ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ è¿›è¡Œåè®­ç»ƒï¼Œæå‡å°å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬SQLä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ‰€ææ–¹æ³•SLM-SQLåœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœï¼Œå¹³å‡æå‡31.4ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-992a77cb1808ff2672fac9fd5780f454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f37dbec1586d4a90907b7fb98c1e8977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ea54f1e24a23cc7f907042631574ebd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47130b3ffe885f0aa73e207a11070c85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38a9849d86176115bd955cd046ce0734.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Falcon-H1-A-Family-of-Hybrid-Head-Language-Models-Redefining-Efficiency-and-Performance"><a href="#Falcon-H1-A-Family-of-Hybrid-Head-Language-Models-Redefining-Efficiency-and-Performance" class="headerlink" title="Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance"></a>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance</h2><p><strong>Authors:Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha</strong></p>
<p>In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research. </p>
<blockquote>
<p>åœ¨æ­¤æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Falcon-H1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»åˆ—ï¼Œå…·æœ‰æ··åˆæ¶æ„è®¾è®¡ï¼Œé’ˆå¯¹å„ç§ç”¨ä¾‹è¿›è¡Œäº†é«˜æ€§èƒ½å’Œæ•ˆç‡çš„ä¼˜åŒ–ã€‚ä¸åŒäºæ—©æœŸä»…åŸºäºTransformeræˆ–Mambaæ¶æ„çš„Falconæ¨¡å‹ï¼ŒFalcon-H1é‡‡ç”¨äº†å¹¶è¡Œæ··åˆæ–¹æ³•ï¼Œå°†åŸºäºTransformerçš„æ³¨æ„åŠ›ä¸ä»¥çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ç›¸ç»“åˆï¼Œåè€…ä»¥å‡ºè‰²çš„é•¿ä¸Šä¸‹æ–‡è®°å¿†å’Œè®¡ç®—æ•ˆç‡è€Œé—»åã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†æ¨¡å‹è®¾è®¡ã€æ•°æ®ç­–ç•¥å’Œè®­ç»ƒåŠ¨æ€ï¼ŒæŒ‘æˆ˜äº†è¯¥é¢†åŸŸçš„ä¼ ç»Ÿå®è·µã€‚Falcon-H1ä»¥å¤šç§é…ç½®å‘å¸ƒï¼ŒåŒ…æ‹¬åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒä¼˜å‹ï¼Œå‚æ•°ä»0.5Bã€1.5Bã€1.5B-deepã€3Bã€7Båˆ°34Bã€‚è¿˜æä¾›é‡åŒ–æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œæ€»è®¡è¶…è¿‡30ä¸ªæ£€æŸ¥ç‚¹åœ¨Hugging Face Hubä¸Šæä¾›ã€‚Falcon-H1æ¨¡å‹å±•ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå‡ºè‰²çš„å‚æ•°å’Œè®­ç»ƒæ•ˆç‡ã€‚æ——èˆ°äº§å“Falcon-H1-34Båœ¨å‚æ•°æ›´å°‘å’Œæ•°æ®æ›´å°‘çš„æƒ…å†µä¸‹ï¼Œä¸Qwen3-32Bã€Qwen2.5-72Bå’ŒLlama3.3-70Bç­‰æ¨¡å‹ç›¸åŒ¹æ•Œæˆ–è¡¨ç°æ›´å¥½ã€‚è¾ƒå°çš„æ¨¡å‹ä¹Ÿæ˜¾ç¤ºå‡ºç±»ä¼¼è¶‹åŠ¿ï¼šFalcon-H1-1.5B-Deepä¸å½“å‰é¢†å…ˆçš„7B-10Bæ¨¡å‹ç›¸åŒ¹æ•Œï¼Œè€ŒFalcon-H1-0.5Bçš„è¡¨ç°ä¸2024å¹´çš„å…¸å‹7Bæ¨¡å‹ç›¸å½“ã€‚è¿™äº›æ¨¡å‹åœ¨æ¨ç†ã€æ•°å­¦ã€å¤šè¯­ç§ä»»åŠ¡ã€æŒ‡ä»¤éµå¾ªå’Œç§‘å­¦çŸ¥è¯†ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ”¯æŒé«˜è¾¾256Kä¸ªä¸Šä¸‹æ–‡æ ‡è®°å’Œ18ç§è¯­è¨€ï¼ŒFalcon-H1é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚æ‰€æœ‰æ¨¡å‹éƒ½åœ¨è®¸å¯çš„å¼€æºè®¸å¯è¯ä¸‹å‘å¸ƒï¼Œè¿™å‡¸æ˜¾äº†æˆ‘ä»¬è‡´åŠ›äºå¯è®¿é—®å’Œæœ‰å½±å“åŠ›çš„AIç ”ç©¶çš„æ‰¿è¯ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22448v1">PDF</a> Technical report of Falcon-H1 model series</p>
<p><strong>Summary</strong></p>
<p>æœ¬æŠ¥å‘Šä»‹ç»äº†æ–°å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—â€”â€”Falcon-H1ã€‚å®ƒé‡‡ç”¨æ··åˆæ¶æ„è®¾è®¡ï¼Œç»“åˆäº†Transformerçš„æ³¨æ„åŠ›æœºåˆ¶å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚Falcon-H1æ¨¡å‹ç³»åˆ—å…·æœ‰å¤šç§é…ç½®ï¼ŒåŒ…æ‹¬åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒä¼˜å‹ï¼Œå‚æ•°ä»0.5Båˆ°34Bä¸ç­‰ã€‚è¿™äº›æ¨¡å‹åœ¨å‚æ•°å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå…¶ä¸­æ——èˆ°ç‰ˆFalcon-H1-34Båœ¨å‚æ•°æ›´å°‘ã€æ•°æ®æ›´å°‘çš„æƒ…å†µä¸‹ï¼Œä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹å¦‚Qwen3-32Bç­‰ç›¸åª²ç¾ã€‚æ­¤å¤–ï¼ŒFalcon-H1è¿˜æ”¯æŒå¤šè¾¾256Kä¸ªä¸Šä¸‹æ–‡æ ‡è®°å’Œ18ç§è¯­è¨€ï¼Œé€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Falcon-H1æ˜¯ç»“åˆTransformerå’ŒSSMçš„æ–°å‹å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚</li>
<li>å®ƒé‡‡ç”¨äº†æ··åˆæ¶æ„è®¾è®¡ä»¥å®ç°é«˜æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>Falcon-H1æ¨¡å‹ç³»åˆ—å…·æœ‰å¤šç§é…ç½®ï¼ŒåŒ…æ‹¬ä¸åŒå‚æ•°å¤§å°çš„åŸºç¡€å‹å’ŒæŒ‡ä»¤è°ƒä¼˜å‹æ¨¡å‹ã€‚</li>
<li>æ——èˆ°ç‰ˆFalcon-H1-34Båœ¨å‚æ•°å’Œæ•°æ®ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ç›¸åª²ç¾ã€‚</li>
<li>Falcon-H1æ¨¡å‹åœ¨æ¨ç†ã€æ•°å­¦ã€å¤šè¯­è¨€ä»»åŠ¡ã€æŒ‡ä»¤éµå¾ªå’Œç§‘å­¦çŸ¥è¯†ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>Falcon-H1æ”¯æŒå¤šè¾¾256Kä¸ªä¸Šä¸‹æ–‡æ ‡è®°å’Œ18ç§è¯­è¨€ï¼Œé€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce7b68b599b015742eef6f9e2a5c39db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7516e8853b9e9f40b97ae8f240bee4c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14c7153eaac3869e09ac279621d74ce3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Systematic-Evaluation-of-Knowledge-Graph-Repair-with-Large-Language-Models"><a href="#Systematic-Evaluation-of-Knowledge-Graph-Repair-with-Large-Language-Models" class="headerlink" title="Systematic Evaluation of Knowledge Graph Repair with Large Language   Models"></a>Systematic Evaluation of Knowledge Graph Repair with Large Language   Models</h2><p><strong>Authors:Tung-Wei Lin, Gabe Fierro, Han Li, Tianzhen Hong, Pierluigi Nuzzo, Alberto Sangiovanni-Vinentelli</strong></p>
<p>We present a systematic approach for evaluating the quality of knowledge graph repairs with respect to constraint violations defined in shapes constraint language (SHACL). Current evaluation methods rely on \emph{ad hoc} datasets, which limits the rigorous analysis of repair systems in more general settings. Our method addresses this gap by systematically generating violations using a novel mechanism, termed violation-inducing operations (VIOs). We use the proposed evaluation framework to assess a range of repair systems which we build using large language models. We analyze the performance of these systems across different prompting strategies. Results indicate that concise prompts containing both the relevant violated SHACL constraints and key contextual information from the knowledge graph yield the best performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„è¯„ä¼°çŸ¥è¯†å›¾è°±ä¿®å¤è´¨é‡çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é’ˆå¯¹å½¢çŠ¶çº¦æŸè¯­è¨€ï¼ˆSHACLï¼‰ä¸­å®šä¹‰çš„çº¦æŸè¿è§„æƒ…å†µã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºç‰¹å®šçš„æ•°æ®é›†ï¼Œè¿™åœ¨æ›´ä¸€èˆ¬çš„ç¯å¢ƒä¸­é™åˆ¶äº†ä¿®å¤ç³»ç»Ÿçš„ä¸¥æ ¼åˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸€ç§æ–°å‹æœºåˆ¶â€”â€”è¿è§„è¯±å¯¼æ“ä½œï¼ˆVIOsï¼‰æ¥ç³»ç»Ÿåœ°ç”Ÿæˆè¿è§„æƒ…å†µï¼Œä»¥è§£å†³è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬ä½¿ç”¨æå‡ºçš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„å¤šç§ä¿®å¤ç³»ç»Ÿã€‚æˆ‘ä»¬åˆ†æäº†è¿™äº›ç³»ç»Ÿåœ¨ä¸åŒæç¤ºç­–ç•¥ä¸‹çš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒåŒ…å«ç›¸å…³è¿åçš„SHACLçº¦æŸå’Œæ¥è‡ªçŸ¥è¯†å›¾è°±çš„å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç®€æ´æç¤ºä¼šäº§ç”Ÿæœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22419v1">PDF</a> </p>
<p><strong>Summary</strong><br>ï¼šæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºSHACLçº¦æŸè¯­è¨€çš„çŸ¥è¯†å›¾è°±ä¿®å¤è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚å½“å‰è¯„ä¼°æ–¹æ³•ä¾èµ–äºç‰¹å®šæ•°æ®é›†ï¼Œé™åˆ¶äº†ä¿®å¤ç³»ç»Ÿåœ¨æ›´å¹¿æ³›ç¯å¢ƒä¸­çš„ä¸¥è°¨åˆ†æã€‚æœ¬æ–‡æ–¹æ³•é€šè¿‡å¼•å…¥ä¸€ç§æ–°å‹æœºåˆ¶â€”â€”è¿è§„è¯±å¯¼æ“ä½œï¼ˆVIOsï¼‰æ¥ç³»ç»ŸåŒ–ç”Ÿæˆè¿è§„ï¼Œä»¥è¯„ä¼°ä¿®å¤ç³»ç»Ÿã€‚åŒæ—¶ï¼Œæœ¬æ–‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº†ä¸€ç³»åˆ—ä¿®å¤ç³»ç»Ÿï¼Œå¹¶åˆ†æäº†ä¸åŒæç¤ºç­–ç•¥ä¸‹çš„ç³»ç»Ÿæ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŒ…å«ç›¸å…³è¿åçš„SHACLçº¦æŸå’Œå…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç®€æ´æç¤ºèƒ½å¸¦æ¥æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°çŸ¥è¯†å›¾è°±ä¿®å¤è´¨é‡çš„æ–¹æ³•ï¼ŒåŸºäºSHACLçº¦æŸè¯­è¨€ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•å—é™äºç‰¹å®šæ•°æ®é›†ï¼Œæ–°çš„æ–¹æ³•é€šè¿‡è¿è§„è¯±å¯¼æ“ä½œï¼ˆVIOsï¼‰ç³»ç»ŸåŒ–ç”Ÿæˆè¿è§„ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºäº†ä¸€ç³»åˆ—ä¿®å¤ç³»ç»Ÿã€‚</li>
<li>åˆ†æå‘ç°ï¼ŒåŒ…å«ç›¸å…³è¿åçš„SHACLçº¦æŸå’Œå…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç®€æ´æç¤ºèƒ½æé«˜ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¿®å¤ç³»ç»Ÿåœ¨å¤„ç†ç‰¹å®šç±»å‹çš„çº¦æŸè¿è§„æ—¶è¡¨ç°æœ€ä½³ã€‚</li>
<li>è¿™ç§æ–¹æ³•ä¸ºçŸ¥è¯†å›¾è°±ä¿®å¤é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-de965dda44a277c7d22d1d933418c286.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7df20d5925aa7b944a3389fd3f96d1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11bdbe6a3bcff9bbc810725454670a73.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NeedleChain-Measuring-Intact-Long-Context-Reasoning-Capability-of-Large-Language-Models"><a href="#NeedleChain-Measuring-Intact-Long-Context-Reasoning-Capability-of-Large-Language-Models" class="headerlink" title="NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large   Language Models"></a>NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large   Language Models</h2><p><strong>Authors:Hyeonseok Moon, Heuiseok Lim</strong></p>
<p>The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Modelsâ€™ (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/hyeonseokk/NeedleChain">https://github.com/hyeonseokk/NeedleChain</a> </p>
<blockquote>
<p>â€œé’ˆå°–åœ¨ç¨»è‰å †ä¸­çš„æŸ¥æ‰¾â€ï¼ˆNeedle-in-a-Haystackï¼ŒNIAHï¼‰åŸºå‡†æµ‹è¯•è¢«å¹¿æ³›ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é•¿æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼ˆLCï¼‰çš„ç†è§£èƒ½åŠ›ã€‚å®ƒè¯„ä»·çš„æ˜¯åœ¨ä¸€ä¸ªå¤§é‡æ— å…³è¯­å¥ä¸­è¯†åˆ«å‡ºä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚å°½ç®¡æ­¤æ–¹æ³•è¢«å¹¿æ³›æ¥å—ä¸ºè¯„ä¼°é•¿æ–‡æœ¬ç†è§£çš„æ ‡å‡†ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå®ƒå¯èƒ½ä¼šé«˜ä¼°LLMçš„çœŸæ­£é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚GPT-4oï¼Œåœ¨å®Œå…¨èå…¥ä»…ç”±ä¸æŸ¥è¯¢ç›¸å…³çš„åä¸ªå¥å­ç»„æˆçš„ç»™å®šä¸Šä¸‹æ–‡æ—¶ä¹Ÿé¢ä¸´å›°éš¾ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œâ€œé’ˆé“¾â€ï¼ˆNeedleChainï¼‰ï¼Œå…¶ä¸­çš„ä¸Šä¸‹æ–‡å®Œå…¨ç”±ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ç»„æˆï¼Œè¦æ±‚LLMå®Œå…¨ç†è§£è¾“å…¥æ‰èƒ½æ­£ç¡®å›ç­”é—®é¢˜ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å…è®¸çµæ´»çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨ç†é¡ºåºï¼Œä¸ºLLMçš„æ€§èƒ½æä¾›äº†æ›´å…¨é¢çš„åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œå¼•äººæ³¨ç›®çš„ç­–ç•¥æ¥æé«˜LLMå¯¹é•¿æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ï¼šROPEæ”¶ç¼©æ³•ã€‚æˆ‘ä»¬å¯¹å„ç§å…ˆè¿›çš„LLMè¿›è¡Œçš„å®éªŒæ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤§é‡ä¸Šä¸‹æ–‡å’Œå®Œå…¨ç†è§£ä¸Šä¸‹æ–‡ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚ç›¸å…³æºä»£ç å’Œæ•°æ®é›†å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/hyeonseokk/NeedleChain">https://github.com/hyeonseokk/NeedleChain</a> ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22411v1">PDF</a> 13 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç†è§£é•¿æ–‡æœ¬çš„èƒ½åŠ›å¸¸ç”¨Haystackä¸­çš„é’ˆï¼ˆNIAHï¼‰åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°ã€‚è¯¥æµ‹è¯•è¯„ä¼°æ¨¡å‹åœ¨å¤§é‡æ— å…³æ–‡æœ¬ä¸­è¯†åˆ«æŸ¥è¯¢ç›¸å…³ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç°NIAHå¯èƒ½é«˜ä¼°äº†LLMçš„çœŸå®é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„æ¨¡å‹å¦‚GPT-4oåœ¨ä»…åŒ…å«æŸ¥è¯¢ç›¸å…³çš„åå¥ä¸Šä¸‹æ–‡ä¸­ä¹Ÿéš¾ä»¥å®Œå…¨èå…¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•â€”â€”NeedleChainï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡å®Œå…¨ç”±æŸ¥è¯¢ç›¸å…³ä¿¡æ¯ç»„æˆï¼Œè¦æ±‚LLMå®Œå…¨ç†è§£è¾“å…¥æ‰èƒ½æ­£ç¡®å›ç­”ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å…è®¸çµæ´»çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨ç†é¡ºåºï¼Œä¸ºè¯„ä¼°LLMæ€§èƒ½æä¾›äº†æ›´å…¨é¢çš„åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥â€”â€”ROPE Contractionï¼Œä»¥æé«˜LLMå¯¹é•¿æ–‡æœ¬çš„ç†è§£èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œå„ç§å…ˆè¿›çš„LLMåœ¨å¤„ç†å¤§é‡æ–‡æœ¬å’Œå®Œå…¨ç†è§£å®ƒä»¬ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NIAHåŸºå‡†æµ‹è¯•è¢«å¹¿æ³›ç”¨äºè¯„ä¼°LLMç†è§£é•¿æ–‡æœ¬çš„èƒ½åŠ›ï¼Œä½†å¯èƒ½é«˜ä¼°äº†LLMçš„çœŸå®æ€§èƒ½ã€‚</li>
<li>å¼•å…¥NeedleChainåŸºå‡†æµ‹è¯•ï¼Œå®Œå…¨ç”±æŸ¥è¯¢ç›¸å…³ä¿¡æ¯ç»„æˆä¸Šä¸‹æ–‡ï¼Œæ›´å…¨é¢åœ°è¯„ä¼°LLMçš„æ€§èƒ½ã€‚</li>
<li>NeedleChainå…è®¸çµæ´»çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨ç†é¡ºåºã€‚</li>
<li>æå‡ºäº†æé«˜LLMé•¿æ–‡æœ¬ç†è§£èƒ½åŠ›çš„ç®€å•è€Œæœ‰æ•ˆçš„ç­–ç•¥â€”â€”ROPE Contractionã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…ˆè¿›LLMåœ¨å¤„ç†å¤§é‡æ–‡æœ¬å’Œå®Œå…¨ç†è§£å®ƒä»¬ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚</li>
<li>NeedleChainåŸºå‡†æµ‹è¯•å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hyeonseokk/NeedleChain%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hyeonseokk/NeedleChainæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6fb05881ef397e0e77d98237f17a9d3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c14aab35196aeda8be80d3680ea6f239.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b177684d2d2b1b03cde6da98154c54a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5e97bdf7322c0ee11f5dc6b32e7705e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-949bc730934ce725261dfe77620b4489.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding"><a href="#UI-AGILE-Advancing-GUI-Agents-with-Effective-Reinforcement-Learning-and-Precise-Inference-Time-Grounding" class="headerlink" title="UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and   Precise Inference-Time Grounding"></a>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and   Precise Inference-Time Grounding</h2><p><strong>Authors:Shuquan Lian, Yuhang Wu, Jia Ma, Zihan Song, Bingqi Chen, Xiawu Zheng, Hui Li</strong></p>
<p>The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a â€œSimple Thinkingâ€ reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å‡ºç°æ¨åŠ¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†èƒ½åŠ›çš„æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GUIä»£ç†è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯ä»ç„¶é¢ä¸´ç€æ¨ç†è®¾è®¡ã€å¥–åŠ±æ— æ•ˆå’Œè§†è§‰å™ªå£°çš„å›°å¢ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UI-AGILEï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µå¢å¼ºGUIä»£ç†çš„ç»¼åˆæ¡†æ¶ã€‚åœ¨è®­ç»ƒæ–¹é¢ï¼Œæˆ‘ä»¬å¯¹æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹æå‡ºäº†ä¸€ç³»åˆ—æ”¹è¿›ï¼š1ï¼‰è¿ç»­å¥–åŠ±åŠŸèƒ½ï¼Œä»¥æ¿€åŠ±é«˜ç²¾åº¦æ¥åœ°ï¼›2ï¼‰â€œç®€å•æ€è€ƒâ€å¥–åŠ±ï¼Œä»¥å¹³è¡¡è§„åˆ’ä¸é€Ÿåº¦å’Œæ¥åœ°å‡†ç¡®æ€§ï¼›3ï¼‰åŸºäºè£å‰ªçš„é‡æ–°é‡‡æ ·ç­–ç•¥ï¼Œä»¥ç¼“è§£ç¨€ç–å¥–åŠ±é—®é¢˜å¹¶æé«˜å¤æ‚ä»»åŠ¡ä¸Šçš„å­¦ä¹ æ•ˆæœã€‚å¯¹äºæ¨ç†ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†è§£æ¥åœ°ä¸é€‰æ‹©æ³•ï¼Œé€šè¿‡å°†å›¾åƒåˆ†å‰²æˆæ›´å°ã€æ›´æ˜“ç®¡ç†çš„éƒ¨åˆ†ï¼Œæ˜¾è‘—æé«˜äº†åœ¨é«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨ä¸Šçš„æ¥åœ°å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUI-AGILEåœ¨ScreenSpot-Proå’ŒScreenSpot-v2ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨æˆ‘ä»¬æå‡ºçš„è®­ç»ƒå’Œæ¨ç†å¢å¼ºæ–¹æ³•ï¼Œåœ¨ScreenSpot-Proä¸Šçš„æœ€ä½³åŸºçº¿åŸºç¡€ä¸Šæé«˜äº†23%çš„æ¥åœ°å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22025v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å…´èµ·å¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä½“èƒ½åŠ›çš„æ¨åŠ¨ä½œç”¨ã€‚é’ˆå¯¹ç°æœ‰GUIæ™ºèƒ½ä½“çš„è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚æ¨ç†è®¾è®¡å›°å¢ƒã€å¥–åŠ±æœºåˆ¶æ— æ•ˆå’Œè§†è§‰å™ªå£°ç­‰ï¼Œæœ¬æ–‡æå‡ºäº†UI-AGILEæ¡†æ¶ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µæå‡GUIæ™ºèƒ½ä½“çš„æ€§èƒ½ã€‚è®­ç»ƒæ–¹é¢ï¼Œä½œè€…å¯¹ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹è¿›è¡Œäº†ä¸€ç³»åˆ—æ”¹è¿›ï¼ŒåŒ…æ‹¬è¿ç»­å¥–åŠ±å‡½æ•°ã€â€œç®€å•æ€è€ƒâ€å¥–åŠ±å’ŒåŸºäºè£å‰ªçš„é‡æ–°é‡‡æ ·ç­–ç•¥ã€‚æ¨ç†æ–¹é¢ï¼Œæå‡ºäº†ä¸€ç§åä¸ºåˆ†è§£æ¥åœ°ä¸é€‰æ‹©çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å°†å›¾åƒåˆ†å‰²æˆè¾ƒå°çš„éƒ¨åˆ†æ¥æé«˜åœ¨é«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨ä¸Šçš„æ¥åœ°ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒUI-AGILEåœ¨ScreenSpot-Proå’ŒScreenSpot-v2ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨æœ¬æ–‡æå‡ºçš„è®­ç»ƒå’Œæ¨ç†å¢å¼ºæ–¹æ³•ï¼Œåœ¨ScreenSpot-Proä¸Šçš„æ¥åœ°ç²¾åº¦æ¯”æœ€ä½³åŸºçº¿æé«˜äº†23%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ¨åŠ¨äº†GUIæ™ºèƒ½ä½“èƒ½åŠ›çš„æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>ç°æœ‰GUIæ™ºèƒ½ä½“çš„è®­ç»ƒå’Œæ¨ç†æŠ€æœ¯é¢ä¸´å›°å¢ƒï¼Œå¦‚æ¨ç†è®¾è®¡ã€å¥–åŠ±æœºåˆ¶æ— æ•ˆå’Œè§†è§‰å™ªå£°ç­‰é—®é¢˜ã€‚</li>
<li>UI-AGILEæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡GUIæ™ºèƒ½ä½“çš„è®­ç»ƒå’Œæ¨ç†æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒé˜¶æ®µçš„æ”¹è¿›åŒ…æ‹¬è¿ç»­å¥–åŠ±å‡½æ•°ã€â€œç®€å•æ€è€ƒâ€å¥–åŠ±å’ŒåŸºäºè£å‰ªçš„é‡æ–°é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>æ¨ç†é˜¶æ®µæå‡ºäº†ä¸€ç§æ–°çš„åˆ†è§£æ¥åœ°ä¸é€‰æ‹©æ–¹æ³•ï¼Œæé«˜äº†åœ¨é«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨ä¸Šçš„æ¥åœ°ç²¾åº¦ã€‚</li>
<li>UI-AGILEåœ¨ScreenSpot-Proå’ŒScreenSpot-v2ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22025">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b970548c2e88f21acecfb4570fe8b117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d9b970fb882bb410249af1d8d75e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b563e1fbc362d852361194d3d02fff35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c8a6cdb74148dcf18ba3d162a8be5e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01169681927e06c6abc4b235a8176279.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks"><a href="#Reasoning-Language-Models-for-Root-Cause-Analysis-in-5G-Wireless-Networks" class="headerlink" title="Reasoning Language Models for Root Cause Analysis in 5G Wireless   Networks"></a>Reasoning Language Models for Root Cause Analysis in 5G Wireless   Networks</h2><p><strong>Authors:Mohamed Sana, Nicola Piovesan, Antonio De Domenico, Yibin Kang, Haozhe Zhang, Merouane Debbah, Fadhel Ayed</strong></p>
<p>Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management. </p>
<blockquote>
<p>ç§»åŠ¨ç½‘ç»œä¸­çš„æ ¹æœ¬åŸå› åˆ†æï¼ˆRCAï¼‰ç”±äºéœ€è¦å¯è§£é‡Šæ€§ã€é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œå› æœæ¨ç†ï¼Œä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒRCAçš„è½»é‡çº§æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†TeleLogsï¼Œè¿™æ˜¯ä¸€ä¸ªç»è¿‡æ³¨é‡Šçš„æ•…éšœæ’é™¤é—®é¢˜æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°RCAèƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œç°æœ‰çš„å¼€æºæ¨ç†LLMåœ¨å¤„ç†è¿™äº›é—®é¢˜æ—¶é‡åˆ°å›°éš¾ï¼Œè¿™çªæ˜¾äº†é¢†åŸŸç‰¹å®šé€‚åº”æ€§çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå°†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥æé«˜LLMçš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¯¹ä¸€ç³»åˆ—RCAæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ•´åˆé¢†åŸŸçŸ¥è¯†å¹¶ç”Ÿæˆç»“æ„åŒ–ã€å¤šæ­¥éª¤çš„è¯Šæ–­è§£é‡Šï¼Œæé«˜å¯è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚è·¨å¤šä¸ªLLMå¤§å°çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ¨ç†å’Œéæ¨ç†æ¨¡å‹ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—çš„æå‡ï¼ŒåŒ…æ‹¬å¯¹éšæœºæµ‹è¯•å˜é‡çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœå±•ç¤ºäº†é¢†åŸŸé€‚åº”ã€å¢å¼ºæ¨ç†çš„LLMåœ¨ç½‘ç»œæ“ä½œå’Œç®¡ç†ä¸­çš„å®é™…å’Œå¯è§£é‡ŠRCAçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21974v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œé’ˆå¯¹ç§»åŠ¨ç½‘ç»œä¸­çš„æ ¹æœ¬åŸå› åˆ†æï¼ˆRCAï¼‰æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½»é‡çº§æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥TeleLogsæ•°æ®é›†ï¼Œè¯„ä¼°ç°æœ‰æ¨ç†LLMåœ¨æ•…éšœæ’é™¤é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œå‘ç°éœ€è¦é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œé€‚åº”ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜LLMçš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚è¯¥æ–¹æ³•å¯å¾®è°ƒä¸€ç³»åˆ—RCAæ¨¡å‹ï¼Œæ•´åˆé¢†åŸŸçŸ¥è¯†ï¼Œç”Ÿæˆç»“æ„åŒ–ã€å¤šæ­¥éª¤çš„è¯Šæ–­è§£é‡Šï¼Œæé«˜è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªLLMå°ºå¯¸ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºæœ€æ–°æ¨ç†å’Œéæ¨ç†æ¨¡å‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶åœ¨éšæœºæµ‹è¯•å˜ä½“ä¸Šå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™è¯æ˜äº†é¢†åŸŸé€‚åº”çš„æ¨ç†å¢å¼ºLLMåœ¨ç½‘ç»œæ“ä½œå’Œç®¡ç†ä¸­çš„å®é™…å’Œå¯è§£é‡ŠRCAæ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨ç½‘ç»œä¸­çš„æ ¹æœ¬åŸå› åˆ†æï¼ˆRCAï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¯è§£é‡Šæ€§ã€é¢†åŸŸä¸“ä¸šçŸ¥è¯†å’Œå› æœæ¨ç†ã€‚</li>
<li>æå‡ºä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½»é‡çº§æ¡†æ¶è¿›è¡ŒRCAã€‚</li>
<li>å¼•å…¥TeleLogsæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°æ¨ç†LLMåœ¨æ•…éšœæ’é™¤é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¼€æ”¾æºç æ¨ç†LLMåœ¨è¿™äº›é—®é¢˜ä¸Šè¡¨ç°æŒ£æ‰ï¼Œéœ€è¦é¢†åŸŸç‰¹å®šé€‚åº”ã€‚</li>
<li>æå‡ºä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜LLMçš„å‡†ç¡®æ€§å’Œæ¨ç†è´¨é‡ã€‚</li>
<li>æ–¹æ³•å¯ç”Ÿæˆç»“æ„åŒ–ã€å¤šæ­¥éª¤çš„è¯Šæ–­è§£é‡Šï¼Œæé«˜è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eff662b0ba4077f724ffe760004d8b3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-018dd22e5e0beeb4caa61e38e8c26f03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d1540c074228ea344e5b18c92c64abd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f1b6bc7fe7824e443e1c15973c04d3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-564b33a7cdd461df70b3a9647cbe78b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14d77f994d65a5027398a0489b124e9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7af241a61e986f3ef2b770d0debc33ce.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="EDGE-GRPO-Entropy-Driven-GRPO-with-Guided-Error-Correction-for-Advantage-Diversity"><a href="#EDGE-GRPO-Entropy-Driven-GRPO-with-Guided-Error-Correction-for-Advantage-Diversity" class="headerlink" title="EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for   Advantage Diversity"></a>EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for   Advantage Diversity</h2><p><strong>Authors:Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang</strong></p>
<p>Large Language Models (LLMs) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/EDGE-GRPO">https://github.com/ZhangXJ199/EDGE-GRPO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨é€æ­¥æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚ç„¶è€Œï¼Œä¾èµ–ç¨€ç–å¥–åŠ±è§„åˆ™çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ç»å¸¸é‡åˆ°ç»„å†…ç›¸åŒå¥–åŠ±çš„é—®é¢˜ï¼Œå¯¼è‡´ä¼˜åŠ¿å´©æºƒé—®é¢˜ã€‚ç°æœ‰å·¥ä½œé€šå¸¸ä»ä¸¤ä¸ªè§’åº¦æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼šé€šè¿‡å¼ºåˆ¶æ¨¡å‹åå°„æ¥å¢å¼ºå“åº”å¤šæ ·æ€§ï¼Œä»¥åŠå¼•å…¥å†…éƒ¨åé¦ˆæ¥å¢å¼ºè®­ç»ƒä¿¡å·ï¼ˆä¼˜åŠ¿ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†ææ¨¡å‹åå°„çš„å±€é™æ€§ï¼Œå¹¶ç ”ç©¶ç»†ç²’åº¦æ ·æœ¬çº§åˆ«å“åº”çš„ç­–ç•¥ç†µã€‚åŸºäºæˆ‘ä»¬çš„å®éªŒå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†EDGE-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é‡‡ç”¨ç†µé©±åŠ¨ä¼˜åŠ¿å’Œå¯¼å‘è¯¯å·®æ ¡æ­£ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ä¼˜åŠ¿å´©æºƒé—®é¢˜ã€‚åœ¨å‡ ä¸ªä¸»è¦çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ZhangXJ199/EDGE-GRPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZhangXJ199/EDGE-GRPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21848v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡é€æ­¥æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œä¾èµ–ç¨€ç–å¥–åŠ±è§„åˆ™çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç®—æ³•å¸¸é‡åˆ°ç»„å†…ç›¸åŒå¥–åŠ±çš„é—®é¢˜ï¼Œå¯¼è‡´ä¼˜åŠ¿å´©æºƒã€‚ç°æœ‰å·¥ä½œä¸»è¦ä»ä¸¤ä¸ªè§’åº¦è§£å†³è¿™ä¸€é—®é¢˜ï¼šå¼ºåˆ¶æ¨¡å‹åæ€ä»¥å¢å¼ºå“åº”å¤šæ ·æ€§ï¼Œä»¥åŠå¼•å…¥å†…éƒ¨åé¦ˆä»¥å¢åŠ è®­ç»ƒä¿¡å·ï¼ˆä¼˜åŠ¿ï¼‰ã€‚æœ¬ç ”ç©¶é¦–å…ˆåˆ†æäº†æ¨¡å‹åæ€çš„å±€é™æ€§ï¼Œå¹¶è°ƒæŸ¥äº†å“åº”ç­–ç•¥ç†µçš„ç²¾ç»†æ ·æœ¬æ°´å¹³ã€‚åŸºäºå®éªŒå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†EDGE-GRPOç®—æ³•ï¼Œé‡‡ç”¨ç†µé©±åŠ¨ä¼˜åŠ¿å’Œå¼•å¯¼é”™è¯¯ä¿®æ­£ï¼Œæœ‰æ•ˆè§£å†³ä¼˜åŠ¿å´©æºƒé—®é¢˜ã€‚åœ¨å¤šä¸ªä¸»è¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸‹é€æ­¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Group Relative Policy Optimization (GRPO) ç®—æ³•é¢ä¸´ç»„å†…ç›¸åŒå¥–åŠ±çš„é—®é¢˜ï¼Œå³ä¼˜åŠ¿å´©æºƒã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆåŒ…æ‹¬å¼ºåˆ¶æ¨¡å‹åæ€å’Œå¼•å…¥å†…éƒ¨åé¦ˆã€‚</li>
<li>æœ¬ç ”ç©¶åˆ†æäº†æ¨¡å‹åæ€çš„å±€é™æ€§ï¼Œå¹¶è°ƒæŸ¥äº†å“åº”ç­–ç•¥ç†µçš„ç²¾ç»†æ ·æœ¬æ°´å¹³ã€‚</li>
<li>æå‡ºäº†EDGE-GRPOç®—æ³•ï¼Œé€šè¿‡ç†µé©±åŠ¨ä¼˜åŠ¿å’Œå¼•å¯¼é”™è¯¯ä¿®æ­£è§£å†³ä¼˜åŠ¿å´©æºƒé—®é¢˜ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒEDGE-GRPOç®—æ³•å±•ç°å‡ºä¼˜è¶Šæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21848">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-49d7a391f75849f2b6fd443091f68b21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78228bc30078a0eea005c58f8eae2859.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c7d8b58ef35b36e888de31d5d898812d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c412a144dad3f4b52144108f015bcec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8fb1eda475128a71c3cf7d046c5eaa6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AutoTIR-Autonomous-Tools-Integrated-Reasoning-via-Reinforcement-Learning"><a href="#AutoTIR-Autonomous-Tools-Integrated-Reasoning-via-Reinforcement-Learning" class="headerlink" title="AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement   Learning"></a>AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement   Learning</h2><p><strong>Authors:Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, Li Du</strong></p>
<p>Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/AutoTIR">https://github.com/weiyifan1023/AutoTIR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é¢å‘æ¨ç†çš„åè®­ç»ƒå¢å¼ºï¼Œè¿›åŒ–ä¸ºå¼ºå¤§çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰ã€‚å·¥å…·é›†æˆæ¨ç†ï¼ˆTIRï¼‰é€šè¿‡èå…¥å¤–éƒ¨å·¥å…·è¿›ä¸€æ­¥æ‰©å±•äº†å®ƒä»¬çš„èƒ½åŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºåƒµåŒ–çš„é¢„å®šä¹‰å·¥å…·ä½¿ç”¨æ¨¡å¼ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼±æ ¸å¿ƒè¯­è¨€æŠ€èƒ½ã€‚å—äººç±»è‡ªé€‚åº”é€‰æ‹©å·¥å…·èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†AutoTIRï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒä½¿LLMèƒ½å¤Ÿè‡ªä¸»å†³å®šæ˜¯å¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­è°ƒç”¨å·¥å…·ä»¥åŠè°ƒç”¨å“ªä¸ªå·¥å…·ï¼Œè€Œä¸æ˜¯éµå¾ªé™æ€çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚AutoTIRåˆ©ç”¨æ··åˆå¥–åŠ±æœºåˆ¶ï¼Œè”åˆä¼˜åŒ–é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç­”æ¡ˆæ­£ç¡®æ€§ã€ç»“æ„è¾“å‡ºéµå¾ªå’Œé”™è¯¯å·¥å…·ä½¿ç”¨çš„æƒ©ç½šï¼Œä»è€Œé¼“åŠ±ç²¾ç¡®æ¨ç†å’Œæœ‰æ•ˆçš„å·¥å…·é›†æˆã€‚åœ¨å¤šæ ·åŒ–çŸ¥è¯†å¯†é›†å‹ã€æ•°å­¦å’Œä¸€èˆ¬è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒAutoTIRå®ç°äº†å“è¶Šçš„æ•´ä½“æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨å·¥å…·ä½¿ç”¨è¡Œä¸ºä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¼ºåŒ–å­¦ä¹ åœ¨LLMä¸­æ„å»ºçœŸæ­£é€šç”¨å’Œå¯æ‰©å±•çš„TIRèƒ½åŠ›çš„æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/AutoTIR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/weiyifan1023/AutoTIRæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21836v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ¨ç†å¯¼å‘çš„åæœŸè®­ç»ƒè¿›åŒ–ä¸ºå¼ºå¤§çš„å¤§å‹æ¨ç†æ¨¡å‹ã€‚å·¥å…·é›†æˆæ¨ç†æŠ€æœ¯è¿›ä¸€æ­¥æ‰©å±•äº†å…¶åŠŸèƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢„è®¾çš„å·¥å…·ä½¿ç”¨æ¨¡å¼ï¼Œå¯èƒ½ä¼šå‰Šå¼±å…¶æ ¸å¿ƒè¯­è¨€ç«äº‰åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„AutoTIRæ¡†æ¶ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©æ˜¯å¦è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œè¿›è¡Œæ¨ç†å†³ç­–ã€‚AutoTIRä½¿ç”¨æ··åˆå¥–åŠ±æœºåˆ¶ä¼˜åŒ–ä»»åŠ¡ç‰¹å®šç­”æ¡ˆçš„æ­£ç¡®æ€§ã€ç»“æ„åŒ–è¾“å‡ºçš„éµå¾ªæ€§ä»¥åŠå¯¹ä¸æ­£ç¡®å·¥å…·ä½¿ç”¨çš„æƒ©ç½šï¼Œä»è€Œå®ç°ç²¾ç¡®æ¨ç†å’Œé«˜æ•ˆå·¥å…·é›†æˆã€‚è¯„ä¼°è¡¨æ˜ï¼ŒAutoTIRåœ¨å¤šç§çŸ¥è¯†å¯†é›†å‹ã€æ•°å­¦å’Œä¸€èˆ¬è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šçš„æ€»ä½“æ€§èƒ½ä¼˜äºåŸºå‡†æµ‹è¯•ï¼Œå±•ç°å‡ºè‰¯å¥½çš„å·¥å…·ä½¿ç”¨è¡Œä¸ºæ³›åŒ–èƒ½åŠ›ã€‚è¯¥ä»£ç å’Œæ•°æ®åœ¨GitHubä¸Šå¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡æ¨ç†å¯¼å‘çš„åæœŸè®­ç»ƒå¯è¿›åŒ–ä¸ºå¤§å‹æ¨ç†æ¨¡å‹ã€‚</li>
<li>å·¥å…·é›†æˆæ¨ç†æŠ€æœ¯æ‰©å±•äº†è¯­è¨€æ¨¡å‹çš„åŠŸèƒ½ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¾èµ–é¢„è®¾å·¥å…·ä½¿ç”¨æ¨¡å¼çš„å±€é™æ€§ã€‚</li>
<li>AutoTIRæ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½è‡ªä¸»å†³å®šæ˜¯å¦ä½¿ç”¨å¤–éƒ¨å·¥å…·è¿›è¡Œæ¨ç†ã€‚</li>
<li>AutoTIRä½¿ç”¨æ··åˆå¥–åŠ±æœºåˆ¶ä¼˜åŒ–ä»»åŠ¡æ€§èƒ½ï¼ŒåŒ…æ‹¬ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€ç»“æ„åŒ–è¾“å‡ºçš„éµå¾ªæ€§å’Œå¯¹ä¸æ­£ç¡®å·¥å…·ä½¿ç”¨çš„æƒ©ç½šã€‚</li>
<li>AutoTIRåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¼˜äºåŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰è‰¯å¥½çš„å·¥å…·ä½¿ç”¨è¡Œä¸ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-17f6d7bc1cd007c9a8418c663125f9e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-daff2ca5e66efdbe0591ba29fb28c5fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab5feed5f2d9ef8280964cc390799f48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bfd4c995a2d4e80c314fdb7dd065169.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MixGRPO-Unlocking-Flow-based-GRPO-Efficiency-with-Mixed-ODE-SDE"><a href="#MixGRPO-Unlocking-Flow-based-GRPO-Efficiency-with-Mixed-ODE-SDE" class="headerlink" title="MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE"></a>MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</h2><p><strong>Authors:Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, Zhao Zhong</strong></p>
<p>Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\href{<a target="_blank" rel="noopener" href="https://github.com/Tencent-Hunyuan/MixGRPO%7D%7BMixGRPO%7D$">https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$</a>. </p>
<blockquote>
<p>å°½ç®¡GRPOåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¢å¼ºäº†å›¾åƒç”Ÿæˆä¸­äººç±»åå¥½å¯¹é½çš„æµé‡åŒ¹é…æ¨¡å‹ï¼Œä½†FlowGRPOç­‰æ–¹æ³•ä»è¡¨ç°å‡ºç”±äºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æŒ‡å®šçš„æ‰€æœ‰å»å™ªæ­¥éª¤éƒ½éœ€è¦é‡‡æ ·å’Œä¼˜åŒ–è€Œå¯¼è‡´çš„ä¸é«˜æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†$\textbf{MixGRPO}$ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ··åˆå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰å’Œå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„é›†æˆï¼Œåˆ©ç”¨æ··åˆé‡‡æ ·ç­–ç•¥çš„çµæ´»æ€§ã€‚è¿™ç®€åŒ–äº†MDPä¸­çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œæé«˜äº†æ•ˆç‡å¹¶æå‡äº†æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒMixGRPOå¼•å…¥äº†ä¸€ä¸ªæ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œä»…åœ¨çª—å£å†…ä½¿ç”¨SDEé‡‡æ ·å’ŒGRPOæŒ‡å¯¼çš„ä¼˜åŒ–ï¼Œè€Œåœ¨çª—å£å¤–åº”ç”¨ODEé‡‡æ ·ã€‚è¿™ç§è®¾è®¡å°†é‡‡æ ·éšæœºæ€§é™åˆ¶åœ¨çª—å£çš„æ—¶é—´æ­¥å†…ï¼Œä»è€Œå‡å°‘äº†ä¼˜åŒ–å¼€é”€ï¼Œå¹¶å…è®¸æ›´é›†ä¸­çš„æ¢¯åº¦æ›´æ–°ä»¥åŠ é€Ÿæ”¶æ•›ã€‚æ­¤å¤–ï¼Œç”±äºæ»‘åŠ¨çª—å£ä¹‹å¤–çš„æ—¶é—´æ­¥ä¸å‚ä¸ä¼˜åŒ–ï¼Œå› æ­¤æ”¯æŒæ›´é«˜é˜¶çš„æ±‚è§£å™¨è¿›è¡Œé‡‡æ ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´å¿«çš„å˜ä½“ï¼Œç§°ä¸º$\textbf{MixGRPO-Flash}$ï¼Œå®ƒè¿›ä¸€æ­¥æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶å®ç°äº†ç›¸å½“çš„æ€§èƒ½ã€‚MixGRPOåœ¨äººç±»åå¥½å¯¹é½çš„å¤šä¸ªç»´åº¦ä¸Šéƒ½å–å¾—äº†å®è´¨æ€§çš„æ”¶ç›Šï¼Œåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢éƒ½è¶…è¶Šäº†DanceGRPOï¼Œè®­ç»ƒæ—¶é—´é™ä½äº†è¿‘50%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMixGRPO-Flashè¿›ä¸€æ­¥å°†è®­ç»ƒæ—¶é—´å‡å°‘äº†71%ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨$\href{<a target="_blank" rel="noopener" href="https://github.com/Tencent-Hunyuan/MixGRPO%7D%7BMixGRPO%7D$%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21802v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶MixGRPOï¼Œå®ƒé€šè¿‡æ··åˆé‡‡æ ·ç­–ç•¥ï¼Œåˆ©ç”¨éšæœºå¾®åˆ†æ–¹ç¨‹å’Œæ™®é€šå¾®åˆ†æ–¹ç¨‹æ¥ä¼˜åŒ–Markovå†³ç­–è¿‡ç¨‹ä¸­çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚MixGRPOå¼•å…¥æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œåªåœ¨çª—å£å†…ä½¿ç”¨SDEé‡‡æ ·å’ŒGRPOå¼•å¯¼çš„ä¼˜åŒ–ï¼Œçª—å£å¤–åº”ç”¨ODEé‡‡æ ·ã€‚è¯¥æ–¹æ³•é™ä½äº†ä¼˜åŒ–å¼€é”€ï¼Œå¹¶æ”¯æŒæ›´é«˜é˜¶çš„é‡‡æ ·æ±‚è§£å™¨ï¼Œè¿›ä¸€æ­¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚å…¶ä¸­ï¼ŒMixGRPO-Flashä½œä¸ºæ›´é«˜æ•ˆå˜ç§ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­äº†çº¦71%ã€‚æ­¤æ–°æ–¹æ³•åœ¨äººç±»åå¥½å¯¹é½çš„å¤šä¸ªç»´åº¦ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MixGRPOæ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å›¾åƒç”Ÿæˆä¸­äººç±»åå¥½å¯¹é½çš„æµåŒ¹é…æ¨¡å‹ã€‚</li>
<li>å®ƒç»“åˆäº†éšæœºå¾®åˆ†æ–¹ç¨‹å’Œæ™®é€šå¾®åˆ†æ–¹ç¨‹æ¥ä¼˜åŒ–Markovå†³ç­–è¿‡ç¨‹ã€‚</li>
<li>MixGRPOå¼•å…¥äº†æ»‘åŠ¨çª—å£æœºåˆ¶ï¼Œåªåœ¨ç‰¹å®šçª—å£å†…è¿›è¡Œé‡‡æ ·å’Œä¼˜åŒ–ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•æ”¯æŒæ›´é«˜é˜¶çš„é‡‡æ ·æ±‚è§£å™¨ï¼Œè¿›ä¸€æ­¥æå‡äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>MixGRPO-Flashä½œä¸ºMixGRPOçš„å˜ç§ï¼Œè®­ç»ƒæ—¶é—´ç¼©çŸ­äº†çº¦71%ã€‚</li>
<li>MixGRPOåœ¨äººç±»åå¥½å¯¹é½çš„å¤šä¸ªç»´åº¦ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5264b6e26575c8f8553ca9e59feeb2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e699cfbe3e3784b29f90dfeed79f8899.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-225a3524c97f4b9f31ba60fb16ee1b4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef751e3b4f6b936bc3744f0bd5c09104.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Libra-Assessing-and-Improving-Reward-Model-by-Learning-to-Think"><a href="#Libra-Assessing-and-Improving-Reward-Model-by-Learning-to-Think" class="headerlink" title="Libra: Assessing and Improving Reward Model by Learning to Think"></a>Libra: Assessing and Improving Reward Model by Learning to Think</h2><p><strong>Authors:Meng Zhou, Bei Li, Jiahao Liu, Xiaowen Shi, Yang Bai, Rongxiang Weng, Jingang Wang, Xunliang Cai</strong></p>
<p>Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¥–åŠ±æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»æµçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒèŒƒå¼ä¾èµ–äºåŸºäºè§„åˆ™æˆ–åŸºäºå‚è€ƒçš„å¥–åŠ±ï¼Œè¿™å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®çš„å±€é™æ€§ï¼š1ï¼‰éœ€è¦ç²¾ç»†æ ‡æ³¨çš„å‚è€ƒç­”æ¡ˆæ‰èƒ½è·å¾—å¥–åŠ±ï¼›2ï¼‰éœ€è¦çº¦æŸçš„è¾“å‡ºæ ¼å¼ã€‚è¿™äº›å±€é™æ€§ä»æ ¹æœ¬ä¸Šé˜»ç¢äº†å¼ºåŒ–å­¦ä¹ æ•°æ®çš„è¿›ä¸€æ­¥æ‰©å±•å’Œæ¨¡å‹æ¨ç†æ€§èƒ½çš„æŒç»­æå‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢è¯„ä¼°å’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­æ€§èƒ½çš„æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªé¢å‘æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼ˆLibra Benchï¼‰ï¼Œè¯¥ç³»ç»Ÿæ˜¯ä»å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜å’Œé«˜é˜¶æ¨ç†æ¨¡å‹ä¸­æ”¶é›†æ„å»ºçš„ï¼Œä»¥è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†åœºæ™¯ä¸­çš„åŸºå‡†æµ‹è¯•å±€é™æ€§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§é€šè¿‡â€œå­¦ä¹ æ€è€ƒâ€æ–¹æ³•æ¥æ”¹è¿›ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚åŸºäºè¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†Libra-RMç³»åˆ—ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å…·æœ‰æ¨ç†èƒ½åŠ›çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¿›è¡Œäº†å…¨é¢çš„ä¸‹æ¸¸å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜äº†æˆ‘ä»¬çš„Libra Benchä¸ä¸‹æ¸¸åº”ç”¨çš„ç›¸å…³æ€§ï¼Œä»¥åŠLibra-RMåœ¨æ— éœ€æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹è¿›ä¸€æ­¥æ”¹è¿›æ¨ç†æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21645v1">PDF</a> Work In Progress</p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å½“å‰å¥–åŠ±æ¨¡å‹åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ï¼Œå­˜åœ¨ä¾èµ–ç²¾ç»†æ ‡æ³¨çš„å‚è€ƒç­”æ¡ˆå’Œè¾“å‡ºæ ¼å¼å—é™ç­‰å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬æ„å»ºLibra BenchåŸºå‡†æµ‹è¯•é›†å’Œæ”¹è¿›ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ–¹æ³•ã€‚Libra Benchæ¶µç›–å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜å’Œé«˜çº§æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚æ–°æ–¹æ³•é€šè¿‡é‡‡ç”¨å­¦ä¹ æ€è€ƒçš„æ–¹æ³•æ”¹è¿›ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œå¹¶å¼€å‘å‡ºå…·æœ‰æ¨ç†èƒ½åŠ›çš„Libra-RMç³»åˆ—å¥–åŠ±æ¨¡å‹ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜Libra Benchä¸ä¸‹æ¸¸åº”ç”¨çš„ç›¸å…³æ€§ï¼Œä»¥åŠLibra-RMåœ¨è¿›ä¸€æ­¥æé«˜æ¨ç†æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¢å¼ºäº†è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¥–åŠ±æ¨¡å‹åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä¾èµ–ç²¾ç»†æ ‡æ³¨çš„å‚è€ƒç­”æ¡ˆå’Œè¾“å‡ºæ ¼å¼å—é™ã€‚</li>
<li>Libra Benchçš„æ„å»ºæ—¨åœ¨è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬å„ç§æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ã€‚</li>
<li>é€šè¿‡å­¦ä¹ æ€è€ƒçš„æ–¹æ³•æ”¹è¿›ç”Ÿæˆå¥–åŠ±æ¨¡å‹ã€‚</li>
<li>Libra-RMç³»åˆ—å¥–åŠ±æ¨¡å‹å…·æœ‰æ¨ç†èƒ½åŠ›ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†Libra Benchä¸ä¸‹æ¸¸åº”ç”¨çš„ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59e625c4e3018af02988fc8cd4d3ad3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d84129b3ffee915ec99734cc14a5ed4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-042417aef56647b4a5f61730d1e0dcfe.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Enhanced-Reinforcement-Learning-for-Diverse-and-Novel-Recommendations"><a href="#Large-Language-Model-Enhanced-Reinforcement-Learning-for-Diverse-and-Novel-Recommendations" class="headerlink" title="Large Language Model-Enhanced Reinforcement Learning for Diverse and   Novel Recommendations"></a>Large Language Model-Enhanced Reinforcement Learning for Diverse and   Novel Recommendations</h2><p><strong>Authors:Jiin Woo, Alireza Bagheri Garakani, Tianchen Zhou, Zhishen Huang, Yan Gao</strong></p>
<p>In recommendation systems, diversity and novelty are essential for capturing varied user preferences and encouraging exploration, yet many systems prioritize click relevance. While reinforcement learning (RL) has been explored to improve diversity, it often depends on random exploration that may not align with user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a novel method that leverages large language models (LLMs) as reference policies to suggest novel items, while training a lightweight policy to refine these suggestions using system-specific data. The method formulates training as a bilevel optimization between actor and critic networks, enabling the critic to selectively favor promising novel actions and the actor to improve its policy beyond LLM recommendations. To mitigate overestimation of unreliable LLM suggestions, we apply regularization that anchors critic values for unexplored items close to well-estimated dataset actions. Experiments on real-world datasets show that LAAC outperforms existing baselines in diversity, novelty, and accuracy, while remaining robust on imbalanced data, effectively integrating LLM knowledge without expensive fine-tuning. </p>
<blockquote>
<p>åœ¨æ¨èç³»ç»Ÿä¸­ï¼Œå¤šæ ·æ€§å’Œæ–°é¢–æ€§å¯¹äºæ•æ‰å„ç§ç”¨æˆ·åå¥½å’Œé¼“åŠ±æ¢ç´¢è‡³å…³é‡è¦ï¼Œç„¶è€Œè®¸å¤šç³»ç»Ÿæ›´ä¼˜å…ˆç‚¹å‡»ç›¸å…³æ€§ã€‚è™½ç„¶å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²è¢«æ¢ç´¢ç”¨äºæé«˜å¤šæ ·æ€§ï¼Œä½†å®ƒé€šå¸¸ä¾èµ–äºéšæœºæ¢ç´¢ï¼Œå¯èƒ½ä¸ç”¨æˆ·å…´è¶£ä¸ç¬¦ã€‚æˆ‘ä»¬æå‡ºäº†LAACï¼ˆLLMå¼•å¯¼çš„å¯¹æŠ—æ€§æ¼”å‘˜è¯„è®ºå®¶ï¼ŒLLM-guided Adversarial Actor Criticï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå‚è€ƒç­–ç•¥æ¥å»ºè®®æ–°é¢–é¡¹ç›®çš„æ–°æ–¹æ³•ï¼ŒåŒæ—¶è®­ç»ƒä¸€ä¸ªè½»é‡çº§ç­–ç•¥ï¼Œä½¿ç”¨ç³»ç»Ÿç‰¹å®šæ•°æ®æ¥å®Œå–„è¿™äº›å»ºè®®ã€‚è¯¥æ–¹æ³•å°†è®­ç»ƒåˆ¶å®šä¸ºæ¼”å‘˜å’Œè¯„è®ºå®¶ç½‘ç»œä¹‹é—´çš„åŒçº§ä¼˜åŒ–ï¼Œä½¿è¯„è®ºå®¶èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°æ”¯æŒæœ‰å‰é€”çš„æ–°é¢–è¡ŒåŠ¨ï¼Œå¹¶ä½¿æ¼”å‘˜èƒ½å¤Ÿè¶…è¶ŠLLMå»ºè®®æ”¹è¿›å…¶ç­–ç•¥ã€‚ä¸ºäº†å‡è½»å¯¹ä¸å¯é çš„LLMå»ºè®®çš„è¿‡åº¦ä¼°è®¡ï¼Œæˆ‘ä»¬åº”ç”¨äº†å°†æœªæ¢ç´¢é¡¹ç›®çš„è¯„è®ºå®¶ä»·å€¼é”šå®šåˆ°ä¼°ç®—è‰¯å¥½çš„æ•°æ®é›†è¡ŒåŠ¨é™„è¿‘çš„æ­£åˆ™åŒ–ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLAACåœ¨å¤šæ ·æ€§ã€æ–°é¢–æ€§å’Œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ï¼ŒåŒæ—¶åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šä¿æŒç¨³å¥ï¼Œæœ‰æ•ˆåœ°æ•´åˆäº†LLMçŸ¥è¯†ï¼Œæ— éœ€æ˜‚è´µçš„å¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21274v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºLAACçš„æ–°å‹æ¨èç³»ç»Ÿæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå‚è€ƒç­–ç•¥æ¥æ¨èæ–°é¢–é¡¹ç›®ï¼ŒåŒæ—¶è®­ç»ƒè½»é‡çº§ç­–ç•¥ä½¿ç”¨ç³»ç»Ÿç‰¹å®šæ•°æ®æ¥å®Œå–„è¿™äº›å»ºè®®ã€‚LAACé€šè¿‡æ¼”å‘˜å’Œè¯„è®ºå®¶ç½‘ç»œä¹‹é—´çš„äºŒå±‚ä¼˜åŒ–è¿›è¡Œè®­ç»ƒï¼Œä½¿è¯„è®ºå®¶èƒ½å¤Ÿæœ‰é€‰æ‹©åœ°æ”¯æŒæœ‰å‰æ™¯çš„æ–°è¡ŒåŠ¨ï¼Œè€Œæ¼”å‘˜åˆ™èƒ½åœ¨è¶…è¶ŠLLMå»ºè®®çš„åŸºç¡€ä¸Šæ”¹è¿›å…¶ç­–ç•¥ã€‚ä¸ºç¼“è§£å¯¹ä¸å¯é LLMå»ºè®®çš„è¿‡åº¦ä¼°è®¡ï¼Œæœ¬æ–‡é‡‡ç”¨æ­£åˆ™åŒ–æ–¹æ³•ï¼Œå°†æœªç»æ¢ç´¢é¡¹ç›®çš„è¯„è®ºå®¶å€¼é”šå®šåˆ°ä¼°ç®—è‰¯å¥½çš„æ•°æ®é›†æ“ä½œä¸Šã€‚å®éªŒè¡¨æ˜ï¼ŒLAACåœ¨å¤šæ ·æ€§ã€æ–°é¢–æ€§å’Œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ï¼ŒåŒæ—¶åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®æ—¶ä¿æŒç¨³å¥ï¼Œæœ‰æ•ˆåœ°èåˆäº†LLMçŸ¥è¯†ï¼Œæ— éœ€æ˜‚è´µçš„å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿä¸­å¤šæ ·æ€§å’Œæ–°é¢–æ€§å¯¹äºæ•æ‰ç”¨æˆ·åå¥½å’Œé¼“åŠ±æ¢ç´¢è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è®¸å¤šæ¨èç³»ç»Ÿè¿‡äºæ³¨é‡ç‚¹å‡»ç›¸å…³æ€§ï¼Œå¿½è§†äº†å¤šæ ·æ€§å’Œæ–°é¢–æ€§ã€‚</li>
<li>LAACæ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå‚è€ƒç­–ç•¥æ¥æ¨èæ–°é¡¹ç›®ã€‚</li>
<li>LAACé€šè¿‡æ¼”å‘˜å’Œè¯„è®ºå®¶ç½‘ç»œçš„äºŒå±‚ä¼˜åŒ–è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜ç­–ç•¥è´¨é‡å’Œæ¨èå¤šæ ·æ€§ã€‚</li>
<li>LAACé‡‡ç”¨æ­£åˆ™åŒ–æ¥ç¼“è§£å¯¹ä¸å¯é LLMå»ºè®®çš„è¿‡åº¦ä¼°è®¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLAACåœ¨å¤šæ ·æ€§ã€æ–°é¢–æ€§å’Œå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21274">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8b9edcb11ef6c6b50de7791cfe974d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16f0d6235a1471f664e3d56ad3995a78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ac6cbe79a3e38aa2dcb9e7ed175bbc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7bcd0090731f194fe8f975abcfebb49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9f13c1a4bcad058f999ec1a02f4fcbc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning"><a href="#LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning" class="headerlink" title="LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning"></a>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning</h2><p><strong>Authors:Yining Huang, Bin Li, Keke Tang, Meilian Chen</strong></p>
<p>Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by â€œThinking, Fast and Slow,â€ which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different â€œsubregionsâ€ of an LLMâ€™s parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines. </p>
<blockquote>
<p>å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI-O1ï¼Œåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—ç›Šäºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚ç„¶è€Œï¼Œæå‡å®ƒä»¬çš„æ€§èƒ½é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ã€å¤§å‹æ¨¡å‹ä»¥åŠç²¾ç»†è°ƒæ•´æ‰€æœ‰å‚æ•°ã€‚è™½ç„¶å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦è§£å†³é¢†åŸŸé€‚åº”æˆ–é€å±‚åˆ†é…é—®é¢˜ï¼Œè€Œä¸æ˜¯æ˜¾å¼åœ°é’ˆå¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œä¸åŒçš„å“åº”éœ€æ±‚è°ƒæ•´ã€‚å—ã€Šæ€è€ƒï¼Œå¿«ä¸æ…¢ã€‹çš„å¯å‘ï¼Œè¯¥ä¹¦æè¿°äº†ä¸¤ç§æˆªç„¶ä¸åŒçš„æ€ç»´æ–¹å¼â€”â€”ç³»ç»Ÿ1ï¼ˆå¿«é€Ÿã€ç›´è§‚ã€é€šå¸¸è‡ªåŠ¨ï¼‰å’Œç³»ç»Ÿ2ï¼ˆè¾ƒæ…¢ã€æ›´åŠ å®¡æ…å’Œåˆ†ææ€§ï¼‰â€”â€”æˆ‘ä»¬ç±»æ¯”æ¨æ–­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°çš„â€œä¸åŒå­åŒºåŸŸâ€å¯èƒ½åŒæ ·ä¸“é•¿äºå¿«é€Ÿç›´è§‚ååº”çš„ä»»åŠ¡ä¸éœ€è¦å¤šæ­¥éª¤é€»è¾‘æ¨ç†çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA-PARåŒç³»ç»ŸLoRAæ¡†æ¶ï¼Œå®ƒæ ¹æ®ç³»ç»Ÿ1æˆ–ç³»ç»Ÿ2çš„éœ€æ±‚æ¥åˆ’åˆ†æ•°æ®å’Œå‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡å‹è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨å¯¹ä»»åŠ¡æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œæ ¹æ®é‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ’åˆ†ï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒç³»ç»Ÿ1ä»»åŠ¡ä»¥å¢å¼ºçŸ¥è¯†å’Œç›´è§‰ï¼Œç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç²¾ç‚¼ç³»ç»Ÿ2ä»»åŠ¡ä»¥åŠ å¼ºæ›´æ·±å±‚æ¬¡çš„é€»è¾‘æ€è€ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œå³SFTå’ŒRLï¼Œé™ä½äº†æ´»åŠ¨å‚æ•°çš„ä½¿ç”¨é‡ï¼ŒåŒæ—¶è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°çš„PEFTåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20999v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹ç”Ÿæˆæ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O1å—ç›Šäºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œä½†è¦æå‡å…¶æ€§èƒ½é€šå¸¸éœ€è¦å¤§é‡æ•°æ®ã€å¤§å‹æ¨¡å‹åŠå…¨å‚æ•°å¾®è°ƒã€‚è™½ç„¶å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é¢†åŸŸé€‚åº”æˆ–é€å±‚åˆ†é…ï¼Œå¹¶æœªæ˜ç¡®é’ˆå¯¹æ•°æ®å‚æ•°é€‚åº”ä¸åŒå“åº”éœ€æ±‚ã€‚å—â€œå¿«é€Ÿä¸æ…¢é€Ÿæ€è€ƒâ€å¯å‘ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°å­åŒºåŸŸåˆ’åˆ†ä¸ºé€‚åº”å¿«é€Ÿç›´è§‰ååº”ä¸éœ€è¦å¤šæ­¥éª¤é€»è¾‘æ¨ç†çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œæå‡ºLoRA-PARåŒç³»ç»Ÿæ¡†æ¶ï¼ŒæŒ‰ç³»ç»Ÿ1æˆ–ç³»ç»Ÿ2éœ€æ±‚åˆ’åˆ†æ•°æ®ä¸å‚æ•°ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ä½¿ç”¨æ›´å°‘ä½†æ›´é›†ä¸­çš„å‚æ•°ã€‚é€šè¿‡å¤šä»»åŠ¡è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨è¿›è¡Œåˆ†ç±»ï¼ŒåŸºäºé‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒç³»ç»Ÿ1ä»»åŠ¡å¢å¼ºçŸ¥è¯†å’Œç›´è§‰ï¼Œç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®Œå–„ç³»ç»Ÿ2ä»»åŠ¡ä»¥å¼ºåŒ–æ·±åº¦é€»è¾‘æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥åœ¨é™ä½æ´»åŠ¨å‚æ•°ä½¿ç”¨çš„åŒæ—¶ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°PEFTåŸºå‡†çº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹ç”Ÿæˆæ¨¡å‹å—ç›Šäºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ã€‚</li>
<li>å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨é¢†åŸŸé€‚åº”å’Œé€å±‚åˆ†é…ã€‚</li>
<li>å—â€œå¿«é€Ÿä¸æ…¢é€Ÿæ€è€ƒâ€å¯å‘ï¼Œæå‡ºLLMå‚æ•°å­åŒºåŸŸåˆ’åˆ†ä»¥é€‚åº”ä¸åŒä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>æå‡ºLoRA-PARåŒç³»ç»Ÿæ¡†æ¶ï¼ŒæŒ‰ç³»ç»Ÿ1å’Œç³»ç»Ÿ2éœ€æ±‚åˆ’åˆ†æ•°æ®å’Œå‚æ•°ã€‚</li>
<li>é‡‡ç”¨å¤šä»»åŠ¡è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨åˆ†ç±»ä»»åŠ¡æ•°æ®ï¼ŒåŸºäºé‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ’åˆ†ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¼ºåŒ–çŸ¥è¯†å’Œç›´è§‰ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¼ºåŒ–æ·±åº¦é€»è¾‘æ€è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0810715b92b44bb39ec2f5b01cf629b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb76039cbbf7be98a38de2b90f4a279e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcb2dd9a528120e29fb4850cd5d9cb67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2580bb41ded3d58d93d174d53008b11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c283ec3ed495975a4cbbaba5edd58222.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Learning-Only-with-Images-Visual-Reinforcement-Learning-with-Reasoning-Rendering-and-Visual-Feedback"><a href="#Learning-Only-with-Images-Visual-Reinforcement-Learning-with-Reasoning-Rendering-and-Visual-Feedback" class="headerlink" title="Learning Only with Images: Visual Reinforcement Learning with Reasoning,   Rendering, and Visual Feedback"></a>Learning Only with Images: Visual Reinforcement Learning with Reasoning,   Rendering, and Visual Feedback</h2><p><strong>Authors:Yang Chen, Yufan Shen, Wenxuan Huang, Sheng Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Jiajun Bu, Botian Shi, Yu Qiao</strong></p>
<p>Multimodal Large Language Models (MLLMs) exhibit impressive performance across various visual tasks. Subsequent investigations into enhancing their visual reasoning abilities have significantly expanded their performance envelope. However, a critical bottleneck in the advancement of MLLMs toward deep visual reasoning is their heavy reliance on curated image-text supervision. To solve this problem, we introduce a novel framework termed <code>Reasoning-Rendering-Visual-Feedback&#39;&#39; (RRVF), which enables MLLMs to learn complex visual reasoning from only raw images. This framework builds on the </code>Asymmetry of Verificationâ€™â€™ principle to train MLLMs, i.e., verifying the rendered output against a source image is easier than generating it. We demonstrate that this relative ease provides an ideal reward signal for optimization via Reinforcement Learning (RL) training, reducing reliance on the image-text supervision. Guided by the above principle, RRVF implements a closed-loop iterative process encompassing reasoning, rendering, and visual feedback components, enabling the model to perform self-correction through multi-turn interactions, while this pipeline can be optimized end-to-end by the GRPO algorithm. Extensive evaluations are conducted on image-to-code generation across two diverse domains: data charts and web interfaces. The RRVF-trained model not only outperforms existing open-source MLLMs and supervised fine-tuning baselines but also exhibits superior generalization to unseen datasets. Critically, the modelâ€™s performance surpasses that of the more advanced MLLM used to provide the feedback signal during training. This work establishes a self-improvement paradigm that offers a viable path to robust, generalizable models without reliance on explicit supervision. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/L-O-I/RRVF">https://github.com/L-O-I/RRVF</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚åç»­å…³äºæé«˜å…¶è§†è§‰æ¨ç†èƒ½åŠ›çš„ç ”ç©¶æå¤§åœ°æ‰©å¤§äº†å…¶æ€§èƒ½èŒƒå›´ã€‚ç„¶è€Œï¼Œåœ¨æ¨åŠ¨MLLMså®ç°æ·±åº¦è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨ä¸€ä¸ªå…³é”®ç“¶é¢ˆï¼Œå³å®ƒä»¬ä¸¥é‡ä¾èµ–äºç²¾é€‰çš„å›¾åƒæ–‡æœ¬ç›‘ç£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œæ¨ç†æ¸²æŸ“è§†è§‰åé¦ˆâ€ï¼ˆRRVFï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿MLLMsèƒ½å¤Ÿä»åŸå§‹å›¾åƒä¸­å­¦ä¹ å¤æ‚çš„è§†è§‰æ¨ç†ã€‚è¯¥æ¡†æ¶å»ºç«‹åœ¨â€œéªŒè¯ä¸å¯¹ç§°æ€§â€åŸåˆ™çš„åŸºç¡€ä¸Šæ¥è®­ç»ƒMLLMsï¼Œå³éªŒè¯æ¸²æŸ“è¾“å‡ºä¸æºå›¾åƒç›¸æ¯”æ›´å®¹æ˜“ç”Ÿæˆã€‚æˆ‘ä»¬è¯æ˜è¿™ç§ç›¸å¯¹å®¹æ˜“æ€§ä¸ºé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒä¼˜åŒ–æä¾›äº†ç†æƒ³çš„å¥–åŠ±ä¿¡å·ï¼Œå‡å°‘äº†å›¾åƒæ–‡æœ¬ç›‘ç£çš„ä¾èµ–ã€‚åœ¨ä»¥ä¸ŠåŸåˆ™çš„å¼•å¯¼ä¸‹ï¼ŒRRVFå®ç°äº†ä¸€ä¸ªé—­ç¯è¿­ä»£è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨ç†ã€æ¸²æŸ“å’Œè§†è§‰åé¦ˆç»„ä»¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å¤šè½®äº¤äº’è¿›è¡Œè‡ªæˆ‘æ ¡æ­£ï¼ŒåŒæ—¶è¿™ä¸ªæµç¨‹å¯ä»¥é€šè¿‡GRPOç®—æ³•è¿›è¡Œç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚åœ¨æ•°æ®å›¾è¡¨å’Œç½‘é¡µç•Œé¢ä¸¤ä¸ªä¸åŒé¢†åŸŸè¿›è¡Œäº†å›¾åƒåˆ°ä»£ç ç”Ÿæˆçš„å¤§é‡è¯„ä¼°ã€‚é€šè¿‡RRVFè®­ç»ƒçš„æ¨¡å‹ä¸ä»…ä¼˜äºç°æœ‰çš„å¼€æºMLLMså’Œç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œè€Œä¸”åœ¨è¿›è¡Œæœªè§æ•°æ®é›†çš„æ¨ç†æ—¶è¡¨ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ã€‚å…³é”®çš„æ˜¯ï¼Œè¯¥æ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºæä¾›åé¦ˆä¿¡å·çš„æ›´å…ˆè¿›çš„MLLMã€‚è¿™é¡¹å·¥ä½œå»ºç«‹äº†ä¸€ç§è‡ªæˆ‘æ”¹è¿›çš„æ¨¡å¼ï¼Œä¸ºæ„å»ºä¸ä¾èµ–æ˜¾å¼ç›‘ç£çš„ç¨³å¥ã€é€šç”¨æ¨¡å‹æä¾›äº†å¯è¡Œçš„è·¯å¾„ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/L-O-I/RRVF%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/L-O-I/RRVFä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20766v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å…¶å¯¹é«˜è´¨é‡å›¾æ–‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–é™åˆ¶äº†å…¶åœ¨æ·±åº¦è§†è§‰æ¨ç†é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚ä¸ºè§£å†³æ­¤ç“¶é¢ˆï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºâ€œReasoning-Rendering-Visual-Feedbackâ€ï¼ˆRRVFï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œä½¿MLLMsä»…é€šè¿‡åŸå§‹å›¾åƒå­¦ä¹ å¤æ‚çš„è§†è§‰æ¨ç†ã€‚è¯¥æ¡†æ¶åŸºäºâ€œéªŒè¯ä¸å¯¹ç§°æ€§â€åŸç†è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–ï¼Œå‡å°‘å¯¹å›¾æ–‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚RRVFæ¡†æ¶åœ¨å›¾åƒåˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ï¼ˆæ•°æ®å›¾è¡¨å’Œç½‘é¡µç•Œé¢ï¼‰ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰å¼€æºMLLMså’Œç›‘ç£å¾®è°ƒåŸºçº¿ï¼Œä¸”å¯¹æ–°æ•°æ®é›†å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å·¥ä½œä¸ºæ— æ˜ç¡®ç›‘ç£çš„ç¨³å¥ã€æ³›åŒ–æ¨¡å‹æä¾›äº†å¯è¡Œçš„è‡ªæˆ‘æ”¹è¿›èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>MLLMsåœ¨æ·±åº¦è§†è§‰æ¨ç†æ–¹é¢å­˜åœ¨å¯¹é«˜è´¨é‡å›¾æ–‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚</li>
<li>å¼•å…¥åä¸ºRRVFçš„æ–°å‹æ¡†æ¶ï¼Œä½¿MLLMsä»…é€šè¿‡åŸå§‹å›¾åƒå­¦ä¹ å¤æ‚çš„è§†è§‰æ¨ç†ã€‚</li>
<li>RRVFæ¡†æ¶åŸºäºâ€œéªŒè¯ä¸å¯¹ç§°æ€§â€åŸç†ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>RRVFæ¡†æ¶åœ¨å›¾åƒåˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°æ˜¾è‘—ï¼Œè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>RRVFè®­ç»ƒçš„æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨æ–°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5de7a97a06a4c3ba90ed7d4cb44cc70b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9928df9022f0320078e09f2fd281190a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-254de0243a5c814d71e2dc75d99d18a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e96e94f9718173c3c938b84f31242a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8f2e9a62e004ceb29644aade0fce8d6.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Geometric-Mean-Policy-Optimization"><a href="#Geometric-Mean-Policy-Optimization" class="headerlink" title="Geometric-Mean Policy Optimization"></a>Geometric-Mean Policy Optimization</h2><p><strong>Authors:Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei</strong></p>
<p>Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at <a target="_blank" rel="noopener" href="https://github.com/callsys/GMPO">https://github.com/callsys/GMPO</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–ä»¤ç‰Œçº§åˆ«å¥–åŠ±çš„ç®—æœ¯å¹³å‡å€¼ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒGRPOåœ¨å¤„ç†å…·æœ‰å¼‚å¸¸é‡è¦æ€§åŠ æƒå¥–åŠ±çš„ä»¤ç‰Œæ—¶ä¼šå‡ºç°ç­–ç•¥æ›´æ–°ä¸ç¨³å®šçš„é—®é¢˜ï¼Œè¿™è¡¨ç°ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­é‡è¦æ€§é‡‡æ ·æ¯”ç‡çš„æç«¯åŒ–ï¼Œå³å½“å‰ç­–ç•¥å’Œæ—§ç­–ç•¥åˆ†é…ç»™ä»¤ç‰Œé‡‡æ ·æ¦‚ç‡ä¹‹é—´çš„æ¯”ç‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ä½•å‡å€¼ç­–ç•¥ä¼˜åŒ–ï¼ˆGMPOï¼‰ï¼Œå®ƒæ˜¯GRPOçš„ä¸€ç§ç¨³å®šå˜ä½“ã€‚GMPOä¸åŒäºä¼˜åŒ–ç®—æœ¯å‡å€¼ï¼Œè€Œæ˜¯æœ€å¤§åŒ–ä»¤ç‰Œçº§åˆ«å¥–åŠ±çš„å‡ ä½•å‡å€¼ï¼Œè¿™ä»æœ¬è´¨ä¸Šè®²å¯¹å¼‚å¸¸å€¼ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œå¹¶ä¿æŒäº†é‡è¦æ€§é‡‡æ ·æ¯”ç‡æ›´ç¨³å®šçš„èŒƒå›´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…¨é¢çš„ç†è®ºå’Œå®éªŒåˆ†æï¼Œä»¥è¯æ˜GMPOçš„è®¾è®¡å’Œç¨³å®šæ€§ä¼˜åŠ¿ã€‚é™¤äº†æé«˜ç¨³å®šæ€§å¤–ï¼ŒGMPO-7Båœ¨å¤šæ•°å­¦æœ¯æ•°å­¦åŸºå‡†æµ‹è¯•å’Œå¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºGRPOï¼Œå¹³å‡é«˜å‡º4.1%ï¼ˆåŒ…æ‹¬AIME24ã€AMCã€MATH500ã€OlympiadBenchã€Minervaå’ŒGeometry3Kï¼‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/callsys/GMPO%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/callsys/GMPOä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20673v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/callsys/GMPO">https://github.com/callsys/GMPO</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æœ€è¿‘é€šè¿‡Group Relative Policy Optimization (GRPO)ç­‰æŠ€æœ¯çš„æå‡å¾—åˆ°äº†å¢å¼ºï¼Œä½†GRPOåœ¨å¤„ç†å…·æœ‰å¼‚å¸¸é‡è¦æ€§åŠ æƒå¥–åŠ±çš„ä»¤ç‰Œæ—¶ä¼šå‡ºç°æ”¿ç­–æ›´æ–°ä¸ç¨³å®šçš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ç¨³å®šçš„å˜ä½“â€”â€”Geometric-Mean Policy Optimization (GMPO)ï¼Œå®ƒé€šè¿‡æœ€å¤§åŒ–ä»¤ç‰Œçº§åˆ«å¥–åŠ±çš„å‡ ä½•å¹³å‡å€¼æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œå¯¹å¼‚å¸¸å€¼å…·æœ‰è¾ƒä½çš„æ•æ„Ÿæ€§ï¼Œå¹¶èƒ½ç»´æŒæ›´ç¨³å®šçš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡ã€‚GMPOä¸ä»…æé«˜äº†ç¨³å®šæ€§ï¼Œè¿˜åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œå¤šæ¨¡å¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜äºGRPOçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GRPOé€šè¿‡ä¼˜åŒ–ä»¤ç‰Œçº§åˆ«å¥–åŠ±çš„ç®—æœ¯å¹³å‡å€¼å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GRPOåœ¨å¤„ç†å…·æœ‰å¼‚å¸¸é‡è¦æ€§åŠ æƒå¥–åŠ±çš„ä»¤ç‰Œæ—¶ä¼šå‡ºç°æ”¿ç­–æ›´æ–°ä¸ç¨³å®šçš„é—®é¢˜ã€‚</li>
<li>GMPOæ˜¯GRPOçš„ä¸€ç§ç¨³å®šå˜ä½“ï¼Œé€šè¿‡æœ€å¤§åŒ–ä»¤ç‰Œçº§åˆ«å¥–åŠ±çš„å‡ ä½•å¹³å‡å€¼æ¥ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>GMPOå¯¹å¼‚å¸¸å€¼å…·æœ‰è¾ƒä½çš„æ•æ„Ÿæ€§ï¼Œå¹¶èƒ½ç»´æŒæ›´ç¨³å®šçš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡ã€‚</li>
<li>GMPOåœ¨æé«˜ç¨³å®šæ€§çš„åŒæ—¶ï¼Œè¿˜åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡ä¼˜äºGRPO 4.1%ã€‚</li>
<li>GMPOåœ¨å¤šæ¨¡å¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šç›¸æ¯”GRPOæœ‰1.4%çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20673">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4f5c028dfaec857be85b699dbc71a4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3c86f2d558210acd7bb52dec93dbf5f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Kimi-K2-Open-Agentic-Intelligence"><a href="#Kimi-K2-Open-Agentic-Intelligence" class="headerlink" title="Kimi K2: Open Agentic Intelligence"></a>Kimi K2: Open Agentic Intelligence</h2><p><strong>Authors: Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, Xinxing Zu</strong></p>
<p>We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.   Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual â€“ surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Kimi K2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰32äº¿æ¿€æ´»å‚æ•°å’Œ1ä¸‡äº¿æ€»å‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†MuonClipä¼˜åŒ–å™¨ï¼Œå®ƒåœ¨Muonçš„åŸºç¡€ä¸Šé‡‡ç”¨æ–°é¢–çš„QK-clipæŠ€æœ¯ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼ŒåŒæ—¶äº«å—Muonçš„é«˜çº§ä»¤ç‰Œæ•ˆç‡ã€‚åŸºäºMuonClipï¼ŒK2åœ¨15.5ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ²¡æœ‰æŸå¤±å³°å€¼ã€‚åœ¨è®­ç»ƒåï¼ŒK2ç»å†äº†å¤šé˜¶æ®µè®­ç»ƒåå¤„ç†è¿‡ç¨‹ï¼Œä»¥å¤§è§„æ¨¡ä»£ç†æ•°æ®åˆæˆç®¡é“å’Œè”åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µä¸ºç‰¹è‰²ï¼Œæ¨¡å‹é€šè¿‡ä¸ç°å®å’Œåˆæˆç¯å¢ƒçš„äº’åŠ¨æé«˜å…¶èƒ½åŠ›ã€‚Kimi K2åœ¨å¼€æºéæ€è€ƒæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ™ºèƒ½èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒK2åœ¨Tau2-Benchä¸Šè·å¾—66.1åˆ†ï¼Œåœ¨ACEBenchï¼ˆè‹±è¯­ï¼‰ä¸Šè·å¾—76.5åˆ†ï¼Œåœ¨SWE-Benchç»è¿‡éªŒè¯çš„æƒ…å†µä¸‹è·å¾—65.8åˆ†ï¼Œåœ¨SWE-Benchå¤šè¯­è¨€æƒ…å†µä¸‹è·å¾—47.3åˆ†ï¼Œåœ¨éæ€è€ƒè®¾ç½®ä¸­è¶…è¶Šäº†å¤§å¤šæ•°å¼€æ”¾å’Œå°é—­æºä»£ç çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒè¿˜åœ¨ç¼–ç ã€æ•°å­¦å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œåœ¨LiveCodeBench v6ä¸Šè·å¾—53.7åˆ†ï¼Œåœ¨AIME 2025ä¸Šè·å¾—49.5åˆ†ï¼Œåœ¨GPQA-Diamondä¸Šè·å¾—75.1åˆ†ï¼Œåœ¨OJBenchä¸Šè·å¾—27.1åˆ†ï¼Œæ‰€æœ‰è¿™äº›æˆç»©éƒ½æ²¡æœ‰ç»è¿‡æ·±å…¥æ€è€ƒã€‚è¿™äº›ç»“æœå°†Kimi K2å®šä½ä¸ºè¿„ä»Šä¸ºæ­¢æœ€å¼ºå¤§çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¹‹ä¸€ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹å’Œæ™ºèƒ½ä»»åŠ¡æ–¹é¢ã€‚æˆ‘ä»¬å‘å¸ƒäº†åŸºç¡€å’Œè®­ç»ƒåçš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›æœªæ¥å¯¹æ™ºèƒ½çš„ç ”ç©¶å’Œåº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20534v1">PDF</a> tech report of Kimi K2</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†åä¸ºKimi K2çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ä¸ä¼˜å¼‚çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹å¼•å…¥æ··åˆä¸“å®¶æŠ€æœ¯ï¼ˆMoEï¼‰ï¼Œå…·å¤‡é«˜åº¦çµæ´»æ€§ä¸ç²¾å‡†åº¦ã€‚é€šè¿‡é‡‡ç”¨åˆ›æ–°çš„MuonClipä¼˜åŒ–å™¨ï¼ŒK2åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒç¨³å®šï¼Œå¹¶æå‡äº†ä»¤ç‰Œæ•ˆç‡ã€‚ç»è¿‡é¢„è®­ç»ƒä¸å¤šé˜¶æ®µåè®­ç»ƒï¼ŒK2åœ¨æ™ºèƒ½æ•°æ®åˆæˆç®¡é“ä¸å¼ºåŒ–å­¦ä¹ è”åˆé˜¶æ®µä¸­ä¸æ–­æå‡è‡ªèº«èƒ½åŠ›ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒKimi K2å‡å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹ä¸æ™ºèƒ½ä»»åŠ¡æ–¹é¢è¡¨ç°çªå‡ºã€‚æœ¬æ–‡æœ€åå…¬å¸ƒäº†åŸºç¡€ä¸åè®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œä»¥ä¾¿æœªæ¥ç ”ç©¶ä¸åº”ç”¨æ™ºèƒ½ä½“æ™ºèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Kimi K2æ˜¯ä¸€ä¸ªé‡‡ç”¨æ··åˆä¸“å®¶æŠ€æœ¯çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡å¼ºå¤§çš„èƒ½åŠ›ä¸ä¼˜å¼‚çš„è¡¨ç°ã€‚</li>
<li>MuonClipä¼˜åŒ–å™¨çš„å¼•å…¥æé«˜äº†æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§ä¸ä»¤ç‰Œæ•ˆç‡ã€‚</li>
<li>Kimi K2ç»å†äº†é¢„è®­ç»ƒä¸å¤šé˜¶æ®µåè®­ç»ƒï¼ŒåŒ…æ‹¬æ™ºèƒ½æ•°æ®åˆæˆç®¡é“ä¸å¼ºåŒ–å­¦ä¹ è”åˆé˜¶æ®µã€‚</li>
<li>Kimi K2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯ä»¶å·¥ç¨‹ä¸æ™ºèƒ½ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>Kimi K2åœ¨éæ€è€ƒæ¨¡å¼ä¸‹å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç¼–ç ã€æ•°å­¦ä¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Kimi K2çš„å‘å¸ƒæ—¨åœ¨ä¿ƒè¿›æœªæ¥å¯¹æ™ºèƒ½ä½“æ™ºèƒ½çš„ç ”ç©¶ä¸åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdf6512818782fda8077a9a8b930bb0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58059939dc908ea0f9952d27d68bd7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d30cccdeaca40f78d47eaa958fd98cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d9510d2805e09b4fe8e67a97ff5fb103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0eae08138791278695ec17a2d2e089d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-faac1d219007a8119b0591bfe7d511de.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f473f53deeea858c036aa8a1ad4b3a97.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Falcon-H1 A Family of Hybrid-Head Language Models Redefining Efficiency   and Performance
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5116ffe95d313b44dd593feaa8eb7e93.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  MOSPA Human Motion Generation Driven by Spatial Audio
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
