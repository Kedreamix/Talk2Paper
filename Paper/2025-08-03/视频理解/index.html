<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-08-03  VideoMind An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9288ed48417716f6b40a6abd40c993b6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-03-更新"><a href="#2025-08-03-更新" class="headerlink" title="2025-08-03 更新"></a>2025-08-03 更新</h1><h2 id="VideoMind-An-Omni-Modal-Video-Dataset-with-Intent-Grounding-for-Deep-Cognitive-Video-Understanding"><a href="#VideoMind-An-Omni-Modal-Video-Dataset-with-Intent-Grounding-for-Deep-Cognitive-Video-Understanding" class="headerlink" title="VideoMind: An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding"></a>VideoMind: An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding</h2><p><strong>Authors:Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin</strong></p>
<p>This paper introduces VideoMind, a video-centric omni-modal dataset designed for deep video content cognition and enhanced multi-modal feature representation. The dataset comprises 103K video samples (3K reserved for testing), each paired with audio and systematically detailed textual descriptions. Specifically, every video and its audio is described across three hierarchical layers (factual, abstract, and intent), progressing from surface to depth. It contains over 22 million words, averaging ~225 words per sample. VideoMind’s key distinction from existing datasets is its provision of intent expressions, which require contextual integration across the entire video and are not directly observable. These deep-cognitive expressions are generated using a Chain-of-Thought (COT) approach, prompting the mLLM through step-by-step reasoning. Each description includes annotations for subject, place, time, event, action, and intent, supporting downstream recognition tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually validated samples for evaluating deep-cognitive video understanding. We design hybrid-cognitive retrieval experiments, scored by multi-level retrieval metrics, to appropriately assess deep video comprehension. Evaluation results for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a powerful benchmark for fine-grained cross-modal alignment and advances fields requiring in-depth video understanding, such as emotion and intent recognition. The data is publicly available on GitHub, HuggingFace, and OpenDataLab, <a target="_blank" rel="noopener" href="https://github.com/cdx-cindy/VideoMind">https://github.com/cdx-cindy/VideoMind</a>. </p>
<blockquote>
<p>本文介绍了VideoMind，这是一个以视频为中心的跨模态数据集，旨在用于深度视频内容认知和增强的多模态特征表示。该数据集包含103K个视频样本（其中3K用于测试），每个样本都配备了音频和系统的详细文本描述。具体来说，每个视频及其音频都在三个层次（事实、抽象和意图）上进行描述，从表面到深度逐步深入。它包含超过2200万个单词，每个样本平均约225个单词。VideoMind与现有数据集的主要区别在于它提供了意图表达，这需要整合整个视频的上下文，并且无法直接观察到。这些深度认知表达是通过“思维链”（COT）方法生成的，通过逐步推理来提示大型语言模型。每个描述都包括主题、地点、时间、事件、行为和意图的注释，支持下游识别任务。重要的是，我们建立了3000个手动验证样本的金标准基准，用于评估深度认知视频理解。我们设计了混合认知检索实验，通过多级检索指标进行评分，以适当评估深度视频理解。已发布了对模型（如InternVideo、VAST、UMT-L）的评估结果。VideoMind是细粒度跨模态对齐的强大基准，并推动了需要深度视频理解（如情感和意图识别）的领域的发展。数据已在GitHub、HuggingFace和OpenDataLab上公开可用，<a target="_blank" rel="noopener" href="https://github.com/cdx-cindy/VideoMind%E3%80%82">https://github.com/cdx-cindy/VideoMind。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18552v1">PDF</a> 7 pages; 14 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了VideoMind视频主导的多模态数据集，该数据集旨在用于深度视频内容认知和增强多模态特征表示。数据集包含103K视频样本（其中3K用于测试），每个样本都配有音频和详细的文本描述。每个视频及其音频的描述分为三个层次结构（事实、抽象和意图），从表面到深度逐步深入。VideoMind的关键区别在于它提供了意图表达，这需要整合整个视频中的上下文，不能直接观察到。通过Chain-of-Thought方法生成深度认知表达，为下游识别任务提供支持。数据集已在GitHub、HuggingFace和OpenDataLab上公开发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoMind是一个视频为中心的多模态数据集，旨在用于深度视频内容认知。</li>
<li>数据集包含超过10万个视频样本，每个样本都带有音频和详细的文本描述。</li>
<li>VideoMind引入了意图表达的概念，这需要整合整个视频的上下文信息。</li>
<li>数据集采用Chain-of-Thought方法生成深度认知表达。</li>
<li>VideoMind为下游识别任务提供支持，如情绪识别和意图识别。</li>
<li>VideoMind已建立了一个黄金标准基准，包括用于评估深度视频理解的3000个手动验证样本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18552">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ac9c8b4b44eb2400eb8bb4f328a4fcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-937fa04dff50736ce76be372932c5de1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e158110408225e78d6233799818e286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5a0d3c63c237dfeb89125bf5d5ba37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3308ee1aee4a0f3ee652d1a763a77ad.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EgoExoBench-A-Benchmark-for-First-and-Third-person-View-Video-Understanding-in-MLLMs"><a href="#EgoExoBench-A-Benchmark-for-First-and-Third-person-View-Video-Understanding-in-MLLMs" class="headerlink" title="EgoExoBench: A Benchmark for First- and Third-person View Video   Understanding in MLLMs"></a>EgoExoBench: A Benchmark for First- and Third-person View Video   Understanding in MLLMs</h2><p><strong>Authors:Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, Jiangmiao Pang</strong></p>
<p>Transferring and integrating knowledge across first-person (egocentric) and third-person (exocentric) viewpoints is intrinsic to human intelligence, enabling humans to learn from others and convey insights from their own experiences. Despite rapid progress in multimodal large language models (MLLMs), their ability to perform such cross-view reasoning remains unexplored. To address this, we introduce EgoExoBench, the first benchmark for egocentric-exocentric video understanding and reasoning. Built from publicly available datasets, EgoExoBench comprises over 7,300 question-answer pairs spanning eleven sub-tasks organized into three core challenges: semantic alignment, viewpoint association, and temporal reasoning. We evaluate 13 state-of-the-art MLLMs and find that while these models excel on single-view tasks, they struggle to align semantics across perspectives, accurately associate views, and infer temporal dynamics in the ego-exo context. We hope EgoExoBench can serve as a valuable resource for research on embodied agents and intelligent assistants seeking human-like cross-view intelligence. </p>
<blockquote>
<p>将第一人称（以自我为中心）和第三人称（以外部为中心）观点之间的知识转移和整合是人类智慧的核心部分，使人类能够学习他人的知识并传达自己的经验见解。尽管多模态大型语言模型（MLLMs）发展迅速，但它们执行这种跨视角推理的能力仍然未被探索。为了解决这一问题，我们推出了EgoExoBench，这是首个针对以自我为中心和以外部为中心的视频理解和推理的基准测试。EgoExoBench由公开可用的数据集构建而成，包含超过7300个问答对，涵盖11个子任务，分为三大挑战：语义对齐、视角关联和时序推理。我们评估了13种最先进的多模态大型语言模型，发现这些模型在单视角任务上表现优异，但在跨视角的语义对齐、准确关联视角以及在以自我为中心和以外部为中心的上下文中的时间动态推理方面遇到困难。我们希望EgoExoBench能成为研究智能主体和智能助手等寻求人类式跨视角智能的宝贵资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了人类智能中跨第一人称（以自我为中心）和第三人称（以外部为中心）观点的知识转移和整合的重要性。尽管多模态大型语言模型（MLLMs）发展迅速，但它们在这种跨视角推理方面的能力尚未被探索。为解决这一问题，本文引入了EgoExoBench，这是一个用于以自我为中心和以外部为中心的视频理解和推理的基准测试。EgoExoBench包含超过7300个问答对，涵盖11个子任务，分为三大核心挑战：语义对齐、视角关联和时序推理。对13种最新MLLMs的评估发现，这些模型在单视角任务上表现优异，但在跨视角语义对齐、准确关联视角以及在以自我为中心和以外部为中心的上下文中推断时间动态方面存在困难。期望EgoExoBench能成为研究具有人类智能特征的跨视角智能的宝贵资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类智能涉及跨第一人称和第三人称观点的知识转移和整合。</li>
<li>多模态大型语言模型（MLLMs）在跨视角推理方面的能力尚未得到充分探索。</li>
<li>EgoExoBench是首个针对以自我为中心和以外部为中心的视频理解和推理的基准测试。</li>
<li>EgoExoBench包含7300多个问答对，涵盖语义对齐、视角关联和时序推理三大核心挑战。</li>
<li>最新MLLMs在单视角任务上表现良好，但在跨视角任务方面存在困难。</li>
<li>MLLMs在语义对齐、准确关联视角以及推断时间动态方面需要改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18342">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-669f58b80dee84e4cf9d2c731622ad65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cfe59a2f4e2ae017b32b7a1db98964e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-094be5f32a9b56b850230383a892c5ea.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SV3-3B-A-Sports-Video-Understanding-Model-for-Action-Recognition"><a href="#SV3-3B-A-Sports-Video-Understanding-Model-for-Action-Recognition" class="headerlink" title="SV3.3B: A Sports Video Understanding Model for Action Recognition"></a>SV3.3B: A Sports Video Understanding Model for Action Recognition</h2><p><strong>Authors:Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam</strong></p>
<p>This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at <a target="_blank" rel="noopener" href="https://huggingface.co/sportsvision/SV3.3B">https://huggingface.co/sportsvision/SV3.3B</a>. </p>
<blockquote>
<p>本文旨在应对自动运动视频分析的挑战。传统的视频分析模型因计算量大、需要服务器端处理且对运动动作缺乏精细理解而受到限制。现有方法难以捕捉运动分析中必不可少的关键生物力学转变细节，这些转变通常会在几秒内发生，如准备阶段、执行阶段和后续阶段。为了克服这些局限性，我们推出了SV3.3B，这是一款轻量级的3.3B参数视频理解模型。它结合了新型时间运动差分采样和自监督学习，可实现高效的设备端部署。我们的方法采用基于DWT-VGG16-LDA的关键帧提取机制，智能地从运动序列中识别出最具有代表性的16个帧，然后通过掩码去噪目标的V-DWT-JEPA2编码器进行预处理，并使用微调的大型语言模型（LLM）解码器进行运动动作描述生成。在NSVA篮球数据集的一个子集上进行评估，SV3.3B在传统的文本生成指标和运动特定评估标准上均表现出卓越性能，优于包括GPT-4o变体在内的更大闭源模型，同时大大减少了计算需求。我们的模型在生成技术详细、分析丰富的运动描述方面表现出卓越能力，在真实验证指标上实现了对GPT-4o的29.2%的提升，在信息密度、动作复杂度和测量精度等关键指标上都有显著改进，对于全面的运动分析至关重要。模型可通过<a target="_blank" rel="noopener" href="https://huggingface.co/sportsvision/SV3.3B">https://huggingface.co/sportsvision/SV3.3B</a> 访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17844v1">PDF</a> 8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对自动运动视频分析的新方法SV3.3B。传统方法受限于计算密集型模型，需要在服务器端处理，且对运动员动作的精细理解不足。SV3.3B模型结合了新颖的时间运动差异采样与自我监督学习，实现了高效的设备端部署。通过DWT-VGG16-LDA的帧提取机制，智能识别运动序列中最具代表性的16帧。采用预训练的V-DWT-JEPA2编码器与LLM解码器进行微调，用于生成运动动作描述。在NSVA篮球数据集的一个子集上评估，SV3.3B模型在文本生成指标和运动专项评估标准上均表现出卓越性能，相较于GPT-4o等大型闭源模型有明显优势，同时计算需求更低。模型公开于huggingface.co&#x2F;sportsvision&#x2F;SV3.3B。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SV3.3B模型解决了自动运动视频分析中的挑战，包括计算密集型模型和缺乏精细动作理解的问题。</li>
<li>SV3.3B结合了时间运动差异采样和自我监督学习，实现高效设备部署。</li>
<li>采用DWT-VGG16-LDA的帧提取机制智能识别最具代表性的帧。</li>
<li>V-DWT-JEPA2编码器与LLM解码器用于生成详细且富有分析性的运动描述。</li>
<li>SV3.3B在NSVA篮球数据集上表现优越，相较于GPT-4o等大型模型有显著提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17844">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-849675fd8190ecd401b8821b76acc74b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e3c8da66a16fadcf210875c2e0c6d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9288ed48417716f6b40a6abd40c993b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a98190c1b0ff68fbb54db355e5a250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f015ba3d080d94a3c61e20d986ed1468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68042344fc28bf715e4c2c6f29a88588.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Controllable-Hybrid-Captioner-for-Improved-Long-form-Video-Understanding"><a href="#Controllable-Hybrid-Captioner-for-Improved-Long-form-Video-Understanding" class="headerlink" title="Controllable Hybrid Captioner for Improved Long-form Video Understanding"></a>Controllable Hybrid Captioner for Improved Long-form Video Understanding</h2><p><strong>Authors:Kuleen Sasse, Efsun Sarioglu Kayi, Arun Reddy</strong></p>
<p>Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video. </p>
<blockquote>
<p>视频数据，特别是长视频，极为密集且高维。基于文本的视频内容摘要提供了一种比原始视频更紧凑地表示查询相关内容的方式。此外，文本表示很容易被最新的大型语言模型（LLM）所接纳，这使得能够推理视频内容以回答复杂的自然语言查询。为解决此问题，我们依赖于由操作视频较短片段的视频字幕器逐步构建的基于文本的存储器，其中时空建模是计算可行的。我们探索了提高仅由短视频字幕构成的活动日志质量的方法。由于视频字幕往往侧重于人类行为，而问题可能与场景中的其他信息有关，我们寻求使用视觉语言模型（VLM）丰富存储器中的静态场景描述。我们的视频理解系统依赖于LaViLa视频字幕器结合LLM来回答有关视频的问题。我们首先探索了将视频划分为有意义的片段的不同方式，以使文本描述更准确地反映视频内容的结构。此外，我们利用LLaVA VLM将静态场景描述纳入字幕管道，从而得到更详细和完整的字幕日志，并扩大了可从文本记忆中提问的空间。最后，我们已经成功地对LaViLa视频字幕器进行了微调，以产生动作和场景字幕，与为这两个任务使用单独的字幕模型相比，这大大提高了字幕管道的效率。我们的模型——可控混合字幕器，可以根据视频中的场景变化等特殊输入令牌交替生成不同类型的字幕。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17047v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>视频数据，特别是长视频，极为密集且高维。文本对视频内容的摘要提供了一种比原始视频更紧凑的方式来表示查询相关内容。此外，文本表示很容易被当前最先进的自然语言大型模型（LLM）所摄取，能够推理视频内容以回答复杂的自然语言查询。为解决此问题，我们依靠视频描述者基于较短的视频片段逐步构建文本基础记忆来解决时空建模的计算可行性。我们探索了提高仅由短视频字幕构成的活动日志质量的方法。由于视频字幕往往侧重于人类行为，而问题可能涉及场景中的其他信息，我们寻求使用视觉语言模型（VLM）丰富记忆，加入静态场景描述。我们的视频理解系统依赖于LaViLa视频描述器与大型语言模型结合回答视频问题。我们探索了将视频划分为有意义片段的不同方式，以使文本描述更准确地反映视频内容结构。此外，我们借助LLaVA VLM将静态场景描述纳入描述管道，得到更详细和完整的字幕日志，扩大了可从文本记忆中回答问题空间。最后，我们已经成功微调了LaViLa视频描述器以产生动作和场景字幕，与使用两个任务的单独描述模型相比大大提高了描述管道的效率。我们的模型可控混合描述器可以根据检测到视频中的场景变化特殊输入令牌交替不同类型字幕。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频数据具有高密度和高维度特性，文本摘要为查询相关内容提供了更紧凑的表示方式。</li>
<li>大型语言模型（LLM）能够摄取文本表示并进行视频内容的推理，以回答复杂的自然语言查询。</li>
<li>通过将视频划分为有意义片段来构建文本基础记忆，以实现时空建模的计算可行性。</li>
<li>结合短视频字幕和静态场景描述提高活动日志质量。</li>
<li>使用视觉语言模型（VLM）加入静态场景描述丰富记忆内容。</li>
<li>LaViLa视频描述器与大型语言模型结合形成强大的视频理解系统。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17047">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-797dcf2264827b3e1f86f224bc5d2d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a4dc65f4c11423ce7c47e052c695e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fc426551145b8cf0777a42c9e13bc61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fa747d8aa62fb2a8dead0bddc8be2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9161ea8f77515579e23c737f29543ddb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-Your-Model-Separate-Yolks-with-a-Water-Bottle-Benchmarking-Physical-Commonsense-Understanding-in-Video-Generation-Models"><a href="#Can-Your-Model-Separate-Yolks-with-a-Water-Bottle-Benchmarking-Physical-Commonsense-Understanding-in-Video-Generation-Models" class="headerlink" title="Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical   Commonsense Understanding in Video Generation Models"></a>Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical   Commonsense Understanding in Video Generation Models</h2><p><strong>Authors:Enes Sanli, Baris Sarper Tezcan, Aykut Erdem, Erkut Erdem</strong></p>
<p>Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models. </p>
<blockquote>
<p>近期文本到视频（T2V）生成的进展已经能够合成具有视觉吸引力和时间连贯性的视频来自自然语言。然而，这些模型通常在基本物理常识方面表现不足，产生的输出违反了因果、物体行为和工具使用的直观预期。为了解决这个问题，我们提出了PhysVidBench，这是一个旨在评估T2V系统物理推理能力的基准测试。该基准测试包括383个精心策划的提示，强调工具使用、材料属性和程序交互，以及在物理可行性至关重要的领域。对于每个提示，我们使用各种最先进模型生成视频，并采用三阶段评估流程：（1）根据提示制定基于物理的问题，（2）使用视觉语言模型为生成的视频添加标题，（3）任务语言模型仅使用标题回答涉及物理的若干问题。这种间接策略避免了直接基于视频的评估中常见的幻觉问题。通过突出优势和工具介导的行动，以及当前T2V评估中被忽视的领域，PhysVidBench为评估生成视频模型中的物理常识提供了一个结构化、可解释的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15824v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本至视频生成（T2V）技术在生成具有视觉吸引力和时间连贯性的视频方面取得了进展，但在基本物理常识方面存在缺陷，产生的输出违反了因果、物体行为和工具使用等方面的直观预期。为解决这一问题，本文提出PhysVidBench基准测试，旨在评估T2V系统的物理推理能力。该基准测试包含383个精心策划的提示，强调工具使用、材料属性和程序性交互，以及在物理可信度至关重要的领域。采用三阶段评估流程对生成的视频进行评价，包括从提示中制定基于物理的问题、使用视觉语言模型为生成的视频添加字幕，以及仅使用字幕回答涉及物理的多个问题。这种间接策略避免了在直接视频评估中的常见幻觉问题。PhysVidBench通过强调便利性和工具介导的行动，提供了评估生成视频模型中的物理常识的结构化、可解释框架，弥补了当前T2V评估中被忽视的领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本至视频生成（T2V）技术在生成视觉上吸引人的视频方面取得进展，但缺乏基本物理常识。</li>
<li>PhysVidBench基准测试旨在评估T2V系统在物理推理方面的能力。</li>
<li>该基准测试包含383个精心策划的提示，涵盖工具使用、材料属性和程序性交互等领域。</li>
<li>采用三阶段评估流程，包括制定基于物理的问题、为生成视频添加字幕以及回答涉及物理的问题。</li>
<li>间接评估策略有助于避免直接视频评估中的幻觉问题。</li>
<li>PhysVidBench提供了一个结构化、可解释的框架，用于评估生成视频模型中的物理常识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15824">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-85c588b81ba14c26c3628f090c1dec19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cdaab6342d764fe2e0ceb8ca4646068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6885412997772a22eebbb92d253e7874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd284b617951012cb92fbcbe2ae701db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7272a12da6157b70d521836baa08449.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DynImg-Key-Frames-with-Visual-Prompts-are-Good-Representation-for-Multi-Modal-Video-Understanding"><a href="#DynImg-Key-Frames-with-Visual-Prompts-are-Good-Representation-for-Multi-Modal-Video-Understanding" class="headerlink" title="DynImg: Key Frames with Visual Prompts are Good Representation for   Multi-Modal Video Understanding"></a>DynImg: Key Frames with Visual Prompts are Good Representation for   Multi-Modal Video Understanding</h2><p><strong>Authors:Xiaoyi Bao, Chenwei Xie, Hao Tang, Tingyu Weng, Xiaofeng Wang, Yun Zheng, Xingang Wang</strong></p>
<p>In recent years, the introduction of Multi-modal Large Language Models (MLLMs) into video understanding tasks has become increasingly prevalent. However, how to effectively integrate temporal information remains a critical research focus. Traditional approaches treat spatial and temporal information separately. Due to issues like motion blur, it is challenging to accurately represent the spatial information of rapidly moving objects. This can lead to temporally important regions being underemphasized during spatial feature extraction, which in turn hinders accurate spatio-temporal interaction and video understanding. To address this limitation, we propose an innovative video representation method called Dynamic-Image (DynImg). Specifically, we introduce a set of non-key frames as temporal prompts to highlight the spatial areas containing fast-moving objects. During the process of visual feature extraction, these prompts guide the model to pay additional attention to the fine-grained spatial features corresponding to these regions. Moreover, to maintain the correct sequence for DynImg, we employ a corresponding 4D video Rotary Position Embedding. This retains both the temporal and spatial adjacency of DynImg, helping MLLM understand the spatio-temporal order within this combined format. Experimental evaluations reveal that DynImg surpasses the state-of-the-art methods by approximately 2% across multiple video understanding benchmarks, proving the effectiveness of our temporal prompts in enhancing video comprehension. </p>
<blockquote>
<p>近年来，多模态大型语言模型（MLLMs）在视频理解任务中的引入越来越普遍。然而，如何有效地整合时间信息仍然是关键的研究重点。传统的方法分别处理空间和时间信息。由于运动模糊等问题，准确表示快速移动物体的空间信息具有挑战性。这可能导致在提取空间特征时忽视了时间上重要的区域，从而妨碍准确的时空交互和视频理解。为了解决这一局限性，我们提出了一种创新的视频表示方法，称为动态图像（DynImg）。具体来说，我们引入了一系列非关键帧作为时间提示，以突出包含快速移动物体的空间区域。在提取视觉特征的过程中，这些提示引导模型额外关注这些区域对应的精细空间特征。此外，为了保持DynImg的正确序列，我们采用相应的4D视频旋转位置嵌入。这保留了DynImg的时间和空间邻接性，帮助MLLM理解这种组合格式内的时空顺序。实验评估表明，DynImg在多个视频理解基准测试上超越了最先进的方法，提高了大约2%，证明了我们的时间提示在提高视频理解能力方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15569v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>近年来，多模态大型语言模型（MLLMs）在视频理解任务中的应用越来越普遍。然而，如何有效地整合时间信息仍然是一个关键的研究重点。传统方法分别处理空间和时间信息，由于运动模糊等问题，准确表示快速移动物体的空间信息具有挑战性。为此，我们提出了一种创新的视频表示方法——动态图像（DynImg）。我们通过引入一系列非关键帧作为时间提示来突出包含快速移动物体的空间区域。这些提示在视觉特征提取过程中引导模型额外关注这些区域的精细空间特征。同时，为了保持DynImg的正确序列，我们采用了相应的4D视频旋转位置嵌入，这保留了DynImg的时间和空间邻接性，帮助MLLM理解这种组合格式中的时空顺序。实验评估表明，DynImg在多个视频理解基准测试中比最新技术领先约2%，证明了我们的时间提示在提高视频理解能力方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在视频理解中的应用越来越重要。</li>
<li>整合时间信息是视频理解的关键，传统方法处理空间和时间信息存在挑战。</li>
<li>引入非关键帧作为时间提示可以突出快速移动物体的空间区域。</li>
<li>动态图像（DynImg）方法通过引导模型关注精细空间特征来提高视频理解。</li>
<li>4D视频旋转位置嵌入技术用于保持DynImg的正确序列和时空邻接性。</li>
<li>DynImg在多个视频理解基准测试中表现优异，证明其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15569">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-564b240b0b6447243280f4dd4bc2b74e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68bb5393712ed4c6d3c8410e6b1ad697.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-718b609808a77d5fbf4d95c21920c3d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6925f60083e27b591f09fde954f154b4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8577c124f2f2b957f9eb7ed8c1121359.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-03  UPRE Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a63c44975d3d4faa55d2ab02950c7f5.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-03  Efficient Masked Attention Transformer for Few-Shot Classification and   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
