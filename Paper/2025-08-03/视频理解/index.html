<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  VideoMind An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-9288ed48417716f6b40a6abd40c993b6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-03
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    27 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-03-æ›´æ–°"><a href="#2025-08-03-æ›´æ–°" class="headerlink" title="2025-08-03 æ›´æ–°"></a>2025-08-03 æ›´æ–°</h1><h2 id="VideoMind-An-Omni-Modal-Video-Dataset-with-Intent-Grounding-for-Deep-Cognitive-Video-Understanding"><a href="#VideoMind-An-Omni-Modal-Video-Dataset-with-Intent-Grounding-for-Deep-Cognitive-Video-Understanding" class="headerlink" title="VideoMind: An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding"></a>VideoMind: An Omni-Modal Video Dataset with Intent Grounding for   Deep-Cognitive Video Understanding</h2><p><strong>Authors:Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin</strong></p>
<p>This paper introduces VideoMind, a video-centric omni-modal dataset designed for deep video content cognition and enhanced multi-modal feature representation. The dataset comprises 103K video samples (3K reserved for testing), each paired with audio and systematically detailed textual descriptions. Specifically, every video and its audio is described across three hierarchical layers (factual, abstract, and intent), progressing from surface to depth. It contains over 22 million words, averaging ~225 words per sample. VideoMindâ€™s key distinction from existing datasets is its provision of intent expressions, which require contextual integration across the entire video and are not directly observable. These deep-cognitive expressions are generated using a Chain-of-Thought (COT) approach, prompting the mLLM through step-by-step reasoning. Each description includes annotations for subject, place, time, event, action, and intent, supporting downstream recognition tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually validated samples for evaluating deep-cognitive video understanding. We design hybrid-cognitive retrieval experiments, scored by multi-level retrieval metrics, to appropriately assess deep video comprehension. Evaluation results for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a powerful benchmark for fine-grained cross-modal alignment and advances fields requiring in-depth video understanding, such as emotion and intent recognition. The data is publicly available on GitHub, HuggingFace, and OpenDataLab, <a target="_blank" rel="noopener" href="https://github.com/cdx-cindy/VideoMind">https://github.com/cdx-cindy/VideoMind</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†VideoMindï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†é¢‘ä¸ºä¸­å¿ƒçš„è·¨æ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨ç”¨äºæ·±åº¦è§†é¢‘å†…å®¹è®¤çŸ¥å’Œå¢å¼ºçš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ•°æ®é›†åŒ…å«103Kä¸ªè§†é¢‘æ ·æœ¬ï¼ˆå…¶ä¸­3Kç”¨äºæµ‹è¯•ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½é…å¤‡äº†éŸ³é¢‘å’Œç³»ç»Ÿçš„è¯¦ç»†æ–‡æœ¬æè¿°ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªè§†é¢‘åŠå…¶éŸ³é¢‘éƒ½åœ¨ä¸‰ä¸ªå±‚æ¬¡ï¼ˆäº‹å®ã€æŠ½è±¡å’Œæ„å›¾ï¼‰ä¸Šè¿›è¡Œæè¿°ï¼Œä»è¡¨é¢åˆ°æ·±åº¦é€æ­¥æ·±å…¥ã€‚å®ƒåŒ…å«è¶…è¿‡2200ä¸‡ä¸ªå•è¯ï¼Œæ¯ä¸ªæ ·æœ¬å¹³å‡çº¦225ä¸ªå•è¯ã€‚VideoMindä¸ç°æœ‰æ•°æ®é›†çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒæä¾›äº†æ„å›¾è¡¨è¾¾ï¼Œè¿™éœ€è¦æ•´åˆæ•´ä¸ªè§†é¢‘çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¸”æ— æ³•ç›´æ¥è§‚å¯Ÿåˆ°ã€‚è¿™äº›æ·±åº¦è®¤çŸ¥è¡¨è¾¾æ˜¯é€šè¿‡â€œæ€ç»´é“¾â€ï¼ˆCOTï¼‰æ–¹æ³•ç”Ÿæˆçš„ï¼Œé€šè¿‡é€æ­¥æ¨ç†æ¥æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ¯ä¸ªæè¿°éƒ½åŒ…æ‹¬ä¸»é¢˜ã€åœ°ç‚¹ã€æ—¶é—´ã€äº‹ä»¶ã€è¡Œä¸ºå’Œæ„å›¾çš„æ³¨é‡Šï¼Œæ”¯æŒä¸‹æ¸¸è¯†åˆ«ä»»åŠ¡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å»ºç«‹äº†3000ä¸ªæ‰‹åŠ¨éªŒè¯æ ·æœ¬çš„é‡‘æ ‡å‡†åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ·±åº¦è®¤çŸ¥è§†é¢‘ç†è§£ã€‚æˆ‘ä»¬è®¾è®¡äº†æ··åˆè®¤çŸ¥æ£€ç´¢å®éªŒï¼Œé€šè¿‡å¤šçº§æ£€ç´¢æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼Œä»¥é€‚å½“è¯„ä¼°æ·±åº¦è§†é¢‘ç†è§£ã€‚å·²å‘å¸ƒäº†å¯¹æ¨¡å‹ï¼ˆå¦‚InternVideoã€VASTã€UMT-Lï¼‰çš„è¯„ä¼°ç»“æœã€‚VideoMindæ˜¯ç»†ç²’åº¦è·¨æ¨¡æ€å¯¹é½çš„å¼ºå¤§åŸºå‡†ï¼Œå¹¶æ¨åŠ¨äº†éœ€è¦æ·±åº¦è§†é¢‘ç†è§£ï¼ˆå¦‚æƒ…æ„Ÿå’Œæ„å›¾è¯†åˆ«ï¼‰çš„é¢†åŸŸçš„å‘å±•ã€‚æ•°æ®å·²åœ¨GitHubã€HuggingFaceå’ŒOpenDataLabä¸Šå…¬å¼€å¯ç”¨ï¼Œ<a target="_blank" rel="noopener" href="https://github.com/cdx-cindy/VideoMind%E3%80%82">https://github.com/cdx-cindy/VideoMindã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18552v1">PDF</a> 7 pages; 14 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VideoMindè§†é¢‘ä¸»å¯¼çš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨ç”¨äºæ·±åº¦è§†é¢‘å†…å®¹è®¤çŸ¥å’Œå¢å¼ºå¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚æ•°æ®é›†åŒ…å«103Kè§†é¢‘æ ·æœ¬ï¼ˆå…¶ä¸­3Kç”¨äºæµ‹è¯•ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½é…æœ‰éŸ³é¢‘å’Œè¯¦ç»†çš„æ–‡æœ¬æè¿°ã€‚æ¯ä¸ªè§†é¢‘åŠå…¶éŸ³é¢‘çš„æè¿°åˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡ç»“æ„ï¼ˆäº‹å®ã€æŠ½è±¡å’Œæ„å›¾ï¼‰ï¼Œä»è¡¨é¢åˆ°æ·±åº¦é€æ­¥æ·±å…¥ã€‚VideoMindçš„å…³é”®åŒºåˆ«åœ¨äºå®ƒæä¾›äº†æ„å›¾è¡¨è¾¾ï¼Œè¿™éœ€è¦æ•´åˆæ•´ä¸ªè§†é¢‘ä¸­çš„ä¸Šä¸‹æ–‡ï¼Œä¸èƒ½ç›´æ¥è§‚å¯Ÿåˆ°ã€‚é€šè¿‡Chain-of-Thoughtæ–¹æ³•ç”Ÿæˆæ·±åº¦è®¤çŸ¥è¡¨è¾¾ï¼Œä¸ºä¸‹æ¸¸è¯†åˆ«ä»»åŠ¡æä¾›æ”¯æŒã€‚æ•°æ®é›†å·²åœ¨GitHubã€HuggingFaceå’ŒOpenDataLabä¸Šå…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoMindæ˜¯ä¸€ä¸ªè§†é¢‘ä¸ºä¸­å¿ƒçš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ—¨åœ¨ç”¨äºæ·±åº¦è§†é¢‘å†…å®¹è®¤çŸ¥ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡10ä¸‡ä¸ªè§†é¢‘æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½å¸¦æœ‰éŸ³é¢‘å’Œè¯¦ç»†çš„æ–‡æœ¬æè¿°ã€‚</li>
<li>VideoMindå¼•å…¥äº†æ„å›¾è¡¨è¾¾çš„æ¦‚å¿µï¼Œè¿™éœ€è¦æ•´åˆæ•´ä¸ªè§†é¢‘çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æ•°æ®é›†é‡‡ç”¨Chain-of-Thoughtæ–¹æ³•ç”Ÿæˆæ·±åº¦è®¤çŸ¥è¡¨è¾¾ã€‚</li>
<li>VideoMindä¸ºä¸‹æ¸¸è¯†åˆ«ä»»åŠ¡æä¾›æ”¯æŒï¼Œå¦‚æƒ…ç»ªè¯†åˆ«å’Œæ„å›¾è¯†åˆ«ã€‚</li>
<li>VideoMindå·²å»ºç«‹äº†ä¸€ä¸ªé»„é‡‘æ ‡å‡†åŸºå‡†ï¼ŒåŒ…æ‹¬ç”¨äºè¯„ä¼°æ·±åº¦è§†é¢‘ç†è§£çš„3000ä¸ªæ‰‹åŠ¨éªŒè¯æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ac9c8b4b44eb2400eb8bb4f328a4fcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-937fa04dff50736ce76be372932c5de1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e158110408225e78d6233799818e286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5a0d3c63c237dfeb89125bf5d5ba37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3308ee1aee4a0f3ee652d1a763a77ad.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="EgoExoBench-A-Benchmark-for-First-and-Third-person-View-Video-Understanding-in-MLLMs"><a href="#EgoExoBench-A-Benchmark-for-First-and-Third-person-View-Video-Understanding-in-MLLMs" class="headerlink" title="EgoExoBench: A Benchmark for First- and Third-person View Video   Understanding in MLLMs"></a>EgoExoBench: A Benchmark for First- and Third-person View Video   Understanding in MLLMs</h2><p><strong>Authors:Yuping He, Yifei Huang, Guo Chen, Baoqi Pei, Jilan Xu, Tong Lu, Jiangmiao Pang</strong></p>
<p>Transferring and integrating knowledge across first-person (egocentric) and third-person (exocentric) viewpoints is intrinsic to human intelligence, enabling humans to learn from others and convey insights from their own experiences. Despite rapid progress in multimodal large language models (MLLMs), their ability to perform such cross-view reasoning remains unexplored. To address this, we introduce EgoExoBench, the first benchmark for egocentric-exocentric video understanding and reasoning. Built from publicly available datasets, EgoExoBench comprises over 7,300 question-answer pairs spanning eleven sub-tasks organized into three core challenges: semantic alignment, viewpoint association, and temporal reasoning. We evaluate 13 state-of-the-art MLLMs and find that while these models excel on single-view tasks, they struggle to align semantics across perspectives, accurately associate views, and infer temporal dynamics in the ego-exo context. We hope EgoExoBench can serve as a valuable resource for research on embodied agents and intelligent assistants seeking human-like cross-view intelligence. </p>
<blockquote>
<p>å°†ç¬¬ä¸€äººç§°ï¼ˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼‰å’Œç¬¬ä¸‰äººç§°ï¼ˆä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒï¼‰è§‚ç‚¹ä¹‹é—´çš„çŸ¥è¯†è½¬ç§»å’Œæ•´åˆæ˜¯äººç±»æ™ºæ…§çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œä½¿äººç±»èƒ½å¤Ÿå­¦ä¹ ä»–äººçš„çŸ¥è¯†å¹¶ä¼ è¾¾è‡ªå·±çš„ç»éªŒè§è§£ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•è¿…é€Ÿï¼Œä½†å®ƒä»¬æ‰§è¡Œè¿™ç§è·¨è§†è§’æ¨ç†çš„èƒ½åŠ›ä»ç„¶æœªè¢«æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoExoBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£å’Œæ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚EgoExoBenchç”±å…¬å¼€å¯ç”¨çš„æ•°æ®é›†æ„å»ºè€Œæˆï¼ŒåŒ…å«è¶…è¿‡7300ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–11ä¸ªå­ä»»åŠ¡ï¼Œåˆ†ä¸ºä¸‰å¤§æŒ‘æˆ˜ï¼šè¯­ä¹‰å¯¹é½ã€è§†è§’å…³è”å’Œæ—¶åºæ¨ç†ã€‚æˆ‘ä»¬è¯„ä¼°äº†13ç§æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨å•è§†è§’ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è·¨è§†è§’çš„è¯­ä¹‰å¯¹é½ã€å‡†ç¡®å…³è”è§†è§’ä»¥åŠåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒçš„ä¸Šä¸‹æ–‡ä¸­çš„æ—¶é—´åŠ¨æ€æ¨ç†æ–¹é¢é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å¸Œæœ›EgoExoBenchèƒ½æˆä¸ºç ”ç©¶æ™ºèƒ½ä¸»ä½“å’Œæ™ºèƒ½åŠ©æ‰‹ç­‰å¯»æ±‚äººç±»å¼è·¨è§†è§’æ™ºèƒ½çš„å®è´µèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†äººç±»æ™ºèƒ½ä¸­è·¨ç¬¬ä¸€äººç§°ï¼ˆä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒï¼‰å’Œç¬¬ä¸‰äººç§°ï¼ˆä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒï¼‰è§‚ç‚¹çš„çŸ¥è¯†è½¬ç§»å’Œæ•´åˆçš„é‡è¦æ€§ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å‘å±•è¿…é€Ÿï¼Œä½†å®ƒä»¬åœ¨è¿™ç§è·¨è§†è§’æ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†EgoExoBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£å’Œæ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚EgoExoBenchåŒ…å«è¶…è¿‡7300ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–11ä¸ªå­ä»»åŠ¡ï¼Œåˆ†ä¸ºä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šè¯­ä¹‰å¯¹é½ã€è§†è§’å…³è”å’Œæ—¶åºæ¨ç†ã€‚å¯¹13ç§æœ€æ–°MLLMsçš„è¯„ä¼°å‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨å•è§†è§’ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è·¨è§†è§’è¯­ä¹‰å¯¹é½ã€å‡†ç¡®å…³è”è§†è§’ä»¥åŠåœ¨ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒçš„ä¸Šä¸‹æ–‡ä¸­æ¨æ–­æ—¶é—´åŠ¨æ€æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœŸæœ›EgoExoBenchèƒ½æˆä¸ºç ”ç©¶å…·æœ‰äººç±»æ™ºèƒ½ç‰¹å¾çš„è·¨è§†è§’æ™ºèƒ½çš„å®è´µèµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»æ™ºèƒ½æ¶‰åŠè·¨ç¬¬ä¸€äººç§°å’Œç¬¬ä¸‰äººç§°è§‚ç‚¹çš„çŸ¥è¯†è½¬ç§»å’Œæ•´åˆã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·¨è§†è§’æ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>EgoExoBenchæ˜¯é¦–ä¸ªé’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒå’Œä»¥å¤–éƒ¨ä¸ºä¸­å¿ƒçš„è§†é¢‘ç†è§£å’Œæ¨ç†çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>EgoExoBenchåŒ…å«7300å¤šä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›–è¯­ä¹‰å¯¹é½ã€è§†è§’å…³è”å’Œæ—¶åºæ¨ç†ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>æœ€æ–°MLLMsåœ¨å•è§†è§’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è·¨è§†è§’ä»»åŠ¡æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>MLLMsåœ¨è¯­ä¹‰å¯¹é½ã€å‡†ç¡®å…³è”è§†è§’ä»¥åŠæ¨æ–­æ—¶é—´åŠ¨æ€æ–¹é¢éœ€è¦æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-669f58b80dee84e4cf9d2c731622ad65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cfe59a2f4e2ae017b32b7a1db98964e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-094be5f32a9b56b850230383a892c5ea.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SV3-3B-A-Sports-Video-Understanding-Model-for-Action-Recognition"><a href="#SV3-3B-A-Sports-Video-Understanding-Model-for-Action-Recognition" class="headerlink" title="SV3.3B: A Sports Video Understanding Model for Action Recognition"></a>SV3.3B: A Sports Video Understanding Model for Action Recognition</h2><p><strong>Authors:Sai Varun Kodathala, Yashwanth Reddy Vutukoori, Rakesh Vunnam</strong></p>
<p>This paper addresses the challenge of automated sports video analysis, which has traditionally been limited by computationally intensive models requiring server-side processing and lacking fine-grained understanding of athletic movements. Current approaches struggle to capture the nuanced biomechanical transitions essential for meaningful sports analysis, often missing critical phases like preparation, execution, and follow-through that occur within seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B parameter video understanding model that combines novel temporal motion difference sampling with self-supervised learning for efficient on-device deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction mechanism that intelligently identifies the 16 most representative frames from sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through mask-denoising objectives and an LLM decoder fine-tuned for sports action description generation. Evaluated on a subset of the NSVA basketball dataset, SV3.3B achieves superior performance across both traditional text generation metrics and sports-specific evaluation criteria, outperforming larger closed-source models including GPT-4o variants while maintaining significantly lower computational requirements. Our model demonstrates exceptional capability in generating technically detailed and analytically rich sports descriptions, achieving 29.2% improvement over GPT-4o in ground truth validation metrics, with substantial improvements in information density, action complexity, and measurement precision metrics essential for comprehensive athletic analysis. Model Available at <a target="_blank" rel="noopener" href="https://huggingface.co/sportsvision/SV3.3B">https://huggingface.co/sportsvision/SV3.3B</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨åº”å¯¹è‡ªåŠ¨è¿åŠ¨è§†é¢‘åˆ†æçš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è§†é¢‘åˆ†ææ¨¡å‹å› è®¡ç®—é‡å¤§ã€éœ€è¦æœåŠ¡å™¨ç«¯å¤„ç†ä¸”å¯¹è¿åŠ¨åŠ¨ä½œç¼ºä¹ç²¾ç»†ç†è§£è€Œå—åˆ°é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰è¿åŠ¨åˆ†æä¸­å¿…ä¸å¯å°‘çš„å…³é”®ç”Ÿç‰©åŠ›å­¦è½¬å˜ç»†èŠ‚ï¼Œè¿™äº›è½¬å˜é€šå¸¸ä¼šåœ¨å‡ ç§’å†…å‘ç”Ÿï¼Œå¦‚å‡†å¤‡é˜¶æ®µã€æ‰§è¡Œé˜¶æ®µå’Œåç»­é˜¶æ®µã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SV3.3Bï¼Œè¿™æ˜¯ä¸€æ¬¾è½»é‡çº§çš„3.3Bå‚æ•°è§†é¢‘ç†è§£æ¨¡å‹ã€‚å®ƒç»“åˆäº†æ–°å‹æ—¶é—´è¿åŠ¨å·®åˆ†é‡‡æ ·å’Œè‡ªç›‘ç£å­¦ä¹ ï¼Œå¯å®ç°é«˜æ•ˆçš„è®¾å¤‡ç«¯éƒ¨ç½²ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŸºäºDWT-VGG16-LDAçš„å…³é”®å¸§æå–æœºåˆ¶ï¼Œæ™ºèƒ½åœ°ä»è¿åŠ¨åºåˆ—ä¸­è¯†åˆ«å‡ºæœ€å…·æœ‰ä»£è¡¨æ€§çš„16ä¸ªå¸§ï¼Œç„¶åé€šè¿‡æ©ç å»å™ªç›®æ ‡çš„V-DWT-JEPA2ç¼–ç å™¨è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä½¿ç”¨å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£ç å™¨è¿›è¡Œè¿åŠ¨åŠ¨ä½œæè¿°ç”Ÿæˆã€‚åœ¨NSVAç¯®çƒæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒSV3.3Båœ¨ä¼ ç»Ÿçš„æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡å’Œè¿åŠ¨ç‰¹å®šè¯„ä¼°æ ‡å‡†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºåŒ…æ‹¬GPT-4oå˜ä½“åœ¨å†…çš„æ›´å¤§é—­æºæ¨¡å‹ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘äº†è®¡ç®—éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç”ŸæˆæŠ€æœ¯è¯¦ç»†ã€åˆ†æä¸°å¯Œçš„è¿åŠ¨æè¿°æ–¹é¢è¡¨ç°å‡ºå“è¶Šèƒ½åŠ›ï¼Œåœ¨çœŸå®éªŒè¯æŒ‡æ ‡ä¸Šå®ç°äº†å¯¹GPT-4oçš„29.2%çš„æå‡ï¼Œåœ¨ä¿¡æ¯å¯†åº¦ã€åŠ¨ä½œå¤æ‚åº¦å’Œæµ‹é‡ç²¾åº¦ç­‰å…³é”®æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå¯¹äºå…¨é¢çš„è¿åŠ¨åˆ†æè‡³å…³é‡è¦ã€‚æ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://huggingface.co/sportsvision/SV3.3B">https://huggingface.co/sportsvision/SV3.3B</a> è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17844v1">PDF</a> 8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è‡ªåŠ¨è¿åŠ¨è§†é¢‘åˆ†æçš„æ–°æ–¹æ³•SV3.3Bã€‚ä¼ ç»Ÿæ–¹æ³•å—é™äºè®¡ç®—å¯†é›†å‹æ¨¡å‹ï¼Œéœ€è¦åœ¨æœåŠ¡å™¨ç«¯å¤„ç†ï¼Œä¸”å¯¹è¿åŠ¨å‘˜åŠ¨ä½œçš„ç²¾ç»†ç†è§£ä¸è¶³ã€‚SV3.3Bæ¨¡å‹ç»“åˆäº†æ–°é¢–çš„æ—¶é—´è¿åŠ¨å·®å¼‚é‡‡æ ·ä¸è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œå®ç°äº†é«˜æ•ˆçš„è®¾å¤‡ç«¯éƒ¨ç½²ã€‚é€šè¿‡DWT-VGG16-LDAçš„å¸§æå–æœºåˆ¶ï¼Œæ™ºèƒ½è¯†åˆ«è¿åŠ¨åºåˆ—ä¸­æœ€å…·ä»£è¡¨æ€§çš„16å¸§ã€‚é‡‡ç”¨é¢„è®­ç»ƒçš„V-DWT-JEPA2ç¼–ç å™¨ä¸LLMè§£ç å™¨è¿›è¡Œå¾®è°ƒï¼Œç”¨äºç”Ÿæˆè¿åŠ¨åŠ¨ä½œæè¿°ã€‚åœ¨NSVAç¯®çƒæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ä¸Šè¯„ä¼°ï¼ŒSV3.3Bæ¨¡å‹åœ¨æ–‡æœ¬ç”ŸæˆæŒ‡æ ‡å’Œè¿åŠ¨ä¸“é¡¹è¯„ä¼°æ ‡å‡†ä¸Šå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºGPT-4oç­‰å¤§å‹é—­æºæ¨¡å‹æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ŒåŒæ—¶è®¡ç®—éœ€æ±‚æ›´ä½ã€‚æ¨¡å‹å…¬å¼€äºhuggingface.co&#x2F;sportsvision&#x2F;SV3.3Bã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SV3.3Bæ¨¡å‹è§£å†³äº†è‡ªåŠ¨è¿åŠ¨è§†é¢‘åˆ†æä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¡ç®—å¯†é›†å‹æ¨¡å‹å’Œç¼ºä¹ç²¾ç»†åŠ¨ä½œç†è§£çš„é—®é¢˜ã€‚</li>
<li>SV3.3Bç»“åˆäº†æ—¶é—´è¿åŠ¨å·®å¼‚é‡‡æ ·å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼Œå®ç°é«˜æ•ˆè®¾å¤‡éƒ¨ç½²ã€‚</li>
<li>é‡‡ç”¨DWT-VGG16-LDAçš„å¸§æå–æœºåˆ¶æ™ºèƒ½è¯†åˆ«æœ€å…·ä»£è¡¨æ€§çš„å¸§ã€‚</li>
<li>V-DWT-JEPA2ç¼–ç å™¨ä¸LLMè§£ç å™¨ç”¨äºç”Ÿæˆè¯¦ç»†ä¸”å¯Œæœ‰åˆ†ææ€§çš„è¿åŠ¨æè¿°ã€‚</li>
<li>SV3.3Båœ¨NSVAç¯®çƒæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºGPT-4oç­‰å¤§å‹æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-849675fd8190ecd401b8821b76acc74b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18e3c8da66a16fadcf210875c2e0c6d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9288ed48417716f6b40a6abd40c993b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a98190c1b0ff68fbb54db355e5a250.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f015ba3d080d94a3c61e20d986ed1468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68042344fc28bf715e4c2c6f29a88588.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Controllable-Hybrid-Captioner-for-Improved-Long-form-Video-Understanding"><a href="#Controllable-Hybrid-Captioner-for-Improved-Long-form-Video-Understanding" class="headerlink" title="Controllable Hybrid Captioner for Improved Long-form Video Understanding"></a>Controllable Hybrid Captioner for Improved Long-form Video Understanding</h2><p><strong>Authors:Kuleen Sasse, Efsun Sarioglu Kayi, Arun Reddy</strong></p>
<p>Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video. </p>
<blockquote>
<p>è§†é¢‘æ•°æ®ï¼Œç‰¹åˆ«æ˜¯é•¿è§†é¢‘ï¼Œæä¸ºå¯†é›†ä¸”é«˜ç»´ã€‚åŸºäºæ–‡æœ¬çš„è§†é¢‘å†…å®¹æ‘˜è¦æä¾›äº†ä¸€ç§æ¯”åŸå§‹è§†é¢‘æ›´ç´§å‡‘åœ°è¡¨ç¤ºæŸ¥è¯¢ç›¸å…³å†…å®¹çš„æ–¹å¼ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬è¡¨ç¤ºå¾ˆå®¹æ˜“è¢«æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰€æ¥çº³ï¼Œè¿™ä½¿å¾—èƒ½å¤Ÿæ¨ç†è§†é¢‘å†…å®¹ä»¥å›ç­”å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬ä¾èµ–äºç”±æ“ä½œè§†é¢‘è¾ƒçŸ­ç‰‡æ®µçš„è§†é¢‘å­—å¹•å™¨é€æ­¥æ„å»ºçš„åŸºäºæ–‡æœ¬çš„å­˜å‚¨å™¨ï¼Œå…¶ä¸­æ—¶ç©ºå»ºæ¨¡æ˜¯è®¡ç®—å¯è¡Œçš„ã€‚æˆ‘ä»¬æ¢ç´¢äº†æé«˜ä»…ç”±çŸ­è§†é¢‘å­—å¹•æ„æˆçš„æ´»åŠ¨æ—¥å¿—è´¨é‡çš„æ–¹æ³•ã€‚ç”±äºè§†é¢‘å­—å¹•å¾€å¾€ä¾§é‡äºäººç±»è¡Œä¸ºï¼Œè€Œé—®é¢˜å¯èƒ½ä¸åœºæ™¯ä¸­çš„å…¶ä»–ä¿¡æ¯æœ‰å…³ï¼Œæˆ‘ä»¬å¯»æ±‚ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸°å¯Œå­˜å‚¨å™¨ä¸­çš„é™æ€åœºæ™¯æè¿°ã€‚æˆ‘ä»¬çš„è§†é¢‘ç†è§£ç³»ç»Ÿä¾èµ–äºLaViLaè§†é¢‘å­—å¹•å™¨ç»“åˆLLMæ¥å›ç­”æœ‰å…³è§†é¢‘çš„é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†å°†è§†é¢‘åˆ’åˆ†ä¸ºæœ‰æ„ä¹‰çš„ç‰‡æ®µçš„ä¸åŒæ–¹å¼ï¼Œä»¥ä½¿æ–‡æœ¬æè¿°æ›´å‡†ç¡®åœ°åæ˜ è§†é¢‘å†…å®¹çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨LLaVA VLMå°†é™æ€åœºæ™¯æè¿°çº³å…¥å­—å¹•ç®¡é“ï¼Œä»è€Œå¾—åˆ°æ›´è¯¦ç»†å’Œå®Œæ•´çš„å­—å¹•æ—¥å¿—ï¼Œå¹¶æ‰©å¤§äº†å¯ä»æ–‡æœ¬è®°å¿†ä¸­æé—®çš„ç©ºé—´ã€‚æœ€åï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸåœ°å¯¹LaViLaè§†é¢‘å­—å¹•å™¨è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥äº§ç”ŸåŠ¨ä½œå’Œåœºæ™¯å­—å¹•ï¼Œä¸ä¸ºè¿™ä¸¤ä¸ªä»»åŠ¡ä½¿ç”¨å•ç‹¬çš„å­—å¹•æ¨¡å‹ç›¸æ¯”ï¼Œè¿™å¤§å¤§æé«˜äº†å­—å¹•ç®¡é“çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å¯æ§æ··åˆå­—å¹•å™¨ï¼Œå¯ä»¥æ ¹æ®è§†é¢‘ä¸­çš„åœºæ™¯å˜åŒ–ç­‰ç‰¹æ®Šè¾“å…¥ä»¤ç‰Œäº¤æ›¿ç”Ÿæˆä¸åŒç±»å‹çš„å­—å¹•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17047v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘æ•°æ®ï¼Œç‰¹åˆ«æ˜¯é•¿è§†é¢‘ï¼Œæä¸ºå¯†é›†ä¸”é«˜ç»´ã€‚æ–‡æœ¬å¯¹è§†é¢‘å†…å®¹çš„æ‘˜è¦æä¾›äº†ä¸€ç§æ¯”åŸå§‹è§†é¢‘æ›´ç´§å‡‘çš„æ–¹å¼æ¥è¡¨ç¤ºæŸ¥è¯¢ç›¸å…³å†…å®¹ã€‚æ­¤å¤–ï¼Œæ–‡æœ¬è¡¨ç¤ºå¾ˆå®¹æ˜“è¢«å½“å‰æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆLLMï¼‰æ‰€æ‘„å–ï¼Œèƒ½å¤Ÿæ¨ç†è§†é¢‘å†…å®¹ä»¥å›ç­”å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬ä¾é è§†é¢‘æè¿°è€…åŸºäºè¾ƒçŸ­çš„è§†é¢‘ç‰‡æ®µé€æ­¥æ„å»ºæ–‡æœ¬åŸºç¡€è®°å¿†æ¥è§£å†³æ—¶ç©ºå»ºæ¨¡çš„è®¡ç®—å¯è¡Œæ€§ã€‚æˆ‘ä»¬æ¢ç´¢äº†æé«˜ä»…ç”±çŸ­è§†é¢‘å­—å¹•æ„æˆçš„æ´»åŠ¨æ—¥å¿—è´¨é‡çš„æ–¹æ³•ã€‚ç”±äºè§†é¢‘å­—å¹•å¾€å¾€ä¾§é‡äºäººç±»è¡Œä¸ºï¼Œè€Œé—®é¢˜å¯èƒ½æ¶‰åŠåœºæ™¯ä¸­çš„å…¶ä»–ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯»æ±‚ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸°å¯Œè®°å¿†ï¼ŒåŠ å…¥é™æ€åœºæ™¯æè¿°ã€‚æˆ‘ä»¬çš„è§†é¢‘ç†è§£ç³»ç»Ÿä¾èµ–äºLaViLaè§†é¢‘æè¿°å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆå›ç­”è§†é¢‘é—®é¢˜ã€‚æˆ‘ä»¬æ¢ç´¢äº†å°†è§†é¢‘åˆ’åˆ†ä¸ºæœ‰æ„ä¹‰ç‰‡æ®µçš„ä¸åŒæ–¹å¼ï¼Œä»¥ä½¿æ–‡æœ¬æè¿°æ›´å‡†ç¡®åœ°åæ˜ è§†é¢‘å†…å®¹ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å€ŸåŠ©LLaVA VLMå°†é™æ€åœºæ™¯æè¿°çº³å…¥æè¿°ç®¡é“ï¼Œå¾—åˆ°æ›´è¯¦ç»†å’Œå®Œæ•´çš„å­—å¹•æ—¥å¿—ï¼Œæ‰©å¤§äº†å¯ä»æ–‡æœ¬è®°å¿†ä¸­å›ç­”é—®é¢˜ç©ºé—´ã€‚æœ€åï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸå¾®è°ƒäº†LaViLaè§†é¢‘æè¿°å™¨ä»¥äº§ç”ŸåŠ¨ä½œå’Œåœºæ™¯å­—å¹•ï¼Œä¸ä½¿ç”¨ä¸¤ä¸ªä»»åŠ¡çš„å•ç‹¬æè¿°æ¨¡å‹ç›¸æ¯”å¤§å¤§æé«˜äº†æè¿°ç®¡é“çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯æ§æ··åˆæè¿°å™¨å¯ä»¥æ ¹æ®æ£€æµ‹åˆ°è§†é¢‘ä¸­çš„åœºæ™¯å˜åŒ–ç‰¹æ®Šè¾“å…¥ä»¤ç‰Œäº¤æ›¿ä¸åŒç±»å‹å­—å¹•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘æ•°æ®å…·æœ‰é«˜å¯†åº¦å’Œé«˜ç»´åº¦ç‰¹æ€§ï¼Œæ–‡æœ¬æ‘˜è¦ä¸ºæŸ¥è¯¢ç›¸å…³å†…å®¹æä¾›äº†æ›´ç´§å‡‘çš„è¡¨ç¤ºæ–¹å¼ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæ‘„å–æ–‡æœ¬è¡¨ç¤ºå¹¶è¿›è¡Œè§†é¢‘å†…å®¹çš„æ¨ç†ï¼Œä»¥å›ç­”å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚</li>
<li>é€šè¿‡å°†è§†é¢‘åˆ’åˆ†ä¸ºæœ‰æ„ä¹‰ç‰‡æ®µæ¥æ„å»ºæ–‡æœ¬åŸºç¡€è®°å¿†ï¼Œä»¥å®ç°æ—¶ç©ºå»ºæ¨¡çš„è®¡ç®—å¯è¡Œæ€§ã€‚</li>
<li>ç»“åˆçŸ­è§†é¢‘å­—å¹•å’Œé™æ€åœºæ™¯æè¿°æé«˜æ´»åŠ¨æ—¥å¿—è´¨é‡ã€‚</li>
<li>ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åŠ å…¥é™æ€åœºæ™¯æè¿°ä¸°å¯Œè®°å¿†å†…å®¹ã€‚</li>
<li>LaViLaè§†é¢‘æè¿°å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆå½¢æˆå¼ºå¤§çš„è§†é¢‘ç†è§£ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-797dcf2264827b3e1f86f224bc5d2d59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8a4dc65f4c11423ce7c47e052c695e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fc426551145b8cf0777a42c9e13bc61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0fa747d8aa62fb2a8dead0bddc8be2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9161ea8f77515579e23c737f29543ddb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-Your-Model-Separate-Yolks-with-a-Water-Bottle-Benchmarking-Physical-Commonsense-Understanding-in-Video-Generation-Models"><a href="#Can-Your-Model-Separate-Yolks-with-a-Water-Bottle-Benchmarking-Physical-Commonsense-Understanding-in-Video-Generation-Models" class="headerlink" title="Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical   Commonsense Understanding in Video Generation Models"></a>Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical   Commonsense Understanding in Video Generation Models</h2><p><strong>Authors:Enes Sanli, Baris Sarper Tezcan, Aykut Erdem, Erkut Erdem</strong></p>
<p>Recent progress in text-to-video (T2V) generation has enabled the synthesis of visually compelling and temporally coherent videos from natural language. However, these models often fall short in basic physical commonsense, producing outputs that violate intuitive expectations around causality, object behavior, and tool use. Addressing this gap, we present PhysVidBench, a benchmark designed to evaluate the physical reasoning capabilities of T2V systems. The benchmark includes 383 carefully curated prompts, emphasizing tool use, material properties, and procedural interactions, and domains where physical plausibility is crucial. For each prompt, we generate videos using diverse state-of-the-art models and adopt a three-stage evaluation pipeline: (1) formulate grounded physics questions from the prompt, (2) caption the generated video with a vision-language model, and (3) task a language model to answer several physics-involved questions using only the caption. This indirect strategy circumvents common hallucination issues in direct video-based evaluation. By highlighting affordances and tool-mediated actions, areas overlooked in current T2V evaluations, PhysVidBench provides a structured, interpretable framework for assessing physical commonsense in generative video models. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆçš„è¿›å±•å·²ç»èƒ½å¤Ÿåˆæˆå…·æœ‰è§†è§‰å¸å¼•åŠ›å’Œæ—¶é—´è¿è´¯æ€§çš„è§†é¢‘æ¥è‡ªè‡ªç„¶è¯­è¨€ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸åœ¨åŸºæœ¬ç‰©ç†å¸¸è¯†æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œäº§ç”Ÿçš„è¾“å‡ºè¿åäº†å› æœã€ç‰©ä½“è¡Œä¸ºå’Œå·¥å…·ä½¿ç”¨çš„ç›´è§‚é¢„æœŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PhysVidBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°T2Vç³»ç»Ÿç‰©ç†æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬383ä¸ªç²¾å¿ƒç­–åˆ’çš„æç¤ºï¼Œå¼ºè°ƒå·¥å…·ä½¿ç”¨ã€ææ–™å±æ€§å’Œç¨‹åºäº¤äº’ï¼Œä»¥åŠåœ¨ç‰©ç†å¯è¡Œæ€§è‡³å…³é‡è¦çš„é¢†åŸŸã€‚å¯¹äºæ¯ä¸ªæç¤ºï¼Œæˆ‘ä»¬ä½¿ç”¨å„ç§æœ€å…ˆè¿›æ¨¡å‹ç”Ÿæˆè§†é¢‘ï¼Œå¹¶é‡‡ç”¨ä¸‰é˜¶æ®µè¯„ä¼°æµç¨‹ï¼šï¼ˆ1ï¼‰æ ¹æ®æç¤ºåˆ¶å®šåŸºäºç‰©ç†çš„é—®é¢˜ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºç”Ÿæˆçš„è§†é¢‘æ·»åŠ æ ‡é¢˜ï¼Œï¼ˆ3ï¼‰ä»»åŠ¡è¯­è¨€æ¨¡å‹ä»…ä½¿ç”¨æ ‡é¢˜å›ç­”æ¶‰åŠç‰©ç†çš„è‹¥å¹²é—®é¢˜ã€‚è¿™ç§é—´æ¥ç­–ç•¥é¿å…äº†ç›´æ¥åŸºäºè§†é¢‘çš„è¯„ä¼°ä¸­å¸¸è§çš„å¹»è§‰é—®é¢˜ã€‚é€šè¿‡çªå‡ºä¼˜åŠ¿å’Œå·¥å…·ä»‹å¯¼çš„è¡ŒåŠ¨ï¼Œä»¥åŠå½“å‰T2Vè¯„ä¼°ä¸­è¢«å¿½è§†çš„é¢†åŸŸï¼ŒPhysVidBenchä¸ºè¯„ä¼°ç”Ÿæˆè§†é¢‘æ¨¡å‹ä¸­çš„ç‰©ç†å¸¸è¯†æä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15824v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬è‡³è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰æŠ€æœ¯åœ¨ç”Ÿæˆå…·æœ‰è§†è§‰å¸å¼•åŠ›å’Œæ—¶é—´è¿è´¯æ€§çš„è§†é¢‘æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨åŸºæœ¬ç‰©ç†å¸¸è¯†æ–¹é¢å­˜åœ¨ç¼ºé™·ï¼Œäº§ç”Ÿçš„è¾“å‡ºè¿åäº†å› æœã€ç‰©ä½“è¡Œä¸ºå’Œå·¥å…·ä½¿ç”¨ç­‰æ–¹é¢çš„ç›´è§‚é¢„æœŸã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºPhysVidBenchåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°T2Vç³»ç»Ÿçš„ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«383ä¸ªç²¾å¿ƒç­–åˆ’çš„æç¤ºï¼Œå¼ºè°ƒå·¥å…·ä½¿ç”¨ã€ææ–™å±æ€§å’Œç¨‹åºæ€§äº¤äº’ï¼Œä»¥åŠåœ¨ç‰©ç†å¯ä¿¡åº¦è‡³å…³é‡è¦çš„é¢†åŸŸã€‚é‡‡ç”¨ä¸‰é˜¶æ®µè¯„ä¼°æµç¨‹å¯¹ç”Ÿæˆçš„è§†é¢‘è¿›è¡Œè¯„ä»·ï¼ŒåŒ…æ‹¬ä»æç¤ºä¸­åˆ¶å®šåŸºäºç‰©ç†çš„é—®é¢˜ã€ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºç”Ÿæˆçš„è§†é¢‘æ·»åŠ å­—å¹•ï¼Œä»¥åŠä»…ä½¿ç”¨å­—å¹•å›ç­”æ¶‰åŠç‰©ç†çš„å¤šä¸ªé—®é¢˜ã€‚è¿™ç§é—´æ¥ç­–ç•¥é¿å…äº†åœ¨ç›´æ¥è§†é¢‘è¯„ä¼°ä¸­çš„å¸¸è§å¹»è§‰é—®é¢˜ã€‚PhysVidBenché€šè¿‡å¼ºè°ƒä¾¿åˆ©æ€§å’Œå·¥å…·ä»‹å¯¼çš„è¡ŒåŠ¨ï¼Œæä¾›äº†è¯„ä¼°ç”Ÿæˆè§†é¢‘æ¨¡å‹ä¸­çš„ç‰©ç†å¸¸è¯†çš„ç»“æ„åŒ–ã€å¯è§£é‡Šæ¡†æ¶ï¼Œå¼¥è¡¥äº†å½“å‰T2Vè¯„ä¼°ä¸­è¢«å¿½è§†çš„é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è‡³è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰æŠ€æœ¯åœ¨ç”Ÿæˆè§†è§‰ä¸Šå¸å¼•äººçš„è§†é¢‘æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†ç¼ºä¹åŸºæœ¬ç‰©ç†å¸¸è¯†ã€‚</li>
<li>PhysVidBenchåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°T2Vç³»ç»Ÿåœ¨ç‰©ç†æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•åŒ…å«383ä¸ªç²¾å¿ƒç­–åˆ’çš„æç¤ºï¼Œæ¶µç›–å·¥å…·ä½¿ç”¨ã€ææ–™å±æ€§å’Œç¨‹åºæ€§äº¤äº’ç­‰é¢†åŸŸã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µè¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬åˆ¶å®šåŸºäºç‰©ç†çš„é—®é¢˜ã€ä¸ºç”Ÿæˆè§†é¢‘æ·»åŠ å­—å¹•ä»¥åŠå›ç­”æ¶‰åŠç‰©ç†çš„é—®é¢˜ã€‚</li>
<li>é—´æ¥è¯„ä¼°ç­–ç•¥æœ‰åŠ©äºé¿å…ç›´æ¥è§†é¢‘è¯„ä¼°ä¸­çš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>PhysVidBenchæä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ã€å¯è§£é‡Šçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆè§†é¢‘æ¨¡å‹ä¸­çš„ç‰©ç†å¸¸è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-85c588b81ba14c26c3628f090c1dec19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cdaab6342d764fe2e0ceb8ca4646068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6885412997772a22eebbb92d253e7874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd284b617951012cb92fbcbe2ae701db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7272a12da6157b70d521836baa08449.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DynImg-Key-Frames-with-Visual-Prompts-are-Good-Representation-for-Multi-Modal-Video-Understanding"><a href="#DynImg-Key-Frames-with-Visual-Prompts-are-Good-Representation-for-Multi-Modal-Video-Understanding" class="headerlink" title="DynImg: Key Frames with Visual Prompts are Good Representation for   Multi-Modal Video Understanding"></a>DynImg: Key Frames with Visual Prompts are Good Representation for   Multi-Modal Video Understanding</h2><p><strong>Authors:Xiaoyi Bao, Chenwei Xie, Hao Tang, Tingyu Weng, Xiaofeng Wang, Yun Zheng, Xingang Wang</strong></p>
<p>In recent years, the introduction of Multi-modal Large Language Models (MLLMs) into video understanding tasks has become increasingly prevalent. However, how to effectively integrate temporal information remains a critical research focus. Traditional approaches treat spatial and temporal information separately. Due to issues like motion blur, it is challenging to accurately represent the spatial information of rapidly moving objects. This can lead to temporally important regions being underemphasized during spatial feature extraction, which in turn hinders accurate spatio-temporal interaction and video understanding. To address this limitation, we propose an innovative video representation method called Dynamic-Image (DynImg). Specifically, we introduce a set of non-key frames as temporal prompts to highlight the spatial areas containing fast-moving objects. During the process of visual feature extraction, these prompts guide the model to pay additional attention to the fine-grained spatial features corresponding to these regions. Moreover, to maintain the correct sequence for DynImg, we employ a corresponding 4D video Rotary Position Embedding. This retains both the temporal and spatial adjacency of DynImg, helping MLLM understand the spatio-temporal order within this combined format. Experimental evaluations reveal that DynImg surpasses the state-of-the-art methods by approximately 2% across multiple video understanding benchmarks, proving the effectiveness of our temporal prompts in enhancing video comprehension. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„å¼•å…¥è¶Šæ¥è¶Šæ™®éã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆæ—¶é—´ä¿¡æ¯ä»ç„¶æ˜¯å…³é”®çš„ç ”ç©¶é‡ç‚¹ã€‚ä¼ ç»Ÿçš„æ–¹æ³•åˆ†åˆ«å¤„ç†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ã€‚ç”±äºè¿åŠ¨æ¨¡ç³Šç­‰é—®é¢˜ï¼Œå‡†ç¡®è¡¨ç¤ºå¿«é€Ÿç§»åŠ¨ç‰©ä½“çš„ç©ºé—´ä¿¡æ¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿™å¯èƒ½å¯¼è‡´åœ¨æå–ç©ºé—´ç‰¹å¾æ—¶å¿½è§†äº†æ—¶é—´ä¸Šé‡è¦çš„åŒºåŸŸï¼Œä»è€Œå¦¨ç¢å‡†ç¡®çš„æ—¶ç©ºäº¤äº’å’Œè§†é¢‘ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è§†é¢‘è¡¨ç¤ºæ–¹æ³•ï¼Œç§°ä¸ºåŠ¨æ€å›¾åƒï¼ˆDynImgï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç³»åˆ—éå…³é”®å¸§ä½œä¸ºæ—¶é—´æç¤ºï¼Œä»¥çªå‡ºåŒ…å«å¿«é€Ÿç§»åŠ¨ç‰©ä½“çš„ç©ºé—´åŒºåŸŸã€‚åœ¨æå–è§†è§‰ç‰¹å¾çš„è¿‡ç¨‹ä¸­ï¼Œè¿™äº›æç¤ºå¼•å¯¼æ¨¡å‹é¢å¤–å…³æ³¨è¿™äº›åŒºåŸŸå¯¹åº”çš„ç²¾ç»†ç©ºé—´ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¿æŒDynImgçš„æ­£ç¡®åºåˆ—ï¼Œæˆ‘ä»¬é‡‡ç”¨ç›¸åº”çš„4Dè§†é¢‘æ—‹è½¬ä½ç½®åµŒå…¥ã€‚è¿™ä¿ç•™äº†DynImgçš„æ—¶é—´å’Œç©ºé—´é‚»æ¥æ€§ï¼Œå¸®åŠ©MLLMç†è§£è¿™ç§ç»„åˆæ ¼å¼å†…çš„æ—¶ç©ºé¡ºåºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDynImgåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæé«˜äº†å¤§çº¦2%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ—¶é—´æç¤ºåœ¨æé«˜è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15569v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šæ™®éã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆæ—¶é—´ä¿¡æ¯ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®çš„ç ”ç©¶é‡ç‚¹ã€‚ä¼ ç»Ÿæ–¹æ³•åˆ†åˆ«å¤„ç†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œç”±äºè¿åŠ¨æ¨¡ç³Šç­‰é—®é¢˜ï¼Œå‡†ç¡®è¡¨ç¤ºå¿«é€Ÿç§»åŠ¨ç‰©ä½“çš„ç©ºé—´ä¿¡æ¯å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è§†é¢‘è¡¨ç¤ºæ–¹æ³•â€”â€”åŠ¨æ€å›¾åƒï¼ˆDynImgï¼‰ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç³»åˆ—éå…³é”®å¸§ä½œä¸ºæ—¶é—´æç¤ºæ¥çªå‡ºåŒ…å«å¿«é€Ÿç§»åŠ¨ç‰©ä½“çš„ç©ºé—´åŒºåŸŸã€‚è¿™äº›æç¤ºåœ¨è§†è§‰ç‰¹å¾æå–è¿‡ç¨‹ä¸­å¼•å¯¼æ¨¡å‹é¢å¤–å…³æ³¨è¿™äº›åŒºåŸŸçš„ç²¾ç»†ç©ºé—´ç‰¹å¾ã€‚åŒæ—¶ï¼Œä¸ºäº†ä¿æŒDynImgçš„æ­£ç¡®åºåˆ—ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ç›¸åº”çš„4Dè§†é¢‘æ—‹è½¬ä½ç½®åµŒå…¥ï¼Œè¿™ä¿ç•™äº†DynImgçš„æ—¶é—´å’Œç©ºé—´é‚»æ¥æ€§ï¼Œå¸®åŠ©MLLMç†è§£è¿™ç§ç»„åˆæ ¼å¼ä¸­çš„æ—¶ç©ºé¡ºåºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒDynImgåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­æ¯”æœ€æ–°æŠ€æœ¯é¢†å…ˆçº¦2%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ—¶é—´æç¤ºåœ¨æé«˜è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>æ•´åˆæ—¶é—´ä¿¡æ¯æ˜¯è§†é¢‘ç†è§£çš„å…³é”®ï¼Œä¼ ç»Ÿæ–¹æ³•å¤„ç†ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥éå…³é”®å¸§ä½œä¸ºæ—¶é—´æç¤ºå¯ä»¥çªå‡ºå¿«é€Ÿç§»åŠ¨ç‰©ä½“çš„ç©ºé—´åŒºåŸŸã€‚</li>
<li>åŠ¨æ€å›¾åƒï¼ˆDynImgï¼‰æ–¹æ³•é€šè¿‡å¼•å¯¼æ¨¡å‹å…³æ³¨ç²¾ç»†ç©ºé—´ç‰¹å¾æ¥æé«˜è§†é¢‘ç†è§£ã€‚</li>
<li>4Dè§†é¢‘æ—‹è½¬ä½ç½®åµŒå…¥æŠ€æœ¯ç”¨äºä¿æŒDynImgçš„æ­£ç¡®åºåˆ—å’Œæ—¶ç©ºé‚»æ¥æ€§ã€‚</li>
<li>DynImgåœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-564b240b0b6447243280f4dd4bc2b74e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68bb5393712ed4c6d3c8410e6b1ad697.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-718b609808a77d5fbf4d95c21920c3d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6925f60083e27b591f09fde954f154b4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-03/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8577c124f2f2b957f9eb7ed8c1121359.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  UPRE Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-03/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a63c44975d3d4faa55d2ab02950c7f5.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-03  Efficient Masked Attention Transformer for Few-Shot Classification and   Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
