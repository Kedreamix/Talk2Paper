<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-02-17  Utilizing 3D Fast Spin Echo Anatomical Imaging to Reduce the Number of   Contrast Preparations in $T_{1ρ}$ Quantification of Knee Cartilage Using   Learning-Based Methods">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4ccec59a98e20bb864e5b8a69ef88467.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    36 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-17-更新"><a href="#2025-02-17-更新" class="headerlink" title="2025-02-17 更新"></a>2025-02-17 更新</h1><h2 id="Utilizing-3D-Fast-Spin-Echo-Anatomical-Imaging-to-Reduce-the-Number-of-Contrast-Preparations-in-T-1ρ-Quantification-of-Knee-Cartilage-Using-Learning-Based-Methods"><a href="#Utilizing-3D-Fast-Spin-Echo-Anatomical-Imaging-to-Reduce-the-Number-of-Contrast-Preparations-in-T-1ρ-Quantification-of-Knee-Cartilage-Using-Learning-Based-Methods" class="headerlink" title="Utilizing 3D Fast Spin Echo Anatomical Imaging to Reduce the Number of   Contrast Preparations in $T_{1ρ}$ Quantification of Knee Cartilage Using   Learning-Based Methods"></a>Utilizing 3D Fast Spin Echo Anatomical Imaging to Reduce the Number of   Contrast Preparations in $T_{1ρ}$ Quantification of Knee Cartilage Using   Learning-Based Methods</h2><p><strong>Authors:Junru Zhong, Chaoxing Huang, Ziqiang Yu, Fan Xiao, Siyue Li, Tim-Yun Michael Ong, Ki-Wai Kevin Ho, Queenie Chan, James F. Griffith, Weitian Chen</strong></p>
<p>Purpose: To propose and evaluate an accelerated $T_{1\rho}$ quantification method that combines $T_{1\rho}$-weighted fast spin echo (FSE) images and proton density (PD)-weighted anatomical FSE images, leveraging deep learning models for $T_{1\rho}$ mapping. The goal is to reduce scan time and facilitate integration into routine clinical workflows for osteoarthritis (OA) assessment. Methods: This retrospective study utilized MRI data from 40 participants (30 OA patients and 10 healthy volunteers). A volume of PD-weighted anatomical FSE images and a volume of $T_{1\rho}$-weighted images acquired at a non-zero spin-lock time were used as input to train deep learning models, including a 2D U-Net and a multi-layer perceptron (MLP). $T_{1\rho}$ maps generated by these models were compared with ground truth maps derived from a traditional non-linear least squares (NLLS) fitting method using four $T_{1\rho}$-weighted images. Evaluation metrics included mean absolute error (MAE), mean absolute percentage error (MAPE), regional error (RE), and regional percentage error (RPE). Results: Deep learning models achieved RPEs below 5% across all evaluated scenarios, outperforming NLLS methods, especially in low signal-to-noise conditions. The best results were obtained using the 2D U-Net, which effectively leveraged spatial information for accurate $T_{1\rho}$ fitting. The proposed method demonstrated compatibility with shorter TSLs, alleviating RF hardware and specific absorption rate (SAR) limitations. Conclusion: The proposed approach enables efficient $T_{1\rho}$ mapping using PD-weighted anatomical images, reducing scan time while maintaining clinical standards. This method has the potential to facilitate the integration of quantitative MRI techniques into routine clinical practice, benefiting OA diagnosis and monitoring. </p>
<blockquote>
<p>目的：提出并评估一种加速的$T_{1\rho}$定量方法，该方法结合了$T_{1\rho}$加权快速自旋回波（FSE）图像和质子密度（PD）加权解剖FSE图像，利用深度学习模型进行$T_{1\rho}$映射。旨在缩短扫描时间，促进定量磁共振成像技术融入临床日常工作中，用于骨关节炎（OA）的评估。</p>
</blockquote>
<p>方法：这项回顾性研究使用了40名参与者（30名OA患者和10名健康志愿者）的MRI数据。使用体积PD加权解剖FSE图像和体积$T_{1\rho}$加权图像（在非零自旋锁定时间获取）来训练深度学习模型，包括二维U-Net和多层感知器（MLP）。将这些模型生成的$T_{1\rho$地图与基于四幅$T_{1\rho}$加权图像的传统非线性最小二乘（NLLS）拟合方法得出的真实地图进行比较。评估指标包括平均绝对误差（MAE）、平均绝对百分比误差（MAPE）、区域误差（RE）和区域百分比误差（RPE）。</p>
<p>结果：深度学习模型的RPE在所有评估场景中均低于5%，优于NLLS方法，特别是在低信噪比条件下。使用二维U-Net获得最佳结果，该网络有效利用空间信息，实现准确的$T_{1\rho}$拟合。所提出的方法证明与较短的TSL兼容，减轻了射频硬件和特定吸收率（SAR）的限制。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08973v1">PDF</a> Submitted to Magnetic Resonance in Medicine</p>
<p><strong>摘要</strong><br>    深度学习模型结合$T_{1\rho}$加权快速自旋回波（FSE）图像和质子密度（PD）加权解剖FSE图像，实现$T_{1\rho}$量化方法的加速，用于骨关节炎（OA）评估。通过回顾性研究利用MRI数据训练和评估模型，结果显示深度学习模型，特别是2D U-Net，在$T_{1\rho}$映射中表现优异，具有缩短扫描时间并维持临床标准的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>研究旨在提出并评估一种结合$T_{1\rho}$加权快速自旋回波（FSE）图像和质子密度（PD）加权解剖FSE图像的加速$T_{1\rho}$量化方法。</li>
<li>利用深度学习模型，包括2D U-Net和多层感知器（MLP），进行$T_{1\rho}$映射，以缩短扫描时间并促进在日常临床工作中的集成。</li>
<li>回顾性研究使用来自40名参与者（30名OA患者和10名健康志愿者）的MRI数据训练和评估模型。</li>
<li>深度学习模型在$T_{1\rho}$映射方面表现出优异的性能，与基于传统非线性最小二乘（NLLS）拟合方法生成的地面真实地图相比，评价指标包括平均绝对误差（MAE）、平均绝对百分比误差（MAPE）、区域误差（RE）和区域百分比误差（RPE）。</li>
<li>2D U-Net在有效利用空间信息以进行准确的$T_{1\rho}$拟合方面表现出最佳结果。</li>
<li>所提出的方法与较短的时间间隔（TSLs）兼容，减轻了射频硬件和特定吸收率（SAR）的限制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b1782a4384c231701c17e2ac47664a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84ad763227ea6b3c5e8aee1e1d2a0fd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ae6b1501f5752e78e56731ab98cce02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f3a15f50bfa67904e188a617612fc8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1399151777a3c97a2791e337b16b7c38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81d10455e474dbcf6f16bba55960c3de.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Exploring-Test-Time-Adaptation-for-Subcortical-Segmentation-of-the-Fetal-Brain-in-3D-Ultrasound"><a href="#Exploring-Test-Time-Adaptation-for-Subcortical-Segmentation-of-the-Fetal-Brain-in-3D-Ultrasound" class="headerlink" title="Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal   Brain in 3D Ultrasound"></a>Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal   Brain in 3D Ultrasound</h2><p><strong>Authors:Joshua Omolegan, Pak Hei Yeung, Madeleine K. Wyburd, Linde Hesse, Monique Haak, Intergrowth-21st Consortium, Ana I. L. Namburete, Nicola K. Dinsdale</strong></p>
<p>Monitoring the growth of subcortical regions of the fetal brain in ultrasound (US) images can help identify the presence of abnormal development. Manually segmenting these regions is a challenging task, but recent work has shown that it can be automated using deep learning. However, applying pretrained models to unseen freehand US volumes often leads to a degradation of performance due to the vast differences in acquisition and alignment. In this work, we first demonstrate that test time adaptation (TTA) can be used to improve model performance in the presence of both real and simulated domain shifts. We further propose a novel TTA method by incorporating a normative atlas as a prior for anatomy. In the presence of various types of domain shifts, we benchmark the performance of different TTA methods and demonstrate the improvements brought by our proposed approach, which may further facilitate automated monitoring of fetal brain development. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation">https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation</a>. </p>
<blockquote>
<p>监测胎儿大脑皮层下区域在超声（US）图像中的生长情况有助于识别异常发育的存在。手动分割这些区域是一项具有挑战性的任务，但最近的工作表明，可以使用深度学习来自动化完成。然而，将预训练模型应用于未见过的自由手超声体积常常会导致性能下降，这是由于采集和对齐方面的巨大差异。在这项工作中，我们首先证明测试时间适应（TTA）可用于在存在真实和模拟域偏移的情况下提高模型性能。我们进一步提出了一种新型的TTA方法，该方法结合规范性图谱作为解剖学的先验知识。在各种类型的域偏移存在的情况下，我们对不同的TTA方法进行了性能评估，并展示了我们所提出方法所带来的改进，这可能有助于进一步促进胎儿大脑发育的自动化监测。我们的代码可在[<a target="_blank" rel="noopener" href="https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation找到。]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08774v1">PDF</a> 5 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了利用超声图像监测胎儿大脑亚皮层区域发育情况的重要性，并指出手动分割这些区域是一项挑战。然而，近期的研究显示可通过深度学习实现自动化。但当将预训练模型应用于未见的手绘超声体积数据时，性能往往会下降，因为采集和校准方面存在巨大差异。本文首先验证了测试时间适应（TTA）技术在真实和模拟域偏移情况下的模型性能提升作用。此外，本文提出了一种新的TTA方法，通过纳入规范图谱作为解剖学先验。在不同类型的域偏移情况下，本文评估了不同TTA方法的性能，并展示了所提出的方法在自动化监测胎儿大脑发育方面的改进优势。相关代码可在​​<a target="_blank" rel="noopener" href="https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation%E2%80%8B%E2%80%8B%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation​​获取。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>超声图像监测胎儿大脑亚皮层区域发育情况对识别异常发育具有重要意义。</li>
<li>手动分割胎儿大脑的亚皮层区域是一项挑战，但深度学习可实现自动化。</li>
<li>预训练模型应用于手绘超声体积数据时性能可能下降，原因是数据的采集和校准差异。</li>
<li>测试时间适应技术（TTA）能提升模型在真实和模拟域偏移情况下的性能。</li>
<li>本文提出了一种新的TTA方法，结合规范图谱作为解剖学先验来提升性能。</li>
<li>在不同类型的域偏移情况下，本文评估了多种TTA方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08774">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9862bc755dbd14ba558cf0a0641d903d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcfb990401544b2ec3b226b4383f07a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e962eea5bb7a8dd73f652e37c608c2a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d08949e48f53a816663052261a32957.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Improving-Lesion-Segmentation-in-Medical-Images-by-Global-and-Regional-Feature-Compensation"><a href="#Improving-Lesion-Segmentation-in-Medical-Images-by-Global-and-Regional-Feature-Compensation" class="headerlink" title="Improving Lesion Segmentation in Medical Images by Global and Regional   Feature Compensation"></a>Improving Lesion Segmentation in Medical Images by Global and Regional   Feature Compensation</h2><p><strong>Authors:Chuhan Wang, Zhenghao Chen, Jean Y. H. Yang, Jinman Kim</strong></p>
<p>Automated lesion segmentation of medical images has made tremendous improvements in recent years due to deep learning advancements. However, accurately capturing fine-grained global and regional feature representations remains a challenge. Many existing methods obtain suboptimal performance on complex lesion segmentation due to information loss during typical downsampling operations and the insufficient capture of either regional or global features. To address these issues, we propose the Global and Regional Compensation Segmentation Framework (GRCSF), which introduces two key innovations: the Global Compensation Unit (GCU) and the Region Compensation Unit (RCU). The proposed GCU addresses resolution loss in the U-shaped backbone by preserving global contextual features and fine-grained details during multiscale downsampling. Meanwhile, the RCU introduces a self-supervised learning (SSL) residual map generated by Masked Autoencoders (MAE), obtained as pixel-wise differences between reconstructed and original images, to highlight regions with potential lesions. These SSL residual maps guide precise lesion localization and segmentation through a patch-based cross-attention mechanism that integrates regional spatial and pixel-level features. Additionally, the RCU incorporates patch-level importance scoring to enhance feature fusion by leveraging global spatial information from the backbone. Experiments on two publicly available medical image segmentation datasets, including brain stroke lesion and coronary artery calcification datasets, demonstrate that our GRCSF outperforms state-of-the-art methods, confirming its effectiveness across diverse lesion types and its potential as a generalizable lesion segmentation solution. </p>
<blockquote>
<p>医学图像自动化病灶分割在最近几年由于深度学习的发展而取得了巨大的进步。然而，精准捕捉精细粒度的全局和局部特征表示仍然是一个挑战。由于典型的下采样操作过程中的信息损失以及对局部或全局特征的捕捉不足，许多现有方法在复杂病灶分割方面的表现并不理想。为了解决这些问题，我们提出了全球和区域补偿分割框架（GRCSF），它引入了两个关键的创新点：全球补偿单元（GCU）和区域补偿单元（RCU）。所提出的GCU通过在多尺度下采样过程中保留全局上下文特征和精细细节，解决了U形主干中的分辨率损失问题。同时，RCU引入了一种由Masked Autoencoders（MAE）生成的自监督学习（SSL）残差图，该图是通过重建图像和原始图像之间的像素级差异获得的，以突出潜在病灶区域。这些SSL残差图通过基于补丁的交叉注意力机制引导精确病灶定位和分割，该机制融合了局部空间特征和像素级特征。此外，RCU还结合了补丁级别的重要性评分，通过利用主干网络中的全局空间信息来增强特征融合。在包括脑卒中病灶和冠状动脉钙化数据集在内的两个公开可用的医学图像分割数据集上的实验表明，我们的GRCSF优于最新方法，证实了其在多种病灶类型中的有效性，以及其作为可推广的病灶分割解决方案的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.08675v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为全局与区域补偿分割框架（GRCSF）的方法，用于解决医学图像中精细粒度全局和区域特征表示捕捉的挑战。该框架包含两个关键创新点：全局补偿单元（GCU）和区域补偿单元（RCU）。GCU解决在U型骨干网络中的分辨率损失问题，通过多尺度下采样过程中保留全局上下文特征和精细细节。RCU则引入自监督学习（SSL）残差图，由Masked Autoencoders生成的像素级差异图突出潜在病变区域。这些SSL残差图通过基于补丁的交叉注意力机制引导精确病变定位和分割，融合区域空间特征和像素级特征。在公开可用的医学图像分割数据集上的实验结果表明，GRCSF方法优于当前主流方法，表现出对不同病变类型的良好泛化潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>深度学习在医学图像自动化病变分割方面取得巨大进步，但仍面临捕捉精细粒度全局和区域特征表示的难题。</li>
<li>现有方法在复杂病变分割上表现不佳，主要由于下采样操作中的信息损失以及对区域或全局特征捕捉不足。</li>
<li>提出Global and Regional Compensation Segmentation Framework (GRCSF)，包含Global Compensation Unit (GCU) 和 Region Compensation Unit (RCU) 两个关键创新点。</li>
<li>GCU解决U型骨干网络中的分辨率损失问题，通过多尺度下采样保留全局上下文特征和精细细节。</li>
<li>RCU引入自监督学习（SSL）残差图，突出潜在病变区域，通过基于补丁的交叉注意力机制引导精确病变定位和分割。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.08675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d61e5beacb98eeb897fc74d4686f720.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30f91894f0ac912b0ee0e2c0a6e99967.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DM-Mamba-Dual-domain-Multi-scale-Mamba-for-MRI-reconstruction"><a href="#DM-Mamba-Dual-domain-Multi-scale-Mamba-for-MRI-reconstruction" class="headerlink" title="DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction"></a>DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction</h2><p><strong>Authors:Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi</strong></p>
<p>The accelerated MRI reconstruction poses a challenging ill-posed inverse problem due to the significant undersampling in k-space. Deep neural networks, such as CNNs and ViT, have shown substantial performance improvements for this task while encountering the dilemma between global receptive fields and efficient computation. To this end, this paper pioneers exploring Mamba, a new paradigm for long-range dependency modeling with linear complexity, for efficient and effective MRI reconstruction. However, directly applying Mamba to MRI reconstruction faces three significant issues: (1) Mamba’s row-wise and column-wise scanning disrupts k-space’s unique spectrum, leaving its potential in k-space learning unexplored. (2) Existing Mamba methods unfold feature maps with multiple lengthy scanning paths, leading to long-range forgetting and high computational burden. (3) Mamba struggles with spatially-varying contents, resulting in limited diversity of local representations. To address these, we propose a dual-domain multi-scale Mamba for MRI reconstruction from the following perspectives: (1) We pioneer vision Mamba in k-space learning. A circular scanning is customized for spectrum unfolding, benefiting the global modeling of k-space. (2) We propose a multi-scale Mamba with an efficient scanning strategy in both image and k-space domains. It mitigates long-range forgetting and achieves a better trade-off between efficiency and performance. (3) We develop a local diversity enhancement module to improve the spatially-varying representation of Mamba. Extensive experiments are conducted on three public datasets for MRI reconstruction under various undersampling patterns. Comprehensive results demonstrate that our method significantly outperforms state-of-the-art methods with lower computational cost. Implementation code will be available at <a target="_blank" rel="noopener" href="https://github.com/XiaoMengLiLiLi/DM-Mamba">https://github.com/XiaoMengLiLiLi/DM-Mamba</a>. </p>
<blockquote>
<p>加速MRI重建是一个具有挑战性的不适定反问题，主要是因为k空间存在大量的欠采样。深度神经网络，如卷积神经网络（CNN）和Vision Transformer（ViT），在此任务上显示出显著的性能提升，但在全局感受野和高效计算之间遇到了困境。鉴于此，本文开创性地探索了Mamba，这是一种具有线性复杂度的长距离依赖建模新范式，用于高效且有效的MRI重建。然而，直接将Mamba应用于MRI重建面临三个主要问题：（1）Mamba的行扫描和列扫描破坏了k空间的独特频谱，使其对k空间学习的潜力尚未得到探索。（2）现有的Mamba方法展开特征映射具有多条冗长的扫描路径，导致长距离遗忘和高计算负担。（3）Mamba在空间内容变化方面遇到困难，导致局部表示多样性有限。为解决这些问题，我们从以下角度提出了用于MRI重建的双域多尺度Mamba：（1）我们开创性地提出了k空间中的视觉Mamba。针对频谱展开定制了循环扫描，有利于k空间的全局建模。（2）我们提出了一个在图像和k空间域都具有高效扫描策略的多尺度Mamba。它减轻了长距离遗忘问题，并在效率和性能之间取得了更好的平衡。（3）我们开发了一个局部多样性增强模块，以提高Mamba的空间变化表示。在三个公共数据集上进行了广泛的MRI重建实验，实验结果表明，我们的方法在较低的计算成本下显著优于最先进的方法。相关实现代码将发布在<a target="_blank" rel="noopener" href="https://github.com/XiaoMengLiLiLi/DM-Mamba%E3%80%82">https://github.com/XiaoMengLiLiLi/DM-Mamba。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08163v2">PDF</a> </p>
<p><strong>摘要</strong><br>    这篇论文探讨了在MRI重建中的挑战性问题，针对k空间中显著的欠采样现象，提出了一种新的建模方法Mamba。尽管Mamba在解决长距离依赖建模方面具有线性复杂性的优势，但在直接应用于MRI重建时面临三个主要问题。为了克服这些问题，本文提出了一种双域多尺度Mamba方法，在k空间和图像域进行高效扫描策略，并开发了一个局部多样性增强模块来改善Mamba的空间变化表示。实验结果表明，该方法在降低计算成本的同时，显著优于现有技术。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>Mamba作为一种新的长距离依赖建模方法，具有线性复杂性，在MRI重建中具有潜力。</li>
<li>直接应用Mamba于MRI重建面临三个主要问题：对k空间独特谱的干扰、特征映射展开路径过长以及空间内容变化导致的局部表示多样性有限。</li>
<li>提出了一种双域多尺度Mamba方法，首次在k空间中进行视觉Mamba学习，并定制了循环扫描以展开频谱，有利于全局k空间建模。</li>
<li>通过在图像和k空间域中采用高效扫描策略的多尺度Mamba，减轻了长距离遗忘问题，实现了效率和性能之间的更好权衡。</li>
<li>开发了局部多样性增强模块，提高了Mamba的空间变化表示。</li>
<li>在多个公共数据集上的实验表明，该方法在MRI重建方面显著优于现有技术，计算成本更低。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08163">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3d7ea188637ae63208bdf4dfef63927e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba5d86c1f2df3e7675d79755a39c9046.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b07a4f395a05bf5ae662d4aa6cf320b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fc573d035ecb47cf000924ad89bfc68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d77fe7286b8bb0c4e7c90e02dc479df1.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Heuristical-Comparison-of-Vision-Transformers-Against-Convolutional-Neural-Networks-for-Semantic-Segmentation-on-Remote-Sensing-Imagery"><a href="#Heuristical-Comparison-of-Vision-Transformers-Against-Convolutional-Neural-Networks-for-Semantic-Segmentation-on-Remote-Sensing-Imagery" class="headerlink" title="Heuristical Comparison of Vision Transformers Against Convolutional   Neural Networks for Semantic Segmentation on Remote Sensing Imagery"></a>Heuristical Comparison of Vision Transformers Against Convolutional   Neural Networks for Semantic Segmentation on Remote Sensing Imagery</h2><p><strong>Authors:Ashim Dahal, Saydul Akbar Murad, Nick Rahimi</strong></p>
<p>Vision Transformers (ViT) have recently brought a new wave of research in the field of computer vision. These models have performed particularly well in image classification and segmentation. Research on semantic and instance segmentation has accelerated with the introduction of the new architecture, with over 80% of the top 20 benchmarks for the iSAID dataset based on either the ViT architecture or the attention mechanism behind its success. This paper focuses on the heuristic comparison of three key factors of using (or not using) ViT for semantic segmentation of remote sensing aerial images on the iSAID dataset. The experimental results observed during this research were analyzed based on three objectives. First, we studied the use of a weighted fused loss function to maximize the mean Intersection over Union (mIoU) score and Dice score while minimizing entropy or class representation loss. Second, we compared transfer learning on Meta’s MaskFormer, a ViT-based semantic segmentation model, against a generic UNet Convolutional Neural Network (CNN) based on mIoU, Dice scores, training efficiency, and inference time. Third, we examined the trade-offs between the two models in comparison to current state-of-the-art segmentation models. We show that the novel combined weighted loss function significantly boosts the CNN model’s performance compared to transfer learning with ViT. The code for this implementation can be found at: <a target="_blank" rel="noopener" href="https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation">https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation</a>. </p>
<blockquote>
<p>Vision Transformers（ViT）最近为计算机视觉领域带来了新的研究浪潮。这些模型在图像分类和分割方面表现出色。新架构的引入加速了语义分割和实例分割的研究，iSAID数据集的前20个榜单中有超过80%是基于ViT架构或其成功背后的注意力机制。本文重点关注在iSAID数据集上使用（或不使用）ViT进行遥感图像语义分割的三个关键因素的启发式比较。研究过程中的实验结果基于以下三个目标进行分析。首先，我们研究了使用加权融合损失函数，以最大化平均交并比（mIoU）得分和Dice得分，同时最小化��e或类表示损失。其次，我们比较了Meta的MaskFormer（一种基于ViT的语义分割模型）的迁移学习与基于mIoU、Dice得分、训练效率和推理时间的通用UNet卷积神经网络（CNN）的迁移学习。第三，我们比较了这两种模型与当前最先进的分割模型的得失。我们表明，与ViT的迁移学习相比，新型组合加权损失函数显著提高了CNN模型的性能。该实现的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation">https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09101v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文研究了使用Vision Transformers（ViT）进行遥感图像语义分割的优缺点，并对比了ViT架构和基于注意力机制的模型在iSAID数据集上的表现。文章重点探讨了三个关键方面：使用加权融合损失函数的效果、与基于ViT的MaskFormer模型的迁移学习对比，以及与传统CNN模型的性能对比。实验结果表明，加权损失函数能有效提升CNN模型性能，相较于ViT迁移学习有更好的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li><p>Vision Transformers（ViT）在图像分类和分割领域表现优异，已成为计算机视觉领域的新研究热点。</p>
</li>
<li><p>iSAID数据集的前20名榜单中，超过80%是基于ViT架构或注意力机制的模型。</p>
</li>
<li><p>加权融合损失函数用于最大化平均交并比（mIoU）和Dice系数，同时最小化熵或类别表示损失。</p>
</li>
<li><p>对比了基于ViT的MaskFormer模型和通用UNet卷积神经网络（CNN）在mIoU、Dice系数、训练效率和推理时间上的表现。</p>
</li>
<li><p>加权损失函数显著提升CNN模型性能，相较于ViT迁移学习有更好的表现。</p>
</li>
<li><p>研究结果提供了遥感图像语义分割的新视角和方法论。</p>
</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09101">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-417b5bf8614515af23c47c85131ebc38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73afbad8ac8d34059a643a0505bebe91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0b782ce64a1396a6eb68627281a8575.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2f4fbc3caa3641f4926ebf35519a7c95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37eb1c6f292a5d4e9f1fb558c9ced14b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fad05694cea91fc8cfc80ae663ab943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12eaf63c35279df8cfd6d41c28acba2c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Unified-Model-for-Compressed-Sensing-MRI-Across-Undersampling-Patterns"><a href="#A-Unified-Model-for-Compressed-Sensing-MRI-Across-Undersampling-Patterns" class="headerlink" title="A Unified Model for Compressed Sensing MRI Across Undersampling Patterns"></a>A Unified Model for Compressed Sensing MRI Across Undersampling Patterns</h2><p><strong>Authors:Armeet Singh Jatyani, Jiayun Wang, Aditi Chandrashekar, Zihui Wu, Miguel Liu-Schiaffini, Bahareh Tolooshams, Anima Anandkumar</strong></p>
<p>Compressed Sensing MRI reconstructs images of the body’s internal anatomy from undersampled measurements, thereby reducing the scan time - the time subjects need to remain still. Recently, deep neural networks have shown great potential for reconstructing high-fidelity images from highly undersampled measurements in the frequency space. However, one needs to train multiple models for different undersampling patterns and desired output image resolutions, since most networks operate on a fixed discretization. Such approaches are highly impractical in clinical settings, where undersampling patterns and image resolutions are frequently changed to accommodate different real-time imaging and diagnostic requirements.   We propose a unified model robust to different measurement undersampling patterns and image resolutions in compressed sensing MRI. Our model is based on neural operators, a discretization-agnostic architecture. Neural operators are employed in both image and measurement space, which capture local and global image features for MRI reconstruction. Empirically, we achieve consistent performance across different undersampling rates and patterns, with an average 11 percent SSIM and 4dB PSNR improvement over a state-of-the-art CNN, End-to-End VarNet. For efficiency, our inference speed is also 1,400x faster than diffusion methods. The resolution-agnostic design also enhances zero-shot super-resolution and extended field of view in reconstructed images. Our unified model offers a versatile solution for MRI, adapting seamlessly to various measurement undersampling and imaging resolutions, making it highly effective for flexible and reliable clinical imaging. Our code is available at <a target="_blank" rel="noopener" href="https://armeet.ca/nomri">https://armeet.ca/nomri</a>. </p>
<blockquote>
<p>压缩感知MRI通过欠采样的测量重建人体内部解剖图像，从而减少扫描时间——即患者需要保持静止的时间。最近，深度神经网络在频率空间从高度欠采样的测量中重建高保真图像方面显示出巨大潜力。然而，由于大多数网络在固定的离散化上运行，因此需要针对不同的欠采样模式和所需的输出图像分辨率训练多个模型。在临床环境中，欠采样模式和图像分辨率经常更改以适应不同的实时成像和诊断要求，因此这种方法不太实用。我们提出了一种适用于压缩感知MRI中不同测量欠采样模式和图像分辨率的统一模型。我们的模型基于神经算子，这是一种不受离散化影响的架构。神经算子在图像和测量空间中都被采用，能够捕捉MRI重建的局部和全局图像特征。从经验上看，我们在不同的欠采样率和模式上实现了稳定的性能，与最先进的CNN End-to-End VarNet相比，平均SSIM提高了11%，PSNR提高了4dB。为了提高效率，我们的推理速度也比扩散方法快1400倍。分辨率无关的设计还提高了重建图像的零射击超分辨率和扩展视野。我们的统一模型为MRI提供了一个通用解决方案，可以无缝适应各种测量欠采样和成像分辨率，使其成为灵活可靠的临床成像的高效工具。我们的代码可在<a target="_blank" rel="noopener" href="https://armeet.ca/nomri%E8%8E%B7%E5%8F%96%E3%80%82">https://armeet.ca/nomri获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16290v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于压缩感知的MRI通过欠采样的测量重建身体内部结构图像，减少了扫描时间。近期深度神经网络在频率空间高度欠采样的测量重建高保真图像方面展现出巨大潜力。然而，大多数网络在固定离散化上操作，需要针对不同的欠采样模式和所需的输出图像分辨率训练多个模型，这在临床环境中非常不实用。我们提出一种统一的模型，适应不同的测量欠采样模式和图像分辨率。该模型基于神经算子，是一种不受离散化影响的架构。神经算子在图像和测量空间中均被采用，可捕捉局部和全局图像特征用于MRI重建。与当前先进技术相比，我们的模型在不同的欠采样率和模式上表现一致，平均SSIM提高11%，PSNR提高4dB。同时，我们的推理速度比扩散方法快1400倍。分辨率无关的设计还提高了重建图像的零射束超分辨率和扩展视野。我们的统一模型为MRI提供了通用解决方案，可轻松适应各种测量欠采样和成像分辨率，为临床成像提供了灵活可靠的选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>压缩感知MRI可从欠采样测量重建图像，缩短扫描时间。</li>
<li>深度神经网络在重建高频空间高度欠采样的图像方面表现出巨大潜力。</li>
<li>现有方法需要在不同欠采样模式和输出图像分辨率上训练多个模型，缺乏实用性。</li>
<li>提出一种基于神经算子的统一模型，适应不同的测量欠采样模式和图像分辨率。</li>
<li>该模型在多种欠采样情况下表现稳定，相较于先进技术有显著的SSIM和PSNR提升。</li>
<li>模型推理速度显著提高，且具备零射束超分辨率和扩展视野能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0f957fd37f39cc3f18efb36679e58e28.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3ab3f350b3cdb124fabff5e0588202a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0480049752d21465ab16f3f2e30561f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-962d828834551507a476975771920715.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-875f71c574f205f34e1d0b52979d679b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Fully-Unsupervised-Dynamic-MRI-Reconstruction-via-Diffeo-Temporal-Equivariance"><a href="#Fully-Unsupervised-Dynamic-MRI-Reconstruction-via-Diffeo-Temporal-Equivariance" class="headerlink" title="Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal   Equivariance"></a>Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal   Equivariance</h2><p><strong>Authors:Andrew Wang, Mike Davies</strong></p>
<p>Reconstructing dynamic MRI image sequences from undersampled accelerated measurements is crucial for faster and higher spatiotemporal resolution real-time imaging of cardiac motion, free breathing motion and many other applications. Classical paradigms, such as gated cine MRI, assume periodicity, disallowing imaging of true motion. Supervised deep learning methods are fundamentally flawed as, in dynamic imaging, ground truth fully-sampled videos are impossible to truly obtain. We propose an unsupervised framework to learn to reconstruct dynamic MRI sequences from undersampled measurements alone by leveraging natural geometric spatiotemporal equivariances of MRI. Dynamic Diffeomorphic Equivariant Imaging (DDEI) significantly outperforms state-of-the-art unsupervised methods such as SSDU on highly accelerated dynamic cardiac imaging. Our method is agnostic to the underlying neural network architecture and can be used to adapt the latest models and post-processing approaches. Our code and video demos are at <a target="_blank" rel="noopener" href="https://github.com/Andrewwango/ddei">https://github.com/Andrewwango/ddei</a>. </p>
<blockquote>
<p>从欠采样的加速测量值重建动态MRI图像序列对于更快、更高时空分辨率的实时心脏运动成像、自由呼吸运动成像以及许多其他应用至关重要。经典的模式，如门控电影MRI，假设周期性，不允许对真实运动进行成像。监督深度学习方法存在根本缺陷，因为在动态成像中，不可能真正获得完全采样的地面真实视频。我们提出了一种无监督的框架，通过利用MRI的自然几何时空等价性，仅从欠采样测量中学习重建动态MRI序列。动态微分等价成像（DDEI）在高度加速的动态心脏成像方面显著优于最新无监督方法如SSDU。我们的方法与底层神经网络架构无关，可用于适应最新模型和后期处理方法。我们的代码和视频演示可在<a target="_blank" rel="noopener" href="https://github.com/Andrewwango/ddei%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/Andrewwango/ddei查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08646v2">PDF</a> Conference paper at ISBI 2025</p>
<p><strong>Summary</strong></p>
<p>从欠采样的加速测量中重建动态MRI图像序列对于更快、更高时空分辨率的实时心脏运动成像、自由呼吸运动成像以及许多其他应用至关重要。现有的方法如门控电影MRI存在周期性假设，无法真正成像。由于无法获取动态成像的真实地面全采样视频，监督深度学习方法存在根本缺陷。我们提出了一种利用MRI自然几何时空等价性的无监督框架来重建动态MRI序列。动态微分等价成像（DDEI）在高度加速的动态心脏成像上显著优于最先进的状态无监督方法，如SSDU。我们的方法不依赖于底层神经网络架构，可用于适应最新模型和后期处理方法。我们的代码和视频演示可在<a target="_blank" rel="noopener" href="https://github.com/Andrewwango/ddei%E6%9F%A5%E7%9C%8B%E3%80%82">https://github.com/Andrewwango/ddei查看。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>动态MRI图像序列重建是加快实时成像速度和提高时空分辨率的关键。</li>
<li>传统方法存在周期性假设问题，无法真正捕捉运动状态。</li>
<li>由于无法获取真实的地面全采样视频，监督深度学习方法存在根本问题。</li>
<li>提出了一种无监督框架，利用MRI的自然几何时空等价性进行重建。</li>
<li>动态微分等价成像（DDEI）在高度加速的动态心脏成像上表现优异。</li>
<li>DDEI方法具有通用性，不依赖于特定的神经网络架构，可适应最新模型和后期处理方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08646">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-200761d26c364357d149f355f6d90b6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aed6b48bc99f3f2c2869c8a68ad7e638.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31d7887549efb95ab073af3720c386b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9918b3f607e209965ab7ce93ebccf81.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-level-Asymmetric-Contrastive-Learning-for-Volumetric-Medical-Image-Segmentation-Pre-training"><a href="#Multi-level-Asymmetric-Contrastive-Learning-for-Volumetric-Medical-Image-Segmentation-Pre-training" class="headerlink" title="Multi-level Asymmetric Contrastive Learning for Volumetric Medical Image   Segmentation Pre-training"></a>Multi-level Asymmetric Contrastive Learning for Volumetric Medical Image   Segmentation Pre-training</h2><p><strong>Authors:Shuang Zeng, Lei Zhu, Xinliang Zhang, Micky C Nnamdi, Wenqi Shi, J Ben Tamo, Qian Chen, Hangzhou He, Lujia Jin, Zifeng Tian, Qiushi Ren, Zhaoheng Xie, Yanye Lu</strong></p>
<p>Medical image segmentation is a fundamental yet challenging task due to the arduous process of acquiring large volumes of high-quality labeled data from experts. Contrastive learning offers a promising but still problematic solution to this dilemma. Firstly existing medical contrastive learning strategies focus on extracting image-level representation, which ignores abundant multi-level representations. Furthermore they underutilize the decoder either by random initialization or separate pre-training from the encoder, thereby neglecting the potential collaboration between the encoder and decoder. To address these issues, we propose a novel multi-level asymmetric contrastive learning framework named MACL for volumetric medical image segmentation pre-training. Specifically, we design an asymmetric contrastive learning structure to pre-train encoder and decoder simultaneously to provide better initialization for segmentation models. Moreover, we develop a multi-level contrastive learning strategy that integrates correspondences across feature-level, image-level, and pixel-level representations to ensure the encoder and decoder capture comprehensive details from representations of varying scales and granularities during the pre-training phase. Finally, experiments on 8 medical image datasets indicate our MACL framework outperforms existing 11 contrastive learning strategies. i.e. Our MACL achieves a superior performance with more precise predictions from visualization figures and 1.72%, 7.87%, 2.49% and 1.48% Dice higher than previous best results on ACDC, MMWHS, HVSMR and CHAOS with 10% labeled data, respectively. And our MACL also has a strong generalization ability among 5 variant U-Net backbones. Our code will be released at <a target="_blank" rel="noopener" href="https://github.com/stevezs315/MACL">https://github.com/stevezs315/MACL</a>. </p>
<blockquote>
<p>医学图像分割是一项基础且具有挑战性的任务，主要是因为从专家那里获取大量高质量标记数据的艰巨过程。对比学习为解决这一困境提供了有前景但仍有问题的解决方案。首先，现有的医学对比学习策略侧重于提取图像级别的表示，这忽略了丰富的多层次表示。此外，它们对解码器的利用不足，要么是通过随机初始化，要么是编码器独立进行预训练，从而忽略了编码器和解码器之间的潜在协作。为了解决这些问题，我们提出了一种名为MACL的新型多层次不对称对比学习框架，用于医学图像分割的预训练。具体来说，我们设计了一种不对称对比学习结构，可以同时对编码器和解码器进行预训练，为分割模型提供更好的初始化。此外，我们开发了一种多层次对比学习策略，该策略结合了特征级别、图像级别和像素级别表示之间的对应关系，以确保编码器和解码器在预训练阶段捕获不同规模和粒度的表示中的综合细节。最后，在8个医学图像数据集上的实验表明，我们的MACL框架优于现有的11种对比学习策略。例如，使用我们的MACL，可视化图预测更准确，在ACDC、MMWHS、HVSMR和CHAOS数据集上分别比之前的最佳结果高出1.72%、7.87%、2.49%和1.48%。我们的MACL还具有在5种变体U-Net骨干网中强大的泛化能力。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/stevezs315/MACL%E4%B8%8A%E5%8F%91%E6%98%BE%E3%80%82">https://github.com/stevezs315/MACL上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11876v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>医学图像分割是一项基础且具挑战性的任务，主要由于获取大量高质量专家标注数据的困难。对比学习为此提供了有前景的解决方案，但现有医学对比学习策略主要关注图像级别的表示，忽略了多级别的丰富表示。此外，他们未充分利用解码器，或与编码器进行独立预训练，忽视了二者的协作潜力。针对这些问题，我们提出了名为MACL的体积医学图像分割预训练的多级别不对称对比学习框架。我们设计了不对称对比学习结构，同时预训练编码器和解码器，为分割模型提供更好的初始化。同时，我们发展了多级别对比学习策略，整合特征级别、图像级别和像素级别表示的对应关系，确保编码器和解码器在预训练阶段捕获不同尺度和粒度的综合细节。在8个医学图像数据集上的实验表明，我们的MACL框架优于现有的11种对比学习策略，具有更精确预测和更高的Dice系数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割面临获取高质量标注数据的挑战。</li>
<li>现有对比学习策略主要关注图像级别表示，忽略了多级别表示。</li>
<li>MACL框架通过不对称对比学习结构同时预训练编码器和解码器。</li>
<li>MACL采用多级别对比学习策略，整合不同级别的表示对应关系。</li>
<li>MACL在多个医学图像数据集上表现优越，具有更高的Dice系数和更精确的预测。</li>
<li>MACL框架具有强大的泛化能力，适用于多种U-Net变体。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2309.11876">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5058068fee7b114a97ac48faa8c10d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d9eb742709cda02c2e2ad75df504609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1014e9c06cb0bce3ebf5edf73f6c289b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ccec59a98e20bb864e5b8a69ef88467.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1dbc1710bb8ddeb575b4e1e95fc116b8.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-02-17  TokenSynth A Token-based Neural Synthesizer for Instrument Cloning and   Text-to-Instrument
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-15/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2dcc09f8fe93c0d9bedcec151c2dae43.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-02-15  Diffusing DeBias a Recipe for Turning a Bug into a Feature
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
