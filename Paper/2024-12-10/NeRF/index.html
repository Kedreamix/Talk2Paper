<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Dynamic EventNeRF Reconstructing General Dynamic Scenes from Multi-view   Event Cameras">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-aa0498f2de39d156ab924e917d63e25f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    28.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    116 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Dynamic-EventNeRF-Reconstructing-General-Dynamic-Scenes-from-Multi-view-Event-Cameras"><a href="#Dynamic-EventNeRF-Reconstructing-General-Dynamic-Scenes-from-Multi-view-Event-Cameras" class="headerlink" title="Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view   Event Cameras"></a>Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view   Event Cameras</h2><p><strong>Authors:Viktor Rudnev, Gereon Fox, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik</strong></p>
<p>Volumetric reconstruction of dynamic scenes is an important problem in computer vision. It is especially challenging in poor lighting and with fast motion. It is partly due to the limitations of RGB cameras: To capture fast motion without much blur, the framerate must be increased, which in turn requires more lighting. In contrast, event cameras, which record changes in pixel brightness asynchronously, are much less dependent on lighting, making them more suitable for recording fast motion. We hence propose the first method to spatiotemporally reconstruct a scene from sparse multi-view event streams and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF models, one per short recording segment. The individual segments are supervised with a set of event- and RGB-based losses and sparse-view regularisation. We assemble a real-world multi-view camera rig with six static event cameras around the object and record a benchmark multi-view event stream dataset of challenging motions. Our work outperforms RGB-based baselines, producing state-of-the-art results, and opens up the topic of multi-view event-based reconstruction as a new path for fast scene capture beyond RGB cameras. The code and the data will be released soon at <a target="_blank" rel="noopener" href="https://4dqv.mpi-inf.mpg.de/DynEventNeRF/">https://4dqv.mpi-inf.mpg.de/DynEventNeRF/</a> </p>
<blockquote>
<p>åŠ¨æ€åœºæ™¯çš„ä½“ç§¯é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚åœ¨å…‰çº¿ä¸è¶³å’Œå¿«é€Ÿè¿åŠ¨çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ç”±äºRGBç›¸æœºçš„å±€é™æ€§æ‰€è‡´ï¼šä¸ºäº†åœ¨ä¸æ¨¡ç³Šçš„æƒ…å†µä¸‹æ•æ‰å¿«é€Ÿè¿åŠ¨ï¼Œå¿…é¡»å¢åŠ å¸§ç‡ï¼Œè¿™åè¿‡æ¥åˆéœ€è¦æ›´å¤šçš„å…‰çº¿ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäº‹ä»¶ç›¸æœºèƒ½å¤Ÿå¼‚æ­¥è®°å½•åƒç´ äº®åº¦çš„å˜åŒ–ï¼Œå¯¹å…‰çº¿çš„ä¾èµ–æ€§è¾ƒå°ï¼Œå› æ­¤æ›´é€‚åˆè®°å½•å¿«é€Ÿè¿åŠ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ç§ä»ç¨€ç–çš„å¤šè§†è§’äº‹ä»¶æµå’Œç¨€ç–RGBå¸§ä¸­é‡å»ºåœºæ™¯æ—¶ç©ºä¿¡æ¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç³»åˆ—è·¨æ·¡å…¥æ·¡å‡ºæ—¶é—´æ¡ä»¶çš„NeRFæ¨¡å‹åºåˆ—ï¼Œæ¯ä¸ªçŸ­æœŸè®°å½•ç‰‡æ®µä¸€ä¸ªã€‚å„ä¸ªç‰‡æ®µå—åˆ°äº‹ä»¶å’ŒRGBåŸºæŸå¤±ä»¥åŠç¨€ç–è§†å›¾æ­£åˆ™åŒ–çš„ç›‘ç£ã€‚æˆ‘ä»¬ä½¿ç”¨å…­ä¸ªé™æ€äº‹ä»¶ç›¸æœºå›´ç»•å¯¹è±¡æ„å»ºäº†ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„å¤šè§†è§’ç›¸æœºè£…ç½®ï¼Œå¹¶è®°å½•äº†å…·æœ‰æŒ‘æˆ˜æ€§è¿åŠ¨çš„åŸºå‡†å¤šè§†è§’äº‹ä»¶æµæ•°æ®é›†ã€‚æˆ‘ä»¬çš„å·¥ä½œä¼˜äºåŸºäºRGBçš„åŸºçº¿ï¼Œå–å¾—äº†æœ€æ–°ç»“æœï¼Œå¹¶å¼€å¯äº†åŸºäºå¤šè§†è§’äº‹ä»¶é‡å»ºçš„ä¸»é¢˜ä½œä¸ºè¶…è¶ŠRGBç›¸æœºè¿›è¡Œå¿«é€Ÿåœºæ™¯æ•è·çš„æ–°é€”å¾„ã€‚ä»£ç å’Œæ•°æ®å°†å¾ˆå¿«åœ¨<a target="_blank" rel="noopener" href="https://4dqv.mpi-inf.mpg.de/DynEventNeRF/%E5%8F%91%E5%B8%83%E3%80%82">https://4dqv.mpi-inf.mpg.de/DynEventNeRF/å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06770v1">PDF</a> 15 pages, 11 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºäº‹ä»¶æµå’ŒRGBå¸§çš„æ—¶ç©ºåœºæ™¯é‡å»ºæ–¹æ³•ï¼Œé‡‡ç”¨è·¨æ·¡å…¥çš„æ—¶é—´æ¡ä»¶NeRFæ¨¡å‹åºåˆ—ï¼Œä»¥åº”å¯¹åŠ¨æ€åœºæ™¯åœ¨å¼±å…‰æ¡ä»¶ä¸‹çš„é‡å»ºæŒ‘æˆ˜ã€‚é€šè¿‡ç»“åˆäº‹ä»¶ç›¸æœºè®°å½•çš„åƒç´ äº®åº¦å˜åŒ–ï¼Œè¯¥æ–¹æ³•åœ¨è®°å½•å¿«é€Ÿè¿åŠ¨åœºæ™¯æ—¶å…·æœ‰è¾ƒä½çš„å…‰çº¿ä¾èµ–æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºäºRGBçš„åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†å‰æ²¿çš„é‡å»ºæ•ˆæœï¼Œä¸ºäº‹ä»¶æµçš„å¤šè§†è§’é‡å»ºå¼€å¯äº†æ–°çš„ç ”ç©¶è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨æ€åœºæ™¯çš„ä½“ç§¯é‡å»ºæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼±å…‰å’Œå¿«é€Ÿè¿åŠ¨æ¡ä»¶ä¸‹æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>äº‹ä»¶ç›¸æœºèƒ½å¤Ÿè®°å½•åƒç´ äº®åº¦çš„å¼‚æ­¥å˜åŒ–ï¼Œé€‚åˆå½•åˆ¶å¿«é€Ÿè¿åŠ¨åœºæ™¯ï¼Œç›¸è¾ƒäºRGBç›¸æœºå…·æœ‰è¾ƒä½çš„å…‰çº¿ä¾èµ–æ€§ã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªåŸºäºç¨€ç–å¤šè§†è§’äº‹ä»¶æµå’ŒRGBå¸§çš„æ—¶ç©ºåœºæ™¯é‡å»ºæ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨è·¨æ·¡å…¥çš„æ—¶é—´æ¡ä»¶NeRFæ¨¡å‹åºåˆ—è¿›è¡Œå»ºæ¨¡ï¼Œå¯¹ä¸ªåˆ«æ®µè½è¿›è¡ŒåŸºäºäº‹ä»¶å’ŒRGBçš„æŸå¤±ä»¥åŠç¨€ç–è§†å›¾æ­£åˆ™åŒ–çš„ç›‘ç£ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„å¤šè§†è§’ç›¸æœºç³»ç»Ÿï¼ŒåŒ…å«å…­ä¸ªé™æ€äº‹ä»¶ç›¸æœºï¼Œå¹¶å½•åˆ¶äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§è¿åŠ¨çš„å¤šè§†è§’äº‹ä»¶æµæ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºåŸºäºRGBçš„åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†å…ˆè¿›çš„é‡å»ºç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2fc719d7140ad09f48c46ba998d1addd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5602cf7521b57a90000f4446c519557d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2583caff942fc23ea89b2560c2fb15e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-444f5eb29ae9fe5ff4d3ba85d51befe9.jpg" align="middle">
</details>




<h2 id="Deblur4DGS-4D-Gaussian-Splatting-from-Blurry-Monocular-Video"><a href="#Deblur4DGS-4D-Gaussian-Splatting-from-Blurry-Monocular-Video" class="headerlink" title="Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video"></a>Deblur4DGS: 4D Gaussian Splatting from Blurry Monocular Video</h2><p><strong>Authors:Renlong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, Wangmeng Zuo</strong></p>
<p>Recent 4D reconstruction methods have yielded impressive results but rely on sharp videos as supervision. However, motion blur often occurs in videos due to camera shake and object movement, while existing methods render blurry results when using such videos for reconstructing 4D models. Although a few NeRF-based approaches attempted to address the problem, they struggled to produce high-quality results, due to the inaccuracy in estimating continuous dynamic representations within the exposure time. Encouraged by recent works in 3D motion trajectory modeling using 3D Gaussian Splatting (3DGS), we suggest taking 3DGS as the scene representation manner, and propose the first 4D Gaussian Splatting framework to reconstruct a high-quality 4D model from blurry monocular video, named Deblur4DGS. Specifically, we transform continuous dynamic representations estimation within an exposure time into the exposure time estimation. Moreover, we introduce exposure regularization to avoid trivial solutions, as well as multi-frame and multi-resolution consistency ones to alleviate artifacts. Furthermore, to better represent objects with large motion, we suggest blur-aware variable canonical Gaussians. Beyond novel-view synthesis, Deblur4DGS can be applied to improve blurry video from multiple perspectives, including deblurring, frame interpolation, and video stabilization. Extensive experiments on the above four tasks show that Deblur4DGS outperforms state-of-the-art 4D reconstruction methods. The codes are available at <a target="_blank" rel="noopener" href="https://github.com/ZcsrenlongZ/Deblur4DGS">https://github.com/ZcsrenlongZ/Deblur4DGS</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„4Dé‡å»ºæ–¹æ³•å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬ä¾èµ–äºæ¸…æ™°è§†é¢‘ä½œä¸ºç›‘ç£ã€‚ç„¶è€Œï¼Œç”±äºç›¸æœºæŠ–åŠ¨å’Œç‰©ä½“ç§»åŠ¨ï¼Œè§†é¢‘ä¸­çš„è¿åŠ¨æ¨¡ç³Šç»å¸¸å‘ç”Ÿï¼Œè€Œå½“ä½¿ç”¨æ­¤ç±»è§†é¢‘è¿›è¡Œ4Dæ¨¡å‹é‡å»ºæ—¶ï¼Œç°æœ‰æ–¹æ³•ä¼šäº§ç”Ÿæ¨¡ç³Šçš„ç»“æœã€‚å°½ç®¡æœ‰å‡ ç§åŸºäºNeRFçš„æ–¹æ³•è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºåœ¨æ›å…‰æ—¶é—´å†…ä¼°è®¡è¿ç»­åŠ¨æ€è¡¨ç¤ºçš„ä¸å‡†ç¡®æ€§ï¼Œå®ƒä»¬å¾ˆéš¾äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœã€‚å—åˆ°è¿‘æœŸä½¿ç”¨3Dé«˜æ–¯æ¶‚æŠ¹ï¼ˆ3DGSï¼‰è¿›è¡Œ3Dè¿åŠ¨è½¨è¿¹å»ºæ¨¡çš„å·¥ä½œçš„å¯å‘ï¼Œæˆ‘ä»¬å»ºè®®å°†3DGSä½œä¸ºåœºæ™¯è¡¨ç¤ºæ–¹å¼ï¼Œå¹¶æå‡ºç¬¬ä¸€ä¸ª4Dé«˜æ–¯æ¶‚æŠ¹æ¡†æ¶ï¼Œå¯ä»¥ä»æ¨¡ç³Šçš„å•ç›®è§†é¢‘ä¸­é‡å»ºé«˜è´¨é‡çš„4Dæ¨¡å‹ï¼Œåä¸ºDeblur4DGSã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ›å…‰æ—¶é—´å†…è¿ç»­åŠ¨æ€è¡¨ç¤ºçš„ä¼°è®¡è½¬åŒ–ä¸ºæ›å…‰æ—¶é—´ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ›å…‰æ­£åˆ™åŒ–ä»¥é¿å…å¹³å‡¡è§£ï¼Œä»¥åŠå¤šå¸§å’Œå¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§æ¥è§£å†³ä¼ªå½±é—®é¢˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ›´å¥½åœ°è¡¨ç¤ºå…·æœ‰å¤§è¿åŠ¨çš„ç‰©ä½“ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨æ¨¡ç³Šæ„ŸçŸ¥å¯å˜è§„èŒƒé«˜æ–¯ã€‚é™¤äº†æ–°å‹è§†å›¾åˆæˆå¤–ï¼ŒDeblur4DGSè¿˜å¯ä»¥åº”ç”¨äºä»å¤šä¸ªè§’åº¦æé«˜æ¨¡ç³Šè§†é¢‘çš„è´¨é‡ï¼ŒåŒ…æ‹¬å»æ¨¡ç³Šã€å¸§å†…æ’å€¼å’Œè§†é¢‘ç¨³å®šã€‚åœ¨ä»¥ä¸Šå››ä¸ªä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDeblur4DGSä¼˜äºæœ€æ–°çš„4Dé‡å»ºæ–¹æ³•ã€‚ä»£ç å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZcsrenlongZ/Deblur4DGS%E3%80%82">https://github.com/ZcsrenlongZ/Deblur4DGSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06424v1">PDF</a> 17 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDeblur4DGSçš„4Dé«˜æ–¯å–·ç»˜æ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ¨¡ç³Šçš„å•ç›®è§†é¢‘ä¸­é‡å»ºé«˜è´¨é‡çš„4Dæ¨¡å‹ã€‚é€šè¿‡é‡‡ç”¨3Dé«˜æ–¯å–·ç»˜ä½œä¸ºåœºæ™¯è¡¨ç°æ–¹å¼ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ¨¡ç³Šè§†é¢‘ä¸‹é‡å»º4Dæ¨¡å‹æ—¶äº§ç”Ÿçš„é—®é¢˜ã€‚é€šè¿‡ä¼°è®¡æ›å…‰æ—¶é—´æ¥è§£å†³è¿ç»­åŠ¨æ€è¡¨ç¤ºçš„ä¼°è®¡é—®é¢˜ï¼Œå¼•å…¥æ›å…‰æ­£åˆ™åŒ–ä»¥åŠå¤šå¸§å’Œå¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§æ¥å‡å°‘ä¼ªå½±ã€‚æ­¤å¤–ï¼Œå¯¹äºå¤§åŠ¨ä½œç‰©ä½“æœ‰æ›´å¥½çš„è¡¨ç°èƒ½åŠ›ã€‚é™¤äº†æ–°å‹è§†å›¾åˆæˆå¤–ï¼ŒDeblur4DGSè¿˜å¯åº”ç”¨äºå»æ¨¡ç³Šã€å¸§æ’å€¼å’Œè§†é¢‘ç¨³å®šç­‰å¤šä¸ªæ–¹é¢ã€‚å®éªŒè¡¨æ˜ï¼ŒDeblur4DGSåœ¨å››é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æœ€ä½³4Dé‡å»ºæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Deblur4DGSæ˜¯é¦–ä¸ªèƒ½å¤Ÿä»æ¨¡ç³Šçš„å•ç›®è§†é¢‘ä¸­é‡å»ºé«˜è´¨é‡4Dæ¨¡å‹çš„æ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨3Dé«˜æ–¯å–·ç»˜ä½œä¸ºåœºæ™¯è¡¨ç°æ–¹å¼ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æ¨¡ç³Šè§†é¢‘ä¸‹çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ä¼°è®¡æ›å…‰æ—¶é—´è§£å†³è¿ç»­åŠ¨æ€è¡¨ç¤ºçš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ›å…‰æ­£åˆ™åŒ–ã€å¤šå¸§å’Œå¤šåˆ†è¾¨ç‡ä¸€è‡´æ€§æ¥å‡å°‘ä¼ªå½±ã€‚</li>
<li>é€‚ç”¨äºå¤§åŠ¨ä½œç‰©ä½“çš„è¡¨ç°èƒ½åŠ›æ›´å¼ºã€‚</li>
<li>é™¤äº†æ–°å‹è§†å›¾åˆæˆï¼ŒDeblur4DGSè¿˜å¯åº”ç”¨äºå»æ¨¡ç³Šã€å¸§æ’å€¼å’Œè§†é¢‘ç¨³å®šç­‰ä»»åŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb4709786b9ef5c49feda7de5a777dbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35024d82687819b373aaa935de3f9899.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6bd3ba1717d7318727ec5abbb2a4e394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b69bf3ecf99c8bc81f5bf524de313771.jpg" align="middle">
</details>




<h2 id="Exploring-the-Impact-of-Synthetic-Data-on-Human-Gesture-Recognition-Tasks-Using-GANs"><a href="#Exploring-the-Impact-of-Synthetic-Data-on-Human-Gesture-Recognition-Tasks-Using-GANs" class="headerlink" title="Exploring the Impact of Synthetic Data on Human Gesture Recognition   Tasks Using GANs"></a>Exploring the Impact of Synthetic Data on Human Gesture Recognition   Tasks Using GANs</h2><p><strong>Authors:George Kontogiannis, Pantelis Tzamalis, Sotiris Nikoletseas</strong></p>
<p>In the evolving domain of Human Activity Recognition (HAR) using Internet of Things (IoT) devices, there is an emerging interest in employing Deep Generative Models (DGMs) to address data scarcity, enhance data quality, and improve classification metrics scores. Among these types of models, Generative Adversarial Networks (GANs) have arisen as a powerful tool for generating synthetic data that mimic real-world scenarios with high fidelity. However, Human Gesture Recognition (HGR), a subset of HAR, particularly in healthcare applications, using time series data such as allergic gestures, remains highly unexplored.   In this paper, we examine and evaluate the performance of two GANs in the generation of synthetic gesture motion data that compose a part of an open-source benchmark dataset. The data is related to the disease identification domain and healthcare, specifically to allergic rhinitis. We also focus on these AI modelsâ€™ performance in terms of fidelity, diversity, and privacy. Furthermore, we examine the scenario if the synthetic data can substitute real data, in training scenarios and how well models trained on synthetic data can be generalized for the allergic rhinitis gestures. In our work, these gestures are related to 6-axes accelerometer and gyroscope data, serving as multi-variate time series instances, and retrieved from smart wearable devices. To the best of our knowledge, this study is the first to explore the feasibility of synthesizing motion gestures for allergic rhinitis from wearable IoT device data using Generative Adversarial Networks (GANs) and testing their impact on the generalization of gesture recognition systems. It is worth noting that, even if our method has been applied to a specific category of gestures, it is designed to be generalized and can be deployed also to other motion data in the HGR domain. </p>
<blockquote>
<p>åœ¨äººç±»æ´»åŠ¨è¯†åˆ«ï¼ˆHARï¼‰ä¸æ–­è¿›åŒ–çš„é¢†åŸŸä¸­ï¼Œåˆ©ç”¨ç‰©è”ç½‘ï¼ˆIoTï¼‰è®¾å¤‡ï¼Œäººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£é‡‡ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼ˆDGMsï¼‰æ¥è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæé«˜æ•°æ®è´¨é‡ï¼Œå¹¶æ”¹å–„åˆ†ç±»æŒ‡æ ‡å¾—åˆ†ã€‚åœ¨è¿™äº›ç±»å‹çš„æ¨¡å‹ä¸­ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å·²æˆä¸ºç”Ÿæˆåˆæˆæ•°æ®çš„å¼ºå¤§å·¥å…·ï¼Œè¿™äº›åˆæˆæ•°æ®ä»¥é«˜ä¿çœŸåº¦æ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚ç„¶è€Œï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­ï¼Œä½¿ç”¨æ—¶é—´åºåˆ—æ•°æ®å¦‚è¿‡æ•æ€§æ‰‹åŠ¿è¿›è¡Œçš„äººç±»æ‰‹åŠ¿è¯†åˆ«ï¼ˆHGRï¼‰ä»æ˜¯HARçš„ä¸€ä¸ªå­é›†ï¼Œå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06389v1">PDF</a> 8 pages, 5 figures, 20th International Conference on Distributed   Computing in Smart Systems and the Internet of Things (DCOSS-IoT), 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨ç‰©è”ç½‘è®¾å¤‡åœ¨äººä½“æ´»åŠ¨è¯†åˆ«é¢†åŸŸä¸­çš„æ–°å…´æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†æ‰‹åŠ¿è¯†åˆ«ä¸­çš„åº”ç”¨ã€‚ç‰¹åˆ«æ˜¯é’ˆå¯¹è¿‡æ•æ€§é¼»ç‚ç­‰ç–¾ç—…çš„å§¿æ€è¯†åˆ«ï¼Œç ”ç©¶åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆåˆæˆå§¿æ€è¿åŠ¨æ•°æ®ï¼Œè¯„ä¼°å…¶æ€§èƒ½ã€‚ç ”ç©¶å‘ç°åˆæˆæ•°æ®å¯ä»¥æ›¿ä»£çœŸå®æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶æœ‰æœ›æé«˜æ¨¡å‹åœ¨è¿‡æ•æ€§é¼»æ‰‹åŠ¿è¯†åˆ«æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œåˆæˆè¿‡æ•æ€§é¼»æ‰‹åŠ¿æ•°æ®çš„å¯è¡Œæ€§å¹¶æµ‹è¯•äº†å¯¹æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿçš„å½±å“ã€‚è¿™é¡¹æŠ€æœ¯çš„è®¾è®¡ç›®çš„ä¸ä»…æ˜¯åº”ç”¨äºç‰¹å®šçš„ç–¾ç—…ç±»å‹ï¼Œä¹Ÿé€‚ç”¨äºå…¶ä»–é¢†åŸŸçš„è¿åŠ¨å§¿æ€è¯†åˆ«ã€‚æœ¬æ–‡çš„åˆ†æé›†ä¸­äºæ•°æ®è´¨é‡ã€éšç§ä¿æŠ¤å’Œæ¨¡å‹çš„æ€§èƒ½è¯„ä¼°ç­‰æ–¹é¢ã€‚è¯¥æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºæé«˜äº†æ•°æ®è´¨é‡å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰åŠ©äºè§£å†³å®é™…åº”ç”¨ä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ç ”ç©¶ä¸ºè¯¥é¢†åŸŸçš„å‘å±•æä¾›äº†é‡è¦çš„è§è§£å’Œå¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨ç‰©è”ç½‘è®¾å¤‡åœ¨äººä½“æ´»åŠ¨è¯†åˆ«é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯æ‰‹åŠ¿è¯†åˆ«ã€‚</li>
<li>é‡‡ç”¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥ç”Ÿæˆåˆæˆå§¿æ€è¿åŠ¨æ•°æ®ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨è¿‡æ•æ€§é¼»æ‰‹åŠ¿è¯†åˆ«æ–¹é¢ï¼Œä½†è¯¥æŠ€æœ¯é€‚ç”¨äºå…¶ä»–è¿åŠ¨å§¿æ€è¯†åˆ«é¢†åŸŸã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†åˆæˆæ•°æ®çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬çœŸå®æ•°æ®æ›¿ä»£è®­ç»ƒçš„å¯è¡Œæ€§åŠæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢äº†åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œåˆæˆè¿‡æ•æ€§é¼»æ‰‹åŠ¿æ•°æ®çš„å¯è¡Œæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1fb8e5a3129f31f373458ddc6a205c0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d70a432dc0c4eb7c886d8666bb41c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0147b021b54311d6f02662da630a76f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea8e23a48ba1a47c507f5bcf6f9a852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d016c1a6f943bcbac939ab1f5d6e913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29e4564d11b6f5a33951a6e3e462b25a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-097b25674f4fd539aa6b73dc18b992d4.jpg" align="middle">
</details>




<h2 id="A-Tiered-GAN-Approach-for-Monet-Style-Image-Generation"><a href="#A-Tiered-GAN-Approach-for-Monet-Style-Image-Generation" class="headerlink" title="A Tiered GAN Approach for Monet-Style Image Generation"></a>A Tiered GAN Approach for Monet-Style Image Generation</h2><p><strong>Authors:FNU Neha, Deepshikha Bhati, Deepak Kumar Shukla, Md Amiruzzaman</strong></p>
<p>Generative Adversarial Networks (GANs) have proven to be a powerful tool in generating artistic images, capable of mimicking the styles of renowned painters, such as Claude Monet. This paper introduces a tiered GAN model to progressively refine image quality through a multi-stage process, enhancing the generated images at each step. The model transforms random noise into detailed artistic representations, addressing common challenges such as instability in training, mode collapse, and output quality. This approach combines downsampling and convolutional techniques, enabling the generation of high-quality Monet-style artwork while optimizing computational efficiency. Experimental results demonstrate the architectureâ€™s ability to produce foundational artistic structures, though further refinements are necessary for achieving higher levels of realism and fidelity to Monetâ€™s style. Future work focuses on improving training methodologies and model complexity to bridge the gap between generated and true artistic images. Additionally, the limitations of traditional GANs in artistic generation are analyzed, and strategies to overcome these shortcomings are proposed. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å·²è¢«è¯æ˜æ˜¯ç”Ÿæˆè‰ºæœ¯å›¾åƒçš„å¼ºå¤§å·¥å…·ï¼Œèƒ½å¤Ÿæ¨¡ä»¿è‘—åç”»å®¶ï¼ˆå¦‚å…‹åŠ³å¾·Â·è«å¥ˆï¼‰çš„é£æ ¼ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ†å±‚GANæ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹é€æ­¥æ”¹è¿›å›¾åƒè´¨é‡ï¼Œæ¯ä¸€æ­¥éƒ½å¢å¼ºç”Ÿæˆçš„å›¾åƒã€‚è¯¥æ¨¡å‹å°†éšæœºå™ªå£°è½¬æ¢ä¸ºè¯¦ç»†çš„è‰ºæœ¯è¡¨ç¤ºå½¢å¼ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒå’Œè¾“å‡ºè´¨é‡ç­‰å¸¸è§æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é™é‡‡æ ·å’Œå·ç§¯æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è«å¥ˆé£æ ¼çš„è‰ºæœ¯ä½œå“ï¼ŒåŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¶æ„ç”ŸæˆåŸºç¡€è‰ºæœ¯ç»“æ„çš„èƒ½åŠ›ï¼Œä½†è¦å®ç°æ›´é«˜ç¨‹åº¦çš„é€¼çœŸåº¦å’Œå¯¹è«å¥ˆé£æ ¼çš„å¿ å®åº¦ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æœªæ¥çš„å·¥ä½œé‡ç‚¹æ˜¯é€šè¿‡æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹å¤æ‚æ€§æ¥ç¼©å°ç”Ÿæˆçš„è‰ºæœ¯å›¾åƒå’ŒçœŸæ­£çš„è‰ºæœ¯å›¾åƒä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œè¿˜åˆ†æäº†ä¼ ç»ŸGANåœ¨è‰ºæœ¯åˆ›ä½œä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†å…‹æœè¿™äº›ä¸è¶³çš„å¯¹ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05724v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å±‚æ¬¡åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹é€æ­¥ä¼˜åŒ–å›¾åƒè´¨é‡ã€‚æ¨¡å‹èƒ½å¤Ÿå°†éšæœºå™ªå£°è½¬åŒ–ä¸ºè¯¦ç»†çš„è‰ºæœ¯è¡¨ç¤ºå½¢å¼ï¼Œè§£å†³è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒå’Œè¾“å‡ºè´¨é‡å·®ç­‰å¸¸è§é—®é¢˜ã€‚é€šè¿‡ç»“åˆä¸‹é‡‡æ ·å’Œå·ç§¯æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è«å¥ˆé£æ ¼è‰ºæœ¯ä½œå“ï¼ŒåŒæ—¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿç”ŸæˆåŸºç¡€è‰ºæœ¯ç»“æ„ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ä»¥å®ç°æ›´é«˜æ°´å¹³çš„çœŸå®æ„Ÿå’Œè«å¥ˆé£æ ¼çš„å¿ å®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„åˆ†å±‚æ¨¡å‹ï¼Œç”¨äºé€æ­¥ä¼˜åŒ–å›¾åƒè´¨é‡ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿå°†éšæœºå™ªå£°è½¬åŒ–ä¸ºè¯¦ç»†çš„è‰ºæœ¯è¡¨ç¤ºï¼Œè§£å†³è®­ç»ƒä¸­çš„ä¸ç¨³å®šæ€§å’Œæ¨¡å¼å´©æºƒé—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆä¸‹é‡‡æ ·å’Œå·ç§¯æŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è«å¥ˆé£æ ¼è‰ºæœ¯ä½œå“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¶æ„å…·å¤‡ç”ŸæˆåŸºç¡€è‰ºæœ¯ç»“æ„çš„èƒ½åŠ›ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ä»¥æé«˜çœŸå®æ„Ÿå’Œå¿ å®åº¦ã€‚</li>
<li>æœªæ¥å·¥ä½œæ–¹å‘åŒ…æ‹¬æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹å¤æ‚æ€§ï¼Œä»¥ç¼©å°ç”Ÿæˆå›¾åƒå’ŒçœŸå®è‰ºæœ¯ä½œå“ä¹‹é—´çš„å·®è·ã€‚</li>
<li>è®ºæ–‡åˆ†æäº†ä¼ ç»ŸGANsåœ¨è‰ºæœ¯åˆ›ä½œæ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†å…‹æœè¿™äº›ä¸è¶³çš„ç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ae3beb2f5cfa86192d14b90205a6aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a308208650edb032417369c9d7213930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd75fcdb7682802b443fe46aa38eb8d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2dfaef8f81b410f940eeb328ac97223.jpg" align="middle">
</details>




<h2 id="Perturb-and-Revise-Flexible-3D-Editing-with-Generative-Trajectories"><a href="#Perturb-and-Revise-Flexible-3D-Editing-with-Generative-Trajectories" class="headerlink" title="Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories"></a>Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories</h2><p><strong>Authors:Susung Hong, Johanna Karras, Ricardo Martin-Brualla, Ira Kemelmacher-Shlizerman</strong></p>
<p>The fields of 3D reconstruction and text-based 3D editing have advanced significantly with the evolution of text-based diffusion models. While existing 3D editing methods excel at modifying color, texture, and style, they struggle with extensive geometric or appearance changes, thus limiting their applications. We propose Perturb-and-Revise, which makes possible a variety of NeRF editing. First, we perturb the NeRF parameters with random initializations to create a versatile initialization. We automatically determine the perturbation magnitude through analysis of the local loss landscape. Then, we revise the edited NeRF via generative trajectories. Combined with the generative process, we impose identity-preserving gradients to refine the edited NeRF. Extensive experiments demonstrate that Perturb-and-Revise facilitates flexible, effective, and consistent editing of color, appearance, and geometry in 3D. For 360{\deg} results, please visit our project page: <a target="_blank" rel="noopener" href="https://susunghong.github.io/Perturb-and-Revise">https://susunghong.github.io/Perturb-and-Revise</a>. </p>
<blockquote>
<p>éšç€åŸºäºæ–‡æœ¬çš„æ‰©æ•£æ¨¡å‹çš„æ¼”å˜ï¼Œ3Dé‡å»ºå’ŒåŸºäºæ–‡æœ¬çš„3Dç¼–è¾‘é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚è™½ç„¶ç°æœ‰çš„3Dç¼–è¾‘æ–¹æ³•åœ¨ä¿®æ”¹é¢œè‰²ã€çº¹ç†å’Œé£æ ¼æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¿›è¡Œå¤§è§„æ¨¡çš„å‡ ä½•æˆ–å¤–è§‚æ›´æ”¹æ—¶å´é‡åˆ°äº†å›°éš¾ï¼Œä»è€Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†Perturb-and-Reviseæ–¹æ³•ï¼Œä½¿å„ç§NeRFç¼–è¾‘æˆä¸ºå¯èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡éšæœºåˆå§‹åŒ–æ‰°åŠ¨NeRFå‚æ•°æ¥åˆ›å»ºä¸€ä¸ªé€šç”¨çš„åˆå§‹åŒ–æ–¹æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡åˆ†æå±€éƒ¨æŸå¤±æ™¯è§‚è‡ªåŠ¨ç¡®å®šæ‰°åŠ¨å¹…åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ç”Ÿæˆè½¨è¿¹æ¥ä¿®è®¢ç¼–è¾‘åçš„NeRFã€‚ç»“åˆç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬æ–½åŠ ä¿æŒèº«ä»½ä¸å˜çš„æ¢¯åº¦æ¥ä¼˜åŒ–ç¼–è¾‘åçš„NeRFã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPerturb-and-Reviseèƒ½å¤Ÿå®ç°çµæ´»ã€æœ‰æ•ˆä¸”ä¸€è‡´çš„3Då½©è‰²ã€å¤–è§‚å’Œå‡ ä½•ç¼–è¾‘ã€‚æœ‰å…³360Â°çš„ç»“æœï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://susunghong.github.io/Perturb-and-Revise">https://susunghong.github.io/Perturb-and-Revise</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05279v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://susunghong.github.io/Perturb-and-Revise">https://susunghong.github.io/Perturb-and-Revise</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„3Dé‡å»ºå’Œæ–‡æœ¬ä¸ºåŸºç¡€çš„3Dç¼–è¾‘é¢†åŸŸçš„æœ€æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰3Dç¼–è¾‘æ–¹æ³•åœ¨å‡ ä½•æˆ–å¤–è§‚å˜åŒ–æ–¹é¢çš„å±€é™ï¼Œæå‡ºä¸€ç§åä¸ºPerturb-and-Reviseçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ‰°åŠ¨NeRFå‚æ•°å¹¶è‡ªåŠ¨ç¡®å®šæ‰°åŠ¨å¹…åº¦ï¼Œå®ç°äº†çµæ´»çš„3Dç¼–è¾‘ï¼Œå¯ä»¥ä¿®æ”¹é¢œè‰²ã€å¤–è§‚å’Œå‡ ä½•å½¢çŠ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„æ¼”è¿›æ¨åŠ¨äº†3Dé‡å»ºå’Œæ–‡æœ¬åŸºç¡€çš„3Dç¼–è¾‘é¢†åŸŸçš„å‘å±•ã€‚</li>
<li>ç°æœ‰3Dç¼–è¾‘æ–¹æ³•åœ¨å‡ ä½•æˆ–å¤–è§‚å˜åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>Perturb-and-Reviseæ–¹æ³•é€šè¿‡æ‰°åŠ¨NeRFå‚æ•°ï¼Œå®ç°äº†å¤šæ ·åŒ–çš„3Dç¼–è¾‘ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è‡ªåŠ¨ç¡®å®šæ‰°åŠ¨å¹…åº¦ï¼Œæé«˜äº†ç¼–è¾‘çš„çµæ´»æ€§ã€æ•ˆæœå’Œä¸€è‡´æ€§ã€‚</li>
<li>Perturb-and-Reviseæ–¹æ³•å¯ä»¥ä¿®æ”¹é¢œè‰²ã€å¤–è§‚å’Œå‡ ä½•å½¢çŠ¶ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç”Ÿæˆè¿‡ç¨‹ï¼Œé€šè¿‡æ–½åŠ èº«ä»½ä¿ç•™æ¢¯åº¦æ¥ä¼˜åŒ–ç¼–è¾‘çš„NeRFã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ed9cbc0a09c82dd9bf278132662b787.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eafc4d5431be7b8e059e53f46146868a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4515838293630e7ea6e6f78d569a2b30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d2226da7f9eab85a012516dff139676.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28cfb95251b0fa103b9ca62ecdc4229.jpg" align="middle">
</details>




<h2 id="MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting"><a href="#MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting" class="headerlink" title="MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting"></a>MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu</strong></p>
<p>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>. </p>
<blockquote>
<p>é‡å»ºé«˜ä¿çœŸ3Då¤´åƒå¯¹äºè™šæ‹Ÿç°å®ç­‰å„ç§åº”ç”¨è‡³å…³é‡è¦ã€‚å¼€åˆ›æ€§çš„æ–¹æ³•ä½¿ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰é‡å»ºé€¼çœŸçš„å¤´åƒï¼Œä½†å—é™äºè®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦ã€‚åŸºäº3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰çš„æœ€è¿‘çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è®­ç»ƒå’Œæ¸²æŸ“çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œ3DGSçš„è¡¨é¢ä¸ä¸€è‡´å¯¼è‡´å‡ ä½•ç²¾åº¦ä¸ä½³ï¼›åæ¥çš„2DGSä½¿ç”¨2Dè¡¨é¢æé«˜äº†å‡ ä½•ç²¾åº¦ï¼Œä½†ç‰ºç‰²äº†æ¸²æŸ“ä¿çœŸåº¦ã€‚ä¸ºäº†ç»“åˆ2DGSå’Œ3DGSçš„ä¼˜ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMixedGaussianAvatarçš„æ–°æ–¹æ³•ï¼Œç”¨äºçœŸå®ä¸”å‡ ä½•ç²¾ç¡®çš„å¤´åƒé‡å»ºã€‚æˆ‘ä»¬çš„ä¸»è¦æƒ³æ³•æ˜¯åˆ©ç”¨2Dé«˜æ–¯é‡å»º3Då¤´åƒçš„è¡¨é¢ï¼Œä»¥ç¡®ä¿å‡ ä½•ç²¾åº¦ã€‚æˆ‘ä»¬å°†2Dé«˜æ–¯é™„åŠ åˆ°FLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼ä¸Šï¼Œå¹¶åœ¨2DGSæ¸²æŸ“è´¨é‡ä¸è¶³çš„åœ°æ–¹è¿æ¥åˆ°é¢å¤–çš„3Dé«˜æ–¯ï¼Œåˆ›å»ºæ··åˆçš„2D-3Dé«˜æ–¯è¡¨ç¤ºã€‚è¿™äº›2D-3Dé«˜æ–¯å¯ä»¥ä½¿ç”¨FLAMEå‚æ•°è¿›è¡ŒåŠ¨ç”»å¤„ç†ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè®­ç»ƒ2Dé«˜æ–¯ï¼Œç„¶åå¯¹æ··åˆçš„2D-3Dé«˜æ–¯è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®éªŒå±•ç¤ºäº†MixedGaussianAvatarçš„ä¼˜è¶Šæ€§ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04955v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://chenvoid.github.io/MGA/">https://chenvoid.github.io/MGA/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMixedGaussianAvatarçš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†3Dé«˜æ–¯å¹³é“ºï¼ˆ3DGSï¼‰å’ŒäºŒç»´é«˜æ–¯å¹³é“ºï¼ˆ2DGSï¼‰çš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨é‡å»ºå…·æœ‰çœŸå®æ„Ÿå’Œå‡ ä½•ç²¾åº¦çš„å¤´éƒ¨è™šæ‹Ÿå½¢è±¡ã€‚MixedGaussianAvataråˆ©ç”¨äºŒç»´é«˜æ–¯é‡å»ºå¤´éƒ¨ä¸‰ç»´è¡¨é¢çš„å‡ ä½•ç²¾åº¦ï¼Œå¹¶é€šè¿‡é™„åŠ åˆ°FLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼å’Œè¿æ¥é¢å¤–çš„ä¸‰ç»´é«˜æ–¯æ¥æé«˜æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜é‡‡ç”¨äº†ä¸€ç§æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œå…ˆè®­ç»ƒäºŒç»´é«˜æ–¯ï¼Œå†å¾®è°ƒæ··åˆçš„äºŒç»´-ä¸‰ç»´é«˜æ–¯ã€‚è¯¥æ–¹æ³•åœ¨å®éªŒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†MixedGaussianAvataræ–¹æ³•ï¼Œç»“åˆäº†äºŒç»´é«˜æ–¯å¹³é“ºå’Œä¸‰ç»´é«˜æ–¯å¹³é“ºçš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨é‡å»ºçœŸå®ä¸”å‡ ä½•ç²¾ç¡®çš„å¤´éƒ¨è™šæ‹Ÿå½¢è±¡ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨äºŒç»´é«˜æ–¯é‡å»ºå¤´éƒ¨ä¸‰ç»´è¡¨é¢çš„å‡ ä½•ç²¾åº¦ï¼Œé€šè¿‡é™„åŠ åˆ°FLAMEæ¨¡å‹çš„ä¸‰è§’ç½‘æ ¼å®ç°ã€‚</li>
<li>ä¸ºäº†æé«˜æ¸²æŸ“è´¨é‡ï¼Œåœ¨éœ€è¦çš„åœ°æ–¹è¿æ¥äº†é¢å¤–çš„ä¸‰ç»´é«˜æ–¯ã€‚</li>
<li>MixedGaussianAvataræ–¹æ³•é‡‡ç”¨äº†æ¸è¿›çš„è®­ç»ƒç­–ç•¥ï¼Œå…ˆè®­ç»ƒäºŒç»´é«˜æ–¯ï¼Œå†å¾®è°ƒæ··åˆçš„äºŒç»´-ä¸‰ç»´é«˜æ–¯ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®éªŒä¸­çš„è¡¨ç°å“è¶Šï¼Œèƒ½å¤Ÿé‡å»ºå‡ºé«˜è´¨é‡çš„å¤´éƒ¨è™šæ‹Ÿå½¢è±¡ã€‚</li>
<li>ç ”ç©¶æˆæœå°†åœ¨GitHubä¸Šå‘å¸ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-34f42332d014b46369069fd2d1d3a994.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ef5787956810f1e111d21adf0bdcf5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce4f964cf25207a6db5a28f7f85bd755.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7e93cc4f1cccfe010d043da886dc390.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d0573a7ab1441e50500057923302b87.jpg" align="middle">
</details>




<h2 id="Few-Shot-Learning-with-Adaptive-Weight-Masking-in-Conditional-GANs"><a href="#Few-Shot-Learning-with-Adaptive-Weight-Masking-in-Conditional-GANs" class="headerlink" title="Few-Shot Learning with Adaptive Weight Masking in Conditional GANs"></a>Few-Shot Learning with Adaptive Weight Masking in Conditional GANs</h2><p><strong>Authors:Jiacheng Hu, Zhen Qi, Jianjun Wei, Jiajing Chen, Runyuan Bao, Xinyu Qiu</strong></p>
<p>Deep learning has revolutionized various fields, yet its efficacy is hindered by overfitting and the requirement of extensive annotated data, particularly in few-shot learning scenarios where limited samples are available. This paper introduces a novel approach to few-shot learning by employing a Residual Weight Masking Conditional Generative Adversarial Network (RWM-CGAN) for data augmentation. The proposed model integrates residual units within the generator to enhance network depth and sample quality, coupled with a weight mask regularization technique in the discriminator to improve feature learning from small-sample categories. This method addresses the core issues of robustness and generalization in few-shot learning by providing a controlled and clear augmentation of the sample space. Extensive experiments demonstrate that RWM-CGAN not only expands the sample space effectively but also enriches the diversity and quality of generated samples, leading to significant improvements in detection and classification accuracy on public datasets. The paper contributes to the advancement of few-shot learning by offering a practical solution to the challenges posed by data scarcity and the need for rapid generalization to new tasks or categories. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»é©æ–°äº†å¤šä¸ªé¢†åŸŸï¼Œä½†å…¶æ•ˆæœå—åˆ°äº†è¿‡æ‹Ÿåˆå’Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬æ•°é‡æœ‰é™çš„åœºæ™¯ä¸‹çš„å­¦ä¹ æ›´å°‘çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ©ç”¨å‰©ä½™æƒé‡æ©ç æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆRWM-CGANï¼‰è¿›è¡Œæ•°æ®å¢å¼ºçš„æ–°å‹å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨ç”Ÿæˆå™¨ä¸­é›†æˆæ®‹å·®å•å…ƒï¼Œä»¥æé«˜ç½‘ç»œæ·±åº¦å’Œæ ·æœ¬è´¨é‡ï¼ŒåŒæ—¶åœ¨é‰´åˆ«å™¨ä¸­é‡‡ç”¨æƒé‡æ©ç æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜å°æ ·æœ¬ç±»åˆ«çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æä¾›å¯æ§ä¸”æ¸…æ™°çš„æ ·æœ¬ç©ºé—´æ‰©å±•æ¥è§£å†³å°‘æ ·æœ¬å­¦ä¹ çš„æ ¸å¿ƒé—®é¢˜å’Œæ³›åŒ–é—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRWM-CGANä¸ä»…æœ‰æ•ˆåœ°æ‰©å±•äº†æ ·æœ¬ç©ºé—´ï¼Œè€Œä¸”ä¸°å¯Œäº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œæ˜¾è‘—æé«˜äº†å…¬å…±æ•°æ®é›†ä¸Šçš„æ£€æµ‹å’Œåˆ†ç±»å‡†ç¡®æ€§ã€‚è¯¥è®ºæ–‡ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œå¿«é€Ÿæ³›åŒ–åˆ°æ–°ä»»åŠ¡æˆ–ç±»åˆ«æ‰€é¢ä¸´çš„æŒ‘æˆ˜æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆï¼Œä¸ºå°‘æ ·æœ¬å­¦ä¹ é¢†åŸŸçš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03105v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æ·±åº¦å­¦ä¹ è™½åœ¨å¤šé¢†åŸŸå¼•èµ·é©å‘½æ€§å˜é©ï¼Œä½†åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼Œå…¶æ•ˆæœå—é™äºè¿‡æ‹ŸåˆåŠéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„è¦æ±‚ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§é‡‡ç”¨æ®‹å·®æƒé‡æ©è†œæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆRWM-CGANï¼‰è¿›è¡Œæ•°æ®å¢å¼ºçš„æ–°å‹å°æ ·æœ¬å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå™¨ä¸­çš„æ®‹å·®å•å…ƒå¢å¼ºç½‘ç»œæ·±åº¦å’Œæ ·æœ¬è´¨é‡ï¼Œç»“åˆåˆ¤åˆ«å™¨ä¸­çš„æƒé‡æ©è†œæ­£åˆ™åŒ–æŠ€æœ¯ï¼Œæé«˜å°æ ·æœ¬ç±»åˆ«çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚æ­¤æ–¹æ³•é€šè¿‡æä¾›å¯æ§ä¸”æ¸…æ™°çš„æ ·æœ¬ç©ºé—´æ‰©å±•ï¼Œè§£å†³äº†å°æ ·æœ¬å­¦ä¹ ä¸­é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æ ¸å¿ƒé—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRWM-CGANä¸ä»…æœ‰æ•ˆåœ°æ‰©å±•äº†æ ·æœ¬ç©ºé—´ï¼Œè¿˜æé«˜äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ£€æµ‹å’Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œå¿«é€Ÿæ³›åŒ–åˆ°æ–°ä»»åŠ¡æˆ–ç±»åˆ«æ‰€å¸¦æ¥çš„æŒ‘æˆ˜æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å°æ ·æœ¬å­¦ä¹ çš„è¿›æ­¥ã€‚</p>
<p><strong>è¦ç‚¹é€Ÿè§ˆ</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹å—é™äºè¿‡æ‹Ÿåˆå’Œæ ‡æ³¨æ•°æ®éœ€æ±‚ã€‚</li>
<li>æœ¬æ–‡é‡‡ç”¨RWM-CGANè¿›è¡Œæ•°æ®å¢å¼ºï¼Œè§£å†³å°æ ·æœ¬å­¦ä¹ ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–é—®é¢˜ã€‚</li>
<li>RWM-CGANé€šè¿‡ç”Ÿæˆå™¨ä¸­çš„æ®‹å·®å•å…ƒå¢å¼ºç½‘ç»œæ€§èƒ½ã€‚</li>
<li>åˆ¤åˆ«å™¨ä¸­çš„æƒé‡æ©è†œæ­£åˆ™åŒ–æŠ€æœ¯æé«˜å°æ ·æœ¬ç±»åˆ«çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>RWM-CGANæœ‰æ•ˆæ‰©å±•æ ·æœ¬ç©ºé—´ï¼Œæé«˜ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
<li>åœ¨å…¬å¼€æ•°æ®é›†ä¸Šï¼ŒRWM-CGANæ˜¾è‘—æé«˜æ£€æµ‹å’Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-252786a598c1b975d0500f413f8ea6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63d782b2ce5fc208e80f853c893c9b93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd09ce23f0a5602ca5fda6a170870e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-212cd64da07652f1d16f7b1689678909.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5da1851c68a4eb3fe67f49f456a57e3.jpg" align="middle">
</details>




<h2 id="dc-GAN-Dual-Conditioned-GAN-for-Face-Demorphing-From-a-Single-Morph"><a href="#dc-GAN-Dual-Conditioned-GAN-for-Face-Demorphing-From-a-Single-Morph" class="headerlink" title="dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph"></a>dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph</h2><p><strong>Authors:Nitish Shukla, Arun Ross</strong></p>
<p>A facial morph is an image created by combining two face images pertaining to two distinct identities. Face demorphing inverts the process and tries to recover the original images constituting a facial morph. While morph attack detection (MAD) techniques can be used to flag morph images, they do not divulge any visual information about the faces used to create them. Demorphing helps address this problem. Existing demorphing techniques are either very restrictive (assume identities during testing) or produce feeble outputs (both outputs look very similar). In this paper, we overcome these issues by proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph images. Our method overcomes morph-replication and produces high quality reconstructions of the bonafide images used to create the morphs. Moreover, our method is highly generalizable across demorphing paradigms (differential&#x2F;reference-free). We conduct experiments on AMSL, FRLL-Morphs and MorDiff datasets to showcase the efficacy of our method. </p>
<blockquote>
<p>é¢éƒ¨å½¢æ€å›¾åƒæ˜¯é€šè¿‡ç»“åˆä¸¤å¼ å±äºä¸åŒèº«ä»½çš„äººè„¸å›¾åƒè€Œäº§ç”Ÿçš„ã€‚é¢éƒ¨å»å½¢æ€åŒ–åˆ™é€†è½¬è¿™ä¸€è¿‡ç¨‹ï¼Œè¯•å›¾æ¢å¤æ„æˆé¢éƒ¨å½¢æ€å›¾åƒçš„åŸå§‹å›¾åƒã€‚è™½ç„¶å½¢æ€æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰æŠ€æœ¯å¯ç”¨äºæ ‡è®°å½¢æ€å›¾åƒï¼Œä½†å®ƒä»¬ä¸ä¼šæ³„éœ²ç”¨äºåˆ›å»ºå½¢æ€å›¾åƒçš„ä»»ä½•é¢éƒ¨è§†è§‰ä¿¡æ¯ã€‚å»å½¢æ€åŒ–æœ‰åŠ©äºè§£å†³è¿™ä¸€é—®é¢˜ã€‚ç°æœ‰çš„å»å½¢æ€åŒ–æŠ€æœ¯åœ¨æ“ä½œä¸Šè¦ä¹ˆå­˜åœ¨é™åˆ¶ï¼ˆå‡è®¾æµ‹è¯•æ—¶çš„èº«ä»½ä¿¡æ¯ï¼‰ï¼Œè¦ä¹ˆè¾“å‡ºæ•ˆæœä¸ä½³ï¼ˆä¸¤ä¸ªè¾“å‡ºéå¸¸ç›¸ä¼¼ï¼‰ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºGANçš„å»å½¢æ€åŒ–æ–¹æ³•dc-GANï¼Œè¯¥æ–¹æ³•ä»¥å½¢æ€å›¾åƒä¸ºæ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œå¹¶æˆåŠŸé‡å»ºäº†ç”¨äºåˆ›å»ºå½¢æ€å›¾åƒçš„çœŸå®å›¾åƒçš„ä¼˜è´¨ç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é«˜åº¦é€šç”¨åŒ–å„ç§å»å½¢æ€åŒ–æ¨¡å¼ï¼ˆå·®åˆ†&#x2F;æ— å‚è€ƒï¼‰ã€‚æˆ‘ä»¬åœ¨AMSLã€FRLL-Morphså’ŒMorDiffæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œä»¥å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14494v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºGANæŠ€æœ¯çš„é¢éƒ¨å›¾åƒå¤åŸæŠ€æœ¯dc-GANã€‚è¯¥æŠ€æœ¯èƒ½å…‹æœç°æœ‰çš„é¢éƒ¨å¤åŸæŠ€æœ¯çš„ç¼ºé™·ï¼Œé€šè¿‡å¯¹å½¢æ€å›¾åƒçš„æ¡ä»¶çº¦æŸï¼Œå®ç°å¯¹åŸå§‹é¢éƒ¨å›¾åƒçš„å¤åŸï¼Œæé«˜å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒï¼Œå±•ç¤ºäº†dc-GANæ–¹æ³•çš„ä¼˜è‰¯æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>dc-GANæ˜¯åŸºäºGANæŠ€æœ¯çš„é¢éƒ¨å›¾åƒå¤åŸæ–¹æ³•ã€‚</li>
<li>è¯¥æŠ€æœ¯èƒ½å…‹æœç°æœ‰é¢éƒ¨å¤åŸæŠ€æœ¯çš„ç¼ºé™·ï¼Œå¦‚å‡è®¾èº«ä»½æˆ–è¾“å‡ºè´¨é‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>dc-GANé€šè¿‡å¯¹å½¢æ€å›¾åƒçš„æ¡ä»¶çº¦æŸï¼Œå®ç°é«˜è´¨é‡çš„åŸå§‹é¢éƒ¨å›¾åƒå¤åŸã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥åº”ç”¨äºä¸åŒçš„å¤åŸæ¨¡å¼ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-445a4b23d88124f026866a9ef750a3dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85cf68766da569b5bf63c8a5f7291052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5413bb21b27da3cdb4125e50c7a9c6f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe7e53675735e2b37f039998ada34977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3ad372c8eea425fbd5dc03e4e57f70e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e520aa4d7945bfdc264ce02d2ec2079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1e5b3debb53022d2214dd1aeb72c52f.jpg" align="middle">
</details>




<h2 id="RNG-Relightable-Neural-Gaussians"><a href="#RNG-Relightable-Neural-Gaussians" class="headerlink" title="RNG: Relightable Neural Gaussians"></a>RNG: Relightable Neural Gaussians</h2><p><strong>Authors:Jiahui Fan, Fujun Luan, Jian Yang, MiloÅ¡ HaÅ¡an, Beibei Wang</strong></p>
<p>3D Gaussian Splatting (3DGS) has shown impressive results for the novel view synthesis task, where lighting is assumed to be fixed. However, creating relightable 3D assets, especially for objects with ill-defined shapes (fur, fabric, etc.), remains a challenging task. The decomposition between light, geometry, and material is ambiguous, especially if either smooth surface assumptions or surfacebased analytical shading models do not apply. We propose Relightable Neural Gaussians (RNG), a novel 3DGS-based framework that enables the relighting of objects with both hard surfaces or soft boundaries, while avoiding assumptions on the shading model. We condition the radiance at each point on both view and light directions. We also introduce a shadow cue, as well as a depth refinement network to improve shadow accuracy. Finally, we propose a hybrid forward-deferred fitting strategy to balance geometry and appearance quality. Our method achieves significantly faster training (1.3 hours) and rendering (60 frames per second) compared to a prior method based on neural radiance fields and produces higher-quality shadows than a concurrent 3DGS-based method. </p>
<blockquote>
<p>3Dé«˜æ–¯æ¨¡ç³Šï¼ˆ3DGSï¼‰åœ¨æ–°è§†è§’åˆæˆä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè¯¥ä»»åŠ¡å‡è®¾å…‰ç…§æ˜¯å›ºå®šçš„ã€‚ç„¶è€Œï¼Œåˆ›å»ºå¯é‡æ–°ç…§æ˜çš„3Dèµ„äº§ï¼Œå°¤å…¶æ˜¯å¯¹äºå½¢çŠ¶ä¸æ˜ç¡®ï¼ˆå¦‚æ¯›å‘ã€ç»‡ç‰©ç­‰ï¼‰çš„å¯¹è±¡ï¼Œä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚å…‰çº¿ã€å‡ ä½•å½¢çŠ¶å’Œææ–™ä¹‹é—´çš„åˆ†è§£æ˜¯æ¨¡ç³Šçš„ï¼Œå°¤å…¶æ˜¯å½“å…‰æ»‘è¡¨é¢å‡è®¾æˆ–åŸºäºè¡¨é¢çš„åˆ†æç€è‰²æ¨¡å‹ä¸é€‚ç”¨æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†å¯é‡æ–°ç…§æ˜çš„ç¥ç»é«˜æ–¯ï¼ˆRNGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäº3DGSçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°å…·æœ‰ç¡¬è¡¨é¢æˆ–è½¯è¾¹ç•Œçš„å¯¹è±¡çš„é‡æ–°ç…§æ˜ï¼ŒåŒæ—¶é¿å…äº†å¯¹ç€è‰²æ¨¡å‹çš„å‡è®¾ã€‚æˆ‘ä»¬å°†æ¯ä¸ªç‚¹çš„è¾å°„åº¦ä¸è§†å›¾å’Œå…‰çº¿çš„æ–¹å‘è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªé˜´å½±çº¿ç´¢å’Œä¸€ä¸ªæ·±åº¦ç»†åŒ–ç½‘ç»œæ¥æé«˜é˜´å½±çš„å‡†ç¡®æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆçš„å‰å‘å»¶è¿Ÿæ‹Ÿåˆç­–ç•¥æ¥å¹³è¡¡å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸å‰ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„æ–¹æ³•ç›¸æ¯”æ˜¾è‘—æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“ï¼ˆè®­ç»ƒæ—¶é—´ä¸º1.3å°æ—¶ï¼Œæ¸²æŸ“å¸§ç‡ä¸ºæ¯ç§’60å¸§ï¼‰ï¼Œå¹¶ä¸”ç›¸å¯¹äºå½“å‰çš„åŸºäº3DGSçš„æ–¹æ³•äº§ç”Ÿäº†æ›´é«˜è´¨é‡çš„é˜´å½±ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19702v4">PDF</a> Submission version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºRelightable Neural Gaussiansï¼ˆRNGï¼‰çš„3DGSæ¡†æ¶ï¼Œå®ç°äº†å¯¹å…·æœ‰ç¡¬è¡¨é¢æˆ–è½¯è¾¹ç•Œå¯¹è±¡çš„é‡æ–°ç…§æ˜åŠŸèƒ½ï¼Œæ— éœ€å¯¹é˜´å½±æ¨¡å‹è¿›è¡Œå‡è®¾ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è§†ç‚¹å’Œå…‰çº¿æ–¹å‘çš„è¾å°„æ¡ä»¶ï¼Œå¼•å…¥äº†é˜´å½±çº¿ç´¢å’Œæ·±åº¦ä¼˜åŒ–ç½‘ç»œä»¥æé«˜é˜´å½±ç²¾åº¦ï¼Œå¹¶é‡‡ç”¨æ··åˆçš„å‰å‘å»¶è¿Ÿæ‹Ÿåˆç­–ç•¥æ¥å¹³è¡¡å‡ ä½•å’Œå¤–è§‚è´¨é‡ã€‚ç›¸è¾ƒäºåŸºäºç¥ç»è¾å°„åœºçš„å‰æ–¹æ³•ï¼ŒRNGæ–¹æ³•è®­ç»ƒæ›´å¿«ï¼ˆä»…1.3å°æ—¶ï¼‰ï¼Œæ¸²æŸ“é€Ÿåº¦æ›´é«˜ï¼ˆæ¯ç§’60å¸§ï¼‰ï¼Œä¸”ç”Ÿæˆçš„é˜´å½±è´¨é‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RNGåŸºäº3DGSæ¡†æ¶å®ç°äº†ç‰©ä½“çš„é‡æ–°ç…§æ˜åŠŸèƒ½ï¼Œé€‚ç”¨äºç¡¬è¡¨é¢å’Œè½¯è¾¹ç•Œå¯¹è±¡ã€‚</li>
<li>RNGæ— éœ€å¯¹é˜´å½±æ¨¡å‹è¿›è¡Œå‡è®¾ï¼Œæé«˜äº†è¾å°„æ¡ä»¶çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥é˜´å½±çº¿ç´¢å’Œæ·±åº¦ä¼˜åŒ–ç½‘ç»œï¼Œæé«˜äº†é˜´å½±ç²¾åº¦ã€‚</li>
<li>é‡‡ç”¨æ··åˆçš„å‰å‘å»¶è¿Ÿæ‹Ÿåˆç­–ç•¥ï¼Œå¹³è¡¡å‡ ä½•å’Œå¤–è§‚è´¨é‡ã€‚</li>
<li>RNGæ–¹æ³•è®­ç»ƒé€Ÿåº¦å¿«ï¼Œä»…éœ€è¦1.3å°æ—¶ï¼Œæ¸²æŸ“é€Ÿåº¦é«˜ï¼Œæ¯ç§’å¯è¾¾60å¸§ã€‚</li>
<li>RNGç”Ÿæˆçš„é˜´å½±è´¨é‡é«˜äºåŒæœŸåŸºäº3DGSçš„æ–¹æ³•ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6d0c9f722ee13815e4e798e0dff830fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88b9ba6708ead0103c9e2f694f9501a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-869fad09772e91ff80e36e134fe6351e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f095439ac664cd1bb401bfee522b07f7.jpg" align="middle">
</details>




<h2 id="TFS-NeRF-Template-Free-NeRF-for-Semantic-3D-Reconstruction-of-Dynamic-Scene"><a href="#TFS-NeRF-Template-Free-NeRF-for-Semantic-3D-Reconstruction-of-Dynamic-Scene" class="headerlink" title="TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic   Scene"></a>TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic   Scene</h2><p><strong>Authors:Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi</strong></p>
<p>Despite advancements in Neural Implicit models for 3D surface reconstruction, handling dynamic environments with interactions between arbitrary rigid, non-rigid, or deformable entities remains challenging. The generic reconstruction methods adaptable to such dynamic scenes often require additional inputs like depth or optical flow or rely on pre-trained image features for reasonable outcomes. These methods typically use latent codes to capture frame-by-frame deformations. Another set of dynamic scene reconstruction methods, are entity-specific, mostly focusing on humans, and relies on template models. In contrast, some template-free methods bypass these requirements and adopt traditional LBS (Linear Blend Skinning) weights for a detailed representation of deformable object motions, although they involve complex optimizations leading to lengthy training times. To this end, as a remedy, this paper introduces TFS-NeRF, a template-free 3D semantic NeRF for dynamic scenes captured from sparse or single-view RGB videos, featuring interactions among two entities and more time-efficient than other LBS-based approaches. Our framework uses an Invertible Neural Network (INN) for LBS prediction, simplifying the training process. By disentangling the motions of interacting entities and optimizing per-entity skinning weights, our method efficiently generates accurate, semantically separable geometries. Extensive experiments demonstrate that our approach produces high-quality reconstructions of both deformable and non-deformable objects in complex interactions, with improved training efficiency compared to existing methods. </p>
<blockquote>
<p>å°½ç®¡ä¸‰ç»´è¡¨é¢é‡å»ºçš„ç¥ç»éšå¼æ¨¡å‹å–å¾—äº†è¿›å±•ï¼Œä½†å¤„ç†åŠ¨æ€ç¯å¢ƒä»¥åŠä»»æ„åˆšæ€§ã€éåˆšæ€§æˆ–å¯å˜å½¢å®ä½“ä¹‹é—´çš„äº¤äº’ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚èƒ½å¤Ÿé€‚åº”è¿™ç§åŠ¨æ€åœºæ™¯çš„é€šç”¨é‡å»ºæ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è¾“å…¥ï¼Œå¦‚æ·±åº¦æˆ–å…‰æµï¼Œæˆ–ä¾èµ–äºé¢„è®­ç»ƒçš„å›¾åƒç‰¹å¾æ¥è·å¾—åˆç†çš„ç»“æœã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä½¿ç”¨æ½œåœ¨ä»£ç æ¥æ•æ‰å¸§åˆ°å¸§çš„å˜å½¢ã€‚å¦ä¸€ç»„åŠ¨æ€åœºæ™¯é‡å»ºæ–¹æ³•æ˜¯é’ˆå¯¹ç‰¹å®šå®ä½“çš„ï¼Œä¸»è¦å…³æ³¨äººç±»ï¼Œå¹¶ä¾èµ–äºæ¨¡æ¿æ¨¡å‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸€äº›æ— æ¨¡æ¿çš„æ–¹æ³•ç»•è¿‡äº†è¿™äº›è¦æ±‚ï¼Œå¹¶é‡‡ç”¨ä¼ ç»Ÿçš„LBSï¼ˆçº¿æ€§æ··åˆè’™çš®ï¼‰æƒé‡æ¥è¯¦ç»†è¡¨ç¤ºå¯å˜å½¢ç‰©ä½“çš„è¿åŠ¨ï¼Œå°½ç®¡å®ƒä»¬æ¶‰åŠå¤æ‚çš„ä¼˜åŒ–ï¼Œå¯¼è‡´è®­ç»ƒæ—¶é—´è¾ƒé•¿ã€‚é‰´äºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†TFS-NeRFï¼Œè¿™æ˜¯ä¸€ç§æ— æ¨¡æ¿çš„3Dè¯­ä¹‰NeRFï¼Œç”¨äºä»ç¨€ç–æˆ–å•è§†å›¾RGBè§†é¢‘ä¸­æ•æ‰åŠ¨æ€åœºæ™¯ï¼Œå…·æœ‰ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„äº¤äº’å¹¶ä¸”æ¯”å…¶ä»–åŸºäºLBSçš„æ–¹æ³•æ›´é«˜æ•ˆã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNï¼‰è¿›è¡ŒLBSé¢„æµ‹ï¼Œç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡åˆ†ç¦»äº¤äº’å®ä½“çš„è¿åŠ¨å¹¶ä¼˜åŒ–æ¯ä¸ªå®ä½“çš„è’™çš®æƒé‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é«˜æ•ˆç”Ÿæˆå‡†ç¡®ä¸”è¯­ä¹‰ä¸Šå¯åˆ†ç¦»çš„å½¢çŠ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤æ‚äº¤äº’ä¸­å¯¹å¯å˜å½¢å’Œéå¯å˜å½¢ç‰©ä½“çš„é‡å»ºè´¨é‡å¾ˆé«˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè®­ç»ƒæ•ˆç‡ä¹Ÿæ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17459v4">PDF</a> Accepted in NeurIPS 2024 <a target="_blank" rel="noopener" href="https://github.com/sbsws88/TFS-NeRF">https://github.com/sbsws88/TFS-NeRF</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨åŠ¨æ€ç¯å¢ƒåœºæ™¯ä¸‹NeRFç¥ç»ç½‘ç»œæ¨¡å‹çš„é—®é¢˜ã€‚ç°æœ‰çš„NeRFé‡å»ºæ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚äº¤äº’ä¸­çš„åŠ¨æ€å®ä½“ï¼ŒåŒ…æ‹¬åˆšæ€§å’Œéåˆšæ€§å®ä½“ä¹‹é—´çš„äº¤äº’ã€‚æŸäº›æ–¹æ³•ä¾èµ–é¢å¤–çš„è¾“å…¥æˆ–é¢„å…ˆè®­ç»ƒçš„å›¾åƒç‰¹å¾è¿›è¡Œé‡å»ºï¼Œå¹¶ä¸»è¦é€šè¿‡éšä»£ç æ•æ‰å¸§é—´å˜å½¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— æ¨¡æ¿çš„3Dè¯­ä¹‰NeRFæ¨¡å‹â€”â€” TFS-NeRFï¼Œç”¨äºä»ç¨€ç–æˆ–å•è§†è§’RGBè§†é¢‘ä¸­æ•è·åŠ¨æ€åœºæ™¯å’Œäº¤äº’ï¼Œå…¶æ¯”åŸºäºLBSçš„æ–¹æ³•æ›´åŠ é«˜æ•ˆã€‚ä½¿ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNï¼‰è¿›è¡ŒLBSé¢„æµ‹å¹¶ä¼˜åŒ–å®ä½“çš®éª¨åŒ–æƒé‡ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸‹å¯¹å¯å˜å½¢å’Œéå¯å˜å½¢ç‰©ä½“çš„é«˜è´¨é‡é‡å»ºèƒ½åŠ›ï¼Œå¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŠ¨æ€ç¯å¢ƒåœºæ™¯ä¸‹çš„NeRFé‡å»ºä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åŒ…å«å¤šç§å®ä½“çš„å¤æ‚äº¤äº’æ—¶ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„è¾“å…¥æˆ–ä¾èµ–é¢„è®­ç»ƒå›¾åƒç‰¹å¾è¿›è¡Œé‡å»ºã€‚</li>
<li>TFS-NeRFæ˜¯ä¸€ç§æ— æ¨¡æ¿çš„3Dè¯­ä¹‰NeRFæ¨¡å‹ï¼Œé€‚ç”¨äºä»ç¨€ç–æˆ–å•è§†è§’RGBè§†é¢‘æ•è·åŠ¨æ€åœºæ™¯å’Œäº¤äº’ã€‚</li>
<li>TFS-NeRFä½¿ç”¨å¯é€†ç¥ç»ç½‘ç»œï¼ˆINNï¼‰ç®€åŒ–LBSé¢„æµ‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ç²¾ç¡®åœ°ç”Ÿæˆè¯­ä¹‰ä¸Šå¯åˆ†ç¦»çš„å½¢çŠ¶ï¼Œå¹¶ä¼˜åŒ–å®ä½“é—´çš„äº¤äº’è¿åŠ¨ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸‹çš„é«˜è´¨é‡é‡å»ºèƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c12b176fbfc52bfb7f05d9f7906279ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb424499969d54fdd9045373920cad06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d818e1f55ccba7eb66141fd19b46756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f2acaeb8ca20d7d7409a716a003c831.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48dc33cdc2060bd26d25ba3ad05b17eb.jpg" align="middle">
</details>




<h2 id="Visual-Localization-in-3D-Maps-Comparing-Point-Cloud-Mesh-and-NeRF-Representations"><a href="#Visual-Localization-in-3D-Maps-Comparing-Point-Cloud-Mesh-and-NeRF-Representations" class="headerlink" title="Visual Localization in 3D Maps: Comparing Point Cloud, Mesh, and NeRF   Representations"></a>Visual Localization in 3D Maps: Comparing Point Cloud, Mesh, and NeRF   Representations</h2><p><strong>Authors:Lintong Zhang, Yifu Tao, Jiarong Lin, Fu Zhang, Maurice Fallon</strong></p>
<p>Recent advances in mapping techniques have enabled the creation of highly accurate dense 3D maps during robotic missions, such as point clouds, meshes, or NeRF-based representations. These developments present new opportunities for reusing these maps for localization. However, there remains a lack of a unified approach that can operate seamlessly across different map representations. This paper presents and evaluates a global visual localization system capable of localizing a single camera image across various 3D map representations built using both visual and lidar sensing. Our system generates a database by synthesizing novel views of the scene, creating RGB and depth image pairs. Leveraging the precise 3D geometric map, our method automatically defines rendering poses, reducing the number of database images while preserving retrieval performance. To bridge the domain gap between real query camera images and synthetic database images, our approach utilizes learning-based descriptors and feature detectors. We evaluate the systemâ€™s performance through extensive real-world experiments conducted in both indoor and outdoor settings, assessing the effectiveness of each map representation and demonstrating its advantages over traditional structure-from-motion (SfM) localization approaches. The results show that all three map representations can achieve consistent localization success rates of 55% and higher across various environments. NeRF synthesized images show superior performance, localizing query images at an average success rate of 72%. Furthermore, we demonstrate an advantage over SfM-based approaches that our synthesized database enables localization in the reverse travel direction which is unseen during the mapping process. Our system, operating in real-time on a mobile laptop equipped with a GPU, achieves a processing rate of 1Hz. </p>
<blockquote>
<p>åœ¨æµ‹ç»˜æŠ€æœ¯æ–¹é¢çš„æœ€æ–°è¿›å±•ä½¿å¾—åœ¨æœºå™¨äººä»»åŠ¡æœŸé—´èƒ½å¤Ÿåˆ›å»ºé«˜åº¦ç²¾ç¡®çš„ä¸‰ç»´å¯†é›†åœ°å›¾ï¼Œä¾‹å¦‚ç‚¹äº‘ã€ç½‘æ ¼æˆ–åŸºäºNeRFçš„è¡¨ç¤ºã€‚è¿™äº›å‘å±•å¯¹äºåˆ©ç”¨è¿™äº›åœ°å›¾è¿›è¡Œå®šä½æä¾›äº†æ–°çš„æœºä¼šã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ç¼ºä¹ä¸€ç§å¯ä»¥åœ¨ä¸åŒåœ°å›¾è¡¨ç¤ºä¹‹é—´æ— ç¼æ“ä½œçš„ç»Ÿä¸€æ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»å¹¶è¯„ä¼°äº†ä¸€ç§å…¨çƒè§†è§‰å®šä½ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿä½¿ç”¨è§†è§‰å’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨å¯¹å„ç§ä¸‰ç»´åœ°å›¾è¡¨ç¤ºä¸­çš„å•å¼ ç›¸æœºå›¾åƒè¿›è¡Œå®šä½ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡åˆæˆåœºæ™¯çš„æ–°è§†è§’æ¥ç”Ÿæˆæ•°æ®åº“ï¼Œåˆ›å»ºRGBå’Œæ·±åº¦å›¾åƒå¯¹ã€‚åˆ©ç”¨ç²¾ç¡®çš„3Då‡ ä½•åœ°å›¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è‡ªåŠ¨å®šä¹‰æ¸²æŸ“å§¿æ€ï¼Œå‡å°‘æ•°æ®åº“å›¾åƒçš„æ•°é‡ï¼ŒåŒæ—¶ä¿ç•™æ£€ç´¢æ€§èƒ½ã€‚ä¸ºäº†å¼¥çœŸå®æŸ¥è¯¢ç›¸æœºå›¾åƒå’Œåˆæˆæ•°æ®åº“å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åŸºäºå­¦ä¹ çš„æè¿°ç¬¦å’Œç‰¹å¾æ£€æµ‹å™¨ã€‚æˆ‘ä»¬é€šè¿‡å®¤å†…å’Œå®¤å¤–ç¯å¢ƒä¸­çš„å¤§é‡çœŸå®ä¸–ç•Œå®éªŒå¯¹ç³»ç»Ÿçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯„ä¼°äº†å„ç§åœ°å›¾è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†å…¶ä¸ä¼ ç»Ÿçš„ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å®šä½æ–¹æ³•çš„ä¼˜åŠ¿ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰ä¸‰ç§åœ°å›¾è¡¨ç¤ºéƒ½èƒ½åœ¨å„ç§ç¯å¢ƒä¸­å®ç°ä¸€è‡´çš„å®šä½æˆåŠŸç‡è¾¾åˆ°æˆ–è¶…è¿‡55%ã€‚NeRFåˆæˆçš„å›¾åƒæ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒæŸ¥è¯¢å›¾åƒçš„å¹³å‡å®šä½æˆåŠŸç‡è¾¾åˆ°72%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸åŸºäºSfMçš„æ–¹æ³•ç›¸æ¯”çš„ä¼˜åŠ¿ï¼Œå³æˆ‘ä»¬çš„åˆæˆæ•°æ®åº“èƒ½å¤Ÿåœ¨é€†å‘æ—…è¡Œæ–¹å‘ä¸Šè¿›è¡Œå®šä½ï¼Œè¿™åœ¨æ˜ å°„è¿‡ç¨‹ä¸­æ˜¯çœ‹ä¸è§çš„ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨é…å¤‡GPUçš„ç§»åŠ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šå®æ—¶è¿è¡Œï¼Œå¤„ç†é€Ÿåº¦è¾¾åˆ°æ¯ç§’ä¸€å¸§ï¼ˆ1Hzï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11966v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹æœºå™¨äººä»»åŠ¡ä¸­çš„é«˜å¯†åº¦ä¸‰ç»´åœ°å›¾åˆ›å»ºæŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§å…¨å±€è§†è§‰å®šä½ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå¯åœ¨å¤šç§ä¸‰ç»´åœ°å›¾è¡¨ç¤ºä¸­è¿›è¡Œå®šä½ï¼Œå¹¶åˆ©ç”¨è§†è§‰å’Œæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨ç”Ÿæˆæ•°æ®åº“ã€‚é€šè¿‡åˆæˆåœºæ™¯çš„æ–°è§†è§’ï¼Œåˆ›å»ºRGBå’Œæ·±åº¦å›¾åƒå¯¹ï¼Œè‡ªåŠ¨å®šä¹‰æ¸²æŸ“å§¿æ€ï¼Œå‡å°‘æ•°æ®åº“å›¾åƒæ•°é‡åŒæ—¶ä¿ç•™æ£€ç´¢æ€§èƒ½ã€‚è®ºæ–‡è¿˜è¯„ä¼°äº†ä¸åŒåœ°å›¾è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å®¤å†…å’Œå®¤å¤–ç¯å¢ƒä¸­è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå±•ç¤ºå…¶åœ¨å„ç§ç¯å¢ƒä¸­çš„ç¨³å®šæ€§èƒ½å’Œå¯¹ä¼ ç»Ÿç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰å®šä½æ–¹æ³•çš„ä¼˜åŠ¿ã€‚åˆ©ç”¨NeRFåˆæˆå›¾åƒçš„å¹³å‡æˆåŠŸç‡è¾¾åˆ°72%ï¼Œå¹¶åœ¨åå‘æ—…è¡Œæ–¹å‘ä¸Šçš„å®šä½å…·æœ‰ä¼˜åŠ¿ã€‚è¯¥ç³»ç»Ÿåœ¨é…å¤‡GPUçš„ç§»åŠ¨ç¬”è®°æœ¬ä¸Šå®ç°å®æ—¶å¤„ç†ï¼Œå¤„ç†é€Ÿç‡è¾¾åˆ°1Hzã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„æ˜ å°„æŠ€æœ¯èƒ½å¤Ÿåˆ›å»ºé«˜åº¦å‡†ç¡®çš„ä¸‰ç»´åœ°å›¾ï¼Œå¦‚ç‚¹äº‘ã€ç½‘æ ¼æˆ–åŸºäºNeRFçš„è¡¨ç¤ºã€‚</li>
<li>ç¼ºä¹èƒ½åœ¨ä¸åŒåœ°å›¾è¡¨ç¤ºä¹‹é—´æ— ç¼æ“ä½œçš„ç»Ÿä¸€æ–¹æ³•ã€‚</li>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å…¨å±€è§†è§‰å®šä½ç³»ç»Ÿï¼Œå¯å®šä½åœ¨ç”±è§†è§‰å’Œæ¿€å…‰é›·è¾¾æ„ŸçŸ¥ç”Ÿæˆçš„å„ç§ä¸‰ç»´åœ°å›¾è¡¨ç¤ºä¸­çš„å•ä¸€ç›¸æœºå›¾åƒã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡åˆæˆåœºæ™¯çš„æ–°è§†è§’ç”Ÿæˆæ•°æ®åº“ï¼Œè‡ªåŠ¨å®šä¹‰æ¸²æŸ“å§¿æ€ï¼Œå‡å°‘æ•°æ®åº“å›¾åƒæ•°é‡åŒæ—¶ä¿ç•™æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡è¯„ä¼°äº†ä¸åŒåœ°å›¾è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨å„ç§ç¯å¢ƒä¸­çš„ç¨³å®šæ€§èƒ½å’Œå¯¹ä¼ ç»ŸSfMå®šä½æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</li>
<li>åˆ©ç”¨NeRFåˆæˆå›¾åƒçš„å®šä½æˆåŠŸç‡è¾ƒé«˜ï¼Œå¹³å‡è¾¾åˆ°72%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b534227ff46e39e60595af80d025cda.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b124b92a6223ae253c35bf5d86fc2c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08e24fcc9db48779265de89a9966f308.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3c0a4f4d20c99a1bf0bfa6aff1bb304.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6886d055876d0a7cba042fa55a3b848e.jpg" align="middle">
</details>




<h2 id="PartGS-Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics"><a href="#PartGS-Learning-Part-aware-3D-Representations-by-Fusing-2D-Gaussians-and-Superquadrics" class="headerlink" title="PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and   Superquadrics"></a>PartGS:Learning Part-aware 3D Representations by Fusing 2D Gaussians and   Superquadrics</h2><p><strong>Authors:Zhirui Gao, Renjiao Yi, Yuhang Huang, Wei Chen, Chenyang Zhu, Kai Xu</strong></p>
<p>Low-level 3D representations, such as point clouds, meshes, NeRFs, and 3D Gaussians, are commonly used to represent 3D objects or scenes. However, human perception typically understands 3D objects at a higher level as a composition of parts or structures rather than points or voxels. Representing 3D objects or scenes as semantic parts can benefit further understanding and applications. In this paper, we introduce $\textbf{PartGS}$, $\textbf{part}$-aware 3D reconstruction by a hybrid representation of 2D $\textbf{G}$aussians and $\textbf{S}$uperquadrics, which parses objects or scenes into semantic parts, digging 3D structural clues from multi-view image inputs. Accurate structured geometry reconstruction and high-quality rendering are achieved at the same time. Our method simultaneously optimizes superquadric meshes and Gaussians by coupling their parameters within our hybrid representation. On one hand, this hybrid representation inherits the advantage of superquadrics to represent different shape primitives, supporting flexible part decomposition of scenes. On the other hand, 2D Gaussians capture complex texture and geometry details, ensuring high-quality appearance and geometry reconstruction. Our method is fully unsupervised and outperforms existing state-of-the-art approaches in extensive experiments on DTU, ShapeNet, and real-life datasets. </p>
<blockquote>
<p>ä½çº§åˆ«çš„ä¸‰ç»´è¡¨ç¤ºï¼Œå¦‚ç‚¹äº‘ã€ç½‘æ ¼ã€NeRFå’Œé«˜æ–¯åˆ†å¸ƒç­‰ï¼Œé€šå¸¸ç”¨äºè¡¨ç¤ºä¸‰ç»´ç‰©ä½“æˆ–åœºæ™¯ã€‚ç„¶è€Œï¼Œäººç±»çš„æ„ŸçŸ¥é€šå¸¸æ˜¯åœ¨æ›´é«˜å±‚æ¬¡ä¸Šç†è§£ä¸‰ç»´ç‰©ä½“ï¼Œå°†å…¶è§†ä¸ºéƒ¨ä»¶æˆ–ç»“æ„çš„ç»„åˆï¼Œè€Œéç‚¹æˆ–ä½“ç´ ã€‚å°†ä¸‰ç»´ç‰©ä½“æˆ–åœºæ™¯è¡¨ç¤ºä¸ºè¯­ä¹‰éƒ¨ä»¶å¯ä»¥è¿›ä¸€æ­¥ä¿ƒè¿›ç†è§£å’Œåº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†PartGSï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡äºŒç»´é«˜æ–¯å’Œè¶…çº§äºŒæ¬¡å…ƒçš„æ··åˆè¡¨ç¤ºæ¥è¿›è¡Œéƒ¨ä»¶æ„ŸçŸ¥çš„ä¸‰ç»´é‡å»ºã€‚å®ƒèƒ½å¤Ÿå°†ç‰©ä½“æˆ–åœºæ™¯è§£æä¸ºè¯­ä¹‰éƒ¨ä»¶ï¼Œä»å¤šè§†è§’å›¾åƒè¾“å…¥ä¸­æŒ–æ˜ä¸‰ç»´ç»“æ„çº¿ç´¢ã€‚åŒæ—¶å®ç°äº†ç²¾ç¡®çš„å‡ ä½•ç»“æ„é‡å»ºå’Œé«˜è´¨é‡æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ··åˆè¡¨ç¤ºä¸­çš„å‚æ•°è€¦åˆæ¥åŒæ—¶ä¼˜åŒ–è¶…çº§äºŒæ¬¡å…ƒç½‘æ ¼å’Œé«˜æ–¯åˆ†å¸ƒã€‚ä¸€æ–¹é¢ï¼Œè¿™ç§æ··åˆè¡¨ç¤ºç»§æ‰¿äº†è¶…çº§äºŒæ¬¡å…ƒè¡¨ç¤ºä¸åŒå½¢çŠ¶åŸå§‹éƒ¨ä»¶çš„ä¼˜åŠ¿ï¼Œæ”¯æŒåœºæ™¯çµæ´»çš„éƒ¨åˆ†åˆ†è§£ã€‚å¦ä¸€æ–¹é¢ï¼ŒäºŒç»´é«˜æ–¯åˆ†å¸ƒæ•æ‰å¤æ‚çš„çº¹ç†å’Œå‡ ä½•ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„å¤–è²Œå’Œå‡ ä½•é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å®Œå…¨æ— ç›‘ç£ï¼Œåœ¨DTUã€ShapeNetå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒä¸­è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10789v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPartGSçš„3Dé‡å»ºæ–¹æ³•ï¼Œå®ƒé‡‡ç”¨æ··åˆè¡¨ç¤ºæ–¹å¼ï¼Œç»“åˆäº†äºŒç»´é«˜æ–¯å’Œè¶…çº§æ›²é¢ï¼ˆsuperquadricsï¼‰ï¼Œå°†å¯¹è±¡æˆ–åœºæ™¯è§£æä¸ºè¯­ä¹‰éƒ¨åˆ†ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä»å¤šè§†è§’å›¾åƒè¾“å…¥ä¸­æŒ–æ˜å‡º3Dç»“æ„çº¿ç´¢ï¼Œå®ç°ç²¾ç¡®çš„ç»“æ„å‡ ä½•é‡å»ºå’Œé«˜è´¨é‡æ¸²æŸ“ã€‚PartGSåŒæ—¶ä¼˜åŒ–è¶…çº§æ›²é¢ç½‘æ ¼å’Œé«˜æ–¯åˆ†å¸ƒï¼Œé€šè¿‡æ··åˆè¡¨ç¤ºä¸­çš„å‚æ•°è€¦åˆæ¥å®ç°ã€‚è¯¥æ–¹æ³•æ—¢ç»§æ‰¿äº†è¶…çº§æ›²é¢è¡¨ç¤ºä¸åŒå½¢çŠ¶åŸè¯­çš„ä¼˜åŠ¿ï¼Œæ”¯æŒåœºæ™¯çµæ´»çš„éƒ¨åˆ†åˆ†è§£ï¼Œåˆé€šè¿‡äºŒç»´é«˜æ–¯æ•æ‰å¤æ‚çš„çº¹ç†å’Œå‡ ä½•ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„å¤–è§‚å’Œå‡ ä½•é‡å»ºã€‚è¯¥æ–¹æ³•å®Œå…¨æ— ç›‘ç£ï¼Œåœ¨DTUã€ShapeNetå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PartGSé‡‡ç”¨æ··åˆè¡¨ç¤ºæ–¹æ³•ï¼Œç»“åˆäº†äºŒç»´é«˜æ–¯å’Œè¶…çº§æ›²é¢ï¼ˆsuperquadricsï¼‰ï¼Œä»¥è§£æ3Då¯¹è±¡æˆ–åœºæ™¯çš„è¯­ä¹‰éƒ¨åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä»å¤šè§†è§’å›¾åƒä¸­æŒ–æ˜å‡º3Dç»“æ„çº¿ç´¢ï¼Œå®ç°ç²¾ç¡®çš„ç»“æ„å‡ ä½•é‡å»ºå’Œé«˜è´¨é‡æ¸²æŸ“ã€‚</li>
<li>PartGSå¯ä»¥åŒæ—¶ä¼˜åŒ–è¶…çº§æ›²é¢ç½‘æ ¼å’Œé«˜æ–¯åˆ†å¸ƒï¼Œé€šè¿‡å‚æ•°è€¦åˆæ¥å®ç°è¿™ä¸€åŠŸèƒ½ã€‚</li>
<li>è¶…çº§æ›²é¢èƒ½å¤Ÿè¡¨ç¤ºä¸åŒçš„å½¢çŠ¶åŸè¯­ï¼Œæ”¯æŒåœºæ™¯çµæ´»çš„éƒ¨åˆ†åˆ†è§£ã€‚</li>
<li>äºŒç»´é«˜æ–¯å¯ä»¥æ•æ‰å¤æ‚çš„çº¹ç†å’Œå‡ ä½•ç»†èŠ‚ï¼Œç¡®ä¿é«˜è´¨é‡çš„å¤–è§‚å’Œå‡ ä½•é‡å»ºã€‚</li>
<li>PartGSæ–¹æ³•å®Œå…¨æ— ç›‘ç£ï¼Œé€‚ç”¨äºå¤šç§æ•°æ®é›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9fe9774926824b5022d6d3f00f85bfd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be53ca8f5c4d86af4fa29aab559165ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5143e1f9af3ecc203b9ed18f02481dcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31838f0c1baa1e0074b8f3d08d95b604.jpg" align="middle">
</details>




<h2 id="LumiGauss-Relightable-Gaussian-Splatting-in-the-Wild"><a href="#LumiGauss-Relightable-Gaussian-Splatting-in-the-Wild" class="headerlink" title="LumiGauss: Relightable Gaussian Splatting in the Wild"></a>LumiGauss: Relightable Gaussian Splatting in the Wild</h2><p><strong>Authors:Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, Marek Kowalski</strong></p>
<p>Decoupling lighting from geometry using unconstrained photo collections is notoriously challenging. Solving it would benefit many users as creating complex 3D assets takes days of manual labor. Many previous works have attempted to address this issue, often at the expense of output fidelity, which questions the practicality of such methods. We introduce LumiGauss - a technique that tackles 3D reconstruction of scenes and environmental lighting through 2D Gaussian Splatting. Our approach yields high-quality scene reconstructions and enables realistic lighting synthesis under novel environment maps. We also propose a method for enhancing the quality of shadows, common in outdoor scenes, by exploiting spherical harmonics properties. Our approach facilitates seamless integration with game engines and enables the use of fast precomputed radiance transfer. We validate our method on the NeRF-OSR dataset, demonstrating superior performance over baseline methods. Moreover, LumiGauss can synthesize realistic images for unseen environment maps. Our code: <a target="_blank" rel="noopener" href="https://github.com/joaxkal/lumigauss">https://github.com/joaxkal/lumigauss</a>. </p>
<blockquote>
<p>ä½¿ç”¨æ— çº¦æŸç…§ç‰‡é›†å°†ç…§æ˜ä¸å‡ ä½•åˆ†ç¦»æ˜¯ä¼—æ‰€å‘¨çŸ¥çš„æŒ‘æˆ˜ã€‚è§£å†³è¿™ä¸€é—®é¢˜å°†ä½¿è®¸å¤šç”¨æˆ·å—ç›Šï¼Œå› ä¸ºåˆ›å»ºå¤æ‚çš„3Dèµ„äº§éœ€è¦æ•°å¤©çš„äººå·¥åŠ³åŠ¨ã€‚è®¸å¤šæ—©æœŸä½œå“è¯•å›¾è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šå¸¸ä»¥ç‰ºç‰²è¾“å‡ºä¿çœŸåº¦ä¸ºä»£ä»·ï¼Œè¿™å¼•å‘äº†å¯¹æ­¤ç±»æ–¹æ³•å®ç”¨æ€§çš„è´¨ç–‘ã€‚æˆ‘ä»¬å¼•å…¥äº†LumiGaussæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡äºŒç»´é«˜æ–¯è´´å›¾è§£å†³åœºæ™¯çš„ä¸‰ç»´é‡å»ºå’Œç¯å¢ƒç…§æ˜é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿäº†é«˜è´¨é‡çš„åœºæ™¯é‡å»ºï¼Œå¹¶åœ¨æ–°çš„ç¯å¢ƒè´´å›¾ä¸‹å®ç°äº†é€¼çœŸçš„ç…§æ˜åˆæˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨çƒé¢è°æ³¢å±æ€§æ¥æé«˜å®¤å¤–åœºæ™¯ä¸­å¸¸è§é˜´å½±è´¨é‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ¸¸æˆå¼•æ“æ— ç¼é›†æˆï¼Œå¹¶åˆ©ç”¨å¿«é€Ÿé¢„è®¡ç®—è¾å°„ä¼ è¾“ã€‚æˆ‘ä»¬åœ¨NeRF-OSRæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨åŸºçº¿æ–¹æ³•ä¸Šçš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼ŒLumiGaussè¿˜å¯ä»¥ä¸ºæœªè§çš„ç¯å¢ƒåœ°å›¾åˆæˆé€¼çœŸçš„å›¾åƒã€‚æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/joaxkal/lumigauss%E3%80%82">https://github.com/joaxkal/lumigaussã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04474v2">PDF</a> Accepted at WACV2025</p>
<p><strong>Summary</strong><br>åŸºäºæœªçº¦æŸç…§ç‰‡é›†ï¼Œå®ç°å…‰ç…§ä¸å‡ ä½•çš„è§£è€¦å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LumiGaussæŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨äºŒç»´é«˜æ–¯splatæ–¹æ³•é‡å»ºä¸‰ç»´åœºæ™¯å¹¶åˆæˆç¯å¢ƒå…‰ç…§ã€‚è¯¥æ–¹æ³•å¯å®ç°é«˜è´¨é‡åœºæ™¯é‡å»ºï¼Œå¹¶åœ¨æ–°å‹ç¯å¢ƒè´´å›¾ä¸‹ç”Ÿæˆé€¼çœŸçš„å…‰ç…§æ•ˆæœã€‚åŒæ—¶ï¼Œé€šè¿‡åˆ©ç”¨çƒé¢è°æ³¢å±æ€§æ”¹è¿›æˆ·å¤–åœºæ™¯ä¸­å¸¸è§çš„é˜´å½±è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ¸¸æˆå¼•æ“æ— ç¼é›†æˆï¼Œå¹¶é‡‡ç”¨å¿«é€Ÿé¢„è®¡ç®—è¾å°„ä¼ è¾“ã€‚åœ¨NeRF-OSRæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥åˆæˆé’ˆå¯¹æœªè§ç¯å¢ƒæ˜ å°„çš„é€¼çœŸå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LumiGaussæŠ€æœ¯è§£å†³äº†åœ¨è§£è€¦å…‰ç…§å’Œå‡ ä½•æ—¶çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æŠ€æœ¯ä½¿ç”¨äºŒç»´é«˜æ–¯splatæ–¹æ³•è¿›è¡Œä¸‰ç»´åœºæ™¯çš„é‡å»ºå’Œç¯å¢ƒå…‰ç…§çš„åˆæˆã€‚</li>
<li>LumiGaussèƒ½å®ç°é«˜è´¨é‡çš„åœºæ™¯é‡å»ºï¼Œå¹¶ç”Ÿæˆé€¼çœŸçš„å…‰ç…§æ•ˆæœã€‚</li>
<li>é€šè¿‡åˆ©ç”¨çƒé¢è°æ³¢å±æ€§ï¼Œæ”¹è¿›äº†æˆ·å¤–åœºæ™¯ä¸­çš„é˜´å½±è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸æ¸¸æˆå¼•æ“æ— ç¼é›†æˆï¼Œå¹¶é‡‡ç”¨å¿«é€Ÿé¢„è®¡ç®—è¾å°„ä¼ è¾“ã€‚</li>
<li>åœ¨NeRF-OSRæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†LumiGaussæ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37590b752e9fedcb8e80fd96ed87b71a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4d8519a5831263c1fa14dcbd21ace15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-183b4b1f5584c92e19ac89e20b636209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c879dedaed1b93cbf266f97635060f35.jpg" align="middle">
</details>




<h2 id="Evaluating-Modern-Approaches-in-3D-Scene-Reconstruction-NeRF-vs-Gaussian-Based-Methods"><a href="#Evaluating-Modern-Approaches-in-3D-Scene-Reconstruction-NeRF-vs-Gaussian-Based-Methods" class="headerlink" title="Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs   Gaussian-Based Methods"></a>Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs   Gaussian-Based Methods</h2><p><strong>Authors:Yiming Zhou, Zixuan Zeng, Andi Chen, Xiaofan Zhou, Haowei Ni, Shiyao Zhang, Panfeng Li, Liangxi Liu, Mengyao Zheng, Xupeng Chen</strong></p>
<p>Exploring the capabilities of Neural Radiance Fields (NeRF) and Gaussian-based methods in the context of 3D scene reconstruction, this study contrasts these modern approaches with traditional Simultaneous Localization and Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we assess performance based on tracking accuracy, mapping fidelity, and view synthesis. Findings reveal that NeRF excels in view synthesis, offering unique capabilities in generating new perspectives from existing data, albeit at slower processing speeds. Conversely, Gaussian-based methods provide rapid processing and significant expressiveness but lack comprehensive scene completion. Enhanced by global optimization and loop closure techniques, newer methods like NICE-SLAM and SplaTAM not only surpass older frameworks such as ORB-SLAM2 in terms of robustness but also demonstrate superior performance in dynamic and complex environments. This comparative analysis bridges theoretical research with practical implications, shedding light on future developments in robust 3D scene reconstruction across various real-world applications. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†Neural Radiance Fieldsï¼ˆNeRFï¼‰å’Œé«˜æ–¯åŸºæ–¹æ³•åœ¨3Dåœºæ™¯é‡å»ºæ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶å°†è¿™äº›ç°ä»£æ–¹æ³•ä¸ä¼ ç»Ÿçš„Simultaneous Localization and Mappingï¼ˆSLAMï¼‰ç³»ç»Ÿè¿›è¡Œäº†å¯¹æ¯”ã€‚æˆ‘ä»¬åˆ©ç”¨Replicaå’ŒScanNetç­‰æ•°æ®é›†ï¼Œæ ¹æ®è·Ÿè¸ªç²¾åº¦ã€æ˜ å°„ä¿çœŸåº¦å’Œè§†å›¾åˆæˆæ¥è¯„ä¼°æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒNeRFåœ¨è§†å›¾åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»ç°æœ‰æ•°æ®ä¸­ç”Ÿæˆæ–°çš„è§†è§’ï¼Œå°½ç®¡å¤„ç†é€Ÿåº¦è¾ƒæ…¢ã€‚ç›¸åï¼Œé«˜æ–¯åŸºæ–¹æ³•å¤„ç†é€Ÿåº¦å¿«ï¼Œè¡¨è¾¾åŠ›å¼ºï¼Œä½†åœºæ™¯å®Œæˆåº¦ä¸å¤Ÿå…¨é¢ã€‚é€šè¿‡å…¨å±€ä¼˜åŒ–å’Œé—­ç¯æŠ€æœ¯å¢å¼ºï¼ŒNICE-SLAMå’ŒSplaTAMç­‰æ–°æ–¹æ³•ä¸ä»…æé«˜äº†é²æ£’æ€§ï¼Œè¶…è¶Šäº†æ—©æœŸçš„ORB-SLAM2æ¡†æ¶ï¼Œè€Œä¸”åœ¨åŠ¨æ€å’Œå¤æ‚ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™ä¸€æ¯”è¾ƒåˆ†æå°†ç†è®ºç ”ç©¶ä¸å®é™…åº”ç”¨ç›¸ç»“åˆï¼Œä¸ºå„ç§ç°å®åº”ç”¨ä¸­çš„ç¨³å¥3Dåœºæ™¯é‡å»ºçš„æœªæ¥å‘å±•æä¾›äº†å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.04268v2">PDF</a> Accepted by 2024 6th International Conference on Data-driven   Optimization of Complex Systems</p>
<p><strong>Summary</strong></p>
<p>NeRFä¸åŸºäºé«˜æ–¯çš„æ–¹æ³•åœ¨3Dåœºæ™¯é‡å»ºä¸­çš„æ€§èƒ½ç ”ç©¶ï¼Œä¸ä¼ ç»ŸSLAMç³»ç»Ÿç›¸æ¯”ï¼Œæ¢è®¨äº†NeRFçš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚ç ”ç©¶é€šè¿‡Replicaå’ŒScanNetæ•°æ®é›†è¯„ä¼°äº†è·Ÿè¸ªç²¾åº¦ã€åœ°å›¾ç²¾åº¦å’Œè§†è§’åˆæˆçš„æ€§èƒ½ã€‚NeRFåœ¨è§†è§’åˆæˆæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å¤„ç†é€Ÿåº¦è¾ƒæ…¢ã€‚åŸºäºé«˜æ–¯çš„æ–¹æ³•å¤„ç†é€Ÿåº¦å¿«ä¸”è¡¨ç°åŠ›å¼ºï¼Œä½†åœºæ™¯å®Œæ•´æ€§æ–¹é¢æœ‰æ‰€æ¬ ç¼ºã€‚æ–°å‹æ–¹æ³•å¦‚NICE-SLAMå’ŒSplaTAMä¸ä»…æé«˜äº†ç¨³å¥æ€§ï¼Œè€Œä¸”åœ¨åŠ¨æ€å’Œå¤æ‚ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºç†è®ºç ”ç©¶å’Œå®é™…åº”ç”¨ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºæœªæ¥çš„ç¨³å¥3Dåœºæ™¯é‡å»ºçš„æœªæ¥å‘å±•æä¾›äº†å¯ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFåœ¨è§†è§’åˆæˆæ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆæ–°çš„è§†è§’æ•°æ®ã€‚</li>
<li>åŸºäºé«˜æ–¯çš„æ–¹æ³•å¤„ç†é€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½åœ¨åœºæ™¯å®Œæ•´æ€§æ–¹é¢æœ‰æ‰€ä¸è¶³ã€‚</li>
<li>NeRFå¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œéœ€è¦ä¼˜åŒ–ä»¥æé«˜æ•ˆç‡ã€‚</li>
<li>æ–°æ–¹æ³•å¦‚NICE-SLAMå’ŒSplaTAMåœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­è¡¨ç°å‡ºæ›´å¥½çš„ç¨³å¥æ€§ã€‚</li>
<li>å¯¹æ¯”åˆ†ææœ‰åŠ©äºç†è§£ä¸åŒæ–¹æ³•åœ¨3Dåœºæ™¯é‡å»ºä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹3Dåœºæ™¯é‡å»ºçš„æœªæ¥å‘å±•å…·æœ‰å¯ç¤ºä½œç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8ac5b012571a7d5ba656106756b1a21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f28031f4c253632ac8620524c3344355.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f350bfec3df9e3a83d8efffeaa59b7a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cc1851d7d7f64286dc388e48b5539cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c91b6005b19b0614ba374c1a5e72697.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62b901b2eb3c672bcb45a04d595c0308.jpg" align="middle">
</details>




<h2 id="IE-NeRF-Inpainting-Enhanced-Neural-Radiance-Fields-in-the-Wild"><a href="#IE-NeRF-Inpainting-Enhanced-Neural-Radiance-Fields-in-the-Wild" class="headerlink" title="IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild"></a>IE-NeRF: Inpainting Enhanced Neural Radiance Fields in the Wild</h2><p><strong>Authors:Shuaixian Wang, Haoran Xu, Yaokun Li, Jiwei Chen, Guang Tan</strong></p>
<p>We present a novel approach for synthesizing realistic novel views using Neural Radiance Fields (NeRF) with uncontrolled photos in the wild. While NeRF has shown impressive results in controlled settings, it struggles with transient objects commonly found in dynamic and time-varying scenes. Our framework called \textit{Inpainting Enhanced NeRF}, or \ours, enhances the conventional NeRF by drawing inspiration from the technique of image inpainting. Specifically, our approach extends the Multi-Layer Perceptrons (MLP) of NeRF, enabling it to simultaneously generate intrinsic properties (static color, density) and extrinsic transient masks. We introduce an inpainting module that leverages the transient masks to effectively exclude occlusions, resulting in improved volume rendering quality. Additionally, we propose a new training strategy with frequency regularization to address the sparsity issue of low-frequency transient components. We evaluate our approach on internet photo collections of landmarks, demonstrating its ability to generate high-quality novel views and achieve state-of-the-art performance. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åˆæˆé€¼çœŸçš„æ–°è§†è§’çš„æ–°æ–¹æ³•ï¼Œé€‚ç”¨äºä¸å—æ§åˆ¶çš„é‡å¤–ç…§ç‰‡ã€‚è™½ç„¶NeRFåœ¨å—æ§ç¯å¢ƒä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†åœ¨åŠ¨æ€å’Œæ—¶å˜åœºæ™¯ä¸­å¸¸è§çš„ç¬æ€å¯¹è±¡ä¸Šå´é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç§°ä¸ºâ€œä¿®å¤å¢å¼ºNeRFâ€ï¼Œæˆ–ç®€ç§°ä¸ºâ€œæˆ‘ä»¬çš„æ–¹æ³•â€ï¼Œå®ƒé€šè¿‡å€Ÿé‰´å›¾åƒä¿®å¤æŠ€æœ¯æ¥å¢å¼ºä¼ ç»Ÿçš„NeRFã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰©å±•äº†NeRFçš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œä½¿å…¶èƒ½å¤ŸåŒæ—¶ç”Ÿæˆå†…åœ¨å±æ€§ï¼ˆé™æ€é¢œè‰²ã€å¯†åº¦ï¼‰å’Œå¤–åœ¨ç¬æ€æ©è†œã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¿®å¤æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨ç¬æ€æ©è†œæœ‰æ•ˆåœ°æ’é™¤é®æŒ¡ï¼Œä»è€Œæé«˜ä½“ç§¯æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¸¦æœ‰é¢‘ç‡æ­£åˆ™åŒ–çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥è§£å†³ç¬æ€ç»„ä»¶ä½é¢‘çš„ç¨€ç–æ€§é—®é¢˜ã€‚æˆ‘ä»¬åœ¨åœ°æ ‡äº’è”ç½‘ç…§ç‰‡é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶ç”Ÿæˆé«˜è´¨é‡æ–°è§†è§’çš„èƒ½åŠ›ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10695v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆçœŸå®è§†è§’çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨é‡ç”Ÿç¯å¢ƒä¸­çš„éæ§åˆ¶ç…§ç‰‡ã€‚æˆ‘ä»¬æå‡ºåä¸ºâ€œå¢å¼ºNeRFä¿®å¤â€çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ä¼ ç»ŸNeRFæŠ€æœ¯ï¼Œçµæ„Ÿæ¥æºäºå›¾åƒä¿®å¤æŠ€æœ¯ã€‚å®ƒæ‰©å±•äº†NeRFçš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆå›ºæœ‰å±æ€§ï¼ˆé™æ€é¢œè‰²ã€å¯†åº¦ï¼‰å’Œå¤–ç½®ç¬æ€æ©è†œã€‚é€šè¿‡å¼•å…¥ä¿®å¤æ¨¡å—å¹¶åˆ©ç”¨ç¬æ€æ©è†œæœ‰æ•ˆæ’é™¤é®æŒ¡ç‰©ï¼Œæé«˜äº†ä½“ç§¯æ¸²æŸ“è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„å¸¦æœ‰é¢‘ç‡æ­£åˆ™åŒ–çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥è§£å†³ç¬æ€ç»„ä»¶çš„ä½é¢‘ç¨€ç–æ€§é—®é¢˜ã€‚æˆ‘ä»¬åœ¨äº’è”ç½‘åœ°æ ‡ç…§ç‰‡é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜å…¶èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†è§’å¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯çš„åˆæˆçœŸå®è§†è§’æ–°æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨é‡ç”Ÿç¯å¢ƒä¸­çš„éæ§åˆ¶ç…§ç‰‡è¿›è¡ŒNeRFåˆæˆã€‚</li>
<li>æå‡ºåä¸ºâ€œå¢å¼ºNeRFä¿®å¤â€çš„æ¡†æ¶ï¼Œç»“åˆå›¾åƒä¿®å¤æŠ€æœ¯ï¼Œå¢å¼ºNeRFè¡¨ç°ã€‚</li>
<li>é€šè¿‡æ‰©å±•å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œå¯åŒæ—¶ç”Ÿæˆå›ºæœ‰å±æ€§å’Œå¤–ç½®ç¬æ€æ©è†œã€‚</li>
<li>åˆ©ç”¨ç¬æ€æ©è†œä¿®å¤æ¨¡å—æ’é™¤é®æŒ¡ç‰©ï¼Œæé«˜ä½“ç§¯æ¸²æŸ“è´¨é‡ã€‚</li>
<li>æå‡ºå¸¦æœ‰é¢‘ç‡æ­£åˆ™åŒ–çš„æ–°è®­ç»ƒç­–ç•¥ï¼Œè§£å†³ç¬æ€ç»„ä»¶çš„ä½é¢‘ç¨€ç–æ€§é—®é¢˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b35ebfffabddb9f4902892248bde9a8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5783c2fcf10141d017f36100978a4f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ccc09580cc92e90612ea6be5d519facd.jpg" align="middle">
</details>




<h2 id="Radiance-Fields-from-Photons"><a href="#Radiance-Fields-from-Photons" class="headerlink" title="Radiance Fields from Photons"></a>Radiance Fields from Photons</h2><p><strong>Authors:Sacha Jungerman, Aryan Garg, Mohit Gupta</strong></p>
<p>Neural radiance fields, or NeRFs, have become the de facto approach for high-quality view synthesis from a collection of images captured from multiple viewpoints. However, many issues remain when capturing images in-the-wild under challenging conditions, such as low light, high dynamic range, or rapid motion leading to smeared reconstructions with noticeable artifacts. In this work, we introduce quanta radiance fields, a novel class of neural radiance fields that are trained at the granularity of individual photons using single-photon cameras (SPCs). We develop theory and practical computational techniques for building radiance fields and estimating dense camera poses from unconventional, stochastic, and high-speed binary frame sequences captured by SPCs. We demonstrate, both via simulations and a SPC hardware prototype, high-fidelity reconstructions under high-speed motion, in low light, and for extreme dynamic range settings. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²ç»æˆä¸ºä»å¤šä¸ªè§†è§’æ•è·çš„å›¾åƒé›†ä¸­è¿›è¡Œé«˜è´¨é‡è§†å›¾åˆæˆçš„ä¸€ç§å®é™…æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼ˆå¦‚ä½å…‰ç…§ã€é«˜åŠ¨æ€èŒƒå›´æˆ–å¿«é€Ÿè¿åŠ¨ç­‰ï¼‰è¿›è¡Œé‡å¤–å›¾åƒæ•è·æ—¶ä»å­˜åœ¨è®¸å¤šé—®é¢˜ï¼Œè¿™ä¼šå¯¼è‡´é‡å»ºæ¨¡ç³Šå¹¶å‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†é‡å­è¾å°„åœºï¼ˆQuanta Radiance Fieldsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»è¾å°„åœºï¼Œå®ƒä½¿ç”¨å•å…‰å­ç›¸æœºï¼ˆSPCï¼‰ä»¥å•ä¸ªå…‰å­ä¸ºå•ä½è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬ä¸ºæ„å»ºè¾å°„åœºå’Œä¼°è®¡ç”±SPCæ•è·çš„éä¼ ç»Ÿã€éšæœºå’Œé«˜é€ŸäºŒè¿›åˆ¶å¸§åºåˆ—çš„å¯†é›†ç›¸æœºå§¿æ€ï¼Œå‘å±•ç†è®ºå’Œå®ç”¨è®¡ç®—æŠ€æœ¯ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿå’ŒSPCç¡¬ä»¶åŸå‹è¯æ˜ï¼Œåœ¨é«˜é€Ÿè¿åŠ¨ã€ä½å…‰ç…§å’Œæç«¯åŠ¨æ€èŒƒå›´è®¾ç½®ä¸‹ï¼Œå¯ä»¥å®ç°é«˜ä¿çœŸé‡å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.09386v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å·²ç»æˆä¸ºä»å¤šä¸ªè§†è§’æ•è·çš„å›¾åƒè¿›è¡Œé«˜è´¨é‡è§†å›¾åˆæˆçš„ä¸»æµæ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨é‡å¤–æ‹æ‘„æ—¶ï¼Œç”±äºä½å…‰ã€é«˜åŠ¨æ€èŒƒå›´æˆ–å¿«é€Ÿè¿åŠ¨ç­‰æŒ‘æˆ˜æ¡ä»¶ï¼Œä»å­˜åœ¨è®¸å¤šéš¾é¢˜ï¼Œå¯¼è‡´é‡å»ºæ¨¡ç³Šå¹¶å‡ºç°æ˜æ˜¾çš„ä¼ªå½±ã€‚æœ¬ç ”ç©¶å¼•å…¥é‡å­è¾å°„åœºï¼ˆquanta radiance fieldsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œè¾å°„åœºï¼Œä»¥å•ä¸ªå…‰å­ç›¸æœºï¼ˆSPCï¼‰ä¸ºå•ä½è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬æ„å»ºäº†è¾å°„åœºçš„ç†è®ºåŠå®ç”¨è®¡ç®—æŠ€æœ¯ï¼Œå¹¶ä¼°ç®—äº†æ¥è‡ªSPCæ•è·çš„éä¼ ç»Ÿã€éšæœºå’Œé«˜é€ŸäºŒè¿›åˆ¶å¸§åºåˆ—çš„å¯†é›†ç›¸æœºå§¿æ€ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹Ÿå’ŒSPCç¡¬ä»¶åŸå‹æ¼”ç¤ºäº†åœ¨é«˜é€Ÿè¿åŠ¨ã€ä½å…‰å’Œé«˜åŠ¨æ€èŒƒå›´è®¾ç½®ä¸‹çš„é«˜ä¿çœŸé‡å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFå·²æˆä¸ºé«˜è´¨é‡è§†å›¾åˆæˆçš„æ ‡å‡†æ–¹æ³•ï¼Œä½†åœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„é‡å¤–æ‹æ‘„ä»å­˜åœ¨è¯¸å¤šé—®é¢˜ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†é‡å­è¾å°„åœºï¼ˆquanta radiance fieldsï¼‰â€”â€”æ–°å‹ç¥ç»ç½‘ç»œè¾å°„åœºæ–¹æ³•ï¼Œé€‚ç”¨äºå•ä¸ªå…‰å­ç›¸æœºï¼ˆSPCï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç†è®ºåŠå®ç”¨è®¡ç®—æŠ€æœ¯æ„å»ºè¾å°„åœºï¼Œå¹¶ä¼°ç®—å¯†é›†ç›¸æœºå§¿æ€ã€‚</li>
<li>é‡å­è¾å°„åœºå¯ä»¥åœ¨é«˜é€Ÿè¿åŠ¨ã€ä½å…‰å’Œé«˜åŠ¨æ€èŒƒå›´è®¾ç½®ä¸‹è¿›è¡Œé«˜ä¿çœŸé‡å»ºã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå’ŒSPCç¡¬ä»¶åŸå‹éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æŠ€æœ¯æœ‰åŠ©äºæ”¹è¿›åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å›¾åƒæ•è·å’Œé‡å»ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71eb6f59f20fd50d6eb4cd1e7d45ca3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4393eeb12b20ed2ec5f61f94e132af2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-184dbb4d5182717eba6fc12572268ac0.jpg" align="middle">
</details>




<h2 id="LLaNA-Large-Language-and-NeRF-Assistant"><a href="#LLaNA-Large-Language-and-NeRF-Assistant" class="headerlink" title="LLaNA: Large Language and NeRF Assistant"></a>LLaNA: Large Language and NeRF Assistant</h2><p><strong>Authors:Andrea Amaduzzi, Pierluigi Zama Ramirez, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated an excellent understanding of images and 3D data. However, both modalities have shortcomings in holistically capturing the appearance and geometry of objects. Meanwhile, Neural Radiance Fields (NeRFs), which encode information within the weights of a simple Multi-Layer Perceptron (MLP), have emerged as an increasingly widespread modality that simultaneously encodes the geometry and photorealistic appearance of objects. This paper investigates the feasibility and effectiveness of ingesting NeRF into MLLM. We create LLaNA, the first general-purpose NeRF-language assistant capable of performing new tasks such as NeRF captioning and Q&amp;A. Notably, our method directly processes the weights of the NeRFâ€™s MLP to extract information about the represented objects without the need to render images or materialize 3D data structures. Moreover, we build a dataset of NeRFs with text annotations for various NeRF-language tasks with no human intervention. Based on this dataset, we develop a benchmark to evaluate the NeRF understanding capability of our method. Results show that processing NeRF weights performs favourably against extracting 2D or 3D representations from NeRFs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»å±•ç°å‡ºå¯¹å›¾åƒå’Œ3Dæ•°æ®çš„å‡ºè‰²ç†è§£ã€‚ç„¶è€Œï¼Œä¸¤ç§æ¨¡å¼åœ¨å…¨é¢æ•æ‰ç‰©ä½“çš„å¤–è§‚å’Œå‡ ä½•ç‰¹å¾æ–¹é¢éƒ½å­˜åœ¨ä¸è¶³ã€‚åŒæ—¶ï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä½œä¸ºä¸€ç§æ—¥ç›Šæ™®åŠçš„æ¨¡å¼ï¼Œå®ƒé€šè¿‡ç®€å•çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„æƒé‡ç¼–ç ä¿¡æ¯ï¼Œèƒ½å¤ŸåŒæ—¶ç¼–ç ç‰©ä½“çš„å‡ ä½•å’Œé€¼çœŸçš„å¤–è§‚ã€‚æœ¬æ–‡ç ”ç©¶äº†å°†NeRFçº³å…¥MLLMçš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åˆ›å»ºäº†LLaNAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šç”¨çš„NeRFè¯­è¨€åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œæ–°çš„ä»»åŠ¡ï¼Œå¦‚NeRFæè¿°å’Œé—®ç­”ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥å¤„ç†NeRFçš„MLPæƒé‡æ¥æå–æœ‰å…³æ‰€è¡¨ç¤ºå¯¹è±¡çš„ä¿¡æ¯ï¼Œæ— éœ€å‘ˆç°å›¾åƒæˆ–å½¢æˆä¸‰ç»´æ•°æ®ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¸¦æœ‰å„ç§NeRFè¯­è¨€ä»»åŠ¡çš„æ–‡æœ¬æ³¨é‡Šçš„NeRFæ•°æ®é›†ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„NeRFç†è§£èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œå¤„ç†NeRFæƒé‡æ¯”ä»NeRFä¸­æå–äºŒç»´æˆ–ä¸‰ç»´è¡¨ç¤ºæ›´ä¸ºæœ‰åˆ©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11840v2">PDF</a> Under review. Project page: <a target="_blank" rel="noopener" href="https://andreamaduzzi.github.io/llana/">https://andreamaduzzi.github.io/llana/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†Neural Radiance Fieldsï¼ˆNeRFï¼‰æŠ€æœ¯å¼•å…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„ç ”ç©¶ã€‚é€šè¿‡åˆ›å»ºåä¸ºLLaNAçš„é¦–ä¸ªé€šç”¨NeRFè¯­è¨€åŠ©ç†ï¼Œè¯¥è®ºæ–‡å±•ç¤ºäº†å°†NeRFä¸è¯­è¨€å¤„ç†ç»“åˆçš„èƒ½åŠ›ï¼Œå®ç°äº†NeRFæè¿°å’Œé—®ç­”ç­‰æ–°å‹ä»»åŠ¡ã€‚è¯¥ç ”ç©¶ç›´æ¥å¤„ç†NeRFçš„MLPæƒé‡ä»¥æå–å¯¹è±¡ä¿¡æ¯ï¼Œæ— éœ€æ¸²æŸ“å›¾åƒæˆ–å®ç°3Dæ•°æ®ç»“æ„ã€‚æ­¤å¤–ï¼Œè®ºæ–‡å»ºç«‹äº†ä¸€ä¸ªå¸¦æœ‰æ–‡æœ¬æ³¨é‡Šçš„NeRFæ•°æ®é›†ï¼Œç”¨äºå„ç§NeRFè¯­è¨€ä»»åŠ¡ï¼Œå¹¶åŸºäºæ­¤æ•°æ®é›†å¼€å‘äº†ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤„ç†NeRFæƒé‡çš„æ–¹æ³•ä¼˜äºä»NeRFä¸­æå–2Dæˆ–3Dè¡¨ç¤ºçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨ç†è§£å’Œå¤„ç†å›¾åƒå’Œ3Dæ•°æ®ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä»å­˜åœ¨å¯¹ç‰©ä½“å¤–è§‚å’Œå‡ ä½•ç»“æ„æ•´ä½“æ•æ‰çš„ä¸è¶³ã€‚</li>
<li>Neural Radiance Fields (NeRF)æŠ€æœ¯èƒ½åŒæ—¶ç¼–ç ç‰©ä½“çš„å‡ ä½•å’Œé€¼çœŸçš„å¤–è§‚ä¿¡æ¯ã€‚</li>
<li>LLaNAæ˜¯é¦–ä¸ªé€šç”¨NeRFè¯­è¨€åŠ©ç†ï¼Œèƒ½æ‰§è¡ŒNeRFæè¿°å’Œé—®ç­”ç­‰æ–°å‹ä»»åŠ¡ã€‚</li>
<li>è¯¥ç ”ç©¶ç›´æ¥å¤„ç†NeRFçš„MLPæƒé‡æå–ä¿¡æ¯ï¼Œæ— éœ€é€šè¿‡å›¾åƒæ¸²æŸ“æˆ–å®ç°3Dæ•°æ®ç»“æ„ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªå¸¦æœ‰æ–‡æœ¬æ³¨é‡Šçš„NeRFæ•°æ®é›†ï¼Œç”¨äºå„ç§NeRFè¯­è¨€ä»»åŠ¡ï¼Œä¸”è¯¥æ•°æ®é›†æ— éœ€äººå·¥å¹²é¢„ã€‚</li>
<li>åŸºäºè¯¥æ•°æ®é›†å¼€å‘äº†ä¸€ä¸ªè¯„ä¼°æ–¹æ³•ï¼Œä»¥è¯„ä¼°å¯¹NeRFçš„ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0fe5220e7fbfa05e2e1a9a0b32702a80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5518be5d5aa2e4668cded306ad906a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-600e159f20c2038c3bd6acd9bb52a325.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2568b7fa8db0fe2a2ce7075866517cd1.jpg" align="middle">
</details>




<h2 id="PUP-3D-GS-Principled-Uncertainty-Pruning-for-3D-Gaussian-Splatting"><a href="#PUP-3D-GS-Principled-Uncertainty-Pruning-for-3D-Gaussian-Splatting" class="headerlink" title="PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting"></a>PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting</h2><p><strong>Authors:Alex Hanson, Allen Tu, Vasu Singla, Mayuka Jayawardhana, Matthias Zwicker, Tom Goldstein</strong></p>
<p>Recent advances in novel view synthesis have enabled real-time rendering speeds with high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a foundational point-based parametric 3D scene representation, models scenes as large sets of 3D Gaussians. However, complex scenes can consist of millions of Gaussians, resulting in high storage and memory requirements that limit the viability of 3D-GS on devices with limited resources. Current techniques for compressing these pretrained models by pruning Gaussians rely on combining heuristics to determine which Gaussians to remove. At high compression ratios, these pruned scenes suffer from heavy degradation of visual fidelity and loss of foreground details. In this paper, we propose a principled sensitivity pruning score that preserves visual fidelity and foreground details at significantly higher compression ratios than existing approaches. It is computed as a second-order approximation of the reconstruction error on the training views with respect to the spatial parameters of each Gaussian. Additionally, we propose a multi-round prune-refine pipeline that can be applied to any pretrained 3D-GS model without changing its training pipeline. After pruning 90% of Gaussians, a substantially higher percentage than previous methods, our PUP 3D-GS pipeline increases average rendering speed by 3.56$\times$ while retaining more salient foreground information and achieving higher image quality metrics than existing techniques on scenes from the Mip-NeRF 360, Tanks &amp; Temples, and Deep Blending datasets. </p>
<blockquote>
<p>æœ€æ–°çš„è§†ç‚¹åˆæˆæŠ€æœ¯è¿›å±•å·²ç»å®ç°äº†é«˜é‡å»ºç²¾åº¦çš„å®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚ä¸‰ç»´é«˜æ–¯å®šä½æŠ€æœ¯ï¼ˆ3D-GSï¼‰æ˜¯ä¸€ç§åŸºäºç‚¹çš„å‚æ•°åŒ–ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå°†åœºæ™¯å»ºæ¨¡ä¸ºå¤§é‡ä¸‰ç»´é«˜æ–¯æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå¤æ‚åœºæ™¯å¯èƒ½åŒ…å«æ•°ç™¾ä¸‡ä¸ªé«˜æ–¯æ•°æ®ï¼Œå¯¼è‡´å­˜å‚¨å’Œå†…å­˜éœ€æ±‚å¾ˆé«˜ï¼Œé™åˆ¶äº†èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šä½¿ç”¨ä¸‰ç»´é«˜æ–¯å®šä½æŠ€æœ¯çš„å¯è¡Œæ€§ã€‚å½“å‰é€šè¿‡ä¿®å‰ªé«˜æ–¯å€¼æ¥å‹ç¼©è¿™äº›é¢„è®­ç»ƒæ¨¡å‹çš„æŠ€æœ¯ä¾èµ–äºç»“åˆå¯å‘å¼æ–¹æ³•æ¥ç¡®å®šè¦åˆ é™¤å“ªäº›é«˜æ–¯å€¼ã€‚åœ¨é«˜å‹ç¼©ç‡ä¸‹ï¼Œè¿™äº›ä¿®å‰ªåçš„åœºæ™¯åœ¨è§†è§‰ä¿çœŸåº¦ä¸Šé­å—ä¸¥é‡æŸå¤±ï¼Œå‰æ™¯ç»†èŠ‚ä¸¢å¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰åŸåˆ™çš„æ•æ„Ÿæ€§ä¿®å‰ªè¯„åˆ†ï¼Œèƒ½å¤Ÿåœ¨æ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„å‹ç¼©ç‡ä¸‹ä¿æŒè§†è§‰ä¿çœŸåº¦å’Œå‰æ™¯ç»†èŠ‚ã€‚å®ƒæ˜¯é€šè¿‡è®¡ç®—è®­ç»ƒè§†å›¾ä¸Šç›¸å¯¹äºæ¯ä¸ªé«˜æ–¯çš„ç©ºé—´å‚æ•°çš„é‡å»ºè¯¯å·®çš„äºŒé˜¶è¿‘ä¼¼æ¥è®¡ç®—çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šè½®ä¿®å‰ªå’Œç²¾ç‚¼ç®¡é“ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„ä¸‰ç»´é«˜æ–¯å®šä½æ¨¡å‹ï¼Œè€Œæ— éœ€æ›´æ”¹å…¶è®­ç»ƒç®¡é“ã€‚ä¿®å‰ª90%çš„é«˜æ–¯åï¼Œæˆ‘ä»¬çš„PUP 3D-GSç®¡é“åœ¨ä¿æŒæ›´çªå‡ºçš„å‰æ™¯ä¿¡æ¯å’Œå®ç°æ¯”ç°æœ‰æŠ€æœ¯åœ¨Mip-NeRF 360ã€å¦å…‹ä¸å¯ºåº™ä»¥åŠæ·±åº¦æ··åˆæ•°æ®é›†ä¸Šçš„æ›´é«˜å›¾åƒè´¨é‡æŒ‡æ ‡çš„åŒæ—¶ï¼Œæé«˜äº†å¹³å‡æ¸²æŸ“é€Ÿåº¦é«˜è¾¾3.56å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.10219v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºæ•æ„Ÿåº¦è¯„ä¼°çš„è£å‰ªç­–ç•¥ï¼Œç”¨äºå‹ç¼©ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆ3D-GSï¼‰æ¨¡å‹ï¼Œèƒ½åœ¨é«˜å‹ç¼©ç‡ä¸‹ä¿æŒè§†è§‰ä¿çœŸåº¦å’Œå‰æ™¯ç»†èŠ‚ã€‚è¯¥ç­–ç•¥é€šè¿‡è®¡ç®—é‡å»ºè¯¯å·®çš„äºŒé˜¶è¿‘ä¼¼å€¼æ¥ç¡®å®šæ¯ä¸ªé«˜æ–¯çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºå¤šè½®è£å‰ª-ä¼˜åŒ–æµç¨‹ï¼Œå¯åº”ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„3D-GSæ¨¡å‹ï¼Œæ— éœ€æ”¹å˜å…¶è®­ç»ƒæµç¨‹ã€‚ç ”ç©¶åœ¨Mip-NeRF 360ã€Tanks &amp; Templeså’ŒDeep Blendingæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è£å‰ª90%çš„é«˜æ–¯åï¼Œå¹³å‡æ¸²æŸ“é€Ÿåº¦æé«˜3.56å€ï¼ŒåŒæ—¶ä¿ç•™æ›´å¤šæ˜¾è‘—çš„å‰æ™¯ä¿¡æ¯ï¼Œè¾¾åˆ°æ›´é«˜çš„å›¾åƒè´¨é‡æŒ‡æ ‡ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åˆ©ç”¨æ•æ„Ÿåº¦è¯„ä¼°çš„è£å‰ªç­–ç•¥å‹ç¼©ä¸‰ç»´é«˜æ–¯åœºæ™¯è¡¨ç¤ºæ¨¡å‹ï¼Œå®ç°é«˜å‹ç¼©ç‡ä¸‹çš„è§†è§‰ä¿çœŸåº¦ä¿æŒã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºé‡å»ºè¯¯å·®äºŒé˜¶è¿‘ä¼¼å€¼çš„æ•æ„Ÿåº¦è¯„åˆ†æœºåˆ¶ï¼Œç¡®å®šæ¯ä¸ªé«˜æ–¯çš„é‡è¦æ€§ã€‚</li>
<li>å¼•å…¥å¤šè½®è£å‰ª-ä¼˜åŒ–æµç¨‹ï¼Œé€‚ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„3D-GSæ¨¡å‹ï¼Œæ— éœ€æ”¹å˜å…¶è®­ç»ƒæµç¨‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¹…æé«˜æ¸²æŸ“é€Ÿåº¦çš„åŒæ—¶ï¼Œä¿ç•™äº†æ˜¾è‘—çš„å‰æ™¯ä¿¡æ¯ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„å›¾åƒè´¨é‡æŒ‡æ ‡ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œèƒ½å¤Ÿåœ¨æ›´é«˜çš„å‹ç¼©ç‡ä¸‹å·¥ä½œï¼Œä¸”æ€§èƒ½è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºèµ„æºå—é™çš„è®¾å¤‡ä¸Šçš„å®æ—¶æ¸²æŸ“åº”ç”¨å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e391c046149bcc8211ed71bdb28cea6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f2e2c16c57d98f70a977b230d638211.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bddcc0df99686e472c7ebcb8734970e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfda0825b6baf796b47c2d699e98c643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a03d0121d53e6c44de8b8879810d467.jpg" align="middle">
</details>




<h2 id="Benchmarking-Neural-Radiance-Fields-for-Autonomous-Robots-An-Overview"><a href="#Benchmarking-Neural-Radiance-Fields-for-Autonomous-Robots-An-Overview" class="headerlink" title="Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview"></a>Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview</h2><p><strong>Authors:Yuhang Ming, Xingrui Yang, Weihan Wang, Zheng Chen, Jinglun Feng, Yifan Xing, Guofeng Zhang</strong></p>
<p>Neural Radiance Fields (NeRF) have emerged as a powerful paradigm for 3D scene representation, offering high-fidelity renderings and reconstructions from a set of sparse and unstructured sensor data. In the context of autonomous robotics, where perception and understanding of the environment are pivotal, NeRF holds immense promise for improving performance. In this paper, we present a comprehensive survey and analysis of the state-of-the-art techniques for utilizing NeRF to enhance the capabilities of autonomous robots. We especially focus on the perception, localization and navigation, and decision-making modules of autonomous robots and delve into tasks crucial for autonomous operation, including 3D reconstruction, segmentation, pose estimation, simultaneous localization and mapping (SLAM), navigation and planning, and interaction. Our survey meticulously benchmarks existing NeRF-based methods, providing insights into their strengths and limitations. Moreover, we explore promising avenues for future research and development in this domain. Notably, we discuss the integration of advanced techniques such as 3D Gaussian splatting (3DGS), large language models (LLM), and generative AIs, envisioning enhanced reconstruction efficiency, scene understanding, decision-making capabilities. This survey serves as a roadmap for researchers seeking to leverage NeRFs to empower autonomous robots, paving the way for innovative solutions that can navigate and interact seamlessly in complex environments. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„3Dåœºæ™¯è¡¨ç¤ºèŒƒå¼ï¼Œèƒ½å¤Ÿä»ç¨€ç–çš„éç»“æ„åŒ–ä¼ æ„Ÿå™¨æ•°æ®ä¸­ç”Ÿæˆé«˜ä¿çœŸæ¸²æŸ“å’Œé‡å»ºã€‚åœ¨è‡ªä¸»æœºå™¨äººé¢†åŸŸï¼Œç¯å¢ƒçš„æ„ŸçŸ¥å’Œç†è§£è‡³å…³é‡è¦ï¼ŒNeRFåœ¨æå‡æœºå™¨äººæ€§èƒ½æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚æœ¬æ–‡å…¨é¢å›é¡¾å’Œåˆ†æäº†åˆ©ç”¨NeRFå¢å¼ºè‡ªä¸»æœºå™¨äººèƒ½åŠ›çš„æœ€æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨è‡ªä¸»æœºå™¨äººçš„æ„ŸçŸ¥ã€å®šä½å’Œå¯¼èˆªä»¥åŠå†³ç­–æ¨¡å—ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†è‡ªä¸»æ“ä½œçš„å…³é”®ä»»åŠ¡ï¼ŒåŒ…æ‹¬3Dé‡å»ºã€åˆ†å‰²ã€å§¿æ€ä¼°è®¡ã€åŒæ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰ã€å¯¼èˆªå’Œè§„åˆ’ä»¥åŠäº¤äº’ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥ä»”ç»†è¯„ä¼°äº†ç°æœ‰çš„åŸºäºNeRFçš„æ–¹æ³•ï¼Œæ·±å…¥äº†è§£å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†æœªæ¥è¯¥é¢†åŸŸç ”ç©¶å’Œå‘å±•çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è®¨è®ºäº†é›†æˆå…ˆè¿›æŠ€æœ¯ï¼Œå¦‚3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼Œè®¾æƒ³æé«˜é‡å»ºæ•ˆç‡ã€åœºæ™¯ç†è§£å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬ç»¼è¿°ä¸ºå¸Œæœ›åˆ©ç”¨NeRFèµ‹èƒ½è‡ªä¸»æœºå™¨äººçš„ç ”ç©¶è€…æä¾›äº†è·¯çº¿å›¾ï¼Œä¸ºåœ¨å¤æ‚ç¯å¢ƒä¸­æ— ç¼å¯¼èˆªå’Œäº¤äº’çš„åˆ›æ–°è§£å†³æ–¹æ¡ˆé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.05526v3">PDF</a> 32 pages, 5 figures, 8 tables</p>
<p><strong>Summary</strong></p>
<p>NeRFæŠ€æœ¯ä¸ºè‡ªä¸»æœºå™¨äººæä¾›äº†å¼ºå¤§çš„ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½æé«˜æ„ŸçŸ¥ã€å®šä½ã€å¯¼èˆªå’Œå†³ç­–ç­‰å…³é”®æ¨¡å—çš„æ€§èƒ½ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†åˆ©ç”¨NeRFæŠ€æœ¯å¢å¼ºè‡ªä¸»æœºå™¨äººèƒ½åŠ›çš„æœ€æ–°è¿›å±•ï¼Œæ¢è®¨äº†å…¶åœ¨ä¸‰ç»´é‡å»ºã€åˆ†å‰²ã€å§¿æ€ä¼°è®¡ã€SLAMã€å¯¼èˆªè§„åˆ’ä»¥åŠäº¤äº’ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¦‚é›†æˆ3DGSã€LLMå’Œç”Ÿæˆå¼AIç­‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯æˆä¸ºè‡ªä¸»æœºå™¨äººé¢†åŸŸå¼ºå¤§çš„ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li>NeRFèƒ½æé«˜è‡ªä¸»æœºå™¨äººçš„æ„ŸçŸ¥ã€å®šä½ã€å¯¼èˆªå’Œå†³ç­–ç­‰å…³é”®æ¨¡å—æ€§èƒ½ã€‚</li>
<li>ç»¼è¿°äº†åˆ©ç”¨NeRFæŠ€æœ¯å¢å¼ºè‡ªä¸»æœºå™¨äººçš„æœ€æ–°è¿›å±•ã€‚</li>
<li>æ¢è®¨äº†NeRFåœ¨ä¸‰ç»´é‡å»ºã€åˆ†å‰²ã€å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>ä»‹ç»äº†NeRFä¸SLAMã€å¯¼èˆªè§„åˆ’ä»¥åŠäº¤äº’ç­‰ä»»åŠ¡çš„ç»“åˆã€‚</li>
<li>æ¢è®¨äº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬é›†æˆ3DGSã€LLMå’Œç”Ÿæˆå¼AIç­‰æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2660267a9a5884994ee8dccbb775487.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53a93195ec232e95631328ba353f9a8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87811ac48594dd77a181b5b9bafa534b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec9a29b48f67e9870f05f7ea559f232f.jpg" align="middle">
</details>




<h2 id="DGE-Direct-Gaussian-3D-Editing-by-Consistent-Multi-view-Editing"><a href="#DGE-Direct-Gaussian-3D-Editing-by-Consistent-Multi-view-Editing" class="headerlink" title="DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing"></a>DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing</h2><p><strong>Authors:Minghao Chen, Iro Laina, Andrea Vedaldi</strong></p>
<p>We consider the problem of editing 3D objects and scenes based on open-ended language instructions. A common approach to this problem is to use a 2D image generator or editor to guide the 3D editing process, obviating the need for 3D data. However, this process is often inefficient due to the need for iterative updates of costly 3D representations, such as neural radiance fields, either through individual view edits or score distillation sampling. A major disadvantage of this approach is the slow convergence caused by aggregating inconsistent information across views, as the guidance from 2D models is not multi-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a method that addresses these issues in two stages. First, we modify a given high-quality image editor like InstructPix2Pix to be multi-view consistent. To do so, we propose a training-free approach that integrates cues from the 3D geometry of the underlying scene. Second, given a multi-view consistent edited sequence of images, we directly and efficiently optimize the 3D representation, which is based on 3D Gaussian Splatting. Because it avoids incremental and iterative edits, DGE is significantly more accurate and efficient than existing approaches and offers additional benefits, such as enabling selective editing of parts of the scene. </p>
<blockquote>
<p>æˆ‘ä»¬è€ƒè™‘äº†åŸºäºå¼€æ”¾è¯­è¨€æŒ‡ä»¤ç¼–è¾‘3Då¯¹è±¡å’Œåœºæ™¯çš„é—®é¢˜ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„å¸¸è§æ–¹æ³•æ˜¯ä½¿ç”¨2Då›¾åƒç”Ÿæˆå™¨æˆ–ç¼–è¾‘å™¨æ¥æŒ‡å¯¼3Dç¼–è¾‘è¿‡ç¨‹ï¼Œä»è€Œæ— éœ€3Dæ•°æ®ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦è¿­ä»£æ›´æ–°æ˜‚è´µçš„3Dè¡¨ç¤ºï¼ˆå¦‚ç¥ç»è¾å°„åœºï¼‰ï¼Œè¿™ä¸€è¿‡ç¨‹é€šå¸¸æ•ˆç‡ä¸é«˜ï¼Œæ— è®ºæ˜¯é€šè¿‡å•ç‹¬çš„è§†å›¾ç¼–è¾‘è¿˜æ˜¯åˆ†æ•°è’¸é¦é‡‡æ ·ã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦ç¼ºç‚¹æ˜¯ç”±è·¨è§†å›¾çš„ä¸ä¸€è‡´ä¿¡æ¯èšåˆå¯¼è‡´çš„æ”¶æ•›ç¼“æ…¢ï¼Œå› ä¸ºæ¥è‡ª2Dæ¨¡å‹çš„æŒ‡å¯¼å¹¶éå¤šè§†å›¾ä¸€è‡´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†Direct Gaussian Editorï¼ˆDGEï¼‰æ–¹æ³•ï¼Œåˆ†ä¸¤ä¸ªé˜¶æ®µè§£å†³è¿™äº›é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¿®æ”¹ç»™å®šçš„é«˜è´¨é‡å›¾åƒç¼–è¾‘å™¨ï¼ˆå¦‚InstructPix2Pixï¼‰ï¼Œä½¿å…¶å…·æœ‰å¤šè§†å›¾ä¸€è‡´æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åº•å±‚åœºæ™¯3Då‡ ä½•çš„çº¿ç´¢ã€‚å…¶æ¬¡ï¼Œç»™å®šä¸€ä¸ªå…·æœ‰å¤šè§†å›¾ä¸€è‡´æ€§çš„å›¾åƒç¼–è¾‘åºåˆ—ï¼Œæˆ‘ä»¬ç›´æ¥æœ‰æ•ˆåœ°ä¼˜åŒ–åŸºäº3Dé«˜æ–¯æ‹¼è´´çš„3Dè¡¨ç¤ºã€‚ç”±äºé¿å…äº†å¢é‡å’Œè¿­ä»£ç¼–è¾‘ï¼ŒDGEæ¯”ç°æœ‰æ–¹æ³•æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆï¼Œå¹¶æä¾›äº†é¢å¤–çš„ä¼˜åŠ¿ï¼Œä¾‹å¦‚èƒ½å¤Ÿé€‰æ‹©æ€§åœ°ç¼–è¾‘åœºæ™¯çš„éƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.18929v3">PDF</a> ECCV 2024. Project Page: <a target="_blank" rel="noopener" href="https://silent-chen.github.io/DGE/">https://silent-chen.github.io/DGE/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³åŸºäºå¼€æ”¾è¯­è¨€æŒ‡ä»¤ç¼–è¾‘3Dç‰©ä½“å’Œåœºæ™¯çš„é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨2Då›¾åƒç”Ÿæˆå™¨æˆ–ç¼–è¾‘å™¨æŒ‡å¯¼3Dç¼–è¾‘è¿‡ç¨‹ï¼Œæ— éœ€3Dæ•°æ®ï¼Œä½†æ•ˆç‡ä½ä¸‹ï¼Œä¸”éœ€è¿­ä»£æ›´æ–°æ˜‚è´µçš„3Dè¡¨ç¤ºã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥Direct Gaussian Editorï¼ˆDGEï¼‰æ–¹æ³•ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œæ”¹é€ ç°æœ‰é«˜è´¨é‡å›¾åƒç¼–è¾‘å™¨å¦‚InstructPix2Pixï¼Œå®ç°å¤šè§†è§’ä¸€è‡´æ€§ï¼›å…¶æ¬¡ï¼Œç»™å®šå¤šè§†è§’ä¸€è‡´æ€§çš„ç¼–è¾‘å›¾åƒåºåˆ—ï¼Œç›´æ¥é«˜æ•ˆä¼˜åŒ–åŸºäº3Dé«˜æ–¯æ˜ å°„çš„3Dè¡¨ç¤ºã€‚DGEæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¹¶æä¾›äº†é¢å¤–çš„å¥½å¤„ï¼Œå¦‚èƒ½å¤Ÿé€‰æ‹©æ€§åœ°ç¼–è¾‘åœºæ™¯çš„æŸäº›éƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç¼–è¾‘åŸºäºå¼€æ”¾è¯­è¨€æŒ‡ä»¤çš„3Dç‰©ä½“å’Œåœºæ™¯æ•ˆç‡ä½ä¸‹ï¼Œä¸”éœ€è¦è¿­ä»£æ›´æ–°æ˜‚è´µçš„3Dè¡¨ç¤ºã€‚</li>
<li>Direct Gaussian Editor (DGE) æ–¹æ³•å¼•å…¥ä¸¤ä¸ªé˜¶æ®µæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µæ”¹é€ é«˜è´¨é‡å›¾åƒç¼–è¾‘å™¨ä»¥å®ç°å¤šè§†è§’ä¸€è‡´æ€§ï¼Œé€šè¿‡ç»“åˆåœºæ™¯åº•å±‚3Då‡ ä½•çš„çº¿ç´¢å®ç°æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåŸºäºå¤šè§†è§’ä¸€è‡´æ€§çš„ç¼–è¾‘å›¾åƒåºåˆ—ï¼Œç›´æ¥ä¼˜åŒ–åŸºäº3Dé«˜æ–¯æ˜ å°„çš„3Dè¡¨ç¤ºï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>DGEé¿å…äº†å¢é‡å’Œè¿­ä»£ç¼–è¾‘ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>DGEæä¾›äº†é¢å¤–å¥½å¤„ï¼Œå¦‚èƒ½å¤Ÿé€‰æ‹©æ€§åœ°ç¼–è¾‘åœºæ™¯çš„æŸäº›éƒ¨åˆ†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea9f12eb85f51db975261dacdd7ee0ad.jpg" align="middle">
</details>




<h2 id="MaGRITTe-Manipulative-and-Generative-3D-Realization-from-Image-Topview-and-Text"><a href="#MaGRITTe-Manipulative-and-Generative-3D-Realization-from-Image-Topview-and-Text" class="headerlink" title="MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview   and Text"></a>MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview   and Text</h2><p><strong>Authors:Takayuki Hara, Tatsuya Harada</strong></p>
<p>The generation of 3D scenes from user-specified conditions offers a promising avenue for alleviating the production burden in 3D applications. Previous studies required significant effort to realize the desired scene, owing to limited control conditions. We propose a method for controlling and generating 3D scenes under multimodal conditions using partial images, layout information represented in the top view, and text prompts. Combining these conditions to generate a 3D scene involves the following significant difficulties: (1) the creation of large datasets, (2) reflection on the interaction of multimodal conditions, and (3) domain dependence of the layout conditions. We decompose the process of 3D scene generation into 2D image generation from the given conditions and 3D scene generation from 2D images. 2D image generation is achieved by fine-tuning a pretrained text-to-image model with a small artificial dataset of partial images and layouts, and 3D scene generation is achieved by layout-conditioned depth estimation and neural radiance fields (NeRF), thereby avoiding the creation of large datasets. The use of a common representation of spatial information using 360-degree images allows for the consideration of multimodal condition interactions and reduces the domain dependence of the layout control. The experimental results qualitatively and quantitatively demonstrated that the proposed method can generate 3D scenes in diverse domains, from indoor to outdoor, according to multimodal conditions. </p>
<blockquote>
<p>ä»ç”¨æˆ·æŒ‡å®šçš„æ¡ä»¶ç”Ÿæˆ3Dåœºæ™¯ï¼Œä¸ºç¼“è§£3Dåº”ç”¨ä¸­çš„ç”Ÿäº§è´Ÿæ‹…æä¾›äº†æœ‰å‰æ™¯çš„é€”å¾„ã€‚ç”±äºæ§åˆ¶æ¡ä»¶çš„é™åˆ¶ï¼Œå…ˆå‰çš„ç ”ç©¶éœ€è¦å¤§é‡çš„åŠªåŠ›æ‰èƒ½å®ç°æ‰€éœ€çš„åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹åˆ©ç”¨éƒ¨åˆ†å›¾åƒã€ä¿¯è§†å›¾ä¸­çš„å¸ƒå±€ä¿¡æ¯å’Œæ–‡æœ¬æç¤ºè¿›è¡Œæ§åˆ¶å’Œç”Ÿæˆ3Dåœºæ™¯çš„æ–¹æ³•ã€‚ç»“åˆè¿™äº›æ¡ä»¶ç”Ÿæˆ3Dåœºæ™¯æ¶‰åŠä»¥ä¸‹é‡å¤§æŒ‘æˆ˜ï¼š(1)åˆ›å»ºå¤§è§„æ¨¡æ•°æ®é›†ï¼›(2)å¯¹å¤šæ¨¡æ€æ¡ä»¶äº¤äº’çš„åæ€ï¼›(3)å¸ƒå±€æ¡ä»¶çš„é¢†åŸŸä¾èµ–æ€§ã€‚æˆ‘ä»¬å°†3Dåœºæ™¯çš„ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºåŸºäºç»™å®šæ¡ä»¶çš„äºŒç»´å›¾åƒç”Ÿæˆå’ŒåŸºäºäºŒç»´å›¾åƒçš„3Dåœºæ™¯ç”Ÿæˆã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸éƒ¨åˆ†å›¾åƒå’Œå¸ƒå±€çš„å°å‹äººå·¥æ•°æ®é›†æ¥å®ç°äºŒç»´å›¾åƒç”Ÿæˆï¼Œé€šè¿‡å¸ƒå±€è°ƒèŠ‚çš„æ·±åº¦ä¼°è®¡å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¥å®ç°ä¸‰ç»´åœºæ™¯çš„ç”Ÿæˆï¼Œä»è€Œé¿å…äº†åˆ›å»ºå¤§è§„æ¨¡æ•°æ®é›†çš„éœ€è¦ã€‚ä½¿ç”¨åŸºäºå…¨æ™¯å›¾çš„å¸¸è§ç©ºé—´ä¿¡æ¯è¡¨ç¤ºå…è®¸è€ƒè™‘å¤šæ¨¡æ€æ¡ä»¶çš„äº¤äº’ï¼Œå¹¶é™ä½äº†å¸ƒå±€æ§åˆ¶çš„é¢†åŸŸä¾èµ–æ€§ã€‚å®éªŒå®šæ€§å’Œå®šé‡ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ ¹æ®å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆä¸åŒé¢†åŸŸçš„å®¤å†…å’Œå®¤å¤–ä¸‰ç»´åœºæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.00345v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://hara012.github.io/MaGRITTe-project">https://hara012.github.io/MaGRITTe-project</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨éƒ¨åˆ†å›¾åƒã€ä¿¯è§†å›¾ä¸­çš„å¸ƒå±€ä¿¡æ¯å’Œæ–‡æœ¬æç¤ºï¼Œåœ¨å¤šæ¨¡æ€æ¡ä»¶ä¸‹æ§åˆ¶å’Œç”Ÿæˆ3Dåœºæ™¯çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†3Dåœºæ™¯ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºåŸºäºç»™å®šæ¡ä»¶çš„2Då›¾åƒç”Ÿæˆå’Œä»2Då›¾åƒç”Ÿæˆ3Dåœºæ™¯ã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å’Œå°è§„æ¨¡çš„äººå·¥æ•°æ®é›†ï¼Œå®ç°2Då›¾åƒç”Ÿæˆï¼›åˆ©ç”¨å¸ƒå±€æ¡ä»¶æ·±åº¦ä¼°è®¡å’Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å®ç°3Dåœºæ™¯ç”Ÿæˆï¼Œé¿å…äº†å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ›å»ºã€‚ä½¿ç”¨360åº¦å›¾åƒçš„å…±åŒç©ºé—´ä¿¡æ¯è¡¨ç¤ºï¼Œè€ƒè™‘å¤šæ¨¡æ€æ¡ä»¶äº¤äº’ï¼Œé™ä½å¸ƒå±€æ§åˆ¶çš„é¢†åŸŸä¾èµ–æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯æ ¹æ®å¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆä»å®¤å†…åˆ°å®¤å¤–ä¸åŒé¢†åŸŸçš„3Dåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§åˆ©ç”¨éƒ¨åˆ†å›¾åƒã€ä¿¯è§†å›¾ä¸­çš„å¸ƒå±€ä¿¡æ¯å’Œæ–‡æœ¬æç¤ºï¼Œç»“åˆå¤šæ¨¡æ€æ¡ä»¶ç”Ÿæˆ3Dåœºæ™¯çš„æ–¹æ³•ã€‚</li>
<li>å°†3Dåœºæ™¯ç”Ÿæˆè¿‡ç¨‹åˆ†è§£ä¸ºåŸºäºç»™å®šæ¡ä»¶çš„2Då›¾åƒç”Ÿæˆå’Œä»2Då›¾åƒåˆ°3Dåœºæ™¯çš„è½¬æ¢ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å®ç°2Då›¾åƒç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨å¸ƒå±€æ¡ä»¶æ·±åº¦ä¼°è®¡å’ŒNeRFå®ç°3Dåœºæ™¯ç”Ÿæˆï¼Œé¿å…åˆ›å»ºå¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>ä½¿ç”¨360åº¦å›¾åƒçš„å…±åŒç©ºé—´ä¿¡æ¯è¡¨ç¤ºï¼Œè€ƒè™‘å¤šæ¨¡æ€æ¡ä»¶äº¤äº’ã€‚</li>
<li>æ–¹æ³•å¯ç”Ÿæˆä»å®¤å†…åˆ°å®¤å¤–ä¸åŒé¢†åŸŸçš„3Dåœºæ™¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-99a7afa609a61ee91491fdb8310995e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6497452af89f7710d53bc9428c4e4be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8708882d4ab0174602f1ff5af36b042d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a0d123a752e66c36cfd8d062dab99ad.jpg" align="middle">
</details>




<h2 id="CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians"><a href="#CoherentGS-Sparse-Novel-View-Synthesis-with-Coherent-3D-Gaussians" class="headerlink" title="CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians"></a>CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians</h2><p><strong>Authors:Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, Nima Khademi Kalantari</strong></p>
<p>The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes. </p>
<blockquote>
<p>åŸºäºå›¾åƒçš„3Dé‡å»ºé¢†åŸŸåœ¨è¿‡å»å‡ å¹´ä¸­å‘å±•è¿…é€Ÿï¼Œå…ˆæ˜¯å¼•å…¥äº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ï¼Œæœ€è¿‘åˆå¼•å…¥äº†3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰ã€‚åè€…åœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠé‡å»ºè´¨é‡æ–¹é¢éƒ½æ¯”NeRFæœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚è™½ç„¶3DGSå¯¹äºå¯†é›†è¾“å…¥å›¾åƒæ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºæç¨€ç–è¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚3å¼ å›¾åƒï¼‰çš„æ›´å…·æŒ‘æˆ˜æ€§çš„è®¾ç½®ï¼Œå…¶éç»“æ„åŒ–çš„ç‚¹äº‘å¼è¡¨ç¤ºå¾ˆå¿«å°±ä¼šè¿‡åº¦æ‹Ÿåˆï¼Œä»è€Œäº§ç”Ÿä»æ–°è§†è§’çœ‹å»çš„çœ‹ä¼¼æ‚ä¹±æ— ç« çš„é’ˆçŠ¶è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­å¼•å…¥ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºã€‚ç„¶åæˆ‘ä»¬å¯¹é«˜æ–¯ä½“ï¼ˆç‰¹åˆ«æ˜¯å…¶ä½ç½®ï¼‰æ–½åŠ çº¦æŸï¼Œé˜²æ­¢å®ƒä»¬åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ç‹¬ç«‹ç§»åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡éšå¼å·ç§¯è§£ç å™¨å’Œå…¨å˜å·®æŸå¤±åˆ†åˆ«å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸã€‚ç”±äºé«˜æ–¯ä½“çš„è¿è´¯æ€§ï¼Œæˆ‘ä»¬é€šè¿‡åŸºäºæµçš„æŸå¤±å‡½æ•°è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–ã€‚ä¸ºäº†æ”¯æŒæˆ‘ä»¬çš„æ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æ¯ä¸ªè¾“å…¥è§†å›¾çš„å•ç›®æ·±åº¦ä¼°è®¡æ¥åˆå§‹åŒ–é«˜æ–¯ä½“çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å„ç§åœºæ™¯ä¸Šå±•ç¤ºäº†ä¸æœ€å…ˆè¿›çš„ç¨€ç–è§†å›¾NeRFæ–¹æ³•ç›¸æ¯”çš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.19495v2">PDF</a> ECCV2024, Project page:   <a target="_blank" rel="noopener" href="https://people.engr.tamu.edu/nimak/Papers/CoherentGS">https://people.engr.tamu.edu/nimak/Papers/CoherentGS</a>, Code:   <a target="_blank" rel="noopener" href="https://github.com/avinashpaliwal/CoherentGS">https://github.com/avinashpaliwal/CoherentGS</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæä¾›çš„æ–‡æœ¬ï¼Œç ”ç©¶è€…ç ”ç©¶äº†ä½¿ç”¨NeRFè¿›è¡Œå›¾åƒçš„ä¸‰ç»´é‡å»ºé—®é¢˜ã€‚æå‡ºäº†ä¸€ç§å…·æœ‰ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºçš„æ–¹æ³•ï¼Œä»¥è§£å†³åœ¨ç¨€ç–è¾“å…¥å›¾åƒæƒ…å†µä¸‹ï¼Œé‡å»ºè´¨é‡ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–ï¼Œå¹¶é€šè¿‡å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸä»¥åŠæµæŸå¤±å‡½æ•°æ¥è¿›ä¸€æ­¥çº¦æŸä¼˜åŒ–è¿‡ç¨‹ã€‚ç›¸è¾ƒäºç°æœ‰çš„ç¨€ç–è§†å›¾NeRFæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§åœºæ™¯ä¸‹çš„è¡¨ç°æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£ï¼Œç”¨ç²¾ç®€å’Œæ¸…æ™°çš„åˆ—è¡¨å‘ˆç°ï¼š</p>
<ol>
<li>è¯¥æ–‡æœ¬è®¨è®ºäº†åœ¨ä»å›¾åƒè¿›è¡Œä¸‰ç»´é‡å»ºé¢†åŸŸä¸­çš„æœ€æ–°å‘å±•ï¼Œé‡ç‚¹å…³æ³¨Neural Radiance Fieldï¼ˆNeRFï¼‰å’Œ3D Gaussian Splattingï¼ˆ3DGSï¼‰çš„ä¼˜åŠ£ã€‚</li>
<li>ç›¸è¾ƒäºNeRFï¼Œ3DGSåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä»¥åŠé‡å»ºè´¨é‡ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚ä½†å¯¹äºç¨€ç–è¾“å…¥å›¾åƒï¼Œå…¶è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>é’ˆå¯¹ç¨€ç–è¾“å…¥å›¾åƒçš„é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†æ­£åˆ™åŒ–ä¼˜åŒ–å’ŒåŸºäºæ·±åº¦çš„åˆå§‹åŒ–æ–¹æ³•ã€‚</li>
<li>å¼•å…¥ç»“æ„åŒ–é«˜æ–¯è¡¨ç¤ºï¼Œå¹¶é€šè¿‡åœ¨äºŒç»´å›¾åƒç©ºé—´ä¸­çš„æ§åˆ¶æ¥å®ç°ã€‚</li>
<li>é€šè¿‡å¼•å…¥å•è§†å›¾å’Œå¤šè§†å›¾çº¦æŸä»¥åŠæ€»å˜å¼‚æŸå¤±å‡½æ•°ï¼Œå¯¹ä¼˜åŒ–è¿‡ç¨‹è¿›è¡Œè¿›ä¸€æ­¥çº¦æŸã€‚</li>
<li>ä¸ºäº†æ”¯æŒæ­£åˆ™åŒ–ä¼˜åŒ–ï¼Œç ”ç©¶è€…è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨å•ç›®æ·±åº¦ä¼°è®¡åœ¨æ¯ä¸ªè¾“å…¥è§†å›¾è¿›è¡Œé«˜æ–¯åˆå§‹åŒ–çš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bafcb8466fb4ce4c201dba500aeb9a9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0532b6cba479d4c7b3b98a7fcf75e7a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba5395494f10dc095357ef70300aa798.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d59f164c493ae66471659e6a4ba3729.jpg" align="middle">
</details>




<h2 id="ThermoNeRF-Joint-RGB-and-Thermal-Novel-View-Synthesis-for-Building-Facades-using-Multimodal-Neural-Radiance-Fields"><a href="#ThermoNeRF-Joint-RGB-and-Thermal-Novel-View-Synthesis-for-Building-Facades-using-Multimodal-Neural-Radiance-Fields" class="headerlink" title="ThermoNeRF: Joint RGB and Thermal Novel View Synthesis for Building   Facades using Multimodal Neural Radiance Fields"></a>ThermoNeRF: Joint RGB and Thermal Novel View Synthesis for Building   Facades using Multimodal Neural Radiance Fields</h2><p><strong>Authors:Mariam Hassan, Florent Forest, Olga Fink, Malcolm Mielle</strong></p>
<p>Thermal scene reconstruction holds great potential for various applications, such as analyzing building energy consumption and performing non-destructive infrastructure testing. However, existing methods typically require dense scene measurements and often rely on RGB images for 3D geometry reconstruction, projecting thermal information post-reconstruction. This can lead to inconsistencies between the reconstructed geometry and temperature data and their actual values. To address this challenge, we propose ThermoNeRF, a novel multimodal approach based on Neural Radiance Fields that jointly renders new RGB and thermal views of a scene, and ThermoScenes, a dataset of paired RGB+thermal images comprising 8 scenes of building facades and 8 scenes of everyday objects. To address the lack of texture in thermal images, ThermoNeRF uses paired RGB and thermal images to learn scene density, while separate networks estimate color and temperature data. Unlike comparable studies, our focus is on temperature reconstruction and experimental results demonstrate that ThermoNeRF achieves an average mean absolute error of 1.13C and 0.41C for temperature estimation in buildings and other scenes, respectively, representing an improvement of over 50% compared to using concatenated RGB+thermal data as input to a standard NeRF. Code and dataset are available online. </p>
<blockquote>
<p>çƒ­åœºæ™¯é‡å»ºåœ¨å¤šç§åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¾‹å¦‚åˆ†æå»ºç­‘èƒ½è€—å’Œæ‰§è¡Œéç ´åæ€§åŸºç¡€è®¾æ–½æµ‹è¯•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å¯†é›†çš„åœºæ™¯æµ‹é‡ï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ–äºRGBå›¾åƒè¿›è¡Œä¸‰ç»´å‡ ä½•é‡å»ºï¼Œåœ¨é‡å»ºåæŠ•å°„çƒ­ä¿¡æ¯ã€‚è¿™å¯èƒ½å¯¼è‡´é‡å»ºçš„å‡ ä½•å½¢çŠ¶ä¸æ¸©åº¦æ•°æ®åŠå…¶å®é™…å€¼ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ThermoNeRFï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„æ–°å‹å¤šæ¨¡å¼æ–¹æ³•ï¼Œå¯å…±åŒå‘ˆç°åœºæ™¯çš„RGBå’Œçƒ­åŠ›è§†å›¾ï¼Œä»¥åŠThermoScenesæ•°æ®é›†ï¼ŒåŒ…å«ç”±å»ºç­‘å¤–è§‚å’Œæ—¥å¸¸å¯¹è±¡ç»„æˆçš„é…å¯¹RGB+çƒ­åŠ›å›¾åƒã€‚ä¸ºäº†åº”å¯¹çƒ­åŠ›å›¾åƒç¼ºä¹çº¹ç†çš„é—®é¢˜ï¼ŒThermoNeRFä½¿ç”¨é…å¯¹RGBå’Œçƒ­åŠ›å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼Œè€Œå•ç‹¬çš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦æ•°æ®ã€‚ä¸ç±»ä¼¼çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯æ¸©åº¦é‡å»ºï¼Œå®éªŒç»“æœè¡¨æ˜ï¼ŒThermoNeRFåœ¨å»ºç­‘ç‰©å’Œå…¶ä»–åœºæ™¯çš„æ¸©å·®ä¼°è®¡ä¸­å¹³å‡ç»å¯¹è¯¯å·®è¾¾åˆ°1.13æ‘„æ°åº¦å’Œ0.41æ‘„æ°åº¦ï¼Œä¸ä½¿ç”¨æ ‡å‡†NeRFå°†æ‹¼æ¥çš„RGB+çƒ­åŠ›æ•°æ®ä½œä¸ºè¾“å…¥ç›¸æ¯”ï¼Œæé«˜äº†è¶…è¿‡50%ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨ç½‘ä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12154v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ThermoNeRFè¿™ä¸€åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œå¯è”åˆæ¸²æŸ“åœºæ™¯çš„RGBå’Œçº¢å¤–å›¾åƒï¼Œå¹¶è§£å†³äº†ç°æœ‰åœºæ™¯é‡å»ºæ–¹æ³•åœ¨å¤„ç†çƒ­ä¿¡æ¯æ—¶çš„ä¸è¶³ã€‚é€šè¿‡å¼•å…¥ThermoScenesæ•°æ®é›†ï¼Œå®ç°äº†å¯¹å»ºç­‘ç‰©å¤–è§‚å’Œæ—¥å¸¸ç‰©ä½“çš„RGBä¸çƒ­å›¾åƒçš„é…å¯¹ã€‚ThermoNeRFé€šè¿‡é‡‡ç”¨é…å¯¹çš„RGBå’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ï¼ŒåŒæ—¶å•ç‹¬çš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦æ•°æ®ã€‚è¯¥æ–¹æ³•ä¸“æ³¨äºæ¸©åº¦é‡å»ºï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿçš„NeRFç›¸æ¯”ï¼ŒThermoNeRFåœ¨å»ºç­‘ç‰©å’Œå…¶ä»–åœºæ™¯çš„æ¸©åº¦ä¼°è®¡æ–¹é¢å®ç°äº†å¹³å‡ç»å¯¹è¯¯å·®çš„æ˜¾è‘—é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ThermoNeRFæ˜¯ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å¤šæ¨¡æ€æ–¹æ³•ï¼Œèƒ½å¤Ÿè”åˆæ¸²æŸ“åœºæ™¯çš„RGBå’Œçº¢å¤–å›¾åƒã€‚</li>
<li>ç°æœ‰åœºæ™¯é‡å»ºæ–¹æ³•åœ¨å¤„ç†çƒ­ä¿¡æ¯æ—¶å­˜åœ¨ä¸è¶³ï¼Œè€ŒThermoNeRFè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ThermoScenesæ•°æ®é›†åŒ…å«é…å¯¹çš„RGBå’Œçƒ­å›¾åƒï¼Œç”¨äºè®­ç»ƒå’Œæµ‹è¯•ThermoNeRFã€‚</li>
<li>ThermoNeRFé€šè¿‡é‡‡ç”¨é…å¯¹çš„RGBå’Œçƒ­å›¾åƒæ¥å­¦ä¹ åœºæ™¯å¯†åº¦ã€‚</li>
<li>ThermoNeRFé‡‡ç”¨å•ç‹¬çš„ç½‘ç»œä¼°è®¡é¢œè‰²å’Œæ¸©åº¦æ•°æ®ï¼Œä»¥è§£å†³çƒ­å›¾åƒä¸­çº¹ç†ç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>ThermoNeRFä¸“æ³¨äºæ¸©åº¦é‡å»ºï¼Œå¹¶åœ¨å»ºç­‘ç‰©å’Œå…¶ä»–åœºæ™¯çš„æ¸©åº¦ä¼°è®¡æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e74d11ec6e5793a544bdc2317df2621.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa0498f2de39d156ab924e917d63e25f.jpg" align="middle">
</details>




<h2 id="BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis"><a href="#BioNeRF-Biologically-Plausible-Neural-Radiance-Fields-for-View-Synthesis" class="headerlink" title="BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis"></a>BioNeRF: Biologically Plausible Neural Radiance Fields for View   Synthesis</h2><p><strong>Authors:Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P. Costa, Ahsan Adeel, JoÃ£o Paulo Papa</strong></p>
<p>This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the sceneâ€™s 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†BioNeRFï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿç‰©å­¦ä¸Šåˆç†çš„æ¶æ„ï¼Œå®ƒé‡‡ç”¨ä¸‰ç»´è¡¨ç¤ºå¯¹åœºæ™¯è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€šè¿‡è¾å°„åœºåˆæˆæ–°çš„è§†å›¾ã€‚ç”±äºNeRFä¾èµ–äºç½‘ç»œæƒé‡æ¥å­˜å‚¨åœºæ™¯çš„ä¸‰ç»´è¡¨ç¤ºï¼ŒBioNeRFå®ç°äº†ä¸€ç§å—è®¤çŸ¥å¯å‘çš„æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†æ¥è‡ªå¤šä¸ªæºçš„è¾“å…¥èåˆåˆ°ä¸€ä¸ªç±»ä¼¼è®°å¿†çš„ç»“æ„ä¸­ï¼Œæé«˜äº†å­˜å‚¨èƒ½åŠ›ï¼Œå¹¶æå–äº†æ›´å¤šå†…åœ¨å’Œç›¸å…³çš„ä¿¡æ¯ã€‚BioNeRFè¿˜æ¨¡ä»¿äº†é”¥ä½“ç»†èƒåœ¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢çš„è¡Œä¸ºï¼Œå…¶ä¸­è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡ä¸ä¸¤ä¸ªåç»­ç¥ç»æ¨¡å‹çš„è¾“å…¥ç›¸ç»“åˆï¼Œä¸€ä¸ªè´Ÿè´£ç”Ÿæˆä½“ç§¯å¯†åº¦ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ç”Ÿæˆç”¨äºæ¸²æŸ“åœºæ™¯çš„é¢œè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBioNeRFåœ¨ä¸¤é¡¹æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯æˆæœçš„è´¨é‡è¡¡é‡æ ‡å‡†ï¼Œè¿™ä¸¤é¡¹æ•°æ®é›†åŒ…æ‹¬çœŸå®ä¸–ç•Œå›¾åƒå’Œåˆæˆæ•°æ®ï¼Œå¹¶ç¼–ç äº†äººç±»æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.07310v3">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†BioNeRFï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰ç”Ÿç‰©å­¦å¯è¡Œæ€§çš„æ¶æ„ï¼Œç”¨äºä»¥ä¸‰ç»´å½¢å¼å‘ˆç°åœºæ™¯ï¼Œå¹¶é€šè¿‡è¾å°„åœºåˆæˆæ–°è§†å›¾ã€‚BioNeRFé€šè¿‡è®¤çŸ¥å¯å‘æœºåˆ¶å°†å¤šä¸ªæ¥æºçš„è¾“å…¥èåˆåˆ°ä¸€ä¸ªè®°å¿†ç»“æ„ä¸­ï¼Œä»¥æé«˜å­˜å‚¨èƒ½åŠ›å¹¶æå–æ›´å¤šå†…åœ¨å…³è”ä¿¡æ¯ã€‚åŒæ—¶ï¼Œå®ƒæ¨¡ä»¿é‡‘å­—å¡”ç»†èƒçš„è¡Œä¸ºæ¥å¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå°†è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡ä¸ä¸¤ä¸ªåç»­ç¥ç»ç½‘ç»œæ¨¡å‹çš„è¾“å…¥ç›¸ç»“åˆï¼Œä¸€ä¸ªè´Ÿè´£ç”Ÿæˆä½“ç§¯å¯†åº¦ï¼Œå¦ä¸€ä¸ªè´Ÿè´£ç”Ÿæˆæ¸²æŸ“åœºæ™¯çš„é¢œè‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®å›¾åƒå’Œåˆæˆæ•°æ®ä¸Šï¼ŒBioNeRFåœ¨ç¼–ç äººç±»æ„ŸçŸ¥çš„è´¨é‡åº¦é‡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BioNeRFæ˜¯ä¸€ä¸ªç»“åˆäº†ç”Ÿç‰©å­¦åŸç†çš„NeRFæ¶æ„ã€‚</li>
<li>å®ƒé€šè¿‡è®¤çŸ¥å¯å‘æœºåˆ¶èåˆå¤šæºè¾“å…¥åˆ°ä¸€ä¸ªè®°å¿†ç»“æ„ä¸­ã€‚</li>
<li>BioNeRFæé«˜äº†å­˜å‚¨èƒ½åŠ›å¹¶æå–æ›´å¤šå†…åœ¨å’Œå…³è”ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¶æ„æ¨¡ä»¿é‡‘å­—å¡”ç»†èƒå¤„ç†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ–¹å¼ã€‚</li>
<li>BioNeRFä½¿ç”¨è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸ä¸¤ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ç»“åˆï¼Œåˆ†åˆ«è´Ÿè´£ç”Ÿæˆä½“ç§¯å¯†åº¦å’Œé¢œè‰²ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒBioNeRFåœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d45944546ddfeddba8ffe24bdfb453d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2aae75ca7a4ddef7cd1550e14ae719a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861a620ba8d0ef8e81a0e9c8b3bcffa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f94a8124fa1fcd15828196f53e65812.jpg" align="middle">
</details>




<h2 id="Deepfake-for-the-Good-Generating-Avatars-through-Face-Swapping-with-Implicit-Deepfake-Generation"><a href="#Deepfake-for-the-Good-Generating-Avatars-through-Face-Swapping-with-Implicit-Deepfake-Generation" class="headerlink" title="Deepfake for the Good: Generating Avatars through Face-Swapping with   Implicit Deepfake Generation"></a>Deepfake for the Good: Generating Avatars through Face-Swapping with   Implicit Deepfake Generation</h2><p><strong>Authors:Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, SÅ‚awomir Tadeja, Jacek Tabor, PrzemysÅ‚aw Spurek</strong></p>
<p>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the objectâ€™s shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the objectâ€™s characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Deepfakes refers to artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such simple strategies can produce plausible 3D deepfake-based avatars. </p>
<blockquote>
<p>ä¼—å¤šæ–°å…´çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹è®¡ç®—æœºå›¾å½¢å­¦äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚å…¶ä¸­æœ€æœ‰å‰é€”çš„çªç ´ä¹‹ä¸€æ˜¯ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯æ‹¼è´´ï¼ˆGSï¼‰çš„å…´èµ·ã€‚NeRFä½¿ç”¨å°‘é‡å·²çŸ¥ç›¸æœºä½ç½®çš„å›¾ç‰‡å°†ç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ç¼–ç åˆ°ç¥ç»ç½‘ç»œæƒé‡ä¸­ï¼Œä»è€Œç”Ÿæˆæ–°çš„è§†è§’ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGSé€šè¿‡åœ¨ä¸€ç»„é«˜æ–¯åˆ†å¸ƒä¸­ç¼–ç ç‰©ä½“ç‰¹æ€§ï¼Œå®ç°äº†è®­ç»ƒå’Œæ¨ç†çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¸é™ä½æ¸²æŸ“è´¨é‡ã€‚è¿™ä¸¤ç§æŠ€æœ¯åœ¨ç©ºé—´è®¡ç®—å’Œå…¶ä»–é¢†åŸŸæ‰¾åˆ°äº†è®¸å¤šç”¨ä¾‹ã€‚å¦ä¸€æ–¹é¢ï¼Œæ·±åº¦ä¼ªé€ æ–¹æ³•çš„å‡ºç°å¼•èµ·äº†å¾ˆå¤§çš„äº‰è®®ã€‚æ·±åº¦ä¼ªé€ æ˜¯æŒ‡ä½¿ç”¨äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘å¯†åˆ‡æ¨¡ä»¿çœŸå®é•œå¤´ã€‚ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥ä¿®æ”¹é¢éƒ¨ç‰¹å¾ï¼Œèƒ½å¤Ÿåˆ›å»ºå‡ºèº«ä»½æˆ–è¡¨æƒ…æ”¹å˜çš„å½¢è±¡ï¼Œå¯¹çœŸäººè¡¨ç°å‡ºæƒŠäººçš„çœŸå®æ„Ÿã€‚å°½ç®¡å­˜åœ¨äº‰è®®ï¼Œä½†é«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ å¯ä»¥æä¾›ä¸‹ä¸€ä»£è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåŒ–èº«åˆ›å»ºå’Œæ¸¸æˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ç»“åˆæ‰€æœ‰è¿™äº›æ–°å…´æŠ€æœ¯æ¥è·å¾—æ›´åˆç†çš„ç»“æœã€‚æˆ‘ä»¬çš„ImplicitDeepfakeä½¿ç”¨ç»å…¸çš„æ·±åº¦ä¼ªé€ ç®—æ³•åˆ†åˆ«ä¿®æ”¹æ‰€æœ‰è®­ç»ƒå›¾åƒï¼Œç„¶ååœ¨ä¿®æ”¹åçš„é¢éƒ¨ä¸Šè®­ç»ƒNeRFå’ŒGSã€‚è¿™ç§ç®€å•çš„ç­–ç•¥å¯ä»¥äº§ç”Ÿåˆç†çš„åŸºäºæ·±åº¦ä¼ªé€ çš„3DåŒ–èº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06390v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯æ¶‚æ–‘æŠ€æœ¯ï¼ˆGSï¼‰ç­‰æ·±åº¦å­¦ä¹ æŠ€æœ¯å¯¹è®¡ç®—æœºå›¾å½¢å­¦äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚NeRFä½¿ç”¨å·²çŸ¥ç›¸æœºä½ç½®çš„å°‘é‡å›¾åƒåœ¨ç¥ç»ç½‘ç»œæƒé‡ä¸­ç¼–ç å¯¹è±¡çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œä»¥ç”Ÿæˆæ–°è§†å›¾ã€‚GSé€šè¿‡åœ¨ä¸€ç»„é«˜æ–¯åˆ†å¸ƒä¸­ç¼–ç å¯¹è±¡ç‰¹æ€§ï¼Œå®ç°äº†è®­ç»ƒå’Œæ¨ç†çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¸é™ä½æ¸²æŸ“è´¨é‡ã€‚è¿™ä¸¤ç§æŠ€æœ¯éƒ½åœ¨ç©ºé—´è®¡ç®—ç­‰é¢†åŸŸæ‰¾åˆ°äº†è®¸å¤šç”¨ä¾‹ã€‚å¦ä¸€æ–¹é¢ï¼Œæ·±åº¦ä¼ªé€ æ–¹æ³•çš„å‡ºç°å¼•èµ·äº†å¾ˆå¤§çš„äº‰è®®ã€‚æ·±åº¦ä¼ªé€ æŒ‡çš„æ˜¯ä½¿ç”¨äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘æ¨¡ä»¿çœŸå®çš„ç”»é¢éå¸¸é€¼çœŸã€‚æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ç»“åˆæ‰€æœ‰è¿™äº›æ–°å…´æŠ€æœ¯æ¥è·å¾—æ›´å¯ä¿¡çš„ç»“æœã€‚æˆ‘ä»¬çš„ImplicitDeepfakeä½¿ç”¨ç»å…¸æ·±åº¦ä¼ªé€ ç®—æ³•åˆ†åˆ«ä¿®æ”¹æ‰€æœ‰è®­ç»ƒå›¾åƒï¼Œç„¶ååœ¨ä¿®æ”¹åçš„é¢éƒ¨ä¸Šè®­ç»ƒNeRFå’ŒGSã€‚è¿™ç§ç®€å•çš„ç­–ç•¥å¯ä»¥äº§ç”ŸåŸºäºæ·±åº¦ä¼ªé€ çš„3Dé€¼çœŸå¤´åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯å¦‚NeRFå’Œé«˜æ–¯æ¶‚æ–‘ï¼ˆGSï¼‰å¯¹è®¡ç®—æœºå›¾å½¢å­¦äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚</li>
<li>NeRFé€šè¿‡ç¼–ç å¯¹è±¡åœ¨ç¥ç»ç½‘ç»œæƒé‡ä¸­çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œç”Ÿæˆæ–°è§†å›¾ã€‚</li>
<li>GSèƒ½åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒæ¸²æŸ“è´¨é‡ã€‚</li>
<li>æ·±åº¦ä¼ªé€ æ–¹æ³•å¼•å‘äº†å…³äºçœŸå®æ€§å’Œé“å¾·ä½¿ç”¨çš„äº‰è®®ã€‚</li>
<li>æ·±åº¦ä¼ªé€ å¯ä»¥ç”¨äºç”Ÿæˆé€¼çœŸçš„äººå·¥æ™ºèƒ½è§†é¢‘ï¼Œå¯ä»¥ä¿®æ”¹é¢éƒ¨ç‰¹å¾ï¼Œåˆ›å»ºæ”¹å˜çš„èº«ä»½æˆ–è¡¨æƒ…ã€‚</li>
<li>æ·±åº¦ä¼ªé€ åœ¨avataråˆ›å»ºå’Œæ¸¸æˆä¸­å…·æœ‰æ½œåŠ›ï¼Œå½“è´¨é‡è¶³å¤Ÿå¥½æ—¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3aa0c5bd5d55fab6b3456408bcfa7ba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c6dd62daeb59781f75d66815146fbb5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59bdfd70e3985fd56be81d718f9a9c8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0eb9d0ad2fd4b43afa5f960c84523bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e6dceac7784672ecd9f51fb468ecff5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-89629fe07055252ace8bfce59c0bd9b0.jpg" align="middle">
</details>




<h2 id="Efficient-Dynamic-NeRF-Based-Volumetric-Video-Coding-with-Rate-Distortion-Optimization"><a href="#Efficient-Dynamic-NeRF-Based-Volumetric-Video-Coding-with-Rate-Distortion-Optimization" class="headerlink" title="Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate   Distortion Optimization"></a>Efficient Dynamic-NeRF Based Volumetric Video Coding with Rate   Distortion Optimization</h2><p><strong>Authors:Zhiyu Zhang, Guo Lu, Huanxiong Liang, Anni Tang, Qiang Hu, Li Song</strong></p>
<p>Volumetric videos, benefiting from immersive 3D realism and interactivity, hold vast potential for various applications, while the tremendous data volume poses significant challenges for compression. Recently, NeRF has demonstrated remarkable potential in volumetric video compression thanks to its simple representation and powerful 3D modeling capabilities, where a notable work is ReRF. However, ReRF separates the modeling from compression process, resulting in suboptimal compression efficiency. In contrast, in this paper, we propose a volumetric video compression method based on dynamic NeRF in a more compact manner. Specifically, we decompose the NeRF representation into the coefficient fields and the basis fields, incrementally updating the basis fields in the temporal domain to achieve dynamic modeling. Additionally, we perform end-to-end joint optimization on the modeling and compression process to further improve the compression efficiency. Extensive experiments demonstrate that our method achieves higher compression efficiency compared to ReRF on various datasets. </p>
<blockquote>
<p>ä½“ç§¯è§†é¢‘å—ç›Šäºæ²‰æµ¸å¼3Dç°å®å’Œäº¤äº’æ€§ï¼Œåœ¨å¤šç§åº”ç”¨ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œè€Œåºå¤§çš„æ•°æ®é‡ç»™å‹ç¼©å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼ŒNeRFç”±äºå…¶ç®€å•çš„è¡¨ç¤ºå½¢å¼å’Œå¼ºå¤§çš„3Då»ºæ¨¡èƒ½åŠ›ï¼Œåœ¨ä½“ç§¯è§†é¢‘å‹ç¼©æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œå…¶ä¸­ä¸€é¡¹é‡è¦å·¥ä½œæ˜¯ReRFã€‚ç„¶è€Œï¼ŒReRFå°†å»ºæ¨¡ä¸å‹ç¼©è¿‡ç¨‹åˆ†å¼€ï¼Œå¯¼è‡´å‹ç¼©æ•ˆç‡ä¸ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€NeRFçš„æ›´ç´§å‡‘çš„ä½“ç§¯è§†é¢‘å‹ç¼©æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†NeRFè¡¨ç¤ºåˆ†è§£ä¸ºç³»æ•°åœºå’ŒåŸºåœºï¼Œåœ¨æ—¶åºåŸŸä¸­å¢é‡æ›´æ–°åŸºåœºä»¥å®ç°åŠ¨æ€å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å»ºæ¨¡å’Œå‹ç¼©è¿‡ç¨‹è¿›è¡Œç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ï¼Œä»¥è¿›ä¸€æ­¥æé«˜å‹ç¼©æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç›¸å¯¹äºReRFå®ç°äº†æ›´é«˜çš„å‹ç¼©æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.01380v2">PDF</a> Accepted by IEEE ICME 2024</p>
<p><strong>æ‘˜è¦</strong></p>
<p>NeRFæŠ€æœ¯å› å…¶åœ¨ä½“ç§¯è§†é¢‘å‹ç¼©ä¸­çš„æ˜¾è‘—æ½œåŠ›è€Œå—åˆ°å…³æ³¨ï¼Œå¾—ç›Šäºå…¶ç®€å•çš„è¡¨ç°å’Œå¼ºå¤§çš„3Då»ºæ¨¡èƒ½åŠ›ã€‚å½“å‰ï¼Œå°½ç®¡å·²æœ‰è¯¸å¦‚ReRFç­‰æ–¹æ³•è¿›è¡Œä½“ç§¯è§†é¢‘å‹ç¼©ï¼Œä½†å…¶åœ¨å»ºæ¨¡ä¸å‹ç¼©è¿‡ç¨‹ä¸­å­˜åœ¨åˆ†ç¦»ç°è±¡ï¼Œå¯¼è‡´å‹ç¼©æ•ˆç‡è¾ƒä½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€NeRFçš„ä½“ç§¯è§†é¢‘å‹ç¼©æ–¹æ³•ï¼Œå°†NeRFè¡¨ç¤ºåˆ†è§£ä¸ºç³»æ•°åœºå’ŒåŸºåœºï¼Œåœ¨æ—¶åºåŸŸä¸­å¢é‡æ›´æ–°åŸºåœºä»¥å®ç°åŠ¨æ€å»ºæ¨¡ã€‚åŒæ—¶ï¼Œå¯¹å»ºæ¨¡å’Œå‹ç¼©è¿‡ç¨‹è¿›è¡Œç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜å‹ç¼©æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å‹ç¼©æ•ˆç‡é«˜äºReRFã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>NeRFæŠ€æœ¯åœ¨ä½“ç§¯è§†é¢‘å‹ç¼©ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œå¾—ç›Šäºå…¶ç®€å•çš„è¡¨ç°å’Œå¼ºå¤§çš„3Då»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•å¦‚ReRFåœ¨å»ºæ¨¡ä¸å‹ç¼©è¿‡ç¨‹ä¸­å­˜åœ¨åˆ†ç¦»ï¼Œå¯¼è‡´å‹ç¼©æ•ˆç‡è¾ƒä½ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºåŠ¨æ€NeRFçš„ä½“ç§¯è§†é¢‘å‹ç¼©æ–¹æ³•ï¼Œå°†NeRFåˆ†è§£ä¸ºç³»æ•°åœºå’ŒåŸºåœºã€‚</li>
<li>å¢é‡æ›´æ–°åŸºåœºåœ¨æ—¶åºåŸŸä¸­å®ç°åŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>ç«¯åˆ°ç«¯çš„è”åˆä¼˜åŒ–æé«˜äº†å‹ç¼©æ•ˆç‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å‹ç¼©æ•ˆç‡é«˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7872770e78a75971615216d16ea7c643.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47493b52e0e6aa419175da17168203c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e427a690cee4eeb584431c116142772.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39985ef415f95092b99400f3f589f29.jpg" align="middle">
</details>




<h2 id="ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Field"><a href="#ProvNeRF-Modeling-per-Point-Provenance-in-NeRFs-as-a-Stochastic-Field" class="headerlink" title="ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field"></a>ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Field</h2><p><strong>Authors:Kiyohiro Nakayama, Mikaela Angelina Uy, Yang You, Ke Li, Leonidas J. Guibas</strong></p>
<p>Neural radiance fields (NeRFs) have gained popularity with multiple works showing promising results across various applications. However, to the best of our knowledge, existing works do not explicitly model the distribution of training camera poses, or consequently the triangulation quality, a key factor affecting reconstruction quality dating back to classical vision literature. We close this gap with ProvNeRF, an approach that models the \textbf{provenance} for each point â€“ i.e., the locations where it is likely visible â€“ of NeRFs as a stochastic field. We achieve this by extending implicit maximum likelihood estimation (IMLE) to functional space with an optimizable objective. We show that modeling per-point provenance during the NeRF optimization enriches the model with information on triangulation leading to improvements in novel view synthesis and uncertainty estimation under the challenging sparse, unconstrained view setting against competitive baselines. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰åœ¨å¤šä»½å·¥ä½œä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¹¶åœ¨å„ç§åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬äº†è§£ï¼Œç°æœ‰å·¥ä½œå¹¶æ²¡æœ‰æ˜¾å¼åœ°æ¨¡æ‹Ÿè®­ç»ƒç›¸æœºå§¿æ€çš„åˆ†å¸ƒï¼Œä¹Ÿæ²¡æœ‰æ¨¡æ‹Ÿç”±æ­¤äº§ç”Ÿçš„ä¸‰è§’æµ‹é‡è´¨é‡ï¼Œè¿™æ˜¯ä»ç»å…¸è§†è§‰æ–‡çŒ®å¼€å§‹å°±å½±å“é‡å»ºè´¨é‡çš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬å€ŸåŠ©ProvNeRFæ–¹æ³•å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œè¯¥æ–¹æ³•ä¸ºNeRFçš„æ¯ä¸ªç‚¹â€”â€”å³å¯èƒ½å¯è§çš„ä½ç½®â€”â€”å»ºç«‹ä¸€ä¸ªéšæœºåœºæ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡å°†éšæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆIMLEï¼‰æ‰©å±•åˆ°åŠŸèƒ½ç©ºé—´ï¼Œå¹¶ä½¿ç”¨å¯ä¼˜åŒ–çš„ç›®æ ‡æ¥å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨NeRFä¼˜åŒ–è¿‡ç¨‹ä¸­ä¸ºæ¯ä¸ªç‚¹å»ºæ¨¡æ¥æºä¿¡æ¯ï¼Œå¢åŠ äº†å…³äºä¸‰è§’æµ‹é‡çš„ä¿¡æ¯ï¼Œè¿›è€Œæ”¹è¿›äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¨€ç–ã€æ— çº¦æŸè§†å›¾è®¾ç½®ä¸‹çš„æ–°å‹è§†å›¾åˆæˆå’Œä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç›¸è¾ƒäºç«äº‰æ€§åŸºçº¿æœ‰æ˜æ˜¾æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.08140v3">PDF</a> 38th Conference on Neural Information Processing Systems (NeurIPS   2024)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Neural Radiance Fieldsï¼ˆNeRFï¼‰çš„æ–°åº”ç”¨ï¼Œæå‡ºäº†ä¸€ç§åä¸ºProvNeRFçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å»ºæ¨¡æ¯ä¸ªç‚¹çš„æ¥æºä¿¡æ¯æ¥å¡«å……ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œä»¥æé«˜é‡å»ºè´¨é‡ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨éšæœºåœºæ¨¡æ‹ŸNeRFæ¯ä¸ªç‚¹çš„å¯è§åŒºåŸŸã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†éšå¼æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ‰©å±•åˆ°åŠŸèƒ½ç©ºé—´å¹¶ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œå»ºæ¨¡æ¯ä¸ªç‚¹çš„æ¥æºä¿¡æ¯å¯ä»¥ä¸°å¯Œæ¨¡å‹ä¸­çš„ä¸‰è§’ä¿¡æ¯ï¼Œä»è€Œæ”¹å–„æ–°çš„è§†å›¾åˆæˆå’Œä¸ç¡®å®šæ€§ä¼°è®¡åœ¨æŒ‘æˆ˜ç¨€ç–çº¦æŸè§†è§’ä¸‹çš„åŸºå‡†ç«äº‰æƒ…å†µã€‚æ­¤æ–¹æ³•ä¸ä»…å¯ä»¥ä¸°å¯Œæ¨¡å‹çš„ä¸‰è§’åŒ–ä¿¡æ¯ï¼Œè¿˜å¯ä»¥æé«˜é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NeRFæŠ€æœ¯åœ¨å¤šä¸ªåº”ç”¨ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶æœªæ˜ç¡®å»ºæ¨¡è®­ç»ƒç›¸æœºå§¿æ€åˆ†å¸ƒå’Œä¸‰è§’åŒ–è´¨é‡è¿™ä¸€å…³é”®å› ç´ ã€‚</li>
<li>ProvNeRFæ–¹æ³•å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œé€šè¿‡å»ºæ¨¡æ¯ä¸ªç‚¹çš„æ¥æºä¿¡æ¯ï¼ˆå³å¯èƒ½å¯è§çš„ä½ç½®ï¼‰ä½œä¸ºéšæœºåœºã€‚</li>
<li>ProvNeRFæ‰©å±•äº†éšå¼æœ€å¤§ä¼¼ç„¶ä¼°è®¡åˆ°åŠŸèƒ½ç©ºé—´ï¼Œå®ç°äº†ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a88800d94773e52d935226f56f22f3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269ad0d4ac53199841873eb11a99d981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63fbadfd917389c415de71db6cbda62f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c47690ee68f64e729d06a8ee82dc5d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d615a98e82f276c1d6062a3509634f75.jpg" align="middle">
</details>




<h2 id="GauFRe-Gaussian-Deformation-Fields-for-Real-time-Dynamic-Novel-View-Synthesis"><a href="#GauFRe-Gaussian-Deformation-Fields-for-Real-time-Dynamic-Novel-View-Synthesis" class="headerlink" title="GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View   Synthesis"></a>GauFRe: Gaussian Deformation Fields for Real-time Dynamic Novel View   Synthesis</h2><p><strong>Authors:Yiqing Liang, Numair Khan, Zhengqin Li, Thu Nguyen-Phuoc, Douglas Lanman, James Tompkin, Lei Xiao</strong></p>
<p>We propose a method that achieves state-of-the-art rendering quality and efficiency on monocular dynamic scene reconstruction using deformable 3D Gaussians. Implicit deformable representations commonly model motion with a canonical space and time-dependent backward-warping deformation field. Our method, GauFRe, uses a forward-warping deformation to explicitly model non-rigid transformations of scene geometry. Specifically, we propose a template set of 3D Gaussians residing in a canonical space, and a time-dependent forward-warping deformation field to model dynamic objects. Additionally, we tailor a 3D Gaussian-specific static component supported by an inductive bias-aware initialization approach which allows the deformation field to focus on moving scene regions, improving the rendering of complex real-world motion. The differentiable pipeline is optimized end-to-end with a self-supervised rendering loss. Experiments show our method achieves competitive results and higher efficiency than both previous state-of-the-art NeRF and Gaussian-based methods. For real-world scenes, GauFRe can train in ~20 mins and offer 96 FPS real-time rendering on an RTX 3090 GPU. Project website: <a target="_blank" rel="noopener" href="https://lynl7130.github.io/gaufre/index.html">https://lynl7130.github.io/gaufre/index.html</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨å¯å˜å½¢ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒå®ç°äº†å•ç›®åŠ¨æ€åœºæ™¯é‡å»ºçš„æœ€æ–°æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡ã€‚éšå¼å¯å˜å½¢è¡¨ç¤ºé€šå¸¸é€šè¿‡è§„èŒƒç©ºé—´å’Œä¸æ—¶é—´ç›¸å…³çš„å‘åæ‰­æ›²å˜å½¢åœºå¯¹è¿åŠ¨è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•GauFReä½¿ç”¨æ­£å‘æ‰­æ›²å˜å½¢æ˜¾å¼åœ°æ¨¡æ‹Ÿåœºæ™¯å‡ ä½•çš„éåˆšæ€§å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—ä½äºè§„èŒƒç©ºé—´ä¸­çš„ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒæ¨¡æ¿ï¼Œä»¥åŠä¸€ä¸ªä¸æ—¶é—´ç›¸å…³çš„æ­£å‘æ‰­æ›²å˜å½¢åœºæ¥æ¨¡æ‹ŸåŠ¨æ€ç‰©ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸‰ç»´é«˜æ–¯åˆ†å¸ƒçš„é™æ€ç»„ä»¶è¿›è¡Œå®šåˆ¶ï¼Œé‡‡ç”¨å½’çº³åè§æ„ŸçŸ¥åˆå§‹åŒ–æ–¹æ³•ï¼Œä½¿å˜å½¢åœºä¸“æ³¨äºç§»åŠ¨åœºæ™¯åŒºåŸŸï¼Œæé«˜äº†å¤æ‚ç°å®ä¸–ç•Œè¿åŠ¨çš„æ¸²æŸ“æ•ˆæœã€‚å¯å¾®ç®¡é“é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ¸²æŸ“æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¹‹å‰çš„å…ˆè¿›NeRFå’Œé«˜æ–¯æ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœå’Œæ›´é«˜çš„æ•ˆç‡ã€‚å¯¹äºç°å®ä¸–ç•Œåœºæ™¯ï¼ŒGauFReå¯ä»¥åœ¨å¤§çº¦20åˆ†é’Ÿå†…è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨RTX 3090 GPUä¸Šæä¾›96 FPSçš„å®æ—¶æ¸²æŸ“ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://lynl7130.github.io/gaufre/index.html">https://lynl7130.github.io/gaufre/index.html</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.11458v2">PDF</a> 11 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong><br>åŸºäºå¯å˜å½¢3Dé«˜æ–¯æ¨¡å‹çš„å•ç›®åŠ¨æ€åœºæ™¯é‡å»ºæ–¹æ³•GauFReå®ç°äº†å‡ºè‰²çš„æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡ã€‚å®ƒé‡‡ç”¨æ­£å‘å˜å½¢æŠ€æœ¯æ˜¾å¼å»ºæ¨¡åœºæ™¯å‡ ä½•çš„éåˆšæ€§å˜æ¢ï¼Œå¹¶ç»“åˆæ¨¡æ¿é›†å’Œé™æ€ç»„ä»¶ä¼˜åŒ–ç«¯åˆ°ç«¯çš„å¯å¾®ç®¡é“ï¼Œå®ç°äº†å®æ—¶æ¸²æŸ“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¯å˜å½¢3Dé«˜æ–¯æ¨¡å‹çš„æ–¹æ³•GauFReï¼Œç”¨äºå•ç›®åŠ¨æ€åœºæ™¯é‡å»ºã€‚</li>
<li>é‡‡ç”¨æ­£å‘å˜å½¢æŠ€æœ¯ï¼Œæ˜¾å¼å»ºæ¨¡åœºæ™¯å‡ ä½•çš„éåˆšæ€§å˜æ¢ã€‚</li>
<li>é€šè¿‡æ¨¡æ¿é›†å’Œé™æ€ç»„ä»¶ä¼˜åŒ–ç«¯åˆ°ç«¯çš„å¯å¾®ç®¡é“ã€‚</li>
<li>æ–¹æ³•å®ç°äº†é«˜æ•ˆçš„å®æ—¶æ¸²æŸ“ï¼Œè®­ç»ƒæ—¶é—´çŸ­ï¼Œæ¸²æŸ“é€Ÿåº¦å¿«ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒGauFReåœ¨æ¸²æŸ“è´¨é‡å’Œæ•ˆç‡æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>æ”¯æŒè‡ªæˆ‘ç›‘ç£æ¸²æŸ“æŸå¤±ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ¸²æŸ“è´¨é‡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-21312884f5edfc983554726290f2233b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8ba69eab24fadcf9a82f4a069c47087.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a881aa2160b802519ccca7f10341a96a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bcd51659d52feb427f50fde1dbe9cef.jpg" align="middle">
</details>




<h2 id="EvaSurf-Efficient-View-Aware-Implicit-Textured-Surface-Reconstruction"><a href="#EvaSurf-Efficient-View-Aware-Implicit-Textured-Surface-Reconstruction" class="headerlink" title="EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction"></a>EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction</h2><p><strong>Authors:Jingnan Gao, Zhuo Chen, Yichao Yan, Bowen Pan, Zhe Wang, Jiangjing Lyu, Xiaokang Yang</strong></p>
<p>Reconstructing real-world 3D objects has numerous applications in computer vision, such as virtual reality, video games, and animations. Ideally, 3D reconstruction methods should generate high-fidelity results with 3D consistency in real-time. Traditional methods match pixels between images using photo-consistency constraints or learned features, while differentiable rendering methods like Neural Radiance Fields (NeRF) use differentiable volume rendering or surface-based representation to generate high-fidelity scenes. However, these methods require excessive runtime for rendering, making them impractical for daily applications. To address these challenges, we present $\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware implicit textured $\textbf{Surf}$ace reconstruction method. In our method, we first employ an efficient surface-based model with a multi-view supervision module to ensure accurate mesh reconstruction. To enable high-fidelity rendering, we learn an implicit texture embedded with view-aware encoding to capture view-dependent information. Furthermore, with the explicit geometry and the implicit texture, we can employ a lightweight neural shader to reduce the expense of computation and further support real-time rendering on common mobile devices. Extensive experiments demonstrate that our method can reconstruct high-quality appearance and accurate mesh on both synthetic and real-world datasets. Moreover, our method can be trained in just 1-2 hours using a single GPU and run on mobile devices at over 40 FPS (Frames Per Second), with a final package required for rendering taking up only 40-50 MB. </p>
<blockquote>
<p>é‡å»ºçœŸå®ä¸–ç•Œçš„3Dç‰©ä½“åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå…·æœ‰è®¸å¤šåº”ç”¨ï¼Œä¾‹å¦‚è™šæ‹Ÿç°å®ã€è§†é¢‘æ¸¸æˆå’ŒåŠ¨ç”»ã€‚ç†æƒ³çš„3Dé‡å»ºæ–¹æ³•åº”è¯¥èƒ½å¤Ÿå®æ—¶ç”Ÿæˆå…·æœ‰3Dä¸€è‡´æ€§çš„é«˜ä¿çœŸç»“æœã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡ç…§ç‰‡ä¸€è‡´æ€§çº¦æŸæˆ–å­¦ä¹ ç‰¹å¾æ¥åŒ¹é…å›¾åƒä¹‹é—´çš„åƒç´ ï¼Œè€Œåƒç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¿™æ ·çš„å¯å¾®åˆ†æ¸²æŸ“æ–¹æ³•åˆ™ä½¿ç”¨å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“æˆ–åŸºäºè¡¨é¢çš„è¡¨ç¤ºæ¥ç”Ÿæˆé«˜ä¿çœŸåœºæ™¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ¸²æŸ“æ‰€éœ€çš„è¿è¡Œæ—¶é—´è¿‡é•¿ï¼Œä¸é€‚åˆæ—¥å¸¸åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09806v4">PDF</a> Accepted by TVCG2024. Project Page:   <a target="_blank" rel="noopener" href="http://g-1nonly.github.io/EvaSurf-Website/">http://g-1nonly.github.io/EvaSurf-Website/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é«˜æ•ˆçš„ä¸‰ç»´é‡å»ºæ–¹æ³•EvaSurfï¼Œé€‚ç”¨äºè®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„è™šæ‹Ÿä¸–ç•Œã€è§†é¢‘æ¸¸æˆå’ŒåŠ¨ç”»ç­‰åº”ç”¨ã€‚EvaSurfé‡‡ç”¨åŸºäºè¡¨é¢çš„æ¨¡å‹å’Œå¤šè§†è§’ç›‘ç£æ¨¡å—ç¡®ä¿å‡†ç¡®çš„ä¸‰ç»´ç½‘æ ¼é‡å»ºã€‚åŒæ—¶ï¼Œä¸ºäº†è¿›è¡Œé«˜ä¿çœŸæ¸²æŸ“ï¼Œè¯¥æ–¹æ³•å­¦ä¹ äº†å…·æœ‰è§†è§’æ„ŸçŸ¥ç¼–ç çš„éšå«çº¹ç†ï¼Œæ•æ‰è§†è§’ç›¸å…³çš„ä¿¡æ¯ã€‚ç»“åˆæ˜ç¡®çš„å‡ ä½•å½¢çŠ¶å’Œéšå«çº¹ç†ï¼Œé‡‡ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œç€è‰²å™¨ï¼Œå‡å°‘è®¡ç®—æˆæœ¬ï¼Œæ”¯æŒåœ¨æ™®é€šç§»åŠ¨è®¾å¤‡ä¸Šè¿›è¡Œå®æ—¶æ¸²æŸ“ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šéƒ½èƒ½é‡å»ºå‡ºé«˜è´¨é‡å¤–è§‚å’Œç²¾ç¡®ç½‘æ ¼ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯åœ¨å•ä¸ªGPUä¸Šè®­ç»ƒä»…éœ€1-2å°æ—¶ï¼Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šä»¥è¶…è¿‡æ¯ç§’40å¸§çš„é€Ÿåº¦è¿è¡Œï¼Œæœ€ç»ˆæ¸²æŸ“æ‰€éœ€çš„è½¯ä»¶åŒ…å¤§å°ä»…ä¸º40-50MBã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>EvaSurfæ˜¯ä¸€ç§é«˜æ•ˆçš„ä¸‰ç»´é‡å»ºæ–¹æ³•ï¼Œé€‚ç”¨äºå¤šç§åº”ç”¨ã€‚</li>
<li>EvaSurfé‡‡ç”¨åŸºäºè¡¨é¢çš„æ¨¡å‹å’Œå¤šè§†è§’ç›‘ç£æ¨¡å—ç¡®ä¿å‡†ç¡®çš„ä¸‰ç»´ç½‘æ ¼é‡å»ºã€‚</li>
<li>EvaSurfç»“åˆéšå«çº¹ç†å’Œæ˜ç¡®å‡ ä½•å½¢çŠ¶è¿›è¡Œé«˜ä¿çœŸæ¸²æŸ“ã€‚</li>
<li>éšå«çº¹ç†å…·æœ‰è§†è§’æ„ŸçŸ¥ç¼–ç ï¼Œèƒ½æ•æ‰è§†è§’ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>EvaSurfé‡‡ç”¨è½»é‡çº§ç¥ç»ç½‘ç»œç€è‰²å™¨ï¼Œå‡å°‘è®¡ç®—æˆæœ¬ï¼Œæ”¯æŒå®æ—¶æ¸²æŸ“ã€‚</li>
<li>EvaSurfå¯åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šé‡å»ºå‡ºé«˜è´¨é‡å¤–è§‚å’Œç²¾ç¡®ç½‘æ ¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6101186f1d5708c0f649321671b4167.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-377afb5e1b9f8e62bcdc97e2c0561f53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-200fcce5fd20b2f7b611b8afb65d111e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38a8437bb4fa2274c5283c32eed9d781.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4136d98007c34792d647dd203d93655.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d4d65b9dcd8030f0cddbe1d0fc62439.jpg" align="middle">
</details>




<h2 id="VoxNeRF-Bridging-Voxel-Representation-and-Neural-Radiance-Fields-for-Enhanced-Indoor-View-Synthesis"><a href="#VoxNeRF-Bridging-Voxel-Representation-and-Neural-Radiance-Fields-for-Enhanced-Indoor-View-Synthesis" class="headerlink" title="VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for   Enhanced Indoor View Synthesis"></a>VoxNeRF: Bridging Voxel Representation and Neural Radiance Fields for   Enhanced Indoor View Synthesis</h2><p><strong>Authors:Sen Wang, Qing Cheng, Stefano Gasperini, Wei Zhang, Shun-Cheng Wu, Niclas Zeller, Daniel Cremers, Nassir Navab</strong></p>
<p>The generation of high-fidelity view synthesis is essential for robotic navigation and interaction but remains challenging, particularly in indoor environments and real-time scenarios. Existing techniques often require significant computational resources for both training and rendering, and they frequently result in suboptimal 3D representations due to insufficient geometric structuring. To address these limitations, we introduce VoxNeRF, a novel approach that utilizes easy-to-obtain geometry priors to enhance both the quality and efficiency of neural indoor reconstruction and novel view synthesis. We propose an efficient voxel-guided sampling technique that allocates computational resources selectively to the most relevant segments of rays based on a voxel-encoded geometry prior, significantly reducing training and rendering time. Additionally, we incorporate a robust depth loss to improve reconstruction and rendering quality in sparse view settings. Our approach is validated with extensive experiments on ScanNet and ScanNet++ where VoxNeRF outperforms existing state-of-the-art methods and establishes a new benchmark for indoor immersive interpolation and extrapolation settings. </p>
<blockquote>
<p>é«˜ä¿çœŸè§†å›¾åˆæˆçš„ç”Ÿæˆå¯¹äºæœºå™¨äººçš„å¯¼èˆªå’Œäº¤äº’è‡³å…³é‡è¦ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å®¤å†…ç¯å¢ƒå’Œå®æ—¶åœºæ™¯ä¸­ã€‚ç°æœ‰æŠ€æœ¯é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºè¿›è¡Œè®­ç»ƒå’Œæ¸²æŸ“ï¼Œå¹¶ä¸”ç”±äºå‡ ä½•ç»“æ„ä¸è¶³ï¼Œå®ƒä»¬å¸¸å¸¸äº§ç”Ÿä¸ç†æƒ³çš„3Dè¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†VoxNeRFï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ˜“äºè·å¾—çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†æ¥æé«˜ç¥ç»å®¤å†…é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆçš„è´¨é‡å’Œæ•ˆç‡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ä½“ç´ å¼•å¯¼é‡‡æ ·æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ ¹æ®ä½“ç´ ç¼–ç çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼Œæœ‰é€‰æ‹©åœ°ä¸ºæœ€ç›¸å…³çš„å°„çº¿æ®µåˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œå¤§å¤§å‡å°‘äº†è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¨³å¥çš„æ·±åº¦æŸå¤±ï¼Œä»¥æé«˜ç¨€ç–è§†å›¾è®¾ç½®ä¸­çš„é‡å»ºå’Œæ¸²æŸ“è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ScanNetå’ŒScanNet++ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†VoxNeRFçš„ä¼˜è¶Šæ€§ï¼Œå®ƒè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸ºå®¤å†…æ²‰æµ¸å¼æ’å€¼å’Œå¤–æ¨è®¾ç½®å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.05289v2">PDF</a> 8 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†VoxNeRFï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨æ˜“äºè·å¾—çš„å‡ ä½•å…ˆéªŒä¿¡æ¯æ¥æé«˜å®¤å†…é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆè´¨é‡å’Œæ•ˆç‡çš„æ–°æ–¹æ³•ã€‚å®ƒé€šè¿‡æœ‰æ•ˆçš„ä½“ç´ å¼•å¯¼é‡‡æ ·æŠ€æœ¯ï¼Œæ ¹æ®ä½“ç´ ç¼–ç çš„å‡ ä½•å…ˆéªŒä¿¡æ¯ï¼Œæœ‰é€‰æ‹©åœ°ä¸ºæœ€é‡è¦çš„å°„çº¿æ®µåˆ†é…è®¡ç®—èµ„æºï¼Œä»è€Œæ˜¾è‘—å‡å°‘è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§ç¨³å¥çš„æ·±åº¦æŸå¤±ï¼Œä»¥æé«˜ç¨€ç–è§†å›¾è®¾ç½®ä¸‹çš„é‡å»ºå’Œæ¸²æŸ“è´¨é‡ã€‚åœ¨ScanNetå’ŒScanNet++ä¸Šçš„å®éªŒéªŒè¯äº†VoxNeRFçš„ä¼˜è¶Šæ€§ï¼Œå®ƒè¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°æ–¹æ³•ï¼Œä¸ºå®¤å†…æ²‰æµ¸å¼æ’å€¼å’Œæ’è¡¥è®¾ç½®å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VoxNeRFè§£å†³äº†å®¤å†…ç¯å¢ƒä¸­é«˜ä¿çœŸè§†å›¾åˆæˆçš„é—®é¢˜ï¼Œå¯¹äºæœºå™¨äººå¯¼èˆªå’Œäº¤äº’è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯å­˜åœ¨è®¡ç®—èµ„æºéœ€æ±‚å¤§ã€è®­ç»ƒåŠæ¸²æŸ“æ•ˆç‡ä¸é«˜ä»¥åŠ3Dè¡¨ç¤ºè´¨é‡æ¬ ä½³çš„é—®é¢˜ã€‚</li>
<li>VoxNeRFå¼•å…¥äº†ä¸€ç§åˆ©ç”¨å‡ ä½•å…ˆéªŒä¿¡æ¯çš„åˆ›æ–°æ–¹æ³•ï¼Œä»¥æé«˜ç¥ç»å®¤å†…é‡å»ºå’Œæ–°é¢–è§†å›¾åˆæˆçš„è´¨é‡ã€‚</li>
<li>é‡‡ç”¨é«˜æ•ˆçš„ä½“ç´ å¼•å¯¼é‡‡æ ·æŠ€æœ¯ï¼ŒåŸºäºä½“ç´ ç¼–ç çš„å‡ ä½•å…ˆéªŒä¿¡æ¯é€‰æ‹©æ€§åœ°åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>é€šè¿‡å¼•å…¥ç¨³å¥çš„æ·±åº¦æŸå¤±ï¼Œæé«˜äº†ç¨€ç–è§†å›¾è®¾ç½®ä¸‹çš„é‡å»ºå’Œæ¸²æŸ“è´¨é‡ã€‚</li>
<li>åœ¨ScanNetå’ŒScanNet++ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVoxNeRFçš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œä¸ºå®¤å†…ç¯å¢ƒæ’å€¼å’Œæ’è¡¥è®¾ç½®æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fa57732c3be3d7f4ff6e90be4970199.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-425cb54239f6ef5877df82ca94835392.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58225d05c4f8a4774bc9b15542e3bef0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd993e70c8c2c1a9921af35fc31827fe.jpg" align="middle">
</details>




<h2 id="Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN"><a href="#Unpaired-Optical-Coherence-Tomography-Angiography-Image-Super-Resolution-via-Frequency-Aware-Inverse-Consistency-GAN" class="headerlink" title="Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN"></a>Unpaired Optical Coherence Tomography Angiography Image Super-Resolution   via Frequency-Aware Inverse-Consistency GAN</h2><p><strong>Authors:Weiwen Zhang, Dawei Yang, Haoxuan Che, An Ran Ran, Carol Y. Cheung, Hao Chen</strong></p>
<p>For optical coherence tomography angiography (OCTA) images, a limited scanning rate leads to a trade-off between field-of-view (FOV) and imaging resolution. Although larger FOV images may reveal more parafoveal vascular lesions, their application is greatly hampered due to lower resolution. To increase the resolution, previous works only achieved satisfactory performance by using paired data for training, but real-world applications are limited by the challenge of collecting large-scale paired images. Thus, an unpaired approach is highly demanded. Generative Adversarial Network (GAN) has been commonly used in the unpaired setting, but it may struggle to accurately preserve fine-grained capillary details, which are critical biomarkers for OCTA. In this paper, our approach aspires to preserve these details by leveraging the frequency information, which represents details as high-frequencies ($\textbf{hf}$) and coarse-grained backgrounds as low-frequencies ($\textbf{lf}$). In general, we propose a GAN-based unpaired super-resolution method for OCTA images and exceptionally emphasize $\textbf{hf}$ fine capillaries through a dual-path generator. To facilitate a precise spectrum of the reconstructed image, we also propose a frequency-aware adversarial loss for the discriminator and introduce a frequency-aware focal consistency loss for end-to-end optimization. Experiments show that our method outperforms other state-of-the-art unpaired methods both quantitatively and visually. </p>
<blockquote>
<p>å¯¹äºå…‰å­¦ç›¸å¹²å±‚æè¡€ç®¡é€ å½±ï¼ˆOCTAï¼‰å›¾åƒï¼Œæœ‰é™çš„æ‰«æé€Ÿç‡å¯¼è‡´è§†é‡ï¼ˆFOVï¼‰ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡ã€‚è™½ç„¶æ›´å¤§çš„è§†é‡å›¾åƒå¯èƒ½ä¼šæ­ç¤ºæ›´å¤šçš„æ—è§†ç½‘è†œè¡€ç®¡ç—…å˜ï¼Œä½†ç”±äºåˆ†è¾¨ç‡è¾ƒä½ï¼Œå…¶åº”ç”¨å—åˆ°äº†å¾ˆå¤§çš„é˜»ç¢ã€‚ä¸ºäº†å¢åŠ åˆ†è¾¨ç‡ï¼Œä»¥å‰çš„å·¥ä½œåªæœ‰é€šè¿‡ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒæ‰èƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„æ•ˆæœï¼Œä½†ç°å®ä¸–ç•Œçš„åº”ç”¨å—åˆ°æ”¶é›†å¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æŒ‘æˆ˜çš„é™åˆ¶ã€‚å› æ­¤ï¼Œå¯¹æœªé…å¯¹æ–¹æ³•çš„éœ€æ±‚éå¸¸é«˜ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å·²åœ¨æœªé…å¯¹è®¾ç½®ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½†å®ƒå¯èƒ½éš¾ä»¥å‡†ç¡®ä¿ç•™æ¯›ç»†è¡€ç®¡çš„ç»†å¾®ç»†èŠ‚ï¼Œè¿™å¯¹äºOCTAæ¥è¯´æ˜¯éå¸¸é‡è¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚æœ¬æ–‡çš„æ–¹æ³•æ—¨åœ¨é€šè¿‡åˆ©ç”¨é¢‘ç‡ä¿¡æ¯æ¥ä¿ç•™è¿™äº›ç»†èŠ‚ï¼Œå°†æ¯›ç»†è¡€ç®¡ç»†èŠ‚è§†ä¸ºé«˜é¢‘ï¼ˆhfï¼‰ï¼Œå°†ç²—ç²’èƒŒæ™¯è§†ä¸ºä½é¢‘ï¼ˆlfï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„æœªé…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ç”¨äºOCTAå›¾åƒï¼Œå¹¶é€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨ç‰¹åˆ«å¼ºè°ƒäº†é«˜é¢‘æ¯›ç»†è¡€ç®¡ã€‚ä¸ºäº†ä¿ƒè¿›é‡å»ºå›¾åƒçš„ç²¾ç¡®é¢‘è°±ï¼Œæˆ‘ä»¬è¿˜ä¸ºé‰´åˆ«å™¨æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±ï¼Œå¹¶ä¸ºç«¯åˆ°ç«¯ä¼˜åŒ–å¼•å…¥äº†é¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰ä¸Šå‡ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æœªé…å¯¹æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17269v2">PDF</a> 11 pages, 10 figures, in IEEE J-BHI, 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ï¼Œç”¨äºå¤„ç†OCTAå›¾åƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢‘ç‡ä¿¡æ¯ï¼Œé€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨çªå‡ºé«˜é¢‘ç»†èŠ‚å’Œä½é¢‘èƒŒæ™¯ï¼Œå®ç°é«˜ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥æé«˜å›¾åƒé‡å»ºçš„ç²¾ç¡®åº¦ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œè§†è§‰ä¸Šéƒ½ä¼˜äºå…¶ä»–å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹äºOCTAå›¾åƒï¼Œæ‰«æç‡çš„é™åˆ¶å¯¼è‡´è§†é‡ä¸æˆåƒåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è™½ç„¶è¾ƒå¤§çš„è§†é‡å¯èƒ½æ­ç¤ºæ›´å¤šçš„çœ¼å‘¨å›´è¡€ç®¡ç—…å˜ï¼Œä½†å…¶åº”ç”¨å—åˆ°åˆ†è¾¨ç‡ä½çš„é™åˆ¶ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶é€šè¿‡ä½¿ç”¨é…å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒæ¥è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œä½†ç°å®ä¸–ç•Œçš„åº”ç”¨å—åˆ°æ”¶é›†å¤§è§„æ¨¡é…å¯¹å›¾åƒçš„æŒ‘æˆ˜é™åˆ¶ã€‚å› æ­¤ï¼Œæ— é…å¯¹æ–¹æ³•çš„éœ€æ±‚è¿«åˆ‡ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ— é…å¯¹è®¾ç½®ä¸­å·²è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†å¯èƒ½éš¾ä»¥å‡†ç¡®ä¿ç•™é‡è¦çš„æ¯›ç»†è¡€ç®¡ç»†èŠ‚ã€‚è¿™äº›æ¯›ç»†è¡€ç®¡ç»†èŠ‚å¯¹äºOCTAæ˜¯å…³é”®çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨é¢‘ç‡ä¿¡æ¯ï¼Œé€šè¿‡åŒè·¯å¾„ç”Ÿæˆå™¨æå‡ºä¸€ç§åŸºäºGANçš„æ— é…å¯¹è¶…åˆ†è¾¨ç‡æ–¹æ³•ã€‚é«˜é¢‘ä»£è¡¨ç»†èŠ‚ï¼Œä½é¢‘ä»£è¡¨ç²—é¢—ç²’èƒŒæ™¯ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨ä¿ç•™æ¯›ç»†è¡€ç®¡ç»†èŠ‚ã€‚</li>
<li>ä¸ºäº†å®ç°ç²¾ç¡®é‡å»ºçš„å›¾åƒé¢‘è°±ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†é¢‘ç‡æ„ŸçŸ¥å¯¹æŠ—æŸå¤±å’Œé¢‘ç‡æ„ŸçŸ¥ç„¦ç‚¹ä¸€è‡´æ€§æŸå¤±ã€‚è¿™äº›æŸå¤±æœ‰åŠ©äºç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a7d2dacbfa04387e19bffb611cddd3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37207abe4a6a009c6c161251db6a0dc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dc134bb1beea380f62fe84bb2cb6682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78a06df289a30d0915c38a304ff0732a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46d39c0c7ae0f29f88c8af6d66816c47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811b53fc9884b14c64110c9698ed318d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8c5b27ba5f5430f411959f118701cd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833009dbaad71b8fe443d1578e47e112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b31d7d25c3842dd3c4d8bc5b8a631ae5.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Speech/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-084788d1a3c9bf4a759bc1df2b42fdad.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Deblur4DGS 4D Gaussian Splatting from Blurry Monocular Video
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">9514.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
