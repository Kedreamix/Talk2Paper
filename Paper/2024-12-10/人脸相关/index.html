<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="äººè„¸ç›¸å…³">
    <meta name="description" content="äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Exploring Depth Information for Detecting Manipulated Face Videos">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>äººè„¸ç›¸å…³ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-46824d2f6082475e2c89c230472a59e3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">äººè„¸ç›¸å…³</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                                <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                äººè„¸ç›¸å…³
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    68 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Exploring-Depth-Information-for-Detecting-Manipulated-Face-Videos"><a href="#Exploring-Depth-Information-for-Detecting-Manipulated-Face-Videos" class="headerlink" title="Exploring Depth Information for Detecting Manipulated Face Videos"></a>Exploring Depth Information for Detecting Manipulated Face Videos</h2><p><strong>Authors:Haoyue Wang, Sheng Li, Ji He, Zhenxing Qian, Xinpeng Zhang, Shaolin Fan</strong></p>
<p>Face manipulation detection has been receiving a lot of attention for the reliability and security of the face images&#x2F;videos. Recent studies focus on using auxiliary information or prior knowledge to capture robust manipulation traces, which are shown to be promising. As one of the important face features, the face depth map, which has shown to be effective in other areas such as face recognition or face detection, is unfortunately paid little attention to in literature for face manipulation detection. In this paper, we explore the possibility of incorporating the face depth map as auxiliary information for robust face manipulation detection. To this end, we first propose a Face Depth Map Transformer (FDMT) to estimate the face depth map patch by patch from an RGB face image, which is able to capture the local depth anomaly created due to manipulation. The estimated face depth map is then considered as auxiliary information to be integrated with the backbone features using a Multi-head Depth Attention (MDA) mechanism that is newly designed. We also propose an RGB-Depth Inconsistency Attention (RDIA) module to effectively capture the inter-frame inconsistency for multi-frame input. Various experiments demonstrate the advantage of our proposed method for face manipulation detection. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«æŠ€æœ¯ä¸­çš„å›¾åƒ&#x2F;è§†é¢‘æ“çºµæ£€æµ‹å› å…¶å¯é æ€§å’Œå®‰å…¨æ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è¿‘æœŸçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨è¾…åŠ©ä¿¡æ¯æˆ–å…ˆéªŒçŸ¥è¯†æ¥æ•æ‰ç¨³å¥çš„æ“çºµç—•è¿¹ï¼Œè¿™æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ã€‚ä½œä¸ºäººè„¸ç‰¹å¾çš„é‡è¦ç»„æˆéƒ¨åˆ†ä¹‹ä¸€ï¼Œäººè„¸æ·±åº¦å›¾åœ¨äººè„¸è¯†åˆ«æˆ–äººè„¸æ£€æµ‹ç­‰é¢†åŸŸå·²ç»å±•ç°å‡ºå…¶æœ‰æ•ˆæ€§ï¼Œä½†åœ¨äººè„¸æ“çºµæ£€æµ‹çš„æ–‡çŒ®ä¸­å´é²œæœ‰ç ”ç©¶æ¶‰åŠã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†äººè„¸æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ä¿¡æ¯ç”¨äºç¨³å¥çš„äººè„¸æ“çºµæ£€æµ‹çš„å¯èƒ½æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§Face Depth Map Transformerï¼ˆFDMTï¼‰æ–¹æ³•ï¼Œèƒ½å¤Ÿä»RGBäººè„¸å›¾åƒä¸­é€å—ä¼°è®¡äººè„¸æ·±åº¦å›¾ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰åˆ°ç”±äºæ“çºµè€Œäº§ç”Ÿçš„å±€éƒ¨æ·±åº¦å¼‚å¸¸ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä¼°è®¡çš„äººè„¸æ·±åº¦å›¾è§†ä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œä½¿ç”¨æ–°è®¾è®¡çš„Multi-head Depth Attentionï¼ˆMDAï¼‰æœºåˆ¶å°†å…¶ä¸ä¸»å¹²ç‰¹å¾è¿›è¡Œèåˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§RGB-Depth Inconsistency Attentionï¼ˆRDIAï¼‰æ¨¡å—ï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰å¤šå¸§è¾“å…¥çš„å¸§é—´ä¸ä¸€è‡´æ€§ã€‚å„ç§å®éªŒè¯æ˜äº†æˆ‘ä»¬åœ¨äººè„¸æ“çºµæ£€æµ‹æ–¹é¢æå‡ºçš„æ–¹æ³•çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18572v1">PDF</a> 12 pages, 10 figures. arXiv admin note: substantial text overlap with   arXiv:2212.14230</p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«æŠ€æœ¯é¢ä¸´ç€æ“æ§æ€§é—®é¢˜ï¼Œäººä»¬äºŸéœ€æå‡ç›¸å…³å¯é æ€§åŠå®‰å…¨æ€§ã€‚é’ˆå¯¹æ·±åº¦å›¾çš„æ¢è®¨ä¸€ç›´è¢«å¿½è§†çš„é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡æ·±åº¦å›¾æ•æ‰å±€éƒ¨æ·±åº¦å¼‚å¸¸è¿›è¡Œäººè„¸è¯†åˆ«æ£€æµ‹ç ”ç©¶ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§Face Depth Map Transformeræ¨¡å‹ï¼ˆFDMTï¼‰è¿›è¡Œäººè„¸æ·±åº¦å›¾çš„è·å–ä¸åˆ†æï¼Œå¹¶ç”¨Multi-head Depth Attentionæœºåˆ¶å°†ç»“æœå¼•å…¥åŸæœ‰æ¨¡å‹æå‡è¯†åˆ«æ•ˆæœã€‚åŒæ—¶ï¼Œè®¾è®¡RGB-Depth Inconsistency Attentionæ¨¡å—ï¼Œç”¨äºæ•æ‰å¤šå¸§è¾“å…¥é—´çš„å¸§é—´ä¸ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äººè„¸è¯†åˆ«æ£€æµ‹ä¸­å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸æ“æ§é—®é¢˜åœ¨äººè„¸è¯†åˆ«æŠ€æœ¯ä¸­è‡³å…³é‡è¦ï¼Œå¯¹äººè„¸å›¾åƒå’Œè§†é¢‘çš„å®‰å…¨æ€§å’Œå¯é æ€§æå‡ºæŒ‘æˆ˜ã€‚</li>
<li>äººè„¸æ·±åº¦å›¾ä½œä¸ºè¾…åŠ©ä¿¡æ¯å¯¹äºäººè„¸è¯†åˆ«æ“æ§æ£€æµ‹å…·æœ‰æ½œåŠ›ï¼Œä½†ç›¸å…³ç ”ç©¶è¾ƒä¸ºåŒ®ä¹ã€‚</li>
<li>æå‡ºFace Depth Map Transformerï¼ˆFDMTï¼‰æ¨¡å‹ç”¨äºä»RGBäººè„¸å›¾åƒä¸­ä¼°è®¡äººè„¸æ·±åº¦å›¾ï¼Œæ•æ‰å› æ“æ§äº§ç”Ÿçš„å±€éƒ¨æ·±åº¦å¼‚å¸¸ã€‚</li>
<li>é‡‡ç”¨Multi-head Depth Attentionæœºåˆ¶é›†æˆä¼°è®¡å¾—åˆ°çš„äººè„¸æ·±åº¦å›¾ä¸ä¸»å¹²ç‰¹å¾ã€‚</li>
<li>è®¾è®¡RGB-Depth Inconsistency Attentionæ¨¡å—ç”¨ä»¥æœ‰æ•ˆæ•æ‰å¤šå¸§è¾“å…¥é—´çš„å¸§é—´ä¸ä¸€è‡´æ€§ï¼Œä»¥å¢å¼ºå¯¹åŠ¨æ€æ“çºµçš„æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæ‰€æå‡ºçš„æ–¹æ³•åœ¨äººè„¸è¯†åˆ«æ£€æµ‹æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2425f62b5505f29effe02bb1437f2205.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6ab10f5572059f51ab88885e1bb9051e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3b68eaffc94d306c04b74d12195da745.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7e88367b5bf15a119d488835dcb28baa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b0c3a5395e54ca3d16249782aeabae70.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f03eeb1d63e91a527205daf0e2b83d3.jpg" align="middle">
</details>




<h2 id="in-Car-Biometrics-iCarB-Datasets-for-Driver-Recognition-Face-Fingerprint-and-Voice"><a href="#in-Car-Biometrics-iCarB-Datasets-for-Driver-Recognition-Face-Fingerprint-and-Voice" class="headerlink" title="in-Car Biometrics (iCarB) Datasets for Driver Recognition: Face,   Fingerprint, and Voice"></a>in-Car Biometrics (iCarB) Datasets for Driver Recognition: Face,   Fingerprint, and Voice</h2><p><strong>Authors:Vedrana Krivokuca Hahn, Jeremy Maceiras, Alain Komaty, Philip Abbet, Sebastien Marcel</strong></p>
<p>We present three biometric datasets (iCarB-Face, iCarB-Fingerprint, iCarB-Voice) containing face videos, fingerprint images, and voice samples, collected inside a car from 200 consenting volunteers. The data was acquired using a near-infrared camera, two fingerprint scanners, and two microphones, while the volunteers were seated in the driverâ€™s seat of the car. The data collection took place while the car was parked both indoors and outdoors, and different â€œnoisesâ€ were added to simulate non-ideal biometric data capture that may be encountered in real-life driver recognition. Although the datasets are specifically tailored to in-vehicle biometric recognition, their utility is not limited to the automotive environment. The iCarB datasets, which are available to the research community, can be used to: (i) evaluate and benchmark face, fingerprint, and voice recognition systems (we provide several evaluation protocols); (ii) create multimodal pseudo-identities, to train&#x2F;test multimodal fusion algorithms; (iii) create Presentation Attacks from the biometric data, to evaluate Presentation Attack Detection algorithms; (iv) investigate demographic and environmental biases in biometric systems, using the provided metadata. To the best of our knowledge, ours are the largest and most diverse publicly available in-vehicle biometric datasets. Most other datasets contain only one biometric modality (usually face), while our datasets consist of three modalities, all acquired in the same automotive environment. Moreover, iCarB-Fingerprint seems to be the first publicly available in-vehicle fingerprint dataset. Finally, the iCarB datasets boast a rare level of demographic diversity among the 200 data subjects, including a 50&#x2F;50 gender split, skin colours across the whole Fitzpatrick-scale spectrum, and a wide age range (18-60+). So, these datasets will be valuable for advancing biometrics research. </p>
<blockquote>
<p>æˆ‘ä»¬æä¾›äº†ä¸‰ä¸ªç”Ÿç‰©è¯†åˆ«æ•°æ®é›†ï¼ˆiCarB-Faceã€iCarB-Fingerprintã€iCarB-Voiceï¼‰ï¼Œå…¶ä¸­åŒ…å«ä»200ååŒæ„å‚ä¸çš„å¿—æ„¿è€…åœ¨è½¦å†…æ”¶é›†çš„é¢éƒ¨è§†é¢‘ã€æŒ‡çº¹å›¾åƒå’Œè¯­éŸ³æ ·æœ¬ã€‚æ•°æ®ä½¿ç”¨è¿‘çº¢å¤–ç›¸æœºã€ä¸¤ä¸ªæŒ‡çº¹æ‰«æä»ªå’Œä¸¤ä¸ªéº¦å…‹é£åœ¨å¿—æ„¿è€…ååœ¨æ±½è½¦é©¾é©¶åº§æ—¶é‡‡é›†ã€‚æ•°æ®æ”¶é›†åœ¨æ±½è½¦å®¤å†…å’Œå®¤å¤–åœæ”¾æ—¶è¿›è¡Œï¼Œå¹¶æ·»åŠ äº†ä¸åŒçš„â€œå™ªéŸ³â€æ¥æ¨¡æ‹Ÿåœ¨çœŸå®ç”Ÿæ´»ä¸­å¯èƒ½ä¼šé‡åˆ°çš„ä¸ç†æƒ³çš„ç”Ÿç‰©è¯†åˆ«æ•°æ®é‡‡é›†æƒ…å†µã€‚å°½ç®¡è¿™äº›æ•°æ®é›†ç‰¹åˆ«é’ˆå¯¹è½¦å†…ç”Ÿç‰©è¯†åˆ«è®¾è®¡ï¼Œä½†å…¶ç”¨é€”ä¸é™äºæ±½è½¦ç¯å¢ƒã€‚iCarBæ•°æ®é›†å¯¹ç ”ç©¶ç¾¤ä½“å¼€æ”¾ä½¿ç”¨ï¼Œå¯ç”¨äºï¼šï¼ˆiï¼‰è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•é¢éƒ¨ã€æŒ‡çº¹å’Œè¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆæˆ‘ä»¬æä¾›äº†å¤šä¸ªè¯„ä¼°åè®®ï¼‰ï¼›ï¼ˆiiï¼‰åˆ›å»ºå¤šæ¨¡æ€ä¼ªèº«ä»½ï¼Œä»¥è®­ç»ƒå’Œæµ‹è¯•å¤šæ¨¡æ€èåˆç®—æ³•ï¼›ï¼ˆiiiï¼‰ä»ç”Ÿç‰©è¯†åˆ«æ•°æ®ä¸­åˆ›å»ºå‘ˆç°æ”»å‡»ï¼Œä»¥è¯„ä¼°å‘ˆç°æ”»å‡»æ£€æµ‹ç®—æ³•ï¼›ï¼ˆivï¼‰ä½¿ç”¨æä¾›çš„å…ƒæ•°æ®ç ”ç©¶ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­çš„ç§æ—å’Œç¯å¢ƒåè§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ˜¯æœ€å¤§ä¸”æœ€å¤šå…ƒåŒ–çš„å…¬å¼€å¯ç”¨è½¦å†…ç”Ÿç‰©è¯†åˆ«æ•°æ®é›†ã€‚å…¶ä»–å¤§å¤šæ•°æ•°æ®é›†åªåŒ…å«ä¸€ç§ç”Ÿç‰©è¯†åˆ«æ¨¡å¼ï¼ˆé€šå¸¸æ˜¯é¢éƒ¨ï¼‰ï¼Œè€Œæˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä¸‰ç§æ¨¡å¼ï¼Œå‡åœ¨åŒä¸€æ±½è½¦ç¯å¢ƒä¸­é‡‡é›†ã€‚æ­¤å¤–ï¼ŒiCarB-Fingerprintä¼¼ä¹æ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„è½¦å†…æŒ‡çº¹æ•°æ®é›†ã€‚æœ€åï¼ŒiCarBæ•°æ®é›†åœ¨200ä¸ªæ•°æ®ä¸»ä½“ä¹‹é—´å…·æœ‰ç½•è§çš„ç§æ—å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬50&#x2F;50çš„æ€§åˆ«åˆ†å¸ƒã€è·¨è¶Šæ•´ä¸ªFitzpatrickè°±çš„çš®è‚¤é¢œè‰²å’Œå¹¿æ³›çš„å¹´é¾„èŒƒå›´ï¼ˆ18-60+ï¼‰ã€‚å› æ­¤ï¼Œè¿™äº›æ•°æ®é›†å¯¹äºæ¨åŠ¨ç”Ÿç‰©è¯†åˆ«ç ”ç©¶å°†å…·æœ‰å®è´µä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17305v1">PDF</a> 8 pages, 13 figures, 4 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ä¸ªè½¦è½½ç”Ÿç‰©è¯†åˆ«æ•°æ®é›†ï¼ˆiCarB-Faceã€iCarB-Fingerprintã€iCarB-Voiceï¼‰ï¼ŒåŒ…å«é¢éƒ¨è§†é¢‘ã€æŒ‡çº¹å›¾åƒå’Œè¯­éŸ³æ ·æœ¬ï¼Œæ˜¯åœ¨è½¦å†…ä»200åå¿—æ„¿è€…èº«ä¸Šæ”¶é›†çš„ã€‚æ•°æ®ä½¿ç”¨è¿‘çº¢å¤–ç›¸æœºã€ä¸¤ä¸ªæŒ‡çº¹æ‰«æä»ªå’Œä¸¤ä¸ªéº¦å…‹é£åœ¨å¿—æ„¿è€…ååœ¨é©¾é©¶åº§ä¸Šæ—¶é‡‡é›†ã€‚æ•°æ®æ”¶é›†åœ¨å®¤å†…å’Œå®¤å¤–åœæ”¾çš„æ±½è½¦ä¸­è¿›è¡Œï¼Œå¹¶æ·»åŠ äº†ä¸åŒçš„â€œå™ªéŸ³â€ä»¥æ¨¡æ‹Ÿç°å®é©¾é©¶è¯†åˆ«ä¸­å¯èƒ½é‡åˆ°çš„ä¸ç†æƒ³çš„ç”Ÿç‰©è¯†åˆ«æ•°æ®æ•è·æƒ…å†µã€‚iCarBæ•°æ®é›†è™½ç„¶ä¸“ä¸ºè½¦å†…ç”Ÿç‰©è¯†åˆ«è®¾è®¡ï¼Œä½†å…¶ç”¨é€”ä¸é™äºæ±½è½¦ç¯å¢ƒã€‚è¿™äº›æ•°æ®é›†å¯¹äºæ¨è¿›ç”Ÿç‰©è¯†åˆ«ç ”ç©¶å…·æœ‰ä»·å€¼ï¼Œå¯ç”¨äºè¯„ä¼°å’Œé¢æˆé¢éƒ¨ã€æŒ‡çº¹å’Œè¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼ˆæˆ‘ä»¬æä¾›å¤šä¸ªè¯„ä¼°åè®®ï¼‰ï¼›åˆ›å»ºå¤šæ¨¡æ€ä¼ªèº«ä»½ï¼Œä»¥è®­ç»ƒ&#x2F;æµ‹è¯•å¤šæ¨¡æ€èåˆç®—æ³•ï¼›ä»ç”Ÿç‰©è¯†åˆ«æ•°æ®ä¸­åˆ›å»ºå‘ˆç°æ”»å‡»ï¼Œä»¥è¯„ä¼°å‘ˆç°æ”»å‡»æ£€æµ‹ç®—æ³•ï¼›ä½¿ç”¨æä¾›çš„å…ƒæ•°æ®ç ”ç©¶ç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­çš„äººå£ç»Ÿè®¡å’Œç¯å¢ƒåè§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†æ˜¯ç›®å‰æœ€å¤§ä¸”æœ€å¤šå…ƒçš„è½¦å†…ç”Ÿç‰©è¯†åˆ«æ•°æ®é›†ã€‚å…¶ä»–å¤§å¤šæ•°æ•°æ®é›†åªåŒ…å«ä¸€ç§ç”Ÿç‰©è¯†åˆ«æ¨¡å¼ï¼ˆé€šå¸¸æ˜¯é¢éƒ¨ï¼‰ï¼Œè€Œæˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä¸‰ç§æ¨¡å¼ï¼Œéƒ½æ˜¯åœ¨ç›¸åŒçš„æ±½è½¦ç¯å¢ƒä¸­æ”¶é›†çš„ã€‚æ­¤å¤–ï¼ŒiCarB-Fingerprintä¼¼ä¹æ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„è½¦å†…æŒ‡çº¹æ•°æ®é›†ã€‚æœ€åï¼ŒiCarBæ•°æ®é›†çš„æ•°æ®ä¸»ä½“å…·æœ‰ç½•è§çš„æ°‘æ—å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬50&#x2F;50çš„æ€§åˆ«æ¯”ä¾‹ã€è·¨è¶Šæ•´ä¸ªFitzpatrickå°ºåº¦çš„è‚¤è‰²ä»¥åŠå¹¿æ³›çš„å¹´é¾„èŒƒå›´ï¼ˆ18-60+ï¼‰ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸‰ä¸ªè½¦è½½ç”Ÿç‰©è¯†åˆ«æ•°æ®é›†iCarB-Faceã€iCarB-Fingerprintå’ŒiCarB-Voiceã€‚</li>
<li>æ•°æ®é›†åŒ…å«é¢éƒ¨è§†é¢‘ã€æŒ‡çº¹å›¾åƒå’Œè¯­éŸ³æ ·æœ¬ï¼Œåœ¨çœŸå®çš„é©¾é©¶ç¯å¢ƒä¸­ä»å¿—æ„¿è€…èº«ä¸Šæ”¶é›†ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºè¯„ä¼°é¢éƒ¨ã€æŒ‡çº¹å’Œè¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œæä¾›å¤šä¸ªè¯„ä¼°åè®®ã€‚</li>
<li>å¯ç”¨äºåˆ›å»ºå¤šæ¨¡æ€ä¼ªèº«ä»½ã€å‘ˆç°æ”»å‡»æ£€æµ‹ç®—æ³•çš„ç ”ç©¶ä»¥åŠç”Ÿç‰©è¯†åˆ«ç³»ç»Ÿä¸­çš„åè§ç ”ç©¶ã€‚</li>
<li>ä¸å…¶ä»–æ•°æ®é›†ç›¸æ¯”ï¼ŒiCarBæ•°æ®é›†åŒ…å«å¤šç§ç”Ÿç‰©è¯†åˆ«æ¨¡å¼ï¼Œå¹¶åœ¨ç›¸åŒçš„æ±½è½¦ç¯å¢ƒä¸­æ”¶é›†ã€‚</li>
<li>iCarB-Fingerprintæ˜¯é¦–ä¸ªå…¬å¼€å¯ç”¨çš„è½¦å†…æŒ‡çº¹æ•°æ®é›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3d75ff1916324920adcc3c6aab5e845b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-611bf974d69f5fe863c057b971766710.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7920e4dd059996b81570ba25c99043de.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8f74977ac028bdd2265346009f78c0ed.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-90ecfd320faccae008ec035d1a39978c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9a6c82d6f0b05f5cee415a449b1e69db.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e598fcb9e724ba598b9aa60cb0ded9f5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b7d371f6ed49cbed1b9053ca666927fb.jpg" align="middle">
</details>




<h2 id="Local-and-Global-Feature-Attention-Fusion-Network-for-Face-Recognition"><a href="#Local-and-Global-Feature-Attention-Fusion-Network-for-Face-Recognition" class="headerlink" title="Local and Global Feature Attention Fusion Network for Face Recognition"></a>Local and Global Feature Attention Fusion Network for Face Recognition</h2><p><strong>Authors:Wang Yu, Wei Wei</strong></p>
<p>Recognition of low-quality face images remains a challenge due to invisible or deformation in partial facial regions. For low-quality images dominated by missing partial facial regions, local region similarity contributes more to face recognition (FR). Conversely, in cases dominated by local face deformation, excessive attention to local regions may lead to misjudgments, while global features exhibit better robustness. However, most of the existing FR methods neglect the bias in feature quality of low-quality images introduced by different factors. To address this issue, we propose a Local and Global Feature Attention Fusion (LGAF) network based on feature quality. The network adaptively allocates attention between local and global features according to feature quality and obtains more discriminative and high-quality face features through local and global information complementarity. In addition, to effectively obtain fine-grained information at various scales and increase the separability of facial features in high-dimensional space, we introduce a Multi-Head Multi-Scale Local Feature Extraction (MHMS) module. Experimental results demonstrate that the LGAF achieves the best average performance on $4$ validation sets (CFP-FP, CPLFW, AgeDB, and CALFW), and the performance on TinyFace and SCFace outperforms the state-of-the-art methods (SoTA). </p>
<blockquote>
<p>é’ˆå¯¹ä½è´¨é‡äººè„¸å›¾åƒçš„è¯†åˆ«ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºéƒ¨åˆ†é¢éƒ¨åŒºåŸŸå­˜åœ¨ä¸å¯è§æˆ–å˜å½¢çš„æƒ…å†µã€‚å¯¹äºä»¥ç¼ºå¤±éƒ¨åˆ†é¢éƒ¨åŒºåŸŸä¸ºä¸»çš„ä½è´¨é‡å›¾åƒï¼Œå±€éƒ¨åŒºåŸŸç›¸ä¼¼æ€§å¯¹äººè„¸è¯†åˆ«ï¼ˆFRï¼‰çš„è´¡çŒ®æ›´å¤§ã€‚ç›¸åï¼Œåœ¨å±€éƒ¨é¢éƒ¨å˜å½¢å ä¸»å¯¼çš„æƒ…å†µä¸‹ï¼Œè¿‡åº¦å…³æ³¨å±€éƒ¨åŒºåŸŸå¯èƒ½å¯¼è‡´è¯¯åˆ¤ï¼Œè€Œå…¨å±€ç‰¹å¾è¡¨ç°å‡ºæ›´å¥½çš„é²æ£’æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„FRæ–¹æ³•å¿½è§†äº†ä¸åŒå› ç´ å¯¼è‡´çš„ä½è´¨é‡å›¾åƒç‰¹å¾è´¨é‡çš„åå·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾è´¨é‡çš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾æ³¨æ„åŠ›èåˆï¼ˆLGAFï¼‰ç½‘ç»œã€‚è¯¥ç½‘ç»œæ ¹æ®ç‰¹å¾è´¨é‡è‡ªé€‚åº”åœ°åˆ†é…å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œå¹¶é€šè¿‡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„äº’è¡¥æ€§è·å¾—æ›´å…·åŒºåˆ†æ€§å’Œé«˜è´¨é‡çš„äººè„¸ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåœ°è·å–å„ç§å°ºåº¦çš„ç²¾ç»†ä¿¡æ¯ï¼Œå¹¶åœ¨é«˜ç»´ç©ºé—´ä¸­å¢åŠ é¢éƒ¨ç‰¹å¾çš„å¯åˆ†æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå¤´å¤šå°ºåº¦å±€éƒ¨ç‰¹å¾æå–ï¼ˆMHMSï¼‰æ¨¡å—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLGAFåœ¨CFP-FPã€CPLFWã€AgeDBå’ŒCALFWå››ä¸ªéªŒè¯é›†ä¸Šçš„å¹³å‡æ€§èƒ½æœ€ä½³ï¼Œå¹¶ä¸”åœ¨TinyFaceå’ŒSCFaceä¸Šçš„æ€§èƒ½è¶…è¿‡äº†å½“å‰æœ€å‰æ²¿çš„æ–¹æ³•ï¼ˆSoTAï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16169v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä½è´¨é‡äººè„¸å›¾åƒè¯†åˆ«ä¸­çš„å±€éƒ¨åŒºåŸŸç¼ºå¤±æˆ–å˜å½¢é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºç‰¹å¾è´¨é‡çš„å±€éƒ¨ä¸å…¨å±€ç‰¹å¾æ³¨æ„åŠ›èåˆï¼ˆLGAFï¼‰ç½‘ç»œã€‚è¯¥ç½‘ç»œæ ¹æ®ç‰¹å¾è´¨é‡è‡ªé€‚åº”åˆ†é…å±€éƒ¨å’Œå…¨å±€ç‰¹å¾çš„æ³¨æ„åŠ›ï¼Œé€šè¿‡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„äº’è¡¥æ€§ï¼Œè·å–æ›´å…·åˆ¤åˆ«åŠ›å’Œé«˜è´¨é‡çš„äººè„¸ç‰¹å¾ã€‚åŒæ—¶ï¼Œå¼•å…¥å¤šå¤´å¤šå°ºåº¦å±€éƒ¨ç‰¹å¾æå–ï¼ˆMHMSï¼‰æ¨¡å—ï¼Œä»¥è·å–å¤šå°ºåº¦ç²¾ç»†ä¿¡æ¯ï¼Œæé«˜é«˜ç»´ç©ºé—´ä¸­é¢éƒ¨ç‰¹å¾çš„å¯åˆ†æ€§ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªéªŒè¯é›†ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½è´¨é‡äººè„¸å›¾åƒè¯†åˆ«é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦è¡¨ç°ä¸ºå±€éƒ¨é¢éƒ¨åŒºåŸŸçš„ç¼ºå¤±æˆ–å˜å½¢ã€‚</li>
<li>å±€éƒ¨åŒºåŸŸç›¸ä¼¼æ€§å¯¹ç¼ºå¤±éƒ¨åˆ†é¢éƒ¨åŒºåŸŸçš„ä½è´¨é‡å›¾åƒè¯†åˆ«æ›´é‡è¦ï¼Œè€Œå±€éƒ¨é¢éƒ¨å˜å½¢æ—¶è¿‡åº¦å…³æ³¨å±€éƒ¨å¯èƒ½å¯¼è‡´è¯¯åˆ¤ï¼Œå…¨å±€ç‰¹å¾æ›´å…·é²æ£’æ€§ã€‚</li>
<li>ç°æœ‰çš„äººè„¸è¯†åˆ«æ–¹æ³•å¿½è§†äº†ç”±ä¸åŒå› ç´ å¼•èµ·çš„ä½è´¨é‡å›¾åƒç‰¹å¾è´¨é‡çš„åå·®ã€‚</li>
<li>æå‡ºçš„LGAFç½‘ç»œåŸºäºç‰¹å¾è´¨é‡è‡ªé€‚åº”åˆ†é…å±€éƒ¨å’Œå…¨å±€ç‰¹å¾çš„æ³¨æ„åŠ›ã€‚</li>
<li>LGAFç½‘ç»œé€šè¿‡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„äº’è¡¥æ€§ï¼Œè·å¾—æ›´å…·åˆ¤åˆ«åŠ›å’Œé«˜è´¨é‡çš„äººè„¸ç‰¹å¾ã€‚</li>
<li>MHMSæ¨¡å—ç”¨äºè·å–å¤šå°ºåº¦ç²¾ç»†ä¿¡æ¯ï¼Œæé«˜é¢éƒ¨ç‰¹å¾åœ¨é«˜ç»´ç©ºé—´ä¸­çš„å¯åˆ†æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1c490b3e7515ac466418badd88ed2645.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6e08bfba1fb76ee806c24830c96f7a96.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2713b9070b613d2c9db7e25920cb04f0.jpg" align="middle">
</details>




<h2 id="Enhancing-the-Transferability-of-Adversarial-Attacks-on-Face-Recognition-with-Diverse-Parameters-Augmentation"><a href="#Enhancing-the-Transferability-of-Adversarial-Attacks-on-Face-Recognition-with-Diverse-Parameters-Augmentation" class="headerlink" title="Enhancing the Transferability of Adversarial Attacks on Face Recognition   with Diverse Parameters Augmentation"></a>Enhancing the Transferability of Adversarial Attacks on Face Recognition   with Diverse Parameters Augmentation</h2><p><strong>Authors:Fengfan Zhou, Bangjie Yin, Hefei Ling, Qianyu Zhou, Wenxuan Wang</strong></p>
<p>Face Recognition (FR) models are vulnerable to adversarial examples that subtly manipulate benign face images, underscoring the urgent need to improve the transferability of adversarial attacks in order to expose the blind spots of these systems. Existing adversarial attack methods often overlook the potential benefits of augmenting the surrogate model with diverse initializations, which limits the transferability of the generated adversarial examples. To address this gap, we propose a novel method called Diverse Parameters Augmentation (DPA) attack method, which enhances surrogate models by incorporating diverse parameter initializations, resulting in a broader and more diverse set of surrogate models. Specifically, DPA consists of two key stages: Diverse Parameters Optimization (DPO) and Hard Model Aggregation (HMA). In the DPO stage, we initialize the parameters of the surrogate model using both pre-trained and random parameters. Subsequently, we save the models in the intermediate training process to obtain a diverse set of surrogate models. During the HMA stage, we enhance the feature maps of the diversified surrogate models by incorporating beneficial perturbations, thereby further improving the transferability. Experimental results demonstrate that our proposed attack method can effectively enhance the transferability of the crafted adversarial face examples. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ¨¡å‹å®¹æ˜“å—åˆ°å¯¹æŠ—æ ·æœ¬çš„å½±å“ï¼Œè¿™äº›å¯¹æŠ—æ ·æœ¬ä¼šå¾®å¦™åœ°æ“ä½œè‰¯æ€§çš„äººè„¸å›¾åƒï¼Œè¿™çªæ˜¾äº†æé«˜å¯¹æŠ—æ”»å‡»è½¬ç§»æ€§çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»¥æš´éœ²è¿™äº›ç³»ç»Ÿçš„ç›²ç‚¹ã€‚ç°æœ‰çš„å¯¹æŠ—æ”»å‡»æ–¹æ³•å¾€å¾€å¿½è§†äº†ç”¨å¤šç§åˆå§‹åŒ–å¢å¼ºä»£ç†æ¨¡å‹çš„æ½œåœ¨ä¼˜åŠ¿ï¼Œä»è€Œé™åˆ¶äº†ç”Ÿæˆå¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå¤šæ ·æ€§å‚æ•°å¢å¼ºï¼ˆDPAï¼‰æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡èå…¥å¤šç§å‚æ•°åˆå§‹åŒ–æ¥å¢å¼ºä»£ç†æ¨¡å‹ï¼Œä»è€Œäº§ç”Ÿæ›´å¹¿æ³›ã€æ›´å¤šæ ·åŒ–çš„ä»£ç†æ¨¡å‹é›†åˆã€‚å…·ä½“æ¥è¯´ï¼ŒDPAåŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šå¤šæ ·å‚æ•°ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¡¬æ¨¡å‹èšåˆï¼ˆHMAï¼‰ã€‚åœ¨DPOé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒå‚æ•°å’Œéšæœºå‚æ•°æ¥åˆå§‹åŒ–ä»£ç†æ¨¡å‹çš„å‚æ•°ã€‚éšåï¼Œæˆ‘ä»¬åœ¨ä¸­é—´è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æ¨¡å‹ï¼Œä»¥è·å¾—å¤šæ ·åŒ–çš„ä»£ç†æ¨¡å‹é›†åˆã€‚åœ¨HMAé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡èå…¥æœ‰ç›Šçš„æ‰°åŠ¨æ¥å¢å¼ºå¤šæ ·åŒ–ä»£ç†æ¨¡å‹çš„ç‰¹å¾æ˜ å°„ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜è¿ç§»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ”»å‡»æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜æ„å»ºå¯¹æŠ—é¢éƒ¨æ ·æœ¬çš„è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15555v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«æ¨¡å‹æ˜“å—å¯¹æŠ—æ ·æœ¬æ”»å‡»ï¼Œä¸ºæ­¤éœ€æ”¹è¿›å¯¹æŠ—æ”»å‡»çš„è¿ç§»æ€§ã€‚ç°æœ‰æ”»å‡»æ–¹æ³•å¿½ç•¥åˆ©ç”¨å¤šæ ·åŒ–åˆå§‹åŒ–å¢å¼ºæ›¿ä»£æ¨¡å‹çš„æ½œåŠ›ï¼Œé™åˆ¶äº†å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºä¸€ç§åä¸ºDPAï¼ˆå¤šæ ·åŒ–å‚æ•°å¢å¼ºï¼‰çš„æ–°å‹æ”»å‡»æ–¹æ³•ã€‚å®ƒé‡‡ç”¨ä¸¤ä¸ªé˜¶æ®µâ€”â€”å¤šæ ·åŒ–å‚æ•°ä¼˜åŒ–å’Œç¡¬æ¨¡å‹èšåˆâ€”â€”åˆ©ç”¨é¢„è®­ç»ƒå‚æ•°å’Œéšæœºå‚æ•°åˆå§‹åŒ–æ›¿ä»£æ¨¡å‹ï¼Œä»è€Œè·å–æ›´å…¨é¢ã€å¤šæ ·çš„æ¨¡å‹é›†ï¼Œå¢å¼ºäº†å¯¹æŠ—æ ·æœ¬çš„è¿ç§»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•å¯æœ‰æ•ˆæå‡æ„é€ çš„å¯¹æŠ—æ€§é¢éƒ¨ç¤ºä¾‹çš„è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«æ¨¡å‹æ˜“å—å¯¹æŠ—æ ·æœ¬æ”»å‡»ï¼Œéœ€è¦åŠ å¼ºå…¶å¯¹æŠ—æ”»å‡»çš„è¿ç§»æ€§ä»¥æš´éœ²ç³»ç»Ÿç›²ç‚¹ã€‚</li>
<li>ç°æœ‰æ”»å‡»æ–¹æ³•å¿½ç•¥äº†åˆ©ç”¨å¤šæ ·åŒ–åˆå§‹åŒ–å¢å¼ºæ›¿ä»£æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
<li>DPAæ”»å‡»æ–¹æ³•é€šè¿‡ç»“åˆé¢„è®­ç»ƒå‚æ•°å’Œéšæœºå‚æ•°åˆå§‹åŒ–å¢å¼ºæ›¿ä»£æ¨¡å‹ï¼Œè·å–æ›´å…¨é¢ã€å¤šæ ·çš„æ¨¡å‹é›†ã€‚</li>
<li>DPAæ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šå¤šæ ·åŒ–å‚æ•°ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¡¬æ¨¡å‹èšåˆï¼ˆHMAï¼‰ã€‚</li>
<li>åœ¨DPOé˜¶æ®µï¼Œæ¨¡å‹å‚æ•°è¿›è¡Œå¤šæ ·åŒ–åˆå§‹åŒ–ï¼Œå¹¶åœ¨ä¸­é—´è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æ¨¡å‹ï¼Œä»¥è·å–å¤šæ ·çš„æ›¿ä»£æ¨¡å‹é›†ã€‚</li>
<li>åœ¨HMAé˜¶æ®µï¼Œé€šè¿‡èå…¥æœ‰ç›Šæ‰°åŠ¨å¢å¼ºäº†å¤šæ ·åŒ–æ›¿ä»£æ¨¡å‹çš„ç‰¹æ€§æ˜ å°„ï¼Œè¿›ä¸€æ­¥æé«˜è¿ç§»æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b74c70653d27fde02b722752acc60060.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4bfa1c457fa80523cba8fba4d8b2e10.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-1ebaaf66128b05428ecd46dc480c2500.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5c33ae980963477c8e87d8c8e74dc214.jpg" align="middle">
</details>




<h2 id="CLIP-Unreasonable-Potential-in-Single-Shot-Face-Recognition"><a href="#CLIP-Unreasonable-Potential-in-Single-Shot-Face-Recognition" class="headerlink" title="CLIP Unreasonable Potential in Single-Shot Face Recognition"></a>CLIP Unreasonable Potential in Single-Shot Face Recognition</h2><p><strong>Authors:Nhan T. Luu</strong></p>
<p>Face recognition is a core task in computer vision designed to identify and authenticate individuals by analyzing facial patterns and features. This field intersects with artificial intelligence image processing and machine learning with applications in security authentication and personalization. Traditional approaches in facial recognition focus on capturing facial features like the eyes, nose and mouth and matching these against a database to verify identities. However challenges such as high false positive rates have persisted often due to the similarity among individuals facial features. Recently Contrastive Language Image Pretraining (CLIP) a model developed by OpenAI has shown promising advancements by linking natural language processing with vision tasks allowing it to generalize across modalities. Using CLIPâ€™s vision language correspondence and single-shot finetuning the model can achieve lower false positive rates upon deployment without the need of mass facial features extraction. This integration demonstrating CLIPâ€™s potential to address persistent issues in face recognition model performance without complicating our training paradigm. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œé€šè¿‡åˆ†æå’Œè¯†åˆ«é¢éƒ¨æ¨¡å¼å’Œç‰¹å¾æ¥è¯†åˆ«å’ŒéªŒè¯ä¸ªäººèº«ä»½ã€‚è¿™ä¸ªé¢†åŸŸä¸äººå·¥æ™ºèƒ½å›¾åƒå¤„ç†å’Œæœºå™¨å­¦ä¹ æœ‰äº¤é›†ï¼Œåœ¨èº«ä»½è®¤è¯å’Œå®‰å…¨ä¸ªæ€§åŒ–æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„é¢éƒ¨è¯†åˆ«æ–¹æ³•ä¸»è¦å…³æ³¨æ•æ‰çœ¼ç›ã€é¼»å­å’Œå˜´å·´ç­‰é¢éƒ¨ç‰¹å¾ï¼Œå¹¶ä¸æ•°æ®åº“ä¸­çš„ä¿¡æ¯è¿›è¡ŒåŒ¹é…ä»¥éªŒè¯èº«ä»½ã€‚ç„¶è€Œï¼Œç”±äºä¸ªä½“ä¹‹é—´é¢éƒ¨ç‰¹å¾çš„ç›¸ä¼¼æ€§ï¼Œä¸€ç›´å­˜åœ¨é«˜è¯¯æŠ¥ç‡ç­‰æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼ŒOpenAIå¼€å‘çš„Contrastive Language Image Pretrainingï¼ˆCLIPï¼‰æ¨¡å‹é€šè¿‡å°†è‡ªç„¶è¯­è¨€å¤„ç†ä¸è§†è§‰ä»»åŠ¡ç›¸è”ç³»ï¼Œå±•ç¤ºäº†åœ¨è·¨æ¨¡æ€æ³›åŒ–æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡ä½¿ç”¨CLIPçš„è§†è§‰è¯­è¨€å¯¹åº”å…³ç³»å’Œå•æ¬¡å¾®è°ƒæŠ€æœ¯ï¼Œæ¨¡å‹å¯ä»¥åœ¨éƒ¨ç½²æ—¶å®ç°è¾ƒä½çš„è¯¯æŠ¥ç‡ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§è§„æ¨¡çš„é¢éƒ¨ç‰¹å¾æå–ã€‚è¿™ç§é›†æˆå±•ç¤ºäº†CLIPè§£å†³äººè„¸è¯†åˆ«æ¨¡å‹æ€§èƒ½æŒç»­é—®é¢˜çš„æ½œåŠ›ï¼Œè€Œä¸ä¼šä½¿è®­ç»ƒæ¨¡å¼å¤æ‚åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12319v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººè„¸è¯†åˆ«æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œé€šè¿‡åˆ†æé¢éƒ¨æ¨¡å¼ç‰¹å¾æ¥è¯†åˆ«å’ŒéªŒè¯ä¸ªäººèº«ä»½ã€‚æ­¤é¢†åŸŸä¸äººå·¥æ™ºèƒ½å›¾åƒå¤„ç†åŠæœºå™¨å­¦ä¹ ç›¸ç»“åˆï¼Œåœ¨èº«ä»½éªŒè¯åŠä¸ªæ€§åŒ–åº”ç”¨æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ä¼ ç»Ÿäººè„¸è¯†åˆ«æ–¹æ³•ä¾§é‡äºæ•æ‰çœ¼éƒ¨ã€é¼»å­å’Œå˜´å·´ç­‰é¢éƒ¨ç‰¹å¾ï¼Œå¹¶ä¸æ•°æ®åº“è¿›è¡ŒåŒ¹é…ä»¥éªŒè¯èº«ä»½ã€‚ç„¶è€Œï¼Œç”±äºä¸ªä½“é¢éƒ¨ç‰¹å¾çš„ç›¸ä¼¼æ€§ï¼Œä¸€ç›´å­˜åœ¨é«˜è¯¯æŠ¥ç‡çš„é—®é¢˜ã€‚æœ€è¿‘ï¼ŒOpenAIå¼€å‘çš„CLIPæ¨¡å‹é€šè¿‡è¿æ¥è‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰ä»»åŠ¡ï¼Œå±•ç°å‡ºè·¨æ¨¡æ€çš„é€šç”¨æ€§æ½œåŠ›ã€‚CLIPçš„è§†è§‰è¯­è¨€å¯¹åº”å’Œå•æ¬¡å¾®è°ƒèƒ½å¤Ÿå®ç°è¾ƒä½çš„éƒ¨ç½²è¯¯æŠ¥ç‡ï¼Œæ— éœ€å¤§é‡é¢éƒ¨ç‰¹å¾æå–ã€‚è¿™è¡¨æ˜CLIPæ¨¡å‹å…·æœ‰è§£å†³äººè„¸è¯†åˆ«æ¨¡å‹æŒä¹…æ€§é—®é¢˜ï¼ŒåŒæ—¶ç®€åŒ–è®­ç»ƒæ¨¡å¼çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«æ˜¯è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒä»»åŠ¡ï¼Œæ¶‰åŠèº«ä»½è¯†åˆ«å’ŒéªŒè¯ã€‚</li>
<li>ä¼ ç»Ÿäººè„¸è¯†åˆ«æ–¹æ³•ä¸»è¦å…³æ³¨é¢éƒ¨ç‰¹å¾çš„æ•æ‰å’Œæ•°æ®åº“åŒ¹é…ã€‚</li>
<li>æŒ‘æˆ˜ä¹‹ä¸€æ˜¯é«˜è¯¯æŠ¥ç‡ï¼Œè¿™æºäºä¸ªä½“é—´é¢éƒ¨ç‰¹å¾çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>CLIPæ¨¡å‹ç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰ä»»åŠ¡ï¼Œå±•ç°å‡ºè·¨æ¨¡æ€æ½œåŠ›ã€‚</li>
<li>CLIPé€šè¿‡è§†è§‰è¯­è¨€å¯¹åº”å’Œå•æ¬¡å¾®è°ƒï¼Œé™ä½éƒ¨ç½²æ—¶çš„è¯¯æŠ¥ç‡ï¼Œå‡å°‘å¯¹é¢éƒ¨ç‰¹å¾æå–çš„éœ€æ±‚ã€‚</li>
<li>CLIPæ¨¡å‹å…·æœ‰è§£å†³äººè„¸è¯†åˆ«æ¨¡å‹æŒä¹…æ€§é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-761e63e5f2702743ad5ce68d6c04f648.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-59f09d15c8244e0d028e0e3d4fe70400.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0785f430c4d4295e208f4b05007c0fe0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b6a8f2fb1f22b99a10f9a2a0010fb3ac.jpg" align="middle">
</details>




<h2 id="HyperFace-Generating-Synthetic-Face-Recognition-Datasets-by-Exploring-Face-Embedding-Hypersphere"><a href="#HyperFace-Generating-Synthetic-Face-Recognition-Datasets-by-Exploring-Face-Embedding-Hypersphere" class="headerlink" title="HyperFace: Generating Synthetic Face Recognition Datasets by Exploring   Face Embedding Hypersphere"></a>HyperFace: Generating Synthetic Face Recognition Datasets by Exploring   Face Embedding Hypersphere</h2><p><strong>Authors:Hatef Otroshi Shahreza, SÃ©bastien Marcel</strong></p>
<p>Face recognition datasets are often collected by crawling Internet and without individualsâ€™ consents, raising ethical and privacy concerns. Generating synthetic datasets for training face recognition models has emerged as a promising alternative. However, the generation of synthetic datasets remains challenging as it entails adequate inter-class and intra-class variations. While advances in generative models have made it easier to increase intra-class variations in face datasets (such as pose, illumination, etc.), generating sufficient inter-class variation is still a difficult task. In this paper, we formulate the dataset generation as a packing problem on the embedding space (represented on a hypersphere) of a face recognition model and propose a new synthetic dataset generation approach, called HyperFace. We formalize our packing problem as an optimization problem and solve it with a gradient descent-based approach. Then, we use a conditional face generator model to synthesize face images from the optimized embeddings. We use our generated datasets to train face recognition models and evaluate the trained models on several benchmarking real datasets. Our experimental results show that models trained with HyperFace achieve state-of-the-art performance in training face recognition using synthetic datasets. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«æ•°æ®é›†é€šå¸¸é€šè¿‡ç½‘ä¸Šçˆ¬è™«æ”¶é›†ï¼Œå¹¶ä¸”æœªç»ä¸ªäººåŒæ„ï¼Œå¼•å‘äº†ä¼¦ç†å’Œéšç§é—®é¢˜ã€‚ä½¿ç”¨åˆæˆæ•°æ®é›†æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹å·²æˆä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç”Ÿæˆåˆæˆæ•°æ®é›†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™éœ€è¦è¶³å¤Ÿçš„ç±»é—´å’Œç±»å†…å˜åŒ–ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥ä½¿å¾—å¢åŠ äººè„¸æ•°æ®é›†å†…çš„ç±»å†…å˜åŒ–å˜å¾—æ›´åŠ å®¹æ˜“ï¼ˆå¦‚å§¿æ€ã€ç…§æ˜ç­‰ï¼‰ï¼Œä½†ç”Ÿæˆè¶³å¤Ÿçš„ç±»é—´å˜åŒ–ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ•°æ®é›†ç”Ÿæˆé—®é¢˜è½¬åŒ–ä¸ºäººè„¸è¯†åˆ«æ¨¡å‹åµŒå…¥ç©ºé—´ï¼ˆè¡¨ç¤ºä¸ºè¶…çƒä½“ï¼‰ä¸Šçš„æ‰“åŒ…é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºHyperFaceã€‚æˆ‘ä»¬å°†æ‰“åŒ…é—®é¢˜å½¢å¼åŒ–ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é‡‡ç”¨åŸºäºæ¢¯åº¦ä¸‹é™çš„æ–¹æ³•æ¥è§£å†³ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ¡ä»¶äººè„¸ç”Ÿæˆå™¨æ¨¡å‹ä»ä¼˜åŒ–åçš„åµŒå…¥ä¸­åˆæˆäººè„¸å›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆçš„æ•°æ®é›†æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ï¼Œå¹¶åœ¨å‡ ä¸ªåŸºå‡†çœŸå®æ•°æ®é›†ä¸Šè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨HyperFaceè®­ç»ƒæ¨¡å‹åœ¨åˆ©ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒäººè„¸è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.08470v1">PDF</a> Accepted in NeurIPS 2024 Safe Generative AI Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨é¢éƒ¨è¯†åˆ«æ•°æ®é›†æ”¶é›†è¿‡ç¨‹ä¸­å­˜åœ¨çš„ä¼¦ç†å’Œéšç§é—®é¢˜ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ–°å‹åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•â€”â€”HyperFaceã€‚è¯¥æ–¹æ³•å°†æ•°æ®é›†ç”Ÿæˆé—®é¢˜è¡¨è¿°ä¸ºåµŒå…¥ç©ºé—´çš„æ’åˆ—é—®é¢˜ï¼Œå¹¶åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•è¿›è¡Œæ±‚è§£ã€‚é€šè¿‡åˆæˆé¢éƒ¨å›¾åƒä¼˜åŒ–åµŒå…¥ï¼Œè¿›è€Œè®­ç»ƒé¢éƒ¨è¯†åˆ«æ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨HyperFaceè®­ç»ƒçš„æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨è¯†åˆ«æ•°æ®é›†çš„æ”¶é›†å¸¸æ¶‰åŠä¼¦ç†å’Œéšç§é—®é¢˜ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†ä½œä¸ºè®­ç»ƒé¢éƒ¨è¯†åˆ«æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆå¤‡å—å…³æ³¨ã€‚</li>
<li>ç”Ÿæˆåˆæˆæ•°æ®é›†é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è¶³å¤Ÿçš„ç±»é—´å’Œç±»å†…å˜åŒ–ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯æ˜“äºå¢åŠ ç±»å†…å˜åŒ–ï¼Œä½†ç”Ÿæˆè¶³å¤Ÿçš„ç±»é—´å˜åŒ–ä»ç„¶å›°éš¾ã€‚</li>
<li>æœ¬æ–‡å°†æ•°æ®é›†ç”Ÿæˆè¡¨è¿°ä¸ºåµŒå…¥ç©ºé—´çš„æ’åˆ—é—®é¢˜ï¼Œå¹¶æå‡ºHyperFaceæ–¹æ³•ã€‚</li>
<li>HyperFaceæ–¹æ³•é€šè¿‡ä¼˜åŒ–åµŒå…¥ç©ºé—´è§£å†³æ’åˆ—é—®é¢˜ï¼Œå¹¶ä½¿ç”¨åŸºäºæ¢¯åº¦çš„ä¸‹é™æ–¹æ³•è¿›è¡Œæ±‚è§£ã€‚</li>
<li>ä½¿ç”¨æ¡ä»¶é¢éƒ¨ç”Ÿæˆå™¨æ¨¡å‹ä»ä¼˜åŒ–çš„åµŒå…¥ä¸­åˆæˆé¢éƒ¨å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-985f54057c1faaf9a745efc79f105acd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-46824d2f6082475e2c89c230472a59e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e0fa7b3b0a848b4a808efcda7b1f0022.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c240758b8ee256743732c29ff212735d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c5598e48a7995712d82a481e4f8a4d34.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-deb528a02f1fc56b8e8dac0001eed6ad.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-008fc4e264c76979d8c4eae9a0be6fa9.jpg" align="middle">
</details>




<h2 id="A-Real-time-Face-Mask-Detection-and-Social-Distancing-System-for-COVID-19-using-Attention-InceptionV3-Model"><a href="#A-Real-time-Face-Mask-Detection-and-Social-Distancing-System-for-COVID-19-using-Attention-InceptionV3-Model" class="headerlink" title="A Real-time Face Mask Detection and Social Distancing System for   COVID-19 using Attention-InceptionV3 Model"></a>A Real-time Face Mask Detection and Social Distancing System for   COVID-19 using Attention-InceptionV3 Model</h2><p><strong>Authors:Abdullah Al Asif, Farhana Chowdhury Tisha</strong></p>
<p>One of the deadliest pandemics is now happening in the current world due to COVID-19. This contagious virus is spreading like wildfire around the whole world. To minimize the spreading of this virus, World Health Organization (WHO) has made protocols mandatory for wearing face masks and maintaining 6 feet physical distance. In this paper, we have developed a system that can detect the proper maintenance of that distance and people are properly using masks or not. We have used the customized attention-inceptionv3 model in this system for the identification of those two components. We have used two different datasets along with 10,800 images including both with and without Face Mask images. The training accuracy has been achieved 98% and validation accuracy 99.5%. The system can conduct a precision value of around 98.2% and the frame rate per second (FPS) was 25.0. So, with this system, we can identify high-risk areas with the highest possibility of the virus spreading zone. This may help authorities to take necessary steps to locate those risky areas and alert the local people to ensure proper precautions in no time. </p>
<blockquote>
<p>å½“å‰ä¸–ç•Œæ­£åœ¨å‘ç”Ÿä¸€åœºç”±COVID-19å¼•èµ·çš„è‡´å‘½å¤§æµè¡Œç—…ã€‚è¿™ç§ä¼ æŸ“ç—…ç—…æ¯’æ­£åœ¨å…¨çƒèŒƒå›´å†…å¦‚é‡ç«èˆ¬è”“å»¶ã€‚ä¸ºäº†å°½é‡å‡å°‘è¿™ç§ç—…æ¯’çš„ä¼ æ’­ï¼Œä¸–ç•Œå«ç”Ÿç»„ç»‡ï¼ˆWHOï¼‰åˆ¶å®šäº†å¼ºåˆ¶ä½©æˆ´å£ç½©å’Œä¿æŒå…­è‹±å°ºç‰©ç†è·ç¦»çš„è§„å®šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿï¼Œå¯ä»¥æ£€æµ‹äººä»¬æ˜¯å¦éµå®ˆè¿™ä¸€è·ç¦»è§„å®šå¹¶æ­£ç¡®ä½¿ç”¨å£ç½©ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å®šåˆ¶çš„æ³¨æ„åŠ›æœºåˆ¶inceptionv3æ¨¡å‹æ¥è¯†åˆ«è¿™ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œå…±åŒ…å«10,800å¼ å›¾åƒï¼Œå…¶ä¸­åŒ…æ‹¬æˆ´å£ç½©å’Œä¸æˆ´å£ç½©çš„å›¾åƒã€‚è®­ç»ƒå‡†ç¡®ç‡è¾¾åˆ°äº†98%ï¼ŒéªŒè¯å‡†ç¡®ç‡è¾¾åˆ°äº†99.5%ã€‚è¯¥ç³»ç»Ÿçš„ç²¾åº¦å€¼å¯è¾¾çº¦98.2%ï¼Œæ¯ç§’å¸§ç‡ï¼ˆFPSï¼‰ä¸º25.0ã€‚å› æ­¤ï¼Œå€ŸåŠ©æ­¤ç³»ç»Ÿï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«ç—…æ¯’ä¼ æ’­å¯èƒ½æ€§æœ€é«˜çš„é«˜é£é™©åŒºåŸŸã€‚è¿™å¯ä»¥å¸®åŠ©å½“å±€é‡‡å–æªæ–½è¿…é€Ÿå®šä½è¿™äº›å±é™©åŒºåŸŸï¼Œå¹¶æé†’å½“åœ°å±…æ°‘ç¡®ä¿é‡‡å–é€‚å½“çš„é¢„é˜²æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05312v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«æŠ€æœ¯åœ¨æŠ—ç–«ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä¸€ç ”ç©¶é€šè¿‡å®šåˆ¶åŒ–çš„attention-inceptionv3æ¨¡å‹æ£€æµ‹äººä»¬æ˜¯å¦ä½©æˆ´å£ç½©ä»¥åŠä¿æŒç¤¾äº¤è·ç¦»çš„æƒ…å†µï¼Œåˆ©ç”¨ä¸¤ä¸ªæ•°æ®é›†å…±10,800å¼ å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå‡†ç¡®ç‡è¾¾98%ï¼Œå¯å¸®åŠ©è¯†åˆ«é«˜é£é™©åŒºåŸŸå¹¶é‡‡å–æªæ–½é˜²æ­¢ç—…æ¯’ä¼ æ’­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COVID-19ç–«æƒ…æˆä¸ºå½“å‰å…¨çƒæœ€ä¸¥é‡çš„æµè¡Œç—…ä¹‹ä¸€ã€‚</li>
<li>ä¸–ç•Œå«ç”Ÿç»„ç»‡ï¼ˆWHOï¼‰ä¸ºå‡å°‘ç—…æ¯’ä¼ æ’­åˆ¶å®šäº†æˆ´å£ç½©å’Œä¿æŒå…­è‹±å°ºè·ç¦»çš„è§„å®šã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨å®šåˆ¶çš„attention-inceptionv3æ¨¡å‹å¼€å‘äº†å¯æ£€æµ‹äººä»¬æ˜¯å¦éµå®ˆæˆ´å£ç½©å’Œä¿æŒç¤¾äº¤è·ç¦»çš„ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†å…±åŒ…å«è¶…è¿‡ä¸€ä¸‡å¼ å›¾åƒè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬æˆ´å£ç½©å’Œä¸æˆ´å£ç½©çš„å›¾åƒã€‚</li>
<li>è®­ç»ƒå‡†ç¡®ç‡è¾¾åˆ°äº†98%ï¼ŒéªŒè¯å‡†ç¡®ç‡è¾¾åˆ°äº†99.5%ã€‚ç³»ç»Ÿçš„ç²¾åº¦å€¼çº¦ä¸º98.2%ï¼Œæ¯ç§’å¸§é€Ÿç‡ä¸º25ã€‚</li>
<li>è¯¥ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«ç—…æ¯’ä¼ æ’­å¯èƒ½æ€§æœ€é«˜çš„é«˜é£é™©åŒºåŸŸã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f58708fcbd5f3d6db2a4b1d4e32f4641.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-039750362cfa68ca19d2cf5272e94d96.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-99b2cbe3c44ad5a1a320bf2258c1ee6a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eb896b0f9ec46fcbc232b3cc6636720e.jpg" align="middle">
</details>




<h2 id="An-Enhancement-of-Haar-Cascade-Algorithm-Applied-to-Face-Recognition-for-Gate-Pass-Security"><a href="#An-Enhancement-of-Haar-Cascade-Algorithm-Applied-to-Face-Recognition-for-Gate-Pass-Security" class="headerlink" title="An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for   Gate Pass Security"></a>An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for   Gate Pass Security</h2><p><strong>Authors:Clarence A. Antipona, Romeo R. Magsino, Raymund M. Dioses, Khatalyn E. Mata</strong></p>
<p>This study is focused on enhancing the Haar Cascade Algorithm to decrease the false positive and false negative rate in face matching and face detection to increase the accuracy rate even under challenging conditions. The face recognition library was implemented with Haar Cascade Algorithm in which the 128-dimensional vectors representing the unique features of a face are encoded. A subprocess was applied where the grayscale image from Haar Cascade was converted to RGB to improve the face encoding. Logical process and face filtering are also used to decrease non-face detection. The Enhanced Haar Cascade Algorithm produced a 98.39% accuracy rate (21.39% increase), 63.59% precision rate, 98.30% recall rate, and 72.23% in F1 Score. In comparison, the Haar Cascade Algorithm achieved a 46.70% to 77.00% accuracy rate, 44.15% precision rate, 98.61% recall rate, and 47.01% in F1 Score. Both algorithms used the Confusion Matrix Test with 301,950 comparisons using the same dataset of 550 images. The 98.39% accuracy rate shows a significant decrease in false positive and false negative rates in facial recognition. Face matching and face detection are more accurate in images with complex backgrounds, lighting variations, and occlusions, or even those with similar attributes. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è‡´åŠ›äºæ”¹è¿›Haar Cascadeç®—æ³•ï¼Œæ—¨åœ¨é™ä½äººè„¸åŒ¹é…å’Œäººè„¸æ£€æµ‹ä¸­çš„è¯¯æŠ¥å’Œæ¼æŠ¥ç‡ï¼Œä»è€Œæé«˜äººè„¸è¯†åˆ«çš„å‡†ç¡®ç‡ï¼Œå³ä½¿é¢å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚äººè„¸è¯†åˆ«åº“é‡‡ç”¨Haar Cascadeç®—æ³•å®ç°ï¼Œå…¶ä¸­ä½¿ç”¨128ç»´å‘é‡å¯¹äººè„¸çš„ç‹¬ç‰¹ç‰¹å¾è¿›è¡Œç¼–ç ã€‚åº”ç”¨äº†ä¸€ä¸ªå­å¤„ç†è¿‡ç¨‹ï¼Œå°†Haar Cascadeçš„ç°åº¦å›¾åƒè½¬æ¢ä¸ºRGBå›¾åƒï¼Œä»¥æ”¹å–„äººè„¸ç¼–ç ã€‚è¿˜ä½¿ç”¨äº†é€»è¾‘è¿‡ç¨‹å’Œé¢éƒ¨è¿‡æ»¤æ¥å‡å°‘éé¢éƒ¨æ£€æµ‹ã€‚å¢å¼ºå‹Haar Cascadeç®—æ³•è¾¾åˆ°äº†98.39%çš„å‡†ç¡®ç‡ï¼ˆæé«˜äº†21.39%ï¼‰ï¼Œç²¾ç¡®åº¦ä¸º63.59%ï¼Œå¬å›ç‡ä¸º98.3%ï¼ŒF1åˆ†æ•°ä¸º72.23%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒHaar Cascadeç®—æ³•çš„å‡†ç¡®ç‡ä¸º46.7%è‡³77%ï¼Œç²¾ç¡®åº¦ä¸º44.15%ï¼Œå¬å›ç‡ä¸º98.6%ï¼ŒF1åˆ†æ•°ä¸º47%ã€‚ä¸¤ç§ç®—æ³•éƒ½ä½¿ç”¨æ··æ·†çŸ©é˜µæµ‹è¯•è¿›è¡Œæ¯”å¯¹ï¼Œä½¿ç”¨åŒä¸€æ•°æ®é›†è¿›è¡Œ30ä¸‡å¤šæ¬¡æ¯”è¾ƒï¼Œæ¶‰åŠå›¾åƒè¾¾550å¼ ã€‚98.39%çš„å‡†ç¡®ç‡è¡¨æ˜é¢éƒ¨è¯†åˆ«ä¸­çš„è¯¯æŠ¥å’Œæ¼æŠ¥ç‡æ˜¾è‘—é™ä½ã€‚äººè„¸åŒ¹é…å’Œäººè„¸æ£€æµ‹åœ¨å…·æœ‰å¤æ‚èƒŒæ™¯ã€å…‰ç…§å˜åŒ–å’Œé®æŒ¡çš„å›¾åƒä¸­æ›´ä¸ºå‡†ç¡®ï¼Œç”šè‡³å¯¹äºå…·æœ‰ç›¸ä¼¼ç‰¹å¾çš„å›¾åƒä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03831v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶æ”¹è¿›äº†Haar Cascadeç®—æ³•ï¼Œä»¥æé«˜äººè„¸è¯†åˆ«å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æŒ‘æˆ˜æ€§æ¡ä»¶ä¸‹ï¼ŒåŒ…æ‹¬èƒŒæ™¯å¤æ‚ã€å…‰çº¿å˜åŒ–åŠé®æŒ¡ç­‰æƒ…å†µã€‚è¯¥ç®—æ³•æé«˜äº†å‡†ç¡®ç‡ï¼Œé™ä½äº†è¯¯æŠ¥å’Œæ¼æŠ¥ç‡ã€‚æ”¹è¿›åçš„ç®—æ³•åœ¨é¢éƒ¨åŒ¹é…å’Œæ£€æµ‹æ–¹é¢è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚æ­¤å¤–ï¼Œä½¿ç”¨é€»è¾‘è¿‡ç¨‹å’Œé¢éƒ¨è¿‡æ»¤æŠ€æœ¯å‡å°‘äº†éé¢éƒ¨æ£€æµ‹ã€‚ç»è¿‡æµ‹è¯•éªŒè¯ï¼Œå¢å¼ºåçš„Haar Cascadeç®—æ³•è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨æ”¹è¿›Haar Cascadeç®—æ³•ä»¥æé«˜é¢éƒ¨è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å°†ç°åº¦å›¾åƒè½¬æ¢ä¸ºRGBå›¾åƒæ”¹å–„äº†é¢éƒ¨ç¼–ç è¿‡ç¨‹ã€‚</li>
<li>ä½¿ç”¨é€»è¾‘è¿‡ç¨‹å’Œé¢éƒ¨è¿‡æ»¤æŠ€æœ¯æ¥å‡å°‘éé¢éƒ¨æ£€æµ‹çš„é”™è¯¯ç‡ã€‚</li>
<li>å¢å¼ºåçš„Haar Cascadeç®—æ³•æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œè¾¾åˆ°äº†98.39%ï¼Œä¸åŸå§‹ç®—æ³•ç›¸æ¯”æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨å¤æ‚èƒŒæ™¯ã€å…‰çº¿å˜åŒ–åŠé®æŒ¡ç­‰æ¡ä»¶ä¸‹è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç®—æ³•æµ‹è¯•ä½¿ç”¨äº†æ··æ·†çŸ©é˜µæµ‹è¯•ï¼Œä¸åŸå§‹æ•°æ®é›†è¿›è¡Œäº†å¤§é‡æ¯”è¾ƒï¼Œè¯æ˜äº†ç®—æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bc821a44ebd8203d55ece5d4ce52e19b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c339620c007d99159995df6aa5c3eb89.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-51114b2eb50c0f796a5a8a448179f731.jpg" align="middle">
</details>




<h2 id="Digi2Real-Bridging-the-Realism-Gap-in-Synthetic-Data-Face-Recognition-via-Foundation-Models"><a href="#Digi2Real-Bridging-the-Realism-Gap-in-Synthetic-Data-Face-Recognition-via-Foundation-Models" class="headerlink" title="Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition   via Foundation Models"></a>Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition   via Foundation Models</h2><p><strong>Authors:Anjith George, Sebastien Marcel</strong></p>
<p>The accuracy of face recognition systems has improved significantly in the past few years, thanks to the large amount of data collected and the advancement in neural network architectures. However, these large-scale datasets are often collected without explicit consent, raising ethical and privacy concerns. To address this, there have been proposals to use synthetic datasets for training face recognition models. Yet, such models still rely on real data to train the generative models and generally exhibit inferior performance compared to those trained on real datasets. One of these datasets, DigiFace, uses a graphics pipeline to generate different identities and different intra-class variations without using real data in training the models. However, the performance of this approach is poor on face recognition benchmarks, possibly due to the lack of realism in the images generated from the graphics pipeline. In this work, we introduce a novel framework for realism transfer aimed at enhancing the realism of synthetically generated face images. Our method leverages the large-scale face foundation model, and we adapt the pipeline for realism enhancement. By integrating the controllable aspects of the graphics pipeline with our realism enhancement technique, we generate a large amount of realistic variations-combining the advantages of both approaches. Our empirical evaluations demonstrate that models trained using our enhanced dataset significantly improve the performance of face recognition systems over the baseline. The source code and datasets will be made available publicly: <a target="_blank" rel="noopener" href="https://www.idiap.ch/paper/digi2real">https://www.idiap.ch/paper/digi2real</a> </p>
<blockquote>
<p>è¿‘å‡ å¹´æ¥ï¼Œç”±äºæ”¶é›†äº†å¤§é‡çš„æ•°æ®ä»¥åŠç¥ç»ç½‘ç»œæ¶æ„çš„è¿›å±•ï¼Œäººè„¸è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®æ€§å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚ç„¶è€Œï¼Œè¿™äº›å¤§è§„æ¨¡æ•°æ®é›†å¾€å¾€æ˜¯åœ¨æ²¡æœ‰æ˜ç¡®åŒæ„çš„æƒ…å†µä¸‹æ”¶é›†çš„ï¼Œå¼•å‘äº†ä¼¦ç†å’Œéšç§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå·²ç»æœ‰äººæè®®ä½¿ç”¨åˆæˆæ•°æ®é›†æ¥è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶éœ€è¦çœŸå®æ•°æ®æ¥è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä¸”ä¸åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”ï¼Œé€šå¸¸è¡¨ç°å‡ºè¾ƒå·®çš„æ€§èƒ½ã€‚å…¶ä¸­ä¹‹ä¸€çš„DigiFaceæ•°æ®é›†ä½¿ç”¨å›¾å½¢ç®¡é“ç”Ÿæˆä¸åŒçš„èº«ä»½å’Œä¸åŒçš„ç±»å†…å˜åŒ–ï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çœŸå®æ•°æ®ã€‚ç„¶è€Œï¼Œè¯¥æ–¹æ³•åœ¨äººè„¸è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œå¯èƒ½æ˜¯ç”±äºå›¾å½¢ç®¡é“ç”Ÿæˆçš„å›¾åƒç¼ºä¹çœŸå®æ„Ÿæ‰€è‡´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç°å®ä¸»ä¹‰è½¬ç§»æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åˆæˆç”Ÿæˆçš„äººè„¸å›¾åƒçš„çœŸå®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡äººè„¸åŸºç¡€æ¨¡å‹ï¼Œå¹¶è°ƒæ•´ç®¡é“ä»¥å¢å¼ºç°å®æ„Ÿã€‚é€šè¿‡å°†å›¾å½¢ç®¡é“çš„å¯æ§æ–¹é¢ä¸æˆ‘ä»¬çš„å¢å¼ºç°å®æŠ€æœ¯ç›¸ç»“åˆï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å¤§é‡é€¼çœŸçš„å˜åŒ–ï¼Œç»“åˆäº†è¿™ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬å¢å¼ºçš„æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨äººè„¸è¯†åˆ«ç³»ç»Ÿçš„æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¿‡äº†åŸºçº¿ã€‚æºä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://www.idiap.ch/paper/digi2real">https://www.idiap.ch/paper/digi2real</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02188v3">PDF</a> The dataset would be available here:   <a target="_blank" rel="noopener" href="https://www.idiap.ch/paper/digi2real">https://www.idiap.ch/paper/digi2real</a></p>
<p><strong>Summary</strong>ï¼š<br>äººè„¸è¯†åˆ«ç³»ç»Ÿçš„å‡†ç¡®åº¦è¿‘å¹´æ¥æ˜¾è‘—æå‡ï¼Œä¸»è¦å¾—ç›Šäºå¤§é‡æ•°æ®çš„æ”¶é›†ä¸ç¥ç»ç½‘ç»œæ¶æ„çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œæ•°æ®æ”¶é›†å¸¸ç¼ºä¹æ˜ç¡®åŒæ„ï¼Œå¼•å‘ä¼¦ç†ä¸éšç§é—®é¢˜ã€‚ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œå·²æœ‰æè®®ä½¿ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œäººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™äº›æ¨¡å‹ä»ä¾èµ–çœŸå®æ•°æ®è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œä¸”æ€»ä½“è¡¨ç°é€šå¸¸ä¸å¦‚åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹ç°å®æ„Ÿè¿ç§»æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åˆæˆäººè„¸å›¾åƒçš„ç°å®æ„Ÿã€‚æ–°æ–¹æ³•åˆ©ç”¨å¤§è§„æ¨¡äººè„¸åŸºç¡€æ¨¡å‹ï¼Œå¹¶è°ƒæ•´ç®¡é“ä»¥è¿›è¡Œç°å®æ„Ÿå¢å¼ºã€‚é€šè¿‡ç»“åˆå›¾å½¢ç®¡é“çš„å¯æ§æ€§ä¸ç°å®æ„Ÿå¢å¼ºæŠ€æœ¯ï¼Œç”Ÿæˆäº†å¤§é‡é€¼çœŸçš„å˜åŒ–ï¼Œç»“åˆäº†ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨äººè„¸è¯†åˆ«ç³»ç»Ÿæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººè„¸è¯†åˆ«ç³»ç»Ÿå‡†ç¡®åº¦å› æ•°æ®æ”¶é›†å’Œç¥ç»ç½‘ç»œè¿›æ­¥è€Œæ˜¾è‘—æé«˜ã€‚</li>
<li>æ•°æ®æ”¶é›†ç¼ºä¹æ˜ç¡®åŒæ„å¼•å‘ä¼¦ç†å’Œéšç§é—®é¢˜ã€‚</li>
<li>åˆæˆæ•°æ®é›†ç”¨äºäººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒå­˜åœ¨ä¾èµ–çœŸå®æ•°æ®å’Œæ€§èƒ½é—®é¢˜ã€‚</li>
<li>æ–°å‹ç°å®æ„Ÿè¿ç§»æ¡†æ¶æé«˜åˆæˆäººè„¸å›¾åƒçš„ç°å®æ„Ÿã€‚</li>
<li>ç»“åˆå›¾å½¢ç®¡é“å¯æ§æ€§ä¸ç°å®æ„Ÿå¢å¼ºæŠ€æœ¯ç”Ÿæˆå¤§é‡é€¼çœŸå˜åŒ–ã€‚</li>
<li>ä½¿ç”¨å¢å¼ºæ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹åœ¨äººè„¸è¯†åˆ«ç³»ç»Ÿæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚</li>
<li>å…¬å¼€æä¾›æºä»£ç å’Œæ•°æ®é›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0a11d8213db851f8184bd135de008401.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5d4a3d87973d574c11509f949c5cd05f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5ea6538a3c5a46bb49d559a48b9a06fb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b71810fbe9bb3d05d3ab8eef11046914.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-caa4322c9b1d0c3042ea4a963a395171.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ec0a32c0a1d339bed4ad3f59d772046b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5df08e8fd5ace3d0e3539e90faa1f4b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9608dcd41d30f6d045b06eea3e0d0024.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6b688a534f2c32bee7e76509b118419d.jpg" align="middle">
</details>




<h2 id="Bridging-the-Gaps-Utilizing-Unlabeled-Face-Recognition-Datasets-to-Boost-Semi-Supervised-Facial-Expression-Recognition"><a href="#Bridging-the-Gaps-Utilizing-Unlabeled-Face-Recognition-Datasets-to-Boost-Semi-Supervised-Facial-Expression-Recognition" class="headerlink" title="Bridging the Gaps: Utilizing Unlabeled Face Recognition Datasets to   Boost Semi-Supervised Facial Expression Recognition"></a>Bridging the Gaps: Utilizing Unlabeled Face Recognition Datasets to   Boost Semi-Supervised Facial Expression Recognition</h2><p><strong>Authors:Jie Song, Mengqiao He, Jinhua Feng, Bairong Shen</strong></p>
<p>In recent years, Facial Expression Recognition (FER) has gained increasing attention. Most current work focuses on supervised learning, which requires a large amount of labeled and diverse images, while FER suffers from the scarcity of large, diverse datasets and annotation difficulty. To address these problems, we focus on utilizing large unlabeled Face Recognition (FR) datasets to boost semi-supervised FER. Specifically, we first perform face reconstruction pre-training on large-scale facial images without annotations to learn features of facial geometry and expression regions, followed by two-stage fine-tuning on FER datasets with limited labels. In addition, to further alleviate the scarcity of labeled and diverse images, we propose a Mixup-based data augmentation strategy tailored for facial images, and the loss weights of real and virtual images are determined according to the intersection-over-union (IoU) of the faces in the two images. Experiments on RAF-DB, AffectNet, and FERPlus show that our method outperforms existing semi-supervised FER methods and achieves new state-of-the-art performance. Remarkably, with only 5%, 25% training sets,our method achieves 64.02% on AffectNet,and 88.23% on RAF-DB, which is comparable to fully supervised state-of-the-art methods. Codes will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/zhelishisongjie/SSFER">https://github.com/zhelishisongjie/SSFER</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆFERï¼‰è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚ç›®å‰çš„å¤§å¤šæ•°å·¥ä½œéƒ½é›†ä¸­åœ¨ç›‘ç£å­¦ä¹ ä¸Šï¼Œè¿™éœ€è¦å¤§é‡æœ‰æ ‡ç­¾å’Œå¤šæ ·åŒ–çš„å›¾åƒã€‚ç„¶è€Œï¼ŒFERé¢ä¸´ç€å¤§å‹å¤šæ ·åŒ–æ•°æ®é›†ç¼ºä¹å’Œæ ‡æ³¨å›°éš¾çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä¸“æ³¨äºåˆ©ç”¨å¤§å‹æ— æ ‡ç­¾çš„äººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ•°æ®é›†æ¥ä¿ƒè¿›åŠç›‘ç£çš„FERã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ— æ³¨é‡Šçš„å¤§è§„æ¨¡é¢éƒ¨å›¾åƒä¸Šè¿›è¡Œé¢éƒ¨é‡å»ºé¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ é¢éƒ¨å‡ ä½•å’Œè¡¨æƒ…åŒºåŸŸçš„ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨æœ‰é™çš„æ ‡ç­¾åœ¨FERæ•°æ®é›†ä¸Šè¿›è¡Œä¸¤é˜¶æ®µå¾®è°ƒã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥ç¼“è§£æœ‰æ ‡ç­¾å’Œå¤šæ ·åŒ–å›¾åƒçš„ç¨€ç¼ºæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMixupçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¸“ä¸ºé¢éƒ¨å›¾åƒå®šåˆ¶ã€‚çœŸå®å’Œè™šæ‹Ÿå›¾åƒçš„æŸå¤±æƒé‡æ˜¯æ ¹æ®ä¸¤å¼ å›¾ä¸­é¢éƒ¨äº¤å¹¶æ¯”ï¼ˆIoUï¼‰æ¥ç¡®å®šçš„ã€‚åœ¨RAF-DBã€AffectNetå’ŒFERPlusä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„åŠç›‘ç£FERæ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä½¿ç”¨5%ã€25%çš„è®­ç»ƒé›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AffectNetä¸Šè¾¾åˆ°äº†64.02%ï¼Œåœ¨RAF-DBä¸Šè¾¾åˆ°äº†88.23%ï¼Œè¿™ä¸å…¨ç›‘ç£çš„å…ˆè¿›æ–¹æ³•ç›¸å½“ã€‚ä»£ç å°†åœ¨[<a target="_blank" rel="noopener" href="https://github.com/zhelishisongjie/SSFER%E4%B8%8A%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%E3%80%82]">https://github.com/zhelishisongjie/SSFERä¸Šå…¬å¼€æä¾›ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17622v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œé¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆFERï¼‰å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨å›°éš¾çš„é—®é¢˜ï¼Œç ”ç©¶é‡ç‚¹åˆ©ç”¨æ— æ ‡ç­¾çš„å¤§è§„æ¨¡äººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ•°æ®é›†ä¿ƒè¿›åŠç›‘ç£FERã€‚å…ˆè¿›è¡Œé¢éƒ¨é‡å»ºé¢„è®­ç»ƒï¼Œå­¦ä¹ é¢éƒ¨å‡ ä½•å’Œè¡¨æƒ…åŒºåŸŸçš„ç‰¹å¾ï¼Œå†è¿›è¡Œæœ‰é™æ ‡ç­¾çš„FERæ•°æ®é›†çš„ä¸¤é˜¶æ®µå¾®è°ƒã€‚æ­¤å¤–ï¼Œæå‡ºåŸºäºMixupçš„é¢éƒ¨å›¾åƒæ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ ¹æ®ä¸¤å¼ å›¾åƒä¸­é¢éƒ¨çš„äº¤é›†æ¯”ï¼ˆIoUï¼‰ç¡®å®šçœŸå®å’Œè™šæ‹Ÿå›¾åƒçš„æŸå¤±æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰åŠç›‘ç£FERæ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚ä»…ä½¿ç”¨5%ã€25%çš„è®­ç»ƒé›†ï¼Œè¯¥æ–¹æ³•åœ¨AffectNetä¸Šè¾¾åˆ°64.02%ï¼Œåœ¨RAF-DBä¸Šè¾¾åˆ°88.23%ï¼Œä¸å…¨ç›‘ç£çš„å…ˆè¿›æ–¹æ³•ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨è¡¨æƒ…è¯†åˆ«ï¼ˆFERï¼‰è¿‘å¹´æ¥å—åˆ°å…³æ³¨ï¼Œä½†ä»é¢ä¸´æ•°æ®é›†å¤§å’Œæ ‡æ³¨å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨æ— æ ‡ç­¾çš„å¤§è§„æ¨¡äººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ•°æ®é›†æ¥ä¿ƒè¿›åŠç›‘ç£FERã€‚</li>
<li>å…ˆè¿›è¡Œé¢éƒ¨é‡å»ºé¢„è®­ç»ƒï¼Œå­¦ä¹ é¢éƒ¨å‡ ä½•å’Œè¡¨æƒ…åŒºåŸŸçš„ç‰¹å¾ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒï¼Œåˆ©ç”¨æœ‰é™æ ‡ç­¾çš„FERæ•°æ®é›†ã€‚</li>
<li>æå‡ºåŸºäºMixupçš„é¢éƒ¨å›¾åƒæ•°æ®å¢å¼ºç­–ç•¥ï¼Œæ ¹æ®é¢éƒ¨äº¤é›†æ¯”ï¼ˆIoUï¼‰ç¡®å®šæŸå¤±æƒé‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–åŠç›‘ç£FERæ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5731c29ef48928934893579bb68c533e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bbb8a80f837e66f9284cffcc87891162.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-239e7e4299065a45789ad55f40811386.jpg" align="middle">
</details>




<h2 id="CemiFace-Center-based-Semi-hard-Synthetic-Face-Generation-for-Face-Recognition"><a href="#CemiFace-Center-based-Semi-hard-Synthetic-Face-Generation-for-Face-Recognition" class="headerlink" title="CemiFace: Center-based Semi-hard Synthetic Face Generation for Face   Recognition"></a>CemiFace: Center-based Semi-hard Synthetic Face Generation for Face   Recognition</h2><p><strong>Authors:Zhonglin Sun, Siyang Song, Ioannis Patras, Georgios Tzimiropoulos</strong></p>
<p>Privacy issue is a main concern in developing face recognition techniques. Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples. In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. Inspired by this, we propose a novel diffusion-based approach (namely Center-based Semi-hard Synthetic Face Generation (CemiFace)) which produces facial samples with various levels of similarity to the subject center, thus allowing to generate face datasets containing effective discriminative samples for training face recognition. Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«æŠ€æœ¯çš„å¼€å‘è¿‡ç¨‹ä¸­ï¼Œéšç§é—®é¢˜æ˜¯ä¸»è¦å…³æ³¨ç‚¹ä¹‹ä¸€ã€‚å°½ç®¡åˆæˆäººè„¸å›¾åƒå¯ä»¥åœ¨ä¿æŒæœ‰æ•ˆçš„äººè„¸è¯†åˆ«ï¼ˆFRï¼‰æ€§èƒ½çš„åŒæ—¶ï¼Œéƒ¨åˆ†ç¼“è§£æ½œåœ¨çš„æ³•å¾‹é£é™©ï¼Œä½†ç”±ç°æœ‰ç”Ÿæˆæ–¹æ³•åˆæˆçš„äººè„¸å›¾åƒè®­ç»ƒFRæ¨¡å‹æ—¶ï¼Œç»å¸¸å‡ºç°æ€§èƒ½ä¸‹é™é—®é¢˜ï¼ŒåŸå› æ˜¯è¿™äº›åˆæˆæ ·æœ¬çš„è¾¨åˆ«èƒ½åŠ›ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å“ªäº›å› ç´ æœ‰åŠ©äºè®­ç»ƒç¨³å›ºçš„äººè„¸è¯†åˆ«æ¨¡å‹ï¼Œå¹¶å‘ç°ä¸èº«ä»½ä¸­å¿ƒæœ‰ä¸€å®šç¨‹åº¦ç›¸ä¼¼æ€§çš„é¢éƒ¨å›¾åƒåœ¨è®­ç»ƒFRæ¨¡å‹çš„æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºæå¤§æœ‰æ•ˆæ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹æ–¹æ³•ï¼ˆç§°ä¸ºåŸºäºä¸­å¿ƒçš„åŠç¡¬åˆæˆäººè„¸ç”Ÿæˆï¼ˆCemiFaceï¼‰ï¼‰ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒç¨‹åº¦ç›¸ä¼¼æ€§çš„é¢éƒ¨æ ·æœ¬ï¼Œä»è€Œèƒ½å¤Ÿç”ŸæˆåŒ…å«æœ‰æ•ˆåˆ¤åˆ«æ ·æœ¬çš„äººè„¸æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒäººè„¸è¯†åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é€‚åº¦çš„ç›¸ä¼¼æ€§ä¸‹ï¼Œåœ¨ç”Ÿæˆçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥äº§ç”Ÿä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18876v2">PDF</a> accepted to NeurIPS 2024. Camera-ready version</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºéšç§æ‹…å¿§ï¼Œæœ¬æ–‡æ¢ç´¢äº†å¦‚ä½•é€šè¿‡åˆæˆé¢éƒ¨å›¾åƒæŠ€æœ¯è¿›è¡Œäººè„¸è¯†åˆ«ï¼Œé™ä½æ³•å¾‹é£é™©çš„åŒæ—¶ä¿è¯è‰¯å¥½çš„è¯†åˆ«æ€§èƒ½ã€‚å½“å‰é‡‡ç”¨ç°æœ‰ç”ŸæˆæŠ€æœ¯ç”Ÿæˆçš„é¢éƒ¨å›¾åƒè´¨é‡éš¾ä»¥é‰´åˆ«ï¼Œå¯¼è‡´äººè„¸è¯†åˆ«æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚æœ¬ç ”ç©¶å‘ç°ä¸èº«ä»½ä¸­å¿ƒç›¸ä¼¼åº¦è¾ƒé«˜çš„é¢éƒ¨å›¾åƒå¯¹äººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒè‡³å…³é‡è¦ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹åˆæˆé¢éƒ¨å›¾åƒæ–¹æ³•â€”â€”ä¸­å¿ƒåŒ–åŠç¡¬åˆæˆé¢éƒ¨ç”Ÿæˆï¼ˆCemiFaceï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸åŒç›¸ä¼¼åº¦æ°´å¹³çš„é¢éƒ¨æ ·æœ¬ï¼Œä»è€Œæ„å»ºåŒ…å«æœ‰æ•ˆé‰´åˆ«æ ·æœ¬çš„è®­ç»ƒæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨é€‚åº¦ç›¸ä¼¼åº¦ä¸‹ï¼Œä½¿ç”¨CemiFaceç”Ÿæˆçš„è®­ç»ƒæ•°æ®é›†è®­ç»ƒçš„äººè„¸è¯†åˆ«æ¨¡å‹æ€§èƒ½è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆæˆé¢éƒ¨å›¾åƒæŠ€æœ¯å¯ä»¥ç¼“è§£äººè„¸è¯†åˆ«æŠ€æœ¯ä¸­çš„éšç§é—®é¢˜å’Œæ³•å¾‹é£é™©ã€‚</li>
<li>å½“å‰äººè„¸è¯†åˆ«æ¨¡å‹å› ç”Ÿæˆæ ·æœ¬çš„é‰´åˆ«èƒ½åŠ›ä¸è¶³è€Œé¢ä¸´æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>ä¸èº«ä»½ä¸­å¿ƒæœ‰ä¸€å®šç›¸ä¼¼åº¦çš„é¢éƒ¨å›¾åƒå¯¹äººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒè‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºçš„CemiFaceæ–¹æ³•åŸºäºæ‰©æ•£æŠ€æœ¯ç”Ÿæˆé¢éƒ¨æ ·æœ¬ï¼Œå…·å¤‡ä¸åŒå±‚æ¬¡çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>CemiFaceæ–¹æ³•ç”Ÿæˆçš„è®­ç»ƒæ•°æ®é›†èƒ½æœ‰æ•ˆæå‡äººè„¸è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>åœ¨é€‚åº¦ç›¸ä¼¼åº¦æ¡ä»¶ä¸‹ï¼ŒCemiFaceç”Ÿæˆçš„æ•°æ®é›†è®­ç»ƒæ¨¡å‹è¡¨ç°å¯ä¸ä¼ ç»Ÿæ–¹æ³•ç«äº‰ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºäººè„¸è¯†åˆ«æŠ€æœ¯å¸¦æ¥äº†æ–°çš„åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•å’Œæ€è€ƒæ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c853b601b45092ac9bb0dd775fc3aa3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b0be9fbb769486a7fb63a5576f878c6f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5aa0348a833a2d624580861c2ba35d55.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-fbcfb37df51f8c6bb4ce008a2bb45f89.jpg" align="middle">
</details>




<h2 id="ID-3-Identity-Preserving-yet-Diversified-Diffusion-Models-for-Synthetic-Face-Recognition"><a href="#ID-3-Identity-Preserving-yet-Diversified-Diffusion-Models-for-Synthetic-Face-Recognition" class="headerlink" title="ID$^3$: Identity-Preserving-yet-Diversified Diffusion Models for   Synthetic Face Recognition"></a>ID$^3$: Identity-Preserving-yet-Diversified Diffusion Models for   Synthetic Face Recognition</h2><p><strong>Authors:Shen Li, Jianqing Xu, Jiaying Wu, Miao Xiong, Ailin Deng, Jiazhen Ji, Yuge Huang, Wenjie Feng, Shouhong Ding, Bryan Hooi</strong></p>
<p>Synthetic face recognition (SFR) aims to generate synthetic face datasets that mimic the distribution of real face data, which allows for training face recognition models in a privacy-preserving manner. Despite the remarkable potential of diffusion models in image generation, current diffusion-based SFR models struggle with generalization to real-world faces. To address this limitation, we outline three key objectives for SFR: (1) promoting diversity across identities (inter-class diversity), (2) ensuring diversity within each identity by injecting various facial attributes (intra-class diversity), and (3) maintaining identity consistency within each identity group (intra-class identity preservation). Inspired by these goals, we introduce a diffusion-fueled SFR model termed $\text{ID}^3$. $\text{ID}^3$ employs an ID-preserving loss to generate diverse yet identity-consistent facial appearances. Theoretically, we show that minimizing this loss is equivalent to maximizing the lower bound of an adjusted conditional log-likelihood over ID-preserving data. This equivalence motivates an ID-preserving sampling algorithm, which operates over an adjusted gradient vector field, enabling the generation of fake face recognition datasets that approximate the distribution of real-world faces. Extensive experiments across five challenging benchmarks validate the advantages of $\text{ID}^3$. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«åˆæˆï¼ˆSFRï¼‰æ—¨åœ¨ç”Ÿæˆæ¨¡æ‹ŸçœŸå®äººè„¸æ•°æ®åˆ†å¸ƒçš„äººè„¸åˆæˆæ•°æ®é›†ï¼Œä»è€Œä»¥éšç§ä¿æŠ¤çš„æ–¹å¼è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ï¼Œä½†å½“å‰çš„åŸºäºæ‰©æ•£çš„SFRæ¨¡å‹åœ¨æ¨å¹¿åˆ°ç°å®ä¸–ç•Œäººè„¸æ—¶ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸ºSFRæ¦‚è¿°äº†ä¸‰ä¸ªå…³é”®ç›®æ ‡ï¼šï¼ˆ1ï¼‰ä¿ƒè¿›èº«ä»½é—´çš„å¤šæ ·æ€§ï¼ˆç±»é—´å¤šæ ·æ€§ï¼‰ï¼Œï¼ˆ2ï¼‰é€šè¿‡æ³¨å…¥å„ç§é¢éƒ¨ç‰¹å¾ç¡®ä¿æ¯ä¸ªèº«ä»½å†…çš„å¤šæ ·æ€§ï¼ˆç±»å†…å¤šæ ·æ€§ï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰ä¿æŒæ¯ä¸ªèº«ä»½ç»„å†…èº«ä»½çš„ä¸€è‡´æ€§ï¼ˆç±»å†…èº«ä»½ä¿ç•™ï¼‰ã€‚å—è¿™äº›ç›®æ ‡çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”±æ‰©æ•£é©±åŠ¨çš„SFRæ¨¡å‹ï¼Œç§°ä¸ºIDÂ³ã€‚IDÂ³é‡‡ç”¨äº†ä¸€ç§ä¿èº«ä»½æŸå¤±çš„æ–¹æ³•ï¼Œä»¥ç”Ÿæˆå¤šæ ·ä¸”èº«ä»½ä¸€è‡´çš„é¢éƒ¨å¤–è§‚ã€‚ä»ç†è®ºä¸Šè®²ï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€å°åŒ–è¿™ç§æŸå¤±ç­‰åŒäºæœ€å¤§åŒ–ä¿èº«ä»½æ•°æ®è°ƒæ•´æ¡ä»¶å¯¹æ•°ä¼¼ç„¶çš„ä¸‹ç•Œã€‚è¿™ç§ç­‰ä»·æ€§æ¿€å‘äº†ä¸€ç§ä¿èº«ä»½çš„é‡‡æ ·ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è°ƒæ•´åçš„æ¢¯åº¦çŸ¢é‡åœºä¸Šè¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿç”Ÿæˆæ¨¡æ‹Ÿç°å®ä¸–ç•Œäººè„¸åˆ†å¸ƒçš„å‡å†’äººè„¸è¯†åˆ«æ•°æ®é›†ã€‚åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒéªŒè¯äº†IDÂ³çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17576v2">PDF</a> Accepted to NeurIPS 2024</p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«é¢†åŸŸä¸­ï¼Œåˆæˆäººè„¸è¯†åˆ«ï¼ˆSFRï¼‰æ—¨åœ¨ç”Ÿæˆæ¨¡æ‹ŸçœŸå®äººè„¸æ•°æ®åˆ†å¸ƒçš„äººè„¸æ•°æ®é›†ï¼Œå®ç°éšç§ä¿æŠ¤ä¸‹çš„äººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒã€‚å½“å‰åŸºäºæ‰©æ•£æ¨¡å‹çš„SFRå­˜åœ¨æ³›åŒ–åˆ°çœŸå®ä¸–ç•Œäººè„¸çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸‰ä¸ªå…³é”®ç›®æ ‡ï¼šä¿ƒè¿›èº«ä»½é—´çš„å¤šæ ·æ€§ã€ç¡®ä¿åŒä¸€èº«ä»½å†…ä¸åŒé¢éƒ¨ç‰¹å¾çš„å¤šæ ·æ€§å’Œç»´æŒåŒä¸€èº«ä»½ç»„å†…çš„èº«ä»½ä¸€è‡´æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºIDÂ³çš„æ‰©æ•£é©±åŠ¨çš„SFRæ¨¡å‹ã€‚IDÂ³é‡‡ç”¨èº«ä»½ä¿æŒæŸå¤±æ¥ç”Ÿæˆå¤šæ ·ä¸”èº«ä»½ä¸€è‡´çš„é¢éƒ¨å¤–è§‚ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€å°åŒ–æ­¤æŸå¤±ç­‰åŒäºæœ€å¤§åŒ–è°ƒæ•´åçš„èº«ä»½ä¿æŒæ•°æ®çš„æ¡ä»¶å¯¹æ•°ä¼¼ç„¶çš„ä¸‹ç•Œã€‚è¿™æ¨åŠ¨äº†èº«ä»½ä¿æŒé‡‡æ ·ç®—æ³•çš„å‘å±•ï¼Œè¯¥ç®—æ³•åœ¨è°ƒæ•´åçš„æ¢¯åº¦çŸ¢é‡åœºä¸Šè¿›è¡Œæ“ä½œï¼Œèƒ½å¤Ÿç”Ÿæˆæ¨¡æ‹ŸçœŸå®ä¸–ç•Œäººè„¸åˆ†å¸ƒçš„è™šå‡äººè„¸è¯†åˆ«æ•°æ®é›†ã€‚åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒéªŒè¯äº†IDÂ³çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆäººè„¸è¯†åˆ«ï¼ˆSFRï¼‰å…³æ³¨ç”Ÿæˆæ¨¡æ‹ŸçœŸå®äººè„¸æ•°æ®åˆ†å¸ƒçš„æ•°æ®é›†ï¼Œå®ç°éšç§ä¿æŠ¤ä¸‹çš„äººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹åœ¨SFRä¸­å­˜åœ¨æ³›åŒ–é—®é¢˜ï¼Œéœ€è¦ä¿ƒè¿›èº«ä»½é—´çš„å¤šæ ·æ€§ã€ç¡®ä¿åŒä¸€èº«ä»½å†…çš„å¤šæ ·æ€§ä»¥åŠç»´æŒèº«ä»½ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥åä¸ºIDÂ³çš„æ‰©æ•£é©±åŠ¨çš„SFRæ¨¡å‹ï¼Œé‡‡ç”¨èº«ä»½ä¿æŒæŸå¤±ç”Ÿæˆå¤šæ ·ä¸”ä¸€è‡´çš„é¢éƒ¨å¤–è§‚ã€‚</li>
<li>æœ€å°åŒ–èº«ä»½ä¿æŒæŸå¤±ç­‰åŒäºæœ€å¤§åŒ–è°ƒæ•´åçš„æ¡ä»¶å¯¹æ•°ä¼¼ç„¶çš„ä¸‹ç•Œã€‚</li>
<li>å¼€å‘äº†èº«ä»½ä¿æŒé‡‡æ ·ç®—æ³•ï¼Œåœ¨è°ƒæ•´åçš„æ¢¯åº¦çŸ¢é‡åœºä¸Šæ“ä½œï¼Œç”Ÿæˆæ¨¡æ‹ŸçœŸå®ä¸–ç•Œäººè„¸åˆ†å¸ƒçš„è™šå‡æ•°æ®é›†ã€‚</li>
<li>IDÂ³æ¨¡å‹åœ¨äº”ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-828a30b9d3abb939f3e554ec7d5ba509.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a19af205d709f97d57a7df4cb85e2302.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-054baacb982fc5e53f3dc63776e2fb4f.jpg" align="middle">
</details>




<h2 id="From-Pixels-to-Words-Leveraging-Explainability-in-Face-Recognition-through-Interactive-Natural-Language-Processing"><a href="#From-Pixels-to-Words-Leveraging-Explainability-in-Face-Recognition-through-Interactive-Natural-Language-Processing" class="headerlink" title="From Pixels to Words: Leveraging Explainability in Face Recognition   through Interactive Natural Language Processing"></a>From Pixels to Words: Leveraging Explainability in Face Recognition   through Interactive Natural Language Processing</h2><p><strong>Authors:Ivan DeAndres-Tame, Muhammad Faisal, Ruben Tolosana, Rouqaiah Al-Refai, Ruben Vera-Rodriguez, Philipp TerhÃ¶rst</strong></p>
<p>Face Recognition (FR) has advanced significantly with the development of deep learning, achieving high accuracy in several applications. However, the lack of interpretability of these systems raises concerns about their accountability, fairness, and reliability. In the present study, we propose an interactive framework to enhance the explainability of FR models by combining model-agnostic Explainable Artificial Intelligence (XAI) and Natural Language Processing (NLP) techniques. The proposed framework is able to accurately answer various questions of the user through an interactive chatbot. In particular, the explanations generated by our proposed method are in the form of natural language text and visual representations, which for example can describe how different facial regions contribute to the similarity measure between two faces. This is achieved through the automatic analysis of the outputâ€™s saliency heatmaps of the face images and a BERT question-answering model, providing users with an interface that facilitates a comprehensive understanding of the FR decisions. The proposed approach is interactive, allowing the users to ask questions to get more precise information based on the userâ€™s background knowledge. More importantly, in contrast to previous studies, our solution does not decrease the face recognition performance. We demonstrate the effectiveness of the method through different experiments, highlighting its potential to make FR systems more interpretable and user-friendly, especially in sensitive applications where decision-making transparency is crucial. </p>
<blockquote>
<p>éšç€æ·±åº¦å­¦ä¹ çš„ä¸æ–­å‘å±•ï¼Œäººè„¸è¯†åˆ«ï¼ˆFRï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œåœ¨å¤šä¸ªåº”ç”¨ä¸­å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¼•å‘äº†äººä»¬å¯¹å…¶è´£ä»»ã€å…¬å¹³å’Œå¯é æ€§çš„å…³æ³¨ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§äº¤äº’å¼æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨¡å‹æ— å…³çš„å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ï¼Œå¢å¼ºäººè„¸è¯†åˆ«æ¨¡å‹çš„è§£é‡Šæ€§ã€‚æ‰€æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿé€šè¿‡äº¤äº’å¼èŠå¤©æœºå™¨äººå‡†ç¡®å›ç­”ç”¨æˆ·çš„å„ç§é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œç”±æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç”Ÿæˆçš„è§£é‡Šä»¥è‡ªç„¶è¯­è¨€æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºçš„å½¢å¼å‡ºç°ï¼Œä¾‹å¦‚ï¼Œå¯ä»¥æè¿°ä¸åŒé¢éƒ¨åŒºåŸŸå¦‚ä½•è´¡çŒ®äºä¸¤å¼ è„¸ä¹‹é—´çš„ç›¸ä¼¼åº¦æµ‹é‡ã€‚è¿™æ˜¯é€šè¿‡è‡ªåŠ¨åˆ†æé¢éƒ¨å›¾åƒè¾“å‡ºçš„æ˜¾è‘—æ€§çƒ­å›¾å’Œä½¿ç”¨BERTé—®ç­”æ¨¡å‹å®ç°çš„ï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ç§ç•Œé¢ï¼Œä¿ƒè¿›å¯¹äººè„¸è¯†åˆ«å†³ç­–çš„å…¨é¢ç†è§£ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¯äº¤äº’å¼çš„ï¼Œå…è®¸ç”¨æˆ·æå‡ºé—®é¢˜ä»¥è·å¾—åŸºäºç”¨æˆ·èƒŒæ™¯çŸ¥è¯†çš„æ›´ç²¾ç¡®ä¿¡æ¯ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä¸ä»¥å‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¹¶ä¸ä¼šé™ä½äººè„¸è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä¸åŒçš„å®éªŒè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨ä½¿äººè„¸è¯†åˆ«ç³»ç»Ÿæ›´å…·è§£é‡Šæ€§å’Œç”¨æˆ·å‹å¥½æ€§æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å†³ç­–é€æ˜æ€§è‡³å…³é‡è¦çš„æ•æ„Ÿåº”ç”¨ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.16089v2">PDF</a> </p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«æŠ€æœ¯ç»“åˆæ·±åº¦å­¦ä¹ å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç¼ºä¹è§£é‡Šæ€§å¼•å‘å…³æ³¨ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§äº¤äº’å¼æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆæ¨¡å‹æ— å…³çš„XAIå’ŒNLPæŠ€æœ¯å¢å¼ºäººè„¸è¯†åˆ«æ¨¡å‹çš„è§£é‡Šæ€§ã€‚æ¡†æ¶å¯å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä»¥è‡ªç„¶è¯­è¨€æ–‡æœ¬å’Œè§†è§‰å½¢å¼è§£é‡Šé¢éƒ¨è¯†åˆ«å†³ç­–è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä¸åŒé¢éƒ¨åŒºåŸŸå¯¹ç›¸ä¼¼åº¦åº¦é‡çš„è´¡çŒ®ç­‰ã€‚æ–¹æ³•äº’åŠ¨æ€§å¼ºï¼Œå¯æ ¹æ®ç”¨æˆ·èƒŒæ™¯çŸ¥è¯†æä¾›ç²¾ç¡®ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸é™ä½äººè„¸è¯†åˆ«æ€§èƒ½ï¼Œå…·æœ‰æ½œåŠ›ä½¿FRç³»ç»Ÿæ›´å¯è§£é‡Šå’Œç”¨æˆ·å‹å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«æŠ€æœ¯ç»“åˆæ·±åº¦å­¦ä¹ å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç¼ºä¹è§£é‡Šæ€§çš„äººè„¸è¯†åˆ«ç³»ç»Ÿå¼•å‘å¯¹è´£ä»»æ€§ã€å…¬å¹³æ€§å’Œå¯é æ€§çš„å…³æ³¨ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§äº¤äº’å¼æ¡†æ¶ï¼Œå¢å¼ºäººè„¸è¯†åˆ«æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>æ¡†æ¶ç»“åˆæ¨¡å‹æ— å…³çš„XAIå’ŒNLPæŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶å¯å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä»¥è‡ªç„¶è¯­è¨€æ–‡æœ¬å’Œè§†è§‰å½¢å¼æä¾›è§£é‡Šã€‚</li>
<li>æ–¹æ³•å…·æœ‰å¼ºå¤§çš„äº’åŠ¨æ€§ï¼Œå¯æ ¹æ®ç”¨æˆ·èƒŒæ™¯çŸ¥è¯†æä¾›ç²¾ç¡®ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•ä¸é™ä½äººè„¸è¯†åˆ«æ€§èƒ½ï¼Œæœ‰åŠ©äºä½¿FRç³»ç»Ÿæ›´å¯è§£é‡Šå’Œç”¨æˆ·å‹å¥½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-77fee304413d6cefdcbdd9268671381d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f4d6758b55c7ed4d233554ec273b3607.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-33b8700da8f24d9220b072a087c60843.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2cabbdf916cae1e0fedba07c4efc36e6.jpg" align="middle">
</details>




<h2 id="Deep-Learning-Technology-for-Face-Forgery-Detection-A-Survey"><a href="#Deep-Learning-Technology-for-Face-Forgery-Detection-A-Survey" class="headerlink" title="Deep Learning Technology for Face Forgery Detection: A Survey"></a>Deep Learning Technology for Face Forgery Detection: A Survey</h2><p><strong>Authors:Lixia Ma, Puning Yang, Yuting Xu, Ziming Yang, Peipei Li, Huaibo Huang</strong></p>
<p>Currently, the rapid development of computer vision and deep learning has enabled the creation or manipulation of high-fidelity facial images and videos via deep generative approaches. This technology, also known as deepfake, has achieved dramatic progress and become increasingly popular in social media. However, the technology can generate threats to personal privacy and national security by spreading misinformation. To diminish the risks of deepfake, it is desirable to develop powerful forgery detection methods to distinguish fake faces from real faces. This paper presents a comprehensive survey of recent deep learning-based approaches for facial forgery detection. We attempt to provide the reader with a deeper understanding of the current advances as well as the major challenges for deepfake detection based on deep learning. We present an overview of deepfake techniques and analyse the characteristics of various deepfake datasets. We then provide a systematic review of different categories of deepfake detection and state-of-the-art deepfake detection methods. The drawbacks of existing detection methods are analyzed, and future research directions are discussed to address the challenges in improving both the performance and generalization of deepfake detection. </p>
<blockquote>
<p>å½“å‰ï¼Œè®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ çš„å¿«é€Ÿå‘å±•ä½¿å¾—é€šè¿‡æ·±åº¦ç”Ÿæˆæ–¹æ³•åˆ›å»ºæˆ–æ“ä½œé«˜ä¿çœŸé¢éƒ¨å›¾åƒå’Œè§†é¢‘æˆä¸ºå¯èƒ½ã€‚è¿™é¡¹æŠ€æœ¯ä¹Ÿè¢«ç§°ä¸ºæ·±åº¦ä¼ªé€ æŠ€æœ¯ï¼Œå·²ç»å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œå¹¶åœ¨ç¤¾äº¤åª’ä½“ä¸Šè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œè¯¥æŠ€æœ¯å¯ä»¥é€šè¿‡ä¼ æ’­é”™è¯¯ä¿¡æ¯å¯¹ä¸ªäººéšç§å’Œå›½å®¶å®‰å…¨æ„æˆå¨èƒã€‚ä¸ºäº†å‡å°‘æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„é£é™©ï¼Œå¼€å‘å¼ºå¤§çš„ä¼ªé€ æ£€æµ‹æ–¹æ³•æ¥åŒºåˆ†çœŸå‡é¢å­”æ˜¯éå¸¸å¿…è¦çš„ã€‚æœ¬æ–‡å¯¹åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨ä¼ªé€ æ£€æµ‹æœ€æ–°æ–¹æ³•è¿›è¡Œäº†å…¨é¢ç»¼è¿°ã€‚æˆ‘ä»¬è¯•å›¾ä¸ºè¯»è€…æä¾›å¯¹å½“å‰è¿›å±•ä»¥åŠåŸºäºæ·±åº¦å­¦ä¹ è¿›è¡Œæ·±åº¦ä¼ªé€ æ£€æµ‹çš„ä¸»è¦æŒ‘æˆ˜çš„æ›´æ·±å…¥ç†è§£ã€‚æœ¬æ–‡ä»‹ç»äº†æ·±åº¦ä¼ªé€ æŠ€æœ¯æ¦‚å†µï¼Œåˆ†æäº†å„ç§æ·±åº¦ä¼ªé€ æ•°æ®é›†çš„ç‰¹ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ä¸åŒç±»å‹çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•å’Œæœ€æ–°æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿå›é¡¾ã€‚åˆ†æäº†ç°æœ‰æ£€æµ‹æ–¹æ³•çš„ä¸è¶³ï¼Œå¹¶è®¨è®ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä»¥è§£å†³æé«˜æ·±åº¦ä¼ªé€ æ£€æµ‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14289v3">PDF</a> The paper â€œDeep Learning Technology for Face Forgery Detection: A   Surveyâ€ is hereby formally withdrawn. The reason for this withdrawal is that   I did not adequately consult and obtain proper authorization from the   corresponding author during the submission process. I sincerely apologize for   any inconvenience this may have caused the journal, reviewers, and readers</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨ä¼ªé€ æ£€æµ‹æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œä»‹ç»äº†æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„ç‰¹ç‚¹ä»¥åŠå½“å‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æ¦‚è¿°äº†æ·±åº¦ä¼ªé€ æŠ€æœ¯ï¼Œåˆ†æäº†ä¸åŒæ·±åº¦ä¼ªé€ æ•°æ®é›†çš„ç‰¹ç‚¹ï¼Œå¹¶å¯¹ç°æœ‰çš„æ£€æµ‹æ–¹æ³•çš„ç¼ºç‚¹è¿›è¡Œäº†è®¨è®ºï¼ŒåŒæ—¶æå‡ºäº†æ”¹è¿›æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œå·²æˆä¸ºç¤¾äº¤åª’ä½“ä¸­çš„çƒ­é—¨æŠ€æœ¯ã€‚</li>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯å¯¹ä¸ªäººéšç§å’Œå›½å®¶å®‰å…¨æ„æˆå¨èƒï¼Œéœ€è¦å¼€å‘å¼ºå¤§çš„ä¼ªé€ æ£€æµ‹æ–¹æ³•æ¥åŒºåˆ†çœŸå®å’Œä¼ªé€ çš„é¢å­”ã€‚</li>
<li>å½“å‰å­˜åœ¨å¤šç§åŸºäºæ·±åº¦å­¦ä¹ çš„é¢éƒ¨ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚æ€§èƒ½æå‡å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„ç‰¹ç‚¹ï¼Œå¹¶åˆ†æäº†ä¸åŒæ·±åº¦ä¼ªé€ æ•°æ®é›†çš„ç‰¹æ€§ã€‚</li>
<li>æ–‡ç« å¯¹ç°æœ‰çš„æ£€æµ‹æ–¹æ³•è¿›è¡Œç³»ç»Ÿè¯„ä»·ï¼Œå¹¶æŒ‡å‡ºäº†å®ƒä»¬çš„ç¼ºç‚¹ã€‚</li>
<li>ä¸ºäº†æé«˜æ£€æµ‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•æ–°çš„æ£€æµ‹æ–¹æ³•å’Œç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e52002fd683473d602bb6a8c8630bcfe.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da897884bd921b4beb46758b50e7e621.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2872c343a45ce2d0c5f3412de4108103.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7002aee162169cb90ba3321dc96596e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f587aba358e0189173f11faa73f2a17.jpg" align="middle">
</details>




<h2 id="VIGFace-Virtual-Identity-Generation-for-Privacy-Free-Face-Recognition"><a href="#VIGFace-Virtual-Identity-Generation-for-Privacy-Free-Face-Recognition" class="headerlink" title="VIGFace: Virtual Identity Generation for Privacy-Free Face Recognition"></a>VIGFace: Virtual Identity Generation for Privacy-Free Face Recognition</h2><p><strong>Authors:Minsoo Kim, Min-Cheol Sagong, Gi Pyo Nam, Junghyun Cho, Ig-Jae Kim</strong></p>
<p>Deep learning-based face recognition continues to face challenges due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns. To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images. Our idea originates from pre-assigning virtual identities in the feature space. Initially, we train the face recognition model using a real face dataset and create a feature space for both real and virtual identities, where virtual prototypes are orthogonal to other prototypes. Subsequently, we generate synthetic images by using the diffusion model based on the feature space. The diffusion model is capable of reconstructing authentic human facial representations from established real prototypes, while synthesizing virtual entities from devised virtual prototypes. Our proposed framework provides two significant benefits. Firstly, it allows one to create virtual facial images without concerns about privacy and portrait rights, guaranteeing that the generated virtual face images are clearly differentiated from existing individuals. Secondly, it serves as an effective augmentation method by incorporating real existing images. Further experiments demonstrate the superiority of our virtual face dataset and framework, outperforming the previous state-of-the-art on various face recognition benchmarks. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„äººè„¸è¯†åˆ«ä»ç„¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå…¶ä¾èµ–äºä»ç½‘ç»œçˆ¬è™«è·å¾—çš„å¤§é‡æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†çš„æ”¶é›†æˆæœ¬é«˜æ˜‚ï¼Œå¹¶å¼•å‘äº†ç°å®ä¸–ç•Œä¸­çš„éšç§æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VIGFaceè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆåˆæˆé¢éƒ¨å›¾åƒã€‚æˆ‘ä»¬çš„çµæ„Ÿæ¥æºäºåœ¨ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œè™šæ‹Ÿèº«ä»½çš„é¢„å…ˆåˆ†é…ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨çœŸå®é¢éƒ¨æ•°æ®é›†è®­ç»ƒäººè„¸è¯†åˆ«æ¨¡å‹ï¼Œå¹¶ä¸ºçœŸå®å’Œè™šæ‹Ÿèº«ä»½åˆ›å»ºç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­è™šæ‹ŸåŸå‹ä¸å…¶ä»–åŸå‹æ­£äº¤ã€‚éšåï¼Œæˆ‘ä»¬åŸºäºç‰¹å¾ç©ºé—´ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒã€‚æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»æ—¢å®šçš„çœŸå®åŸå‹é‡å»ºå‡ºçœŸå®çš„äººè„¸è¡¨ç¤ºï¼ŒåŒæ—¶æ ¹æ®è®¾å®šçš„è™šæ‹ŸåŸå‹åˆæˆè™šæ‹Ÿå®ä½“ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶æä¾›äº†ä¸¤å¤§é‡è¦ä¼˜åŠ¿ã€‚é¦–å…ˆï¼Œå®ƒä½¿äººä»¬èƒ½å¤Ÿåˆ›å»ºæ— éœ€æ‹…å¿ƒéšç§å’Œè‚–åƒæƒçš„è™šæ‹Ÿé¢éƒ¨å›¾åƒï¼Œä¿è¯ç”Ÿæˆçš„è™šæ‹Ÿé¢éƒ¨å›¾åƒä¸ç°æœ‰ä¸ªä½“æ˜ç¡®åŒºåˆ†å¼€æ¥ã€‚å…¶æ¬¡ï¼Œå®ƒä½œä¸ºä¸€ç§æœ‰æ•ˆçš„å¢å¼ºæ–¹æ³•ï¼Œç»“åˆäº†ç°æœ‰çš„çœŸå®å›¾åƒã€‚è¿›ä¸€æ­¥çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è™šæ‹Ÿé¢éƒ¨æ•°æ®é›†å’Œæ¡†æ¶ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨å„ç§äººè„¸è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.08277v2">PDF</a> </p>
<p><strong>Summary</strong><br>äººè„¸è¯†åˆ«é¢†åŸŸé¢ä¸´æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚å’Œéšç§é—®é¢˜çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºVIGFaceæ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆåˆæˆé¢éƒ¨å›¾åƒæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æ¡†æ¶åˆ©ç”¨ç‰¹å¾ç©ºé—´é¢„åˆ†é…è™šæ‹Ÿèº«ä»½çš„æ–¹å¼ï¼Œè®­ç»ƒæ¨¡å‹å¹¶åˆ›å»ºåŒ…å«çœŸå®å’Œè™šæ‹Ÿèº«ä»½çš„ç‰¹å¾ç©ºé—´ã€‚åˆ©ç”¨æ‰©æ•£æ¨¡å‹åŸºäºç‰¹å¾ç©ºé—´ç”Ÿæˆåˆæˆå›¾åƒï¼Œèƒ½å¤Ÿé‡å»ºçœŸå®äººè„¸è¡¨ç¤ºå¹¶ä»è®¾å®šçš„è™šæ‹ŸåŸå‹ä¸­åˆæˆè™šæ‹Ÿå®ä½“ã€‚æ­¤æ¡†æ¶ä¸ä»…è§£å†³äº†éšç§å’Œè‚–åƒæƒé—®é¢˜ï¼Œè¿˜ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„è™šæ‹Ÿé¢éƒ¨æ•°æ®é›†å’Œæ¡†æ¶ä¼˜äºå½“å‰æœ€ä½³æ°´å¹³çš„äººè„¸è¯†åˆ«åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®é‡‡é›†çš„é«˜æˆæœ¬å’Œå¯¹éšç§çš„æ‹…å¿§ã€‚</li>
<li>VIGFaceæ¡†æ¶é€šè¿‡ç”Ÿæˆåˆæˆé¢éƒ¨å›¾åƒæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨ç‰¹å¾ç©ºé—´é¢„åˆ†é…è™šæ‹Ÿèº«ä»½æ¥è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºåŸºäºç‰¹å¾ç©ºé—´ç”Ÿæˆåˆæˆå›¾åƒã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿé‡å»ºçœŸå®äººè„¸è¡¨ç¤ºå¹¶åˆæˆè™šæ‹Ÿå®ä½“ã€‚</li>
<li>æ¡†æ¶è§£å†³äº†éšç§å’Œè‚–åƒæƒé—®é¢˜ï¼Œå¹¶ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-334749fa558afae484297c03f9faddbb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-db39ee9b0ef7d8ba40ce26a923ef9094.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dd6aff49ff6265f120adbb186d79b58f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13a0cd8026fbacf1beb3cfe14d08cbd4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-842d192440084b607ec25d551ffd9ab9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-eb7230c5dfecc17b49b2c4c00ecad9b3.jpg" align="middle">
</details>




<h2 id="Facial-recognition-technology-and-human-raters-can-predict-political-orientation-from-images-of-expressionless-faces-even-when-controlling-for-demographics-and-self-presentation"><a href="#Facial-recognition-technology-and-human-raters-can-predict-political-orientation-from-images-of-expressionless-faces-even-when-controlling-for-demographics-and-self-presentation" class="headerlink" title="Facial recognition technology and human raters can predict political   orientation from images of expressionless faces even when controlling for   demographics and self-presentation"></a>Facial recognition technology and human raters can predict political   orientation from images of expressionless faces even when controlling for   demographics and self-presentation</h2><p><strong>Authors:Michal Kosinski, Poruz Khambatta, Yilun Wang</strong></p>
<p>Carefully standardized facial images of 591 participants were taken in the laboratory, while controlling for self-presentation, facial expression, head orientation, and image properties. They were presented to human raters and a facial recognition algorithm: both humans (r&#x3D;.21) and the algorithm (r&#x3D;.22) could predict participantsâ€™ scores on a political orientation scale (Cronbachâ€™s alpha&#x3D;.94) decorrelated with age, gender, and ethnicity. These effects are on par with how well job interviews predict job success, or alcohol drives aggressiveness. Algorithmâ€™s predictive accuracy was even higher (r&#x3D;.31) when it leveraged information on participantsâ€™ age, gender, and ethnicity. Moreover, the associations between facial appearance and political orientation seem to generalize beyond our sample: The predictive model derived from standardized images (while controlling for age, gender, and ethnicity) could predict political orientation (r&#x3D;.13) from naturalistic images of 3,401 politicians from the U.S., UK, and Canada. The analysis of facial features associated with political orientation revealed that conservatives tended to have larger lower faces. The predictability of political orientation from standardized images has critical implications for privacy, the regulation of facial recognition technology, and understanding the origins and consequences of political orientation. </p>
<blockquote>
<p>åœ¨å®éªŒå®¤é‡Œï¼Œæˆ‘ä»¬å¯¹591åå‚ä¸è€…çš„é¢éƒ¨å›¾åƒè¿›è¡Œäº†ç»†è‡´çš„æ ‡å‡†åŒ–é‡‡é›†ï¼ŒåŒæ—¶æ§åˆ¶äº†è‡ªæˆ‘å‘ˆç°ã€é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨æœå‘å’Œå›¾åƒå±æ€§ã€‚è¿™äº›å›¾åƒè¢«å±•ç¤ºç»™äººç±»è¯„åˆ†è€…å’Œä¸€ä¸ªé¢éƒ¨è¯†åˆ«ç®—æ³•ï¼šäººç±»ï¼ˆr&#x3D;.21ï¼‰å’Œç®—æ³•ï¼ˆr&#x3D;.22ï¼‰éƒ½èƒ½é¢„æµ‹å‚ä¸è€…åœ¨æ”¿æ²»å€¾å‘é‡è¡¨ä¸Šçš„å¾—åˆ†ï¼ˆCronbachçš„alpha&#x3D;.94ï¼‰ï¼Œå¹¶ä¸”ä¸å¹´é¾„ã€æ€§åˆ«å’Œç§æ—æ— å…³ã€‚è¿™äº›æ•ˆæœä¸é¢è¯•é¢„æµ‹å·¥ä½œæˆåŠŸçš„ç¨‹åº¦ï¼Œæˆ–é…’ç²¾å¼•å‘æ”»å‡»è¡Œä¸ºçš„ç¨‹åº¦ç›¸å½“ã€‚å½“ç®—æ³•åˆ©ç”¨å‚ä¸è€…çš„å¹´é¾„ã€æ€§åˆ«å’Œç§æ—ä¿¡æ¯æ—¶ï¼Œå…¶é¢„æµ‹ç²¾åº¦ç”šè‡³æ›´é«˜ï¼ˆr&#x3D;.31ï¼‰ã€‚æ­¤å¤–ï¼Œé¢éƒ¨å¤–è§‚ä¸æ”¿æ²»å€¾å‘ä¹‹é—´çš„è”ç³»ä¼¼ä¹å¯ä»¥æ¨å¹¿åˆ°æˆ‘ä»¬çš„æ ·æœ¬ä¹‹å¤–ï¼šä»æ ‡å‡†åŒ–å›¾åƒä¸­å¾—å‡ºçš„é¢„æµ‹æ¨¡å‹ï¼ˆåœ¨æ§åˆ¶å¹´é¾„ã€æ€§åˆ«å’Œç§æ—çš„åŒæ—¶ï¼‰å¯ä»¥é¢„æµ‹æ¥è‡ªç¾å›½ã€è‹±å›½å’ŒåŠ æ‹¿å¤§çš„3401åæ”¿æ²»å®¶çš„æ”¿æ²»å€¾å‘ï¼ˆr&#x3D;.13ï¼‰ã€‚å¯¹ä¸æ”¿æ²»å€¾å‘ç›¸å…³çš„é¢éƒ¨ç‰¹å¾çš„åˆ†ææ˜¾ç¤ºï¼Œä¿å®ˆä¸»ä¹‰è€…å¾€å¾€å…·æœ‰è¾ƒå¤§çš„ä¸‹è„¸éƒ¨ã€‚ä»æ ‡å‡†åŒ–å›¾åƒé¢„æµ‹æ”¿æ²»å€¾å‘å…·æœ‰å…³é”®çš„éšç§æ„ä¹‰ï¼Œå¯¹é¢éƒ¨è¯†åˆ«æŠ€æœ¯çš„ç›‘ç®¡å’Œå¯¹æ”¿æ²»å€¾å‘èµ·æºå’Œåæœçš„ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.16343v4">PDF</a> Now published at American Psychologist (2024)</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶é€šè¿‡å®éªŒå®¤æ ‡å‡†åŒ–é‡‡é›†çš„é¢éƒ¨å›¾åƒï¼Œæ¢è®¨äº†é¢éƒ¨å¤–è§‚ä¸æ”¿æ²»å€¾å‘ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œäººç±»å’Œé¢éƒ¨è¯†åˆ«ç®—æ³•éƒ½èƒ½æ ¹æ®é¢éƒ¨ç‰¹å¾é¢„æµ‹å‚ä¸è€…çš„æ”¿æ²»å€¾å‘ï¼Œä¸”ç®—æ³•çš„é¢„æµ‹å‡†ç¡®æ€§è¾ƒé«˜ã€‚æ­¤å¤–ï¼Œè¿™ç§å…³è”ä¼¼ä¹å¯ä»¥æ¨å¹¿åˆ°è‡ªç„¶çŠ¶æ€ä¸‹çš„æ”¿æ²»äººç‰©å›¾åƒä¸Šã€‚è¯¥ç ”ç©¶è¿˜å‘ç°ä¿å®ˆæ´¾çš„é¢éƒ¨ç‰¹å¾æœ‰æ‰€ä¸åŒã€‚è¯¥ç ”ç©¶å¯¹äºéšç§ã€é¢éƒ¨è¯†åˆ«æŠ€æœ¯çš„ç›‘ç®¡ä»¥åŠæ”¿æ²»å€¾å‘çš„èµ·æºå’Œåæœçš„ç†è§£å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é€šè¿‡æ§åˆ¶å®éªŒå®¤ç¯å¢ƒä¸­çš„è‡ªæˆ‘å‘ˆç°ã€é¢éƒ¨è¡¨æƒ…ã€å¤´éƒ¨æ–¹å‘å’Œå›¾åƒå±æ€§ç­‰å› ç´ ï¼Œé‡‡é›†äº†591åå‚ä¸è€…çš„é¢éƒ¨å›¾åƒã€‚</li>
<li>äººç±»å’Œé¢éƒ¨è¯†åˆ«ç®—æ³•éƒ½èƒ½æ ¹æ®é¢éƒ¨ç‰¹å¾é¢„æµ‹å‚ä¸è€…çš„æ”¿æ²»å€¾å‘ã€‚è¿™ç§é¢„æµ‹çš„å‡†ç¡®åº¦ç±»ä¼¼äºå…¶ä»–é¢†åŸŸçš„é¢„æµ‹å‡†ç¡®æ€§ï¼Œä¾‹å¦‚é¢è¯•é¢„æµ‹å·¥ä½œæˆåŠŸä¸å¦æˆ–é…’ç²¾å½±å“æ”»å‡»æ€§ç­‰æ–¹é¢ã€‚</li>
<li>ç®—æ³•çš„é¢„æµ‹å‡†ç¡®æ€§æ›´é«˜ï¼Œç‰¹åˆ«æ˜¯å½“å®ƒåˆ©ç”¨å‚ä¸è€…çš„å¹´é¾„ã€æ€§åˆ«å’Œç§æ—ä¿¡æ¯æ—¶ï¼Œå…¶é¢„æµ‹ç›¸å…³æ€§è¾¾åˆ°r&#x3D;.31ã€‚</li>
<li>æ”¿æ²»äººç‰©çš„è‡ªç„¶çŠ¶æ€ä¸‹çš„é¢éƒ¨å›¾åƒä¹Ÿå¯ä»¥ç”¨æ¥é¢„æµ‹å…¶æ”¿æ²»å€¾å‘ã€‚è¿™ç§å…³è”ä¸ä»…ä»…é€‚ç”¨äºæ ·æœ¬èŒƒå›´ï¼Œè¿˜å¯ä»¥åœ¨æ›´å¤§çš„èŒƒå›´å†…è¿›è¡Œæ¨å¹¿ã€‚</li>
<li>ç ”ç©¶å‘ç°ä¿å®ˆæ´¾çš„é¢éƒ¨ç‰¹å¾è¡¨ç°ä¸ºä¸‹è„¸éƒ¨åˆ†è¾ƒå¤§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-21002a4ef4f45ce5886e2f9ee709b134.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-885831123d349175ed2d26b16ab841e5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c7546dab6725a546c1fc6cf95e1be475.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d6868911784632fba0c0ff81fecd40da.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3a3920df5fb84e86a91d6f3c82699454.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0c9ccdb2b331b41ae44521b76527a4d7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-525f7ca3ca7517157cac5d68b68e2aa8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-431da7c85adfcfee9cfc763bbc71c473.jpg" align="middle">
</details>




<h2 id="Learning-to-mask-Towards-generalized-face-forgery-detection"><a href="#Learning-to-mask-Towards-generalized-face-forgery-detection" class="headerlink" title="Learning to mask: Towards generalized face forgery detection"></a>Learning to mask: Towards generalized face forgery detection</h2><p><strong>Authors:Jianwei Fei, Yunshu Dai, Huaming Wang, Zhihua Xia</strong></p>
<p>Generalizability to unseen forgery types is crucial for face forgery detectors. Recent works have made significant progress in terms of generalization by synthetic forgery data augmentation. In this work, we explore another path for improving the generalization. Our goal is to reduce the features that are easy to learn in the training phase, so as to reduce the risk of overfitting on specific forgery types. Specifically, in our method, a teacher network takes as input the face images and generates an attention map of the deep features by a diverse multihead attention ViT. The attention map is used to guide a student network to focus on the low-attended features by reducing the highly-attended deep features. A deep feature mixup strategy is also proposed to synthesize forgeries in the feature domain. Experiments demonstrate that, without data augmentation, our method is able to achieve promising performances on unseen forgeries and highly compressed data. </p>
<blockquote>
<p>å¯¹äºäººè„¸ä¼ªé€ æ£€æµ‹å™¨æ¥è¯´ï¼Œå¯¹æœªè§è¿‡çš„ä¼ªé€ ç±»å‹çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚è¿‘æœŸçš„å·¥ä½œé€šè¿‡åˆæˆä¼ªé€ æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œåœ¨æ³›åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¦ä¸€æ¡æé«˜æ³›åŒ–çš„è·¯å¾„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡å°‘é‚£äº›åœ¨è®­ç»ƒé˜¶æ®µå®¹æ˜“å­¦ä¹ çš„ç‰¹å¾ï¼Œä»¥é™ä½å¯¹ç‰¹å®šä¼ªé€ ç±»å‹è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæ•™å¸ˆç½‘ç»œä»¥äººè„¸å›¾åƒä¸ºè¾“å…¥ï¼Œé€šè¿‡å¤šæ ·åŒ–çš„å¤šå¤´æ³¨æ„åŠ›ViTç”Ÿæˆæ·±åº¦ç‰¹å¾çš„æ³¨æ„åŠ›å›¾ã€‚æ³¨æ„åŠ›å›¾è¢«ç”¨æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œå…³æ³¨ä½å…³æ³¨åº¦ç‰¹å¾ï¼Œé€šè¿‡å‡å°‘é«˜åº¦å…³æ³¨çš„æ·±åº¦ç‰¹å¾æ¥å®ç°ã€‚è¿˜æå‡ºäº†ä¸€ç§æ·±åº¦ç‰¹å¾æ··åˆç­–ç•¥ï¼Œç”¨äºåœ¨ç‰¹å¾åŸŸä¸­åˆæˆä¼ªé€ æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸è¿›è¡Œæ•°æ®å¢å¼ºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ä¼ªé€ æ•°æ®å’Œé«˜åº¦å‹ç¼©çš„æ•°æ®ä¸Šå®ç°æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2212.14309v2">PDF</a> Incorrect experimental setting</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¯¹äºäººè„¸ä¼ªé€ æ£€æµ‹å™¨è€Œè¨€ï¼Œå®ç°å¯¹æ–°ç±»å‹ä¼ªé€ çš„é€šç”¨æ€§æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿‘æœŸçš„ç ”ç©¶é€šè¿‡åˆæˆä¼ªé€ æ•°æ®å¢å¼ºæŠ€æœ¯å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¦ä¸€æ¡æé«˜é€šç”¨æ€§çš„è·¯å¾„ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‡å°‘åœ¨è®­ç»ƒé˜¶æ®µæ˜“äºå­¦ä¹ çš„ç‰¹å¾ï¼Œä»è€Œé™ä½å¯¹ç‰¹å®šä¼ªé€ ç±»å‹è¿‡åº¦æ‹Ÿåˆçš„é£é™©ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæ•™å¸ˆç½‘ç»œä»¥äººè„¸å›¾åƒä¸ºè¾“å…¥ï¼Œé€šè¿‡å¤šæ ·åŒ–çš„å¤šå¤´æ³¨æ„åŠ›è§†è§‰è½¬æ¢å™¨ç”Ÿæˆæ·±åº¦ç‰¹å¾çš„æ³¨æ„åŠ›å›¾ã€‚æ³¨æ„åŠ›å›¾è¢«ç”¨æ¥å¼•å¯¼å­¦ç”Ÿç½‘ç»œå…³æ³¨ä½å…³æ³¨åº¦çš„ç‰¹å¾ï¼Œé€šè¿‡å‡å°‘é«˜åº¦å…³æ³¨çš„æ·±åº¦ç‰¹å¾æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿˜æå‡ºäº†ä¸€ç§æ·±åº¦ç‰¹å¾æ··åˆç­–ç•¥ï¼Œç”¨äºåœ¨ç‰¹å¾åŸŸä¸­åˆæˆä¼ªé€ æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿ä¸ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¹Ÿèƒ½åœ¨æœªè§è¿‡çš„ä¼ªé€ å’Œé«˜åº¦å‹ç¼©çš„æ•°æ®ä¸Šå–å¾—ä»¤äººé¼“èˆçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººè„¸ä¼ªé€ æ£€æµ‹å™¨å¯¹æ–°ç±»å‹ä¼ªé€ çš„é€šç”¨æ€§è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡é™ä½æ˜“äºå­¦ä¹ çš„ç‰¹å¾æ¥å¢å¼ºä¼ªé€ æ£€æµ‹å™¨çš„é€šç”¨æ€§ã€‚</li>
<li>æ•™å¸ˆç½‘ç»œä½¿ç”¨å¤šæ ·åŒ–çš„å¤šå¤´æ³¨æ„åŠ›è§†è§‰è½¬æ¢å™¨ç”Ÿæˆæ·±åº¦ç‰¹å¾çš„æ³¨æ„åŠ›å›¾ã€‚</li>
<li>æ³¨æ„åŠ›å›¾ç”¨äºæŒ‡å¯¼å­¦ç”Ÿç½‘ç»œå…³æ³¨ä½å…³æ³¨åº¦ç‰¹å¾ã€‚</li>
<li>æå‡ºä¸€ç§æ·±åº¦ç‰¹å¾æ··åˆç­–ç•¥ï¼Œç”¨äºåœ¨ç‰¹å¾åŸŸä¸­åˆæˆä¼ªé€ æ•°æ®ã€‚</li>
<li>æ–¹æ³•åœ¨æ— éœ€æ•°æ®å¢å¼ºçš„æƒ…å†µä¸‹ï¼Œåœ¨æœªè§è¿‡çš„ä¼ªé€ å’Œé«˜åº¦å‹ç¼©çš„æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæé«˜äººè„¸ä¼ªé€ æ£€æµ‹å™¨çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bfb2aba678a15f41a2f4e8234d33767e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-aaf76f33e460e40886c9aa0ba084e5a3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-44a23f32787a2775fbac91d5e622dfc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-04d2ba6796350f94928a9de096dcb1ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e9f87cbfb96677cca97a01c0e4579ed.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                                    <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f7e8a03334c7b295d75eec44a9ef4132.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Delve into Visual Contrastive Decoding for Hallucination Mitigation of   Large Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de829c8eefa8229fee894461d46ba5e6.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  No Annotations for Object Detection in Art through Stable Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">6050.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
