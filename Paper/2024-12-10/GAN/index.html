<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Take Fake as Real Realistic-like Robust Black-box Adversarial Attack to   Evade AIGC Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5237f15bcbbce4298b010ed16cb47cca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    27.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    113 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Take-Fake-as-Real-Realistic-like-Robust-Black-box-Adversarial-Attack-to-Evade-AIGC-Detection"><a href="#Take-Fake-as-Real-Realistic-like-Robust-Black-box-Adversarial-Attack-to-Evade-AIGC-Detection" class="headerlink" title="Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to   Evade AIGC Detection"></a>Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to   Evade AIGC Detection</h2><p><strong>Authors:Caiyun Xie, Dengpan Ye, Yunming Zhang, Long Tang, Yunna Lv, Jiacheng Deng, Jiawei Song</strong></p>
<p>The security of AI-generated content (AIGC) detection based on GANs and diffusion models is closely related to the credibility of multimedia content. Malicious adversarial attacks can evade these developing AIGC detection. However, most existing adversarial attacks focus only on GAN-generated facial images detection, struggle to be effective on multi-class natural images and diffusion-based detectors, and exhibit poor invisibility. To fill this gap, we first conduct an in-depth analysis of the vulnerability of AIGC detectors and discover the feature that detectors vary in vulnerability to different post-processing. Then, considering the uncertainty of detectors in real-world scenarios, and based on the discovery, we propose a Realistic-like Robust Black-box Adversarial attack (R$^2$BA) with post-processing fusion optimization. Unlike typical perturbations, R$^2$BA uses real-world post-processing, i.e., Gaussian blur, JPEG compression, Gaussian noise and light spot to generate adversarial examples. Specifically, we use a stochastic particle swarm algorithm with inertia decay to optimize post-processing fusion intensity and explore the detectorâ€™s decision boundary. Guided by the detectorâ€™s fake probability, R$^2$BA enhances&#x2F;weakens the detector-vulnerable&#x2F;detector-robust post-processing intensity to strike a balance between adversariality and invisibility. Extensive experiments on popular&#x2F;commercial AIGC detectors and datasets demonstrate that R$^2$BA exhibits impressive anti-detection performance, excellent invisibility, and strong robustness in GAN-based and diffusion-based cases. Compared to state-of-the-art white-box and black-box attacks, R$^2$BA shows significant improvements of 15% and 21% in anti-detection performance under the original and robust scenario respectively, offering valuable insights for the security of AIGC detection in real-world applications. </p>
<blockquote>
<p>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æ£€æµ‹çš„å®‰å…¨æ€§ï¼Œä¸å¤šåª’ä½“å†…å®¹çš„å¯ä¿¡åº¦å¯†åˆ‡ç›¸å…³ã€‚æ¶æ„çš„å¯¹æŠ—æ€§æ”»å‡»å¯ä»¥èº²é¿æ­£åœ¨å‘å±•çš„AIGCæ£€æµ‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°å¯¹æŠ—æ€§æ”»å‡»ä»…ä¸“æ³¨äºGANç”Ÿæˆçš„é¢éƒ¨å›¾åƒæ£€æµ‹ï¼Œå¯¹äºå¤šç±»è‡ªç„¶å›¾åƒå’ŒåŸºäºæ‰©æ•£çš„æ£€æµ‹å™¨æ•ˆæœä¸ä½³ï¼Œéšèº«æ€§è¾ƒå·®ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹AIGCæ£€æµ‹å™¨çš„è„†å¼±æ€§è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶å‘ç°äº†æ£€æµ‹å™¨åœ¨ä¸åŒåå¤„ç†ä¸­çš„è„†å¼±æ€§ç‰¹å¾å­˜åœ¨å·®å¼‚ã€‚ç„¶åï¼Œè€ƒè™‘åˆ°çœŸå®åœºæ™¯ä¸­æ£€æµ‹å™¨çš„ä¸ç¡®å®šæ€§ï¼ŒåŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰åå¤„ç†èåˆä¼˜åŒ–çš„ç±»ä¼¼ç°å®çš„ç¨³å¥é»‘ç›’å¯¹æŠ—æ€§æ”»å‡»ï¼ˆR$^2$BAï¼‰ã€‚ä¸åŒäºå…¸å‹çš„æ‰°åŠ¨ï¼ŒR$^2$BAé‡‡ç”¨ç°å®ä¸–ç•Œåå¤„ç†ï¼Œä¾‹å¦‚é«˜æ–¯æ¨¡ç³Šã€JPEGå‹ç¼©ã€é«˜æ–¯å™ªå£°å’Œå…‰æ–‘æ¥ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨å…·æœ‰æƒ¯æ€§è¡°å‡çš„éšæœºç²’å­ç¾¤ç®—æ³•æ¥ä¼˜åŒ–åå¤„ç†èåˆå¼ºåº¦ï¼Œå¹¶æ¢ç´¢æ£€æµ‹å™¨çš„å†³ç­–è¾¹ç•Œã€‚åœ¨æ£€æµ‹å™¨çš„è™šå‡æ¦‚ç‡å¼•å¯¼ä¸‹ï¼ŒR$^2$BAå¢å¼º&#x2F;å‡å¼±äº†æ£€æµ‹å™¨è„†å¼±&#x2F;æ£€æµ‹å™¨ç¨³å¥çš„åå¤„ç†å¼ºåº¦ï¼Œåœ¨å¯¹æŠ—æ€§å’Œéšèº«æ€§ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚åœ¨æµè¡Œ&#x2F;å•†ä¸šAIGCæ£€æµ‹å™¨å’Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR$^2$BAå…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„æŠ—æ£€æµ‹æ€§èƒ½ã€å‡ºè‰²çš„éšèº«æ€§å’Œå¼ºå¤§çš„ç¨³å¥æ€§ï¼Œåœ¨åŸºäºGANå’ŒåŸºäºæ‰©æ•£çš„æƒ…å†µä¸‹éƒ½è¡¨ç°è‰¯å¥½ã€‚ä¸æœ€å…ˆè¿›çš„ç™½ç›’å’Œé»‘ç›’æ”»å‡»ç›¸æ¯”ï¼ŒR$^2$BAåœ¨åŸå§‹å’Œç¨³å¥åœºæ™¯ä¸‹çš„æŠ—æ£€æµ‹æ€§èƒ½åˆ†åˆ«æé«˜äº†15%å’Œ21%ï¼Œä¸ºAIGCåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ£€æµ‹å®‰å…¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06727v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æ£€æµ‹çš„å®‰å…¨æ€§ä¸å¤šåª’ä½“å†…å®¹çš„å¯ä¿¡åº¦å¯†åˆ‡ç›¸å…³ã€‚å­˜åœ¨æ¶æ„å¯¹æŠ—æ”»å‡»å¯ä»¥èº²é¿å‘å±•ä¸­çš„AIGCæ£€æµ‹ã€‚é’ˆå¯¹ç°æœ‰å¯¹æŠ—æ”»å‡»åœ¨GANç”Ÿæˆçš„é¢éƒ¨å›¾åƒæ£€æµ‹ä¸Šçš„å±€é™æ€§ï¼Œæœ¬æ–‡æ·±å…¥åˆ†æäº†AIGCæ£€æµ‹å™¨çš„è„†å¼±æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç°å®æ€§å¼ºçš„ç¨³å¥é»‘ç›’å¯¹æŠ—æ”»å‡»ï¼ˆR^2BAï¼‰æ–¹æ³•ï¼Œç»“åˆåå¤„ç†èåˆä¼˜åŒ–ç­–ç•¥ã€‚R^2BAåˆ©ç”¨çœŸå®ä¸–ç•Œçš„åå¤„ç†ï¼Œå¦‚é«˜æ–¯æ¨¡ç³Šã€JPEGå‹ç¼©ã€é«˜æ–¯å™ªå£°å’Œå…‰æ–‘ï¼Œç”Ÿæˆå¯¹æŠ—æ ·æœ¬ã€‚å®éªŒè¯æ˜ï¼ŒR^2BAåœ¨æµè¡Œçš„AIGCæ£€æµ‹å™¨å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æŠ—æ£€æµ‹æ€§èƒ½å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰æ£€æµ‹çš„å®‰å…¨æ€§ä¸å¤šåª’ä½“å¯ä¿¡åº¦ç´§å¯†ç›¸å…³ã€‚</li>
<li>å­˜åœ¨æ¶æ„å¯¹æŠ—æ”»å‡»å¯ä»¥ç»•è¿‡å‘å±•ä¸­çš„AIGCæ£€æµ‹æŠ€æœ¯ã€‚</li>
<li>å½“å‰å¯¹æŠ—æ”»å‡»ä¸»è¦å…³æ³¨GANç”Ÿæˆçš„é¢éƒ¨å›¾åƒæ£€æµ‹ï¼Œä½†åœ¨å¤šç±»åˆ«è‡ªç„¶å›¾åƒå’Œæ‰©æ•£æ¨¡å‹æ£€æµ‹å™¨ä¸Šçš„æ•ˆæœæœ‰é™ã€‚</li>
<li>æ·±å…¥ç ”ç©¶AIGCæ£€æµ‹å™¨çš„è„†å¼±æ€§ï¼Œå‘ç°ä¸åŒåå¤„ç†å¯¼è‡´çš„æ£€æµ‹å™¨å·®å¼‚ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹é»‘ç›’å¯¹æŠ—æ”»å‡»æ–¹æ³•R^2BAï¼Œç»“åˆåå¤„ç†èåˆä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>R^2BAåˆ©ç”¨çœŸå®ä¸–ç•Œçš„åå¤„ç†ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼Œä»¥å¢å¼ºæ£€æµ‹å™¨çš„å¹³è¡¡å¯¹æŠ—æ€§å’Œéšè”½æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6baf110d6886f9b0f9920674c8e790d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1efbc44e0ac3a5377aee23bb88468b1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf5941acc5f5c33c5057413dba1eb29e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15cc07086a66b8149da7ce91ee35af91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3aa15c8651e8c4a37d6648d24d505b3.jpg" align="middle">
</details>




<h2 id="Rendering-Refined-Stable-Diffusion-for-Privacy-Compliant-Synthetic-Data"><a href="#Rendering-Refined-Stable-Diffusion-for-Privacy-Compliant-Synthetic-Data" class="headerlink" title="Rendering-Refined Stable Diffusion for Privacy Compliant Synthetic Data"></a>Rendering-Refined Stable Diffusion for Privacy Compliant Synthetic Data</h2><p><strong>Authors:Kartik Patwari, David Schneider, Xiaoxiao Sun, Chen-Nee Chuah, Lingjuan Lyu, Vivek Sharma</strong></p>
<p>Growing privacy concerns and regulations like GDPR and CCPA necessitate pseudonymization techniques that protect identity in image datasets. However, retaining utility is also essential. Traditional methods like masking and blurring degrade quality and obscure critical context, especially in human-centric images. We introduce Rendering-Refined Stable Diffusion (RefSD), a pipeline that combines 3D-rendering with Stable Diffusion, enabling prompt-based control over human attributes while preserving posture. Unlike standard diffusion models that fail to retain posture or GANs that lack realism and flexible attribute control, RefSD balances posture preservation, realism, and customization. We also propose HumanGenAI, a framework for human perception and utility evaluation. Human perception assessments reveal attribute-specific strengths and weaknesses of RefSD. Our utility experiments show that models trained on RefSD pseudonymized data outperform those trained on real data in detection tasks, with further performance gains when combining RefSD with real data. For classification tasks, we consistently observe performance improvements when using RefSD data with real data, confirming the utility of our pseudonymized data. </p>
<blockquote>
<p>éšç€éšç§é—®é¢˜çš„æ—¥ç›Šçªå‡ºä»¥åŠGDPRå’ŒCCPAç­‰æ³•è§„çš„å‡ºå°ï¼Œä¿æŠ¤å›¾åƒæ•°æ®é›†ä¸­çš„èº«ä»½éšç§æˆä¸ºå¿…éœ€ï¼Œè¿™å‚¬ç”Ÿäº†åŒ¿ååŒ–æŠ€æœ¯ã€‚ç„¶è€Œï¼Œä¿æŒæ•°æ®å®ç”¨æ€§ä¹ŸåŒæ ·å…³é”®ã€‚åƒæ©è”½å’Œæ¨¡ç³Šå¤„ç†è¿™æ ·çš„ä¼ ç»Ÿæ–¹æ³•ä¼šé™ä½è´¨é‡å¹¶æ©ç›–å…³é”®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥äººç±»ä¸ºä¸­å¿ƒçš„å›¾åƒä¸­æ›´æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æ¨å‡ºäº†ç»è¿‡æ¸²æŸ“ä¼˜åŒ–çš„ç¨³å®šæ‰©æ•£ï¼ˆRefSDï¼‰æµç¨‹ï¼Œå®ƒç»“åˆäº†3Dæ¸²æŸ“å’Œç¨³å®šæ‰©æ•£æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°åŸºäºæç¤ºæ§åˆ¶äººç±»å±æ€§åŒæ—¶ä¿æŒå§¿æ€ã€‚ä¸åŒäºæ— æ³•ä¿æŒå§¿æ€çš„æ ‡å‡†æ‰©æ•£æ¨¡å‹æˆ–ç¼ºä¹çœŸå®æ„Ÿå’Œçµæ´»å±æ€§æ§åˆ¶çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼ŒRefSDåœ¨å§¿æ€ä¿ç•™ã€çœŸå®æ€§å’Œå®šåˆ¶åŒ–ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†HumanGenAIæ¡†æ¶ï¼Œç”¨äºäººç±»æ„ŸçŸ¥å’Œæ•ˆç”¨è¯„ä¼°ã€‚äººç±»æ„ŸçŸ¥è¯„ä¼°æ­ç¤ºäº†RefSDåœ¨ç‰¹å®šå±æ€§æ–¹é¢çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ã€‚æˆ‘ä»¬çš„æ•ˆç”¨å®éªŒè¡¨æ˜ï¼Œåœ¨æ£€æµ‹ä»»åŠ¡ä¸­ï¼ŒåŸºäºRefSDåŒ¿ååŒ–æ•°æ®è®­ç»ƒçš„æ¨¡å‹è¡¨ç°ä¼˜äºåŸºäºçœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶ä¸”å½“å°†RefSDä¸çœŸå®æ•°æ®ç»“åˆæ—¶ï¼Œæ€§èƒ½ä¼šå¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨RefSDæ•°æ®ä¸çœŸå®æ•°æ®ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ€§èƒ½æŒç»­æ”¹å–„ï¼Œè¯å®äº†åŒ¿ååŒ–æ•°æ®çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¾åƒæ•°æ®é›†éšç§ä¿æŠ¤éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼ŒGDPRå’ŒCCPAç­‰æ³•è§„ä¿ƒä½¿å¯¹ä¼ªååŒ–æŠ€æœ¯çš„éœ€æ±‚ä¸Šå‡ã€‚æœ¬ç ”ç©¶æ¨å‡ºä¸€ç§åä¸ºRendering-Refined Stable Diffusionï¼ˆRefSDï¼‰çš„ç®¡é“ï¼Œç»“åˆ3Dæ¸²æŸ“ä¸Stable DiffusionæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå§¿æ€çš„åŒæ—¶å®ç°åŸºäºæç¤ºçš„äººç±»å±æ€§æ§åˆ¶ï¼Œå¹¶å…·å¤‡çœŸå®æ„Ÿå’Œå¯å®šåˆ¶æ€§ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†HumanGenAIæ¡†æ¶ç”¨äºäººç±»æ„ŸçŸ¥å’Œå®ç”¨æ€§è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒRefSDæŠ€æœ¯åœ¨å±æ€§æ„ŸçŸ¥ä¸Šå…·æœ‰ä¼˜ç‚¹ä¸ä¸è¶³ï¼Œä¸”åœ¨æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚åŒæ—¶ï¼Œä½¿ç”¨RefSDæ•°æ®çš„æ¨¡å‹åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šä¹Ÿè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶ä¼ªååŒ–æ•°æ®çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ•°æ®é›†çš„éšç§ä¿æŠ¤éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œä¿ƒä½¿ä¼ªååŒ–æŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>Rendering-Refined Stable Diffusionï¼ˆRefSDï¼‰ç»“åˆ3Dæ¸²æŸ“ä¸Stable DiffusionæŠ€æœ¯ï¼Œä¿æŒå§¿æ€çš„åŒæ—¶å®ç°åŸºäºæç¤ºçš„äººç±»å±æ€§æ§åˆ¶ã€‚</li>
<li>RefSDæŠ€æœ¯å®ç°äº†çœŸå®æ„Ÿå’Œå¯å®šåˆ¶æ€§çš„å¹³è¡¡ã€‚</li>
<li>HumanGenAIæ¡†æ¶ç”¨äºè¯„ä¼°å›¾åƒæ•°æ®çš„äººç±»æ„ŸçŸ¥å’Œå®ç”¨æ€§ã€‚</li>
<li>RefSDæŠ€æœ¯åœ¨å±æ€§æ„ŸçŸ¥æ–¹é¢å…·æœ‰ä¸€å®šçš„ä¼˜ç‚¹å’Œå±€é™ã€‚</li>
<li>ä½¿ç”¨RefSDæŠ€æœ¯çš„æ¨¡å‹åœ¨æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0795f1dfddfc953df611278a6a774b6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fbfbdf5104df33c3e008b48f9c9532d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-903ecaa12e966be3ea3a413a90c6e549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689354782ff009dba009c12c7acc208b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c33d3238c4e5afc5b3c7eb2e1325e66.jpg" align="middle">
</details>




<h2 id="Accelerating-Video-Diffusion-Models-via-Distribution-Matching"><a href="#Accelerating-Video-Diffusion-Models-via-Distribution-Matching" class="headerlink" title="Accelerating Video Diffusion Models via Distribution Matching"></a>Accelerating Video Diffusion Models via Distribution Matching</h2><p><strong>Authors:Yuanzhi Zhu, Hanshu Yan, Huan Yang, Kai Zhang, Junnan Li</strong></p>
<p>Generative models, particularly diffusion models, have made significant success in data synthesis across various modalities, including images, videos, and 3D assets. However, current diffusion models are computationally intensive, often requiring numerous sampling steps that limit their practical application, especially in video generation. This work introduces a novel framework for diffusion distillation and distribution matching that dramatically reduces the number of inference steps while maintaining-and potentially improving-generation quality. Our approach focuses on distilling pre-trained diffusion models into a more efficient few-step generator, specifically targeting video generation. By leveraging a combination of video GAN loss and a novel 2D score distribution matching loss, we demonstrate the potential to generate high-quality video frames with substantially fewer sampling steps. To be specific, the proposed method incorporates a denoising GAN discriminator to distil from the real data and a pre-trained image diffusion model to enhance the frame quality and the prompt-following capabilities. Experimental results using AnimateDiff as the teacher model showcase the methodâ€™s effectiveness, achieving superior performance in just four sampling steps compared to existing techniques. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ‰©æ•£æ¨¡å‹ï¼Œåœ¨å„ç§æ¨¡æ€çš„æ•°æ®åˆæˆä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œ3Dèµ„äº§ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„é‡‡æ ·æ­¥éª¤ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢ã€‚è¿™é¡¹å·¥ä½œä»‹ç»äº†ä¸€ç§æ–°çš„æ‰©æ•£è’¸é¦å’Œåˆ†å¸ƒåŒ¹é…æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ç»´æŒç”šè‡³å¯èƒ½æé«˜ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘äº†æ¨ç†æ­¥éª¤çš„æ•°é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºå°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è’¸é¦åˆ°ä¸€ä¸ªæ›´æœ‰æ•ˆç‡çš„å¤šæ­¥ç”Ÿæˆå™¨ä¸­ï¼Œç‰¹åˆ«é’ˆå¯¹è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡ç»“åˆè§†é¢‘GANæŸå¤±å’Œä¸€ç§æ–°çš„2Dåˆ†æ•°åˆ†å¸ƒåŒ¹é…æŸå¤±ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç”¨æ›´å°‘çš„é‡‡æ ·æ­¥éª¤ç”Ÿæˆé«˜è´¨é‡è§†é¢‘å¸§çš„æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æå‡ºçš„æ–¹æ³•é‡‡ç”¨ä¸€ä¸ªå»å™ªGANé‰´åˆ«å™¨ä»çœŸå®æ•°æ®å’Œé¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æç‚¼ç²¾åï¼Œä»¥æå‡å¸§è´¨é‡å’Œéµå¾ªæç¤ºçš„èƒ½åŠ›ã€‚ä½¿ç”¨AnimateDiffä½œä¸ºæ•™å¸ˆæ¨¡å‹è¿›è¡Œçš„å®éªŒç»“æœå±•ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨ä»…å››ä¸ªé‡‡æ ·æ­¥éª¤å†…ä¾¿å®ç°äº†å¯¹ç°æœ‰æŠ€æœ¯çš„è¶…è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05899v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒã€è§†é¢‘å’Œ3Dèµ„äº§ç­‰å¤šä¸ªé¢†åŸŸçš„æ•°æ®åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå½“å‰æ‰©æ•£æ¨¡å‹è®¡ç®—é‡å¤§ï¼Œéœ€è¦å¤§é‡é‡‡æ ·æ­¥éª¤ï¼Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘ç”Ÿæˆæ–¹é¢ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ‰©æ•£è’¸é¦å’Œåˆ†å¸ƒåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡è’¸é¦é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ¨ç†æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ–¹æ³•ä¸“æ³¨äºé’ˆå¯¹è§†é¢‘ç”Ÿæˆçš„æ›´é«˜æ•ˆçš„å‡ æ­¥ç”Ÿæˆå™¨ã€‚é€šè¿‡ç»“åˆè§†é¢‘GANæŸå¤±å’Œæ–°çš„äºŒç»´åˆ†æ•°åˆ†å¸ƒåŒ¹é…æŸå¤±ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å‡å°‘é‡‡æ ·æ­¥éª¤çš„åŒæ—¶ç”Ÿæˆé«˜è´¨é‡è§†é¢‘å¸§çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²åœ¨å¤šé¢†åŸŸæ•°æ®åˆæˆä¸Šå–å¾—æ˜¾è‘—æˆåŠŸï¼Œä½†è®¡ç®—é‡å¤§å’Œé‡‡æ ·æ­¥éª¤å¤šé™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ‰©æ•£è’¸é¦å’Œåˆ†å¸ƒåŒ¹é…æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡è’¸é¦é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ¨ç†æ­¥éª¤ã€‚</li>
<li>æ–¹æ³•ä¸“æ³¨äºé’ˆå¯¹è§†é¢‘ç”Ÿæˆçš„æ›´é«˜æ•ˆç”Ÿæˆå™¨è®¾è®¡ã€‚</li>
<li>ç»“åˆè§†é¢‘GANæŸå¤±å’ŒäºŒç»´åˆ†æ•°åˆ†å¸ƒåŒ¹é…æŸå¤±ï¼Œç”Ÿæˆé«˜è´¨é‡è§†é¢‘å¸§ã€‚</li>
<li>ä½¿ç”¨AnimateDiffä½œä¸ºæ•™å¸ˆæ¨¡å‹è¿›è¡Œå®éªŒï¼Œå±•ç¤ºæ–°æ–¹æ³•åœ¨å››ä¸ªé‡‡æ ·æ­¥éª¤å†…å³å¯å®ç°ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be77249bdeffad0f9551d95a3964e934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64c35a3f326e3ce2e8ef6345fdc35613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05ebd7a23779e2f8a17598bc11a3098c.jpg" align="middle">
</details>




<h2 id="A-Tiered-GAN-Approach-for-Monet-Style-Image-Generation"><a href="#A-Tiered-GAN-Approach-for-Monet-Style-Image-Generation" class="headerlink" title="A Tiered GAN Approach for Monet-Style Image Generation"></a>A Tiered GAN Approach for Monet-Style Image Generation</h2><p><strong>Authors:FNU Neha, Deepshikha Bhati, Deepak Kumar Shukla, Md Amiruzzaman</strong></p>
<p>Generative Adversarial Networks (GANs) have proven to be a powerful tool in generating artistic images, capable of mimicking the styles of renowned painters, such as Claude Monet. This paper introduces a tiered GAN model to progressively refine image quality through a multi-stage process, enhancing the generated images at each step. The model transforms random noise into detailed artistic representations, addressing common challenges such as instability in training, mode collapse, and output quality. This approach combines downsampling and convolutional techniques, enabling the generation of high-quality Monet-style artwork while optimizing computational efficiency. Experimental results demonstrate the architectureâ€™s ability to produce foundational artistic structures, though further refinements are necessary for achieving higher levels of realism and fidelity to Monetâ€™s style. Future work focuses on improving training methodologies and model complexity to bridge the gap between generated and true artistic images. Additionally, the limitations of traditional GANs in artistic generation are analyzed, and strategies to overcome these shortcomings are proposed. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å·²è¢«è¯æ˜æ˜¯ç”Ÿæˆè‰ºæœ¯å›¾åƒçš„å¼ºå¤§å·¥å…·ï¼Œèƒ½å¤Ÿæ¨¡ä»¿è‘—åç”»å®¶å¦‚å…‹åŠ³å¾·Â·è«å¥ˆçš„é£æ ¼ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ†å±‚GANæ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹é€æ­¥æ”¹è¿›å›¾åƒè´¨é‡ï¼Œæ¯ä¸€æ­¥éƒ½å¢å¼ºç”Ÿæˆçš„å›¾åƒã€‚è¯¥æ¨¡å‹å°†éšæœºå™ªå£°è½¬æ¢ä¸ºè¯¦ç»†çš„è‰ºæœ¯è¡¨ç¤ºï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒå’Œè¾“å‡ºè´¨é‡ç­‰å¸¸è§æŒ‘æˆ˜ã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†é™é‡‡æ ·å’Œå·ç§¯æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¼˜åŒ–è®¡ç®—æ•ˆç‡çš„åŒæ—¶ç”Ÿæˆé«˜è´¨é‡çš„è«å¥ˆé£æ ¼è‰ºæœ¯ä½œå“ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ¶æ„äº§ç”ŸåŸºç¡€è‰ºæœ¯ç»“æ„çš„èƒ½åŠ›ï¼Œä½†ä¸ºäº†å®ç°æ›´é«˜æ°´å¹³çš„ç°å®ä¸»ä¹‰å’Œå¯¹è«å¥ˆé£æ ¼çš„å¿ å®åº¦ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚æœªæ¥çš„å·¥ä½œé‡ç‚¹å°†æ”¾åœ¨æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹å¤æ‚æ€§ä¸Šï¼Œä»¥ç¼©å°ç”Ÿæˆå›¾åƒå’ŒçœŸå®è‰ºæœ¯å›¾åƒä¹‹é—´çš„å·®è·ã€‚æ­¤å¤–ï¼Œè¿˜åˆ†æäº†ä¼ ç»ŸGANåœ¨è‰ºæœ¯åˆ›ä½œä¸­çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†å…‹æœè¿™äº›ä¸è¶³çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05724v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§åˆ†å±‚GANæ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹é€æ­¥æ”¹è¿›å›¾åƒè´¨é‡ã€‚æ¨¡å‹èƒ½å¤Ÿå°†éšæœºå™ªå£°è½¬æ¢ä¸ºè¯¦ç»†çš„è‰ºæœ¯è¡¨ç¤ºå½¢å¼ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šã€æ¨¡å¼å´©æºƒå’Œè¾“å‡ºè´¨é‡ç­‰é—®é¢˜ã€‚ç»“åˆä¸‹é‡‡æ ·å’Œå·ç§¯æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è‰ºæœ¯ä½œå“ï¼Œå¦‚æ¨¡ä»¿å…‹åŠ³å¾·Â·è«å¥ˆçš„é£æ ¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿç”ŸæˆåŸºç¡€è‰ºæœ¯ç»“æ„ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥æ”¹è¿›ä»¥å®ç°æ›´é«˜çš„çœŸå®æ„Ÿå’Œå¯¹è«å¥ˆé£æ ¼çš„å¿ å®åº¦ã€‚æœªæ¥å·¥ä½œé‡ç‚¹å°†æ”¾åœ¨æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹å¤æ‚æ€§ä¸Šï¼Œä»¥ç¼©å°ç”Ÿæˆå›¾åƒå’ŒçœŸå®è‰ºæœ¯ä½œå“ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†å±‚GANæ¨¡å‹é€šè¿‡å¤šé˜¶æ®µè¿‡ç¨‹é€æ­¥æ”¹è¿›å›¾åƒè´¨é‡ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿è‘—åç”»å®¶çš„é£æ ¼ï¼Œå¦‚å…‹åŠ³å¾·Â·è«å¥ˆã€‚</li>
<li>é€šè¿‡ç»“åˆä¸‹é‡‡æ ·å’Œå·ç§¯æŠ€æœ¯ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è‰ºæœ¯ä½œå“ã€‚</li>
<li>æ¨¡å‹è§£å†³äº†ä¼ ç»ŸGANåœ¨å›¾åƒç”Ÿæˆä¸­å¸¸è§çš„æŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å¼å´©æºƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¶æ„åœ¨ç”ŸæˆåŸºç¡€è‰ºæœ¯ç»“æ„æ–¹é¢è¡¨ç°å‡ºèƒ½åŠ›ï¼Œä½†ä»éœ€æ”¹è¿›ä»¥è¾¾æˆæ›´é«˜çš„çœŸå®æ€§å’Œå¯¹ç‰¹å®šé£æ ¼çš„å¿ å®åº¦ã€‚</li>
<li>æœªæ¥å·¥ä½œæ–¹å‘åŒ…æ‹¬æ”¹è¿›è®­ç»ƒæ–¹æ³•å’Œæ¨¡å‹å¤æ‚æ€§ï¼Œä»¥ç¼©å°ç”Ÿæˆå›¾åƒå’ŒçœŸå®è‰ºæœ¯ä½œå“ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ae3beb2f5cfa86192d14b90205a6aec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a308208650edb032417369c9d7213930.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd75fcdb7682802b443fe46aa38eb8d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2dfaef8f81b410f940eeb328ac97223.jpg" align="middle">
</details>




<h2 id="LAA-Net-A-Physical-prior-knowledge-Based-Network-for-Robust-Nighttime-Depth-Estimation"><a href="#LAA-Net-A-Physical-prior-knowledge-Based-Network-for-Robust-Nighttime-Depth-Estimation" class="headerlink" title="LAA-Net: A Physical-prior-knowledge Based Network for Robust Nighttime   Depth Estimation"></a>LAA-Net: A Physical-prior-knowledge Based Network for Robust Nighttime   Depth Estimation</h2><p><strong>Authors:Kebin Peng, Haotang Li, Zhenyu Qi, Huashan Chen, Zi Wang, Wei Zhang, Sen He</strong></p>
<p>Existing self-supervised monocular depth estimation (MDE) models attempt to improve nighttime performance by using GANs to transfer nighttime images into their daytime versions. However, this can introduce inconsistencies due to the complexities of real-world daytime lighting variations, which may finally lead to inaccurate estimation results. To address this issue, we leverage physical-prior-knowledge about light wavelength and light attenuation during nighttime. Specifically, our model, Light-Attenuation-Aware Network (LAA-Net), incorporates physical insights from Rayleigh scattering theory for robust nighttime depth estimation: LAA-Net is trained based on red channel values because red light preserves more information under nighttime scenarios due to its longer wavelength. Additionally, based on Beer-Lambert law, we introduce Red Channel Attenuation (RCA) loss to guide LAA-Netâ€™s training. Experiments on the RobotCar-Night, nuScenes-Night, RobotCar-Day, and KITTI datasets demonstrate that our model outperforms SOTA models. </p>
<blockquote>
<p>ç°æœ‰çš„è‡ªç›‘ç£å•çœ¼æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰æ¨¡å‹å°è¯•ä½¿ç”¨GANå°†å¤œé—´å›¾åƒè½¬æ¢ä¸ºæ—¥é—´ç‰ˆæœ¬ï¼Œä»¥æé«˜å¤œé—´æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºç°å®ä¸–ç•Œç™½å¤©å…‰ç…§å˜åŒ–çš„å¤æ‚æ€§ï¼Œè¿™å¯èƒ½ä¼šå¼•å…¥ä¸ä¸€è‡´æ€§ï¼Œæœ€ç»ˆå¯¼è‡´ä¼°è®¡ç»“æœä¸å‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨å…³äºå¤œé—´å…‰çº¿æ³¢é•¿å’Œå…‰çº¿è¡°å‡çš„ç‰©ç†å…ˆéªŒçŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹â€”â€”Light-Attenuation-Aware Networkï¼ˆLAA-Netï¼‰ç»“åˆäº†ç‘åˆ©æ•£å°„ç†è®ºçš„ç‰©ç†æ´å¯Ÿï¼Œä»¥å®ç°ç¨³å¥çš„å¤œé—´æ·±åº¦ä¼°è®¡ï¼šLAA-NetåŸºäºçº¢è‰²é€šé“å€¼è¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºç”±äºçº¢è‰²å…‰çš„æ³¢é•¿è¾ƒé•¿ï¼Œå…¶åœ¨å¤œé—´åœºæ™¯ä¸­å¯ä»¥ä¿ç•™æ›´å¤šä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒåŸºäºæ¯”å°”-æœ—ä¼¯å®šå¾‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†Red Channel Attenuationï¼ˆRCAï¼‰æŸå¤±æ¥æŒ‡å¯¼LAA-Netçš„è®­ç»ƒã€‚åœ¨RobotCar-Nightã€nuScenes-Nightã€RobotCar-Dayå’ŒKITTIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºæœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04666v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºç‰©ç†ç‰¹æ€§çš„å¤œé—´åœºæ™¯æ·±åº¦ä¼°ç®—ç½‘ç»œLAA-Netåˆ©ç”¨å…‰çš„æ³¢é•¿å’Œå¤œé—´è¡°å‡ç‰¹æ€§ï¼Œé€šè¿‡å¼•å…¥Rayleighæ•£å°„ç†è®ºçš„ç‰©ç†æ´å¯Ÿï¼Œå¹¶ç»“åˆRed Channel Attenuationï¼ˆRCAï¼‰æŸå¤±ï¼Œæé«˜äº†å¤œé—´æ·±åº¦ä¼°ç®—çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜å…¶ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ©ç”¨GANså¯¹å¤œé—´å›¾åƒè¿›è¡Œè½¬è¯‘ï¼Œç°æœ‰æ¨¡å‹å­˜åœ¨æ—¥é—´å…‰ç…§å˜åŒ–å¼•å…¥çš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡ç»“æœä¸å‡†ç¡®çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§ç»“åˆç‰©ç†ç‰¹æ€§çš„å¤œé—´æ·±åº¦ä¼°ç®—æ¨¡å‹LAA-Netï¼Œåˆ©ç”¨å…‰çš„æ³¢é•¿å’Œå¤œé—´è¡°å‡ç‰¹æ€§è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>LAA-NetåŸºäºå¤œé—´åœºæ™¯ä¸­çº¢è‰²é€šé“ä¿¡æ¯çš„é‡è¦æ€§è¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºçº¢è‰²å…‰æ³¢é•¿è¾ƒé•¿ï¼Œåœ¨å¤œé—´åœºæ™¯ä¸‹èƒ½ä¿ç•™æ›´å¤šä¿¡æ¯ã€‚</li>
<li>å¼•å…¥åŸºäºBeer-Lambertå®šå¾‹çš„Red Channel Attenuationï¼ˆRCAï¼‰æŸå¤±ï¼Œç”¨äºæŒ‡å¯¼LAA-Netçš„è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLAA-Netåœ¨å¤œé—´åœºæ™¯æ·±åº¦ä¼°ç®—ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>LAA-Netåœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¯æ˜äº†å…¶æ™®é€‚æ€§å’Œæ€§èƒ½ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-466f26de621329cdf292d05967911a50.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-14f16dca4a70328c8e917220b35005c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cb675e8833f13039603916a4f8dfc30.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-914d932dc84286b56f55448289ab748c.jpg" align="middle">
</details>




<h2 id="A-Framework-For-Image-Synthesis-Using-Supervised-Contrastive-Learning"><a href="#A-Framework-For-Image-Synthesis-Using-Supervised-Contrastive-Learning" class="headerlink" title="A Framework For Image Synthesis Using Supervised Contrastive Learning"></a>A Framework For Image Synthesis Using Supervised Contrastive Learning</h2><p><strong>Authors:Yibin Liu, Jianyu Zhang, Li Zhang, Shijian Li, Gang Pan</strong></p>
<p>Text-to-image (T2I) generation aims at producing realistic images corresponding to text descriptions. Generative Adversarial Network (GAN) has proven to be successful in this task. Typical T2I GANs are 2 phase methods that first pretrain an inter-modal representation from aligned image-text pairs and then use GAN to train image generator on that basis. However, such representation ignores the inner-modal semantic correspondence, e.g. the images with same label. The semantic label in priory describes the inherent distribution pattern with underlying cross-image relationships, which is supplement to the text description for understanding the full characteristics of image. In this paper, we propose a framework leveraging both inter- and inner-modal correspondence by label guided supervised contrastive learning. We extend the T2I GANs to two parameter-sharing contrast branches in both pretraining and generation phases. This integration effectively clusters the semantically similar image-text pair representations, thereby fostering the generation of higher-quality images. We demonstrate our framework on four novel T2I GANs by both single-object dataset CUB and multi-object dataset COCO, achieving significant improvements in the Inception Score (IS) and Frechet Inception Distance (FID) metrics of imagegeneration evaluation. Notably, on more complex multi-object COCO, our framework improves FID by 30.1%, 27.3%, 16.2% and 17.1% for AttnGAN, DM-GAN, SSA-GAN and GALIP, respectively. We also validate our superiority by comparing with other label guided T2I GANs. The results affirm the effectiveness and competitiveness of our approach in advancing the state-of-the-art GAN for T2I generation </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æˆåŠŸçš„ã€‚å…¸å‹çš„T2I GANsæ˜¯åˆ†ä¸ºä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆä½¿ç”¨å¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹é¢„è®­ç»ƒä¸€ä¸ªè·¨æ¨¡æ€è¡¨ç¤ºï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šä½¿ç”¨GANè®­ç»ƒå›¾åƒç”Ÿæˆå™¨ã€‚ç„¶è€Œï¼Œè¿™ç§è¡¨ç¤ºå¿½ç•¥äº†å†…æ¨¡æ€è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œä¾‹å¦‚å…·æœ‰ç›¸åŒæ ‡ç­¾çš„å›¾åƒã€‚å…ˆéªŒä¸­çš„è¯­ä¹‰æ ‡ç­¾æè¿°äº†å›¾åƒä¹‹é—´å…³ç³»çš„å†…åœ¨åˆ†å¸ƒæ¨¡å¼ï¼Œè¿™æ˜¯å¯¹æ–‡æœ¬æè¿°çš„ç†è§£å›¾åƒçš„å®Œæ•´ç‰¹å¾çš„è¡¥å……ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨è·¨æ¨¡æ€å’Œå†…æ¨¡æ€å¯¹åº”å…³ç³»çš„æ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥å®ç°ã€‚æˆ‘ä»¬å°†T2I GANsæ‰©å±•åˆ°é¢„è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µçš„ä¸¤ä¸ªå‚æ•°å…±äº«å¯¹æ¯”åˆ†æ”¯ã€‚è¿™ç§é›†æˆæœ‰æ•ˆåœ°èšç±»äº†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å›¾åƒæ–‡æœ¬å¯¹è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›äº†æ›´é«˜è´¨é‡å›¾åƒçš„ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨å•ç›®æ ‡æ•°æ®é›†CUBå’Œå¤šç›®æ ‡æ•°æ®é›†COCOä¸Šçš„å››ç§æ–°å‹T2I GANsä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œåœ¨å›¾åƒç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡Inception Scoreï¼ˆISï¼‰å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯åœ¨æ›´å¤æ‚çš„å¤šç›®æ ‡COCOä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ”¹è¿›äº†FIDæŒ‡æ ‡ï¼Œåˆ†åˆ«ä¸ºAttnGANã€DM-GANã€SSA-GANå’ŒGALIPæé«˜äº†30.1%ã€27.3%ã€16.2%å’Œ17.1%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¸å…¶ä»–æ ‡ç­¾å¼•å¯¼çš„T2I GANsè¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯æˆ‘ä»¬çš„ä¼˜è¶Šæ€§ã€‚ç»“æœè¯å®äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ¨è¿›T2Iç”Ÿæˆçš„æœ€æ–°GANæŠ€æœ¯ä¸­çš„æœ‰æ•ˆæ€§å’Œç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03957v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆçš„ç›®æ ‡æ˜¯ç”Ÿæˆä¸æ–‡æœ¬æè¿°å¯¹åº”çš„çœŸå®å›¾åƒã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚å…¸å‹çš„T2I GANsæ˜¯åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆä½¿ç”¨å¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹é¢„è®­ç»ƒè·¨æ¨¡æ€è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨GANåœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒå›¾åƒç”Ÿæˆå™¨ã€‚ç„¶è€Œï¼Œè¿™ç§è¡¨ç¤ºå¿½ç•¥äº†å†…æ¨¡æ€è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œä¾‹å¦‚å…·æœ‰ç›¸åŒæ ‡ç­¾çš„å›¾åƒã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªæ¡†æ¶ï¼Œåˆ©ç”¨è·¨æ¨¡æ€å’Œå†…æ¨¡æ€å¯¹åº”å…³ç³»ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€‚åœ¨é¢„è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µï¼Œæœ¬æ–‡å°†T2I GANsæ‰©å±•åˆ°ä¸¤ä¸ªå‚æ•°å…±äº«å¯¹æ¯”åˆ†æ”¯ã€‚è¿™ç§é›†æˆæœ‰æ•ˆåœ°èšç±»äº†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å›¾åƒæ–‡æœ¬å¯¹è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚åœ¨å•ç›®æ ‡æ•°æ®é›†CUBå’Œå¤šç›®æ ‡æ•°æ®é›†COCOä¸Šï¼Œæœ¬æ–‡çš„æ–¹æ³•åœ¨å››ç§æ–°å‹çš„T2I GANsä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨å›¾åƒç”Ÿæˆçš„Inception Scoreï¼ˆISï¼‰å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰æŒ‡æ ‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚ç‰¹åˆ«æ˜¯åœ¨æ›´å¤æ‚çš„COCOæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ”¹è¿›äº†FIDåˆ†æ•°è¶…è¿‡å…¶ä»–å››ç§æ–¹æ³•ã€‚é€šè¿‡ä¸å…¶ä»–æ ‡ç­¾å¼•å¯¼çš„T2I GANsçš„æ¯”è¾ƒï¼ŒéªŒè¯äº†æœ¬æ–‡æ–¹æ³•çš„å…ˆè¿›æ€§å’Œç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iç”Ÿæˆçš„ç›®æ ‡ä¸ºæ ¹æ®æ–‡æœ¬æè¿°ç”ŸæˆçœŸå®å›¾åƒï¼ŒGANåœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å…¸å‹T2I GANsé‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œå…ˆé¢„è®­ç»ƒè·¨æ¨¡æ€è¡¨ç¤ºï¼Œå†ä½¿ç”¨GANè¿›è¡Œå›¾åƒç”Ÿæˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†å†…æ¨¡æ€è¯­ä¹‰å¯¹åº”ï¼Œæœ¬æ–‡æ–¹æ³•é€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥åˆ©ç”¨è·¨æ¨¡æ€å’Œå†…æ¨¡æ€å¯¹åº”å…³ç³»ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•åœ¨é¢„è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µæ‰©å±•äº†T2I GANsè‡³ä¸¤ä¸ªå‚æ•°å…±äº«å¯¹æ¯”åˆ†æ”¯ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆèšç±»äº†è¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒæ–‡æœ¬å¯¹è¡¨ç¤ºï¼Œæé«˜äº†å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>åœ¨å¤šç§æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæœ¬æ–‡æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†å›¾åƒç”Ÿæˆçš„FIDæŒ‡æ ‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e4fa8e5002aff79fa0f993cd8f600d8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c977d0c995d40a5daed4d2bf1c843d5.jpg" align="middle">
</details>




<h2 id="NODE-AdvGAN-Improving-the-transferability-and-perceptual-similarity-of-adversarial-examples-by-dynamic-system-driven-adversarial-generative-model"><a href="#NODE-AdvGAN-Improving-the-transferability-and-perceptual-similarity-of-adversarial-examples-by-dynamic-system-driven-adversarial-generative-model" class="headerlink" title="NODE-AdvGAN: Improving the transferability and perceptual similarity of   adversarial examples by dynamic-system-driven adversarial generative model"></a>NODE-AdvGAN: Improving the transferability and perceptual similarity of   adversarial examples by dynamic-system-driven adversarial generative model</h2><p><strong>Authors:Xinheng Xie, Yue Wu, Cuiyu He</strong></p>
<p>Understanding adversarial examples is crucial for improving the modelâ€™s robustness, as they introduce imperceptible perturbations that deceive models. Effective adversarial examples, therefore, offer the potential to train more robust models by removing their singularities. We propose NODE-AdvGAN, a novel approach that treats adversarial generation as a continuous process and employs a Neural Ordinary Differential Equation (NODE) for simulating the dynamics of the generator. By mimicking the iterative nature of traditional gradient-based methods, NODE-AdvGAN generates smoother and more precise perturbations that preserve high perceptual similarity when added to benign images. We also propose a new training strategy, NODE-AdvGAN-T, which enhances transferability in black-box attacks by effectively tuning noise parameters during training. Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more effective adversarial examples that achieve higher attack success rates while preserving better perceptual quality than traditional GAN-based methods. </p>
<blockquote>
<p>ç†è§£å¯¹æŠ—æ ·æœ¬å¯¹äºæé«˜æ¨¡å‹çš„é²æ£’æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¼•å…¥äº†éš¾ä»¥å¯Ÿè§‰çš„æ‰°åŠ¨æ¥æ¬ºéª—æ¨¡å‹ã€‚å› æ­¤ï¼Œæœ‰æ•ˆçš„å¯¹æŠ—æ ·æœ¬å¯ä»¥é€šè¿‡æ¶ˆé™¤å…¶å¥‡å¼‚æ€§æ¥è®­ç»ƒæ›´ç¨³å¥çš„æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†NODE-AdvGANï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹æŠ—ç”Ÿæˆè§†ä¸ºè¿ç»­è¿‡ç¨‹çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNODEï¼‰æ¥æ¨¡æ‹Ÿç”Ÿæˆå™¨çš„åŠ¨æ€ã€‚é€šè¿‡æ¨¡ä»¿ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•çš„è¿­ä»£æ€§è´¨ï¼ŒNODE-AdvGANç”Ÿæˆæ›´å¹³æ»‘ã€æ›´ç²¾ç¡®çš„æ‰°åŠ¨ï¼Œåœ¨æ·»åŠ åˆ°è‰¯æ€§å›¾åƒæ—¶ä¿æŒé«˜åº¦æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥NODE-AdvGAN-Tï¼Œå®ƒé€šè¿‡æœ‰æ•ˆè°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„å™ªå£°å‚æ•°ï¼Œæé«˜äº†åœ¨é»‘ç›’æ”»å‡»ä¸­çš„å¯è¿ç§»æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒNODE-AdvGANå’ŒNODE-AdvGAN-Tç”Ÿæˆæ›´æœ‰æ•ˆçš„å¯¹æŠ—æ ·æœ¬ï¼Œåœ¨ä¿æŒæ¯”ä¼ ç»ŸåŸºäºGANçš„æ–¹æ³•æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03539v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¯¹æŠ—æ€§æ ·æœ¬çš„ç†è§£å¯¹äºæé«˜æ¨¡å‹çš„ç¨³å¥æ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¼•å…¥äº†éš¾ä»¥å¯Ÿè§‰çš„æ‰°åŠ¨æ¥æ¬ºéª—æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹çš„å¯¹æŠ—ç”Ÿæˆæ–¹æ³•NODE-AdvGANï¼Œå°†å¯¹æŠ—ç”Ÿæˆè§†ä¸ºä¸€ä¸ªè¿ç»­è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNODEï¼‰æ¥æ¨¡æ‹Ÿç”Ÿæˆå™¨çš„åŠ¨æ€ã€‚é€šè¿‡æ¨¡ä»¿ä¼ ç»Ÿçš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•çš„è¿­ä»£æ€§è´¨ï¼ŒNODE-AdvGANç”Ÿæˆæ›´å¹³æ»‘ã€æ›´ç²¾ç¡®çš„æ‰°åŠ¨ï¼Œåœ¨æ·»åŠ åˆ°è‰¯æ€§å›¾åƒæ—¶ä¿æŒé«˜åº¦æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç­–ç•¥NODE-AdvGAN-Tï¼Œé€šè¿‡æœ‰æ•ˆè°ƒæ•´è®­ç»ƒè¿‡ç¨‹ä¸­çš„å™ªå£°å‚æ•°ï¼Œæé«˜äº†åœ¨é»‘ç›’æ”»å‡»ä¸­çš„è¿ç§»æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒNODE-AdvGANå’ŒNODE-AdvGAN-Tç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬æ”»å‡»æˆåŠŸç‡æ›´é«˜ï¼ŒåŒæ—¶ä¿æŒäº†æ¯”ä¼ ç»ŸåŸºäºGANçš„æ–¹æ³•æ›´å¥½çš„æ„ŸçŸ¥è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ€§æ ·æœ¬å¯¹æ¨¡å‹ç¨³å¥æ€§è‡³å…³é‡è¦ï¼Œèƒ½æ­ç¤ºæ¨¡å‹æ¼æ´ã€‚</li>
<li>NODE-AdvGANä½¿ç”¨ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆNODEï¼‰æ¨¡æ‹Ÿç”Ÿæˆå™¨çš„åŠ¨æ€ï¼Œå°†å¯¹æŠ—ç”Ÿæˆè§†ä¸ºè¿ç»­è¿‡ç¨‹ã€‚</li>
<li>NODE-AdvGANç”Ÿæˆçš„æ‰°åŠ¨æ›´å¹³æ»‘ã€ç²¾ç¡®ï¼Œä¿æŒé«˜åº¦æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚</li>
<li>æå‡ºäº†æ–°çš„è®­ç»ƒç­–ç•¥NODE-AdvGAN-Tï¼Œæé«˜åœ¨é»‘ç›’æ”»å‡»ä¸­çš„è¿ç§»æ€§ã€‚</li>
<li>é€šè¿‡æœ‰æ•ˆè°ƒæ•´å™ªå£°å‚æ•°ï¼Œå¢å¼ºæ¨¡å‹çš„æ”»å‡»æ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒNODE-AdvGANå’ŒNODE-AdvGAN-Tç”Ÿæˆçš„å¯¹æŠ—æ ·æœ¬æ”»å‡»æˆåŠŸç‡æ›´é«˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d658ef3fbb89fbd0ff64f4f820fa8212.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dcda4b273886c7efbf52b5cbf494998.jpg" align="middle">
</details>




<h2 id="Scaling-Image-Tokenizers-with-Grouped-Spherical-Quantization"><a href="#Scaling-Image-Tokenizers-with-Grouped-Spherical-Quantization" class="headerlink" title="Scaling Image Tokenizers with Grouped Spherical Quantization"></a>Scaling Image Tokenizers with Grouped Spherical Quantization</h2><p><strong>Authors:Jiangtao Wang, Zhen Qin, Yifan Zhang, Vincent Tao Hu, BjÃ¶rn Ommer, Rania Briq, Stefan Kesselheim</strong></p>
<p>Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50. </p>
<blockquote>
<p>ç”±äºè§†è§‰ä»¤ç‰ŒåŒ–å™¨çš„å¯æ‰©å±•æ€§å’Œç´§å‡‘æ€§ï¼Œå®ƒä»¬å·²ç»å¸å¼•äº†å¤§é‡å…³æ³¨ã€‚ä¹‹å‰çš„ç ”ç©¶ä¾èµ–äºæ—§å¼çš„åŸºäºGANçš„è¶…å‚æ•°ã€æœ‰åæ¯”è¾ƒï¼Œä»¥åŠå¯¹ç¼©æ”¾è¡Œä¸ºç¼ºä¹ç»¼åˆåˆ†æã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆ†ç»„çƒé¢é‡åŒ–ï¼ˆGSQï¼‰ï¼Œå®ƒé‡‡ç”¨çƒé¢ä»£ç æœ¬åˆå§‹åŒ–å¹¶æŸ¥æ‰¾æ­£åˆ™åŒ–æ¥çº¦æŸä»£ç æœ¬æ½œåœ¨ç©ºé—´åˆ°çƒé¢ã€‚æˆ‘ä»¬å¯¹å›¾åƒä»¤ç‰ŒåŒ–å™¨è®­ç»ƒç­–ç•¥çš„å®è¯åˆ†æè¡¨æ˜ï¼ŒGSQ-GANåœ¨è¾ƒå°‘çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°å†…å®ç°äº†ä¼˜äºæœ€æ–°æŠ€æœ¯çš„é‡å»ºè´¨é‡ï¼Œä¸ºç¼©æ”¾ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†GSQçš„ç¼©æ”¾è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯åœ¨æ½œåœ¨ç»´åº¦ã€ä»£ç æœ¬å¤§å°å’Œå‹ç¼©ç‡æ–¹é¢åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåœ¨é«˜ã€ä½ç©ºé—´å‹ç¼©çº§åˆ«ä¸‹è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºï¼Œå¼ºè°ƒäº†åœ¨é«˜ç»´æ½œåœ¨ç©ºé—´è¡¨ç¤ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒGSQå¯ä»¥é‡æ–°ç»„ç»‡é«˜ç»´æ½œåœ¨ç©ºé—´åˆ°ç´§å‡‘ã€ä½ç»´ç©ºé—´ï¼Œä»è€Œå®ç°é«˜æ•ˆç¼©æ”¾å¹¶æé«˜äº†è´¨é‡ã€‚å› æ­¤ï¼ŒGSQ-GANå®ç°äº†16å€ä¸‹é‡‡æ ·ï¼Œé‡å»ºFIDï¼ˆrFIDï¼‰ä¸º0.50ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02632v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†Grouped Spherical Quantizationï¼ˆGSQï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é‡‡ç”¨çƒå½¢ç¼–ç æœ¬åˆå§‹åŒ–å’ŒæŸ¥æ‰¾æ­£åˆ™åŒ–æ¥çº¦æŸç¼–ç æœ¬æ½œåœ¨ç©ºé—´ä¸ºçƒå½¢è¡¨é¢ã€‚å®è¯åˆ†æè¡¨æ˜ï¼ŒGSQ-GANåœ¨å›¾åƒä»¤ç‰ŒåŒ–è®­ç»ƒç­–ç•¥ä¸Šå®ç°äº†è¾ƒé«˜çš„é‡å»ºè´¨é‡ï¼Œå…·æœ‰è¾ƒå°‘çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç³»ç»Ÿç ”ç©¶äº†GSQåœ¨æ½œåœ¨ç»´åº¦ã€ç¼–ç æœ¬å¤§å°å’Œå‹ç¼©ç‡æ–¹é¢çš„æ‰©å±•è¡Œä¸ºåŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼ŒGSQèƒ½å¤Ÿé‡ç»„é«˜ç»´æ½œåœ¨ç©ºé—´ä¸ºç´§å‡‘çš„ä½ç»´ç©ºé—´ï¼Œä»è€Œå®ç°é«˜æ•ˆæ‰©å±•å¹¶æ”¹å–„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GSQæŠ€æœ¯å¼•å…¥çƒå½¢ç¼–ç æœ¬åˆå§‹åŒ–å’ŒæŸ¥æ‰¾æ­£åˆ™åŒ–ï¼Œæœ‰æ•ˆçº¦æŸç¼–ç æœ¬æ½œåœ¨ç©ºé—´ã€‚</li>
<li>GSQ-GANåœ¨å›¾åƒä»¤ç‰ŒåŒ–è®­ç»ƒç­–ç•¥ä¸Šå®ç°é«˜è´¨é‡é‡å»ºï¼Œå‡å°‘è®­ç»ƒè¿­ä»£æ¬¡æ•°ã€‚</li>
<li>ç³»ç»Ÿç ”ç©¶äº†GSQåœ¨æ½œåœ¨ç»´åº¦ã€ç¼–ç æœ¬å¤§å°å’Œå‹ç¼©ç‡æ–¹é¢çš„æ‰©å±•è¡Œä¸ºã€‚</li>
<li>é«˜ã€ä½ç©ºé—´å‹ç¼©çº§åˆ«ä¸‹è¡¨ç°å‡ºä¸åŒè¡Œä¸ºï¼ŒæŒ‘æˆ˜é«˜ç»´æ½œåœ¨ç©ºé—´çš„è¡¨ç¤ºã€‚</li>
<li>GSQèƒ½å¤Ÿé‡ç»„é«˜ç»´æ½œåœ¨ç©ºé—´ä¸ºä½ç»´ç©ºé—´ï¼Œå®ç°é«˜æ•ˆæ‰©å±•ã€‚</li>
<li>GSQ-GANå®ç°16å€ä¸‹é‡‡æ ·ï¼Œé‡å»ºFIDï¼ˆrFIDï¼‰ä¸º0.50ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-79c45b09c70486435bacfa9527286889.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e692638fe409d36b91aa88893b4aaae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd2dfe7581bee0dd9924bb5ea7c1991a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed06db7ea7b84428903b234fe22f493e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdafea3b6bc15bc9ec3310352308889a.jpg" align="middle">
</details>




<h2 id="Continual-Learning-of-Personalized-Generative-Face-Models-with-Experience-Replay"><a href="#Continual-Learning-of-Personalized-Generative-Face-Models-with-Experience-Replay" class="headerlink" title="Continual Learning of Personalized Generative Face Models with   Experience Replay"></a>Continual Learning of Personalized Generative Face Models with   Experience Replay</h2><p><strong>Authors:Annie N. Wang, Luchao Qi, Roni Sengupta</strong></p>
<p>We introduce a novel continual learning problem: how to sequentially update the weights of a personalized 2D and 3D generative face model as new batches of photos in different appearances, styles, poses, and lighting are captured regularly. We observe that naive sequential fine-tuning of the model leads to catastrophic forgetting of past representations of the individualâ€™s face. We then demonstrate that a simple random sampling-based experience replay method is effective at mitigating catastrophic forgetting when a relatively large number of images can be stored and replayed. However, for long-term deployment of these models with relatively smaller storage, this simple random sampling-based replay technique also forgets past representations. Thus, we introduce a novel experience replay algorithm that combines random sampling with StyleGANâ€™s latent space to represent the buffer as an optimal convex hull. We observe that our proposed convex hull-based experience replay is more effective in preventing forgetting than a random sampling baseline and the lower bound. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„æŒç»­å­¦ä¹ é—®é¢˜ï¼šå¦‚ä½•æŒ‰é¡ºåºæ›´æ–°ä¸ªæ€§åŒ–2Då’Œ3Dç”Ÿæˆé¢éƒ¨æ¨¡å‹çš„æƒé‡ï¼Œä»¥é€‚åº”å®šæœŸæ•è·çš„ä¸åŒå¤–è§‚ã€é£æ ¼ã€å§¿åŠ¿å’Œå…‰ç…§æ¡ä»¶çš„æ–°æ‰¹æ¬¡ç…§ç‰‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¨¡å‹ç®€å•çš„é¡ºåºå¾®è°ƒä¼šå¯¼è‡´ä¸ªäººé¢éƒ¨è¿‡å»è¡¨å¾çš„ç¾éš¾æ€§é—å¿˜ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸€ç§åŸºäºç®€å•éšæœºé‡‡æ ·çš„ç»éªŒå›æ”¾æ–¹æ³•èƒ½å¤Ÿåœ¨ç›¸å¯¹è¾ƒå¤§çš„å›¾åƒå­˜å‚¨å’Œé‡æ’­æ—¶æœ‰æ•ˆå‡è½»ç¾éš¾æ€§é—å¿˜ã€‚ä½†å¯¹äºç›¸å¯¹è¾ƒå°çš„å­˜å‚¨é•¿æœŸéƒ¨ç½²è¿™äº›æ¨¡å‹æ—¶ï¼Œè¿™ç§ç®€å•çš„åŸºäºéšæœºé‡‡æ ·çš„å›æ”¾æŠ€æœ¯ä¹Ÿä¼šå¿˜è®°è¿‡å»çš„è¡¨å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç»éªŒå›æ”¾ç®—æ³•ï¼Œå®ƒå°†éšæœºé‡‡æ ·ä¸StyleGANçš„æ½œåœ¨ç©ºé—´ç›¸ç»“åˆï¼Œå°†ç¼“å†²åŒºè¡¨ç¤ºä¸ºæœ€ä¼˜å‡¸åŒ…ã€‚æˆ‘ä»¬å‘ç°ï¼ŒåŸºäºå‡¸åŒ…çš„ç»éªŒå›æ”¾æ¯”éšæœºé‡‡æ ·åŸºçº¿æ›´èƒ½æœ‰æ•ˆåœ°é˜²æ­¢é—å¿˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02627v1">PDF</a> Accepted to WACV 2025. Project page (incl. supplementary materials):   <a target="_blank" rel="noopener" href="https://anniedde.github.io/personalizedcontinuallearning.github.io/">https://anniedde.github.io/personalizedcontinuallearning.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªæ–°å‹æŒç»­å­¦ä¹ é—®é¢˜ï¼šå¦‚ä½•æŒ‰é¡ºåºæ›´æ–°ä¸ªæ€§åŒ–2Då’Œ3Dç”Ÿæˆé¢éƒ¨æ¨¡å‹çš„æƒé‡ï¼Œä»¥åº”å¯¹ä¸æ–­æ•æ‰åˆ°çš„é¢éƒ¨æ–°ç…§ç‰‡æ‰¹æ¬¡çš„ä¸åŒå¤–è§‚ã€é£æ ¼ã€å§¿åŠ¿å’Œå…‰ç…§æ¡ä»¶ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ç®€å•çš„æ¨¡å‹é¡ºåºå¾®è°ƒä¼šå¯¼è‡´å¯¹ä¸ªä½“é¢éƒ¨è¿‡å»è¡¨ç¤ºçš„ç¾éš¾æ€§é—å¿˜ã€‚é€šè¿‡å±•ç¤ºä¸€ä¸ªç®€å•çš„åŸºäºéšæœºé‡‡æ ·çš„ç»éªŒå›æ”¾æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå‰ææ˜¯èƒ½å¤Ÿå­˜å‚¨å’Œå›æ”¾å¤§é‡çš„å›¾åƒã€‚ä½†å¯¹äºéœ€è¦é•¿æœŸéƒ¨ç½²çš„æ¨¡å‹æ¥è¯´ï¼Œå­˜å‚¨ç›¸å¯¹æœ‰é™ï¼Œå› æ­¤è¿™ç§ç®€å•çš„éšæœºé‡‡æ ·å›æ”¾æŠ€æœ¯ä¹Ÿä¼šé—å¿˜è¿‡å»çš„è¡¨ç¤ºã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç»éªŒå›æ”¾ç®—æ³•ï¼Œç»“åˆäº†éšæœºé‡‡æ ·ä¸StyleGANçš„æ½œåœ¨ç©ºé—´ï¼Œå°†ç¼“å†²åŒºè¡¨ç¤ºä¸ºæœ€ä¼˜å‡¸åŒ…ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºå‡¸åŒ…çš„ç»éªŒå›æ”¾æ–¹æ³•æ¯”éšæœºé‡‡æ ·åŸºçº¿æœ‰æ›´å¥½çš„é˜²æ­¢é—å¿˜æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹æŒç»­å­¦ä¹ é—®é¢˜ï¼šå¦‚ä½•æ›´æ–°ä¸ªæ€§åŒ–ç”Ÿæˆé¢éƒ¨æ¨¡å‹çš„æƒé‡ä»¥åº”å¯¹æ–°ç…§ç‰‡çš„ä¸åŒæ¡ä»¶ã€‚</li>
<li>è§‚å¯Ÿåˆ°ç®€å•æ¨¡å‹é¡ºåºå¾®è°ƒä¼šå¯¼è‡´ä¸ªä½“é¢éƒ¨è¡¨ç¤ºçš„ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºéšæœºé‡‡æ ·çš„ç»éªŒå›æ”¾æ–¹æ³•ï¼Œåœ¨å­˜å‚¨å……è¶³æ—¶èƒ½æœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>å¯¹äºå­˜å‚¨æœ‰é™çš„é•¿æœŸéƒ¨ç½²åœºæ™¯ï¼Œç®€å•çš„éšæœºé‡‡æ ·å›æ”¾æŠ€æœ¯ä¼šé—å¿˜è¿‡å»çš„è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç»éªŒå›æ”¾ç®—æ³•ï¼Œç»“åˆäº†éšæœºé‡‡æ ·ä¸StyleGANçš„æ½œåœ¨ç©ºé—´ï¼Œå°†ç¼“å†²åŒºè¡¨ç¤ºä¸ºæœ€ä¼˜å‡¸åŒ…ã€‚</li>
<li>åŸºäºå‡¸åŒ…çš„ç»éªŒå›æ”¾æ–¹æ³•æ¯”éšæœºé‡‡æ ·åŸºçº¿æ›´æœ‰æ•ˆåœ°é˜²æ­¢é—å¿˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-010a3605fa0fb662d7f169953c14e8e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75fad8bcbfd0572ccafb35fa6b98c761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1851727f53b9ff7de67769b31cd594b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b940e8f681af97a3355339f9207317b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e31cdc1d48f96c2aca3efab29751871c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fcfa5d978373b63efcb1ff8b8ab16c4.jpg" align="middle">
</details>




<h2 id="WEM-GAN-Wavelet-transform-based-facial-expression-manipulation"><a href="#WEM-GAN-Wavelet-transform-based-facial-expression-manipulation" class="headerlink" title="WEM-GAN: Wavelet transform based facial expression manipulation"></a>WEM-GAN: Wavelet transform based facial expression manipulation</h2><p><strong>Authors:Dongya Sun, Yunfei Hu, Xianzhe Zhang, Yingsong Hu</strong></p>
<p>Facial expression manipulation aims to change human facial expressions without affecting face recognition. In order to transform the facial expressions to target expressions, previous methods relied on expression labels to guide the manipulation process. However, these methods failed to preserve the details of facial features, which causes the weakening or the loss of identity information in the output image. In our work, we propose WEM-GAN, in short for wavelet-based expression manipulation GAN, which puts more efforts on preserving the details of the original image in the editing process. Firstly, we take advantage of the wavelet transform technique and combine it with our generator with a U-net autoencoder backbone, in order to improve the generatorâ€™s ability to preserve more details of facial features. Secondly, we also implement the high-frequency component discriminator, and use high-frequency domain adversarial loss to further constrain the optimization of our model, providing the generated face image with more abundant details. Additionally, in order to narrow the gap between generated facial expressions and target expressions, we use residual connections between encoder and decoder, while also using relative action units (AUs) several times. Extensive qualitative and quantitative experiments have demonstrated that our model performs better in preserving identity features, editing capability, and image generation quality on the AffectNet dataset. It also shows superior performance in metrics such as Average Content Distance (ACD) and Expression Distance (ED). </p>
<blockquote>
<p>é¢éƒ¨è¡¨æƒ…æ“çºµæ—¨åœ¨æ”¹å˜äººç±»é¢éƒ¨è¡¨æƒ…è€Œä¸å½±å“é¢éƒ¨è¯†åˆ«ã€‚ä¸ºäº†å°†é¢éƒ¨è¡¨æƒ…è½¬å˜ä¸ºç›®æ ‡è¡¨æƒ…ï¼Œä¹‹å‰çš„æ–¹æ³•ä¾èµ–äºè¡¨æƒ…æ ‡ç­¾æ¥æŒ‡å¯¼æ“çºµè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœªèƒ½ä¿ç•™é¢éƒ¨ç‰¹å¾çš„ç»†èŠ‚ï¼Œå¯¼è‡´è¾“å‡ºå›¾åƒä¸­çš„èº«ä»½ä¿¡æ¯å‡å¼±æˆ–ä¸¢å¤±ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WEM-GANï¼Œå³åŸºäºå°æ³¢çš„è¡¨æƒ…æ“çºµGANçš„ç®€ç§°ï¼Œå®ƒåœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­æ›´åŠ æ³¨é‡ä¿ç•™åŸå§‹å›¾åƒçš„ç»†èŠ‚ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å°æ³¢å˜æ¢æŠ€æœ¯ä¸æˆ‘ä»¬çš„ç”Ÿæˆå™¨ç›¸ç»“åˆï¼Œç”Ÿæˆå™¨é‡‡ç”¨U-netè‡ªç¼–ç å™¨éª¨å¹²ç½‘ï¼Œä»¥æé«˜ç”Ÿæˆå™¨ä¿ç•™é¢éƒ¨ç‰¹å¾ç»†èŠ‚çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†é«˜é¢‘åˆ†é‡é‰´åˆ«å™¨ï¼Œå¹¶ä½¿ç”¨é«˜é¢‘åŸŸå¯¹æŠ—æŸå¤±æ¥è¿›ä¸€æ­¥çº¦æŸæˆ‘ä»¬æ¨¡å‹çš„ä¼˜åŒ–ï¼Œä¸ºç”Ÿæˆçš„é¢éƒ¨å›¾åƒæä¾›æ›´ä¸°å¯Œçš„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¼©å°ç”Ÿæˆé¢éƒ¨è¡¨æƒ…ä¸ç›®æ ‡è¡¨æƒ…ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´ä½¿ç”¨æ®‹å·®è¿æ¥ï¼ŒåŒæ—¶ä½¿ç”¨ç›¸å¯¹åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰å¤šæ¬¡ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨èº«ä»½ç‰¹å¾ä¿ç•™ã€ç¼–è¾‘èƒ½åŠ›å’Œå›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢ï¼Œåœ¨AffectNetæ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´å¥½ã€‚å®ƒåœ¨å¹³å‡å†…å®¹è·ç¦»ï¼ˆACDï¼‰å’Œè¡¨æƒ…è·ç¦»ï¼ˆEDï¼‰ç­‰æŒ‡æ ‡ä¸Šä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02530v1">PDF</a> </p>
<p><strong>Summary</strong><br>é¢éƒ¨è¡¨æƒ…æ“çºµæ—¨åœ¨æ”¹å˜äººç±»é¢éƒ¨è¡¨æƒ…è€Œä¸å½±å“é¢éƒ¨è¯†åˆ«ã€‚ä»¥å¾€çš„æ–¹æ³•ä¾èµ–äºè¡¨æƒ…æ ‡ç­¾æ¥æŒ‡å¯¼æ“çºµè¿‡ç¨‹ï¼Œä½†æœªèƒ½ä¿ç•™é¢éƒ¨ç‰¹å¾çš„ç»†èŠ‚ï¼Œå¯¼è‡´è¾“å‡ºå›¾åƒä¸­çš„èº«ä»½ä¿¡æ¯å‡å¼±æˆ–ä¸¢å¤±ã€‚æˆ‘ä»¬æå‡ºåŸºäºå°æ³¢çš„é¢éƒ¨è¡¨æƒ…æ“çºµGANï¼ˆWEM-GANï¼‰ï¼Œæ³¨é‡åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿ç•™åŸå§‹å›¾åƒçš„ç»†èŠ‚ã€‚å€ŸåŠ©å°æ³¢å˜æ¢æŠ€æœ¯ä¸å¸¦æœ‰U-netè‡ªç¼–ç å™¨éª¨æ¶çš„ç”Ÿæˆå™¨ç›¸ç»“åˆï¼Œæé«˜ç”Ÿæˆå™¨ä¿ç•™é¢éƒ¨ç‰¹å¾ç»†èŠ‚çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†é«˜é¢‘åˆ†é‡é‰´åˆ«å™¨å¹¶ä½¿ç”¨é«˜é¢‘åŸŸå¯¹æŠ—æŸå¤±è¿›ä¸€æ­¥çº¦æŸæ¨¡å‹çš„ä¼˜åŒ–ï¼Œä¸ºç”Ÿæˆçš„é¢éƒ¨å›¾åƒæä¾›æ›´ä¸°å¯Œçš„ç»†èŠ‚ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨èº«ä»½ç‰¹å¾ä¿ç•™ã€ç¼–è¾‘èƒ½åŠ›å’Œå›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°æ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨è¡¨æƒ…æ“çºµæ—¨åœ¨ä¸æ”¹å˜é¢éƒ¨è¯†åˆ«çš„æƒ…å†µä¸‹è°ƒæ•´äººç±»é¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>æ—©æœŸæ–¹æ³•ä¾èµ–è¡¨æƒ…æ ‡ç­¾å¼•å¯¼æ“çºµè¿‡ç¨‹ï¼Œä½†ä¼šä¸¢å¤±é¢éƒ¨ç‰¹å¾çš„ç»†èŠ‚ï¼Œå½±å“èº«ä»½ä¿¡æ¯çš„ä¿ç•™ã€‚</li>
<li>WEM-GANåŸºäºå°æ³¢å˜æ¢æŠ€æœ¯ï¼Œæ—¨åœ¨ä¿ç•™åŸå§‹å›¾åƒç»†èŠ‚ï¼Œæé«˜ç”Ÿæˆå™¨æ€§èƒ½ã€‚</li>
<li>é«˜é¢‘åˆ†é‡é‰´åˆ«å™¨å’Œé«˜é¢‘åŸŸå¯¹æŠ—æŸå¤±ç”¨äºä¸°å¯Œç”Ÿæˆçš„é¢éƒ¨å›¾åƒç»†èŠ‚ã€‚</li>
<li>é€šè¿‡æ®‹å·®è¿æ¥å’Œç›¸å¯¹åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰çš„å¤šæ¬¡ä½¿ç”¨ï¼Œç¼©å°ç”Ÿæˆé¢éƒ¨è¡¨æƒ…ä¸ç›®æ ‡è¡¨æƒ…ä¹‹é—´çš„å·®è·ã€‚</li>
<li>åœ¨AffectNetæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜ï¼ŒWEM-GANåœ¨ä¿ç•™èº«ä»½ç‰¹å¾ã€ç¼–è¾‘èƒ½åŠ›å’Œå›¾åƒç”Ÿæˆè´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc91186b1cd59b2106446369ee170a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64b98b9883b90784c94096808f760d4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e52cc34059de10459bbecb0aca09102.jpg" align="middle">
</details>




<h2 id="CLERF-Contrastive-LEaRning-for-Full-Range-Head-Pose-Estimation"><a href="#CLERF-Contrastive-LEaRning-for-Full-Range-Head-Pose-Estimation" class="headerlink" title="CLERF: Contrastive LEaRning for Full Range Head Pose Estimation"></a>CLERF: Contrastive LEaRning for Full Range Head Pose Estimation</h2><p><strong>Authors:Ting-Ruen Wei, Haowei Liu, Huei-Chung Hu, Xuyang Wu, Yi Fang, Hsin-Tai Wu</strong></p>
<p>We introduce a novel framework for representation learning in head pose estimation (HPE). Previously such a scheme was difficult due to head pose data sparsity, making triplet sampling infeasible. Recent progress in 3D generative adversarial networks (3D-aware GAN) has opened the door for easily sampling triplets (anchor, positive, negative). We perform contrastive learning on extensively augmented data including geometric transformations and demonstrate that contrastive learning allows networks to learn genuine features that contribute to accurate HPE. On the other hand, we observe that existing HPE works struggle to predict head poses as accurately when test image rotation matrices are slightly out of the training dataset distribution. Experiments show that our methodology performs on par with state-of-the-art models on standard test datasets and outperforms them when images are slightly rotated&#x2F; flipped or full range head pose. To the best of our knowledge, we are the first to deliver a true full range HPE model capable of accurately predicting any head pose including upside-down pose. Furthermore, we compared with other existing full-yaw range models and demonstrated superior results. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å¤´éƒ¨å§¿æ€ä¼°è®¡ï¼ˆHPEï¼‰è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚ä¹‹å‰ç”±äºå¤´éƒ¨å§¿æ€æ•°æ®ç¨€ç–ï¼Œä½¿å¾—ä¸‰å…ƒç»„é‡‡æ ·å˜å¾—ä¸å¯è¡Œï¼Œå› æ­¤è¿™æ ·çš„æ–¹æ¡ˆå¾ˆéš¾å®ç°ã€‚æœ€è¿‘ï¼Œåœ¨ä¸‰ç»´ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆå…·æœ‰ä¸‰ç»´æ„ŸçŸ¥èƒ½åŠ›çš„GANï¼‰æ–¹é¢çš„è¿›å±•ä¸ºè½»æ¾é‡‡æ ·ä¸‰å…ƒç»„ï¼ˆé”šç‚¹ã€æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬ï¼‰æ‰“å¼€äº†å¤§é—¨ã€‚æˆ‘ä»¬å¯¹å¤§é‡æ‰©å……çš„æ•°æ®æ‰§è¡Œå¯¹æ¯”å­¦ä¹ ï¼ŒåŒ…æ‹¬å‡ ä½•å˜æ¢ï¼Œå¹¶è¯æ˜å¯¹æ¯”å­¦ä¹ å¯ä»¥ä½¿ç½‘ç»œå­¦ä¹ æœ‰åŠ©äºå‡†ç¡®HPEçš„çœŸå®ç‰¹å¾ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰çš„HPEå·¥ä½œåœ¨æµ‹è¯•å›¾åƒæ—‹è½¬çŸ©é˜µç•¥è¶…å‡ºè®­ç»ƒæ•°æ®é›†åˆ†å¸ƒæ—¶ï¼Œéš¾ä»¥å‡†ç¡®é¢„æµ‹å¤´éƒ¨å§¿æ€ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼Œåœ¨å›¾åƒç¨å¾®æ—‹è½¬ã€ç¿»è½¬æˆ–å…¨èŒƒå›´å¤´éƒ¨å§¿æ€çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºå®ƒä»¬ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæä¾›èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹åŒ…æ‹¬å€’ç«‹å§¿åŠ¿åœ¨å†…çš„ä»»ä½•å¤´éƒ¨å§¿æ€çš„çœŸæ­£å…¨èŒƒå›´HPEæ¨¡å‹çš„å›¢é˜Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸å…¶ä»–ç°æœ‰çš„å…¨åèˆªèŒƒå›´æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å–å¾—äº†æ›´å¥½çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02066v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤´éƒ¨å§¿æ€ä¼°è®¡ï¼ˆHPEï¼‰çš„æ–°å‹è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚å€ŸåŠ©æœ€è¿‘3Dç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆ3D-aware GANï¼‰çš„è¿›æ­¥ï¼Œå®ç°äº†è½»æ¾é‡‡æ ·ä¸‰å…ƒç»„ï¼ˆé”šç‚¹ã€æ­£é¢ã€è´Ÿé¢ï¼‰ï¼Œå¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ åœ¨å¹¿æ³›æ‰©å……çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬å‡ ä½•å˜æ¢ã€‚å¯¹æ¯”å­¦ä¹ ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ çœŸå®ç‰¹å¾ï¼Œæœ‰åŠ©äºå®ç°å‡†ç¡®çš„HPEã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è¶…å‡ºè®­ç»ƒæ•°æ®é›†åˆ†å¸ƒèŒƒå›´çš„å›¾åƒæ—‹è½¬çŸ©é˜µä¸Šå‡†ç¡®é¢„æµ‹å¤´éƒ¨å§¿æ€ï¼Œæ˜¯ç›®å‰é¦–ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹åŒ…æ‹¬å€’ç«‹å§¿æ€åœ¨å†…çš„ä»»ä½•å¤´éƒ¨å§¿æ€çš„å…¨èŒƒå›´HPEæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹å¤´éƒ¨å§¿æ€ä¼°è®¡ï¼ˆHPEï¼‰çš„è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ã€‚</li>
<li>åˆ©ç”¨å¯¹æ¯”å­¦ä¹ åœ¨æ‰©å……çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨3Dç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆ3D-aware GANï¼‰å®ç°è½»æ¾é‡‡æ ·ä¸‰å…ƒç»„ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨å›¾åƒè½»å¾®æ—‹è½¬ã€ç¿»è½¬æˆ–å…¨èŒƒå›´å¤´éƒ¨å§¿æ€çš„æƒ…å†µä¸‹è¿›è¡Œå‡†ç¡®é¢„æµ‹ã€‚</li>
<li>æ¨¡å‹æ˜¯é¦–ä¸ªèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ä»»ä½•å¤´éƒ¨å§¿æ€çš„å…¨èŒƒå›´HPEæ¨¡å‹ï¼ŒåŒ…æ‹¬å€’ç«‹å§¿æ€ã€‚</li>
<li>ä¸å…¶ä»–å…¨åèˆªèŒƒå›´æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºä¼˜è¶Šçš„ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ae8cd046d5eebe9f67f1272bb96291b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c520c5d3e68202129f2729b8239c004.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a626ebfd766afbc67cd86730cec0fd7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-128d6765a6e9c2212ec173d001df8b0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f230b8fdc921ec2858ec99aa113bf3c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5177209accec95eaba96e4368da228cd.jpg" align="middle">
</details>




<h2 id="XQ-GAN-An-Open-source-Image-Tokenization-Framework-for-Autoregressive-Generation"><a href="#XQ-GAN-An-Open-source-Image-Tokenization-Framework-for-Autoregressive-Generation" class="headerlink" title="XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive   Generation"></a>XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive   Generation</h2><p><strong>Authors:Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Jindong Wang, Zhe Lin, Bhiksha Raj</strong></p>
<p>Image tokenizers play a critical role in shaping the performance of subsequent generative models. Since the introduction of VQ-GAN, discrete image tokenization has undergone remarkable advancements. Improvements in architecture, quantization techniques, and training recipes have significantly enhanced both image reconstruction and the downstream generation quality. In this paper, we present XQ-GAN, an image tokenization framework designed for both image reconstruction and generation tasks. Our framework integrates state-of-the-art quantization techniques, including vector quantization (VQ), residual quantization (RQ), multi-scale residual quantization (MSVQ), product quantization (PQ), lookup-free quantization (LFQ), and binary spherical quantization (BSQ), within a highly flexible and customizable training environment. On the standard ImageNet 256x256 benchmark, our released model achieves an rFID of 0.64, significantly surpassing MAGVIT-v2 (0.9 rFID) and VAR (0.9 rFID). Furthermore, we demonstrate that using XQ-GAN as a tokenizer improves gFID metrics alongside rFID. For instance, with the same VAR architecture, XQ-GAN+VAR achieves a gFID of 2.6, outperforming VARâ€™s 3.3 gFID by a notable margin. To support further research, we provide pre-trained weights of different image tokenizers for the community to directly train the subsequent generative models on it or fine-tune for specialized tasks. </p>
<blockquote>
<p>å›¾åƒä»¤ç‰ŒåŒ–å™¨åœ¨å¡‘é€ åç»­ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚è‡ªä»å¼•å…¥VQ-GANä»¥æ¥ï¼Œç¦»æ•£å›¾åƒä»¤ç‰ŒåŒ–å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æ¶æ„ã€é‡åŒ–æŠ€æœ¯å’Œè®­ç»ƒé…æ–¹çš„æ”¹è¿›æ˜¾è‘—æé«˜äº†å›¾åƒé‡å»ºå’Œä¸‹æ¸¸ç”Ÿæˆè´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†XQ-GANï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡è€Œè®¾è®¡çš„å›¾åƒä»¤ç‰ŒåŒ–æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ•´åˆäº†æœ€å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬çŸ¢é‡é‡åŒ–ï¼ˆVQï¼‰ã€æ®‹å·®é‡åŒ–ï¼ˆRQï¼‰ã€å¤šå°ºåº¦æ®‹å·®é‡åŒ–ï¼ˆMSVQï¼‰ã€äº§å“é‡åŒ–ï¼ˆPQï¼‰ã€æ— æŸ¥æ‰¾é‡åŒ–ï¼ˆLFQï¼‰å’ŒäºŒè¿›ä½çƒé¢é‡åŒ–ï¼ˆBSQï¼‰ï¼Œåœ¨ä¸€ä¸ªé«˜åº¦çµæ´»å’Œå¯å®šåˆ¶çš„è®­ç»ƒç¯å¢ƒä¸­ã€‚åœ¨æ ‡å‡†çš„ImageNet 256x256åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‘å¸ƒçš„æ¨¡å‹å®ç°äº†0.64çš„rFIDï¼Œæ˜¾è‘—è¶…è¶Šäº†MAGVIT-v2ï¼ˆ0.9 rFIDï¼‰å’ŒVARï¼ˆ0.9 rFIDï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ä½¿ç”¨XQ-GANä½œä¸ºä»¤ç‰ŒåŒ–å™¨å¯ä»¥æé«˜rFIDä»¥å¤–çš„gFIDæŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ç›¸åŒçš„VARæ¶æ„ï¼ŒXQ-GAN+VARå®ç°äº†2.6çš„gFIDï¼Œæ˜¾è‘—è¶…è¶Šäº†VARçš„3.3 gFIDã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä¸ºç¤¾åŒºæä¾›äº†ä¸åŒå›¾åƒä»¤ç‰Œå™¨çš„é¢„è®­ç»ƒæƒé‡ï¼Œå¯ä»¥ç›´æ¥åœ¨å…¶ä¸Šè®­ç»ƒåç»­çš„ç”Ÿæˆæ¨¡å‹ï¼Œæˆ–è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01762v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/lxa9867/ImageFolder">https://github.com/lxa9867/ImageFolder</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†XQ-GANå›¾åƒä»¤ç‰ŒåŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬å‘é‡é‡åŒ–ã€æ®‹å·®é‡åŒ–ç­‰ï¼Œç”¨äºå›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ã€‚åœ¨ImageNet 256x256æ ‡å‡†ä¸Šï¼ŒXQ-GANæ¨¡å‹å®ç°äº†rFID 0.64çš„é«˜æ€§èƒ½ï¼Œè¶…è¶Šäº†MAGVIT-v2å’ŒVARç­‰æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨XQ-GANä½œä¸ºä»¤ç‰ŒåŒ–å™¨å¯ä»¥æ”¹å–„gFIDæŒ‡æ ‡ã€‚è¿˜æä¾›é¢„è®­ç»ƒå›¾åƒä»¤ç‰ŒåŒ–å™¨æƒé‡ï¼Œä¾›ç¤¾åŒºç›´æ¥ä½¿ç”¨æˆ–å¾®è°ƒç‰¹å®šä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XQ-GANæ˜¯ä¸€ä¸ªå›¾åƒä»¤ç‰ŒåŒ–æ¡†æ¶ï¼Œç”¨äºå›¾åƒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†å¤šç§å…ˆè¿›çš„é‡åŒ–æŠ€æœ¯ï¼Œå¦‚å‘é‡é‡åŒ–ã€æ®‹å·®é‡åŒ–ç­‰ã€‚</li>
<li>åœ¨ImageNet 256x256æ ‡å‡†ä¸Šï¼ŒXQ-GANå®ç°äº†rFID 0.64çš„é«˜æ€§èƒ½ã€‚</li>
<li>XQ-GANæ˜¾è‘—è¶…è¶Šäº†MAGVIT-v2å’ŒVARç­‰æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨XQ-GANä½œä¸ºä»¤ç‰ŒåŒ–å™¨å¯ä»¥æ”¹å–„gFIDæŒ‡æ ‡ã€‚</li>
<li>XQ-GAN+VARç»„åˆå®ç°äº†ä¼˜äºVARçš„gFIDæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f9f6297bfc03fe418b34c21197abcb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0071e73d78d7683671181e0cbf8250ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2563134cd5768811d06b35c5988a9382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dd2046f28b0506c7b280b445bb74bf2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8be1789b3e1f9188123084d1cf6f15af.jpg" align="middle">
</details>




<h2 id="Multi-resolution-Guided-3D-GANs-for-Medical-Image-Translation"><a href="#Multi-resolution-Guided-3D-GANs-for-Medical-Image-Translation" class="headerlink" title="Multi-resolution Guided 3D GANs for Medical Image Translation"></a>Multi-resolution Guided 3D GANs for Medical Image Translation</h2><p><strong>Authors:Juhyung Ha, Jong Sung Park, David Crandall, Eleftherios Garyfallidis, Xuhong Zhang</strong></p>
<p>Medical image translation is the process of converting from one imaging modality to another, in order to reduce the need for multiple image acquisitions from the same patient. This can enhance the efficiency of treatment by reducing the time, equipment, and labor needed. In this paper, we introduce a multi-resolution guided Generative Adversarial Network (GAN)-based framework for 3D medical image translation. Our framework uses a 3D multi-resolution Dense-Attention UNet (3D-mDAUNet) as the generator and a 3D multi-resolution UNet as the discriminator, optimized with a unique combination of loss functions including voxel-wise GAN loss and 2.5D perception loss. Our approach yields promising results in volumetric image quality assessment (IQA) across a variety of imaging modalities, body regions, and age groups, demonstrating its robustness. Furthermore, we propose a synthetic-to-real applicability assessment as an additional evaluation to assess the effectiveness of synthetic data in downstream applications such as segmentation. This comprehensive evaluation shows that our method produces synthetic medical images not only of high-quality but also potentially useful in clinical applications. Our code is available at github.com&#x2F;juhha&#x2F;3D-mADUNet. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒç¿»è¯‘æ˜¯å°†ä¸€ç§æˆåƒæ¨¡å¼è½¬æ¢ä¸ºå¦ä¸€ç§æˆåƒæ¨¡å¼çš„è¿‡ç¨‹ï¼Œä»¥å‡å°‘å¯¹åŒä¸€æ‚£è€…å¤šæ¬¡é‡‡é›†å›¾åƒçš„éœ€æ±‚ã€‚è¿™å¯ä»¥é€šè¿‡å‡å°‘æ‰€éœ€çš„æ—¶é—´ã€è®¾å¤‡å’ŒåŠ³åŠ¨åŠ›æ¥æé«˜æ²»ç–—æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºå¤šåˆ†è¾¨ç‡å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„3DåŒ»å­¦å›¾åƒç¿»è¯‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨3Då¤šåˆ†è¾¨ç‡Dense-Attention UNetï¼ˆ3D-mDAUNetï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä½¿ç”¨3Då¤šåˆ†è¾¨ç‡UNetä½œä¸ºåˆ¤åˆ«å™¨ï¼Œé€šè¿‡åŒ…æ‹¬ä½“ç´ çº§GANæŸå¤±å’Œ2.5Dæ„ŸçŸ¥æŸå¤±çš„ç‹¬ç‰¹ç»„åˆçš„æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æˆåƒæ¨¡å¼ã€èº«ä½“åŒºåŸŸå’Œå¹´é¾„ç»„çš„ä½“ç§¯å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä¸­äº§ç”Ÿäº†æœ‰å‰æ™¯çš„ç»“æœï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆåˆ°ç°å®çš„é€‚ç”¨æ€§è¯„ä¼°ï¼Œä½œä¸ºä¸€ç§é¢å¤–çš„è¯„ä¼°ï¼Œä»¥è¯„ä¼°åˆæˆæ•°æ®åœ¨å¦‚ä¸‹æµåº”ç”¨ï¼ˆä¾‹å¦‚åˆ†å‰²ï¼‰ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„åˆæˆåŒ»å­¦å›¾åƒä¸ä»…è´¨é‡é«˜ï¼Œè€Œä¸”åœ¨ä¸´åºŠåº”ç”¨ä¸­ä¹Ÿå…·æœ‰æ½œåœ¨çš„æœ‰ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨github.com&#x2F;juhha&#x2F;3D-mADUNetæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00575v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤šåˆ†è¾¨ç‡å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„3DåŒ»å­¦å›¾åƒè½¬æ¢æ¡†æ¶ï¼Œåˆ©ç”¨3Då¤šåˆ†è¾¨ç‡Dense-Attention UNetä½œä¸ºç”Ÿæˆå™¨ï¼Œ3Då¤šåˆ†è¾¨ç‡UNetä½œä¸ºåˆ¤åˆ«å™¨ï¼Œé€šè¿‡ç»“åˆå¤šç§æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬ä½“ç´ çº§çš„GANæŸå¤±å’Œ2.5Dæ„ŸçŸ¥æŸå¤±ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§æˆåƒæ¨¡æ€ã€ä¸åŒéƒ¨ä½å’Œå¹´é¾„æ®µéƒ½æœ‰è¾ƒå¥½çš„ä½“ç§¯å›¾åƒè´¨é‡è¯„ä¼°è¡¨ç°ï¼Œä¸”æå‡ºåˆæˆåˆ°ç°å®çš„é€‚ç”¨æ€§è¯„ä¼°ï¼Œè¯æ˜åˆæˆæ•°æ®åœ¨åç»­åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤æ¡†æ¶å¯æé«˜åŒ»å­¦å›¾åƒå¤„ç†çš„æ•ˆç‡å’Œæ²»ç–—è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè½¬æ¢æ—¨åœ¨å°†ä¸€ç§æˆåƒæ¨¡æ€è½¬æ¢ä¸ºå¦ä¸€ç§ï¼Œä»¥å‡å°‘å¤šæ¬¡å›¾åƒé‡‡é›†çš„éœ€æ±‚ï¼Œæé«˜æ²»ç–—æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¤šåˆ†è¾¨ç‡å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„3DåŒ»å­¦å›¾åƒè½¬æ¢æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶ä¸­ä½¿ç”¨äº†åˆ›æ–°çš„ç»“åˆæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€ä¸ªä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶çš„ç”Ÿæˆå™¨å’Œä¸€ä¸ªç”¨äºåˆ¤åˆ«çœŸä¼ªçš„åˆ¤åˆ«å™¨ã€‚</li>
<li>ç»“åˆäº†å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ä½“ç´ çº§çš„GANæŸå¤±å’Œæ„ŸçŸ¥æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§æˆåƒæ¨¡æ€ã€ä¸åŒéƒ¨ä½å’Œå¹´é¾„æ®µå…·æœ‰ç¨³å¥çš„è¯„ä¼°è¡¨ç°ã€‚</li>
<li>é€šè¿‡åˆæˆåˆ°ç°å®çš„é€‚ç”¨æ€§è¯„ä¼°éªŒè¯äº†åˆæˆæ•°æ®åœ¨åç»­ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰ä¸­çš„æœ‰æ•ˆæ€§ã€‚è¿™æ„å‘³ç€æ‰€ç”Ÿæˆçš„åˆæˆå›¾åƒå¯ä¸ºå®é™…åº”ç”¨æä¾›æ”¯æŒã€‚ </li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šä¾›å…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œå­¦ä¹ ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e709ee79d8f838575b8d877af7e59a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a771e2a8610d752cf67f48a7f32d7e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9765ed42d6c70a76a95b7898ddc9d5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf30a65bff9b16a9d474abd103adfdca.jpg" align="middle">
</details>




<h2 id="A-conditional-Generative-Adversarial-network-model-for-the-Weather4Cast-2024-Challenge"><a href="#A-conditional-Generative-Adversarial-network-model-for-the-Weather4Cast-2024-Challenge" class="headerlink" title="A conditional Generative Adversarial network model for the Weather4Cast   2024 Challenge"></a>A conditional Generative Adversarial network model for the Weather4Cast   2024 Challenge</h2><p><strong>Authors:Atharva Deshpande, Kaushik Gopalan, Jeet Shah, Hrishikesh Simu</strong></p>
<p>This study explores the application of deep learning for rainfall prediction, leveraging the Spinning Enhanced Visible and Infrared Imager (SEVIRI) High rate information transmission (HRIT) data as input and the Operational Program on the Exchange of weather RAdar information (OPERA) ground-radar reflectivity data as ground truth. We use the mean of 4 InfraRed frequency channels as the input. The radiance images are forecasted up to 4 hours into the future using a dense optical flow algorithm. A conditional generative adversarial network (GAN) model is employed to transform the predicted radiance images into rainfall images which are aggregated over the 4 hour forecast period to generate cumulative rainfall values. This model scored a value of approximately 7.5 as the Continuous Ranked Probability Score (CRPS) in the Weather4Cast 2024 competition and placed 1st on the core challenge leaderboard. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨é™æ°´é¢„æµ‹ä¸­çš„åº”ç”¨ï¼Œåˆ©ç”¨SEVIRIï¼ˆå¢å¼ºå¯è§å…‰å’Œçº¢å¤–æˆåƒä»ªï¼‰çš„é«˜é€Ÿç‡ä¿¡æ¯ä¼ è¾“ï¼ˆHRITï¼‰æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œä»¥åŠç”¨äºæ°”è±¡é›·è¾¾ä¿¡æ¯äº¤æ¢çš„æ“ä½œç¨‹åºï¼ˆOPERAï¼‰åœ°é¢é›·è¾¾åå°„æ•°æ®ä½œä¸ºçœŸå®å‚ç…§ã€‚æˆ‘ä»¬ä½¿ç”¨çº¢å¤–é¢‘é“çš„å¹³å‡å€¼ä½œä¸ºè¾“å…¥ã€‚ä½¿ç”¨å¯†é›†çš„å…‰æµç®—æ³•é¢„æµ‹æœªæ¥é•¿è¾¾4å°æ—¶çš„è¾å°„å›¾åƒã€‚é‡‡ç”¨æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¨¡å‹å°†é¢„æµ‹çš„è¾å°„å›¾åƒè½¬æ¢ä¸ºé™æ°´å›¾åƒï¼Œè¿™äº›å›¾åƒæ±‡é›†åœ¨ä¸ºæœŸ4å°æ—¶çš„é¢„æµ‹æœŸå†…ï¼Œç”Ÿæˆç´¯è®¡é™æ°´é‡å€¼ã€‚åœ¨Weather4Cast 2024ç«èµ›ä¸­ï¼Œè¯¥æ¨¡å‹çš„è¿ç»­æ’åæ¦‚ç‡å¾—åˆ†ï¼ˆCRPSï¼‰çº¦ä¸º7.5åˆ†ï¼Œå¹¶åœ¨æ ¸å¿ƒæŒ‘æˆ˜æ’è¡Œæ¦œä¸Šæ’åç¬¬ä¸€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00451v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œé™é›¨é¢„æµ‹ï¼Œä½¿ç”¨SEVIRIé«˜ä¼ è¾“é€Ÿç‡æ•°æ®å’ŒOPERAåœ°é¢é›·è¾¾åå°„æ•°æ®ä½œä¸ºå‚è€ƒçœŸå®æ•°æ®ã€‚ä»¥çº¢å¤–é¢‘é“çš„å‡å€¼ä½œä¸ºè¾“å…¥ï¼Œåˆ©ç”¨å¯†é›†å…‰æµç®—æ³•é¢„æµ‹æœªæ¥å››å°æ—¶çš„è¾å°„å›¾åƒã€‚é‡‡ç”¨æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¨¡å‹å°†é¢„æµ‹çš„è¾å°„å›¾åƒè½¬åŒ–ä¸ºé™é›¨å›¾åƒï¼Œå¹¶åŸºäºé¢„æµ‹æœŸå†…4å°æ—¶çš„é™é›¨å›¾åƒç”Ÿæˆç´¯è®¡é™é›¨é‡å€¼ã€‚åœ¨Weather4Cast 2024ç«èµ›ä¸­ï¼Œè¯¥æ¨¡å‹è¿ç»­æ’åæ¦‚ç‡å¾—åˆ†çº¦ä¸º7.5ï¼Œæ ¸å¿ƒæŒ‘æˆ˜æ’è¡Œæ¦œæ’åç¬¬ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨é™é›¨é¢„æµ‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†SEVIRI HRITæ•°æ®å’ŒOPERAåœ°é¢é›·è¾¾åå°„æ•°æ®ä½œä¸ºè¾“å…¥å’Œå‚è€ƒçœŸå®æ•°æ®ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨çº¢å¤–é¢‘é“å‡å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹æœªæ¥å››å°æ—¶çš„è¾å°„å›¾åƒã€‚</li>
<li>é‡‡ç”¨å¯†é›†å…‰æµç®—æ³•è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¨¡å‹ï¼Œå°†é¢„æµ‹çš„è¾å°„å›¾åƒè½¬åŒ–ä¸ºé™é›¨å›¾åƒã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåœ¨å››å°æ—¶çš„é¢„æµ‹æœŸå†…ç”Ÿæˆç´¯è®¡é™é›¨é‡å€¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc74d457abae825d99834aa828f2323.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3b67c505e1b90460b4e76082acdcbc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-545087b1035d35ce1ed51309c1233df8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b3863900ea168f61b0e59f64aae30e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8c3a517d9e00bd46fded8bf2c47d075.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01a50aad37f4398033749d51f685f8ed.jpg" align="middle">
</details>




<h2 id="LumiNet-Latent-Intrinsics-Meets-Diffusion-Models-for-Indoor-Scene-Relighting"><a href="#LumiNet-Latent-Intrinsics-Meets-Diffusion-Models-for-Indoor-Scene-Relighting" class="headerlink" title="LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene   Relighting"></a>LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene   Relighting</h2><p><strong>Authors:Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, Anand Bhattad</strong></p>
<p>We introduce LumiNet, a novel architecture that leverages generative models and latent intrinsic representations for effective lighting transfer. Given a source image and a target lighting image, LumiNet synthesizes a relit version of the source scene that captures the targetâ€™s lighting. Our approach makes two key contributions: a data curation strategy from the StyleGAN-based relighting model for our training, and a modified diffusion-based ControlNet that processes both latent intrinsic properties from the source image and latent extrinsic properties from the target image. We further improve lighting transfer through a learned adaptor (MLP) that injects the targetâ€™s latent extrinsic properties via cross-attention and fine-tuning.   Unlike traditional ControlNet, which generates images with conditional maps from a single scene, LumiNet processes latent representations from two different images - preserving geometry and albedo from the source while transferring lighting characteristics from the target. Experiments demonstrate that our method successfully transfers complex lighting phenomena including specular highlights and indirect illumination across scenes with varying spatial layouts and materials, outperforming existing approaches on challenging indoor scenes using only images as input. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†LumiNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨å†…åœ¨è¡¨ç¤ºæ¥è¿›è¡Œæœ‰æ•ˆçš„å…‰ç…§è½¬ç§»ã€‚ç»™å®šæºå›¾åƒå’Œç›®æ ‡å…‰ç…§å›¾åƒï¼ŒLumiNetåˆæˆæºåœºæ™¯çš„é‡ç…§ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬æ•æ‰ç›®æ ‡çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åšå‡ºäº†ä¸¤ä¸ªä¸»è¦è´¡çŒ®ï¼šä¸€æ˜¯åŸºäºStyleGANçš„é‡ç…§æ¨¡å‹çš„æ•°æ®æ•´ç†ç­–ç•¥ï¼Œç”¨äºæˆ‘ä»¬çš„è®­ç»ƒï¼›äºŒæ˜¯å¯¹åŸºäºæ‰©æ•£çš„ControlNetè¿›è¡Œäº†ä¿®æ”¹ï¼Œè¯¥ç½‘ç»œå¤„ç†æ¥è‡ªæºå›¾åƒçš„æ½œåœ¨å†…åœ¨å±æ€§å’Œæ¥è‡ªç›®æ ‡å›¾åƒæ½œåœ¨å¤–åœ¨å±æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡å­¦ä¹ çš„é€‚é…å™¨ï¼ˆMLPï¼‰æ”¹è¿›å…‰ç…§è½¬ç§»ï¼Œè¯¥é€‚é…å™¨é€šè¿‡äº¤å‰æ³¨æ„åŠ›å’Œå¾®è°ƒæ³¨å…¥ç›®æ ‡æ½œåœ¨å¤–åœ¨å±æ€§ã€‚ä¸ä¼ ç»Ÿçš„ControlNetä¸åŒï¼Œåè€…æ ¹æ®å•ä¸ªåœºæ™¯ç”Ÿæˆå¸¦æœ‰æ¡ä»¶æ˜ å°„çš„å›¾åƒï¼ŒLumiNetå¤„ç†æ¥è‡ªä¸¤ä¸ªä¸åŒå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºâ€”â€”ä¿ç•™æºåœºæ™¯çš„å‡ ä½•å½¢çŠ¶å’Œåå°„ç‡ï¼ŒåŒæ—¶è½¬ç§»ç›®æ ‡åœºæ™¯çš„å…‰ç…§ç‰¹æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨è¶Šå…·æœ‰ä¸åŒç©ºé—´å¸ƒå±€å’Œææ–™çš„ä¸åŒåœºæ™¯æ—¶ï¼ŒæˆåŠŸåœ°è½¬ç§»äº†åŒ…æ‹¬é«˜å…‰å’Œé—´æ¥ç…§æ˜ç­‰å¤æ‚å…‰ç…§ç°è±¡ï¼Œä»…åœ¨è¾“å…¥å›¾åƒçš„æƒ…å†µä¸‹å°±å¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®¤å†…åœºæ™¯ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00177v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://luminet-relight.github.io/">https://luminet-relight.github.io</a></p>
<p><strong>Summary</strong></p>
<p>LumiNetæ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨å†…åœ¨è¡¨å¾è¿›è¡Œé«˜æ•ˆçš„å…‰ç…§è½¬ç§»ã€‚ç»™å®šæºå›¾åƒå’Œç›®æ ‡å…‰ç…§å›¾åƒï¼ŒLumiNetå¯ä»¥åˆæˆé‡æ–°ç…§æ˜çš„æºåœºæ™¯ï¼Œæ•æ‰ç›®æ ‡çš„å…‰ç…§ã€‚å…¶æ–¹æ³•åŒ…æ‹¬æ•°æ®æ•´ç†ç­–ç•¥ã€åŸºäºStyleGANçš„ç…§æ˜æ¨¡å‹è®­ç»ƒï¼Œä»¥åŠå¤„ç†æºå›¾åƒæ½œåœ¨å†…åœ¨å±æ€§ä¸ç›®æ ‡å›¾åƒæ½œåœ¨å¤–åœ¨å±æ€§çš„æ”¹è¿›å‹æ‰©æ•£æ§åˆ¶ç½‘ç»œï¼ˆControlNetï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨ç›®æ ‡æ½œåœ¨å¤–åœ¨å±æ€§çš„å­¦ä¹ é€‚é…å™¨ï¼ˆMLPï¼‰è¿›è¡Œè·¨æ³¨æ„åŠ›å’Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æ”¹å–„äº†å…‰ç…§è½¬ç§»ã€‚ä¸åŒäºä¼ ç»ŸControlNetåªå¤„ç†å•ä¸€åœºæ™¯çš„å›¾åƒç”Ÿæˆï¼ŒLumiNetèƒ½å¤Ÿå¤„ç†æ¥è‡ªä¸¤ä¸ªä¸åŒå›¾åƒçš„æ½œåœ¨è¡¨å¾ï¼Œæ—¢ä¿ç•™æºåœºæ™¯çš„å‡ ä½•å½¢çŠ¶å’Œåå°„ç‡ï¼Œåˆè½¬ç§»ç›®æ ‡åœºæ™¯çš„å…‰ç…§ç‰¹æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åœºæ™¯ç©ºé—´å¸ƒå±€å’Œææ–™å„å¼‚çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸè½¬ç§»å¤æ‚çš„å…‰ç…§ç°è±¡ï¼ŒåŒ…æ‹¬é«˜å…‰å’Œé—´æ¥ç…§æ˜ï¼Œä¸”åœ¨ä»…ä½¿ç”¨å›¾åƒä½œä¸ºè¾“å…¥çš„æƒ…å†µä¸‹ï¼Œä¼˜äºç°æœ‰æ–¹æ³•åœ¨å®¤å†…åœºæ™¯çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LumiNetæ˜¯ä¸€ç§åŸºäºç”Ÿæˆæ¨¡å‹å’Œæ½œåœ¨è¡¨å¾çš„æ–°å‹æ¶æ„ï¼Œç”¨äºå®ç°æœ‰æ•ˆçš„å…‰ç…§è½¬ç§»ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡æ•°æ®æ•´ç†ç­–ç•¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨StyleGANè¿›è¡Œå…‰ç…§æ¨¡å‹è®­ç»ƒã€‚</li>
<li>LumiNetä½¿ç”¨æ”¹è¿›å‹ControlNetå¤„ç†æºå›¾åƒçš„æ½œåœ¨å†…åœ¨å±æ€§ä¸ç›®æ ‡å›¾åƒçš„æ½œåœ¨å¤–åœ¨å±æ€§ã€‚</li>
<li>é€šè¿‡å­¦ä¹ é€‚é…å™¨ï¼ˆMLPï¼‰è¿›è¡Œè·¨æ³¨æ„åŠ›å’Œå¾®è°ƒæ¥æ”¹è¿›å…‰ç…§è½¬ç§»ã€‚</li>
<li>LumiNetèƒ½å¤„ç†æ¥è‡ªä¸¤ä¸ªä¸åŒå›¾åƒçš„æ½œåœ¨è¡¨å¾ï¼ŒåŒæ—¶ä¿ç•™å‡ ä½•å½¢çŠ¶å’Œåå°„ç‡å¹¶è½¬ç§»å…‰ç…§ç‰¹æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…‰ç…§è½¬ç§»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å…‰ç…§ç°è±¡å’Œå¤šç§åœºæ™¯æ–¹é¢ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0ab3a98c2a288d6702b123d742bd9e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d2010404b39e72615580bc5922f1e0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93632787188c463a49a2ef9460aa6eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9ca7d0f2595ab0508d69a627556d7ad.jpg" align="middle">
</details>




<h2 id="A-Novel-Approach-to-Image-Steganography-Using-Generative-Adversarial-Networks"><a href="#A-Novel-Approach-to-Image-Steganography-Using-Generative-Adversarial-Networks" class="headerlink" title="A Novel Approach to Image Steganography Using Generative Adversarial   Networks"></a>A Novel Approach to Image Steganography Using Generative Adversarial   Networks</h2><p><strong>Authors:Waheed Rehman</strong></p>
<p>The field of steganography has long been focused on developing methods to securely embed information within various digital media while ensuring imperceptibility and robustness. However, the growing sophistication of detection tools and the demand for increased data hiding capacity have revealed limitations in traditional techniques. In this paper, we propose a novel approach to image steganography that leverages the power of generative adversarial networks (GANs) to address these challenges. By employing a carefully designed GAN architecture, our method ensures the creation of stego-images that are visually indistinguishable from their original counterparts, effectively thwarting detection by advanced steganalysis tools. Additionally, the adversarial training paradigm optimizes the balance between embedding capacity, imperceptibility, and robustness, enabling more efficient and secure data hiding. We evaluate our proposed method through a series of experiments on benchmark datasets and compare its performance against baseline techniques, including least significant bit (LSB) substitution and discrete cosine transform (DCT)-based methods. Our results demonstrate significant improvements in metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and robustness against detection. This work not only contributes to the advancement of image steganography but also provides a foundation for exploring GAN-based approaches for secure digital communication. </p>
<blockquote>
<p>éšå†™æœ¯é¢†åŸŸé•¿æœŸä»¥æ¥ä¸€ç›´è‡´åŠ›äºå¼€å‘èƒ½å¤Ÿåœ¨å„ç§æ•°å­—åª’ä½“ä¸­å®‰å…¨åµŒå…¥ä¿¡æ¯çš„æ–¹æ³•ï¼ŒåŒæ—¶ç¡®ä¿ä¸å¯å¯Ÿè§‰æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œæ£€æµ‹å·¥å…·çš„æ—¥ç›Šæˆç†Ÿå’Œå¯¹å¢åŠ æ•°æ®éšè—å®¹é‡çš„éœ€æ±‚å·²ç»æ˜¾ç¤ºå‡ºä¼ ç»ŸæŠ€æœ¯çš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è§£å†³è¿™äº›æŒ‘æˆ˜çš„å›¾åƒéšå†™æ–°æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„GANæ¶æ„ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ç¡®ä¿ç”Ÿæˆçš„éšå†™å›¾åƒåœ¨è§†è§‰ä¸Šæ— æ³•ä¸åŸå§‹å›¾åƒåŒºåˆ†å¼€ï¼Œä»è€Œæœ‰æ•ˆé˜»æ­¢é«˜çº§éšå†™åˆ†æå·¥å…·çš„æ£€æµ‹ã€‚æ­¤å¤–ï¼Œå¯¹æŠ—è®­ç»ƒèŒƒå¼ä¼˜åŒ–äº†åµŒå…¥å®¹é‡ã€ä¸å¯å¯Ÿè§‰æ€§å’Œç¨³å¥æ€§ä¹‹é—´çš„å¹³è¡¡ï¼Œä½¿æ•°æ®éšè—æ›´åŠ é«˜æ•ˆå’Œå®‰å…¨ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå¹¶å°†å…¶æ€§èƒ½ä¸åŒ…æ‹¬æœ€ä½æœ‰æ•ˆä½ï¼ˆLSBï¼‰æ›¿æ¢å’ŒåŸºäºç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰çš„æ–¹æ³•åœ¨å†…çš„åŸºçº¿æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰ä»¥åŠå¯¹æŠ—æ£€æµ‹çš„ç¨³å¥æ€§ç­‰æ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ¨åŠ¨äº†å›¾åƒéšå†™æŠ€æœ¯çš„å‘å±•ï¼Œè€Œä¸”ä¸ºåŸºäºGANçš„å®‰å…¨æ•°å­—é€šä¿¡æ–¹æ³•çš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00094v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å›¾åƒéšå†™åˆ†ææ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»ŸæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ£€æµ‹å·¥å…·çš„æ—¥ç›Šæˆç†Ÿå’Œå¯¹æ›´å¤§éšè—å®¹é‡çš„éœ€æ±‚ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„GANæ¶æ„åˆ›å»ºéšå†™å›¾åƒï¼Œè¿™äº›å›¾åƒåœ¨è§†è§‰ä¸Šå‡ ä¹ä¸åŸå§‹å›¾åƒæ— æ³•åŒºåˆ†ï¼Œæœ‰æ•ˆæŠµæŠ—å…ˆè¿›çš„éšå†™åˆ†æå·¥å…·ã€‚æ­¤å¤–ï¼Œå¯¹æŠ—æ€§è®­ç»ƒèŒƒå¼ä¼˜åŒ–äº†åµŒå…¥å®¹é‡ã€ä¸å¯å¯Ÿè§‰æ€§å’Œç¨³å¥æ€§ä¹‹é—´çš„å¹³è¡¡ï¼Œæé«˜äº†æ•°æ®éšè—çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶åˆ©ç”¨GANsè§£å†³å›¾åƒéšå†™åˆ†æä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>GANæ¶æ„è¢«ç”¨äºåˆ›å»ºä¸åŸå§‹å›¾åƒè§†è§‰ä¸Šä¸å¯åŒºåˆ†çš„éšå†™å›¾åƒã€‚</li>
<li>å¯¹æŠ—æ€§è®­ç»ƒèŒƒå¼æé«˜äº†åµŒå…¥å®¹é‡ã€ä¸å¯å¯Ÿè§‰æ€§å’Œç¨³å¥æ€§çš„å¹³è¡¡ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆæŠµæŠ—å…ˆè¿›çš„éšå†™åˆ†æå·¥å…·ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’ŒæŠ—æ£€æµ‹æ–¹é¢çš„æ€§èƒ½æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†å›¾åƒéšå†™åˆ†æçš„å‘å±•ï¼Œè¿˜ä¸ºåŸºäºGANsçš„å®‰å…¨æ•°å­—é€šä¿¡æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e92514775105a0f16b9397825beda3bb.jpg" align="middle">
</details>




<h2 id="Addressing-Vulnerabilities-in-AI-Image-Detection-Challenges-and-Proposed-Solutions"><a href="#Addressing-Vulnerabilities-in-AI-Image-Detection-Challenges-and-Proposed-Solutions" class="headerlink" title="Addressing Vulnerabilities in AI-Image Detection: Challenges and   Proposed Solutions"></a>Addressing Vulnerabilities in AI-Image Detection: Challenges and   Proposed Solutions</h2><p><strong>Authors:Justin Jiang</strong></p>
<p>The rise of advanced AI models like Generative Adversarial Networks (GANs) and diffusion models such as Stable Diffusion has made the creation of highly realistic images accessible, posing risks of misuse in misinformation and manipulation. This study evaluates the effectiveness of convolutional neural networks (CNNs), as well as DenseNet architectures, for detecting AI-generated images. Using variations of the CIFAKE dataset, including images generated by different versions of Stable Diffusion, we analyze the impact of updates and modifications such as Gaussian blurring, prompt text changes, and Low-Rank Adaptation (LoRA) on detection accuracy. The findings highlight vulnerabilities in current detection methods and propose strategies to enhance the robustness and reliability of AI-image detection systems. </p>
<blockquote>
<p>éšç€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç­‰é«˜çº§AIæ¨¡å‹å’ŒStable Diffusionç­‰æ‰©æ•£æ¨¡å‹çš„å…´èµ·ï¼Œåˆ›å»ºé«˜åº¦é€¼çœŸçš„å›¾åƒå˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè¿™å¸¦æ¥äº†è¯¯ç”¨å’Œè¯¯å¯¼ä¿¡æ¯çš„é£é™©ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒDenseNetæ¶æ„åœ¨æ£€æµ‹AIç”Ÿæˆçš„å›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨äº†CIFARçš„å˜ä½“æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç”±ä¸åŒç‰ˆæœ¬çš„Stable Diffusionç”Ÿæˆçš„å›¾åƒï¼Œåˆ†æäº†é«˜æ–¯æ¨¡ç³Šå¤„ç†ã€æç¤ºæ–‡æœ¬æ›´æ”¹å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç­‰æ›´æ–°å’Œä¿®æ”¹å¯¹æ£€æµ‹å‡†ç¡®ç‡çš„å½±å“ã€‚ç ”ç©¶æŒ‡å‡ºäº†å½“å‰æ£€æµ‹æ–¹æ³•çš„æ¼æ´å¹¶æå‡ºäº†å¢å¼ºAIå›¾åƒæ£€æµ‹ç³»ç»Ÿç¨³å¥æ€§å’Œå¯é æ€§çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00073v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å…ˆè¿›AIæ¨¡å‹å¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œç¨³å®šæ‰©æ•£ç­‰æ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼Œä½¿å¾—ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒæˆä¸ºå¯èƒ½ï¼Œä½†ä¹Ÿå­˜åœ¨è¢«è¯¯ç”¨å¯¼è‡´è¯¯å¯¼å’Œæ“çºµçš„é£é™©ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’ŒDenseNetæ¶æ„åœ¨æ£€æµ‹AIç”Ÿæˆå›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ä½¿ç”¨åŒ…æ‹¬ä¸åŒç‰ˆæœ¬çš„ç¨³å®šæ‰©æ•£ç”Ÿæˆçš„å›¾åƒåœ¨å†…çš„CIFAKEæ•°æ®é›†å˜ç§ï¼Œåˆ†æäº†æ›´æ–°å’Œä¿®æ”¹å¦‚é«˜æ–¯æ¨¡ç³Šã€æç¤ºæ–‡æœ¬å˜åŒ–å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰å¯¹æ£€æµ‹ç²¾åº¦çš„å½±å“ã€‚ç ”ç©¶çªå‡ºäº†å½“å‰æ£€æµ‹æ–¹æ³•çš„æ¼æ´ï¼Œå¹¶æå‡ºäº†å¢å¼ºAIå›¾åƒæ£€æµ‹ç³»ç»Ÿç¨³å¥æ€§å’Œå¯é æ€§çš„ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIæ¨¡å‹å¦‚GANså’Œç¨³å®šæ‰©æ•£èƒ½ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒï¼Œå­˜åœ¨è¢«è¯¯ç”¨å¯¼è‡´è¯¯å¯¼å’Œæ“çºµçš„é£é™©ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†CNNå’ŒDenseNetæ¶æ„åœ¨æ£€æµ‹AIç”Ÿæˆå›¾åƒæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä½¿ç”¨CIFAKEæ•°æ®é›†å˜ç§æ¥åˆ†æä¸åŒå› ç´ ï¼ŒåŒ…æ‹¬å›¾åƒç”Ÿæˆæ–¹æ³•çš„æ›´æ–°å’Œä¿®æ”¹ï¼Œå¯¹æ£€æµ‹ç²¾åº¦çš„å½±å“ã€‚</li>
<li>å½“å‰æ£€æµ‹æ–¹æ³•å­˜åœ¨æ¼æ´ï¼Œéœ€è¦å¢å¼ºAIå›¾åƒæ£€æµ‹ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚</li>
<li>é«˜æ–¯æ¨¡ç³Šã€æç¤ºæ–‡æœ¬å˜åŒ–å’ŒLoRAç­‰ä¿®æ”¹å¯èƒ½å½±å“AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ›´æ–°å’Œæ”¹è¿›ç°æœ‰AIå›¾åƒæ£€æµ‹ç³»ç»Ÿçš„å¿…è¦æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf1a411d5a1e29ab766f94d5ec9e2dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcdcf68ea5cd7ef78ba9674326361105.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-457a4da43ccccbadf8204bb7fb45a298.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-683d4406a32193735b37004f45a5c0ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cba7d0118fa10f33fdc436a472cd32a.jpg" align="middle">
</details>




<h2 id="3D-Wasserstein-generative-adversarial-network-with-dense-U-Net-based-discriminator-for-preclinical-fMRI-denoising"><a href="#3D-Wasserstein-generative-adversarial-network-with-dense-U-Net-based-discriminator-for-preclinical-fMRI-denoising" class="headerlink" title="3D Wasserstein generative adversarial network with dense U-Net based   discriminator for preclinical fMRI denoising"></a>3D Wasserstein generative adversarial network with dense U-Net based   discriminator for preclinical fMRI denoising</h2><p><strong>Authors:Sima Soltanpour, Arnold Chang, Dan Madularu, Praveen Kulkarni, Craig Ferris, Chris Joslin</strong></p>
<p>Functional magnetic resonance imaging (fMRI) is extensively used in clinical and preclinical settings to study brain function, however, fMRI data is inherently noisy due to physiological processes, hardware, and external noise. Denoising is one of the main preprocessing steps in any fMRI analysis pipeline. This process is challenging in preclinical data in comparison to clinical data due to variations in brain geometry, image resolution, and low signal-to-noise ratios. In this paper, we propose a structure-preserved algorithm based on a 3D Wasserstein generative adversarial network with a 3D dense U-net based discriminator called, 3D U-WGAN. We apply a 4D data configuration to effectively denoise temporal and spatial information in analyzing preclinical fMRI data. GAN-based denoising methods often utilize a discriminator to identify significant differences between denoised and noise-free images, focusing on global or local features. To refine the fMRI denoising model, our method employs a 3D dense U-Net discriminator to learn both global and local distinctions. To tackle potential over-smoothing, we introduce an adversarial loss and enhance perceptual similarity by measuring feature space distances. Experiments illustrate that 3D U-WGAN significantly improves image quality in resting-state and task preclinical fMRI data, enhancing signal-to-noise ratio without introducing excessive structural changes in existing methods. The proposed method outperforms state-of-the-art methods when applied to simulated and real data in a fMRI analysis pipeline. </p>
<blockquote>
<p>åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰åœ¨ä¸´åºŠå’Œä¸´åºŠå‰ç¯å¢ƒä¸­å¹¿æ³›åº”ç”¨äºç ”ç©¶è„‘åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿç†è¿‡ç¨‹ã€ç¡¬ä»¶å’Œå¤–éƒ¨å™ªå£°ï¼ŒfMRIæ•°æ®æœ¬è´¨ä¸Šå­˜åœ¨å™ªå£°ã€‚å»å™ªæ˜¯ä»»ä½•fMRIåˆ†æç®¡é“ä¸­çš„ä¸»è¦é¢„å¤„ç†æ­¥éª¤ä¹‹ä¸€ã€‚ä¸ä¸´åºŠæ•°æ®ç›¸æ¯”ï¼Œå»å™ªåœ¨ä¸´åºŠå‰æ•°æ®ä¸­æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨å¤§è„‘å‡ ä½•ç»“æ„ã€å›¾åƒåˆ†è¾¨ç‡å’Œä¿¡å™ªæ¯”ä½çš„å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº3D Wassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„ç»“æ„ä¿ç•™ç®—æ³•ï¼Œè¯¥ç®—æ³•å…·æœ‰ä¸€ä¸ªåä¸º3D U-WGANçš„åŸºäº3Då¯†é›†U-netçš„é‰´åˆ«å™¨ã€‚æˆ‘ä»¬é‡‡ç”¨4Dæ•°æ®é…ç½®ï¼Œä»¥æœ‰æ•ˆåœ°å¯¹ä¸´åºŠå‰fMRIæ•°æ®è¿›è¡Œå»å™ªå¤„ç†å¹¶åˆ†ææ—¶ç©ºä¿¡æ¯ã€‚åŸºäºGANçš„å»å™ªæ–¹æ³•é€šå¸¸ä½¿ç”¨é‰´åˆ«å™¨æ¥è¯†åˆ«å»å™ªå›¾åƒå’Œæ— å™ªå£°å›¾åƒä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œä¾§é‡äºå…¨å±€æˆ–å±€éƒ¨ç‰¹å¾ã€‚ä¸ºäº†æ”¹è¿›fMRIå»å™ªæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ä¸ªä¸‰ç»´å¯†é›†U-Neté‰´åˆ«å™¨æ¥å­¦ä¹ å…¨å±€å’Œå±€éƒ¨çš„åŒºåˆ«ã€‚ä¸ºäº†è§£å†³å¯èƒ½çš„è¿‡åº¦å¹³æ»‘é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯¹æŠ—æŸå¤±å¹¶é€šè¿‡æµ‹é‡ç‰¹å¾ç©ºé—´è·ç¦»æ¥æé«˜æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œä¸‰ç»´U-WGANæ˜¾è‘—æé«˜äº†é™æ¯çŠ¶æ€å’Œä»»åŠ¡æ€ä¸´åºŠå‰fMRIçš„å›¾åƒè´¨é‡ï¼Œæé«˜äº†ä¿¡å™ªæ¯”ï¼ŒåŒæ—¶åœ¨ç°æœ‰æ–¹æ³•ä¸­ä¸ä¼šå¼•èµ·è¿‡åº¦çš„ç»“æ„å˜åŒ–ã€‚å½“åº”ç”¨äºæ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®æ•°æ®æ—¶ï¼Œè¯¥æ–¹æ³•åœ¨fMRIåˆ†æç®¡é“ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19345v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºä¸‰ç»´Wassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆ3D U-WGANï¼‰çš„ç»“æ„ä¿ç•™ç®—æ³•ï¼Œç”¨äºå¯¹ä¸´åºŠå‰çš„åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ•°æ®è¿›è¡Œå»å™ªå¤„ç†ã€‚è¯¥ç®—æ³•é‡‡ç”¨å››ç»´æ•°æ®é…ç½®ï¼Œæœ‰æ•ˆå»é™¤äº†æ—¶ç©ºå™ªå£°ï¼Œæé«˜äº†å›¾åƒè´¨é‡ã€‚é€šè¿‡å¼•å…¥å¯¹æŠ—æŸå¤±å’Œç‰¹å¾ç©ºé—´è·ç¦»çš„åº¦é‡ï¼Œè§£å†³äº†è¿‡åº¦å¹³æ»‘çš„é—®é¢˜ï¼Œå¢å¼ºäº†æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨é™æ¯æ€å’Œä»»åŠ¡æ€çš„fMRIæ•°æ®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†ä¿¡å™ªæ¯”ï¼Œä¸”åœ¨ç°æœ‰æ–¹æ³•ä¸­æœªå¼•å…¥è¿‡å¤šçš„ç»“æ„å˜åŒ–ã€‚ä¸ç°æœ‰å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šçš„è¡¨ç°æ›´ä¼˜ç§€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´Wassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆ3D U-WGANï¼‰çš„ç®—æ³•ï¼Œç”¨äºå¯¹ä¸´åºŠå‰çš„åŠŸèƒ½ç£å…±æŒ¯æˆåƒï¼ˆfMRIï¼‰æ•°æ®è¿›è¡Œé¢„å¤„ç†å»å™ªã€‚</li>
<li>è¯¥ç®—æ³•é‡‡ç”¨å››ç»´æ•°æ®é…ç½®ï¼Œæ—¨åœ¨æœ‰æ•ˆå»é™¤fMRIæ•°æ®ä¸­çš„æ—¶ç©ºå™ªå£°ã€‚</li>
<li>è®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ª3Då¯†é›†U-Netåˆ¤åˆ«å™¨æ¥åŒæ—¶å­¦ä¹ å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œä»¥ä¼˜åŒ–å»å™ªæ¨¡å‹ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿‡åº¦å¹³æ»‘çš„é—®é¢˜ï¼Œå¼•å…¥äº†å¯¹æŠ—æŸå¤±ï¼Œå¹¶é€šè¿‡æµ‹é‡ç‰¹å¾ç©ºé—´è·ç¦»å¢å¼ºäº†æ„ŸçŸ¥ç›¸ä¼¼æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•åœ¨é™æ¯æ€å’Œä»»åŠ¡æ€çš„fMRIæ•°æ®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å›¾åƒè´¨é‡å¹¶å¢å¼ºä¿¡å™ªæ¯”ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç®—æ³•åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®æ•°æ®ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eeefb26d321d7575c380bf2a5c27787a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c543592b4ed205ace3f175a210661b8.jpg" align="middle">
</details>




<h2 id="HiFiVFS-High-Fidelity-Video-Face-Swapping"><a href="#HiFiVFS-High-Fidelity-Video-Face-Swapping" class="headerlink" title="HiFiVFS: High Fidelity Video Face Swapping"></a>HiFiVFS: High Fidelity Video Face Swapping</h2><p><strong>Authors:Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, Chengjie Wang</strong></p>
<p>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively. </p>
<blockquote>
<p>é¢éƒ¨æ›¿æ¢æ—¨åœ¨ç”Ÿæˆç»“åˆæºèº«ä»½å’Œç›®æ ‡å±æ€§çš„ç»“æœã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„é¢éƒ¨æ›¿æ¢ä¸Šã€‚åœ¨å¤„ç†è§†é¢‘æ—¶ï¼Œæ¯ä¸€å¸§éƒ½æ˜¯ç‹¬ç«‹å¤„ç†çš„ï¼Œå¾ˆéš¾ä¿è¯æ—¶é—´ç¨³å®šæ€§ã€‚ä»æ¨¡å‹çš„è§’åº¦æ¥çœ‹ï¼Œé¢éƒ¨æ›¿æ¢æ­£é€æ¸ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå› ä¸ºDMså·²æ˜¾ç¤ºå‡ºå…·æœ‰æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚å½“å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šå¸¸é‡‡ç”¨å›¾åƒä¿®å¤æŠ€æœ¯ï¼Œè¿™åœ¨ä¿ç•™å…‰ç…§å’Œå¦†å®¹ç­‰ç²¾ç»†å±æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜ä¿çœŸè§†é¢‘é¢éƒ¨æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—æ€§å­¦ä¹ æå–èº«ä»½åˆ†ç¦»å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è¯¦ç»†çš„èº«ä»½æ³¨å…¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘é¢éƒ¨æ›¿æ¢ä¸­å®ç°äº†å®šæ€§å’Œå®šé‡ä¸Šçš„æœ€å…ˆè¿›æ°´å¹³ï¼ˆSOTAï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18293v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é«˜ä¿çœŸè§†é¢‘äººè„¸æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„å¼ºç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒï¼Œè§£å†³ç°æœ‰è§†é¢‘äººè„¸æ›¿æ¢æŠ€æœ¯åœ¨æ—¶é—´ç¨³å®šæ€§å’Œç»†èŠ‚å±æ€§ä¿å­˜æ–¹é¢çš„é—®é¢˜ã€‚é€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—å­¦ä¹ ï¼Œæ„å»ºç»†ç²’åº¦å±æ€§æ¨¡å—ä»¥æå–èº«ä»½åˆ†ç¦»å’Œç»†ç²’åº¦å±æ€§ç‰¹å¾ã€‚å¼•å…¥è¯¦ç»†èº«ä»½æ³¨å…¥ï¼Œè¿›ä¸€æ­¥æé«˜èº«ä»½ç›¸ä¼¼æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘äººè„¸æ›¿æ¢æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘äººè„¸æ›¿æ¢é¢ä¸´æ—¶é—´ç¨³å®šæ€§å’Œç»†èŠ‚å±æ€§ä¿å­˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒäººè„¸æ›¿æ¢ï¼Œå¤„ç†è§†é¢‘æ—¶ç‹¬ç«‹å¤„ç†æ¯ä¸€å¸§ï¼Œéš¾ä»¥ç¡®ä¿æ—¶é—´ç¨³å®šæ€§ã€‚</li>
<li>äººè„¸æ›¿æ¢æŠ€æœ¯é€æ¸ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå› ä¸ºDMså±•ç°å‡ºæ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹æ–¹æ³•å¸¸é‡‡ç”¨è¡¥å…¨æŠ€æœ¯ï¼Œéš¾ä»¥ä¿å­˜å…‰ç…§å’Œå¦†å®¹ç­‰ç»†ç²’åº¦å±æ€§ã€‚</li>
<li>æå‡ºçš„é«˜ä¿çœŸè§†é¢‘äººè„¸æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶åˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒã€‚</li>
<li>æ„å»ºç»†ç²’åº¦å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—å­¦ä¹ æå–èº«ä»½åˆ†ç¦»å’Œç»†ç²’åº¦å±æ€§ç‰¹å¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-237d77b3aa4044afa7c8f5589ddec756.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31457317ca85d09e8d2b9d29ee530829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52192381a3ec2d89bb7e63e4b4659e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad5d321886942a17610359ff7f9c6e2.jpg" align="middle">
</details>




<h2 id="Towards-Lensless-Image-Deblurring-with-Prior-Embedded-Implicit-Neural-Representations-in-the-Low-Data-Regime"><a href="#Towards-Lensless-Image-Deblurring-with-Prior-Embedded-Implicit-Neural-Representations-in-the-Low-Data-Regime" class="headerlink" title="Towards Lensless Image Deblurring with Prior-Embedded Implicit Neural   Representations in the Low-Data Regime"></a>Towards Lensless Image Deblurring with Prior-Embedded Implicit Neural   Representations in the Low-Data Regime</h2><p><strong>Authors:Abeer Banerjee, Sanjay Singh</strong></p>
<p>The field of computational imaging has witnessed a promising paradigm shift with the emergence of untrained neural networks, offering novel solutions to inverse computational imaging problems. While existing techniques have demonstrated impressive results, they often operate either in the high-data regime, leveraging Generative Adversarial Networks (GANs) as image priors, or through untrained iterative reconstruction in a data-agnostic manner. This paper delves into lensless image reconstruction, a subset of computational imaging that replaces traditional lenses with computation, enabling the development of ultra-thin and lightweight imaging systems. To the best of our knowledge, we are the first to leverage implicit neural representations for lensless image deblurring, achieving reconstructions without the requirement of prior training. We perform prior-embedded untrained iterative optimization to enhance reconstruction performance and speed up convergence, effectively bridging the gap between the no-data and high-data regimes. Through a thorough comparative analysis encompassing various untrained and low-shot methods, including under-parameterized non-convolutional methods and domain-restricted low-shot methods, we showcase the superior performance of our approach by a significant margin. </p>
<blockquote>
<p>è®¡ç®—æˆåƒé¢†åŸŸè§è¯äº†æœªè®­ç»ƒç¥ç»ç½‘ç»œçš„å‡ºç°æ‰€å¸¦æ¥çš„å……æ»¡å¸Œæœ›çš„æ¨¡å¼è½¬å˜ï¼Œä¸ºè§£å†³åè®¡ç®—æˆåƒé—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶ç°æœ‰æŠ€æœ¯å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨é«˜æ•°æ®çŠ¶æ€ä¸‹è¿è¡Œï¼Œåˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä½œä¸ºå›¾åƒå…ˆéªŒï¼Œæˆ–è€…ä»¥æ•°æ®æ— å…³çš„æ–¹å¼é€šè¿‡æœªè®­ç»ƒçš„è¿­ä»£é‡å»ºã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†æ— é€é•œå›¾åƒé‡å»ºï¼Œè¿™æ˜¯è®¡ç®—æˆåƒçš„ä¸€ä¸ªå­é›†ï¼Œç”¨è®¡ç®—ä»£æ›¿ä¼ ç»Ÿé€é•œï¼Œä¸ºå®ç°è¶…è–„å’Œè½»ä¾¿çš„æˆåƒç³»ç»Ÿçš„å‘å±•åˆ›é€ äº†æ¡ä»¶ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡åˆ©ç”¨éšå¼ç¥ç»è¡¨å¾è¿›è¡Œæ— é€é•œå›¾åƒå»æ¨¡ç³Šå¤„ç†ï¼Œå®ç°æ— éœ€é¢„å…ˆè®­ç»ƒçš„é‡å»ºã€‚æˆ‘ä»¬æ‰§è¡ŒåµŒå…¥å…ˆéªŒçš„æœªè®­ç»ƒè¿­ä»£ä¼˜åŒ–ï¼Œä»¥æé«˜é‡å»ºæ€§èƒ½å¹¶åŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼Œæœ‰æ•ˆåœ°ç¼©å°äº†æ— æ•°æ®å’Œé«˜æ•°æ®çŠ¶æ€ä¹‹é—´çš„é¸¿æ²Ÿã€‚é€šè¿‡å¯¹å„ç§æœªè®­ç»ƒå’Œå°‘é‡æ‹æ‘„çš„æ–¹æ³•è¿›è¡Œå…¨é¢æ¯”è¾ƒåˆ†æï¼ŒåŒ…æ‹¬å‚æ•°ä¸è¶³çš„å·ç§¯æ–¹æ³•ä»¥åŠå—åŸŸé™åˆ¶çš„ä½å°„å‡»æ–¹æ³•ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™äº›æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18189v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è®¡ç®—æˆåƒé¢†åŸŸå‡ºç°äº†æœªè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä¸ºé€†è®¡ç®—æˆåƒé—®é¢˜æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡ä¸“æ³¨äºæ— é€é•œå›¾åƒé‡å»ºï¼Œåˆ©ç”¨éšå¼ç¥ç»è¡¨å¾è¿›è¡Œæ— é€é•œå›¾åƒå»æ¨¡ç³Šï¼Œæ— éœ€é¢„å…ˆè®­ç»ƒå³å¯å®ç°é‡å»ºã€‚é€šè¿‡åµŒå…¥å…ˆéªŒçš„æœªè®­ç»ƒè¿­ä»£ä¼˜åŒ–ï¼Œæé«˜äº†é‡å»ºæ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦ï¼Œç¼©å°äº†æ— æ•°æ®å’Œé«˜æ•°æ®ä¹‹é—´çš„é¸¿æ²Ÿã€‚å¯¹æ¯”åˆ†æäº†å„ç§æœªè®­ç»ƒå’Œä½æ ·æœ¬æ–¹æ³•ï¼Œæœ¬æ–‡æ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœªè®­ç»ƒç¥ç»ç½‘ç»œåœ¨è®¡ç®—æˆåƒé¢†åŸŸå±•ç°å‡ºæ–°çš„è§£å†³æ–¹æ¡ˆæ½œåŠ›ã€‚</li>
<li>æ— é€é•œå›¾åƒé‡å»ºä½œä¸ºè®¡ç®—æˆåƒçš„ä¸€ä¸ªå­é›†ï¼Œæ›¿æ¢äº†ä¼ ç»Ÿé€é•œï¼Œæœ‰åŠ©äºå¼€å‘è¶…è–„å’Œè½»å‹çš„æˆåƒç³»ç»Ÿã€‚</li>
<li>é¦–æ¬¡å°è¯•åˆ©ç”¨éšå¼ç¥ç»è¡¨å¾è¿›è¡Œæ— é€é•œå›¾åƒå»æ¨¡ç³Šã€‚</li>
<li>åµŒå…¥å…ˆéªŒçš„æœªè®­ç»ƒè¿­ä»£ä¼˜åŒ–æé«˜äº†é‡å»ºæ€§èƒ½å’Œæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>ç¼©å°äº†æ— æ•°æ®å’Œé«˜æ•°æ®ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå®ç°äº†æ›´å¹¿æ³›çš„é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚</li>
<li>ä¸å…¶ä»–æœªè®­ç»ƒå’Œä½æ ·æœ¬æ–¹æ³•ç›¸æ¯”ï¼ŒåŒ…æ‹¬å‚æ•°ä¸è¶³çš„éå·ç§¯æ–¹æ³•å’Œå—åŸŸé™åˆ¶çš„ä½æ ·æœ¬æ–¹æ³•ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6487a189b09faa6425ca92cdb4c385e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c5eeda13fec1be7d565703ae03973c9.jpg" align="middle">
</details>




<h2 id="OSDFace-One-Step-Diffusion-Model-for-Face-Restoration"><a href="#OSDFace-One-Step-Diffusion-Model-for-Face-Restoration" class="headerlink" title="OSDFace: One-Step Diffusion Model for Face Restoration"></a>OSDFace: One-Step Diffusion Model for Face Restoration</h2><p><strong>Authors:Jingkai Wang, Jue Gong, Lin Zhang, Zheng Chen, Xing Liu, Hong Gu, Yutong Liu, Yulun Zhang, Xiaokang Yang</strong></p>
<p>Diffusion models have demonstrated impressive performance in face restoration. Yet, their multi-step inference process remains computationally intensive, limiting their applicability in real-world scenarios. Moreover, existing methods often struggle to generate face images that are harmonious, realistic, and consistent with the subjectâ€™s identity. In this work, we propose OSDFace, a novel one-step diffusion model for face restoration. Specifically, we propose a visual representation embedder (VRE) to better capture prior information and understand the input face. In VRE, low-quality faces are processed by a visual tokenizer and subsequently embedded with a vector-quantized dictionary to generate visual prompts. Additionally, we incorporate a facial identity loss derived from face recognition to further ensure identity consistency. We further employ a generative adversarial network (GAN) as a guidance model to encourage distribution alignment between the restored face and the ground truth. Experimental results demonstrate that OSDFace surpasses current state-of-the-art (SOTA) methods in both visual quality and quantitative metrics, generating high-fidelity, natural face images with high identity consistency. The code and model will be released at <a target="_blank" rel="noopener" href="https://github.com/jkwang28/OSDFace">https://github.com/jkwang28/OSDFace</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨äººè„¸ä¿®å¤æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶å¤šæ­¥æ¨ç†è¿‡ç¨‹ä»ç„¶è®¡ç®—å¯†é›†ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥ç”Ÿæˆå’Œè°ã€é€¼çœŸä¸”ä¸äººè„¸èº«ä»½ä¸€è‡´çš„äººè„¸å›¾åƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OSDFaceï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºäººè„¸ä¿®å¤çš„æ–°å‹ä¸€æ­¥æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†è§‰è¡¨ç¤ºåµŒå…¥å™¨ï¼ˆVREï¼‰æ¥æ›´å¥½åœ°æ•è·å…ˆéªŒä¿¡æ¯å¹¶ç†è§£è¾“å…¥çš„äººè„¸ã€‚åœ¨VREä¸­ï¼Œä½è´¨é‡çš„äººè„¸é€šè¿‡è§†è§‰æ ‡è®°å™¨è¿›è¡Œå¤„ç†ï¼Œéšåä½¿ç”¨å‘é‡é‡åŒ–å­—å…¸åµŒå…¥ç”Ÿæˆè§†è§‰æç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäººè„¸è¯†åˆ«ä¸­å¾—åˆ°çš„é¢éƒ¨èº«ä»½æŸå¤±æ¥è¿›ä¸€æ­¥ç¡®ä¿èº«ä»½ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä½œä¸ºæŒ‡å¯¼æ¨¡å‹ï¼Œä»¥é¼“åŠ±ä¿®å¤çš„äººè„¸ä¸åœ°é¢å®å†µä¹‹é—´çš„åˆ†å¸ƒå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSDFaceåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢éƒ½è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„ï¼ˆSOTAï¼‰æ–¹æ³•ï¼Œç”Ÿæˆäº†é«˜ä¿çœŸã€è‡ªç„¶çš„äººè„¸å›¾åƒï¼Œå…·æœ‰å¾ˆé«˜çš„èº«ä»½ä¸€è‡´æ€§ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jkwang28/OSDFace%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/jkwang28/OSDFaceä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17163v1">PDF</a> 8 pages, 6 figures. The code and model will be available at   <a target="_blank" rel="noopener" href="https://github.com/jkwang28/OSDFace">https://github.com/jkwang28/OSDFace</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºOSDFaceçš„æ–°å‹ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé¢éƒ¨ä¿®å¤ã€‚è¯¥æ¨¡å‹é€šè¿‡è§†è§‰è¡¨ç¤ºåµŒå…¥å™¨ï¼ˆVREï¼‰æ›´å¥½åœ°æ•æ‰å…ˆéªŒä¿¡æ¯å¹¶ç†è§£è¾“å…¥é¢éƒ¨ã€‚æ¨¡å‹è¿˜ç»“åˆäº†é¢éƒ¨èº«ä»½æŸå¤±ä»¥ç¡®ä¿èº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä½œä¸ºå¼•å¯¼æ¨¡å‹æ¥é¼“åŠ±æ¢å¤é¢éƒ¨ä¸çœŸå®é¢éƒ¨ä¹‹é—´çš„åˆ†å¸ƒå¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOSDFaceåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡æ–¹é¢å‡è¶…è¶Šäº†å½“å‰å…ˆè¿›æ°´å¹³ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸã€è‡ªç„¶çš„é¢éƒ¨å›¾åƒï¼Œå…·æœ‰é«˜åº¦çš„èº«ä»½ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OSDFaceæ˜¯ä¸€ä¸ªæ–°å‹çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºé¢éƒ¨ä¿®å¤ï¼Œæ—¨åœ¨è§£å†³å¤šæ­¥æ¨ç†è®¡ç®—é‡å¤§å’Œåœ¨ç°å®åœºæ™¯åº”ç”¨æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è§†è§‰è¡¨ç¤ºåµŒå…¥å™¨ï¼ˆVREï¼‰æ•æ‰å…ˆéªŒä¿¡æ¯å¹¶ç†è§£è¾“å…¥é¢éƒ¨ã€‚</li>
<li>ä½¿ç”¨è§†è§‰ä»¤ç‰Œå™¨å’Œå‘é‡é‡åŒ–è¯å…¸ç”Ÿæˆè§†è§‰æç¤ºæ¥å¤„ç†ä½è´¨é‡é¢éƒ¨å›¾åƒã€‚</li>
<li>ç»“åˆé¢éƒ¨èº«ä»½æŸå¤±ä»¥ç¡®ä¿ç”Ÿæˆçš„é¢éƒ¨å›¾åƒä¸ä¸»ä½“èº«ä»½ä¸€è‡´ã€‚</li>
<li>é‡‡ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä½œä¸ºå¼•å¯¼æ¨¡å‹ï¼Œä½¿ä¿®å¤åçš„é¢éƒ¨ä¸çœŸå®é¢éƒ¨ä¹‹é—´çš„åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒOSDFaceåœ¨è§†è§‰è´¨é‡å’Œå®šé‡æŒ‡æ ‡ä¸Šè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8c54b38011a24627292df50a763a3d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c20e78007df6d2082e2bbcaa5edc68a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db290179d5fa55d8bdd2a3ccbc70c5ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa7ed66a4926d3c8b98edeee296c75a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ece57040e0bd4a644f0483889ef35f21.jpg" align="middle">
</details>




<h2 id="LegoPET-Hierarchical-Feature-Guided-Conditional-Diffusion-for-PET-Image-Reconstruction"><a href="#LegoPET-Hierarchical-Feature-Guided-Conditional-Diffusion-for-PET-Image-Reconstruction" class="headerlink" title="LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image   Reconstruction"></a>LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image   Reconstruction</h2><p><strong>Authors:Yiran Sun, Osama Mawlawi</strong></p>
<p>Positron emission tomography (PET) is widely utilized for cancer detection due to its ability to visualize functional and biological processes in vivo. PET images are usually reconstructed from histogrammed raw data (sinograms) using traditional iterative techniques (e.g., OSEM, MLEM). Recently, deep learning (DL) methods have shown promise by directly mapping raw sinogram data to PET images. However, DL approaches that are regression-based or GAN-based often produce overly smoothed images or introduce various artifacts respectively. Image-conditioned diffusion probabilistic models (cDPMs) are another class of likelihood-based DL techniques capable of generating highly realistic and controllable images. While cDPMs have notable strengths, they still face challenges such as maintain correspondence and consistency between input and output images when they are from different domains (e.g., sinogram vs. image domain) as well as slow convergence rates. To address these limitations, we introduce LegoPET, a hierarchical feature guided conditional diffusion model for high-perceptual quality PET image reconstruction from sinograms. We conducted several experiments demonstrating that LegoPET not only improves the performance of cDPMs but also surpasses recent DL-based PET image reconstruction techniques in terms of visual quality and pixel-level PSNR&#x2F;SSIM metrics. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yransun/LegoPET">https://github.com/yransun/LegoPET</a>. </p>
<blockquote>
<p>æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ç”±äºå…¶èƒ½å¤Ÿå¯è§†åŒ–ä½“å†…åŠŸèƒ½å’Œç”Ÿç‰©è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œå¹¿æ³›ç”¨äºç™Œç—‡æ£€æµ‹ã€‚PETå›¾åƒé€šå¸¸ä»ç›´æ–¹åŒ–åŸå§‹æ•°æ®ï¼ˆè¾›å‹’æ ¼ï¼‰ä½¿ç”¨ä¼ ç»Ÿçš„è¿­ä»£æŠ€æœ¯ï¼ˆä¾‹å¦‚OSEMã€MLEMï¼‰é‡å»ºã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•é€šè¿‡ç›´æ¥æ˜ å°„åŸå§‹è¾›å‹’æ ¼æ•°æ®åˆ°PETå›¾åƒæ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºå›å½’æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„DLæ–¹æ³•é€šå¸¸ä¼šäº§ç”Ÿè¿‡äºå¹³æ»‘çš„å›¾åƒæˆ–åˆ†åˆ«å¼•å…¥å„ç§ä¼ªå½±ã€‚å›¾åƒæ¡ä»¶æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆcPMSï¼‰æ˜¯å¦ä¸€ç±»åŸºäºå¯èƒ½æ€§çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸä¸”å¯æ§çš„å›¾åƒã€‚è™½ç„¶cPMSå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚åœ¨è¾“å…¥å’Œè¾“å‡ºå›¾åƒæ¥è‡ªä¸åŒåŸŸï¼ˆä¾‹å¦‚è¾›å‹’æ ¼ä¸å›¾åƒåŸŸï¼‰æ—¶ä¿æŒå¯¹åº”æ€§å’Œä¸€è‡´æ€§ï¼Œä»¥åŠæ”¶æ•›é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†LegoPETï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚ç‰¹å¾å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»è¾›å‹’æ ¼å¯¹PETå›¾åƒè¿›è¡Œé«˜è´¨é‡æ„ŸçŸ¥é‡å»ºã€‚æˆ‘ä»¬è¿›è¡Œäº†å‡ é¡¹å®éªŒï¼Œç»“æœè¡¨æ˜LegoPETä¸ä»…æé«˜äº†cPMSçš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨è§†è§‰è´¨é‡å’Œåƒç´ çº§å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰&#x2F;ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰æŒ‡æ ‡æ–¹é¢è¶…è¶Šäº†æœ€è¿‘çš„åŸºäºDLçš„PETå›¾åƒé‡å»ºæŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yransun/LegoPET%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yransun/LegoPETæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16629v1">PDF</a> 5 pages, 3 figures</p>
<p><strong>Summary</strong><br>PETå›¾åƒé‡å»ºæŠ€æœ¯åœ¨åŒ»å­¦è¯Šæ–­ä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚ä¼ ç»Ÿçš„è¿­ä»£æŠ€æœ¯ï¼Œå¦‚OSEMå’ŒMLEMï¼Œé€šå¸¸ä»ç›´æ–¹åŒ–çš„åŸå§‹æ•°æ®ï¼ˆè¾›æ°å›¾ï¼‰é‡å»ºå›¾åƒã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•é€šè¿‡ç›´æ¥æ˜ å°„åŸå§‹è¾›æ°å›¾æ•°æ®åˆ°PETå›¾åƒæ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼ŒåŸºäºå›å½’æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„DLæ–¹æ³•å¸¸å¸¸äº§ç”Ÿè¿‡äºå¹³æ»‘çš„å›¾åƒæˆ–å¼•å…¥å„ç§ä¼ªå½±ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†LegoPETï¼Œä¸€ç§åˆ†çº§ç‰¹å¾å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºä»è¾›æ°å›¾è¿›è¡Œé«˜è´¨é‡çš„PETå›¾åƒé‡å»ºã€‚å®éªŒè¡¨æ˜ï¼ŒLegoPETä¸ä»…æå‡äº†åŸºäºç‰¹å¾é‡‘å­—å¡”çš„ç½‘ç»œæ€§èƒ½ï¼Œè¿˜åœ¨è§†è§‰è´¨é‡å’Œåƒç´ çº§PSNR&#x2F;SSIMæŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€æ–°çš„DL-based PETå›¾åƒé‡å»ºæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETå¹¿æ³›ç”¨äºç™Œç—‡æ£€æµ‹ï¼Œèƒ½å¤Ÿå¯è§†åŒ–ä½“å†…åŠŸèƒ½å’Œç”Ÿç‰©è¿‡ç¨‹ã€‚</li>
<li>ä¼ ç»Ÿè¿­ä»£æŠ€æœ¯å¦‚OSEMå’ŒMLEMé€šå¸¸ç”¨äºä»è¾›æ°å›¾é‡å»ºPETå›¾åƒã€‚</li>
<li>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•å¯ç›´æ¥æ˜ å°„è¾›æ°å›¾æ•°æ®åˆ°PETå›¾åƒï¼Œå…·æœ‰æ½œåŠ›ã€‚</li>
<li>åŸºäºå›å½’æˆ–GANçš„DLæ–¹æ³•å¯èƒ½äº§ç”Ÿè¿‡äºå¹³æ»‘çš„å›¾åƒæˆ–å¼•å…¥ä¼ªå½±ã€‚</li>
<li>cDPMsæ˜¯ä¸€ç§åŸºäºæ¦‚ç‡çš„DLæŠ€æœ¯ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åº¦é€¼çœŸä¸”å¯æ§çš„å›¾åƒã€‚</li>
<li>cDPMsé¢ä¸´è·¨åŸŸå¯¹åº”æ€§å’Œä¸€è‡´æ€§çš„æŒ‘æˆ˜ï¼ˆå¦‚è¾›æ°å›¾ä¸å›¾åƒåŸŸä¹‹é—´ï¼‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62c994ffe7bd791bc5f23da154067037.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02ff8a50ca7d972a0fdef8c6bb7ce2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a12211f409c037af300ef45dd2d380dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5237f15bcbbce4298b010ed16cb47cca.jpg" align="middle">
</details>




<h2 id="MyTimeMachine-Personalized-Facial-Age-Transformation"><a href="#MyTimeMachine-Personalized-Facial-Age-Transformation" class="headerlink" title="MyTimeMachine: Personalized Facial Age Transformation"></a>MyTimeMachine: Personalized Facial Age Transformation</h2><p><strong>Authors:Luchao Qi, Jiaye Wu, Bang Gong, Annie N. Wang, David W. Jacobs, Roni Sengupta</strong></p>
<p>Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the personâ€™s appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20$\sim$40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches. </p>
<blockquote>
<p>é¢éƒ¨è€åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œå®ƒé«˜åº¦ä¾èµ–äºæ€§åˆ«ã€ç§æ—ã€ç”Ÿæ´»æ–¹å¼ç­‰å¤šç§å› ç´ ï¼Œå› æ­¤ï¼Œè¦æƒ³å‡†ç¡®é¢„æµ‹ä»»ä½•ä¸ªä½“çš„è€åŒ–æƒ…å†µå¹¶å­¦ä¹ å…¨å±€è€åŒ–å…ˆéªŒçŸ¥è¯†ï¼Œæ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æŠ€æœ¯è™½ç„¶èƒ½å¤Ÿäº§ç”ŸçœŸå®ä¸”åˆç†çš„è€åŒ–ç»“æœï¼Œä½†é‡æ–°è€åŒ–çš„å›¾åƒå¾€å¾€ä¸ç›®æ ‡å¹´é¾„çš„äººçš„å¤–è²Œä¸ç›¸ç¬¦ï¼Œå› æ­¤éœ€è¦è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®ã€‚åœ¨è™šæ‹Ÿè€åŒ–çš„è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œä¾‹å¦‚åœ¨ç”µå½±å’Œç”µè§†å‰§çš„VFXç‰¹æ•ˆä¸­ï¼Œé€šå¸¸å¯ä»¥è·å–åˆ°ç”¨æˆ·åœ¨çŸ­æ—¶é—´å†…ï¼ˆ20~40å¹´ï¼‰çš„ä¸ªäººç…§ç‰‡é›†æ¥å±•ç¤ºè€åŒ–è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œç®€å•å°è¯•å¯¹ä¸ªäººç…§ç‰‡é›†è¿›è¡Œå…¨å±€è€åŒ–æŠ€æœ¯çš„ä¸ªæ€§åŒ–è®¾ç½®å¾€å¾€ä¼šå¤±è´¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MyTimeMachineï¼ˆMyTMï¼‰ï¼Œå®ƒå°†å…¨å±€è€åŒ–å…ˆéªŒçŸ¥è¯†ä¸ä¸ªäººç…§ç‰‡é›†ï¼ˆä»…ä½¿ç”¨50å¼ å›¾åƒï¼‰ç›¸ç»“åˆï¼Œä»¥å­¦ä¹ ä¸ªæ€§åŒ–çš„å¹´é¾„è½¬å˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹é€‚é…å™¨ç½‘ç»œï¼Œå®ƒå°†ä¸ªæ€§åŒ–è€åŒ–ç‰¹å¾ä¸å…¨å±€è€åŒ–ç‰¹å¾ç›¸ç»“åˆï¼Œå¹¶ä½¿ç”¨StyleGAN2ç”Ÿæˆé‡æ–°è€åŒ–çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸‰ç§æŸå¤±å‡½æ•°æ¥ä¸ªæ€§åŒ–é€‚é…å™¨ç½‘ç»œï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è€åŒ–æŸå¤±ã€å¤–æ¨æ­£åˆ™åŒ–å’Œè‡ªé€‚åº”w-normæ­£åˆ™åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥æ‰©å±•åˆ°è§†é¢‘ï¼Œå®ç°é«˜è´¨é‡ã€èº«ä»½ä¿ç•™ã€æ—¶é—´ä¸€è‡´çš„è€åŒ–æ•ˆæœï¼Œä¸å®é™…ç›®æ ‡å¹´é¾„çš„å¤–è²Œç›¸ç¬¦ï¼Œè¯æ˜äº†å…¶åœ¨æœ€å…ˆè¿›æ–¹æ³•ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14521v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://mytimemachine.github.io/">https://mytimemachine.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢éƒ¨è¡°è€çš„å¤æ‚æ€§ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æŠ€æœ¯è™½ç„¶èƒ½ç”Ÿæˆé€¼çœŸçš„è¡°è€ç»“æœï¼Œä½†é‡æ–°ç”Ÿæˆçš„å›¾åƒå¾€å¾€ä¸èƒ½åæ˜ ç›®æ ‡å¹´é¾„çš„å¤–è§‚ç‰¹å¾ï¼Œå› æ­¤éœ€è¦ä¸ªæ€§åŒ–ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§ç»“åˆå…¨å±€è¡°è€å…ˆéªŒå’Œä¸ªäººç…§ç‰‡é›†ï¼ˆä»…ä½¿ç”¨50å¼ å›¾åƒï¼‰å­¦ä¹ ä¸ªæ€§åŒ–å¹´é¾„è½¬æ¢çš„æ–¹æ³•â€”â€”MyTimeMachineï¼ˆMyTMï¼‰ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°çš„Adapterç½‘ç»œï¼Œç»“åˆä¸ªæ€§åŒ–è¡°è€ç‰¹å¾å’Œå…¨å±€è¡°è€ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨StyleGAN2ç”Ÿæˆé‡æ–°è¡°è€çš„å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸‰ç§æŸå¤±å‡½æ•°æ¥ä¸ªæ€§åŒ–Adapterç½‘ç»œã€‚è¯¥æ–¹æ³•å¯æ‰©å±•åˆ°è§†é¢‘ï¼Œå®ç°é«˜è´¨é‡ã€èº«ä»½ä¿ç•™ã€æ—¶é—´ä¸€è‡´çš„è¡°è€æ•ˆæœï¼Œä¸å®é™…ç›®æ ‡å¹´é¾„çš„å¤–è§‚ç›¸åŒ¹é…ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨è¡°è€æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œå—åˆ°å¤šç§å› ç´ çš„å½±å“ï¼Œå¦‚æ€§åˆ«ã€ç§æ—å’Œç”Ÿæ´»æ–¹å¼ç­‰ï¼Œå› æ­¤å‡†ç¡®é¢„æµ‹ä¸ªä½“çš„è¡°è€å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯è™½ç„¶èƒ½äº§ç”Ÿé€¼çœŸçš„è¡°è€ç»“æœï¼Œä½†é‡æ–°ç”Ÿæˆçš„å›¾åƒå¾€å¾€æ— æ³•åæ˜ ç›®æ ‡å¹´é¾„çš„ä¸ªäººå¤–è§‚ç‰¹å¾ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•MyTimeMachineï¼ˆMyTMï¼‰ï¼Œç»“åˆå…¨å±€è¡°è€å…ˆéªŒå’Œä¸ªäººç…§ç‰‡é›†ï¼ˆä½¿ç”¨æå°‘çš„å›¾åƒï¼‰æ¥å­¦ä¹ ä¸ªæ€§åŒ–çš„å¹´é¾„è½¬æ¢ã€‚</li>
<li>å¼•å…¥äº†æ–°çš„Adapterç½‘ç»œï¼Œç»“åˆäº†ä¸ªæ€§åŒ–è¡°è€ç‰¹å¾å’Œå…¨å±€è¡°è€ç‰¹å¾ï¼Œç”Ÿæˆäº†é‡æ–°è¡°è€çš„å›¾åƒã€‚</li>
<li>ä½¿ç”¨äº†ä¸‰ç§æŸå¤±å‡½æ•°æ¥ä¸ªæ€§åŒ–Adapterç½‘ç»œï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è¡°è€æŸå¤±ã€å¤–æ¨æ­£åˆ™åŒ–å’Œè‡ªé€‚åº”w-normæ­£åˆ™åŒ–ã€‚</li>
<li>MyTimeMachineæ–¹æ³•å¯ä»¥æ‰©å±•åˆ°è§†é¢‘ï¼Œäº§ç”Ÿé«˜è´¨é‡ã€èº«ä»½ä¿ç•™ã€æ—¶é—´ä¸€è‡´çš„è¡°è€æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fdbe95498912c05bd1cdf10e0c3ac78b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5045782c44847db202c22ae1c3a30ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf36987c182119aafc14fca769428ad3.jpg" align="middle">
</details>




<h2 id="dc-GAN-Dual-Conditioned-GAN-for-Face-Demorphing-From-a-Single-Morph"><a href="#dc-GAN-Dual-Conditioned-GAN-for-Face-Demorphing-From-a-Single-Morph" class="headerlink" title="dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph"></a>dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph</h2><p><strong>Authors:Nitish Shukla, Arun Ross</strong></p>
<p>A facial morph is an image created by combining two face images pertaining to two distinct identities. Face demorphing inverts the process and tries to recover the original images constituting a facial morph. While morph attack detection (MAD) techniques can be used to flag morph images, they do not divulge any visual information about the faces used to create them. Demorphing helps address this problem. Existing demorphing techniques are either very restrictive (assume identities during testing) or produce feeble outputs (both outputs look very similar). In this paper, we overcome these issues by proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph images. Our method overcomes morph-replication and produces high quality reconstructions of the bonafide images used to create the morphs. Moreover, our method is highly generalizable across demorphing paradigms (differential&#x2F;reference-free). We conduct experiments on AMSL, FRLL-Morphs and MorDiff datasets to showcase the efficacy of our method. </p>
<blockquote>
<p>é¢éƒ¨å½¢æ€å­¦æ˜¯ä¸€ç§é€šè¿‡èåˆä¸¤å¼ å±äºä¸åŒèº«ä»½çš„äººè„¸å›¾åƒè€Œç”Ÿæˆçš„æ–°å‹å›¾åƒã€‚é¢éƒ¨åå½¢æ€å­¦åˆ™æ˜¯å¯¹è¿™ä¸€è¿‡ç¨‹è¿›è¡Œåè½¬ï¼Œè¯•å›¾æ¢å¤æ„æˆé¢éƒ¨å½¢æ€çš„åŸå§‹å›¾åƒã€‚è™½ç„¶å½¢æ€æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰æŠ€æœ¯å¯ä»¥ç”¨äºæ ‡è®°å½¢æ€å›¾åƒï¼Œä½†å®ƒä»¬ä¸ä¼šé€éœ²ç”¨äºåˆ›å»ºå®ƒä»¬çš„ä»»ä½•é¢éƒ¨è§†è§‰ä¿¡æ¯ã€‚åå½¢æ€å­¦æœ‰åŠ©äºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç°æœ‰çš„åå½¢æ€å­¦æŠ€æœ¯è¦ä¹ˆéå¸¸å—é™ï¼ˆåœ¨æµ‹è¯•æœŸé—´å‡å®šèº«ä»½ï¼‰ï¼Œè¦ä¹ˆäº§ç”Ÿçš„è¾“å‡ºè¾ƒå¼±ï¼ˆä¸¤ä¸ªè¾“å‡ºçœ‹èµ·æ¥éå¸¸ç›¸ä¼¼ï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºdc-GANï¼Œä¸€ç§åŸºäºæ–°å‹GANä¸”æ ¹æ®å½¢æ€å›¾åƒè¿›è¡Œæ¡ä»¶è®¾å®šçš„åå½¢æ€å­¦æ–¹æ³•ï¼Œå…‹æœäº†è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œå¹¶ç”Ÿæˆäº†ç”¨äºåˆ›å»ºå½¢æ€çš„é«˜å“è´¨é‡å»ºçš„è¯šä¿¡å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¾ˆé«˜çš„åå½¢æ€å­¦èŒƒå¼ï¼ˆå·®å¼‚&#x2F;å‚è€ƒè‡ªç”±ï¼‰çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬åœ¨AMSLã€FRLL-Morphså’ŒMorDiffæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œä»¥å±•ç¤ºæˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14494v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºGANçš„é¢éƒ¨å»å½¢æ€æŠ€æœ¯ï¼ˆdc-GANï¼‰ã€‚è¯¥æŠ€æœ¯å…‹æœäº†ç°æœ‰é¢éƒ¨å»å½¢æ€æ–¹æ³•å­˜åœ¨çš„é™åˆ¶å’Œé—®é¢˜ï¼Œèƒ½å¤ŸåŸºäºå½¢æ€å›¾åƒè¿›è¡Œæ¡ä»¶åŒ–æ“ä½œï¼Œè§£å†³äº†é¢éƒ¨å½¢æ€å›¾åƒå¤åŸçš„é—®é¢˜ã€‚æ–°æ–¹æ³•ä¸ä»…å…‹æœäº†å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œè¿˜èƒ½åœ¨è·¨å»å½¢æ€èŒƒå¼ï¼ˆå·®åˆ†&#x2F;å‚è€ƒè‡ªç”±ï¼‰çš„æƒ…å†µä¸‹å®ç°é«˜è´¨é‡é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AMSLã€FRLL-Morphså’ŒMorDiffæ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢éƒ¨å½¢æ€æ˜¯ä¸€ç§ç”±ä¸¤ä¸ªä¸åŒèº«ä»½çš„é¢éƒ¨å›¾åƒç»“åˆè€Œæˆçš„å›¾åƒã€‚</li>
<li>å»å½¢æ€æŠ€æœ¯è¯•å›¾æ¢å¤æ„æˆé¢éƒ¨å½¢æ€çš„åŸå§‹å›¾åƒã€‚</li>
<li>å½¢æ€æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰æŠ€æœ¯å¯ä»¥æ ‡è®°å½¢æ€å›¾åƒï¼Œä½†ä¸æä¾›å…³äºåˆ›å»ºå®ƒä»¬æ‰€ä½¿ç”¨çš„é¢éƒ¨çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>ç°æœ‰å»å½¢æ€æŠ€æœ¯å­˜åœ¨é™åˆ¶æˆ–è¾“å‡ºè´¨é‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>dc-GANæ˜¯ä¸€ç§åŸºäºGANçš„æ–°å‹å»å½¢æ€æ–¹æ³•ï¼Œå¯åŸºäºå½¢æ€å›¾åƒè¿›è¡Œæ¡ä»¶æ“ä½œã€‚</li>
<li>dc-GANå…‹æœäº†å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œå¹¶èƒ½é«˜è´¨é‡åœ°é‡å»ºç”¨äºåˆ›å»ºå½¢æ€çš„åŸå§‹å›¾åƒã€‚</li>
<li>dc-GANå…·æœ‰é«˜åº¦æ³›åŒ–æ€§ï¼Œå¯åº”ç”¨äºä¸åŒçš„å»å½¢æ€èŒƒå¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-445a4b23d88124f026866a9ef750a3dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85cf68766da569b5bf63c8a5f7291052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5413bb21b27da3cdb4125e50c7a9c6f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe7e53675735e2b37f039998ada34977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3ad372c8eea425fbd5dc03e4e57f70e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e520aa4d7945bfdc264ce02d2ec2079.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1e5b3debb53022d2214dd1aeb72c52f.jpg" align="middle">
</details>




<h2 id="HyperGAN-CLIP-A-Unified-Framework-for-Domain-Adaptation-Image-Synthesis-and-Manipulation"><a href="#HyperGAN-CLIP-A-Unified-Framework-for-Domain-Adaptation-Image-Synthesis-and-Manipulation" class="headerlink" title="HyperGAN-CLIP: A Unified Framework for Domain Adaptation, Image   Synthesis and Manipulation"></a>HyperGAN-CLIP: A Unified Framework for Domain Adaptation, Image   Synthesis and Manipulation</h2><p><strong>Authors:Abdul Basit Anees, Ahmet Canberk Baykal, Muhammed Burak Kizil, Duygu Ceylan, Erkut Erdem, Aykut Erdem</strong></p>
<p>Generative Adversarial Networks (GANs), particularly StyleGAN and its variants, have demonstrated remarkable capabilities in generating highly realistic images. Despite their success, adapting these models to diverse tasks such as domain adaptation, reference-guided synthesis, and text-guided manipulation with limited training data remains challenging. Towards this end, in this study, we present a novel framework that significantly extends the capabilities of a pre-trained StyleGAN by integrating CLIP space via hypernetworks. This integration allows dynamic adaptation of StyleGAN to new domains defined by reference images or textual descriptions. Additionally, we introduce a CLIP-guided discriminator that enhances the alignment between generated images and target domains, ensuring superior image quality. Our approach demonstrates unprecedented flexibility, enabling text-guided image manipulation without the need for text-specific training data and facilitating seamless style transfer. Comprehensive qualitative and quantitative evaluations confirm the robustness and superior performance of our framework compared to existing methods. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œç‰¹åˆ«æ˜¯StyleGANåŠå…¶å˜ä½“ï¼Œåœ¨ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒæ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å–å¾—äº†æˆåŠŸï¼Œä½†è¿™äº›æ¨¡å‹åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹é€‚åº”å„ç§ä»»åŠ¡ï¼Œå¦‚åŸŸé€‚åº”ã€å‚è€ƒå¼•å¯¼åˆæˆå’Œæ–‡æœ¬å¼•å¯¼æ“ä½œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¶…ç½‘ç»œé›†æˆäº†CLIPç©ºé—´ï¼Œä»è€Œæå¤§åœ°æ‰©å±•äº†é¢„è®­ç»ƒçš„StyleGANçš„èƒ½åŠ›ã€‚è¿™ç§é›†æˆå…è®¸æ ¹æ®å‚è€ƒå›¾åƒæˆ–æ–‡æœ¬æè¿°å®šä¹‰çš„æ–°åŸŸåŠ¨æ€åœ°é€‚åº”StyleGANã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†CLIPå¼•å¯¼çš„é‰´åˆ«å™¨ï¼Œå¢å¼ºäº†ç”Ÿæˆå›¾åƒä¸ç›®æ ‡åŸŸä¹‹é—´çš„å¯¹é½ï¼Œç¡®ä¿å›¾åƒè´¨é‡ä¸Šä¹˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿå®ç°æ— éœ€ç‰¹å®šæ–‡æœ¬è®­ç»ƒæ•°æ®çš„æ–‡æœ¬å¼•å¯¼å›¾åƒæ“ä½œï¼Œå¹¶å®ç°æ— ç¼é£æ ¼è½¬æ¢ã€‚ç»¼åˆçš„å®šæ€§å’Œå®šé‡è¯„ä¼°è¯å®ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å…·æœ‰ç¨³å¥æ€§å’Œå“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12832v1">PDF</a> Accepted for publication in SIGGRAPH Asia 2024. Project Website:   <a target="_blank" rel="noopener" href="https://cyberiada.github.io/HyperGAN-CLIP/">https://cyberiada.github.io/HyperGAN-CLIP/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡æ•´åˆCLIPç©ºé—´å’Œè¶…ç½‘ç»œï¼ˆhypernetworksï¼‰æ¥æ‰©å±•é¢„è®­ç»ƒStyleGANçš„èƒ½åŠ›çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶å®ç°äº†åŠ¨æ€é€‚åº”ç”±å‚è€ƒå›¾åƒæˆ–æ–‡æœ¬æè¿°å®šä¹‰çš„æ–°é¢†åŸŸï¼Œå¹¶å¼•å…¥CLIPå¯¼å‘åˆ¤åˆ«å™¨ï¼Œæé«˜ç”Ÿæˆå›¾åƒä¸ç›®æ ‡é¢†åŸŸä¹‹é—´çš„å¯¹é½åº¦ï¼Œç¡®ä¿å›¾åƒè´¨é‡ä¸Šä¹˜ã€‚æ­¤æ–°æ–¹æ³•å…·æœ‰å‰æ‰€æœªæœ‰çš„çµæ´»æ€§ï¼Œå¯å®ç°æ— éœ€ç‰¹å®šæ–‡æœ¬è®­ç»ƒæ•°æ®çš„æ–‡æœ¬å¯¼å‘å›¾åƒæ“ä½œï¼Œå¹¶èƒ½å®ç°æ— ç¼é£æ ¼è½¬æ¢ã€‚è¯¥æ¡†æ¶çš„ç¨³å¥æ€§å’Œå“è¶Šæ€§èƒ½å¾—åˆ°äº†ç»¼åˆå®šæ€§å’Œå®šé‡è¯„ä¼°çš„è¯å®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°æ¡†æ¶æˆåŠŸæ•´åˆCLIPç©ºé—´å’Œè¶…ç½‘ç»œï¼Œæ˜¾è‘—æ‰©å±•äº†StyleGANæ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†åŠ¨æ€é€‚åº”å‚è€ƒå›¾åƒæˆ–æ–‡æœ¬æè¿°å®šä¹‰çš„æ–°é¢†åŸŸã€‚</li>
<li>å¼•å…¥CLIPå¯¼å‘åˆ¤åˆ«å™¨ï¼Œå¢å¼ºäº†ç”Ÿæˆå›¾åƒä¸ç›®æ ‡é¢†åŸŸä¹‹é—´çš„å¯¹é½åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¿è¯å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œå…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§ã€‚</li>
<li>æ— éœ€ç‰¹å®šæ–‡æœ¬è®­ç»ƒæ•°æ®å³å¯å®ç°æ–‡æœ¬å¯¼å‘çš„å›¾åƒæ“ä½œã€‚</li>
<li>æ–°æ¡†æ¶èƒ½å®ç°æ— ç¼é£æ ¼è½¬æ¢ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96f15aec87713737a2334ed0a2f6c5d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4484d4e96bb3ce8847fcc9b0c7df6d7b.jpg" align="middle">
</details>




<h2 id="Local-Lesion-Generation-is-Effective-for-Capsule-Endoscopy-Image-Data-Augmentation-in-a-Limited-Data-Setting"><a href="#Local-Lesion-Generation-is-Effective-for-Capsule-Endoscopy-Image-Data-Augmentation-in-a-Limited-Data-Setting" class="headerlink" title="Local Lesion Generation is Effective for Capsule Endoscopy Image Data   Augmentation in a Limited Data Setting"></a>Local Lesion Generation is Effective for Capsule Endoscopy Image Data   Augmentation in a Limited Data Setting</h2><p><strong>Authors:Adrian B. ChÅ‚opowiec, Adam R. ChÅ‚opowiec, Krzysztof Galus, Wojciech Cebula, Martin Tabakov</strong></p>
<p>Limited medical imaging datasets challenge deep learning models by increasing risks of overfitting and reduced generalization, particularly in Generative Adversarial Networks (GANs), where discriminators may overfit, leading to training divergence. This constraint also impairs classification models trained on small datasets. Generative Data Augmentation (GDA) addresses this by expanding training datasets with synthetic data, although it requires training a generative model. We propose and evaluate two local lesion generation approaches to address the challenge of augmenting small medical image datasets. The first approach employs the Poisson Image Editing algorithm, a classical image processing technique, to create realistic image composites that outperform current state-of-the-art methods. The second approach introduces a novel generative method, leveraging a fine-tuned Image Inpainting GAN to synthesize realistic lesions within specified regions of real training images. A comprehensive comparison of the two proposed methods demonstrates that effective local lesion generation in a data-constrained setting allows for reaching new state-of-the-art results in capsule endoscopy lesion classification. Combination of our techniques achieves a macro F1-score of 33.07%, surpassing the previous best result by 7.84 percentage points (p.p.) on the highly imbalanced Kvasir Capsule Dataset, a benchmark for capsule endoscopy. To the best of our knowledge, this work is the first to apply a fine-tuned Image Inpainting GAN for GDA in medical imaging, demonstrating that an image-conditional GAN can be adapted effectively to limited datasets to generate high-quality examples, facilitating effective data augmentation. Additionally, we show that combining this GAN-based approach with classical image processing techniques further improves the results. </p>
<blockquote>
<p>æœ‰é™çš„åŒ»å­¦æˆåƒæ•°æ®é›†é€šè¿‡å¢åŠ è¿‡æ‹Ÿåˆå’Œé™ä½æ³›åŒ–çš„é£é™©æ¥æŒ‘æˆ˜æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­ï¼Œé‰´åˆ«å™¨å¯èƒ½ä¼šè¿‡æ‹Ÿåˆï¼Œä»è€Œå¯¼è‡´è®­ç»ƒå‘æ•£ã€‚è¿™ä¸€é™åˆ¶ä¹Ÿå½±å“äº†åœ¨å°å‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„åˆ†ç±»æ¨¡å‹ã€‚ç”Ÿæˆæ•°æ®å¢å¼ºï¼ˆGDAï¼‰é€šè¿‡ç”¨åˆæˆæ•°æ®æ‰©å……è®­ç»ƒæ•°æ®é›†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°½ç®¡è¿™éœ€è¦è®­ç»ƒä¸€ä¸ªç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºå¹¶è¯„ä¼°äº†ä¸¤ç§å±€éƒ¨ç—…å˜ç”Ÿæˆæ–¹æ³•æ¥åº”å¯¹å°å‹åŒ»å­¦å›¾åƒæ•°æ®é›†å¢å¼ºæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç¬¬ä¸€ç§æ–¹æ³•é‡‡ç”¨Poissonå›¾åƒç¼–è¾‘ç®—æ³•è¿™ä¸€ç»å…¸å›¾åƒå¤„ç†æŠ€æœ¯ï¼Œåˆ›å»ºé€¼çœŸçš„å›¾åƒç»„åˆï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç¬¬äºŒç§æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°çš„ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ç»è¿‡ç²¾ç»†è°ƒæ•´çš„å›¾åƒä¿®å¤GANåœ¨çœŸå®è®­ç»ƒå›¾åƒçš„æŒ‡å®šåŒºåŸŸå†…åˆæˆé€¼çœŸçš„ç—…å˜ã€‚å¯¹è¿™ä¸¤ç§æ–¹æ³•çš„å…¨é¢æ¯”è¾ƒè¡¨æ˜ï¼Œåœ¨æ•°æ®å—é™çš„ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„å±€éƒ¨ç—…å˜ç”Ÿæˆï¼Œå¯ä»¥åœ¨èƒ¶å›Šå†…é•œç—…å˜åˆ†ç±»æ–¹é¢è¾¾åˆ°æ–°çš„æœ€å…ˆè¿›çš„æˆæœã€‚ç»“åˆæˆ‘ä»¬çš„æŠ€æœ¯ï¼Œåœ¨é«˜åº¦ä¸å¹³è¡¡çš„Kvasirèƒ¶å›Šæ•°æ®é›†ä¸Šè¾¾åˆ°äº†33.07%çš„å®F1åˆ†æ•°ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å¥½çš„ç»“æœ7.84ä¸ªç™¾åˆ†ç‚¹ï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ¶å›Šå†…é•œçš„åŸºå‡†æµ‹è¯•é›†ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡å°†ç»è¿‡ç²¾ç»†è°ƒæ•´çš„å›¾åƒä¿®å¤GANåº”ç”¨äºåŒ»å­¦æˆåƒçš„GDAï¼Œè¡¨æ˜å›¾åƒæ¡ä»¶GANå¯ä»¥æœ‰æ•ˆåœ°é€‚åº”æœ‰é™æ•°æ®é›†ï¼Œç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ï¼Œå®ç°æœ‰æ•ˆçš„æ•°æ®å¢å¼ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå°†åŸºäºGANçš„æ–¹æ³•ä¸ç»å…¸å›¾åƒå¤„ç†æŠ€æœ¯ç›¸ç»“åˆå¯ä»¥è¿›ä¸€æ­¥æé«˜ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03098v2">PDF</a> 54 pages, 35 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦æˆåƒæ•°æ®é›†æœ‰é™çš„é—®é¢˜ï¼Œç ”ç©¶æå‡ºä¸¤ç§å±€éƒ¨ç—…å˜ç”Ÿæˆæ–¹æ³•ï¼Œä»¥æ‰©å……å°æ•°æ®é›†å¹¶æå‡åˆ†ç±»æ¨¡å‹æ€§èƒ½ã€‚ç¬¬ä¸€ç§æ–¹æ³•ä½¿ç”¨Poissonå›¾åƒç¼–è¾‘ç®—æ³•ç”Ÿæˆåˆæˆå›¾åƒï¼›ç¬¬äºŒç§æ–¹æ³•åˆ™è¿ç”¨å¾®è°ƒåçš„å›¾åƒä¿®å¤ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆImage Inpainting GANï¼‰åœ¨çœŸå®å›¾åƒæŒ‡å®šåŒºåŸŸå†…åˆæˆçœŸå®ç—…å˜ã€‚ä¸¤ç§æ–¹æ³•ç»“åˆè¾¾åˆ°äº†æ–°çš„å…ˆè¿›ç»“æœï¼Œåœ¨èƒ¶å›Šå†…é•œç—…å˜åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚ç ”ç©¶æ˜¯é¦–æ¬¡å°†å¾®è°ƒåçš„Image Inpainting GANåº”ç”¨äºåŒ»å­¦æˆåƒçš„æ•°æ®å¢å¼ºï¼Œå¹¶è¯æ˜å…¶é€‚åº”æœ‰é™æ•°æ®é›†ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬çš„æœ‰æ•ˆæ€§ã€‚ç»“åˆç»å…¸å›¾åƒå¤„ç†æŠ€æœ¯è¿›ä¸€æ­¥æé«˜äº†ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒæ•°æ®é›†æœ‰é™çš„é—®é¢˜ä¼šå¢å¤§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„è¿‡æ‹Ÿåˆé£é™©ï¼Œé™ä½å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç”Ÿæˆæ•°æ®å¢å¼ºï¼ˆGDAï¼‰é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥æ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œä½†éœ€è¦è®­ç»ƒç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>ç¬¬ä¸€ç§æå‡ºçš„å±€éƒ¨ç—…å˜ç”Ÿæˆæ–¹æ³•åˆ©ç”¨Poissonå›¾åƒç¼–è¾‘ç®—æ³•ç”ŸæˆçœŸå®å›¾åƒå¤åˆä½“ï¼Œå…¶æ€§èƒ½ä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>ç¬¬äºŒç§æ–¹æ³•ä½¿ç”¨å¾®è°ƒåçš„Image Inpainting GANåœ¨çœŸå®è®­ç»ƒå›¾åƒçš„æŒ‡å®šåŒºåŸŸåˆæˆçœŸå®ç—…å˜ã€‚</li>
<li>ç»“åˆä¸¤ç§æå‡ºçš„æ–¹æ³•åœ¨æ•°æ®å—é™çš„æƒ…å†µä¸‹å®ç°äº†æœ‰æ•ˆçš„å±€éƒ¨ç—…å˜ç”Ÿæˆï¼Œå¹¶åœ¨èƒ¶å›Šå†…é•œç—…å˜åˆ†ç±»ä¸Šè¾¾åˆ°äº†æ–°çš„å…ˆè¿›ç»“æœã€‚</li>
<li>åœ¨Kvasirèƒ¶å›Šæ•°æ®é›†ä¸Šçš„å®è§‚F1åˆ†æ•°è¾¾åˆ°äº†33.07%ï¼Œè¾ƒä¹‹å‰æœ€ä½³ç»“æœæé«˜äº†7.84ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8994323afe2d8ebb04785af4b201a5ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad306dcdc263ec356455c7ebed896cdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e9d771c5740ef60856da70e460bae5d.jpg" align="middle">
</details>




<h2 id="HairDiffusion-Vivid-Multi-Colored-Hair-Editing-via-Latent-Diffusion"><a href="#HairDiffusion-Vivid-Multi-Colored-Hair-Editing-via-Latent-Diffusion" class="headerlink" title="HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion"></a>HairDiffusion: Vivid Multi-Colored Hair Editing via Latent Diffusion</h2><p><strong>Authors:Yu Zeng, Yang Zhang, Jiachen Liu, Linlin Shen, Kaijun Deng, Weizhao He, Jinbao Wang</strong></p>
<p>Hair editing is a critical image synthesis task that aims to edit hair color and hairstyle using text descriptions or reference images, while preserving irrelevant attributes (e.g., identity, background, cloth). Many existing methods are based on StyleGAN to address this task. However, due to the limited spatial distribution of StyleGAN, it struggles with multiple hair color editing and facial preservation. Considering the advancements in diffusion models, we utilize Latent Diffusion Models (LDMs) for hairstyle editing. Our approach introduces Multi-stage Hairstyle Blend (MHB), effectively separating control of hair color and hairstyle in diffusion latent space. Additionally, we train a warping module to align the hair color with the target region. To further enhance multi-color hairstyle editing, we fine-tuned a CLIP model using a multi-color hairstyle dataset. Our method not only tackles the complexity of multi-color hairstyles but also addresses the challenge of preserving original colors during diffusion editing. Extensive experiments showcase the superiority of our method in editing multi-color hairstyles while preserving facial attributes given textual descriptions and reference images. </p>
<blockquote>
<p>å¤´å‘ç¼–è¾‘æ˜¯ä¸€é¡¹å…³é”®çš„å›¾åƒåˆæˆä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿ç”¨æ–‡æœ¬æè¿°æˆ–å‚è€ƒå›¾åƒæ¥ç¼–è¾‘å‘è‰²å’Œå‘å‹ï¼ŒåŒæ—¶ä¿ç•™æ— å…³å±æ€§ï¼ˆä¾‹å¦‚èº«ä»½ã€èƒŒæ™¯ã€æœè£…ï¼‰ã€‚è®¸å¤šç°æœ‰æ–¹æ³•åŸºäºStyleGANæ¥è§£å†³æ­¤ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºStyleGANçš„ç©ºé—´åˆ†å¸ƒæœ‰é™ï¼Œå®ƒåœ¨å¤šè‰²å‘è‰²ç¼–è¾‘å’Œé¢éƒ¨ä¿ç•™æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚è€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹çš„è¿›æ­¥ï¼Œæˆ‘ä»¬åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰è¿›è¡Œå‘å‹ç¼–è¾‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†å¤šé˜¶æ®µå‘å‹æ··åˆï¼ˆMHBï¼‰ï¼Œæœ‰æ•ˆåœ°åœ¨æ‰©æ•£æ½œåœ¨ç©ºé—´ä¸­åˆ†ç¦»äº†å‘è‰²å’Œå‘å‹æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå˜å½¢æ¨¡å—ï¼Œä»¥å°†å‘è‰²ä¸ç›®æ ‡åŒºåŸŸå¯¹é½ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¤šè‰²å‘å‹ç¼–è¾‘ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šè‰²å‘å‹æ•°æ®é›†å¯¹CLIPæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è§£å†³äº†å¤šè‰²å‘å‹ç¼–è¾‘çš„å¤æ‚æ€§ï¼Œè¿˜è§£å†³äº†åœ¨æ‰©æ•£ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿ç•™åŸå§‹é¢œè‰²çš„æŒ‘æˆ˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»™å®šæ–‡æœ¬æè¿°å’Œå‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹ï¼Œåœ¨ç¼–è¾‘å¤šè‰²å‘å‹çš„åŒæ—¶ä¿ç•™é¢éƒ¨å±æ€§æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21789v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬æè¿°æˆ–å‚è€ƒå›¾åƒè¿›è¡Œå¤´å‘ç¼–è¾‘æ˜¯ä¸€é¡¹é‡è¦çš„å›¾åƒåˆæˆä»»åŠ¡ï¼Œæ—¨åœ¨æ”¹å˜å‘è‰²å’Œå‘å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–æ— å…³å±æ€§ã€‚å°½ç®¡è®¸å¤šç°æœ‰æ–¹æ³•åŸºäºStyleGANï¼Œä½†å…¶æœ‰é™çš„ç©ºé—´åˆ†å¸ƒå¯¼è‡´åœ¨å¤šé‡å‘è‰²ç¼–è¾‘å’Œé¢éƒ¨ä¿ç•™æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelsï¼ŒLDMsï¼‰è¿›è¡Œå‘å‹ç¼–è¾‘ï¼Œæå‡ºå¤šé˜¶æ®µå‘å‹èåˆï¼ˆMHBï¼‰æ–¹æ³•ï¼Œæœ‰æ•ˆåˆ†ç¦»å‘è‰²å’Œå‘å‹åœ¨æ‰©æ•£æ½œåœ¨ç©ºé—´ä¸­çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼Œè®­ç»ƒäº†é¢œè‰²å¯¹é½æ¨¡å—ï¼Œä½¿å‘è‰²ä¸ç›®æ ‡åŒºåŸŸç›¸åŒ¹é…ã€‚é€šè¿‡å¾®è°ƒCLIPæ¨¡å‹å’Œå¤šè‰²å‘å‹æ•°æ®é›†ï¼Œä¸ä»…è§£å†³äº†å¤šè‰²å‘å‹çš„å¤æ‚æ€§ï¼Œè¿˜è§£å†³äº†æ‰©æ•£ç¼–è¾‘è¿‡ç¨‹ä¸­ä¿æŒåŸå§‹é¢œè‰²çš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç¼–è¾‘å¤šè‰²å‘å‹æ—¶å…·æœ‰ä¼˜è¶Šæ€§ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æè¿°å’Œå‚è€ƒå›¾åƒä¿ç•™é¢éƒ¨ç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤´å‘ç¼–è¾‘æ˜¯å›¾åƒåˆæˆä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡æ–‡æœ¬æè¿°æˆ–å‚è€ƒå›¾åƒç¼–è¾‘å¤´å‘é¢œè‰²å’Œå‘å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶ä»–å±æ€§ã€‚</li>
<li>StyleGANç”±äºç©ºé—´åˆ†å¸ƒé™åˆ¶ï¼Œåœ¨å¤šé‡å‘è‰²ç¼–è¾‘å’Œé¢éƒ¨ä¿ç•™æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨Latent Diffusion Modelsï¼ˆLDMsï¼‰è¿›è¡Œå‘å‹ç¼–è¾‘ï¼Œæå‡ºå¤šé˜¶æ®µå‘å‹èåˆï¼ˆMHBï¼‰æ–¹æ³•ä»¥æ”¹å–„å‘è‰²å’Œå‘å‹æ§åˆ¶ã€‚</li>
<li>é€šè¿‡è®­ç»ƒé¢œè‰²å¯¹é½æ¨¡å—ï¼Œä½¿ä¿®æ”¹åçš„å‘è‰²ä¸ç›®æ ‡åŒºåŸŸç›¸åŒ¹é…ã€‚</li>
<li>åˆ©ç”¨å¤šè‰²å‘å‹æ•°æ®é›†å¾®è°ƒCLIPæ¨¡å‹ï¼Œå¢å¼ºå¤šè‰²å‘å‹çš„ç¼–è¾‘èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•èƒ½åœ¨ä¿æŒé¢éƒ¨ç‰¹å¾çš„åŒæ—¶è¿›è¡Œå¤šè‰²å‘å‹çš„ç¼–è¾‘ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ç¼–è¾‘å¤šè‰²å‘å‹æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7b04056c2cbeae97355cbb81e2bb9b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5927d82c4b5d0fc4850cc5c11e343ad3.jpg" align="middle">
</details>




<h2 id="Unsupervised-Panoptic-Interpretation-of-Latent-Spaces-in-GANs-Using-Space-Filling-Vector-Quantization"><a href="#Unsupervised-Panoptic-Interpretation-of-Latent-Spaces-in-GANs-Using-Space-Filling-Vector-Quantization" class="headerlink" title="Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using   Space-Filling Vector Quantization"></a>Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using   Space-Filling Vector Quantization</h2><p><strong>Authors:Mohammad Hassan Vali, Tom BÃ¤ckstrÃ¶m</strong></p>
<p>Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions that require exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space and thus make it interpretable. We apply this technique to model the latent space of pretrained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space that determines which part of the latent space corresponds to what specific generative factors. Furthermore, we demonstrate that each line of SFVQâ€™s curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also showed that the points located on an SFVQ line can be used for controllable data augmentation. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å­¦ä¹ ä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œè¯¥ç©ºé—´çš„æ ·æœ¬å¯ä»¥æ˜ å°„åˆ°ç°å®ä¸–ç•Œå›¾åƒã€‚è¿™æ ·çš„æ½œåœ¨ç©ºé—´å¾ˆéš¾è§£é‡Šã€‚ä¸€äº›æ—©æœŸçš„ç›‘ç£æ–¹æ³•æ—¨åœ¨åˆ›å»ºä¸€ä¸ªå¯è§£é‡Šçš„æ½œåœ¨ç©ºé—´æˆ–å‘ç°å¯è§£é‡Šçš„æ–¹å‘ï¼Œè¿™éœ€è¦åˆ©ç”¨æ•°æ®æ ‡ç­¾æˆ–æ³¨é‡Šçš„åˆæˆæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºç©ºé—´å¡«å……å‘é‡é‡åŒ–ï¼ˆSFVQï¼‰çš„å‘é‡é‡åŒ–ä¿®æ”¹æ–¹æ³•ï¼Œå®ƒåœ¨åˆ†æ®µçº¿æ€§æ›²çº¿ä¸Šå¯¹æ•°æ®è¿›è¡Œé‡åŒ–ã€‚SFVQå¯ä»¥æ•è·æ½œåœ¨ç©ºé—´çš„åŸºæœ¬å½¢æ€ç»“æ„ï¼Œä»è€Œä½¿å…¶å…·æœ‰å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬å°†æ­¤æŠ€æœ¯åº”ç”¨äºå„ç§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„StyleGAN2å’ŒBigGANç½‘ç»œçš„æ½œåœ¨ç©ºé—´å»ºæ¨¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSFVQæ›²çº¿æä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ½œåœ¨ç©ºé—´å¯è§£é‡Šæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç¡®å®šäº†æ½œåœ¨ç©ºé—´çš„å“ªä¸€éƒ¨åˆ†å¯¹åº”äºå“ªäº›ç‰¹å®šçš„ç”Ÿæˆå› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜SFVQæ›²çº¿çš„æ¯ä¸€è¡Œéƒ½å¯èƒ½æ˜¯ä¸€ä¸ªå¯è§£é‡Šçš„æ–¹å‘ï¼Œç”¨äºåº”ç”¨å¯ç†è§£çš„å›¾åƒè½¬æ¢ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½äºSFVQçº¿ä¸Šçš„ç‚¹å¯ç”¨äºå¯æ§çš„æ•°æ®å¢å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20573v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å­¦ä¹ ä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œè¯¥ç©ºé—´çš„æ ·æœ¬å¯ä»¥æ˜ å°„åˆ°çœŸå®å›¾åƒã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºç©ºé—´å¡«å……çŸ¢é‡é‡åŒ–ï¼ˆSFVQï¼‰çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ— éœ€æ•°æ®æ ‡ç­¾æˆ–åˆæˆæ ·æœ¬æ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œä½¿æ½œåœ¨ç©ºé—´å…·æœ‰å¯è§£é‡Šæ€§ã€‚é€šè¿‡åº”ç”¨SFVQæŠ€æœ¯äºé¢„è®­ç»ƒçš„StyleGAN2å’ŒBigGANç½‘ç»œçš„ä¸åŒæ•°æ®é›†ä¸Šï¼Œå®éªŒè¡¨æ˜SFVQæ›²çº¿æä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ½œåœ¨ç©ºé—´è§£é‡Šæ¨¡å‹ï¼Œå¯ä»¥ç¡®å®šæ½œåœ¨ç©ºé—´çš„å“ªä¸€éƒ¨åˆ†å¯¹åº”äºå“ªäº›ç‰¹å®šçš„ç”Ÿæˆå› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†SFVQæ›²çº¿çš„æ¯ä¸€è¡Œéƒ½å¯èƒ½æŒ‡ä»£ä¸€ä¸ªå¯è§£é‡Šçš„æ–¹å‘ï¼Œç”¨äºåº”ç”¨å¯ç†è§£çš„å›¾åƒè½¬æ¢ã€‚åŒæ—¶ï¼Œä½äºSFVQçº¿ä¸Šçš„ç‚¹ä¹Ÿå¯ç”¨äºå¯æ§çš„æ•°æ®å¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å­¦ä¹ æ½œåœ¨ç©ºé—´ï¼Œå°†å…¶æ ·æœ¬æ˜ å°„åˆ°çœŸå®å›¾åƒã€‚</li>
<li>æ½œåœ¨ç©ºé—´å¾ˆéš¾è§£é‡Šï¼Œæ—©æœŸç›‘ç£æ–¹æ³•æ—¨åœ¨åˆ›å»ºå¯è§£é‡Šçš„ç©ºé—´æˆ–å‘ç°å¯è§£é‡Šçš„æ–¹å‘ï¼Œä½†éœ€è¦æ•°æ®æ ‡ç­¾æˆ–åˆæˆæ ·æœ¬æ ‡æ³¨ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºç©ºé—´å¡«å……çŸ¢é‡é‡åŒ–ï¼ˆSFVQï¼‰çš„æŠ€æœ¯ï¼Œå¯ä»¥åœ¨æ— éœ€æ ‡æ³¨çš„æƒ…å†µä¸‹ä½¿æ½œåœ¨ç©ºé—´å…·æœ‰å¯è§£é‡Šæ€§ã€‚</li>
<li>SFVQé€šè¿‡é‡åŒ–æ•°æ®åœ¨åˆ†æ®µçº¿æ€§æ›²çº¿ä¸Šï¼Œèƒ½å¤Ÿæ•æ‰æ½œåœ¨ç©ºé—´çš„åŸºæœ¬å½¢æ€ç»“æ„ã€‚</li>
<li>åº”ç”¨SFVQåˆ°é¢„è®­ç»ƒç½‘ç»œï¼ˆå¦‚StyleGAN2å’ŒBigGANï¼‰çš„ä¸åŒæ•°æ®é›†ä¸Šï¼Œå®éªŒè¯å®SFVQæ›²çº¿æä¾›äº†ä¸€ä¸ªè§£é‡Šæ½œåœ¨ç©ºé—´çš„é€šç”¨æ¨¡å‹ã€‚</li>
<li>SFVQæ›²çº¿çš„æ¯ä¸€è¡Œéƒ½å¯èƒ½ä»£è¡¨ä¸€ä¸ªå¯è§£é‡Šçš„æ–¹å‘ï¼Œå¯è¿›è¡Œå¯æ§çš„å›¾åƒè½¬æ¢ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5f77ff4570e402ea84d169c37934370.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0096b0fd4525a5c07987695ecf8d9fc5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-609af50c9320e06abebacaf6b9a5b180.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9ea1106462cdaee9c511d70363d4d8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd7f7df8171717d4bf5bc5aa5dc8b3bc.jpg" align="middle">
</details>




<h2 id="Medical-Imaging-Complexity-and-its-Effects-on-GAN-Performance"><a href="#Medical-Imaging-Complexity-and-its-Effects-on-GAN-Performance" class="headerlink" title="Medical Imaging Complexity and its Effects on GAN Performance"></a>Medical Imaging Complexity and its Effects on GAN Performance</h2><p><strong>Authors:William Cagas, Chan Ko, Blake Hsiao, Shryuk Grandhi, Rishi Bhattacharya, Kevin Zhu, Michael Lam</strong></p>
<p>The proliferation of machine learning models in diverse clinical applications has led to a growing need for high-fidelity, medical image training data. Such data is often scarce due to cost constraints and privacy concerns. Alleviating this burden, medical image synthesis via generative adversarial networks (GANs) emerged as a powerful method for synthetically generating photo-realistic images based on existing sets of real medical images. However, the exact image set size required to efficiently train such a GAN is unclear. In this work, we experimentally establish benchmarks that measure the relationship between a sample dataset size and the fidelity of the generated images, given the datasetâ€™s distribution of image complexities. We analyze statistical metrics based on delentropy, an image complexity measure rooted in Shannonâ€™s entropy in information theory. For our pipeline, we conduct experiments with two state-of-the-art GANs, StyleGAN 3 and SPADE-GAN, trained on multiple medical imaging datasets with variable sample sizes. Across both GANs, general performance improved with increasing training set size but suffered with increasing complexity. </p>
<blockquote>
<p>åœ¨å¤šç§ä¸´åºŠåº”ç”¨ä¸­ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ™®åŠå¯¼è‡´äº†å¯¹äºé«˜ä¿çœŸåŒ»ç–—å›¾åƒè®­ç»ƒæ•°æ®æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚ç”±äºæˆæœ¬çº¦æŸå’Œéšç§æ‹…å¿§ï¼Œæ­¤ç±»æ•°æ®é€šå¸¸å¾ˆç¨€ç¼ºã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€è´Ÿæ‹…ï¼Œé€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒåˆæˆå‡ºç°ä¸ºä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œå¯ä»¥åŸºäºç°æœ‰çš„çœŸå®åŒ»å­¦å›¾åƒé›†åˆæˆç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚ç„¶è€Œï¼Œä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒè¿™æ ·çš„GANæ‰€éœ€è¦çš„ç²¾ç¡®å›¾åƒé›†å¤§å°å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®éªŒå»ºç«‹äº†åŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•è¡¡é‡æ ·æœ¬æ•°æ®é›†å¤§å°ä¸ç”Ÿæˆå›¾åƒçš„ä¿çœŸåº¦ä¹‹é—´çš„å…³ç³»ï¼Œè€ƒè™‘åˆ°æ•°æ®é›†å›¾åƒå¤æ‚æ€§çš„åˆ†å¸ƒã€‚æˆ‘ä»¬åˆ†æäº†åŸºäºä¿¡æ¯ç†µçš„delentropyçš„ç»Ÿè®¡æŒ‡æ ‡ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¿¡æ¯è®ºä¸­çš„é¦™å†œç†µçš„å›¾åƒå¤æ‚æ€§åº¦é‡ã€‚åœ¨æˆ‘ä»¬çš„ç®¡é“ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªæœ€å…ˆè¿›çš„GANï¼ˆStyleGAN 3å’ŒSPADE-GANï¼‰è¿›è¡Œäº†å®éªŒï¼Œå®ƒä»¬åœ¨å¤šä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ ·æœ¬å¤§å°å¯å˜ã€‚åœ¨è¿™ä¸¤ç§GANä¸­ï¼Œéšç€è®­ç»ƒé›†å¤§å°çš„å¢åŠ ï¼Œæ€»ä½“æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œä½†éšç€å¤æ‚æ€§çš„å¢åŠ ï¼Œæ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17959v1">PDF</a> Accepted to ACCV, Workshop on Generative AI for Synthetic Medical   Data</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åˆæˆåŒ»å­¦å›¾åƒçš„æ–¹æ³•ï¼Œä»¥è§£å†³å› æˆæœ¬é™åˆ¶å’Œéšç§æ‹…å¿§å¯¼è‡´çš„åŒ»å­¦å›¾åƒè®­ç»ƒæ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚å®éªŒå»ºç«‹äº†æ•°æ®é›†æ ·æœ¬é‡ä¸ç”Ÿæˆå›¾åƒä¿çœŸåº¦ä¹‹é—´çš„åŸºå‡†å…³ç³»ï¼Œåˆ†æåŸºäºä¿¡æ¯è®ºä¸­çš„é¦™å†œç†µå»¶ä¼¸å‡ºçš„å›¾åƒå¤æ‚åº¦åº¦é‡æ–¹æ³•â€”â€”å¾·å°”å¡”ç†µã€‚å®éªŒé‡‡ç”¨ä¸¤ç§å…ˆè¿›çš„GANsï¼ˆStyleGAN 3å’ŒSPADE-GANï¼‰ï¼Œåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå‘ç°éšç€è®­ç»ƒé›†æ ·æœ¬é‡çš„å¢åŠ ï¼Œæ€»ä½“æ€§èƒ½æå‡ï¼Œä½†åœ¨å¤æ‚åº¦å¢åŠ çš„æƒ…å†µä¸‹æ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ ä»»åŠ¡å¯¹é«˜ä¿çœŸåŒ»å­¦å›¾åƒè®­ç»ƒæ•°æ®çš„éœ€æ±‚æ—¥ç›Šå¢å¼ºã€‚</li>
<li>GANsèƒ½æœ‰æ•ˆåˆæˆé€¼çœŸçš„åŒ»å­¦å›¾åƒã€‚</li>
<li>è®­ç»ƒé›†æ ·æœ¬é‡ä¸ç”Ÿæˆå›¾åƒçš„è´¨é‡å­˜åœ¨ç›´æ¥å…³ç³»ã€‚</li>
<li>ä½¿ç”¨äº†åŸºäºå¾·å°”å¡”ç†µçš„ç»Ÿè®¡æŒ‡æ ‡è¡¡é‡å›¾åƒå¤æ‚åº¦å¯¹è®­ç»ƒè¿‡ç¨‹çš„å½±å“ã€‚</li>
<li>éšç€è®­ç»ƒé›†æ ·æœ¬é‡å¢åŠ ï¼Œæ€»ä½“æ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6b2c55b5a4cb62ab46d46992a8439a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-badd48b65c63c0a89c9d14f0a503982a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b28eb2df1ab5ee33c43abc25033bb6.jpg" align="middle">
</details>




<h2 id="3D-GANTex-3D-Face-Reconstruction-with-StyleGAN3-based-Multi-View-Images-and-3DDFA-based-Mesh-Generation"><a href="#3D-GANTex-3D-Face-Reconstruction-with-StyleGAN3-based-Multi-View-Images-and-3DDFA-based-Mesh-Generation" class="headerlink" title="3D-GANTex: 3D Face Reconstruction with StyleGAN3-based Multi-View Images   and 3DDFA based Mesh Generation"></a>3D-GANTex: 3D Face Reconstruction with StyleGAN3-based Multi-View Images   and 3DDFA based Mesh Generation</h2><p><strong>Authors:Rohit Das, Tzung-Han Lin, Ko-Chih Wang</strong></p>
<p>Geometry and texture estimation from a single face image is an ill-posed problem since there is very little information to work with. The problem further escalates when the face is rotated at a different angle. This paper tries to tackle this problem by introducing a novel method for texture estimation from a single image by first using StyleGAN and 3D Morphable Models. The method begins by generating multi-view faces using the latent space of GAN. Then 3DDFA trained on 3DMM estimates a 3D face mesh as well as a high-resolution texture map that is consistent with the estimated face shape. The result shows that the generated mesh is of high quality with near to accurate texture representation. </p>
<blockquote>
<p>ä»å•å¼ äººè„¸å›¾åƒä¸­ä¼°è®¡å‡ ä½•å’Œçº¹ç†æ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ï¼Œå› ä¸ºå¯ç”¨ä¿¡æ¯éå¸¸å°‘ã€‚å½“äººè„¸ä»¥ä¸åŒè§’åº¦æ—‹è½¬æ—¶ï¼Œè¿™ä¸ªé—®é¢˜ä¼šè¿›ä¸€æ­¥åŠ å‰§ã€‚æœ¬æ–‡è¯•å›¾é€šè¿‡å¼•å…¥ä¸€ç§ä»å•å¼ å›¾åƒè¿›è¡Œçº¹ç†ä¼°è®¡çš„æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨StyleGANå’Œ3Då¯å˜å½¢æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨GANçš„æ½œåœ¨ç©ºé—´ç”Ÿæˆå¤šè§’åº¦äººè„¸ï¼Œç„¶åä½¿ç”¨åŸºäº3DMMè®­ç»ƒçš„3DDFAä¼°è®¡ä¸€ä¸ªä¸ä¼°è®¡çš„äººè„¸å½¢çŠ¶ä¸€è‡´çš„é«˜åˆ†è¾¨ç‡çº¹ç†è´´å›¾ä»¥åŠ3Däººè„¸ç½‘æ ¼ã€‚ç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„ç½‘æ ¼å…·æœ‰é«˜è´¨é‡çš„è¿‘ä¼¼å‡†ç¡®çº¹ç†è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16009v1">PDF</a> 7 pages, 4 figures, 2 tables, pre-print version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºStyleGANå’Œ3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰çš„å•å›¾åƒçº¹ç†ä¼°è®¡æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨GANçš„æ½œåœ¨ç©ºé—´ç”Ÿæˆå¤šè§’åº¦äººè„¸ï¼Œå¹¶ç»“åˆ3DDFAè¿›è¡Œ3Däººè„¸ç½‘æ ¼å’Œçº¹ç†å›¾çš„ä¼°ç®—ï¼Œä»è€Œè§£å†³å•äººè„¸å›¾åƒå‡ ä½•å’Œçº¹ç†ä¼°è®¡çš„ä¸é€‚å®šé—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„ç½‘æ ¼å…·æœ‰é«˜è´¨é‡ä¸”çº¹ç†è¡¨ç¤ºå‡†ç¡®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•äººè„¸å›¾åƒå‡ ä½•å’Œçº¹ç†ä¼°è®¡æ˜¯ä¸€ä¸ªä¸é€‚å®šé—®é¢˜ï¼Œç¼ºä¹è¶³å¤Ÿçš„ä¿¡æ¯è¿›è¡Œå‡†ç¡®åˆ†æã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨StyleGANç”Ÿæˆå¤šè§’åº¦äººè„¸å›¾åƒï¼Œè§£å†³äº†å•ä¸€è§†è§’ä¿¡æ¯ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥3Då¯å˜å½¢æ¨¡å‹ï¼ˆ3DMMï¼‰ä»¥è¾…åŠ©ä¼°ç®—3Däººè„¸ç½‘æ ¼ã€‚</li>
<li>3DDFAè¢«ç”¨äºåŸºäº3DMMçš„ä¼°è®¡ï¼Œç”Ÿæˆä¸ä¼°è®¡çš„äººè„¸å½¢çŠ¶ä¸€è‡´çš„é«˜åˆ†è¾¨ç‡çº¹ç†å›¾ã€‚</li>
<li>ç”Ÿæˆçš„ç½‘æ ¼å…·æœ‰é«˜è´¨é‡ï¼Œçº¹ç†è¡¨ç¤ºå‡†ç¡®ã€‚</li>
<li>è¯¥æ–¹æ³•æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºä»å•ä¸€å›¾åƒä¸­ä¼°è®¡äººè„¸çš„å‡ ä½•å’Œçº¹ç†ä¿¡æ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-876dcd643881cbe8c2fc905701e4b448.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-830b063c6ced591208a8e58bfa979561.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8040fab80107283ebd67fb573b83d7f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51104107ae5fef6c6e66005f522671d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2a598bd3ac413dba0a9fe1c499fe64e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-802ec736d7626d0e95df81263c4b6705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ad82c0cb9b38eee789d56d432d510cf.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ba1148f524c897fed52125ae4e1dc003.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  MixedGaussianAvatar Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79d8c530a223fa85974e2a7b72c453cc.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Swap Path Network for Robust Person Search Pre-training
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">11676k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
