<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  It Takes Two Real-time Co-Speech Two-person&#39;s Interaction Generation   via Reactive Auto-regressive Diffusion Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c66bfe10dfd878860a466abb92c19030.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    53 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="It-Takes-Two-Real-time-Co-Speech-Two-personâ€™s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model"><a href="#It-Takes-Two-Real-time-Co-Speech-Two-personâ€™s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model" class="headerlink" title="It Takes Two: Real-time Co-Speech Two-personâ€™s Interaction Generation   via Reactive Auto-regressive Diffusion Model"></a>It Takes Two: Real-time Co-Speech Two-personâ€™s Interaction Generation   via Reactive Auto-regressive Diffusion Model</h2><p><strong>Authors:Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, Taku Komura</strong></p>
<p>Conversational scenarios are very common in real-world settings, yet existing co-speech motion synthesis approaches often fall short in these contexts, where one personâ€™s audio and gestures will influence the otherâ€™s responses. Additionally, most existing methods rely on offline sequence-to-sequence frameworks, which are unsuitable for online applications. In this work, we introduce an audio-driven, auto-regressive system designed to synthesize dynamic movements for two characters during a conversation. At the core of our approach is a diffusion-based full-body motion synthesis model, which is conditioned on the past states of both characters, speech audio, and a task-oriented motion trajectory input, allowing for flexible spatial control. To enhance the modelâ€™s ability to learn diverse interactions, we have enriched existing two-person conversational motion datasets with more dynamic and interactive motions. We evaluate our system through multiple experiments to show it outperforms across a variety of tasks, including single and two-person co-speech motion generation, as well as interactive motion generation. To the best of our knowledge, this is the first system capable of generating interactive full-body motions for two characters from speech in an online manner. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œå¯¹è¯åœºæ™¯éå¸¸å¸¸è§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ååŒè¯­éŸ³è¿åŠ¨åˆæˆæ–¹æ³•åœ¨è¿™äº›åœºæ™¯ä¸­å¾€å¾€è¡¨ç°ä¸è¶³ï¼Œä¸€ä¸ªäººçš„éŸ³é¢‘å’Œæ‰‹åŠ¿ä¼šå½±å“å¦ä¸€æ–¹çš„ååº”ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºç¦»çº¿åºåˆ—åˆ°åºåˆ—æ¡†æ¶ï¼Œè¿™ä¸é€‚ç”¨äºåœ¨çº¿åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éŸ³é¢‘é©±åŠ¨çš„ã€è‡ªå›å½’ç³»ç»Ÿï¼Œæ—¨åœ¨åˆæˆå¯¹è¯è¿‡ç¨‹ä¸­ä¸¤ä¸ªè§’è‰²çš„åŠ¨æ€åŠ¨ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯åŸºäºæ‰©æ•£çš„å…¨èº«è¿åŠ¨åˆæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å—ä¸¤ä¸ªè§’è‰²è¿‡å»çŠ¶æ€ã€è¯­éŸ³éŸ³é¢‘å’Œé¢å‘ä»»åŠ¡çš„è¿åŠ¨è½¨è¿¹è¾“å…¥çš„åˆ¶çº¦ï¼Œå¯å®ç°çµæ´»çš„ç©ºé—´æ§åˆ¶ã€‚ä¸ºäº†æé«˜æ¨¡å‹å­¦ä¹ å„ç§äº¤äº’çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸°å¯Œäº†ç°æœ‰çš„ä¸¤äººå¯¹è¯è¿åŠ¨æ•°æ®é›†ï¼ŒåŠ å…¥äº†æ›´å¤šåŠ¨æ€å’Œäº¤äº’åŠ¨ä½œã€‚æˆ‘ä»¬é€šè¿‡å¤šæ¬¡å®éªŒè¯„ä¼°äº†æˆ‘ä»¬çš„ç³»ç»Ÿï¼Œè¡¨æ˜å®ƒåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºå…¶ä»–ç³»ç»Ÿï¼ŒåŒ…æ‹¬å•äººåŠä¸¤äººååŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆä»¥åŠäº¤äº’è¿åŠ¨ç”Ÿæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿä»¥åœ¨çº¿æ–¹å¼ä»è¯­éŸ³ä¸­ç”Ÿæˆä¸¤ä¸ªè§’è‰²çš„äº¤äº’å¼å…¨èº«åŠ¨ä½œçš„ä½“ç³»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02419v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºéŸ³é¢‘çš„ã€è‡ªå›å½’çš„ç³»ç»Ÿï¼Œç”¨äºåˆæˆå¯¹è¯ä¸­ä¸¤ä¸ªè§’è‰²çš„åŠ¨æ€åŠ¨ä½œã€‚ç³»ç»Ÿé‡‡ç”¨åŸºäºæ‰©æ•£ç®—æ³•çš„å…¨èº«åŠ¨ä½œåˆæˆæ¨¡å‹ï¼Œæ ¹æ®ä¸¤ä¸ªè§’è‰²çš„è¿‡å»çŠ¶æ€ã€è¯­éŸ³éŸ³é¢‘å’Œä»»åŠ¡å¯¼å‘çš„åŠ¨ä½œè½¨è¿¹è¾“å…¥è¿›è¡Œæ¡ä»¶åŒ–ï¼Œå®ç°çµæ´»çš„ç©ºé—´æ§åˆ¶ã€‚é€šè¿‡ä¸°å¯Œä¸¤äººå¯¹è¯åŠ¨ä½œæ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤šæ ·åŒ–çš„äº¤äº’åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å•äººåŠåŒäººå¯¹è¯åŠ¨ä½œç”Ÿæˆã€äº¤äº’åŠ¨ä½œç”Ÿæˆç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨çº¿ç”Ÿæˆä¸¤ä¸ªè§’è‰²äº’åŠ¨å…¨èº«åŠ¨ä½œçš„è¯­éŸ³é©±åŠ¨ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç³»ç»Ÿæ˜¯åŸºäºéŸ³é¢‘çš„è‡ªå›å½’ç³»ç»Ÿï¼Œç”¨äºåˆæˆå¯¹è¯ä¸­ä¸¤ä¸ªè§’è‰²çš„åŠ¨æ€åŠ¨ä½œã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨æ‰©æ•£ç®—æ³•æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åŸºäºä¸¤ä¸ªè§’è‰²çš„è¿‡å»çŠ¶æ€ã€è¯­éŸ³éŸ³é¢‘å’Œä»»åŠ¡å¯¼å‘çš„åŠ¨ä½œè½¨è¿¹è¿›è¡Œæ¡ä»¶åŒ–ã€‚</li>
<li>ç³»ç»Ÿå¯å®ç°çµæ´»çš„ç©ºé—´æ§åˆ¶ï¼Œå³æ ¹æ®å¯¹è¯åœºæ™¯ä¸­çš„äº’åŠ¨è°ƒæ•´è§’è‰²çš„åŠ¨ä½œã€‚</li>
<li>é€šè¿‡ä¸°å¯Œæ•°æ®é›†ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å¤šæ ·åŒ–çš„äº¤äº’åŠ¨ä½œï¼Œæé«˜ç”ŸæˆåŠ¨ä½œçš„çœŸå®æ„Ÿå’Œè‡ªç„¶åº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å•äººåŠåŒäººå¯¹è¯åŠ¨ä½œç”Ÿæˆã€äº¤äº’åŠ¨ä½œç”Ÿæˆç­‰ã€‚</li>
<li>è¯¥ç³»ç»Ÿæ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨çº¿ç”Ÿæˆä¸¤ä¸ªè§’è‰²äº’åŠ¨å…¨èº«åŠ¨ä½œçš„è¯­éŸ³é©±åŠ¨ç³»ç»Ÿã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0f6255b72b8539d8aab13e42acd7a48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54ef72cfbbecac1eb389d95bcf9efdce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-023425356e8d0b3df9cd25fa3d3bf131.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cf361129e15b5b04e85bcc554daf426.jpg" align="middle">
</details>




<h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p>
<p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. </p>
<blockquote>
<p>éšç€åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè‚–åƒå›¾åƒåŠ¨ç”»å·²ç»å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œç”±äºå®ƒçš„è¿­ä»£é‡‡æ ·ç‰¹æ€§ï¼Œå®ƒåœ¨æ—¶é—´ä¸€è‡´çš„è§†é¢‘ç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†FLOATï¼Œä¸€ç§åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬å°†ç”Ÿæˆå»ºæ¨¡ä»åŸºäºåƒç´ çš„æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°å­¦ä¹ çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†æ—¶é—´ä¸€è‡´è¿åŠ¨çš„æœ‰æ•ˆè®¾è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œé…å¤‡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¸§çº§æ¡ä»¶æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…ç»ªå¢å¼ºï¼Œèƒ½å¤Ÿå®ç°è¡¨è¾¾æ€§è¿åŠ¨çš„è‡ªç„¶èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºæœ€æ–°çš„éŸ³é¢‘é©±åŠ¨è‚–åƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•FLOATã€‚è¯¥æ–¹æ³•å°†ç”Ÿæˆå»ºæ¨¡ä»åƒç´ çº§çš„æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°å­¦ä¹ çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†é«˜æ•ˆè®¾è®¡çš„æ—¶åºä¸€è‡´è¿åŠ¨ã€‚é€šè¿‡å¼•å…¥åŸºäºå˜å‹å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œä»¥åŠç®€å•æœ‰æ•ˆçš„å¸§æ¡ä»¶æœºåˆ¶ï¼Œè¯¥æ–¹æ³•æ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºï¼Œèƒ½å¤Ÿè‡ªç„¶èå…¥è¡¨è¾¾æ€§åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰éŸ³é¢‘é©±åŠ¨çš„è‚–åƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FLOATæ˜¯ä¸€ç§åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å°†ç”Ÿæˆå»ºæ¨¡ä»åƒç´ çº§æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œä»¥å®ç°é«˜æ•ˆè®¾è®¡çš„æ—¶åºä¸€è‡´è¿åŠ¨ã€‚</li>
<li>é€šè¿‡å¼•å…¥åŸºäºå˜å‹å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œå®ç°äº†å¯¹è¿åŠ¨çš„ç²¾ç¡®é¢„æµ‹å’Œæ§åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨äº†ç®€å•æœ‰æ•ˆçš„å¸§æ¡ä»¶æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶èå…¥è¡¨è¾¾æ€§åŠ¨ä½œã€‚</li>
<li>FLOATæ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºï¼Œèƒ½å¤Ÿæ ¹æ®éŸ³é¢‘ä¿¡æ¯è°ƒæ•´è§’è‰²çš„è¡¨æƒ…å’ŒåŠ¨ä½œã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c48114dadf2693ebec847e1f4161b7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-932be8d0b3a67e4d27e3b20089557ffb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c0ae55682bf86c0002a9cdf127c986e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c66bfe10dfd878860a466abb92c19030.jpg" align="middle">
</details>




<h2 id="SVP-Style-Enhanced-Vivid-Portrait-Talking-Head-Diffusion-Model"><a href="#SVP-Style-Enhanced-Vivid-Portrait-Talking-Head-Diffusion-Model" class="headerlink" title="SVP: Style-Enhanced Vivid Portrait Talking Head Diffusion Model"></a>SVP: Style-Enhanced Vivid Portrait Talking Head Diffusion Model</h2><p><strong>Authors:Weipeng Tan, Chuming Lin, Chengming Xu, Xiaozhong Ji, Junwei Zhu, Chengjie Wang, Yunsheng Wu, Yanwei Fu</strong></p>
<p>Talking Head Generation (THG), typically driven by audio, is an important and challenging task with broad application prospects in various fields such as digital humans, film production, and virtual reality. While diffusion model-based THG methods present high quality and stable content generation, they often overlook the intrinsic style which encompasses personalized features such as speaking habits and facial expressions of a video. As consequence, the generated video content lacks diversity and vividness, thus being limited in real life scenarios. To address these issues, we propose a novel framework named Style-Enhanced Vivid Portrait (SVP) which fully leverages style-related information in THG. Specifically, we first introduce the novel probabilistic style prior learning to model the intrinsic style as a Gaussian distribution using facial expressions and audio embedding. The distribution is learned through the â€˜bespokedâ€™ contrastive objective, effectively capturing the dynamic style information in each video. Then we finetune a pretrained Stable Diffusion (SD) model to inject the learned intrinsic style as a controlling signal via cross attention. Experiments show that our model generates diverse, vivid, and high-quality videos with flexible control over intrinsic styles, outperforming existing state-of-the-art methods. </p>
<blockquote>
<p>åŸºäºéŸ³é¢‘çš„Talking Head Generationï¼ˆTHGï¼‰æ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œåœ¨æ•°å­—äººç±»ã€ç”µå½±åˆ¶ä½œå’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚è™½ç„¶åŸºäºæ‰©æ•£æ¨¡å‹çš„THGæ–¹æ³•å‘ˆç°å‡ºé«˜è´¨é‡ä¸”å†…å®¹ç¨³å®šç”Ÿæˆï¼Œä½†å®ƒä»¬å¸¸å¸¸å¿½ç•¥äº†å†…åœ¨é£æ ¼ï¼ŒåŒ…æ‹¬è¯´è¯ä¹ æƒ¯å’Œè§†é¢‘çš„é¢éƒ¨è¡¨æƒ…ç­‰ä¸ªæ€§åŒ–ç‰¹å¾ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„è§†é¢‘å†…å®¹ç¼ºä¹å¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ï¼Œåœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºStyle-Enhanced Vivid Portraitï¼ˆSVPï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œå……åˆ†åˆ©ç”¨THGä¸­çš„é£æ ¼ç›¸å…³ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥æ–°é¢–çš„æ¦‚ç‡é£æ ¼å…ˆéªŒå­¦ä¹ ï¼Œä½¿ç”¨é¢éƒ¨è¡¨æƒ…å’ŒéŸ³é¢‘åµŒå…¥æ¥æ¨¡æ‹Ÿå†…åœ¨é£æ ¼çš„é«˜æ–¯åˆ†å¸ƒã€‚è¯¥åˆ†å¸ƒæ˜¯é€šè¿‡â€œå®šåˆ¶â€å¯¹æ¯”ç›®æ ‡å­¦ä¹ å¾—åˆ°çš„ï¼Œæœ‰æ•ˆæ•æ‰æ¯ä¸ªè§†é¢‘ä¸­çš„åŠ¨æ€é£æ ¼ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹é¢„è®­ç»ƒçš„Stable Diffusionï¼ˆSDï¼‰æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›å°†å­¦ä¹ åˆ°çš„å†…åœ¨é£æ ¼ä½œä¸ºæ§åˆ¶ä¿¡å·æ³¨å…¥ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ã€ç”ŸåŠ¨ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œå¯¹å†…åœ¨é£æ ¼å…·æœ‰çµæ´»çš„æ§åˆ¶åŠ›ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03270v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨éŸ³é¢‘é©±åŠ¨çš„â€œè¯´è¯äººç”Ÿæˆâ€ï¼ˆTalking Head Generationï¼ŒTHGï¼‰ä»»åŠ¡ä¸­ï¼Œæ–°çš„æ¡†æ¶Style-Enhanced Vivid Portraitï¼ˆSVPï¼‰å……åˆ†åˆ©ç”¨äº†ä¸é£æ ¼ç›¸å…³çš„ä¿¡æ¯ï¼Œè§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹æ–¹æ³•å¿½ç•¥å†…åœ¨é£æ ¼ï¼ˆå¦‚è¯´è¯ä¹ æƒ¯å’Œé¢éƒ¨è¡¨æƒ…ï¼‰çš„é—®é¢˜ã€‚SVPé€šè¿‡å¼•å…¥æ¦‚ç‡æ€§é£æ ¼å…ˆéªŒå­¦ä¹ ï¼Œå°†å†…åœ¨é£æ ¼å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶é€šè¿‡å¯¹æ¯”ç›®æ ‡æœ‰æ•ˆæ•æ‰è§†é¢‘ä¸­çš„åŠ¨æ€é£æ ¼ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒSVPè¿˜å¾®è°ƒäº†é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥å­¦åˆ°çš„å†…åœ¨é£æ ¼ä½œä¸ºæ§åˆ¶ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼ŒSVPèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–ã€ç”ŸåŠ¨ã€é«˜è´¨é‡çš„è§†é¢‘ï¼Œå¯¹å†…åœ¨é£æ ¼å…·æœ‰çµæ´»çš„æ§åˆ¶ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Talking Head Generation (THG)æ˜¯ä¸€ä¸ªåœ¨æ•°å­—äººç±»ã€ç”µå½±åˆ¶ä½œå’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯çš„é‡è¦ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•åœ¨THGä¸­å¾€å¾€å¿½ç•¥å†…åœ¨é£æ ¼ï¼Œå¯¼è‡´ç”Ÿæˆå†…å®¹ç¼ºä¹å¤šæ ·æ€§å’Œç”ŸåŠ¨æ€§ã€‚</li>
<li>Style-Enhanced Vivid Portrait (SVP)æ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒå……åˆ†åˆ©ç”¨ä¸é£æ ¼ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>SVPé€šè¿‡æ¦‚ç‡æ€§é£æ ¼å…ˆéªŒå­¦ä¹ å»ºæ¨¡å†…åœ¨é£æ ¼ä¸ºé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶å¼•å…¥â€œbespokedâ€å¯¹æ¯”ç›®æ ‡æ¥æ•æ‰è§†é¢‘ä¸­çš„åŠ¨æ€é£æ ¼ä¿¡æ¯ã€‚</li>
<li>SVPé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ï¼Œæ³¨å…¥å­¦åˆ°çš„å†…åœ¨é£æ ¼ä½œä¸ºæ§åˆ¶ä¿¡å·ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSVPèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–ã€ç”ŸåŠ¨çš„è§†é¢‘ï¼Œå¯¹å†…åœ¨é£æ ¼å…·æœ‰çµæ´»æ§åˆ¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5890365713074886ca56233ac736345a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fcfd026aea3431ed82565993d9b913b.jpg" align="middle">
</details>




<h2 id="Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming"><a href="#Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming" class="headerlink" title="Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"></a>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</h2><p><strong>Authors:Zhifei Xie, Changqiao Wu</strong></p>
<p>Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original modelâ€™s language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method â€œAny Model Can Talkâ€. We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research. </p>
<blockquote>
<p>æœ€è¿‘çš„è‡ªç„¶è¯­è¨€æ¨¡å‹è¿›å±•æ˜¾è‘—ã€‚GPT-4oä½œä¸ºä¸€ä¸ªæ–°é‡Œç¨‹ç¢‘ï¼Œå·²ç»èƒ½å¤Ÿå®ç°ä¸äººç±»å®æ—¶å¯¹è¯ï¼Œå±•ç°å‡ºè¿‘ä¹äººç±»è‡ªç„¶çš„æµç•…åº¦ã€‚è¿™æ ·çš„äººæœºäº¤äº’éœ€è¦æ¨¡å‹èƒ½å¤Ÿç›´æ¥å¯¹éŸ³é¢‘æ¨¡å¼è¿›è¡Œæ¨ç†å¹¶ä»¥æµå¼æ–¹å¼ç”Ÿæˆè¾“å‡ºã€‚ç„¶è€Œï¼Œè¿™ä»ç„¶æ˜¯å½“å‰å­¦æœ¯æ¨¡å‹æ— æ³•ä¼åŠçš„èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¾èµ–äºé¢å¤–çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿæ¥è¿›è¡Œè¯­éŸ³åˆæˆï¼Œå¯¼è‡´ä¸å¯å–çš„å»¶è¿Ÿã€‚æœ¬æ–‡ä»‹ç»äº†Mini-Omniï¼Œä¸€ä¸ªåŸºäºéŸ³é¢‘çš„ç«¯åˆ°ç«¯å¯¹è¯æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿›è¡Œå®æ—¶è¯­éŸ³äº¤äº’ã€‚ä¸ºäº†å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–‡æœ¬æŒ‡å¯¼çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨æ‰¹é‡å¹¶è¡Œç­–ç•¥æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æœ‰åŠ©äºåœ¨æœ€å°é€€åŒ–çš„æƒ…å†µä¸‹ä¿ç•™åŸå§‹æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ï¼Œä½¿å…¶ä»–å·¥ä½œèƒ½å¤Ÿå»ºç«‹å®æ—¶äº¤äº’èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¿™ç§è®­ç»ƒæ–¹æ³•ç§°ä¸ºâ€œä»»ä½•æ¨¡å‹éƒ½èƒ½è¯´è¯â€ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†VoiceAssistant-400Kæ•°æ®é›†ï¼Œå¯¹ä¼˜åŒ–è¯­éŸ³è¾“å‡ºçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMini-Omniæ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨ç«¯åˆ°ç«¯ã€å¼€æºçš„å®æ—¶è¯­éŸ³äº¤äº’æ¨¡å‹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16725v3">PDF</a> Technical report, work in progress. Demo and code:   <a target="_blank" rel="noopener" href="https://github.com/gpt-omni/mini-omni">https://github.com/gpt-omni/mini-omni</a></p>
<p><strong>Summary</strong></p>
<p>GPT-4oå±•ç°äº†ä¸äººç±»è¿‘ä¹è‡ªç„¶æµç•…çš„å®æ—¶å¯¹è¯èƒ½åŠ›ã€‚å½“å‰å­¦æœ¯æ¨¡å‹ä»æ— æ³•ç›´æ¥å¤„ç†éŸ³é¢‘æ¨¡æ€è¿›è¡Œæ¨ç†å’Œç”Ÿæˆè¾“å‡ºï¼Œä¾èµ–äºTTSç³»ç»Ÿè¿›è¡Œè¯­éŸ³åˆæˆå¸¦æ¥ä¸ä¾¿ã€‚æœ¬ç ”ç©¶æå‡ºäº†Mini-Omniï¼Œé¦–ä¸ªåŸºäºéŸ³é¢‘ç«¯åˆ°ç«¯çš„ä¼šè¯æ¨¡å‹å®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚é‡‡ç”¨æ–‡æœ¬æŒ‡ä»¤è¯­éŸ³ç”Ÿæˆæ–¹æ³•å’Œæ‰¹å¹¶è¡Œæ¨æ–­ç­–ç•¥æå‡æ€§èƒ½ï¼Œä¸”èƒ½ä¿ç•™åŸå§‹æ¨¡å‹è¯­è¨€èƒ½åŠ›ï¼Œæœ‰åŠ©äºå…¶ä»–å·¥ä½œå»ºç«‹å®æ—¶äº¤äº’åŠŸèƒ½ã€‚åŒæ—¶ä»‹ç»VoiceAssistant-400Kæ•°æ®é›†ç”¨äºä¼˜åŒ–è¯­éŸ³è¾“å‡ºæ¨¡å‹ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æœ‰ä»·å€¼æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oå±•ç°äº†ä¸äººç±»è‡ªç„¶çš„å®æ—¶å¯¹è¯èƒ½åŠ›ã€‚</li>
<li>å½“å‰å­¦æœ¯æ¨¡å‹åœ¨å¤„ç†éŸ³é¢‘æ¨¡æ€è¿›è¡Œæ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¾èµ–TTSç³»ç»Ÿé€ æˆä¸ä¾¿ã€‚</li>
<li>Mini-Omniæ˜¯ä¸€ä¸ªåŸºäºéŸ³é¢‘ç«¯åˆ°ç«¯çš„ä¼šè¯æ¨¡å‹ï¼Œå®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚</li>
<li>Mini-Omnié‡‡ç”¨æ–‡æœ¬æŒ‡ä»¤è¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>æ‰¹å¹¶è¡Œæ¨æ–­ç­–ç•¥ç”¨äºæå‡Mini-Omniçš„æ€§èƒ½ã€‚</li>
<li>Mini-Omnièƒ½ä¿ç•™åŸå§‹æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›ï¼Œæœ‰åŠ©äºå…¶ä»–å·¥ä½œå»ºç«‹å®æ—¶äº¤äº’åŠŸèƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bc5a1cc9e49bdeb2bb93e564870560f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65205ae6b15cfac1ebb1b53671bdf6bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a663ab63b6a0b7ea62a7c36d45cbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f6edc5df9a1a7deaa89927cf545f98c.jpg" align="middle">
</details>




<h2 id="Talking-the-Talk-Does-Not-Entail-Walking-the-Walk-On-the-Limits-of-Large-Language-Models-in-Lexical-Entailment-Recognition"><a href="#Talking-the-Talk-Does-Not-Entail-Walking-the-Walk-On-the-Limits-of-Large-Language-Models-in-Lexical-Entailment-Recognition" class="headerlink" title="Talking the Talk Does Not Entail Walking the Walk: On the Limits of   Large Language Models in Lexical Entailment Recognition"></a>Talking the Talk Does Not Entail Walking the Walk: On the Limits of   Large Language Models in Lexical Entailment Recognition</h2><p><strong>Authors:Candida M. Greco, Lucio La Cava, Andrea Tagarelli</strong></p>
<p>Verbs form the backbone of language, providing the structure and meaning to sentences. Yet, their intricate semantic nuances pose a longstanding challenge. Understanding verb relations through the concept of lexical entailment is crucial for comprehending sentence meanings and grasping verb dynamics. This work investigates the capabilities of eight Large Language Models in recognizing lexical entailment relations among verbs through differently devised prompting strategies and zero-&#x2F;few-shot settings over verb pairs from two lexical databases, namely WordNet and HyperLex. Our findings unveil that the models can tackle the lexical entailment recognition task with moderately good performance, although at varying degree of effectiveness and under different conditions. Also, utilizing few-shot prompting can enhance the modelsâ€™ performance. However, perfectly solving the task arises as an unmet challenge for all examined LLMs, which raises an emergence for further research developments on this topic. </p>
<blockquote>
<p>åŠ¨è¯æ˜¯è¯­è¨€çš„ä¸»å¹²ï¼Œä¸ºå¥å­æä¾›ç»“æ„å’Œæ„ä¹‰ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¤æ‚çš„è¯­ä¹‰ç»†å¾®å·®åˆ«æ„æˆäº†ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚é€šè¿‡è¯æ±‡è•´æ¶µçš„æ¦‚å¿µç†è§£åŠ¨è¯å…³ç³»å¯¹äºç†è§£å¥å­æ„ä¹‰å’ŒæŠŠæ¡åŠ¨è¯åŠ¨æ€è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸¤ç§ä¸åŒçš„æç¤ºç­–ç•¥å’Œé›¶&#x2F;å°‘é•œå¤´è®¾ç½®ï¼Œè°ƒæŸ¥äº†å…«ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«æ¥è‡ªWordNetå’ŒHyperLexä¸¤ä¸ªè¯æ±‡æ•°æ®åº“çš„åŠ¨è¯ä¹‹é—´çš„è¯æ±‡è•´æ¶µå…³ç³»çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°å¤„ç†è¯æ±‡è•´æ¶µè¯†åˆ«ä»»åŠ¡ï¼Œå°½ç®¡åœ¨æœ‰æ•ˆæ€§å’Œæ¡ä»¶ä¸Šå­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨å°‘é•œå¤´æç¤ºå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºæ‰€æœ‰è¢«æ£€æµ‹çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œå®Œç¾å®Œæˆä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„æŒ‘æˆ˜ï¼Œè¿™ä¸ºè¯¥è¯é¢˜çš„è¿›ä¸€æ­¥ç ”ç©¶å‘å±•æå‡ºäº†è¦æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14894v2">PDF</a> Accepted for publication at The 2024 Conference on Empirical Methods   in Natural Language Processing (EMNLP-2024) - Findings</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†å…«å¤§è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«åŠ¨è¯ä¹‹é—´çš„è¯æ±‡è•´æ¶µå…³ç³»æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ä¸åŒçš„æç¤ºç­–ç•¥å’Œé›¶&#x2F;å°‘æ ·æœ¬è®¾ç½®ï¼Œç ”ç©¶æ¶‰åŠäº†ä»WordNetå’ŒHyperLexä¸¤ä¸ªè¯æ±‡æ•°æ®åº“ä¸­é€‰å–çš„åŠ¨è¯å¯¹ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°å®Œæˆè¯æ±‡è•´æ¶µè¯†åˆ«ä»»åŠ¡ï¼Œä½†ä¸åŒæ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§å­˜åœ¨å·®å¼‚ã€‚åˆ©ç”¨å°‘æ ·æœ¬æç¤ºå¯ä»¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å®Œç¾è§£å†³æ­¤ä»»åŠ¡å¯¹äºæ‰€æœ‰æ£€æŸ¥çš„è¯­è¨€æ¨¡å‹æ¥è¯´ä»æ˜¯æœªå®ç°çš„æŒ‘æˆ˜ï¼Œå› æ­¤æœ‰å¿…è¦å¯¹æ­¤è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ä¸å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€ä¸­çš„åŠ¨è¯ä¸ºå¥å­æä¾›äº†ç»“æ„å’Œæ„ä¹‰ã€‚</li>
<li>è¯æ±‡è•´æ¶µåœ¨ç†è§£å¥å­å«ä¹‰å’ŒæŠŠæ¡åŠ¨è¯åŠ¨æ€æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>å…«ç§å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«åŠ¨è¯é—´çš„è¯æ±‡è•´æ¶µå…³ç³»æ–¹é¢è¡¨ç°å‡ºä¸€å®šçš„èƒ½åŠ›ã€‚</li>
<li>ä¸åŒæç¤ºç­–ç•¥å’Œé›¶&#x2F;å°‘æ ·æœ¬è®¾ç½®ä¼šå½±å“è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨è§£å†³è¯æ±‡è•´æ¶µè¯†åˆ«ä»»åŠ¡æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æé«˜ã€‚</li>
<li>åˆ©ç”¨å°‘æ ·æœ¬æç¤ºå¯ä»¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d4f8136ed447f9fb3e6332f10074669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-552b9766323f7595859b191afc0eae61.jpg" align="middle">
</details>




<h2 id="AudioMarkBench-Benchmarking-Robustness-of-Audio-Watermarking"><a href="#AudioMarkBench-Benchmarking-Robustness-of-Audio-Watermarking" class="headerlink" title="AudioMarkBench: Benchmarking Robustness of Audio Watermarking"></a>AudioMarkBench: Benchmarking Robustness of Audio Watermarking</h2><p><strong>Authors:Hongbin Liu, Moyang Guo, Zhengyuan Jiang, Lun Wang, Neil Zhenqiang Gong</strong></p>
<p>The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common&#x2F;adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/moyangkuo/AudioMarkBench">https://github.com/moyangkuo/AudioMarkBench</a>. </p>
<blockquote>
<p>éšç€æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„è¿›æ­¥ï¼Œåˆæˆè¯­éŸ³çš„é€¼çœŸåº¦ä¸æ–­æé«˜ï¼Œå¼•å‘äº†å…³äºå†’å……å’Œè™šå‡ä¿¡æ¯ä¼ æ’­çš„ä¼¦ç†æ‹…å¿§ã€‚éŸ³é¢‘æ°´å°æŠ€æœ¯é€šè¿‡åµŒå…¥äººç±»éš¾ä»¥å¯Ÿè§‰çš„æ°´å°åˆ°äººå·¥æ™ºèƒ½ç”Ÿæˆçš„éŸ³é¢‘ä¸­ï¼Œæä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ°´å°å¯¹äºå¸¸è§„&#x2F;å¯¹æŠ—æ€§å¹²æ‰°çš„ç¨³å¥æ€§ä»ç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬æ¨å‡ºäº†AudioMarkBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿçš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘æ°´å°å¯¹æ°´å°ç§»é™¤å’Œæ°´å°ä¼ªé€ å¹²æ‰°çš„ç¨³å¥æ€§ã€‚AudioMarkBenchåŒ…æ‹¬ä½¿ç”¨Common-Voiceåˆ›å»ºçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§è¯­è¨€ã€ç”Ÿç‰©æ€§åˆ«å’Œå¹´é¾„ï¼Œè¿˜åŒ…æ‹¬3ç§æœ€æ–°æ°´å°æ–¹æ³•ä»¥åŠ1peirone5ç§æ‰°åŠ¨ç±»å‹ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•åœ¨æ— æ¡†ã€é»‘æ¡†å’Œç™½æ¡†è®¾ç½®ä¸­å¯¹å¹²æ‰°çš„ç¨³å¥æ€§è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å½“å‰æ°´å°æŠ€æœ¯çš„æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥å’Œå…¬å¹³çš„éŸ³é¢‘æ°´å°è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/moyangkuo/AudioMarkBench%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/moyangkuo/AudioMarkBenchå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06979v2">PDF</a> To appear in NeurIPS Datasets and Benchmarks, 2024</p>
<p><strong>Summary</strong><br>     éšç€æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹çš„è¿›æ­¥ï¼Œåˆæˆè¯­éŸ³çš„é€¼çœŸåº¦ä¸æ–­æå‡ï¼Œå¼•å‘äº†å…³äºèº«ä»½å†’å……å’Œè™šå‡ä¿¡æ¯çš„ä¼¦ç†é—®é¢˜ã€‚éŸ³é¢‘æ°´å°æŠ€æœ¯ä¸ºåœ¨AIç”Ÿæˆçš„éŸ³é¢‘ä¸­åµŒå…¥äººç±»æ— æ³•å¯Ÿè§‰çš„æ°´å°æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ°´å°å¯¹å¸¸è§&#x2F;å¯¹æŠ—æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†AudioMarkBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿè¯„ä¼°éŸ³é¢‘æ°´å°ç¨³å¥æ€§çš„åŸºå‡†æµ‹è¯•ï¼Œé’ˆå¯¹æ°´å°ç§»é™¤å’Œä¼ªé€ ã€‚AudioMarkBenchåŒ…æ‹¬ä½¿ç”¨Common-Voiceåˆ›å»ºçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§è¯­è¨€ã€æ€§åˆ«å’Œå¹´é¾„ï¼Œ3ç§å…ˆè¿›çš„æ°´å°æ–¹æ³•ï¼Œå’Œ15ç§æ‰°åŠ¨ç±»å‹ã€‚æˆ‘ä»¬è¯„ä¼°è¿™äº›æ–¹æ³•åœ¨æ— ç›’ã€é»‘ç›’å’Œç™½ç›’è®¾ç½®ä¸­å¯¹æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚ç ”ç©¶å‘ç°å½“å‰æ°´å°æŠ€æœ¯çš„æ¼æ´ï¼Œå¼ºè°ƒéœ€è¦æ›´ç¨³å¥å’Œå…¬å¹³çš„éŸ³é¢‘æ°´å°è§£å†³æ–¹æ¡ˆã€‚æ•°æ®é›†å’Œä»£ç å…¬å¼€å¯è®¿é—®äº<a target="_blank" rel="noopener" href="https://github.com/moyangkuo/AudioMarkBench%E3%80%82">https://github.com/moyangkuo/AudioMarkBenchã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè¯­éŸ³é€¼çœŸåº¦çš„æå‡å¼•å‘äº†å…³äºèº«ä»½å†’å……å’Œè™šå‡ä¿¡æ¯çš„ä¼¦ç†æ‹…å¿§ã€‚</li>
<li>éŸ³é¢‘æ°´å°æŠ€æœ¯ä¸ºAIç”Ÿæˆçš„éŸ³é¢‘ä¸­åµŒå…¥ä¸å¯å¯Ÿè§‰çš„æ°´å°æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>éŸ³é¢‘æ°´å°å¯¹äºå„ç§æ‰°åŠ¨ï¼ˆåŒ…æ‹¬å¸¸è§å’Œå¯¹æŠ—æ€§çš„ï¼‰çš„ç¨³å¥æ€§å°šå¾…ç ”ç©¶ã€‚</li>
<li>AudioMarkBenchæ˜¯é¦–ä¸ªè¯„ä¼°éŸ³é¢‘æ°´å°ç¨³å¥æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ–°æ•°æ®é›†å’Œä¸‰ç§å…ˆè¿›çš„æ°´å°æ–¹æ³•ã€‚</li>
<li>æµ‹è¯•äº†æ°´å°æ–¹æ³•åœ¨ä¸åŒè®¾ç½®ï¼ˆæ— ç›’ã€é»‘ç›’ã€ç™½ç›’ï¼‰ä¸­å¯¹æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚</li>
<li>å½“å‰éŸ³é¢‘æ°´å°æŠ€æœ¯å­˜åœ¨æ¼æ´ï¼Œéœ€è¦æ›´ç¨³å¥å’Œå…¬å¹³çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08915683a4c758690683b2f6e7a55794.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-162e3e837d0021b08f938cabddea0622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c52264182cf4eb9c617de2585a2f4cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd1ab33af7d9e984de40e97c71a6eb18.jpg" align="middle">
</details>




<h2 id="Controllable-Talking-Face-Generation-by-Implicit-Facial-Keypoints-Editing"><a href="#Controllable-Talking-Face-Generation-by-Implicit-Facial-Keypoints-Editing" class="headerlink" title="Controllable Talking Face Generation by Implicit Facial Keypoints   Editing"></a>Controllable Talking Face Generation by Implicit Facial Keypoints   Editing</h2><p><strong>Authors:Dong Zhao, Jiaying Shi, Wenjun Li, Shudong Wang, Shenghui Xu, Zhaoming Pan</strong></p>
<p>Audio-driven talking face generation has garnered significant interest within the domain of digital human research. Existing methods are encumbered by intricate model architectures that are intricately dependent on each other, complicating the process of re-editing image or video inputs. In this work, we present ControlTalk, a talking face generation method to control face expression deformation based on driven audio, which can construct the head pose and facial expression including lip motion for both single image or sequential video inputs in a unified manner. By utilizing a pre-trained video synthesis renderer and proposing the lightweight adaptation, ControlTalk achieves precise and naturalistic lip synchronization while enabling quantitative control over mouth opening shape. Our experiments show that our method is superior to state-of-the-art performance on widely used benchmarks, including HDTF and MEAD. The parameterized adaptation demonstrates remarkable generalization capabilities, effectively handling expression deformation across same-ID and cross-ID scenarios, and extending its utility to out-of-domain portraits, regardless of languages. Code is available at <a target="_blank" rel="noopener" href="https://github.com/NetEase-Media/ControlTalk">https://github.com/NetEase-Media/ControlTalk</a>. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯åœ¨æ•°å­—äººç±»ç ”ç©¶é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…´è¶£ã€‚ç°æœ‰æ–¹æ³•å—åˆ°å¤æ‚æ¨¡å‹æ¶æ„çš„æŸç¼šï¼Œè¿™äº›æ¶æ„å½¼æ­¤ä¹‹é—´æœ‰ç€é”™ç»¼å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œä»è€ŒåŠ å‰§äº†é‡æ–°ç¼–è¾‘å›¾åƒæˆ–è§†é¢‘è¾“å…¥çš„è¿‡ç¨‹çš„éš¾åº¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ControlTalkï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé©±åŠ¨éŸ³é¢‘çš„æ§åˆ¶é¢éƒ¨è¡¨æƒ…å˜å½¢çš„æ–¹æ³•ï¼Œå¯ä»¥ç»Ÿä¸€åœ°ä¸ºå•å¼ å›¾åƒæˆ–è¿ç»­è§†é¢‘è¾“å…¥æ„å»ºå¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬å˜´å”‡è¿åŠ¨ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘åˆæˆæ¸²æŸ“å™¨å¹¶æå‡ºè½»é‡çº§é€‚é…ï¼ŒControlTalkå®ç°äº†ç²¾ç¡®ä¸”è‡ªç„¶çš„å”‡éƒ¨åŒæ­¥ï¼ŒåŒæ—¶å®ç°å¯¹å˜´å·´å¼€å£å½¢çŠ¶çš„å®šé‡æ§åˆ¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯ï¼ŒåŒ…æ‹¬HDTFå’ŒMEADã€‚å‚æ•°åŒ–é€‚é…è¡¨ç°å‡ºæ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ï¼Œæœ‰æ•ˆå¤„ç†åŒä¸€èº«ä»½å’Œè·¨èº«ä»½åœºæ™¯ä¸‹çš„è¡¨æƒ…å˜å½¢ï¼Œå¹¶å°†å…¶æ•ˆç”¨æ‰©å±•åˆ°è·¨åŸŸè‚–åƒï¼Œä¸å—è¯­è¨€é™åˆ¶ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/NetEase-Media/ControlTalk%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/NetEase-Media/ControlTalkè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02880v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ§åˆ¶å¯¹è¯ï¼ˆControlTalkï¼‰æ˜¯ä¸€ç§åŸºäºéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…æ§åˆ¶æ–¹æ³•ï¼Œèƒ½å¤Ÿç»Ÿä¸€å¤„ç†å•å¼ å›¾ç‰‡æˆ–è¿ç»­è§†é¢‘è¾“å…¥çš„å¤´éƒ¨ä½å§¿å’Œé¢éƒ¨è¡¨æƒ…ï¼ŒåŒ…æ‹¬å”‡åŠ¨ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘åˆæˆæ¸²æŸ“å™¨ï¼Œå¹¶æå‡ºè½»é‡çº§é€‚é…æ–¹æ¡ˆï¼Œå®ç°äº†ç²¾å‡†è‡ªç„¶çš„å”‡åŒæ­¥ï¼ŒåŒæ—¶å®ç°å¯¹å¼€å£å½¢çŠ¶çš„é‡åŒ–æ§åˆ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒControlTalkåœ¨HDTFå’ŒMEADç­‰å¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰æŠ€æœ¯æ°´å¹³ï¼Œå‚æ•°åŒ–é€‚é…å±•ç°å‡ºå“è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†åŒä¸€èº«ä»½å’Œä¸åŒèº«ä»½çš„é¢éƒ¨è¡¨æƒ…å˜å½¢ï¼Œå¹¶æ‰©å±•åº”ç”¨äºè·¨é¢†åŸŸè‚–åƒï¼Œä¸å—è¯­è¨€é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ControlTalkæ˜¯ä¸€ç§åŸºäºéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨è¡¨æƒ…æ§åˆ¶æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå¯¹è¯æ—¶çš„é¢éƒ¨åŠ¨ç”»ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥å¤„ç†å•å¼ å›¾ç‰‡æˆ–è¿ç»­è§†é¢‘è¾“å…¥ï¼Œç»Ÿä¸€è¿›è¡Œå¤´éƒ¨å§¿æ€å’Œé¢éƒ¨è¡¨æƒ…çš„ç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘åˆæˆæ¸²æŸ“å™¨ï¼Œå®ç°ç²¾å‡†è‡ªç„¶çš„å”‡åŒæ­¥ã€‚</li>
<li>é€šè¿‡è½»é‡çº§é€‚é…æ–¹æ¡ˆï¼ŒControlTalkå®ç°äº†å¯¹å¼€å£å½¢çŠ¶çš„é‡åŒ–æ§åˆ¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒControlTalkåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ–¹æ³•çš„å‚æ•°åŒ–é€‚é…å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒåœºæ™¯çš„é¢éƒ¨è¡¨æƒ…å˜å½¢ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d82a99b2cbe66e653dd1a93ecd89fa92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc66dc7e872f3f60ca15a718b6745f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66a0bf2daa3ecc479fcf1883ce5dc48f.jpg" align="middle">
</details>




<h2 id="SPEAK-Speech-Driven-Pose-and-Emotion-Adjustable-Talking-Head-Generation"><a href="#SPEAK-Speech-Driven-Pose-and-Emotion-Adjustable-Talking-Head-Generation" class="headerlink" title="SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation"></a>SPEAK: Speech-Driven Pose and Emotion-Adjustable Talking Head Generation</h2><p><strong>Authors:Changpeng Cai, Guinan Guo, Jiao Li, Junhao Su, Fei Shen, Chenghao He, Jing Xiao, Yuanxu Chen, Lei Dai, Feiyu Zhu</strong></p>
<p>Most earlier researches on talking face generation have focused on the synchronization of lip motion and speech content. However, head pose and facial emotions are equally important characteristics of natural faces. While audio-driven talking face generation has seen notable advancements, existing methods either overlook facial emotions or are limited to specific individuals and cannot be applied to arbitrary subjects. In this paper, we propose a novel one-shot Talking Head Generation framework (SPEAK) that distinguishes itself from the general Talking Face Generation by enabling emotional and postural control. Specifically, we introduce Inter-Reconstructed Feature Disentanglement (IRFD) module to decouple facial features into three latent spaces. Then we design a face editing module that modifies speech content and facial latent codes into a single latent space. Subsequently, we present a novel generator that employs modified latent codes derived from the editing module to regulate emotional expression, head poses, and speech content in synthesizing facial animations. Extensive trials demonstrate that our method ensures lip synchronization with the audio while enabling decoupled control of facial features, it can generate realistic talking head with coordinated lip motions, authentic facial emotions, and smooth head movements. The demo video is available: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/SPEAK-8A22">https://anonymous.4open.science/r/SPEAK-8A22</a> </p>
<blockquote>
<p>æ—©æœŸå…³äºè¯´è¯äººè„¸ç”Ÿæˆçš„ç ”ç©¶å¤§å¤šèšç„¦äºå”‡éƒ¨è¿åŠ¨ä¸è¯­éŸ³å†…å®¹çš„åŒæ­¥ã€‚ç„¶è€Œï¼Œå¤´éƒ¨å§¿æ€å’Œé¢éƒ¨æƒ…ç»ªåŒæ ·æ˜¯è‡ªç„¶äººè„¸çš„é‡è¦ç‰¹å¾ã€‚å°½ç®¡éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç°æœ‰æ–¹æ³•è¦ä¹ˆå¿½è§†é¢éƒ¨æƒ…ç»ªï¼Œè¦ä¹ˆä»…é™äºç‰¹å®šä¸ªä½“ï¼Œæ— æ³•åº”ç”¨äºä»»æ„ä¸»ä½“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸€æ¬¡æ€§è¯´è¯äººå¤´ç”Ÿæˆæ¡†æ¶ï¼ˆSPEAKï¼‰ï¼Œå®ƒä¸ä¸€èˆ¬çš„è¯´è¯è„¸ç”Ÿæˆç›¸åŒºåˆ«ï¼Œèƒ½å¤Ÿå®ç°æƒ…ç»ªä¸å§¿æ€æ§åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†é‡å»ºç‰¹å¾è§£è€¦ï¼ˆIRFDï¼‰æ¨¡å—ï¼Œå°†é¢éƒ¨ç‰¹å¾è§£è€¦ä¸ºä¸‰ä¸ªæ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¢éƒ¨ç¼–è¾‘æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿä¿®æ”¹è¯­éŸ³å†…å®¹å’Œé¢éƒ¨æ½œåœ¨ä»£ç ï¼Œå°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€æ½œåœ¨ç©ºé—´ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨é‡‡ç”¨ç¼–è¾‘æ¨¡å—ç”Ÿæˆçš„ä¿®æ”¹åçš„æ½œåœ¨ä»£ç ï¼Œåœ¨åˆæˆé¢éƒ¨åŠ¨ç”»æ—¶è°ƒæ§æƒ…ç»ªè¡¨è¾¾ã€å¤´éƒ¨å§¿æ€å’Œè¯­éŸ³å†…å®¹ã€‚å¤§é‡è¯•éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½ä¿è¯éŸ³é¢‘çš„å”‡éƒ¨åŒæ­¥ï¼ŒåŒæ—¶å®ç°é¢éƒ¨ç‰¹å¾çš„è§£è€¦æ§åˆ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰åè°ƒå”‡éƒ¨è¿åŠ¨ã€çœŸå®é¢éƒ¨æƒ…ç»ªå’Œæµç•…å¤´éƒ¨åŠ¨ä½œçš„ç°å®æ„Ÿè¯´è¯äººå¤´ã€‚æ¼”ç¤ºè§†é¢‘é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/SPEAK-8A22%E3%80%82">https://anonymous.4open.science/r/SPEAK-8A22ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07257v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSPEAKçš„æ–°å‹ä¸€æ¬¡è°ˆè¯å¤´ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥IRFDæ¨¡å—å°†é¢éƒ¨ç‰¹å¾è§£è€¦ä¸ºä¸‰ä¸ªæ½œåœ¨ç©ºé—´ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªé¢éƒ¨ç¼–è¾‘æ¨¡å—æ¥ä¿®æ”¹è¯­éŸ³å†…å®¹å’Œé¢éƒ¨æ½œåœ¨ä»£ç ã€‚é€šè¿‡é‡‡ç”¨ä¿®æ”¹åçš„æ½œåœ¨ä»£ç ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ§åˆ¶æƒ…æ„Ÿè¡¨è¾¾ã€å¤´éƒ¨å§¿åŠ¿å’Œè¯­éŸ³å†…å®¹ï¼Œä»è€Œåˆæˆå…·æœ‰åè°ƒçš„å”‡éƒ¨è¿åŠ¨ã€çœŸå®çš„é¢éƒ¨è¡¨æƒ…å’Œæµç•…çš„å¤´éƒ¨åŠ¨ä½œçš„è°ˆè¯å¤´éƒ¨åŠ¨ç”»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—©æœŸè°ˆè¯é¢éƒ¨ç”Ÿæˆç ”ç©¶ä¸»è¦å…³æ³¨å”‡éƒ¨è¿åŠ¨å’Œè¯­éŸ³å†…å®¹çš„åŒæ­¥ã€‚</li>
<li>å¤´éƒ¨å§¿åŠ¿å’Œé¢éƒ¨æƒ…æ„Ÿæ˜¯è‡ªç„¶é¢éƒ¨åŒæ ·é‡è¦çš„ç‰¹å¾ã€‚</li>
<li>å½“å‰éŸ³é¢‘é©±åŠ¨è°ˆè¯é¢éƒ¨ç”Ÿæˆæ–¹æ³•è¦ä¹ˆå¿½ç•¥é¢éƒ¨æƒ…æ„Ÿï¼Œè¦ä¹ˆä»…é™äºç‰¹å®šä¸ªä½“ï¼Œä¸èƒ½åº”ç”¨äºä»»æ„ä¸»ä½“ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸€æ¬¡è°ˆè¯å¤´ç”Ÿæˆæ¡†æ¶ï¼ˆSPEAKï¼‰ï¼Œèƒ½å¤Ÿæ§åˆ¶æƒ…æ„Ÿå’Œå§¿åŠ¿ã€‚<br>5.SPEAKæ¡†æ¶é€šè¿‡IRFDæ¨¡å—å°†é¢éƒ¨ç‰¹å¾è§£è€¦ä¸ºä¸‰ä¸ªæ½œåœ¨ç©ºé—´ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªé¢éƒ¨ç¼–è¾‘æ¨¡å—æ¥ä¿®æ”¹è¯­éŸ³å’Œé¢éƒ¨æ½œåœ¨ä»£ç ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨ä¿®æ”¹åçš„æ½œåœ¨ä»£ç ï¼ŒSPEAKæ¡†æ¶å¯ä»¥åˆæˆå…·æœ‰åè°ƒçš„å”‡éƒ¨è¿åŠ¨ã€çœŸå®çš„é¢éƒ¨è¡¨æƒ…å’Œæµç•…çš„å¤´éƒ¨åŠ¨ä½œçš„è°ˆè¯å¤´éƒ¨åŠ¨ç”»ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-048d71659731706d92dae75b3186e567.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e3a39896d69a619c85393b78371c5ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e539cf7d6c78de900031cbbd942699.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-428f21ef9b517ac02ee31915e4b59103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67760397cd16c0e3daa74b620ee54652.jpg" align="middle">
</details>




<h2 id="VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time"><a href="#VASA-1-Lifelike-Audio-Driven-Talking-Faces-Generated-in-Real-Time" class="headerlink" title="VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time"></a>VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time</h2><p><strong>Authors:Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, Baining Guo</strong></p>
<p>We introduce VASA, a framework for generating lifelike talking faces with appealing visual affective skills (VAS) given a single static image and a speech audio clip. Our premiere model, VASA-1, is capable of not only generating lip movements that are exquisitely synchronized with the audio, but also producing a large spectrum of facial nuances and natural head motions that contribute to the perception of authenticity and liveliness. The core innovations include a holistic facial dynamics and head movement generation model that works in a face latent space, and the development of such an expressive and disentangled face latent space using videos. Through extensive experiments including evaluation on a set of new metrics, we show that our method significantly outperforms previous methods along various dimensions comprehensively. Our method not only delivers high video quality with realistic facial and head dynamics but also supports the online generation of 512x512 videos at up to 40 FPS with negligible starting latency. It paves the way for real-time engagements with lifelike avatars that emulate human conversational behaviors. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†VASAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ç»™å®šçš„å•å¼ é™æ€å›¾åƒå’Œè¯­éŸ³éŸ³é¢‘ç‰‡æ®µä¸Šç”Ÿæˆå…·æœ‰å¸å¼•åŠ›çš„è§†è§‰æƒ…æ„ŸæŠ€èƒ½ï¼ˆVASï¼‰çš„é€¼çœŸè°ˆè¯é¢å­”ã€‚æˆ‘ä»¬çš„æ——èˆ°å‹å·VASA-1ä¸ä»…èƒ½å¤Ÿç”Ÿæˆä¸éŸ³é¢‘ç²¾ç»†åŒæ­¥çš„å˜´å”‡è¿åŠ¨ï¼Œè¿˜èƒ½äº§ç”Ÿä¸€ç³»åˆ—é¢éƒ¨ç»†å¾®å·®åˆ«å’Œè‡ªç„¶å¤´éƒ¨è¿åŠ¨ï¼Œè¿™äº›éƒ½æœ‰åŠ©äºæ„ŸçŸ¥çœŸå®æ€§å’Œç”ŸåŠ¨æ€§ã€‚æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬åœ¨é¢éƒ¨æ½œåœ¨ç©ºé—´ä¸­å·¥ä½œçš„æ•´ä½“é¢éƒ¨åŠ¨æ€å’Œå¤´éƒ¨è¿åŠ¨ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠä½¿ç”¨è§†é¢‘å¼€å‘å¦‚æ­¤è¡¨è¾¾æ€§å’Œè„±ç¦»çš„é¢éƒ¨æ½œåœ¨ç©ºé—´ã€‚é€šè¿‡å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬åœ¨æ–°æŒ‡æ ‡é›†ä¸Šçš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ä¸ªæ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æä¾›é«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰é€¼çœŸçš„é¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€ï¼Œè¿˜æ”¯æŒåœ¨çº¿ç”Ÿæˆ512x512åˆ†è¾¨ç‡çš„è§†é¢‘ï¼Œå¸§ç‡é«˜è¾¾40 FPSï¼Œå¯åŠ¨å»¶è¿Ÿå¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¿™ä¸ºä¸æ¨¡æ‹Ÿäººç±»å¯¹è¯è¡Œä¸ºçš„é€¼çœŸåŒ–èº«è¿›è¡Œå®æ—¶äº¤äº’é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10667v2">PDF</a> NeurIPS 2024 (Oral) Camera ready. Project webpage:   <a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/project/vasa-1/">https://www.microsoft.com/en-us/research/project/vasa-1/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>VASAæ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨å•å¼ é™æ€å›¾åƒå’Œè¯­éŸ³éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå…·æœ‰é€¼çœŸæƒ…æ„Ÿè¡¨è¾¾èƒ½åŠ›çš„è¯´è¯é¢å­”ã€‚å…¶æ ¸å¿ƒæ¨¡å‹VASA-1ä¸ä»…èƒ½å¤Ÿç²¾å‡†åŒæ­¥éŸ³é¢‘ç”Ÿæˆå”‡åŠ¨ï¼Œè¿˜èƒ½ç”Ÿæˆå¤šç§é¢éƒ¨è¡¨æƒ…å’Œè‡ªç„¶çš„å¤´éƒ¨åŠ¨ä½œï¼Œå¢å¼ºçœŸå®æ„Ÿå’Œç”ŸåŠ¨æ€§ã€‚ä¸»è¦åˆ›æ–°åŒ…æ‹¬é¢éƒ¨åŠ¨æ€å’Œå¤´éƒ¨åŠ¨ä½œçš„æ•´ä½“ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨é¢éƒ¨æ½œåœ¨ç©ºé—´å†…è¿ä½œï¼Œä»¥åŠé€šè¿‡è§†é¢‘å¼€å‘å¦‚æ­¤è¡¨è¾¾å’Œè„±ç¦»çš„é¢éƒ¨æ½œåœ¨ç©ºé—´ã€‚å®éªŒè¯æ˜ï¼Œæ²¿å¤šä¸ªç»´åº¦å…¨é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚æ­¤æ–¹æ³•ä¸ä»…æä¾›é«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰é€¼çœŸçš„é¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€ï¼Œè¿˜æ”¯æŒåœ¨çº¿ç”Ÿæˆ512x512åˆ†è¾¨ç‡çš„è§†é¢‘ï¼Œå¸§ç‡é«˜è¾¾æ¯ç§’40å¸§ï¼Œå‡ ä¹æ— åˆå§‹å»¶è¿Ÿã€‚è¿™ä¸ºå®æ—¶ä¸æ¨¡æ‹Ÿäººç±»å¯¹è¯è¡Œä¸ºçš„é€¼çœŸåŒ–èº«äº’åŠ¨å¼€è¾Ÿäº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VASAæ˜¯ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è¯´è¯å¤´æ¨¡å‹çš„æ¡†æ¶ï¼Œä½¿ç”¨é™æ€å›¾åƒå’Œè¯­éŸ³éŸ³é¢‘ç‰‡æ®µç”Ÿæˆå…·æœ‰å¸å¼•åŠ›çš„è§†è§‰æƒ…æ„ŸæŠ€èƒ½ï¼ˆVASï¼‰ã€‚</li>
<li>VASA-1æ¨¡å‹èƒ½ç²¾å‡†åŒæ­¥éŸ³é¢‘ç”Ÿæˆå”‡åŠ¨ï¼Œç”Ÿæˆå¤šç§é¢éƒ¨è¡¨æƒ…å’Œè‡ªç„¶çš„å¤´éƒ¨åŠ¨ä½œï¼Œå¢å¼ºäº†çœŸå®æ„Ÿå’Œç”ŸåŠ¨æ€§ã€‚</li>
<li>æ¡†æ¶çš„ä¸»è¦åˆ›æ–°åŒ…æ‹¬æ•´ä½“é¢éƒ¨åŠ¨æ€å’Œå¤´éƒ¨åŠ¨ä½œç”Ÿæˆæ¨¡å‹åœ¨é¢éƒ¨æ½œåœ¨ç©ºé—´å†…çš„å·¥ä½œæ–¹å¼ã€‚</li>
<li>é€šè¿‡è§†é¢‘å¼€å‘äº†ä¸€ç§è¡¨è¾¾å’Œè„±ç¦»çš„é¢éƒ¨æ½œåœ¨ç©ºé—´ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒVASAæ–¹æ³•åœ¨å„ç§ç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…æä¾›é«˜è´¨é‡çš„è§†é¢‘ï¼Œå…·æœ‰é€¼çœŸçš„é¢éƒ¨å’Œå¤´éƒ¨åŠ¨æ€ï¼Œè¿˜æ”¯æŒåœ¨çº¿ç”Ÿæˆé«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œå¸§ç‡é«˜è¾¾æ¯ç§’40å¸§ï¼Œå»¶è¿Ÿå°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d483a8041dc106cecb76cd05487f601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abbef904af493ff9f6c144397a2ac1b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f8f8673d1617fc8071697ec633f6467.jpg" align="middle">
</details>




<h2 id="If-CLIP-Could-Talk-Understanding-Vision-Language-Model-Representations-Through-Their-Preferred-Concept-Descriptions"><a href="#If-CLIP-Could-Talk-Understanding-Vision-Language-Model-Representations-Through-Their-Preferred-Concept-Descriptions" class="headerlink" title="If CLIP Could Talk: Understanding Vision-Language Model Representations   Through Their Preferred Concept Descriptions"></a>If CLIP Could Talk: Understanding Vision-Language Model Representations   Through Their Preferred Concept Descriptions</h2><p><strong>Authors:Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach</strong></p>
<p>Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize textual features that are important for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate features that are important for the VLM. Then, we inspect the descriptions to identify features that contribute to VLM representations. Using EX2, we find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat (e.g., North America) to represent visual concepts. Also, our analysis reveals that different VLMs prioritize different attributes in their representations. Overall, we show that VLMs do not simply match images to scene descriptions and that non-visual or even spurious descriptions significantly influence their representations. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å·¥ä½œé€šå¸¸å‡è®¾è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¡¨ç¤ºæ˜¯åŸºäºè§†è§‰å±æ€§ï¼Œå¦‚å½¢çŠ¶ã€‚ç„¶è€Œï¼Œå°šä¸æ¸…æ¥šVLMåœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¼˜å…ˆåˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥ä»£è¡¨æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è¡¨å¾å¯¹VLMé‡è¦çš„æ–‡æœ¬ç‰¹å¾ï¼Œç§°ä¸ºâ€œæå–ä¸æ¢ç´¢â€ï¼ˆEX2ï¼‰ã€‚EX2ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸VLMåå¥½å¯¹é½ï¼Œç”Ÿæˆèå…¥å¯¹VLMé‡è¦çš„ç‰¹å¾çš„æè¿°ã€‚ç„¶åï¼Œæˆ‘ä»¬æ£€æŸ¥è¿™äº›æè¿°ä»¥è¯†åˆ«å¯¹VLMè¡¨ç¤ºæœ‰è´¡çŒ®çš„ç‰¹å¾ã€‚ä½¿ç”¨EX2ï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡æ²¡æœ‰æä¾›ä»»ä½•æœ‰ç”¨ä¿¡æ¯ï¼Œä½†è¯¯å¯¼æ€§æè¿°åœ¨VLMè¡¨ç¤ºä¸­èµ·åˆ°äº†é‡è¦ä½œç”¨ï¼Œä¾‹å¦‚ï¼Œâ€œç‚¹å‡»æ”¾å¤§æ¦‚å¿µçš„ç…§ç‰‡â€ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æœ‰ä¿¡æ¯çš„æè¿°ä¸­ï¼ŒVLMåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºéè§†è§‰å±æ€§ï¼ˆä¾‹å¦‚æ –æ¯åœ°ï¼ˆå¦‚åŒ—ç¾ï¼‰ï¼‰æ¥è¡¨ç¤ºè§†è§‰æ¦‚å¿µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†æè¿˜å‘ç°ä¸åŒçš„VLMåœ¨å…¶è¡¨ç¤ºä¸­ä¼˜å…ˆè€ƒè™‘ä¸åŒçš„å±æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†VLMå¹¶ä¸åªæ˜¯ç®€å•åœ°å°†å›¾åƒä¸åœºæ™¯æè¿°ç›¸åŒ¹é…ï¼Œéè§†è§‰ç”šè‡³è¯¯å¯¼æ€§çš„æè¿°å¯¹å…¶è¡¨ç¤ºäº§ç”Ÿäº†é‡å¤§å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16442v2">PDF</a> EMNLP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è¡¨ç¤ºæ¦‚å¿µæ—¶å¦‚ä½•ä¼˜å…ˆå¤„ç†è§†è§‰å±æ€§ã€‚é€šè¿‡æå‡ºä¸€ç§åä¸ºEX2çš„æ–°æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå¯¹VLMé‡è§†çš„æ–‡æœ¬ç‰¹å¾è¿›è¡Œäº†æ·±å…¥å‰–æã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¼ºåŒ–å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œç”¨äºåˆ†æè¯­è¨€æ¨¡å‹ä¸­VLMçš„åå¥½ã€‚åˆ†æç»“æœæ˜¾ç¤ºï¼Œéè§†è§‰å±æ€§ï¼ˆå¦‚ç¯å¢ƒï¼‰åœ¨æè¿°è§†è§‰æ¦‚å¿µæ–¹é¢èµ·ç€é‡è¦ä½œç”¨ï¼ŒåŒæ—¶å‘ç°ä¸€äº›æ— å…³çš„æè¿°ä¹Ÿèƒ½å½±å“VLMçš„è§£è¯»ã€‚æ€»ä¹‹ï¼Œæœ¬ç ”ç©¶è¡¨æ˜VLMä¸ä»…å°†å›¾åƒä¸åœºæ™¯æè¿°ç›¸å¯¹åº”ï¼ŒåŒæ—¶æ–‡æœ¬å±æ€§å’Œæ¦‚å¿µä¹Ÿå¯èƒ½æå¤§åœ°å½±å“å…¶è§£é‡Šèƒ½åŠ›ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå‡†ç¡®è¯„ä¼°ç†è§£å’Œåˆ†æä½¿ç”¨å¤æ‚çš„è‡ªç„¶è¯­è¨€è¾…åŠ©æŠ€æœ¯åœ¨å¤šåª’ä½“åˆ†æä¸­çš„åº”ç”¨éå¸¸å…³é”®ã€‚å› ä¸ºå¹¶éæ‰€æœ‰è¾“å…¥éƒ½åŒ…å«ç›´è§‚ç›¸å…³çš„ç‰¹å¾ï¼Œæœ‰æ—¶VLMå¯èƒ½éœ€è¦æ›´å¤æ‚ã€éç›´æ¥çš„æ¨ç†æ‰èƒ½å½¢æˆç²¾ç¡®çš„è§£é‡Šå’Œè¡¨ç¤ºã€‚è¿™ä¸€ç‚¹æé†’æˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­ä¸èƒ½ä»…ä¾èµ–ç®€å•çš„å‡è®¾æˆ–æœªç»éªŒè¯çš„æ¨¡å¼æ¥ç†è§£æˆ–ä¼˜åŒ–è¿™äº›ç³»ç»Ÿçš„è¡Œä¸ºã€‚ç›¸åï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡æ›´ç»†è‡´çš„ç ”ç©¶å’Œæ›´å…¨é¢çš„åˆ†ææ¥ç¡®ä¿è¿™äº›ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚è¿™å°†æœ‰åŠ©äºå¼€å‘æ›´å‡†ç¡®ã€æ›´å…·é²æ£’æ€§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ”¹è¿›äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨å¤æ‚åœºæ™¯ä¸­çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›å¥ å®šåŸºç¡€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šåœ°å…³æ³¨å’Œç ”ç©¶è‡ªç„¶è¯­è¨€åœ¨å¤šåª’ä½“åˆ†æå’Œè§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¤æ‚æ€§ä»¥åŠå®ƒåœ¨å…¶ä¸­çš„ä½œç”¨æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-451df1b131008bdea484c3cc506d44aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e5a0364c0b89aa8f747aea97b167a86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db320a58da9581e5d17cd7902d9dca50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-313b4e8386e5895c0bc66b8946f625e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ffb1d187583d0b2574ad5254b617774.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ce28e16bace03c0796249a024590268.jpg" align="middle">
</details>




<h2 id="FT2TF-First-Person-Statement-Text-To-Talking-Face-Generation"><a href="#FT2TF-First-Person-Statement-Text-To-Talking-Face-Generation" class="headerlink" title="FT2TF: First-Person Statement Text-To-Talking Face Generation"></a>FT2TF: First-Person Statement Text-To-Talking Face Generation</h2><p><strong>Authors:Xingjian Diao, Ming Cheng, Wayner Barrios, SouYoung Jin</strong></p>
<p>Talking face generation has gained immense popularity in the computer vision community, with various applications including AR, VR, teleconferencing, digital assistants, and avatars. Traditional methods are mainly audio-driven, which have to deal with the inevitable resource-intensive nature of audio storage and processing. To address such a challenge, we propose FT2TF - First-Person Statement Text-To-Talking Face Generation, a novel one-stage end-to-end pipeline for talking face generation driven by first-person statement text. Different from previous work, our model only leverages visual and textual information without any other sources (e.g., audio&#x2F;landmark&#x2F;pose) during inference. Extensive experiments are conducted on LRS2 and LRS3 datasets, and results on multi-dimensional evaluation metrics are reported. Both quantitative and qualitative results showcase that FT2TF outperforms existing relevant methods and reaches the state-of-the-art. This achievement highlights our modelâ€™s capability to bridge first-person statements and dynamic face generation, providing insightful guidance for future work. </p>
<blockquote>
<p>é¢éƒ¨ç”ŸæˆæŠ€æœ¯å·²åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè·å¾—äº†æå¤§çš„æ™®åŠï¼Œå¹¶å¹¿æ³›åº”ç”¨äºARã€VRã€è§†é¢‘ä¼šè®®ã€æ•°å­—åŠ©ç†å’ŒåŒ–èº«ç­‰å„ç§åº”ç”¨åœºæ™¯ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¸»è¦æ˜¯éŸ³é¢‘é©±åŠ¨çš„ï¼Œå¿…é¡»åº”å¯¹éŸ³é¢‘å­˜å‚¨å’Œå¤„ç†èµ„æºå¯†é›†å‹çš„ä¸å¯é¿å…çš„æ€§è´¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FT2TFâ€”â€”åŸºäºç¬¬ä¸€äººç§°å™è¿°æ–‡æœ¬çš„è¯´è¯é¢éƒ¨ç”ŸæˆæŠ€æœ¯ã€‚è¿™æ˜¯ä¸€ç§æ–°å‹çš„ã€ç«¯åˆ°ç«¯çš„ä¸€ç«™å¼æµç¨‹ï¼Œç”¨äºæ ¹æ®ç¬¬ä¸€äººç§°å™è¿°æ–‡æœ¬ç”Ÿæˆè¯´è¯é¢éƒ¨ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œä¸ä¾èµ–ä»»ä½•å…¶ä»–æ¥æºï¼ˆå¦‚éŸ³é¢‘&#x2F;åœ°æ ‡&#x2F;å§¿åŠ¿ï¼‰ã€‚æˆ‘ä»¬åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶æŠ¥å‘Šäº†å¤šç»´è¯„ä»·æŒ‡æ ‡çš„ç»“æœã€‚å®šé‡å’Œå®šæ€§ç»“æœå‡è¡¨æ˜ï¼ŒFT2TFä¼˜äºç°æœ‰çš„ç›¸å…³æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚è¿™ä¸€æˆå°±å‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¨¡å‹å°†ç¬¬ä¸€äººç§°å™è¿°ä¸åŠ¨æ€é¢éƒ¨ç”Ÿæˆç›¸ç»“åˆçš„èƒ½åŠ›ï¼Œä¸ºæœªæ¥å·¥ä½œæä¾›äº†æœ‰ç›Šçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.05430v2">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬çš„ç¬¬ä¸€äººç§°å™è¿°è¿›è¡Œè¯´è¯äººè„¸ç”Ÿæˆçš„æŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²å¹¿å—æ¬¢è¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢å¼ºç°å®ã€è™šæ‹Ÿç°å®ã€è¿œç¨‹ä¼šè®®ã€æ•°å­—åŠ©ç†å’ŒåŒ–èº«ç­‰åº”ç”¨ä¸­ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–éŸ³é¢‘é©±åŠ¨ï¼Œé¢ä¸´éŸ³é¢‘å­˜å‚¨å’Œå¤„ç†çš„èµ„æºå¯†é›†å‹æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FT2TFâ€”â€”åŸºäºç¬¬ä¸€äººç§°å™è¿°æ–‡æœ¬åˆ°è¯´è¯äººè„¸ç”Ÿæˆçš„ä¸€ç«™å¼ç«¯åˆ°ç«¯ç®¡é“ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæ— éœ€å…¶ä»–æ¥æºï¼ˆå¦‚éŸ³é¢‘&#x2F;åœ°æ ‡&#x2F;å§¿æ€ï¼‰ã€‚åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶åœ¨å¤šç»´è¯„ä»·æŒ‡æ ‡ä¸ŠæŠ¥å‘Šäº†ç»“æœã€‚å®šé‡å’Œå®šæ€§ç»“æœå‡è¡¨æ˜ï¼ŒFT2TFä¼˜äºç°æœ‰ç›¸å…³æ–¹æ³•å¹¶è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œè¿™çªæ˜¾äº†æˆ‘ä»¬æ¨¡å‹å°†ç¬¬ä¸€äººç§°å™è¿°ä¸åŠ¨æ€äººè„¸ç”Ÿæˆç›¸ç»“åˆçš„èƒ½åŠ›ï¼Œä¸ºæœªæ¥å·¥ä½œæä¾›äº†æœ‰ç›Šçš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¹¶åº”ç”¨äºARã€VRã€è¿œç¨‹ä¼šè®®ç­‰åœºæ™¯ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–éŸ³é¢‘é©±åŠ¨ï¼Œå­˜åœ¨èµ„æºå¯†é›†å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>FT2TFæ˜¯ä¸€ç§æ–°é¢–çš„ä¸€ç«™å¼ç«¯åˆ°ç«¯ç®¡é“ï¼ŒåŸºäºæ–‡æœ¬çš„ç¬¬ä¸€äººç§°å™è¿°è¿›è¡Œè¯´è¯äººè„¸ç”Ÿæˆã€‚</li>
<li>FT2TFæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>FT2TFåœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>FT2TFä¼˜äºç°æœ‰æ–¹æ³•å¹¶è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œçªæ˜¾äº†æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0be3f7b6e599b54fa5655efa449462f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16abd5f08effb328b5b2e5c12dde8df2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1215753f18bf0b1af5a60655736bbe5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eca55ba849d8d0c692682c9eac1de8ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b870f9d1f27b794a6c4c8147bfa595e.jpg" align="middle">
</details>




<h2 id="VAST-Vivify-Your-Talking-Avatar-via-Zero-Shot-Expressive-Facial-Style-Transfer"><a href="#VAST-Vivify-Your-Talking-Avatar-via-Zero-Shot-Expressive-Facial-Style-Transfer" class="headerlink" title="VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style   Transfer"></a>VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style   Transfer</h2><p><strong>Authors:Liyang Chen, Zhiyong Wu, Runnan Li, Weihong Bao, Jun Ling, Xu Tan, Sheng Zhao</strong></p>
<p>Current talking face generation methods mainly focus on speech-lip synchronization. However, insufficient investigation on the facial talking style leads to a lifeless and monotonous avatar. Most previous works fail to imitate expressive styles from arbitrary video prompts and ensure the authenticity of the generated video. This paper proposes an unsupervised variational style transfer model (VAST) to vivify the neutral photo-realistic avatars. Our model consists of three key components: a style encoder that extracts facial style representations from the given video prompts; a hybrid facial expression decoder to model accurate speech-related movements; a variational style enhancer that enhances the style space to be highly expressive and meaningful. With our essential designs on facial style learning, our model is able to flexibly capture the expressive facial style from arbitrary video prompts and transfer it onto a personalized image renderer in a zero-shot manner. Experimental results demonstrate the proposed approach contributes to a more vivid talking avatar with higher authenticity and richer expressiveness. </p>
<blockquote>
<p>å½“å‰çš„äººè„¸è¯´è¯ç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨è¯­éŸ³ä¸å˜´å”‡çš„åŒæ­¥ã€‚ç„¶è€Œï¼Œå¯¹é¢éƒ¨è¯´è¯é£æ ¼çš„ç ”ç©¶ä¸è¶³å¯¼è‡´ç”Ÿæˆçš„è™šæ‹Ÿè§’è‰²ç¼ºä¹ç”Ÿå‘½åŠ›å’Œå•è°ƒã€‚ä¹‹å‰çš„å¤§å¤šæ•°å·¥ä½œæ— æ³•æ¨¡ä»¿æ¥è‡ªä»»æ„è§†é¢‘æç¤ºçš„è¡¨è¾¾é£æ ¼ï¼Œä¹Ÿæ— æ³•ç¡®ä¿ç”Ÿæˆè§†é¢‘çš„çœŸå®æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„å˜é£æ ¼è½¬ç§»æ¨¡å‹ï¼ˆVASTï¼‰ï¼Œä»¥èµ‹äºˆä¸­æ€§é€¼çœŸçš„è™šæ‹Ÿè§’è‰²ç”Ÿå‘½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šä¸€ä¸ªé£æ ¼ç¼–ç å™¨ï¼Œç”¨äºä»ç»™å®šçš„è§†é¢‘æç¤ºä¸­æå–é¢éƒ¨é£æ ¼è¡¨ç¤ºï¼›ä¸€ä¸ªæ··åˆé¢éƒ¨è¡¨æƒ…è§£ç å™¨ï¼Œä»¥æ¨¡æ‹Ÿå‡†ç¡®çš„è¯­éŸ³ç›¸å…³åŠ¨ä½œï¼›ä¸€ä¸ªå˜é£æ ¼å¢å¼ºå™¨ï¼Œç”¨äºå¢å¼ºé£æ ¼ç©ºé—´ï¼Œä½¿å…¶å…·æœ‰é«˜åº¦è¡¨è¾¾åŠ›å’Œæ„ä¹‰ã€‚é€šè¿‡å¯¹é¢éƒ¨é£æ ¼å­¦ä¹ çš„å…³é”®è®¾è®¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°æ•æ‰æ¥è‡ªä»»æ„è§†é¢‘æç¤ºçš„è¡¨è¾¾æ€§é¢éƒ¨é£æ ¼ï¼Œå¹¶å°†å…¶ä»¥é›¶æ ·æœ¬çš„æ–¹å¼è½¬ç§»åˆ°ä¸ªæ€§åŒ–å›¾åƒæ¸²æŸ“å™¨ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æœ‰åŠ©äºåˆ›å»ºä¸€ä¸ªæ›´ç”ŸåŠ¨ã€æ›´çœŸå®ã€æ›´å…·è¡¨ç°åŠ›çš„è¯´è¯è™šæ‹Ÿè§’è‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04830v3">PDF</a> Accepted by ICCV2023</p>
<p><strong>Summary</strong><br>ï¼šå½“å‰ä¸»æµçš„é¢éƒ¨ç”ŸæˆæŠ€æœ¯ä¸»è¦å…³æ³¨è¯­éŸ³ä¸å˜´å”‡çš„åŒæ­¥ï¼Œä½†å¯¹é¢éƒ¨è¯´è¯é£æ ¼çš„ç ”ç©¶ä¸è¶³å¯¼è‡´ç”Ÿæˆçš„è™šæ‹Ÿå½¢è±¡ç¼ºä¹ç”Ÿå‘½åŠ›å’Œå¤šæ ·æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£çš„å˜é£æ ¼è½¬ç§»æ¨¡å‹ï¼ˆVASTï¼‰ï¼Œæ—¨åœ¨ä½¿ä¸­æ€§é€¼çœŸçš„å¤´åƒæ›´åŠ ç”ŸåŠ¨ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä»ç»™å®šè§†é¢‘æç¤ºä¸­æå–é¢éƒ¨é£æ ¼è¡¨ç¤ºçš„é£æ ¼ç¼–ç å™¨ï¼›æ¨¡æ‹Ÿç²¾ç¡®è¯­éŸ³ç›¸å…³åŠ¨ä½œçš„æ··åˆé¢éƒ¨è¡¨æƒ…è§£ç å™¨ï¼›å¢å¼ºé£æ ¼ç©ºé—´ä»¥å®ç°é«˜åº¦è¡¨è¾¾å’Œæœ‰æ„ä¹‰çš„å˜é£æ ¼å¢å¼ºå™¨ã€‚é€šè¿‡å¯¹é¢éƒ¨é£æ ¼å­¦ä¹ çš„å…³é”®è®¾è®¡ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°æ•æ‰ä»»æ„è§†é¢‘æç¤ºä¸­çš„è¡¨è¾¾æ€§é¢éƒ¨é£æ ¼ï¼Œå¹¶å°†å…¶é›¶æ ·æœ¬æ–¹å¼è½¬ç§»åˆ°ä¸ªæ€§åŒ–å›¾åƒæ¸²æŸ“å™¨ä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰åŠ©äºåˆ›å»ºæ›´ç”ŸåŠ¨ã€æ›´çœŸå®ã€æ›´å…·è¡¨ç°åŠ›çš„è¯´è¯å¤´åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é¢éƒ¨ç”ŸæˆæŠ€æœ¯ä¸»è¦å…³æ³¨è¯­éŸ³ä¸å˜´å”‡åŒæ­¥ï¼Œå¿½è§†äº†é¢éƒ¨è¯´è¯é£æ ¼çš„å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºçš„æ— ç›‘ç£å˜é£æ ¼è½¬ç§»æ¨¡å‹ï¼ˆVASTï¼‰æ—¨åœ¨å¢å¼ºä¸­æ€§å¤´åƒçš„ç”ŸåŠ¨æ€§ã€‚</li>
<li>æ¨¡å‹åŒ…æ‹¬é£æ ¼ç¼–ç å™¨ã€æ··åˆé¢éƒ¨è¡¨æƒ…è§£ç å™¨å’Œå˜é£æ ¼å¢å¼ºå™¨ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>é£æ ¼ç¼–ç å™¨ä»è§†é¢‘æç¤ºä¸­æå–é¢éƒ¨é£æ ¼è¡¨ç¤ºã€‚</li>
<li>æ··åˆé¢éƒ¨è¡¨æƒ…è§£ç å™¨æ¨¡æ‹Ÿè¯­éŸ³ç›¸å…³çš„ç²¾ç¡®åŠ¨ä½œã€‚</li>
<li>å˜é£æ ¼å¢å¼ºå™¨èƒ½å¤Ÿå¢å¼ºé£æ ¼ç©ºé—´ï¼Œä½¿è¡¨è¾¾æ›´åŠ ä¸°å¯Œå¤šå½©ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e689f3d922179f5f39d7a2882859cb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a819020fd662736bf40be61db67f133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a66b31dd4d8b90a061ab8f1b7532e83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b019b9e44fa8336d68249df48a51c5f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73bf539f51040af219e1a9529abc6acf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44add23e767fc59670cf6c5f95c16891.jpg" align="middle">
</details>




<h2 id="DAE-Talker-High-Fidelity-Speech-Driven-Talking-Face-Generation-with-Diffusion-Autoencoder"><a href="#DAE-Talker-High-Fidelity-Speech-Driven-Talking-Face-Generation-with-Diffusion-Autoencoder" class="headerlink" title="DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with   Diffusion Autoencoder"></a>DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with   Diffusion Autoencoder</h2><p><strong>Authors:Chenpeng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, Jiang Bian</strong></p>
<p>While recent research has made significant progress in speech-driven talking face generation, the quality of the generated video still lags behind that of real recordings. One reason for this is the use of handcrafted intermediate representations like facial landmarks and 3DMM coefficients, which are designed based on human knowledge and are insufficient to precisely describe facial movements. Additionally, these methods require an external pretrained model for extracting these representations, whose performance sets an upper bound on talking face generation. To address these limitations, we propose a novel method called DAE-Talker that leverages data-driven latent representations obtained from a diffusion autoencoder (DAE). DAE contains an image encoder that encodes an image into a latent vector and a DDIM image decoder that reconstructs the image from it. We train our DAE on talking face video frames and then extract their latent representations as the training target for a Conformer-based speech2latent model. This allows DAE-Talker to synthesize full video frames and produce natural head movements that align with the content of speech, rather than relying on a predetermined head pose from a template video. We also introduce pose modelling in speech2latent for pose controllability. Additionally, we propose a novel method for generating continuous video frames with the DDIM image decoder trained on individual frames, eliminating the need for modelling the joint distribution of consecutive frames directly. Our experiments show that DAE-Talker outperforms existing popular methods in lip-sync, video fidelity, and pose naturalness. We also conduct ablation studies to analyze the effectiveness of the proposed techniques and demonstrate the pose controllability of DAE-Talker. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„ç ”ç©¶åœ¨è¯­éŸ³é©±åŠ¨çš„å¤´éƒ¨åˆ†è¯´ç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç”Ÿæˆè§†é¢‘çš„è´¨é‡Œä»ç„¶è½åäºçœŸå®å½•åƒã€‚é€ æˆè¿™ä¸€é—®é¢˜çš„å…¶ä¸­ä¸€ä¸ªåŸå› æ˜¯ä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„ä¸­ä»‹è¡¨ç¤ºï¼Œå¦‚é¢éƒ¨æ ‡å¿—å’Œ3DMMç³»æ•°ï¼Œè¿™äº›è¡¨ç¤ºæ˜¯åŸºäºäººç±»çŸ¥è¯†è®¾è®¡çš„ï¼Œä¸è¶³ä»¥ç²¾ç¡®æè¿°é¢éƒ¨åŠ¨ä½œã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¤–éƒ¨é¢„è®­ç»ƒæ¨¡å‹æ¥æå–è¿™äº›è¡¨ç¤ºï¼Œå…¶æ€§èƒ½ä¸ºå¤´éƒ¨åˆ†è¯´è¯ç”Ÿæˆè®¾ç½®äº†ä¸Šé™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºDAE-Talkerçš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä»æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEï¼‰è·å¾—çš„æ•°æ®é©±åŠ¨æ½œåœ¨è¡¨ç¤ºã€‚DAEåŒ…å«ä¸€ä¸ªå°†å›¾åƒç¼–ç ä¸ºæ½œåœ¨å‘é‡çš„å›¾åƒç¼–ç å™¨å’Œä¸€ä¸ªä»è¯¥å‘é‡é‡å»ºå›¾åƒçš„DDIMå›¾åƒè§£ç å™¨ã€‚æˆ‘ä»¬åœ¨è¯´è¯çš„é¢éƒ¨è§†é¢‘å¸§ä¸Šè®­ç»ƒæˆ‘ä»¬çš„DAEï¼Œç„¶åæå–å…¶æ½œåœ¨è¡¨ç¤ºä½œä¸ºåŸºäºConformerçš„speech2latentæ¨¡å‹çš„è®­ç»ƒç›®æ ‡ã€‚è¿™ä½¿å¾—DAE-Talkerèƒ½å¤Ÿåˆæˆå®Œæ•´çš„è§†é¢‘å¸§å¹¶äº§ç”Ÿä¸è¯­éŸ³å†…å®¹ç›¸ç¬¦çš„è‡ªç„¶å¤´éƒ¨åŠ¨ä½œï¼Œè€Œä¸æ˜¯ä¾èµ–äºæ¨¡æ¿è§†é¢‘çš„é¢„è®¾å¤´éƒ¨å§¿åŠ¿ã€‚æˆ‘ä»¬è¿˜ä¸ºspeech2latentå¼•å…¥äº†å§¿åŠ¿å»ºæ¨¡ä»¥å®ç°å§¿åŠ¿å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨DDIMå›¾åƒè§£ç å™¨å¯¹å•ç‹¬å¸§è¿›è¡Œè®­ç»ƒä»¥ç”Ÿæˆè¿ç»­è§†é¢‘å¸§çš„æ–°æ–¹æ³•ï¼Œä»è€Œæ— éœ€ç›´æ¥å¯¹è¿ç»­å¸§çš„è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å”‡åŒæ­¥ã€è§†é¢‘ä¿çœŸåº¦å’Œå§¿åŠ¿è‡ªç„¶æ€§æ–¹é¢ï¼ŒDAE-Talkerä¼˜äºç°æœ‰çš„æµè¡Œæ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å‰”é™¤ç ”ç©¶ä»¥åˆ†ææ‰€æå‡ºæŠ€æœ¯çš„æœ‰æ•ˆæ€§å¹¶å±•ç¤ºDAE-Talkerçš„å§¿åŠ¿å¯æ§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2303.17550v6">PDF</a> Accepted to ACM Multimedia 2023</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå°½ç®¡åœ¨è¯­éŸ³é©±åŠ¨è°ˆè¯å¤´ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ä»ç„¶è½åäºçœŸå®å½•éŸ³ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†DAE-Talkeræ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEï¼‰è·å¾—æ•°æ®é©±åŠ¨æ½œåœ¨è¡¨ç¤ºï¼Œèƒ½åˆæˆå®Œæ•´è§†é¢‘å¸§å¹¶äº§ç”Ÿä¸è¯­éŸ³å†…å®¹å¯¹é½çš„è‡ªç„¶å¤´éƒ¨è¿åŠ¨ã€‚æ–°æ–¹æ³•æå‡äº†å”‡å½¢åŒæ­¥ã€è§†é¢‘ä¿çœŸåº¦å’Œå§¿åŠ¿è‡ªç„¶åº¦ï¼Œå¹¶å…·å¤‡å§¿åŠ¿æ§åˆ¶èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³é©±åŠ¨è°ˆè¯å¤´ç”Ÿæˆè™½æœ‰æ‰€è¿›å±•ï¼Œä½†ç”Ÿæˆè§†é¢‘è´¨é‡ä¸çœŸå®å½•éŸ³ç›¸æ¯”ä»æœ‰å·®è·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ‰‹å·¥è®¾è®¡çš„ä¸­é—´è¡¨ç¤ºï¼Œå¦‚é¢éƒ¨æ ‡å¿—å’Œ3DMMç³»æ•°ï¼Œä¸è¶³ä»¥ç²¾ç¡®æè¿°é¢éƒ¨è¿åŠ¨ã€‚</li>
<li>DAE-Talkerä½¿ç”¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDAEï¼‰è·å¾—æ½œåœ¨è¡¨ç¤ºï¼Œæ”¹è¿›äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>DAE-Talkerèƒ½åˆæˆå®Œæ•´è§†é¢‘å¸§ï¼Œäº§ç”Ÿè‡ªç„¶å¤´éƒ¨è¿åŠ¨ï¼Œä¸è¯­éŸ³å†…å®¹å¯¹é½ã€‚</li>
<li>å¼•å…¥å§¿åŠ¿å»ºæ¨¡ä»¥å¢å¼ºè¯­éŸ³ç”Ÿæˆè§†é¢‘çš„å§¿åŠ¿æ§åˆ¶èƒ½åŠ›ã€‚</li>
<li>æ–°æ–¹æ³•æå‡äº†å”‡å½¢åŒæ­¥ã€è§†é¢‘ä¿çœŸåº¦å’Œå§¿åŠ¿è‡ªç„¶åº¦ï¼Œå®éªŒè¯æ˜å…¶æ€§èƒ½ä¼˜äºç°æœ‰æµè¡Œæ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b812c24339d13a71911fd26bc39b7156.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07dc01c83f2139531c19e29de1547c7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0aef592d745d84ec9f05fc59e484b3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66bee2b907834538379792fcfd2b3f8d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f51710e03a41a3dd36b4fa779dd6dd6.jpg" align="middle">
</details>




<h2 id="Memories-are-One-to-Many-Mapping-Alleviators-in-Talking-Face-Generation"><a href="#Memories-are-One-to-Many-Mapping-Alleviators-in-Talking-Face-Generation" class="headerlink" title="Memories are One-to-Many Mapping Alleviators in Talking Face Generation"></a>Memories are One-to-Many Mapping Alleviators in Talking Face Generation</h2><p><strong>Authors:Anni Tang, Tianyu He, Xu Tan, Jun Ling, Li Song</strong></p>
<p>Talking face generation aims at generating photo-realistic video portraits of a target person driven by input audio. Due to its nature of one-to-many mapping from the input audio to the output video (e.g., one speech content may have multiple feasible visual appearances), learning a deterministic mapping like previous works brings ambiguity during training, and thus causes inferior visual results. Although this one-to-many mapping could be alleviated in part by a two-stage framework (i.e., an audio-to-expression model followed by a neural-rendering model), it is still insufficient since the prediction is produced without enough information (e.g., emotions, wrinkles, etc.). In this paper, we propose MemFace to complement the missing information with an implicit memory and an explicit memory that follow the sense of the two stages respectively. More specifically, the implicit memory is employed in the audio-to-expression model to capture high-level semantics in the audio-expression shared space, while the explicit memory is employed in the neural-rendering model to help synthesize pixel-level details. Our experimental results show that our proposed MemFace surpasses all the state-of-the-art results across multiple scenarios consistently and significantly. </p>
<blockquote>
<p>é¢éƒ¨è°ˆè¯ç”Ÿæˆçš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥éŸ³é¢‘ç”Ÿæˆç›®æ ‡äººç‰©çš„ç…§ç‰‡çº§çœŸå®è§†é¢‘è‚–åƒã€‚ç”±äºå…¶ä»è¾“å…¥éŸ³é¢‘åˆ°è¾“å‡ºè§†é¢‘çš„ä¸€å¯¹å¤šæ˜ å°„ç‰¹æ€§ï¼ˆä¾‹å¦‚ï¼Œä¸€ç§è¯­éŸ³å†…å®¹å¯èƒ½å…·æœ‰å¤šç§å¯è¡Œçš„è§†è§‰å¤–è§‚ï¼‰ï¼Œåƒä¹‹å‰çš„å·¥ä½œé‚£æ ·å­¦ä¹ ç¡®å®šæ€§æ˜ å°„ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥æ¨¡ç³Šæ€§ï¼Œä»è€Œå¯¼è‡´è§†è§‰ç»“æœè¾ƒå·®ã€‚è™½ç„¶è¿™ç§ä¸€å¯¹å¤šæ˜ å°„å¯ä»¥é€šè¿‡ä¸¤é˜¶æ®µæ¡†æ¶éƒ¨åˆ†ç¼“è§£ï¼ˆå³éŸ³é¢‘åˆ°è¡¨æƒ…æ¨¡å‹ï¼Œç„¶åæ˜¯ç¥ç»æ¸²æŸ“æ¨¡å‹ï¼‰ï¼Œä½†ç”±äºé¢„æµ‹ç»“æœç¼ºä¹è¶³å¤Ÿçš„ä¿¡æ¯ï¼ˆä¾‹å¦‚æƒ…ç»ªã€çš±çº¹ç­‰ï¼‰ï¼Œå› æ­¤ä»ç„¶ä¸è¶³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºMemFaceï¼Œé€šè¿‡éšæ€§è®°å¿†å’Œæ˜¾æ€§è®°å¿†æ¥è¡¥å……ç¼ºå¤±çš„ä¿¡æ¯ï¼Œè¿™ä¸¤é˜¶æ®µåˆ†åˆ«éµå¾ªå„è‡ªçš„æ„ä¹‰ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œéšæ€§è®°å¿†è¢«åº”ç”¨äºéŸ³é¢‘åˆ°è¡¨æƒ…æ¨¡å‹ï¼Œä»¥æ•è·éŸ³é¢‘è¡¨æƒ…å…±äº«ç©ºé—´ä¸­çš„é«˜çº§è¯­ä¹‰ï¼Œè€Œæ˜¾æ€§è®°å¿†åˆ™è¢«åº”ç”¨äºç¥ç»æ¸²æŸ“æ¨¡å‹ï¼Œä»¥å¸®åŠ©åˆæˆåƒç´ çº§ç»†èŠ‚ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MemFaceåœ¨å¤šåœºæ™¯ä¸‹ä¸€è´¯ä¸”æ˜¾è‘—åœ°è¶…è¶Šäº†æ‰€æœ‰æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2212.05005v4">PDF</a> IEEE Transactions on Pattern Analysis and Machine Intelligence   (2024). Project page: see <a target="_blank" rel="noopener" href="https://memoryface.github.io/">https://memoryface.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘ç›®æ ‡äººç‰©çš„è¯­éŸ³é©±åŠ¨çš„è§†é¢‘è‚–åƒç”ŸæˆæŠ€æœ¯ã€‚ç”±äºéŸ³é¢‘è¾“å…¥ä¸è§†é¢‘è¾“å‡ºä¹‹é—´çš„ä¸€åˆ°å¤šæ˜ å°„å…³ç³»ï¼Œä½¿å¾—å­¦ä¹ ç¡®å®šæ€§æ˜ å°„å­˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­§ä¹‰æ€§ï¼Œå¯¼è‡´è§†è§‰ç»“æœè´¨é‡ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMemFaceçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡éšå¼è®°å¿†å’Œæ˜¾å¼è®°å¿†æ¥è¡¥å……ç¼ºå¤±ä¿¡æ¯ã€‚éšå¼è®°å¿†ç”¨äºæ•æ‰éŸ³é¢‘ä¸è¡¨æƒ…å…±äº«ç©ºé—´ä¸­çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ï¼Œæ˜¾å¼è®°å¿†åˆ™ç”¨äºåˆæˆåƒç´ çº§ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMemFaceåœ¨å¤šä¸ªåœºæ™¯ä¸‹å‡è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å‘ç›®æ ‡äººç‰©çš„è¯­éŸ³é©±åŠ¨çš„è§†é¢‘è‚–åƒç”ŸæˆæŠ€æœ¯æ—¨åœ¨ç”ŸæˆçœŸå®æ„Ÿå¼ºçš„è§†é¢‘è‚–åƒã€‚</li>
<li>éŸ³é¢‘è¾“å…¥ä¸è§†é¢‘è¾“å‡ºä¹‹é—´å­˜åœ¨ä¸€åˆ°å¤šæ˜ å°„å…³ç³»ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­§ä¹‰æ€§ã€‚</li>
<li>MemFaceé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒ…æ‹¬éŸ³é¢‘åˆ°è¡¨æƒ…æ¨¡å‹å’Œç¥ç»æ¸²æŸ“æ¨¡å‹ã€‚</li>
<li>éšå¼è®°å¿†ç”¨äºæ•æ‰éŸ³é¢‘ä¸è¡¨æƒ…å…±äº«ç©ºé—´ä¸­çš„é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>æ˜¾å¼è®°å¿†ç”¨äºåˆæˆåƒç´ çº§ç»†èŠ‚ï¼Œæé«˜è§†é¢‘è´¨é‡ã€‚</li>
<li>MemFaceåœ¨å¤šä¸ªåœºæ™¯ä¸‹å‡è¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f98ac737bce7990a4027434bb1b884b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11d7578fc75fd8e1ac8d4a28f7774e0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab26fa1b50458ea8bc953889d295b411.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4205ff8af0655c24641f1047ce5319fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535df829996a313d5ee605b4fc8d2067.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9b8af394d33e303a111b6fbe5b092658.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  [MASK] is All You Need
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ba1148f524c897fed52125ae4e1dc003.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  MixedGaussianAvatar Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
