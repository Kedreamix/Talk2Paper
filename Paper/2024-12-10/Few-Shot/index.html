<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  JAPAGEN Efficient Few/Zero-shot Learning via Japanese Training Dataset   Generation with LLM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-102376073a70ff046fee9027769501d3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    28.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    114 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-10-æ›´æ–°"><a href="#2024-12-10-æ›´æ–°" class="headerlink" title="2024-12-10 æ›´æ–°"></a>2024-12-10 æ›´æ–°</h1><h2 id="JAPAGEN-Efficient-Few-Zero-shot-Learning-via-Japanese-Training-Dataset-Generation-with-LLM"><a href="#JAPAGEN-Efficient-Few-Zero-shot-Learning-via-Japanese-Training-Dataset-Generation-with-LLM" class="headerlink" title="JAPAGEN: Efficient Few&#x2F;Zero-shot Learning via Japanese Training Dataset   Generation with LLM"></a>JAPAGEN: Efficient Few&#x2F;Zero-shot Learning via Japanese Training Dataset   Generation with LLM</h2><p><strong>Authors:Takuro Fujii, Satoru Katsumata</strong></p>
<p>Recently some studies have highlighted the potential of Large Language Models (LLMs) as effective generators of supervised training data, offering advantages such as enhanced inference efficiency and reduced costs associated with data collection. However, these studies have predominantly focused on English language tasks. In this paper, we address the fundamental research question: Can LLMs serve as proficient training data generators for other language tasks? Specifically, we leverage LLMs to synthesize supervised training data under few-shot and zero-shot learning scenarios across six diverse Japanese downstream tasks. Subsequently, we utilize this synthesized data to train compact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our experimental findings underscore that JAPAGEN achieves robust performance in classification tasks that necessitate formal text inputs, demonstrating competitive results compared to conventional LLM prompting strategies. </p>
<blockquote>
<p>æœ€è¿‘çš„ä¸€äº›ç ”ç©¶çªå‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæœ‰æ•ˆç›‘ç£è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨çš„æ½œåŠ›ï¼Œæä¾›äº†æé«˜æ¨ç†æ•ˆç‡å’Œé™ä½æ•°æ®é‡‡é›†æˆæœ¬ç­‰ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä»»åŠ¡ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºæœ¬çš„ç ”ç©¶é—®é¢˜ï¼šLLMèƒ½å¦ä¸ºå…¶ä»–è¯­è¨€ä»»åŠ¡æä¾›ç†Ÿç»ƒçš„è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼Ÿå…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å…­ç§ä¸åŒçš„æ—¥è¯­ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨LLMåœ¨å°‘é‡å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹åˆæˆç›‘ç£è®­ç»ƒæ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›åˆæˆçš„æ•°æ®æ¥è®­ç»ƒç´§å‡‘æ¨¡å‹ï¼ˆä¾‹å¦‚BERTï¼‰ã€‚è¿™ç§æ–°æ–¹æ³•è¢«ç§°ä¸ºJAPAGENã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒJAPAGENåœ¨éœ€è¦æ­£å¼æ–‡æœ¬è¾“å…¥çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ï¼Œä¸ä¼ ç»Ÿçš„LLMæç¤ºç­–ç•¥ç›¸æ¯”ï¼Œå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06738v1">PDF</a> Accepted by PACLIC38 (2024)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºç›‘ç£è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨çš„æ½œåŠ›æ—¥ç›Šå—åˆ°å…³æ³¨ï¼Œå¯æé«˜æ¨ç†æ•ˆç‡å’Œé™ä½æ•°æ®é‡‡é›†æˆæœ¬ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼šLLMèƒ½å¦ä¸ºå…¶ä»–è¯­è¨€ä»»åŠ¡æä¾›é«˜æ•ˆè®­ç»ƒæ•°æ®ç”Ÿæˆï¼Ÿç ”ç©¶åˆ©ç”¨LLMåœ¨å…­ç§ä¸åŒæ—¥è¯­ä¸‹æ¸¸ä»»åŠ¡ä¸­åˆæˆç›‘ç£è®­ç»ƒæ•°æ®ï¼Œå¹¶ç”¨å…¶è®­ç»ƒç´§å‡‘æ¨¡å‹ï¼ˆå¦‚BERTï¼‰ã€‚å®éªŒç»“æœè¯æ˜ï¼Œåœ¨éœ€è¦æ­£å¼æ–‡æœ¬è¾“å…¥çš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œä¸ä¼ ç»ŸLLMæç¤ºç­–ç•¥ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¯ä½œä¸ºç›‘ç£è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼Œç”¨äºå¤šç§è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>åœ¨æ—¥è¯­ä¸‹æ¸¸ä»»åŠ¡çš„å°‘æ•°å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ï¼ŒLLMåˆæˆç›‘ç£è®­ç»ƒæ•°æ®ã€‚</li>
<li>åˆ©ç”¨åˆæˆæ•°æ®è®­ç»ƒç´§å‡‘æ¨¡å‹ï¼ˆå¦‚BERTï¼‰ã€‚</li>
<li>æå‡ºçš„æ–°æ–¹æ³•JAPAGENåœ¨éœ€è¦æ­£å¼æ–‡æœ¬è¾“å…¥çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
<li>JAPAGENæ–¹æ³•ä¸ä¼ ç»Ÿçš„LLMæç¤ºç­–ç•¥ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>LLMåœ¨æ•°æ®åˆæˆä¸­çš„åº”ç”¨æœ‰åŠ©äºæå‡æ¨ç†æ•ˆç‡å¹¶é™ä½æ•°æ®é‡‡é›†æˆæœ¬ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºLLMåœ¨å…¶ä»–è¯­è¨€ä»»åŠ¡ä¸­çš„æ•°æ®ç”Ÿæˆèƒ½åŠ›æä¾›äº†æœ‰åŠ›è¯æ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dca2fe3ecbad4ca6b8c39225c64c25f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6606b337aa1178b8817db6e0a7e2b166.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7831717f83a924d829afa0b801a90a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-300da75883c08486d606f85f26526b03.jpg" align="middle">
</details>




<h2 id="Class-Balance-Matters-to-Active-Class-Incremental-Learning"><a href="#Class-Balance-Matters-to-Active-Class-Incremental-Learning" class="headerlink" title="Class Balance Matters to Active Class-Incremental Learning"></a>Class Balance Matters to Active Class-Incremental Learning</h2><p><strong>Authors:Zitong Huang, Ze Chen, Yuanze Li, Bowen Dong, Erjin Zhou, Yong Liu, Rick Siow Mong Goh, Chun-Mei Feng, Wangmeng Zuo</strong></p>
<p>Few-Shot Class-Incremental Learning has shown remarkable efficacy in efficient learning new concepts with limited annotations. Nevertheless, the heuristic few-shot annotations may not always cover the most informative samples, which largely restricts the capability of incremental learner. We aim to start from a pool of large-scale unlabeled data and then annotate the most informative samples for incremental learning. Based on this premise, this paper introduces the Active Class-Incremental Learning (ACIL). The objective of ACIL is to select the most informative samples from the unlabeled pool to effectively train an incremental learner, aiming to maximize the performance of the resulting model. Note that vanilla active learning algorithms suffer from class-imbalanced distribution among annotated samples, which restricts the ability of incremental learning. To achieve both class balance and informativeness in chosen samples, we propose Class-Balanced Selection (CBS) strategy. Specifically, we first cluster the features of all unlabeled images into multiple groups. Then for each cluster, we employ greedy selection strategy to ensure that the Gaussian distribution of the sampled features closely matches the Gaussian distribution of all unlabeled features within the cluster. Our CBS can be plugged and played into those CIL methods which are based on pretrained models with prompts tunning technique. Extensive experiments under ACIL protocol across five diverse datasets demonstrate that CBS outperforms both random selection and other SOTA active learning approaches. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/1170300714/CBS">https://github.com/1170300714/CBS</a>. </p>
<blockquote>
<p>å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ åœ¨æœ‰é™æ ‡æ³¨ä¸‹é«˜æ•ˆå­¦ä¹ æ–°æ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚ç„¶è€Œï¼Œå¯å‘å¼å°‘é‡æ ‡æ³¨å¹¶ä¸æ€»èƒ½æ¶µç›–æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å¢é‡å­¦ä¹ çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»å¤§é‡æœªæ ‡æ³¨æ•°æ®ä¸­å¼€å§‹ï¼Œç„¶åä¸ºå¢é‡å­¦ä¹ æ ‡æ³¨æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ã€‚åŸºäºè¿™ä¸€å‰æï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸»åŠ¨ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆACILï¼‰ã€‚ACILçš„ç›®æ ‡æ˜¯ä»æœªæ ‡æ³¨çš„æ± ä¸­é€‰å–æœ€æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œä»¥æœ‰æ•ˆåœ°è®­ç»ƒå¢é‡å­¦ä¹ å™¨ï¼Œæ—¨åœ¨æœ€å¤§åŒ–æ‰€å¾—æ¨¡å‹çš„è¡¨ç°ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒåŸå§‹ä¸»åŠ¨å­¦ä¹ æ–¹æ³•åœ¨æ ‡æ³¨æ ·æœ¬ä¸­å­˜åœ¨ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡çš„é—®é¢˜ï¼Œè¿™é™åˆ¶äº†å¢é‡å­¦ä¹ çš„èƒ½åŠ›ã€‚ä¸ºäº†å®ç°æ‰€é€‰æ ·æœ¬çš„ç±»åˆ«å¹³è¡¡å’Œä¿¡æ¯ä¸°å¯Œæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç±»åˆ«å¹³è¡¡é€‰æ‹©ï¼ˆCBSï¼‰ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ‰€æœ‰æœªæ ‡æ³¨å›¾åƒçš„ç‰¹å¾èšæˆå¤šä¸ªç»„ã€‚ç„¶åå¯¹äºæ¯ä¸ªç»„ï¼Œæˆ‘ä»¬é‡‡ç”¨è´ªå¿ƒé€‰æ‹©ç­–ç•¥ï¼Œä»¥ç¡®ä¿é‡‡æ ·ç‰¹å¾çš„é«˜æ–¯åˆ†å¸ƒä¸èšç±»å†…æ‰€æœ‰æœªæ ‡æ³¨ç‰¹å¾çš„é«˜æ–¯åˆ†å¸ƒç´§å¯†åŒ¹é…ã€‚æˆ‘ä»¬çš„CBSå¯ä»¥æ’å…¥åˆ°åŸºäºé¢„è®­ç»ƒæ¨¡å‹å¹¶å¸¦æœ‰æç¤ºè°ƒæ•´æŠ€æœ¯çš„é‚£äº›CILæ–¹æ³•ä¸­ã€‚åœ¨ACILåè®®ä¸‹çš„äº”ä¸ªä¸åŒæ•°æ®é›†çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCBSåœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†éšæœºé€‰æ‹©å’Œå…¶ä»–å…ˆè¿›çš„ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/1170300714/CBS%E3%80%82">https://github.com/1170300714/CBSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06642v1">PDF</a> ACM MM 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®çš„ä¸»åŠ¨ç±»å¢é‡å­¦ä¹ ï¼ˆACILï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ—¨åœ¨ä»æœªæ ‡è®°çš„æ•°æ®æ± ä¸­é€‰å–æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œä»¥è®­ç»ƒå¢é‡å­¦ä¹ å™¨ï¼Œæ—¨åœ¨æœ€å¤§åŒ–æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºè§£å†³æ™®é€šä¸»åŠ¨å­¦ä¹ ä¸­æ ‡æ³¨æ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡é—®é¢˜ï¼Œæå‡ºäº†ç±»å¹³è¡¡é€‰æ‹©ï¼ˆCBSï¼‰ç­–ç•¥ã€‚CBSé€šè¿‡å¯¹æœªæ ‡è®°å›¾åƒçš„ç‰¹å¾è¿›è¡Œèšç±»ï¼Œå¹¶åœ¨æ¯ä¸ªèšç±»ä¸­é‡‡ç”¨è´ªå©ªé€‰æ‹©ç­–ç•¥ï¼Œç¡®ä¿é‡‡æ ·ç‰¹å¾çš„é«˜æ–¯åˆ†å¸ƒä¸èšç±»å†…æ‰€æœ‰æœªæ ‡è®°ç‰¹å¾çš„é«˜æ–¯åˆ†å¸ƒç´§å¯†åŒ¹é…ã€‚åœ¨äº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„ACILåè®®ä¸‹çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCBSåœ¨éšæœºé€‰æ‹©å’Œå…¶ä»–å…ˆè¿›çš„ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Class-Incremental Learning (FSCIL) åœ¨æœ‰é™æ ‡æ³¨ä¸‹å­¦ä¹ æ–°æ¦‚å¿µå…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
<li>å¯å‘å¼å°‘é‡æ ‡æ³¨å¯èƒ½æ— æ³•è¦†ç›–æœ€å…·æœ‰ä¿¡æ¯é‡çš„æ ·æœ¬ï¼Œé™åˆ¶äº†å¢é‡å­¦ä¹ çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºåŸºäºå¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®çš„Active Class-Incremental Learning (ACIL)ï¼Œæ—¨åœ¨é€‰å–æœ€å…·ä¿¡æ¯é‡çš„æ ·æœ¬ä»¥è®­ç»ƒå¢é‡å­¦ä¹ å™¨ã€‚</li>
<li>æ™®é€šä¸»åŠ¨å­¦ä¹ æ–¹æ³•å­˜åœ¨æ ‡æ³¨æ ·æœ¬çš„ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥Class-Balanced Selection (CBS) ç­–ç•¥ï¼Œé€šè¿‡ç‰¹å¾èšç±»å¹¶é‡‡ç”¨è´ªå©ªé€‰æ‹©ç­–ç•¥æ¥ç¡®ä¿ç±»åˆ«å¹³è¡¡å’Œæ ·æœ¬ä¿¡æ¯æ€§ã€‚</li>
<li>CBSå¯æ’å…¥åŸºäºé¢„è®­ç»ƒæ¨¡å‹å’Œæç¤ºè°ƒæ•´æŠ€æœ¯çš„CILæ–¹æ³•ä¸­ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9044a270fbb076a1049a4195f7c4e587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d603d4f6d0e74f98de3441e1803729fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57e0c1cbb5a6f982506bdc8183513379.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a77bb80fe9c7929244efe08768dd4d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b2f1a0c8c72eb4028b658ea88925962.jpg" align="middle">
</details>




<h2 id="Integrating-Expert-Labels-into-LLM-based-Emission-Goal-Detection-Example-Selection-vs-Automatic-Prompt-Design"><a href="#Integrating-Expert-Labels-into-LLM-based-Emission-Goal-Detection-Example-Selection-vs-Automatic-Prompt-Design" class="headerlink" title="Integrating Expert Labels into LLM-based Emission Goal Detection:   Example Selection vs Automatic Prompt Design"></a>Integrating Expert Labels into LLM-based Emission Goal Detection:   Example Selection vs Automatic Prompt Design</h2><p><strong>Authors:Marco Wrzalik, Adrian Ulges, Anne Uersfeld, Florian Faust</strong></p>
<p>We address the detection of emission reduction goals in corporate reports, an important task for monitoring companiesâ€™ progress in addressing climate change. Specifically, we focus on the issue of integrating expert feedback in the form of labeled example passages into LLM-based pipelines, and compare the two strategies of (1) a dynamic selection of few-shot examples and (2) the automatic optimization of the prompt by the LLM itself. Our findings on a public dataset of 769 climate-related passages from real-world business reports indicate that automatic prompt optimization is the superior approach, while combining both methods provides only limited benefit. Qualitative results indicate that optimized prompts do indeed capture many intricacies of the targeted emission goal extraction task. </p>
<blockquote>
<p>æˆ‘ä»¬å…³æ³¨ä¼ä¸šæŠ¥å‘Šä¸­æ’æ”¾å‡å°‘ç›®æ ‡æ£€æµ‹çš„é—®é¢˜ï¼Œè¿™æ˜¯ç›‘æµ‹å…¬å¸åº”å¯¹æ°”å€™å˜åŒ–è¿›å±•çš„é‡è¦ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå°†ä¸“å®¶åé¦ˆä»¥æ ‡æ³¨æ®µè½çš„å½¢å¼æ•´åˆåˆ°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç®¡é“ä¸­çš„é—®é¢˜ï¼Œå¹¶æ¯”è¾ƒï¼ˆ1ï¼‰åŠ¨æ€é€‰æ‹©å°‘é‡æ ·æœ¬å’Œï¼ˆ2ï¼‰ç”±å¤§å‹è¯­è¨€æ¨¡å‹æœ¬èº«è‡ªåŠ¨ä¼˜åŒ–æç¤ºè¿™ä¸¤ç§ç­–ç•¥ã€‚æˆ‘ä»¬åœ¨åŒ…å«ç°å®ä¸–ç•Œå•†ä¸šæŠ¥å‘Šä¸­ä¸æ°”å€™ç›¸å…³çš„769ä¸ªæ®µè½çš„å…¬å¼€æ•°æ®é›†ä¸Šçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯æ›´ä¼˜è¶Šçš„æ–¹æ³•ï¼Œè€Œç»“åˆè¿™ä¸¤ç§æ–¹æ³•çš„å¥½å¤„æœ‰é™ã€‚å®šæ€§ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–çš„æç¤ºç¡®å®èƒ½å¤Ÿæ•æ‰åˆ°æœ‰é’ˆå¯¹æ€§çš„æ’æ”¾ç›®æ ‡æå–ä»»åŠ¡çš„è®¸å¤šç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06432v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨ä¼ä¸šæŠ¥å‘Šä¸­æ’æ”¾å‡å°‘ç›®æ ‡æ£€æµ‹è¿™ä¸€ä»»åŠ¡ï¼Œæ—¨åœ¨ç›‘æµ‹å…¬å¸åœ¨åº”å¯¹æ°”å€™å˜åŒ–æ–¹é¢çš„è¿›å±•ã€‚ç ”ç©¶é›†ä¸­åœ¨å¦‚ä½•å°†ä¸“å®¶åé¦ˆä»¥æ ‡æ³¨æ ·æœ¬æ®µè½çš„å½¢å¼èå…¥LLMç®¡é“ï¼Œå¹¶æ¯”è¾ƒäº†ä¸¤ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰åŠ¨æ€é€‰æ‹©å°‘é‡æ ·æœ¬å’Œï¼ˆ2ï¼‰LLMè‡ªåŠ¨ä¼˜åŒ–æç¤ºã€‚åœ¨åŒ…å«769ç¯‡ä¸æ°”å€™ç›¸å…³çš„ä¼ä¸šæŠ¥å‘Šå…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯æ›´ä¼˜ç­–ç•¥ï¼Œè€Œç»“åˆä¸¤ç§æ–¹æ³•åªå¸¦æ¥æœ‰é™æ•ˆç›Šã€‚ä¼˜åŒ–æç¤ºèƒ½å¤Ÿå‡†ç¡®æ•æ‰æ’æ”¾ç›®æ ‡æå–ä»»åŠ¡çš„ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ’æ”¾å‡å°‘ç›®æ ‡æ£€æµ‹æ˜¯ç›‘æµ‹å…¬å¸åº”å¯¹æ°”å€™å˜åŒ–è¿›å±•çš„é‡è¦ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶èšç„¦äºå°†ä¸“å®¶åé¦ˆèå…¥LLMç®¡é“çš„æ–¹æ³•ã€‚</li>
<li>æ¯”è¾ƒäº†åŠ¨æ€é€‰æ‹©å°‘é‡æ ·æœ¬å’ŒLLMè‡ªåŠ¨ä¼˜åŒ–æç¤ºä¸¤ç§ç­–ç•¥ã€‚</li>
<li>è‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯æ›´ä¼˜ç­–ç•¥ï¼Œç»“åˆä¸¤ç§æ–¹æ³•æ•ˆç›Šæœ‰é™ã€‚</li>
<li>ä¼˜åŒ–æç¤ºèƒ½å¤Ÿå‡†ç¡®æ•æ‰æ’æ”¾ç›®æ ‡æå–ä»»åŠ¡çš„ç»†èŠ‚ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†åŒ…å«769ç¯‡ä¸æ°”å€™ç›¸å…³çš„ä¼ä¸šæŠ¥å‘Šå…¬å¼€æ•°æ®é›†è¿›è¡Œå®éªŒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1256a9d4a8ef68eee380970ca93b0405.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e02251f5d5339a8085451d81bcb62c17.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a755722e4384a3542fc3a7f635d4d6b3.jpg" align="middle">
</details>




<h2 id="SGIA-Enhancing-Fine-Grained-Visual-Classification-with-Sequence-Generative-Image-Augmentation"><a href="#SGIA-Enhancing-Fine-Grained-Visual-Classification-with-Sequence-Generative-Image-Augmentation" class="headerlink" title="SGIA: Enhancing Fine-Grained Visual Classification with Sequence   Generative Image Augmentation"></a>SGIA: Enhancing Fine-Grained Visual Classification with Sequence   Generative Image Augmentation</h2><p><strong>Authors:Qiyu Liao, Xin Yuan, Min Xu, Dadong Wang</strong></p>
<p>In Fine-Grained Visual Classification (FGVC), distinguishing highly similar subcategories remains a formidable challenge, often necessitating datasets with extensive variability. The acquisition and annotation of such FGVC datasets are notably difficult and costly, demanding specialized knowledge to identify subtle distinctions among closely related categories. Our study introduces a novel approach employing the Sequence Latent Diffusion Model (SLDM) for augmenting FGVC datasets, called Sequence Generative Image Augmentation (SGIA). Our method features a unique Bridging Transfer Learning (BTL) process, designed to minimize the domain gap between real and synthetically augmented data. This approach notably surpasses existing methods in generating more realistic image samples, providing a diverse range of pose transformations that extend beyond the traditional rigid transformations and style changes in generative augmentation. We demonstrate the effectiveness of our augmented dataset with substantial improvements in FGVC tasks on various datasets, models, and training strategies, especially in few-shot learning scenarios. Our method outperforms conventional image augmentation techniques in benchmark tests on three FGVC datasets, showcasing superior realism, variability, and representational quality. Our work sets a new benchmark and outperforms the previous state-of-the-art models in classification accuracy by 0.5% for the CUB-200-2011 dataset and advances the application of generative models in FGVC data augmentation. </p>
<blockquote>
<p>åœ¨ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­ï¼ŒåŒºåˆ†é«˜åº¦ç›¸ä¼¼çš„å­ç±»åˆ«ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦å…·æœ‰å¹¿æ³›å¯å˜æ€§çš„æ•°æ®é›†ã€‚æ­¤ç±»FGVCæ•°æ®é›†çš„è·å–å’Œæ³¨é‡Šæ˜¾ç„¶å›°éš¾ä¸”æˆæœ¬é«˜ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†æ¥è¯†åˆ«ç›¸å…³ç±»åˆ«ä¹‹é—´çš„ç»†å¾®åŒºåˆ«ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼•å…¥äº†ä¸€ç§é‡‡ç”¨åºåˆ—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆSLDMï¼‰å¯¹FGVCæ•°æ®é›†è¿›è¡Œå¢å¼ºæ–°æ–¹æ³•ï¼Œç§°ä¸ºåºåˆ—ç”Ÿæˆå›¾åƒå¢å¼ºï¼ˆSGIAï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ç‹¬ç‰¹çš„æ¡¥æ¥è¿ç§»å­¦ä¹ ï¼ˆBTLï¼‰è¿‡ç¨‹ï¼Œæ—¨åœ¨ç¼©å°çœŸå®å’Œåˆæˆå¢å¼ºæ•°æ®ä¹‹é—´çš„åŸŸå·®è·ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆæ›´é€¼çœŸçš„å›¾åƒæ ·æœ¬æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†å¤šç§å§¿åŠ¿å˜æ¢ï¼Œè¿™äº›å˜æ¢è¶…å‡ºäº†ä¼ ç»Ÿåˆšæ€§å˜æ¢å’Œç”Ÿæˆå¢å¼ºä¸­çš„é£æ ¼å˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡åœ¨å„ç§æ•°æ®é›†ã€æ¨¡å‹å’ŒåŸ¹è®­ç­–ç•¥ä¸Šçš„FGVCä»»åŠ¡ä¸­ï¼Œä½¿ç”¨å¢å¼ºçš„æ•°æ®é›†è¿›è¡Œæ¼”ç¤ºï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·å­¦ä¹ åœºæ™¯ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªFGVCæ•°æ®é›†çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å›¾åƒå¢å¼ºæŠ€æœ¯è¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†å“è¶Šçš„é€¼çœŸåº¦ã€å¯å˜æ€§å’Œä»£è¡¨æ€§è´¨é‡ã€‚æˆ‘ä»¬çš„å·¥ä½œæ ‘ç«‹äº†æ–°çš„åŸºå‡†çº¿ï¼Œå¹¶åœ¨CUB-200-2011æ•°æ®é›†ä¸Šæé«˜äº†åˆ†ç±»ç²¾åº¦ï¼Œç›¸è¾ƒäºä¹‹å‰çš„å…ˆè¿›æ¨¡å‹é«˜å‡º0.5%ï¼Œå¹¶æ¨åŠ¨äº†ç”Ÿæˆæ¨¡å‹åœ¨FGVCæ•°æ®å¢å¼ºä¸­çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06138v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­é«˜åº¦ç›¸ä¼¼å­ç±»åˆ«çš„åŒºåˆ†éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåºåˆ—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆSLDMï¼‰çš„æ–°å‹æ•°æ®é›†å¢å¼ºæ–¹æ³•â€”â€”åºåˆ—ç”Ÿæˆå›¾åƒå¢å¼ºï¼ˆSGIAï¼‰ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç‹¬ç‰¹çš„æ¡¥æ¥è¿ç§»å­¦ä¹ ï¼ˆBTLï¼‰è¿‡ç¨‹ï¼Œç¼©å°äº†çœŸå®å’Œåˆæˆå¢å¼ºæ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®è·ã€‚SGIAåœ¨ç”Ÿæˆæ›´é€¼çœŸçš„å›¾åƒæ ·æœ¬æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæä¾›äº†å¤šæ ·åŒ–çš„å§¿æ€è½¬æ¢ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿåˆšæ€§è½¬æ¢å’Œé£æ ¼å˜åŒ–çš„ç”Ÿæˆå¢å¼ºã€‚åœ¨å¤šä¸ªæ•°æ®é›†ã€æ¨¡å‹å’ŒåŸ¹è®­ç­–ç•¥ä¸Šï¼Œå°¤å…¶æ˜¯å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼Œæœ¬ç ”ç©¶å¢å¼ºäº†æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†FGVCä»»åŠ¡çš„æ•ˆæœã€‚åœ¨ä¸‰ä¸ªFGVCæ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ€§ã€å¯å˜æ€§å’Œä»£è¡¨æ€§æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿå›¾åƒå¢å¼ºæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹ç»†ç²’åº¦è§†è§‰åˆ†ç±»ï¼ˆFGVCï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œæå‡ºåºåˆ—ç”Ÿæˆå›¾åƒå¢å¼ºï¼ˆSGIAï¼‰æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨åºåˆ—æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆSLDMï¼‰è¿›è¡Œæ•°æ®é›†å¢å¼ºã€‚</li>
<li>å¼•å…¥æ¡¥æ¥è¿ç§»å­¦ä¹ ï¼ˆBTLï¼‰è¿‡ç¨‹ï¼Œç¼©å°çœŸå®ä¸åˆæˆæ•°æ®é—´çš„é¢†åŸŸå·®è·ã€‚</li>
<li>SGIAç”Ÿæˆæ›´é€¼çœŸçš„å›¾åƒæ ·æœ¬ï¼Œæä¾›å¤šæ ·åŒ–çš„å§¿æ€è½¬æ¢ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ã€æ¨¡å‹å’ŒåŸ¹è®­ç­–ç•¥ä¸Šï¼Œå°¤å…¶æ˜¯å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒSGIAæé«˜äº†FGVCä»»åŠ¡çš„æ•ˆæœã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ˜¾ç¤ºSGIAåœ¨çœŸå®æ€§ã€å¯å˜æ€§å’Œä»£è¡¨æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿå›¾åƒå¢å¼ºæŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶è®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå¹¶åœ¨CUB-200-2011æ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡æ¯”å…ˆå‰æœ€å…ˆè¿›çš„æ¨¡å‹é«˜å‡º0.5%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c84a7b71c8446d5b5016121e009eb13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43835d388317388edf26c253dcb74dfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5133b71dad6a09464f759d6e89d16a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4b6f49d20deead5b58d1a98bf675b51.jpg" align="middle">
</details>




<h2 id="Evaluating-and-Mitigating-Social-Bias-for-Large-Language-Models-in-Open-ended-Settings"><a href="#Evaluating-and-Mitigating-Social-Bias-for-Large-Language-Models-in-Open-ended-Settings" class="headerlink" title="Evaluating and Mitigating Social Bias for Large Language Models in   Open-ended Settings"></a>Evaluating and Mitigating Social Bias for Large Language Models in   Open-ended Settings</h2><p><strong>Authors:Zhao Liu</strong></p>
<p>Current social bias benchmarks for Large Language Models (LLMs) primarily rely on pre-defined question formats like multiple-choice, limiting their ability to reflect the complexity and open-ended nature of real-world interactions. To address this gap, we extend an existing BBQ dataset introduced by incorporating fill-in-the-blank and short-answer question types, designed to evaluate biases in an open-ended setting. Our finding reveals that LLMs tend to produce responses that are more biased against certain protected attributes, like age and socio-economic status. On the other hand, these biased outputs produced by LLMs can serve as valuable contexts and chains of thought for debiasing. Our debiasing approach combined zero-shot, few-shot, and chain-of-thought could significantly reduce the level of bias to almost 0. We open-source our evaluation and debiasing code hoping to encourage further measurements and mitigation of bias and stereotype in LLMs. </p>
<blockquote>
<p>ç›®å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¤¾ä¼šåè§åŸºå‡†ä¸»è¦ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„é—®é¢˜æ ¼å¼ï¼Œå¦‚é€‰æ‹©é¢˜ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åæ˜ ç°å®ä¸–ç•Œäº’åŠ¨çš„å¤æ‚æ€§å’Œå¼€æ”¾æ€§çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å¡«ç©ºå’Œç®€ç­”é—®é¢˜ç±»å‹æ‰©å±•äº†ç°æœ‰çš„BBQæ•°æ®é›†ï¼Œæ—¨åœ¨åœ¨å¼€æ”¾ç¯å¢ƒä¸­è¯„ä¼°åè§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMäº§ç”Ÿçš„å›ç­”å¾€å¾€æ›´å€¾å‘äºå¯¹æŸäº›å—ä¿æŠ¤å±æ€§ï¼ˆå¦‚å¹´é¾„å’Œç¤¾ä¼šç»æµåœ°ä½ï¼‰å­˜åœ¨åè§ã€‚å¦ä¸€æ–¹é¢ï¼ŒLLMäº§ç”Ÿçš„è¿™äº›åè§è¾“å‡ºå¯ä»¥ä½œä¸ºæœ‰ä»·å€¼çš„ä¸Šä¸‹æ–‡å’Œæ€è€ƒé“¾æ¥è¿›è¡Œå»åè§å¤„ç†ã€‚æˆ‘ä»¬çš„å»åè§æ–¹æ³•ç»“åˆäº†é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€è€ƒé“¾ï¼Œå¯ä»¥æ˜¾è‘—é™ä½åè§æ°´å¹³è‡³å‡ ä¹ä¸ºé›¶ã€‚æˆ‘ä»¬å¼€æºæˆ‘ä»¬çš„è¯„ä¼°å’Œå»åè§ä»£ç ï¼Œå¸Œæœ›èƒ½è¿›ä¸€æ­¥é¼“åŠ±å¯¹LLMä¸­çš„åè§å’Œåˆ»æ¿å°è±¡è¿›è¡Œè¡¡é‡å’Œç¼“è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06134v1">PDF</a> 12 panges</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¤¾ä¼šåè§åŸºå‡†æµ‹è¯•çš„é—®é¢˜ï¼Œè¿™äº›æµ‹è¯•ä¸»è¦ä¾èµ–äºé¢„è®¾çš„é—®é¢˜æ ¼å¼ï¼ˆå¦‚é€‰æ‹©é¢˜ï¼‰ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œäº¤äº’çš„å¤æ‚æ€§å’Œå¼€æ”¾æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…å¯¹BBQæ•°æ®é›†è¿›è¡Œäº†æ‰©å±•ï¼ŒåŠ å…¥äº†å¡«ç©ºå’Œç®€ç­”é—®é¢˜ç±»å‹æ¥è¯„ä¼°å¼€æ”¾ç¯å¢ƒä¸­å­˜åœ¨çš„åè§ã€‚ç ”ç©¶å‘ç°LLMäº§ç”Ÿçš„å“åº”æ›´æ˜“å¯¹æŸäº›å—ä¿æŠ¤å±æ€§ï¼ˆå¦‚å¹´é¾„å’Œç¤¾ä¼šç»æµåœ°ä½ï¼‰äº§ç”Ÿåè§ã€‚ä½†å¦ä¸€æ–¹é¢ï¼ŒLLMäº§ç”Ÿçš„åè§è¾“å‡ºå¯ä»¥ä½œä¸ºå»åè§çš„ä¸Šä¸‹æ–‡å’Œæ€è€ƒé“¾çš„æœ‰ä»·å€¼æ¥æºã€‚ç»“åˆé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€è€ƒé“¾çš„å»åè§æ–¹æ³•ï¼Œèƒ½æ˜¾è‘—é™ä½åè§æ°´å¹³è‡³æ¥è¿‘é›¶ã€‚åŒæ—¶å…¬å¼€è¯„ä¼°å»åè§ä»£ç çš„ç›®çš„æ˜¯é¼“åŠ±å¯¹LLMä¸­çš„åè§å’Œåˆ»æ¿å°è±¡è¿›è¡Œè¿›ä¸€æ­¥æµ‹é‡å’Œç¼“è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¤¾ä¼šåè§åŸºå‡†æµ‹è¯•å—é™äºé¢„è®¾é—®é¢˜æ ¼å¼ï¼Œæ— æ³•åæ˜ çœŸå®ä¸–ç•Œäº¤äº’çš„å¤æ‚æ€§ã€‚</li>
<li>é€šè¿‡æ‰©å±•BBQæ•°æ®é›†ï¼ŒåŠ å…¥äº†å¡«ç©ºå’Œç®€ç­”é—®é¢˜ç±»å‹æ¥è¯„ä¼°å¼€æ”¾ç¯å¢ƒä¸­çš„åè§ã€‚</li>
<li>LLMäº§ç”Ÿçš„å“åº”æ›´æ˜“å¯¹æŸäº›å—ä¿æŠ¤å±æ€§ï¼ˆå¦‚å¹´é¾„å’Œç¤¾ä¼šç»æµåœ°ä½ï¼‰äº§ç”Ÿåè§ã€‚</li>
<li>LLMäº§ç”Ÿçš„åè§è¾“å‡ºå¯ä½œä¸ºå»åè§çš„ä¸Šä¸‹æ–‡å’Œæ€è€ƒé“¾çš„æœ‰ä»·å€¼æ¥æºã€‚</li>
<li>ç»“åˆé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ€è€ƒé“¾çš„å»åè§æ–¹æ³•èƒ½æ˜¾è‘—é™ä½åè§æ°´å¹³ã€‚</li>
<li>ä½œè€…å…¬å¼€äº†è¯„ä¼°å»åè§ä»£ç ï¼Œä»¥é¼“åŠ±å¯¹LLMä¸­çš„åè§å’Œåˆ»æ¿å°è±¡è¿›è¡Œè¿›ä¸€æ­¥æµ‹é‡å’Œç¼“è§£ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-da4de40d9ab43ad9b4e40bfe8e546944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f2ba0168dc84048a2c3c3f2495c0041.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf426ea3ad3d6bdd6d9f21788b82560b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e7d7a48a2a56b3dd9dfcd2b6dd757c4.jpg" align="middle">
</details>




<h2 id="Are-foundation-models-for-computer-vision-good-conformal-predictors"><a href="#Are-foundation-models-for-computer-vision-good-conformal-predictors" class="headerlink" title="Are foundation models for computer vision good conformal predictors?"></a>Are foundation models for computer vision good conformal predictors?</h2><p><strong>Authors:Leo Fillioux, Julio Silva-RodrÃ­guez, Ismail Ben Ayed, Paul-Henry CournÃ¨de, Maria Vakalopoulou, Stergios Christodoulidis, Jose Dolz</strong></p>
<p>Recent advances in self-supervision and constrastive learning have brought the performance of foundation models to unprecedented levels in a variety of tasks. Fueled by this progress, these models are becoming the prevailing approach for a wide array of real-world vision problems, including risk-sensitive and high-stakes applications. However, ensuring safe deployment in these scenarios requires a more comprehensive understanding of their uncertainty modeling capabilities, which has been barely explored. In this work, we delve into the behavior of vision and vision-language foundation models under Conformal Prediction (CP), a statistical framework that provides theoretical guarantees of marginal coverage of the true class. Across extensive experiments including popular vision classification benchmarks, well-known foundation vision models, and three CP methods, our findings reveal that foundation models are well-suited for conformalization procedures, particularly those integrating Vision Transformers. Furthermore, we show that calibrating the confidence predictions of these models leads to efficiency degradation of the conformal set on adaptive CP methods. In contrast, few-shot adaptation to downstream tasks generally enhances conformal scores, where we identify Adapters as a better conformable alternative compared to Prompt Learning strategies. Our empirical study identifies APS as particularly promising in the context of vision foundation models, as it does not violate the marginal coverage property across multiple challenging, yet realistic scenarios. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè‡ªç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ çš„è¿›æ­¥æ¨åŠ¨äº†åŸºç¡€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ€§èƒ½è¾¾åˆ°äº†å‰æ‰€æœªæœ‰çš„æ°´å¹³ã€‚å—è¿™ä¸€è¿›å±•çš„æ¨åŠ¨ï¼Œè¿™äº›æ¨¡å‹æ­£æˆä¸ºåŒ…æ‹¬é£é™©æ•æ„Ÿå’Œé«˜é£é™©åº”ç”¨åœ¨å†…çš„å¹¿æ³›ç°å®ä¸–ç•Œè§†è§‰é—®é¢˜çš„ä¸»æµè§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¦åœ¨è¿™äº›åœºæ™¯ä¸­ç¡®ä¿å…¶å®‰å…¨éƒ¨ç½²ï¼Œéœ€è¦æ›´å…¨é¢åœ°äº†è§£å®ƒä»¬çš„ä¸ç¡®å®šæ€§å»ºæ¨¡èƒ½åŠ›ï¼Œè¿™ä¸€é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†è§†è§‰å’Œè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨ç¬¦åˆé¢„æµ‹ï¼ˆCPï¼‰ä¸‹çš„è¡Œä¸ºï¼Œè¿™æ˜¯ä¸€ä¸ªæä¾›çœŸå®ç±»åˆ«è¾¹ç¼˜è¦†ç›–ç†è®ºä¿è¯çš„ç»Ÿè®¡æ¡†æ¶ã€‚é€šè¿‡åŒ…æ‹¬æµè¡Œçš„è§†è§‰åˆ†ç±»åŸºå‡†æµ‹è¯•ã€çŸ¥åçš„åŸºç¡€è§†è§‰æ¨¡å‹å’Œä¸‰ç§CPæ–¹æ³•çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°åŸºç¡€æ¨¡å‹éå¸¸é€‚åˆäºç¬¦åˆåŒ–ç¨‹åºï¼Œç‰¹åˆ«æ˜¯é›†æˆäº†è§†è§‰å˜å‹å™¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæ ¡å‡†è¿™äº›æ¨¡å‹çš„ç½®ä¿¡é¢„æµ‹ä¼šå¯¼è‡´è‡ªé€‚åº”CPæ–¹æ³•ä¸Šçš„ç¬¦åˆé›†æ•ˆç‡é™ä½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„å°‘é‡æ ·æœ¬é€‚åº”é€šå¸¸èƒ½æé«˜ç¬¦åˆåˆ†æ•°ï¼Œæˆ‘ä»¬å‘ç°é€‚é…å™¨æ˜¯ç›¸æ¯”æç¤ºå­¦ä¹ ç­–ç•¥çš„æ›´å¥½ç¬¦åˆé€‰æ‹©ã€‚æˆ‘ä»¬çš„å®è¯ç ”ç©¶å‘ç°åœ¨è§†è§‰åŸºç¡€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ï¼ŒAPSç‰¹åˆ«å…·æœ‰å‰æ™¯ï¼Œå› ä¸ºå®ƒä¸ä¼šè¿åå¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä½†ç°å®çš„åœºæ™¯çš„è¾¹ç¼˜è¦†ç›–å±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06082v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè‡ªç›‘ç£ä¸å¯¹æ¯”å­¦ä¹ æŠ€æœ¯çš„è¿›å±•æå¤§åœ°æå‡äº†åŸºç¡€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹åœ¨å¤„ç†é£é™©æ•æ„Ÿå’Œé«˜é£é™©åº”ç”¨ç­‰çœŸå®ä¸–ç•Œè§†è§‰é—®é¢˜ä¸Šè¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­å®‰å…¨éƒ¨ç½²æ¨¡å‹éœ€è¦æ·±å…¥äº†è§£å…¶ä¸ç¡®å®šæ€§å»ºæ¨¡èƒ½åŠ›ï¼Œè¿™æ–¹é¢çš„ç ”ç©¶å´è¾ƒä¸ºç¼ºä¹ã€‚æœ¬æ–‡æ¢ç©¶äº†åœ¨Conformal Predictionï¼ˆCPï¼‰æ¡†æ¶ä¸‹ï¼Œè§†è§‰ä¸è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„è¡Œä¸ºè¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹éå¸¸é€‚åˆè¿›è¡Œå½¢å¼åŒ–è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯æ•´åˆäº†Vision Transformersçš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ ¡å‡†è¿™äº›æ¨¡å‹çš„ç½®ä¿¡é¢„æµ‹è™½ç„¶æœ‰åŠ©äºæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä½†åœ¨è‡ªé€‚åº”CPæ–¹æ³•ä¸­ä¼šç•¥å¾®é™ä½å½¢å¼åŒ–é›†åˆçš„æ•ˆç‡ã€‚ç›¸åï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­é€šè¿‡å°‘é‡æ•°æ®è¿›è¡Œé€‚åº”æ€§è°ƒæ•´é€šå¸¸èƒ½æé«˜å½¢å¼åŒ–å¾—åˆ†ï¼Œå…¶ä¸­Adaptersä½œä¸ºä¸€ç§æ›´å¥½çš„å¯é€‚åº”æ›¿ä»£æ–¹æ¡ˆå—åˆ°é‡è§†ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæœ¬æ–‡å‘ç°APSåœ¨è§†è§‰åŸºç¡€æ¨¡å‹çš„æƒ…å¢ƒä¸­ç‰¹åˆ«å…·æœ‰å‰æ™¯ï¼Œå®ƒåœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®åœºæ™¯ä¸­ä¸ä¼šè¿åè¾¹é™…è¦†ç›–å±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè‡ªç›‘ç£ä¸å¯¹æ¯”å­¦ä¹ æŠ€æœ¯çš„è¿›å±•ä¿ƒè¿›äº†åŸºç¡€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­çš„æ€§èƒ½æå‡ã€‚</li>
<li>åŸºç¡€æ¨¡å‹åœ¨å¤„ç†é£é™©æ•æ„Ÿå’Œé«˜é£é™©åº”ç”¨ç­‰çœŸå®ä¸–ç•Œè§†è§‰é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨Conformal Predictionæ¡†æ¶ä¸‹æ¢ç©¶äº†è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸ç¡®å®šæ€§å»ºæ¨¡è¡Œä¸ºã€‚</li>
<li>åŸºç¡€æ¨¡å‹é€‚åˆè¿›è¡Œå½¢å¼åŒ–è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯æ•´åˆäº†Vision Transformersçš„æ¨¡å‹ã€‚</li>
<li>æ ¡å‡†æ¨¡å‹çš„ç½®ä¿¡é¢„æµ‹èƒ½æé«˜å‡†ç¡®æ€§ä½†å¯èƒ½é™ä½å½¢å¼åŒ–é›†åˆçš„æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å°‘é‡æ•°æ®å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§è°ƒæ•´èƒ½æé«˜å½¢å¼åŒ–å¾—åˆ†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f9fb6138ba02f50ac3653b677f303b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bde1f7ff49a142a383f283442d8fdd3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96f5c2a13be510657a943399133b3609.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e1b70e504b5ecec40d87f0889d26494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-750b0958dccd2fca4ef2de3d27ca5850.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2889a1582afb02ec119741e13336a85.jpg" align="middle">
</details>




<h2 id="PromptRefine-Enhancing-Few-Shot-Performance-on-Low-Resource-Indic-Languages-with-Example-Selection-from-Related-Example-Banks"><a href="#PromptRefine-Enhancing-Few-Shot-Performance-on-Low-Resource-Indic-Languages-with-Example-Selection-from-Related-Example-Banks" class="headerlink" title="PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks"></a>PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic   Languages with Example Selection from Related Example Banks</h2><p><strong>Authors:Soumya Suvra Ghosal, Soumyabrata Pal, Koyel Mukherjee, Dinesh Manocha</strong></p>
<p>Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PromptRefine, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PromptRefine leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks â€“ Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms existing frameworks for retrieving examples. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒICLçš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå°‘æ ·æœ¬æ¼”ç¤ºçš„é€‰æ‹©ï¼Œè¿™ä½¿å¾—é€‰æ‹©æœ€ä¼˜è´¨çš„ä¾‹å­æˆä¸ºä¸€ä¸ªæŒç»­çš„ç ”ç©¶æŒ‘æˆ˜ã€‚åœ¨èµ„æºæœ‰é™çš„å°åº¦è¯­è¨€ä¸­ï¼Œè¿™ä¸ªé—®é¢˜è¿›ä¸€æ­¥æ”¾å¤§ï¼Œå› ä¸ºç¼ºä¹çœŸå®æ•°æ®çš„ç¨€ç¼ºæ€§ä½¿å¾—é€‰æ‹©è¿‡ç¨‹å¤æ‚åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PromptRefineï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¤ºä¾‹é€‰æ‹©çš„æ–°å‹äº¤æ›¿æœ€å°åŒ–æ–¹æ³•ï¼Œå¯æé«˜ä½èµ„æºå°åº¦è¯­è¨€ä¸Šçš„ICLæ€§èƒ½ã€‚PromptRefineåˆ©ç”¨æ¥è‡ªç›¸å…³çš„é«˜èµ„æºå°åº¦è¯­è¨€çš„è¾…åŠ©ç¤ºä¾‹åº“ï¼Œå¹¶é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯æ¥å¯¹é½è¯­è¨€ç‰¹å®šçš„æ£€ç´¢å™¨ï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„è·¨è¯­è¨€æ£€ç´¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨æ‰€é€‰çš„ç¤ºä¾‹ä¸­èå…¥äº†å¤šæ ·æ€§ï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›å’Œå‡å°‘åè§ã€‚é€šè¿‡å››é¡¹æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„å…¨é¢è¯„ä¼°â€”â€”è·¨è¯­è¨€é—®ç­”ã€å¤šè¯­è¨€é—®ç­”ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€æ‘˜è¦ï¼Œä½¿ç”¨æœ€å‰æ²¿çš„LLMå¦‚LLAMA-3.1-8Bã€LLAMA-2-7Bã€Qwen-2-7Bå’ŒQwen-2.5-7Bï¼Œæˆ‘ä»¬è¯æ˜äº†PromptRefineåœ¨æ£€ç´¢ç¤ºä¾‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05710v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒICLæ€§èƒ½é«˜åº¦ä¾èµ–äºæ‰€é€‰çš„å°‘é‡æ ·æœ¬æ¼”ç¤ºï¼Œé€‰æ‹©æœ€ä¼˜è´¨çš„ä¾‹å­æˆä¸ºæŒç»­çš„ç ”ç©¶æŒ‘æˆ˜ã€‚åœ¨èµ„æºæœ‰é™çš„å°åº¦è¯­è¨€ç¯å¢ƒä¸­ï¼Œè¿™ä¸€é—®é¢˜æ›´åŠ çªå‡ºã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPromptRefineçš„æ–°å‹äº¤æ›¿æœ€å°åŒ–æ–¹æ³•ï¼Œç”¨äºç¤ºä¾‹é€‰æ‹©ï¼Œå¯æé«˜ä½èµ„æºå°åº¦è¯­è¨€ç¯å¢ƒä¸‹çš„ICLæ€§èƒ½ã€‚PromptRefineåˆ©ç”¨æ¥è‡ªç›¸å…³çš„é«˜èµ„æºå°åº¦è¯­è¨€çš„è¾…åŠ©ç¤ºä¾‹åº“ï¼Œé‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯ï¼Œå®ç°å¯¹è¯­è¨€ç‰¹å®šæ£€ç´¢å™¨çš„å¯¹é½ï¼Œå®ç°æœ‰æ•ˆçš„è·¨è¯­è¨€æ£€ç´¢ã€‚æ­¤å¤–ï¼Œé€šè¿‡æé«˜æ‰€é€‰ç¤ºä¾‹çš„å¤šæ ·æ€§æ¥æé«˜æ³›åŒ–èƒ½åŠ›å¹¶å‡å°‘åè§ã€‚åœ¨å››ä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬è·¨è¯­è¨€é—®ç­”ã€å¤šè¯­è¨€é—®ç­”ã€æœºå™¨ç¿»è¯‘å’Œè·¨è¯­è¨€æ‘˜è¦ç­‰ä»»åŠ¡ï¼Œä½¿ç”¨æœ€å‰æ²¿çš„LLMå¦‚LLAMA-3.1-8Bã€LLAMA-2-7Bã€Qwen-2-7Bå’ŒQwen-2.5-7Bï¼Œè¡¨æ˜PromptRefineåœ¨æ£€ç´¢ç¤ºä¾‹æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œä½†ç¤ºä¾‹é€‰æ‹©æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>åœ¨èµ„æºæœ‰é™çš„å°åº¦è¯­è¨€ä¸­ï¼Œç¤ºä¾‹é€‰æ‹©çš„é‡è¦æ€§æ›´åŠ çªå‡ºã€‚</li>
<li>PromptRefineæ˜¯ä¸€ç§æ–°å‹äº¤æ›¿æœ€å°åŒ–æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›ä½èµ„æºå°åº¦è¯­è¨€ç¯å¢ƒä¸‹çš„ç¤ºä¾‹é€‰æ‹©ã€‚</li>
<li>PromptRefineåˆ©ç”¨è¾…åŠ©ç¤ºä¾‹åº“å’Œå¤šä»»åŠ¡å­¦ä¹ æŠ€æœ¯å®ç°è·¨è¯­è¨€æ£€ç´¢ã€‚</li>
<li>æé«˜æ‰€é€‰ç¤ºä¾‹çš„å¤šæ ·æ€§å¯ä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¹¶å‡å°‘åè§ã€‚</li>
<li>åœ¨å¤šä¸ªæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPromptRefineåœ¨æ£€ç´¢ç¤ºä¾‹æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cae67112da53149396c82bed051ced1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cfdcff6188f312ac93217331a41cf49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2820e8118b949b91ee7eb3e4dd0423b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9c17558b74d0b4e5e43df0555548b77.jpg" align="middle">
</details>




<h2 id="Diversity-Over-Quantity-A-Lesson-From-Few-Shot-Relation-Classification"><a href="#Diversity-Over-Quantity-A-Lesson-From-Few-Shot-Relation-Classification" class="headerlink" title="Diversity Over Quantity: A Lesson From Few Shot Relation Classification"></a>Diversity Over Quantity: A Lesson From Few Shot Relation Classification</h2><p><strong>Authors:Amir DN Cohen, Shauli Ravfogel, Shaltiel Shmidman, Yoav Goldberg</strong></p>
<p>In few-shot relation classification (FSRC), models must generalize to novel relations with only a few labeled examples. While much of the recent progress in NLP has focused on scaling data size, we argue that diversity in relation types is more crucial for FSRC performance. In this work, we demonstrate that training on a diverse set of relations significantly enhances a modelâ€™s ability to generalize to unseen relations, even when the overall dataset size remains fixed.   We introduce REBEL-FS, a new FSRC benchmark that incorporates an order of magnitude more relation types than existing datasets. Through systematic experiments, we show that increasing the diversity of relation types in the training data leads to consistent gains in performance across various few-shot learning scenarios, including high-negative settings. Our findings challenge the common assumption that more data alone leads to better performance and suggest that targeted data curation focused on diversity can substantially reduce the need for large-scale datasets in FSRC. </p>
<blockquote>
<p>åœ¨å°‘æ ·æœ¬å…³ç³»åˆ†ç±»ï¼ˆFSRCï¼‰ä¸­ï¼Œæ¨¡å‹å¿…é¡»åœ¨ä»…æœ‰å°‘é‡æ ‡æ³¨æ ·æœ¬çš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æ–°å‹å…³ç³»ã€‚è™½ç„¶è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æœ€æ–°è¿›å±•ä¸»è¦é›†ä¸­åœ¨æ‰©å¤§æ•°æ®è§„æ¨¡ä¸Šï¼Œä½†æˆ‘ä»¬ä¸»å¼ å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯¹äºFSRCæ€§èƒ½æ¥è¯´æ›´ä¸ºå…³é”®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å¤šæ ·åŒ–çš„å…³ç³»é›†ä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ¨å¹¿åˆ°æœªè§å…³ç³»çš„èƒ½åŠ›ï¼Œå³ä½¿åœ¨æ•´ä½“æ•°æ®é›†å¤§å°ä¿æŒä¸å˜çš„æƒ…å†µä¸‹äº¦æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬å¼•å…¥äº†REBEL-FSï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„FSRCåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…å«çš„å…³ç³»ç±»å‹æ¯”ç°æœ‰æ•°æ®é›†å¤šå‡ºä¸€ä¸ªæ•°é‡çº§ã€‚é€šè¿‡ç³»ç»Ÿå®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜å¢åŠ è®­ç»ƒæ•°æ®ä¸­å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯ä»¥åœ¨å„ç§å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­è·å¾—æ€§èƒ½ä¸Šçš„æŒç»­æå‡ï¼ŒåŒ…æ‹¬é«˜è´Ÿè®¾ç½®ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å•ä¸€åœ°è®¤ä¸ºæ›´å¤šæ•°æ®ä¼šå¸¦æ¥æ›´å¥½æ€§èƒ½çš„å¸¸è§å‡è®¾ï¼Œå¹¶å»ºè®®æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ”¶é›†åº”ä¾§é‡äºå¤šæ ·æ€§ï¼Œè¿™å¯ä»¥åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå‡å°‘FSRCä¸­å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05434v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å°å‹æ•°æ®é›†ä¸Šçš„å…³ç³»åˆ†ç±»ï¼ˆFSRCï¼‰ä¸­ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å°¤ä¸ºé‡è¦ã€‚ä½œè€…è®¤ä¸ºï¼Œå…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯¹äºFSRCæ€§èƒ½è‡³å…³é‡è¦ã€‚é€šè¿‡å¼•å…¥REBEL-FSæ–°åŸºå‡†æµ‹è¯•é›†ï¼Œè¯¥æµ‹è¯•é›†åŒ…å«çš„å…³ç³»ç±»å‹æ¯”ç°æœ‰æ•°æ®é›†å¤šå¾—å¤šï¼Œä½œè€…å±•ç¤ºäº†åœ¨è®­ç»ƒæ•°æ®ä¸Šå¢åŠ å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹å¯¹æœªè§å…³ç³»çš„æ³›åŒ–èƒ½åŠ›ï¼Œå³ä½¿åœ¨æ•°æ®é›†å¤§å°ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œä½œè€…é€šè¿‡å®éªŒè¡¨æ˜ï¼Œå¢åŠ å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯ä»¥åœ¨å„ç§å°å‹å­¦ä¹ åœºæ™¯ä¸­å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬é«˜è´Ÿä¾‹è®¾ç½®ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†æ›´å¤šæ•°æ®ä¼šè‡ªåŠ¨å¸¦æ¥æ›´å¥½æ€§èƒ½çš„å¸¸è§å‡è®¾ï¼Œå¹¶æš—ç¤ºæœ‰é’ˆå¯¹æ€§çš„æ•°æ®é›†ä¸­å…³æ³¨å¤šæ ·æ€§å¯ä»¥å¤§å¤§å‡å°‘FSRCä¸­å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å°å‹æ•°æ®é›†ä¸Šçš„å…³ç³»åˆ†ç±»ï¼ˆFSRCï¼‰ä¸­ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›éå¸¸é‡è¦ã€‚</li>
<li>å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯¹äºFSRCæ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥REBEL-FSæ–°åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«å¤§é‡å…³ç³»ç±»å‹ï¼Œæœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨è®­ç»ƒæ•°æ®ä¸Šå¢åŠ å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹å¯¹æœªè§å…³ç³»çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ‰é’ˆå¯¹æ€§çš„æ•°æ®é›†ä¸­å…³æ³¨å¤šæ ·æ€§å¯ä»¥å¤§å¤§å‡å°‘FSRCä¸­å¯¹å¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>å¢åŠ å…³ç³»ç±»å‹çš„å¤šæ ·æ€§å¯ä»¥åœ¨å„ç§å°å‹å­¦ä¹ åœºæ™¯ä¸­å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ec4c8b6ba6f96a230773bc89cb8983e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5585e2766a53cac03e301274c8b7a51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a31c0b828aacfdb4b874589ed0d8455a.jpg" align="middle">
</details>




<h2 id="Generative-Model-Based-Fusion-for-Improved-Few-Shot-Semantic-Segmentation-of-Infrared-Images"><a href="#Generative-Model-Based-Fusion-for-Improved-Few-Shot-Semantic-Segmentation-of-Infrared-Images" class="headerlink" title="Generative Model-Based Fusion for Improved Few-Shot Semantic   Segmentation of Infrared Images"></a>Generative Model-Based Fusion for Improved Few-Shot Semantic   Segmentation of Infrared Images</h2><p><strong>Authors:Junno Yun, Mehmet AkÃ§akaya</strong></p>
<p>Infrared (IR) imaging is commonly used in various scenarios, including autonomous driving, fire safety and defense applications. Thus, semantic segmentation of such images is of great interest. However, this task faces several challenges, including data scarcity, differing contrast and input channel number compared to natural images, and emergence of classes not represented in databases in certain scenarios, such as defense applications. Few-shot segmentation (FSS) provides a framework to overcome these issues by segmenting query images using a few labeled support samples. However, existing FSS models for IR images require paired visible RGB images, which is a major limitation since acquiring such paired data is difficult or impossible in some applications. In this work, we develop new strategies for FSS of IR images by using generative modeling and fusion techniques. To this end, we propose to synthesize auxiliary data to provide additional channel information to complement the limited contrast in the IR images, as well as IR data synthesis for data augmentation. Here, the former helps the FSS model to better capture the relationship between the support and query sets, while the latter addresses the issue of data scarcity. Finally, to further improve the former aspect, we propose a novel fusion ensemble module for integrating the two different modalities. Our methods are evaluated on different IR datasets, and improve upon the state-of-the-art (SOTA) FSS models. </p>
<blockquote>
<p>çº¢å¤–ï¼ˆIRï¼‰æˆåƒåœ¨è‡ªåŠ¨é©¾é©¶ã€æ¶ˆé˜²å®‰å…¨ä»¥åŠå›½é˜²åº”ç”¨ç­‰å¤šç§åœºæ™¯ä¸­éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼Œå¯¹è¿™ç±»å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²å…·æœ‰æå¤§çš„ç ”ç©¶ä»·å€¼ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä»»åŠ¡é¢ä¸´ç€è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ç¨€ç¼ºã€ä¸è‡ªç„¶å›¾åƒç›¸æ¯”å¯¹æ¯”åº¦ä¸åŒä»¥åŠè¾“å…¥é€šé“æ•°é‡ä¸åŒï¼Œä»¥åŠåœ¨æŸäº›åœºæ™¯ï¼ˆå¦‚å›½é˜²åº”ç”¨ï¼‰ä¸­æ•°æ®åº“ä¸­æœªå‡ºç°çš„æ–°å…´ç±»åˆ«ç­‰é—®é¢˜ã€‚å°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰é€šè¿‡åˆ©ç”¨å°‘é‡æ ‡è®°æ”¯æŒæ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œä¸ºå…‹æœè¿™äº›é—®é¢˜æä¾›äº†æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰ç”¨äºçº¢å¤–å›¾åƒçš„å°æ ·æœ¬åˆ†å‰²æ¨¡å‹éœ€è¦é…å¯¹å¯è§å…‰RGBå›¾åƒï¼Œè¿™åœ¨æŸäº›åº”ç”¨ä¸­è·å–æ­¤ç±»é…å¯¹æ•°æ®æ˜¯å›°éš¾æˆ–ä¸å¯èƒ½çš„ï¼Œè¿™æ˜¯ä¸€é¡¹ä¸»è¦é™åˆ¶ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘æ–°çš„ç­–ç•¥æ¥è§£å†³çº¢å¤–å›¾åƒçš„å°æ ·æœ¬åˆ†å‰²é—®é¢˜ï¼Œé‡‡ç”¨ç”Ÿæˆå»ºæ¨¡å’ŒèåˆæŠ€æœ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆæˆè¾…åŠ©æ•°æ®ä»¥æä¾›é¢å¤–çš„é€šé“ä¿¡æ¯æ¥è¡¥å……çº¢å¤–å›¾åƒä¸­æœ‰é™çš„å¯¹æ¯”åº¦ä¿¡æ¯ï¼Œä»¥åŠçº¢å¤–æ•°æ®åˆæˆç”¨äºæ•°æ®å¢å¼ºã€‚å…¶ä¸­ï¼Œå‰è€…æœ‰åŠ©äºå°æ ·æœ¬åˆ†å‰²æ¨¡å‹æ›´å¥½åœ°æ•æ‰æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†ä¹‹é—´çš„å…³ç³»ï¼Œè€Œåè€…è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥æå‡å‰è€…çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èåˆé›†æˆæ¨¡å—ï¼Œç”¨äºæ•´åˆä¸¤ç§ä¸åŒçš„æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„çº¢å¤–æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ”¹è¿›äº†æœ€æ–°çš„å°æ ·æœ¬åˆ†å‰²æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05341v1">PDF</a> Winter Conference on Applications of Computer Vision (WACV), 2025</p>
<p><strong>Summary</strong></p>
<p>çº¢å¤–å›¾åƒçš„è¯­ä¹‰åˆ†å‰²åœ¨è®¸å¤šåº”ç”¨åœºæ™¯ä¸­è‡³å…³é‡è¦ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ã€ç«ç¾ç›‘æ§å’Œå†›äº‹åº”ç”¨ç­‰ã€‚ç„¶è€Œï¼Œç”±äºå…¶ç‹¬æœ‰çš„ç‰¹æ€§å¦‚æ•°æ®ç¨€ç¼ºã€å¯¹æ¯”åº¦å’Œè¾“å…¥é€šé“æ•°é‡ä¸åŒç­‰ï¼Œä»¥åŠæŸäº›åœºæ™¯ä¸­æ–°å…´ç±»åˆ«åœ¨æ•°æ®åº“ä¸­æœªä½“ç°çš„é—®é¢˜ï¼Œä½¿å¾—åˆ†å‰²ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºæ–°çš„ç­–ç•¥æ¥è§£å†³çº¢å¤–å›¾åƒçš„å°‘æ ·æœ¬åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆå»ºæ¨¡å’ŒèåˆæŠ€æœ¯ï¼Œåˆæˆè¾…åŠ©æ•°æ®æä¾›é¢å¤–çš„é€šé“ä¿¡æ¯æ¥è¡¥å……çº¢å¤–å›¾åƒçš„æœ‰é™å¯¹æ¯”åº¦ï¼Œå¹¶ç”¨äºæ•°æ®å¢å¼ºã€‚åŒæ—¶ï¼Œèåˆé›†æˆæ¨¡å—è¢«æå‡ºç”¨äºæ•´åˆä¸¤ç§ä¸åŒçš„æ¨¡æ€ï¼Œä»¥è¿›ä¸€æ­¥æå‡åˆ†å‰²æ•ˆæœã€‚æ­¤æ–¹æ³•åœ¨ä¸åŒçº¢å¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å°‘æ ·æœ¬åˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¢å¤–å›¾åƒçš„è¯­ä¹‰åˆ†å‰²åœ¨è®¸å¤šé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼ŒåŒ…æ‹¬è‡ªåŠ¨é©¾é©¶ã€ç«ç¾ç›‘æ§å’Œå†›äº‹åº”ç”¨ç­‰ã€‚</li>
<li>çº¢å¤–å›¾åƒåˆ†å‰²é¢ä¸´æ•°æ®ç¨€ç¼ºã€å¯¹æ¯”åº¦å’Œè¾“å…¥é€šé“æ•°é‡ç­‰é—®é¢˜ã€‚</li>
<li>å°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æ¡†æ¶ï¼Œä½†ç°æœ‰æ¨¡å‹éœ€è¦é…å¯¹çš„æœ‰è‰²RGBå›¾åƒï¼Œè¿™åœ¨æŸäº›åº”ç”¨ä¸­éš¾ä»¥å®ç°ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ç”Ÿæˆå»ºæ¨¡å’ŒèåˆæŠ€æœ¯ï¼Œæå‡ºæ–°çš„ç­–ç•¥æ¥è§£å†³çº¢å¤–å›¾åƒçš„å°‘æ ·æœ¬åˆ†å‰²é—®é¢˜ã€‚</li>
<li>åˆæˆè¾…åŠ©æ•°æ®æä¾›é¢å¤–çš„é€šé“ä¿¡æ¯ï¼Œä»¥è¡¥å……çº¢å¤–å›¾åƒçš„æœ‰é™å¯¹æ¯”åº¦ï¼Œå¹¶ç”¨äºæ•°æ®å¢å¼ºã€‚</li>
<li>èåˆé›†æˆæ¨¡å—ç”¨äºæ•´åˆä¸¤ç§ä¸åŒæ¨¡æ€ï¼Œè¿›ä¸€æ­¥æå‡åˆ†å‰²æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-543124edf9146b29d564f5fefdb4255e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa8978e9aec1c61a797d502a7ea7938c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c51e2cdeb0d62a60af66d306140f328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41154820d49466739dbbe5eaf1be10c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-55b502614a5f4a44a6083d3950208a22.jpg" align="middle">
</details>




<h2 id="The-Prompt-Canvas-A-Literature-Based-Practitioner-Guide-for-Creating-Effective-Prompts-in-Large-Language-Models"><a href="#The-Prompt-Canvas-A-Literature-Based-Practitioner-Guide-for-Creating-Effective-Prompts-in-Large-Language-Models" class="headerlink" title="The Prompt Canvas: A Literature-Based Practitioner Guide for Creating   Effective Prompts in Large Language Models"></a>The Prompt Canvas: A Literature-Based Practitioner Guide for Creating   Effective Prompts in Large Language Models</h2><p><strong>Authors:Michael Hewing, Vincent Leinhos</strong></p>
<p>The rise of large language models (LLMs) has highlighted the importance of prompt engineering as a crucial technique for optimizing model outputs. While experimentation with various prompting methods, such as Few-shot, Chain-of-Thought, and role-based techniques, has yielded promising results, these advancements remain fragmented across academic papers, blog posts and anecdotal experimentation. The lack of a single, unified resource to consolidate the fieldâ€™s knowledge impedes the progress of both research and practical application. This paper argues for the creation of an overarching framework that synthesizes existing methodologies into a cohesive overview for practitioners. Using a design-based research approach, we present the Prompt Canvas, a structured framework resulting from an extensive literature review on prompt engineering that captures current knowledge and expertise. By combining the conceptual foundations and practical strategies identified in prompt engineering, the Prompt Canvas provides a practical approach for leveraging the potential of Large Language Models. It is primarily designed as a learning resource for pupils, students and employees, offering a structured introduction to prompt engineering. This work aims to contribute to the growing discourse on prompt engineering by establishing a unified methodology for researchers and providing guidance for practitioners. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ï¼Œæç¤ºå·¥ç¨‹ä½œä¸ºä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„å…³é”®æŠ€æœ¯ï¼Œå…¶é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚è™½ç„¶é€šè¿‡å°‘æ ·æœ¬ã€æ€ç»´é“¾å’ŒåŸºäºè§’è‰²çš„æŠ€æœ¯ç­‰æç¤ºæ–¹æ³•çš„å®éªŒå·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†è¿™äº›è¿›å±•ä»ç„¶åˆ†æ•£åœ¨å­¦æœ¯è®ºæ–‡ã€åšå®¢æ–‡ç« å’Œç»éªŒå®éªŒä¹‹ä¸­ã€‚ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€æ•´åˆè¯¥é¢†åŸŸçŸ¥è¯†çš„å•ä¸€èµ„æºï¼Œé˜»ç¢äº†ç ”ç©¶å’Œå®é™…åº”ç”¨çš„å‘å±•ã€‚æœ¬æ–‡ä¸»å¼ åˆ›å»ºä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œå°†ç°æœ‰æ–¹æ³•è¿›è¡Œç»¼åˆæ¦‚è¿°ï¼Œä¸ºä»ä¸šè€…æä¾›å®ç”¨çš„æŒ‡å—ã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºè®¾è®¡çš„ç ”ç©¶æ–¹æ³•ï¼Œæå‡ºäº†æç¤ºç”»å¸ƒï¼ˆPrompt Canvasï¼‰ï¼Œè¿™æ˜¯é€šè¿‡å¯¹æç¤ºå·¥ç¨‹è¿›è¡Œå¹¿æ³›çš„æ–‡çŒ®ç»¼è¿°è€Œå¾—å‡ºçš„ç»“æ„åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿæ•æ‰å½“å‰çš„çŸ¥è¯†å’Œä¸“ä¸šçŸ¥è¯†ã€‚æç¤ºç”»å¸ƒç»“åˆäº†æç¤ºå·¥ç¨‹ä¸­çš„æ¦‚å¿µåŸºç¡€å’Œå®ç”¨ç­–ç•¥ï¼Œä¸ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›æä¾›äº†ä¸€ç§å®ç”¨æ–¹æ³•ã€‚å®ƒä¸»è¦ä¸ºå­¦ä¹ è€…ã€å­¦ç”Ÿå’Œé›‡å‘˜è®¾è®¡ï¼Œæä¾›äº†ä¸€ä¸ªç»“æ„åŒ–ä»‹ç»æç¤ºå·¥ç¨‹çš„èµ„æºã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹æç¤ºå·¥ç¨‹å»ºç«‹ç»Ÿä¸€æ–¹æ³•è®ºï¼Œä¸ºç ”ç©¶äººå‘˜æä¾›æŒ‡å¯¼ï¼Œå¹¶ä¸ºä»ä¸šè€…åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05127v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·å‡¸æ˜¾äº†æç¤ºå·¥ç¨‹ä½œä¸ºä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„é‡è¦æŠ€æœ¯ã€‚æœ¬æ–‡é€šè¿‡è®¾è®¡ç ”ç©¶æ³•æå‡ºPrompt Canvasæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¹¿æ³›æ–‡çŒ®ç»¼è¿°æ•´åˆäº†ç°æœ‰çš„æç¤ºå·¥ç¨‹æ–¹æ³•å’Œç­–ç•¥ï¼Œä¸ºä»ä¸šè€…æä¾›äº†ä¸€ä¸ªç³»ç»Ÿçš„çŸ¥è¯†æ¦‚è§ˆã€‚å…¶ç›®çš„æ˜¯ä¸ºç ”ç©¶è€…å»ºç«‹ç»Ÿä¸€çš„æ–¹æ³•è®ºï¼Œä¸ºä»ä¸šè€…æä¾›æŒ‡å¯¼ï¼Œå¹¶ä½œä¸ºå­¦ä¹ èµ„æºä¾›å­¦ç”Ÿå’Œå…¶ä»–äººå‘˜å­¦ä¹ æç¤ºå·¥ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·çªå‡ºäº†æç¤ºå·¥ç¨‹åœ¨ä¼˜åŒ–æ¨¡å‹è¾“å‡ºä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç›®å‰å­˜åœ¨å¤šç§æç¤ºæ–¹æ³•ï¼Œå¦‚Few-shotã€Chain-of-Thoughtå’ŒåŸºäºè§’è‰²çš„æŠ€æœ¯ï¼Œå·²ç»äº§ç”Ÿäº†æœ‰å‰æ™¯çš„ç»“æœã€‚</li>
<li>å½“å‰ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„èµ„æºæ¥æ•´åˆè¯¥é¢†åŸŸçš„çŸ¥è¯†ï¼Œè¿™é˜»ç¢äº†ç ”ç©¶å’Œå®é™…åº”ç”¨çš„å‘å±•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Prompt Canvasæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ–‡çŒ®ç»¼è¿°çš„ç»“æ„åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ•´åˆç°æœ‰çš„æç¤ºå·¥ç¨‹æ–¹æ³•å’Œç­–ç•¥ã€‚</li>
<li>Prompt Canvasç»“åˆäº†æ¦‚å¿µåŸºç¡€å’Œå®ç”¨ç­–ç•¥ï¼Œä¸ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåŠ›æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸»è¦ä½œä¸ºé’ˆå¯¹å­¦å‘˜ã€å­¦ç”Ÿå’Œå‘˜å·¥çš„å­¦ä¹ èµ„æºï¼Œä¸ºä»–ä»¬æä¾›ç»“æ„åŒ–çš„æç¤ºå·¥ç¨‹ä»‹ç»ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47411c2d85d95fa9d795aad3d9c44654.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-365d8fc4f56ad7d626e96a7d8d872593.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5da221c9c777e94c75e5ed0b40997315.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d7f6e964697abaf2d8425d3d4319ebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc9abae61fcbeb267535a29812207c85.jpg" align="middle">
</details>




<h2 id="Steps-are-all-you-need-Rethinking-STEM-Education-with-Prompt-Engineering"><a href="#Steps-are-all-you-need-Rethinking-STEM-Education-with-Prompt-Engineering" class="headerlink" title="Steps are all you need: Rethinking STEM Education with Prompt   Engineering"></a>Steps are all you need: Rethinking STEM Education with Prompt   Engineering</h2><p><strong>Authors:Krishnasai Addala, Kabir Dev Paul Baghel, Chhavi Kirtani, Avinash Anand, Rajiv Ratn Shah</strong></p>
<p>Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination. By utilizing a Mixture of Experts (MoE) Model, along with analogical prompting, we are able to show improved model performance when compared to the baseline on standard LLMs. We also survey the limits of these prompting techniques and the effects they have on model performance. Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data. </p>
<blockquote>
<p>åœ¨ç‰©ç†é—®ç­”ä»»åŠ¡ä¸­åº”ç”¨å°‘é‡æç¤ºå’Œæ€ç»´é“¾æç¤ºå·²æ˜¾ç¤ºå‡ºå…¶æ½œåŠ›ï¼Œä½†ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹å›ºæœ‰çš„æ•°å­¦èƒ½åŠ›ç¼ºå¤±ï¼Œè¿™äº›æŠ€æœ¯ä»å—åˆ°ä¸€å®šé™åˆ¶ï¼Œå¹¶ä¸”å®¹æ˜“å‡ºç°å¹»è§‰ã€‚é€šè¿‡åˆ©ç”¨æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å’Œç±»æ¯”æç¤ºï¼Œä¸åŸºå‡†çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨æ ‡å‡†æ¨¡å‹ä¸Šå±•ç¤ºæ”¹è¿›åçš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è°ƒæŸ¥äº†è¿™äº›æç¤ºæŠ€æœ¯çš„å±€é™æ€§åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç±»æ¯”æ€ç»´é“¾æç¤ºï¼ˆAnalogical CoT promptingï¼‰è¿™ä¸€æç¤ºæŠ€æœ¯ï¼Œæ—¨åœ¨è®©è¾ƒå°çš„å¼€æºæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ç±»æ¯”æç¤ºï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¸€ç›´å›°æ‰°ç€å®ƒä»¬ï¼Œå¯èƒ½æ˜¯å› ä¸ºç¼ºä¹ä¸“ä¸šè®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05023v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å°‘æ ·æœ¬æŠ€æœ¯ä¸é“¾å¼æ€ç»´æç¤ºå¯¹äºç‰©ç†é—®ç­”ä»»åŠ¡æœ‰åº”ç”¨å‰æ™¯ï¼Œä½†ä»å—é™äºå¤§å‹è¯­è¨€æ¨¡å‹å†…åœ¨çš„æ•°å­¦èƒ½åŠ›ä¸æ˜“äºäº§ç”Ÿå¹»è§‰çš„é—®é¢˜ã€‚åˆ©ç”¨æ··åˆä¸“å®¶æ¨¡å‹ä¸ç±»æ¯”æç¤ºï¼Œå¯ä»¥æ”¹å–„æ¨¡å‹åœ¨æ ‡å‡†å¤§å‹è¯­è¨€æ¨¡å‹åŸºçº¿ä¹‹ä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†è¿™äº›æç¤ºæŠ€æœ¯çš„å±€é™æ€§åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†ç±»æ¯”æ€ç»´é“¾æç¤ºæŠ€æœ¯ï¼Œæ—¨åœ¨ä½¿å°å‹å¼€æºæ¨¡å‹ä¹Ÿèƒ½åˆ©ç”¨ç±»æ¯”æç¤ºï¼Œè¿™ä¹‹å‰ç”±äºç¼ºå°‘ä¸“ä¸šè®­ç»ƒæ•°æ®è€Œå—åˆ°é˜»ç¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬ä¸é“¾å¼æ€ç»´æç¤ºåœ¨ç‰©ç†é—®ç­”ä»»åŠ¡ä¸­æœ‰åº”ç”¨å‰æ™¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦èƒ½åŠ›ä¸Šä»æœ‰å±€é™ï¼Œæ˜“äº§ç”Ÿå¹»è§‰ã€‚</li>
<li>æ··åˆä¸“å®¶æ¨¡å‹ä¸ç±»æ¯”æç¤ºèƒ½æ”¹å–„æ¨¡å‹è¡¨ç°ã€‚</li>
<li>æç¤ºæŠ€æœ¯å­˜åœ¨å±€é™æ€§ï¼Œå¯¹æ¨¡å‹æ€§èƒ½æœ‰å½±å“ã€‚</li>
<li>é¦–æ¬¡æå‡ºç±»æ¯”æ€ç»´é“¾æç¤ºæŠ€æœ¯ã€‚</li>
<li>è¯¥æŠ€æœ¯ä½¿å°å‹å¼€æºæ¨¡å‹ä¹Ÿèƒ½åˆ©ç”¨ç±»æ¯”æç¤ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10db561b55a6fadc13cab8484b4b19fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28f2a3fad3d357e690fb694346fa9207.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c4b99e57c7933c58498a4a9f9145418.jpg" align="middle">
</details>




<h2 id="PETapter-Leveraging-PET-style-classification-heads-for-modular-few-shot-parameter-efficient-fine-tuning"><a href="#PETapter-Leveraging-PET-style-classification-heads-for-modular-few-shot-parameter-efficient-fine-tuning" class="headerlink" title="PETapter: Leveraging PET-style classification heads for modular few-shot   parameter-efficient fine-tuning"></a>PETapter: Leveraging PET-style classification heads for modular few-shot   parameter-efficient fine-tuning</h2><p><strong>Authors:Jonas Rieger, Mattes Ruckdeschel, Gregor Wiedemann</strong></p>
<p>Few-shot learning and parameter-efficient fine-tuning (PEFT) are crucial to overcome the challenges of data scarcity and ever growing language model sizes. This applies in particular to specialized scientific domains, where researchers might lack expertise and resources to fine-tune high-performing language models to nuanced tasks. We propose PETapter, a novel method that effectively combines PEFT methods with PET-style classification heads to boost few-shot learning capabilities without the significant computational overhead typically associated with full model training. We validate our approach on three established NLP benchmark datasets and one real-world dataset from communication research. We show that PETapter not only achieves comparable performance to full few-shot fine-tuning using pattern-exploiting training (PET), but also provides greater reliability and higher parameter efficiency while enabling higher modularity and easy sharing of the trained modules, which enables more researchers to utilize high-performing NLP-methods in their research. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆfew-shot learningï¼‰å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºå…‹æœæ•°æ®ç¨€ç¼ºå’Œä¸æ–­å¢é•¿çš„è¯­è¨€æ¨¡å‹è§„æ¨¡æ‰€å¸¦æ¥çš„æŒ‘æˆ˜è‡³å…³é‡è¦ã€‚è¿™åœ¨ç‰¹å®šçš„ç§‘å­¦é¢†åŸŸå°¤å…¶é€‚ç”¨ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œç ”ç©¶äººå‘˜å¯èƒ½ç¼ºä¹ä¸“ä¸šçŸ¥è¯†å’Œèµ„æºæ¥å¾®è°ƒé«˜æ€§èƒ½è¯­è¨€æ¨¡å‹ä»¥æ‰§è¡Œå¾®å¦™çš„ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•PETapterï¼Œå®ƒæœ‰æ•ˆåœ°ç»“åˆäº†PEFTæ–¹æ³•ä¸PETé£æ ¼çš„åˆ†ç±»å¤´ï¼Œå¯ä»¥åœ¨ä¸å¼•å…¥ä¸å…¨æ¨¡å‹è®­ç»ƒç›¸å…³çš„é‡å¤§è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæå‡å°‘é‡å­¦ä¹ çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæˆç†Ÿçš„NLPåŸºå‡†æ•°æ®é›†å’Œä¸€ä¸ªé€šä¿¡ç ”ç©¶é¢†åŸŸçš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜äº†PETapterä¸ä»…å®ç°äº†ä¸åˆ©ç”¨æ¨¡å¼è®­ç»ƒï¼ˆPETï¼‰è¿›è¡Œçš„å…¨å°‘é‡ç²¾ç»†è°ƒæ•´ç›¸å½“çš„æ€§èƒ½ï¼Œè€Œä¸”æä¾›äº†æ›´é«˜çš„å¯é æ€§å’Œå‚æ•°æ•ˆç‡ï¼ŒåŒæ—¶å¢å¼ºäº†æ¨¡å—çš„æ›´é«˜å¯é‡ç”¨æ€§å’Œæ˜“å…±äº«æ€§ï¼Œä»è€Œä½¿å¾—æ›´å¤šçš„ç ”ç©¶äººå‘˜èƒ½å¤Ÿåœ¨å…¶ç ”ç©¶ä¸­åˆ©ç”¨é«˜æ€§èƒ½çš„NLPæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04975v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰ä¸few-shotå­¦ä¹ ç›¸ç»“åˆçš„æ–°æ–¹æ³•â€”â€”PETapterã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä¸“ä¸šç§‘å­¦é¢†åŸŸï¼Œè§£å†³æ•°æ®ç¨€ç¼ºå’Œä¸æ–­å¢é•¿çš„è¯­è¨€æ¨¡å‹è§„æ¨¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚PETapterç»“åˆäº†PEFTæ–¹æ³•å’ŒPETé£æ ¼åˆ†ç±»å¤´ï¼Œæé«˜äº†å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½äº†å…¨æ¨¡å‹è®­ç»ƒçš„å·¨å¤§è®¡ç®—å¼€é”€ã€‚åœ¨å¤šä¸ªNLPåŸºå‡†æ•°æ®é›†å’Œé€šä¿¡ç ”ç©¶é¢†åŸŸçš„çœŸå®æ•°æ®é›†ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒPETapterä¸ä»…å®ç°äº†ä¸å…¨å°‘æ ·æœ¬å¾®è°ƒç›¸è¿‘çš„æ€§èƒ½ï¼Œè¿˜æä¾›äº†æ›´é«˜çš„å¯é æ€§å’Œå‚æ•°æ•ˆç‡ï¼ŒåŒæ—¶å®ç°äº†æ›´é«˜çš„æ¨¡å—åŒ–å’Œæ˜“äºåˆ†äº«çš„å·²è®­ç»ƒæ¨¡å—ï¼Œä½¿å¾—æ›´å¤šç ”ç©¶è€…èƒ½å¤Ÿåœ¨å…¶ç ”ç©¶ä¸­åˆ©ç”¨é«˜æ€§èƒ½çš„NLPæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETapterç»“åˆäº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰å’Œfew-shotå­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³æ•°æ®ç¨€ç¼ºå’Œå¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒæŒ‘æˆ˜ã€‚</li>
<li>æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºä¸“ä¸šç§‘å­¦é¢†åŸŸï¼Œå…¶ä¸­ç ”ç©¶è€…å¯èƒ½ç¼ºä¹ä¸“ä¸šçŸ¥è¯†å’Œèµ„æºæ¥å¾®è°ƒé«˜æ€§èƒ½è¯­è¨€æ¨¡å‹ä»¥æ‰§è¡Œç»†å¾®ä»»åŠ¡ã€‚</li>
<li>PETapteré€šè¿‡ä½¿ç”¨PEFTæ–¹æ³•å’ŒPETé£æ ¼åˆ†ç±»å¤´ï¼Œåœ¨æé«˜å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„åŒæ—¶ï¼Œé™ä½äº†é€šå¸¸ä¸å…¨æ¨¡å‹è®­ç»ƒç›¸å…³çš„å·¨å¤§è®¡ç®—å¼€é”€ã€‚</li>
<li>åœ¨å¤šä¸ªNLPåŸºå‡†æ•°æ®é›†å’Œé€šä¿¡ç ”ç©¶é¢†åŸŸçš„çœŸå®æ•°æ®é›†ä¸ŠéªŒè¯äº†PETapterçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>PETapterå®ç°äº†ä¸å…¨å°‘æ ·æœ¬å¾®è°ƒç›¸è¿‘çš„æ€§èƒ½ã€‚</li>
<li>PETapteræä¾›äº†æ›´é«˜çš„å¯é æ€§å’Œå‚æ•°æ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-056eb6207b90719eb240294208b66534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc06e681ff18eb87b5f91ee3fac911b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93e625f8c2e352688ab67c6522cdd890.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19832843da222ff612913d138e3c52b5.jpg" align="middle">
</details>




<h2 id="A-Federated-Approach-to-Few-Shot-Hate-Speech-Detection-for-Marginalized-Communities"><a href="#A-Federated-Approach-to-Few-Shot-Hate-Speech-Detection-for-Marginalized-Communities" class="headerlink" title="A Federated Approach to Few-Shot Hate Speech Detection for Marginalized   Communities"></a>A Federated Approach to Few-Shot Hate Speech Detection for Marginalized   Communities</h2><p><strong>Authors:Haotian Ye, Axel Wisiorek, Antonis Maronikolakis, Ã–zge AlaÃ§am, Hinrich SchÃ¼tze</strong></p>
<p>Hate speech online remains an understudied issue for marginalized communities, and has seen rising relevance, especially in the Global South, which includes developing societies with increasing internet penetration. In this paper, we aim to provide marginalized communities living in societies where the dominant language is low-resource with a privacy-preserving tool to protect themselves from hate speech on the internet by filtering offensive content in their native languages. Our contribution in this paper is twofold: 1) we release REACT (REsponsive hate speech datasets Across ConTexts), a collection of high-quality, culture-specific hate speech detection datasets comprising seven distinct target groups in eight low-resource languages, curated by experienced data collectors; 2) we propose a solution to few-shot hate speech detection utilizing federated learning (FL), a privacy-preserving and collaborative learning approach, to continuously improve a central model that exhibits robustness when tackling different target groups and languages. By keeping the training local to the usersâ€™ devices, we ensure the privacy of the usersâ€™ data while benefitting from the efficiency of federated learning. Furthermore, we personalize client models to target-specific training data and evaluate their performance. Our results indicate the effectiveness of FL across different target groups, whereas the benefits of personalization on few-shot learning are not clear. </p>
<blockquote>
<p>ç½‘ç»œä¸Šçš„ä»‡æ¨è¨€è®ºä»ç„¶æ˜¯ä¸€ä¸ªé’ˆå¯¹è¾¹ç¼˜åŒ–ç¾¤ä½“çš„é—®é¢˜ï¼Œå¹¶ä¸”å…¶é‡è¦æ€§æ—¥ç›Šå¢åŠ ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¨çƒå—æ–¹ï¼ŒåŒ…æ‹¬äº’è”ç½‘æ™®åŠç‡ä¸æ–­å¢åŠ çš„å‘å±•ä¸­å›½å®¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å‘ç”Ÿæ´»åœ¨ä¸»å¯¼è¯­è¨€ä¸ºèµ„æºç¨€ç¼ºçš„ç¤¾ä¼šçš„è¾¹ç¼˜åŒ–ç¾¤ä½“æä¾›ä¸€ä¸ªä¿æŠ¤éšç§çš„å·¥å…·ï¼Œä»¥è¿‡æ»¤å…¶æ¯è¯­ä¸­çš„å†’çŠ¯å†…å®¹ï¼Œä»è€Œä¿æŠ¤ä»–ä»¬å…å—ç½‘ç»œä¸Šçš„ä»‡æ¨è¨€è®ºçš„ä¼¤å®³ã€‚æœ¬æ–‡æˆ‘ä»¬çš„è´¡çŒ®æœ‰ä¸¤æ–¹é¢ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å‘å¸ƒäº†REACTï¼ˆè·¨è¯­å¢ƒå“åº”ä»‡æ¨è¨€è®ºæ•°æ®é›†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç»„é«˜è´¨é‡ã€å…·æœ‰æ–‡åŒ–ç‰¹å¼‚æ€§çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ•°æ®é›†ï¼ŒåŒ…å«ä¸ƒä¸ªä¸åŒçš„ç›®æ ‡ç¾¤ä½“å’Œå…«ç§èµ„æºç¨€ç¼ºçš„è¯­è¨€ï¼Œç”±ç»éªŒä¸°å¯Œçš„æ•°æ®æ”¶é›†è€…ç¼–åˆ¶ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰è§£å†³å°æ ·æœ¬ä»‡æ¨è¨€è®ºæ£€æµ‹é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚è”é‚¦å­¦ä¹ æ˜¯ä¸€ç§ä¿æŠ¤éšç§çš„åä½œå­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿä¸æ–­æ”¹è¿›ä¸­å¿ƒæ¨¡å‹ï¼Œåœ¨å¤„ç†ä¸åŒç›®æ ‡ç¾¤ä½“å’Œè¯­è¨€æ—¶è¡¨ç°å‡ºç¨³å¥æ€§ã€‚é€šè¿‡å°†è®­ç»ƒä¿æŒåœ¨ç”¨æˆ·è®¾å¤‡ä¸Šï¼Œæˆ‘ä»¬ç¡®ä¿äº†ç”¨æˆ·æ•°æ®çš„éšç§æ€§ï¼ŒåŒæ—¶äº«å—äº†è”é‚¦å­¦ä¹ çš„æ•ˆç‡ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å®¢æˆ·æ¨¡å‹è¿›è¡Œäº†é’ˆå¯¹ç‰¹å®šç›®æ ‡æ•°æ®çš„è®­ç»ƒï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè”é‚¦å­¦ä¹ åœ¨ä¸åŒç›®æ ‡ç¾¤ä½“ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ªæ€§åŒ–åœ¨å°æ ·æœ¬å­¦ä¹ ä¸Šçš„ä¼˜åŠ¿å°šä¸æ¸…æ¥šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04942v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨ä¸ºä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­çš„è¾¹ç¼˜åŒ–ç¤¾åŒºæä¾›ä¸€ç§ä¿æŠ¤ä¸ªäººéšç§çš„å·¥å…·ï¼Œå¯¹æŠ—ç½‘ç»œä¸Šä»‡æ¨è¨€è®ºçš„å¨èƒã€‚ç ”ç©¶å‘å¸ƒäº†REACTæ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸ƒä¸ªç›®æ ‡ç¾¤ä½“å’Œå…«ç§ä½èµ„æºè¯­è¨€çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæå‡ºäº†åˆ©ç”¨è”é‚¦å­¦ä¹ è§£å†³å°‘æ•°æ ·æœ¬ä»‡æ¨è¨€è®ºæ£€æµ‹é—®é¢˜çš„æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¿æŠ¤ç”¨æˆ·éšç§çš„åŒæ—¶ï¼Œæé«˜äº†æ¨¡å‹çš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç›®æ ‡ç¾¤ä½“å’Œè¯­è¨€ã€‚è™½ç„¶ä¸ªæ€§åŒ–è®­ç»ƒåœ¨å°‘æ•°æ ·æœ¬å­¦ä¹ ä¸Šçš„ä¼˜åŠ¿å°šä¸æ˜æ˜¾ï¼Œä½†ç ”ç©¶ç»“æœè¡¨æ˜è”é‚¦å­¦ä¹ åœ¨ä¸åŒç›®æ ‡ç¾¤ä½“ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‡æ¨è¨€è®ºåœ¨çº¿å¯¹è¾¹ç¼˜åŒ–ç¤¾åŒºçš„å½±å“æ—¥ç›Šä¸¥é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¨çƒå—æ–¹å›½å®¶ã€‚</li>
<li>ç ”ç©¶å‘å¸ƒäº†REACTæ•°æ®é›†ï¼ŒåŒ…å«ä¸ƒä¸ªç›®æ ‡ç¾¤ä½“å’Œå…«ç§ä½èµ„æºè¯­è¨€çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>è”é‚¦å­¦ä¹ è¢«ç”¨æ¥è§£å†³å°‘æ•°æ ·æœ¬ä»‡æ¨è¨€è®ºæ£€æµ‹é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§ä¿æŠ¤ç”¨æˆ·éšç§çš„åä½œå­¦ä¹ æ–¹å¼ã€‚</li>
<li>è”é‚¦å­¦ä¹ æ¨¡å‹å±•ç°å‡ºå¯¹ä¸åŒç›®æ ‡ç¾¤ä½“å’Œè¯­è¨€çš„é²æ£’æ€§ã€‚</li>
<li>ç”¨æˆ·æ•°æ®è®­ç»ƒä¿æŒæœ¬åœ°åŒ–ï¼Œç¡®ä¿ç”¨æˆ·éšç§ã€‚</li>
<li>å°è¯•ä¸ªæ€§åŒ–å®¢æˆ·ç«¯æ¨¡å‹ä»¥é’ˆå¯¹ç‰¹å®šç›®æ ‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½†ä¸ªæ€§åŒ–åœ¨å°‘æ•°æ ·æœ¬å­¦ä¹ ä¸Šçš„ä¼˜åŠ¿å°šä¸æ˜æ˜¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b47f260694665454c3f1b19fccb8e003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb8fa38be5375881e9490bdf57d667af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dda5ef889368852c5b48ee4b61e1df2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a358d2011279c07190b911074ec8b939.jpg" align="middle">
</details>




<h2 id="Rethinking-Time-Series-Forecasting-with-LLMs-via-Nearest-Neighbor-Contrastive-Learning"><a href="#Rethinking-Time-Series-Forecasting-with-LLMs-via-Nearest-Neighbor-Contrastive-Learning" class="headerlink" title="Rethinking Time Series Forecasting with LLMs via Nearest Neighbor   Contrastive Learning"></a>Rethinking Time Series Forecasting with LLMs via Nearest Neighbor   Contrastive Learning</h2><p><strong>Authors:Jayanie Bogahawatte, Sachith Seneviratne, Maneesha Perera, Saman Halgamuge</strong></p>
<p>Adapting Large Language Models (LLMs) that are extensively trained on abundant text data, and customizing the input prompt to enable time series forecasting has received considerable attention. While recent work has shown great potential for adapting the learned prior of LLMs, the formulation of the prompt to finetune LLMs remains challenging as prompt should be aligned with time series data. Additionally, current approaches do not effectively leverage word token embeddings which embody the rich representation space learned by LLMs. This emphasizes the need for a robust approach to formulate the prompt which utilizes the word token embeddings while effectively representing the characteristics of the time series. To address these challenges, we propose NNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting via LLMs. First, we generate time series compatible text prototypes such that each text prototype represents both word token embeddings in its neighborhood and time series characteristics via end-to-end finetuning. Next, we draw inspiration from Nearest Neighbor Contrastive Learning to formulate the prompt while obtaining the top-$k$ nearest neighbor time series compatible text prototypes. We then fine-tune the layer normalization and positional embeddings of the LLM, keeping the other layers intact, reducing the trainable parameters and decreasing the computational cost. Our comprehensive experiments demonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving competitive or superior performance over the state-of-the-art methods in long-term and short-term forecasting tasks. </p>
<blockquote>
<p>é€‚åº”åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šç»è¿‡å¹¿æ³›è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶é€šè¿‡å®šåˆ¶è¾“å…¥æç¤ºæ¥å®ç°æ—¶é—´åºåˆ—é¢„æµ‹å·²ç»å¼•èµ·äº†ç›¸å½“å¤§çš„å…³æ³¨ã€‚è™½ç„¶æœ€è¿‘çš„å·¥ä½œæ˜¾ç¤ºé€‚åº”LLMçš„å…ˆéªŒçŸ¥è¯†å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†è°ƒæ•´LLMçš„æç¤ºçš„å…¬å¼ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæç¤ºåº”è¯¥ä¸æ—¶é—´åºåˆ—æ•°æ®å¯¹é½ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•å¹¶æ²¡æœ‰æœ‰æ•ˆåˆ©ç”¨LLMæ‰€å­¦ä¹ åˆ°çš„ä¸°å¯Œçš„å•è¯æ ‡è®°åµŒå…¥ç©ºé—´ã€‚è¿™å¼ºè°ƒäº†éœ€è¦ä¸€ç§ç¨³å¥çš„æ–¹æ³•æ¥åˆ¶å®šæç¤ºï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å•è¯æ ‡è®°åµŒå…¥åŒæ—¶æœ‰æ•ˆåœ°è¡¨ç¤ºæ—¶é—´åºåˆ—çš„ç‰¹å¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†NNCL-TLLMï¼šåŸºäºLLMçš„æ—¶é—´åºåˆ—é¢„æµ‹æœ€è¿‘é‚»å¯¹æ¯”å­¦ä¹ ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆä¸æ—¶é—´åºåˆ—å…¼å®¹çš„æ–‡æœ¬åŸå‹ï¼Œä½¿æ¯ä¸ªæ–‡æœ¬åŸå‹éƒ½èƒ½ä»£è¡¨å…¶é‚»åŸŸä¸­çš„å•è¯æ ‡è®°åµŒå…¥å’Œé€šè¿‡ç«¯åˆ°ç«¯å¾®è°ƒä½“ç°æ—¶é—´åºåˆ—çš„ç‰¹å¾ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»æœ€è¿‘é‚»å¯¹æ¯”å­¦ä¹ ä¸­æ±²å–çµæ„Ÿï¼Œåˆ¶å®šæç¤ºå¹¶è·å–å‰kä¸ªæœ€æ¥è¿‘çš„æ—¶é—´åºåˆ—å…¼å®¹æ–‡æœ¬åŸå‹ã€‚ç„¶åæˆ‘ä»¬å¯¹LLMçš„å±‚å½’ä¸€åŒ–å’Œä½ç½®åµŒå…¥è¿›è¡Œå¾®è°ƒï¼Œä¿æŒå…¶ä»–å±‚ä¸å˜ï¼Œä»è€Œå‡å°‘å¯è®­ç»ƒå‚æ•°å’Œè®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒNNCL-TLLMåœ¨å°‘æ ·æœ¬é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†ä¸æœ€æ–°æŠ€æœ¯ç«äº‰æˆ–æ›´ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04806v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯å—åˆ°å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡LLMçš„å…ˆéªŒçŸ¥è¯†é€‚åº”å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†è°ƒæ•´æç¤ºä»¥ä¸æ—¶é—´åºåˆ—æ•°æ®å¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºNNCL-TLLMæ–¹æ³•ï¼Œé€šè¿‡æœ€è¿‘é‚»å¯¹æ¯”å­¦ä¹ ï¼Œç”Ÿæˆä¸æ—¶é—´åºåˆ—å…¼å®¹çš„æ–‡æœ¬åŸå‹ï¼Œå¹¶åˆ©ç”¨LLMè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒNNCL-TLLMåœ¨å°‘æ ·æœ¬é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>è°ƒæ•´æç¤ºä»¥ä¸æ—¶é—´åºåˆ—æ•°æ®å¯¹é½æ˜¯å½“å‰çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>NNCL-TLLMæ–¹æ³•é€šè¿‡ç”Ÿæˆä¸æ—¶é—´åºåˆ—å…¼å®¹çš„æ–‡æœ¬åŸå‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>NNCL-TLLMåˆ©ç”¨æœ€è¿‘é‚»å¯¹æ¯”å­¦ä¹ æŠ€æœ¯ï¼Œç»“åˆLLMè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚</li>
<li>NNCL-TLLMåœ¨å°‘æ ·æœ¬é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>NNCL-TLLMåœ¨é•¿æœŸå’ŒçŸ­æœŸé¢„æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–è¶…è¿‡ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-661d28b26235ff72cc7eff1a97d6b877.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5097d2f71326ef2e306d6333ff7a35f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cfcb9342beeb7032db50dc8574a0aac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-800fb557f24124a989376cd15ba21b39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3924f93e3929d8259e576f0e3f1bf58.jpg" align="middle">
</details>




<h2 id="KNN-MMD-Cross-Domain-Wi-Fi-Sensing-Based-on-Local-Distribution-Alignment"><a href="#KNN-MMD-Cross-Domain-Wi-Fi-Sensing-Based-on-Local-Distribution-Alignment" class="headerlink" title="KNN-MMD: Cross Domain Wi-Fi Sensing Based on Local Distribution   Alignment"></a>KNN-MMD: Cross Domain Wi-Fi Sensing Based on Local Distribution   Alignment</h2><p><strong>Authors:Zijian Zhao, Zhijie Cai, Tingwei Chen, Xiaoyang Li, Hang Li, Guangxu Zhu</strong></p>
<p>As a key technology in Integrated Sensing and Communications (ISAC), Wi-Fi sensing has gained widespread application in various settings such as homes, offices, and public spaces. By analyzing the patterns of Channel State Information (CSI), we can obtain information about peopleâ€™s actions for tasks like person identification, gesture recognition, and fall detection. However, the CSI is heavily influenced by the environment, such that even minor environmental changes can significantly alter the CSI patterns. This will cause the performance deterioration and even failure when applying the Wi-Fi sensing model trained in one environment to another. To address this problem, we introduce a K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD) model, a few-shot method for cross-domain Wi-Fi sensing. We propose a local distribution alignment method within each category, which outperforms traditional Domain Adaptation (DA) methods based on global alignment. Besides, our method can determine when to stop training, which cannot be realized by most DA methods. As a result, our method is more stable and can be better used in practice. The effectiveness of our method are evaluated in several cross-domain Wi-Fi sensing tasks, including gesture recognition, person identification, fall detection, and action recognition, using both a public dataset and a self-collected dataset. In one-shot scenario, our method achieves accuracy of 93.26%, 81.84%, 77.62%, and 75.30% in the four tasks respectively. To facilitate future research, we will make our code and dataset publicly available upon publication. </p>
<blockquote>
<p>ä½œä¸ºé›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰çš„å…³é”®æŠ€æœ¯ï¼ŒWi-Fiæ„ŸçŸ¥åœ¨å®¶åº­ã€åŠå…¬å®¤å’Œå…¬å…±åœºæ‰€ç­‰å„ç§åœºæ™¯ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚é€šè¿‡åˆ†æä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰çš„æ¨¡å¼ï¼Œæˆ‘ä»¬å¯ä»¥è·å–äººä»¬çš„è¡Œä¸ºä¿¡æ¯ï¼Œç”¨äºäººå‘˜è¯†åˆ«ã€æ‰‹åŠ¿è¯†åˆ«å’Œè·Œå€’æ£€æµ‹ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒCSIå—åˆ°ç¯å¢ƒçš„é«˜åº¦é‡è§†å½±å“ï¼Œå³ä½¿å¾®å°çš„ç¯å¢ƒå˜åŒ–ä¹Ÿå¯èƒ½æ˜¾è‘—æ”¹å˜CSIæ¨¡å¼ã€‚è¿™å°†å¯¼è‡´å°†åœ¨ä¸€ç§ç¯å¢ƒä¸­è®­ç»ƒçš„Wi-Fiæ„ŸçŸ¥æ¨¡å‹åº”ç”¨åˆ°å¦ä¸€ç§ç¯å¢ƒæ—¶æ€§èƒ½ä¸‹é™ç”šè‡³å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºKè¿‘é‚»æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆKNN-MMDï¼‰çš„æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè·¨åŸŸWi-Fiæ„ŸçŸ¥çš„å°‘é‡æ ·æœ¬æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå±€éƒ¨åˆ†å¸ƒå¯¹é½çš„æ–¹æ³•ï¼Œåœ¨å„ç±»å†…éƒ¨è¿›è¡Œå¯¹é½ï¼Œè¿™ä¼˜äºåŸºäºå…¨å±€å¯¹é½çš„ä¼ ç»ŸåŸŸè‡ªé€‚åº”ï¼ˆDAï¼‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å†³å®šä½•æ—¶åœæ­¢è®­ç»ƒï¼Œè€Œå¤§å¤šæ•°DAæ–¹æ³•æ— æ³•å®ç°è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ›´åŠ ç¨³å®šï¼Œå¹¶èƒ½æ›´å¥½åœ°ç”¨äºå®è·µã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªè·¨åŸŸWi-Fiæ„ŸçŸ¥ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†è¯„ä¼°ï¼ŒåŒ…æ‹¬æ‰‹åŠ¿è¯†åˆ«ã€äººå‘˜è¯†åˆ«ã€è·Œå€’æ£€æµ‹å’ŒåŠ¨ä½œè¯†åˆ«ç­‰ä»»åŠ¡ï¼Œä½¿ç”¨äº†å…¬å…±æ•°æ®é›†å’Œè‡ªæˆ‘æ”¶é›†çš„æ•°æ®é›†ã€‚åœ¨å•æ ·æœ¬åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å››é¡¹ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°93.26%ã€81.84%ã€77.62%å’Œ75.30%ã€‚ä¸ºäº†ä¾¿äºæœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä¼šåœ¨å‘å¸ƒè®ºæ–‡çš„åŒæ—¶å…¬å¼€ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Wi-Fiæ„ŸçŸ¥ä½œä¸ºå…³é”®æŠ€æœ¯åœ¨é›†æˆæ„ŸçŸ¥å’Œé€šä¿¡ï¼ˆISACï¼‰é¢†åŸŸçš„åº”ç”¨ï¼Œé€šè¿‡è§£æä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰çš„æ¨¡å¼ï¼Œå¯ä»¥å®ç°äººç‰©è¯†åˆ«ã€åŠ¨ä½œè¯†åˆ«ç­‰åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºç¯å¢ƒå¯¹CSIæ¨¡å¼çš„å½±å“è¾ƒå¤§ï¼Œè·¨ç¯å¢ƒåº”ç”¨Wi-Fiæ„ŸçŸ¥æ¨¡å‹ä¼šå‡ºç°æ€§èƒ½ä¸‹é™ç”šè‡³å¤±æ•ˆçš„é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºKæœ€è¿‘é‚»æœ€å¤§å‡å€¼å·®å¼‚ï¼ˆKNN-MMDï¼‰æ¨¡å‹çš„è·¨åŸŸWi-Fiæ„ŸçŸ¥æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å±€éƒ¨åˆ†å¸ƒå¯¹é½ç­–ç•¥ï¼Œåœ¨å„ç±»åˆ«å†…éƒ¨è¿›è¡Œå¯¹é½ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„å…¨å±€å¯¹é½æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½ç¡®å®šè®­ç»ƒä½•æ—¶åœæ­¢ï¼Œè¿™æ˜¯å¤§å¤šæ•°é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•æ— æ³•å®ç°çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨åŸŸWi-Fiæ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡è¾ƒé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wi-Fiæ„ŸçŸ¥æŠ€æœ¯é€šè¿‡åˆ†æä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰çš„æ¨¡å¼ï¼Œå¯åº”ç”¨äºäººç‰©è¯†åˆ«ã€åŠ¨ä½œè¯†åˆ«ç­‰ä»»åŠ¡ã€‚</li>
<li>ç¯å¢ƒå› ç´ å¯¹CSIæ¨¡å¼å½±å“æ˜¾è‘—ï¼Œå¯¼è‡´Wi-Fiæ„ŸçŸ¥æ¨¡å‹è·¨ç¯å¢ƒåº”ç”¨æ—¶æ€§èƒ½ä¸‹é™ã€‚</li>
<li>å¼•å…¥KNN-MMDæ¨¡å‹ä½œä¸ºè·¨åŸŸWi-Fiæ„ŸçŸ¥çš„è§£å†³ç­–ç•¥ã€‚</li>
<li>KNN-MMDæ¨¡å‹é‡‡ç”¨å±€éƒ¨åˆ†å¸ƒå¯¹é½ç­–ç•¥ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿå…¨å±€å¯¹é½æ–¹æ³•ã€‚</li>
<li>KNN-MMDæ¨¡å‹èƒ½è‡ªåŠ¨ç¡®å®šè®­ç»ƒä½•æ—¶åœæ­¢ï¼Œè¿™æ˜¯å¤§å¤šæ•°é¢†åŸŸè‡ªé€‚åº”æ–¹æ³•æ— æ³•å®ç°çš„ã€‚</li>
<li>åœ¨å¤šä¸ªè·¨åŸŸWi-Fiæ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼ŒåŒ…æ‹¬æ‰‹åŠ¿è¯†åˆ«ã€äººç‰©è¯†åˆ«ç­‰ï¼ŒKNN-MMDæ¨¡å‹è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab38e7d2c685466acb2855378c2a3d1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c15e46c34a310b7ed71f5db41527f6f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff2e5746f014a5795a7f18993433a837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc9b3324fab6aeab267098767a83963f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28f73268d3f8ba2c002397f96ac2260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db19452007c2a3e3cef1ab8e49268c1a.jpg" align="middle">
</details>




<h2 id="Improving-LLM-Group-Fairness-on-Tabular-Data-via-In-Context-Learning"><a href="#Improving-LLM-Group-Fairness-on-Tabular-Data-via-In-Context-Learning" class="headerlink" title="Improving LLM Group Fairness on Tabular Data via In-Context Learning"></a>Improving LLM Group Fairness on Tabular Data via In-Context Learning</h2><p><strong>Authors:Valeriia Cherepanova, Chia-Jung Lee, Nil-Jana Akpinar, Riccardo Fogliato, Martin Andres Bertran, Michael Kearns, James Zou</strong></p>
<p>Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debiasing approaches for natural language tasks do not directly translate to mitigating group unfairness in tabular settings. In this work, we systematically investigate four empirical approaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning. Through experiments on four tabular datasets using both open-source and proprietary LLMs, we show the effectiveness of these methods in enhancing demographic parity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable approach based on their specific requirements and constraints. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä½æ•°æ®çŠ¶æ€ä¸‹å·²è¢«è¯æ˜åœ¨è¡¨æ ¼é¢„æµ‹ä»»åŠ¡ä¸Šéå¸¸æœ‰æ•ˆï¼Œå®ƒä»¬åˆ©ç”¨å†…éƒ¨çŸ¥è¯†å’Œä»æŒ‡ä»¤å’Œç¤ºä¾‹ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMå¯èƒ½ä¼šç”Ÿæˆä¸æ»¡è¶³ç¾¤ä½“å…¬å¹³æ€§çš„é¢„æµ‹ç»“æœï¼Œå³åœ¨ç¾¤ä½“é—´äº§ç”Ÿä¸å…¬å¹³çš„ç»“æœã€‚é‡è¦çš„æ˜¯ï¼Œç”¨äºè‡ªç„¶è¯­è¨€ä»»åŠ¡çš„ä¼ ç»Ÿå»åæ–¹æ³•å¹¶ä¸èƒ½ç›´æ¥è½¬åŒ–ä¸ºå‡è½»è¡¨æ ¼è®¾ç½®ä¸­ç¾¤ä½“ä¸å…¬å¹³çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†å››ç§å®è¯æ–¹æ³•ï¼Œä»¥æé«˜LLMåœ¨è¡¨æ ¼æ•°æ®é›†ä¸Šçš„é¢„æµ‹ç»“æœçš„ç¾¤ä½“å…¬å¹³æ€§ï¼ŒåŒ…æ‹¬å…¬å¹³æç¤ºä¼˜åŒ–ã€è½¯æç¤ºè°ƒæ•´ã€å°‘æ•°æ¡ˆä¾‹çš„æˆ˜ç•¥é€‰æ‹©ä»¥åŠé€šè¿‡é“¾å¼æ€ç»´æ¨ç†è¿›è¡Œé¢„æµ‹çš„è‡ªæˆ‘ä¿®æ­£ã€‚æˆ‘ä»¬åœ¨å››ä¸ªè¡¨æ ¼æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œé‡‡ç”¨äº†å¼€æºå’Œä¸“ç”¨çš„LLMæ¨¡å‹ï¼Œå®éªŒç»“æœè¡¨æ˜è¿™äº›æ–¹æ³•åœ¨æé«˜äººå£ç»Ÿè®¡å­¦å¹³è¡¡æ€§çš„åŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æå¯ä¸ºä»ä¸šè€…åœ¨é€‰æ‹©æœ€é€‚åˆä»–ä»¬ç‰¹å®šéœ€æ±‚å’Œçº¦æŸçš„æ–¹æ³•æ—¶æä¾›å®é™…çš„æ“ä½œå»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04642v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä½æ•°æ®æƒ…å†µä¸‹å¯¹è¡¨æ ¼é¢„æµ‹ä»»åŠ¡æœ‰æ•ˆï¼Œä½†å¯èƒ½æ— æ³•æ»¡è¶³ç¾¤ä½“å…¬å¹³æ€§ã€‚æœ¬æ–‡ç³»ç»Ÿæ¢è®¨äº†å››ç§æé«˜LLMåœ¨è¡¨æ ¼æ•°æ®é›†ä¸Šé¢„æµ‹ç¾¤ä½“å…¬å¹³æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å…¬å¹³æç¤ºä¼˜åŒ–ã€è½¯æç¤ºè°ƒæ•´ã€æˆ˜ç•¥é€‰æ‹©å°‘é‡ç¤ºä¾‹å’Œé€šè¿‡æ€ç»´é“¾æ¨ç†è‡ªæˆ‘ä¿®æ­£é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨æé«˜äººå£ç»Ÿè®¡å­¦å…¬å¹³æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†è¾ƒé«˜çš„æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è¡¨æ ¼é¢„æµ‹ä»»åŠ¡ä¸­å±•ç°æœ‰æ•ˆæ€§ï¼Œå°¤å…¶åœ¨ä½æ•°æ®æƒ…å†µä¸‹ã€‚</li>
<li>LLMså¯èƒ½æ— æ³•æ»¡è¶³ç¾¤ä½“å…¬å¹³æ€§è¦æ±‚ï¼Œåœ¨è¡¨æ ¼æ•°æ®é›†ä¸­äº§ç”Ÿä¸å…¬å¹³çš„é¢„æµ‹ç»“æœã€‚</li>
<li>å…¬å¹³æç¤ºä¼˜åŒ–ã€è½¯æç¤ºè°ƒæ•´ã€æˆ˜ç•¥é€‰æ‹©å°‘é‡ç¤ºä¾‹å’Œæ€ç»´é“¾æ¨ç†æ˜¯æé«˜LLMç¾¤ä½“å…¬å¹³æ€§çš„å››ç§å®è¯æ–¹æ³•ã€‚</li>
<li>è¿™äº›æ–¹æ³•åœ¨å¢å¼ºäººå£ç»Ÿè®¡å­¦å…¬å¹³æ€§çš„åŒæ—¶ï¼Œä¿æŒäº†è¾ƒé«˜çš„æ•´ä½“é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>é’ˆå¯¹ä¸åŒéœ€æ±‚å’Œçº¦æŸï¼Œä»ä¸šè€…å¯æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©åˆé€‚çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡çš„å®éªŒç»“æœåŸºäºå››ä¸ªè¡¨æ ¼æ•°æ®é›†å’Œå¼€æºåŠä¸“æœ‰LLMsã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-abb8c06c8975a62076f890afcaaeb82e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e763796046c7a033a7e6e113016bc52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ce381652cdfe6ed4731f341bf421a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a69ae11dde992db8190325ae311369a.jpg" align="middle">
</details>




<h2 id="CLIP-FSAC-Few-Shot-Anomaly-Classification-with-Anomaly-Descriptor-Based-on-CLIP"><a href="#CLIP-FSAC-Few-Shot-Anomaly-Classification-with-Anomaly-Descriptor-Based-on-CLIP" class="headerlink" title="CLIP-FSAC++: Few-Shot Anomaly Classification with Anomaly Descriptor   Based on CLIP"></a>CLIP-FSAC++: Few-Shot Anomaly Classification with Anomaly Descriptor   Based on CLIP</h2><p><strong>Authors:Zuo Zuo, Jiahao Dong, Yao Wu, Yanyun Qu, Zongze Wu</strong></p>
<p>Industrial anomaly classification (AC) is an indispensable task in industrial manufacturing, which guarantees quality and safety of various product. To address the scarcity of data in industrial scenarios, lots of few-shot anomaly detection methods emerge recently. In this paper, we propose an effective few-shot anomaly classification (FSAC) framework with one-stage training, dubbed CLIP-FSAC++. Specifically, we introduce a cross-modality interaction module named Anomaly Descriptor following image and text encoders, which enhances the correlation of visual and text embeddings and adapts the representations of CLIP from pre-trained data to target data. In anomaly descriptor, image-to-text cross-attention module is used to obtain image-specific text embeddings and text-to-image cross-attention module is used to obtain text-specific visual embeddings. Then these modality-specific embeddings are used to enhance original representations of CLIP for better matching ability. Comprehensive experiment results are provided for evaluating our method in few-normal shot anomaly classification on VisA and MVTEC-AD for 1, 2, 4 and 8-shot settings. The source codes are at <a target="_blank" rel="noopener" href="https://github.com/Jay-zzcoder/clip-fsac-pp">https://github.com/Jay-zzcoder/clip-fsac-pp</a> </p>
<blockquote>
<p>å·¥ä¸šå¼‚å¸¸åˆ†ç±»ï¼ˆACï¼‰æ˜¯å·¥ä¸šåˆ¶é€ ä¸­ä¸å¯æˆ–ç¼ºçš„ä»»åŠ¡ï¼Œå®ƒä¿è¯äº†å„ç§äº§å“çš„è´¨é‡ä¸å®‰å…¨ã€‚ä¸ºäº†è§£å†³å·¥ä¸šåœºæ™¯ä¸­æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæœ€è¿‘å‡ºç°äº†è®¸å¤šå°æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„å°æ ·æœ¬å¼‚å¸¸åˆ†ç±»ï¼ˆFSACï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸€é˜¶æ®µè®­ç»ƒï¼Œè¢«ç§°ä¸ºCLIP-FSAC++ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ä¹‹åå¼•å…¥äº†ä¸€ä¸ªåä¸ºAnomaly Descriptorçš„è·¨æ¨¡æ€äº¤äº’æ¨¡å—ï¼Œè¯¥æ¨¡å—å¢å¼ºäº†è§†è§‰å’Œæ–‡æœ¬åµŒå…¥çš„ç›¸å…³æ€§ï¼Œå¹¶å°†CLIPçš„é¢„è®­ç»ƒæ•°æ®è¡¨ç¤ºé€‚åº”äºç›®æ ‡æ•°æ®ã€‚åœ¨å¼‚å¸¸æè¿°ç¬¦ä¸­ï¼Œä½¿ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„è·¨æ³¨æ„åŠ›æ¨¡å—æ¥è·å¾—å›¾åƒç‰¹å®šçš„æ–‡æœ¬åµŒå…¥ï¼Œä»¥åŠä½¿ç”¨æ–‡æœ¬åˆ°å›¾åƒçš„è·¨æ³¨æ„åŠ›æ¨¡å—æ¥è·å¾—æ–‡æœ¬ç‰¹å®šçš„è§†è§‰åµŒå…¥ã€‚ç„¶åï¼Œè¿™äº›æ¨¡æ€ç‰¹å®šçš„åµŒå…¥è¢«ç”¨æ¥å¢å¼ºCLIPçš„åŸå§‹è¡¨ç¤ºï¼Œä»¥æé«˜åŒ¹é…èƒ½åŠ›ã€‚æˆ‘ä»¬æä¾›äº†å…¨é¢çš„å®éªŒç»“æœï¼Œä»¥è¯„ä¼°æˆ‘ä»¬åœ¨VisAå’ŒMVTEC-ADä¸Šè¿›è¡Œçš„å°æ ·æœ¬æ­£å¸¸å¼‚å¸¸åˆ†ç±»æ–¹æ³•ï¼ŒåŒ…æ‹¬1ã€2ã€4å’Œ8ç§è®¾ç½®ã€‚æºä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Jay-zzcoder/clip-fsac-pp%E3%80%82">https://github.com/Jay-zzcoder/clip-fsac-ppã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03829v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„åŸºäºCLIPæ¨¡å‹çš„ä¸€é˜¶æ®µè®­ç»ƒå°‘æ ·æœ¬å¼‚å¸¸åˆ†ç±»ï¼ˆFSACï¼‰æ¡†æ¶ï¼Œåä¸ºCLIP-FSAC++ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è·¨æ¨¡æ€äº¤äº’æ¨¡å—Anomaly Descriptorï¼Œé€šè¿‡å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨å¢å¼ºè§†è§‰å’Œæ–‡æœ¬åµŒå…¥çš„ç›¸å…³æ€§ï¼Œå¹¶å°†CLIPçš„é¢„è®­ç»ƒæ•°æ®è¡¨ç¤ºé€‚åº”äºç›®æ ‡æ•°æ®ã€‚è¯¥æ¡†æ¶é€šè¿‡å›¾åƒåˆ°æ–‡æœ¬çš„è·¨æ³¨æ„åŠ›æ¨¡å—å’Œæ–‡æœ¬åˆ°å›¾åƒçš„è·¨æ³¨æ„åŠ›æ¨¡å—è·å–æ¨¡æ€ç‰¹å®šåµŒå…¥ï¼Œä»¥å¢å¼ºCLIPçš„åŸå§‹è¡¨ç¤ºï¼Œæé«˜åŒ¹é…èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºCLIPæ¨¡å‹çš„å°‘æ ·æœ¬å¼‚å¸¸åˆ†ç±»ï¼ˆFSACï¼‰æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶ä¸­å¼•å…¥äº†Anomaly Descriptorè¿™ä¸€è·¨æ¨¡æ€äº¤äº’æ¨¡å—ã€‚</li>
<li>Anomaly Descriptoré€šè¿‡å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨å¢å¼ºè§†è§‰å’Œæ–‡æœ¬åµŒå…¥çš„ç›¸å…³æ€§ã€‚</li>
<li>æ¡†æ¶ä½¿ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„è·¨æ³¨æ„åŠ›æ¨¡å—å’Œæ–‡æœ¬åˆ°å›¾åƒçš„è·¨æ³¨æ„åŠ›æ¨¡å—è·å–æ¨¡æ€ç‰¹å®šåµŒå…¥ã€‚</li>
<li>CLIPçš„é¢„è®­ç»ƒæ•°æ®è¡¨ç¤ºè¢«é€‚åº”äºç›®æ ‡æ•°æ®ã€‚</li>
<li>åœ¨VisAå’ŒMVTEC-ADæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8087ec4baf0291768c7339091318cce6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d485043d745eff6b51664b1cf5b073.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db1484be2f1156f502eee7dad86a70d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d32c371309bcf7833f77dc50465a696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebde84011e7168f9c9ee40bd9bb61e6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da19398a6649b3063d6979f5e0920e5b.jpg" align="middle">
</details>




<h2 id="The-broader-spectrum-of-in-context-learning"><a href="#The-broader-spectrum-of-in-context-learning" class="headerlink" title="The broader spectrum of in-context learning"></a>The broader spectrum of in-context learning</h2><p><strong>Authors:Andrew Kyle Lampinen, Stephanie C. Y. Chan, Aaditya K. Singh, Murray Shanahan</strong></p>
<p>The ability of language models to learn a task from a few examples in context has generated substantial interest. Here, we provide a perspective that situates this type of supervised few-shot learning within a much broader spectrum of meta-learned in-context learning. Indeed, we suggest that any distribution of sequences in which context non-trivially decreases loss on subsequent predictions can be interpreted as eliciting a kind of in-context learning. We suggest that this perspective helps to unify the broad set of in-context abilities that language models exhibit $\unicode{x2014}$ such as adapting to tasks from instructions or role play, or extrapolating time series. This perspective also sheds light on potential roots of in-context learning in lower-level processing of linguistic dependencies (e.g. coreference or parallel structures). Finally, taking this perspective highlights the importance of generalization, which we suggest can be studied along several dimensions: not only the ability to learn something novel, but also flexibility in learning from different presentations, and in applying what is learned. We discuss broader connections to past literature in meta-learning and goal-conditioned agents, and other perspectives on learning and adaptation. We close by suggesting that research on in-context learning should consider this broader spectrum of in-context capabilities and types of generalization. </p>
<blockquote>
<p>è¯­è¨€æ¨¡å‹ä»å°‘æ•°å‡ ä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ä»»åŠ¡çš„èƒ½åŠ›å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè§†è§’ï¼Œå°†è¿™ç±»ç›‘ç£å‹å°æ ·æœ¬å­¦ä¹ ç½®äºæ›´å¹¿æ³›çš„å…ƒè¯­å¢ƒå­¦ä¹ èŒƒå›´å†…ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬æå‡ºï¼Œä»»ä½•åˆ†å¸ƒåºåˆ—ï¼Œå…¶ä¸­ä¸Šä¸‹æ–‡å¯¹åç»­é¢„æµ‹çš„æŸå¤±æœ‰éå¹³å‡¡å‡å°‘ï¼Œéƒ½å¯ä»¥è§£é‡Šä¸ºæ¿€å‘äº†ä¸€ç§è¯­å¢ƒå­¦ä¹ ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ä¸ªè§†è§’æœ‰åŠ©äºç»Ÿä¸€è¯­è¨€æ¨¡å‹æ‰€è¡¨ç°å‡ºçš„å¹¿æ³›è¯­å¢ƒèƒ½åŠ›ï¼Œå¦‚æ ¹æ®æŒ‡ä»¤æˆ–è§’è‰²æ‰®æ¼”é€‚åº”ä»»åŠ¡ï¼Œæˆ–å¤–æ¨æ—¶é—´åºåˆ—ç­‰ã€‚è¿™ä¸ªè§†è§’è¿˜æ­ç¤ºäº†è¯­å¢ƒå­¦ä¹ çš„æ½œåœ¨æ ¹æºåœ¨äºè¯­è¨€ä¾èµ–æ€§çš„ä½çº§å¤„ç†ï¼ˆä¾‹å¦‚æ ¸å¿ƒå¼•ç”¨æˆ–å¹¶è¡Œç»“æ„ï¼‰ã€‚æœ€åï¼Œä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œå¼ºè°ƒäº†æ³›åŒ–çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬è®¤ä¸ºæ³›åŒ–å¯ä»¥ä»å¤šä¸ªç»´åº¦è¿›è¡Œç ”ç©¶ï¼šä¸ä»…æ˜¯å­¦ä¹ æ–°äº‹ç‰©çš„èƒ½åŠ›ï¼Œè¿˜åŒ…æ‹¬ä»ä¸åŒè¡¨ç°ä¸­å­¦ä¹ ä»¥åŠåº”ç”¨æ‰€å­¦å†…å®¹çš„çµæ´»æ€§ã€‚æˆ‘ä»¬è®¨è®ºäº†ä¸å…ƒå­¦ä¹ å’Œç›®æ ‡æ¡ä»¶ä»£ç†çš„è¿‡å»æ–‡çŒ®çš„æ›´å¹¿æ³›è”ç³»ï¼Œä»¥åŠå­¦ä¹ å’Œé€‚åº”çš„å…¶ä»–è§†è§’ã€‚æœ€åï¼Œæˆ‘ä»¬å»ºè®®å¯¹è¯­å¢ƒå­¦ä¹ çš„ç ”ç©¶åº”è€ƒè™‘æ›´å¹¿æ³›çš„è¯­å¢ƒèƒ½åŠ›å’Œæ³›åŒ–ç±»å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03782v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€æ¨¡å‹ä»å°‘æ•°å‡ ä¸ªä¾‹å­ä¸­å­¦ä¹ ä»»åŠ¡çš„æ½œåŠ›ï¼Œå¹¶å°†å…¶ç½®äºæ›´å¹¿æ³›çš„å…ƒå­¦ä¹ è¯­å¢ƒä¸­ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä»»ä½•é€šè¿‡ä¸Šä¸‹æ–‡å‡å°‘åç»­é¢„æµ‹æŸå¤±çš„åºåˆ—åˆ†å¸ƒéƒ½å¯ä»¥è¢«è§£é‡Šä¸ºä¸€ç§è¯­å¢ƒå­¦ä¹ ã€‚è¿™ç§è§†è§’æœ‰åŠ©äºç»Ÿä¸€è¯­è¨€æ¨¡å‹å±•ç°çš„å„ç§è¯­å¢ƒèƒ½åŠ›ï¼Œå¦‚ä»æŒ‡ä»¤æˆ–è§’è‰²æ‰®æ¼”ä¸­é€‚åº”ä»»åŠ¡ï¼Œæˆ–å¤–æ¨æ—¶é—´åºåˆ—ç­‰ã€‚åŒæ—¶ï¼Œè¯¥è§†è§’ä¹Ÿæ­ç¤ºäº†è¯­å¢ƒå­¦ä¹ çš„æ½œåœ¨æ ¹æºåœ¨äºè¯­è¨€ä¾èµ–æ€§çš„ä½çº§å¤„ç†ï¼Œå¹¶å¼ºè°ƒäº†æ³›åŒ–çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬å­¦ä¹ æ–°äº‹ç‰©çš„èƒ½åŠ›ã€ä»ä¸åŒå‘ˆç°æ–¹å¼ä¸­å­¦ä¹ çš„çµæ´»æ€§ä»¥åŠåº”ç”¨æ‰€å­¦å†…å®¹çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä»å°‘æ•°å‡ ä¸ªä¾‹å­ä¸­å­¦ä¹ ä»»åŠ¡çš„æ½œåŠ›å¼•å‘äº†å¹¿æ³›å…³æ³¨ã€‚</li>
<li>è¯­å¢ƒå­¦ä¹ å¯ä»¥çœ‹ä½œæ˜¯å…ƒå­¦ä¹ çš„ä¸€ç§å½¢å¼ï¼Œé€‚ç”¨äºå¹¿æ³›çš„åºåˆ—åˆ†å¸ƒã€‚</li>
<li>è¯­å¢ƒå­¦ä¹ çš„è§†è§’æœ‰åŠ©äºç†è§£è¯­è¨€æ¨¡å‹çš„å„ç§èƒ½åŠ›ï¼Œå¦‚é€‚åº”ä»»åŠ¡ã€å¤–æ¨æ—¶é—´åºåˆ—ç­‰ã€‚</li>
<li>è¯­å¢ƒå­¦ä¹ çš„æ½œåœ¨æ ¹æºå¯èƒ½åœ¨äºè¯­è¨€ä¾èµ–æ€§çš„ä½çº§å¤„ç†ã€‚</li>
<li>æ³›åŒ–åœ¨è¯­è¨€å­¦ä¹ ä¸­éå¸¸é‡è¦ï¼ŒåŒ…æ‹¬å­¦ä¹ æ–°äº‹ç‰©ã€ä¸åŒå‘ˆç°æ–¹å¼çš„çµæ´»æ€§ä»¥åŠåº”ç”¨æ‰€å­¦å†…å®¹çš„èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†ä¸å…ƒå­¦ä¹ å’Œç›®æ ‡æ¡ä»¶ä»£ç†ç›¸å…³çš„è¿‡å»æ–‡çŒ®çš„å¹¿æ³›è”ç³»ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-703e5825975de06f71e35f45a805bf17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e36a9b769dfd8c24a4aab4510be7d0f.jpg" align="middle">
</details>




<h2 id="UniVAD-A-Training-free-Unified-Model-for-Few-shot-Visual-Anomaly-Detection"><a href="#UniVAD-A-Training-free-Unified-Model-for-Few-shot-Visual-Anomaly-Detection" class="headerlink" title="UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly   Detection"></a>UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly   Detection</h2><p><strong>Authors:Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang</strong></p>
<p>Visual Anomaly Detection (VAD) aims to identify abnormal samples in images that deviate from normal patterns, covering multiple domains, including industrial, logical, and medical fields. Due to the domain gaps between these fields, existing VAD methods are typically tailored to each domain, with specialized detection techniques and model architectures that are difficult to generalize across different domains. Moreover, even within the same domain, current VAD approaches often follow a â€œone-category-one-modelâ€ paradigm, requiring large amounts of normal samples to train class-specific models, resulting in poor generalizability and hindering unified evaluation across domains. To address this issue, we propose a generalized few-shot VAD method, UniVAD, capable of detecting anomalies across various domains, such as industrial, logical, and medical anomalies, with a training-free unified model. UniVAD only needs few normal samples as references during testing to detect anomalies in previously unseen objects, without training on the specific domain. Specifically, UniVAD employs a Contextual Component Clustering ($C^3$) module based on clustering and vision foundation models to segment components within the image accurately, and leverages Component-Aware Patch Matching (CAPM) and Graph-Enhanced Component Modeling (GECM) modules to detect anomalies at different semantic levels, which are aggregated to produce the final detection result. We conduct experiments on nine datasets spanning industrial, logical, and medical fields, and the results demonstrate that UniVAD achieves state-of-the-art performance in few-shot anomaly detection tasks across multiple domains, outperforming domain-specific anomaly detection models. The code will be made publicly available. </p>
<blockquote>
<p>è§†è§‰å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­çš„å¼‚å¸¸æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬åç¦»äº†æ­£å¸¸æ¨¡å¼ï¼Œæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—é¢†åŸŸã€‚ç”±äºè¿™äº›é¢†åŸŸä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ï¼Œç°æœ‰çš„VADæ–¹æ³•é€šå¸¸é’ˆå¯¹æ¯ä¸ªé¢†åŸŸè¿›è¡Œå®šåˆ¶ï¼Œå…·æœ‰ä¸“é—¨åŒ–çš„æ£€æµ‹æŠ€æœ¯å’Œæ¨¡å‹æ¶æ„ï¼Œéš¾ä»¥åœ¨ä¸åŒé¢†åŸŸä¸­æ¨å¹¿ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨åŒä¸€é¢†åŸŸå†…ï¼Œå½“å‰çš„VADæ–¹æ³•é€šå¸¸éµå¾ªâ€œä¸€ç±»ä¸€æ¨¡å‹â€çš„æ¨¡å¼ï¼Œéœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬æ¥è®­ç»ƒç‰¹å®šç±»åˆ«çš„æ¨¡å‹ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›è¾ƒå·®ï¼Œé˜»ç¢äº†è·¨é¢†åŸŸçš„ç»Ÿä¸€è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„å°‘æ ·æœ¬VADæ–¹æ³•UniVADï¼Œèƒ½å¤Ÿæ£€æµ‹å„ç§é¢†åŸŸçš„å¼‚å¸¸ï¼Œå¦‚å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—å¼‚å¸¸ï¼Œé‡‡ç”¨æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ¨¡å‹ã€‚UniVADåœ¨æµ‹è¯•æ—¶åªéœ€è¦å°‘æ•°æ­£å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒï¼Œå³å¯æ£€æµ‹ä»¥å‰æœªè§è¿‡çš„å¯¹è±¡ä¸­çš„å¼‚å¸¸ï¼Œè€Œæ— éœ€åœ¨ç‰¹å®šé¢†åŸŸä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼ŒUniVADé‡‡ç”¨åŸºäºèšç±»å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç»„ä»¶èšç±»ï¼ˆC^3ï¼‰æ¨¡å—ï¼Œç²¾ç¡®åˆ†å‰²å›¾åƒå†…çš„ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨ç»„ä»¶æ„ŸçŸ¥è¡¥ä¸åŒ¹é…ï¼ˆCAPMï¼‰å’Œå›¾å¢å¼ºç»„ä»¶å»ºæ¨¡ï¼ˆGECMï¼‰æ¨¡å—åœ¨ä¸åŒçš„è¯­ä¹‰çº§åˆ«æ£€æµ‹å¼‚å¸¸ï¼Œè¿™äº›å¼‚å¸¸è¢«èšåˆä»¥äº§ç”Ÿæœ€ç»ˆçš„æ£€æµ‹ç»“æœã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–äº†å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—é¢†åŸŸï¼Œç»“æœè¡¨æ˜UniVADåœ¨è·¨å¤šä¸ªé¢†åŸŸçš„å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºé¢†åŸŸç‰¹å®šçš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03342v2">PDF</a> project page: <a target="_blank" rel="noopener" href="https://uni-vad.github.io/">https://uni-vad.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸Šä¸‹æ–‡ç»„ä»¶èšç±»ï¼ˆCÂ³æ¨¡å—ï¼‰çš„UniVADæ–¹æ³•èƒ½å¤Ÿå®ç°è·¨é¢†åŸŸçš„è§†è§‰å¼‚å¸¸æ£€æµ‹ã€‚å®ƒåªéœ€å°‘é‡çš„æ­£å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒï¼Œå³å¯æ£€æµ‹æœªçŸ¥å¯¹è±¡ä¸­çš„å¼‚å¸¸ï¼Œæ— éœ€é’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ç»„ä»¶æ„ŸçŸ¥è¡¥ä¸åŒ¹é…ï¼ˆCAPMï¼‰å’Œå›¾å¢å¼ºç»„ä»¶å»ºæ¨¡ï¼ˆGECMï¼‰æ¨¡å—ï¼ŒUniVADèƒ½å¤Ÿåœ¨ä¸åŒçš„è¯­ä¹‰çº§åˆ«ä¸Šæ£€æµ‹å¼‚å¸¸ï¼Œå¹¶èšåˆç»“æœä»¥äº§ç”Ÿæœ€ç»ˆçš„æ£€æµ‹ç»“æœã€‚åœ¨è·¨è¶Šå·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—é¢†åŸŸçš„ä¹ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVADåœ¨è·¨é¢†åŸŸçš„å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºç‰¹å®šé¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniVADæ–¹æ³•æ—¨åœ¨è§£å†³è§†è§‰å¼‚å¸¸æ£€æµ‹ä¸­çš„è·¨é¢†åŸŸé—®é¢˜ï¼Œè¦†ç›–å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—ç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
<li>ç°æœ‰VADæ–¹æ³•é€šå¸¸é’ˆå¯¹æ¯ä¸ªé¢†åŸŸè¿›è¡Œå®šåˆ¶ï¼Œè€ŒUniVADåˆ™æå‡ºä¸€ç§é€šç”¨çš„å°‘æ ·æœ¬è§£å†³æ–¹æ¡ˆã€‚</li>
<li>UniVADåˆ©ç”¨CÂ³æ¨¡å—è¿›è¡Œä¸Šä¸‹æ–‡ç»„ä»¶èšç±»ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ†å‰²å›¾åƒå†…çš„ç»„ä»¶ã€‚</li>
<li>é€šè¿‡CAPMå’ŒGECMæ¨¡å—ï¼ŒUniVADèƒ½å¤Ÿåœ¨ä¸åŒè¯­ä¹‰çº§åˆ«ä¸Šæ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>UniVADä»…éœ€å°‘é‡æ­£å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒï¼Œå³å¯æ£€æµ‹æœªçŸ¥å¯¹è±¡ä¸­çš„å¼‚å¸¸ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUniVADåœ¨å¤šä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šå®ç°äº†è·¨é¢†åŸŸçš„å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b66eb13e9d4e68305daf08043f3b5867.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1258690da287beb7bae925ef19f37a4a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fedf9887d8e4a560ad2a7c633088d83d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fcaca991f91dcc186d626421d880a2a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f0caac08fa6b2e27412cb631a7ec29f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b8e53d4a9bf60ec112832999ef3cbeed.jpg" align="middle">
</details>




<h2 id="Intent-driven-In-context-Learning-for-Few-shot-Dialogue-State-Tracking"><a href="#Intent-driven-In-context-Learning-for-Few-shot-Dialogue-State-Tracking" class="headerlink" title="Intent-driven In-context Learning for Few-shot Dialogue State Tracking"></a>Intent-driven In-context Learning for Few-shot Dialogue State Tracking</h2><p><strong>Authors:Zihao Yi, Zhe Xu, Ying Shen</strong></p>
<p>Dialogue state tracking (DST) plays an essential role in task-oriented dialogue systems. However, userâ€™s input may contain implicit information, posing significant challenges for DST tasks. Additionally, DST data includes complex information, which not only contains a large amount of noise unrelated to the current turn, but also makes constructing DST datasets expensive. To address these challenges, we introduce Intent-driven In-context Learning for Few-shot DST (IDIC-DST). By extracting userâ€™s intent, we propose an Intent-driven Dialogue Information Augmentation module to augment the dialogue information, which can track dialogue states more effectively. Moreover, we mask noisy information from DST data and rewrite userâ€™s input in the Intent-driven Examples Retrieval module, where we retrieve similar examples. We then utilize a pre-trained large language model to update the dialogue state using the augmented dialogue information and examples. Experimental results demonstrate that IDIC-DST achieves state-of-the-art performance in few-shot settings on MultiWOZ 2.1 and MultiWOZ 2.4 datasets. </p>
<blockquote>
<p>å¯¹è¯çŠ¶æ€è·Ÿè¸ªï¼ˆDSTï¼‰åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ç³»ç»Ÿä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç„¶è€Œï¼Œç”¨æˆ·çš„è¾“å…¥å¯èƒ½åŒ…å«éšå«ä¿¡æ¯ï¼Œç»™DSTä»»åŠ¡å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼ŒDSTæ•°æ®åŒ…å«å¤æ‚ä¿¡æ¯ï¼Œä¸ä»…åŒ…å«å¤§é‡ä¸å½“å‰å›åˆæ— å…³çš„å™ªå£°ï¼Œè€Œä¸”æ„å»ºDSTæ•°æ®é›†çš„æˆæœ¬ä¹Ÿå¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºå°‘é‡DSTçš„æ„å›¾é©±åŠ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIDIC-DSTï¼‰ã€‚é€šè¿‡æå–ç”¨æˆ·çš„æ„å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ„å›¾é©±åŠ¨å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼Œä»¥å¢å¼ºå¯¹è¯ä¿¡æ¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è·Ÿè¸ªå¯¹è¯çŠ¶æ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»DSTæ•°æ®ä¸­å±è”½äº†å˜ˆæ‚çš„ä¿¡æ¯ï¼Œå¹¶åœ¨æ„å›¾é©±åŠ¨çš„ä¾‹å­æ£€ç´¢æ¨¡å—ä¸­é‡å†™äº†ç”¨æˆ·çš„è¾“å…¥ï¼Œæˆ‘ä»¬æ£€ç´¢äº†ç±»ä¼¼çš„ä¾‹å­ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨å¢å¼ºçš„å¯¹è¯ä¿¡æ¯å’Œä¾‹å­æ¥æ›´æ–°å¯¹è¯çŠ¶æ€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°‘é‡è®¾ç½®è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03270v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­çš„é‡è¦è§’è‰²ï¼Œå¹¶æŒ‡å‡ºäº†ç”¨æˆ·è¾“å…¥ä¸­éšå«ä¿¡æ¯å’Œæ•°æ®å¤æ‚æ€§å¯¹DSTä»»åŠ¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ„å›¾é©±åŠ¨çš„å†…è¯­å¢ƒå­¦ä¹ ï¼ˆIDIC-DSTï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ç”¨æˆ·æ„å›¾ï¼Œæ„å»ºäº†ä¸€ä¸ªæ„å›¾é©±åŠ¨å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—ï¼Œä»¥æ›´æœ‰æ•ˆåœ°è¿½è¸ªå¯¹è¯çŠ¶æ€ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•è¿˜é€šè¿‡æ©å™ªå¤„ç†å’Œé‡å†™ç”¨æˆ·è¾“å…¥çš„æ–¹å¼ä¼˜åŒ–äº†æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZ 2.1å’ŒMultiWOZ 2.4æ•°æ®é›†ä¸Šçš„å°æ ·æœ¬è®¾ç½®ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹è¯çŠ¶æ€è¿½è¸ªï¼ˆDSTï¼‰åœ¨ä»»åŠ¡å¯¼å‘å‹å¯¹è¯ç³»ç»Ÿä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç”¨æˆ·è¾“å…¥ä¸­çš„éšå«ä¿¡æ¯å’Œæ•°æ®å¤æ‚æ€§å¯¹DSTä»»åŠ¡å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>IDIC-DSTæ–¹æ³•é€šè¿‡æ„å›¾é©±åŠ¨çš„å†…è¯­å¢ƒå­¦ä¹ æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>æ„å›¾é©±åŠ¨å¯¹è¯ä¿¡æ¯å¢å¼ºæ¨¡å—èƒ½æ›´æœ‰æ•ˆåœ°è¿½è¸ªå¯¹è¯çŠ¶æ€ã€‚</li>
<li>IDIC-DSTé€šè¿‡æ©å™ªå¤„ç†å’Œé‡å†™ç”¨æˆ·è¾“å…¥çš„æ–¹å¼ä¼˜åŒ–äº†æ•°æ®é›†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIDIC-DSTåœ¨MultiWOZæ•°æ®é›†ä¸Šçš„å°æ ·æœ¬è®¾ç½®ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8742c6b8028b15b6244c52f761b6bde9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-947f300819b2f915778e4b9639623290.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1cf68b86876326142a7ddfc117835ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf7ba27ee5a1f9a8924ebb587f9c4db.jpg" align="middle">
</details>




<h2 id="Multi-Level-Correlation-Network-For-Few-Shot-Image-Classification"><a href="#Multi-Level-Correlation-Network-For-Few-Shot-Image-Classification" class="headerlink" title="Multi-Level Correlation Network For Few-Shot Image Classification"></a>Multi-Level Correlation Network For Few-Shot Image Classification</h2><p><strong>Authors:Yunkai Dang, Min Zhang, Zhengyu Chen, Xinliang Zhang, Zheng Wang, Meijun Sun, Donglin Wang</strong></p>
<p>Few-shot image classification(FSIC) aims to recognize novel classes given few labeled images from base classes. Recent works have achieved promising classification performance, especially for metric-learning methods, where a measure at only image feature level is usually used. In this paper, we argue that measure at such a level may not be effective enough to generalize from base to novel classes when using only a few images. Instead, a multi-level descriptor of an image is taken for consideration in this paper. We propose a multi-level correlation network (MLCN) for FSIC to tackle this problem by effectively capturing local information. Concretely, we present the self-correlation module and cross-correlation module to learn the semantic correspondence relation of local information based on learned representations. Moreover, we propose a pattern-correlation module to capture the pattern of fine-grained images and find relevant structural patterns between base classes and novel classes. Extensive experiments and analysis show the effectiveness of our proposed method on four widely-used FSIC benchmarks. The code for our approach is available at: <a target="_blank" rel="noopener" href="https://github.com/Yunkai696/MLCN">https://github.com/Yunkai696/MLCN</a>. </p>
<blockquote>
<p>å°æ ·å›¾åƒåˆ†ç±»ï¼ˆFSICï¼‰çš„ç›®æ ‡æ˜¯è¯†åˆ«æ–°çš„ç±»åˆ«ï¼ŒåŒæ—¶ä»…åŸºäºåŸºç¡€ç±»åˆ«çš„å°‘é‡æ ‡è®°å›¾åƒã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åº¦é‡å­¦ä¹ æ–¹æ³•ï¼Œé€šå¸¸åªä½¿ç”¨å›¾åƒç‰¹å¾å±‚é¢çš„åº¦é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºä»…ä½¿ç”¨å°‘é‡å›¾åƒæ—¶ï¼Œä»…åœ¨å¦‚æ­¤å±‚é¢ä¸Šçš„åº¦é‡å¯èƒ½ä¸è¶³ä»¥ä»åŸºç¡€ç±»åˆ«æ¨å¹¿åˆ°æ–°ç±»åˆ«ã€‚ç›¸åï¼Œæœ¬æ–‡è€ƒè™‘äº†ä¸€å¼ å›¾åƒçš„å¤šå±‚æ¬¡æè¿°ç¬¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºFSICçš„å¤šå±‚æ¬¡å…³è”ç½‘ç»œï¼ˆMLCNï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°æ•è·å±€éƒ¨ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†è‡ªå…³è”æ¨¡å—å’Œäº¤å‰å…³è”æ¨¡å—ï¼ŒåŸºäºå­¦ä¹ åˆ°çš„è¡¨ç¤ºæ¥å­¦ä¹ å±€éƒ¨ä¿¡æ¯çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ¨¡å¼å…³è”æ¨¡å—ï¼Œä»¥æ•è·ç»†ç²’åº¦å›¾åƒçš„æ¨¡å¼ï¼Œå¹¶åœ¨åŸºç¡€ç±»åˆ«å’Œæ–°ç±»åˆ«ä¹‹é—´æ‰¾åˆ°ç›¸å…³çš„ç»“æ„æ¨¡å¼ã€‚å¹¿æ³›çš„å®éªŒå’Œåˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„FSICåŸºå‡†æµ‹è¯•ä¸Šéƒ½éªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Yunkai696/MLCN%E3%80%82">https://github.com/Yunkai696/MLCNã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03159v1">PDF</a> </p>
<p><strong>Summary</strong><br>å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆFSICï¼‰æ—¨åœ¨é€šè¿‡åŸºç¡€ç±»åˆ«çš„å°‘é‡æ ‡è®°å›¾åƒæ¥è¯†åˆ«æ–°å‹ç±»åˆ«ã€‚æœ¬æ–‡æå‡ºä¸€ç§å¤šå±‚æ¬¡å…³è”ç½‘ç»œï¼ˆMLCNï¼‰æ¥è§£å†³è¯¥é—®é¢˜ï¼Œé€šè¿‡æ•æ‰å±€éƒ¨ä¿¡æ¯æ¥å®ç°æœ‰æ•ˆçš„ç‰¹å¾æå–ã€‚è®ºæ–‡å¼•å…¥äº†è‡ªå…³è”æ¨¡å—å’Œè·¨å…³è”æ¨¡å—æ¥å­¦ä¹ åŸºäºå­¦ä¹ è¡¨ç¤ºçš„å±€éƒ¨ä¿¡æ¯çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¹¶æå‡ºæ¨¡å¼å…³è”æ¨¡å—æ¥æ•æ‰ç»†ç²’åº¦å›¾åƒçš„æ¨¡å¼ï¼Œæ‰¾åˆ°åŸºç¡€ç±»åˆ«å’Œæ–°å‹ç±»åˆ«ä¹‹é—´çš„ç›¸å…³ç»“æ„æ¨¡å¼ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„FSICåŸºå‡†æµ‹è¯•é›†ä¸Šæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ï¼ˆFSICï¼‰æ—¨åœ¨åˆ©ç”¨åŸºç¡€ç±»åˆ«çš„å°‘é‡æ ‡è®°å›¾åƒè¯†åˆ«æ–°å‹ç±»åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åº¦é‡å­¦ä¹ æ–¹é¢å–å¾—è¾ƒå¥½æˆç»©ï¼Œä½†ä»…é€šè¿‡å›¾åƒç‰¹å¾çº§åˆ«çš„åº¦é‡å¯èƒ½ä¸å¤Ÿæœ‰æ•ˆã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šå±‚æ¬¡å…³è”ç½‘ç»œï¼ˆMLCNï¼‰æ¥è§£å†³è¯¥é—®é¢˜ï¼Œå…³æ³¨å›¾åƒçš„å¤šå±‚æ¬¡æè¿°ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥è‡ªå…³è”æ¨¡å—å’Œè·¨å…³è”æ¨¡å—æ¥å­¦ä¹ å±€éƒ¨ä¿¡æ¯çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚</li>
<li>æå‡ºæ¨¡å¼å…³è”æ¨¡å—æ¥æ•æ‰ç»†ç²’åº¦å›¾åƒçš„æ¨¡å¼ï¼Œå¹¶æ‰¾åˆ°åŸºç¡€ç±»åˆ«å’Œæ–°å‹ç±»åˆ«ä¹‹é—´çš„ç»“æ„æ¨¡å¼å…³è”ã€‚</li>
<li>åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„FSICåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35fd850330316e7d9f3c08940c047c9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9e6a2dd9fe4b6f2479fae2831e3e6ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71e70f09674df88eae90af483efb10d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ffa3a35fa5490e44a628172952e283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5069e5e8d2cda86043e08a541e080ebb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb0a59238f2038fb53cf3388d841ad13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeb9c1e3f086cf55bf2807be8e3562a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59a158cca3056c0c63f5fd816e487199.jpg" align="middle">
</details>




<h2 id="Few-Shot-Learning-with-Adaptive-Weight-Masking-in-Conditional-GANs"><a href="#Few-Shot-Learning-with-Adaptive-Weight-Masking-in-Conditional-GANs" class="headerlink" title="Few-Shot Learning with Adaptive Weight Masking in Conditional GANs"></a>Few-Shot Learning with Adaptive Weight Masking in Conditional GANs</h2><p><strong>Authors:Jiacheng Hu, Zhen Qi, Jianjun Wei, Jiajing Chen, Runyuan Bao, Xinyu Qiu</strong></p>
<p>Deep learning has revolutionized various fields, yet its efficacy is hindered by overfitting and the requirement of extensive annotated data, particularly in few-shot learning scenarios where limited samples are available. This paper introduces a novel approach to few-shot learning by employing a Residual Weight Masking Conditional Generative Adversarial Network (RWM-CGAN) for data augmentation. The proposed model integrates residual units within the generator to enhance network depth and sample quality, coupled with a weight mask regularization technique in the discriminator to improve feature learning from small-sample categories. This method addresses the core issues of robustness and generalization in few-shot learning by providing a controlled and clear augmentation of the sample space. Extensive experiments demonstrate that RWM-CGAN not only expands the sample space effectively but also enriches the diversity and quality of generated samples, leading to significant improvements in detection and classification accuracy on public datasets. The paper contributes to the advancement of few-shot learning by offering a practical solution to the challenges posed by data scarcity and the need for rapid generalization to new tasks or categories. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²ç»å¯¹å„é¢†åŸŸäº§ç”Ÿäº†é©å‘½æ€§çš„å½±å“ï¼Œä½†åœ¨å°æ ·å­¦ä¹ åœºæ™¯ä¸­ï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°è¿‡æ‹Ÿåˆå’Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é‡‡ç”¨æ®‹å·®æƒé‡æ©è”½æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆRWM-CGANï¼‰è¿›è¡Œæ•°æ®å¢å¼ºçš„å°‘æ ·æœ¬å­¦ä¹ æ–°æ–¹æ³•ã€‚æ‰€ææ¨¡å‹åœ¨ç”Ÿæˆå™¨ä¸­é›†æˆæ®‹å·®å•å…ƒï¼Œä»¥æé«˜ç½‘ç»œæ·±åº¦å’Œæ ·æœ¬è´¨é‡ï¼ŒåŒæ—¶åœ¨é‰´åˆ«å™¨ä¸­é‡‡ç”¨æƒé‡æ©è”½æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥æé«˜å°æ ·æœ¬ç±»åˆ«çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡æä¾›å¯æ§ä¸”æ¸…æ™°çš„æ ·æœ¬ç©ºé—´æ‰©å……ï¼Œè§£å†³äº†å°æ ·å­¦ä¹ ä¸­çš„é²æ£’æ€§å’Œæ³›åŒ–æ€§é—®é¢˜æ ¸å¿ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRWM-CGANä¸ä»…æœ‰æ•ˆåœ°æ‰©å¤§äº†æ ·æœ¬ç©ºé—´ï¼Œè¿˜ä¸°å¯Œäº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ£€æµ‹å’Œåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚æœ¬æ–‡ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œå¿«é€Ÿæ¨å¹¿åˆ°æ–°ä»»åŠ¡æˆ–ç±»åˆ«ç­‰æŒ‘æˆ˜æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å°æ ·å­¦ä¹ çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03105v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ å·²å¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼Œä½†åœ¨å°æ ·ä¾‹å­¦ä¹ åœºæ™¯ä¸‹å­˜åœ¨è¿‡æ‹Ÿåˆå’Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡‡ç”¨æ®‹å·®æƒé‡æ©è†œæ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆRWM-CGANï¼‰è¿›è¡Œå°æ ·ä¾‹å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆå™¨ä¸­çš„æ®‹å·®å•å…ƒå¢å¼ºç½‘ç»œæ·±åº¦å’Œæ ·æœ¬è´¨é‡ï¼Œé€šè¿‡åˆ¤åˆ«å™¨ä¸­çš„æƒé‡æ©è†œæ­£åˆ™åŒ–æŠ€æœ¯æ”¹è¿›å°æ ·æœ¬ç±»åˆ«çš„ç‰¹å¾å­¦ä¹ ã€‚è¯¥æ–¹æ³•è§£å†³äº†å°æ ·ä¾‹å­¦ä¹ ä¸­é²æ£’æ€§å’Œæ³›åŒ–æ€§çš„æ ¸å¿ƒé—®é¢˜ï¼Œé€šè¿‡æ§åˆ¶æ¸…æ™°çš„æ ·æœ¬ç©ºé—´æ‰©å……æ¥å®ç°ã€‚å®éªŒè¡¨æ˜ï¼ŒRWM-CGANä¸ä»…æœ‰æ•ˆåœ°æ‰©å……äº†æ ·æœ¬ç©ºé—´ï¼Œè¿˜æé«˜äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„æ£€æµ‹å’Œåˆ†ç±»ç²¾åº¦æ˜¾è‘—æé«˜ã€‚æœ¬æ–‡ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œå¿«é€Ÿæ³›åŒ–åˆ°æ–°ä»»åŠ¡æˆ–ç±»åˆ«æ‰€å¸¦æ¥çš„æŒ‘æˆ˜æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆï¼Œä¸ºå°æ ·ä¾‹å­¦ä¹ é¢†åŸŸçš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨å°æ ·ä¾‹å­¦ä¹ åœºæ™¯ä¸­é¢ä¸´è¿‡æ‹Ÿåˆå’Œæ ‡æ³¨æ•°æ®éœ€æ±‚çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºRWM-CGANçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ®‹å·®å•å…ƒå’Œæƒé‡æ©è†œæ­£åˆ™åŒ–æŠ€æœ¯æ¥è§£å†³å°æ ·ä¾‹å­¦ä¹ ä¸­çš„é—®é¢˜ã€‚</li>
<li>RWM-CGANèƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰©å……æ ·æœ¬ç©ºé—´ï¼Œæé«˜ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„æ£€æµ‹å’Œåˆ†ç±»ç²¾åº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºå’Œå¿«é€Ÿæ³›åŒ–åˆ°æ–°ä»»åŠ¡æˆ–ç±»åˆ«æ‰€å¸¦æ¥çš„æŒ‘æˆ˜æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æœ¬æ–‡å¯¹å°æ ·ä¾‹å­¦ä¹ é¢†åŸŸçš„å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-252786a598c1b975d0500f413f8ea6ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-63d782b2ce5fc208e80f853c893c9b93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd09ce23f0a5602ca5fda6a170870e86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-212cd64da07652f1d16f7b1689678909.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5da1851c68a4eb3fe67f49f456a57e3.jpg" align="middle">
</details>




<h2 id="Is-Foreground-Prototype-Sufficient-Few-Shot-Medical-Image-Segmentation-with-Background-Fused-Prototype"><a href="#Is-Foreground-Prototype-Sufficient-Few-Shot-Medical-Image-Segmentation-with-Background-Fused-Prototype" class="headerlink" title="Is Foreground Prototype Sufficient? Few-Shot Medical Image Segmentation   with Background-Fused Prototype"></a>Is Foreground Prototype Sufficient? Few-Shot Medical Image Segmentation   with Background-Fused Prototype</h2><p><strong>Authors:Song Tang, Chunxiao Zu, Wenxin Su, Yuan Dong, Mao Ye, Yan Gan, Xiatian Zhu</strong></p>
<p>Few-shot Semantic Segmentation(FSS)aim to adapt a pre-trained model to new classes with as few as a single labeled training sample per class. The existing prototypical work used in natural image scenarios biasedly focus on capturing foregroundâ€™s discrimination while employing a simplistic representation for background, grounded on the inherent observation separation between foreground and background. However, this paradigm is not applicable to medical images where the foreground and background share numerous visual features, necessitating a more detailed description for background. In this paper, we present a new pluggable Background-fused prototype(Bro)approach for FSS in medical images. Instead of finding a commonality of background subjects in support image, Bro incorporates this background with two pivot designs. Specifically, Feature Similarity Calibration(FeaC)initially reduces noise in the support image by employing feature cross-attention with the query image. Subsequently, Hierarchical Channel Adversarial Attention(HiCA)merges the background into comprehensive prototypes. We achieve this by a channel groups-based attention mechanism, where an adversarial Mean-Offset structure encourages a coarse-to-fine fusion. Extensive experiments show that previous state-of-the-art methods, when paired with Bro, experience significant performance improvements. This demonstrates a more integrated way to represent backgrounds specifically for medical image. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨å°†é¢„è®­ç»ƒæ¨¡å‹é€‚åº”æ–°ç±»åˆ«ï¼Œæ¯ç±»åªéœ€ä¸€ä¸ªæ ‡è®°çš„è®­ç»ƒæ ·æœ¬ã€‚ç›®å‰è‡ªç„¶å›¾åƒåœºæ™¯ä¸­ä½¿ç”¨çš„åŸå‹å·¥ä½œåå‘äºæ•æ‰å‰æ™¯çš„è¾¨åˆ«åŠ›ï¼ŒåŒæ—¶é‡‡ç”¨ç®€å•çš„èƒŒæ™¯è¡¨ç¤ºï¼ŒåŸºäºå‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„å›ºæœ‰è§‚å¯Ÿåˆ†ç¦»ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨¡å¼ä¸é€‚ç”¨äºåŒ»å­¦å›¾åƒï¼Œå› ä¸ºå‰æ™¯å’ŒèƒŒæ™¯å…±äº«è®¸å¤šè§†è§‰ç‰¹å¾ï¼Œéœ€è¦å¯¹èƒŒæ™¯è¿›è¡Œæ›´è¯¦ç»†çš„æè¿°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯æ’å…¥å¼èƒŒæ™¯èåˆåŸå‹ï¼ˆBroï¼‰æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒçš„FSSã€‚Broæ–¹æ³•å¹¶æ²¡æœ‰åœ¨æ”¯æŒå›¾åƒä¸­å¯»æ‰¾èƒŒæ™¯ä¸»é¢˜çš„å…±æ€§ï¼Œè€Œæ˜¯ç»“åˆäº†èƒŒæ™¯å’Œä¸¤ä¸ªå…³é”®è®¾è®¡ã€‚å…·ä½“æ¥è¯´ï¼Œç‰¹å¾ç›¸ä¼¼æ€§æ ¡å‡†ï¼ˆFeaCï¼‰é¦–å…ˆé€šè¿‡é‡‡ç”¨ä¸æŸ¥è¯¢å›¾åƒçš„ç‰¹å¾äº¤å‰æ³¨æ„åŠ›æ¥å‡å°‘æ”¯æŒå›¾åƒä¸­çš„å™ªå£°ã€‚éšåï¼Œå±‚æ¬¡åŒ–é€šé“å¯¹æŠ—æ€§æ³¨æ„åŠ›ï¼ˆHiCAï¼‰å°†èƒŒæ™¯åˆå¹¶åˆ°ç»¼åˆåŸå‹ä¸­ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºé€šé“ç»„çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­å¯¹æŠ—æ€§Mean-Offsetç»“æ„é¼“åŠ±ä»ç²—ç•¥åˆ°ç²¾ç»†çš„èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå½“ä¸Broç›¸ç»“åˆæ—¶ï¼Œå®ƒä»¬çš„æ€§èƒ½æœ‰äº†æ˜¾è‘—æé«˜ã€‚è¿™è¡¨æ˜äº†ä¸€ç§æ›´é›†æˆçš„æ–¹å¼æ¥è¡¨ç¤ºåŒ»å­¦å›¾åƒä¸­çš„èƒŒæ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02983v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒçš„æ–°å‹çš„å¯æ’å…¥èƒŒæ™¯èåˆæ¨¡å‹ï¼ˆBroï¼‰ã€‚æ­¤æ¨¡å‹æ”¹å–„äº†ç°æœ‰çš„æ¨¡å‹å¯¹èƒŒæ™¯å¤„ç†ä¸å¤Ÿç»†è‡´çš„é—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å¾ç›¸ä¼¼æ€§æ ¡å‡†ï¼ˆFeaCï¼‰å‡å°‘æ”¯æŒå›¾åƒä¸­çš„å™ªå£°ï¼Œå†é€šè¿‡å±‚æ¬¡åŒ–é€šé“å¯¹æŠ—æ³¨æ„åŠ›ï¼ˆHiCAï¼‰å°†èƒŒæ™¯èåˆåˆ°å…¨é¢çš„åŸå‹ä¸­ã€‚å®éªŒç»“æœè¯æ˜ï¼Œç»“åˆäº†Broçš„æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä¸ºåŒ»å­¦å›¾åƒèƒŒæ™¯è¡¨ç¤ºæä¾›äº†æ›´ä¸€ä½“åŒ–çš„æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FSSæ—¨åœ¨ä½¿ç”¨å°‘é‡æ ‡è®°æ ·æœ¬è¿›è¡Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼ŒåŒ»å­¦å›¾åƒé¢†åŸŸä¸­å°¤ä¸ºéœ€è¦ç»†åŒ–èƒŒæ™¯çš„è¡¨è¿°æ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†è‡ªç„¶å›¾åƒæ—¶æ›´å…³æ³¨å‰æ™¯è¯†åˆ«è€Œå¿½è§†èƒŒæ™¯ã€‚ä½†åŒ»å­¦å›¾åƒä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯æœ‰å¾ˆå¤šå…±åŒç‰¹å¾ï¼Œæ‰€ä»¥éœ€è¦æ›´ä¸ºè¯¦å°½åœ°æè¿°èƒŒæ™¯ã€‚</li>
<li>èƒŒæ™¯èåˆæ¨¡å‹ï¼ˆBroï¼‰æ˜¯ä¸€ä¸ªæ–°çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒçš„FSSä»»åŠ¡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b105ad8594cdba1e46ebc883da774072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0646dd7d9384d26b2709d8c5fa130373.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcf9bb34de6b9698da850ad881be9c29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c28c7af3739ac15d99b9ebedb5a8a9f.jpg" align="middle">
</details>




<h2 id="BANER-Boundary-Aware-LLMs-for-Few-Shot-Named-Entity-Recognition"><a href="#BANER-Boundary-Aware-LLMs-for-Few-Shot-Named-Entity-Recognition" class="headerlink" title="BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition"></a>BANER: Boundary-Aware LLMs for Few-Shot Named Entity Recognition</h2><p><strong>Authors:Quanjiang Guo, Yihong Dong, Ling Tian, Zhao Kang, Yu Zhang, Sijie Wang</strong></p>
<p>Despite the recent success of two-stage prototypical networks in few-shot named entity recognition (NER), challenges such as over&#x2F;under-detected false spans in the span detection stage and unaligned entity prototypes in the type classification stage persist. Additionally, LLMs have not proven to be effective few-shot information extractors in general. In this paper, we propose an approach called Boundary-Aware LLMs for Few-Shot Named Entity Recognition to address these issues. We introduce a boundary-aware contrastive learning strategy to enhance the LLMâ€™s ability to perceive entity boundaries for generalized entity spans. Additionally, we utilize LoRAHub to align information from the target domain to the source domain, thereby enhancing adaptive cross-domain classification capabilities. Extensive experiments across various benchmarks demonstrate that our framework outperforms prior methods, validating its effectiveness. In particular, the proposed strategies demonstrate effectiveness across a range of LLM architectures. The code and data are released on <a target="_blank" rel="noopener" href="https://github.com/UESTC-GQJ/BANER">https://github.com/UESTC-GQJ/BANER</a>. </p>
<blockquote>
<p>å°½ç®¡ä¸¤é˜¶æ®µåŸå‹ç½‘ç»œåœ¨å°‘æ ·æœ¬å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ–¹é¢å–å¾—äº†æœ€æ–°æˆåŠŸï¼Œä½†ä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚åœ¨è·¨åº¦æ£€æµ‹é˜¶æ®µçš„è¿‡åº¦æˆ–æœªæ£€æµ‹åˆ°çš„é”™è¯¯è·¨åº¦ä»¥åŠåœ¨ç±»å‹åˆ†ç±»é˜¶æ®µçš„ä¸å¯¹é½å®ä½“åŸå‹ã€‚æ­¤å¤–ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°šæœªè¯æ˜åœ¨ä¸€èˆ¬å°‘æ ·æœ¬ä¿¡æ¯æå–æ–¹é¢æœ‰æ•ˆã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºè¾¹ç•Œæ„ŸçŸ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆBoundary-Aware LLMsï¼‰çš„Few-Shotå‘½åå®ä½“è¯†åˆ«æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¾¹ç•Œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¹¿ä¹‰å®ä½“è·¨åº¦çš„å®ä½“è¾¹ç•Œæ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨LoRAHubå°†ç›®æ ‡åŸŸçš„ä¿¡æ¯ä¸æºåŸŸå¯¹é½ï¼Œä»è€Œæé«˜äº†è‡ªé€‚åº”è·¨åŸŸåˆ†ç±»èƒ½åŠ›ã€‚åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‰€æå‡ºç­–ç•¥åœ¨å„ç§å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„ä¸­éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ•ˆæœã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/UESTC-GQJ/BANER%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/UESTC-GQJ/BANERä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02228v1">PDF</a> Appear on COLING 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºBoundary-Aware LLMsçš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å°æ ·æœ¬çš„å‘½åå®ä½“è¯†åˆ«é—®é¢˜ã€‚é€šè¿‡å¼•å…¥è¾¹ç•Œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæé«˜LLMå¯¹å®ä½“è¾¹ç•Œçš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨LoRAHubè¿›è¡ŒæºåŸŸå’Œç›®æ ‡åŸŸçš„ä¿¡æ¯å¯¹é½ï¼Œå¢å¼ºäº†è·¨åŸŸåˆ†ç±»çš„é€‚åº”æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆæé«˜äº†å®ä½“è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºBoundary-Aware LLMsæ–¹æ³•è§£å†³å°æ ·æœ¬çš„å‘½åå®ä½“è¯†åˆ«æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥è¾¹ç•Œæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæé«˜LLMå¯¹å®ä½“è¾¹ç•Œçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨LoRAHubè¿›è¡ŒæºåŸŸå’Œç›®æ ‡åŸŸçš„ä¿¡æ¯å¯¹é½ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†å®ä½“è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•é€‚ç”¨äºå¤šç§LLMæ¶æ„ã€‚</li>
<li>å…¬å¼€äº†ä»£ç å’Œæ•°æ®ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e638d5b2b99d8afb71ff385f5b803df7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e37edafda0bb58ac42ad42a5a38f77ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-996f29bc11ea45aaca917419b0602b46.jpg" align="middle">
</details>




<h2 id="Unlocking-Tuning-Free-Few-Shot-Adaptability-in-Visual-Foundation-Models-by-Recycling-Pre-Tuned-LoRAs"><a href="#Unlocking-Tuning-Free-Few-Shot-Adaptability-in-Visual-Foundation-Models-by-Recycling-Pre-Tuned-LoRAs" class="headerlink" title="Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models   by Recycling Pre-Tuned LoRAs"></a>Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models   by Recycling Pre-Tuned LoRAs</h2><p><strong>Authors:Zixuan Hu, Yongxian Wei, Li Shen, Chun Yuan, Dacheng Tao</strong></p>
<p>Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot adaptability without requiring fine-tuning, positioning them ideal for data-limited and real-time applications. However, this adaptability has not yet been replicated in current Visual Foundation Models (VFMs), which require explicit fine-tuning with sufficient tuning data. Besides, the pretraining-finetuning paradigm has led to the surge of numerous task-specific modular components, such as Low-Rank Adaptation (LoRA). For the first time, we explore the potential of reusing diverse pre-tuned LoRAs without accessing their original training data, to achieve tuning-free few-shot adaptation in VFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned LoRAs with a meta-learning objective, using surrogate data generated inversely from pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is empowered to solve new few-shot tasks in a single forward pass, akin to the in-context learning of LLMs. Additionally, we incorporate a double-efficient mechanism tailored to our framework, significantly accelerating the meta-training process while maintaining or even improving performance. Extensive experiments across various few-shot classification benchmarks across both in- and cross-domain scenarios demonstrate the superiority of our framework. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›ï¼Œæ— éœ€å¾®è°ƒå³å¯é€‚åº”æ–°ä»»åŠ¡ï¼Œä½¿å…¶æˆä¸ºæ•°æ®æœ‰é™å’Œå®æ—¶åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚ç„¶è€Œï¼Œè¿™ç§é€‚åº”èƒ½åŠ›åœ¨ç°æœ‰çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ä¸­å°šæœªå®ç°ï¼Œåè€…éœ€è¦å¤§é‡çš„å¾®è°ƒæ•°æ®å’Œæ˜ç¡®çš„å¾®è°ƒè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼å¯¼è‡´äº†å¤§é‡é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å—åŒ–ç»„ä»¶çš„æ¶Œç°ï¼Œå¦‚ä½ç§©é€‚é…ï¼ˆLoRAï¼‰ã€‚æˆ‘ä»¬é¦–æ¬¡æ¢ç´¢äº†å¤ç”¨å¤šç§é¢„è®­ç»ƒLoRAçš„æ½œåŠ›ï¼Œè€Œæ— éœ€è®¿é—®å…¶åŸå§‹è®­ç»ƒæ•°æ®ï¼Œä»¥å®ç°VFMsä¸­çš„æ— å¾®è°ƒå°‘æ ·æœ¬é€‚åº”ã€‚æˆ‘ä»¬çš„æ¡†æ¶LoRA Recycleé€šè¿‡å…ƒå­¦ä¹ ç›®æ ‡å’Œåå‘ç”Ÿæˆçš„ä»£ç†æ•°æ®ä»å¤šç§é¢„è®­ç»ƒLoRAä¸­æç‚¼å‡ºå…ƒLoRAã€‚ä¸€æ—¦è£…å¤‡ä¸Šè¯¥å…ƒLoRAï¼Œè§†è§‰åŸºç¡€æ¨¡å‹å°±èƒ½åƒLLMsé‚£æ ·é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ä¸€æ¬¡æ€§è§£å†³æ–°çš„å°‘æ ·æœ¬ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†é’ˆå¯¹æˆ‘ä»¬æ¡†æ¶è®¾è®¡çš„åŒé‡é«˜æ•ˆæœºåˆ¶ï¼Œæ˜¾è‘—åŠ é€Ÿäº†å…ƒè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒäº†ç”šè‡³æé«˜äº†æ€§èƒ½ã€‚åœ¨ä¸åŒé¢†åŸŸçš„å°‘æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02220v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå±•ç°å‡ºå¼ºå¤§çš„å°‘æ ·æœ¬é€‚åº”èƒ½åŠ›ï¼Œæ— éœ€å¾®è°ƒå³å¯é€‚åº”æ–°ä»»åŠ¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“å‰è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ç¼ºä¹è¿™ç§èƒ½åŠ›ï¼Œéœ€è¦æ˜¾å¼å¾®è°ƒåŠå……è¶³æ•°æ®ã€‚ç ”ç©¶æ¢ç´¢äº†åˆ©ç”¨é¢„è®­ç»ƒLoRAï¼ˆä½ç§©é€‚é…ï¼‰å®ç°VFMsæ— å¾®è°ƒå°‘æ ·æœ¬é€‚é…çš„æ½œåŠ›ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåä¸ºLoRA Recycleçš„æ¡†æ¶ï¼Œåˆ©ç”¨å…ƒå­¦ä¹ ç›®æ ‡ä»å¤šä¸ªé¢„è®­ç»ƒLoRAä¸­æå–å…ƒLoRAï¼Œå¹¶åˆ©ç”¨åå‘ç”Ÿæˆçš„ä»£ç†æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è£…å¤‡å…ƒLoRAçš„VFMèƒ½åƒLLMsé‚£æ ·ï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­è§£å†³æ–°çš„å°‘æ ·æœ¬ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„åŒé‡æœºåˆ¶ï¼Œä»¥åŠ é€Ÿå…ƒè®­ç»ƒè¿‡ç¨‹å¹¶ä¿æŒæˆ–æå‡æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§å°‘æ ·æœ¬åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTå¯åœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹é€‚åº”æ–°ä»»åŠ¡ï¼Œé€‚ç”¨äºæ•°æ®æœ‰é™å’Œå®æ—¶åº”ç”¨ã€‚</li>
<li>å½“å‰è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ç¼ºä¹è¿™ç§å°‘æ ·æœ¬é€‚åº”æ€§ï¼Œéœ€è¦æ˜¾å¼å¾®è°ƒåŠæ•°æ®ã€‚</li>
<li>ç ”ç©¶é¦–æ¬¡æ¢ç´¢åˆ©ç”¨é¢„è®­ç»ƒçš„LoRAï¼ˆä½ç§©é€‚é…ï¼‰å®ç°VFMsçš„æ— å¾®è°ƒå°‘æ ·æœ¬é€‚é…æ½œåŠ›ã€‚</li>
<li>LoRA Recycleæ¡†æ¶é€šè¿‡å…ƒå­¦ä¹ ç›®æ ‡å’Œä»£ç†æ•°æ®æå–å…ƒLoRAã€‚</li>
<li>è£…å¤‡å…ƒLoRAçš„VFMèƒ½åƒLLMsä¸€æ ·ï¼Œå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶æå‡ºä¸€ä¸ªé«˜æ•ˆçš„åŒé‡æœºåˆ¶ï¼Œä»¥åŠ é€Ÿå…ƒè®­ç»ƒè¿‡ç¨‹å¹¶ä¿æŒæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c42229c73b148e64262c7a539bb6e0af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4598f24f211e709661a0d72f18f023eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba2e19004d352cbec8037eacb6fae5b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcce6c788f23679bd3c91d9b8cf1ea08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cdb654e051b6ff6614ad512bada1cc4.jpg" align="middle">
</details>




<h2 id="Robot-Learning-with-Super-Linear-Scaling"><a href="#Robot-Learning-with-Super-Linear-Scaling" class="headerlink" title="Robot Learning with Super-Linear Scaling"></a>Robot Learning with Super-Linear Scaling</h2><p><strong>Authors:Marcel Torne, Arhan Jain, Jiayi Yuan, Vidaaranya Macha, Lars Ankile, Anthony Simeonov, Pulkit Agrawal, Abhishek Gupta</strong></p>
<p>Scaling robot learning requires data collection pipelines that scale favorably with human effort. In this work, we propose Crowdsourcing and Amortizing Human Effort for Real-to-Sim-to-Real(CASHER), a pipeline for scaling up data collection and learning in simulation where the performance scales superlinearly with human effort. The key idea is to crowdsource digital twins of real-world scenes using 3D reconstruction and collect large-scale data in simulation, rather than the real-world. Data collection in simulation is initially driven by RL, bootstrapped with human demonstrations. As the training of a generalist policy progresses across environments, its generalization capabilities can be used to replace human effort with model generated demonstrations. This results in a pipeline where behavioral data is collected in simulation with continually reducing human effort. We show that CASHER demonstrates zero-shot and few-shot scaling laws on three real-world tasks across diverse scenarios. We show that CASHER enables fine-tuning of pre-trained policies to a target scenario using a video scan without any additional human effort. See our project website: <a target="_blank" rel="noopener" href="https://casher-robot-learning.github.io/CASHER/">https://casher-robot-learning.github.io/CASHER/</a> </p>
<blockquote>
<p>åœ¨æ‰©å¤§æœºå™¨äººå­¦ä¹ è§„æ¨¡æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æœ‰åˆ©äºäººç±»æŠ•å…¥çš„æ•°æ®é‡‡é›†æµç¨‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ¨¡æ‹Ÿç¯å¢ƒä¸‹æ•°æ®é‡‡é›†ä¸å­¦ä¹ çš„â€œç¾¤æºåŒ–ä¸æ¶ˆè§£äººåŠ›æŠ•å…¥çš„å®æ—¶ä»¿çœŸé‡‡é›†ï¼ˆCASHERï¼‰â€æµç¨‹ï¼Œè¯¥æµç¨‹å®ç°äº†è¶…è¶ŠäººåŠ›æŠ•å…¥çš„çº¿æ€§å¢é•¿è§„æ¨¡åŒ–æ•ˆåº”ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨ä¸‰ç»´é‡å»ºæŠ€æœ¯ä¼—åŒ…ç°å®åœºæ™¯çš„æ•°å­—åŒèƒèƒï¼Œå¹¶åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è€ŒéçœŸå®ç¯å¢ƒä¸­æ”¶é›†å¤§è§„æ¨¡æ•°æ®ã€‚æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ•°æ®é‡‡é›†æœ€åˆç”±å¼ºåŒ–å­¦ä¹ é©±åŠ¨ï¼Œè¾…ä»¥äººç±»ç¤ºèŒƒã€‚éšç€é€šç”¨ç­–ç•¥åœ¨ä¸åŒç¯å¢ƒä¸­çš„è®­ç»ƒè¿›å±•ï¼Œå…¶é€šç”¨èƒ½åŠ›å¯ç”¨æ¥æ›¿æ¢æ¨¡æ‹Ÿç”Ÿæˆçš„æ¼”ç¤ºä»£æ›¿äººåŠ›æŠ•å…¥ã€‚è¿™å¯¼è‡´äº†ä¸€ç§æ¨¡æ‹Ÿè¡Œä¸ºæ•°æ®é‡‡é›†çš„æµç¨‹ï¼Œå¹¶éšç€è®­ç»ƒè¿‡ç¨‹æŒç»­å‡å°‘äººåŠ›æŠ•å…¥ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªçœŸå®ä»»åŠ¡çš„ä¸åŒåœºæ™¯ä¸­å±•ç¤ºäº†CASHERçš„æ— é¢„è®¾å’Œå°‘é¢„è®¾æ‰©å±•å®šå¾‹ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†CASHERèƒ½å¤Ÿé€šè¿‡è§†é¢‘æ‰«æå¯¹é¢„è®­ç»ƒç­–ç•¥è¿›è¡Œå¾®è°ƒä»¥é€‚åº”ç›®æ ‡åœºæ™¯ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„äººåŠ›æŠ•å…¥ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://casher-robot-learning.github.io/CASHER/">CASHERé¡¹ç›®ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01770v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCASHERçš„æ•°æ®æ”¶é›†æµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒå®ç°æœºå™¨äººå­¦ä¹ çš„è§„æ¨¡åŒ–ã€‚CASHERåˆ©ç”¨3Dé‡å»ºæŠ€æœ¯ä¼—åŒ…æ•°å­—åŒèƒèƒåœºæ™¯ï¼Œåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ”¶é›†å¤§è§„æ¨¡æ•°æ®ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œäººç±»æ¼”ç¤ºé©±åŠ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚éšç€é€šç”¨ç­–ç•¥çš„è®­ç»ƒè¿›å±•ï¼Œåˆ©ç”¨å…¶åœ¨ä¸åŒç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›é€æ­¥å‡å°‘äººåŠ›å‚ä¸ï¼Œè½¬è€Œä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ¼”ç¤ºæ•°æ®ã€‚æœ€ç»ˆå®ç°åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ”¶é›†è¡Œä¸ºæ•°æ®çš„åŒæ—¶ï¼ŒæŒç»­å‡å°‘äººåŠ›æŠ•å…¥ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä»»åŠ¡ä¸­ï¼ŒCASHERå±•ç°å‡ºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ‰©å±•å®šå¾‹çš„ä¼˜åŠ¿ï¼Œå¹¶èƒ½é€šè¿‡è§†é¢‘æ‰«æå¯¹é¢„è®­ç»ƒç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œæ— éœ€é¢å¤–äººåŠ›æŠ•å…¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CASHERæ˜¯ä¸€ç§ç”¨äºæœºå™¨äººå­¦ä¹ çš„æ•°æ®æ”¶é›†æµæ°´çº¿ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒå®ç°è§„æ¨¡åŒ–æ•°æ®æ”¶é›†å’Œå­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨3Dé‡å»ºæŠ€æœ¯ä¼—åŒ…æ•°å­—åŒèƒèƒåœºæ™¯ï¼Œä»¥æ”¶é›†æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤§è§„æ¨¡æ•°æ®ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œäººç±»æ¼”ç¤ºç›¸ç»“åˆï¼Œé©±åŠ¨æ•°æ®æ”¶é›†è¿‡ç¨‹ã€‚</li>
<li>éšç€é€šç”¨ç­–ç•¥çš„è®­ç»ƒè¿›å±•ï¼Œåˆ©ç”¨å…¶åœ¨ä¸åŒç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›é€æ­¥å‡å°‘äººåŠ›å‚ä¸ã€‚</li>
<li>CASHERå®ç°äº†åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­æ”¶é›†è¡Œä¸ºæ•°æ®çš„åŒæ—¶ï¼ŒæŒç»­å‡å°‘äººåŠ›æŠ•å…¥ã€‚</li>
<li>åœ¨ä¸‰ä¸ªçœŸå®ä»»åŠ¡ä¸­ï¼ŒCASHERå±•ç°å‡ºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ‰©å±•å®šå¾‹çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ce8071ee02ef69571c3704b42070f07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-53beba00bab506fbb19b257c66ce4e4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f19b394957f054879ee9e93ba8853418.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-593eabba9ac2dd1aff297b86cc347696.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72dd3ff3486a6aeaee9f1ea06313d5b1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36f7cad9033b1722ca41afe3730379dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0da1568d4b18ab9bb513d5292eb3167.jpg" align="middle">
</details>




<h2 id="Towards-Cross-Lingual-Audio-Abuse-Detection-in-Low-Resource-Settings-with-Few-Shot-Learning"><a href="#Towards-Cross-Lingual-Audio-Abuse-Detection-in-Low-Resource-Settings-with-Few-Shot-Learning" class="headerlink" title="Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings   with Few-Shot Learning"></a>Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings   with Few-Shot Learning</h2><p><strong>Authors:Aditya Narayan Sankaran, Reza Farahbakhsh, Noel Crespi</strong></p>
<p>Online abusive content detection, particularly in low-resource settings and within the audio modality, remains underexplored. We investigate the potential of pre-trained audio representations for detecting abusive language in low-resource languages, in this case, in Indian languages using Few Shot Learning (FSL). Leveraging powerful representations from models such as Wav2Vec and Whisper, we explore cross-lingual abuse detection using the ADIMA dataset with FSL. Our approach integrates these representations within the Model-Agnostic Meta-Learning (MAML) framework to classify abusive language in 10 languages. We experiment with various shot sizes (50-200) evaluating the impact of limited data on performance. Additionally, a feature visualization study was conducted to better understand model behaviour. This study highlights the generalization ability of pre-trained models in low-resource scenarios and offers valuable insights into detecting abusive language in multilingual contexts. </p>
<blockquote>
<p>ç½‘ç»œæ»¥ç”¨å†…å®¹æ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„ç¯å¢ƒå’ŒéŸ³é¢‘æ¨¡å¼ä¸‹ï¼Œä»ç„¶ç ”ç©¶ä¸è¶³ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†é¢„è®­ç»ƒéŸ³é¢‘è¡¨ç¤ºåœ¨æ£€æµ‹ä½èµ„æºè¯­è¨€ä¸­çš„æ”»å‡»æ€§è¯­è¨€çš„æ½œåŠ›ï¼Œæœ¬ä¾‹ä¸­ä¸ºä½¿ç”¨å°æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰çš„å°åº¦è¯­è¨€ã€‚æˆ‘ä»¬åˆ©ç”¨Wav2Vecå’ŒWhisperç­‰æ¨¡å‹çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ï¼Œä½¿ç”¨ADIMAæ•°æ®é›†è¿›è¡Œè·¨è¯­è¨€æ»¥ç”¨æ£€æµ‹FSLã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¿™äº›è¡¨ç¤ºå½¢å¼çº³å…¥æ¨¡å‹æ— å…³å…ƒå­¦ä¹ ï¼ˆMAMLï¼‰æ¡†æ¶ï¼Œä»¥åœ¨10ç§è¯­è¨€ä¸­åˆ†ç±»æ”»å‡»æ€§è¯­è¨€ã€‚æˆ‘ä»¬è¯•éªŒäº†å„ç§å°„å‡»è§„æ¨¡ï¼ˆ50-200ï¼‰ï¼Œè¯„ä¼°æœ‰é™æ•°æ®å¯¹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ç‰¹å¾å¯è§†åŒ–ç ”ç©¶ï¼Œä»¥æ›´å¥½åœ°äº†è§£æ¨¡å‹çš„è¡Œä¸ºã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†é¢„è®­ç»ƒæ¨¡å‹åœ¨èµ„æºåŒ®ä¹åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºåœ¨å¤šè¯­ç§ç¯å¢ƒä¸­æ£€æµ‹æ”»å‡»æ€§è¯­è¨€æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01408v2">PDF</a> Accepted as part of the proceedings of COLING 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢ç´¢äº†é¢„è®­ç»ƒéŸ³é¢‘è¡¨ç¤ºåœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸­æ£€æµ‹æ»¥ç”¨è¯­è¨€çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å°åº¦è¯­è¨€ã€‚ç ”ç©¶åˆ©ç”¨Wav2Vecå’ŒWhisperç­‰æ¨¡å‹çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ï¼Œç»“åˆFew Shot Learningï¼ˆFSLï¼‰æ–¹æ³•ï¼Œä½¿ç”¨ADIMAæ•°æ®é›†è¿›è¡Œè·¨è¯­è¨€æ»¥ç”¨æ£€æµ‹ã€‚è¯¥ç ”ç©¶å°†é¢„è®­ç»ƒæ¨¡å‹é›†æˆåˆ°Model-Agnostic Meta-Learningï¼ˆMAMLï¼‰æ¡†æ¶ä¸­ï¼Œä»¥åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹åˆ†ç±»æ»¥ç”¨è¯­è¨€ï¼Œæ¶‰åŠå¤šç§è¯­è¨€ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†ç‰¹å¾å¯è§†åŒ–ç ”ç©¶ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹è¡Œä¸ºã€‚ç ”ç©¶çªæ˜¾äº†é¢„è®­ç»ƒæ¨¡å‹åœ¨ä½èµ„æºåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºå¤šè¯­è¨€ç¯å¢ƒæ£€æµ‹æ»¥ç”¨è¯­è¨€æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨çº¿æ»¥ç”¨å†…å®¹æ£€æµ‹çš„éŸ³é¢‘æ¨¡æ€æ–¹é¢çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸­ã€‚</li>
<li>ç ”ç©¶å…³æ³¨åœ¨å°åº¦è¯­è¨€ä¸­åˆ©ç”¨é¢„è®­ç»ƒéŸ³é¢‘è¡¨ç¤ºè¿›è¡Œæ»¥ç”¨æ£€æµ‹ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨Few Shot Learning (FSL)æ–¹æ³•è¿›è¡Œè·¨è¯­è¨€æ»¥ç”¨æ£€æµ‹ã€‚</li>
<li>ä½¿ç”¨Wav2Vecå’ŒWhisperç­‰æ¨¡å‹è¿›è¡Œå¼ºå¤§çš„è¡¨ç¤ºï¼Œé›†æˆåˆ°Model-Agnostic Meta-Learning (MAML)æ¡†æ¶ä¸­åˆ†ç±»æ»¥ç”¨è¯­è¨€ã€‚</li>
<li>å®éªŒæ¶‰åŠå¤šç§è¯­è¨€å’Œä¸åŒçš„è®­ç»ƒæ•°æ®é‡ï¼ˆä»50åˆ°200ä¸ªæ ·æœ¬ï¼‰ã€‚</li>
<li>ç‰¹å¾å¯è§†åŒ–ç ”ç©¶æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æ¨¡å‹è¡Œä¸ºã€‚</li>
<li>ç ”ç©¶å‘ç°é¢„è®­ç»ƒæ¨¡å‹åœ¨ä½èµ„æºåœºæ™¯ä¸‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d2c0bc8fa3962e7539cc4493188e274.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a518f577d7b8c414d361bf8bbf0ef5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19d934910aef9e4a6e06c2cc29c52e5b.jpg" align="middle">
</details>




<h2 id="Unleashing-In-context-Learning-of-Autoregressive-Models-for-Few-shot-Image-Manipulation"><a href="#Unleashing-In-context-Learning-of-Autoregressive-Models-for-Few-shot-Image-Manipulation" class="headerlink" title="Unleashing In-context Learning of Autoregressive Models for Few-shot   Image Manipulation"></a>Unleashing In-context Learning of Autoregressive Models for Few-shot   Image Manipulation</h2><p><strong>Authors:Bolin Lai, Felix Juefei-Xu, Miao Liu, Xiaoliang Dai, Nikhil Mehta, Chenguang Zhu, Zeyi Huang, James M. Rehg, Sangmin Lee, Ning Zhang, Tong Xiao</strong></p>
<p>Text-guided image manipulation has experienced notable advancement in recent years. In order to mitigate linguistic ambiguity, few-shot learning with visual examples has been applied for instructions that are underrepresented in the training set, or difficult to describe purely in language. However, learning from visual prompts requires strong reasoning capability, which diffusion models are struggling with. To address this issue, we introduce a novel multi-modal autoregressive model, dubbed $\textbf{InstaManip}$, that can $\textbf{insta}$ntly learn a new image $\textbf{manip}$ulation operation from textual and visual guidance via in-context learning, and apply it to new query images. Specifically, we propose an innovative group self-attention mechanism to break down the in-context learning process into two separate stages â€“ learning and applying, which simplifies the complex problem into two easier tasks. We also introduce a relation regularization method to further disentangle image transformation features from irrelevant contents in exemplar images. Extensive experiments suggest that our method surpasses previous few-shot image manipulation models by a notable margin ($\geq$19% in human evaluation). We also find our model can be further boosted by increasing the number or diversity of exemplar images. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œè¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ä¸ºäº†å‡è½»è¯­è¨€æ¨¡ç³Šæ€§ï¼Œä½¿ç”¨è§†è§‰ç¤ºä¾‹è¿›è¡Œçš„å°æ ·æœ¬å­¦ä¹ è¢«åº”ç”¨äºè®­ç»ƒé›†ä¸­è¡¨ç¤ºä¸è¶³æˆ–ä»…å‡­è¯­è¨€éš¾ä»¥æè¿°çš„æŒ‡ä»¤ã€‚ç„¶è€Œï¼Œä»è§†è§‰æç¤ºä¸­å­¦ä¹ éœ€è¦å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™æ­£æ˜¯æ‰©æ•£æ¨¡å‹æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼è‡ªå›å½’æ¨¡å‹ï¼Œåä¸º<strong>InstaManip</strong>ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå³åˆ»ä»æ–‡æœ¬å’Œè§†è§‰æŒ‡å¯¼ä¸­å­¦ä¹ æ–°çš„å›¾åƒæ“ä½œï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°æ€§çš„åˆ†ç»„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†ä¸Šä¸‹æ–‡å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤ä¸ªå•ç‹¬çš„é˜¶æ®µâ€”â€”å­¦ä¹ å’Œåº”ç”¨ï¼Œå°†å¤æ‚é—®é¢˜ç®€åŒ–ä¸ºä¸¤ä¸ªæ›´å®¹æ˜“çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å…³ç³»æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥è¿›ä¸€æ­¥ä»ç¤ºä¾‹å›¾åƒä¸­çš„æ— å…³å†…å®¹ä¸­åˆ†ç¦»å‡ºå›¾åƒè½¬æ¢ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘æ•°å›¾åƒæ“ä½œæ¨¡å‹ä¸­çš„è¡¨ç°è¶…è¿‡äº†ä¹‹å‰çš„æ¨¡å‹ï¼Œåœ¨äººç±»è¯„ä¼°ä¸­çš„ä¼˜åŠ¿æ˜¾è‘—ï¼ˆæé«˜â‰¥19%ï¼‰ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œé€šè¿‡å¢åŠ ç¤ºä¾‹å›¾åƒçš„æ•°é‡æˆ–å¤šæ ·æ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01027v2">PDF</a> 18 pages, 16 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œæŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚ä¸ºè§£å†³å°‘æ•°å­¦ä¹ æƒ…å†µä¸‹è§†è§‰æç¤ºçš„è¯­è¨€æ¨¡ç³Šå’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹â€”â€”InstaManipã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå³æ—¶ä»æ–‡æœ¬å’Œè§†è§‰æŒ‡å¯¼ä¸­å­¦ä¹ æ–°çš„å›¾åƒæ“ä½œï¼Œå¹¶åº”ç”¨äºæ–°çš„æŸ¥è¯¢å›¾åƒã€‚æ¨¡å‹é€šè¿‡åˆ†ç»„è‡ªæ³¨æ„åŠ›æœºåˆ¶å°†å­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºå­¦ä¹ å’Œåº”ç”¨ä¸¤ä¸ªé˜¶æ®µï¼Œå¹¶å¼•å…¥å…³ç³»æ­£åˆ™åŒ–æ–¹æ³•æ¥è¿›ä¸€æ­¥åˆ†ç¦»å›¾åƒè½¬æ¢ç‰¹å¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å°‘æ•°å›¾åƒæ“ä½œä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å¢åŠ ç¤ºä¾‹å›¾åƒçš„æ•°é‡æˆ–å¤šæ ·æ€§è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ“ä½œæŠ€æœ¯è¿‘å¹´æ¥å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ä¸ºè§£å†³è¯­è¨€æ¨¡ç³Šå’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œå¼•å…¥äº†å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹InstaManipã€‚</li>
<li>InstaManipèƒ½å¤Ÿä»æ–‡æœ¬å’Œè§†è§‰æŒ‡å¯¼ä¸­å­¦ä¹ æ–°çš„å›¾åƒæ“ä½œå¹¶å³æ—¶åº”ç”¨ã€‚</li>
<li>æ¨¡å‹é€šè¿‡åˆ†ç»„è‡ªæ³¨æ„åŠ›æœºåˆ¶å°†å­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºå­¦ä¹ å’Œåº”ç”¨ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>å¼•å…¥å…³ç³»æ­£åˆ™åŒ–æ–¹æ³•æ¥åˆ†ç¦»å›¾åƒè½¬æ¢ç‰¹å¾ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒInstaManipåœ¨å°‘æ•°å›¾åƒæ“ä½œä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c5fe3e351757937da5feebbe542d8a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c82718ab60799dd8341a4f4772053a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17acd74f050d4c78576bda1638f596fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e43b2f713b6ab9442feb248559f1d38.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e82d3791f55d0d0eb07e5443afc45c0.jpg" align="middle">
</details>




<h2 id="Hierarchical-Prompt-Decision-Transformer-Improving-Few-Shot-Policy-Generalization-with-Global-and-Adaptive"><a href="#Hierarchical-Prompt-Decision-Transformer-Improving-Few-Shot-Policy-Generalization-with-Global-and-Adaptive" class="headerlink" title="Hierarchical Prompt Decision Transformer: Improving Few-Shot Policy   Generalization with Global and Adaptive"></a>Hierarchical Prompt Decision Transformer: Improving Few-Shot Policy   Generalization with Global and Adaptive</h2><p><strong>Authors:Zhe Wang, Haozhu Wang, Yanjun Qi</strong></p>
<p>Decision transformers recast reinforcement learning as a conditional sequence generation problem, offering a simple but effective alternative to traditional value or policy-based methods. A recent key development in this area is the integration of prompting in decision transformers to facilitate few-shot policy generalization. However, current methods mainly use static prompt segments to guide rollouts, limiting their ability to provide context-specific guidance. Addressing this, we introduce a hierarchical prompting approach enabled by retrieval augmentation. Our method learns two layers of soft tokens as guiding prompts: (1) global tokens encapsulating task-level information about trajectories, and (2) adaptive tokens that deliver focused, timestep-specific instructions. The adaptive tokens are dynamically retrieved from a curated set of demonstration segments, ensuring context-aware guidance. Experiments across seven benchmark tasks in the MuJoCo and MetaWorld environments demonstrate the proposed approach consistently outperforms all baseline methods, suggesting that hierarchical prompting for decision transformers is an effective strategy to enable few-shot policy generalization. </p>
<blockquote>
<p>å†³ç­–å˜å‹å™¨å°†å¼ºåŒ–å­¦ä¹ é‡æ–°æ„å»ºä¸ºæ¡ä»¶åºåˆ—ç”Ÿæˆé—®é¢˜ï¼Œä¸ºä¼ ç»Ÿçš„åŸºäºä»·å€¼æˆ–æ”¿ç­–çš„æ–¹æ³•æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥é¢†åŸŸçš„æœ€æ–°å…³é”®è¿›å±•æ˜¯åœ¨å†³ç­–å˜å‹å™¨ä¸­èå…¥æç¤ºï¼Œä»¥ä¿ƒè¿›å°‘é‡ç­–ç•¥æ³›åŒ–ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨é™æ€æç¤ºæ®µæ¥æŒ‡å¯¼æ»šåŠ¨è¿‡ç¨‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æä¾›ç‰¹å®šä¸Šä¸‹æ–‡æŒ‡å¯¼çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šè¿‡æ£€ç´¢å¢å¼ºåŠŸèƒ½å®ç°åˆ†å±‚æç¤ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†ä¸¤å±‚è½¯ä»¤ç‰Œä½œä¸ºæŒ‡å¯¼æç¤ºï¼šï¼ˆ1ï¼‰å…¨å±€ä»¤ç‰Œï¼Œå°è£…è½¨è¿¹çš„ä»»åŠ¡çº§åˆ«ä¿¡æ¯ï¼Œï¼ˆ2ï¼‰è‡ªé€‚åº”ä»¤ç‰Œï¼Œä¼ é€’èšç„¦çš„ã€æ—¶é—´æ­¥é•¿ç‰¹å®šçš„æŒ‡ä»¤ã€‚è‡ªé€‚åº”ä»¤ç‰Œæ˜¯ä»ç²¾é€‰çš„æ¼”ç¤ºæ®µé›†ä¸­åŠ¨æ€æ£€ç´¢çš„ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æŒ‡å¯¼ã€‚åœ¨MuJoCoå’ŒMetaWorldç¯å¢ƒä¸­çš„ä¸ƒä¸ªåŸºå‡†ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¿™è¡¨æ˜ä¸ºå†³ç­–å˜å‹å™¨è¿›è¡Œåˆ†å±‚æç¤ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¯å®ç°å°‘é‡ç­–ç•¥æ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00979v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å†³ç­–å˜å‹å™¨é€šè¿‡å°†å¼ºåŒ–å­¦ä¹ é‡æ–°æ„å»ºä¸ºæ¡ä»¶åºåˆ—ç”Ÿæˆé—®é¢˜ï¼Œæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ›¿ä»£ä¼ ç»Ÿä»·å€¼æˆ–ç­–ç•¥åŸºç¡€æ–¹æ³•çš„æ–¹å¼ã€‚æœ€æ–°çš„å…³é”®è¿›å±•æ˜¯å†³ç­–å˜å‹å™¨ä¸­æç¤ºçš„é›†æˆï¼Œä»¥ä¿ƒè¿›å°‘é‡ç­–ç•¥æ³›åŒ–ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¸»è¦ä½¿ç”¨é™æ€æç¤ºæ®µæ¥æŒ‡å¯¼æ¼”ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æä¾›ç‰¹å®šä¸Šä¸‹æ–‡æŒ‡å¯¼çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€šè¿‡æ£€ç´¢å¢å¼ºå®ç°åˆ†å±‚æç¤ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†ä¸¤å±‚è½¯ä»¤ç‰Œä½œä¸ºå¼•å¯¼æç¤ºï¼š1ï¼‰å…¨å±€ä»¤ç‰Œï¼Œå°è£…è½¨è¿¹çš„ä»»åŠ¡çº§ä¿¡æ¯ï¼›2ï¼‰è‡ªé€‚åº”ä»¤ç‰Œï¼Œæä¾›ä¸“æ³¨çš„ã€æ—¶é—´æ­¥é•¿ç‰¹å®šçš„æŒ‡ä»¤ã€‚è‡ªé€‚åº”ä»¤ç‰Œæ˜¯ä»ç²¾é€‰çš„æ¼”ç¤ºæ®µé›†ä¸­åŠ¨æ€æ£€ç´¢çš„ï¼Œä»¥ç¡®ä¿ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æŒ‡å¯¼ã€‚åœ¨MuJoCoå’ŒMetaWorldç¯å¢ƒä¸­çš„ä¸ƒä¸ªåŸºå‡†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å§‹ç»ˆä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¿™è¡¨æ˜ä¸ºå†³ç­–å˜å‹å™¨è¿›è¡Œåˆ†å±‚æç¤ºæ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œå¯å®ç°å°‘é‡ç­–ç•¥æ³›åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†³ç­–å˜å‹å™¨å°†å¼ºåŒ–å­¦ä¹ é‡æ–°å®šä½ä¸ºæ¡ä»¶åºåˆ—ç”Ÿæˆé—®é¢˜ï¼Œæä¾›äº†å¯¹ä¼ ç»Ÿä»·å€¼æˆ–ç­–ç•¥åŸºç¡€çš„æ›¿ä»£æ–¹æ³•ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡é™æ€æç¤ºæ®µå¼•å¯¼å†³ç­–å˜å‹å™¨ï¼Œé™åˆ¶äº†ä¸Šä¸‹æ–‡ç‰¹å®šçš„æŒ‡å¯¼èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡æ£€ç´¢å¢å¼ºå®ç°åˆ†å±‚æç¤ºçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å…¨å±€ä»¤ç‰Œå’Œè‡ªé€‚åº”ä»¤ç‰Œã€‚</li>
<li>å…¨å±€ä»¤ç‰Œå°è£…ä»»åŠ¡çº§åˆ«çš„è½¨è¿¹ä¿¡æ¯ã€‚</li>
<li>è‡ªé€‚åº”ä»¤ç‰Œä»æ¼”ç¤ºæ®µé›†ä¸­åŠ¨æ€æ£€ç´¢ï¼Œæä¾›é’ˆå¯¹æ€§çš„æ—¶é—´æ­¥é•¿ç‰¹å®šæŒ‡ä»¤ã€‚</li>
<li>åœ¨å¤šä¸ªç¯å¢ƒä¸­çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å°‘é‡ç­–ç•¥æ³›åŒ–æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
<li>åˆ†å±‚æç¤ºå¯¹äºæé«˜å†³ç­–å˜å‹å™¨çš„æ€§èƒ½æ˜¯æœ‰æ•ˆçš„ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9090040dd0944f6e9c6bcc9ee64bcb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-097759dbaa6ac9cfc2f46bf0fae40cf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102376073a70ff046fee9027769501d3.jpg" align="middle">
</details>




<h2 id="Prompt-as-Free-Lunch-Enhancing-Diversity-in-Source-Free-Cross-domain-Few-shot-Learning-through-Semantic-Guided-Prompting"><a href="#Prompt-as-Free-Lunch-Enhancing-Diversity-in-Source-Free-Cross-domain-Few-shot-Learning-through-Semantic-Guided-Prompting" class="headerlink" title="Prompt as Free Lunch: Enhancing Diversity in Source-Free Cross-domain   Few-shot Learning through Semantic-Guided Prompting"></a>Prompt as Free Lunch: Enhancing Diversity in Source-Free Cross-domain   Few-shot Learning through Semantic-Guided Prompting</h2><p><strong>Authors:Linhai Zhuo, Zheng Wang, Yuqian Fu, Tianwen Qian</strong></p>
<p>The source-free cross-domain few-shot learning (CD-FSL) task aims to transfer pretrained models to target domains utilizing minimal samples, eliminating the need for source domain data. Addressing this issue requires models to have robust generalization abilities and strong feature representation, aligning with the characteristics of large-scale pretrained models. However, large-scale models tend to lose representational ability in cross-domain scenarios due to limited sample diversity. \zlh{Given the abundant diversity provided by semantic modality, this paper leverages textual modality to enhance training sample diversity with CLP model}, meanwhile improving model transfer efficiency. Specifically, we propose the SeGD-VPT framework, which is divided into two phases. The first step aims to increase feature diversity by adding diversity prompts to each support sample, thereby generating varying input and enhancing sample diversity. Furthermore, we use diversity descriptions of classes to guide semantically meaningful learning of diversity prompts, proposing random combinations and selections of texts to increase textual diversity. Additionally, deep prompt tuning is introduced to enhance the modelâ€™s transfer capability. After training of the first step, support samples with different diversity prompts are input into the CLIP backbone to generate enhanced features. After generation, the second phase trains classifiers using the generated features. Extensive experimental results across several benchmarks verify our method is comparable to SOTA source-utilized models and attain the best performance under the source-free CD-FSL setting. </p>
<blockquote>
<p>æ— æºè·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCD-FSLï¼‰ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨å°‘é‡æ ·æœ¬å°†é¢„è®­ç»ƒæ¨¡å‹è¿ç§»åˆ°ç›®æ ‡åŸŸï¼Œä»è€Œæ¶ˆé™¤å¯¹æºåŸŸæ•°æ®çš„éœ€æ±‚ã€‚è§£å†³æ­¤é—®é¢˜éœ€è¦æ¨¡å‹å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œè¿™ä¸å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾ç›¸å»åˆã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡æ¨¡å‹åœ¨è·¨åŸŸåœºæ™¯ä¸­å¾€å¾€ä¼šå› æ ·æœ¬å¤šæ ·æ€§æœ‰é™è€Œå¤±å»è¡¨ç¤ºèƒ½åŠ›ã€‚é‰´äºè¯­ä¹‰æ¨¡æ€æä¾›çš„ä¸°å¯Œå¤šæ ·æ€§ï¼Œæœ¬æ–‡åˆ©ç”¨æ–‡æœ¬æ¨¡æ€é€šè¿‡CLPæ¨¡å‹å¢å¼ºè®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶æé«˜æ¨¡å‹çš„è¿ç§»æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†SeGD-VPTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€æ­¥æ—¨åœ¨é€šè¿‡ä¸ºæ¯ä¸ªæ”¯æŒæ ·æœ¬æ·»åŠ å¤šæ ·æ€§æç¤ºæ¥å¢åŠ ç‰¹å¾å¤šæ ·æ€§ï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„è¾“å…¥å¹¶å¢å¼ºæ ·æœ¬å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç±»çš„å¤šæ ·æ€§æè¿°æ¥æŒ‡å¯¼å¤šæ ·æ€§æç¤ºçš„è¯­ä¹‰æœ‰æ„ä¹‰å­¦ä¹ ï¼Œæå‡ºæ–‡æœ¬çš„éšæœºç»„åˆå’Œé€‰æ‹©æ¥å¢åŠ æ–‡æœ¬å¤šæ ·æ€§ã€‚å¦å¤–ï¼Œè¿˜å¼•å…¥äº†æ·±åº¦æç¤ºè°ƒæ•´æ¥å¢å¼ºæ¨¡å‹çš„è¿ç§»èƒ½åŠ›ã€‚å®Œæˆç¬¬ä¸€æ­¥çš„è®­ç»ƒåï¼Œå¸¦æœ‰ä¸åŒå¤šæ ·æ€§æç¤ºçš„æ”¯æŒæ ·æœ¬è¢«è¾“å…¥åˆ°CLIPä¸»å¹²ç½‘ä¸­ç”Ÿæˆå¢å¼ºç‰¹å¾ã€‚ç‰¹å¾ç”Ÿæˆåï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨ç”Ÿæˆçš„ç‰¹æ€§è®­ç»ƒåˆ†ç±»å™¨ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸æºåˆ©ç”¨æ¨¡å‹çš„æœ€ä½³æ€§èƒ½ç›¸å½“ï¼Œå¹¶åœ¨æ— æºCD-FSLè®¾ç½®ä¸‹å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00767v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ— æºè·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCD-FSLï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡åŸŸä¸­ä½¿ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè¿ç§»ï¼Œæ— éœ€æºåŸŸæ•°æ®ã€‚æ–‡ç« æå‡ºäº†SeGD-VPTæ¡†æ¶ï¼Œé€šè¿‡å¢åŠ ç‰¹å¾å¤šæ ·æ€§æ¥æé«˜æ¨¡å‹åœ¨è·¨åŸŸåœºæ™¯ä¸­çš„è¡¨ç°ï¼Œåˆ©ç”¨æ–‡æœ¬æ¨¡æ€å¢å¼ºè®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶æé«˜æ¨¡å‹çš„è¿ç§»æ•ˆç‡ã€‚é€šè¿‡ä¸¤ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Œç”Ÿæˆå¢å¼ºç‰¹å¾å¹¶è®­ç»ƒåˆ†ç±»å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸æœ€å…ˆè¿›çš„æºåˆ©ç”¨æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨æ— æºCD-FSLè®¾ç½®ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æºè‡ªç”±è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼ˆCD-FSLï¼‰ä»»åŠ¡æ—¨åœ¨åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡åŸŸä¸­ä½¿ç”¨å°‘é‡æ ·æœ¬è¿›è¡Œè¿ç§»ï¼Œæ— éœ€æºåŸŸæ•°æ®ã€‚</li>
<li>æ¨¡å‹éœ€è¦å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ä»¥åº”å¯¹CD-FSLçš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹åœ¨è·¨åŸŸåœºæ™¯ä¸­å¯èƒ½ä¼šå¤±å»ä»£è¡¨æ€§èƒ½åŠ›ã€‚</li>
<li>æ–‡æœ¬æ¨¡æ€è¢«ç”¨æ¥å¢å¼ºè®­ç»ƒæ ·æœ¬çš„å¤šæ ·æ€§ï¼Œæé«˜æ¨¡å‹è¿ç§»æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†SeGD-VPTæ¡†æ¶ï¼Œé€šè¿‡å¢åŠ ç‰¹å¾å¤šæ ·æ€§å’Œåˆ©ç”¨æ–‡æœ¬æ¨¡æ€æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SeGD-VPTæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µå¢åŠ ç‰¹å¾å¤šæ ·æ€§ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨å¢å¼ºç‰¹å¾è®­ç»ƒåˆ†ç±»å™¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb31e2a63c2456cc111fb5ff3afd0d96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fccd9640caf5f91d079c9dde91ffe0ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bdccc34a0de7bdc80da73216bae4a1a.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1f0f0f4cf7d419c676d174ac15e31f5c.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  MAVias Mitigate any Visual Bias
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-07/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-537b2c67eed64b23a7a9b8bc6b7e1300.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-07  Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET   Image Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
