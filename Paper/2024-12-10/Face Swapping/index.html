<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Face Swapping">
    <meta name="description" content="Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Swap Path Network for Robust Person Search Pre-training">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Face Swapping | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-79d8c530a223fa85974e2a7b72c453cc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Face Swapping</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Face-Swapping/">
                                <span class="chip bg-color">Face Swapping</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                Face Swapping
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    60 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Swap-Path-Network-for-Robust-Person-Search-Pre-training"><a href="#Swap-Path-Network-for-Robust-Person-Search-Pre-training" class="headerlink" title="Swap Path Network for Robust Person Search Pre-training"></a>Swap Path Network for Robust Person Search Pre-training</h2><p><strong>Authors:Lucas Jaffe, Avideh Zakhor</strong></p>
<p>In person search, we detect and rank matches to a query person image within a set of gallery scenes. Most person search models make use of a feature extraction backbone, followed by separate heads for detection and re-identification. While pre-training methods for vision backbones are well-established, pre-training additional modules for the person search task has not been previously examined. In this work, we present the first framework for end-to-end person search pre-training. Our framework splits person search into object-centric and query-centric methodologies, and we show that the query-centric framing is robust to label noise, and trainable using only weakly-labeled person bounding boxes. Further, we provide a novel model dubbed Swap Path Net (SPNet) which implements both query-centric and object-centric training objectives, and can swap between the two while using the same weights. Using SPNet, we show that query-centric pre-training, followed by object-centric fine-tuning, achieves state-of-the-art results on the standard PRW and CUHK-SYSU person search benchmarks, with 96.4% mAP on CUHK-SYSU and 61.2% mAP on PRW. In addition, we show that our method is more effective, efficient, and robust for person search pre-training than recent backbone-only pre-training alternatives. </p>
<blockquote>
<p>åœ¨äººç‰©æœç´¢ä¸­ï¼Œæˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ç”»å»Šåœºæ™¯ä¸­å¯¹æŸ¥è¯¢äººç‰©å›¾åƒè¿›è¡Œæ£€æµ‹å’ŒåŒ¹é…æ’åã€‚å¤§å¤šæ•°äººç‰©æœç´¢æ¨¡å‹éƒ½ä¼šä½¿ç”¨ç‰¹å¾æå–ä¸»å¹²ç½‘ï¼Œç„¶ååˆ†åˆ«ç”¨äºæ£€æµ‹å’Œé‡æ–°è¯†åˆ«ã€‚è™½ç„¶è§†è§‰ä¸»å¹²çš„é¢„è®­ç»ƒæ–¹æ³•å·²ç»ç¡®ç«‹ï¼Œä½†é’ˆå¯¹äººç‰©æœç´¢ä»»åŠ¡é¢„è®­ç»ƒé™„åŠ æ¨¡å—å°šæœªè¢«ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç«¯åˆ°ç«¯äººç‰©æœç´¢é¢„è®­ç»ƒçš„é¦–ä¸ªæ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†äººç‰©æœç´¢åˆ†ä¸ºä»¥ç‰©ä½“ä¸ºä¸­å¿ƒå’Œä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œå¹¶è¯æ˜ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„æ–¹æ³•å¯¹æ ‡ç­¾å™ªå£°å…·æœ‰é²æ£’æ€§ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å¼±æ ‡ç­¾çš„äººç‰©è¾¹ç•Œæ¡†å³å¯è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåä¸ºSwap Path Netï¼ˆSPNetï¼‰çš„æ–°æ¨¡å‹ï¼Œå®ç°äº†ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒå’Œä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„è®­ç»ƒç›®æ ‡ï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨ç›¸åŒæƒé‡æ—¶å¯ä»¥äº’æ¢ã€‚ä½¿ç”¨SPNetï¼Œæˆ‘ä»¬è¯æ˜äº†ä»¥æŸ¥è¯¢ä¸ºä¸­å¿ƒçš„é¢„è®­ç»ƒï¼Œéšåä»¥ç‰©ä½“ä¸ºä¸­å¿ƒçš„å¾®è°ƒï¼Œå¯ä»¥åœ¨æ ‡å‡†çš„PRWå’ŒCUHK-SYSUäººç‰©æœç´¢åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³ç»“æœï¼Œå…¶ä¸­CUHK-SYSUçš„mAPä¸º96.4%ï¼ŒPRWçš„mAPä¸º61.2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨äººç‰©æœç´¢é¢„è®­ç»ƒæ–¹é¢æ¯”æœ€è¿‘çš„ä»…ä½¿ç”¨ä¸»å¹²çš„é¢„è®­ç»ƒæ›¿ä»£æ–¹æ³•æ›´æœ‰æ•ˆã€æ›´é«˜æ•ˆã€æ›´ç¨³å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05433v1">PDF</a> WACV 2025; Code: <a target="_blank" rel="noopener" href="https://github.com/LLNL/spnet">https://github.com/LLNL/spnet</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äººç‰©æœç´¢ä»»åŠ¡çš„é¦–ä¸ªç«¯åˆ°ç«¯é¢„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†äººç‰©æœç´¢åˆ†ä¸ºå¯¹è±¡ä¸­å¿ƒæ–¹æ³•å’ŒæŸ¥è¯¢ä¸­å¿ƒæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹Swap Path Netï¼ˆSPNetï¼‰ï¼Œè¯¥æ¨¡å‹å®ç°äº†è¿™ä¸¤ç§è®­ç»ƒç›®æ ‡ï¼Œå¹¶å¯ä»¥åœ¨ä½¿ç”¨ç›¸åŒæƒé‡çš„åŒæ—¶è¿›è¡Œåˆ‡æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SPNetè¿›è¡ŒæŸ¥è¯¢ä¸­å¿ƒé¢„è®­ç»ƒï¼Œç„¶åè¿›è¡Œå¯¹è±¡ä¸­å¿ƒå¾®è°ƒï¼Œåœ¨PRWå’ŒCUHK-SYSUäººç‰©æœç´¢æ ‡å‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹äººç‰©æœç´¢ä»»åŠ¡çš„é¦–ä¸ªç«¯åˆ°ç«¯é¢„è®­ç»ƒæ¡†æ¶ã€‚</li>
<li>æ¡†æ¶å°†äººç‰©æœç´¢åˆ†ä¸ºå¯¹è±¡ä¸­å¿ƒæ–¹æ³•å’ŒæŸ¥è¯¢ä¸­å¿ƒæ–¹æ³•ã€‚</li>
<li>æŸ¥è¯¢ä¸­å¿ƒæ¡†æ¶èƒ½å¤Ÿåº”å¯¹æ ‡ç­¾å™ªå£°ï¼Œå¹¶ä»…ä½¿ç”¨å¼±æ ‡ç­¾äººç‰©è¾¹ç•Œæ¡†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æå‡ºäº†Swap Path Net (SPNet) æ¨¡å‹ï¼Œå®ç°äº†æŸ¥è¯¢ä¸­å¿ƒå’Œå¯¹è±¡ä¸­å¿ƒè®­ç»ƒç›®æ ‡çš„åˆ‡æ¢ã€‚</li>
<li>SPNetåœ¨CUHK-SYSUå’ŒPRWæ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼ŒCUHK-SYSUä¸Šçš„mAPè¾¾åˆ°96.4%ï¼ŒPRWä¸Šçš„mAPè¾¾åˆ°61.2%ã€‚</li>
<li>ç›¸æ¯”æœ€è¿‘çš„ä»…é’ˆå¯¹éª¨å¹²ç½‘çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨äººç‰©æœç´¢é¢„è®­ç»ƒæ–¹é¢æ›´æœ‰æ•ˆã€æ›´é«˜æ•ˆã€æ›´ç¨³å¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1eadee96e9a52ee2c5009b8fc970fcb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d622fce34228ad74d40d4d37ad825dc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-036279f224d3be99bba7f977595e316c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3b82a62fce5a055e2406e3d6316248e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28ddadcd68bcdf586b9a0ff68db0c269.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-905d0cfa9bc5c51f4f541d6b0432e883.jpg" align="middle">
</details>




<h2 id="InstantSwap-Fast-Customized-Concept-Swapping-across-Sharp-Shape-Differences"><a href="#InstantSwap-Fast-Customized-Concept-Swapping-across-Sharp-Shape-Differences" class="headerlink" title="InstantSwap: Fast Customized Concept Swapping across Sharp Shape   Differences"></a>InstantSwap: Fast Customized Concept Swapping across Sharp Shape   Differences</h2><p><strong>Authors:Chenyang Zhu, Kai Li, Yue Ma, Longxiang Tang, Chengyu Fang, Chubin Chen, Qifeng Chen, Xiu Li</strong></p>
<p>Recent advances in Customized Concept Swapping (CCS) enable a text-to-image model to swap a concept in the source image with a customized target concept. However, the existing methods still face the challenges of inconsistency and inefficiency. They struggle to maintain consistency in both the foreground and background during concept swapping, especially when the shape difference is large between objects. Additionally, they either require time-consuming training processes or involve redundant calculations during inference. To tackle these issues, we introduce InstantSwap, a new CCS method that aims to handle sharp shape disparity at speed. Specifically, we first extract the bbox of the object in the source image automatically based on attention map analysis and leverage the bbox to achieve both foreground and background consistency. For background consistency, we remove the gradient outside the bbox during the swapping process so that the background is free from being modified. For foreground consistency, we employ a cross-attention mechanism to inject semantic information into both source and target concepts inside the box. This helps learn semantic-enhanced representations that encourage the swapping process to focus on the foreground objects. To improve swapping speed, we avoid computing gradients at each timestep but instead calculate them periodically to reduce the number of forward passes, which improves efficiency a lot with a little sacrifice on performance. Finally, we establish a benchmark dataset to facilitate comprehensive evaluation. Extensive evaluations demonstrate the superiority and versatility of InstantSwap. Project Page: <a target="_blank" rel="noopener" href="https://instantswap.github.io/">https://instantswap.github.io/</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå®šåˆ¶æ¦‚å¿µäº¤æ¢ï¼ˆCCSï¼‰çš„è¿›å±•ä½¿å¾—æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹èƒ½å¤Ÿç”¨å®šåˆ¶çš„ç›®æ ‡æ¦‚å¿µæ›¿æ¢æºå›¾åƒä¸­çš„æ¦‚å¿µã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ä¸€è‡´æ€§å’Œæ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚å®ƒä»¬åœ¨æ¦‚å¿µäº¤æ¢æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰æ™¯å’ŒèƒŒæ™¯ä¸­éƒ½å¾ˆéš¾ä¿æŒä¸€è‡´ï¼Œå°¤å…¶æ˜¯ç‰©ä½“å½¢çŠ¶å·®å¼‚è¾ƒå¤§æ—¶ã€‚æ­¤å¤–ï¼Œå®ƒä»¬è¦ä¹ˆéœ€è¦è€—æ—¶çš„è®­ç»ƒè¿‡ç¨‹ï¼Œè¦ä¹ˆåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¶‰åŠå†—ä½™çš„è®¡ç®—ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†InstantSwapï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„CCSæ–¹æ³•ï¼Œæ—¨åœ¨å¿«é€Ÿå¤„ç†æ˜æ˜¾çš„å½¢çŠ¶å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºæ³¨æ„åŠ›å›¾åˆ†æè‡ªåŠ¨æå–æºå›¾åƒä¸­ç‰©ä½“çš„bboxï¼Œå¹¶åˆ©ç”¨bboxæ¥å®ç°å‰æ™¯å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚å¯¹äºèƒŒæ™¯ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬åœ¨äº¤æ¢è¿‡ç¨‹ä¸­ç§»é™¤bboxå¤–çš„æ¢¯åº¦ï¼Œä½¿èƒŒæ™¯ä¸å—ä¿®æ”¹ã€‚å¯¹äºå‰æ™¯ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¯­ä¹‰ä¿¡æ¯æ³¨å…¥æ¡†å†…æºå’Œç›®æ ‡æ¦‚å¿µä¸­ã€‚è¿™æœ‰åŠ©äºå­¦ä¹ è¯­ä¹‰å¢å¼ºçš„è¡¨ç¤ºï¼Œé¼“åŠ±äº¤æ¢è¿‡ç¨‹ä¸“æ³¨äºå‰æ™¯å¯¹è±¡ã€‚ä¸ºäº†æé«˜äº¤æ¢é€Ÿåº¦ï¼Œæˆ‘ä»¬é¿å…åœ¨æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—æ¢¯åº¦ï¼Œè€Œæ˜¯å®šæœŸè®¡ç®—æ¢¯åº¦ï¼Œä»¥å‡å°‘å‰å‘ä¼ é€’çš„æ¬¡æ•°ï¼Œè¿™åœ¨æ€§èƒ½ä¸Šç¨æœ‰ç‰ºç‰²çš„æƒ…å†µä¸‹å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å…¨é¢è¯„ä¼°ã€‚å¹¿æ³›è¯„ä¼°è¯æ˜äº†InstantSwapçš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚<a target="_blank" rel="noopener" href="https://instantswap.github.io/">é¡¹ç›®é¡µé¢</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01197v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://instantswap.github.io/">https://instantswap.github.io/</a>. Github Page:   <a target="_blank" rel="noopener" href="https://github.com/chenyangzhu1/InstantSwap">https://github.com/chenyangzhu1/InstantSwap</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†InstantSwapæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å®šåˆ¶æ¦‚å¿µäº¤æ¢ï¼ˆCCSï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ¦‚å¿µäº¤æ¢æ—¶é¢ä¸´çš„å½¢çŠ¶å·®å¼‚å¤§ã€å‰åæ™¯ä¸ä¸€è‡´åŠæ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åŸºäºæ³¨æ„åŠ›å›¾åˆ†æè‡ªåŠ¨æå–ç›®æ ‡å¯¹è±¡çš„bboxï¼Œå®ç°å‰æ™¯å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚ä¸ºæé«˜äº¤æ¢é€Ÿåº¦ï¼Œé¿å…è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æ¢¯åº¦ï¼Œè€Œæ˜¯å®šæœŸè®¡ç®—æ¢¯åº¦ä»¥å‡å°‘å‰å‘ä¼ é€’æ¬¡æ•°ã€‚æœ€ç»ˆå»ºç«‹äº†åŸºå‡†æ•°æ®é›†è¿›è¡Œç»¼åˆè¯„ä»·ï¼Œè¯æ˜äº†InstantSwapçš„ä¼˜è¶Šæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InstantSwapæ˜¯ä¸€ç§æ–°çš„å®šåˆ¶æ¦‚å¿µäº¤æ¢ï¼ˆCCSï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å½¢çŠ¶å·®å¼‚å¤§ã€å‰åæ™¯ä¸ä¸€è‡´åŠæ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚</li>
<li>InstantSwapé€šè¿‡æ³¨æ„åŠ›å›¾åˆ†æè‡ªåŠ¨æå–ç›®æ ‡å¯¹è±¡çš„bboxï¼Œå®ç°å‰æ™¯å’ŒèƒŒæ™¯ä¸€è‡´æ€§ã€‚</li>
<li>å¯¹äºèƒŒæ™¯ä¸€è‡´æ€§ï¼ŒInstantSwapåœ¨äº¤æ¢è¿‡ç¨‹ä¸­å»é™¤bboxå¤–çš„æ¢¯åº¦ï¼Œé¿å…èƒŒæ™¯è¢«ä¿®æ”¹ã€‚</li>
<li>å¯¹äºå‰æ™¯ä¸€è‡´æ€§ï¼ŒInstantSwapé‡‡ç”¨è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è¯­ä¹‰ä¿¡æ¯æ³¨å…¥æºå’Œç›®æ ‡æ¦‚å¿µä¸­ï¼Œå­¦ä¹ è¯­ä¹‰å¢å¼ºçš„è¡¨ç¤ºã€‚</li>
<li>InstantSwapé€šè¿‡å®šæœŸè®¡ç®—æ¢¯åº¦æé«˜äº¤æ¢é€Ÿåº¦ï¼Œå‡å°‘å‰å‘ä¼ é€’æ¬¡æ•°ï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>InstantSwapå»ºç«‹äº†åŸºå‡†æ•°æ®é›†ï¼Œä¾¿äºè¿›è¡Œç»¼åˆè¯„ä»·ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e701fdb69a86c48d35ef2d36a7a633a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fa463deff7c17942af43bd24d9977a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-871c468b6665a26faffe4c5676bf9d44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79d8c530a223fa85974e2a7b72c453cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c9e15a979bb206fcb4f9991f4e3f690.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab74eb31e5bd08c6774167f0fd7f1b9d.jpg" align="middle">
</details>




<h2 id="GradiSeg-Gradient-Guided-Gaussian-Segmentation-with-Enhanced-3D-Boundary-Precision"><a href="#GradiSeg-Gradient-Guided-Gaussian-Segmentation-with-Enhanced-3D-Boundary-Precision" class="headerlink" title="GradiSeg: Gradient-Guided Gaussian Segmentation with Enhanced 3D   Boundary Precision"></a>GradiSeg: Gradient-Guided Gaussian Segmentation with Enhanced 3D   Boundary Precision</h2><p><strong>Authors:Zehao Li, Wenwei Han, Yujun Cai, Hao Jiang, Baolong Bi, Shuqin Gao, Honglong Zhao, Zhaoqi Wang</strong></p>
<p>While 3D Gaussian Splatting enables high-quality real-time rendering, existing Gaussian-based frameworks for 3D semantic segmentation still face significant challenges in boundary recognition accuracy. To address this, we propose a novel 3DGS-based framework named GradiSeg, incorporating Identity Encoding to construct a deeper semantic understanding of scenes. Our approach introduces two key modules: Identity Gradient Guided Densification (IGD) and Local Adaptive K-Nearest Neighbors (LA-KNN). The IGD module supervises gradients of Identity Encoding to refine Gaussian distributions along object boundaries, aligning them closely with boundary contours. Meanwhile, the LA-KNN module employs position gradients to adaptively establish locality-aware propagation of Identity Encodings, preventing irregular Gaussian spreads near boundaries. We validate the effectiveness of our method through comprehensive experiments. Results show that GradiSeg effectively addresses boundary-related issues, significantly improving segmentation accuracy without compromising scene reconstruction quality. Furthermore, our methodâ€™s robust segmentation capability and decoupled Identity Encoding representation make it highly suitable for various downstream scene editing tasks, including 3D object removal, swapping and so on. </p>
<blockquote>
<p>è™½ç„¶3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯èƒ½å¤Ÿå®ç°é«˜è´¨é‡å®æ—¶æ¸²æŸ“ï¼Œä½†ç°æœ‰çš„åŸºäºé«˜æ–¯æ–¹æ³•çš„3Dè¯­ä¹‰åˆ†å‰²æ¡†æ¶åœ¨è¾¹ç•Œè¯†åˆ«ç²¾åº¦æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäº3DGSçš„æ¡†æ¶ï¼Œåä¸ºGradiSegï¼Œå®ƒç»“åˆäº†èº«ä»½ç¼–ç æ¥æ„å»ºå¯¹åœºæ™¯çš„æ·±å±‚æ¬¡è¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šèº«ä»½æ¢¯åº¦å¼•å¯¼å¯†é›†åŒ–ï¼ˆIGDï¼‰å’Œå±€éƒ¨è‡ªé€‚åº”Kè¿‘é‚»ï¼ˆLA-KNNï¼‰ã€‚IGDæ¨¡å—é€šè¿‡ç›‘ç£èº«ä»½ç¼–ç çš„æ¢¯åº¦æ¥ä¼˜åŒ–å¯¹è±¡è¾¹ç•Œçš„é«˜æ–¯åˆ†å¸ƒï¼Œä½¿å…¶ç´§å¯†å¯¹é½è¾¹ç•Œè½®å»“ã€‚åŒæ—¶ï¼ŒLA-KNNæ¨¡å—åˆ©ç”¨ä½ç½®æ¢¯åº¦è‡ªé€‚åº”åœ°å»ºç«‹èº«ä»½ç¼–ç çš„å±€éƒ¨æ„ŸçŸ¥ä¼ æ’­ï¼Œé˜²æ­¢è¾¹ç•Œé™„è¿‘çš„é«˜æ–¯åˆ†å¸ƒå‡ºç°ä¸è§„åˆ™æ‰©æ•£ã€‚æˆ‘ä»¬é€šè¿‡å…¨é¢çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒGradiSegæœ‰æ•ˆåœ°è§£å†³äº†è¾¹ç•Œç›¸å…³çš„é—®é¢˜ï¼Œåœ¨ä¸å½±å“åœºæ™¯é‡å»ºè´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ–¹æ³•çš„ç¨³å¥åˆ†å‰²èƒ½åŠ›å’Œè§£è€¦çš„èº«ä»½ç¼–ç è¡¨ç¤ºä½¿å…¶æˆä¸ºå„ç§ä¸‹æ¸¸åœºæ™¯ç¼–è¾‘ä»»åŠ¡çš„é«˜åº¦åˆé€‚é€‰æ‹©ï¼ŒåŒ…æ‹¬3Då¯¹è±¡ç§»é™¤ã€æ›¿æ¢ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00392v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº3DGSçš„æ–°å‹æ¡†æ¶GradiSegï¼Œé€šè¿‡å¼•å…¥èº«ä»½ç¼–ç ï¼ˆIdentity Encodingï¼‰æŠ€æœ¯ï¼Œå¢å¼ºå¯¹åœºæ™¯è¯­ä¹‰çš„æ·±å±‚æ¬¡ç†è§£ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šèº«ä»½æ¢¯åº¦å¼•å¯¼å¯†é›†åŒ–ï¼ˆIGDï¼‰å’Œå±€éƒ¨è‡ªé€‚åº”Kè¿‘é‚»ï¼ˆLA-KNNï¼‰ã€‚IGDæ¨¡å—é€šè¿‡ç›‘ç£èº«ä»½ç¼–ç çš„æ¢¯åº¦æ¥ç»†åŒ–å¯¹è±¡è¾¹ç•Œçš„é«˜æ–¯åˆ†å¸ƒï¼Œä½¿å…¶ä¸è¾¹ç•Œè½®å»“ç´§å¯†å¯¹é½ï¼›è€ŒLA-KNNæ¨¡å—åˆ™åˆ©ç”¨ä½ç½®æ¢¯åº¦è‡ªé€‚åº”åœ°å»ºç«‹èº«ä»½ç¼–ç çš„å±€éƒ¨ä¼ æ’­ï¼Œé˜²æ­¢è¾¹ç•Œé™„è¿‘çš„é«˜æ–¯åˆ†å¸ƒä¸è§„åˆ™æ‰©æ•£ã€‚å®éªŒè¯æ˜ï¼ŒGradiSegèƒ½æœ‰æ•ˆè§£å†³è¾¹ç•Œç›¸å…³é—®é¢˜ï¼Œæ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ï¼ŒåŒæ—¶ä¸æŸå®³åœºæ™¯é‡å»ºè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„GradiSegæ¡†æ¶åŸºäº3DGSï¼Œèå…¥Identity EncodingæŠ€æœ¯ï¼Œæ·±åŒ–äº†å¯¹åœºæ™¯è¯­ä¹‰çš„ç†è§£ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šIGDå’ŒLA-KNNï¼Œåˆ†åˆ«å¤„ç†é«˜æ–¯åˆ†å¸ƒçš„ç»†åŒ–å’Œè¾¹ç•Œé™„è¿‘çš„ä¸è§„åˆ™æ‰©æ•£é—®é¢˜ã€‚</li>
<li>IGDæ¨¡å—é€šè¿‡ç›‘ç£èº«ä»½ç¼–ç çš„æ¢¯åº¦æ¥ç»†åŒ–å¯¹è±¡è¾¹ç•Œçš„é«˜æ–¯åˆ†å¸ƒã€‚</li>
<li>LA-KNNæ¨¡å—åˆ©ç”¨ä½ç½®æ¢¯åº¦å»ºç«‹èº«ä»½ç¼–ç çš„å±€éƒ¨ä¼ æ’­ï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>GradiSegåœ¨è§£å†³è¾¹ç•Œé—®é¢˜çš„åŒæ—¶ï¼Œä¸æŸå®³åœºæ™¯é‡å»ºè´¨é‡ã€‚</li>
<li>GradiSegå…·æœ‰å¼ºå¤§çš„åˆ†å‰²èƒ½åŠ›å’Œè§£è€¦çš„èº«ä»½ç¼–ç è¡¨ç¤ºï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸åœºæ™¯ç¼–è¾‘ä»»åŠ¡ï¼Œå¦‚3Då¯¹è±¡ç§»é™¤ã€æ›¿æ¢ç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a583707f4fc91caafd2ba05f74ca9716.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-024b3ed7eb9e1a926dc58c850f603c2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2d1b628c051dfa2eb2d0580ab3bf13a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb6d133f0a1612e9bb04350b25d3ebe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3645fce364364725194b055717dc79b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c7329c71a6c7c8ef95086a9602b592f.jpg" align="middle">
</details>




<h2 id="HiFiVFS-High-Fidelity-Video-Face-Swapping"><a href="#HiFiVFS-High-Fidelity-Video-Face-Swapping" class="headerlink" title="HiFiVFS: High Fidelity Video Face Swapping"></a>HiFiVFS: High Fidelity Video Face Swapping</h2><p><strong>Authors:Xu Chen, Keke He, Junwei Zhu, Yanhao Ge, Wei Li, Chengjie Wang</strong></p>
<p>Face swapping aims to generate results that combine the identity from the source with attributes from the target. Existing methods primarily focus on image-based face swapping. When processing videos, each frame is handled independently, making it difficult to ensure temporal stability. From a model perspective, face swapping is gradually shifting from generative adversarial networks (GANs) to diffusion models (DMs), as DMs have been shown to possess stronger generative capabilities. Current diffusion-based approaches often employ inpainting techniques, which struggle to preserve fine-grained attributes like lighting and makeup. To address these challenges, we propose a high fidelity video face swapping (HiFiVFS) framework, which leverages the strong generative capability and temporal prior of Stable Video Diffusion (SVD). We build a fine-grained attribute module to extract identity-disentangled and fine-grained attribute features through identity desensitization and adversarial learning. Additionally, We introduce detailed identity injection to further enhance identity similarity. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) in video face swapping, both qualitatively and quantitatively. </p>
<blockquote>
<p>é¢éƒ¨æ›¿æ¢æ—¨åœ¨ç”Ÿæˆç»“åˆæºèº«ä»½å’Œç›®æ ‡å±æ€§çš„ç»“æœã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨åŸºäºå›¾åƒçš„é¢éƒ¨æ›¿æ¢ä¸Šã€‚åœ¨å¤„ç†è§†é¢‘æ—¶ï¼Œæ¯ä¸€å¸§éƒ½æ˜¯ç‹¬ç«‹å¤„ç†çš„ï¼Œè¿™å¾ˆéš¾ä¿è¯æ—¶é—´ç¨³å®šæ€§ã€‚ä»æ¨¡å‹çš„è§’åº¦æ¥çœ‹ï¼Œé¢éƒ¨æ›¿æ¢æ­£åœ¨é€æ¸ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå› ä¸ºDMså·²æ˜¾ç¤ºå‡ºå…·æœ‰æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ã€‚å½“å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç»å¸¸é‡‡ç”¨ä¿®å¤æŠ€æœ¯ï¼Œè¿™å¾ˆéš¾ä¿ç•™ç…§æ˜å’Œå¦†å®¹ç­‰ç²¾ç»†å±æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜ä¿çœŸè§†é¢‘é¢éƒ¨æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—æ€§å­¦ä¹ æå–èº«ä»½åˆ†æ•£å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯¦ç»†çš„èº«ä»½æ³¨å…¥ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘é¢éƒ¨æ›¿æ¢ä¸­æ— è®ºæ˜¯å®šæ€§è¿˜æ˜¯å®šé‡è¯„ä¼°éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18293v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>è¯¥æŠ€æœ¯æå‡ºä¸€ç§é«˜ä¿çœŸè§†é¢‘äººè„¸æ›¿æ¢ï¼ˆHiFiVFSï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘äººè„¸æ›¿æ¢ä¸­çš„æ—¶é—´ç¨³å®šæ€§å’Œç²¾ç»†å±æ€§ä¿ç•™é—®é¢˜ã€‚å®ƒç»“åˆäº†ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒçŸ¥è¯†ï¼Œå»ºç«‹äº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—æ€§å­¦ä¹ æå–èº«ä»½åˆ†ç¦»å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥è¯¦ç»†èº«ä»½æ³¨å…¥æŠ€æœ¯è¿›ä¸€æ­¥å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘äººè„¸æ›¿æ¢æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘äººè„¸æ›¿æ¢é¢ä¸´æ—¶é—´ç¨³å®šæ€§å’Œç²¾ç»†å±æ€§ä¿ç•™çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒäººè„¸æ›¿æ¢ï¼Œå¤„ç†è§†é¢‘æ—¶ç‹¬ç«‹å¤„ç†æ¯ä¸€å¸§ï¼Œéš¾ä»¥ç¡®ä¿æ—¶é—´ç¨³å®šæ€§ã€‚</li>
<li>äººè„¸æ›¿æ¢æ¨¡å‹é€æ¸ä»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬å‘æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰ï¼Œå› ä¸ºDMså±•ç°å‡ºæ›´å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹é‡‡ç”¨åŸºäºä¿®å¤çš„æŠ€æœ¯ï¼Œéš¾ä»¥ä¿ç•™å…‰ç…§å’Œå¦†å®¹ç­‰ç²¾ç»†å±æ€§ã€‚</li>
<li>æå‡ºçš„HiFiVFSæ¡†æ¶ç»“åˆäº†ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„ç”Ÿæˆèƒ½åŠ›å’Œæ—¶é—´å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªç²¾ç»†å±æ€§æ¨¡å—ï¼Œé€šè¿‡èº«ä»½è„±æ•å’Œå¯¹æŠ—æ€§å­¦ä¹ æå–èº«ä»½åˆ†ç¦»å’Œç²¾ç»†å±æ€§ç‰¹å¾ã€‚</li>
<li>å¼•å…¥è¯¦ç»†èº«ä»½æ³¨å…¥æŠ€æœ¯å¢å¼ºèº«ä»½ç›¸ä¼¼æ€§ï¼Œè¾¾åˆ°å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-237d77b3aa4044afa7c8f5589ddec756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31457317ca85d09e8d2b9d29ee530829.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52192381a3ec2d89bb7e63e4b4659e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad5d321886942a17610359ff7f9c6e2.jpg" align="middle">
</details>




<h2 id="Facial-Features-Matter-a-Dynamic-Watermark-based-Proactive-Deepfake-Detection-Approach"><a href="#Facial-Features-Matter-a-Dynamic-Watermark-based-Proactive-Deepfake-Detection-Approach" class="headerlink" title="Facial Features Matter: a Dynamic Watermark based Proactive Deepfake   Detection Approach"></a>Facial Features Matter: a Dynamic Watermark based Proactive Deepfake   Detection Approach</h2><p><strong>Authors:Shulin Lan, Kanlin Liu, Yazhou Zhao, Chen Yang, Yingchao Wang, Xingshan Yao, Liehuang Zhu</strong></p>
<p>Current passive deepfake face-swapping detection methods encounter significance bottlenecks in model generalization capabilities. Meanwhile, proactive detection methods often use fixed watermarks which lack a close relationship with the content they protect and are vulnerable to security risks. Dynamic watermarks based on facial features offer a promising solution, as these features provide unique identifiers. Therefore, this paper proposes a Facial Feature-based Proactive deepfake detection method (FaceProtect), which utilizes changes in facial characteristics during deepfake manipulation as a novel detection mechanism. We introduce a GAN-based One-way Dynamic Watermark Generating Mechanism (GODWGM) that uses 128-dimensional facial feature vectors as inputs. This method creates irreversible mappings from facial features to watermarks, enhancing protection against various reverse inference attacks. Additionally, we propose a Watermark-based Verification Strategy (WVS) that combines steganography with GODWGM, allowing simultaneous transmission of the benchmark watermark representing facial features within the image. Experimental results demonstrate that our proposed method maintains exceptional detection performance and exhibits high practicality on images altered by various deepfake techniques. </p>
<blockquote>
<p>å½“å‰è¢«åŠ¨å¼çš„æ·±åº¦ä¼ªé€ é¢éƒ¨æ›¿æ¢æ£€æµ‹æŠ€æœ¯åœ¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ–¹é¢é‡åˆ°é‡å¤§ç“¶é¢ˆã€‚åŒæ—¶ï¼Œä¸»åŠ¨æ£€æµ‹æ–¹æ³•é€šå¸¸ä½¿ç”¨å›ºå®šæ°´å°ï¼Œè¿™äº›æ°´å°ä¸å…¶ä¿æŠ¤çš„å†…å®¹ç¼ºä¹å¯†åˆ‡è”ç³»ï¼Œå¹¶å­˜åœ¨å®‰å…¨é£é™©ã€‚åŸºäºé¢éƒ¨ç‰¹å¾çš„åŠ¨æ€æ°´å°æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºè¿™äº›ç‰¹å¾æä¾›äº†ç‹¬ç‰¹çš„æ ‡è¯†ç¬¦ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé¢éƒ¨ç‰¹å¾çš„ä¸»åŠ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ï¼ˆFaceProtectï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ·±åº¦ä¼ªé€ æ“ä½œæœŸé—´é¢éƒ¨ç‰¹å¾çš„å˜åŒ–ä½œä¸ºæ–°å‹æ£€æµ‹æœºåˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºGANçš„å•å‘åŠ¨æ€æ°´å°ç”Ÿæˆæœºåˆ¶ï¼ˆGODWGMï¼‰ï¼Œè¯¥æœºåˆ¶ä½¿ç”¨128ç»´é¢éƒ¨ç‰¹å¾å‘é‡ä½œä¸ºè¾“å…¥ã€‚è¿™ç§æ–¹æ³•åˆ›å»ºäº†ä»é¢éƒ¨ç‰¹å¾åˆ°æ°´å°çš„ä¸å¯é€†æ˜ å°„ï¼Œå¢å¼ºäº†å¯¹æŠ—å„ç§åå‘æ¨ç†æ”»å‡»çš„ä¿æŠ¤èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ°´å°çš„éªŒè¯ç­–ç•¥ï¼ˆWVSï¼‰ï¼Œå®ƒå°†éšå†™æœ¯ä¸GODWGMç›¸ç»“åˆï¼Œå…è®¸åœ¨å›¾åƒä¸­åŒæ—¶ä¼ è¾“ä»£è¡¨é¢éƒ¨ç‰¹å¾çš„åŸºå‡†æ°´å°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨ç”±å„ç§æ·±åº¦ä¼ªé€ æŠ€æœ¯ä¿®æ”¹çš„å›¾åƒä¸Šä¿æŒäº†å‡ºè‰²çš„æ£€æµ‹æ€§èƒ½ï¼Œå¹¶è¡¨ç°å‡ºé«˜åº¦çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14798v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºé¢éƒ¨ç‰¹å¾çš„åŠ¨æ€æ°´å°å¯¹Deepfakeäººè„¸æ›¿æ¢æŠ€æœ¯ä¸­çš„å›¾åƒä¼ªé€ è¡Œä¸ºè¿›è¡Œç§¯ææ£€æµ‹æä¾›äº†å¯è¡Œæ€§è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡é€šè¿‡ç»“åˆGANæŠ€æœ¯çš„å•å‘åŠ¨æ€æ°´å°ç”Ÿæˆæœºåˆ¶ï¼Œä»¥åˆ©ç”¨é¢éƒ¨ç‰¹å¾å˜åŒ–ä½œä¸ºä¼ªé€ è¡Œä¸ºçš„æ£€æµ‹ä¾æ®ã€‚æ­¤æœºåˆ¶å¯ä»¥æŠµæŠ—é€†å‘æ¨ç†æ”»å‡»å¹¶å¼ºåŒ–æ°´å°çš„ä¿æŠ¤èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡ç»“åˆéšå†™æœ¯å’Œç”Ÿæˆæœºåˆ¶çš„æ°´å°éªŒè¯ç­–ç•¥å¯ä»¥åœ¨å›¾åƒå†…éƒ¨åŒæ­¥ä¼ è¾“é¢éƒ¨ç‰¹å¾çš„æ°´å°æ ‡è®°ï¼Œå®ç°é«˜æ•ˆæ£€æµ‹ã€‚å®éªŒè¯æ˜ï¼Œæ­¤æ–¹æ³•åœ¨å¤šç§DeepfakeæŠ€æœ¯å¤„ç†è¿‡çš„å›¾åƒä¸Šä»èƒ½ä¿æŒå‡ºè‰²çš„æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰è¢«åŠ¨å¼Deepfakeæ£€æµ‹æŠ€æœ¯å­˜åœ¨æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿä¸»åŠ¨æ£€æµ‹æŠ€æœ¯é‡‡ç”¨å›ºå®šæ°´å°ï¼Œä¸ä¿æŠ¤å†…å®¹å…³è”åº¦ä½ä¸”å­˜åœ¨å®‰å…¨é£é™©ã€‚</li>
<li>åŸºäºé¢éƒ¨ç‰¹å¾çš„åŠ¨æ€æ°´å°æä¾›äº†ä¸€ç§æ–°é¢–è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨é¢éƒ¨ç‰¹å¾ä½œä¸ºç‹¬ç‰¹æ ‡è¯†ç¬¦ã€‚</li>
<li>å¼•å…¥çš„FaceProtectæ–¹æ³•åˆ©ç”¨Deepfakeæ“ä½œè¿‡ç¨‹ä¸­çš„é¢éƒ¨ç‰¹å¾å˜åŒ–ä½œä¸ºæ£€æµ‹æœºåˆ¶ã€‚</li>
<li>GANæŠ€æœ¯ç”¨äºåˆ›å»ºä¸å¯é€†çš„æ˜ å°„å…³ç³»ï¼Œå¢å¼ºæ°´å°ä¿æŠ¤èƒ½åŠ›å¯¹æŠ—é€†å‘æ¨ç†æ”»å‡»ã€‚</li>
<li>ç»“åˆéšå†™æœ¯å’Œæ°´å°ç”Ÿæˆæœºåˆ¶çš„éªŒè¯ç­–ç•¥å®ç°é«˜æ•ˆéªŒè¯ï¼Œå…è®¸æ°´å°æ ‡è®°åœ¨å›¾åƒå†…éƒ¨åŒæ­¥ä¼ è¾“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a20f4c014c335315e040296eb2d0d42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44851e3a105adcbaf9c39543cf66fd2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-515cc2166e1d1df38771f1acbe5215fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68ee4b8f1ba77443d4ce1cc64eaef2bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1558726c84cf75e40df740d740d68f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e25fbc98103f0d29c0308adb0e2f3375.jpg" align="middle">
</details>




<h2 id="Saliency-Based-diversity-and-fairness-Metric-and-FaceKeepOriginalAugment-A-Novel-Approach-for-Enhancing-Fairness-and-Diversity"><a href="#Saliency-Based-diversity-and-fairness-Metric-and-FaceKeepOriginalAugment-A-Novel-Approach-for-Enhancing-Fairness-and-Diversity" class="headerlink" title="Saliency-Based diversity and fairness Metric and   FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and   Diversity"></a>Saliency-Based diversity and fairness Metric and   FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and   Diversity</h2><p><strong>Authors:Teerath Kumar, Alessandra Mileo, Malika Bendechache</strong></p>
<p>Data augmentation has become a pivotal tool in enhancing the performance of computer vision tasks, with the KeepOriginalAugment method emerging as a standout technique for its intelligent incorporation of salient regions within less prominent areas, enabling augmentation in both regions. Despite its success in image classification, its potential in addressing biases remains unexplored. In this study, we introduce an extension of the KeepOriginalAugment method, termed FaceKeepOriginalAugment, which explores various debiasing aspects-geographical, gender, and stereotypical biases-in computer vision models. By maintaining a delicate balance between data diversity and information preservation, our approach empowers models to exploit both diverse salient and non-salient regions, thereby fostering increased diversity and debiasing effects. We investigate multiple strategies for determining the placement of the salient region and swapping perspectives to decide which part undergoes augmentation. Leveraging the Image Similarity Score (ISS), we quantify dataset diversity across a range of datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating gender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing the Image-Image Association Score (IIAS) in convolutional neural networks (CNNs) and vision transformers (ViTs). Our findings shows the efficacy of FaceKeepOriginalAugment in promoting fairness and inclusivity within computer vision models, demonstrated by reduced gender bias and enhanced overall fairness. Additionally, we introduce a novel metric, Saliency-Based Diversity and Fairness Metric, which quantifies both diversity and fairness while handling data imbalance across various datasets. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºå·²æˆä¸ºæé«˜è®¡ç®—æœºè§†è§‰ä»»åŠ¡æ€§èƒ½çš„å…³é”®å·¥å…·ï¼Œå…¶ä¸­KeepOriginalAugmentæ–¹æ³•å› å…¶æ™ºèƒ½åœ°å°†æ˜¾è‘—åŒºåŸŸèå…¥ä¸å¤ªçªå‡ºçš„åŒºåŸŸè€Œè„±é¢–è€Œå‡ºï¼Œèƒ½å¤Ÿåœ¨ä¸¤ä¸ªåŒºåŸŸä¸­è¿›è¡Œå¢å¼ºã€‚å°½ç®¡å®ƒåœ¨å›¾åƒåˆ†ç±»æ–¹é¢çš„è¡¨ç°å¾ˆå‡ºè‰²ï¼Œä½†å…¶è§£å†³åè§é—®é¢˜çš„æ½œåŠ›å°šæœªå¾—åˆ°æ¢ç´¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†KeepOriginalAugmentæ–¹æ³•çš„æ‰©å±•ç‰ˆæœ¬ï¼Œç§°ä¸ºFaceKeepOriginalAugmentï¼Œå®ƒæ¢ç´¢äº†è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„å„ç§å»åæ–¹é¢ï¼ŒåŒ…æ‹¬åœ°ç†ã€æ€§åˆ«å’Œåˆ»æ¿å°è±¡åè§ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æ•°æ®å¤šæ ·æ€§å’Œä¿¡æ¯ä¿ç•™ä¹‹é—´ä¿æŒå¾®å¦™çš„å¹³è¡¡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤šæ ·åŒ–å’Œéå¤šæ ·åŒ–çš„æ˜¾è‘—åŒºåŸŸï¼Œä»è€Œä¿ƒè¿›å¢åŠ å¤šæ ·æ€§å’Œå»åè§æ•ˆæœã€‚æˆ‘ä»¬ç ”ç©¶å¤šç§ç­–ç•¥æ¥ç¡®å®šæ˜¾è‘—åŒºåŸŸçš„ä½ç½®å¹¶æ”¹å˜è§†è§’ï¼Œä»¥ç¡®å®šå“ªäº›éƒ¨åˆ†éœ€è¦è¿›è¡Œå¢å¼ºã€‚æˆ‘ä»¬åˆ©ç”¨å›¾åƒç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆISSï¼‰æ¥é‡åŒ–å¤šä¸ªæ•°æ®é›†ï¼ˆåŒ…æ‹¬Flickr Faces HQï¼ˆFFHQï¼‰ã€WIKIã€IMDBã€é‡å¤–äººè„¸ï¼ˆLFWï¼‰ã€UTKäººè„¸å’Œå¤šæ ·åŒ–æ•°æ®é›†ï¼‰çš„æ•°æ®é›†å¤šæ ·æ€§ã€‚æˆ‘ä»¬è¯„ä¼°äº†FaceKeepOriginalAugmentåœ¨å‡è½»CEOã€å·¥ç¨‹å¸ˆã€æŠ¤å£«å’Œæ•™å¸ˆæ•°æ®é›†ä¸­çš„æ€§åˆ«åè§æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸­çš„å›¾åƒå…³è”å¾—åˆ†ï¼ˆIIASï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒFaceKeepOriginalAugmentåœ¨ä¿ƒè¿›è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„å…¬å¹³æ€§å’ŒåŒ…å®¹æ€§æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œè¡¨ç°ä¸ºå‡å°‘äº†æ€§åˆ«åè§å¹¶æé«˜äº†æ•´ä½“å…¬å¹³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†â€”â€”åŸºäºæ˜¾è‘—æ€§çš„å¤šæ ·æ€§å’Œå…¬å¹³æ€§åº¦é‡ï¼Œè¯¥åº¦é‡æ ‡å‡†å¯ä»¥é‡åŒ–å¤šæ ·æ€§å’Œå…¬å¹³æ€§ï¼ŒåŒæ—¶å¤„ç†å„ç§æ•°æ®é›†ä¸­çš„æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00831v1">PDF</a> Paper is underReview in Image and Vision Computing Journal special   issue: Advancing Transparency and Privacy: Explainable AI and Synthetic Data   in Biometrics and Computer Vision</p>
<p><strong>æ‘˜è¦</strong><br>æœ¬æ®µæ–‡å­—è¯¦ç»†ä»‹ç»äº†Data augmentationåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„å…³é”®ä½œç”¨ä»¥åŠæ–°å…´æŠ€æœ¯KeepOriginalAugmentçš„åº”ç”¨æ•ˆæœã€‚ç„¶è€Œå…¶åº”å¯¹åè§çš„é—®é¢˜ä»å¾…æ¢ç´¢ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºä¸€ç§KeepOriginalAugmentçš„æ‰©å±•æ–¹æ³•FaceKeepOriginalAugmentï¼Œå…¶æ¢è®¨è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„åœ°ç†ã€æ€§åˆ«å’Œåˆ»æ¿å°è±¡ç­‰æ–¹é¢çš„åè§é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡å¹³è¡¡æ•°æ®å¤šæ ·æ€§å’Œä¿¡æ¯ä¿ç•™æ¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶é‡‡ç”¨å›¾åƒç›¸ä¼¼åº¦è¯„åˆ†æ¥é‡åŒ–æ•°æ®é›†å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†FaceKeepOriginalAugmentåœ¨ç¼“è§£æ€§åˆ«åè§æ–¹é¢çš„æ•ˆæœï¼ŒåŒæ—¶ä»‹ç»äº†ä¸€ç§æ–°å‹è¯„ä»·æŒ‡æ ‡æ¥é‡åŒ–å¤šæ ·æ€§å’Œå…¬å¹³æ€§ã€‚ç ”ç©¶è¡¨æ˜FaceKeepOriginalAugmentå¯æœ‰æ•ˆä¿ƒè¿›è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„å…¬å¹³æ€§å’ŒåŒ…å®¹æ€§ï¼Œæœ‰åŠ©äºå‡å°‘æ€§åˆ«åè§å¹¶æé«˜æ•´ä½“å…¬å¹³æ€§ã€‚</p>
<p><strong>å…³é”®æ”¶è·ç‚¹</strong></p>
<ul>
<li>æ•°æ®å¢å¼ºå·²æˆä¸ºæé«˜è®¡ç®—æœºè§†è§‰ä»»åŠ¡æ€§èƒ½çš„é‡è¦å·¥å…·ï¼Œè€ŒKeepOriginalAugmentæ–¹æ³•é€šè¿‡æ™ºèƒ½èåˆæ˜¾è‘—åŒºåŸŸå’Œéæ˜¾è‘—åŒºåŸŸå®ç°äº†å¢å¼ºæ•ˆæœã€‚</li>
<li>å°½ç®¡åœ¨å›¾åƒåˆ†ç±»ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†KeepOriginalAugmentåœ¨è§£å†³åè§é—®é¢˜æ–¹é¢çš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶äººå‘˜æå‡ºäº†FaceKeepOriginalAugmentæ–¹æ³•ï¼Œè¿™æ˜¯KeepOriginalAugmentçš„ä¸€ä¸ªæ‰©å±•ï¼Œä¸“æ³¨äºè§£å†³è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„åœ°ç†ã€æ€§åˆ«å’Œåˆ»æ¿å°è±¡åè§ã€‚</li>
<li>FaceKeepOriginalAugmenté€šè¿‡å¹³è¡¡æ•°æ®å¤šæ ·æ€§å’Œä¿¡æ¯ä¿ç•™æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œå¹¶é‡‡ç”¨å›¾åƒç›¸ä¼¼åº¦è¯„åˆ†æ¥é‡åŒ–æ•°æ®é›†å¤šæ ·æ€§ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†FaceKeepOriginalAugmentåœ¨å‡å°‘æ€§åˆ«åè§æ–¹é¢çš„æ•ˆæœï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„è¯„ä»·æŒ‡æ ‡æ¥åŒæ—¶é‡åŒ–å¤šæ ·æ€§å’Œå…¬å¹³æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-04f3961544d0d32ee6cc02f95d51a2c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b06afd4c4a648f5ac9fd2c8374acaee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39b529128186cf40e2e1ecc70f88ee07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-110ba4f138a9f95d7911daa0ed9fd68d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6679a4aeba4168a83e725435db5d01b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0cb95f8491dd400fd55c4bfae7f3d432.jpg" align="middle">
</details>




<h2 id="Face-Anonymization-Made-Simple"><a href="#Face-Anonymization-Made-Simple" class="headerlink" title="Face Anonymization Made Simple"></a>Face Anonymization Made Simple</h2><p><strong>Authors:Han-Wei Kung, Tuomas Varanka, Sanjay Saha, Terence Sim, Nicu Sebe</strong></p>
<p>Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/hanweikung/face_anon_simple">https://github.com/hanweikung/face_anon_simple</a> . </p>
<blockquote>
<p>å½“å‰çš„é¢éƒ¨åŒ¿ååŒ–æŠ€æœ¯é€šå¸¸ä¾èµ–äºé¢éƒ¨è¯†åˆ«æ¨¡å‹è®¡ç®—çš„èº«ä»½ä¸¢å¤±ï¼Œè¿™å¯èƒ½ä¼šä¸å‡†ç¡®ä¸”ä¸å¯é ã€‚æ­¤å¤–ï¼Œè®¸å¤šæ–¹æ³•éœ€è¦é¢å¤–çš„æ•°æ®ï¼ˆä¾‹å¦‚é¢éƒ¨æ ‡è®°å’Œé¢å…·ï¼‰æ¥æŒ‡å¯¼åˆæˆè¿‡ç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨æ‰©æ•£æ¨¡å‹ä¸é‡å»ºæŸå¤±ï¼Œæ— éœ€é¢éƒ¨æ ‡è®°æˆ–é¢å…·ï¼ŒåŒæ—¶ä»èƒ½äº§ç”Ÿå…·æœ‰ç²¾ç»†ç»†èŠ‚çš„å›¾åƒã€‚æˆ‘ä»¬é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°åœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„ç»“æœã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨èº«ä»½åŒ¿ååŒ–ã€é¢éƒ¨ç‰¹å¾ä¿ç•™å’Œå›¾åƒè´¨é‡ä¸‰ä¸ªå…³é”®é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é™¤äº†åŒ¿ååŒ–çš„ä¸»è¦åŠŸèƒ½å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å¯ä»¥é€šè¿‡å°†é¢å¤–çš„é¢éƒ¨å›¾åƒä½œä¸ºè¾“å…¥æ¥æ‰§è¡Œé¢éƒ¨æ›¿æ¢ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶å¤šæ ·æ€§å’Œå¤šç§åº”ç”¨çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanweikung/face_anon_simple%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanweikung/face_anon_simpleæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00762v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“å‰é¢éƒ¨åŒ¿ååŒ–æŠ€æœ¯çš„é—®é¢˜ï¼Œå¦‚ä¾èµ–ä¸å‡†ç¡®å’Œä¸å¯é çš„èº«ä»½æŸå¤±è®¡ç®—æ–¹å¼ï¼Œä»¥åŠéœ€è¦é¢å¤–çš„é¢éƒ¨æ ‡å¿—å’Œæ©è†œæ¥æŒ‡å¯¼åˆæˆè¿‡ç¨‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ–°æ–¹æ³•é‡‡ç”¨æ‰©æ•£æ¨¡å‹ä¸é‡å»ºæŸå¤±ç›¸ç»“åˆï¼Œæ— éœ€é¢éƒ¨æ ‡å¿—æˆ–æ©è†œï¼Œç”Ÿæˆå›¾åƒä»å…·æœ‰ç²¾ç»†çš„ç»†èŠ‚ã€‚åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å®šé‡å’Œå®šæ€§è¯„ä¼°éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨èº«ä»½åŒ¿ååŒ–ã€é¢éƒ¨ç‰¹å¾ä¿ç•™å’Œå›¾åƒè´¨é‡æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜å…·æœ‰é¢éƒ¨æ›¿æ¢åŠŸèƒ½ï¼Œå¯é€šè¿‡å¼•å…¥é¢å¤–çš„é¢éƒ¨å›¾åƒè¿›è¡Œè¾“å…¥ï¼Œå±•ç¤ºå…¶å¤šæ ·æ€§å’Œæ½œåœ¨åº”ç”¨æ½œåŠ›ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanweikung/face_anon_simple%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/hanweikung/face_anon_simpleä¸Šè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰é¢éƒ¨åŒ¿ååŒ–æŠ€æœ¯ä¾èµ–ä¸å‡†ç¡®å’Œä¸å¯é çš„èº«ä»½æŸå¤±è®¡ç®—æ–¹å¼ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç»“åˆé‡å»ºæŸå¤±æˆä¸ºæ–°æ–¹æ³•çš„æ ¸å¿ƒã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€é¢éƒ¨æ ‡å¿—å’Œæ©è†œæ•°æ®ï¼Œç”Ÿæˆå›¾åƒè´¨é‡ç²¾ç»†ã€‚</li>
<li>æ¨¡å‹åœ¨èº«ä»½åŒ¿ååŒ–ã€é¢éƒ¨ç‰¹å¾ä¿ç•™å’Œå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°é¢†å…ˆã€‚</li>
<li>æ¨¡å‹å…·å¤‡é¢éƒ¨æ›¿æ¢åŠŸèƒ½ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>æ¨¡å‹å·²åœ¨ä¸¤ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šé€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°éªŒè¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8745f3cf986d26fd8da5af15d62733fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f5c371ad58bb6f923b4738d19600f0a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ecde5c54f1ef5da36f59232a896fcbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0fc683a5f254083a36741a7d87fe973.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-578a41bee6f55b744fec7bb45ee5532c.jpg" align="middle">
</details>




<h2 id="FuseAnyPart-Diffusion-Driven-Facial-Parts-Swapping-via-Multiple-Reference-Images"><a href="#FuseAnyPart-Diffusion-Driven-Facial-Parts-Swapping-via-Multiple-Reference-Images" class="headerlink" title="FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple   Reference Images"></a>FuseAnyPart: Diffusion-Driven Facial Parts Swapping via Multiple   Reference Images</h2><p><strong>Authors:Zheng Yu, Yaohua Wang, Siying Cui, Aixi Zhang, Wei-Long Zheng, Senzhang Wang</strong></p>
<p>Facial parts swapping aims to selectively transfer regions of interest from the source image onto the target image while maintaining the rest of the target image unchanged. Most studies on face swapping designed specifically for full-face swapping, are either unable or significantly limited when it comes to swapping individual facial parts, which hinders fine-grained and customized character designs. However, designing such an approach specifically for facial parts swapping is challenged by a reasonable multiple reference feature fusion, which needs to be both efficient and effective. To overcome this challenge, FuseAnyPart is proposed to facilitate the seamless â€œfuse-any-partâ€ customization of the face. In FuseAnyPart, facial parts from different people are assembled into a complete face in latent space within the Mask-based Fusion Module. Subsequently, the consolidated feature is dispatched to the Addition-based Injection Module for fusion within the UNet of the diffusion model to create novel characters. Extensive experiments qualitatively and quantitatively validate the superiority and robustness of FuseAnyPart. Source codes are available at <a target="_blank" rel="noopener" href="https://github.com/Thomas-wyh/FuseAnyPart">https://github.com/Thomas-wyh/FuseAnyPart</a>. </p>
<blockquote>
<p>é¢éƒ¨éƒ¨ä»¶æ›¿æ¢æ—¨åœ¨é€‰æ‹©æ€§åœ°å°†æºå›¾åƒçš„æ„Ÿå…´è¶£åŒºåŸŸè½¬ç§»åˆ°ç›®æ ‡å›¾åƒä¸Šï¼ŒåŒæ—¶ä¿æŒç›®æ ‡å›¾åƒçš„å…¶ä»–éƒ¨åˆ†ä¸å˜ã€‚å¤§å¤šæ•°ä¸“é—¨è®¾è®¡ç”¨äºå…¨è„¸æ›¿æ¢çš„é¢éƒ¨æ›¿æ¢ç ”ç©¶ï¼Œåœ¨æ›¿æ¢å•ä¸ªé¢éƒ¨éƒ¨ä»¶æ—¶è¦ä¹ˆæ— æ³•å®ç°ï¼Œè¦ä¹ˆå—åˆ°å¾ˆå¤§é™åˆ¶ï¼Œè¿™é˜»ç¢äº†ç²¾ç»†å’Œå®šåˆ¶çš„è§’è‰²è®¾è®¡ã€‚ç„¶è€Œï¼Œä¸ºé¢éƒ¨éƒ¨ä»¶æ›¿æ¢è®¾è®¡ä¸“é—¨çš„æ–¹æ³•é¢ä¸´åˆç†çš„å¤šå‚è€ƒç‰¹å¾èåˆçš„æŒ‘æˆ˜ï¼Œè¿™éœ€è¦æ—¢é«˜æ•ˆåˆæœ‰æ•ˆã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†FuseAnyPartæ–¹æ³•ï¼Œä»¥å®ç°é¢éƒ¨æ— ç¼â€œèåˆä»»ä½•éƒ¨åˆ†â€çš„å®šåˆ¶ã€‚åœ¨FuseAnyPartä¸­ï¼Œæ¥è‡ªä¸åŒäººçš„é¢éƒ¨éƒ¨ä»¶åœ¨åŸºäºæ©è†œçš„èåˆæ¨¡å—ä¸­åœ¨æ½œåœ¨ç©ºé—´å†…ç»„åˆæˆä¸€å¼ å®Œæ•´çš„è„¸ã€‚éšåï¼Œåˆå¹¶çš„ç‰¹å¾è¢«æ´¾å‘åˆ°åŸºäºæ·»åŠ çš„æ³¨å…¥æ¨¡å—ä¸­ï¼Œåœ¨æ‰©æ•£æ¨¡å‹çš„UNetå†…è¿›è¡Œèåˆï¼Œä»¥åˆ›å»ºæ–°è§’è‰²ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†FuseAnyPartçš„ä¼˜è¶Šæ€§å’Œç¨³å¥æ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Thomas-wyh/FuseAnyPart%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Thomas-wyh/FuseAnyPartæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22771v2">PDF</a> Accepted by the NeurIPS 2024 (Spotlight). Homepage:   <a target="_blank" rel="noopener" href="https://thomas-wyh.github.io/">https://thomas-wyh.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¢éƒ¨é›¶ä»¶äº¤æ¢æ—¨åœ¨ä»æºå›¾åƒä¸­é€‰æ‹©æ€§è½¬ç§»æ„Ÿå…´è¶£çš„åŒºåŸŸåˆ°ç›®æ ‡å›¾åƒä¸Šï¼ŒåŒæ—¶ä¿æŒç›®æ ‡å›¾åƒçš„å…¶ä»–éƒ¨åˆ†ä¸å˜ã€‚å¤§å¤šæ•°ä¸“é—¨è®¾è®¡ç”¨äºå…¨è„¸äº¤æ¢çš„ç ”ç©¶ï¼Œåœ¨è¿›è¡Œé¢éƒ¨é›¶ä»¶äº¤æ¢æ—¶èƒ½åŠ›æœ‰é™æˆ–æ— æ³•å®ç°ï¼Œè¿™é˜»ç¢äº†ç²¾ç»†ç²’åº¦å’Œå®šåˆ¶åŒ–çš„è§’è‰²è®¾è®¡ã€‚ç„¶è€Œï¼Œä¸ºé¢éƒ¨é›¶ä»¶äº¤æ¢è®¾è®¡è¿™æ ·çš„æ–¹æ³•é¢ä¸´ç€åˆç†çš„å¤šå‚è€ƒç‰¹å¾èåˆçš„éš¾é¢˜ï¼Œè¿™éœ€è¦æ—¢é«˜æ•ˆåˆæœ‰æ•ˆã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†FuseAnyPartæ–¹æ³•ï¼Œä»¥å®ç°é¢éƒ¨æ— ç¼èåˆä»»æ„éƒ¨åˆ†çš„å®šåˆ¶ã€‚åœ¨FuseAnyPartä¸­ï¼Œæ¥è‡ªä¸åŒäººçš„é¢éƒ¨é›¶ä»¶åœ¨åŸºäºæ©è†œçš„èåˆæ¨¡å—ä¸­ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„é¢éƒ¨æ½œåœ¨ç©ºé—´ã€‚ç„¶åï¼Œåˆå¹¶çš„ç‰¹å¾è¢«æ´¾å‘åˆ°åŸºäºæ·»åŠ çš„æ³¨å…¥æ¨¡å—ä¸­ï¼Œåœ¨æ‰©æ•£æ¨¡å‹çš„UNetä¸­è¿›è¡Œèåˆä»¥åˆ›å»ºæ–°è§’è‰²ã€‚å¤§é‡çš„å®éªŒå®šæ€§å’Œå®šé‡éªŒè¯äº†FuseAnyPartçš„ä¼˜è¶Šæ€§å’Œç¨³å¥æ€§ã€‚ç›¸å…³æºç å·²å…¬å¼€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Thomas-wyh/FuseAnyPart%E3%80%82">https://github.com/Thomas-wyh/FuseAnyPartã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢éƒ¨é›¶ä»¶äº¤æ¢æ—¨åœ¨é€‰æ‹©æ€§åœ°å°†æºå›¾åƒçš„ç‰¹å®šåŒºåŸŸè½¬ç§»åˆ°ç›®æ ‡å›¾åƒä¸Šï¼ŒåŒæ—¶ä¿æŒç›®æ ‡å›¾åƒçš„å…¶ä»–éƒ¨åˆ†ä¸å˜ã€‚</li>
<li>å½“å‰é¢éƒ¨äº¤æ¢ç ”ç©¶å¤šä¸“æ³¨äºå…¨è„¸äº¤æ¢ï¼Œéš¾ä»¥è¿›è¡Œç²¾ç»†çš„é¢éƒ¨é›¶ä»¶äº¤æ¢ã€‚</li>
<li>è®¾è®¡ç”¨äºé¢éƒ¨é›¶ä»¶äº¤æ¢çš„æ–¹æ³•é¢ä¸´å¤šå‚è€ƒç‰¹å¾èåˆçš„éš¾é¢˜ï¼Œéœ€è¦æ—¢é«˜æ•ˆåˆæœ‰æ•ˆã€‚</li>
<li>FuseAnyPartæ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­ç»„åˆæ¥è‡ªä¸åŒäººçš„é¢éƒ¨é›¶ä»¶ä»¥åˆ›å»ºæ–°çš„å®Œæ•´é¢éƒ¨ã€‚</li>
<li>FuseAnyPartåŒ…å«åŸºäºæ©è†œçš„èåˆæ¨¡å—å’ŒåŸºäºæ·»åŠ çš„æ³¨å…¥æ¨¡å—ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒFuseAnyPartå…·æœ‰ä¼˜è¶Šæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efd2ae5450ceadda82c42db5d7cd6440.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abc42916c6f7702838f7ad03f3b373b0.jpg" align="middle">
</details>




<h2 id="DeCoRe-Decoding-by-Contrasting-Retrieval-Heads-to-Mitigate-Hallucinations"><a href="#DeCoRe-Decoding-by-Contrasting-Retrieval-Heads-to-Mitigate-Hallucinations" class="headerlink" title="DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate   Hallucinations"></a>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate   Hallucinations</h2><p><strong>Authors:Aryo Pradipta Gema, Chen Jin, Ahmed Abdulaal, Tom Diethe, Philip Teare, Beatrice Alex, Pasquale Minervini, Amrutha Saseendran</strong></p>
<p>Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%). </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸å‡ºç°å¹»è§‰ï¼Œé€šè¿‡è¯¯ä»£è¡¨æä¾›çš„ä¸Šä¸‹æ–‡æˆ–é”™è¯¯å›å¿†å†…éƒ¨çŸ¥è¯†ï¼Œäº§ç”Ÿä¸çœŸå®æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯†åˆ«å‡ºTransformeræ¶æ„ä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼Œç§°ä¸ºæ£€ç´¢å¤´ï¼Œè´Ÿè´£æå–ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬å‡è®¾æ©ç›–è¿™äº›æ£€ç´¢å¤´ä¼šå¯¼è‡´å¹»è§‰ï¼Œå¹¶ä¸”å¯¹æ¯”åŸºç¡€LLMå’Œæ©ç›–LLMçš„è¾“å‡ºå¯ä»¥å‡å°‘å¹»è§‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„è§£ç ç­–ç•¥â€”â€”é€šè¿‡å¯¹æ¯”æ£€ç´¢å¤´è¿›è¡Œè§£ç ï¼ˆDeCoReï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ”¾å¤§ä¸Šä¸‹æ–‡å’Œæ¨¡å‹å‚æ•°ä¸­ä¿¡æ¯çš„æ–¹æ³•ã€‚DeCoReé€šè¿‡åŠ¨æ€å¯¹æ¯”åŸºç¡€LLMå’Œæ©ç›–LLMçš„è¾“å‡ºï¼Œä½¿ç”¨æ¡ä»¶ç†µä½œä¸ºæŒ‡å¯¼ï¼Œç¼“è§£å¯èƒ½äº§ç”Ÿçš„å¹»è§‰å“åº”ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¯å®ï¼ŒDeCoReåœ¨éœ€è¦é«˜ä¸Šä¸‹æ–‡çœŸå®æ€§çš„ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æ”¹è¿›ï¼Œå¦‚åœ¨æ‘˜è¦ï¼ˆXSumæå‡18.6%ï¼‰ã€æ‰§è¡ŒæŒ‡ä»¤ï¼ˆMemoTrapæå‡10.9%ï¼‰å’Œå¼€æ”¾ä¹¦ç±é—®ç­”ï¼ˆNQ-Openæå‡2.4%å’ŒNQ-Swapæå‡5.5%ï¼‰ç­‰ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18860v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆè¾“å‡ºæ—¶ä¼šå‡ºç°å¹»è§‰ï¼Œäº§ç”Ÿä¸çœŸå®æˆ–äº‹å®é”™è¯¯çš„è¾“å‡ºï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè¯¯è§£ä¸Šä¸‹æ–‡æˆ–é”™è¯¯å›å¿†å†…éƒ¨çŸ¥è¯†æ‰€è‡´ã€‚ç ”ç©¶è®¤ä¸ºTransformeræ¶æ„ä¸­çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ï¼ˆç§°ä¸ºæ£€ç´¢å¤´ï¼‰è´Ÿè´£æå–ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè€Œå±è”½è¿™äº›æ£€ç´¢å¤´å¯èƒ½å¯¼è‡´å¹»è§‰ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºDeCoReçš„æ–°å‹è§£ç ç­–ç•¥ï¼Œé€šè¿‡å¯¹æ¯”åŸºç¡€LLMå’Œå±è”½LLMçš„è¾“å‡ºï¼Œå¼ºåŒ–ä¸Šä¸‹æ–‡å’Œæ¨¡å‹å‚æ•°ä¸­çš„ä¿¡æ¯ï¼Œä»¥å‡å°‘å¹»è§‰ã€‚å®éªŒè¯æ˜ï¼ŒDeCoReåœ¨éœ€è¦é«˜ä¸Šä¸‹æ–‡å¿ å®åº¦çš„ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ï¼Œå¦‚æ‘˜è¦ï¼ˆXSumæå‡18.6%ï¼‰ã€æŒ‡ä»¤éµå¾ªï¼ˆMemoTrapæå‡10.9%ï¼‰å’Œå¼€æ”¾é—®ç­”ï¼ˆNQ-Openæå‡2.4%ï¼ŒNQ-Swapæå‡5.5%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰æ—¶ä¼šäº§ç”Ÿå¹»è§‰è¾“å‡ºï¼Œè¿™å¯èƒ½ä¸è¯¯è§£ä¸Šä¸‹æ–‡æˆ–é”™è¯¯å›å¿†å†…éƒ¨çŸ¥è¯†æœ‰å…³ã€‚</li>
<li>æ£€ç´¢å¤´æ˜¯Transformeræ¶æ„ä¸­è´Ÿè´£æå–ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ç‰¹å®šæ³¨æ„åŠ›å¤´ã€‚</li>
<li>å±è”½æ£€ç´¢å¤´å¯èƒ½å¯¼è‡´å¹»è§‰è¾“å‡ºã€‚</li>
<li>DeCoReæ˜¯ä¸€ç§æ–°å‹è§£ç ç­–ç•¥ï¼Œé€šè¿‡å¯¹æ¯”åŸºç¡€LLMå’Œå±è”½LLMçš„è¾“å‡ºï¼Œå‡å°‘å¹»è§‰ã€‚</li>
<li>DeCoReé€šè¿‡å¼ºåŒ–ä¸Šä¸‹æ–‡å’Œæ¨¡å‹å‚æ•°ä¸­çš„ä¿¡æ¯æ¥å·¥ä½œã€‚</li>
<li>DeCoReåœ¨éœ€è¦é«˜ä¸Šä¸‹æ–‡å¿ å®åº¦çš„ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—ï¼Œå¦‚æ‘˜è¦ã€æŒ‡ä»¤éµå¾ªå’Œå¼€æ”¾é—®ç­”ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1aae5e28e3fa471803a57bd182be062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7f62d231075c59daf813aad33168d03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8e479269ebb4b980135f19fec144caa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-411e6c0037a630261d7456a150bf3f11.jpg" align="middle">
</details>




<h2 id="ExpertFlow-Optimized-Expert-Activation-and-Token-Allocation-for-Efficient-Mixture-of-Experts-Inference"><a href="#ExpertFlow-Optimized-Expert-Activation-and-Token-Allocation-for-Efficient-Mixture-of-Experts-Inference" class="headerlink" title="ExpertFlow: Optimized Expert Activation and Token Allocation for   Efficient Mixture-of-Experts Inference"></a>ExpertFlow: Optimized Expert Activation and Token Allocation for   Efficient Mixture-of-Experts Inference</h2><p><strong>Authors:Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon</strong></p>
<p>Sparse Mixture of Experts (MoE) models, while outperforming dense Large Language Models (LLMs) in terms of performance, face significant deployment challenges during inference due to their high memory demands. Existing offloading techniques, which involve swapping activated and idle experts between the GPU and CPU, often suffer from rigid expert caching mechanisms. These mechanisms fail to adapt to dynamic routing, leading to inefficient cache utilization, or incur prohibitive costs for prediction training. To tackle these inference-specific challenges, we introduce ExpertFlow, a comprehensive system specifically designed to enhance inference efficiency by accommodating flexible routing and enabling efficient expert scheduling between CPU and GPU. This reduces overhead and boosts system performance. Central to our approach is a predictive routing path-based offloading mechanism that utilizes a lightweight predictor to accurately forecast routing paths before computation begins. This proactive strategy allows for real-time error correction in expert caching, significantly increasing cache hit ratios and reducing the frequency of expert transfers, thereby minimizing I&#x2F;O overhead. Additionally, we implement a dynamic token scheduling strategy that optimizes MoE inference by rearranging input tokens across different batches. This method not only reduces the number of activated experts per batch but also improves computational efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up to 93.72% GPU memory savings and enhances inference speed by 2 to 10 times compared to baseline methods, highlighting its effectiveness and utility as a robust solution for resource-constrained inference scenarios. </p>
<blockquote>
<p>ç¨€ç–æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹è™½ç„¶åœ¨æ€§èƒ½æ–¹é¢è¶…è¶Šäº†å¯†é›†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´ç€éƒ¨ç½²çš„å·¨å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯¹å†…å­˜çš„éœ€æ±‚å¾ˆé«˜ã€‚ç°æœ‰çš„å¸è½½æŠ€æœ¯ï¼Œæ¶‰åŠåœ¨GPUå’ŒCPUä¹‹é—´äº¤æ¢æ¿€æ´»å’Œç©ºé—²çš„ä¸“å®¶ï¼Œé€šå¸¸å—åˆ°åƒµåŒ–ä¸“å®¶ç¼“å­˜æœºåˆ¶çš„å½±å“ã€‚è¿™äº›æœºåˆ¶æ— æ³•é€‚åº”åŠ¨æ€è·¯ç”±ï¼Œå¯¼è‡´ç¼“å­˜åˆ©ç”¨ç‡ä½ï¼Œæˆ–è€…ä¸ºé¢„æµ‹è®­ç»ƒé€ æˆé«˜æ˜‚çš„æˆæœ¬ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æ¨ç†ç‰¹å®šçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ExpertFlowï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„ç»¼åˆç³»ç»Ÿï¼Œé€šè¿‡çµæ´»çš„è·¯ç”±å’ŒCPUä¸GPUä¹‹é—´çš„é«˜æ•ˆä¸“å®¶è°ƒåº¦ï¼Œæé«˜æ¨ç†æ•ˆç‡ã€‚è¿™é™ä½äº†å¼€é”€å¹¶æé«˜äº†ç³»ç»Ÿæ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åŸºäºé¢„æµ‹è·¯ç”±è·¯å¾„çš„å¸è½½æœºåˆ¶ï¼Œåˆ©ç”¨è½»é‡çº§é¢„æµ‹å™¨åœ¨è®¡ç®—å¼€å§‹ä¹‹å‰å‡†ç¡®é¢„æµ‹è·¯ç”±è·¯å¾„ã€‚è¿™ç§ä¸»åŠ¨ç­–ç•¥å…è®¸å®æ—¶çº æ­£ä¸“å®¶ç¼“å­˜ä¸­çš„é”™è¯¯ï¼Œæ˜¾è‘—æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼Œå‡å°‘ä¸“å®¶è½¬ç§»çš„é¢‘ç‡ï¼Œä»è€Œæœ€å°åŒ–I&#x2F;Oå¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†åŠ¨æ€ä»¤ç‰Œè°ƒåº¦ç­–ç•¥ï¼Œé€šè¿‡é‡æ–°å®‰æ’ä¸åŒæ‰¹æ¬¡çš„è¾“å…¥ä»¤ç‰Œæ¥ä¼˜åŒ–MoEæ¨ç†ã€‚è¿™ç§æ–¹æ³•ä¸ä»…å‡å°‘äº†æ¯æ‰¹æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼Œè€Œä¸”æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒExpertFlowå®ç°äº†é«˜è¾¾93.72%çš„GPUå†…å­˜èŠ‚çœï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†2åˆ°10å€ï¼Œçªæ˜¾äº†å…¶åœ¨èµ„æºå—é™æ¨ç†åœºæ™¯ä¸­çš„ç¨³å¥æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17954v1">PDF</a> Mixture-of-Experts, Inference, Offloading</p>
<p><strong>Summary</strong></p>
<p>åŸºäºSparse Mixture of Expertsæ¨¡å‹çš„ä¼˜åŒ–æ–¹æ¡ˆExpertFlowèƒ½æœ‰æ•ˆæé«˜æ¨ç†æ•ˆç‡ã€‚å®ƒåˆ©ç”¨çµæ´»çš„è·¯ç”±æœºåˆ¶åœ¨CPUå’ŒGPUé—´å®ç°é«˜æ•ˆä¸“å®¶è°ƒåº¦ï¼Œé™ä½å¼€é”€å¹¶æå‡ç³»ç»Ÿæ€§èƒ½ã€‚å…¶é€šè¿‡é¢„æµ‹è·¯ç”±è·¯å¾„è¿›è¡Œä»»åŠ¡å¸è½½ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€ä»¤ç‰Œè°ƒåº¦ç­–ç•¥ä¼˜åŒ–MoEæ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¯æ˜ï¼ŒExpertFlowå¯å®ç°é«˜è¾¾93.72%çš„GPUå†…å­˜èŠ‚çœï¼Œå¹¶è¾ƒåŸºçº¿æ–¹æ³•æé«˜æ¨ç†é€Ÿåº¦2è‡³10å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ExpertFlowæ—¨åœ¨è§£å†³Sparse Mixture of Expertsæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­é¢ä¸´çš„å†…å­˜éœ€æ±‚é«˜çš„é—®é¢˜ã€‚</li>
<li>å®ƒé€šè¿‡çµæ´»çš„è·¯ç”±æœºåˆ¶å®ç°CPUå’ŒGPUé—´çš„ä¸“å®¶è°ƒåº¦ï¼Œä»¥æé«˜æ¨ç†æ•ˆç‡ã€‚</li>
<li>ExpertFlowé‡‡ç”¨é¢„æµ‹è·¯ç”±è·¯å¾„çš„å¸è½½æœºåˆ¶ï¼Œåˆ©ç”¨è½»é‡çº§é¢„æµ‹å™¨åœ¨è®¡ç®—å¼€å§‹å‰å‡†ç¡®é¢„æµ‹è·¯ç”±è·¯å¾„ã€‚</li>
<li>åŠ¨æ€ä»¤ç‰Œè°ƒåº¦ç­–ç•¥ç”¨äºä¼˜åŒ–MoEæ¨ç†è¿‡ç¨‹ï¼Œå‡å°‘æ¯æ‰¹æ¬¡æ¿€æ´»çš„ä¸“å®¶æ•°é‡å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>ExpertFlowé€šè¿‡å®æ—¶é”™è¯¯æ ¡æ­£çš„ä¸“å®¶ç¼“å­˜æœºåˆ¶ï¼Œæ˜¾è‘—æé«˜ç¼“å­˜å‘½ä¸­ç‡å¹¶å‡å°‘ä¸“å®¶è½¬ç§»é¢‘ç‡ï¼Œä»è€Œæœ€å°åŒ–I&#x2F;Oå¼€é”€ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒExpertFlowåœ¨GPUå†…å­˜èŠ‚çœå’Œæ¨ç†é€Ÿåº¦æå‡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c03e16027429acd5e954579dfce7ab8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c94f9135c1cb1d231eb771eeb26aabfe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7cf90ef9077a8d2da3b8be49d8fe2a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc928630e056061847adff993b93bf0d.jpg" align="middle">
</details>




<h2 id="Active-Fake-DeepFake-Camouflage"><a href="#Active-Fake-DeepFake-Camouflage" class="headerlink" title="Active Fake: DeepFake Camouflage"></a>Active Fake: DeepFake Camouflage</h2><p><strong>Authors:Pu Sun, Honggang Qi, Yuezun Li</strong></p>
<p>DeepFake technology has gained significant attention due to its ability to manipulate facial attributes with high realism, raising serious societal concerns. Face-Swap DeepFake is the most harmful among these techniques, which fabricates behaviors by swapping original faces with synthesized ones. Existing forensic methods, primarily based on Deep Neural Networks (DNNs), effectively expose these manipulations and have become important authenticity indicators. However, these methods mainly concentrate on capturing the blending inconsistency in DeepFake faces, raising a new security issue, termed Active Fake, emerges when individuals intentionally create blending inconsistency in their authentic videos to evade responsibility. This tactic is called DeepFake Camouflage. To achieve this, we introduce a new framework for creating DeepFake camouflage that generates blending inconsistencies while ensuring imperceptibility, effectiveness, and transferability. This framework, optimized via an adversarial learning strategy, crafts imperceptible yet effective inconsistencies to mislead forensic detectors. Extensive experiments demonstrate the effectiveness and robustness of our method, highlighting the need for further research in active fake detection. </p>
<blockquote>
<p>æ·±åº¦ä¼ªé€ æŠ€æœ¯å› å…¶èƒ½å¤Ÿé«˜åº¦é€¼çœŸåœ°æ“çºµé¢éƒ¨ç‰¹å¾è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œè¿™å¼•å‘äº†ç¤¾ä¼šçš„ä¸¥é‡æ‹…å¿§ã€‚é¢éƒ¨äº¤æ¢æ·±åº¦ä¼ªé€ æ˜¯è¿™äº›æŠ€æœ¯ä¸­æœ€å…·å±å®³æ€§çš„ï¼Œå®ƒé€šè¿‡äº¤æ¢åŸå§‹é¢å­”å¹¶åˆæˆæ–°çš„é¢å­”æ¥ä¼ªé€ è¡Œä¸ºã€‚ç°æœ‰çš„å–è¯æ–¹æ³•ä¸»è¦åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æš´éœ²è¿™äº›æ“çºµè¡Œä¸ºï¼Œå¹¶æˆä¸ºé‡è¦çš„çœŸå®æ€§æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ•æ‰æ·±åº¦ä¼ªé€ é¢å­”çš„èåˆä¸ä¸€è‡´æ€§ä¸Šï¼Œå½“ä¸ªäººæ•…æ„åœ¨å…¶çœŸå®è§†é¢‘ä¸­åˆ¶é€ èåˆä¸ä¸€è‡´æ€§ä»¥é€ƒé¿è´£ä»»æ—¶ï¼Œå‡ºç°äº†ä¸€ä¸ªæ–°çš„å®‰å…¨é—®é¢˜ï¼Œç§°ä¸ºæ´»åŠ¨ä¼ªè£…ï¼ˆActive Fakeï¼‰ã€‚è¿™ç§ç­–ç•¥è¢«ç§°ä¸ºæ·±åº¦ä¼ªé€ ä¼ªè£…ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åˆ›å»ºæ·±åº¦ä¼ªé€ ä¼ªè£…æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨ç”Ÿæˆèåˆä¸ä¸€è‡´æ€§çš„åŒæ—¶ç¡®ä¿äº†ä¸å¯æ„ŸçŸ¥æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è½¬ç§»æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯¹æŠ—å­¦ä¹ ç­–ç•¥è¿›è¡Œä¼˜åŒ–ï¼Œèƒ½å¤Ÿåˆ¶é€ å‡ºä¸å¯å¯Ÿè§‰ä½†æœ‰æ•ˆçš„ä¸ä¸€è‡´æ€§æ¥è¯¯å¯¼å–è¯æ£€æµ‹å™¨ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œå¼ºè°ƒäº†æ´»åŠ¨å‡æ£€æµ‹ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03200v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦ä¼ªé€ æŠ€æœ¯å› å…¶èƒ½å¤Ÿé«˜åº¦é€¼çœŸåœ°æ“çºµé¢éƒ¨ç‰¹å¾è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¼•å‘äº†ç¤¾ä¼šçš„ä¸¥é‡å…³åˆ‡ã€‚å…¶ä¸­ï¼ŒFace-Swap DeepFakeæŠ€æœ¯é€šè¿‡æ›¿æ¢åŸå§‹é¢éƒ¨ä¸ºåˆæˆé¢éƒ¨æ¥åˆ¶é€ è¡Œä¸ºå‡è±¡ï¼Œå…¶å±å®³æœ€å¤§ã€‚ç°æœ‰çš„åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„å–è¯æ–¹æ³•èƒ½æœ‰æ•ˆæ­éœ²è¿™äº›æ“çºµè¡Œä¸ºï¼Œæˆä¸ºé‡è¦çš„çœŸå®æ€§æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ•æ‰æ·±åº¦ä¼ªé€ é¢éƒ¨èåˆçš„ä¸ä¸€è‡´æ€§ï¼Œä»è€Œå¼•å‘äº†ä¸€ä¸ªæ–°çš„å®‰å…¨é—®é¢˜â€”â€”ä¸»åŠ¨ä¼ªè£…ï¼Œå³ä¸ªäººæœ‰æ„åœ¨å…¶çœŸå®è§†é¢‘ä¸Šåˆ¶é€ èåˆçš„ä¸ä¸€è‡´æ€§ä»¥é€ƒé¿è´£ä»»çš„è¡Œä¸ºï¼Œç§°ä¸ºæ·±åº¦ä¼ªé€ ä¼ªè£…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„æ·±åº¦ä¼ªé€ ä¼ªè£…æ¡†æ¶ï¼Œé€šè¿‡ç”Ÿæˆéš¾ä»¥å¯Ÿè§‰çš„èåˆä¸ä¸€è‡´æ€§æ¥å®ç°ä¼ªè£…æ•ˆæœï¼Œç¡®ä¿éšè”½æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚é‡‡ç”¨å¯¹æŠ—æ€§å­¦ä¹ ç­–ç•¥ä¼˜åŒ–çš„è¯¥æ¡†æ¶èƒ½ç”Ÿæˆéš¾ä»¥è§‰å¯Ÿçš„ä¸ä¸€è‡´æ€§ä»¥æ¬ºéª—é‰´å®šæ£€æµ‹å™¨ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†ä¸»åŠ¨ä¼ªè£…æ£€æµ‹ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯å› å…¶é«˜åº¦é€¼çœŸåœ°æ“çºµé¢éƒ¨ç‰¹å¾çš„èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå¼•å‘äº†ç¤¾ä¼šå…³æ³¨ã€‚</li>
<li>Face-Swap DeepFakeæŠ€æœ¯é€šè¿‡æ›¿æ¢é¢éƒ¨åˆ¶é€ å‡è±¡ï¼Œæˆä¸ºæ·±åº¦ä¼ªé€ ä¸­æœ€å±é™©çš„æŠ€æœ¯ä¹‹ä¸€ã€‚</li>
<li>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å–è¯æ–¹æ³•èƒ½æœ‰æ•ˆæ­éœ²æ·±åº¦ä¼ªé€ è¡Œä¸ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•æ‰æ·±åº¦ä¼ªé€ é¢éƒ¨èåˆçš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´å‡ºç°æ–°çš„å®‰å…¨é—®é¢˜â€”â€”ä¸»åŠ¨ä¼ªè£…ï¼ˆActive Fakeï¼‰ã€‚</li>
<li>ä¸»åŠ¨ä¼ªè£…æ˜¯æŒ‡ä¸ªäººæœ‰æ„åœ¨çœŸå®è§†é¢‘ä¸Šåˆ¶é€ èåˆä¸ä¸€è‡´æ€§ä»¥é€ƒé¿è´£ä»»çš„è¡Œä¸ºã€‚</li>
<li>ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†æ–°çš„æ·±åº¦ä¼ªé€ ä¼ªè£…æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆéš¾ä»¥è§‰å¯Ÿçš„èåˆä¸ä¸€è‡´æ€§ï¼Œç¡®ä¿éšè”½æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è¿ç§»æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ba1c873260dd8ba3783030cc28b65a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bff79153b12dee0e2fbf6d1c19bc00f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4281ab67748153f5a988eecd0003fdf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-695caf7ffdc98748ba2935663cb7ba8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a60fa40a5e3343d5a781e369122b005.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84365c924b393ca78441022692ce4f0d.jpg" align="middle">
</details>




<h2 id="DeepDRK-Deep-Dependency-Regularized-Knockoff-for-Feature-Selection"><a href="#DeepDRK-Deep-Dependency-Regularized-Knockoff-for-Feature-Selection" class="headerlink" title="DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection"></a>DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection</h2><p><strong>Authors:Hongyu Shen, Yici Yan, Zhizhen Zhao</strong></p>
<p>Model-X knockoff has garnered significant attention among various feature selection methods due to its guarantees for controlling the false discovery rate (FDR). Since its introduction in parametric design, knockoff techniques have evolved to handle arbitrary data distributions using deep learning-based generative models. However, we have observed limitations in the current implementations of the deep Model-X knockoff framework. Notably, the â€œswap propertyâ€ that knockoffs require often faces challenges at the sample level, resulting in diminished selection power. To address these issues, we develop â€œDeep Dependency Regularized Knockoff (DeepDRK),â€ a distribution-free deep learning method that effectively balances FDR and power. In DeepDRK, we introduce a novel formulation of the knockoff model as a learning problem under multi-source adversarial attacks. By employing an innovative perturbation technique, we achieve lower FDR and higher power. Our model outperforms existing benchmarks across synthetic, semi-synthetic, and real-world datasets, particularly when sample sizes are small and data distributions are non-Gaussian. </p>
<blockquote>
<p>æ¨¡å‹Xçš„å‡å†’æ ·æœ¬åœ¨å„ç§ç‰¹å¾é€‰æ‹©æ–¹æ³•ä¸­å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒèƒ½ä¿è¯æ§åˆ¶è¯¯å‘ç°ç‡ï¼ˆFDRï¼‰ã€‚è‡ªä»å‚æ•°åŒ–è®¾è®¡å¼•å…¥ä»¥æ¥ï¼Œå‡å†’æŠ€æœ¯å·²ç»å‘å±•ä½¿ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹æ¥å¤„ç†ä»»æ„æ•°æ®åˆ†å¸ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å½“å‰æ·±åº¦æ¨¡å‹Xå‡å†’æ¡†æ¶çš„å®ç°å­˜åœ¨å±€é™æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå‡å†’æ ·æœ¬æ‰€éœ€çš„â€œæ›¿æ¢å±æ€§â€åœ¨æ ·æœ¬å±‚é¢ç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´é€‰æ‹©èƒ½åŠ›ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†â€œæ·±åº¦ä¾èµ–æ­£åˆ™åŒ–å‡å†’ï¼ˆDeepDRKï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€åˆ†å¸ƒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡FDRå’ŒåŠŸç‡ã€‚åœ¨DeepDRKä¸­ï¼Œæˆ‘ä»¬å°†å‡å†’æ¨¡å‹ä½œä¸ºä¸€ä¸ªå¤šæºå¯¹æŠ—æ”»å‡»ä¸‹çš„å­¦ä¹ é—®é¢˜è¿›è¡Œäº†æ–°é¢–çš„è¡¨è¾¾ã€‚é€šè¿‡é‡‡ç”¨åˆ›æ–°çš„æ‰°åŠ¨æŠ€æœ¯ï¼Œæˆ‘ä»¬å®ç°äº†è¾ƒä½çš„FDRå’Œè¾ƒé«˜çš„åŠŸç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆæˆã€åŠåˆæˆå’ŒçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬é‡å°ä¸”æ•°æ®åˆ†å¸ƒéé«˜æ–¯çš„æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.17176v2">PDF</a> 33 pages, 15 figures, 9 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Model-Xä»¿å†’ç‰¹å¾é€‰æ‹©æ–¹æ³•å› å…¶æ§åˆ¶è¯¯å‘ç°ç‡ï¼ˆFDRï¼‰çš„ä¿è¯è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚è‡ªä»åœ¨å‚æ•°åŒ–è®¾è®¡ä¸­å¼•å…¥ä»¿å†’æŠ€æœ¯ä»¥æ¥ï¼Œè¯¥æŠ€æœ¯å·²æ¼”å˜ä¸ºä½¿ç”¨åŸºäºæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹å¤„ç†ä»»æ„æ•°æ®åˆ†å¸ƒã€‚ç„¶è€Œï¼Œæˆ‘ä»¬åœ¨ç°æœ‰çš„æ·±åº¦Model-Xä»¿å†’æ¡†æ¶å®ç°ä¸­è§‚å¯Ÿåˆ°å±€é™æ€§ã€‚ç‰¹åˆ«æ˜¯ä»¿å†’æ‰€éœ€çš„â€œæ›¿æ¢å±æ€§â€åœ¨æ ·æœ¬å±‚é¢ç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´é€‰æ‹©èƒ½åŠ›ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†â€œæ·±åº¦ä¾èµ–æ­£åˆ™åŒ–ä»¿å†’ï¼ˆDeepDRKï¼‰â€ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¹³è¡¡FDRå’ŒåŠŸç‡çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œæ— éœ€è€ƒè™‘æ•°æ®åˆ†å¸ƒã€‚åœ¨DeepDRKä¸­ï¼Œæˆ‘ä»¬å°†ä»¿å†’æ¨¡å‹ä½œä¸ºä¸€ä¸ªåœ¨å¤šé‡æ¥æºå¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„å­¦ä¹ é—®é¢˜åˆ›æ–°æ€§åœ°æå‡ºã€‚é€šè¿‡é‡‡ç”¨æ–°é¢–çš„æ‰°åŠ¨æŠ€æœ¯ï¼Œæˆ‘ä»¬å®ç°äº†æ›´ä½çš„FDRå’Œæ›´é«˜çš„åŠŸç‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åˆæˆã€åŠåˆæˆå’ŒçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬é‡å°ä¸”æ•°æ®åˆ†å¸ƒéé«˜æ–¯çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Model-Xä»¿å†’ç‰¹å¾é€‰æ‹©æ–¹æ³•å…³æ³¨æ§åˆ¶è¯¯å‘ç°ç‡ï¼ˆFDRï¼‰ã€‚</li>
<li>å½“å‰çš„æ·±åº¦Model-Xä»¿å†’æ¡†æ¶åœ¨æ ·æœ¬å±‚é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ç°â€œæ›¿æ¢å±æ€§â€æ—¶ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†æ·±åº¦ä¾èµ–æ­£åˆ™åŒ–ä»¿å†’ï¼ˆDeepDRKï¼‰æ–¹æ³•ã€‚</li>
<li>DeepDRKæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¹³è¡¡FDRå’ŒåŠŸç‡çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå¯å¤„ç†ä»»æ„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>DeepDRKå°†ä»¿å†’æ¨¡å‹åˆ›æ–°åœ°è¡¨è¿°ä¸ºä¸€ä¸ªåœ¨å¤šé‡æ¥æºå¯¹æŠ—æ€§æ”»å‡»ä¸‹çš„å­¦ä¹ é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ–°é¢–çš„æ‰°åŠ¨æŠ€æœ¯ï¼ŒDeepDRKå®ç°äº†æ›´ä½çš„FDRå’Œæ›´é«˜çš„åŠŸç‡ã€‚</li>
<li>åœ¨åˆæˆã€åŠåˆæˆå’ŒçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šï¼ŒDeepDRKæ¨¡å‹è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ·æœ¬é‡å°ä¸”æ•°æ®åˆ†å¸ƒéé«˜æ–¯çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ce9ebce626b8a5e62648e1ceed26600.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-956302723522161fb6fd066ba71e28fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5e9f87c606239aabf8f91a2f2d3210b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3084053535f2a518d6cb0eeaf37c030.jpg" align="middle">
</details>




<h2 id="Deepfake-for-the-Good-Generating-Avatars-through-Face-Swapping-with-Implicit-Deepfake-Generation"><a href="#Deepfake-for-the-Good-Generating-Avatars-through-Face-Swapping-with-Implicit-Deepfake-Generation" class="headerlink" title="Deepfake for the Good: Generating Avatars through Face-Swapping with   Implicit Deepfake Generation"></a>Deepfake for the Good: Generating Avatars through Face-Swapping with   Implicit Deepfake Generation</h2><p><strong>Authors:Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, SÅ‚awomir Tadeja, Jacek Tabor, PrzemysÅ‚aw Spurek</strong></p>
<p>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the objectâ€™s shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the objectâ€™s characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Deepfakes refers to artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such simple strategies can produce plausible 3D deepfake-based avatars. </p>
<blockquote>
<p>æ–°å…´çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹è®¡ç®—æœºå›¾å½¢å­¦äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚å…¶ä¸­æœ€æœ‰å‰é€”çš„çªç ´ä¹‹ä¸€æ˜¯ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯æ‹¼è´´ï¼ˆGSï¼‰çš„å…´èµ·ã€‚NeRFä½¿ç”¨ç¥ç»ç½‘ç»œæƒé‡ç¼–ç å¯¹è±¡çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œä½¿ç”¨å°‘é‡å·²çŸ¥ç›¸æœºä½ç½®çš„å›¾ç‰‡æ¥ç”Ÿæˆæ–°å‹è§†å›¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGSé€šè¿‡åœ¨ä¸€ç»„é«˜æ–¯åˆ†å¸ƒä¸­ç¼–ç å¯¹è±¡ç‰¹å¾ï¼Œæä¾›äº†åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼Œè€Œä¸ä¼šé™ä½æ¸²æŸ“è´¨é‡ã€‚è¿™ä¸¤ç§æŠ€æœ¯åœ¨ç©ºé—´è®¡ç®—å’Œå…¶ä»–é¢†åŸŸæ‰¾åˆ°äº†è®¸å¤šç”¨ä¾‹ã€‚å¦ä¸€æ–¹é¢ï¼Œæ·±åº¦ä¼ªé€ æ–¹æ³•çš„å‡ºç°å¼•èµ·äº†å¾ˆå¤§çš„äº‰è®®ã€‚æ·±åº¦ä¼ªé€ æ˜¯æŒ‡ä½¿ç”¨äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘ç´§å¯†æ¨¡ä»¿çœŸå®é•œå¤´ã€‚ä½¿ç”¨ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥ä¿®æ”¹é¢éƒ¨ç‰¹å¾ï¼Œèƒ½å¤Ÿåˆ›å»ºå‡ºæ”¹å˜èº«ä»½æˆ–è¡¨æƒ…çš„è™šå‡è§†é¢‘ï¼Œå…¶å¤–è§‚æƒŠäººåœ°é€¼çœŸäºçœŸäººã€‚å°½ç®¡å­˜åœ¨äº‰è®®ï¼Œä½†é«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ å¯ä»¥ä¸ºåŒ–èº«åˆ›å»ºå’Œæ¸¸æˆæä¾›ä¸‹ä¸€ä»£è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ç»“åˆæ‰€æœ‰è¿™äº›æ–°å…´æŠ€æœ¯æ¥è·å¾—æ›´åˆç†çš„ç»“æœã€‚æˆ‘ä»¬çš„ImplicitDeepfakeä½¿ç”¨ç»å…¸çš„æ·±åº¦ä¼ªé€ ç®—æ³•åˆ†åˆ«ä¿®æ”¹æ‰€æœ‰è®­ç»ƒå›¾åƒï¼Œç„¶ååœ¨ä¿®æ”¹åçš„é¢éƒ¨ä¸Šè®­ç»ƒNeRFå’ŒGSã€‚è¿™ç§ç®€å•çš„ç­–ç•¥å¯ä»¥äº§ç”Ÿåˆç†çš„åŸºäºæ·±åº¦ä¼ªé€ çš„3DåŒ–èº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06390v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æŠ€æœ¯å¦‚ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å’Œé«˜æ–¯å¹³é“ºï¼ˆGSï¼‰åœ¨è®¡ç®—æœºå›¾å½¢å­¦ä¸­äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚NeRFä½¿ç”¨å·²çŸ¥ç›¸æœºä½ç½®çš„å‡ å¼ å›¾åƒæ¥ç”Ÿæˆæ–°å‹è§†è§’ï¼Œå°†ç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ç¼–ç åœ¨ç¥ç»ç½‘ç»œæƒé‡ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒGSé€šè¿‡ç¼–ç ç‰©ä½“ç‰¹å¾åœ¨ä¸€ç³»åˆ—é«˜æ–¯åˆ†å¸ƒä¸­å®ç°äº†è®­ç»ƒå’Œæ¨ç†çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¸é™ä½æ¸²æŸ“è´¨é‡ã€‚è¿™ä¸¤ç§æŠ€æœ¯å¹¿æ³›åº”ç”¨äºç©ºé—´è®¡ç®—ç­‰é¢†åŸŸã€‚å¦ä¸€æ–¹é¢ï¼Œæ·±åº¦ä¼ªé€ æ–¹æ³•çš„å‡ºç°å¼•å‘äº†å·¨å¤§äº‰è®®ã€‚æ·±åº¦ä¼ªé€ æŒ‡ä½¿ç”¨äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è§†é¢‘ï¼Œèƒ½å¤Ÿæ¨¡ä»¿çœŸå®ç”»é¢ã€‚é€šè¿‡ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥ä¿®æ”¹é¢éƒ¨ç‰¹å¾ï¼Œåˆ›é€ å‡ºå…·æœ‰æé«˜çœŸå®æ„Ÿçš„èº«ä»½æˆ–è¡¨æƒ…å˜åŒ–ã€‚å°†æ–°å…´æŠ€æœ¯ç»“åˆèµ·æ¥çš„éšæ€§æ·±åº¦ä¼ªé€ æŠ€æœ¯å¯ä»¥å®ç°æ›´é€¼çœŸçš„ç»“æœã€‚æœ¬ç ”ç©¶é‡‡ç”¨ä¼ ç»Ÿæ·±åº¦ä¼ªé€ ç®—æ³•å•ç‹¬ä¿®æ”¹æ‰€æœ‰è®­ç»ƒå›¾åƒï¼Œç„¶åå¯¹ä¿®æ”¹åçš„é¢éƒ¨è¿›è¡ŒNeRFå’ŒGSè®­ç»ƒã€‚è¿™ç§ç®€å•ç­–ç•¥å¯ä»¥äº§ç”ŸåŸºäºæ·±åº¦ä¼ªé€ çš„é€¼çœŸä¸‰ç»´è§’è‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯å¦‚NeRFå’ŒGSåœ¨è®¡ç®—æœºå›¾å½¢å­¦ä¸­äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´è®¡ç®—ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>æ·±åº¦ä¼ªé€ æ–¹æ³•å¼•å‘äº†äº‰è®®ï¼Œä½†å…¶å…·æœ‰æ½œåŠ›ä¸ºä¸‹ä¸€ä»£æ¸¸æˆå’Œè™šæ‹Ÿè§’è‰²åˆ›å»ºæä¾›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>éšæ€§æ·±åº¦ä¼ªé€ ç»“åˆäº†æ–°å…´æŠ€æœ¯ä»¥å®ç°æ›´é€¼çœŸçš„ç»“æœã€‚</li>
<li>ä¼ ç»Ÿæ·±åº¦ä¼ªé€ ç®—æ³•ç”¨äºå•ç‹¬ä¿®æ”¹è®­ç»ƒå›¾åƒï¼Œè¿›è€Œåœ¨ä¿®æ”¹åçš„é¢éƒ¨ä¸Šè¿›è¡ŒNeRFå’ŒGSè®­ç»ƒã€‚</li>
<li>è¿™ç§ç­–ç•¥å¯ä»¥äº§ç”ŸåŸºäºæ·±åº¦ä¼ªé€ çš„é€¼çœŸä¸‰ç»´è§’è‰²æ¨¡å‹ã€‚</li>
<li>ç»“åˆNeRFå’ŒGSæŠ€æœ¯çš„æ·±åº¦ä¼ªé€ æ–¹æ³•å…·æœ‰æ½œåŠ›åœ¨è™šæ‹Ÿè§’è‰²åˆ›å»ºå’Œæ¸²æŸ“ä¸­å®ç°æ›´é«˜çº§åˆ«çš„çœŸå®æ„Ÿå’Œè‡ªç„¶åº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3aa0c5bd5d55fab6b3456408bcfa7ba8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c6dd62daeb59781f75d66815146fbb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59bdfd70e3985fd56be81d718f9a9c8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0eb9d0ad2fd4b43afa5f960c84523bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e6dceac7784672ecd9f51fb468ecff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89629fe07055252ace8bfce59c0bd9b0.jpg" align="middle">
</details>




<h2 id="Lips-Are-Lying-Spotting-the-Temporal-Inconsistency-between-Audio-and-Visual-in-Lip-Syncing-DeepFakes"><a href="#Lips-Are-Lying-Spotting-the-Temporal-Inconsistency-between-Audio-and-Visual-in-Lip-Syncing-DeepFakes" class="headerlink" title="Lips Are Lying: Spotting the Temporal Inconsistency between Audio and   Visual in Lip-Syncing DeepFakes"></a>Lips Are Lying: Spotting the Temporal Inconsistency between Audio and   Visual in Lip-Syncing DeepFakes</h2><p><strong>Authors:Weifeng Liu, Tianyi She, Jiawei Liu, Boheng Li, Dongyu Yao, Ziyou Liang, Run Wang</strong></p>
<p>In recent years, DeepFake technology has achieved unprecedented success in high-quality video synthesis, but these methods also pose potential and severe security threats to humanity. DeepFake can be bifurcated into entertainment applications like face swapping and illicit uses such as lip-syncing fraud. However, lip-forgery videos, which neither change identity nor have discernible visual artifacts, present a formidable challenge to existing DeepFake detection methods. Our preliminary experiments have shown that the effectiveness of the existing methods often drastically decrease or even fail when tackling lip-syncing videos. In this paper, for the first time, we propose a novel approach dedicated to lip-forgery identification that exploits the inconsistency between lip movements and audio signals. We also mimic human natural cognition by capturing subtle biological links between lips and head regions to boost accuracy. To better illustrate the effectiveness and advances of our proposed method, we create a high-quality LipSync dataset, AVLips, by employing the state-of-the-art lip generators. We hope this high-quality and diverse dataset could be well served the further research on this challenging and interesting field. Experimental results show that our approach gives an average accuracy of more than 95.3% in spotting lip-syncing videos, significantly outperforming the baselines. Extensive experiments demonstrate the capability to tackle deepfakes and the robustness in surviving diverse input transformations. Our method achieves an accuracy of up to 90.2% in real-world scenarios (e.g., WeChat video call) and shows its powerful capabilities in real scenario deployment. To facilitate the progress of this research community, we release all resources at <a target="_blank" rel="noopener" href="https://github.com/AaronComo/LipFD">https://github.com/AaronComo/LipFD</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒDeepFakeæŠ€æœ¯åœ¨é«˜è´¨é‡è§†é¢‘åˆæˆæ–¹é¢å–å¾—äº†å‰æ‰€æœªæœ‰çš„æˆåŠŸï¼Œä½†è¿™äº›æ–¹æ³•ä¹Ÿç»™äººç±»å¸¦æ¥äº†æ½œåœ¨ä¸”ä¸¥é‡çš„å®‰å…¨å¨èƒã€‚DeepFakeå¯ä»¥äºŒåˆ†å¨±ä¹åº”ç”¨å¦‚æ¢è„¸å’Œéæ³•ç”¨é€”å¦‚å”‡åŒæ­¥æ¬ºè¯ˆã€‚ç„¶è€Œï¼Œæ—¢ä¸æ”¹å˜èº«ä»½ä¹Ÿæ²¡æœ‰æ˜æ˜¾è§†è§‰ä¼ªé€ çš„å”‡ä¼ªé€ è§†é¢‘ï¼Œç»™ç°æœ‰çš„DeepFakeæ£€æµ‹æ–¹æ³•å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„åˆæ­¥å®éªŒè¡¨æ˜ï¼Œç°æœ‰æ–¹æ³•åœ¨åº”å¯¹å”‡åŒæ­¥è§†é¢‘æ—¶ï¼Œå…¶æœ‰æ•ˆæ€§å¾€å¾€ä¼šå¤§å¹…ä¸‹é™ç”šè‡³å¤±æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§ä¸“é—¨ç”¨äºå”‡ä¼ªé€ è¯†åˆ«çš„å”‡è¯†åˆ«æŠ€æœ¯æ–°æ–¹æ³•ã€‚æ­¤æ–¹æ³•åˆ©ç”¨å”‡éƒ¨è¿åŠ¨ä¸éŸ³é¢‘ä¿¡å·ä¹‹é—´çš„ä¸ä¸€è‡´æ€§è¿›è¡Œæ£€æµ‹ã€‚é€šè¿‡æ•æ‰å”‡éƒ¨ä¸å¤´éƒ¨åŒºåŸŸä¹‹é—´çš„å¾®å¦™ç”Ÿç‰©å­¦è”ç³»æ¥æ¨¡ä»¿äººç±»çš„è‡ªç„¶è®¤çŸ¥è¿‡ç¨‹ä»¥æé«˜å‡†ç¡®æ€§ã€‚ä¸ºäº†æ›´å¥½åœ°è¯´æ˜æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå…ˆè¿›æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨æœ€å…ˆè¿›çš„å”‡ç”Ÿæˆå™¨åˆ›å»ºäº†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„AVLips LipSyncæ•°æ®é›†ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªé«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†èƒ½å¤ŸæœåŠ¡äºè¿™ä¸€å……æ»¡æŒ‘æˆ˜å’Œæœ‰è¶£çš„é¢†åŸŸçš„ç ”ç©¶å·¥ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯†åˆ«å”‡åŒæ­¥è§†é¢‘æ–¹é¢å¹³å‡å‡†ç¡®ç‡è¶…è¿‡95.3%ï¼Œæ˜æ˜¾ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•å¤„ç†æ·±åº¦ä¼ªé€ çš„èƒ½åŠ›å’Œåº”å¯¹å„ç§è¾“å…¥è½¬æ¢çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®åœºæ™¯ï¼ˆä¾‹å¦‚å¾®ä¿¡è§†é¢‘é€šè¯ï¼‰ä¸­çš„å‡†ç¡®ç‡é«˜è¾¾90.2%ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®åœºæ™¯éƒ¨ç½²çš„å¼ºå¤§èƒ½åŠ›ã€‚ä¸ºäº†ä¿ƒè¿›è¯¥é¢†åŸŸç ”ç©¶çš„è¿›æ­¥ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/AaronComo/LipFD%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%89%80%E6%9C%89%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/AaronComo/LipFDä¸Šå‘å¸ƒäº†æ‰€æœ‰èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.15668v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºæ·±åº¦ä¼ªé€ æŠ€æœ¯è¿‘å¹´æ¥çš„å¿«é€Ÿå‘å±•åŠå…¶åœ¨è§†é¢‘åˆæˆé¢†åŸŸçš„å·¨å¤§æˆåŠŸï¼Œå®ƒæ‰€å¸¦æ¥çš„æ½œåœ¨å®‰å…¨å¨èƒä¹Ÿé€æ¸å‡¸æ˜¾ã€‚æœ¬æ–‡é’ˆå¯¹å”‡åŒæ­¥ä¼ªé€ è§†é¢‘è¯†åˆ«éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å”‡åŠ¨ä¸éŸ³é¢‘ä¿¡å·ä¸ä¸€è‡´æ€§çš„æ–°å‹è¯†åˆ«æ–¹æ³•ï¼Œå¹¶æ¨¡ä»¿äººç±»è‡ªç„¶è®¤çŸ¥ï¼Œæ•æ‰å”‡éƒ¨å’Œå¤´éƒ¨åŒºåŸŸä¹‹é—´çš„å¾®å¦™ç”Ÿç‰©è”ç³»ä»¥æé«˜å‡†ç¡®æ€§ã€‚ä¸ºæ›´å¥½åœ°éªŒè¯æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„LipSyncæ•°æ®é›†AVLipsã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¹³å‡å‡†ç¡®ç‡è¾¾95.3%ä»¥ä¸Šï¼Œä¸”åœ¨çœŸå®åœºæ™¯éƒ¨ç½²ä¸­å±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œæœ€é«˜å‡†ç¡®ç‡å¯è¾¾90.2%ã€‚ä¸ºä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œç›¸å…³èµ„æºå·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ æŠ€æœ¯åœ¨è§†é¢‘åˆæˆé¢†åŸŸå–å¾—å·¨å¤§æˆåŠŸï¼Œä½†å¸¦æ¥äº†æ½œåœ¨çš„å®‰å…¨å¨èƒã€‚</li>
<li>å”‡åŒæ­¥ä¼ªé€ è§†é¢‘è¯†åˆ«é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•æ•ˆæœå¸¸æ˜¾è‘—ä¸‹é™æˆ–å¤±æ•ˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è¯†åˆ«æ–¹æ³•ï¼Œåˆ©ç”¨å”‡åŠ¨ä¸éŸ³é¢‘ä¿¡å·çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>æ¨¡ä»¿äººç±»è‡ªç„¶è®¤çŸ¥ï¼Œæ•æ‰å”‡éƒ¨ä¸å¤´éƒ¨åŒºåŸŸçš„è”ç³»æå‡å‡†ç¡®æ€§ã€‚</li>
<li>åˆ›å»ºäº†é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„LipSyncæ•°æ®é›†AVLipsä»¥æ¨åŠ¨ç ”ç©¶ã€‚</li>
<li>æ–¹æ³•å¹³å‡å‡†ç¡®ç‡è¶…è¿‡95.3%ï¼ŒçœŸå®åœºæ™¯éƒ¨ç½²ä¸­æœ€é«˜å‡†ç¡®ç‡å¯è¾¾90.2%ã€‚</li>
<li>ä¸ºä¿ƒè¿›ç ”ç©¶è¿›å±•ï¼Œç›¸å…³èµ„æºå·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-57e8ea8459fe96e2b15e9006a4aefb07.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a519c8d09d9edfcfa5c07628b3b02744.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc573fe14b6f21e0f5dfa8f74f52e2c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-568626bef17b55fa562059e3c0dd5c76.jpg" align="middle">
</details>




<h2 id="Learning-Disentangled-Representation-for-One-shot-Progressive-Face-Swapping"><a href="#Learning-Disentangled-Representation-for-One-shot-Progressive-Face-Swapping" class="headerlink" title="Learning Disentangled Representation for One-shot Progressive Face   Swapping"></a>Learning Disentangled Representation for One-shot Progressive Face   Swapping</h2><p><strong>Authors:Qi Li, Weining Wang, Chengzhong Xu, Zhenan Sun, Ming-Hsuan Yang</strong></p>
<p>Although face swapping has attracted much attention in recent years, it remains a challenging problem. Existing methods leverage a large number of data samples to explore the intrinsic properties of face swapping without considering the semantic information of face images. Moreover, the representation of the identity information tends to be fixed, leading to suboptimal face swapping. In this paper, we present a simple yet efficient method named FaceSwapper, for one-shot face swapping based on Generative Adversarial Networks. Our method consists of a disentangled representation module and a semantic-guided fusion module. The disentangled representation module comprises an attribute encoder and an identity encoder, which aims to achieve the disentanglement of the identity and attribute information. The identity encoder is more flexible, and the attribute encoder contains more attribute details than its competitors. Benefiting from the disentangled representation, FaceSwapper can swap face images progressively. In addition, semantic information is introduced into the semantic-guided fusion module to control the swapped region and model the pose and expression more accurately. Experimental results show that our method achieves state-of-the-art results on benchmark datasets with fewer training samples. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/liqi-casia/FaceSwapper">https://github.com/liqi-casia/FaceSwapper</a>. </p>
<blockquote>
<p>å°½ç®¡äººè„¸æ›¿æ¢è¿‘å¹´æ¥å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†å®ƒä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•åˆ©ç”¨å¤§é‡æ•°æ®æ ·æœ¬æ¢ç´¢äººè„¸æ›¿æ¢çš„å†…åœ¨å±æ€§ï¼Œè€Œæ²¡æœ‰è€ƒè™‘äººè„¸å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œèº«ä»½ä¿¡æ¯çš„è¡¨ç¤ºå¾€å¾€å›ºå®šï¼Œå¯¼è‡´äººè„¸æ›¿æ¢æ•ˆæœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•ï¼Œåä¸ºFaceSwapperï¼ŒåŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œè¿›è¡Œä¸€æ¬¡äººè„¸æ›¿æ¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªåˆ†è§£è¡¨ç¤ºæ¨¡å—å’Œä¸€ä¸ªè¯­ä¹‰å¼•å¯¼èåˆæ¨¡å—ã€‚åˆ†è§£è¡¨ç¤ºæ¨¡å—åŒ…æ‹¬ä¸€ä¸ªå±æ€§ç¼–ç å™¨å’Œä¸€ä¸ªèº«ä»½ç¼–ç å™¨ï¼Œæ—¨åœ¨å®ç°èº«ä»½å’Œå±æ€§ä¿¡æ¯çš„åˆ†è§£ã€‚èº«ä»½ç¼–ç å™¨æ›´çµæ´»ï¼Œè€Œå±æ€§ç¼–ç å™¨åŒ…å«æ¯”å…¶ç«äº‰å¯¹æ‰‹æ›´å¤šçš„å±æ€§ç»†èŠ‚ã€‚å¾—ç›Šäºåˆ†è§£è¡¨ç¤ºï¼ŒFaceSwapperå¯ä»¥é€æ­¥æ›¿æ¢äººè„¸å›¾åƒã€‚æ­¤å¤–ï¼Œè¯­ä¹‰ä¿¡æ¯è¢«å¼•å…¥åˆ°è¯­ä¹‰å¼•å¯¼èåˆæ¨¡å—ä¸­ï¼Œä»¥æ§åˆ¶äº¤æ¢åŒºåŸŸå¹¶æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå§¿åŠ¿å’Œè¡¨æƒ…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„ç»“æœï¼Œå¹¶ä¸”ä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ ·æœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/liqi-casia/FaceSwapper%E3%80%82">https://github.com/liqi-casia/FaceSwapperã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2203.12985v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„åä¸ºFaceSwapperçš„ä¸€é”®å¼äººè„¸æ›¿æ¢æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªè§£çº ç¼ è¡¨ç¤ºæ¨¡å—å’Œä¸€ä¸ªè¯­ä¹‰å¼•å¯¼èåˆæ¨¡å—ï¼Œæ—¨åœ¨å®ç°èº«ä»½å’Œå±æ€§ä¿¡æ¯çš„è§£çº ç¼ è¡¨ç¤ºï¼Œå¹¶å¼•å…¥è¯­ä¹‰ä¿¡æ¯ä»¥æ›´å‡†ç¡®åœ°æ§åˆ¶æ›¿æ¢åŒºåŸŸå’Œæ¨¡æ‹Ÿå§¿æ€è¡¨æƒ…ã€‚FaceSwapperå…·æœ‰é«˜æ•ˆæ€§èƒ½ï¼Œå¯ä»¥åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°å“è¶Šçš„ç»“æœï¼Œå¹¶ä¸”è®­ç»ƒæ ·æœ¬éœ€æ±‚è¾ƒå°‘ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Face swappingä¾ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯¾é¢˜ï¼Œå°½ç®¡è¿‘å¹´æ¥å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å¿½ç•¥äººè„¸å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿ç”¨å¤§é‡æ•°æ®æ ·æœ¬æ¢ç´¢äººè„¸æ›¿æ¢çš„å†…åœ¨å±æ€§ã€‚</li>
<li>FaceSwapperæ–¹æ³•åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå®ç°äº†ä¸€é”®å¼äººè„¸æ›¿æ¢ã€‚</li>
<li>FaceSwapperåŒ…æ‹¬ä¸€ä¸ªè§£çº ç¼ è¡¨ç¤ºæ¨¡å—ï¼Œæ—¨åœ¨å®ç°èº«ä»½å’Œå±æ€§ä¿¡æ¯çš„è§£çº ç¼ ã€‚</li>
<li>FaceSwapperå…·æœ‰æ›´çµæ´»çš„èº«ä»½ç¼–ç å™¨å’ŒåŒ…å«æ›´å¤šå±æ€§ç»†èŠ‚çš„å±æ€§ç¼–ç å™¨ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¯­ä¹‰ä¿¡æ¯ï¼ŒFaceSwapperå¯ä»¥æ›´å‡†ç¡®åœ°æ§åˆ¶æ›¿æ¢åŒºåŸŸå’Œæ¨¡æ‹Ÿå§¿æ€è¡¨æƒ…ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95629834f3cf6bc54341ed79654d363e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c6bc1519301445ad2d7edffc97eaa88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4935a78407e5af55526c5f65e64a1a49.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Face%20Swapping/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Face%20Swapping/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Face-Swapping/">
                                    <span class="chip bg-color">Face Swapping</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5237f15bcbbce4298b010ed16cb47cca.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Take Fake as Real Realistic-like Robust Black-box Adversarial Attack to   Evade AIGC Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Speech/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17124.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
