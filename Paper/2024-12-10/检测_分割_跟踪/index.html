<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  No Annotations for Object Detection in Art through Stable Diffusion">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-de829c8eefa8229fee894461d46ba5e6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    29.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    120 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion"><a href="#No-Annotations-for-Object-Detection-in-Art-through-Stable-Diffusion" class="headerlink" title="No Annotations for Object Detection in Art through Stable Diffusion"></a>No Annotations for Object Detection in Art through Stable Diffusion</h2><p><strong>Authors:Patrick Ramos, Nicolas Gonthier, Selina Khan, Yuta Nakashima, Noa Garcia</strong></p>
<p>Object detection in art is a valuable tool for the digital humanities, as it allows for faster identification of objects in artistic and historical images compared to humans. However, annotating such images poses significant challenges due to the need for specialized domain expertise. We present NADA (no annotations for detection in art), a pipeline that leverages diffusion modelsâ€™ art-related knowledge for object detection in paintings without the need for full bounding box supervision. Our method, which supports both weakly-supervised and zero-shot scenarios and does not require any fine-tuning of its pretrained components, consists of a class proposer based on large vision-language models and a class-conditioned detector based on Stable Diffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt, outperforming prior work in weakly-supervised detection, while being the first work for zero-shot object detection in art. Code is available at <a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada">https://github.com/patrick-john-ramos/nada</a> </p>
<blockquote>
<p>è‰ºæœ¯ä¸­çš„ç›®æ ‡æ£€æµ‹å¯¹äºæ•°å­—äººæ–‡æ¥è¯´æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ï¼Œå› ä¸ºå®ƒä¸äººç±»ç›¸æ¯”ï¼Œå¯ä»¥æ›´å¿«åœ°è¯†åˆ«è‰ºæœ¯å’Œå†å²å›¾åƒä¸­çš„ç›®æ ‡ã€‚ç„¶è€Œï¼Œå¯¹è¿™äº›å›¾åƒè¿›è¡Œæ ‡æ³¨å´é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†NADAï¼ˆè‰ºæœ¯ä¸­æ— éœ€æ³¨é‡Šçš„æ£€æµ‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®¡é“ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ä¸è‰ºæœ¯ç›¸å…³çš„çŸ¥è¯†ï¼Œåœ¨ç»˜ç”»ä¸­è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œæ— éœ€å®Œæ•´çš„è¾¹ç•Œæ¡†ç›‘ç£ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¼±ç›‘ç£å’Œæ— æºåœºæ™¯ï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹å…¶é¢„è®­ç»ƒç»„ä»¶è¿›è¡Œä»»ä½•å¾®è°ƒï¼Œå®ƒç”±åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ç±»æå‡ºè€…å’ŒåŸºäºStable Diffusionçš„ç±»æ¡ä»¶æ£€æµ‹å™¨ç»„æˆã€‚NADAåœ¨ArtDL 2.0å’ŒIconArtä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨å¼±ç›‘ç£æ£€æµ‹æ–¹é¢ä¼˜äºå…ˆå‰çš„å·¥ä½œï¼Œå¹¶ä¸”æ˜¯è‰ºæœ¯ä¸­æ— æºç›®æ ‡æ£€æµ‹çš„ç¬¬ä¸€é¡¹å·¥ä½œã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/patrick-john-ramos/nada%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/patrick-john-ramos/nadaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06286v1">PDF</a> 8 pages, 6 figures, to be published in WACV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‰ºæœ¯é¢†åŸŸçš„æ‰©æ•£æ¨¡å‹æŠ€æœ¯ï¼ŒNADAæ¨¡å‹å¯å®ç°æ— æ ‡æ³¨å¯¹è±¡æ£€æµ‹ï¼Œå¯å¹¿æ³›åº”ç”¨äºç”»ä½œä¸­ï¼Œæ— éœ€ç²¾ç»†æ ‡æ³¨æ¡†ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œç¨³å®šæ‰©æ•£æŠ€æœ¯ï¼Œæ”¯æŒå¼±ç›‘ç£å’Œæ— æºæ£€æµ‹åœºæ™¯ï¼Œé¢„è®­ç»ƒç»„ä»¶æ— éœ€å¾®è°ƒã€‚NADAæ¨¡å‹åœ¨ArtDL 2.0å’ŒIconArtä¸¤ä¸ªè‰ºæœ¯å“æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¼±ç›‘ç£æ£€æµ‹å·¥ä½œï¼Œå¹¶ä¸”æ˜¯è‰ºæœ¯å“é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹çš„é¦–æ¬¡å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NADAæ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è‰ºæœ¯ç›¸å…³çŸ¥è¯†è¿›è¡Œç”»ä½œä¸­çš„å¯¹è±¡æ£€æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å®Œæ•´çš„è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œé™ä½äº†æ ‡æ³¨éš¾åº¦ã€‚</li>
<li>NADAæ¨¡å‹åŸºäºå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œç¨³å®šæ‰©æ•£æŠ€æœ¯æ„å»ºã€‚</li>
<li>NADAæ¨¡å‹æ”¯æŒå¼±ç›‘ç£å’Œæ— æºæ£€æµ‹åœºæ™¯åº”ç”¨ã€‚</li>
<li>é¢„è®­ç»ƒç»„ä»¶æ— éœ€å¾®è°ƒå³å¯åº”ç”¨äºè‰ºæœ¯å“æ•°æ®é›†ã€‚</li>
<li>NADAæ¨¡å‹åœ¨ArtDL 2.0å’ŒIconArtæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å¼±ç›‘ç£æ£€æµ‹æ–¹æ³•ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a14511f959caceadb54e21f82a49b6a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4139a38513f0b2f267b75456dd3dfdfc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b04ddd24cf532a8716dc0a4011c96116.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9735771e9b15404efe8fd522bb09352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21873767c7771c024dd0a90f4d9774c2.jpg" align="middle">
</details>




<h2 id="A-Real-Time-Defense-Against-Object-Vanishing-Adversarial-Patch-Attacks-for-Object-Detection-in-Autonomous-Vehicles"><a href="#A-Real-Time-Defense-Against-Object-Vanishing-Adversarial-Patch-Attacks-for-Object-Detection-in-Autonomous-Vehicles" class="headerlink" title="A Real-Time Defense Against Object Vanishing Adversarial Patch Attacks   for Object Detection in Autonomous Vehicles"></a>A Real-Time Defense Against Object Vanishing Adversarial Patch Attacks   for Object Detection in Autonomous Vehicles</h2><p><strong>Authors:Jaden Mu</strong></p>
<p>Autonomous vehicles (AVs) increasingly use DNN-based object detection models in vision-based perception. Correct detection and classification of obstacles is critical to ensure safe, trustworthy driving decisions. Adversarial patches aim to fool a DNN with intentionally generated patterns concentrated in a localized region of an image. In particular, object vanishing patch attacks can cause object detection models to fail to detect most or all objects in a scene, posing a significant practical threat to AVs.   This work proposes ADAV (Adversarial Defense for Autonomous Vehicles), a novel defense methodology against object vanishing patch attacks specifically designed for autonomous vehicles. Unlike existing defense methods which have high latency or are designed for static images, ADAV runs in real-time and leverages contextual information from prior frames in an AVâ€™s video feed. ADAV checks if the object detectorâ€™s output for the target frame is temporally consistent with the output from a previous reference frame to detect the presence of a patch. If the presence of a patch is detected, ADAV uses gradient-based attribution to localize adversarial pixels that break temporal consistency. This two stage procedure allows ADAV to efficiently process clean inputs, and both stages are optimized to be low latency. ADAV is evaluated using real-world driving data from the Berkeley Deep Drive BDD100K dataset, and demonstrates high adversarial and clean performance. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„å¯¹è±¡æ£€æµ‹æ¨¡å‹è¿›è¡Œè§†è§‰æ„ŸçŸ¥ã€‚æ­£ç¡®æ£€æµ‹å’Œåˆ†ç±»éšœç¢ç‰©å¯¹äºç¡®ä¿å®‰å…¨å¯é çš„é©¾é©¶å†³ç­–è‡³å…³é‡è¦ã€‚å¯¹æŠ—æ€§è¡¥ä¸æ—¨åœ¨ç”¨æœ‰æ„ç”Ÿæˆçš„å›¾æ¡ˆæ¬ºéª—DNNï¼Œè¿™äº›å›¾æ¡ˆé›†ä¸­åœ¨å›¾åƒçš„å±€éƒ¨åŒºåŸŸã€‚ç‰¹åˆ«æ˜¯ï¼Œå¯¹è±¡æ¶ˆå¤±è¡¥ä¸æ”»å‡»å¯èƒ½å¯¼è‡´å¯¹è±¡æ£€æµ‹æ¨¡å‹æ— æ³•æ£€æµ‹åˆ°åœºæ™¯ä¸­çš„å¤§å¤šæ•°æˆ–æ‰€æœ‰å¯¹è±¡ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦æ„æˆé‡å¤§å®é™…å¨èƒã€‚</p>
</blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ADAVï¼ˆé’ˆå¯¹è‡ªåŠ¨é©¾é©¶æ±½è½¦çš„å¯¹æŠ—æ€§é˜²å¾¡ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå¯¹æŠ—è‡ªåŠ¨é©¾é©¶æ±½è½¦æ‰€é¢ä¸´çš„å¯¹è±¡æ¶ˆå¤±è¡¥ä¸æ”»å‡»çš„æ–°å‹é˜²å¾¡æ–¹æ³•ã€‚ä¸å…·æœ‰é«˜éŸ³å»¶è¿Ÿæˆ–ä¸“ä¸ºé™æ€å›¾åƒè®¾è®¡çš„ç°æœ‰é˜²å¾¡æ–¹æ³•ä¸åŒï¼ŒADAVå…·æœ‰å®æ—¶è¿è¡Œèƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨è‡ªåŠ¨é©¾é©¶æ±½è½¦è§†é¢‘é¦ˆé€ä¸­å…ˆå‰å¸§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ADAVæ£€æŸ¥ç›®æ ‡å¸§çš„å¯¹è±¡æ£€æµ‹å™¨çš„è¾“å‡ºæ˜¯å¦ä¸å…ˆå‰å‚è€ƒå¸§çš„è¾“å‡ºåœ¨æ—¶é—´ä¸Šä¸€è‡´ï¼Œä»¥æ£€æµ‹è¡¥ä¸çš„å­˜åœ¨ã€‚å¦‚æœæ£€æµ‹åˆ°è¡¥ä¸çš„å­˜åœ¨ï¼ŒADAVä¼šä½¿ç”¨åŸºäºæ¢¯åº¦çš„å½’å±æ¥å®šä½ç ´åæ—¶é—´ä¸€è‡´æ€§çš„å¯¹æŠ—æ€§åƒç´ ã€‚è¿™ä¸ªä¸¤é˜¶æ®µçš„è¿‡ç¨‹å…è®¸ADAVæœ‰æ•ˆåœ°å¤„ç†å¹²å‡€è¾“å…¥ï¼Œä¸¤ä¸ªé˜¶æ®µéƒ½ç»è¿‡ä¼˜åŒ–ä»¥å®ç°ä½å»¶è¿Ÿã€‚ä½¿ç”¨Berkeley Deep Drive BDD100Kæ•°æ®é›†çš„ç°å®ä¸–ç•Œé©¾é©¶æ•°æ®å¯¹ADAVè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºå…¶è‰¯å¥½çš„å¯¹æŠ—æ€§å’Œæ¸…æ´æ€§èƒ½ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06215v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVï¼‰å¯¹è±¡æ£€æµ‹æ¨¡å‹åœ¨å®é™…é©¾é©¶ç¯å¢ƒä¸­é¢ä¸´æ”»å‡»é£é™©ã€‚ADAVé˜²å¾¡ç­–ç•¥æ˜¯ä¸€ç§é’ˆå¯¹å¯¹è±¡æ¶ˆå¤±è¡¥ä¸æ”»å‡»çš„æ–°å‹å®æ—¶é˜²å¾¡æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨è§†é¢‘æµçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜é˜²å¾¡æ•ˆç‡ã€‚ADAVæ£€æµ‹è¡¥ä¸å­˜åœ¨ä¸å¦ï¼Œå¹¶é€šè¿‡æ¢¯åº¦å½’å› å®šä½ç ´åæ—¶é—´ä¸€è‡´æ€§çš„åƒç´ ç‚¹ã€‚è¯¥æ–¹æ³•åœ¨Berkeley Deep Drive BDD100Kæ•°æ®é›†ä¸Šçš„çœŸå®é©¾é©¶æ•°æ®è¯„ä¼°ä¸­è¡¨ç°å‡ºé«˜å¯¹æŠ—æ€§å’Œæ¸…æ´æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨åŸºäºæ·±åº¦ç¥ç»ç½‘ç»œçš„å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºè§†è§‰æ„ŸçŸ¥ã€‚</li>
<li>å¯¹æŠ—è¡¥ä¸å¯èƒ½ä½¿DNNå¯¹è±¡æ£€æµ‹æ¨¡å‹æ— æ³•æ­£ç¡®è¯†åˆ«éšœç¢ç‰©ï¼Œé€ æˆå®‰å…¨å¨èƒã€‚</li>
<li>ADAVæ˜¯ä¸€ç§é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å®æ—¶é˜²å¾¡ç­–ç•¥ï¼Œæ—¨åœ¨æŠµå¾¡å¯¹è±¡æ¶ˆå¤±è¡¥ä¸æ”»å‡»ã€‚</li>
<li>ADAVåˆ©ç”¨è§†é¢‘æµçš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥æé«˜é˜²å¾¡æ•ˆç‡ï¼Œè¿è¡Œé€Ÿåº¦å¿«ä¸”å…·æœ‰ä½å»¶è¿Ÿç‰¹æ€§ã€‚</li>
<li>ADAVé€šè¿‡æ£€æŸ¥ç›®æ ‡å¸§çš„å¯¹è±¡æ£€æµ‹å™¨è¾“å‡ºæ˜¯å¦ä¸å…ˆå‰å‚è€ƒå¸§çš„è¾“å‡ºåœ¨æ—¶é—´ä¸Šä¸€è‡´æ¥æ£€æµ‹è¡¥ä¸çš„å­˜åœ¨ã€‚</li>
<li>ADAVé‡‡ç”¨æ¢¯åº¦å½’å› æŠ€æœ¯æ¥å®šä½ç ´åæ—¶é—´ä¸€è‡´æ€§çš„åƒç´ ç‚¹ï¼Œå³å¯¹æŠ—æ€§åƒç´ ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f65858a63ed62643d95ee4e730c65d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45a011fc2ee25013d574984828fa53ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50d93d2550598afe330e5941640e3718.jpg" align="middle">
</details>




<h2 id="PoLaRIS-Dataset-A-Maritime-Object-Detection-and-Tracking-Dataset-in-Pohang-Canal"><a href="#PoLaRIS-Dataset-A-Maritime-Object-Detection-and-Tracking-Dataset-in-Pohang-Canal" class="headerlink" title="PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in   Pohang Canal"></a>PoLaRIS Dataset: A Maritime Object Detection and Tracking Dataset in   Pohang Canal</h2><p><strong>Authors:Jiwon Choi, Dongjin Cho, Gihyeon Lee, Hogyun Kim, Geonmo Yang, Joowan Kim, Younggun Cho</strong></p>
<p>Maritime environments often present hazardous situations due to factors such as moving ships or buoys, which become obstacles under the influence of waves. In such challenging conditions, the ability to detect and track potentially hazardous objects is critical for the safe navigation of marine robots. To address the scarcity of comprehensive datasets capturing these dynamic scenarios, we introduce a new multi-modal dataset that includes image and point-wise annotations of maritime hazards. Our dataset provides detailed ground truth for obstacle detection and tracking, including objects as small as 10$\times$10 pixels, which are crucial for maritime safety. To validate the datasetâ€™s effectiveness as a reliable benchmark, we conducted evaluations using various methodologies, including \ac{SOTA} techniques for object detection and tracking. These evaluations are expected to contribute to performance improvements, particularly in the complex maritime environment. To the best of our knowledge, this is the first dataset offering multi-modal annotations specifically tailored to maritime environments. Our dataset is available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/polaris-dataset">https://sites.google.com/view/polaris-dataset</a>. </p>
<blockquote>
<p>æµ·æ´‹ç¯å¢ƒå¸¸å¸¸ç”±äºèˆ¹åªæˆ–æµ®æ ‡çš„ç§»åŠ¨ç­‰å› ç´ è€Œé¢ä¸´å±é™©æƒ…å†µï¼Œè¿™äº›ç‰©ä½“åœ¨æ³¢æµªçš„å½±å“ä¸‹æˆä¸ºéšœç¢ã€‚åœ¨è¿™æ ·å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹ï¼Œæ£€æµ‹å’Œè·Ÿè¸ªæ½œåœ¨å±é™©ç‰©ä½“çš„èƒ½åŠ›å¯¹äºæµ·æ´‹æœºå™¨äººçš„å®‰å…¨å¯¼èˆªè‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³ç¼ºä¹æ•æ‰è¿™äº›åŠ¨æ€åœºæ™¯çš„å…¨é¢æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡å¼æ•°æ®é›†ï¼ŒåŒ…æ‹¬æµ·æ´‹å±é™©çš„æµ·æµªå›¾åƒå’Œç‚¹æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ•°æ®é›†ä¸ºéšœç¢æ£€æµ‹å’Œè·Ÿè¸ªæä¾›äº†è¯¦ç»†çš„çœŸå®åœ°é¢ä¿¡æ¯ï¼ŒåŒ…æ‹¬å°åˆ°10x10åƒç´ çš„ç‰©ä½“ï¼Œè¿™å¯¹äºæµ·ä¸Šå®‰å…¨è‡³å…³é‡è¦ã€‚ä¸ºäº†éªŒè¯æ•°æ®é›†ä½œä¸ºå¯é åŸºå‡†çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å„ç§æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ç”¨äºç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªã€‚è¿™äº›è¯„ä¼°æœ‰æœ›å¯¹æ€§èƒ½æå‡åšå‡ºè´¡çŒ®ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚çš„æµ·æ´‹ç¯å¢ƒä¸­ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºæµ·æ´‹ç¯å¢ƒé‡èº«å®šåˆ¶çš„å¤šæ¨¡å¼æ³¨é‡Šæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/polaris-dataset%E6%89%BE%E5%88%B0%E3%80%82">https://sites.google.com/view/polaris-datasetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æµ·äº‹å±é™©ç‰©ä½“æ£€æµ‹ä¸è·Ÿè¸ªæ•°æ®é›†ï¼ŒåŒ…å«å›¾åƒå’Œç‚¹æ ‡æ³¨ä¿¡æ¯ï¼Œæ—¨åœ¨è§£å†³æµ·äº‹ç¯å¢ƒä¸­å¤æ‚åœºæ™¯çš„ç¼ºå¤±é—®é¢˜ã€‚è¯¥æ•°æ®é›†è¯¦ç»†æä¾›äº†éšœç¢ç‰©æ£€æµ‹çš„åœ°é¢çœŸå®æƒ…å†µï¼ŒåŒ…å«å°è‡³10x10åƒç´ çš„å¯¹è±¡æ ‡æ³¨ä¿¡æ¯ï¼Œèƒ½æœ‰æ•ˆä¿ƒè¿›æµ·äº‹ç¯å¢ƒä¸­çš„ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªæŠ€æœ¯çš„å‘å±•ã€‚ç›®å‰ï¼Œè¯¥æ•°æ®é›†å·²ç»è¢«éªŒè¯ä¸ºä¸€ä¸ªå¯é çš„åŸºå‡†æµ‹è¯•é›†ï¼Œè¯„ä¼°äº†å„ç§å…ˆè¿›çš„ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªæŠ€æœ¯ã€‚è¯¥æ•°æ®é›†å¡«è¡¥äº†ç°æœ‰ç ”ç©¶çš„ç©ºç™½ï¼Œé¦–æ¬¡ä¸“é—¨é’ˆå¯¹æµ·äº‹ç¯å¢ƒæä¾›äº†å¤šæ¨¡æ€æ ‡æ³¨ï¼Œç›®å‰å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°æ•°æ®é›†ä¸“æ³¨äºæµ·äº‹ç¯å¢ƒä¸­å¤æ‚åœºæ™¯çš„éšœç¢æ£€æµ‹å’Œè·Ÿè¸ªä»»åŠ¡ã€‚</li>
<li>æ•°æ®é›†åŒ…å«äº†å¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¦‚å›¾åƒå’Œç‚¹æ ‡æ³¨æ•°æ®ã€‚</li>
<li>æ•°æ®é›†æ³¨é‡å°å‹éšœç¢ç‰©çš„æ ‡æ³¨ï¼Œå…¶å¤§å°ç”šè‡³å¯ä»¥å°åˆ°10x10åƒç´ ã€‚</li>
<li>æ•°æ®é›†è¢«éªŒè¯ä¸ºä¸€ä¸ªå¯é çš„åŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ªæŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ•°æ®é›†å¡«è¡¥äº†ç°æœ‰çš„ç ”ç©¶ç©ºç™½ï¼Œæ˜¯é¦–ä¸ªä¸“é—¨ä¸ºæµ·äº‹ç¯å¢ƒè®¾è®¡çš„å¤šæ¨¡æ€æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†è¯¦ç»†çš„åœ°é¢çœŸå®æƒ…å†µä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¨åŠ¨æµ·äº‹ç¯å¢ƒä¸­çš„éšœç¢æ£€æµ‹å’Œè·Ÿè¸ªæŠ€æœ¯çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e3e137410b0cf4d3d4700ef988c8364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7551098f3f0061fd6ff338febf488ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-705158de5cf0ca904302a89c14af8f72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0900003f769c9ba33e1224e6f11b589.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73e003ade4502b74e7c0c134b89ed1de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-433cd134340d13ee1a1b0004dcbdfe89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44420741b2907ffe0662b57f993d07b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e442a60a919689f1f45f736e112b517.jpg" align="middle">
</details>




<h2 id="GCUNet-A-GNN-Based-Contextual-Learning-Network-for-Tertiary-Lymphoid-Structure-Semantic-Segmentation-in-Whole-Slide-Image"><a href="#GCUNet-A-GNN-Based-Contextual-Learning-Network-for-Tertiary-Lymphoid-Structure-Semantic-Segmentation-in-Whole-Slide-Image" class="headerlink" title="GCUNet: A GNN-Based Contextual Learning Network for Tertiary Lymphoid   Structure Semantic Segmentation in Whole Slide Image"></a>GCUNet: A GNN-Based Contextual Learning Network for Tertiary Lymphoid   Structure Semantic Segmentation in Whole Slide Image</h2><p><strong>Authors:Lei Su, Yang Du</strong></p>
<p>We focus on tertiary lymphoid structure (TLS) semantic segmentation in whole slide image (WSI). Unlike TLS binary segmentation, TLS semantic segmentation identifies boundaries and maturity, which requires integrating contextual information to discover discriminative features. Due to the extensive scale of WSI (e.g., 100,000 \times 100,000 pixels), the segmentation of TLS is usually carried out through a patch-based strategy. However, this prevents the model from accessing information outside of the patches, limiting the performance. To address this issue, we propose GCUNet, a GNN-based contextual learning network for TLS semantic segmentation. Given an image patch (target) to be segmented, GCUNet first progressively aggregates long-range and fine-grained context outside the target. Then, a Detail and Context Fusion block (DCFusion) is designed to integrate the context and detail of the target to predict the segmentation mask. We build four TLS semantic segmentation datasets, called TCGA-COAD, TCGA-LUSC, TCGA-BLCA and INHOUSE-PAAD, and make the former three datasets (comprising 826 WSIs and 15,276 TLSs) publicly available to promote the TLS semantic segmentation. Experiments on these datasets demonstrate the superiority of GCUNet, achieving at least 7.41% improvement in mF1 compared with SOTA. </p>
<blockquote>
<p>æœ¬æ–‡å…³æ³¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­çš„ä¸‰çº§æ·‹å·´ç»“æ„ï¼ˆTLSï¼‰è¯­ä¹‰åˆ†å‰²ã€‚ä¸åŒäºTLSäºŒå…ƒåˆ†å‰²ï¼ŒTLSè¯­ä¹‰åˆ†å‰²éœ€è¦è¯†åˆ«è¾¹ç•Œå’Œæˆç†Ÿåº¦ï¼Œè¿™éœ€è¦æ•´åˆä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å‘ç°åˆ¤åˆ«ç‰¹å¾ã€‚é‰´äºWSIçš„å¤§è§„æ¨¡èŒƒå›´ï¼ˆä¾‹å¦‚ï¼Œ100,000 x 100,000åƒç´ ï¼‰ï¼ŒTLSçš„åˆ†å‰²é€šå¸¸é‡‡ç”¨åŸºäºè¡¥ä¸çš„ç­–ç•¥è¿›è¡Œã€‚ç„¶è€Œï¼Œè¿™ä¼šé˜»æ­¢æ¨¡å‹è®¿é—®è¡¥ä¸ä¹‹å¤–çš„ä¿¡æ¯ï¼Œé™åˆ¶äº†æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GCUNetï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç½‘ç»œï¼Œç”¨äºTLSè¯­ä¹‰åˆ†å‰²ã€‚å¯¹äºè¦åˆ†å‰²çš„å›¾åƒè¡¥ä¸ï¼ˆç›®æ ‡ï¼‰ï¼ŒGCUNeté¦–å…ˆé€æ­¥èšåˆç›®æ ‡å¤–éƒ¨çš„é•¿ç¨‹å’Œç²¾ç»†ä¸Šä¸‹æ–‡ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ä¸ªç»†èŠ‚å’Œä¸Šä¸‹æ–‡èåˆå—ï¼ˆDCFusionï¼‰ï¼Œä»¥æ•´åˆç›®æ ‡å’Œä¸Šä¸‹æ–‡çš„ç»†èŠ‚æ¥é¢„æµ‹åˆ†å‰²æ©è†œã€‚æˆ‘ä»¬å»ºç«‹äº†å››ä¸ªTLSè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯TCGA-COADã€TCGA-LUSCã€TCGA-BLCAå’ŒINHOUSE-PAADï¼Œå¹¶å°†å‰ä¸‰ä¸ªæ•°æ®é›†ï¼ˆåŒ…å«826å¼ WSIå’Œ15,276ä¸ªTLSï¼‰å…¬å¼€ï¼Œä»¥ä¿ƒè¿›TLSè¯­ä¹‰åˆ†å‰²çš„å‘å±•ã€‚åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGCUNetå…·æœ‰ä¼˜è¶Šæ€§ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒmF1æé«˜äº†è‡³å°‘7.41%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06129v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¯¥ç ”ç©¶èšç„¦äºå…¨è§†é‡æ˜¾å¾®å›¾åƒä¸­çš„ä¸‰çº§æ·‹å·´ç»“æ„è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚é’ˆå¯¹è¯¥é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç½‘ç»œGCUNetã€‚GCUNeté€šè¿‡æ¸è¿›å¼èšåˆç›®æ ‡å¤–éƒ¨çš„é•¿ç¨‹å’Œç²¾ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè§£å†³äº†ä¼ ç»ŸåŸºäºè¡¥ä¸ç­–ç•¥çš„TLSåˆ†å‰²æ–¹æ³•æ— æ³•è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº†å››ä¸ªTLSè¯­ä¹‰åˆ†å‰²æ•°æ®é›†å¹¶å…¬å¼€äº†éƒ¨åˆ†æ•°æ®é›†ä»¥ä¿ƒè¿›ç ”ç©¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGCUNetç›¸è¾ƒäºç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒmF1å¾—åˆ†æé«˜äº†è‡³å°‘7.41%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>ç ”ç©¶é‡ç‚¹ä¸ºå…¨è§†é‡æ˜¾å¾®å›¾åƒä¸­çš„ä¸‰çº§æ·‹å·´ç»“æ„ï¼ˆTLSï¼‰è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>TLSè¯­ä¹‰åˆ†å‰²éœ€è¦è¯†åˆ«è¾¹ç•Œå’Œæˆç†Ÿåº¦ï¼Œéœ€æ•´åˆä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å‘ç°åˆ¤åˆ«ç‰¹å¾ã€‚</li>
<li>ç”±äºå…¨è§†é‡å›¾åƒè§„æ¨¡åºå¤§ï¼Œé€šå¸¸é‡‡ç”¨åŸºäºè¡¥ä¸çš„ç­–ç•¥è¿›è¡ŒTLSåˆ†å‰²ï¼Œä½†è¿™é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºGCUNetï¼Œä¸€ä¸ªåŸºäºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„ä¸Šä¸‹æ–‡å­¦ä¹ ç½‘ç»œï¼Œä»¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>GCUNeté€šè¿‡æ¸è¿›å¼èšåˆç›®æ ‡å¤–éƒ¨çš„é•¿ç¨‹å’Œç²¾ç»†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæé«˜TLSåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†å››ä¸ªTLSè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶å…¬å¼€éƒ¨åˆ†æ•°æ®é›†ä»¥ä¿ƒè¿›ç ”ç©¶ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95117cee22c1d0ac7dc151c925c2b4ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-855c01ff545e36145aaedec54806e5e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20e386b2aa27317f2c81b990af7bf2f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-543805bf4af75f4531f2d5abfdbee63f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fcf19d48f6caf2a5e5403a3b50cfdd1.jpg" align="middle">
</details>




<h2 id="Efficient-Semantic-Splatting-for-Remote-Sensing-Multi-view-Segmentation"><a href="#Efficient-Semantic-Splatting-for-Remote-Sensing-Multi-view-Segmentation" class="headerlink" title="Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation"></a>Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation</h2><p><strong>Authors:Zipeng Qi, Hao Chen, Haotian Zhang, Zhengxia Zou, Zhenwei Shi</strong></p>
<p>In this paper, we propose a novel semantic splatting approach based on Gaussian Splatting to achieve efficient and low-latency. Our method projects the RGB attributes and semantic features of point clouds onto the image plane, simultaneously rendering RGB images and semantic segmentation results. Leveraging the explicit structure of point clouds and a one-time rendering strategy, our approach significantly enhances efficiency during optimization and rendering. Additionally, we employ SAM2 to generate pseudo-labels for boundary regions, which often lack sufficient supervision, and introduce two-level aggregation losses at the 2D feature map and 3D spatial levels to improve the view-consistent and spatial continuity. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯splatçš„æ–°é¢–è¯­ä¹‰splatæ–¹æ³•ï¼Œä»¥å®ç°é«˜æ•ˆå’Œä½å»¶è¿Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ç‚¹äº‘çš„RGBå±æ€§å’Œè¯­ä¹‰ç‰¹å¾æŠ•å½±åˆ°å›¾åƒå¹³é¢ä¸Šï¼ŒåŒæ—¶å‘ˆç°RGBå›¾åƒå’Œè¯­ä¹‰åˆ†å‰²ç»“æœã€‚é€šè¿‡åˆ©ç”¨ç‚¹äº‘çš„æ˜ç¡®ç»“æ„å’Œä¸€æ¬¡æ€§æ¸²æŸ“ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¼˜åŒ–å’Œæ¸²æŸ“è¿‡ç¨‹ä¸­å¤§å¤§æé«˜äº†æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨SAM2ä¸ºè¾¹ç•ŒåŒºåŸŸç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¿™äº›åŒºåŸŸé€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„ç›‘ç£ä¿¡æ¯ï¼Œå¹¶åœ¨äºŒç»´ç‰¹å¾å›¾å’Œä¸‰ç»´ç©ºé—´çº§åˆ«å¼•å…¥ä¸¤çº§èšåˆæŸå¤±ï¼Œä»¥æé«˜è§†å›¾ä¸€è‡´æ€§å’Œç©ºé—´è¿ç»­æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05969v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºé«˜æ–¯æ˜ å°„æ³•ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­ä¹‰æ¶‚æŠ¹æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆä¸”ä½å»¶è¿Ÿçš„ç‚¹äº‘RGBå±æ€§å’Œè¯­ä¹‰ç‰¹å¾æŠ•å½±è‡³å›¾åƒå¹³é¢çš„æ¸²æŸ“è¿‡ç¨‹ã€‚é€šè¿‡åˆ©ç”¨ç‚¹äº‘çš„æ˜¾å¼ç»“æ„å’Œä¸€æ¬¡æ¸²æŸ“ç­–ç•¥ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ä¼˜åŒ–å’Œæ¸²æŸ“é˜¶æ®µçš„æ•ˆç‡ã€‚åŒæ—¶ï¼Œä½¿ç”¨SAM2ä¸ºè¾¹ç•ŒåŒºåŸŸç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œå¹¶å¼•å…¥äºŒç»´ç‰¹å¾å›¾å’Œä¸‰ç»´ç©ºé—´çš„ä¸¤çº§èšåˆæŸå¤±ï¼Œä»¥æé«˜è§†å›¾ä¸€è‡´æ€§å’Œç©ºé—´è¿ç»­æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºäº†ä¸€ç§åŸºäºé«˜æ–¯æ¶‚æŠ¹çš„è¯­ä¹‰æ¶‚æŠ¹æ–¹æ³•ï¼Œå®ç°é«˜æ•ˆä½å»¶è¿Ÿçš„ç‚¹äº‘æ¸²æŸ“ã€‚</li>
<li>å°†ç‚¹äº‘çš„RGBå±æ€§å’Œè¯­ä¹‰ç‰¹å¾æŠ•å½±è‡³å›¾åƒå¹³é¢ï¼ŒåŒæ—¶ç”ŸæˆRGBå›¾åƒå’Œè¯­ä¹‰åˆ†å‰²ç»“æœã€‚</li>
<li>åˆ©ç”¨ç‚¹äº‘çš„æ˜¾å¼ç»“æ„å’Œä¸€æ¬¡æ¸²æŸ“ç­–ç•¥æé«˜ä¼˜åŒ–å’Œæ¸²æŸ“æ•ˆç‡ã€‚</li>
<li>ä½¿ç”¨SAM2ç”Ÿæˆè¾¹ç•ŒåŒºåŸŸçš„ä¼ªæ ‡ç­¾ï¼Œè§£å†³ç›‘ç£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äºŒç»´ç‰¹å¾å›¾å’Œä¸‰ç»´ç©ºé—´çš„ä¸¤çº§èšåˆæŸå¤±ï¼Œæé«˜è§†å›¾ä¸€è‡´æ€§å’Œç©ºé—´è¿ç»­æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60778ac7ab810578fdf49d79641cfdeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-938afbcaab2fd784bf1e56b4a8ed630d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d736ab67df80ac632916ef225b983c40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0d5647eeb43d4a997328c2a4a53b302.jpg" align="middle">
</details>




<h2 id="Tiny-Object-Detection-with-Single-Point-Supervision"><a href="#Tiny-Object-Detection-with-Single-Point-Supervision" class="headerlink" title="Tiny Object Detection with Single Point Supervision"></a>Tiny Object Detection with Single Point Supervision</h2><p><strong>Authors:Haoran Zhu, Chang Xu, Ruixiang Zhang, Fang Xu, Wen Yang, Haijian Zhang, Gui-Song Xia</strong></p>
<p>Tiny objects, with their limited spatial resolution, often resemble point-like distributions. As a result, bounding box prediction using point-level supervision emerges as a natural and cost-effective alternative to traditional box-level supervision. However, the small scale and lack of distinctive features of tiny objects make point annotations prone to noise, posing significant hurdles for model robustness. To tackle these challenges, we propose Point Teacherâ€“the first end-to-end point-supervised method for robust tiny object detection in aerial images. To handle label noise from scale ambiguity and location shifts in point annotations, Point Teacher employs the teacher-student architecture and decouples the learning into a two-phase denoising process. In this framework, the teacher network progressively denoises the pseudo boxes derived from noisy point annotations, guiding the student networkâ€™s learning. Specifically, in the first phase, random masking of image regions facilitates regression learning, enabling the teacher to transform noisy point annotations into coarse pseudo boxes. In the second phase, these coarse pseudo boxes are refined using dynamic multiple instance learning, which adaptively selects the most reliable instance from dynamically constructed proposal bags around the coarse pseudo boxes. Extensive experiments on three tiny object datasets (i.e., AI-TOD-v2, SODA-A, and TinyPerson) validate the proposed methodâ€™s effectiveness and robustness against point location shifts. Notably, relying solely on point supervision, our Point Teacher already shows comparable performance with box-supervised learning methods. Codes and models will be made publicly available. </p>
<blockquote>
<p>å¯¹äºå…·æœ‰æœ‰é™ç©ºé—´åˆ†è¾¨ç‡çš„å¾®å°ç‰©ä½“æ¥è¯´ï¼Œç”±äºå…¶å¤–å½¢ç±»ä¼¼ç‚¹çŠ¶åˆ†å¸ƒçš„ç‰¹ç‚¹ï¼Œå› æ­¤é‡‡ç”¨ç‚¹çŠ¶ç›‘ç£ï¼ˆpoint-level supervisionï¼‰çš„è¾¹ç•Œæ¡†é¢„æµ‹æ–¹æ³•æˆä¸ºäº†ä¼ ç»Ÿæ¡†çº§ç›‘ç£çš„ä¸€ç§è‡ªç„¶ä¸”ç»æµçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¾®å°ç‰©ä½“ä½“ç§¯å°ã€ç‰¹å¾ç¼ºä¹è¾¨åˆ«åŠ›çš„ç‰¹ç‚¹ä½¿å¾—ç‚¹æ ‡æ³¨å®¹æ˜“å¼•å…¥å™ªå£°ï¼Œä¸ºæ¨¡å‹ç¨³å¥æ€§å¸¦æ¥æ˜¾è‘—æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Point Teacherâ€”â€”è¿™æ˜¯é¦–ä¸ªç«¯åˆ°ç«¯çš„ç‚¹ç›‘ç£æ–¹æ³•ï¼Œä¸»è¦ç”¨äºé²æ£’çš„èˆªç©ºå›¾åƒå¾®å°ç›®æ ‡æ£€æµ‹ã€‚ä¸ºå¤„ç†ç‚¹æ ‡æ³¨ä¸­å› å°ºåº¦æ¨¡ç³Šå’Œä½ç½®åç§»å¯¼è‡´çš„æ ‡ç­¾å™ªå£°é—®é¢˜ï¼ŒPoint Teacheré‡‡ç”¨å¸ˆç”Ÿæ¶æ„å¹¶å°†å­¦ä¹ è¿‡ç¨‹åˆ†è§£ä¸ºä¸¤é˜¶æ®µçš„é™å™ªè¿‡ç¨‹ã€‚åœ¨è¿™ä¸€æ¡†æ¶ä¸­ï¼Œæ•™å¸ˆç½‘ç»œå¯¹ç”±å™ªå£°ç‚¹æ ‡æ³¨ç”Ÿæˆçš„ä¼ªç›’å­è¿›è¡Œæ¸è¿›é™å™ªå¤„ç†ï¼Œå¼•å¯¼å­¦ç”Ÿç½‘ç»œçš„å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡éšæœºå±è”½å›¾åƒåŒºåŸŸæ¥ä¿ƒè¿›å›å½’å­¦ä¹ ï¼Œä½¿æ•™å¸ˆèƒ½å¤Ÿä»å™ªå£°ç‚¹æ ‡æ³¨ç”Ÿæˆç²—ç•¥çš„ä¼ªç›’å­ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨åŠ¨æ€å¤šå®ä¾‹å­¦ä¹ æ¥ç²¾ç‚¼è¿™äº›ç²—ç•¥çš„ä¼ªç›’å­ï¼Œè¯¥æ–¹æ³•è‡ªé€‚åº”åœ°ä»å›´ç»•ç²—ç•¥ä¼ªç›’æ„å»ºçš„åŠ¨æ€ææ¡ˆåŒ…ä¸­é€‰æ‹©æœ€å¯é çš„å®ä¾‹ã€‚åœ¨ä¸‰ä¸ªå¾®å°ç›®æ ‡æ•°æ®é›†ï¼ˆå³AI-TOD-v2ã€SODA-Aå’ŒTinyPersonï¼‰ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æŠ—å¹²æ‰°èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä¾èµ–ç‚¹ç›‘ç£çš„Point Teacherå·²ç»æ˜¾ç¤ºå‡ºä¸æ¡†ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05837v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPoint Teacherçš„ç‚¹ç›‘ç£æ–¹æ³•ï¼Œç”¨äºç¨³å¥åœ°æ£€æµ‹èˆªç©ºå›¾åƒä¸­çš„å¾®å°ç‰©ä½“ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå»å™ªè¿‡ç¨‹å¤„ç†ç‚¹æ³¨é‡Šä¸­çš„æ ‡ç­¾å™ªå£°ã€‚æ•™å¸ˆç½‘ç»œå°†å™ªå£°ç‚¹æ³¨é‡Šè½¬åŒ–ä¸ºç²—ç•¥çš„ä¼ªç›’å­ï¼Œå¹¶åœ¨ç¬¬äºŒé˜¶æ®µè¿›è¡Œç²¾ç‚¼ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¾®å°ç‰©ä½“æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚ä»…ä¾èµ–ç‚¹ç›‘ç£ï¼ŒPoint Teacherçš„æ€§èƒ½å·²æ¥è¿‘ç›’ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Point-level supervisionä½œä¸ºä¸€ç§è‡ªç„¶ä¸”ç»æµå®æƒ çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºè§£å†³å¾®å°ç‰©ä½“æ£€æµ‹ä¸­çš„ç©ºé—´åˆ†è¾¨ç‡æœ‰é™é—®é¢˜ã€‚</li>
<li>å¾®å°ç‰©ä½“çš„å°ºåº¦å’Œç¼ºä¹æ˜¾è‘—ç‰¹å¾ä½¿å¾—ç‚¹æ³¨é‡Šå®¹æ˜“å—åˆ°å™ªå£°å½±å“ã€‚</li>
<li>Point Teacheræ˜¯é¦–ä¸ªç«¯åˆ°ç«¯çš„ç‚¹ç›‘ç£æ–¹æ³•ï¼Œç”¨äºç¨³å¥åœ°æ£€æµ‹èˆªç©ºå›¾åƒä¸­çš„å¾®å°ç‰©ä½“ã€‚</li>
<li>æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„ç”¨äºå¤„ç†æ ‡ç­¾å™ªå£°ï¼Œé€šè¿‡ä¸¤é˜¶æ®µå»å™ªè¿‡ç¨‹æ¥ä¼˜åŒ–ç‚¹æ³¨é‡Šçš„ä¼ªç›’å­ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡éšæœºæ©è”½å›¾åƒåŒºåŸŸä¿ƒè¿›å›å½’å­¦ä¹ ï¼Œå°†å™ªå£°ç‚¹æ³¨é‡Šè½¬åŒ–ä¸ºç²—ç•¥ä¼ªç›’å­ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µä½¿ç”¨åŠ¨æ€å¤šé‡å®ä¾‹å­¦ä¹ ç²¾ç‚¼ä¼ªç›’å­ï¼Œè‡ªé€‚åº”é€‰æ‹©æœ€å¯é çš„å®ä¾‹ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¾®å°ç‰©ä½“æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†Point Teacherçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b9d18199a818da75dfb2e0bdc627a7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c7a018fcc39238d83560e7e2c1a8216.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ad5f9bb895c42ca3f16ddc3090aae5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb9367e3f8a71a862e8cd49e265f715b.jpg" align="middle">
</details>




<h2 id="Rethinking-Annotation-for-Object-Detection-Is-Annotating-Small-size-Instances-Worth-Its-Cost"><a href="#Rethinking-Annotation-for-Object-Detection-Is-Annotating-Small-size-Instances-Worth-Its-Cost" class="headerlink" title="Rethinking Annotation for Object Detection: Is Annotating Small-size   Instances Worth Its Cost?"></a>Rethinking Annotation for Object Detection: Is Annotating Small-size   Instances Worth Its Cost?</h2><p><strong>Authors:Yusuke Hosoya, Masanori Suganuma, Takayuki Okatani</strong></p>
<p>Detecting objects occupying only small areas in an image is difficult, even for humans. Therefore, annotating small-size object instances is hard and thus costly. This study questions common sense by asking the following: is annotating small-size instances worth its cost? We restate it as the following verifiable question: can we detect small-size instances with a detector trained using training data free of small-size instances? We evaluate a method that upscales input images at test time and a method that downscales images at training time. The experiments conducted using the COCO dataset show the following. The first method, together with a remedy to narrow the domain gap between training and test inputs, achieves at least comparable performance to the baseline detector trained using complete training data. Although the method needs to apply the same detector twice to an input image with different scaling, we show that its distillation yields a single-path detector that performs equally well to the same baseline detector. These results point to the necessity of rethinking the annotation of training data for object detection. </p>
<blockquote>
<p>æ£€æµ‹å›¾åƒä¸­åªå å¾ˆå°é¢ç§¯çš„ç›®æ ‡ç‰©ä½“æ˜¯å¾ˆå›°éš¾çš„ï¼Œå³ä½¿æ˜¯å¯¹äºäººç±»æ¥è¯´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼Œå¯¹å°å‹ç›®æ ‡å®ä¾‹è¿›è¡Œæ ‡æ³¨æ˜¯å¾ˆå›°éš¾ä¸”æˆæœ¬å¾ˆé«˜çš„ã€‚æœ¬ç ”ç©¶é€šè¿‡ä»¥ä¸‹é—®é¢˜è´¨ç–‘å¸¸è¯†ï¼šæ ‡æ³¨å°å‹å®ä¾‹å€¼å¾—å…¶æˆæœ¬å—ï¼Ÿæˆ‘ä»¬å°†å…¶é‡æ–°è¡¨è¿°ä¸ºä»¥ä¸‹å¯éªŒè¯çš„é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦ä½¿ç”¨æœªåŒ…å«å°å‹å®ä¾‹çš„è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œæ£€æµ‹å°å‹å®ä¾‹ï¼Ÿæˆ‘ä»¬è¯„ä¼°äº†ä¸€ç§åœ¨æµ‹è¯•æ—¶å¯¹è¾“å…¥å›¾åƒè¿›è¡Œæ”¾å¤§çš„æ–¹æ³•ï¼Œä»¥åŠä¸€ç§åœ¨è®­ç»ƒæ—¶å¯¹å›¾åƒè¿›è¡Œç¼©å°çš„æ–¹æ³•ã€‚ä½¿ç”¨COCOæ•°æ®é›†è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼šç¬¬ä¸€ç§æ–¹æ³•é…åˆç¼©å°è®­ç»ƒä¸æµ‹è¯•è¾“å…¥ä¹‹é—´é¢†åŸŸå·®è·çš„è¡¥æ•‘æªæ–½ï¼Œè‡³å°‘å¯ä¸ä½¿ç”¨å®Œæ•´è®­ç»ƒæ•°æ®è¿›è¡Œè®­ç»ƒçš„åŸºçº¿æ£€æµ‹å™¨å®ç°ç›¸å½“çš„æ€§èƒ½ã€‚å°½ç®¡è¯¥æ–¹æ³•éœ€è¦å¯¹è¾“å…¥å›¾åƒä¸¤æ¬¡åº”ç”¨ç›¸åŒçš„æ£€æµ‹å™¨å¹¶è¿›è¡Œä¸åŒçš„ç¼©æ”¾ï¼Œä½†æˆ‘ä»¬è¡¨æ˜å…¶æç‚¼äº§ç”Ÿäº†å•è·¯å¾„æ£€æµ‹å™¨ï¼Œå…¶æ€§èƒ½ä¸ç›¸åŒçš„åŸºçº¿æ£€æµ‹å™¨åŒæ ·å‡ºè‰²ã€‚è¿™äº›ç»“æœæŒ‡å‘äº†é‡æ–°æ€è€ƒç›®æ ‡æ£€æµ‹è®­ç»ƒæ•°æ®æ ‡æ³¨çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05611v1">PDF</a> 12 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ£€æµ‹å›¾åƒä¸­å°å°ºå¯¸ç‰©ä½“çš„é—®é¢˜ï¼Œæå‡ºæ ‡æ³¨å°å°ºå¯¸å®ä¾‹æ˜¯å¦å€¼å¾—å…¶æˆæœ¬çš„é—®é¢˜ã€‚é€šè¿‡è¯„ä¼°ä¸€ç§æµ‹è¯•æ—¶æ”¾å¤§è¾“å…¥å›¾åƒçš„æ–¹æ³•ä»¥åŠè®­ç»ƒæ—¶ç¼©å°å›¾åƒçš„æ–¹æ³•ï¼Œå‘ç°è®­ç»ƒæ—¶é‡‡ç”¨å®Œå…¨çš„æ•°æ®é›†å’Œè®­ç»ƒæ•°æ®å…è´¹çš„å°å°ºå¯¸å®ä¾‹çš„æ¢æµ‹å™¨ä¹‹é—´çš„æ€§èƒ½è‡³å°‘ç›¸å½“ã€‚å®éªŒç»“æœè¡¨æ˜éœ€è¦é‡æ–°æ€è€ƒè®­ç»ƒæ•°æ®æ ‡æ³¨çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€æµ‹å›¾åƒä¸­å°å°ºå¯¸ç‰©ä½“æ˜¯å›°éš¾çš„ï¼Œæ ‡æ³¨å°å°ºå¯¸å®ä¾‹æˆæœ¬é«˜ä¸”å›°éš¾ã€‚</li>
<li>ç ”ç©¶è´¨ç–‘äº†æ˜¯å¦éœ€è¦æ ‡æ³¨å°å°ºå¯¸å®ä¾‹æ¥è®­ç»ƒæ£€æµ‹å™¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶æ”¾å¤§è¾“å…¥å›¾åƒçš„æ–¹æ³•æ¥æé«˜å°å°ºå¯¸ç‰©ä½“çš„æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨äº†ä¸€ç§ç¼©å°è®­ç»ƒæ—¶å›¾åƒå¤§å°çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®éªŒä½¿ç”¨COCOæ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œç»“æœè¡¨æ˜ä½¿ç”¨å®Œå…¨æ•°æ®é›†è®­ç»ƒçš„æ£€æµ‹å™¨æ€§èƒ½ç›¸å½“æˆ–æ›´å¥½ã€‚</li>
<li>å°½ç®¡éœ€è¦ä¸¤æ¬¡åº”ç”¨åŒä¸€æ£€æµ‹å™¨äºè¾“å…¥å›¾åƒçš„ä¸åŒç¼©æ”¾çº§åˆ«ï¼Œä½†å¯ä»¥é€šè¿‡è’¸é¦æ–¹æ³•è·å¾—å…·æœ‰ç›¸åŒæ€§èƒ½çš„å•è·¯å¾„æ£€æµ‹å™¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da37640528817718d0e4ef680b4e11c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b62ed76b55c69a30c12cc4aa3ad35c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ae66da916729f22b647977fccd4ef46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c0df66ebd54d4c88bbf4d09d234eebc.jpg" align="middle">
</details>




<h2 id="Generative-Model-Based-Fusion-for-Improved-Few-Shot-Semantic-Segmentation-of-Infrared-Images"><a href="#Generative-Model-Based-Fusion-for-Improved-Few-Shot-Semantic-Segmentation-of-Infrared-Images" class="headerlink" title="Generative Model-Based Fusion for Improved Few-Shot Semantic   Segmentation of Infrared Images"></a>Generative Model-Based Fusion for Improved Few-Shot Semantic   Segmentation of Infrared Images</h2><p><strong>Authors:Junno Yun, Mehmet AkÃ§akaya</strong></p>
<p>Infrared (IR) imaging is commonly used in various scenarios, including autonomous driving, fire safety and defense applications. Thus, semantic segmentation of such images is of great interest. However, this task faces several challenges, including data scarcity, differing contrast and input channel number compared to natural images, and emergence of classes not represented in databases in certain scenarios, such as defense applications. Few-shot segmentation (FSS) provides a framework to overcome these issues by segmenting query images using a few labeled support samples. However, existing FSS models for IR images require paired visible RGB images, which is a major limitation since acquiring such paired data is difficult or impossible in some applications. In this work, we develop new strategies for FSS of IR images by using generative modeling and fusion techniques. To this end, we propose to synthesize auxiliary data to provide additional channel information to complement the limited contrast in the IR images, as well as IR data synthesis for data augmentation. Here, the former helps the FSS model to better capture the relationship between the support and query sets, while the latter addresses the issue of data scarcity. Finally, to further improve the former aspect, we propose a novel fusion ensemble module for integrating the two different modalities. Our methods are evaluated on different IR datasets, and improve upon the state-of-the-art (SOTA) FSS models. </p>
<blockquote>
<p>çº¢å¤–ï¼ˆIRï¼‰æˆåƒåœ¨è‡ªåŠ¨é©¾é©¶ã€æ¶ˆé˜²å®‰å…¨åŠå›½é˜²åº”ç”¨ç­‰å¤šç§åœºæ™¯ä¸­éƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚å› æ­¤ï¼Œè¿™ç±»å›¾åƒçš„è¯­ä¹‰åˆ†å‰²å¼•èµ·äº†æå¤§çš„å…´è¶£ã€‚ç„¶è€Œï¼Œæ­¤ä»»åŠ¡é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ç¨€ç¼ºã€å¯¹æ¯”åº¦å·®å¼‚ä»¥åŠä¸è‡ªç„¶å›¾åƒç›¸æ¯”è¾“å…¥é€šé“æ•°é‡çš„ä¸åŒï¼Œä»¥åŠåœ¨æŸäº›ç‰¹å®šåœºæ™¯ï¼ˆå¦‚å›½é˜²åº”ç”¨ï¼‰ä¸­æ•°æ®åº“ä¸­æœªå‡ºç°çš„æ–°å…´ç±»åˆ«ç­‰é—®é¢˜ã€‚å°‘æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰æä¾›äº†ä¸€ç§é€šè¿‡åˆ©ç”¨å°‘é‡æ ‡è®°æ ·æœ¬å¯¹æŸ¥è¯¢å›¾åƒè¿›è¡Œåˆ†å‰²æ¥å…‹æœè¿™äº›é—®é¢˜çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çº¢å¤–å›¾åƒFSSæ¨¡å‹éœ€è¦é…å¯¹å¯è§RGBå›¾åƒï¼Œè¿™åœ¨æŸäº›åº”ç”¨ä¸­è·å–æ­¤ç±»é…å¯¹æ•°æ®æ˜¯å›°éš¾ç”šè‡³ä¸å¯èƒ½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸»è¦å±€é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘æ–°çš„ç­–ç•¥æ¥å¯¹çº¢å¤–å›¾åƒè¿›è¡ŒFSSï¼Œé‡‡ç”¨ç”Ÿæˆå»ºæ¨¡å’ŒèåˆæŠ€æœ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåˆæˆè¾…åŠ©æ•°æ®ä»¥æä¾›é¢å¤–çš„é€šé“ä¿¡æ¯æ¥è¡¥å……çº¢å¤–å›¾åƒä¸­æœ‰é™çš„å¯¹æ¯”åº¦ï¼Œä»¥åŠçº¢å¤–æ•°æ®åˆæˆç”¨äºæ•°æ®å¢å¼ºã€‚å…¶ä¸­ï¼Œå‰è€…æœ‰åŠ©äºFSSæ¨¡å‹æ›´å¥½åœ°æ•è·æ”¯æŒä¸æŸ¥è¯¢é›†ä¹‹é—´çš„å…³ç³»ï¼Œè€Œåè€…è§£å†³äº†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æœ€åï¼Œä¸ºäº†è¿›ä¸€æ­¥æå‡å‰è€…æ–¹é¢çš„æ•ˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„èåˆé›†æˆæ¨¡å—æ¥æ•´åˆä¸¤ç§ä¸åŒçš„æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªçº¢å¤–æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ”¹è¿›äº†å½“å‰æœ€å…ˆè¿›çš„FSSæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05341v1">PDF</a> Winter Conference on Applications of Computer Vision (WACV), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»çº¢å¤–å›¾åƒè¯­ä¹‰åˆ†å‰²çš„é‡è¦æ€§å’Œé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºåŸºäºç”Ÿæˆæ¨¡å‹å’ŒèåˆæŠ€æœ¯çš„ç­–ç•¥è§£å†³å°‘æ•°æ ·æœ¬çº¢å¤–å›¾åƒåˆ†å‰²é—®é¢˜ã€‚é€šè¿‡åˆæˆè¾…åŠ©æ•°æ®æä¾›é¢å¤–çš„é€šé“ä¿¡æ¯ï¼Œä»¥è§£å†³çº¢å¤–å›¾åƒå¯¹æ¯”åº¦æœ‰é™çš„é—®é¢˜ï¼Œå¹¶é‡‡ç”¨çº¢å¤–æ•°æ®åˆæˆè¿›è¡Œæ•°æ®å¢å¼ºã€‚åŒæ—¶ï¼Œå¼•å…¥èåˆé›†æˆæ¨¡å—æ•´åˆä¸¤ç§ä¸åŒæ¨¡æ€çš„æ•°æ®ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨å¤šä¸ªçº¢å¤–æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å°‘æ•°æ ·æœ¬åˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¢å¤–å›¾åƒè¯­ä¹‰åˆ†å‰²åœ¨è‡ªåŠ¨é©¾é©¶ã€æ¶ˆé˜²å®‰å…¨ã€å›½é˜²åº”ç”¨ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ä»·å€¼ã€‚</li>
<li>çº¢å¤–å›¾åƒè¯­ä¹‰åˆ†å‰²é¢ä¸´æ•°æ®ç¨€ç¼ºã€å¯¹æ¯”åº¦å·®å¼‚ã€è¾“å…¥é€šé“æ•°é‡æœ‰é™ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å°‘æ•°æ ·æœ¬åˆ†å‰²ï¼ˆFSSï¼‰ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æ¡†æ¶ï¼Œä½†ç°æœ‰æ¨¡å‹éœ€è¦é…å¯¹å¯è§RGBå›¾åƒï¼Œè¿™åœ¨æŸäº›åº”ç”¨ä¸­éš¾ä»¥å®ç°ã€‚</li>
<li>æœ¬æ–‡æå‡ºé€šè¿‡ç”Ÿæˆæ¨¡å‹å’ŒèåˆæŠ€æœ¯è§£å†³çº¢å¤–å›¾åƒçš„FSSé—®é¢˜ã€‚</li>
<li>åˆæˆè¾…åŠ©æ•°æ®æä¾›é¢å¤–çš„é€šé“ä¿¡æ¯ï¼Œä»¥è§£å†³çº¢å¤–å›¾åƒå¯¹æ¯”åº¦æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥çº¢å¤–æ•°æ®åˆæˆè¿›è¡Œæ•°æ®å¢å¼ºï¼Œè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-543124edf9146b29d564f5fefdb4255e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa8978e9aec1c61a797d502a7ea7938c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c51e2cdeb0d62a60af66d306140f328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41154820d49466739dbbe5eaf1be10c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55b502614a5f4a44a6083d3950208a22.jpg" align="middle">
</details>




<h2 id="From-classical-techniques-to-convolution-based-models-A-review-of-object-detection-algorithms"><a href="#From-classical-techniques-to-convolution-based-models-A-review-of-object-detection-algorithms" class="headerlink" title="From classical techniques to convolution-based models: A review of   object detection algorithms"></a>From classical techniques to convolution-based models: A review of   object detection algorithms</h2><p><strong>Authors:Fnu Neha, Deepshikha Bhati, Deepak Kumar Shukla, Md Amiruzzaman</strong></p>
<p>Object detection is a fundamental task in computer vision and image understanding, with the goal of identifying and localizing objects of interest within an image while assigning them corresponding class labels. Traditional methods, which relied on handcrafted features and shallow models, struggled with complex visual data and showed limited performance. These methods combined low-level features with contextual information and lacked the ability to capture high-level semantics. Deep learning, especially Convolutional Neural Networks (CNNs), addressed these limitations by automatically learning rich, hierarchical features directly from data. These features include both semantic and high-level representations essential for accurate object detection. This paper reviews object detection frameworks, starting with classical computer vision methods. We categorize object detection approaches into two groups: (1) classical computer vision techniques and (2) CNN-based detectors. We compare major CNN models, discussing their strengths and limitations. In conclusion, this review highlights the significant advancements in object detection through deep learning and identifies key areas for further research to improve performance. </p>
<blockquote>
<p>ç›®æ ‡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾åƒç†è§£ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨å›¾åƒä¸­è¯†åˆ«å’Œå®šä½æ„Ÿå…´è¶£çš„ç›®æ ‡ï¼ŒåŒæ—¶ä¸ºå®ƒä»¬åˆ†é…ç›¸åº”çš„ç±»åˆ«æ ‡ç­¾ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºæ‰‹å·¥ç‰¹å¾å’Œæµ…å±‚æ¨¡å‹ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„è§†è§‰æ•°æ®ï¼Œè¡¨ç°æœ‰é™ã€‚è¿™äº›æ–¹æ³•å°†ä½çº§ç‰¹å¾ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯ç›¸ç»“åˆï¼Œç¼ºä¹æ•è·é«˜çº§è¯­ä¹‰çš„èƒ½åŠ›ã€‚æ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œé€šè¿‡ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œã€åˆ†å±‚çš„ç‰¹ç‚¹ï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ã€‚è¿™äº›ç‰¹å¾åŒ…æ‹¬è¯­ä¹‰å’Œé«˜çº§è¡¨ç¤ºï¼Œå¯¹äºå‡†ç¡®çš„ç›®æ ‡æ£€æµ‹è‡³å…³é‡è¦ã€‚æœ¬æ–‡å›é¡¾äº†ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼Œä»ç»å…¸è®¡ç®—æœºè§†è§‰æ–¹æ³•å¼€å§‹ã€‚æˆ‘ä»¬å°†ç›®æ ‡æ£€æµ‹æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šï¼ˆ1ï¼‰ç»å…¸è®¡ç®—æœºè§†è§‰æŠ€æœ¯å’Œï¼ˆ2ï¼‰åŸºäºCNNçš„æ£€æµ‹å™¨ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸»è¦çš„CNNæ¨¡å‹ï¼Œè®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚æ€»ä¹‹ï¼Œè¿™ç¯‡ç»¼è¿°å¼ºè°ƒäº†æ·±åº¦å­¦ä¹ åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½çš„å…³é”®ç ”ç©¶é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05252v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç›®æ ‡æ£€æµ‹çš„åŸºæœ¬æ¦‚å¿µåŠå…¶åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾åƒç†è§£é¢†åŸŸçš„é‡è¦æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•é¢ä¸´å¤æ‚è§†è§‰æ•°æ®çš„æŒ‘æˆ˜ï¼Œè¡¨ç°æœ‰é™ï¼Œè€Œæ·±åº¦å­¦ä¹ ï¼Œå°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è§£å†³äº†è¿™äº›é—®é¢˜ã€‚æœ¬æ–‡å›é¡¾äº†ç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼ŒåŒ…æ‹¬ç»å…¸è®¡ç®—æœºè§†è§‰æ–¹æ³•å’ŒCNNæ£€æµ‹å™¨ï¼Œå¹¶å¯¹ä¸»è¦CNNæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¼ºè°ƒäº†æ·±åº¦å­¦ä¹ åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢çš„æ˜¾è‘—è¿›å±•ï¼Œå¹¶æŒ‡å‡ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½çš„å…³é”®ç ”ç©¶é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›®æ ‡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰å’Œå›¾åƒç†è§£é¢†åŸŸçš„åŸºæœ¬ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«å’Œå®šä½å›¾åƒä¸­çš„æ„Ÿå…´è¶£å¯¹è±¡ï¼Œå¹¶åˆ†é…ç›¸åº”çš„ç±»åˆ«æ ‡ç­¾ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ‰‹å·¥ç‰¹å¾å’Œæµ…å±‚æ¨¡å‹ï¼Œéš¾ä»¥å¤„ç†å¤æ‚è§†è§‰æ•°æ®ï¼Œæ€§èƒ½æœ‰é™ã€‚</li>
<li>æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œé€šè¿‡è‡ªåŠ¨å­¦ä¹ ä¸°å¯Œçš„å±‚æ¬¡ç‰¹å¾æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™äº›ç‰¹å¾å¯¹äºå‡†ç¡®çš„ç›®æ ‡æ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>ç›®æ ‡æ£€æµ‹æ–¹æ³•å¯åˆ†ä¸ºä¸¤ç±»ï¼šç»å…¸è®¡ç®—æœºè§†è§‰æŠ€æœ¯å’ŒCNNæ£€æµ‹å™¨ã€‚</li>
<li>æœ¬æ–‡æ¯”è¾ƒäº†ä¸»è¦çš„CNNæ¨¡å‹ï¼Œè®¨è®ºäº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9eb79efd1a65324719d760fb810e9478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc86639ff08755178338c20759012601.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-281639346f606d8b83ab3f7fb5f68464.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a14b30d2bda003a153bcc1a8e7874ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a55b94b48d8ed484e15e940ca5293d60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a6a2b27f87f517ae35e43a0f0a0e802.jpg" align="middle">
</details>




<h2 id="DEYOLO-Dual-Feature-Enhancement-YOLO-for-Cross-Modality-Object-Detection"><a href="#DEYOLO-Dual-Feature-Enhancement-YOLO-for-Cross-Modality-Object-Detection" class="headerlink" title="DEYOLO: Dual-Feature-Enhancement YOLO for Cross-Modality Object   Detection"></a>DEYOLO: Dual-Feature-Enhancement YOLO for Cross-Modality Object   Detection</h2><p><strong>Authors:Yishuo Chen, Boran Wang, Xinyu Guo, Wenbin Zhu, Jiasheng He, Xiaobin Liu, Jing Yuan</strong></p>
<p>Object detection in poor-illumination environments is a challenging task as objects are usually not clearly visible in RGB images. As infrared images provide additional clear edge information that complements RGB images, fusing RGB and infrared images has potential to enhance the detection ability in poor-illumination environments. However, existing works involving both visible and infrared images only focus on image fusion, instead of object detection. Moreover, they directly fuse the two kinds of image modalities, which ignores the mutual interference between them. To fuse the two modalities to maximize the advantages of cross-modality, we design a dual-enhancement-based cross-modality object detection network DEYOLO, in which semantic-spatial cross modality and novel bi-directional decoupled focus modules are designed to achieve the detection-centered mutual enhancement of RGB-infrared (RGB-IR). Specifically, a dual semantic enhancing channel weight assignment module (DECA) and a dual spatial enhancing pixel weight assignment module (DEPA) are firstly proposed to aggregate cross-modality information in the feature space to improve the feature representation ability, such that feature fusion can aim at the object detection task. Meanwhile, a dual-enhancement mechanism, including enhancements for two-modality fusion and single modality, is designed in both DECAand DEPAto reduce interference between the two kinds of image modalities. Then, a novel bi-directional decoupled focus is developed to enlarge the receptive field of the backbone network in different directions, which improves the representation quality of DEYOLO. Extensive experiments on M3FD and LLVIP show that our approach outperforms SOTA object detection algorithms by a clear margin. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/chips96/DEYOLO">https://github.com/chips96/DEYOLO</a>. </p>
<blockquote>
<p>åœ¨ä½å…‰ç…§ç¯å¢ƒä¸‹è¿›è¡Œç›®æ ‡æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºåœ¨RGBå›¾åƒä¸­ç›®æ ‡é€šå¸¸ä¸æ¸…æ™°ã€‚ç”±äºçº¢å¤–å›¾åƒæä¾›äº†é¢å¤–çš„æ¸…æ™°è¾¹ç¼˜ä¿¡æ¯ï¼Œå¯ä»¥ä¸RGBå›¾åƒäº’è¡¥ï¼Œå› æ­¤èåˆRGBå’Œçº¢å¤–å›¾åƒæœ‰å¯èƒ½æé«˜åœ¨ä½å…‰ç…§ç¯å¢ƒä¸­çš„æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¶‰åŠå¯è§å…‰å’Œçº¢å¤–å›¾åƒçš„å·¥ä½œåªå…³æ³¨å›¾åƒèåˆï¼Œè€Œä¸æ˜¯ç›®æ ‡æ£€æµ‹ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç›´æ¥èåˆè¿™ä¸¤ç§å›¾åƒæ¨¡å¼ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’å¹²æ‰°ã€‚ä¸ºäº†èåˆè¿™ä¸¤ç§æ¨¡å¼ï¼Œä»¥æœ€å¤§åŒ–è·¨æ¨¡å¼çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºåŒå¢å¼ºçš„è·¨æ¨¡å¼ç›®æ ‡æ£€æµ‹ç½‘ç»œDEYOLOï¼Œå…¶ä¸­è®¾è®¡äº†è¯­ä¹‰ç©ºé—´è·¨æ¨¡å¼å’Œæ–°å‹åŒå‘è§£è€¦ç„¦ç‚¹æ¨¡å—ï¼Œä»¥å®ç°ä»¥æ£€æµ‹ä¸ºä¸­å¿ƒRGB-çº¢å¤–ï¼ˆRGB-IRï¼‰çš„ç›¸äº’å¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆæå‡ºäº†åŒè¯­ä¹‰å¢å¼ºé€šé“æƒé‡åˆ†é…æ¨¡å—ï¼ˆDECAï¼‰å’ŒåŒç©ºé—´å¢å¼ºåƒç´ æƒé‡åˆ†é…æ¨¡å—ï¼ˆDEPAï¼‰ï¼Œä»¥åœ¨ç‰¹å¾ç©ºé—´ä¸­èšåˆè·¨æ¨¡å¼ä¿¡æ¯ï¼Œæé«˜ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ï¼Œä½¿ç‰¹å¾èåˆèƒ½å¤Ÿé’ˆå¯¹ç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚åŒæ—¶ï¼Œåœ¨DECAå’ŒDEPAä¸­éƒ½è®¾è®¡äº†åŒ…æ‹¬ä¸¤ç§æ¨¡å¼èåˆå’Œå•ä¸€æ¨¡å¼çš„å¢å¼ºåœ¨å†…çš„åŒé‡å¢å¼ºæœºåˆ¶ï¼Œä»¥å‡å°‘ä¸¤ç§å›¾åƒæ¨¡å¼ä¹‹é—´çš„å¹²æ‰°ã€‚ç„¶åï¼Œå¼€å‘äº†ä¸€ç§æ–°çš„åŒå‘è§£è€¦ç„¦ç‚¹ï¼Œä»¥ä¸åŒæ–¹å‘æ‰©å¤§ä¸»å¹²ç½‘ç»œçš„æ„Ÿå—é‡ï¼Œæé«˜äº†DEYOLOçš„è¡¨ç¤ºè´¨é‡ã€‚åœ¨M3FDå’ŒLLVIPä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºæœ€æ–°ç›®æ ‡æ£€æµ‹ç®—æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/chips96/DEYOLO">https://github.com/chips96/DEYOLO</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04931v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åœ¨å…‰ç…§ä¸è¶³ç¯å¢ƒä¸‹ï¼Œå¯¹è±¡æ£€æµ‹æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚çº¢å¤–å›¾åƒå¯ä»¥æä¾›æ¸…æ™°çš„è¾¹ç¼˜ä¿¡æ¯ï¼Œä¸RGBå›¾åƒç›¸ç»“åˆå¯æé«˜æ£€æµ‹èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œä»…å…³æ³¨å›¾åƒèåˆè€Œéå¯¹è±¡æ£€æµ‹ï¼Œä¸”ç›´æ¥èåˆä¸¤ç§å›¾åƒæ¨¡å¼ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚ä¸ºäº†æœ€å¤§åŒ–è·¨æ¨¡æ€çš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºåŒå¢å¼ºçš„è·¨æ¨¡æ€å¯¹è±¡æ£€æµ‹ç½‘ç»œDEYOLOï¼Œå…¶ä¸­åŒ…å«è¯­ä¹‰ç©ºé—´è·¨æ¨¡æ€å’Œæ–°å‹åŒå‘è§£è€¦ç„¦ç‚¹æ¨¡å—ï¼Œä»¥å®ç°RGB-çº¢å¤–ï¼ˆRGB-IRï¼‰çš„æ£€æµ‹ä¸­å¿ƒç›¸äº’å¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯¹è±¡æ£€æµ‹åœ¨å…‰ç…§ä¸è¶³ç¯å¢ƒä¸‹æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>çº¢å¤–å›¾åƒæä¾›äº†æ¸…æ™°çš„è¾¹ç¼˜ä¿¡æ¯ï¼Œä¸RGBå›¾åƒèåˆèƒ½æé«˜æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å›¾åƒèåˆè€Œéå¯¹è±¡æ£€æµ‹ã€‚</li>
<li>ç›´æ¥èåˆä¸¤ç§å›¾åƒæ¨¡å¼å¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ç›¸äº’å½±å“ã€‚</li>
<li>DEYOLOç½‘ç»œé€šè¿‡è®¾è®¡åŒå¢å¼ºæœºåˆ¶å®ç°è·¨æ¨¡æ€èåˆï¼Œæé«˜ç‰¹å¾è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>DEYOLOåŒ…å«è¯­ä¹‰ç©ºé—´è·¨æ¨¡æ€å’Œæ–°å‹åŒå‘è§£è€¦ç„¦ç‚¹æ¨¡å—ï¼Œæ—¨åœ¨æé«˜RGB-IRçš„æ£€æµ‹ä¸­å¿ƒç›¸äº’å¢å¼ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd30c0f03db4716fd63e220ee9175ba2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9093d1df529c2b93c08f48e9b154b2cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25e16d2fa1c36b0229b7e886fb62a94c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522361ef8e518a44d67f77826010277d.jpg" align="middle">
</details>




<h2 id="MANTA-A-Large-Scale-Multi-View-and-Visual-Text-Anomaly-Detection-Dataset-for-Tiny-Objects"><a href="#MANTA-A-Large-Scale-Multi-View-and-Visual-Text-Anomaly-Detection-Dataset-for-Tiny-Objects" class="headerlink" title="MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection   Dataset for Tiny Objects"></a>MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection   Dataset for Tiny Objects</h2><p><strong>Authors:Lei Fan, Dongdong Fan, Zhiguang Hu, Yiwen Ding, Donglin Di, Kai Yi, Maurice Pagnucco, Yang Song</strong></p>
<p>We present MANTA, a visual-text anomaly detection dataset for tiny objects. The visual component comprises over 137.3K images across 38 object categories spanning five typical domains, of which 8.6K images are labeled as anomalous with pixel-level annotations. Each image is captured from five distinct viewpoints to ensure comprehensive object coverage. The text component consists of two subsets: Declarative Knowledge, including 875 words that describe common anomalies across various domains and specific categories, with detailed explanations for &lt; what, why, how&gt;, including causes and visual characteristics; and Constructivist Learning, providing 2K multiple-choice questions with varying levels of difficulty, each paired with images and corresponded answer explanations. We also propose a baseline for visual-text tasks and conduct extensive benchmarking experiments to evaluate advanced methods across different settings, highlighting the challenges and efficacy of our dataset. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†MANTAæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¾®å°ç‰©ä½“è§†è§‰æ–‡æœ¬å¼‚å¸¸æ£€æµ‹çš„æ•°æ®é›†ã€‚è§†è§‰éƒ¨åˆ†åŒ…å«è¶…è¿‡38ä¸ªå¯¹è±¡ç±»åˆ«çš„è¶…è¿‡13ä¸‡å¼ å›¾åƒï¼Œè·¨è¶Šäº”ä¸ªå…¸å‹é¢†åŸŸï¼Œå…¶ä¸­æ ‡æ³¨ä¸ºå¼‚å¸¸çš„å›¾åƒæœ‰8åƒå¼ ï¼Œå¹¶å¸¦æœ‰åƒç´ çº§åˆ«çš„æ³¨é‡Šã€‚æ¯ä¸ªå›¾åƒéƒ½æ˜¯ä»äº”ä¸ªä¸åŒçš„è§†è§’æ•è·çš„ï¼Œä»¥ç¡®ä¿å¯¹ç‰©ä½“çš„å…¨é¢è¦†ç›–ã€‚æ–‡æœ¬éƒ¨åˆ†åŒ…å«ä¸¤ä¸ªå­é›†ï¼šæè¿°æ€§çŸ¥è¯†é›†åŒ…æ‹¬æè¿°è·¨ä¸åŒé¢†åŸŸå’Œç‰¹å®šç±»åˆ«çš„å¸¸è§å¼‚å¸¸çš„875ä¸ªå•è¯ï¼ŒåŒ…æ‹¬â€œæ˜¯ä»€ä¹ˆâ€ã€â€œä¸ºä»€ä¹ˆâ€ã€â€œæ€ä¹ˆåŠâ€çš„è¯¦ç»†è§£é‡Šï¼ŒåŒ…æ‹¬åŸå› å’Œè§†è§‰ç‰¹å¾ï¼›ä»¥åŠæ„å»ºä¸»ä¹‰å­¦ä¹ é›†ï¼Œæä¾›éš¾åº¦å„å¼‚çš„2åƒé“é€‰æ‹©é¢˜ï¼Œæ¯ä¸ªé¢˜ç›®éƒ½é…æœ‰å›¾åƒå’Œç›¸åº”çš„ç­”æ¡ˆè§£é‡Šã€‚æˆ‘ä»¬è¿˜ä¸ºè§†è§‰æ–‡æœ¬ä»»åŠ¡æä¾›äº†åŸºå‡†çº¿ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•å®éªŒæ¥è¯„ä¼°ä¸åŒè®¾ç½®ä¸‹çš„é«˜çº§æ–¹æ³•ï¼Œçªå‡ºæˆ‘ä»¬æ•°æ®é›†é¢ä¸´çš„æŒ‘æˆ˜å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04867v1">PDF</a> <a target="_blank" rel="noopener" href="https://grainnet.github.io/MANTA">https://grainnet.github.io/MANTA</a></p>
<p><strong>Summary</strong></p>
<p>MANTAæ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºå¾®å°ç‰©ä½“è§†è§‰æ–‡æœ¬å¼‚å¸¸æ£€æµ‹çš„æ•°æ®é›†ã€‚å®ƒåŒ…å«è§†è§‰å’Œæ–‡æœ¬ä¸¤éƒ¨åˆ†ï¼Œè§†è§‰éƒ¨åˆ†æ¶µç›–38ä¸ªå¯¹è±¡ç±»åˆ«ï¼Œå…±æœ‰è¶…è¿‡137.3Kå¼ å›¾åƒï¼Œå…¶ä¸­8.6Kå¼ å›¾åƒè¢«æ ‡æ³¨ä¸ºå¼‚å¸¸ï¼Œå¹¶æä¾›åƒç´ çº§æ³¨é‡Šã€‚æ–‡æœ¬éƒ¨åˆ†åŒ…æ‹¬æè¿°è·¨åŸŸå’Œç‰¹å®šç±»åˆ«çš„é€šç”¨å¼‚å¸¸çš„Declarative Knowledgeï¼Œä»¥åŠæä¾›å¸¦å›¾åƒå’Œç­”æ¡ˆè§£é‡Šçš„2Ké“é€‰æ‹©é¢˜ä»¥è¾…åŠ©å­¦ä¹ çš„Constructivist Learningã€‚åŒæ—¶ï¼Œæå‡ºåŸºçº¿æ–¹æ³•å¹¶è¿›è¡ŒåŸºå‡†æµ‹è¯•å®éªŒè¯„ä¼°ä¸åŒç¯å¢ƒä¸‹çš„å…ˆè¿›æ–¹æ³•ï¼Œçªå‡ºæ•°æ®é›†çš„æŒ‘æˆ˜æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MANTAæ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºå¾®å°ç‰©ä½“è§†è§‰æ–‡æœ¬å¼‚å¸¸æ£€æµ‹çš„å¤§å‹æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è§†è§‰å’Œæ–‡æœ¬ä¸¤éƒ¨åˆ†ï¼Œæ¶µç›–å¹¿æ³›çš„ç‰©ä½“ç±»åˆ«å’Œé¢†åŸŸã€‚</li>
<li>æ•°æ®é›†ä¸­æœ‰è¯¦ç»†çš„åƒç´ çº§å¼‚å¸¸æ ‡æ³¨ã€‚</li>
<li>æ–‡æœ¬éƒ¨åˆ†åŒ…å«æè¿°å¼‚å¸¸çš„Declarative Knowledgeå’Œç”¨äºè¾…åŠ©å­¦ä¹ çš„Constructivist Learningã€‚</li>
<li>æ•°æ®é›†æä¾›å¤šç§è§†è§’çš„å›¾åƒæ•æ‰ï¼Œç¡®ä¿å…¨é¢çš„ç‰©ä½“è¦†ç›–ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†åŸºçº¿æ–¹æ³•å’ŒåŸºå‡†æµ‹è¯•å®éªŒï¼Œä»¥è¯„ä¼°ä¸åŒç¯å¢ƒä¸‹çš„æ–¹æ³•æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b775e4eee860ef20658a4c468ab56a13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-622cf2bf518a85293b79aadf5d2eabd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de6d411848dcc95e670de8e0996eea54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-250b1e64690be08b69a9e1e6170f4601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96097118568932e7aa7355439ffe2eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62c18dea3ae48bb6457e82e41c70b0d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b25c020c7f5e36dd100fb716307b5e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b501dd70c78119bb3d885e41d188f49d.jpg" align="middle">
</details>




<h2 id="Reflective-Teacher-Semi-Supervised-Multimodal-3D-Object-Detection-in-Birdâ€™s-Eye-View-via-Uncertainty-Measure"><a href="#Reflective-Teacher-Semi-Supervised-Multimodal-3D-Object-Detection-in-Birdâ€™s-Eye-View-via-Uncertainty-Measure" class="headerlink" title="Reflective Teacher: Semi-Supervised Multimodal 3D Object Detection in   Birdâ€™s-Eye-View via Uncertainty Measure"></a>Reflective Teacher: Semi-Supervised Multimodal 3D Object Detection in   Birdâ€™s-Eye-View via Uncertainty Measure</h2><p><strong>Authors:Saheli Hazra, Sudip Das, Rohit Choudhary, Arindam Das, Ganesh Sistu, Ciaran Eising, Ujjwal Bhattacharya</strong></p>
<p>Applying pseudo labeling techniques has been found to be advantageous in semi-supervised 3D object detection (SSOD) in Birdâ€™s-Eye-View (BEV) for autonomous driving, particularly where labeled data is limited. In the literature, Exponential Moving Average (EMA) has been used for adjustments of the weights of teacher network by the student network. However, the same induces catastrophic forgetting in the teacher network. In this work, we address this issue by introducing a novel concept of Reflective Teacher where the student is trained by both labeled and pseudo labeled data while its knowledge is progressively passed to the teacher through a regularizer to ensure retention of previous knowledge. Additionally, we propose Geometry Aware BEV Fusion (GA-BEVFusion) for efficient alignment of multi-modal BEV features, thus reducing the disparity between the modalities - camera and LiDAR. This helps to map the precise geometric information embedded among LiDAR points reliably with the spatial priors for extraction of semantic information from camera images. Our experiments on the nuScenes and Waymo datasets demonstrate: 1) improved performance over state-of-the-art methods in both fully supervised and semi-supervised settings; 2) Reflective Teacher achieves equivalent performance with only 25% and 22% of labeled data for nuScenes and Waymo datasets respectively, in contrast to other fully supervised methods that utilize the full labeled dataset. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰åŠç›‘ç£ä¸‰ç»´ç›®æ ‡æ£€æµ‹ï¼ˆSSODï¼‰ä¸­ï¼Œåº”ç”¨ä¼ªæ ‡ç­¾æŠ€æœ¯å·²è¢«è¯æ˜æ˜¯æœ‰åˆ©çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚æ–‡çŒ®ä¸­ï¼ŒæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰å·²è¢«ç”¨äºå­¦ç”Ÿç½‘ç»œè°ƒæ•´æ•™å¸ˆç½‘ç»œçš„æƒé‡ã€‚ç„¶è€Œï¼Œè¿™ä¼šå¯¼è‡´æ•™å¸ˆç½‘ç»œä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥åå°„æ•™å¸ˆï¼ˆReflective Teacherï¼‰çš„æ–°æ¦‚å¿µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå­¦ç”Ÿåœ¨æœ‰æ ‡ç­¾å’Œä¼ªæ ‡ç­¾æ•°æ®çš„è®­ç»ƒä¸‹ï¼Œé€šè¿‡æ­£åˆ™åŒ–å™¨é€æ¸å°†çŸ¥è¯†ä¼ é€’ç»™æ•™å¸ˆï¼Œä»¥ç¡®ä¿ä¿ç•™ä»¥å‰çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ä½•æ„ŸçŸ¥BEVèåˆï¼ˆGA-BEVFusionï¼‰ï¼Œç”¨äºæœ‰æ•ˆåœ°å¯¹é½å¤šæ¨¡å¼BEVç‰¹å¾ï¼Œä»è€Œå‡å°‘ç›¸æœºå’Œæ¿€å…‰é›·è¾¾ä¹‹é—´çš„æ¨¡å¼å·®å¼‚ã€‚è¿™æœ‰åŠ©äºå¯é åœ°å°†åµŒå…¥æ¿€å…‰é›·è¾¾ç‚¹ä¸­çš„ç²¾ç¡®å‡ ä½•ä¿¡æ¯ä¸ç©ºé—´å…ˆéªŒä¿¡æ¯ç›¸ç»“åˆï¼Œä»ç›¸æœºå›¾åƒä¸­æå–è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨nuSceneså’ŒWaymoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼š1ï¼‰åœ¨å…¨ç›‘ç£å’ŒåŠç›‘ç£è®¾ç½®ä¸‹ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ€§èƒ½æœ‰æ‰€æå‡ï¼›2ï¼‰åå°„æ•™å¸ˆä»…ä½¿ç”¨nuSceneså’ŒWaymoæ•°æ®é›†çš„25%å’Œ22%çš„æ ‡ç­¾æ•°æ®å³å¯å®ç°ç›¸å½“çš„æ€§èƒ½ï¼Œè€Œå…¶ä»–å…¨ç›‘ç£æ–¹æ³•åˆ™éœ€è¦ä½¿ç”¨å®Œæ•´çš„æ ‡ç­¾æ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04337v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¼ªæ ‡ç­¾æŠ€æœ¯åœ¨åŠç›‘ç£ä¸‰ç»´ç‰©ä½“æ£€æµ‹é¢†åŸŸä¸­çš„é¸Ÿç³å›¾è§†è§’ï¼ˆBEVï¼‰å¯¹è‡ªåŠ¨é©¾é©¶æŠ€æœ¯å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡çŒ®ä¸­çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰ç”¨äºè°ƒæ•´æ•™å¸ˆç½‘ç»œçš„æƒé‡ï¼Œè¿™ä¼šå¯¼è‡´æ•™å¸ˆç½‘ç»œé—å¿˜åŸæœ‰çŸ¥è¯†çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥â€œåå°„æ•™å¸ˆâ€æ¦‚å¿µæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå­¦ç”Ÿç½‘ç»œé€šè¿‡æ ‡ç­¾å’Œä¼ªæ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶é€šè¿‡æ­£åˆ™åŒ–å™¨é€æ¸è·å–çŸ¥è¯†æ¥ç¡®ä¿åŸæœ‰çŸ¥è¯†çš„ä¿ç•™ã€‚æ­¤å¤–ï¼Œæå‡ºäº†å‡ ä½•æ„ŸçŸ¥çš„BEVèåˆï¼ˆGA-BEVFusionï¼‰ï¼Œå®ç°å¤šæ¨¡æ€BEVç‰¹å¾çš„æœ‰æ•ˆå¯¹é½ï¼Œä»è€Œå‡å°‘æ‘„åƒå¤´å’Œæ¿€å…‰é›·è¾¾ä¹‹é—´çš„æ¨¡å¼å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨nuSceneså’ŒWaymoæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨æ ‡ç­¾æ•°æ®å‡å°‘æ—¶ï¼Œåå°„æ•™å¸ˆçš„æ€§èƒ½æ›´æ˜¯ä¼˜äºå…¨ç›‘ç£æ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥æŠ€æœ¯å°†ä¸ºè‡ªåŠ¨åŒ–é©¾é©¶çš„åŠç›‘ç£ç‰©ä½“æ£€æµ‹æä¾›æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ªæ ‡ç­¾æŠ€æœ¯åœ¨åŠç›‘ç£ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä¸­å¯¹è‡ªåŠ¨é©¾é©¶æœ‰ç›Šï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡ç­¾æ•°æ®çš„æƒ…å†µä¸‹ã€‚</li>
<li>æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰åœ¨è°ƒæ•´æ•™å¸ˆç½‘ç»œæƒé‡æ—¶å¯èƒ½å¯¼è‡´åŸæœ‰çŸ¥è¯†çš„é—å¿˜ã€‚</li>
<li>æå‡ºâ€œåå°„æ•™å¸ˆâ€æ¦‚å¿µï¼Œé€šè¿‡å­¦ç”Ÿç½‘ç»œç»“åˆæ ‡ç­¾å’Œä¼ªæ ‡ç­¾æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶é€šè¿‡æ­£åˆ™åŒ–å™¨ä¼ é€’çŸ¥è¯†ç»™æ•™å¸ˆç½‘ç»œä»¥ç¡®ä¿åŸæœ‰çŸ¥è¯†çš„ä¿ç•™ã€‚</li>
<li>æå‡ºå‡ ä½•æ„ŸçŸ¥çš„BEVèåˆï¼ˆGA-BEVFusionï¼‰ï¼Œå®ç°å¤šæ¨¡æ€BEVç‰¹å¾çš„æœ‰æ•ˆå¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨nuSceneså’ŒWaymoæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨å‡å°‘æ ‡ç­¾æ•°æ®æ—¶ï¼Œåå°„æ•™å¸ˆçš„æ€§èƒ½ä¸å…¨ç›‘ç£æ–¹æ³•ç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09aa96d08bde98720acb96138c80a876.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5d8b45c0bf06078858ac8fc0a20d320.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f0f75b6e64a55fdb5753b87f5a3fd1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e113830d0779ce35f129b786cf55735f.jpg" align="middle">
</details>




<h2 id="Frequency-Adaptive-Low-Latency-Object-Detection-Using-Events-and-Frames"><a href="#Frequency-Adaptive-Low-Latency-Object-Detection-Using-Events-and-Frames" class="headerlink" title="Frequency-Adaptive Low-Latency Object Detection Using Events and Frames"></a>Frequency-Adaptive Low-Latency Object Detection Using Events and Frames</h2><p><strong>Authors:Haitian Zhang, Xiangyuan Wang, Chang Xu, Xinya Wang, Fang Xu, Huai Yu, Lei Yu, Wen Yang</strong></p>
<p>Fusing Events and RGB images for object detection leverages the robustness of Event cameras in adverse environments and the rich semantic information provided by RGB cameras. However, two critical mismatches: low-latency Events \textit{vs.}<del>high-latency RGB frames; temporally sparse labels in training \textit{vs.}</del>continuous flow in inference, significantly hinder the high-frequency fusion-based object detection. To address these challenges, we propose the \textbf{F}requency-\textbf{A}daptive Low-Latency \textbf{O}bject \textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with high-frequency Events through an Align Module, which reinforces cross-modal style and spatial proximity to address the Event-RGB Mismatch. We further propose a training strategy, Time Shift, which enforces the module to align the prediction from temporally shifted Event-RGB pairs and their original representation, that is, consistent with Event-aligned annotations. This strategy enables the network to use high-frequency Event data as the primary reference while treating low-frequency RGB images as supplementary information, retaining the low-latency nature of the Event stream toward high-frequency detection. Furthermore, we observe that these corrected Event-RGB pairs demonstrate better generalization from low training frequency to higher inference frequencies compared to using Event data alone. Extensive experiments on the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD achieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD achieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB data with only a quarter of the parameters compared to SODFormer, and even maintains robust performance (only a 3 points drop in mAP) under 80$\times$ Event-RGB frequency mismatch. </p>
<blockquote>
<p>èåˆäº‹ä»¶å’ŒRGBå›¾åƒè¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œåˆ©ç”¨äº‹ä»¶ç›¸æœºåœ¨æ¶åŠ£ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’ŒRGBç›¸æœºæä¾›çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œä¸¤ä¸ªå…³é”®ä¸åŒ¹é…é—®é¢˜ï¼šä½å»¶è¿Ÿäº‹ä»¶ä¸é«˜å»¶è¿ŸRGBå¸§ä¹‹é—´çš„ä¸åŒ¹é…ï¼›è®­ç»ƒä¸­çš„æ—¶é—´ç¨€ç–æ ‡ç­¾ä¸æ¨ç†ä¸­çš„è¿ç»­æµä¹‹é—´çš„ä¸åŒ¹é…ï¼Œæ˜¾è‘—é˜»ç¢äº†åŸºäºé«˜é¢‘èåˆçš„ç›®æ ‡æ£€æµ‹ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é¢‘ç‡è‡ªé€‚åº”ä½å»¶è¿Ÿç›®æ ‡æ£€æµ‹å™¨ï¼ˆFAODï¼‰ã€‚FAODé€šè¿‡ä¸€ä¸ªå¯¹é½æ¨¡å—ï¼Œå°†ä½é¢‘çš„RGBå¸§ä¸é«˜é¢‘çš„äº‹ä»¶è¿›è¡Œå¯¹é½ï¼Œè¯¥æ¨¡å—åŠ å¼ºäº†è·¨æ¨¡æ€é£æ ¼å’Œç©ºé—´é‚»è¿‘æ€§ï¼Œä»¥è§£å†³äº‹ä»¶-RGBä¸åŒ¹é…é—®é¢˜ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œå³æ—¶é—´åç§»ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¼ºåˆ¶æ¨¡å—å¯¹é½æ—¶é—´åç§»çš„äº‹ä»¶-RGBå¯¹ä¸å…¶åŸå§‹è¡¨ç¤ºï¼ˆå³ä¸äº‹ä»¶å¯¹é½çš„æ³¨é‡Šä¸€è‡´ï¼‰ã€‚è¯¥ç­–ç•¥ä½¿ç½‘ç»œèƒ½å¤Ÿä½¿ç”¨é«˜é¢‘äº‹ä»¶æ•°æ®ä½œä¸ºä¸»è¦å‚è€ƒï¼Œè€Œå°†ä½é¢‘RGBå›¾åƒè§†ä¸ºè¾…åŠ©ä¿¡æ¯ï¼Œä¿æŒäº‹ä»¶æµçš„ä½å»¶è¿Ÿç‰¹æ€§ä»¥å®ç°é«˜é¢‘æ£€æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œè¿™äº›æ ¡æ­£åçš„äº‹ä»¶-RGBå¯¹åœ¨ä½è®­ç»ƒé¢‘ç‡åˆ°è¾ƒé«˜æ¨ç†é¢‘ç‡ä¹‹é—´è¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ä»…ä½¿ç”¨äº‹ä»¶æ•°æ®ç›¸æ¯”ã€‚åœ¨PKU-DAVIS-SODå’ŒDSEC-Detectionæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„FAODè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨PKU-DAVIS-SODæ•°æ®é›†ä¸Šï¼ŒFAODåœ¨å…¨é…å¯¹äº‹ä»¶-RGBæ•°æ®ä¸­çš„mAPæé«˜äº†9.8ç‚¹ï¼Œè€Œä¸”ä¸SODFormerç›¸æ¯”ï¼Œå‚æ•°åªæœ‰å››åˆ†ä¹‹ä¸€ï¼Œå³ä½¿åœ¨äº‹ä»¶-RGBé¢‘ç‡ä¸åŒ¹é…è¾¾åˆ°80å€çš„æƒ…å†µä¸‹ï¼ŒmAPä¹Ÿåªä¸‹é™äº†3ç‚¹ï¼Œä»ä¿æŒäº†ç¨³å¥çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†èåˆäº‹ä»¶å’ŒRGBå›¾åƒè¿›è¡Œç‰©ä½“æ£€æµ‹çš„æŠ€æœ¯ã€‚é’ˆå¯¹äº‹ä»¶ç›¸æœºåœ¨æ¶åŠ£ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’ŒRGBç›¸æœºæä¾›çš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯ï¼Œæå‡ºä¸€ç§é¢‘ç‡è‡ªé€‚åº”ä½å»¶è¿Ÿç‰©ä½“æ£€æµ‹å™¨ï¼ˆFAODï¼‰ã€‚FAODé€šè¿‡å¯¹ä½é¢‘RGBå¸§ä¸é«˜é¢‘äº‹ä»¶è¿›è¡Œå¯¹é½ï¼Œè§£å†³äº†äº‹ä»¶ä¸RGBå¸§ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚åŒæ—¶ï¼Œé‡‡ç”¨æ—¶é—´åç§»è®­ç»ƒç­–ç•¥ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿåˆ©ç”¨é«˜é¢‘äº‹ä»¶æ•°æ®ä½œä¸ºä¸»è¦å‚è€ƒï¼Œå°†ä½é¢‘ç‡çš„RGBå›¾åƒä½œä¸ºè¡¥å……ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œæ ¡æ­£åçš„äº‹ä»¶-RGBå¯¹åœ¨ä½è®­ç»ƒé¢‘ç‡åˆ°é«˜æ¨ç†é¢‘ç‡çš„æƒ…å†µä¸‹å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚åœ¨PKU-DAVIS-SODå’ŒDSEC-Detectionæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFAODè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨PKU-DAVIS-SODæ•°æ®é›†ä¸Šï¼Œä¸SODFormerç›¸æ¯”ï¼ŒFAODåœ¨å…¨é…å¯¹äº‹ä»¶RGBæ•°æ®ä¸Šçš„mAPæé«˜äº†9.8ç‚¹ï¼Œå³ä½¿åœ¨80å€çš„äº‹ä»¶RGBé¢‘ç‡ä¸åŒ¹é…çš„æƒ…å†µä¸‹ï¼ŒmAPä¹Ÿåªä¸‹é™äº†3ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶ç›¸æœºåœ¨æ¶åŠ£ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§å’ŒRGBç›¸æœºçš„ä¸°å¯Œè¯­ä¹‰ä¿¡æ¯èåˆï¼Œæœ‰åŠ©äºæé«˜ç‰©ä½“æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä½å»¶è¿Ÿäº‹ä»¶ä¸é«˜é¢‘RGBå¸§ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ï¼Œéœ€è¦é€šè¿‡FAODè§£å†³ã€‚</li>
<li>FAODé€šè¿‡Align Moduleå¯¹é½ä½é¢‘RGBå¸§ä¸é«˜é¢‘äº‹ä»¶ï¼Œå¼ºåŒ–è·¨æ¨¡æ€é£æ ¼å’Œç©ºé—´é‚»è¿‘æ€§ã€‚</li>
<li>æå‡ºæ—¶é—´åç§»è®­ç»ƒç­–ç•¥ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿåˆ©ç”¨é«˜é¢‘äº‹ä»¶æ•°æ®ä½œä¸ºä¸»è¦å‚è€ƒã€‚</li>
<li>æ ¡æ­£åçš„äº‹ä»¶-RGBå¯¹åœ¨ä½è®­ç»ƒé¢‘ç‡åˆ°é«˜æ¨ç†é¢‘ç‡çš„æƒ…å†µä¸‹å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>åœ¨PKU-DAVIS-SODå’ŒDSEC-Detectionæ•°æ®é›†ä¸Šï¼ŒFAODè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨PKU-DAVIS-SODæ•°æ®é›†ä¸Šï¼ŒFAODä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-940afe262f453e15e25f81dcd5247f8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1827b0adc6f3da47623388cb2ff6f7c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ac08384131851b4b958f39dc70023db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9db65c164fa5d24827a30bba71356449.jpg" align="middle">
</details>




<h2 id="Exact-Exploring-Space-Time-Perceptive-Clues-for-Weakly-Supervised-Satellite-Image-Time-Series-Semantic-Segmentation"><a href="#Exact-Exploring-Space-Time-Perceptive-Clues-for-Weakly-Supervised-Satellite-Image-Time-Series-Semantic-Segmentation" class="headerlink" title="Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised   Satellite Image Time Series Semantic Segmentation"></a>Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised   Satellite Image Time Series Semantic Segmentation</h2><p><strong>Authors:Hao Zhu, Yan Zhu, Jiayu Xiao, Tianxiang Xiao, Yike Ma, Yucheng Zhang, Feng Dai</strong></p>
<p>Automated crop mapping through Satellite Image Time Series (SITS) has emerged as a crucial avenue for agricultural monitoring and management. However, due to the low resolution and unclear parcel boundaries, annotating pixel-level masks is exceptionally complex and time-consuming in SITS. This paper embraces the weakly supervised paradigm (i.e., only image-level categories available) to liberate the crop mapping task from the exhaustive annotation burden. The unique characteristics of SITS give rise to several challenges in weakly supervised learning: (1) noise perturbation from spatially neighboring regions, and (2) erroneous semantic bias from anomalous temporal periods. To address the above difficulties, we propose a novel method, termed exploring space-time perceptive clues (Exact). First, we introduce a set of spatial clues to explicitly capture the representative patterns of different crops from the most class-relative regions. Besides, we leverage the temporal-to-class interaction of the model to emphasize the contributions of pivotal clips, thereby enhancing the model perception for crop regions. Build upon the space-time perceptive clues, we derive the clue-based CAMs to effectively supervise the SITS segmentation network. Our method demonstrates impressive performance on various SITS benchmarks. Remarkably, the segmentation network trained on Exact-generated masks achieves 95% of its fully supervised performance, showing the bright promise of weakly supervised paradigm in crop mapping scenario. Our code will be publicly available. </p>
<blockquote>
<p>é€šè¿‡å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—ï¼ˆSITSï¼‰è¿›è¡Œè‡ªåŠ¨ä½œç‰©åœ°å›¾ç»˜åˆ¶å·²æˆä¸ºå†œä¸šç›‘æµ‹å’Œç®¡ç†çš„é‡è¦é€”å¾„ã€‚ç„¶è€Œï¼Œç”±äºåˆ†è¾¨ç‡è¾ƒä½å’Œåœ°å—è¾¹ç•Œä¸æ¸…æ™°ï¼Œåœ¨SITSä¸­è¿›è¡Œåƒç´ çº§æ©æ¨¡æ ‡æ³¨æä¸ºå¤æ‚ä¸”è€—æ—¶ã€‚æœ¬æ–‡é’ˆå¯¹åªæœ‰å›¾åƒçº§ç±»åˆ«å¯ç”¨çš„å¼±ç›‘ç£æ¨¡å¼ï¼Œå°†ä½œç‰©åœ°å›¾ç»˜åˆ¶ä»»åŠ¡ä»è¯¦å°½çš„æ ‡æ³¨è´Ÿæ‹…ä¸­è§£æ”¾å‡ºæ¥ã€‚SITSçš„ç‹¬ç‰¹ç‰¹æ€§ç»™å¼±ç›‘ç£å­¦ä¹ å¸¦æ¥äº†å‡ ä¸ªæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰æ¥è‡ªç©ºé—´é‚»è¿‘åŒºåŸŸçš„å™ªå£°å¹²æ‰°ï¼›ï¼ˆ2ï¼‰æ¥è‡ªå¼‚å¸¸æ—¶é—´æ®µçš„é”™è¯¯è¯­ä¹‰åå·®ã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºæ¢ç´¢æ—¶ç©ºæ„ŸçŸ¥çº¿ç´¢ï¼ˆExactï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç»„ç©ºé—´çº¿ç´¢ï¼Œä»¥æ˜¾å¼æ•è·ä¸åŒä½œç‰©çš„ä»£è¡¨æ€§æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼æ¥è‡ªæœ€ç›¸å…³çš„åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨æ¨¡å‹çš„æ—¶ç©ºç±»åˆ«äº¤äº’ä½œç”¨æ¥å¼ºè°ƒå…³é”®ç‰‡æ®µçš„è´¡çŒ®ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹ä½œç‰©åŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åŸºäºæ—¶ç©ºæ„ŸçŸ¥çº¿ç´¢ï¼Œæˆ‘ä»¬å¾—å‡ºåŸºäºçº¿ç´¢çš„CAMsï¼Œä»¥æœ‰æ•ˆç›‘ç£SITSåˆ†å‰²ç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§SITSåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨Exactç”Ÿæˆçš„æ©æ¨¡ä¸Šè®­ç»ƒçš„åˆ†å‰²ç½‘ç»œè¾¾åˆ°äº†å…¨ç›‘ç£æ€§èƒ½çš„95%ï¼Œæ˜¾ç¤ºå‡ºå¼±ç›‘ç£æ¨¡å¼åœ¨ä½œç‰©åœ°å›¾ç»˜åˆ¶åœºæ™¯ä¸­çš„å…‰æ˜å‰æ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03968v1">PDF</a> Under review. Code will be available at   <a target="_blank" rel="noopener" href="https://github.com/MiSsU-HH/Exact">https://github.com/MiSsU-HH/Exact</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—ï¼ˆSITSï¼‰çš„è‡ªåŠ¨ä½œç‰©æ˜ å°„é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨å¼±ç›‘ç£å­¦ä¹ çš„æ–¹æ³•æ¥è§£å†³æ ‡æ³¨ç¹ççš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç©ºé—´çº¿ç´¢å’Œæ—¶ç©ºæ„ŸçŸ¥çº¿ç´¢ï¼ˆExactï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåº”å¯¹å™ªå£°å¹²æ‰°å’Œè¯­ä¹‰åå·®ç­‰æŒ‘æˆ˜ï¼Œæå‡äº†æ¨¡å‹å¯¹ä½œç‰©åŒºåŸŸçš„æ„ŸçŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªSITSåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŸºäºExactç”Ÿæˆçš„æ©è†œè®­ç»ƒçš„åˆ†å‰²ç½‘ç»œæ€§èƒ½è¾¾åˆ°å…¨ç›‘ç£æ€§èƒ½çš„95%ï¼Œå±•ç°å‡ºå¼±ç›‘ç£å­¦ä¹ åœ¨ä½œç‰©æ˜ å°„é¢†åŸŸçš„å¹¿é˜”å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å«æ˜Ÿå›¾åƒæ—¶é—´åºåˆ—ï¼ˆSITSï¼‰åœ¨å†œä¸šç›‘æµ‹å’Œç®¡ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†æ ‡æ³¨ç¹çé™åˆ¶äº†å…¶å‘å±•ã€‚</li>
<li>ä½åˆ†è¾¨ç‡å’Œæ¨¡ç³Šçš„åœ°å—è¾¹ç•Œä½¿å¾—åƒç´ çº§æ©è†œçš„æ ‡æ³¨åœ¨SITSä¸­æä¸ºå¤æ‚å’Œè€—æ—¶ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä»…åˆ©ç”¨å›¾åƒçº§åˆ«çš„ç±»åˆ«ä¿¡æ¯ï¼Œå‡è½»äº†æ ‡æ³¨è´Ÿæ‹…ã€‚</li>
<li>é¢å¯¹SITSä¸­çš„å™ªå£°å¹²æ‰°å’Œè¯­ä¹‰åå·®æŒ‘æˆ˜ï¼Œå¼•å…¥äº†ç©ºé—´çº¿ç´¢å’Œæ—¶ç©ºæ„ŸçŸ¥çº¿ç´¢ï¼ˆExactï¼‰ã€‚</li>
<li>é€šè¿‡ç©ºé—´çº¿ç´¢æ•æ‰ä¸åŒä½œç‰©çš„ä»£è¡¨æ€§æ¨¡å¼ï¼Œå¹¶å¼ºè°ƒå…³é”®æ—¶æ®µçš„é‡è¦æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹ä½œç‰©åŒºåŸŸçš„æ„ŸçŸ¥ã€‚</li>
<li>åŸºäºæ—¶ç©ºæ„ŸçŸ¥çº¿ç´¢ï¼Œç”Ÿæˆäº†çº¿ç´¢å¼•å¯¼çš„CAMsï¼Œæœ‰æ•ˆç›‘ç£äº†SITSåˆ†å‰²ç½‘ç»œã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d88369d19dc1fbfe8176c77f125d3075.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a7c1af2f2014c0a1a3ca3253716915f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b05a28cc9601417bcde97d71e7e65ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1e5c56493ff6e06ca2dba7fee3dc8d5.jpg" align="middle">
</details>




<h2 id="I-2-OL-Net-Intra-Inter-Objectness-Learning-Network-for-Point-Supervised-X-Ray-Prohibited-Item-Detection"><a href="#I-2-OL-Net-Intra-Inter-Objectness-Learning-Network-for-Point-Supervised-X-Ray-Prohibited-Item-Detection" class="headerlink" title="I$^2$OL-Net: Intra-Inter Objectness Learning Network for   Point-Supervised X-Ray Prohibited Item Detection"></a>I$^2$OL-Net: Intra-Inter Objectness Learning Network for   Point-Supervised X-Ray Prohibited Item Detection</h2><p><strong>Authors:Sanjoeng Wong, Yan Yan</strong></p>
<p>Automatic detection of prohibited items in X-ray images plays a crucial role in public security. However, existing methods rely heavily on labor-intensive box annotations. To address this, we investigate X-ray prohibited item detection under labor-efficient point supervision and develop an intra-inter objectness learning network (I$^2$OL-Net). I$^2$OL-Net consists of two key modules: an intra-modality objectness learning (intra-OL) module and an inter-modality objectness learning (inter-OL) module. The intra-OL module designs a local focus Gaussian masking block and a global random Gaussian masking block to collaboratively learn the objectness in X-ray images. Meanwhile, the inter-OL module introduces the wavelet decomposition-based adversarial learning block and the objectness block, effectively reducing the modality discrepancy and transferring the objectness knowledge learned from natural images with box annotations to X-ray images. Based on the above, I$^2$OL-Net greatly alleviates the problem of part domination caused by severe intra-class variations in X-ray images. Experimental results on four X-ray datasets show that I$^2$OL-Net can achieve superior performance with a significant reduction of annotation cost, thus enhancing its accessibility and practicality. </p>
<blockquote>
<p>åœ¨Xå…‰å›¾åƒä¸­è‡ªåŠ¨æ£€æµ‹è¿ç¦ç‰©å“å¯¹å…¬å…±å®‰å…¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºåŠ³åŠ¨å¯†é›†å‹çš„æ¡†æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨åŠ³åŠ¨åŠ›æ•ˆç‡é«˜çš„ç‚¹ç›‘ç£ä¸‹ç ”ç©¶äº†Xå…‰è¿ç¦ç‰©å“æ£€æµ‹ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å†…å¤–å¯¹è±¡æ€§å­¦ä¹ ç½‘ç»œï¼ˆI$^2$OL-Netï¼‰ã€‚I$^2$OL-Netç”±ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šåŒæ¨¡æ€å¯¹è±¡æ€§å­¦ä¹ ï¼ˆintra-OLï¼‰æ¨¡å—å’Œè·¨æ¨¡æ€å¯¹è±¡æ€§å­¦ä¹ ï¼ˆinter-OLï¼‰æ¨¡å—ã€‚Intra-OLæ¨¡å—è®¾è®¡äº†ä¸€ä¸ªå±€éƒ¨ç„¦ç‚¹é«˜æ–¯æ©è†œå—å’Œä¸€ä¸ªå…¨å±€éšæœºé«˜æ–¯æ©è†œå—ï¼Œä»¥ååŒå­¦ä¹ Xå…‰å›¾åƒä¸­çš„å¯¹è±¡æ€§ã€‚åŒæ—¶ï¼Œinter-OLæ¨¡å—å¼•å…¥äº†åŸºäºå°æ³¢åˆ†è§£çš„å¯¹æŠ—æ€§å­¦ä¹ å—å’Œå¯¹è±¡æ€§å—ï¼Œæœ‰æ•ˆåœ°å‡å°‘äº†æ¨¡æ€å·®å¼‚ï¼Œå¹¶å°†ä»å¸¦æœ‰æ¡†æ³¨é‡Šçš„è‡ªç„¶å›¾åƒä¸­å­¦ä¹ åˆ°çš„å¯¹è±¡æ€§çŸ¥è¯†è½¬ç§»åˆ°Xå…‰å›¾åƒä¸Šã€‚åŸºäºä»¥ä¸Šå†…å®¹ï¼ŒI$^2$OL-Netå¤§å¤§ç¼“è§£äº†ç”±Xå…‰å›¾åƒä¸­ä¸¥é‡çš„ç±»å†…å˜åŒ–å¼•èµ·çš„éƒ¨åˆ†ä¸»å¯¼é—®é¢˜ã€‚åœ¨å››ä¸ªXå…‰æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒI$^2$OL-Netåœ¨æ˜¾è‘—é™ä½æ ‡æ³¨æˆæœ¬çš„åŒæ—¶ï¼Œå¯ä»¥å–å¾—å“è¶Šçš„æ€§èƒ½ï¼Œä»è€Œæé«˜äº†å…¶å¯è®¿é—®æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03811v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç ”ç©¶äº†Xå…‰å›¾åƒä¸­çš„è¿ç¦ç‰©å“è‡ªåŠ¨æ£€æµ‹é—®é¢˜ï¼Œæå‡ºä¸€ç§åŠ³åŠ¨æ•ˆç‡é«˜çš„ç‚¹ç›‘ç£æ–¹å¼ä¸‹çš„è¿ç¦ç‰©å“æ£€æµ‹ç½‘ç»œI$^2$OL-Netã€‚è¯¥ç½‘ç»œç”±ä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆï¼šintra-OLæ¨¡å—å’Œinter-OLæ¨¡å—ã€‚å‰è€…é€šè¿‡å±€éƒ¨èšç„¦é«˜æ–¯æ©è†œå—å’Œå…¨å±€éšæœºé«˜æ–¯æ©è†œå—ååŒå­¦ä¹ Xå…‰å›¾åƒä¸­çš„å¯¹è±¡æ€§ï¼›åè€…å¼•å…¥åŸºäºå°æ³¢åˆ†è§£çš„å¯¹æŠ—å­¦ä¹ å—å’Œå¯¹è±¡æ€§å—ï¼Œæœ‰æ•ˆå‡å°‘æ¨¡æ€å·®å¼‚ï¼Œå°†ä»è‡ªç„¶å›¾åƒä¸­å­¦ä¹ åˆ°çš„å¯¹è±¡æ€§çŸ¥è¯†è½¬ç§»åˆ°Xå…‰å›¾åƒä¸Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒI$^2$OL-Netåœ¨å››ä¸ªXå…‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜è¶Šï¼Œä¸”æ ‡æ³¨æˆæœ¬å¤§å¹…é™ä½ï¼Œæé«˜äº†å…¶å¯ç”¨æ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Xå…‰å›¾åƒä¸­çš„è¿ç¦ç‰©å“è‡ªåŠ¨æ£€æµ‹å¯¹å…¬å…±å®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–åŠ³åŠ¨å¯†é›†å‹çš„æ¡†æ ‡æ³¨ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>I$^2$OL-Netç½‘ç»œç”±intra-OLå’Œinter-OLä¸¤ä¸ªå…³é”®æ¨¡å—ç»„æˆã€‚</li>
<li>intra-OLæ¨¡å—é€šè¿‡å±€éƒ¨å’Œå…¨å±€é«˜æ–¯æ©è†œå—å­¦ä¹ Xå…‰å›¾åƒä¸­çš„å¯¹è±¡æ€§ã€‚</li>
<li>inter-OLæ¨¡å—åˆ©ç”¨å°æ³¢åˆ†è§£çš„å¯¹æŠ—å­¦ä¹ å—å’Œå¯¹è±¡æ€§å—ï¼Œå‡å°‘æ¨¡æ€å·®å¼‚ï¼Œå¹¶å°†è‡ªç„¶å›¾åƒä¸­çš„å¯¹è±¡æ€§çŸ¥è¯†è½¬ç§»åˆ°Xå…‰å›¾åƒä¸Šã€‚</li>
<li>I$^2$OL-Netåœ¨å››ä¸ªXå…‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜è¶Šï¼Œé™ä½äº†æ ‡æ³¨æˆæœ¬ï¼Œå¢å¼ºäº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2d66f910bfa866bee48b107d36f26c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de829c8eefa8229fee894461d46ba5e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb6f4a8bff1ba56ff2b8a3ad04d52584.jpg" align="middle">
</details>




<h2 id="Evaluating-Single-Event-Upsets-in-Deep-Neural-Networks-for-Semantic-Segmentation-an-embedded-system-perspective"><a href="#Evaluating-Single-Event-Upsets-in-Deep-Neural-Networks-for-Semantic-Segmentation-an-embedded-system-perspective" class="headerlink" title="Evaluating Single Event Upsets in Deep Neural Networks for Semantic   Segmentation: an embedded system perspective"></a>Evaluating Single Event Upsets in Deep Neural Networks for Semantic   Segmentation: an embedded system perspective</h2><p><strong>Authors:Jon GutiÃ©rrez-Zaballa, Koldo Basterretxea, Javier Echanobe</strong></p>
<p>As the deployment of artifical intelligence (AI) algorithms at edge devices becomes increasingly prevalent, enhancing the robustness and reliability of autonomous AI-based perception and decision systems is becoming as relevant as precision and performance, especially in applications areas considered safety-critical such as autonomous driving and aerospace. This paper delves into the robustness assessment in embedded Deep Neural Networks (DNNs), particularly focusing on the impact of parameter perturbations produced by single event upsets (SEUs) on convolutional neural networks (CNN) for image semantic segmentation. By scrutinizing the layer-by-layer and bit-by-bit sensitivity of various encoder-decoder models to soft errors, this study thoroughly investigates the vulnerability of segmentation DNNs to SEUs and evaluates the consequences of techniques like model pruning and parameter quantization on the robustness of compressed models aimed at embedded implementations. The findings offer valuable insights into the mechanisms underlying SEU-induced failures that allow for evaluating the robustness of DNNs once trained in advance. Moreover, based on the collected data, we propose a set of practical lightweight error mitigation techniques with no memory or computational cost suitable for resource-constrained deployments. The code used to perform the fault injection (FI) campaign is available at <a target="_blank" rel="noopener" href="https://github.com/jonGuti13/TensorFI2">https://github.com/jonGuti13/TensorFI2</a> , while the code to implement proposed techniques is available at <a target="_blank" rel="noopener" href="https://github.com/jonGuti13/parameterProtection">https://github.com/jonGuti13/parameterProtection</a> . </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç®—æ³•åœ¨è¾¹ç¼˜è®¾å¤‡çš„éƒ¨ç½²è¶Šæ¥è¶Šæ™®éï¼Œæé«˜åŸºäºAIçš„è‡ªä¸»æ„ŸçŸ¥å’Œå†³ç­–ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå¯é æ€§å˜å¾—ä¸ç²¾åº¦å’Œæ€§èƒ½åŒæ ·é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®é¢†åŸŸï¼Œå¦‚è‡ªåŠ¨é©¾é©¶å’Œèˆªç©ºèˆªå¤©ç­‰åº”ç”¨ä¸­ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†åµŒå…¥å¼æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„ç¨³å¥æ€§è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯å…³æ³¨ç”±å•äº‹ä»¶æ‰°åŠ¨ï¼ˆSEUï¼‰äº§ç”Ÿçš„å‚æ•°æ‰°åŠ¨å¯¹å›¾åƒè¯­ä¹‰åˆ†å‰²å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å½±å“ã€‚é€šè¿‡å¯¹å„ç§ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„é€å±‚å’Œé€ä½æ•æ„Ÿæ€§è¿›è¡Œä»”ç»†å®¡æŸ¥ï¼Œæœ¬ç ”ç©¶å…¨é¢ç ”ç©¶äº†åˆ†å‰²DNNå¯¹SEUçš„è„†å¼±æ€§ï¼Œå¹¶è¯„ä¼°äº†æ¨¡å‹ä¿®å‰ªå’Œå‚æ•°é‡åŒ–ç­‰æŠ€æœ¯å¯¹é¢å‘åµŒå…¥å¼å®ç°çš„å‹ç¼©æ¨¡å‹çš„ç¨³å¥æ€§çš„å½±å“ã€‚ç ”ç©¶ç»“æœæä¾›äº†æ·±å…¥äº†è§£SEUè¯±å¯¼æ•…éšœæœºåˆ¶çš„æœ‰ä»·å€¼è§è§£ï¼Œè¿™äº›è§è§£å¯ä»¥è¯„ä¼°é¢„å…ˆè®­ç»ƒçš„DNNçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºæ”¶é›†çš„æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€å¥—å®ç”¨çš„è½»é‡çº§é”™è¯¯ç¼“è§£æŠ€æœ¯ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„éƒ¨ç½²ï¼Œå¹¶ä¸”ä¸äº§ç”Ÿä»»ä½•å†…å­˜æˆ–è®¡ç®—æˆæœ¬ã€‚ç”¨äºæ‰§è¡Œæ•…éšœæ³¨å…¥ï¼ˆFIï¼‰è¿åŠ¨çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jonGuti13/TensorFI2%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%8C%E8%80%8C%E5%AE%9E%E7%8E%B0%E6%89%80%E6%8F%90%E5%87%BA%E6%8A%80%E6%9C%AF%E7%9A%84%E4%BB%A3%E7%A0%81%E5%8F%AF%E5%9C%A8https://github.com/jonGuti13/parameterProtection%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jonGuti13/TensorFI2ä¸Šæ‰¾åˆ°ï¼Œè€Œå®ç°æ‰€æå‡ºæŠ€æœ¯çš„ä»£ç å¯åœ¨https://github.com/jonGuti13/parameterProtectionä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03630v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åœ¨åµŒå…¥å¼æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ä¸­è¿›è¡Œé²æ£’æ€§è¯„ä¼°çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å•äº‹ä»¶æ‰°åŠ¨ï¼ˆSEUsï¼‰å¼•èµ·çš„å‚æ•°æ‰°åŠ¨å¯¹å›¾åƒè¯­ä¹‰åˆ†å‰²å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å½±å“ã€‚æ–‡ç« æ·±å…¥æ¢è®¨äº†åˆ†å‰²DNNå¯¹SEUsçš„è„†å¼±æ€§ï¼Œå¹¶è¯„ä¼°äº†æ¨¡å‹ä¿®å‰ªå’Œå‚æ•°é‡åŒ–ç­‰æŠ€æœ¯å¯¹åµŒå…¥å¼å®ç°ä¸­å‹ç¼©æ¨¡å‹çš„é²æ£’æ€§çš„å½±å“ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†ä¸€ç³»åˆ—å®ç”¨çš„è½»é‡çº§é”™è¯¯ç¼“è§£æŠ€æœ¯ï¼Œå¯åœ¨èµ„æºå—é™çš„éƒ¨ç½²ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œè€Œæ— éœ€å¢åŠ å†…å­˜æˆ–è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç®—æ³•åœ¨è¾¹ç¼˜è®¾å¤‡çš„éƒ¨ç½²è¶Šæ¥è¶Šæ™®éï¼Œæé«˜è‡ªä¸»AIæ„ŸçŸ¥å’Œå†³ç­–ç³»ç»Ÿçš„ç¨³å¥æ€§å’Œå¯é æ€§å˜å¾—è‡³å…³é‡è¦ã€‚ç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å…³é”®é¢†åŸŸå¦‚è‡ªåŠ¨é©¾é©¶å’Œèˆªç©ºèˆªå¤©é¢†åŸŸå°¤ä¸ºé‡è¦ã€‚</li>
<li>æœ¬æ–‡é‡ç‚¹å…³æ³¨åµŒå…¥å¼æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰ä¸­çš„é²æ£’æ€§è¯„ä¼°é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å‚æ•°æ‰°åŠ¨å¯¹å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å›¾åƒè¯­ä¹‰åˆ†å‰²çš„å½±å“ã€‚</li>
<li>æ–‡ç« é€šè¿‡é€å±‚å’Œé€ä½æ•æ„Ÿæ€§åˆ†æï¼Œæ·±å…¥æ¢è®¨äº†åˆ†å‰²DNNå¯¹å•äº‹ä»¶æ‰°åŠ¨ï¼ˆSEUsï¼‰çš„è„†å¼±æ€§ã€‚è¯„ä¼°äº†æ¨¡å‹ä¿®å‰ªå’Œå‚æ•°é‡åŒ–æŠ€æœ¯å¯¹å‹ç¼©æ¨¡å‹é²æ£’æ€§çš„å½±å“ã€‚è¿™å¯¹äºåµŒå…¥å¼å®ç°å…·æœ‰å®é™…æ„ä¹‰ã€‚</li>
<li>æ–‡ç« æä¾›äº†æœ‰å…³SEUå¼•èµ·æ•…éšœæœºåˆ¶çš„å®è´µè§è§£ï¼Œæœ‰åŠ©äºè¯„ä¼°é¢„å…ˆè®­ç»ƒçš„DNNçš„é²æ£’æ€§ã€‚è¿™äº›è§è§£å¯¹äºç†è§£AIç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºæ”¶é›†çš„æ•°æ®ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç³»åˆ—å®ç”¨çš„è½»é‡çº§é”™è¯¯ç¼“è§£æŠ€æœ¯ï¼Œé€‚ç”¨äºèµ„æºå—é™çš„éƒ¨ç½²ç¯å¢ƒï¼Œå¹¶ä¸”ä¸ä¼šå¢åŠ å†…å­˜æˆ–è®¡ç®—æˆæœ¬ã€‚è¿™å¯¹äºåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²AIç®—æ³•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æ–‡ç« å…¬å¼€äº†ç”¨äºæ‰§è¡Œæ•…éšœæ³¨å…¥æ´»åŠ¨çš„ä»£ç ï¼Œå¹¶æä¾›äº†å®æ–½æ‰€æè®®æŠ€æœ¯çš„ä»£ç ã€‚è¿™å¯¹äºç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æ¥è¯´æ˜¯ä¸€ä¸ªå®è´µçš„èµ„æºï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›AIç³»ç»Ÿçš„é²æ£’æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3a45328c5bc2909e2708de37a27e9f4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20c8909e14cefd5bf285a9e1b3dc4762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b74fe9137555a171cd13918bdc3845.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8baa01d20e0142fd1a3fa0e6a2b9f28c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6742f1a3a9d52a23b5b5c5b87caab41.jpg" align="middle">
</details>




<h2 id="Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging"><a href="#Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging" class="headerlink" title="Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging"></a>Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging</h2><p><strong>Authors:Luca Ciampi, Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi</strong></p>
<p>We propose a novel two-stage semi-supervised learning approach for training downsampling-upsampling semantic segmentation architectures. The first stage does not use backpropagation. Rather, it exploits the bio-inspired Hebbian principle â€œfire together, wire togetherâ€ as a local learning rule for updating the weights of both convolutional and transpose-convolutional layers, allowing unsupervised discovery of data features. In the second stage, the model is fine-tuned with standard backpropagation on a small subset of labeled data. We evaluate our methodology through experiments conducted on several widely used biomedical datasets, deeming that this domain is paramount in computer vision and is notably impacted by data scarcity. Results show that our proposed method outperforms SOTA approaches across different levels of label availability. Furthermore, we show that using our unsupervised stage to initialize the SOTA approaches leads to performance improvements. The code to replicate our experiments can be found at: <a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-medical-image-segmentation">https://github.com/ciampluca/hebbian-medical-image-segmentation</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µåŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸‹é‡‡æ ·-ä¸Šé‡‡æ ·è¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚ç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨åå‘ä¼ æ’­ã€‚ç›¸åï¼Œå®ƒåˆ©ç”¨ç”Ÿç‰©å¯å‘çš„èµ«å¸ƒåŸåˆ™â€œä¸€èµ·æ”¾ç”µï¼Œä¸€èµ·è¿æ¥â€ä½œä¸ºå±€éƒ¨å­¦ä¹ è§„åˆ™ï¼Œä»¥æ›´æ–°å·ç§¯å±‚å’Œè½¬ç½®å·ç§¯å±‚çš„æƒé‡ï¼Œä»è€Œå®ç°æ•°æ®ç‰¹å¾çš„æ— ç›‘ç£å‘ç°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨ä¸€å°éƒ¨åˆ†æ ‡è®°æ•°æ®çš„æ ‡å‡†åå‘ä¼ æ’­è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€ç³»åˆ—å¹¿æ³›ä½¿ç”¨çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè®¤ä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸæ˜¯è¿™ä¸€æ–¹æ³•çš„å…³é”®åº”ç”¨é¢†åŸŸï¼Œå¹¶ä¸”è¯¥æ–¹æ³•æ˜¾è‘—åœ°å—åˆ°äº†æ•°æ®ç¨€ç¼ºçš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡ç­¾å¯ç”¨æ€§çš„ä¸åŒçº§åˆ«ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å‡ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä½¿ç”¨æˆ‘ä»¬çš„æ— ç›‘ç£é˜¶æ®µæ¥åˆå§‹åŒ–æœ€æ–°æŠ€æœ¯æ–¹æ³•å¯ä»¥æé«˜æ€§èƒ½ã€‚å¤åˆ¶æˆ‘ä»¬å®éªŒçš„ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-medical-image-segmentation">https://github.com/ciampluca/hebbian-medical-image-segmentation</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æå‡ºä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µåŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸‹é‡‡æ ·-ä¸Šé‡‡æ ·è¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚ç¬¬ä¸€é˜¶æ®µä¸é‡‡ç”¨åå‘ä¼ æ’­ï¼Œè€Œæ˜¯åˆ©ç”¨ç”Ÿç‰©å¯å‘çš„æµ·å¸ƒåŸç†ï¼ˆfire togetherï¼Œwire togetherï¼‰ä½œä¸ºå±€éƒ¨å­¦ä¹ è§„åˆ™ï¼Œæ›´æ–°å·ç§¯å’Œè½¬ç½®å·ç§¯å±‚çš„æƒé‡ï¼Œå®ç°æ•°æ®ç‰¹å¾çš„æ— ç›‘ç£å‘ç°ã€‚ç¬¬äºŒé˜¶æ®µåˆ™é‡‡ç”¨æ ‡å‡†åå‘ä¼ æ’­å¯¹å°é‡æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„è®¡ç®—æœºè§†è§‰é¢†åŸŸä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨ä¸åŒæ ‡ç­¾å¯ç”¨æ€§æ°´å¹³ä¸Šè¡¨ç°çªå‡ºã€‚åˆå§‹åŒ–ç°æœ‰æŠ€æœ¯æ—¶é‡‡ç”¨æ— ç›‘ç£é˜¶æ®µå¯æé«˜æ€§èƒ½ã€‚ç›¸å…³å®éªŒä»£ç å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-medical-image-segmentation">https://github.com/ciampluca/hebbian-medical-image-segmentation</a> ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æå‡ºä¸€ç§ä¸¤é˜¶æ®µåŠç›‘ç£å­¦ä¹ æ–¹æ³•ç”¨äºè®­ç»ƒè¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æµ·å¸ƒåŸç†è¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼Œæ›´æ–°å·ç§¯å’Œè½¬ç½®å·ç§¯å±‚çš„æƒé‡ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ ‡å‡†åå‘ä¼ æ’­åœ¨å°‘é‡æ ‡è®°æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒã€‚</li>
<li>åœ¨ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ä¸åŒæ ‡ç­¾å¯ç”¨æ€§æ°´å¹³ä¸Šè¡¨ç°çªå‡ºã€‚</li>
<li>é‡‡ç”¨æ— ç›‘ç£é˜¶æ®µåˆå§‹åŒ–ç°æœ‰æŠ€æœ¯å¯æé«˜æ€§èƒ½ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8ae1ef65d6d78f5a3b2ced3f03e4787c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c309be12fe46d7372170491747a3a3d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1c8f4854d7245b4ec92c20c712c0362.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cc32163d8904c981f8e26dcd1bcaece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7129008c6dc7771d668465564eff26.jpg" align="middle">
</details>




<h2 id="Semantic-Segmentation-Prior-for-Diffusion-Based-Real-World-Super-Resolution"><a href="#Semantic-Segmentation-Prior-for-Diffusion-Based-Real-World-Super-Resolution" class="headerlink" title="Semantic Segmentation Prior for Diffusion-Based Real-World   Super-Resolution"></a>Semantic Segmentation Prior for Diffusion-Based Real-World   Super-Resolution</h2><p><strong>Authors:Jiahua Xiao, Jiawei Zhang, Dongqing Zou, Xiaodan Zhang, Jimmy Ren, Xing Wei</strong></p>
<p>Real-world image super-resolution (Real-ISR) has achieved a remarkable leap by leveraging large-scale text-to-image models, enabling realistic image restoration from given recognition textual prompts. However, these methods sometimes fail to recognize some salient objects, resulting in inaccurate semantic restoration in these regions. Additionally, the same region may have a strong response to more than one prompt and it will lead to semantic ambiguity for image super-resolution. To alleviate the above two issues, in this paper, we propose to consider semantic segmentation as an additional control condition into diffusion-based image super-resolution. Compared to textual prompt conditions, semantic segmentation enables a more comprehensive perception of salient objects within an image by assigning class labels to each pixel. It also mitigates the risks of semantic ambiguities by explicitly allocating objects to their respective spatial regions. In practice, inspired by the fact that image super-resolution and segmentation can benefit each other, we propose SegSR which introduces a dual-diffusion framework to facilitate interaction between the image super-resolution and segmentation diffusion models. Specifically, we develop a Dual-Modality Bridge module to enable updated information flow between these two diffusion models, achieving mutual benefit during the reverse diffusion process. Extensive experiments show that SegSR can generate realistic images while preserving semantic structures more effectively. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ISRï¼‰å€ŸåŠ©å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å®ç°äº†æ˜¾è‘—é£è·ƒï¼Œèƒ½å¤Ÿä»ç»™å®šçš„è¯†åˆ«æ–‡æœ¬æç¤ºä¸­æ¢å¤ç°å®å›¾åƒã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœ‰æ—¶æ— æ³•è¯†åˆ«å‡ºä¸€äº›æ˜¾è‘—ç‰©ä½“ï¼Œå¯¼è‡´è¿™äº›åŒºåŸŸçš„è¯­ä¹‰æ¢å¤ä¸å‡†ç¡®ã€‚æ­¤å¤–ï¼ŒåŒä¸€åŒºåŸŸå¯èƒ½å¯¹å¤šä¸ªæç¤ºæœ‰å¼ºçƒˆçš„ååº”ï¼Œè¿™å°†å¯¼è‡´å›¾åƒè¶…åˆ†è¾¨ç‡çš„è¯­ä¹‰æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå°†è¯­ä¹‰åˆ†å‰²ä½œä¸ºæ‰©æ•£å¼å›¾åƒè¶…åˆ†è¾¨ç‡çš„é™„åŠ æ§åˆ¶æ¡ä»¶ã€‚ä¸æ–‡æœ¬æç¤ºæ¡ä»¶ç›¸æ¯”ï¼Œè¯­ä¹‰åˆ†å‰²é€šè¿‡ä¸ºæ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾ï¼Œä½¿å›¾åƒå†…çš„æ˜¾è‘—ç‰©ä½“å¾—åˆ°æ›´å…¨é¢çš„æ„ŸçŸ¥ã€‚å®ƒè¿˜é€šè¿‡æ˜¾å¼åœ°å°†ç‰©ä½“åˆ†é…ç»™å…¶å„è‡ªçš„ç©ºé—´åŒºåŸŸï¼Œå‡è½»äº†è¯­ä¹‰æ¨¡ç³Šçš„é£é™©ã€‚åœ¨å®è·µä¸­ï¼Œå—åˆ°å›¾åƒè¶…åˆ†è¾¨ç‡å’Œåˆ†å‰²å¯ä»¥ç›¸äº’å—ç›Šçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†SegSRï¼Œå®ƒå¼•å…¥äº†åŒæ‰©æ•£æ¡†æ¶ï¼Œä»¥ä¿ƒè¿›å›¾åƒè¶…åˆ†è¾¨ç‡å’Œåˆ†å‰²æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„äº¤äº’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒæ¨¡æ€æ¡¥æ¥æ¨¡å—ï¼Œä»¥å®ç°è¿™ä¸¤ä¸ªæ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ›´æ–°ä¿¡æ¯æµåŠ¨ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­å®ç°ç›¸äº’å—ç›Šã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSegSRèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼ŒåŒæ—¶æ›´æœ‰æ•ˆåœ°ä¿ç•™è¯­ä¹‰ç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02960v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå€ŸåŠ©å¤§è§„æ¨¡æ–‡æœ¬å›¾åƒæ¨¡å‹ï¼ŒçœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œèƒ½å¤Ÿæ ¹æ®å®é™…æ–‡æœ¬æç¤ºè¿›è¡Œé€¼çœŸçš„å›¾åƒæ¢å¤ã€‚ä½†æ­¤æ–¹æ³•æœ‰æ—¶æ— æ³•è¯†åˆ«æ˜¾è‘—ç‰©ä½“ï¼Œå¯¼è‡´è¯­ä¹‰æ¢å¤ä¸å‡†ç¡®ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå°†è¯­ä¹‰åˆ†å‰²ä½œä¸ºæ‰©æ•£å›¾åƒè¶…åˆ†è¾¨ç‡çš„é™„åŠ æ§åˆ¶æ¡ä»¶ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾ï¼Œæ›´å…¨é¢åœ°æ„ŸçŸ¥å›¾åƒä¸­çš„æ˜¾è‘—ç‰©ä½“ï¼Œå¹¶æ˜¾å¼åœ°å°†ç‰©ä½“åˆ†é…ç»™å…¶ç›¸åº”çš„ç©ºé—´åŒºåŸŸä»¥é™ä½è¯­ä¹‰æ¨¡ç³Šçš„é£é™©ã€‚å—å›¾åƒè¶…åˆ†è¾¨ç‡å’Œåˆ†å‰²å¯ç›¸äº’å—ç›Šçš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºSegSRæ–¹æ³•ï¼Œé‡‡ç”¨åŒæ‰©æ•£æ¡†æ¶ä¿ƒè¿›å›¾åƒè¶…åˆ†è¾¨ç‡å’Œåˆ†å‰²æ‰©æ•£æ¨¡å‹é—´çš„äº¤äº’ã€‚é€šè¿‡å¼€å‘åŒæ¨¡æ€æ¡¥æ¨¡å—ï¼Œå®ç°ä¸¤è€…åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„ä¿¡æ¯æµé€šå’Œç›¸äº’å—ç›Šã€‚å®éªŒè¡¨æ˜ï¼ŒSegSRèƒ½å¤Ÿç”Ÿæˆæ›´çœŸå®ä¸”æœ‰æ•ˆä¿ç•™è¯­ä¹‰ç»“æ„çš„å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹æ–‡æœ¬å›¾åƒæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œå›¾åƒè¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿå®ç°åŸºäºæ–‡æœ¬æç¤ºçš„å›¾åƒæ¢å¤ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹æ— æ³•å‡†ç¡®è¯†åˆ«æ˜¾è‘—ç‰©ä½“ï¼Œå¯¼è‡´è¯­ä¹‰æ¢å¤ä¸å‡†ç¡®ã€‚</li>
<li>è¯­ä¹‰åˆ†å‰²ä½œä¸ºé™„åŠ æ§åˆ¶æ¡ä»¶è¢«å¼•å…¥åˆ°æ‰©æ•£å›¾åƒè¶…åˆ†è¾¨ç‡ä¸­ï¼Œä»¥æ›´å…¨é¢åœ°æ„ŸçŸ¥å›¾åƒä¸­çš„æ˜¾è‘—ç‰©ä½“å¹¶é™ä½è¯­ä¹‰æ¨¡ç³Šé£é™©ã€‚</li>
<li>è¯­ä¹‰åˆ†å‰²é€šè¿‡ä¸ºæ¯ä¸ªåƒç´ åˆ†é…ç±»åˆ«æ ‡ç­¾ï¼Œæ˜¾å¼åœ°å°†ç‰©ä½“åˆ†é…ç»™å…¶ç›¸åº”çš„ç©ºé—´åŒºåŸŸã€‚</li>
<li>SegSRæ–¹æ³•é‡‡ç”¨åŒæ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡åŒæ¨¡æ€æ¡¥æ¨¡å—å®ç°å›¾åƒè¶…åˆ†è¾¨ç‡å’Œåˆ†å‰²æ‰©æ•£æ¨¡å‹é—´çš„äº¤äº’ã€‚</li>
<li>SegSRèƒ½å¤Ÿåœ¨ç”ŸæˆçœŸå®å›¾åƒçš„åŒæ—¶æ›´æœ‰æ•ˆåœ°ä¿ç•™è¯­ä¹‰ç»“æ„ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83f87998ea114f5e2d853dc07d173b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0168eab9991c4df578ab7bacbf9d25a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a58e8db1ff5ce6550cc320659979152a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34633222acddda0c7cc31ba83daf009a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f82baed3b606dafa171eb09effc9a6db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3cb39f74b46a9ee636aa668b80f40703.jpg" align="middle">
</details>




<h2 id="MetaShadow-Object-Centered-Shadow-Detection-Removal-and-Synthesis"><a href="#MetaShadow-Object-Centered-Shadow-Detection-Removal-and-Synthesis" class="headerlink" title="MetaShadow: Object-Centered Shadow Detection, Removal, and Synthesis"></a>MetaShadow: Object-Centered Shadow Detection, Removal, and Synthesis</h2><p><strong>Authors:Tianyu Wang, Jianming Zhang, Haitian Zheng, Zhihong Ding, Scott Cohen, Zhe Lin, Wei Xiong, Chi-Wing Fu, Luis Figueroa, Soo Ye Kim</strong></p>
<p>Shadows are often under-considered or even ignored in image editing applications, limiting the realism of the edited results. In this paper, we introduce MetaShadow, a three-in-one versatile framework that enables detection, removal, and controllable synthesis of shadows in natural images in an object-centered fashion. MetaShadow combines the strengths of two cooperative components: Shadow Analyzer, for object-centered shadow detection and removal, and Shadow Synthesizer, for reference-based controllable shadow synthesis. Notably, we optimize the learning of the intermediate features from Shadow Analyzer to guide Shadow Synthesizer to generate more realistic shadows that blend seamlessly with the scene. Extensive evaluations on multiple shadow benchmark datasets show significant improvements of MetaShadow over the existing state-of-the-art methods on object-centered shadow detection, removal, and synthesis. MetaShadow excels in image-editing tasks such as object removal, relocation, and insertion, pushing the boundaries of object-centered image editing. </p>
<blockquote>
<p>åœ¨å›¾åƒç¼–è¾‘åº”ç”¨ä¸­ï¼Œé˜´å½±å¾€å¾€è¢«å¿½è§†æˆ–å¿½ç•¥ï¼Œè¿™é™åˆ¶äº†ç¼–è¾‘ç»“æœçš„é€¼çœŸæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MetaShadowï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰åˆä¸€çš„é€šç”¨æ¡†æ¶ï¼Œä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ–¹å¼ï¼Œå®ç°å¯¹è‡ªç„¶å›¾åƒä¸­çš„é˜´å½±è¿›è¡Œæ£€æµ‹ã€å»é™¤å’Œå¯æ§åˆæˆã€‚MetaShadowç»“åˆäº†é˜´å½±åˆ†æå™¨å’Œé˜´å½±åˆæˆå™¨ä¸¤ä¸ªåä½œç»„ä»¶çš„ä¼˜åŠ¿ï¼šé˜´å½±åˆ†æå™¨ç”¨äºä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„é˜´å½±æ£€æµ‹å’Œå»é™¤ï¼Œè€Œé˜´å½±åˆæˆå™¨åˆ™ç”¨äºåŸºäºå‚è€ƒçš„é˜´å½±å¯æ§åˆæˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†é˜´å½±åˆ†æå™¨ä¸­é—´ç‰¹å¾çš„å­¦ä¹ ï¼Œä»¥æŒ‡å¯¼é˜´å½±åˆæˆå™¨ç”Ÿæˆæ›´é€¼çœŸçš„é˜´å½±ï¼Œä½¿å…¶æ— ç¼èå…¥åœºæ™¯ä¸­ã€‚åœ¨å¤šä¸ªé˜´å½±åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMetaShadowåœ¨é¢å‘å¯¹è±¡çš„é˜´å½±æ£€æµ‹ã€å»é™¤å’Œåˆæˆæ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„ç®—æ³•ã€‚MetaShadowåœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡ç§»é™¤ã€é‡æ–°å®šä½å’Œæ’å…¥ï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œçªç ´äº†é¢å‘å¯¹è±¡çš„å›¾åƒç¼–è¾‘çš„ç•Œé™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02635v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†MetaShadowï¼Œä¸€ä¸ªé›†æ£€æµ‹ã€ç§»é™¤å’Œå¯æ§åˆæˆä¸ºä¸€ä½“çš„å¤šåŠŸèƒ½æ¡†æ¶ï¼Œç”¨äºä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è‡ªç„¶å›¾åƒé˜´å½±å¤„ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡Shadow Analyzerè¿›è¡Œå¯¹è±¡ä¸ºä¸­å¿ƒçš„é˜´å½±æ£€æµ‹å’Œç§»é™¤ï¼Œé€šè¿‡Shadow Synthesizerè¿›è¡Œå‚è€ƒå¯æ§çš„é˜´å½±åˆæˆã€‚ä¼˜åŒ–ä¸­é—´ç‰¹å¾çš„å­¦ä¹ ï¼Œä½¿åˆæˆçš„é˜´å½±æ›´çœŸå®ã€åœºæ™¯èåˆåº¦æ›´é«˜ã€‚åœ¨å¤šä¸ªé˜´å½±åŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒMetaShadowåœ¨é¢å‘å¯¹è±¡çš„é˜´å½±æ£€æµ‹ã€ç§»é™¤å’Œåˆæˆæ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå°¤å…¶åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡å¦‚å¯¹è±¡ç§»é™¤ã€é‡æ–°å®šä½å’Œæ’å…¥æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MetaShadowæ˜¯ä¸€ä¸ªé›†æ£€æµ‹ã€ç§»é™¤å’Œå¯æ§åˆæˆä¸ºä¸€ä½“çš„å¤šåŠŸèƒ½æ¡†æ¶ï¼Œä¸“æ³¨äºè‡ªç„¶å›¾åƒçš„é˜´å½±å¤„ç†ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªåˆä½œç»„ä»¶ï¼šShadow Analyzerç”¨äºå¯¹è±¡ä¸ºä¸­å¿ƒçš„é˜´å½±æ£€æµ‹å’Œç§»é™¤ï¼Œè€ŒShadow Synthesizerç”¨äºå‚è€ƒå¯æ§çš„é˜´å½±åˆæˆã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–Shadow Analyzerçš„ä¸­é—´ç‰¹å¾å­¦ä¹ ï¼ŒæŒ‡å¯¼Shadow Synthesizerç”Ÿæˆæ›´çœŸå®ã€åœºæ™¯èåˆåº¦æ›´é«˜çš„é˜´å½±ã€‚</li>
<li>MetaShadowåœ¨é¢å‘å¯¹è±¡çš„é˜´å½±æ£€æµ‹ã€ç§»é™¤å’Œåˆæˆæ–¹é¢å‡è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡å¦‚å¯¹è±¡ç§»é™¤ã€é‡æ–°å®šä½å’Œæ’å…¥æ–¹é¢æœ‰å¾ˆå¼ºçš„åº”ç”¨èƒ½åŠ›ã€‚</li>
<li>MetaShadowèƒ½å¤Ÿæå‡å›¾åƒç¼–è¾‘çš„çœŸå®æ„Ÿå’Œè´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9d31509806a706a08c7c5e5a77d602d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03ed38e2f51668926a3480b140ae7744.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98a3e82cd5f3f60826f91c44e6e3c2db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03198c2808ca2c8ca51bed7b258e2d46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a188b9141dbac81284da110ddde698e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63dd84b6cd8d35d3490b50d39a001595.jpg" align="middle">
</details>




<h2 id="GSGTrack-Gaussian-Splatting-Guided-Object-Pose-Tracking-from-RGB-Videos"><a href="#GSGTrack-Gaussian-Splatting-Guided-Object-Pose-Tracking-from-RGB-Videos" class="headerlink" title="GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos"></a>GSGTrack: Gaussian Splatting-Guided Object Pose Tracking from RGB Videos</h2><p><strong>Authors:Zhiyuan Chen, Fan Lu, Guo Yu, Bin Li, Sanqing Qu, Yuan Huang, Changhong Fu, Guang Chen</strong></p>
<p>Tracking the 6DoF pose of unknown objects in monocular RGB video sequences is crucial for robotic manipulation. However, existing approaches typically rely on accurate depth information, which is non-trivial to obtain in real-world scenarios. Although depth estimation algorithms can be employed, geometric inaccuracy can lead to failures in RGBD-based pose tracking methods. To address this challenge, we introduce GSGTrack, a novel RGB-based pose tracking framework that jointly optimizes geometry and pose. Specifically, we adopt 3D Gaussian Splatting to create an optimizable 3D representation, which is learned simultaneously with a graph-based geometry optimization to capture the objectâ€™s appearance features and refine its geometry. However, the joint optimization process is susceptible to perturbations from noisy pose and geometry data. Thus, we propose an object silhouette loss to address the issue of pixel-wise loss being overly sensitive to pose noise during tracking. To mitigate the geometric ambiguities caused by inaccurate depth information, we propose a geometry-consistent image pair selection strategy, which filters out low-confidence pairs and ensures robust geometric optimization. Extensive experiments on the OnePose and HO3D datasets demonstrate the effectiveness of GSGTrack in both 6DoF pose tracking and object reconstruction. </p>
<blockquote>
<p>é’ˆå¯¹å•ç›®RGBè§†é¢‘åºåˆ—ä¸­çš„æœªçŸ¥å¯¹è±¡è¿›è¡Œ6DoFå§¿æ€è·Ÿè¸ªå¯¹äºæœºå™¨äººæ“ä½œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç²¾ç¡®çš„æ·±åº¦ä¿¡æ¯ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­è·å–å¹¶ä¸å®¹æ˜“ã€‚è™½ç„¶å¯ä»¥ä½¿ç”¨æ·±åº¦ä¼°è®¡ç®—æ³•ï¼Œä½†å‡ ä½•ä¸å‡†ç¡®æ€§ä¼šå¯¼è‡´åŸºäºRGBDçš„å§¿æ€è·Ÿè¸ªæ–¹æ³•å¤±è´¥ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†GSGTrackï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºRGBçš„å§¿æ€è·Ÿè¸ªæ¡†æ¶ï¼Œå®ƒè”åˆä¼˜åŒ–å‡ ä½•å’Œå§¿æ€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨3Dé«˜æ–¯å±•å¸ƒæŠ€æœ¯æ¥åˆ›å»ºä¸€ä¸ªå¯ä¼˜åŒ–çš„3Dè¡¨ç¤ºï¼Œè¿™æ˜¯é€šè¿‡ä¸åŸºäºå›¾çš„å‡ ä½•ä¼˜åŒ–åŒæ—¶å­¦ä¹ æ¥æ•æ‰å¯¹è±¡çš„å¤–è§‚ç‰¹å¾å¹¶ç»†åŒ–å…¶å‡ ä½•å½¢çŠ¶ã€‚ç„¶è€Œï¼Œè”åˆä¼˜åŒ–è¿‡ç¨‹å®¹æ˜“å—åˆ°å™ªå£°å§¿æ€å’Œå‡ ä½•æ•°æ®å¼•èµ·çš„æ‰°åŠ¨å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹è±¡è½®å»“æŸå¤±æ¥è§£å†³è·Ÿè¸ªè¿‡ç¨‹ä¸­åƒç´ çº§æŸå¤±å¯¹å§¿æ€å™ªå£°è¿‡äºæ•æ„Ÿçš„é—®é¢˜ã€‚ä¸ºäº†å‡è½»ç”±ä¸å‡†ç¡®çš„æ·±åº¦ä¿¡æ¯å¼•èµ·çš„å‡ ä½•æ¨¡ç³Šæ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‡ ä½•ä¸€è‡´æ€§å›¾åƒå¯¹é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è¿‡æ»¤æ‰ä½ç½®ä¿¡åº¦çš„å¯¹ï¼Œå¹¶ç¡®ä¿ç¨³å¥çš„å‡ ä½•ä¼˜åŒ–ã€‚åœ¨OnePoseå’ŒHO3Dæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGSGTrackåœ¨6DoFå§¿æ€è·Ÿè¸ªå’Œå¯¹è±¡é‡å»ºæ–¹é¢éƒ½éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02267v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†GSGTrackè¿™ä¸€æ–°å‹çš„åŸºäºRGBçš„è§†é¢‘åºåˆ—ä¸­ç‰©ä½“å§¿æ€è·Ÿè¸ªæ–¹æ³•çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ— æ³•è·å¾—å‡†ç¡®çš„æ·±åº¦ä¿¡æ¯æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¯¥é—®é¢˜ï¼Œè¯¥ç ”ç©¶é€šè¿‡é‡‡ç”¨è”åˆä¼˜åŒ–çš„æ–¹å¼èåˆäº†ç‰©ä½“å‡ ä½•ä¿¡æ¯ä¸å§¿æ€æ•°æ®ã€‚ç ”ç©¶æå‡ºåˆ©ç”¨åŸºäºå›¾è®ºçš„å‡ ä½•ä¼˜åŒ–æ–¹æ³•ä¸ä¸‰ç»´é«˜æ–¯è´´å›¾æŠ€æœ¯ï¼Œå…±åŒæ„å»ºå¯ä¼˜åŒ–çš„ä¸‰ç»´è¡¨ç¤ºæ¨¡å‹ã€‚åŒæ—¶ï¼Œå¼•å…¥å¯¹è±¡è½®å»“æŸå¤±æ¥åº”å¯¹å™ªå£°æ•°æ®å¸¦æ¥çš„å¹²æ‰°é—®é¢˜ï¼Œå¹¶é€šè¿‡å‡ ä½•ä¸€è‡´æ€§å›¾åƒé…å¯¹ç­–ç•¥æå‡å‡ ä½•ä¼˜åŒ–çš„ç¨³å¥æ€§ã€‚åœ¨OnePoseå’ŒHO3Dæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒGSGTrackå¯¹äºå…­è‡ªç”±åº¦ï¼ˆ6DoFï¼‰çš„ç‰©ä½“å§¿æ€è·Ÿè¸ªå’Œé‡å»ºæ•ˆæœå“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ol>
<li>GSGTrackè§£å†³äº†åœ¨ç¼ºä¹å‡†ç¡®æ·±åº¦ä¿¡æ¯æƒ…å†µä¸‹ï¼ŒRGBè§†é¢‘åºåˆ—ä¸­ç‰©ä½“å§¿æ€è·Ÿè¸ªçš„éš¾é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨è”åˆä¼˜åŒ–ç­–ç•¥ï¼Œèåˆç‰©ä½“çš„å‡ ä½•ä¿¡æ¯ä¸å§¿æ€æ•°æ®ã€‚</li>
<li>åˆ©ç”¨ä¸‰ç»´é«˜æ–¯è´´å›¾æŠ€æœ¯åˆ›å»ºå¯ä¼˜åŒ–çš„ä¸‰ç»´è¡¨ç¤ºæ¨¡å‹ã€‚</li>
<li>å›¾è®ºå‡ ä½•ä¼˜åŒ–æ–¹æ³•ç”¨äºæ•æ‰ç‰©ä½“çš„å¤–è§‚ç‰¹å¾å¹¶ç²¾ç»†è°ƒæ•´å…¶å‡ ä½•å½¢æ€ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¯¹è±¡è½®å»“æŸå¤±æ¥è§£å†³å› å™ªå£°å¼•èµ·çš„æ•æ„Ÿé—®é¢˜ã€‚</li>
<li>å‡ ä½•ä¸€è‡´æ€§å›¾åƒé…å¯¹ç­–ç•¥å¯æé«˜å‡ ä½•ä¼˜åŒ–çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91c32613c5facc54b83cb0f914e8b03e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96a692f9b640ca6209896d7f5b355340.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c407f60c6cb8e5c938532ed306282de.jpg" align="middle">
</details>




<h2 id="GSOT3D-Towards-Generic-3D-Single-Object-Tracking-in-the-Wild"><a href="#GSOT3D-Towards-Generic-3D-Single-Object-Tracking-in-the-Wild" class="headerlink" title="GSOT3D: Towards Generic 3D Single Object Tracking in the Wild"></a>GSOT3D: Towards Generic 3D Single Object Tracking in the Wild</h2><p><strong>Authors:Yifan Jiao, Yunhao Li, Junhua Ding, Qing Yang, Song Fu, Heng Fan, Libo Zhang</strong></p>
<p>In this paper, we present a novel benchmark, GSOT3D, that aims at facilitating development of generic 3D single object tracking (SOT) in the wild. Specifically, GSOT3D offers 620 sequences with 123K frames, and covers a wide selection of 54 object categories. Each sequence is offered with multiple modalities, including the point cloud (PC), RGB image, and depth. This allows GSOT3D to support various 3D tracking tasks, such as single-modal 3D SOT on PC and multi-modal 3D SOT on RGB-PC or RGB-D, and thus greatly broadens research directions for 3D object tracking. To provide highquality per-frame 3D annotations, all sequences are labeled manually with multiple rounds of meticulous inspection and refinement. To our best knowledge, GSOT3D is the largest benchmark dedicated to various generic 3D object tracking tasks. To understand how existing 3D trackers perform and to provide comparisons for future research on GSOT3D, we assess eight representative point cloud-based tracking models. Our evaluation results exhibit that these models heavily degrade on GSOT3D, and more efforts are required for robust and generic 3D object tracking. Besides, to encourage future research, we present a simple yet effective generic 3D tracker, named PROT3D, that localizes the target object via a progressive spatial-temporal network and outperforms all current solutions by a large margin. By releasing GSOT3D, we expect to advance further 3D tracking in future research and applications. Our benchmark and model as well as the evaluation results will be publicly released at our webpage <a target="_blank" rel="noopener" href="https://github.com/ailovejinx/GSOT3D">https://github.com/ailovejinx/GSOT3D</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•GSOT3Dï¼Œæ—¨åœ¨æ¨åŠ¨é‡ç”Ÿç¯å¢ƒä¸­é€šç”¨ä¸‰ç»´å•ç›®æ ‡è·Ÿè¸ªï¼ˆSOTï¼‰çš„å‘å±•ã€‚å…·ä½“æ¥è¯´ï¼ŒGSOT3Dæä¾›äº†620ä¸ªåºåˆ—ï¼ŒåŒ…å«12.3ä¸‡ä¸ªæ¡†æ¶ï¼Œå¹¶æ¶µç›–äº†54ä¸ªå¯¹è±¡ç±»åˆ«ã€‚æ¯ä¸ªåºåˆ—éƒ½æä¾›å¤šç§æ¨¡å¼ï¼ŒåŒ…æ‹¬ç‚¹äº‘ï¼ˆPCï¼‰ã€RGBå›¾åƒå’Œæ·±åº¦ã€‚è¿™ä½¿å¾—GSOT3Dèƒ½å¤Ÿæ”¯æŒå„ç§ä¸‰ç»´è·Ÿè¸ªä»»åŠ¡ï¼Œå¦‚PCä¸Šçš„å•æ¨¡æ€ä¸‰ç»´SOTå’ŒRGB-PCæˆ–RGB-Dä¸Šçš„å¤šæ¨¡æ€ä¸‰ç»´SOTï¼Œä»è€Œå¤§å¤§æ‹“å®½äº†ä¸‰ç»´ç›®æ ‡è·Ÿè¸ªçš„ç ”ç©¶æ–¹å‘ã€‚ä¸ºäº†æä¾›é«˜è´¨é‡çš„é€å¸§ä¸‰ç»´æ³¨é‡Šï¼Œæ‰€æœ‰åºåˆ—éƒ½æ˜¯ç»è¿‡å¤šè½®ç²¾ç»†æ£€æŸ¥å’Œç²¾ç»†å¤„ç†çš„æ‰‹å·¥æ ‡æ³¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒGSOT3Dæ˜¯ä¸“é—¨ç”¨äºå„ç§é€šç”¨ä¸‰ç»´ç›®æ ‡è·Ÿè¸ªä»»åŠ¡çš„æœ€å¤§åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†äº†è§£ç°æœ‰ä¸‰ç»´è·Ÿè¸ªå™¨çš„æ€§èƒ½ï¼Œå¹¶ä¸ºGSOT3Dä¸Šçš„æœªæ¥ç ”ç©¶æä¾›æ¯”è¾ƒï¼Œæˆ‘ä»¬å¯¹å…«ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„ç‚¹äº‘è·Ÿè¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨GSOT3Dä¸Šä¸¥é‡é€€åŒ–ï¼Œéœ€è¦æ›´å¤šçš„åŠªåŠ›æ¥å®ç°ç¨³å¥å’Œé€šç”¨çš„ä¸‰ç»´ç›®æ ‡è·Ÿè¸ªã€‚æ­¤å¤–ï¼Œä¸ºäº†é¼“åŠ±æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„é€šç”¨ä¸‰ç»´è·Ÿè¸ªå™¨PROT3Dï¼Œå®ƒé€šè¿‡æ¸è¿›æ—¶ç©ºç½‘ç»œå®šä½ç›®æ ‡å¯¹è±¡ï¼Œå¹¶å¤§å¹…åº¦è¶…è¶Šå½“å‰æ‰€æœ‰è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å‘å¸ƒGSOT3Dï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨ä¸­è¿›ä¸€æ­¥æ¨åŠ¨ä¸‰ç»´è·Ÿè¸ªçš„å‘å±•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å‹å’Œè¯„ä¼°ç»“æœå°†åœ¨æˆ‘ä»¬çš„ç½‘é¡µ<a target="_blank" rel="noopener" href="https://github.com/ailovejinx/GSOT3D">https://github.com/ailovejinx/GSOT3D</a>ä¸Šå…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02129v1">PDF</a> 14 pages, 12 figures</p>
<p><strong>Summary</strong><br>GSOT3Dæ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ä¸‰ç»´å•ç›®æ ‡è·Ÿè¸ªï¼ˆSOTï¼‰çš„æ–°åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§æ¨¡æ€æ•°æ®ï¼Œå¦‚ç‚¹äº‘ã€RGBå›¾åƒå’Œæ·±åº¦ä¿¡æ¯ã€‚å®ƒåŒ…å«å¤§é‡æ‰‹åŠ¨æ ‡æ³¨çš„ä¸‰ç»´æ•°æ®åºåˆ—ï¼Œå¹¶æ”¯æŒå¤šç§ä¸‰ç»´è·Ÿè¸ªä»»åŠ¡ã€‚å½“å‰è·Ÿè¸ªæ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰å¾…æé«˜ï¼Œè®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨ä¸‰ç»´è·Ÿè¸ªå™¨PROT3Dï¼Œå…¶åœ¨è¯¥æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚æ•°æ®é›†åŠè¯„ä¼°ç»“æœå‡å·²å…¬å¼€å‘å¸ƒåœ¨GitHubé¡µé¢ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GSOT3Dæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæ¨åŠ¨é€šç”¨ä¸‰ç»´å•ç›®æ ‡è·Ÿè¸ªï¼ˆSOTï¼‰çš„å‘å±•ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šç§æ¨¡æ€æ•°æ®ï¼Œå¦‚ç‚¹äº‘ã€RGBå›¾åƒå’Œæ·±åº¦ä¿¡æ¯ï¼Œè¦†ç›–å¤šç§ä¸‰ç»´è·Ÿè¸ªä»»åŠ¡ã€‚</li>
<li>å½“å‰è·Ÿè¸ªæ¨¡å‹åœ¨GSOT3Dä¸Šçš„æ€§èƒ½ä¸è¶³ï¼Œéœ€è¦æ›´å¤šåŠªåŠ›è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>PROT3Dæ˜¯ä¸€ç§æ–°çš„é€šç”¨ä¸‰ç»´è·Ÿè¸ªå™¨ï¼Œé€šè¿‡æ¸è¿›æ—¶ç©ºç½‘ç»œå®šä½ç›®æ ‡ç‰©ä½“ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49166edeed72dda9e33f0138582173ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd0ec7f6f468e57d82ef7b15f746a26f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-772854c58ac8b2e810111917184c4961.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ef5e305ec1120c54ad850b8d8e18738b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b1844c84f0e90c82010b4608659632.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-600f88744ff0bccd66ef4516b4184d1d.jpg" align="middle">
</details>




<h2 id="Epipolar-Attention-Field-Transformers-for-Birdâ€™s-Eye-View-Semantic-Segmentation"><a href="#Epipolar-Attention-Field-Transformers-for-Birdâ€™s-Eye-View-Semantic-Segmentation" class="headerlink" title="Epipolar Attention Field Transformers for Birdâ€™s Eye View Semantic   Segmentation"></a>Epipolar Attention Field Transformers for Birdâ€™s Eye View Semantic   Segmentation</h2><p><strong>Authors:Christian Witte, Jens Behley, Cyrill Stachniss, Marvin Raaijmakers</strong></p>
<p>Spatial understanding of the semantics of the surroundings is a key capability needed by autonomous cars to enable safe driving decisions. Recently, purely vision-based solutions have gained increasing research interest. In particular, approaches extracting a birdâ€™s eye view (BEV) from multiple cameras have demonstrated great performance for spatial understanding. This paper addresses the dependency on learned positional encodings to correlate image and BEV feature map elements for transformer-based methods. We propose leveraging epipolar geometric constraints to model the relationship between cameras and the BEV by Epipolar Attention Fields. They are incorporated into the attention mechanism as a novel attribution term, serving as an alternative to learned positional encodings. Experiments show that our method EAFormer outperforms previous BEV approaches by 2% mIoU for map semantic segmentation and exhibits superior generalization capabilities compared to implicitly learning the camera configuration. </p>
<blockquote>
<p>å‘¨å›´ç¯å¢ƒçš„è¯­ä¹‰ç©ºé—´ç†è§£æ˜¯è‡ªåŠ¨é©¾é©¶æ±½è½¦åšå‡ºå®‰å…¨é©¾é©¶å†³ç­–æ‰€éœ€çš„å…³é”®èƒ½åŠ›ã€‚æœ€è¿‘ï¼Œçº¯è§†è§‰è§£å†³æ–¹æ¡ˆå·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…´è¶£ã€‚ç‰¹åˆ«æ˜¯ï¼Œä»å¤šä¸ªæ‘„åƒå¤´æå–é¸Ÿç°å›¾ï¼ˆBEVï¼‰çš„æ–¹æ³•åœ¨ç©ºé—´ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚æœ¬æ–‡è§£å†³äº†åŸºäºå˜å‹å™¨çš„æ–¹æ³•å°†å›¾åƒå’ŒBEVç‰¹å¾æ˜ å°„å…ƒç´ ç›¸å…³è”æ‰€ä¾èµ–çš„å­¦ä¹ ä½ç½®ç¼–ç çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºåˆ©ç”¨æå‡ ä½•çº¦æŸæ¥æ¨¡æ‹Ÿæ‘„åƒå¤´å’ŒBEVä¹‹é—´çš„å…³ç³»ï¼Œå³é‡‡ç”¨ææ³¨æ„åŠ›é¢†åŸŸï¼ˆEpipolar Attention Fieldsï¼‰ã€‚å®ƒä»¬è¢«èå…¥æ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºä¸€ä¸ªæ–°çš„å±æ€§é¡¹ï¼Œä½œä¸ºå¯¹å­¦ä¹ ä½ç½®ç¼–ç çš„æ›¿ä»£ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„EAFormeræ–¹æ³•åœ¨åœ°å›¾è¯­ä¹‰åˆ†å‰²ä¸Šçš„mIoUæ€§èƒ½ä¼˜äºä¹‹å‰çš„BEVæ–¹æ³•ï¼Œå¹¶ä¸”ç›¸è¾ƒäºéšå¼å­¦ä¹ ç›¸æœºé…ç½®å±•ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01595v1">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªä¸»é©¾é©¶æ±½è½¦çš„ç©ºé—´è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å¤šç›¸æœºæå–é¸Ÿç°å›¾ï¼ˆBEVï¼‰çš„æ–¹æ³•ã€‚æ–‡ç« é’ˆå¯¹åŸºäºå˜æ¢å™¨çš„æ–¹æ³•å¯¹ä½ç½®ç¼–ç çš„ä¾èµ–é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨æå‡ ä½•çº¦æŸå»ºç«‹ç›¸æœºä¸é¸Ÿç°å›¾ä¹‹é—´çš„å…³ç³»çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œå®ç°äº†åœ°å›¾è¯­ä¹‰åˆ†å‰²çš„æ›´é«˜å‡†ç¡®åº¦ã€‚æ–‡ç« çš„é‡ç‚¹æ˜¯å¼€å‘å‡ºäº†ä¸€ç§ç§°ä¸ºâ€œEAformerâ€çš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥ä½œä¸ºç°æœ‰æ–¹æ¡ˆçš„æ”¹è¿›å‡çº§é€‰é¡¹ã€‚æ–‡ç« å…³æ³¨çº¯ç²¹çš„è§†è§‰è§£å†³æ–¹æ¡ˆæ¥æé«˜è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ä¸­çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶å…³æ³¨äº†åŸºäºæå‡ ä½•çº¦æŸå»ºç«‹æ‘„åƒæœºä¸é¸Ÿç°è§†å›¾ä¹‹é—´çš„æ–°æ–¹æ³•å…³ç³»ï¼Œè€Œä¸æ˜¯ä¾èµ–å­¦ä¹ ä½ç½®ç¼–ç ã€‚å®éªŒç»“æœå±•ç¤ºäº†æ–°æ–¹æ³•åœ¨æé«˜åœ°å›¾è¯­ä¹‰åˆ†å‰²ä¸Šçš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå¹¶å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥ç ”ç©¶çš„æˆæœæœ‰æœ›ä¸ºè‡ªä¸»é©¾é©¶æ±½è½¦æä¾›æ›´å¼ºå¤§çš„ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œä»è€Œæé«˜é©¾é©¶å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>è‡ªä¸»é©¾é©¶æ±½è½¦éœ€è¦ç©ºé—´è¯­ä¹‰ç†è§£èƒ½åŠ›ä»¥ç¡®ä¿å®‰å…¨é©¾é©¶å†³ç­–ã€‚</li>
<li>åŸºäºè§†è§‰çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯ä»å¤šä¸ªç›¸æœºä¸­æå–é¸Ÿç°å›¾ï¼ˆBEVï¼‰çš„æ–¹æ³•ï¼Œå¯¹äºç©ºé—´ç†è§£è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cb5f305ed3c91d072ae28a7b73702a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7be999486c7f08d5ab563200de14f1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-482d59ef60e965ed4dfb0be2eb6d9bea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc2e03122ac4a64fc81b6d274e2f6122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1703ad865d7af0ff8024e3d58e3daa4d.jpg" align="middle">
</details>




<h2 id="Divide-and-Conquer-Confluent-Triple-Flow-Network-for-RGB-T-Salient-Object-Detection"><a href="#Divide-and-Conquer-Confluent-Triple-Flow-Network-for-RGB-T-Salient-Object-Detection" class="headerlink" title="Divide-and-Conquer: Confluent Triple-Flow Network for RGB-T Salient   Object Detection"></a>Divide-and-Conquer: Confluent Triple-Flow Network for RGB-T Salient   Object Detection</h2><p><strong>Authors:Hao Tang, Zechao Li, Dong Zhang, Shengfeng He, Jinhui Tang</strong></p>
<p>RGB-Thermal Salient Object Detection aims to pinpoint prominent objects within aligned pairs of visible and thermal infrared images. Traditional encoder-decoder architectures, while designed for cross-modality feature interactions, may not have adequately considered the robustness against noise originating from defective modalities. Inspired by hierarchical human visual systems, we propose the ConTriNet, a robust Confluent Triple-Flow Network employing a Divide-and-Conquer strategy. Specifically, ConTriNet comprises three flows: two modality-specific flows explore cues from RGB and Thermal modalities, and a third modality-complementary flow integrates cues from both modalities. ConTriNet presents several notable advantages. It incorporates a Modality-induced Feature Modulator in the modality-shared union encoder to minimize inter-modality discrepancies and mitigate the impact of defective samples. Additionally, a foundational Residual Atrous Spatial Pyramid Module in the separated flows enlarges the receptive field, allowing for the capture of multi-scale contextual information. Furthermore, a Modality-aware Dynamic Aggregation Module in the modality-complementary flow dynamically aggregates saliency-related cues from both modality-specific flows. Leveraging the proposed parallel triple-flow framework, we further refine saliency maps derived from different flows through a flow-cooperative fusion strategy, yielding a high-quality, full-resolution saliency map for the final prediction. To evaluate the robustness and stability of our approach, we collect a comprehensive RGB-T SOD benchmark, VT-IMAG, covering various real-world challenging scenarios. Extensive experiments on public benchmarks and our VT-IMAG dataset demonstrate that ConTriNet consistently outperforms state-of-the-art competitors in both common and challenging scenarios. </p>
<blockquote>
<p>RGB-çƒ­æ˜¾è‘—ç›®æ ‡æ£€æµ‹æ—¨åœ¨ç¡®å®šå¯è§å’Œçº¢å¤–çƒ­å›¾åƒé…å¯¹ä¸­çš„çªå‡ºç›®æ ‡ã€‚ä¼ ç»Ÿçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„è™½ç„¶æ—¨åœ¨å®ç°è·¨æ¨¡æ€ç‰¹å¾äº¤äº’ï¼Œä½†å¯èƒ½æœªèƒ½å……åˆ†è€ƒè™‘åˆ°æ¥è‡ªç¼ºé™·æ¨¡æ€çš„å™ªå£°çš„é²æ£’æ€§ã€‚å—åˆ†å±‚äººç±»è§†è§‰ç³»ç»Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ConTriNetï¼Œè¿™æ˜¯ä¸€ä¸ªç¨³å¥çš„æ±‡èšä¸‰æµç½‘ç»œï¼Œé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼ŒConTriNetåŒ…å«ä¸‰ä¸ªæµï¼šä¸¤ä¸ªæ¨¡æ€ç‰¹å®šæµæ¢ç´¢RGBå’Œçƒ­æ¨¡æ€çš„çº¿ç´¢ï¼Œç¬¬ä¸‰ä¸ªæ¨¡æ€äº’è¡¥æµæ•´åˆä¸¤ç§æ¨¡æ€çš„çº¿ç´¢ã€‚ConTriNetå…·æœ‰å‡ ä¸ªæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚å®ƒåœ¨æ¨¡æ€å…±äº«è”åˆç¼–ç å™¨ä¸­å¼•å…¥äº†æ¨¡æ€è¯±å¯¼ç‰¹å¾è°ƒåˆ¶å™¨ï¼Œä»¥æœ€å°åŒ–æ¨¡æ€ä¹‹é—´çš„å·®å¼‚å¹¶å‡è½»ç¼ºé™·æ ·æœ¬çš„å½±å“ã€‚æ­¤å¤–ï¼Œåˆ†ç¦»æµä¸­çš„åŸºç¡€æ®‹å·®ç©ºæ´ç©ºé—´é‡‘å­—å¡”æ¨¡å—æ‰©å¤§äº†æ„Ÿå—é‡ï¼Œèƒ½å¤Ÿæ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæ¨¡æ€äº’è¡¥æµä¸­çš„æ¨¡æ€æ„ŸçŸ¥åŠ¨æ€èšåˆæ¨¡å—åŠ¨æ€èšåˆæ¥è‡ªç‰¹å®šæ¨¡æ€æµçš„æ˜¾è‘—æ€§ç›¸å…³çº¿ç´¢ã€‚åˆ©ç”¨æ‰€æå‡ºçš„å¹¶è¡Œä¸‰æµæ¡†æ¶ï¼Œæˆ‘ä»¬é€šè¿‡ååŒæµèåˆç­–ç•¥è¿›ä¸€æ­¥ç»†åŒ–ä»ä¸åŒæµæ´¾å¯¼å‡ºçš„æ˜¾è‘—æ€§å›¾ï¼Œä»è€Œäº§ç”Ÿé«˜è´¨é‡ã€å…¨åˆ†è¾¨ç‡æ˜¾è‘—æ€§å›¾ç”¨äºæœ€ç»ˆé¢„æµ‹ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„é²æ£’æ€§å’Œç¨³å®šæ€§ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå…¨é¢çš„RGB-T SODåŸºå‡†ï¼ŒVT-IMAGï¼Œæ¶µç›–äº†å„ç§ç°å®ä¸–ç•Œçš„æŒ‘æˆ˜åœºæ™¯ã€‚åœ¨å…¬å…±åŸºå‡†å’Œæˆ‘ä»¬è‡ªå·±çš„VT-IMAGæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒConTriNetåœ¨æ™®é€šå’Œå…·æœ‰æŒ‘æˆ˜çš„åœºæ™¯ä¸­å‡ä¼˜äºæœ€æ–°ç«äº‰å¯¹æ‰‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01556v1">PDF</a> Accepted by IEEE TPAMI. Project page:   <a target="_blank" rel="noopener" href="https://cser-tang-hao.github.io/contrinet.html">https://cser-tang-hao.github.io/contrinet.html</a></p>
<p><strong>Summary</strong>ï¼š<br>RGB-Tæ˜¾è‘—ç›®æ ‡æ£€æµ‹æ—¨åœ¨è¯†åˆ«å¯è§å…‰ä¸çƒ­çº¢å¤–å›¾åƒå¯¹é½åçš„çªå‡ºç‰©ä½“ã€‚é’ˆå¯¹ä¼ ç»Ÿç¼–ç è§£ç æ¶æ„åœ¨å¤„ç†è·¨æ¨¡æ€ç‰¹å¾äº¤äº’æ—¶æœªå……åˆ†è€ƒè™‘åˆ°å™ªå£°å¹²æ‰°çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå±‚æ¬¡äººç±»è§†è§‰ç³»ç»Ÿçš„ConTriNetæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥ï¼ŒåŒ…å«ä¸‰ä¸ªåˆ†æ”¯ï¼šä¸¤ä¸ªæ¨¡æ€ç‰¹å®šåˆ†æ”¯åˆ†åˆ«æ¢ç´¢RGBå’Œçƒ­æ¨¡æ€çš„çº¿ç´¢ï¼Œç¬¬ä¸‰ä¸ªæ¨¡æ€äº’è¡¥åˆ†æ”¯æ•´åˆä¸¤ä¸ªæ¨¡æ€çš„çº¿ç´¢ã€‚ConTriNetå…·å¤‡å¤šä¼˜åŠ¿ï¼šåœ¨å…±äº«è”åˆç¼–ç å™¨ä¸­åŠ å…¥æ¨¡æ€è¯±å¯¼ç‰¹å¾è°ƒåˆ¶å™¨ï¼Œå‡å°‘æ¨¡æ€é—´å·®å¼‚å¹¶å‡è½»ç¼ºé™·æ ·æœ¬çš„å½±å“ï¼›åˆ†ç¦»çš„æµä¸­çš„åŸºç¡€æ®‹å·®ç©ºæ´é‡‘å­—å¡”æ¨¡å—æ‰©å¤§äº†æ„Ÿå—é‡ï¼Œæ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›æ¨¡æ€æ„ŸçŸ¥åŠ¨æ€èšåˆæ¨¡å—åœ¨æ¨¡æ€äº’è¡¥æµä¸­åŠ¨æ€èšåˆæ¥è‡ªä¸¤ä¸ªæ¨¡æ€ç‰¹å®šæµçš„æ˜¾è‘—æ€§ç›¸å…³çº¿ç´¢ã€‚é€šè¿‡å¹¶è¡Œä¸‰æµæ¡†æ¶ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡æµååŒèåˆç­–ç•¥ä¼˜åŒ–æ¥è‡ªä¸åŒæµçš„æ˜¾è‘—æ€§å›¾ï¼Œä»è€Œå¾—åˆ°é«˜è´¨é‡çš„å…¨åˆ†è¾¨ç‡æ˜¾è‘—æ€§å›¾ç”¨äºæœ€ç»ˆé¢„æµ‹ã€‚æˆ‘ä»¬æ”¶é›†äº†å…¨é¢çš„RGB-Tæ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹åŸºå‡†æ•°æ®é›†VT-IMAGï¼Œæ¶µç›–å„ç§ç°å®æŒ‘æˆ˜åœºæ™¯ã€‚åœ¨å…¬å…±åŸºå‡†æ•°æ®é›†å’Œæˆ‘ä»¬çš„VT-IMAGæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒConTriNetåœ¨å¸¸è§„å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å‡è¡¨ç°ä¼˜äºæœ€æ–°ç«äº‰å¯¹æ‰‹ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>RGB-Thermal Salient Object Detection (RGB-TSOD)æ—¨åœ¨è¯†åˆ«å¯è§å…‰å’Œçƒ­çº¢å¤–å›¾åƒä¸­çš„çªå‡ºç‰©ä½“ã€‚</li>
<li>æå‡ºConTriNetæ¨¡å‹ï¼Œé‡‡ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥ï¼ŒåŒ…å«ä¸‰ä¸ªæµï¼šä¸¤ä¸ªæ¨¡æ€ç‰¹å®šæµå’Œä¸€ä¸ªæ¨¡æ€äº’è¡¥æµã€‚</li>
<li>ConTriNetåˆ©ç”¨æ¨¡æ€è¯±å¯¼ç‰¹å¾è°ƒåˆ¶å™¨å‡å°‘æ¨¡æ€é—´å·®å¼‚ï¼Œå¹¶å‡è½»ç¼ºé™·æ ·æœ¬çš„å½±å“ã€‚</li>
<li>ConTriNeté€šè¿‡æ‰©å¤§çš„æ„Ÿå—é‡æ•è·å¤šå°ºåº¦ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>æ¨¡æ€æ„ŸçŸ¥åŠ¨æ€èšåˆæ¨¡å—åŠ¨æ€èšåˆæ¥è‡ªä¸¤ä¸ªæ¨¡æ€çš„æ˜¾è‘—æ€§çº¿ç´¢ã€‚</li>
<li>é€šè¿‡å¹¶è¡Œä¸‰æµæ¡†æ¶å’ŒæµååŒèåˆç­–ç•¥ä¼˜åŒ–æ˜¾è‘—æ€§å›¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e54962717cec2158d9c2658c34971b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fdc530d860a04610ff8b302c6e081068.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f424a31438ca3a2ba6cadf41b210e6f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc1c219a06f9236ca93d8460cfccd474.jpg" align="middle">
</details>




<h2 id="Improving-Object-Detection-by-Modifying-Synthetic-Data-with-Explainable-AI"><a href="#Improving-Object-Detection-by-Modifying-Synthetic-Data-with-Explainable-AI" class="headerlink" title="Improving Object Detection by Modifying Synthetic Data with Explainable   AI"></a>Improving Object Detection by Modifying Synthetic Data with Explainable   AI</h2><p><strong>Authors:Nitish Mital, Simon Malzard, Richard Walters, Celso M. De Melo, Raghuveer Rao, Victoria Nockles</strong></p>
<p>In many computer vision domains the collection of sufficient real-world data is challenging and can severely impact model performance, particularly when running inference on samples that are unseen or underrepresented in training. Synthetically generated images provide a promising solution, but it remains unclear how to design synthetic data to optimally improve model performance, for example whether to introduce more realism or more abstraction in such datasets. Here we propose a novel conceptual approach to improve the performance of computer vision models trained on synthetic images, by using robust Explainable AI (XAI) techniques to guide the modification of 3D models used to generate these images. Importantly, this framework allows both modifications that increase and decrease realism in synthetic data, which can both improve model performance. We illustrate this concept using a real-world example where data are sparse; the detection of vehicles in infrared imagery. We fine-tune an initial YOLOv8 model on the ATR DSIAC infrared dataset and synthetic images generated from 3D mesh models in the Unity gaming engine, and then use XAI saliency maps to guide modification of our Unity models. We show that synthetic data can improve detection of vehicles in orientations unseen in training by 4.6% (to mAP50 scores of 94.6%). We further improve performance by an additional 1.5% (to 96.1%) through our new XAI-guided approach, which reduces misclassifications through both increasing and decreasing the realism of different parts of the synthetic data. These proof-of-concept results pave the way for fine, XAI-controlled curation of synthetic datasets through detailed feature modifications, tailored to improve object detection performance. </p>
<blockquote>
<p>åœ¨è®¸å¤šè®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œæ”¶é›†è¶³å¤Ÿçš„çœŸå®ä¸–ç•Œæ•°æ®æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå¹¶å¯èƒ½ä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§æˆ–ä»£è¡¨æ€§ä¸è¶³çš„æ ·æœ¬è¿›è¡Œæ¨ç†æ—¶ã€‚åˆæˆå›¾åƒæä¾›äº†ä¸€ç§å¾ˆæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å¦‚ä½•è®¾è®¡åˆæˆæ•°æ®ä»¥æœ€ä¼˜æ–¹å¼æé«˜æ¨¡å‹æ€§èƒ½ä»ä¸æ¸…æ¥šï¼Œä¾‹å¦‚ï¼Œæ˜¯åœ¨è¿™äº›æ•°æ®é›†ä¸­å¼•å…¥æ›´å¤šçš„ç°å®ä¸»ä¹‰è¿˜æ˜¯æ›´å¤šçš„æŠ½è±¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè®¡ç®—æœºè§†è§‰æ¨¡å‹è®­ç»ƒçš„æ–°å‹æ¦‚å¿µæ–¹æ³•ï¼Œåˆ©ç”¨ç¨³å¥çš„å¯è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æŠ€æœ¯æ¥æŒ‡å¯¼ç”Ÿæˆè¿™äº›å›¾åƒæ‰€ä½¿ç”¨çš„3Dæ¨¡å‹çš„ä¿®æ”¹ã€‚é‡è¦çš„æ˜¯ï¼Œè¯¥æ¡†æ¶å…è®¸å¢åŠ æˆ–å‡å°‘åˆæˆæ•°æ®ä¸­çš„é€¼çœŸåº¦ï¼Œè¿™éƒ½å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨ç°å®ä¸–ç•Œæ•°æ®ç¨€ç–çš„ä¾‹å­â€”â€”çº¢å¤–å›¾åƒä¸­çš„è½¦è¾†æ£€æµ‹æ¥é˜é‡Šè¿™ä¸€æ¦‚å¿µã€‚æˆ‘ä»¬å¯¹åˆå§‹çš„YOLOv8æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ATR DSIACçº¢å¤–æ•°æ®é›†å’Œç”±Unityæ¸¸æˆå¼•æ“ä¸­çš„3Dç½‘æ ¼æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒï¼Œç„¶åä½¿ç”¨XAIæ˜¾è‘—æ€§å›¾æ¥æŒ‡å¯¼æˆ‘ä»¬çš„Unityæ¨¡å‹çš„ä¿®æ”¹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåˆæˆæ•°æ®å¯ä»¥æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§æ–¹å‘çš„è½¦è¾†æ£€æµ‹æ€§èƒ½ï¼Œæé«˜4.6%ï¼ˆè¾¾åˆ°mAP50åˆ†æ•°ä¸º94.6%ï¼‰ã€‚é€šè¿‡æˆ‘ä»¬çš„æ–°å‹XAIå¼•å¯¼æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå†æé«˜1.5%ï¼ˆè¾¾åˆ°96.1%ï¼‰ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¢åŠ å’Œå‡å°‘åˆæˆæ•°æ®ä¸åŒéƒ¨åˆ†çš„é€¼çœŸåº¦æ¥å‡å°‘è¯¯åˆ†ç±»ã€‚è¿™äº›æ¦‚å¿µéªŒè¯ç»“æœå¼€è¾Ÿäº†é€šè¿‡è¯¦ç»†ç‰¹å¾ä¿®æ”¹ç²¾ç»†æ§åˆ¶XAIåˆæˆæ•°æ®é›†çš„é“è·¯ï¼Œä»¥æ”¹å–„ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01477v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¯è§£é‡Šçš„AIï¼ˆXAIï¼‰æŠ€æœ¯æ”¹è¿›è®¡ç®—æœºè§†è§‰æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆæˆå›¾åƒæ•°æ®é›†ä¸Šã€‚é€šè¿‡XAIæŠ€æœ¯å¼•å¯¼å¯¹ç”¨äºç”Ÿæˆå›¾åƒçš„3Dæ¨¡å‹çš„ä¿®æ”¹ï¼Œè¯¥æ¡†æ¶å…è®¸å¢åŠ æˆ–å‡å°‘åˆæˆæ•°æ®çš„é€¼çœŸåº¦ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨è½¦è¾†çº¢å¤–å›¾åƒæ£€æµ‹çš„å®é™…ä¾‹å­ä¸­ï¼Œå±•ç¤ºäº†åˆæˆæ•°æ®å’Œä½¿ç”¨XAIæŒ‡å¯¼çš„ä¿®æ”¹èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨æœªè®­ç»ƒæ–¹å‘ä¸Šçš„è½¦è¾†æ£€æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆå›¾åƒä¸ºè§£å†³è®¡ç®—æœºè§†è§‰é¢†åŸŸæ•°æ®æ”¶é›†éš¾é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä»…é å¢åŠ é€¼çœŸåº¦å¹¶ä¸èƒ½ä¿è¯æ¨¡å‹æ€§èƒ½çš„æœ€ä¼˜æå‡ã€‚</li>
<li>åˆ©ç”¨Explainable AIï¼ˆXAIï¼‰æŠ€æœ¯å¯ä»¥æŒ‡å¯¼åˆæˆæ•°æ®çš„æ”¹è¿›ã€‚</li>
<li>é€šè¿‡è°ƒæ•´åˆæˆæ•°æ®çš„é€¼çœŸåº¦ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨æœªè®­ç»ƒæ–¹å‘ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨è½¦è¾†çº¢å¤–å›¾åƒæ£€æµ‹çš„å®é™…åº”ç”¨ä¸­ï¼Œåˆæˆæ•°æ®å’ŒXAIæŒ‡å¯¼çš„ä¿®æ”¹æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>XAIå¼•å¯¼çš„ç²¾ç»†è°ƒæ•´èƒ½å¤Ÿå‡å°‘è¯¯åˆ†ç±»ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-78aef6a20c187394fc6ac039caf811dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e68d9bceba624c7cc09f31be26aa138.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d404acfa0c241ae7a9637fc2d1ef320a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa64a920569ece626f1fe76c857bbd4c.jpg" align="middle">
</details>




<h2 id="LMSeg-Unleashing-the-Power-of-Large-Scale-Models-for-Open-Vocabulary-Semantic-Segmentation"><a href="#LMSeg-Unleashing-the-Power-of-Large-Scale-Models-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="LMSeg: Unleashing the Power of Large-Scale Models for Open-Vocabulary   Semantic Segmentation"></a>LMSeg: Unleashing the Power of Large-Scale Models for Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Huadong Tang, Youpeng Zhao, Yan Huang, Min Xu, Jun Wang, Qiang Wu</strong></p>
<p>It is widely agreed that open-vocabulary-based approaches outperform classical closed-set training solutions for recognizing unseen objects in images for semantic segmentation. Existing open-vocabulary approaches leverage vision-language models, such as CLIP, to align visual features with rich semantic features acquired through pre-training on large-scale vision-language datasets. However, the text prompts employed in these methods are short phrases based on fixed templates, failing to capture comprehensive object attributes. Moreover, while the CLIP model excels at exploiting image-level features, it is less effective at pixel-level representation, which is crucial for semantic segmentation tasks. In this work, we propose to alleviate the above-mentioned issues by leveraging multiple large-scale models to enhance the alignment between fine-grained visual features and enriched linguistic features. Specifically, our method employs large language models (LLMs) to generate enriched language prompts with diverse visual attributes for each category, including color, shape&#x2F;size, and texture&#x2F;material. Additionally, for enhanced visual feature extraction, the SAM model is adopted as a supplement to the CLIP visual encoder through a proposed learnable weighted fusion strategy. Built upon these techniques, our method, termed LMSeg, achieves state-of-the-art performance across all major open-vocabulary segmentation benchmarks. The code will be made available soon. </p>
<blockquote>
<p>æ™®éè®¤ä¸ºï¼ŒåŸºäºå¼€æ”¾è¯æ±‡çš„æ–¹æ³•åœ¨è¯­ä¹‰åˆ†å‰²ä¸­è¯†åˆ«æœªè§ç‰©ä½“æ—¶ï¼Œå…¶æ€§èƒ½ä¼˜äºç»å…¸çš„å°é—­é›†è®­ç»ƒè§£å†³æ–¹æ¡ˆã€‚ç°æœ‰çš„å¼€æ”¾è¯æ±‡æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å¯¹é½è§†è§‰ç‰¹å¾ä¸é€šè¿‡å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®é›†é¢„è®­ç»ƒè·å¾—çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸­æ‰€ä½¿ç”¨çš„æ–‡æœ¬æç¤ºæ˜¯åŸºäºå›ºå®šæ¨¡æ¿çš„çŸ­è¯­ï¼Œæ— æ³•æ•æ‰å…¨é¢çš„å¯¹è±¡å±æ€§ã€‚æ­¤å¤–ï¼Œè™½ç„¶CLIPæ¨¡å‹åœ¨åˆ©ç”¨å›¾åƒçº§ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åƒç´ çº§è¡¨ç¤ºæ–¹é¢æ•ˆæœè¾ƒå·®ï¼Œè¿™å¯¹äºè¯­ä¹‰åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡åˆ©ç”¨å¤šä¸ªå¤§è§„æ¨¡æ¨¡å‹æ¥ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œä»¥å¢å¼ºç²¾ç»†è§†è§‰ç‰¹å¾ä¸ä¸°å¯Œè¯­è¨€ç‰¹å¾ä¹‹é—´çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆå…·æœ‰å„ç§è§†è§‰å±æ€§çš„ä¸°å¯Œè¯­è¨€æç¤ºï¼ŒåŒ…æ‹¬é¢œè‰²ã€å½¢çŠ¶&#x2F;å¤§å°ã€çº¹ç†&#x2F;æè´¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºè§†è§‰ç‰¹å¾æå–ï¼Œæˆ‘ä»¬é‡‡ç”¨SAMæ¨¡å‹ä½œä¸ºå¯¹CLIPè§†è§‰ç¼–ç å™¨çš„è¡¥å……ï¼Œé€šè¿‡æå‡ºçš„å¯å­¦ä¹ åŠ æƒèåˆç­–ç•¥ã€‚åŸºäºè¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•â€”â€”LMSegï¼Œåœ¨æ‰€æœ‰ä¸»è¦çš„å¼€æ”¾è¯æ±‡åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å°†å¾ˆå¿«å…¬å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00364v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨å¤šå¤§å‹æ¨¡å‹å¢å¼ºç²¾ç»†è§†è§‰ç‰¹å¾ä¸ä¸°å¯Œè¯­è¨€ç‰¹å¾çš„å¯¹é½ï¼Œè§£å†³ç°æœ‰å¼€æ”¾è¯æ±‡è¡¨æ–¹æ³•åœ¨è¯­ä¹‰åˆ†å‰²ä¸­è¯†åˆ«æœªè§ç‰©ä½“çš„é—®é¢˜ã€‚é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆåŒ…å«å¤šç§è§†è§‰å±æ€§çš„ä¸°å¯Œè¯­è¨€æç¤ºï¼Œå¹¶ç»“åˆSAMæ¨¡å‹å¢å¼ºCLIPè§†è§‰ç¼–ç å™¨çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå®ç°å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¡¨æ–¹æ³•æ™®éä¼˜äºç»å…¸å°é—­é›†è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œç”¨äºè¯†åˆ«å›¾åƒä¸­æœªè§ç‰©ä½“çš„è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>ç°æœ‰å¼€æ”¾è¯æ±‡è¡¨æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å¯¹é½è§†è§‰ç‰¹å¾å’Œé€šè¿‡é¢„è®­ç»ƒè·å¾—ä¸°å¯Œè¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„æ–‡æœ¬æç¤ºåŸºäºå›ºå®šæ¨¡æ¿çš„çŸ­è¯­ï¼Œæ— æ³•æ•æ‰å…¨é¢çš„å¯¹è±¡å±æ€§ã€‚</li>
<li>CLIPæ¨¡å‹åœ¨å›¾åƒçº§ç‰¹å¾åˆ©ç”¨ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨åƒç´ çº§è¡¨ç¤ºä¸Šæ•ˆæœè¾ƒå·®ï¼Œå¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºåˆ©ç”¨å¤šå¤§å‹æ¨¡å‹å¢å¼ºç²¾ç»†è§†è§‰ç‰¹å¾ä¸ä¸°å¯Œè¯­è¨€ç‰¹å¾çš„å¯¹é½ã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”ŸæˆåŒ…å«å„ç§è§†è§‰å±æ€§çš„ä¸°å¯Œè¯­è¨€æç¤ºï¼Œå¦‚é¢œè‰²ã€å½¢çŠ¶&#x2F;å¤§å°ã€çº¹ç†&#x2F;æè´¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-213990aa7bf513aeba72c736a1b1a981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c21531fcf9530d9d1c6e87b33ba793fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0f629b808e725c723a9c81f86de12b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fc7c8cc30befdf109618ada6072b670.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00570716d0a2736c7c8812f2cfae302f.jpg" align="middle">
</details>




<h2 id="SpaRC-Sparse-Radar-Camera-Fusion-for-3D-Object-Detection"><a href="#SpaRC-Sparse-Radar-Camera-Fusion-for-3D-Object-Detection" class="headerlink" title="SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection"></a>SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection</h2><p><strong>Authors:Philipp Wolters, Johannes Gilg, Torben Teepe, Fabian Herzog, Felix Fent, Gerhard Rigoll</strong></p>
<p>In this work, we present SpaRC, a novel Sparse fusion transformer for 3D perception that integrates multi-view image semantics with Radar and Camera point features. The fusion of radar and camera modalities has emerged as an efficient perception paradigm for autonomous driving systems. While conventional approaches utilize dense Birdâ€™s Eye View (BEV)-based architectures for depth estimation, contemporary query-based transformers excel in camera-only detection through object-centric methodology. However, these query-based approaches exhibit limitations in false positive detections and localization precision due to implicit depth modeling. We address these challenges through three key contributions: (1) sparse frustum fusion (SFF) for cross-modal feature alignment, (2) range-adaptive radar aggregation (RAR) for precise object localization, and (3) local self-attention (LSA) for focused query aggregation. In contrast to existing methods requiring computationally intensive BEV-grid rendering, SpaRC operates directly on encoded point features, yielding substantial improvements in efficiency and accuracy. Empirical evaluations on the nuScenes and TruckScenes benchmarks demonstrate that SpaRC significantly outperforms existing dense BEV-based and sparse query-based detectors. Our method achieves state-of-the-art performance metrics of 67.1 NDS and 63.1 AMOTA. The code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/phi-wol/sparc">https://github.com/phi-wol/sparc</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SpaRCï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº3Dæ„ŸçŸ¥çš„æ–°å‹ç¨€ç–èåˆå˜å‹å™¨ï¼Œå®ƒèåˆäº†å¤šè§†å›¾å›¾åƒè¯­ä¹‰ä¸é›·è¾¾å’Œæ‘„åƒå¤´ç‚¹ç‰¹å¾ã€‚é›·è¾¾å’Œæ‘„åƒå¤´çš„èåˆå·²æˆä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„é«˜æ•ˆæ„ŸçŸ¥èŒƒå¼ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨å¯†é›†çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰åŸºæ¶æ„è¿›è¡Œæ·±åº¦ä¼°è®¡ï¼Œä½†å½“ä»£åŸºäºæŸ¥è¯¢çš„å˜å‹å™¨é€šè¿‡ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ–¹æ³•åœ¨ä»…ä½¿ç”¨ç›¸æœºè¿›è¡Œæ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™äº›åŸºäºæŸ¥è¯¢çš„æ–¹æ³•ç”±äºéšå¼æ·±åº¦å»ºæ¨¡è€Œåœ¨è¯¯æŠ¥æ£€æµ‹å’Œå®šä½ç²¾åº¦æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªä¸»è¦è´¡çŒ®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ç”¨äºè·¨æ¨¡æ€ç‰¹å¾å¯¹é½çš„ç¨€ç–æ£±é•œèåˆï¼ˆSFFï¼‰ï¼Œï¼ˆ2ï¼‰ç”¨äºç²¾ç¡®å¯¹è±¡å®šä½çš„é›·è¾¾èŒƒå›´è‡ªé€‚åº”èšåˆï¼ˆRARï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰ç”¨äºæœ‰é‡ç‚¹æŸ¥è¯¢èšåˆçš„å±€éƒ¨è‡ªæ³¨æ„åŠ›ï¼ˆLSAï¼‰ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•éœ€è¦è®¡ç®—å¯†é›†å‹çš„BEVç½‘æ ¼æ¸²æŸ“ï¼ŒSpaRCç›´æ¥åœ¨ç¼–ç çš„ç‚¹ç‰¹å¾ä¸Šè¿è¡Œï¼Œåœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚åœ¨nuSceneså’ŒTruckScenesåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSpaRCæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¯†é›†BEVåŸºå’Œç¨€ç–æŸ¥è¯¢åŸºæ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒNDSä¸º67.1ï¼ŒAMOTAä¸º63.1ã€‚ä»£ç å’Œé¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/phi-wol/sparc%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/phi-wol/sparcä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19860v1">PDF</a> 18 pages, 11 figures</p>
<p><strong>Summary</strong><br>åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºSpaRCçš„æ–°å‹ç¨€ç–èåˆè½¬æ¢å™¨ï¼Œç”¨äºä¸‰ç»´æ„ŸçŸ¥ï¼Œå®ƒå°†é›·è¾¾å’Œæ‘„åƒå¤´çš„ç‚¹ç‰¹å¾ä¸å¤šè§†å›¾å›¾åƒè¯­ä¹‰èåˆã€‚æˆ‘ä»¬é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸‰é¡¹å…³é”®è´¡çŒ®ï¼šç¨€ç–æ£±é•œèåˆï¼ˆSFFï¼‰ç”¨äºè·¨æ¨¡æ€ç‰¹å¾å¯¹é½ï¼ŒèŒƒå›´è‡ªé€‚åº”é›·è¾¾èšåˆï¼ˆRARï¼‰ç”¨äºç²¾ç¡®ç›®æ ‡å®šä½ï¼Œä»¥åŠå±€éƒ¨è‡ªæ³¨æ„åŠ›ï¼ˆLSAï¼‰ç”¨äºé›†ä¸­æŸ¥è¯¢èšåˆã€‚SpaRCç›´æ¥åœ¨ç¼–ç çš„ç‚¹ç‰¹å¾ä¸Šè¿è¡Œï¼Œä¸ç°æœ‰çš„éœ€è¦å¤§é‡è®¡ç®—èµ„æºçš„BEVç½‘æ ¼æ¸²æŸ“æ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œæé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚åœ¨nuSceneså’ŒTruckScenesåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒSpaRCæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¯†é›†BEVåŸºå’Œç¨€ç–æŸ¥è¯¢åŸºæ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€é«˜æ€§èƒ½ï¼š67.1NDSå’Œ63.1AMOTAã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SpaRCæ˜¯ä¸€ä¸ªæ–°å‹ç¨€ç–èåˆè½¬æ¢å™¨ï¼Œæ—¨åœ¨å®ç°å¤šè§†å›¾å›¾åƒè¯­ä¹‰ä¸é›·è¾¾å’Œæ‘„åƒå¤´çš„ç‚¹ç‰¹å¾èåˆã€‚å®ƒç›´æ¥æ“ä½œç‚¹ç‰¹å¾è€Œéå¤æ‚çš„å›¾åƒè§†å›¾æ¨¡å‹ï¼Œæå¤§æå‡äº†è®¡ç®—æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚</li>
<li>ä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯çš„æå‡ºæœ‰æ•ˆè§£å†³äº†é›·è¾¾å’Œæ‘„åƒå¤´èåˆæ‰€é¢ä¸´çš„éš¾é¢˜ï¼ŒåŒ…æ‹¬è·¨æ¨¡æ€ç‰¹å¾å¯¹é½ï¼ˆé€šè¿‡ç¨€ç–æ£±é•œèåˆï¼‰ã€ç²¾ç¡®ç›®æ ‡å®šä½ï¼ˆé€šè¿‡èŒƒå›´è‡ªé€‚åº”é›·è¾¾èšåˆï¼‰ä»¥åŠæŸ¥è¯¢èšåˆçš„é›†ä¸­åŒ–ï¼ˆé€šè¿‡å±€éƒ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚</li>
<li>SpaRCç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰ç€æ˜æ˜¾çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä¾‹å¦‚åœ¨nuSceneså’ŒTruckScenesæµ‹è¯•ä¸­å®ç°æ˜¾è‘—è¶…è¶Šçš„ä¸šç»©æŒ‡æ ‡ï¼ˆä¾‹å¦‚é«˜è¾¾67.1NDSå’Œ63.1AMOTAï¼‰ã€‚è¿™æ˜¾ç¤ºäº†SpaRCåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„å¼ºå¤§æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8c44ceded882a16f97b8230916e31223.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d16814eda324e61060b62556fea828ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a317fdb1cd2f43addb813ae0968cfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55d636484819e75955755bf85511fd1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ec981e8a590cad3a514ee82c893720d.jpg" align="middle">
</details>




<h2 id="Feedback-driven-object-detection-and-iterative-model-improvement"><a href="#Feedback-driven-object-detection-and-iterative-model-improvement" class="headerlink" title="Feedback-driven object detection and iterative model improvement"></a>Feedback-driven object detection and iterative model improvement</h2><p><strong>Authors:SÃ¶nke Tenckhoff, Mario Koddenbrock, Erik Rodner</strong></p>
<p>Automated object detection has become increasingly valuable across diverse applications, yet efficient, high-quality annotation remains a persistent challenge. In this paper, we present the development and evaluation of a platform designed to interactively improve object detection models. The platform allows uploading and annotating images as well as fine-tuning object detection models. Users can then manually review and refine annotations, further creating improved snapshots that are used for automatic object detection on subsequent image uploads - a process we refer to as semi-automatic annotation resulting in a significant gain in annotation efficiency.   Whereas iterative refinement of model results to speed up annotation has become common practice, we are the first to quantitatively evaluate its benefits with respect to time, effort, and interaction savings. Our experimental results show clear evidence for a significant time reduction of up to 53% for semi-automatic compared to manual annotation. Importantly, these efficiency gains did not compromise annotation quality, while matching or occasionally even exceeding the accuracy of manual annotations. These findings demonstrate the potential of our lightweight annotation platform for creating high-quality object detection datasets and provide best practices to guide future development of annotation platforms.   The platform is open-source, with the frontend and backend repositories available on GitHub. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç›®æ ‡æ£€æµ‹åœ¨è®¸å¤šåº”ç”¨ä¸­å˜å¾—è¶Šæ¥è¶Šæœ‰ä»·å€¼ï¼Œç„¶è€Œé«˜æ•ˆã€é«˜è´¨é‡çš„æ ‡æ³¨ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªäº¤äº’å¼æ”¹è¿›ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„å¹³å°çš„å¼€å‘è¯„ä¼°å·¥ä½œã€‚è¯¥å¹³å°å…è®¸ä¸Šä¼ å’Œæ ‡æ³¨å›¾åƒï¼Œå¹¶å¾®è°ƒç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚ç”¨æˆ·éšåå¯ä»¥æ‰‹åŠ¨å®¡æŸ¥å’Œä¿®æ”¹æ ‡æ³¨ï¼Œè¿›ä¸€æ­¥åˆ›å»ºæ”¹è¿›çš„å¿«ç…§ï¼Œè¿™äº›å¿«ç…§ç”¨äºåç»­å›¾åƒä¸Šä¼ çš„è‡ªåŠ¨ç›®æ ‡æ£€æµ‹â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºåŠè‡ªåŠ¨æ ‡æ³¨ï¼Œè¯¥è¿‡ç¨‹åœ¨æ ‡æ³¨æ•ˆç‡ä¸Šè·å¾—äº†æ˜¾è‘—çš„æå‡ã€‚è™½ç„¶é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¨¡å‹ç»“æœä»¥åŠ å¿«æ ‡æ³¨é€Ÿåº¦å·²æˆä¸ºå¸¸è§åšæ³•ï¼Œä½†æˆ‘ä»¬æ˜¯é¦–æ¬¡åœ¨æ—¶é—´ä¸Šã€åŠªåŠ›ç¨‹åº¦ä¸Šå’Œäº¤äº’èŠ‚çœæ–¹é¢å®šé‡è¯„ä¼°å…¶å¥½å¤„ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ‰‹åŠ¨æ ‡æ³¨ç›¸æ¯”ï¼ŒåŠè‡ªåŠ¨æ ‡æ³¨çš„æ—¶é—´å‡å°‘äº†é«˜è¾¾53%ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›æ•ˆç‡çš„æå‡å¹¶æ²¡æœ‰æŸå®³æ ‡æ³¨è´¨é‡ï¼ŒåŒæ—¶è¾¾åˆ°äº†æˆ–å¶å°”ç”šè‡³è¶…è¿‡äº†æ‰‹åŠ¨æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚è¿™äº›å‘ç°è¯æ˜äº†æˆ‘ä»¬è½»é‡çº§æ ‡æ³¨å¹³å°åœ¨åˆ›å»ºé«˜è´¨é‡ç›®æ ‡æ£€æµ‹æ•°æ®é›†æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥æ ‡æ³¨å¹³å°çš„å‘å±•æä¾›äº†æœ€ä½³å®è·µæŒ‡å¯¼ã€‚è¯¥å¹³å°æ˜¯å¼€æºçš„ï¼Œå‰ç«¯å’Œåç«¯ä»“åº“å‡å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19835v1">PDF</a> AI4EA24 preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªäº¤äº’å¼å¹³å°ï¼Œæ—¨åœ¨æé«˜ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥å¹³å°æ”¯æŒå›¾åƒä¸Šä¼ ã€æ ‡æ³¨å’Œæ¨¡å‹å¾®è°ƒã€‚ç”¨æˆ·å¯æ‰‹åŠ¨å®¡æŸ¥å’Œä¿®æ­£æ ‡æ³¨ï¼Œåˆ›å»ºæ”¹è¿›çš„å¿«ç…§ç”¨äºåç»­å›¾åƒä¸Šä¼ çš„è‡ªåŠ¨ç‰©ä½“æ£€æµ‹ã€‚è¯¥å¹³å°é‡‡ç”¨åŠè‡ªåŠ¨æ ‡æ³¨æ–¹å¼ï¼Œæ˜¾è‘—æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠè‡ªåŠ¨æ ‡æ³¨ä¸æ‰‹åŠ¨æ ‡æ³¨ç›¸æ¯”ï¼Œæ—¶é—´å‡å°‘äº†é«˜è¾¾53%ã€‚é‡è¦çš„æ˜¯ï¼Œæ•ˆç‡çš„æå‡å¹¶æ²¡æœ‰æŸå®³æ ‡æ³¨è´¨é‡ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†æ‰‹åŠ¨æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚è¿™ä¸ºåˆ›å»ºé«˜è´¨é‡ç‰©ä½“æ£€æµ‹æ•°æ®é›†æä¾›äº†æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥å¼€å‘æ ‡æ³¨å¹³å°æä¾›äº†æœ€ä½³å®è·µæŒ‡å—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥å¹³å°æ—¨åœ¨æé«˜ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½ï¼Œæ”¯æŒå›¾åƒä¸Šä¼ ã€æ ‡æ³¨å’Œæ¨¡å‹å¾®è°ƒã€‚</li>
<li>ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨å®¡æŸ¥å’Œä¿®æ­£æ ‡æ³¨ï¼Œåˆ›å»ºæ”¹è¿›çš„å¿«ç…§ç”¨äºè‡ªåŠ¨ç‰©ä½“æ£€æµ‹ã€‚</li>
<li>å¹³å°é‡‡ç”¨åŠè‡ªåŠ¨æ ‡æ³¨æ–¹å¼ï¼Œæ˜¾è‘—æé«˜äº†æ ‡æ³¨æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒåŠè‡ªåŠ¨æ ‡æ³¨ä¸æ‰‹åŠ¨æ ‡æ³¨ç›¸æ¯”ï¼Œæ—¶é—´å‡å°‘äº†é«˜è¾¾53%ã€‚</li>
<li>æ•ˆç‡çš„æå‡å¹¶æ²¡æœ‰æŸå®³æ ‡æ³¨è´¨é‡ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¿‡äº†æ‰‹åŠ¨æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥å¹³å°æ˜¯å¼€æºçš„ï¼Œå‰ç«¯å’Œåç«¯ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8982e1f9a8a1ec3d498be4d173dcbf8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06d40879c9a9ceb8bf640acf76ca66c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2d08efcbde5fb94aefa300dac7b79c6.jpg" align="middle">
</details>




<h2 id="Automatic-Prompt-Generation-and-Grounding-Object-Detection-for-Zero-Shot-Image-Anomaly-Detection"><a href="#Automatic-Prompt-Generation-and-Grounding-Object-Detection-for-Zero-Shot-Image-Anomaly-Detection" class="headerlink" title="Automatic Prompt Generation and Grounding Object Detection for Zero-Shot   Image Anomaly Detection"></a>Automatic Prompt Generation and Grounding Object Detection for Zero-Shot   Image Anomaly Detection</h2><p><strong>Authors:Tsun-Hin Cheung, Ka-Chun Fung, Songjiang Lai, Kwan-Ho Lin, Vincent Ng, Kin-Man Lam</strong></p>
<p>Identifying defects and anomalies in industrial products is a critical quality control task. Traditional manual inspection methods are slow, subjective, and error-prone. In this work, we propose a novel zero-shot training-free approach for automated industrial image anomaly detection using a multimodal machine learning pipeline, consisting of three foundation models. Our method first uses a large language model, i.e., GPT-3. generate text prompts describing the expected appearances of normal and abnormal products. We then use a grounding object detection model, called Grounding DINO, to locate the product in the image. Finally, we compare the cropped product image patches to the generated prompts using a zero-shot image-text matching model, called CLIP, to identify any anomalies. Our experiments on two datasets of industrial product images, namely MVTec-AD and VisA, demonstrate the effectiveness of this method, achieving high accuracy in detecting various types of defects and anomalies without the need for model training. Our proposed model enables efficient, scalable, and objective quality control in industrial manufacturing settings. </p>
<blockquote>
<p>è¯†åˆ«å·¥ä¸šäº§å“ä¸­çš„ç¼ºé™·å’Œå¼‚å¸¸æ˜¯è´¨é‡æ§åˆ¶çš„å…³é”®ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„æ‰‹åŠ¨æ£€æŸ¥æ–¹æ³•é€Ÿåº¦æ…¢ã€ä¸»è§‚ã€æ˜“å‡ºé”™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„é›¶æ ·æœ¬æ— éœ€è®­ç»ƒçš„è‡ªé€‚åº”å·¥ä¸šå›¾åƒå¼‚å¸¸æ£€æµ‹æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé‡‡ç”¨å¤šæ¨¡æ€æœºå™¨å­¦ä¹ ç®¡é“ï¼Œç”±ä¸‰ä¸ªåŸºç¡€æ¨¡å‹ç»„æˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå³GPT-3ï¼Œç”Ÿæˆæè¿°æ­£å¸¸å’Œå¼‚å¸¸äº§å“é¢„æœŸå¤–è§‚çš„æ–‡æœ¬æç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨åä¸ºGrounding DINOçš„å®šä½ç›®æ ‡æ£€æµ‹æ¨¡å‹æ¥å®šä½å›¾åƒä¸­çš„äº§å“ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨é›¶æ ·æœ¬å›¾åƒæ–‡æœ¬åŒ¹é…æ¨¡å‹CLIPï¼Œå°†è£å‰ªåçš„äº§å“å›¾åƒå—ä¸ç”Ÿæˆçš„æç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€æµ‹ä»»ä½•å¼‚å¸¸ã€‚æˆ‘ä»¬åœ¨å·¥ä¸šäº§å“å›¾åƒæ•°æ®é›†MVTec-ADå’ŒVisAä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨æ— éœ€æ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°äº†é«˜å‡†ç¡®åº¦çš„å„ç§ç¼ºé™·å’Œå¼‚å¸¸çš„æ£€æµ‹ã€‚æˆ‘ä»¬æå‡ºçš„æ¨¡å‹èƒ½å¤Ÿåœ¨å·¥ä¸šåˆ¶é€ ç¯å¢ƒä¸­å®ç°é«˜æ•ˆã€å¯æ‰©å±•å’Œå®¢è§‚çš„è´¨é‡æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19220v1">PDF</a> Accepted to APSIPA ASC 2024</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå·¥ä¸šäº§å“å›¾åƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æç¤ºï¼Œç»“åˆå®šä½æ¨¡å‹å’Œå›¾åƒæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œå®ç°å¯¹äº§å“å›¾åƒä¸­ç¼ºé™·å’Œå¼‚å¸¸çš„è‡ªåŠ¨æ£€æµ‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œæ— éœ€è®­ç»ƒæ¨¡å‹å³å¯æœ‰æ•ˆæ£€æµ‹å¤šç§ç±»å‹çš„ç¼ºé™·å’Œå¼‚å¸¸ï¼Œä¸ºå·¥ä¸šåˆ¶é€ ç¯å¢ƒä¸­çš„è´¨é‡æ§åˆ¶æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•å’Œå®¢è§‚çš„æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŸºäºå·¥ä¸šäº§å“å›¾åƒçš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–°æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€å®šä½æ¨¡å‹å’Œå›¾åƒæ–‡æœ¬åŒ¹é…æ¨¡å‹ã€‚</li>
<li>è¯­è¨€æ¨¡å‹ç”¨äºç”Ÿæˆæè¿°æ­£å¸¸å’Œå¼‚å¸¸äº§å“å¤–è§‚çš„æ–‡æœ¬æç¤ºã€‚</li>
<li>å®šä½æ¨¡å‹ç”¨äºåœ¨å›¾åƒä¸­å®šä½äº§å“ã€‚</li>
<li>å›¾åƒæ–‡æœ¬åŒ¹é…æ¨¡å‹ç”¨äºå°†è£å‰ªåçš„äº§å“å›¾åƒä¸ç”Ÿæˆçš„æ–‡æœ¬æç¤ºè¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>å®éªŒåœ¨MVTec-ADå’ŒVisAæ•°æ®é›†ä¸ŠéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b38b74363f4799d8d8742188a74d7bc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5647ba0b90fb068d849ae43afc76f1f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6aa92c27e778e5c6ac9f2eb887e2a52e.jpg" align="middle">
</details>




<h2 id="HOT3D-Hand-and-Object-Tracking-in-3D-from-Egocentric-Multi-View-Videos"><a href="#HOT3D-Hand-and-Object-Tracking-in-3D-from-Egocentric-Multi-View-Videos" class="headerlink" title="HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos"></a>HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</h2><p><strong>Authors:Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan</strong></p>
<p>We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB&#x2F;monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground-truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects. In addition to simple pick-up&#x2F;observe&#x2F;put-down actions, HOT3D contains scenarios resembling typical actions in a kitchen, office, and living room environment. The dataset is recorded by two head-mounted devices from Meta: Project Aria, a research prototype of light-weight AR&#x2F;AI glasses, and Quest 3, a production VR headset sold in millions of units. Ground-truth poses were obtained by a professional motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºHOT3Dæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ‰‹å’Œå¯¹è±¡åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„è·Ÿè¸ªçš„å…¬å¼€æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æä¾›äº†è¶…è¿‡833åˆ†é’Ÿï¼ˆè¶…è¿‡370ä¸‡å¼ å›¾åƒï¼‰çš„å¤šè§†è§’RGB&#x2F;å•è‰²å›¾åƒæµï¼Œå±•ç¤ºäº†19ä¸ªä¸»ä½“ä¸33ç§ä¸åŒçš„åˆšæ€§å¯¹è±¡çš„äº’åŠ¨ï¼Œè¿˜åŒ…æ‹¬å¤šç§æ¨¡å¼çš„ä¿¡å·ï¼Œå¦‚çœ¼åŠ¨è¿½è¸ªæˆ–åœºæ™¯ç‚¹äº‘ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬å…¨é¢çš„çœŸå®æ ‡æ³¨ä¿¡æ¯ï¼Œå¦‚ç‰©ä½“ã€æ‰‹å’Œç›¸æœºçš„ä¸‰ç»´å§¿æ€ï¼Œä»¥åŠæ‰‹å’Œå¯¹è±¡çš„ä¸‰ç»´æ¨¡å‹ã€‚é™¤äº†ç®€å•çš„æ‹¿èµ·&#x2F;è§‚å¯Ÿ&#x2F;æ”¾ä¸‹çš„åŠ¨ä½œå¤–ï¼ŒHOT3Dè¿˜åŒ…æ‹¬åœ¨å¨æˆ¿ã€åŠå…¬å®¤å’Œå®¢å…ç¯å¢ƒä¸­å…¸å‹çš„åŠ¨ä½œåœºæ™¯ã€‚è¯¥æ•°æ®é›†æ˜¯é€šè¿‡Metaçš„ä¸¤æ¬¾å¤´æˆ´è®¾å¤‡è®°å½•çš„ï¼šProject Ariaæ˜¯è½»ä¾¿çš„AR&#x2F;AIçœ¼é•œçš„ç ”ç©¶åŸå‹ï¼Œè€ŒQuest 3åˆ™æ˜¯é”€é‡è¾¾æ•°ç™¾ä¸‡çš„ç”Ÿäº§çº§è™šæ‹Ÿç°å®è€³æœºã€‚é€šè¿‡é™„ç€åœ¨æ‰‹å’Œç‰©ä½“ä¸Šçš„å°å‹å…‰å­¦æ ‡è®°ï¼Œé‡‡ç”¨ä¸“ä¸šè¿åŠ¨æ•æ‰ç³»ç»Ÿè·å¾—çœŸå®å§¿æ€ã€‚æ‰‹çš„æ³¨é‡Šä»¥UmeTrackå’ŒMANOæ ¼å¼æä¾›ï¼Œç‰©ä½“åˆ™ç”±å†…éƒ¨æ‰«æä»ªè·å¾—çš„ç‰©ç†åŸºç¡€æ¸²æŸ“ï¼ˆPBRï¼‰ææ–™è¡¨ç¤ºçš„ä¸‰ç»´ç½‘æ ¼æ¨¡å‹è¡¨ç¤ºã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¤šè§†è§’è‡ªæˆ‘ä¸­å¿ƒæ•°æ®å¯¹äºä¸‰ä¸ªæµè¡Œä»»åŠ¡çš„å®ç”¨æ€§ï¼šä¸‰ç»´æ‰‹è·Ÿè¸ªã€å…­è‡ªç”±åº¦å¯¹è±¡å§¿æ€ä¼°è®¡å’Œä¸‰ç»´æ‰‹ä¸­æœªçŸ¥ç‰©ä½“çš„æå‡ã€‚ç»è¿‡è¯„ä¼°çš„å¤šè§†è§’æ–¹æ³•æ˜æ˜¾è¶…è¿‡äº†å•ä¸€è§†è§’çš„å¯¹åº”æ–¹æ³•ï¼Œè€Œè¿™ä¸€åˆ‡çš„ç‹¬ç‰¹åŸºå‡†æµ‹è¯•éƒ½æ˜¯ç”±HOT3Då®ç°çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19167v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2406.09598</p>
<p><strong>æ‘˜è¦</strong></p>
<p>HOT3Dæ˜¯ä¸€ä¸ªå…¬å¼€çš„ã€ç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ‰‹å’Œå¯¹è±¡åœ¨ä¸‰ç»´ç©ºé—´ä¸­çš„è·Ÿè¸ªçš„æ•°æ®é›†ã€‚å®ƒåŒ…å«è¶…è¿‡833åˆ†é’Ÿï¼ˆè¶…è¿‡370ä¸‡å¼ å›¾åƒï¼‰çš„å¤šè§†è§’RGB&#x2F;å•è‰²å›¾åƒæµï¼Œå±•ç¤º19ä¸ªä¸»ä½“ä¸33ä¸ªä¸åŒåˆšæ€§å¯¹è±¡çš„äº’åŠ¨ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€ä¿¡å·å¦‚çœ¼ç›æ³¨è§†æˆ–åœºæ™¯ç‚¹äº‘ç­‰ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰å…¨é¢çš„åœ°é¢çœŸå®æ³¨é‡Šï¼ŒåŒ…æ‹¬å¯¹è±¡ã€æ‰‹å’Œç›¸æœºçš„ä¸‰ç»´å§¿åŠ¿ï¼Œä»¥åŠæ‰‹å’Œå¯¹è±¡çš„ä¸‰ç»´æ¨¡å‹ã€‚è¯¥æ•°æ®é›†è®°å½•äº†å…¸å‹çš„å¨æˆ¿ã€åŠå…¬å®¤å’Œå®¢å…ç¯å¢ƒåœºæ™¯ä¸­çš„åŠ¨ä½œã€‚é€šè¿‡ä¸¤ä¸ªå¤´æˆ´å¼è®¾å¤‡Meta Project Ariaï¼ˆè½»é‡çº§AR&#x2F;AIçœ¼é•œçš„ç ”ç©¶åŸå‹ï¼‰å’ŒQuest 3ï¼ˆé”€é‡è¾¾æ•°ç™¾ä¸‡çš„ç”Ÿäº§çº§è™šæ‹Ÿç°å®è€³æœºï¼‰è¿›è¡Œè®°å½•ã€‚åœ°é¢çœŸå®å§¿åŠ¿é€šè¿‡ä¸“ä¸šè¿åŠ¨æ•æ‰ç³»ç»Ÿè·å¾—ï¼Œè¯¥ç³»ç»Ÿä½¿ç”¨é™„ç€åœ¨æ‰‹å’Œå¯¹è±¡ä¸Šçš„å°å‹å…‰å­¦æ ‡è®°ã€‚æ‰‹æ³¨é‡Šä»¥UmeTrackå’ŒMANOæ ¼å¼æä¾›ï¼Œå¯¹è±¡åˆ™ç”±å†…éƒ¨æ‰«æä»ªè·å¾—çš„å…·æœ‰PBRæè´¨çš„ä¸‰ç»´ç½‘æ ¼è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¤šè§†è§’è‡ªæˆ‘ä¸­å¿ƒæ•°æ®å¯¹äºä¸‰ä¸ªæµè¡Œä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼šä¸‰ç»´æ‰‹è·Ÿè¸ªã€å…­è‡ªç”±åº¦å¯¹è±¡å§¿æ€ä¼°è®¡å’Œä¸‰ç»´æ‰‹éƒ¨æœªçŸ¥å¯¹è±¡çš„å‡é™ã€‚é€šè¿‡HOT3Då”¯ä¸€å¯ç”¨çš„å¤šè§†è§’è¯„ä¼°æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶å•è§†è§’å¯¹åº”ç‰©ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>HOT3Dæ˜¯ä¸€ä¸ªç”¨äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ‰‹å’Œå¯¹è±¡åœ¨ä¸‰ç»´ç©ºé—´è·Ÿè¸ªçš„å…¬å¼€æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤šè§†è§’RGB&#x2F;å•è‰²å›¾åƒæµã€å¤šæ¨¡æ€ä¿¡å·ã€å…¨é¢çš„åœ°é¢çœŸå®æ³¨é‡Šç­‰ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–æ—¥å¸¸æ´»åŠ¨åœºæ™¯ï¼ŒåŒ…æ‹¬å¨æˆ¿ã€åŠå…¬å®¤å’Œå®¢å…ç¯å¢ƒã€‚</li>
<li>ä½¿ç”¨äº†ä¸¤ä¸ªå¤´æˆ´è®¾å¤‡æ¥è®°å½•æ•°æ®ï¼šMeta Project Ariaå’ŒQuest 3ã€‚</li>
<li>åœ°é¢çœŸå®å§¿åŠ¿æ˜¯é€šè¿‡ä¸“ä¸šè¿åŠ¨æ•æ‰ç³»ç»Ÿå’Œå°å‹å…‰å­¦æ ‡è®°è·å¾—çš„ã€‚</li>
<li>å®éªŒè¯æ˜äº†å¤šè§†è§’æ•°æ®å¯¹äºä¸‰ä¸ªä»»åŠ¡çš„æœ‰æ•ˆæ€§ï¼šä¸‰ç»´æ‰‹è·Ÿè¸ªã€å…­è‡ªç”±åº¦å¯¹è±¡å§¿æ€ä¼°è®¡å’Œæ‰‹éƒ¨æœªçŸ¥å¯¹è±¡çš„å‡é™ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-866cd7d9774138d23241e6b82eecb31f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aec4f1ab92f95c067621d1dd3cb03a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a328236049a716573ecdaaf4dda4b002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e92553e35b8817dbe9249980c945fdbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63f5b77b0d7d1bd7808663306bea53ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa6e96117b90bd8cc05ce78430eacea6.jpg" align="middle">
</details>




<h2 id="MaskRIS-Semantic-Distortion-aware-Data-Augmentation-for-Referring-Image-Segmentation"><a href="#MaskRIS-Semantic-Distortion-aware-Data-Augmentation-for-Referring-Image-Segmentation" class="headerlink" title="MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image   Segmentation"></a>MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image   Segmentation</h2><p><strong>Authors:Minhyun Lee, Seungho Lee, Song Park, Dongyoon Han, Byeongho Heo, Hyunjung Shim</strong></p>
<p>Referring Image Segmentation (RIS) is an advanced vision-language task that involves identifying and segmenting objects within an image as described by free-form text descriptions. While previous studies focused on aligning visual and language features, exploring training techniques, such as data augmentation, remains underexplored. In this work, we explore effective data augmentation for RIS and propose a novel training framework called Masked Referring Image Segmentation (MaskRIS). We observe that the conventional image augmentations fall short of RIS, leading to performance degradation, while simple random masking significantly enhances the performance of RIS. MaskRIS uses both image and text masking, followed by Distortion-aware Contextual Learning (DCL) to fully exploit the benefits of the masking strategy. This approach can improve the modelâ€™s robustness to occlusions, incomplete information, and various linguistic complexities, resulting in a significant performance improvement. Experiments demonstrate that MaskRIS can easily be applied to various RIS models, outperforming existing methods in both fully supervised and weakly supervised settings. Finally, MaskRIS achieves new state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg datasets. Code is available at <a target="_blank" rel="noopener" href="https://github.com/naver-ai/maskris">https://github.com/naver-ai/maskris</a>. </p>
<blockquote>
<p>æŒ‡ä»£å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰æ˜¯ä¸€é¡¹å…ˆè¿›çš„è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå®ƒæ¶‰åŠæ ¹æ®è‡ªç”±å½¢å¼çš„æ–‡æœ¬æè¿°æ¥è¯†åˆ«å’Œåˆ†å‰²å›¾åƒä¸­çš„å¯¹è±¡ã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è§†è§‰å’Œè¯­è¨€ç‰¹å¾çš„å¯¹é½ä¸Šï¼Œä½†æ¢ç´¢è®­ç»ƒæŠ€æœ¯ï¼Œå¦‚æ•°æ®å¢å¼ºï¼Œä»ç„¶è¢«ä½ä¼°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†RISçš„æœ‰æ•ˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºMasked Referring Image Segmentationï¼ˆMaskRISï¼‰çš„æ–°å‹è®­ç»ƒæ¡†æ¶ã€‚æˆ‘ä»¬å‘ç°ä¼ ç»Ÿçš„å›¾åƒå¢å¼ºæ–¹æ³•å¯¹äºRISæ¥è¯´æ•ˆæœä¸è¶³ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œç®€å•çš„éšæœºé®æŒ¡å¯ä»¥æ˜¾ç€æé«˜RISçš„æ€§èƒ½ã€‚MaskRISåŒæ—¶ä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬é®æŒ¡ï¼Œç„¶åè¿›è¡ŒDistortion-aware Contextual Learningï¼ˆDCLï¼‰ä»¥å……åˆ†åˆ©ç”¨é®æŒ¡ç­–ç•¥çš„ä¼˜åŠ¿ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹å¯¹é®æŒ¡ã€ä¸å®Œæ•´ä¿¡æ¯å’Œå„ç§è¯­è¨€å¤æ‚æ€§çš„é²æ£’æ€§ï¼Œä»è€Œå¤§å¤§æé«˜æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒMaskRISå¯ä»¥è½»æ¾åº”ç”¨äºå„ç§RISæ¨¡å‹ï¼Œåœ¨å…¨ç›‘ç£å’Œå¼±ç›‘ç£è®¾ç½®ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æœ€åï¼ŒMaskRISåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/naver-ai/maskris%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/naver-ai/maskrisè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19067v1">PDF</a> First two authors contributed equally</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ¢è®¨äº†ç”¨äºå›¾åƒæè¿°åˆ†å‰²ï¼ˆRISï¼‰çš„æœ‰æ•ˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶â€”â€”MaskRISã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸è§„å›¾åƒå¢å¼ºæ–¹æ³•ä¸é€‚ç”¨äºRISï¼Œè€Œç®€å•çš„éšæœºæ©ç å¯ä»¥æ˜¾è‘—æé«˜RISæ€§èƒ½ã€‚MaskRISç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬æ©ç ï¼Œå¹¶é€šè¿‡æ‰­æ›²æ„ŸçŸ¥ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDCLï¼‰å……åˆ†åˆ©ç”¨æ©ç ç­–ç•¥çš„ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•æé«˜äº†æ¨¡å‹å¯¹é®æŒ¡ã€ä¿¡æ¯ä¸å®Œæ•´å’Œå„ç§è¯­è¨€å¤æ‚æ€§çš„é²æ£’æ€§ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒMaskRISå¯è½»æ¾åº”ç”¨äºå„ç§RISæ¨¡å‹ï¼Œåœ¨å…¨ç›‘ç£å’Œå¼±ç›‘ç£è®¾ç½®ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚MaskRISåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked Referring Image Segmentation (MaskRIS)æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å›¾åƒæè¿°åˆ†å‰²ï¼ˆRISï¼‰çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒå¢å¼ºæ–¹æ³•åœ¨RISä¸Šæ•ˆæœä¸ä½³ï¼Œç®€å•éšæœºæ©ç èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>MaskRISç»“åˆå›¾åƒå’Œæ–‡æœ¬æ©ç ç­–ç•¥ä»¥å¢å¼ºæ¨¡å‹å¤„ç†é®æŒ¡å’Œä¸å®Œæ•´ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>MaskRISé€šè¿‡æ‰­æ›²æ„ŸçŸ¥ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆDCLï¼‰å……åˆ†åˆ©ç”¨æ©ç ç­–ç•¥çš„ä¼˜åŠ¿ã€‚</li>
<li>MaskRISæ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹å¯¹å„ç§è¯­è¨€å¤æ‚æ€§çš„é²æ£’æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºMaskRISåœ¨å…¨ç›‘ç£å’Œå¼±ç›‘ç£ç¯å¢ƒä¸‹å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e79d8962a21e8da0c2039372d2e02102.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80b26bf2ccc030c57f5bccc20c05861.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86940e2225a7946bebfc1c4ac60d9f37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b6965e2418104e16daef9d52e7373d2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81162f7667e6d6468df9445655b0d206.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-46824d2f6082475e2c89c230472a59e3.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Exploring Depth Information for Detecting Manipulated Face Videos
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9747fcfed9389ef7a615593f1bc27fa6.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Visual Lexicon Rich Image Features in Language Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16573k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
