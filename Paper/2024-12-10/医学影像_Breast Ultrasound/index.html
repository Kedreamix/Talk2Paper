<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="åŒ»å­¦å½±åƒ/Breast Ultrasound"><meta name="description" content="åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  UNet++ and LSTM combined approach for Breast Ultrasound Image   Segmentation"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>åŒ»å­¦å½±åƒ/Breast Ultrasound | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>é¦–é¡µ</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>æ ‡ç­¾</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>åˆ†ç±»</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>å½’æ¡£</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> é¦–é¡µ</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> æ ‡ç­¾</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> åˆ†ç±»</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> å½’æ¡£</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://picx.zhimg.com/v2-d460005db73d4069a7ca154e972bd229.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/"><span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">åŒ»å­¦å½±åƒ/Breast Ultrasound</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp; 2024-12-11</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp; 2024-12-11</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> æ–‡ç« å­—æ•°:&nbsp;&nbsp; 9.5k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> é˜…è¯»æ—¶é•¿:&nbsp;&nbsp; 40 åˆ†</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="UNet-and-LSTM-combined-approach-for-Breast-Ultrasound-Image-Segmentation"><a href="#UNet-and-LSTM-combined-approach-for-Breast-Ultrasound-Image-Segmentation" class="headerlink" title="UNet++ and LSTM combined approach for Breast Ultrasound Image   Segmentation"></a>UNet++ and LSTM combined approach for Breast Ultrasound Image Segmentation</h2><p><strong>Authors:Saba Hesaraki, Morteza Akbari, Ramin Mousa</strong></p><p>Breast cancer stands as a prevalent cause of fatality among females on a global scale, with prompt detection playing a pivotal role in diminishing mortality rates. The utilization of ultrasound scans in the BUSI dataset for medical imagery pertaining to breast cancer has exhibited commendable segmentation outcomes through the application of UNet and UNet++ networks. Nevertheless, a notable drawback of these models resides in their inattention towards the temporal aspects embedded within the images. This research endeavors to enrich the UNet++ architecture by integrating LSTM layers and self-attention mechanisms to exploit temporal characteristics for segmentation purposes. Furthermore, the incorporation of a Multiscale Feature Extraction Module aims to grasp varied scale features within the UNet++. Through the amalgamation of our proposed methodology with data augmentation on the BUSI with GT dataset, an accuracy rate of 98.88%, specificity of 99.53%, precision of 95.34%, sensitivity of 91.20%, F1-score of 93.74, and Dice coefficient of 92.74% are achieved. These findings demonstrate competitiveness with cutting-edge techniques outlined in existing literature.</p><blockquote><p>ä¹³è…ºç™Œæ˜¯å…¨çƒå¥³æ€§å¸¸è§çš„è‡´å‘½ç–¾ç—…åŸå› ä¹‹ä¸€ï¼ŒåŠæ—¶æ£€æµ‹åœ¨é™ä½æ­»äº¡ç‡æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨ä¹³è…ºç™ŒåŒ»å­¦å›¾åƒæ–¹é¢çš„BUSIæ•°æ®é›†åˆ©ç”¨è¶…å£°æ‰«æï¼Œé€šè¿‡åº”ç”¨UNetå’ŒUNet++ç½‘ç»œå±•ç°äº†ä»¤äººç§°èµçš„åˆ†å‰²æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„ä¸€ä¸ªæ˜¾è‘—ç¼ºç‚¹æ˜¯å¯¹å›¾åƒä¸­åµŒå…¥çš„æš‚æ—¶æ–¹é¢çš„å¿½è§†ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºé€šè¿‡é›†æˆLSTMå±‚å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¸°å¯ŒUNet++æ¶æ„ï¼Œä»¥åˆ©ç”¨æš‚æ—¶ç‰¹å¾è¿›è¡Œåˆ†å‰²ã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—çš„å¼•å…¥æ—¨åœ¨æ•è·UNet++ä¸­çš„ä¸åŒå°ºåº¦ç‰¹å¾ã€‚é€šè¿‡å°†æœ¬ç ”ç©¶æå‡ºçš„æ–¹æ³•ä¸BUSIæ•°æ®é›†ä¸ŠGTæ•°æ®é›†çš„æ•°æ®æ‰©å……ç›¸ç»“åˆï¼Œè·å¾—äº†98.88%çš„å‡†ç¡®ç‡ã€99.53%çš„ç‰¹å¼‚æ€§ã€95.34%çš„ç²¾ç¡®åº¦ã€91.20%çš„çµæ•åº¦ã€93.74%çš„F1åˆ†æ•°å’Œ92.74%çš„Diceç³»æ•°ã€‚è¿™äº›å‘ç°ä¸ç°æœ‰æ–‡çŒ®ä¸­æ¦‚è¿°çš„æœ€å…ˆè¿›æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05585v1">PDF</a></p><p><strong>Summary</strong></p><p>ä¹³è…ºç™Œæ˜¯å…¨çƒå¥³æ€§å¸¸è§çš„è‡´å‘½ç–¾ç—…ï¼ŒåŠæ—¶æ£€æµ‹å¯¹é™ä½æ­»äº¡ç‡è‡³å…³é‡è¦ã€‚è¶…å£°æ‰«æåœ¨BUSIæ•°æ®é›†ä¸Šçš„åŒ»å­¦å›¾åƒåˆ†å‰²è¡¨ç°è‰¯å¥½ï¼Œé€šè¿‡åº”ç”¨UNetå’ŒUNet++ç½‘ç»œå–å¾—æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¿½è§†äº†å›¾åƒä¸­çš„æ—¶é—´ç‰¹æ€§ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡é›†æˆLSTMå±‚å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¸°å¯ŒUNet++æ¶æ„ï¼Œä»¥åˆ©ç”¨æ—¶é—´ç‰¹æ€§è¿›è¡Œåˆ†å‰²ã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—çš„å¼•å…¥æ—¨åœ¨æ•è·UNet++ä¸­çš„ä¸åŒå°ºåº¦ç‰¹å¾ã€‚é€šè¿‡å°†æœ¬ç ”ç©¶æ–¹æ³•ä¸BUSI with GTæ•°æ®é›†çš„æ•°æ®å¢å¼ºç›¸ç»“åˆï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ä¹³è…ºç™Œæ˜¯å…¨çƒå¥³æ€§å¸¸è§çš„è‡´å‘½ç–¾ç—…ï¼ŒåŠæ—¶æ£€æµ‹å¯¹é™ä½æ­»äº¡ç‡æœ‰é‡è¦ä½œç”¨ã€‚</li><li>è¶…å£°æ‰«æåœ¨BUSIæ•°æ®é›†çš„åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°è‰¯å¥½ã€‚</li><li>UNetå’ŒUNet++ç½‘ç»œåœ¨å›¾åƒåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li><li>ç°æœ‰æ¨¡å‹å¿½ç•¥äº†å›¾åƒä¸­çš„æ—¶é—´ç‰¹æ€§ã€‚</li><li>æœ¬ç ”ç©¶é€šè¿‡é›†æˆLSTMå±‚å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¸°å¯ŒUNet++æ¶æ„ï¼Œä»¥åˆ©ç”¨æ—¶é—´ç‰¹æ€§è¿›è¡Œåˆ†å‰²ã€‚</li><li>å¼•å…¥å¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—ä»¥æ•è·ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b8782b804c1a2c0df6c0b89baa9b9fb2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-de0212116fddb8d9b0dc23fae0891a0c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13f69da6d252b45847695820a8ec0b6b.jpg" align="middle"></details><h2 id="A-SAM-guided-and-Match-based-Semi-Supervised-Segmentation-Framework-for-Medical-Imaging"><a href="#A-SAM-guided-and-Match-based-Semi-Supervised-Segmentation-Framework-for-Medical-Imaging" class="headerlink" title="A SAM-guided and Match-based Semi-Supervised Segmentation Framework for   Medical Imaging"></a>A SAM-guided and Match-based Semi-Supervised Segmentation Framework for Medical Imaging</h2><p><strong>Authors:Guoping Xu, Xiaoxue Qian, Hua Chieh Shao, Jax Luo, Weiguo Lu, You Zhang</strong></p><p>This study introduces SAMatch, a SAM-guided Match-based framework for semi-supervised medical image segmentation, aimed at improving pseudo label quality in data-scarce scenarios. While Match-based frameworks are effective, they struggle with low-quality pseudo labels due to the absence of ground truth. SAM, pre-trained on a large dataset, generalizes well across diverse tasks and assists in generating high-confidence prompts, which are then used to refine pseudo labels via fine-tuned SAM. SAMatch is trained end-to-end, allowing for dynamic interaction between the models. Experiments on the ACDC cardiac MRI, BUSI breast ultrasound, and MRLiver datasets show SAMatch achieving state-of-the-art results, with Dice scores of 89.36%, 77.76%, and 80.04%, respectively, using minimal labeled data. SAMatch effectively addresses challenges in semi-supervised segmentation, offering a powerful tool for segmentation in data-limited environments. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/apple1986/SAMatch">https://github.com/apple1986/SAMatch</a>.</p><blockquote><p>æœ¬ç ”ç©¶ä»‹ç»äº†SAMatchï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSAMå¼•å¯¼çš„åŸºäºåŒ¹é…çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æé«˜ä¼ªæ ‡ç­¾è´¨é‡ã€‚è™½ç„¶åŸºäºåŒ¹é…çš„æ¡†æ¶å¾ˆæœ‰æ•ˆï¼Œä½†ç”±äºç¼ºå°‘çœŸå®æ ‡ç­¾ï¼Œå®ƒä»¬éš¾ä»¥å¤„ç†ä½è´¨é‡çš„ä¼ªæ ‡ç­¾ã€‚SAMåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨ä¸åŒçš„ä»»åŠ¡ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ï¼Œå¹¶æœ‰åŠ©äºç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æç¤ºï¼Œç„¶åè¿™äº›æç¤ºè¢«ç”¨äºé€šè¿‡å¾®è°ƒSAMæ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ã€‚SAMatchæ˜¯ç«¯åˆ°ç«¯è¿›è¡Œè®­ç»ƒçš„ï¼Œå…è®¸æ¨¡å‹ä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚åœ¨ACDCå¿ƒè„MRIã€BUSIä¹³è…ºè¶…å£°å’ŒMRLiveræ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAMatchå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º89.36%ã€77.76%å’Œ80.04%ï¼Œä½¿ç”¨çš„æ˜¯å°‘é‡æœ‰æ ‡ç­¾æ•°æ®ã€‚SAMatchæœ‰æ•ˆåœ°è§£å†³äº†åŠç›‘ç£åˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼Œä¸ºæ•°æ®æœ‰é™ç¯å¢ƒä¸­çš„åˆ†å‰²æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/apple1986/SAMatch%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/apple1986/SAMatchè·å–ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16949v1">PDF</a></p><p><strong>Summary</strong></p><p>SAMatchæ¡†æ¶ä»‹ç»ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSAMå¼•å¯¼çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²åŒ¹é…æ¡†æ¶ï¼Œæ—¨åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­æé«˜ä¼ªæ ‡ç­¾è´¨é‡ã€‚å®ƒé€šè¿‡é¢„è®­ç»ƒçš„å¤§å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–ä¸åŒçš„ä»»åŠ¡å¹¶ç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æç¤ºï¼Œè¿›è€Œé€šè¿‡å¾®è°ƒSAMæ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾ã€‚SAMatchæ¡†æ¶é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå…è®¸æ¨¡å‹ä¹‹é—´çš„åŠ¨æ€äº¤äº’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ACDCå¿ƒè„MRIã€BUSIä¹³è…ºè¶…å£°å’ŒMRLiveræ•°æ®é›†ä¸Šï¼ŒSAMatchå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒDiceå¾—åˆ†åˆ†åˆ«ä¸º89.36%ã€77.76%ã€å’Œ80.04%ï¼Œå¹¶ä¸”ä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ•°æ®ã€‚è¯¥æ¡†æ¶è§£å†³äº†åŠç›‘ç£åˆ†å‰²ä¸­çš„æŒ‘æˆ˜ï¼Œä¸ºæ•°æ®æœ‰é™çš„ç¯å¢ƒä¸­çš„åˆ†å‰²æä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>SAMatchæ˜¯ä¸€ä¸ªåŸºäºSAMå¼•å¯¼çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²åŒ¹é…æ¡†æ¶ï¼Œé€‚ç”¨äºæ•°æ®ç¨€ç¼ºåœºæ™¯ã€‚</li><li>SAMatchåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¾ˆå¥½åœ°æ³›åŒ–ä¸åŒä»»åŠ¡ã€‚</li><li>SAMatché€šè¿‡ç”Ÿæˆé«˜ç½®ä¿¡åº¦çš„æç¤ºæ¥ä¼˜åŒ–ä¼ªæ ‡ç­¾è´¨é‡ã€‚</li><li>SAMatché‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒï¼Œå…è®¸æ¨¡å‹é—´çš„åŠ¨æ€äº¤äº’ã€‚</li><li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAMatchå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</li><li>SAMatchçš„Diceå¾—åˆ†åœ¨ACDCå¿ƒè„MRIã€BUSIä¹³è…ºè¶…å£°å’ŒMRLiveræ•°æ®é›†ä¸Šåˆ†åˆ«ä¸º89.36%ã€77.76%ã€å’Œ80.04%ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-36504311e59dc29bdf79f91f7a4c3e3b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-77c34ba33105a02d1bc378ca38d7b70e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b037f0c38dbd548eb850b565ef269bbf.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ace56e497c8eb83353e9c6f1b1cd1f07.jpg" align="middle"></details><h2 id="CP-UNet-Contour-based-Probabilistic-Model-for-Medical-Ultrasound-Images-Segmentation"><a href="#CP-UNet-Contour-based-Probabilistic-Model-for-Medical-Ultrasound-Images-Segmentation" class="headerlink" title="CP-UNet: Contour-based Probabilistic Model for Medical Ultrasound Images   Segmentation"></a>CP-UNet: Contour-based Probabilistic Model for Medical Ultrasound Images Segmentation</h2><p><strong>Authors:Ruiguo Yu, Yiyang Zhang, Yuan Tian, Zhiqiang Liu, Xuewei Li, Jie Gao</strong></p><p>Deep learning-based segmentation methods are widely utilized for detecting lesions in ultrasound images. Throughout the imaging procedure, the attenuation and scattering of ultrasound waves cause contour blurring and the formation of artifacts, limiting the clarity of the acquired ultrasound images. To overcome this challenge, we propose a contour-based probabilistic segmentation model CP-UNet, which guides the segmentation network to enhance its focus on contour during decoding. We design a novel down-sampling module to enable the contour probability distribution modeling and encoding stages to acquire global-local features. Furthermore, the Gaussian Mixture Model utilizes optimized features to model the contour distribution, capturing the uncertainty of lesion boundaries. Extensive experiments with several state-of-the-art deep learning segmentation methods on three ultrasound image datasets show that our method performs better on breast and thyroid lesions segmentation.</p><blockquote><p>åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æ–¹æ³•å¹¿æ³›åº”ç”¨äºè¶…å£°å›¾åƒä¸­çš„ç—…ç¶æ£€æµ‹ã€‚åœ¨æˆåƒè¿‡ç¨‹ä¸­ï¼Œè¶…å£°æ³¢çš„è¡°å‡å’Œæ•£å°„ä¼šå¯¼è‡´è½®å»“æ¨¡ç³Šå’Œä¼ªå½±çš„å½¢æˆï¼Œé™ä½äº†è·å–çš„è¶…å£°å›¾åƒçš„æ¸…æ™°åº¦ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè½®å»“çš„æ¦‚ç‡åˆ†å‰²æ¨¡å‹CP-UNetï¼Œå®ƒå¼•å¯¼åˆ†å‰²ç½‘ç»œåœ¨è§£ç è¿‡ç¨‹ä¸­é‡ç‚¹å…³æ³¨è½®å»“ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ–°å‹çš„ä¸‹é‡‡æ ·æ¨¡å—ï¼Œä»¥å®ç°åœ¨è½®å»“æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡å’Œç¼–ç é˜¶æ®µè·å–å…¨å±€-å±€éƒ¨ç‰¹å¾ã€‚æ­¤å¤–ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹åˆ©ç”¨ä¼˜åŒ–åçš„ç‰¹å¾å¯¹è½®å»“åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œæ•æ‰ç—…ç¶è¾¹ç•Œçš„ä¸ç¡®å®šæ€§ã€‚åœ¨ä¸‰ä¸ªè¶…å£°å›¾åƒæ•°æ®é›†ä¸Šä¸å‡ ç§æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ åˆ†å‰²æ–¹æ³•è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¹³è…ºå’Œç”²çŠ¶è…ºç—…ç¶åˆ†å‰²æ–¹é¢è¡¨ç°æ›´å¥½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14250v1">PDF</a> 4 pages, 4 figures, 2 tables;For icassp2025</p><p><strong>Summary</strong></p><p>æ·±åº¦å­¦ä¹ åœ¨è¶…å£°å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨å·²å¹¿æ³›åº”ç”¨äºç—…ç¶æ£€æµ‹ã€‚ç”±äºè¶…å£°æ³¢çš„è¡°å‡å’Œæ•£å°„å¯¼è‡´çš„è½®å»“æ¨¡ç³Šå’Œä¼ªå½±å½¢æˆï¼Œå½±å“äº†è¶…å£°å›¾åƒçš„æ¸…æ™°åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè½®å»“çš„æ¦‚ç‡åˆ†å‰²æ¨¡å‹CP-UNetï¼Œå¼•å¯¼åˆ†å‰²ç½‘ç»œåœ¨è§£ç è¿‡ç¨‹ä¸­é‡ç‚¹å…³æ³¨è½®å»“ã€‚è®¾è®¡äº†ä¸€ç§æ–°å‹çš„ä¸‹é‡‡æ ·æ¨¡å—ï¼Œç”¨äºå®ç°è½®å»“æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡å’Œç¼–ç é˜¶æ®µçš„å…¨å±€å±€éƒ¨ç‰¹å¾è·å–ã€‚æ­¤å¤–ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹åˆ©ç”¨ä¼˜åŒ–åçš„ç‰¹å¾å¯¹è½®å»“åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œæ•æ‰ç—…ç¶è¾¹ç•Œçš„ä¸ç¡®å®šæ€§ã€‚åœ¨ä¸‰ä¸ªè¶…å£°å›¾åƒæ•°æ®é›†ä¸Šä¸å…ˆè¿›çš„æ·±åº¦å­¦ä¹ åˆ†å‰²æ–¹æ³•è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¹³è…ºå’Œç”²çŠ¶è…ºç—…ç¶åˆ†å‰²æ–¹é¢å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æ·±åº¦å­¦ä¹ åœ¨è¶…å£°å›¾åƒåˆ†å‰²ä¸­å¹¿æ³›åº”ç”¨ï¼Œä¸»è¦ç”¨äºç—…ç¶æ£€æµ‹ã€‚</li><li>è¶…å£°æ³¢å›¾åƒçš„è½®å»“æ¨¡ç³Šå’Œä¼ªå½±å½¢æˆæ˜¯ä¸»è¦çš„æŒ‘æˆ˜ã€‚</li><li>æå‡ºäº†ä¸€ç§åŸºäºè½®å»“çš„æ¦‚ç‡åˆ†å‰²æ¨¡å‹CP-UNetï¼Œå…³æ³¨è½®å»“çš„è§£ç è¿‡ç¨‹ã€‚</li><li>è®¾è®¡äº†ä¸‹é‡‡æ ·æ¨¡å—ï¼Œå®ç°è½®å»“æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡å’Œå…¨å±€å±€éƒ¨ç‰¹å¾çš„è·å–ã€‚</li><li>åˆ©ç”¨é«˜æ–¯æ··åˆæ¨¡å‹æ•æ‰ç—…ç¶è¾¹ç•Œçš„ä¸ç¡®å®šæ€§ã€‚</li><li>åœ¨å¤šä¸ªè¶…å£°å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-103f9740d71d46f37e9b7deba33d789a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b88ad23ecb978a11284d7ace7d7a57bc.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a06dd86a479d5d98de989965c6111ab8.jpg" align="middle"></details><h2 id="DBF-Net-A-Dual-Branch-Network-with-Feature-Fusion-for-Ultrasound-Image-Segmentation"><a href="#DBF-Net-A-Dual-Branch-Network-with-Feature-Fusion-for-Ultrasound-Image-Segmentation" class="headerlink" title="DBF-Net: A Dual-Branch Network with Feature Fusion for Ultrasound Image   Segmentation"></a>DBF-Net: A Dual-Branch Network with Feature Fusion for Ultrasound Image Segmentation</h2><p><strong>Authors:Guoping Xu, Ximing Wu, Wentao Liao, Xinglong Wu, Qing Huang, Chang Li</strong></p><p>Accurately segmenting lesions in ultrasound images is challenging due to the difficulty in distinguishing boundaries between lesions and surrounding tissues. While deep learning has improved segmentation accuracy, there is limited focus on boundary quality and its relationship with body structures. To address this, we introduce UBBS-Net, a dual-branch deep neural network that learns the relationship between body and boundary for improved segmentation. We also propose a feature fusion module to integrate body and boundary information. Evaluated on three public datasets, UBBS-Net outperforms existing methods, achieving Dice Similarity Coefficients of 81.05% for breast cancer, 76.41% for brachial plexus nerves, and 87.75% for infantile hemangioma segmentation. Our results demonstrate the effectiveness of UBBS-Net for ultrasound image segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/apple1986/DBF-Net">https://github.com/apple1986/DBF-Net</a>.</p><blockquote><p>åœ¨è¶…å£°å›¾åƒä¸­ç²¾ç¡®åœ°åˆ†å‰²ç—…å˜ç»„ç»‡æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå¾ˆéš¾åŒºåˆ†ç—…å˜ç»„ç»‡å’Œå‘¨å›´ç»„ç»‡çš„è¾¹ç•Œã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»æé«˜äº†åˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œä½†å¯¹äºè¾¹ç•Œè´¨é‡å’Œå…¶ä¸èº«ä½“ç»“æ„çš„å…³ç³»çš„å…³æ³¨æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†UBBS-Netï¼Œè¿™æ˜¯ä¸€ä¸ªåŒåˆ†æ”¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå®ƒå­¦ä¹ èº«ä½“ä¸è¾¹ç•Œä¹‹é—´çš„å…³ç³»ä»¥æ”¹è¿›åˆ†å‰²ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥æ•´åˆèº«ä½“å’Œè¾¹ç•Œä¿¡æ¯ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒUBBS-Netçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨ä¹³è…ºç™Œã€è‡‚ä¸›ç¥ç»å’Œå©´å„¿è¡€ç®¡ç˜¤çš„åˆ†å‰²ä¸Šå®ç°äº†è¿ªæ–¯ç§‘ç›¸ä¼¼ç³»æ•°ï¼ˆDice Similarity Coefficientï¼‰åˆ†åˆ«ä¸º81.05%ã€76.4 ï¼…å’Œ87.75%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†UBBS-Netåœ¨è¶…å£°å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/apple1986/DBF-Net%E3%80%82">https://github.com/apple1986/DBF-Netã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11116v1">PDF</a></p><p><strong>æ‘˜è¦</strong></p><p>UBBS-Netæ˜¯ä¸€ç§é’ˆå¯¹è¶…å£°å›¾åƒåˆ†å‰²çš„åŒåˆ†æ”¯æ·±åº¦ç¥ç»ç½‘ç»œã€‚å®ƒé€šè¿‡å­¦ä¹ ä¸èº«ä½“ç»“æ„çš„å…³è”æ¥æ”¹å–„è¾¹ç•Œåˆ†å‰²è´¨é‡ã€‚è¯¥ç½‘ç»œå¼•å…¥ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥æ•´åˆèº«ä½“å’Œè¾¹ç•Œä¿¡æ¯ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒUBBS-Netçš„ç‹„æ°ç›¸ä¼¼ç³»æ•°ï¼ˆDice Similarity Coefficientsï¼‰åˆ†åˆ«ä¸ºä¹³è…ºç™Œ81.05%ã€è‡‚ä¸›ç¥ç»76.41%ã€å©´å„¿è¡€ç®¡ç˜¤87.75%ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚ç»“æœè¯æ˜äº†UBBS-Netåœ¨è¶…å£°å›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/apple1986/DBF-Net%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/apple1986/DBF-Netè·å–ã€‚</a></p><p><strong>è¦ç‚¹</strong></p><ol><li>UBBS-Netæ˜¯ä¸€ç§åŒåˆ†æ”¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œç”¨äºè¶…å£°å›¾åƒåˆ†å‰²ï¼Œæ—¨åœ¨æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li><li>ç½‘ç»œå¼•å…¥äº†ç‰¹å¾èåˆæ¨¡å—ï¼Œä»¥æ•´åˆèº«ä½“å’Œè¾¹ç•Œä¿¡æ¯ï¼Œä»è€Œæé«˜è¾¹ç•Œåˆ†å‰²è´¨é‡ã€‚</li><li>UBBS-Netåœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºç™Œã€è‡‚ä¸›ç¥ç»å’Œå©´å„¿è¡€ç®¡ç˜¤çš„åˆ†å‰²æ–¹é¢ã€‚</li><li>ç‹„æ°ç›¸ä¼¼ç³»æ•°æ˜¯è¯„ä¼°è¶…å£°å›¾åƒåˆ†å‰²æ•ˆæœçš„é‡è¦æŒ‡æ ‡ï¼ŒUBBS-Netçš„ç‹„æ°ç›¸ä¼¼ç³»æ•°è¾¾åˆ°äº†è¾ƒé«˜çš„æ°´å¹³ã€‚</li><li>UBBS-Neté€šè¿‡å­¦ä¹ ä¸èº«ä½“ç»“æ„çš„å…³è”æ¥æ”¹å–„è¾¹ç•Œåˆ†å‰²è´¨é‡ï¼Œè¿™æ˜¯å…¶ä¸å…¶ä»–åˆ†å‰²æ–¹æ³•çš„é‡è¦åŒºåˆ«ã€‚</li><li>ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›å…¶ä»–ç ”ç©¶äººå‘˜ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dfda2a016e4244395fd1790bf198a8aa.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5be0ad55d579ba810ab4c49dd87db720.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ed4c00e30ec1bb256e6775e4d5406b80.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-79229d351c98ddebeab72d7d939fba85.jpg" align="middle"></details><h2 id="Deep-Learning-Predicts-Mammographic-Breast-Density-in-Clinical-Breast-Ultrasound-Images"><a href="#Deep-Learning-Predicts-Mammographic-Breast-Density-in-Clinical-Breast-Ultrasound-Images" class="headerlink" title="Deep Learning Predicts Mammographic Breast Density in Clinical Breast   Ultrasound Images"></a>Deep Learning Predicts Mammographic Breast Density in Clinical Breast Ultrasound Images</h2><p><strong>Authors:Arianna Bunnell, Dustin Valdez, Thomas K. Wolfgruber, Brandon Quon, Kailee Hung, Brenda Y. Hernandez, Todd B. Seto, Jeffrey Killeen, Marshall Miyoshi, Peter Sadowski, John A. Shepherd</strong></p><p>Background: Breast density, as derived from mammographic images and defined by the American College of Radiologyâ€™s Breast Imaging Reporting and Data System (BI-RADS), is one of the strongest risk factors for breast cancer. Breast ultrasound (BUS) is an alternative breast cancer screening modality, particularly useful for early detection in low-resource, rural contexts. The purpose of this study was to explore an artificial intelligence (AI) model to predict BI-RADS mammographic breast density category from clinical, handheld BUS imaging. Methods: All data are sourced from the Hawaii and Pacific Islands Mammography Registry. We compared deep learning methods from BUS imaging, as well as machine learning models from image statistics alone. The use of AI-derived BUS density as a risk factor for breast cancer was then compared to clinical BI-RADS breast density while adjusting for age. The BUS data were split by individual into 70&#x2F;20&#x2F;10% groups for training, validation, and testing. Results: 405,120 clinical BUS images from 14.066 women were selected for inclusion in this study, resulting in 9.846 women for training (302,574 images), 2,813 for validation (11,223 images), and 1,406 for testing (4,042 images). On the held-out testing set, the strongest AI model achieves AUROC 0.854 predicting BI-RADS mammographic breast density from BUS imaging and outperforms all shallow machine learning methods based on image statistics. In cancer risk prediction, age-adjusted AI BUS breast density predicted 5-year breast cancer risk with 0.633 AUROC, as compared to 0.637 AUROC from age-adjusted clinical breast density. Conclusions: BI-RADS mammographic breast density can be estimated from BUS imaging with high accuracy using a deep learning model. Furthermore, we demonstrate that AI-derived BUS breast density is predictive of 5-year breast cancer risk in our population.</p><blockquote><p>èƒŒæ™¯ï¼šä¹³è…ºå¯†åº¦æ˜¯ä»ä¹³è…ºXå…‰å›¾åƒä¸­è¡ç”Ÿå‡ºæ¥çš„ï¼Œç”±ç¾å›½æ”¾å°„å­¦é™¢çš„ä¹³è…ºå½±åƒæŠ¥å‘Šå’Œæ•°æ®ç³»ç»Ÿï¼ˆBI-RADSï¼‰å®šä¹‰ï¼Œæ˜¯ä¹³è…ºç™Œçš„æœ€å¼ºé£é™©å› ç´ ä¹‹ä¸€ã€‚ä¹³è…ºè¶…å£°ï¼ˆBUSï¼‰æ˜¯ä¸€ç§æ›¿ä»£çš„ä¹³è…ºç™Œç­›æŸ¥æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„å†œæ‘ç¯å¢ƒä¸­å¯¹æ—©æœŸæ£€æµ‹éå¸¸æœ‰ç”¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢ä¸€ç§äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹ï¼Œæ ¹æ®ä¸´åºŠæ‰‹æŒå¼BUSå›¾åƒé¢„æµ‹BI-RADSä¹³è…ºXå…‰ä¹³è…ºå¯†åº¦ç±»åˆ«ã€‚æ–¹æ³•ï¼šæ‰€æœ‰æ•°æ®å‡æ¥è‡ªå¤å¨å¤·å’Œå¤ªå¹³æ´‹å²›å±¿ä¹³è…ºXå…‰ç™»è®°å¤„ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†æ¥è‡ªBUSæˆåƒçš„æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œä»…åŸºäºå›¾åƒç»Ÿè®¡çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç„¶åï¼Œå°†AIè¡ç”Ÿçš„BUSå¯†åº¦ä½œä¸ºä¹³è…ºç™Œçš„é£é™©å› ç´ ä¸ä¸´åºŠBI-RADSä¹³è…ºå¯†åº¦è¿›è¡Œæ¯”è¾ƒï¼ŒåŒæ—¶è€ƒè™‘å¹´é¾„å› ç´ ã€‚BUSæ•°æ®æŒ‰ä¸ªäººåˆ†ä¸º70&#x2F;20&#x2F;10%çš„ç»„åˆ«ï¼Œç”¨äºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•ã€‚ç»“æœï¼šæœ¬ç ”ç©¶å…¥é€‰äº†æ¥è‡ª14,066åå¦‡å¥³çš„405,120å¼ ä¸´åºŠBUSå›¾åƒï¼Œå…¶ä¸­9,846åå¦‡å¥³ç”¨äºè®­ç»ƒï¼ˆ302,574å¼ å›¾åƒï¼‰ï¼Œ2,813åç”¨äºéªŒè¯ï¼ˆ11,223å¼ å›¾åƒï¼‰ï¼Œ1,406åç”¨äºæµ‹è¯•ï¼ˆ4,042å¼ å›¾åƒï¼‰ã€‚åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šï¼Œè¡¨ç°æœ€ä½³çš„AIæ¨¡å‹çš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ä¸º0.854ï¼Œèƒ½å¤Ÿä»BUSå›¾åƒé¢„æµ‹BI-RADSä¹³è…ºXå…‰ä¹³è…ºå¯†åº¦ï¼Œå¹¶ä¸”åŸºäºå›¾åƒç»Ÿè®¡çš„æµ…å±‚æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½æ›´ä½³ã€‚åœ¨ç™Œç—‡é£é™©é¢„æµ‹æ–¹é¢ï¼Œä¸å¹´é¾„è°ƒæ•´åçš„ä¸´åºŠä¹³è…ºå¯†åº¦ç›¸æ¯”ï¼Œå¹´é¾„è°ƒæ•´çš„AI BUSä¹³è…ºå¯†åº¦é¢„æµ‹çš„5å¹´ä¹³è…ºç™Œé£é™©çš„æ›²çº¿ä¸‹é¢ç§¯ä¸º0.633 AUROCã€‚ç»“è®ºï¼šä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ä»BUSå›¾åƒå‡†ç¡®ä¼°è®¡BI-RADSä¹³è…ºXå…‰ä¹³è…ºå¯†åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜AIè¡ç”Ÿçš„BUSä¹³è…ºå¯†åº¦å¯ä»¥é¢„æµ‹æˆ‘ä»¬äººç¾¤ä¸­çš„5å¹´ä¹³è…ºç™Œé£é™©ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00891v2">PDF</a></p><p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œæ ¹æ®ä¸´åºŠæ‰‹æŒå¼ä¹³è…ºè¶…å£°æ£€æŸ¥å›¾åƒé¢„æµ‹BI-RADSä¹³è…ºå¯†åº¦åˆ†ç±»ã€‚ç ”ç©¶é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œä»…åŸºäºå›¾åƒç»Ÿè®¡çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶æ¯”è¾ƒäº†AIé¢„æµ‹çš„ä¹³è…ºè¶…å£°å¯†åº¦ä¸ä¸´åºŠBI-RADSä¹³è…ºå¯†åº¦ä½œä¸ºä¹³è…ºç™Œé£é™©å› ç´ çš„é¢„æµ‹æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼ŒAIæ¨¡å‹å¯ä»¥ä»ä¹³è…ºè¶…å£°æ£€æŸ¥å›¾åƒä¸­å‡†ç¡®ä¼°è®¡BI-RADSä¹³è…ºå¯†åº¦åˆ†ç±»ï¼Œå¹¶ä¸”å¯¹äº”å¹´ä¹³è…ºç™Œé£é™©çš„é¢„æµ‹è¡¨ç°è‰¯å¥½ã€‚</p><p><strong>Key Takeaways</strong>ï¼š</p><ol><li>æœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨AIæ¨¡å‹ä»æ‰‹æŒå¼ä¹³è…ºè¶…å£°æ£€æŸ¥å›¾åƒé¢„æµ‹BI-RADSä¹³è…ºå¯†åº¦åˆ†ç±»ã€‚</li><li>ç ”ç©¶æ•°æ®æ¥è‡ªå¤å¨å¤·å’Œå¤ªå¹³æ´‹å²›å±¿çš„ä¹³è…ºæ‘„å½±æ³¨å†Œå¤„ã€‚</li><li>ç ”ç©¶é‡‡ç”¨äº†æ·±åº¦å­¦ä¹ å’Œä»…åŸºäºå›¾åƒç»Ÿè®¡çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</li><li>AIæ¨¡å‹å¯ä»¥å‡†ç¡®ä¼°è®¡BI-RADSä¹³è…ºå¯†åº¦åˆ†ç±»ã€‚</li><li>AIé¢„æµ‹çš„ä¹³è…ºè¶…å£°å¯†åº¦ä¸ä¸´åºŠBI-RADSä¹³è…ºå¯†åº¦ç›¸æ¯”ï¼Œåœ¨é¢„æµ‹äº”å¹´ä¹³è…ºç™Œé£é™©æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚</li><li>AIæ¨¡å‹é¢„æµ‹BI-RADSä¹³è…ºå¯†åº¦çš„æœ€ä½³è¡¨ç°æ˜¯AUROC 0.854ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9adeb57ef02c5a99f5f39385f923ac2c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d460005db73d4069a7ca154e972bd229.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5c551cbcd497145648e1b08d95adf254.jpg" align="middle"></details><h2 id="A-Novel-Breast-Ultrasound-Image-Augmentation-Method-Using-Advanced-Neural-Style-Transfer-An-Efficient-and-Explainable-Approach"><a href="#A-Novel-Breast-Ultrasound-Image-Augmentation-Method-Using-Advanced-Neural-Style-Transfer-An-Efficient-and-Explainable-Approach" class="headerlink" title="A Novel Breast Ultrasound Image Augmentation Method Using Advanced   Neural Style Transfer: An Efficient and Explainable Approach"></a>A Novel Breast Ultrasound Image Augmentation Method Using Advanced Neural Style Transfer: An Efficient and Explainable Approach</h2><p><strong>Authors:Lipismita Panigrahi, Prianka Rani Saha, Jurdana Masuma Iqrah, Sushil Prasad</strong></p><p>Clinical diagnosis of breast malignancy (BM) is a challenging problem in the recent era. In particular, Deep learning (DL) models have continued to offer important solutions for early BM diagnosis but their performance experiences overfitting due to the limited volume of breast ultrasound (BUS) image data. Further, large BUS datasets are difficult to manage due to privacy and legal concerns. Hence, image augmentation is a necessary and challenging step to improve the performance of the DL models. However, the current DL-based augmentation models are inadequate and operate as a black box resulting lack of information and justifications about their suitability and efficacy. Additionally, pre and post-augmentation need high-performance computational resources and time to produce the augmented image and evaluate the model performance. Thus, this study aims to develop a novel efficient augmentation approach for BUS images with advanced neural style transfer (NST) and Explainable AI (XAI) harnessing GPU-based parallel infrastructure. We scale and distribute the training of the augmentation model across 8 GPUs using the Horovod framework on a DGX cluster, achieving a 5.09 speedup while maintaining the modelâ€™s accuracy. The proposed model is evaluated on 800 (348 benign and 452 malignant) BUS images and its performance is analyzed with other progressive techniques, using different quantitative analyses. The result indicates that the proposed approach can successfully augment the BUS images with 92.47% accuracy.</p><blockquote><p>ä¹³è…ºç™Œï¼ˆBMï¼‰çš„ä¸´åºŠè¯Šæ–­æ˜¯å½“ä»£çš„ä¸€ä¸ªéš¾é¢˜ã€‚å°¤å…¶æ˜¯æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹åœ¨æ—©æœŸè¯Šæ–­ä¹³è…ºç™Œæ–¹é¢æä¾›äº†é‡è¦çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºä¹³è…ºè¶…å£°ï¼ˆBUSï¼‰å›¾åƒæ•°æ®çš„æ•°é‡æœ‰é™ï¼Œå…¶æ€§èƒ½ä¼šå‡ºç°è¿‡æ‹Ÿåˆç°è±¡ã€‚æ­¤å¤–ï¼Œç”±äºéšç§å’Œæ³•è§„çš„æ‹…å¿§ï¼Œç®¡ç†å¤§å‹çš„BUSæ•°æ®é›†æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œå›¾åƒå¢å¼ºæ˜¯æ”¹è¿›æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„å¿…è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„æ­¥éª¤ã€‚ç„¶è€Œï¼Œå½“å‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å¢å¼ºæ¨¡å‹å­˜åœ¨ä¸è¶³ï¼Œå®ƒä»¬åƒé»‘ç®±ä¸€æ ·è¿ä½œï¼Œç¼ºä¹å…³äºå…¶é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§çš„ä¿¡æ¯å’Œç†ç”±ã€‚æ­¤å¤–ï¼Œå¢å¼ºå‰åéœ€è¦é«˜æ€§èƒ½çš„è®¡ç®—èµ„æºå’Œæ—¶é—´æ¥ç”Ÿæˆå¢å¼ºå›¾åƒå¹¶è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨åˆ©ç”¨åŸºäºGPUçš„å¹¶è¡ŒåŸºç¡€è®¾æ–½å¼€å‘ä¸€ç§æ–°å‹çš„ã€é«˜æ•ˆçš„ä¹³è…ºè¶…å£°å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å…ˆè¿›çš„ç¥ç»æ ·å¼è½¬ç§»ï¼ˆNSTï¼‰å’Œå¯è§£é‡Šçš„AIï¼ˆXAIï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨DGXé›†ç¾¤ä¸Šçš„Horovodæ¡†æ¶ï¼Œåœ¨8ä¸ªGPUä¸Šåˆ†å¸ƒå’Œæ‰©å±•å¢å¼ºæ¨¡å‹çš„è®­ç»ƒï¼Œå®ç°äº†5.09çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨800å¼ ï¼ˆ348å¼ è‰¯æ€§ï¼Œ452å¼ æ¶æ€§ï¼‰BUSå›¾åƒä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä¸å…¶ä»–å…ˆè¿›æŠ€æœ¯è¿›è¡Œæ€§èƒ½åˆ†æï¼Œä½¿ç”¨ä¸åŒçš„å®šé‡åˆ†ææ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æˆåŠŸåœ°å°†BUSå›¾åƒçš„å‡†ç¡®ç‡æé«˜åˆ°92.47%ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00254v1">PDF</a></p><p><strong>æ‘˜è¦</strong></p><p>è¿™ç¯‡æ–‡æœ¬ä»‹ç»äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ å’Œå›¾åƒå¢å¼ºæŠ€æœ¯å¯¹ä¹³è…ºè¶…å£°å›¾åƒè¿›è¡Œè¯Šæ–­çš„æŒ‘æˆ˜ã€‚æ–‡ä¸­æåˆ°ç”±äºæ•°æ®é‡é™åˆ¶ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå› æ­¤æå‡ºäº†åˆ©ç”¨å…ˆè¿›çš„ç¥ç»ç½‘ç»œé£æ ¼è½¬ç§»å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½çš„æ–°çš„é«˜æ•ˆå›¾åƒå¢å¼ºæ–¹æ³•ã€‚è¯¥ç ”ç©¶æ—¨åœ¨å®ç°åœ¨åˆ†å¸ƒå¼GPUç¯å¢ƒä¸­æé«˜æ¨¡å‹è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„å›¾åƒå¢å¼ºå‡†ç¡®ç‡å¯è¾¾92.47%ã€‚</p><p><strong>è¦ç‚¹é€Ÿè§ˆ</strong></p><ul><li>ä¸´åºŠä¹³è…ºæ¶æ€§è‚¿ç˜¤è¯Šæ–­é¢ä¸´æŒ‘æˆ˜ï¼Œæ·±åº¦å­¦ä¹ ä¸ºè§£å†³è¯¥é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œä½†ä»å­˜åœ¨æ•°æ®é™åˆ¶é—®é¢˜ã€‚</li><li>ä¹³è…ºè¶…å£°ï¼ˆBUSï¼‰æ•°æ®é‡æœ‰é™å¯¼è‡´æ·±åº¦å­¦ä¹ æ¨¡å‹è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li><li>å›¾åƒå¢å¼ºæŠ€æœ¯èƒ½æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ç°æœ‰çš„æ–¹æ³•ç¼ºä¹é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚</li><li>æå‡ºä¸€ç§æ–°å‹é«˜æ•ˆçš„å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œç»“åˆäº†ç¥ç»ç½‘ç»œé£æ ¼è½¬ç§»å’Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ã€‚</li><li>ç ”ç©¶é‡‡ç”¨GPUå¹¶è¡ŒåŸºç¡€è®¾æ–½å’ŒHorovodæ¡†æ¶è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œå®ç°äº†æ¨¡å‹çš„åŠ é€Ÿå¹¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eb587b93024e9ff63b8c1f98ad48e2a9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e59dc3b718e635a41150a44fd41f5a9e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1fd7a7ee18f96544ceec6c1aadc30072.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3c7844ebc5b6f8fe2c4d9227e1d0a927.jpg" align="middle"></details><h2 id="MedCLIP-SAMv2-Towards-Universal-Text-Driven-Medical-Image-Segmentation"><a href="#MedCLIP-SAMv2-Towards-Universal-Text-Driven-Medical-Image-Segmentation" class="headerlink" title="MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation"></a>MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image Segmentation</h2><p><strong>Authors:Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao</strong></p><p>Segmentation of anatomical structures and pathological regions in medical images is essential for modern clinical diagnosis, disease research, and treatment planning. While significant advancements have been made in deep learning-based segmentation techniques, many of these methods still suffer from limitations in data efficiency, generalizability, and interactivity. As a result, developing precise segmentation methods that require fewer labeled datasets remains a critical challenge in medical image analysis. Recently, the introduction of foundation models like CLIP and Segment-Anything-Model (SAM), with robust cross-domain representations, has paved the way for interactive and universal image segmentation. However, further exploration of these models for data-efficient segmentation in medical imaging is still needed and highly relevant. In this paper, we introduce MedCLIP-SAMv2, a novel framework that integrates the CLIP and SAM models to perform segmentation on clinical scans using text prompts, in both zero-shot and weakly supervised settings. Our approach includes fine-tuning the BiomedCLIP model with a new Decoupled Hard Negative Noise Contrastive Estimation (DHN-NCE) loss, and leveraging the Multi-modal Information Bottleneck (M2IB) to create visual prompts for generating segmentation masks from SAM in the zero-shot setting. We also investigate using zero-shot segmentation labels within a weakly supervised paradigm to enhance segmentation quality further. Extensive testing across four diverse segmentation tasks and medical imaging modalities (breast tumor ultrasound, brain tumor MRI, lung X-ray, and lung CT) demonstrates the high accuracy of our proposed framework. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/MedCLIP-SAMv2">https://github.com/HealthX-Lab/MedCLIP-SAMv2</a>.</p><blockquote><p>åŒ»å­¦å½±åƒä¸­çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸåˆ†å‰²å¯¹äºç°ä»£ä¸´åºŠè¯Šæ–­ã€ç–¾ç—…ç ”ç©¶å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šè‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºæ·±åº¦å­¦ä¹ çš„åˆ†å‰²æŠ€æœ¯å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è®¸å¤šè¿™äº›æ–¹æ³•åœ¨æ•°æ®æ•ˆç‡ã€é€šç”¨æ€§å’Œäº¤äº’æ€§æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚å› æ­¤ï¼Œå¼€å‘éœ€è¦è¾ƒå°‘æ ‡æ³¨æ•°æ®é›†çš„ç²¾ç¡®åˆ†å‰²æ–¹æ³•ä»ç„¶æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹å…³é”®æŒ‘æˆ˜ã€‚æœ€è¿‘ï¼Œå¼•å…¥å…·æœ‰ç¨³å¥è·¨åŸŸè¡¨ç¤ºèƒ½åŠ›çš„CLIPå’ŒSegment-Anything-Modelï¼ˆSAMï¼‰ç­‰åŸºç¡€æ¨¡å‹ï¼Œä¸ºäº¤äº’å¼å’Œé€šç”¨å›¾åƒåˆ†å‰²é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢è¿™äº›æ¨¡å‹åœ¨åŒ»å­¦æˆåƒä¸­çš„æ•°æ®é«˜æ•ˆåˆ†å‰²ï¼Œè¿™ä»ç„¶æ˜¯éå¸¸ç›¸å…³å’Œå¿…è¦çš„ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19483v3">PDF</a> 10 pages, 2 figures, 6 tables</p><p><strong>Summary</strong><br>è®ºæ–‡æå‡ºMedCLIP-SAMv2æ¡†æ¶ï¼Œç»“åˆCLIPå’ŒSAMæ¨¡å‹ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œä¸´åºŠæ‰«æçš„åˆ†å‰²ï¼Œæ¶‰åŠé›¶æ ·æœ¬å’Œå¼±ç›‘ç£è®¾ç½®ã€‚é€šè¿‡fine-tuning BiomedCLIPæ¨¡å‹å¹¶ä½¿ç”¨DHN-NCEæŸå¤±å’ŒM2IBåˆ›å»ºè§†è§‰æç¤ºï¼Œå®ç°åœ¨åŒ»ç–—å½±åƒä¸­çš„é«˜æ•ˆæ•°æ®åˆ†å‰²ã€‚ç»è¿‡å››é¡¹ä¸åŒåˆ†å‰²ä»»åŠ¡å’ŒåŒ»å­¦å½±åƒæ¨¡æ€çš„å¹¿æ³›æµ‹è¯•ï¼Œè¯æ˜æ¡†æ¶çš„é«˜å‡†ç¡®æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸåˆ†å‰²å¯¹äºç°ä»£ä¸´åºŠè¯Šæ–­ã€ç–¾ç—…ç ”ç©¶å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li><li>è™½ç„¶æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨æ•°æ®æ•ˆç‡ã€é€šç”¨æ€§å’Œäº¤äº’æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li><li>MedCLIP-SAMv2æ¡†æ¶ç»“åˆäº†CLIPå’ŒSAMæ¨¡å‹ï¼Œå®ç°äº†åœ¨é›¶æ ·æœ¬å’Œå¼±ç›‘ç£è®¾ç½®ä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li><li>é€šè¿‡fine-tuning BiomedCLIPæ¨¡å‹å¹¶ä½¿ç”¨DHN-NCEæŸå¤±æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li><li>åˆ©ç”¨M2IBåˆ›å»ºè§†è§‰æç¤ºï¼Œç”ŸæˆSAMçš„åˆ†å‰²æ©è†œã€‚</li><li>è®ºæ–‡åœ¨å¤šç§åŒ»å­¦å½±åƒæ¨¡æ€ï¼ˆå¦‚ä¹³è…ºè‚¿ç˜¤è¶…å£°ã€è„‘è‚¿ç˜¤MRIã€è‚ºéƒ¨Xå…‰å’ŒCTï¼‰è¿›è¡Œäº†å¹¿æ³›æµ‹è¯•ï¼Œè¯æ˜äº†æ¡†æ¶çš„é«˜å‡†ç¡®æ€§ã€‚</li><li>è®ºæ–‡ä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4670d2180ed1edde3865b930bf26eb1f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5f0d9bffea65fee14655e32f58031915.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3f6b508e429bd5879e5ce251ee795876.jpg" align="middle"></details><h2 id="Modifying-the-U-Netâ€™s-Encoder-Decoder-Architecture-for-Segmentation-of-Tumors-in-Breast-Ultrasound-Images"><a href="#Modifying-the-U-Netâ€™s-Encoder-Decoder-Architecture-for-Segmentation-of-Tumors-in-Breast-Ultrasound-Images" class="headerlink" title="Modifying the U-Netâ€™s Encoder-Decoder Architecture for Segmentation of   Tumors in Breast Ultrasound Images"></a>Modifying the U-Netâ€™s Encoder-Decoder Architecture for Segmentation of Tumors in Breast Ultrasound Images</h2><p><strong>Authors:Sina Derakhshandeh, Ali Mahloojifar</strong></p><p>Segmentation is one of the most significant steps in image processing. Segmenting an image is a technique that makes it possible to separate a digital image into various areas based on the different characteristics of pixels in the image. In particular, segmentation of breast ultrasound images is widely used for cancer identification. As a result of image segmentation, it is possible to make early diagnoses of a diseases via medical images in a very effective way. Due to various ultrasound artifacts and noises, including speckle noise, low signal-to-noise ratio, and intensity heterogeneity, the process of accurately segmenting medical images, such as ultrasound images, is still a challenging task. In this paper, we present a new method to improve the accuracy and effectiveness of breast ultrasound image segmentation. More precisely, we propose a Neural Network (NN) based on U-Net and an encoder-decoder architecture. By taking U-Net as the basis, both encoder and decoder parts are developed by combining U-Net with other Deep Neural Networks (Res-Net and MultiResUNet) and introducing a new approach and block (Co-Block), which preserve as much as possible the low-level and the high-level features. Designed network is evaluated using the Breast Ultrasound Images (BUSI) Dataset. It consists of 780 images and the images are categorized into three classes, which are normal, benign, and malignant. According to our extensive evaluations on a public breast ultrasound dataset, designed network segments the breast lesions more accurately than other state-of-the-art deep learning methods. With only 8.88M parameters, our network (CResU-Net) obtained 82.88%, 77.5%, 90.3%, and 98.4% in terms of Dice similarity coefficients (DSC), Intersection over Union (IoU), Area under curve (AUC), and global accuracy (ACC), respectively, on BUSI dataset.</p><blockquote><p>åˆ†å‰²æ˜¯å›¾åƒå¤„ç†ä¸­æœ€å…³é”®çš„æ­¥éª¤ä¹‹ä¸€ã€‚å›¾åƒåˆ†å‰²æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œèƒ½å¤Ÿå°†æ•°å­—å›¾åƒåŸºäºå›¾åƒä¸­åƒç´ çš„ä¸åŒç‰¹å¾åˆ’åˆ†ä¸ºä¸åŒçš„åŒºåŸŸã€‚ç‰¹åˆ«æ˜¯ï¼Œä¹³æˆ¿è¶…å£°æ³¢å›¾åƒçš„åˆ†å‰²è¢«å¹¿æ³›ç”¨äºç™Œç—‡çš„è¯†åˆ«ã€‚é€šè¿‡å›¾åƒåˆ†å‰²ï¼Œå¯ä»¥æœ‰æ•ˆåœ°é€šè¿‡åŒ»å­¦å›¾åƒè¿›è¡Œæ—©æœŸç–¾ç—…è¯Šæ–­ã€‚ç”±äºå„ç§è¶…å£°æ³¢ä¼ªå½±å’Œå™ªå£°ï¼ŒåŒ…æ‹¬æ–‘ç‚¹å™ªå£°ã€ä½ä¿¡å™ªæ¯”å’Œå¼ºåº¦å¼‚è´¨æ€§ï¼Œå‡†ç¡®åˆ†å‰²åŒ»å­¦å›¾åƒï¼ˆå¦‚è¶…å£°æ³¢å›¾åƒï¼‰ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æé«˜ä¹³æˆ¿è¶…å£°æ³¢å›¾åƒåˆ†å‰²å‡†ç¡®æ€§å’Œæ•ˆç‡çš„æ–°æ–¹æ³•ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºU-Netçš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ï¼Œå¹¶ç»“åˆäº†ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æˆ‘ä»¬ä»¥U-Netä¸ºåŸºç¡€ï¼Œç»“åˆU-Netä¸å…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆRes-Netå’ŒMultiResUNetï¼‰è¿›è¡Œå¼€å‘ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•å’Œå—ï¼ˆCo-Blockï¼‰ï¼Œå°½å¯èƒ½åœ°ä¿ç•™ä½çº§å’Œé«˜çº§ç‰¹å¾ã€‚æ‰€è®¾è®¡çš„ç½‘ç»œä½¿ç”¨ä¹³æˆ¿è¶…å£°å›¾åƒï¼ˆBUSIï¼‰æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ•°æ®é›†åŒ…å«780å¼ å›¾åƒï¼Œè¿™äº›å›¾åƒåˆ†ä¸ºä¸‰ç±»ï¼šæ­£å¸¸ã€è‰¯æ€§åŠæ¶æ€§ã€‚æ ¹æ®æˆ‘ä»¬åœ¨å…¬å…±ä¹³æˆ¿è¶…å£°æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°ï¼Œæ‰€è®¾è®¡çš„ç½‘ç»œæ¯”å…¶ä»–æœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ›´å‡†ç¡®åœ°åˆ†å‰²äº†ä¹³è…ºç—…å˜ã€‚æˆ‘ä»¬çš„ç½‘ç»œï¼ˆCResU-Netï¼‰ä»…æœ‰880ä¸‡ä¸ªå‚æ•°ï¼Œåœ¨BUSIæ•°æ®é›†ä¸Šè·å¾—äº†82.88ï¼…ã€77.5ï¼…ã€90.3ï¼…å’Œ98.4ï¼…çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ã€äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ã€æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰å’Œå…¨å±€å‡†ç¡®ç‡ï¼ˆACCï¼‰ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00647v2">PDF</a></p><p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºU-Netå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºæé«˜ä¹³è…ºè¶…å£°å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚é€šè¿‡ç»“åˆU-Netä¸å…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚Res-Netå’ŒMultiResUNetï¼‰ï¼Œå¹¶å¼•å…¥æ–°çš„æ–¹æ³•å’Œå—ï¼ˆCo-Blockï¼‰ï¼Œæå‡ºç½‘ç»œCResU-Netèƒ½å¤Ÿåœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šæ›´å‡†ç¡®åœ°åˆ†å‰²ä¹³è…ºç—…å˜ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å›¾åƒå¤„ç†ä¸­ï¼Œåˆ†å‰²æ˜¯æœ€é‡è¦çš„æ­¥éª¤ä¹‹ä¸€ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºè¶…å£°å›¾åƒä¸­ï¼Œç”¨äºç™Œç—‡è¯†åˆ«ã€‚</li><li>è¶…å£°å›¾åƒåˆ†å‰²é¢ä¸´å¤šç§è¶…å£°ä¼ªåƒå’Œå™ªå£°çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ–‘ç‚¹å™ªå£°ã€ä½ä¿¡å™ªæ¯”å’Œå¼ºåº¦å¼‚è´¨æ€§ã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºU-Netå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„ç¥ç»ç½‘ç»œï¼Œä»¥æé«˜ä¹³è…ºè¶…å£°å›¾åƒåˆ†å‰²çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li><li>è¯¥ç½‘ç»œç»“åˆäº†U-Netä¸å…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚Res-Netå’ŒMultiResUNetï¼‰ï¼Œå¹¶å¼•å…¥äº†Co-Blockï¼Œä»¥å°½å¯èƒ½ä¿ç•™ä½çº§åˆ«å’Œé«˜çº§åˆ«çš„ç‰¹å¾ã€‚</li><li>åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥ç½‘ç»œï¼ˆCResU-Netï¼‰èƒ½å¤Ÿæ›´å‡†ç¡®åœ°åˆ†å‰²ä¹³è…ºç—…å˜ï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½æ›´ä½³ã€‚</li><li>CResU-Netç½‘ç»œå‚æ•°ä»…æœ‰8.88Mï¼Œåœ¨BUSIæ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„è¡¨ç°ï¼Œå…¶Diceç›¸ä¼¼æ€§ç³»æ•°ï¼ˆDSCï¼‰ã€äº¤å¹¶æ¯”ï¼ˆIoUï¼‰ã€æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰å’Œå…¨å±€å‡†ç¡®ç‡ï¼ˆACCï¼‰åˆ†åˆ«ä¸º82.88%ã€77.5%ã€90.3%å’Œ98.4%ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-258c412e41e50247d3e553f158687fbd.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf1c90cab548e4afac7e982938902dcc.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-40b65a71094a083da69aa8bae3d35f8c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ec605a6afccccb7b2602fd03b6f362b9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0d99ccddfe4bcef2fe3e8059a8443456.jpg" align="middle"></details><h2 id="BUSClean-Open-source-software-for-breast-ultrasound-image-pre-processing-and-knowledge-extraction-for-medical-AI"><a href="#BUSClean-Open-source-software-for-breast-ultrasound-image-pre-processing-and-knowledge-extraction-for-medical-AI" class="headerlink" title="BUSClean: Open-source software for breast ultrasound image   pre-processing and knowledge extraction for medical AI"></a>BUSClean: Open-source software for breast ultrasound image pre-processing and knowledge extraction for medical AI</h2><p><strong>Authors:Arianna Bunnell, Kailee Hung, John A. Shepherd, Peter Sadowski</strong></p><p>Development of artificial intelligence (AI) for medical imaging demands curation and cleaning of large-scale clinical datasets comprising hundreds of thousands of images. Some modalities, such as mammography, contain highly standardized imaging. In contrast, breast ultrasound imaging (BUS) can contain many irregularities not indicated by scan metadata, such as enhanced scan modes, sonographer annotations, or additional views. We present an open-source software solution for automatically processing clinical BUS datasets. The algorithm performs BUS scan filtering (flagging of invalid and non-B-mode scans), cleaning (dual-view scan detection, scan area cropping, and caliper detection), and knowledge extraction (BI-RADS Labeling and Measurement fields) from sonographer annotations. Its modular design enables users to adapt it to new settings. Experiments on an internal testing dataset of 430 clinical BUS images achieve &gt;95% sensitivity and &gt;98% specificity in detecting every type of text annotation, &gt;98% sensitivity and specificity in detecting scans with blood flow highlighting, alternative scan modes, or invalid scans. A case study on a completely external, public dataset of BUS scans found that BUSClean identified text annotations and scans with blood flow highlighting with 88.6% and 90.9% sensitivity and 98.3% and 99.9% specificity, respectively. Adaptation of the lesion caliper detection method to account for a type of caliper specific to the case study demonstrates the intended use of BUSClean in new data distributions and improved performance in lesion caliper detection from 43.3% and 93.3% out-of-the-box to 92.1% and 92.3% sensitivity and specificity, respectively. Source code, example notebooks, and sample data are available at <a target="_blank" rel="noopener" href="https://github.com/hawaii-ai/bus-cleaning">https://github.com/hawaii-ai/bus-cleaning</a>.</p><blockquote><p>é’ˆå¯¹åŒ»å­¦å½±åƒé¢†åŸŸçš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¼€å‘ï¼Œéœ€è¦æ•´ç†å¤§è§„æ¨¡çš„ä¸´åºŠæ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†åŒ…å«æ•°åä¸‡å¼ å›¾åƒã€‚æŸäº›æ¨¡æ€ï¼ˆå¦‚ä¹³è…ºæ‘„å½±ï¼‰çš„æˆåƒéå¸¸æ ‡å‡†åŒ–ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¹³è…ºè¶…å£°æˆåƒï¼ˆBUSï¼‰å¯èƒ½åŒ…å«è®¸å¤šç”±æ‰«æå…ƒæ•°æ®æœªæŒ‡ç¤ºçš„ä¸è§„åˆ™æ€§ï¼Œä¾‹å¦‚å¢å¼ºçš„æ‰«ææ¨¡å¼ã€è¶…å£°åŒ»å¸ˆçš„æ³¨é‡Šæˆ–é¢å¤–çš„è§†å›¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‡ªåŠ¨å¤„ç†ä¸´åºŠBUSæ•°æ®é›†çš„å¼€æºè½¯ä»¶è§£å†³æ–¹æ¡ˆã€‚è¯¥ç®—æ³•æ‰§è¡ŒBUSæ‰«æè¿‡æ»¤ï¼ˆæ ‡è®°æ— æ•ˆå’ŒéBæ¨¡å¼æ‰«æï¼‰ã€æ¸…æ´å¤„ç†ï¼ˆåŒè§†å›¾æ‰«ææ£€æµ‹ã€æ‰«æåŒºåŸŸè£å‰ªå’Œå¤¹å…·æ£€æµ‹ï¼‰å’ŒçŸ¥è¯†æå–ï¼ˆä»è¶…å£°åŒ»å¸ˆæ³¨é‡Šä¸­æå–BI-RADSæ ‡ç­¾å’Œæµ‹é‡å­—æ®µï¼‰ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡ä½¿ç”¨æˆ·èƒ½å¤Ÿå°†å…¶é€‚åº”æ–°ç¯å¢ƒã€‚åœ¨åŒ…å«430å¼ ä¸´åºŠBUSå›¾åƒçš„å†…éƒ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ£€æµ‹æ¯ç§ç±»å‹çš„æ–‡æœ¬æ³¨é‡Šçš„çµæ•åº¦å’Œç‰¹å¼‚åº¦å‡å¤§äº95%å’Œ98%ï¼Œæ£€æµ‹å¸¦æœ‰è¡€æµçªå‡ºæ˜¾ç¤ºã€æ›¿ä»£æ‰«ææ¨¡å¼æˆ–æ— æ•ˆæ‰«æçš„çµæ•åº¦åŠç‰¹å¼‚åº¦å‡å¤§äº98%ã€‚å¦ä¸€é¡¹å…³äºå®Œå…¨å¤–éƒ¨çš„å…¬å¼€BUSæ‰«ææ•°æ®é›†çš„ç ”ç©¶å‘ç°ï¼ŒBUSCleanå¯¹æ–‡æœ¬æ³¨é‡Šå’Œå¸¦æœ‰è¡€æµçªå‡ºæ˜¾ç¤ºçš„æ‰«æçš„è¯†åˆ«æ•æ„Ÿåº¦åˆ†åˆ«ä¸º88.6%å’Œ90.9%ï¼Œç‰¹å¼‚åº¦åˆ†åˆ«ä¸º98.3%å’Œ99.9%ã€‚ä¸ºé€‚åº”æ¡ˆä¾‹ç ”ç©¶ä¸­ç‰¹å®šç±»å‹çš„å¤¹å…·è€Œè¿›è¡Œç—…å˜å¤¹å…·æ£€æµ‹æ–¹æ³•çš„è°ƒæ•´ï¼Œè¯æ˜äº†BUSCleanåœ¨æ–°æ•°æ®åˆ†å¸ƒä¸­çš„é¢„æœŸç”¨é€”ä»¥åŠç—…å˜å¤¹å…·æ£€æµ‹æ€§èƒ½çš„æ”¹è¿›ï¼Œä»åˆå§‹çš„çµæ•åº¦å’Œç‰¹å¼‚åº¦ä¸º43.3%å’Œ93.3%ï¼Œæå‡åˆ°ä½¿ç”¨BUSCleanåçš„92.1%å’Œ92.3%ã€‚æºä»£ç ã€ç¤ºä¾‹ç¬”è®°æœ¬å’Œæ ·æœ¬æ•°æ®å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hawaii-ai/bus-cleaning%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hawaii-ai/bus-cleaningè·å–ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11316v3">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹é’ˆå¯¹ä¹³è…ºè¶…å£°å½±åƒçš„å¼€æºè½¯ä»¶è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè‡ªåŠ¨å¤„ç†ä¸´åºŠæ•°æ®é›†ã€‚è¯¥è½¯ä»¶èƒ½å¤Ÿè¿›è¡Œä¹³è…ºè¶…å£°æ‰«æçš„è¿‡æ»¤ã€æ¸…æ´å’ŒçŸ¥è¯†æå–ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ ‡æ³¨æ£€æµ‹ã€è¡€æµé«˜äº®æ‰«ææ£€æµ‹ã€æ›¿ä»£æ‰«ææ¨¡å¼åŠæ— æ•ˆæ‰«ææ£€æµ‹ç­‰ã€‚è½¯ä»¶æ¨¡å—åŒ–çš„è®¾è®¡ä½¿å…¶èƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒã€‚ç»è¿‡å¯¹å†…éƒ¨æµ‹è¯•æ•°æ®é›†çš„å®éªŒéªŒè¯ï¼Œå…¶æ•æ„Ÿæ€§å¤§äº95%ï¼Œç‰¹å¼‚æ€§å¤§äº98%ã€‚æ­¤å¤–ï¼Œè¯¥è§£å†³æ–¹æ¡ˆè¿˜å±•ç¤ºäº†åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è‰¯å¥½æ€§èƒ½ï¼Œå¹¶æä¾›äº†æºä»£ç ã€ç¤ºä¾‹ç¬”è®°æœ¬å’Œæ ·æœ¬æ•°æ®ä»¥ä¾›ä¸‹è½½å’Œä½¿ç”¨ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è¯¥è½¯ä»¶è§£å†³æ–¹æ¡ˆä¸“ä¸ºä¹³è…ºè¶…å£°å½±åƒä¸´åºŠæ•°æ®é›†è®¾è®¡ï¼Œç”¨äºè‡ªåŠ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®ã€‚</li><li>è½¯ä»¶èƒ½å¤Ÿè¿›è¡Œä¹³è…ºè¶…å£°æ‰«æçš„è¿‡æ»¤ã€æ¸…æ´å’ŒçŸ¥è¯†æå–ï¼ŒåŒ…æ‹¬å¤šç§æ£€æµ‹åŠŸèƒ½ã€‚</li><li>è½¯ä»¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç¯å¢ƒå’Œéœ€æ±‚ã€‚</li><li>åœ¨å†…éƒ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºå…¶é«˜æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ã€‚</li><li>è¯¥è§£å†³æ–¹æ¡ˆåœ¨å…¬å…±æ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0f51a53ab5c5ab03bd6c44701b735551.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-911efde3595eb8ce5c6627f15b896a2b.jpg" align="middle"></details><h2 id="Exploiting-Precision-Mapping-and-Component-Specific-Feature-Enhancement-for-Breast-Cancer-Segmentation-and-Identification"><a href="#Exploiting-Precision-Mapping-and-Component-Specific-Feature-Enhancement-for-Breast-Cancer-Segmentation-and-Identification" class="headerlink" title="Exploiting Precision Mapping and Component-Specific Feature Enhancement   for Breast Cancer Segmentation and Identification"></a>Exploiting Precision Mapping and Component-Specific Feature Enhancement for Breast Cancer Segmentation and Identification</h2><p><strong>Authors:Pandiyaraju V, Shravan Venkatraman, Pavan Kumar S, Santhosh Malarvannan, Kannan A</strong></p><p>Breast cancer is one of the leading causes of death globally, and thus there is an urgent need for early and accurate diagnostic techniques. Although ultrasound imaging is a widely used technique for breast cancer screening, it faces challenges such as poor boundary delineation caused by variations in tumor morphology and reduced diagnostic accuracy due to inconsistent image quality. To address these challenges, we propose novel Deep Learning (DL) frameworks for breast lesion segmentation and classification. We introduce a precision mapping mechanism (PMM) for a precision mapping and attention-driven LinkNet (PMAD-LinkNet) segmentation framework that dynamically adapts spatial mappings through morphological variation analysis, enabling precise pixel-level refinement of tumor boundaries. Subsequently, we introduce a component-specific feature enhancement module (CSFEM) for a component-specific feature-enhanced classifier (CSFEC-Net). Through a multi-level attention approach, the CSFEM magnifies distinguishing features of benign, malignant, and normal tissues. The proposed frameworks are evaluated against existing literature and a diverse set of state-of-the-art Convolutional Neural Network (CNN) architectures. The obtained results show that our segmentation model achieves an accuracy of 98.1%, an IoU of 96.9%, and a Dice Coefficient of 97.2%. For the classification model, an accuracy of 99.2% is achieved with F1-score, precision, and recall values of 99.1%, 99.3%, and 99.1%, respectively.</p><blockquote><p>ä¹³è…ºç™Œæ˜¯å…¨çƒå¯¼è‡´æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ€¥éœ€æ—©æœŸä¸”å‡†ç¡®çš„è¯Šæ–­æŠ€æœ¯ã€‚è™½ç„¶è¶…å£°æˆåƒå¹¿æ³›ç”¨äºä¹³è…ºç™Œç­›æŸ¥ï¼Œä½†å®ƒä»é¢ä¸´ç€ç”±äºè‚¿ç˜¤å½¢æ€å˜åŒ–å¯¼è‡´çš„è¾¹ç•Œåˆ’åˆ†ä¸æ¸…ä»¥åŠå›¾åƒè´¨é‡ä¸ä¸€è‡´å¯¼è‡´çš„è¯Šæ–­å‡†ç¡®æ€§é™ä½ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä¹³è…ºç—…ç¶åˆ†å‰²å’Œåˆ†ç±»çš„æ–°å‹æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬ä¸ºç²¾ç¡®æ˜ å°„å’Œæ³¨æ„åŠ›é©±åŠ¨LinkNetï¼ˆPMAD-LinkNetï¼‰åˆ†å‰²æ¡†æ¶å¼•å…¥äº†ä¸€ç§ç²¾åº¦æ˜ å°„æœºåˆ¶ï¼ˆPMMï¼‰ï¼Œé€šè¿‡å½¢æ€å˜åŒ–åˆ†æåŠ¨æ€é€‚åº”ç©ºé—´æ˜ å°„ï¼Œå®ç°å¯¹è‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§ç»†åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä¸ºç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºåˆ†ç±»å™¨ï¼ˆCSFEC-Netï¼‰å¼•å…¥äº†ä¸€ä¸ªç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆCSFEMï¼‰ã€‚é€šè¿‡å¤šçº§æ³¨æ„åŠ›æ–¹æ³•ï¼ŒCSFEMæ”¾å¤§äº†è‰¯æ€§ã€æ¶æ€§å’Œæ­£å¸¸ç»„ç»‡çš„åŒºåˆ†ç‰¹å¾ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ç°æœ‰æ–‡çŒ®å’Œä¸€ç³»åˆ—å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„è¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒã€‚è·å¾—çš„ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„åˆ†å‰²æ¨¡å‹è¾¾åˆ°äº†98.1%çš„å‡†ç¡®ç‡ï¼Œ96.9%çš„IoUï¼Œä»¥åŠ97.2%çš„Diceç³»æ•°ã€‚åˆ†ç±»æ¨¡å‹æ–¹é¢ï¼Œä½¿ç”¨F1åˆ†æ•°ã€ç²¾ç¡®åº¦ã€å¬å›ç‡åˆ†åˆ«è¾¾åˆ°äº†99.2%ã€99.1%ã€99.3%ã€å’Œ99.1%ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02844v5">PDF</a> 27 pages, 18 figures, 6 tables</p><p><strong>Summary</strong><br>ä¹³è…ºç™Œæ˜¯å…¨çƒä¸»è¦çš„è‡´æ­»åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ—©æœŸå‡†ç¡®è¯Šæ–­æŠ€æœ¯è‡³å…³é‡è¦ã€‚é’ˆå¯¹è¶…å£°æˆåƒåœ¨ä¹³è…ºç™Œç­›æŸ¥ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è‚¿ç˜¤å½¢æ€å˜åŒ–å¯¼è‡´çš„è¾¹ç•Œä¸æ¸…å’Œå›¾åƒè´¨é‡ä¸ä¸€è‡´å½±å“è¯Šæ–­å‡†ç¡®åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹çš„æ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œä¹³è…ºç—…ç¶åˆ†å‰²å’Œåˆ†ç±»ã€‚æå‡ºç²¾å‡†æ˜ å°„æœºåˆ¶ï¼ˆPMMï¼‰å’Œæ³¨æ„åŠ›é©±åŠ¨LinkNetï¼ˆPMAD-LinkNetï¼‰åˆ†å‰²æ¡†æ¶ï¼Œå¯¹è‚¿ç˜¤è¾¹ç•Œè¿›è¡Œç²¾ç¡®åƒç´ çº§ä¼˜åŒ–ã€‚åŒæ—¶ï¼Œå¼•å…¥ç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆCSFEMï¼‰å’Œç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºåˆ†ç±»å™¨ï¼ˆCSFEC-Netï¼‰ï¼Œé€šè¿‡å¤šå±‚æ¬¡æ³¨æ„åŠ›æ–¹æ³•å¼ºåŒ–è‰¯ã€æ¶æ€§å’Œæ­£å¸¸ç»„ç»‡çš„è¾¨è¯†ç‰¹å¾ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåˆ†å‰²æ¨¡å‹å‡†ç¡®åº¦è¾¾98.1%ï¼ŒIoUå’ŒDiceç³»æ•°åˆ†åˆ«ä¸º96.9%å’Œ97.2%ï¼›åˆ†ç±»æ¨¡å‹å‡†ç¡®åº¦ä¸º99.2%ï¼ŒF1åˆ†æ•°ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«ä¸º99.1%ã€99.3%å’Œ99.1%ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ä¹³è…ºç™Œæ˜¯å…¨çƒä¸»è¦çš„å¥åº·é—®é¢˜ï¼Œéœ€è¦æ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­æŠ€æœ¯ã€‚</li><li>è¶…å£°æˆåƒåœ¨ä¹³è…ºç™Œç­›æŸ¥ä¸­é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è‚¿ç˜¤å½¢æ€å˜åŒ–å’Œå›¾åƒè´¨é‡ä¸ä¸€è‡´ã€‚</li><li>å¼•å…¥æ–°å‹æ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œä¹³è…ºç—…ç¶åˆ†å‰²å’Œåˆ†ç±»ã€‚</li><li>ç²¾å‡†æ˜ å°„æœºåˆ¶ï¼ˆPMMï¼‰å’Œæ³¨æ„åŠ›é©±åŠ¨LinkNetï¼ˆPMAD-LinkNetï¼‰ç”¨äºç²¾ç¡®åƒç´ çº§ä¼˜åŒ–è‚¿ç˜¤è¾¹ç•Œã€‚</li><li>ç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆCSFEMï¼‰å¼ºåŒ–è‰¯ã€æ¶æ€§å’Œæ­£å¸¸ç»„ç»‡çš„è¾¨è¯†ç‰¹å¾ã€‚</li><li>åˆ†å‰²æ¨¡å‹è¯„ä¼°ç»“æœï¼šå‡†ç¡®åº¦98.1%ï¼ŒIoU 96.9%ï¼ŒDiceç³»æ•°97.2%ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-16b8efc7decc6275f611191f2d7f1b92.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e73a377ac493de01906c5aad1842257f.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">æ–‡ç« ä½œè€…:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">æ–‡ç« é“¾æ¥:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">ç‰ˆæƒå£°æ˜:</i></span> <span class="reprint-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/"><span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;ä¸Šä¸€ç¯‡</div><div class="card"><a href="/Talk2Paper/Paper/2024-12-10/3DGS/"><div class="card-image"><img src="https://picx.zhimg.com/v2-084788d1a3c9bf4a759bc1df2b42fdad.jpg" class="responsive-img" alt="3DGS"> <span class="card-title">3DGS</span></div></a><div class="card-content article-content"><div class="summary block-with-text">3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11 Deblur4DGS 4D Gaussian Splatting from Blurry Monocular Video</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-11</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/3DGS/" class="post-category">3DGS</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/3DGS/"><span class="chip bg-color">3DGS</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-12-10/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"><div class="card-image"><img src="https://picx.zhimg.com/v2-f7e8a03334c7b295d75eec44a9ef4132.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ "> <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span></div></a><div class="card-content article-content"><div class="summary block-with-text">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11 Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-11</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"><span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span class="white-color">5755.2k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="æœ¬ç«™å·²è¿è¡Œ "+c+" å¤©",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="æœ¬ç«™å·²è¿è¡Œ "+l+" å¹´ "+c+" å¤©",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">çŸ¥</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;æœç´¢</span> <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ",clearTimeout(st)):(document.title="Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>