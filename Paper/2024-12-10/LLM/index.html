<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  Training Large Language Models to Reason in a Continuous Latent Space">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-09dcc93b56c00370d2981ad1d66b47e1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    29.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    118 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-10-æ›´æ–°"><a href="#2024-12-10-æ›´æ–°" class="headerlink" title="2024-12-10 æ›´æ–°"></a>2024-12-10 æ›´æ–°</h1><h2 id="Training-Large-Language-Models-to-Reason-in-a-Continuous-Latent-Space"><a href="#Training-Large-Language-Models-to-Reason-in-a-Continuous-Latent-Space" class="headerlink" title="Training Large Language Models to Reason in a Continuous Latent Space"></a>Training Large Language Models to Reason in a Continuous Latent Space</h2><p><strong>Authors:Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, Yuandong Tian</strong></p>
<p>Large language models (LLMs) are restricted to reason in the â€œlanguage spaceâ€, where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed â€œcontinuous thoughtâ€). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å—é™äºâ€œè¯­è¨€ç©ºé—´â€å†…çš„æ¨ç†ï¼Œå®ƒä»¬é€šå¸¸é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰è§£å†³å¤æ‚çš„æ¨ç†é—®é¢˜æ¥è¡¨è¾¾æ¨ç†è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºè¯­è¨€ç©ºé—´å¹¶ä¸æ€»æ˜¯æœ€é€‚åˆæ¨ç†ã€‚ä¾‹å¦‚ï¼Œå¤§å¤šæ•°å•è¯ç¬¦å·ä¸»è¦ç”¨äºæ–‡æœ¬è¿è´¯æ€§ï¼Œå¯¹äºæ¨ç†å¹¶ä¸å¿…è¦ï¼Œè€Œä¸€äº›å…³é”®ç¬¦å·åˆ™éœ€è¦å¤æ‚è§„åˆ’ï¼Œç»™LLMå¸¦æ¥å·¨å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†æ¢ç´¢LLMåœ¨ä¸å—é™åˆ¶æ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†æ½œåŠ›ï¼Œè€Œä¸æ˜¯ä½¿ç”¨è‡ªç„¶è¯­è¨€ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„èŒƒå¼â€”â€”æ¤°å­æ€ç»´ï¼ˆè¿ç»­æ€ç»´é“¾ï¼‰ã€‚æˆ‘ä»¬åˆ©ç”¨LLMçš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºæ¨ç†çŠ¶æ€çš„è¡¨ç¤ºï¼ˆç§°ä¸ºâ€œè¿ç»­æ€ç»´â€ï¼‰ã€‚æˆ‘ä»¬ä¸æ˜¯å°†æ­¤è§£ç ä¸ºå•è¯ç¬¦å·ï¼Œè€Œæ˜¯å°†å…¶åé¦ˆåˆ°LLMä¸­ï¼Œä½œä¸ºæ½œåœ¨ç©ºé—´ä¸­çš„åç»­è¾“å…¥åµŒå…¥ç›´æ¥è¾“å…¥ã€‚å®éªŒè¡¨æ˜ï¼Œæ¤°å­æ€ç»´å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºLLMåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™ç§æ–°å‹æ½œåœ¨æ¨ç†èŒƒå¼å¯¼è‡´äº†æ–°å…´çš„é«˜çº§æ¨ç†æ¨¡å¼ï¼šè¿ç»­æ€ç»´å¯ä»¥ç¼–ç å¤šä¸ªæ›¿ä»£çš„ä¸‹ä¸€æ­¥æ¨ç†æ­¥éª¤ï¼Œå…è®¸æ¨¡å‹è¿›è¡Œå¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰æ¥è§£å†³é—®é¢˜ï¼Œè€Œä¸æ˜¯è¿‡æ—©åœ°æ‰¿è¯ºå•ä¸€ç¡®å®šæ€§è·¯å¾„ï¼Œå¦‚åŒCoTã€‚åœ¨éœ€è¦è§„åˆ’è¿‡ç¨‹ä¸­å¤§é‡å›æº¯çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ¤°å­æ€ç»´çš„è¡¨ç°ä¼˜äºCoTï¼Œæ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ€è€ƒç¬¦å·æ›´å°‘ã€‚è¿™äº›å‘ç°è¯æ˜äº†æ½œåœ¨æ¨ç†çš„æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06769v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å—é™äºâ€œè¯­è¨€ç©ºé—´â€å†…çš„æ¨ç†ï¼Œé€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è§£å†³å¤æ‚æ¨ç†é—®é¢˜ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸»å¼ è¯­è¨€ç©ºé—´å¹¶éæ€»æ˜¯æœ€ä¼˜çš„æ¨ç†æ–¹å¼ã€‚Coconutï¼ˆè¿ç»­æ€ç»´é“¾ï¼‰æ–°èŒƒå¼æ¢ç´¢äº†LLMåœ¨ä¸å—é™åˆ¶æ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†æ½œåŠ›ï¼Œåˆ©ç”¨LLMçš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºæ¨ç†çŠ¶æ€çš„è¡¨ç¤ºï¼ˆå³â€œè¿ç»­æ€ç»´â€ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒCoconutèƒ½æœ‰æ•ˆå¢å¼ºLLMåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„é«˜çº§æ¨ç†æ¨¡å¼ï¼šè¿ç»­æ€ç»´èƒ½ç¼–ç å¤šä¸ªä¸‹ä¸€ä¸ªæ¨ç†æ­¥éª¤ï¼Œä½¿æ¨¡å‹å¾—ä»¥é€šè¿‡å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰è§£å†³é—®é¢˜ï¼Œè€Œéè¿‡æ—©åœ°ç¡®å®šå•ä¸€è·¯å¾„ã€‚åœ¨æŸäº›éœ€è¦è§„åˆ’æ—¶å›æº¯çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­ï¼ŒCoconutè¡¨ç°ä¼˜äºCoTã€‚æ­¤ç ”ç©¶å±•ç¤ºäº†æ½œåœ¨æ¨ç†çš„æ½œåŠ›å¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å—é™äºâ€œè¯­è¨€ç©ºé—´â€ï¼Œä¸»è¦é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è§£å†³æ¨ç†é—®é¢˜ã€‚</li>
<li>è¯­è¨€ç©ºé—´å¹¶éæ€»æ˜¯æœ€ä¼˜çš„æ¨ç†æ–¹å¼ï¼Œå› ä¸ºå…³é”®ä¿¡æ¯å¯èƒ½è¢«å†—ä½™çš„è¯­è¨€æ‰€æ©ç›–ã€‚</li>
<li>Coconutæ˜¯ä¸€ç§æ–°çš„æ¨ç†èŒƒå¼ï¼Œå°†LLMçš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ç”¨ä½œè¿ç»­æ€ç»´çš„è¡¨ç¤ºã€‚</li>
<li>å®éªŒè¯æ˜Coconutèƒ½æé«˜LLMåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>è¿ç»­æ€ç»´å¯ä»¥ç¼–ç å¤šä¸ªå¯èƒ½çš„ä¸‹ä¸€æ­¥æ¨ç†æ­¥éª¤ï¼Œå®ç°å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰ã€‚</li>
<li>åœ¨éœ€è¦è§„åˆ’å›æº¯çš„é€»è¾‘æ¨ç†ä»»åŠ¡ä¸­ï¼ŒCoconutä¼˜äºä¼ ç»Ÿçš„é“¾å¼æ€ç»´æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6374ac8cb9326d8ed6b4a40ef15a7c04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45c7194fb1517e7dca0e15f4caee42f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1100aed63d121948dabc212c1f54cd42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39029a31cbacc01d5860058af630a47f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc48e8aff969255665c08211ee6a022a.jpg" align="middle">
</details>




<h2 id="JAPAGEN-Efficient-Few-Zero-shot-Learning-via-Japanese-Training-Dataset-Generation-with-LLM"><a href="#JAPAGEN-Efficient-Few-Zero-shot-Learning-via-Japanese-Training-Dataset-Generation-with-LLM" class="headerlink" title="JAPAGEN: Efficient Few&#x2F;Zero-shot Learning via Japanese Training Dataset   Generation with LLM"></a>JAPAGEN: Efficient Few&#x2F;Zero-shot Learning via Japanese Training Dataset   Generation with LLM</h2><p><strong>Authors:Takuro Fujii, Satoru Katsumata</strong></p>
<p>Recently some studies have highlighted the potential of Large Language Models (LLMs) as effective generators of supervised training data, offering advantages such as enhanced inference efficiency and reduced costs associated with data collection. However, these studies have predominantly focused on English language tasks. In this paper, we address the fundamental research question: Can LLMs serve as proficient training data generators for other language tasks? Specifically, we leverage LLMs to synthesize supervised training data under few-shot and zero-shot learning scenarios across six diverse Japanese downstream tasks. Subsequently, we utilize this synthesized data to train compact models (e.g., BERT). This novel methodology is termed JAPAGEN. Our experimental findings underscore that JAPAGEN achieves robust performance in classification tasks that necessitate formal text inputs, demonstrating competitive results compared to conventional LLM prompting strategies. </p>
<blockquote>
<p>æœ€è¿‘çš„ä¸€äº›ç ”ç©¶å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæœ‰æ•ˆç›‘ç£è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨çš„æ½œåŠ›ï¼Œæä¾›äº†æé«˜æ¨ç†æ•ˆç‡å’Œé™ä½æ•°æ®é‡‡é›†æˆæœ¬ç­‰ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­ä»»åŠ¡ä¸Šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºæœ¬çš„ç ”ç©¶é—®é¢˜ï¼šLLMèƒ½å¦ä¸ºå…¶ä»–è¯­è¨€ä»»åŠ¡æä¾›ç†Ÿç»ƒçš„è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼Ÿå…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å…­ç§ä¸åŒçš„æ—¥è¯­ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œåˆ©ç”¨LLMåœ¨å°‘é‡æ ·æœ¬ç”šè‡³é›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹åˆæˆç›‘ç£è®­ç»ƒæ•°æ®ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›åˆæˆæ•°æ®æ¥è®­ç»ƒå°å‹æ¨¡å‹ï¼ˆä¾‹å¦‚BERTï¼‰ã€‚è¿™ç§æ–°æ–¹æ³•è¢«ç§°ä¸ºJAPAGENã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒJAPAGENåœ¨éœ€è¦æ­£å¼æ–‡æœ¬è¾“å…¥çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ï¼Œä¸ä¼ ç»Ÿçš„LLMæç¤ºç­–ç•¥ç›¸æ¯”ï¼Œç»“æœå…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06738v1">PDF</a> Accepted by PACLIC38 (2024)</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºç›‘ç£è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨å…·æœ‰æ½œåŠ›ï¼Œèƒ½æé«˜æ¨ç†æ•ˆç‡å’Œé™ä½æ•°æ®é‡‡é›†æˆæœ¬ã€‚è¿™äº›ç ”ç©¶ä¸»è¦å…³æ³¨è‹±è¯­ä»»åŠ¡ã€‚æœ¬æ–‡æ—¨åœ¨æ¢è®¨ï¼šLLMsèƒ½å¦ä¸ºå…¶ä»–è¯­è¨€ä»»åŠ¡æä¾›æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ç”Ÿæˆï¼Ÿé€šè¿‡åˆ©ç”¨LLMsåœ¨å…­ç§ä¸åŒçš„æ—¥è¯­ä¸‹æ¸¸ä»»åŠ¡ä¸­åˆæˆç›‘ç£è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹åº”ç”¨æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•JAPAGENã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒJAPAGENåœ¨éœ€è¦æ­£å¼æ–‡æœ¬è¾“å…¥çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ç¨³å¥ï¼Œä¸ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºç­–ç•¥ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯ä»¥ä½œä¸ºç›‘ç£è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆç”Ÿæˆå™¨ï¼Œä¸ä»…é™äºè‹±è¯­ä»»åŠ¡ã€‚</li>
<li>LLMsèƒ½å¤Ÿåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹åˆæˆç›‘ç£è®­ç»ƒæ•°æ®ã€‚</li>
<li>JAPAGENæ–¹æ³•è¢«æå‡ºå¹¶æˆåŠŸåº”ç”¨äºæ—¥è¯­ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>JAPAGENåœ¨éœ€è¦æ­£å¼æ–‡æœ¬è¾“å…¥çš„åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</li>
<li>JAPAGENä¸ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹æç¤ºç­–ç•¥ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>LLMsç”¨äºæ•°æ®ç”Ÿæˆå¯ä»¥æé«˜æ¨ç†æ•ˆç‡å¹¶é™ä½æ•°æ®é‡‡é›†æˆæœ¬ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dca2fe3ecbad4ca6b8c39225c64c25f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6606b337aa1178b8817db6e0a7e2b166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7831717f83a924d829afa0b801a90a1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-300da75883c08486d606f85f26526b03.jpg" align="middle">
</details>




<h2 id="OmniEvalKit-A-Modular-Lightweight-Toolbox-for-Evaluating-Large-Language-Model-and-its-Omni-Extensions"><a href="#OmniEvalKit-A-Modular-Lightweight-Toolbox-for-Evaluating-Large-Language-Model-and-its-Omni-Extensions" class="headerlink" title="OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large   Language Model and its Omni-Extensions"></a>OmniEvalKit: A Modular, Lightweight Toolbox for Evaluating Large   Language Model and its Omni-Extensions</h2><p><strong>Authors:Yi-Kai Zhang, Xu-Xiang Zhong, Shiyin Lu, Qing-Guo Chen, De-Chuan Zhan, Han-Jia Ye</strong></p>
<p>The rapid advancements in Large Language Models (LLMs) have significantly expanded their applications, ranging from multilingual support to domain-specific tasks and multimodal integration. In this paper, we present OmniEvalKit, a novel benchmarking toolbox designed to evaluate LLMs and their omni-extensions across multilingual, multidomain, and multimodal capabilities. Unlike existing benchmarks that often focus on a single aspect, OmniEvalKit provides a modular, lightweight, and automated evaluation system. It is structured with a modular architecture comprising a Static Builder and Dynamic Data Flow, promoting the seamless integration of new models and datasets. OmniEvalKit supports over 100 LLMs and 50 evaluation datasets, covering comprehensive evaluations across thousands of model-dataset combinations. OmniEvalKit is dedicated to creating an ultra-lightweight and fast-deployable evaluation framework, making downstream applications more convenient and versatile for the AI community. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿè¿›æ­¥æå¤§åœ°æ‰©å±•äº†å®ƒä»¬çš„åº”ç”¨èŒƒå›´ï¼Œä»å¤šè¯­è¨€æ”¯æŒåˆ°ç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡å’Œè·¨æ¨¡æ€é›†æˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniEvalKitï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æµ‹è¯•å·¥å…·ç®±ï¼Œæ—¨åœ¨è¯„ä¼°LLMåŠå…¶è·¨å¤šè¯­è¨€ã€å¤šé¢†åŸŸå’Œè·¨æ¨¡æ€çš„å„ç§æ‰©å±•åŠŸèƒ½ã€‚ä¸ç°æœ‰é€šå¸¸åªå…³æ³¨å•ä¸€æ–¹é¢çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼ŒOmniEvalKitæä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–ã€è½»ä¾¿ã€è‡ªåŠ¨åŒ–çš„è¯„ä¼°ç³»ç»Ÿã€‚å®ƒé‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼ŒåŒ…æ‹¬é™æ€æ„å»ºå™¨å’ŒåŠ¨æ€æ•°æ®æµï¼Œä¿ƒè¿›äº†æ–°æ¨¡å‹å’Œæ•°æ®é›†çš„æ— ç¼é›†æˆã€‚OmniEvalKitæ”¯æŒè¶…è¿‡100ç§LLMå’Œ50ä¸ªè¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–æ•°åƒç§æ¨¡å‹æ•°æ®é›†ç»„åˆçš„å…¨é¢è¯„ä¼°ã€‚OmniEvalKitè‡´åŠ›äºåˆ›å»ºä¸€ä¸ªè¶…è½»é‡çº§ã€å¿«é€Ÿéƒ¨ç½²çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºAIç¤¾åŒºæä¾›æ›´ä¾¿æ·ã€æ›´é€šç”¨çš„ä¸‹æ¸¸åº”ç”¨ç¨‹åºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06693v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OmniEvalKitæ˜¯ä¸€ä¸ªä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›å…¨æ–¹ä½è¯„ä¼°çš„å·¥å…·ç®±ï¼Œå¯è·¨å¤šè¯­ç§ã€å¤šé¢†åŸŸå’Œå¤šåª’ä½“è¿›è¡Œè¯„ä¼°ã€‚å®ƒé‡‡ç”¨æ¨¡å—åŒ–ç»“æ„ï¼ŒåŒ…æ‹¬é™æ€æ„å»ºå™¨å’ŒåŠ¨æ€æ•°æ®æµï¼Œä¿ƒè¿›æ–°æ¨¡å‹å’Œæ•°æ®é›†çš„æ— ç¼é›†æˆã€‚OmniEvalKitæ”¯æŒè¶…è¿‡100ä¸ªLLMå’Œ50ä¸ªè¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–æ•°åƒä¸ªæ¨¡å‹æ•°æ®é›†ç»„åˆçš„è¯„ä¼°ã€‚å®ƒä¸ºAIç¤¾åŒºåˆ›å»ºäº†ä¸€ä¸ªè¶…è½»é‡çº§ã€å¿«é€Ÿéƒ¨ç½²çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿ä¸‹æ¸¸åº”ç”¨ç¨‹åºæ›´åŠ ä¾¿æ·å’Œå¤šåŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniEvalKitæ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è€Œè®¾è®¡çš„å·¥å…·ç®±ã€‚</li>
<li>å®ƒæä¾›è·¨å¤šè¯­ç§ã€å¤šé¢†åŸŸå’Œå¤šåª’ä½“çš„å…¨æ–¹ä½è¯„ä¼°ã€‚</li>
<li>OmniEvalKité‡‡ç”¨æ¨¡å—åŒ–ç»“æ„ï¼ŒåŒ…æ‹¬é™æ€æ„å»ºå™¨å’ŒåŠ¨æ€æ•°æ®æµï¼Œä¿ƒè¿›æ–°æ¨¡å‹å’Œæ•°æ®é›†çš„æ— ç¼é›†æˆã€‚</li>
<li>æ”¯æŒè¶…è¿‡100ä¸ªLLMå’Œ50ä¸ªè¯„ä¼°æ•°æ®é›†ï¼Œæ¶µç›–å¹¿æ³›çš„æ¨¡å‹-æ•°æ®é›†ç»„åˆè¯„ä¼°ã€‚</li>
<li>OmniEvalKitè‡´åŠ›äºåˆ›å»ºä¸€ä¸ªè¶…è½»é‡çº§ã€å¿«é€Ÿéƒ¨ç½²çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ä¸‹æ¸¸åº”ç”¨ç¨‹åºæ›´åŠ ä¾¿æ·å’Œå¤šåŠŸèƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1bcdec82c232af1969ad46bea4d95eae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec5f9fdae08be5394f6abbe3bb27de8.jpg" align="middle">
</details>




<h2 id="Exploring-Critical-Testing-Scenarios-for-Decision-Making-Policies-An-LLM-Approach"><a href="#Exploring-Critical-Testing-Scenarios-for-Decision-Making-Policies-An-LLM-Approach" class="headerlink" title="Exploring Critical Testing Scenarios for Decision-Making Policies: An   LLM Approach"></a>Exploring Critical Testing Scenarios for Decision-Making Policies: An   LLM Approach</h2><p><strong>Authors:Weichao Xu, Huaxin Pei, Jingxuan Yang, Yuchen Shi, Yi Zhang</strong></p>
<p>Recent years have witnessed surprising achievements of decision-making policies across various fields, such as autonomous driving and robotics. Testing for decision-making policies is crucial with the existence of critical scenarios that may threaten their reliability. Numerous research efforts have been dedicated to testing these policies. However, there are still significant challenges, such as low testing efficiency and diversity due to the complexity of the policies and environments under test. Inspired by the remarkable capabilities of large language models (LLMs), in this paper, we propose an LLM-driven online testing framework for efficiently testing decision-making policies. The main idea is to employ an LLM-based test scenario generator to intelligently generate challenging test cases through contemplation and reasoning. Specifically, we first design a â€œgenerate-test-feedbackâ€ pipeline and apply templated prompt engineering to fully leverage the knowledge and reasoning abilities of LLMs. Then, we introduce a multi-scale scenario generation strategy to address the inherent challenges LLMs face in making fine adjustments, further enhancing testing efficiency. Finally, we evaluate the LLM-driven approach on five widely used benchmarks. The experimental results demonstrate that our method significantly outperforms baseline approaches in uncovering both critical and diverse scenarios. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå†³ç­–ç­–ç•¥åœ¨å„ä¸ªé¢†åŸŸå¦‚è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ä¸­å–å¾—äº†ä»¤äººæƒŠè®¶çš„æˆå°±ã€‚ç”±äºå­˜åœ¨å¯èƒ½å¨èƒå…¶å¯é æ€§çš„å…³é”®åœºæ™¯ï¼Œå› æ­¤é’ˆå¯¹å†³ç­–ç­–ç•¥çš„æµ‹è¯•è‡³å…³é‡è¦ã€‚è®¸å¤šç ”ç©¶åŠªåŠ›éƒ½è‡´åŠ›äºæµ‹è¯•è¿™äº›ç­–ç•¥ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¦‚ç”±äºæ”¿ç­–å’Œç¯å¢ƒæµ‹è¯•çš„å¤æ‚æ€§å¯¼è‡´çš„æµ‹è¯•æ•ˆç‡ä½ä¸‹å’Œå¤šæ ·æ€§ä¸è¶³ã€‚æœ¬æ–‡å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡ºè‰²èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„åœ¨çº¿æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆåœ°æµ‹è¯•å†³ç­–ç­–ç•¥ã€‚ä¸»è¦æ€æƒ³æ˜¯é‡‡ç”¨åŸºäºLLMçš„æµ‹è¯•åœºæ™¯ç”Ÿæˆå™¨ï¼Œé€šè¿‡æ·±æ€å’Œæ¨ç†æ™ºèƒ½åœ°ç”Ÿæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡ä¸€ä¸ªâ€œç”Ÿæˆ-æµ‹è¯•-åé¦ˆâ€ç®¡é“ï¼Œåº”ç”¨æ¨¡æ¿æç¤ºå·¥ç¨‹æ¥å……åˆ†åˆ©ç”¨LLMçš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦åœºæ™¯ç”Ÿæˆç­–ç•¥æ¥è§£å†³LLMåœ¨ç»†å¾®è°ƒæ•´æ–¹é¢æ‰€é¢ä¸´çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œè¿›ä¸€æ­¥æé«˜äº†æµ‹è¯•æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šå¯¹LLMé©±åŠ¨çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‘ç°å…³é”®å’Œå¤šæ ·åŒ–åœºæ™¯æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06684v1">PDF</a> 16 pages, 13 figures</p>
<p><strong>Summary</strong><br>å†³ç­–ç­–ç•¥åœ¨å„ç§é¢†åŸŸå¦‚è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ä¸­éƒ½å–å¾—äº†æƒŠäººçš„æˆå°±ã€‚è¿™äº›ç­–ç•¥çš„æµ‹è¯•è‡³å…³é‡è¦ï¼Œå› ä¸ºå­˜åœ¨å…³é”®åœºæ™¯å¯èƒ½ä¼šå¨èƒå…¶å¯é æ€§ã€‚è™½ç„¶æœ‰å¾ˆå¤šå…³äºæµ‹è¯•è¿™äº›ç­–ç•¥çš„ç ”ç©¶åŠªåŠ›ï¼Œä½†ç”±äºç­–ç•¥çš„å¤æ‚æ€§ä»¥åŠæµ‹è¯•ç¯å¢ƒç­‰å› ç´ ï¼Œä»ç„¶å­˜åœ¨è¯¸å¦‚æµ‹è¯•æ•ˆç‡ä½ä¸‹å’Œå¤šæ ·æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åœ¨çº¿æµ‹è¯•æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ç”ŸæˆæŒ‘æˆ˜æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚é¦–å…ˆè®¾è®¡äº†ä¸€ä¸ªâ€œç”Ÿæˆ-æµ‹è¯•-åé¦ˆâ€ç®¡é“ï¼Œå¹¶åº”ç”¨æ¨¡æ¿æç¤ºå·¥ç¨‹æ¥å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œå¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦åœºæ™¯ç”Ÿæˆç­–ç•¥æ¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»†å¾®è°ƒæ•´æ–¹é¢æ‰€é¢ä¸´çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œè¿›ä¸€æ­¥æé«˜äº†æµ‹è¯•æ•ˆç‡ã€‚æœ€åï¼Œåœ¨äº”ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†ä¸Šå¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‘ç°å…³é”®å’Œå¤šæ ·åŒ–åœºæ™¯æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†³ç­–ç­–ç•¥åœ¨å„ç§é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†æµ‹è¯•è¿™äº›ç­–ç•¥æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä½æ•ˆç‡å’Œç¼ºä¹å¤šæ ·æ€§ã€‚</li>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åœ¨çº¿æµ‹è¯•æ¡†æ¶è¢«æå‡ºï¼Œç”¨äºæ™ºèƒ½ç”ŸæˆæŒ‘æˆ˜æ€§çš„æµ‹è¯•ç”¨ä¾‹ã€‚</li>
<li>é€šè¿‡è®¾è®¡â€œç”Ÿæˆ-æµ‹è¯•-åé¦ˆâ€ç®¡é“å’Œåº”ç”¨æ¨¡æ¿æç¤ºå·¥ç¨‹ï¼Œå……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤šå°ºåº¦åœºæ™¯ç”Ÿæˆç­–ç•¥è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç»†å¾®è°ƒæ•´æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‘ç°å…³é”®å’Œå¤šæ ·åŒ–åœºæ™¯æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æ­¤æ¡†æ¶å¯¹äºæé«˜å†³ç­–ç­–ç•¥æµ‹è¯•çš„æ•ˆç‡å’Œå¯é æ€§å…·æœ‰é‡å¤§æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ac7500111c96413996538cf02a665c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ecb2123d9053ff874ecbe40cb538ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0af7794cecff2dc2da2f9d62e186d45d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-805d0cb5b4c4c6f7633da308f42c6183.jpg" align="middle">
</details>




<h2 id="Toward-LLM-Agent-Based-Modeling-of-Transportation-Systems-A-Conceptual-Framework"><a href="#Toward-LLM-Agent-Based-Modeling-of-Transportation-Systems-A-Conceptual-Framework" class="headerlink" title="Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual   Framework"></a>Toward LLM-Agent-Based Modeling of Transportation Systems: A Conceptual   Framework</h2><p><strong>Authors:Tianming Liu, Jirong Yang, Yafeng Yin</strong></p>
<p>In transportation system demand modeling and simulation, agent-based models and microsimulations are current state-of-the-art approaches. However, existing agent-based models still have some limitations on behavioral realism and resource demand that limit their applicability. In this study, leveraging the emerging technology of large language models (LLMs) and LLM-based agents, we propose a general LLM-agent-based modeling framework for transportation systems. We argue that LLM agents not only possess the essential capabilities to function as agents but also offer promising solutions to overcome some limitations of existing agent-based models. Our conceptual framework design closely replicates the decision-making and interaction processes and traits of human travelers within transportation networks, and we demonstrate that the proposed systems can meet critical behavioral criteria for decision-making and learning behaviors using related studies and a demonstrative example of LLM agentsâ€™ learning and adjustment in the bottleneck setting. Although further refinement of the LLM-agent-based modeling framework is necessary, we believe that this approach has the potential to improve transportation system modeling and simulation. </p>
<blockquote>
<p>åœ¨äº¤é€šè¿è¾“ç³»ç»Ÿéœ€æ±‚å»ºæ¨¡ä¸ä»¿çœŸä¸­ï¼ŒåŸºäºä¸»ä½“çš„æ¨¡å‹å’Œå¾®è§‚ä»¿çœŸéƒ½æ˜¯å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯æ‰‹æ®µã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºä¸»ä½“çš„æ¨¡å‹åœ¨è¡Œä¸ºçœŸå®æ€§å’Œèµ„æºéœ€æ±‚æ–¹é¢ä»æœ‰ä¸€äº›å±€é™æ€§ï¼Œé™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ–°å…´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºLLMçš„ä»£ç†æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ä¸ªé€šç”¨çš„äº¤é€šè¿è¾“ç³»ç»ŸLLM-agent-basedå»ºæ¨¡æ¡†æ¶ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒLLMä»£ç†ä¸ä»…å…·å¤‡ä½œä¸ºä»£ç†çš„åŸºæœ¬èƒ½åŠ›ï¼Œè€Œä¸”è¿˜ä¸ºè§£å†³ç°æœ‰åŸºäºä¸»ä½“çš„æ¨¡å‹çš„ä¸€äº›å±€é™æ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ¦‚å¿µæ¡†æ¶è®¾è®¡ç´§å¯†å¤åˆ¶äº†äº¤é€šè¿è¾“ç½‘ç»œä¸­äººç±»æ—…è¡Œè€…çš„å†³ç­–å’Œäº’åŠ¨è¿‡ç¨‹åŠç‰¹å¾ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œç›¸å…³ç ”ç©¶å’ŒLLMä»£ç†åœ¨ç“¶é¢ˆç¯å¢ƒä¸­çš„å­¦ä¹ å’Œè°ƒæ•´ç¤ºèŒƒä¾‹å­å¯ä»¥è¯æ˜ï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿå¯ä»¥æ»¡è¶³å†³ç­–å’Œå­¦ä¹ è¡Œä¸ºçš„å…³é”®è¡Œä¸ºæ ‡å‡†ã€‚å°½ç®¡å¯¹LLM-agent-basedå»ºæ¨¡æ¡†æ¶çš„è¿›ä¸€æ­¥ç»†åŒ–æ˜¯å¿…éœ€çš„ï¼Œä½†æˆ‘ä»¬ç›¸ä¿¡è¿™ç§æ–¹æ³•æœ‰æ½œåŠ›æ”¹è¿›äº¤é€šè¿è¾“ç³»ç»Ÿå»ºæ¨¡å’Œä»¿çœŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06681v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ™ºèƒ½ä½“çš„æ¨¡å‹å’Œå¾®è§‚æ¨¡æ‹Ÿæ˜¯ç›®å‰äº¤é€šç³»ç»Ÿéœ€æ±‚å»ºæ¨¡å’Œä»¿çœŸçš„ä¸»æµæ–¹æ³•ï¼Œä½†ç°æœ‰æ™ºèƒ½ä½“æ¨¡å‹åœ¨è¡Œä¸ºçœŸå®æ€§å’Œèµ„æºéœ€æ±‚æ–¹é¢å­˜åœ¨ä¸€å®šå±€é™æ€§ã€‚æœ¬ç ”ç©¶åˆ©ç”¨æ–°å…´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºLLMçš„æ™ºèƒ½ä½“æŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§é€šç”¨çš„LLMæ™ºèƒ½ä½“æ¨¡å‹æ¡†æ¶ã€‚ç ”ç©¶å‘ç°LLMæ™ºèƒ½ä½“ä¸ä»…å…·å¤‡åŸºæœ¬åŠŸèƒ½æ™ºèƒ½ï¼Œè¿˜è§£å†³äº†ç°æœ‰æ™ºèƒ½ä½“æ¨¡å‹çš„ä¸€äº›å±€é™æ€§é—®é¢˜ã€‚è®¾è®¡çš„æ¦‚å¿µæ¡†æ¶ç´§å¯†å¤åˆ¶äº†äººç±»æ—…è¡Œè€…åœ¨äº¤é€šç½‘ç»œä¸­çš„å†³ç­–å’Œäº’åŠ¨è¿‡ç¨‹ä¸ç‰¹å¾ï¼Œå¹¶å±•ç¤ºäº†è¯¥ç³»ç»Ÿèƒ½æ»¡è¶³å…³é”®çš„å†³ç­–åˆ¶å®šå’Œå­¦ä¹ è¡Œä¸ºæ ‡å‡†ã€‚å°½ç®¡éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›LLMæ™ºèƒ½ä½“æ¨¡å‹æ¡†æ¶ï¼Œä½†æ­¤æ–¹æ³•åœ¨äº¤é€šç³»ç»Ÿå»ºæ¨¡å’Œä»¿çœŸæ–¹é¢æœ‰ç€å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºLLMçš„æ™ºèƒ½ä½“è¢«ç”¨äºæ„å»ºæ–°çš„äº¤é€šç³»ç»Ÿæ¨¡å‹æ¡†æ¶ã€‚</li>
<li>LLMæ™ºèƒ½ä½“å…·å¤‡åŸºæœ¬åŠŸèƒ½æ™ºèƒ½ï¼Œå¹¶è§£å†³äº†ç°æœ‰æ™ºèƒ½ä½“æ¨¡å‹çš„è¡Œä¸ºçœŸå®æ€§å’Œèµ„æºéœ€æ±‚æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ç´§å¯†å¤åˆ¶äº†äººç±»æ—…è¡Œè€…åœ¨äº¤é€šç½‘ç»œä¸­çš„å†³ç­–å’Œäº’åŠ¨è¿‡ç¨‹ä¸ç‰¹å¾ã€‚</li>
<li>LLMæ™ºèƒ½ä½“å¯ä»¥åœ¨ç“¶é¢ˆåœºæ™¯ä¸‹å±•ç¤ºå­¦ä¹ å’Œè°ƒæ•´èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç›¸å…³ç ”ç©¶å’Œå®ä¾‹æ¼”ç¤ºï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿèƒ½æ»¡è¶³å…³é”®çš„å†³ç­–åˆ¶å®šå’Œå­¦ä¹ è¡Œä¸ºæ ‡å‡†ã€‚</li>
<li>è™½ç„¶è¯¥æ¡†æ¶éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›å’Œä¼˜åŒ–ï¼Œä½†å…¶å¯¹äº¤é€šç³»ç»Ÿå»ºæ¨¡å’Œä»¿çœŸçš„æ½œåŠ›å·¨å¤§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä¸ºäº¤é€šç³»ç»Ÿå»ºæ¨¡å¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b1ef461fdbfcf8dc5820e7b918bf2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b28f7ca9a3b52e9165ba5814aba4cf3b.jpg" align="middle">
</details>




<h2 id="I-Donâ€™t-Know-Explicit-Modeling-of-Uncertainty-with-an-IDK-Token"><a href="#I-Donâ€™t-Know-Explicit-Modeling-of-Uncertainty-with-an-IDK-Token" class="headerlink" title="I Donâ€™t Know: Explicit Modeling of Uncertainty with an [IDK] Token"></a>I Donâ€™t Know: Explicit Modeling of Uncertainty with an [IDK] Token</h2><p><strong>Authors:Roi Cohen, Konstantin Dobler, Eden Biran, Gerard de Melo</strong></p>
<p>Large Language Models are known to capture real-world knowledge, allowing them to excel in many downstream tasks. Despite recent advances, these models are still prone to what are commonly known as hallucinations, causing them to emit unwanted and factually incorrect text. In this work, we propose a novel calibration method that can be used to combat hallucinations. We add a special [IDK] (â€œI donâ€™t knowâ€) token to the modelâ€™s vocabulary and introduce an objective function that shifts probability mass to the [IDK] token for incorrect predictions. This approach allows the model to express uncertainty in its output explicitly. We evaluate our proposed method across multiple model architectures and factual downstream tasks. We find that models trained with our method are able to express uncertainty in places where they would previously make mistakes while suffering only a small loss of encoded knowledge. We further perform extensive ablation studies of multiple variations of our approach and provide a detailed analysis of the precision-recall tradeoff of our method. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ•è·ç°å®ä¸–ç•Œçš„çŸ¥è¯†ï¼Œä½¿å…¶åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚å°½ç®¡æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†è¿™äº›æ¨¡å‹ä»ç„¶å®¹æ˜“é­å—æ‰€è°“çš„â€œå¹»è§‰â€å½±å“ï¼Œå¯¼è‡´å®ƒä»¬äº§ç”Ÿä¸æƒ³è¦ä¸”äº‹å®é”™è¯¯çš„æ–‡æœ¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ ¡å‡†æ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥å¯¹æŠ—å¹»è§‰ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹çš„è¯æ±‡è¡¨ä¸­å¢åŠ äº†ä¸€ä¸ªç‰¹æ®Šçš„[IDK]ï¼ˆâ€œæˆ‘ä¸çŸ¥é“â€ï¼‰æ ‡è®°ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œå°†æ¦‚ç‡è´¨é‡è½¬ç§»åˆ°é”™è¯¯é¢„æµ‹çš„[IDK]æ ‡è®°ä¸Šã€‚è¿™ç§æ–¹æ³•å…è®¸æ¨¡å‹åœ¨è¾“å‡ºä¸­æ˜¾å¼è¡¨è¾¾ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬åœ¨å¤šç§æ¨¡å‹æ¶æ„å’Œäº‹å®ä¸‹æ¸¸ä»»åŠ¡ä¸­è¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨æˆ‘ä»¬æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä»¥å‰ä¼šå‡ºé”™çš„åœ°æ–¹è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶åªæŸå¤±å°‘é‡å·²ç¼–ç çš„çŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜å¯¹æ–¹æ³•çš„å¤šç§å˜ä½“è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œå¹¶è¯¦ç»†åˆ†æäº†æˆ‘ä»¬æ–¹æ³•çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹‹é—´çš„æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06676v1">PDF</a> Published at NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è™½èƒ½æ•æ‰ç°å®çŸ¥è¯†å¹¶åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆé”™è¯¯æ–‡æœ¬çš„é£é™©ï¼Œå³æ‰€è°“çš„â€œå¹»è§‰â€ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ ¡å‡†æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡åœ¨æ¨¡å‹è¯æ±‡è¡¨ä¸­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„[IDK]ï¼ˆè¡¨ç¤ºâ€œæˆ‘ä¸çŸ¥é“â€ï¼‰ä»¤ç‰Œï¼Œå¹¶å¼•å…¥ä¸€ä¸ªé’ˆå¯¹ä¸æ­£ç¡®é¢„æµ‹å°†æ¦‚ç‡è´¨é‡è½¬ç§»åˆ°è¯¥ä»¤ç‰Œçš„ç›®æ ‡å‡½æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ˜ç¡®è¡¨è¾¾å…¶è¾“å‡ºçš„ä¸ç¡®å®šæ€§ã€‚è¯„ä¼°è¡¨æ˜ï¼Œç»è¿‡æ­¤æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¹‹å‰çŠ¯é”™çš„åœ°æ–¹è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä»…æŸå¤±å°‘é‡ç¼–ç çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å¤§é‡çš„æ–¹æ³•å˜ä½“æ¶ˆèç ”ç©¶ï¼Œå¹¶å¯¹æ–¹æ³•çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡ä¹‹é—´çš„æƒè¡¡è¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç°å®çŸ¥è¯†å¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆé”™è¯¯æ–‡æœ¬çš„é£é™©ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ ¡å‡†æ–¹æ³•æ¥è§£å†³æ¨¡å‹ç”Ÿæˆé”™è¯¯æ–‡æœ¬çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ·»åŠ [IDK]ä»¤ç‰Œå’Œå¼•å…¥ç›®æ ‡å‡½æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ˜ç¡®è¡¨è¾¾å…¶è¾“å‡ºçš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>è®­ç»ƒåçš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¹‹å‰çŠ¯é”™çš„åœ°æ–¹è¡¨è¾¾ä¸ç¡®å®šæ€§ï¼ŒåŒæ—¶ä»…æŸå¤±å°‘é‡ç¼–ç çŸ¥è¯†ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§æ¨¡å‹æ¶æ„å’Œäº‹å®ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨ç°è‰¯å¥½ã€‚</li>
<li>è¿›è¡Œäº†å¤§é‡çš„æ–¹æ³•å˜ä½“æ¶ˆèç ”ç©¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a900fdcbbf03f413b4ce7b098c335b8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444232274c5100bf43832ef864dd1f80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f372fd12ba8784d33aaa635c61af7de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b27a5e08a7bee9419dcc1a855a0bf4eb.jpg" align="middle">
</details>




<h2 id="ILLUME-Illuminating-Your-LLMs-to-See-Draw-and-Self-Enhance"><a href="#ILLUME-Illuminating-Your-LLMs-to-See-Draw-and-Self-Enhance" class="headerlink" title="ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance"></a>ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance</h2><p><strong>Authors:Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, Hang Xu</strong></p>
<p>In this paper, we introduce ILLUME, a unified multimodal large language model (MLLM) that seamlessly integrates multimodal understanding and generation capabilities within a single large language model through a unified next-token prediction formulation. To address the large dataset size typically required for image-text alignment, we propose to enhance data efficiency through the design of a vision tokenizer that incorporates semantic information and a progressive multi-stage training procedure. This approach reduces the dataset size to just 15M for pretraining â€“ over four times fewer than what is typically needed â€“ while achieving competitive or even superior performance with existing unified MLLMs, such as Janus. Additionally, to promote synergistic enhancement between understanding and generation capabilities, which is under-explored in previous works, we introduce a novel self-enhancing multimodal alignment scheme. This scheme supervises the MLLM to self-assess the consistency between text descriptions and self-generated images, facilitating the model to interpret images more accurately and avoid unrealistic and incorrect predictions caused by misalignment in image generation. Based on extensive experiments, our proposed ILLUME stands out and competes with state-of-the-art unified MLLMs and specialized models across various benchmarks for multimodal understanding, generation, and editing. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ILLUMEï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒé€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å…¬å¼ï¼Œåœ¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ— ç¼é›†æˆäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³å›¾åƒæ–‡æœ¬å¯¹é½é€šå¸¸éœ€è¦çš„å¤§é‡æ•°æ®é›†é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡è®¾è®¡ä¸€ç§è§†è§‰ä»¤ç‰Œå™¨æ¥æé«˜æ•°æ®æ•ˆç‡ï¼Œè¯¥ä»¤ç‰Œå™¨ç»“åˆäº†è¯­ä¹‰ä¿¡æ¯å’Œä¸€ä¸ªæ¸è¿›çš„å¤šé˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•å°†æ•°æ®é›†å¤§å°å‡å°‘åˆ°ä»…ç”¨äºé¢„è®­ç»ƒçš„15Mâ€”â€”æ¯”é€šå¸¸éœ€è¦çš„å°‘äº†å››å€â€”â€”åŒæ—¶å®ç°äº†ä¸ç°æœ‰çš„ç»Ÿä¸€MLLMï¼ˆå¦‚Janusï¼‰ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†ä¿ƒè¿›ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„ååŒå¢å¼ºï¼ˆè¿™åœ¨ä»¥å‰çš„å·¥ä½œä¸­å¾ˆå°‘æ¢ç´¢ï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªå¢å¼ºå¤šæ¨¡æ€å¯¹é½æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆç›‘ç£MLLMè‡ªæˆ‘è¯„ä¼°æ–‡æœ¬æè¿°ä¸è‡ªæˆ‘ç”Ÿæˆå›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¸®åŠ©æ¨¡å‹æ›´å‡†ç¡®åœ°è§£é‡Šå›¾åƒï¼Œå¹¶é¿å…ç”±äºå›¾åƒç”Ÿæˆä¸­çš„é”™ä½è€Œå¯¼è‡´çš„éç°å®å’Œé”™è¯¯çš„é¢„æµ‹ã€‚åŸºäºå¤§é‡å®éªŒï¼Œæˆ‘ä»¬æå‡ºçš„ILLUMEåœ¨å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„å„ç§åŸºå‡†æµ‹è¯•ä¸­è„±é¢–è€Œå‡ºï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„ç»Ÿä¸€MLLMå’Œä¸“ç”¨æ¨¡å‹ç›¸ç«äº‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06673v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ILLUMEï¼Œä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å…¬å¼ï¼Œå°†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ— ç¼é›†æˆåˆ°ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚ä¸ºè§£å†³å›¾åƒæ–‡æœ¬å¯¹é½æ‰€éœ€çš„å¤§é‡æ•°æ®é›†é—®é¢˜ï¼Œé€šè¿‡è®¾è®¡åŒ…å«è¯­ä¹‰ä¿¡æ¯çš„è§†è§‰æ ‡è®°å™¨å’Œæ¸è¿›çš„å¤šé˜¶æ®µè®­ç»ƒç¨‹åºï¼Œæé«˜äº†æ•°æ®æ•ˆç‡ã€‚æ­¤æ–¹æ³•å°†é¢„è®­ç»ƒæ•°æ®é›†å¤§å°å‡å°‘åˆ°ä»…15Mï¼Œç›¸è¾ƒäºé€šå¸¸æ‰€éœ€çš„æ•°æ®é›†å¤§å°å‡å°‘äº†å››å€ä»¥ä¸Šï¼ŒåŒæ—¶å®ç°äº†ä¸ç°æœ‰ç»Ÿä¸€MLLMsï¼ˆå¦‚Janusï¼‰ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†è‡ªæˆ‘æå‡çš„å¤šæ¨¡æ€å¯¹é½æ–¹æ¡ˆï¼Œä»¥ä¿ƒè¿›ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„ååŒå¢å¼ºï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ä»¥å‰çš„ç ”ç©¶ä¸­è¢«å¿½è§†çš„é—®é¢˜ã€‚è¯¥æ–¹æ¡ˆç›‘ç£MLLMè‡ªæˆ‘è¯„ä¼°æ–‡æœ¬æè¿°ä¸è‡ªæˆ‘ç”Ÿæˆçš„å›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¸®åŠ©æ¨¡å‹æ›´å‡†ç¡®åœ°è§£é‡Šå›¾åƒï¼Œé¿å…ç”±äºå›¾åƒç”Ÿæˆä¸­çš„ä¸å¯¹é½è€Œå¯¼è‡´çš„è™šå¹»å’Œé”™è¯¯çš„é¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼Œæå‡ºçš„ILLUMEåœ¨å¤šç§å¤šæ¨¡æ€ç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€å…ˆè¿›çš„ç»Ÿä¸€MLLMså’Œä¸“ç”¨æ¨¡å‹ç›¸ç«äº‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ILLUMEæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé›†æˆäº†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è§†è§‰æ ‡è®°å™¨å’Œå¤šé˜¶æ®µè®­ç»ƒç¨‹åºæé«˜äº†æ•°æ®æ•ˆç‡ï¼Œå‡å°‘é¢„è®­ç»ƒæ•°æ®é›†å¤§å°è‡³15Mã€‚</li>
<li>ä¸ç°æœ‰MLLMsç›¸æ¯”ï¼ŒILLUMEå®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è‡ªæˆ‘æå‡çš„å¤šæ¨¡æ€å¯¹é½æ–¹æ¡ˆï¼Œä¿ƒè¿›ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¹‹é—´çš„ååŒå¢å¼ºã€‚</li>
<li>è¯¥æ–¹æ¡ˆç›‘ç£MLLMè‡ªæˆ‘è¯„ä¼°æ–‡æœ¬æè¿°ä¸å›¾åƒä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæå‡å›¾åƒè§£é‡Šå‡†ç¡®æ€§ã€‚</li>
<li>ILLUMEé¿å…äº†ç”±äºå›¾åƒç”Ÿæˆä¸­çš„ä¸å¯¹é½å¯¼è‡´çš„è™šå¹»å’Œé”™è¯¯çš„é¢„æµ‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c60b27c4bbdd35daf16da6967dbe152f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-988f0a2fd4112ed4fb6dee3a6bff0da8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db726e594e416ab3f2ff6c6eab6c2dd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87870b10f67faf6402f1c101e43c0cb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf36188f3b0dfe6db810bf9cdcba7e2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-795db4a9bb4d38a8f8f4c6de67bc3bdc.jpg" align="middle">
</details>




<h2 id="MuMu-LLaMA-Multi-modal-Music-Understanding-and-Generation-via-Large-Language-Models"><a href="#MuMu-LLaMA-Multi-modal-Music-Understanding-and-Generation-via-Large-Language-Models" class="headerlink" title="MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large   Language Models"></a>MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large   Language Models</h2><p><strong>Authors:Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, Ying Shan</strong></p>
<p>Research on large language models has advanced significantly across text, speech, images, and videos. However, multi-modal music understanding and generation remain underexplored due to the lack of well-annotated datasets. To address this, we introduce a dataset with 167.69 hours of multi-modal data, including text, images, videos, and music annotations. Based on this dataset, we propose MuMu-LLaMA, a model that leverages pre-trained encoders for music, images, and videos. For music generation, we integrate AudioLDM 2 and MusicGen. Our evaluation across four tasksâ€“music understanding, text-to-music generation, prompt-based music editing, and multi-modal music generationâ€“demonstrates that MuMu-LLaMA outperforms state-of-the-art models, showing its potential for multi-modal music applications. </p>
<blockquote>
<p>å…³äºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶åœ¨æ–‡æœ¬ã€è¯­éŸ³ã€å›¾åƒå’Œè§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘æ ‡æ³¨è‰¯å¥½çš„æ•°æ®é›†ï¼Œå¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«167.69å°æ—¶å¤šæ¨¡æ€æ•°æ®çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³ä¹æ³¨é‡Šã€‚åŸºäºæ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†MuMu-LLaMAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘çš„é¢„è®­ç»ƒç¼–ç å™¨ã€‚å¯¹äºéŸ³ä¹ç”Ÿæˆï¼Œæˆ‘ä»¬é›†æˆäº†AudioLDM 2å’ŒMusicGenã€‚æˆ‘ä»¬åœ¨å››é¡¹ä»»åŠ¡ä¸Šçš„è¯„ä¼°â€”â€”éŸ³ä¹ç†è§£ã€æ–‡æœ¬åˆ°éŸ³ä¹ç”Ÿæˆã€åŸºäºæç¤ºçš„éŸ³ä¹ç¼–è¾‘å’Œå¤šæ¨¡æ€éŸ³ä¹ç”Ÿæˆâ€”â€”è¡¨æ˜ï¼ŒMuMu-LLaMAè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€éŸ³ä¹åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06660v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong><br>    è¯¥ç ”ç©¶è§£å†³äº†å¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆé¢†åŸŸçš„ä¸è¶³é—®é¢˜ã€‚ä¸ºäº†æ”¹è¿›ï¼Œç ”ç©¶äººå‘˜æ¨å‡ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³ä¹æ³¨é‡Šçš„å¤šæ¨¡æ€æ•°æ®ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œä»–ä»¬æå‡ºäº†MuMu-LLaMAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘çš„é¢„è®­ç»ƒç¼–ç å™¨ã€‚åœ¨éŸ³ä¹ç”Ÿæˆæ–¹é¢ï¼Œä»–ä»¬æ•´åˆäº†AudioLDM 2å’ŒMusicGenã€‚è¯„ä¼°è¡¨æ˜ï¼ŒMuMu-LLaMAåœ¨å››ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šæ¨¡æ€éŸ³ä¹åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€éŸ³ä¹ç†è§£å’Œç”Ÿæˆé¢†åŸŸç¼ºä¹å……åˆ†ç ”ç©¶çš„èµ„æºé™åˆ¶äº†å…¶å‘å±•ã€‚</li>
<li>æ–°æ•°æ®é›†åŒ…å«äº†æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³ä¹æ³¨é‡Šçš„å¤šæ¨¡æ€æ•°æ®ï¼Œä¸ºç›¸å…³ç ”ç©¶æä¾›äº†ä¸°å¯Œçš„ç´ æã€‚</li>
<li>MuMu-LLaMAæ¨¡å‹åˆ©ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨å¤„ç†éŸ³ä¹ã€å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚</li>
<li>AudioLDM 2å’ŒMusicGenè¢«æ•´åˆåˆ°MuMu-LLaMAæ¨¡å‹ä¸­ç”¨äºéŸ³ä¹ç”Ÿæˆã€‚</li>
<li>MuMu-LLaMAåœ¨å››ä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>è¿™é¡¹ç ”ç©¶ä¸ºå¤šæ¨¡æ€éŸ³ä¹åº”ç”¨æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-122c93da4f6c5a7115de1cd2fe344b35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30327339abe0226890453f3c316956dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02dc51ac650b1e0c7df033adb6acdf40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58c676ca9a2a7d11c04ff7ad9fb443f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fccf51ca681f999e4d64e0a05ad0439f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-400aac7eddd0dd015de0293b26ebcb70.jpg" align="middle">
</details>




<h2 id="Chatbots-im-Schulunterricht-Wir-testen-das-Fobizz-Tool-zur-automatischen-Bewertung-von-Hausaufgaben"><a href="#Chatbots-im-Schulunterricht-Wir-testen-das-Fobizz-Tool-zur-automatischen-Bewertung-von-Hausaufgaben" class="headerlink" title="Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur   automatischen Bewertung von Hausaufgaben"></a>Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur   automatischen Bewertung von Hausaufgaben</h2><p><strong>Authors:Rainer MÃ¼hlhoff, Marte Henningsen</strong></p>
<p>This study examines the AI-powered grading tool â€œAI Grading Assistantâ€ by the German company Fobizz, designed to support teachers in evaluating and providing feedback on student assignments. Against the societal backdrop of an overburdened education system and rising expectations for artificial intelligence as a solution to these challenges, the investigation evaluates the toolâ€™s functional suitability through two test series. The results reveal significant shortcomings: The toolâ€™s numerical grades and qualitative feedback are often random and do not improve even when its suggestions are incorporated. The highest ratings are achievable only with texts generated by ChatGPT. False claims and nonsensical submissions frequently go undetected, while the implementation of some grading criteria is unreliable and opaque. Since these deficiencies stem from the inherent limitations of large language models (LLMs), fundamental improvements to this or similar tools are not immediately foreseeable. The study critiques the broader trend of adopting AI as a quick fix for systemic problems in education, concluding that Fobizzâ€™s marketing of the tool as an objective and time-saving solution is misleading and irresponsible. Finally, the study calls for systematic evaluation and subject-specific pedagogical scrutiny of the use of AI tools in educational contexts. </p>
<blockquote>
<p>æœ¬ç ”ç©¶è€ƒå¯Ÿäº†ç”±å¾·å›½å…¬å¸Fobizzå¼€å‘çš„AIè¾…åŠ©è¯„åˆ†å·¥å…·â€œAIè¯„åˆ†åŠ©æ‰‹â€ï¼Œè¯¥å·¥å…·æ—¨åœ¨æ”¯æŒæ•™å¸ˆè¯„ä¼°å­¦ç”Ÿçš„ä½œä¸šå¹¶æä¾›åé¦ˆæ„è§ã€‚ åœ¨æ•™è‚²åˆ¶åº¦è´Ÿæ‹…è¿‡é‡ä»¥åŠå¯¹äººå·¥æ™ºèƒ½ä½œä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„è§£å†³æ–¹æ¡ˆçš„æœŸæœ›ä¸æ–­ä¸Šå‡çš„ç¤¾ä¼šèƒŒæ™¯ä¸‹ï¼Œæœ¬ç ”ç©¶é€šè¿‡ä¸¤é¡¹æµ‹è¯•ç³»åˆ—è¯„ä¼°äº†è¯¥å·¥å…·çš„åŠŸèƒ½é€‚ç”¨æ€§ã€‚ ç»“æœæ˜¾ç¤ºå­˜åœ¨é‡å¤§ç¼ºé™·ï¼šè¯¥å·¥å…·çš„åˆ†æ•°å’Œå®šæ€§åé¦ˆå¾€å¾€æ˜¯éšæœºçš„ï¼Œå³ä½¿é‡‡çº³äº†å…¶å»ºè®®ä¹Ÿæ— æ³•æ”¹å–„ã€‚ åªæœ‰é€šè¿‡ChatGPTç”Ÿæˆçš„æ–‡æœ¬æ‰èƒ½è·å¾—æœ€é«˜åˆ†ã€‚é”™è¯¯çš„å£°æ˜å’Œéç†æ™ºçš„æäº¤ç»å¸¸æ²¡æœ‰è¢«å‘ç°ï¼Œè€Œä¸€äº›è¯„åˆ†æ ‡å‡†çš„å®æ–½æ—¢ä¸å¯é ä¹Ÿä¸é€æ˜ã€‚ ç”±äºè¿™äº›ç¼ºé™·æºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…åœ¨å±€é™æ€§ï¼Œå› æ­¤å¯¹è¯¥å·¥å…·æˆ–ç±»ä¼¼å·¥å…·çš„åŸºæœ¬æ”¹è¿›å¹¶ä¸ä¹è§‚ã€‚æœ¬ç ”ç©¶æ‰¹è¯„äº†åœ¨æ•™è‚²é¢†åŸŸé‡‡ç”¨äººå·¥æ™ºèƒ½ä½œä¸ºå¿«é€Ÿè§£å†³ç³»ç»Ÿæ€§é—®é¢˜çš„æ›´å¹¿æ³›è¶‹åŠ¿ï¼Œè®¤ä¸ºFobizzå°†è¯¥å·¥å…·å®£ä¼ ä¸ºå®¢è§‚çœæ—¶è§£å†³æ–¹æ¡ˆçš„åšæ³•å…·æœ‰è¯¯å¯¼æ€§å’Œä¸è´Ÿè´£ä»»ã€‚æœ€åï¼Œè¯¥ç ”ç©¶å‘¼åå¯¹åœ¨æ•™è‚²ç¯å¢ƒä¸­ä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·è¿›è¡Œç³»ç»Ÿçš„è¯„ä¼°å’Œå­¦ç§‘ç‰¹å®šçš„æ•™è‚²å®¡æŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06651v1">PDF</a> 32 pages, in German language</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç”±å¾·å›½å…¬å¸Fobizzå¼€å‘çš„AIæ™ºèƒ½è¯„åˆ†å·¥å…·â€AI Grading Assistantâ€ï¼Œæ—¨åœ¨æ”¯æŒæ•™å¸ˆè¯„ä¼°å­¦ç”Ÿä½œä¸šå¹¶æä¾›åé¦ˆã€‚ç ”ç©¶æŒ‡å‡ºè¯¥å·¥å…·åœ¨åŠŸèƒ½ä¸Šå­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œå¦‚è¯„åˆ†éšæœºæ€§å¤§ï¼Œéš¾ä»¥æ”¹å–„è´¨é‡ï¼›éƒ¨åˆ†è¯„ä»·æ ‡å‡†ä¸æ˜ç¡®æˆ–ä¸ä¸¥è°¨ï¼Œç”šè‡³å®¹æ˜“å¿½è§†é”™è¯¯æˆ–ä¸åˆç†çš„æäº¤å†…å®¹ã€‚è¿™äº›é—®é¢˜æºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…åœ¨å±€é™æ€§ï¼Œéš¾ä»¥é¢„è§çŸ­æœŸå†…å¯¹è¯¥å·¥å…·æˆ–ç±»ä¼¼å·¥å…·çš„å®è´¨æ€§æ”¹è¿›ã€‚ç ”ç©¶æ‰¹è¯„äº†æ•™è‚²ç•Œå¹¿æ³›é‡‡ç”¨AIä½œä¸ºå¿«é€Ÿè§£å†³æ–¹æ¡ˆçš„è¶‹åŠ¿ï¼Œè®¤ä¸ºFobizzå°†è¯¥å·¥å…·å®£ä¼ ä¸ºå®¢è§‚å’Œçœæ—¶è§£å†³æ–¹æ¡ˆçš„è¥é”€æ‰‹æ®µä¸è´Ÿè´£ä»»ã€‚æœ€åå‘¼åå¯¹AIå·¥å…·åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨è¿›è¡Œç³»ç»Ÿæ€§è¯„ä¼°å’Œå­¦ç§‘ç‰¹å®šçš„æ•™è‚²å®¡æŸ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI Grading Assistantå·¥å…·å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼Œè¯„åˆ†éšæœºæ€§å¤§ï¼Œéš¾ä»¥æä¾›æœ‰æ•ˆåé¦ˆã€‚</li>
<li>éƒ¨åˆ†è¯„ä»·æ ‡å‡†ä¸æ˜ç¡®æˆ–ä¸ä¸¥è°¨ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®è¯„ä¼°å­¦ç”Ÿä½œä¸šã€‚</li>
<li>å·¥å…·å®¹æ˜“å¿½è§†é”™è¯¯æˆ–ä¸åˆç†çš„æäº¤å†…å®¹ï¼Œå­˜åœ¨å®‰å…¨éšæ‚£ã€‚</li>
<li>è¿™äº›é—®é¢˜çš„æ ¹æºåœ¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…åœ¨å±€é™æ€§ã€‚</li>
<li>å¯¹ç±»ä¼¼å·¥å…·çš„çŸ­æœŸå®è´¨æ€§æ”¹è¿›å‰æ™¯ä¸æ˜æœ—ã€‚</li>
<li>å°†AI Grading Assistantå®£ä¼ ä¸ºå®¢è§‚å’Œçœæ—¶è§£å†³æ–¹æ¡ˆçš„è¥é”€æ‰‹æ®µè¢«æ‰¹è¯„ä¸ºä¸è´Ÿè´£ä»»ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e284e302c363c0aeeda1d3d6903dc8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30c7981873f23b6eec88edc23388e20e.jpg" align="middle">
</details>




<h2 id="MAVias-Mitigate-any-Visual-Bias"><a href="#MAVias-Mitigate-any-Visual-Bias" class="headerlink" title="MAVias: Mitigate any Visual Bias"></a>MAVias: Mitigate any Visual Bias</h2><p><strong>Authors:Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou</strong></p>
<p>Mitigating biases in computer vision models is an essential step towards the trustworthiness of artificial intelligence models. Existing bias mitigation methods focus on a small set of predefined biases, limiting their applicability in visual datasets where multiple, possibly unknown biases exist. To address this limitation, we introduce MAVias, an open-set bias mitigation approach leveraging foundation models to discover spurious associations between visual attributes and target classes. MAVias first captures a wide variety of visual features in natural language via a foundation image tagging model, and then leverages a large language model to select those visual features defining the target class, resulting in a set of language-coded potential visual biases. We then translate this set of potential biases into vision-language embeddings and introduce an in-processing bias mitigation approach to prevent the model from encoding information related to them. Our experiments on diverse datasets, including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAVias effectively detects and mitigates a wide range of biases in visual recognition tasks outperforming current state-of-the-art. </p>
<blockquote>
<p>è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„åè§ç¼“è§£æ˜¯å®ç°äººå·¥æ™ºèƒ½æ¨¡å‹å¯ä¿¡åº¦çš„å…³é”®æ­¥éª¤ã€‚ç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•ä¸»è¦å…³æ³¨ä¸€ç»„é¢„å®šä¹‰çš„åè§ï¼Œè¿™åœ¨å­˜åœ¨å¤šä¸ªå¯èƒ½æœªçŸ¥çš„åè§çš„è§†è§‰æ•°æ®é›†ä¸­é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† MAViasï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹å‘ç°è§†è§‰å±æ€§å’Œç›®æ ‡ç±»åˆ«ä¹‹é—´å¶ç„¶å…³è”çš„å¼€é›†åè§ç¼“è§£æ–¹æ³•ã€‚ MAViasé¦–å…ˆé€šè¿‡åŸºç¡€å›¾åƒæ ‡è®°æ¨¡å‹æ•è·è‡ªç„¶è¯­è¨€ä¸­çš„å¤šç§è§†è§‰ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©å®šä¹‰ç›®æ ‡ç±»åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œå½¢æˆä¸€ç»„è¯­è¨€ç¼–ç çš„æ½œåœ¨è§†è§‰åè§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™ç»„æ½œåœ¨çš„åè§è½¬åŒ–ä¸ºè§†è§‰è¯­è¨€åµŒå…¥ï¼Œå¹¶å¼•å…¥ä¸€ç§å¤„ç†è¿‡ç¨‹ä¸­çš„åè§ç¼“è§£æ–¹æ³•ï¼Œä»¥é˜²æ­¢æ¨¡å‹ç¼–ç ä¸åè§ç›¸å…³çš„ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬CelebAã€Waterbirdsã€ImageNetå’ŒUrbanCarsç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAViasæœ‰æ•ˆåœ°æ£€æµ‹å’Œç¼“è§£äº†è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„å¹¿æ³›åè§ï¼Œå¹¶ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06632v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½æ¨¡å‹çš„è®¡ç®—æœºè§†è§‰éƒ¨åˆ†è¦å‡å°‘åè§æ˜¯å®ç°å…¶å¯ä¿¡èµ–çš„é‡è¦æ­¥éª¤ã€‚ç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é¢„å…ˆå®šä¹‰çš„å°‘æ•°åè§ä¸Šï¼Œæ— æ³•å¤„ç†å«æœ‰å¤šä¸ªæ½œåœ¨æœªçŸ¥åè§çš„è§†è§‰æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MAViasï¼Œè¿™æ˜¯ä¸€ç§å¼€æ”¾å¼åè§ç¼“è§£æ–¹æ³•ï¼Œåˆ©ç”¨åŸºç¡€æ¨¡å‹å‘ç°å¹¶è¯†åˆ«è§†è§‰å±æ€§ä¸ç›®æ ‡ç±»åˆ«ä¹‹é—´çš„å¶ç„¶å…³è”ã€‚MAViasé¦–å…ˆé€šè¿‡å›¾åƒæ ‡è®°åŸºç¡€æ¨¡å‹æ•è·å¤§é‡è‡ªç„¶è¯­è¨€ä¸­çš„è§†è§‰ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©å®šä¹‰ç›®æ ‡ç±»åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œä»è€Œå¾—åˆ°ä¸€ç»„è¯­è¨€ç¼–ç çš„æ½œåœ¨è§†è§‰åè§ã€‚ç„¶åæˆ‘ä»¬å°†è¿™äº›æ½œåœ¨çš„åè§è½¬åŒ–ä¸ºè§†è§‰è¯­è¨€åµŒå…¥ï¼Œå¹¶å¼•å…¥ä¸€ç§å¤„ç†è¿‡ç¨‹ä¸­çš„åè§ç¼“è§£æ–¹æ³•ï¼Œé˜²æ­¢æ¨¡å‹ä¸ä¹‹ç›¸å…³çš„ä¿¡æ¯è¢«ç¼–ç ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬CelebAã€Waterbirdsã€ImageNetå’ŒUrbanCarsç­‰ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAViasæœ‰æ•ˆåœ°æ£€æµ‹å’Œç¼“è§£äº†è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„å¹¿æ³›åè§ï¼Œå¹¶ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡å°‘è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„åè§å¯¹äºå®ç°äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä¿¡ä»»è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åè§ç¼“è§£æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é¢„å®šä¹‰çš„å°‘æ•°åè§ä¸Šï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MAViasæ˜¯ä¸€ç§å¼€æ”¾é›†åè§ç¼“è§£æ–¹æ³•ï¼Œå¯å‘ç°å¹¶è¯†åˆ«è§†è§‰å±æ€§ä¸ç›®æ ‡ç±»åˆ«ä¹‹é—´çš„å¶ç„¶å…³è”ã€‚</li>
<li>MAViasåˆ©ç”¨å›¾åƒæ ‡è®°åŸºç¡€æ¨¡å‹æ•è·è§†è§‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©å®šä¹‰ç›®æ ‡ç±»åˆ«çš„ç‰¹å¾ã€‚</li>
<li>MAViaså°†æ½œåœ¨åè§è½¬åŒ–ä¸ºè§†è§‰è¯­è¨€åµŒå…¥ï¼Œä»¥ä¾¿è¿›è¡Œå¤„ç†è¿‡ç¨‹ä¸­çš„åè§ç¼“è§£ã€‚</li>
<li>MAViasåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒæœ‰æ•ˆåœ°æ£€æµ‹å’Œç¼“è§£äº†è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„å¹¿æ³›åè§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea95fec25a4ad7a5aeb222adaf8a5ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5fe53c2d07dcb80e845ab3f649f3b9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5f5c3269765ecd0ef8e554652a67fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102bdff6b88aaed5962a543f196838d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac371b250ba779b889fc419588d3dd17.jpg" align="middle">
</details>




<h2 id="3D-Spatial-Understanding-in-MLLMs-Disambiguation-and-Evaluation"><a href="#3D-Spatial-Understanding-in-MLLMs-Disambiguation-and-Evaluation" class="headerlink" title="3D Spatial Understanding in MLLMs: Disambiguation and Evaluation"></a>3D Spatial Understanding in MLLMs: Disambiguation and Evaluation</h2><p><strong>Authors:Chun-Peng Chang, Alain Pagani, Didier Stricker</strong></p>
<p>Multimodal Large Language Models (MLLMs) have made significant progress in tasks such as image captioning and question answering. However, while these models can generate realistic captions, they often struggle with providing precise instructions, particularly when it comes to localizing and disambiguating objects in complex 3D environments. This capability is critical as MLLMs become more integrated with collaborative robotic systems. In scenarios where a target object is surrounded by similar objects (distractors), robots must deliver clear, spatially-aware instructions to guide humans effectively. We refer to this challenge as contextual object localization and disambiguation, which imposes stricter constraints than conventional 3D dense captioning, especially regarding ensuring target exclusivity. In response, we propose simple yet effective techniques to enhance the modelâ€™s ability to localize and disambiguate target objects. Our approach not only achieves state-of-the-art performance on conventional metrics that evaluate sentence similarity, but also demonstrates improved 3D spatial understanding through 3D visual grounding model. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒæè¿°å’Œé—®ç­”ç­‰ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„æè¿°ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨æä¾›ç²¾ç¡®æŒ‡ä»¤æ—¶é‡åˆ°å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒä¸­å®šä½å’Œæ¶ˆé™¤å¯¹è±¡æ­§ä¹‰æ—¶ã€‚éšç€MLLMsä¸åä½œæœºå™¨äººç³»ç»Ÿçš„é›†æˆåº¦è¶Šæ¥è¶Šé«˜ï¼Œè¿™ç§èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚åœ¨ç›®æ ‡å¯¹è±¡è¢«ç±»ä¼¼å¯¹è±¡ï¼ˆå¹²æ‰°ç‰©ï¼‰åŒ…å›´çš„æƒ…å†µä¸‹ï¼Œæœºå™¨äººå¿…é¡»æä¾›æ¸…æ™°ã€å…·æœ‰ç©ºé—´æ„ŸçŸ¥çš„æŒ‡ä»¤ä»¥æœ‰æ•ˆåœ°æŒ‡å¯¼äººç±»ã€‚æˆ‘ä»¬å°†è¿™ä¸€æŒ‘æˆ˜ç§°ä¸ºä¸Šä¸‹æ–‡å¯¹è±¡å®šä½å’Œæ­§ä¹‰æ¶ˆé™¤ï¼Œå®ƒæ¯”ä¼ ç»Ÿçš„ä¸‰ç»´å¯†é›†æè¿°æ›´åŠ ä¸¥æ ¼ï¼Œå°¤å…¶éœ€è¦ç¡®ä¿ç›®æ ‡å¯¹è±¡çš„å”¯ä¸€æ€§ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æå‡ºäº†ç®€å•è€Œæœ‰æ•ˆçš„æŠ€æœ¯æ¥å¢å¼ºæ¨¡å‹å¯¹ç›®æ ‡å¯¹è±¡çš„å®šä½å’Œæ¶ˆé™¤æ­§ä¹‰çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…è¾¾åˆ°äº†åŸºäºå¥å­ç›¸ä¼¼æ€§çš„ä¼ ç»Ÿè¯„ä»·æŒ‡æ ‡çš„å…ˆè¿›æ°´å¹³ï¼Œè€Œä¸”é€šè¿‡ä¸‰ç»´è§†è§‰å®šä½æ¨¡å‹å±•ç¤ºäº†å¢å¼ºçš„ä¸‰ç»´ç©ºé—´ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06613v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒæ ‡æ³¨å’Œé—®ç­”ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æä¾›ç²¾ç¡®æŒ‡ä»¤æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚3Dç¯å¢ƒä¸­å¯¹ç›®æ ‡è¿›è¡Œå®šä½å’Œè§£æã€‚éšç€MLLMsä¸åä½œæœºå™¨äººç³»ç»Ÿçš„é›†æˆï¼Œè¿™ç§èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚åœ¨ç›®æ ‡å¯¹è±¡è¢«ç±»ä¼¼ç‰©ä½“åŒ…å›´çš„åœºæ™¯ä¸­ï¼Œæœºå™¨äººå¿…é¡»æä¾›æ¸…æ™°çš„ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤ä»¥æœ‰æ•ˆæŒ‡å¯¼äººç±»ã€‚æˆ‘ä»¬ç§°è¿™ä¸€æŒ‘æˆ˜ä¸ºä¸Šä¸‹æ–‡ç›®æ ‡å®šä½å’Œè§£æï¼Œå®ƒæ¯”ä¼ ç»Ÿçš„3Då¯†é›†æ ‡æ³¨å…·æœ‰æ›´ä¸¥æ ¼çš„çº¦æŸï¼Œç‰¹åˆ«æ˜¯åœ¨ç¡®ä¿ç›®æ ‡ç‹¬ç‰¹æ€§æ–¹é¢ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºç®€å•æœ‰æ•ˆçš„æŠ€æœ¯æ¥æå‡æ¨¡å‹å®šä½å’Œç›®æ ‡è§£æçš„èƒ½åŠ›ã€‚ä¸ä»…åœ¨ä¼ ç»Ÿè¯„ä¼°å¥å­ç›¸ä¼¼æ€§çš„æŒ‡æ ‡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨é€šè¿‡ä¸‰ç»´è§†è§‰å®šä½æ¨¡å‹è¯„ä¼°çš„ä¸‰ç»´ç©ºé—´ç†è§£æ–¹é¢ä¹Ÿè¡¨ç°å‡ºäº†æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒæ ‡æ³¨å’Œé—®ç­”ç­‰æ–¹é¢æœ‰æ˜¾è‘—çš„è¿›æ­¥ã€‚</li>
<li>MLLMsåœ¨æä¾›ç²¾ç¡®æŒ‡ä»¤æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚3Dç¯å¢ƒä¸­å¯¹ç›®æ ‡è¿›è¡Œå®šä½å’Œè§£æã€‚</li>
<li>éšç€MLLMsä¸åä½œæœºå™¨äººç³»ç»Ÿçš„é›†æˆï¼Œç›®æ ‡å®šä½å’Œè§£æèƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>åœ¨ç›®æ ‡ç‰©ä½“è¢«ç±»ä¼¼ç‰©ä½“åŒ…å›´çš„åœºæ™¯ä¸­ï¼Œæœºå™¨äººéœ€è¦ç»™å‡ºæ¸…æ™°çš„ç©ºé—´æ„ŸçŸ¥æŒ‡ä»¤ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç›®æ ‡å®šä½å’Œè§£ææ˜¯æœºå™¨äººå¿…é¡»è§£å†³çš„ä¸€ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„çº¦æŸä»¥ç¡®ä¿ç›®æ ‡çš„ç‹¬ç‰¹æ€§ã€‚</li>
<li>ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæå‡ºäº†ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯æ¥æå‡æ¨¡å‹å®šä½å’Œç›®æ ‡è§£æçš„èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-207d6ab9aef0c597455f2e66557c0b69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e18f36ca28ae7cf116de1b2f8854260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11a9ddf6ff3ab3b92bd734ec760f4559.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4bf5455d954b47348c70cf3f72629cff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1408cb9b54263e4d067f3ae48a36e4db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5636ab93fbbc67def792efa3c12a3d46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78b8cf45b2cea1c81397b5ceeb265599.jpg" align="middle">
</details>




<h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ï¼Œä¹Ÿè¢«ç§°ä¸ºè¯­éŸ³åˆæˆï¼Œæ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆè‡ªç„¶å£°éŸ³çš„äººç±»è¯­éŸ³çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€è¿‘ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ç»è¶…è¶Šäº†åˆæˆç±»ä¼¼äººç±»çš„è¯­éŸ³ï¼Œå‘å±•åˆ°äº†èƒ½å¤Ÿå®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™åŒ…æ‹¬åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ ä¸­çš„æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œåœ¨è¿‡å»çš„å‡ å¹´é‡Œæå¤§åœ°å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ç­‰å¤šç§æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›å¯¹ç ”ç©¶ç°çŠ¶çš„æ¸…æ™°ç†è§£ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†å¯æ§TTSçš„ä¸€èˆ¬æµç¨‹ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œå¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†æ€»ç»“äº†æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œå¹¶å¯¹å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘è¿›è¡Œäº†ä¸€äº›æ¢è®¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç¯‡ç»¼è¿°è®ºæ–‡æä¾›äº†å¯¹æ–°å…´çš„å¯æ§TTSæ–¹æ³•çš„é¦–æ¬¡å…¨é¢å›é¡¾ï¼Œå¯¹å­¦æœ¯ç ”ç©¶äººå‘˜å’Œè¡Œä¸šä»ä¸šè€…éƒ½æœ‰å¾ˆå¤§çš„å‚è€ƒä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v1">PDF</a> A comprehensive survey on controllable TTS, 23 pages, 6 tables, 4   figures, 280 references</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å¯æ§TTSçš„å½“å‰ç ”ç©¶çŠ¶æ€ã€‚éšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ç»è¶…è¶Šäº†ä»…åˆæˆäººç±»è¯­éŸ³çš„é˜¶æ®µï¼Œç°åœ¨èƒ½å¤Ÿå®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆï¼ŒåŒ…æ‹¬ç²¾ç»†æ§åˆ¶åˆæˆè¯­éŸ³çš„å„ç§å±æ€§ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ–‡ç« è¿˜æ¢è®¨äº†æ·±åº¦å­¦ä¹ çš„è¿›æ­¥å¦‚ä½•æå¤§åœ°æ¨åŠ¨äº†å¯æ§TTSçš„å‘å±•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æä¾›äº†å¯¹å¯æ§TTSæ–¹æ³•ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥çš„è¯¦ç»†åˆ†ç±»ï¼Œå¹¶å¯¹æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†æ¦‚è¿°ï¼ŒåŒæ—¶è¿˜ä»‹ç»äº†å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰æŠ€æœ¯æ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆè‡ªç„¶æµç•…çš„äººç±»è¯­éŸ³ã€‚</li>
<li>å¯æ§TTSæŠ€æœ¯å…è®¸å¯¹åˆæˆè¯­éŸ³çš„å„ç§å±æ€§è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚</li>
<li>æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå·²ç»å¤§å¤§å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†å¯¹å¯æ§TTSçš„å½“å‰ç ”ç©¶çŠ¶æ€çš„å…¨é¢ç»¼è¿°ã€‚</li>
<li>æ–‡ç« è¯¦ç»†è®¨è®ºäº†å¯æ§TTSçš„æ–¹æ³•ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ã€‚</li>
<li>ç»¼è¿°æ¶µç›–äº†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡çš„æ¦‚è¿°ã€‚</li>
<li>æ–‡ç« è¿˜æ¢è®¨äº†å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3255132bdca965fc85d69d43568f2f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8fe56ee1b3d384d421df9dbf984646b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e826485c7661728072f0a00efebdb680.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78d17f9628b8ef660154504c576be31e.jpg" align="middle">
</details>




<h2 id="Anchoring-Bias-in-Large-Language-Models-An-Experimental-Study"><a href="#Anchoring-Bias-in-Large-Language-Models-An-Experimental-Study" class="headerlink" title="Anchoring Bias in Large Language Models: An Experimental Study"></a>Anchoring Bias in Large Language Models: An Experimental Study</h2><p><strong>Authors:Jiaxu Lou</strong></p>
<p>Large Language Models (LLMs) like GPT-4 and Gemini have significantly advanced artificial intelligence by enabling machines to generate and comprehend human-like text. Despite their impressive capabilities, LLMs are not immune to limitations, including various biases. While much research has explored demographic biases, the cognitive biases in LLMs have not been equally scrutinized. This study delves into anchoring bias, a cognitive bias where initial information disproportionately influences judgment. Utilizing an experimental dataset, we examine how anchoring bias manifests in LLMs and verify the effectiveness of various mitigation strategies. Our findings highlight the sensitivity of LLM responses to biased hints. At the same time, our experiments show that, to mitigate anchoring bias, one needs to collect hints from comprehensive angles to prevent the LLMs from being anchored to individual pieces of information, while simple algorithms such as Chain-of-Thought, Thoughts of Principles, Ignoring Anchor Hints, and Reflection are not sufficient. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚GPT-4å’ŒåŒå­åº§ï¼Œé€šè¿‡ä½¿æœºå™¨èƒ½å¤Ÿç”Ÿæˆå’Œç†è§£ç±»ä¼¼äººç±»çš„æ–‡æœ¬ï¼Œä»è€Œæ˜¾è‘—åœ°æ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†LLMå¹¶éä¸å—é™åˆ¶ï¼Œå…¶ä¸­åŒ…æ‹¬å„ç§åè§ã€‚è™½ç„¶è®¸å¤šç ”ç©¶å·²ç»æ¢è®¨äº†äººå£åè§ï¼Œä½†LLMä¸­çš„è®¤çŸ¥åè§å°šæœªå—åˆ°åŒç­‰å®¡è§†ã€‚æœ¬ç ”ç©¶æ·±å…¥æ¢è®¨äº†é”šå®šåè§ï¼Œè¿™æ˜¯ä¸€ç§è®¤çŸ¥åè§ï¼Œåˆå§‹ä¿¡æ¯ä¼šä¸æˆæ¯”ä¾‹åœ°å½±å“åˆ¤æ–­ã€‚æˆ‘ä»¬åˆ©ç”¨å®éªŒæ•°æ®é›†ï¼Œç ”ç©¶LLMä¸­é”šå®šåè§çš„è¡¨ç°ï¼Œå¹¶éªŒè¯å„ç§ç¼“è§£ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMå“åº”å¯¹åè§çº¿ç´¢éå¸¸æ•æ„Ÿã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ºäº†ç¼“è§£é”šå®šåè§ï¼Œéœ€è¦ä»ç»¼åˆè§’åº¦æ”¶é›†çº¿ç´¢ï¼Œé˜²æ­¢LLMå±€é™äºä¸ªåˆ«ä¿¡æ¯ï¼Œè€Œç®€å•çš„ç®—æ³•å¦‚æ€ç»´é“¾ã€åŸåˆ™æ€è€ƒã€å¿½ç•¥é”šç‚¹æç¤ºå’Œåæ€ç­‰å¹¶ä¸è¶³ä»¥åº”å¯¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06593v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-4å’ŒGeminiä¸­å­˜åœ¨çš„è®¤çŸ¥åå·®ï¼Œç‰¹åˆ«æ˜¯é”šå®šåå·®ã€‚å®éªŒè¯æ˜ï¼Œåˆå§‹ä¿¡æ¯ä¼šè¿‡åº¦å½±å“LLMçš„åˆ¤æ–­ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†ç¼“è§£é”šå®šåå·®çš„ç­–ç•¥ï¼Œå‘ç°éœ€è¦ä»å¤šè§’åº¦æ”¶é›†æç¤ºä¿¡æ¯æ¥é˜²æ­¢LLMè¢«å•ä¸€ä¿¡æ¯æ‰€å¼•å¯¼ï¼Œç®€å•çš„ç®—æ³•å¦‚æ€ç»´é“¾ã€åŸåˆ™æ€è€ƒã€å¿½ç•¥é”šå®šæç¤ºå’Œåæ€ç­‰å¹¶ä¸è¶³å¤Ÿæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå’Œç†è§£äººç±»æ–‡æœ¬æ–¹é¢å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¹¶éæ²¡æœ‰å±€é™ï¼ŒåŒ…æ‹¬å„ç§åè§ã€‚</li>
<li>é”šå®šåè§æ˜¯è®¤çŸ¥åè§çš„ä¸€ç§ï¼Œå³åˆå§‹ä¿¡æ¯ä¼šä¸æˆæ¯”ä¾‹åœ°å½±å“åˆ¤æ–­ï¼Œåœ¨LLMä¸­åŒæ ·å­˜åœ¨ã€‚</li>
<li>LLMå¯¹å¸¦åè§çš„æç¤ºéå¸¸æ•æ„Ÿï¼Œå…¶å›åº”ä¼šè¢«è¿™äº›æç¤ºæ‰€å½±å“ã€‚</li>
<li>ä¸ºäº†ç¼“è§£é”šå®šåè§ï¼Œéœ€è¦ä»å¤šä¸ªè§’åº¦æ”¶é›†æç¤ºä¿¡æ¯ã€‚</li>
<li>ç®€å•çš„ç®—æ³•å¦‚æ€ç»´é“¾ã€åŸåˆ™æ€è€ƒã€å¿½ç•¥é”šå®šæç¤ºå’Œåæ€å¹¶ä¸èƒ½æœ‰æ•ˆç¼“è§£é”šå®šåè§ã€‚</li>
<li>LLMéœ€è¦æ›´å…¨é¢çš„è®­ç»ƒæ•°æ®ä»¥å‡å°‘åè§ï¼Œæé«˜å†³ç­–çš„å…¬æ­£æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09dcc93b56c00370d2981ad1d66b47e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06bffa4e36fe7686678be7e6d75f20dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8d1154b58a7155bc561dc686085a5a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75a0cb51db44278cd87fc39a3c2e9d3d.jpg" align="middle">
</details>




<h2 id="Data-Quality-Enhancement-on-the-Basis-of-Diversity-with-Large-Language-Models-for-Text-Classification-Uncovered-Difficult-and-Noisy"><a href="#Data-Quality-Enhancement-on-the-Basis-of-Diversity-with-Large-Language-Models-for-Text-Classification-Uncovered-Difficult-and-Noisy" class="headerlink" title="Data Quality Enhancement on the Basis of Diversity with Large Language   Models for Text Classification: Uncovered, Difficult, and Noisy"></a>Data Quality Enhancement on the Basis of Diversity with Large Language   Models for Text Classification: Uncovered, Difficult, and Noisy</h2><p><strong>Authors:Min Zeng, Caiquan Liu, Shiqi Zhang, Li Xie, Chen Sang, Xiaoxin Chen, Xiaoxin Chen</strong></p>
<p>In recent years, the use of large language models (LLMs) for text classification has attracted widespread attention. Despite this, the classification accuracy of LLMs has not yet universally surpassed that of smaller models. LLMs can enhance their performance in text classification through fine-tuning. However, existing data quality research based on LLMs is challenging to apply directly to solve text classification problems. To further improve the performance of LLMs in classification tasks, this paper proposes a data quality enhancement (DQE) method for text classification based on LLMs. This method starts by using a greedy algorithm to select data, dividing the dataset into sampled and unsampled subsets, and then performing fine-tuning of the LLMs using the sampled data. Subsequently, this model is used to predict the outcomes for the unsampled data, categorizing incorrectly predicted data into uncovered, difficult, and noisy data. Experimental results demonstrate that our method effectively enhances the performance of LLMs in text classification tasks and significantly improves training efficiency, saving nearly half of the training time. Our method has achieved state-of-the-art performance in several open-source classification tasks. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒLLMçš„åˆ†ç±»ç²¾åº¦å°šæœªæ™®éè¶…è¿‡å°å‹æ¨¡å‹ã€‚LLMå¯ä»¥é€šè¿‡å¾®è°ƒæé«˜å…¶æ–‡æœ¬åˆ†ç±»æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºLLMçš„ç°æœ‰æ•°æ®è´¨é‡ç ”ç©¶åœ¨ç›´æ¥åº”ç”¨äºè§£å†³æ–‡æœ¬åˆ†ç±»é—®é¢˜æ—¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜LLMåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ–‡æœ¬åˆ†ç±»æ•°æ®è´¨é‡å¢å¼ºï¼ˆDQEï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨è´ªå¿ƒç®—æ³•é€‰æ‹©æ•°æ®ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºé‡‡æ ·å’Œæœªé‡‡æ ·å­é›†ï¼Œç„¶åä½¿ç”¨é‡‡æ ·æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚éšåï¼Œä½¿ç”¨è¯¥æ¨¡å‹å¯¹æœªé‡‡æ ·æ•°æ®è¿›è¡Œç»“æœé¢„æµ‹ï¼Œå°†é¢„æµ‹é”™è¯¯çš„æ•°æ®åˆ†ç±»ä¸ºæœªè¦†ç›–ã€éš¾ä»¥å¤„ç†å’Œæœ‰å™ªéŸ³çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†LLMåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼ŒèŠ‚çœäº†è¿‘ä¸€åŠçš„è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªå¼€æºåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06575v1">PDF</a> Accepted by COLING 2025(main, long paper)</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨è¿‘å¹´æ¥å¤‡å—å…³æ³¨ã€‚è™½ç„¶å…¶åˆ†ç±»ç²¾åº¦å°šæœªå…¨é¢è¶…è¶Šå°å‹æ¨¡å‹ï¼Œä½†é€šè¿‡å¾®è°ƒå¯ä»¥å¢å¼ºå…¶åœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†ç°æœ‰çš„åŸºäºLLMçš„æ•°æ®è´¨é‡ç ”ç©¶ç›´æ¥åº”ç”¨äºè§£å†³æ–‡æœ¬åˆ†ç±»é—®é¢˜æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ–‡æœ¬åˆ†ç±»æ•°æ®è´¨é‡å¢å¼ºï¼ˆDQEï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è´ªå¿ƒç®—æ³•é€‰æ‹©æ•°æ®ï¼Œå°†æ•°æ®é›†åˆ†ä¸ºé‡‡æ ·å’Œæœªé‡‡æ ·å­é›†ï¼Œå¹¶ä½¿ç”¨é‡‡æ ·æ•°æ®è¿›è¡ŒLLMå¾®è°ƒã€‚ç„¶åï¼Œä½¿ç”¨è¯¥æ¨¡å‹å¯¹æœªé‡‡æ ·æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå°†é¢„æµ‹é”™è¯¯çš„æ•°æ®åˆ†ç±»ä¸ºæœªè¦†ç›–ã€éš¾ä»¥å¤„ç†å’Œå™ªå£°æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæé«˜LLMåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒèŠ‚çœè¿‘ä¸€åŠçš„è®­ç»ƒæ—¶é—´ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¼€æºåˆ†ç±»ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>LLMçš„åˆ†ç±»ç²¾åº¦å°šæœªå…¨é¢è¶…è¶Šå°å‹æ¨¡å‹ï¼Œä½†å¯ä»¥é€šè¿‡å¾®è°ƒå¢å¼ºæ€§èƒ½ã€‚</li>
<li>ç›´æ¥å°†ç°æœ‰çš„åŸºäºLLMçš„æ•°æ®è´¨é‡ç ”ç©¶åº”ç”¨äºæ–‡æœ¬åˆ†ç±»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ–‡æœ¬åˆ†ç±»æ•°æ®è´¨é‡å¢å¼ºï¼ˆDQEï¼‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨è´ªå¿ƒç®—æ³•é€‰æ‹©æ•°æ®ï¼Œå¹¶å°†æ•°æ®é›†åˆ†ä¸ºé‡‡æ ·å’Œæœªé‡‡æ ·å­é›†ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é‡‡æ ·æ•°æ®è¿›è¡ŒLLMå¾®è°ƒï¼Œå¹¶é¢„æµ‹æœªé‡‡æ ·æ•°æ®çš„ç»“æœï¼Œå°†é”™è¯¯æ•°æ®åˆ†ç±»ä¸ºæœªè¦†ç›–ã€éš¾ä»¥å¤„ç†å’Œå™ªå£°æ•°æ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e124d5244d32b39305ce290ed8c45b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffcd5f07e6275e0e94cd5a5876b92eec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f25cec85c1784851904db1cca0de3db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7fd0a85a1a0cb30c2d2f6f4a4a5bfcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-786c1635fb0c54c9f29ed8585423befd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32c7ea28f783c93fa1eb319ee0d852eb.jpg" align="middle">
</details>




<h2 id="Sloth-scaling-laws-for-LLM-skills-to-predict-multi-benchmark-performance-across-families"><a href="#Sloth-scaling-laws-for-LLM-skills-to-predict-multi-benchmark-performance-across-families" class="headerlink" title="Sloth: scaling laws for LLM skills to predict multi-benchmark   performance across families"></a>Sloth: scaling laws for LLM skills to predict multi-benchmark   performance across families</h2><p><strong>Authors:Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Yuekai Sun, Mikhail Yurochkin</strong></p>
<p>Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1&#x2F;v2, demonstrating that Sloth predicts LLM performance efficiently and offers insights into scaling behaviors for downstream tasks such as coding and emotional intelligence applications. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼©æ”¾å®šå¾‹åŸºäºè§„æ¨¡ã€è®­ç»ƒæ•°æ®ç­‰å‚æ•°é¢„æµ‹æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸åŒæ¨¡å‹å®¶æ—åœ¨è®­ç»ƒé…ç½®å’Œæ•°æ®å¤„ç†æ–¹é¢çš„å·®å¼‚å¯¼è‡´åŸºå‡†æµ‹è¯•æ€§èƒ½å­˜åœ¨é‡å¤§å·®å¼‚ï¼Œä½¿å¾—å•ä¸€çš„ç¼©æ”¾å®šå¾‹éš¾ä»¥æ¨å¹¿åˆ°æ‰€æœ‰LLMã€‚å¦ä¸€æ–¹é¢ï¼Œé’ˆå¯¹ç‰¹å®šå®¶æ—çš„ç¼©æ”¾å®šå¾‹éœ€è¦ä¸ºæ¯ä¸ªå®¶æ—è®­ç»ƒä¸åŒå¤§å°æ¨¡å‹ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æŠ€èƒ½ç¼©æ”¾å®šå¾‹ï¼ˆSSLawsï¼Œå‘éŸ³ä¸ºSlothï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¼©æ”¾å®šå¾‹ï¼Œå®ƒåˆ©ç”¨å…¬å¼€å¯ç”¨çš„åŸºå‡†æµ‹è¯•æ•°æ®ï¼Œå¹¶å‡è®¾LLMçš„æ€§èƒ½æ˜¯ç”±ä½ç»´æ½œåœ¨æŠ€èƒ½é©±åŠ¨çš„ï¼Œå¦‚æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªã€‚è¿™äº›æ½œåœ¨æŠ€èƒ½å—åˆ°è®¡ç®—èµ„æºçš„å½±å“ï¼Œå¦‚æ¨¡å‹å¤§å°å’Œè®­ç»ƒä»¤ç‰Œï¼Œä½†åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­çš„æ•ˆç‡æœ‰æ‰€ä¸åŒã€‚Slothåˆ©ç”¨åŸºå‡†æµ‹è¯•ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæä¾›æ›´å‡†ç¡®å’Œå¯è§£é‡Šæ€§çš„é¢„æµ‹ï¼ŒåŒæ—¶å‡è½»æ¯ä¸ªå®¶æ—éœ€è¦è®­ç»ƒå¤šä¸ªLLMçš„éœ€æ±‚ã€‚æˆ‘ä»¬æä¾›äº†å…³äºå‚æ•°è¯†åˆ«çš„ç†è®ºç»“æœä»¥åŠåœ¨Open LLM Leaderboard v1&#x2F;v2çš„12ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°ï¼Œè¯æ˜Slothèƒ½å¤Ÿé«˜æ•ˆé¢„æµ‹LLMæ€§èƒ½ï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ç¼–ç å’Œæƒ…æ„Ÿæ™ºèƒ½åº”ç”¨ç¨‹åºï¼‰çš„ç¼©æ”¾è¡Œä¸ºæä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06540v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¼©æ”¾å®šå¾‹åŸºäºå‚æ•°ã€è§„æ¨¡å’Œè®­ç»ƒæ•°æ®æ¥é¢„æµ‹æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¸åŒæ¨¡å‹å®¶æ—åœ¨è®­ç»ƒé…ç½®å’Œæ•°æ®å¤„ç†æ–¹é¢çš„å·®å¼‚å¯¼è‡´åŸºå‡†æµ‹è¯•æ€§èƒ½å­˜åœ¨é‡å¤§å˜åŒ–ï¼Œä½¿å¾—å•ä¸€çš„ç¼©æ”¾å®šå¾‹éš¾ä»¥æ¨å¹¿åº”ç”¨åˆ°æ‰€æœ‰LLMã€‚æœ¬æ–‡æå‡ºæŠ€èƒ½ç¼©æ”¾å®šå¾‹ï¼ˆSSLawsï¼Œå‘éŸ³ä¸ºSlothï¼‰ï¼Œåˆ©ç”¨å…¬å¼€å¯ç”¨çš„åŸºå‡†æµ‹è¯•æ•°æ®ï¼Œå‡è®¾LLMæ€§èƒ½æ˜¯ç”±ä½ç»´æ½œåœ¨æŠ€èƒ½ï¼ˆå¦‚æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªï¼‰é©±åŠ¨çš„ã€‚è¿™äº›æ½œåœ¨æŠ€èƒ½å—åˆ°æ¨¡å‹å¤§å°å’Œè®­ç»ƒä»¤ç‰Œç­‰è®¡ç®—èµ„æºçš„å½±å“ï¼Œä½†åœ¨ä¸åŒçš„æ¨¡å‹å®¶æ—ä¸­å…·æœ‰ä¸åŒçš„æ•ˆç‡ã€‚Slothåˆ©ç”¨åŸºå‡†æµ‹è¯•ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œæä¾›æ›´å‡†ç¡®å’Œå¯è§£é‡Šæ€§çš„é¢„æµ‹ï¼ŒåŒæ—¶å‡è½»äº†å¯¹æ¯ä¸ªå®¶æ—è®­ç»ƒå¤šä¸ªLLMçš„éœ€æ±‚ã€‚æœ¬æ–‡ç»™å‡ºäº†å‚æ•°è¯†åˆ«çš„ç†è®ºç»“æœä»¥åŠåœ¨Open LLM Leaderboard v1&#x2F;v2çš„12ä¸ªçªå‡ºåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°ï¼Œè¡¨æ˜Slothèƒ½å¤Ÿé«˜æ•ˆé¢„æµ‹LLMæ€§èƒ½ï¼Œå¹¶ä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ç¼–ç å’Œæƒ…ç»ªæ™ºèƒ½åº”ç”¨ç¨‹åºï¼‰çš„ç¼©æ”¾è¡Œä¸ºæä¾›è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ€§èƒ½å—åˆ°å‚æ•°ã€è§„æ¨¡å’Œè®­ç»ƒæ•°æ®çš„å½±å“ï¼Œä½†å•ä¸€ç¼©æ”¾å®šå¾‹éš¾ä»¥é€‚ç”¨äºæ‰€æœ‰LLMã€‚</li>
<li>SSLawsï¼ˆSlothï¼‰æ˜¯ä¸€ç§æ–°å‹ç¼©æ”¾å®šå¾‹ï¼Œåˆ©ç”¨å…¬å¼€åŸºå‡†æ•°æ®ï¼Œä¾§é‡äºä½ç»´æ½œåœ¨æŠ€èƒ½æ¥é¢„æµ‹LLMæ€§èƒ½ã€‚</li>
<li>æ½œåœ¨æŠ€èƒ½å—æ¨¡å‹å¤§å°å’Œè®­ç»ƒä»¤ç‰Œç­‰è®¡ç®—èµ„æºå½±å“ï¼Œä½†ä¸åŒæ¨¡å‹å®¶æ—çš„æ•ˆç‡å­˜åœ¨å·®å¼‚ã€‚</li>
<li>Slothèƒ½å¤Ÿå‡†ç¡®é¢„æµ‹LLMæ€§èƒ½ï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªå®¶æ—è®­ç»ƒå¤šä¸ªæ¨¡å‹ã€‚</li>
<li>Slothæä¾›å¯¹ä¸‹æ¸¸ä»»åŠ¡ç¼©æ”¾è¡Œä¸ºçš„è§è§£ï¼Œå¦‚ç¼–ç å’Œæƒ…ç»ªæ™ºèƒ½åº”ç”¨ã€‚</li>
<li>æ–‡ç« é€šè¿‡ç†è®ºåˆ†æå’Œåœ¨12ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®è¯è¯„ä¼°ï¼Œè¯å®äº†Slothçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7acd65aa4a1071401a7d1e3ad52715d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b13a625b01dc9438c1f4e983072f282.jpg" align="middle">
</details>




<h2 id="Integrating-Expert-Labels-into-LLM-based-Emission-Goal-Detection-Example-Selection-vs-Automatic-Prompt-Design"><a href="#Integrating-Expert-Labels-into-LLM-based-Emission-Goal-Detection-Example-Selection-vs-Automatic-Prompt-Design" class="headerlink" title="Integrating Expert Labels into LLM-based Emission Goal Detection:   Example Selection vs Automatic Prompt Design"></a>Integrating Expert Labels into LLM-based Emission Goal Detection:   Example Selection vs Automatic Prompt Design</h2><p><strong>Authors:Marco Wrzalik, Adrian Ulges, Anne Uersfeld, Florian Faust</strong></p>
<p>We address the detection of emission reduction goals in corporate reports, an important task for monitoring companiesâ€™ progress in addressing climate change. Specifically, we focus on the issue of integrating expert feedback in the form of labeled example passages into LLM-based pipelines, and compare the two strategies of (1) a dynamic selection of few-shot examples and (2) the automatic optimization of the prompt by the LLM itself. Our findings on a public dataset of 769 climate-related passages from real-world business reports indicate that automatic prompt optimization is the superior approach, while combining both methods provides only limited benefit. Qualitative results indicate that optimized prompts do indeed capture many intricacies of the targeted emission goal extraction task. </p>
<blockquote>
<p>æˆ‘ä»¬å…³æ³¨ä¼ä¸šæŠ¥å‘Šä¸­å‡æ’ç›®æ ‡æ£€æµ‹çš„é—®é¢˜ï¼Œè¿™æ˜¯ç›‘æµ‹å…¬å¸åº”å¯¹æ°”å€™å˜åŒ–è¿›å±•çš„é‡è¦ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå°†ä¸“å®¶åé¦ˆä»¥æ ‡æ³¨æ®µè½çš„å½¢å¼èå…¥LLMç®¡é“çš„é—®é¢˜ï¼Œå¹¶æ¯”è¾ƒäº†ä¸¤ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰åŠ¨æ€é€‰æ‹©å°‘æ•°æ ·æœ¬ç¤ºä¾‹å’Œï¼ˆ2ï¼‰ç”±LLMæœ¬èº«è‡ªåŠ¨ä¼˜åŒ–æç¤ºã€‚æˆ‘ä»¬åœ¨åŒ…å«ç°å®ä¸–ç•Œå•†ä¸šæŠ¥å‘Šä¸­ä¸æ°”å€™ç›¸å…³çš„769ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯æ›´ä¼˜è¶Šçš„æ–¹æ³•ï¼Œè€Œç»“åˆä¸¤ç§æ–¹æ³•åªæä¾›æœ‰é™çš„ç›Šå¤„ã€‚å®šæ€§ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æç¤ºç¡®å®æ•æ‰åˆ°äº†é’ˆå¯¹æ’æ”¾ç›®æ ‡æå–ä»»åŠ¡çš„è®¸å¤šç»†èŠ‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06432v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>æ–‡æœ¬ä¸“æ³¨äºå°†ä¸“å®¶åé¦ˆä»¥æ ‡æ³¨æ®µè½çš„å½¢å¼èå…¥LLMç®¡é“çš„é—®é¢˜ï¼Œå¹¶æ¯”è¾ƒäº†ä¸¤ç§ç­–ç•¥ï¼šï¼ˆ1ï¼‰åŠ¨æ€é€‰æ‹©å°‘é‡ç¤ºä¾‹ï¼›ï¼ˆ2ï¼‰LLMæœ¬èº«çš„è‡ªåŠ¨æç¤ºä¼˜åŒ–ã€‚åœ¨åŒ…å«çœŸå®å•†ä¸šæŠ¥å‘Šä¸­å…³äºæ°”å€™ç›¸å…³çš„769ä¸ªæ®µè½çš„å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œè‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯æ›´å¥½çš„æ–¹æ³•ï¼Œè€Œç»“åˆä¸¤ç§æ–¹æ³•åˆ™åªèƒ½æä¾›æœ‰é™çš„æ”¶ç›Šã€‚å®šæ€§ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–çš„æç¤ºç¡®å®æ•æ‰åˆ°äº†é’ˆå¯¹æ’æ”¾ç›®æ ‡æå–ä»»åŠ¡çš„è®¸å¤šç»†èŠ‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>é’ˆå¯¹ä¼ä¸šæŠ¥å‘Šä¸­æ’æ”¾å‡å°‘ç›®æ ‡çš„æ£€æµ‹è¿›è¡Œç ”ç©¶ï¼Œè¿™æ˜¯ç›‘æµ‹å…¬å¸åœ¨åº”å¯¹æ°”å€™å˜åŒ–æ–¹é¢è¿›å±•çš„é‡è¦ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶èšç„¦äºå¦‚ä½•å°†ä¸“å®¶åé¦ˆä»¥æ ‡æ³¨æ®µè½çš„å½¢å¼èå…¥LLMç®¡é“çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¸¤ç§ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼šåŠ¨æ€é€‰æ‹©å°‘é‡ç¤ºä¾‹å’ŒLLMè‡ªåŠ¨æç¤ºä¼˜åŒ–ã€‚</li>
<li>åœ¨åŒ…å«çœŸå®å•†ä¸šæŠ¥å‘Šä¸­å…³äºæ°”å€™ç›¸å…³çš„æ®µè½çš„æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œå‘ç°è‡ªåŠ¨æç¤ºä¼˜åŒ–æ˜¯æ›´å¥½çš„ç­–ç•¥ã€‚</li>
<li>ç»“åˆä¸¤ç§ç­–ç•¥çš„æ•ˆæœæœ‰é™ï¼Œæš—ç¤ºåœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œç®€å•çš„è‡ªåŠ¨æç¤ºä¼˜åŒ–å·²è¶³å¤Ÿæœ‰æ•ˆã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1256a9d4a8ef68eee380970ca93b0405.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e02251f5d5339a8085451d81bcb62c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a755722e4384a3542fc3a7f635d4d6b3.jpg" align="middle">
</details>




<h2 id="GameArena-Evaluating-LLM-Reasoning-through-Live-Computer-Games"><a href="#GameArena-Evaluating-LLM-Reasoning-through-Live-Computer-Games" class="headerlink" title="GameArena: Evaluating LLM Reasoning through Live Computer Games"></a>GameArena: Evaluating LLM Reasoning through Live Computer Games</h2><p><strong>Authors:Lanxiang Hu, Qiyu Li, Anze Xie, Nan Jiang, Ion Stoica, Haojian Jin, Hao Zhang</strong></p>
<p>Evaluating the reasoning abilities of large language models (LLMs) is challenging. Existing benchmarks often depend on static datasets, which are vulnerable to data contamination and may get saturated over time, or on binary live human feedback that conflates reasoning with other abilities. As the most prominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in real-world settings, but lacks the granularity in assessing specific reasoning capabilities. We introduce GameArena, a dynamic benchmark designed to evaluate LLM reasoning capabilities through interactive gameplay with humans. GameArena consists of three games designed to test specific reasoning capabilities (e.g., deductive and inductive reasoning), while keeping participants entertained and engaged. We analyze the gaming data retrospectively to uncover the underlying reasoning processes of LLMs and measure their fine-grained reasoning capabilities. We collect over 2000 game sessions and provide detailed assessments of various reasoning capabilities for five state-of-the-art LLMs. Our user study with 100 participants suggests that GameArena improves user engagement compared to Chatbot Arena. For the first time, GameArena enables the collection of step-by-step LLM reasoning data in the wild. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸ä¾èµ–äºé™æ€æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å®¹æ˜“å—åˆ°æ•°æ®æ±¡æŸ“ï¼Œå¹¶å¯èƒ½éšæ—¶é—´è€Œè¶‹äºé¥±å’Œï¼Œæˆ–è€…ä¾èµ–äºäºŒè¿›åˆ¶çš„å®æ—¶äººç±»åé¦ˆï¼Œè¿™ä¼šå°†æ¨ç†ä¸å…¶ä»–èƒ½åŠ›æ··æ·†ã€‚ä½œä¸ºæœ€çªå‡ºçš„åŠ¨æ€åŸºå‡†æµ‹è¯•ï¼ŒChatbot Arenaè¯„ä¼°ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„å¼€æ”¾å¼é—®é¢˜ï¼Œä½†åœ¨è¯„ä¼°ç‰¹å®šæ¨ç†èƒ½åŠ›æ–¹é¢ç¼ºä¹ç²¾ç»†åº¦ã€‚æˆ‘ä»¬å¼•å…¥äº†GameArenaï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡ä¸äººç±»çš„æ¸¸æˆäº’åŠ¨æ¥è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚GameArenaç”±ä¸‰æ¬¾æ¸¸æˆç»„æˆï¼Œæ—¨åœ¨æµ‹è¯•ç‰¹å®šçš„æ¨ç†èƒ½åŠ›ï¼ˆå¦‚æ¼”ç»æ¨ç†å’Œå½’çº³æ¨ç†ï¼‰ï¼ŒåŒæ—¶ä¿æŒå‚ä¸è€…çš„å¨±ä¹æ€§å’Œå‚ä¸åº¦ã€‚æˆ‘ä»¬è¿›è¡Œå›é¡¾æ€§åˆ†æï¼Œä»¥æ­ç¤ºLLMçš„æ½œåœ¨æ¨ç†è¿‡ç¨‹ï¼Œå¹¶è¡¡é‡å…¶ç²¾ç»†çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æ”¶é›†äº†è¶…è¿‡2â€ &#x2F;&gt;æ•°åƒåœºæ¸¸æˆä¼šè¯è®°å½•ï¼Œå¹¶ä¸ºäº”ç§æœ€å‰æ²¿çš„LLMæä¾›äº†å„ç§æ¨ç†èƒ½åŠ›çš„è¯¦ç»†è¯„ä¼°æŠ¥å‘Šã€‚æˆ‘ä»¬çš„æ¶‰åŠä¸€ç™¾åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä¸Chatbot Arenaç›¸æ¯”ï¼ŒGameArenaæé«˜äº†ç”¨æˆ·å‚ä¸åº¦ã€‚GameArenaé¦–æ¬¡å®ç°äº†åœ¨é‡å¤–æ”¶é›†é€æ­¥çš„LLMæ¨ç†æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06394v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰è¯„ä¼°æ–¹æ³•ä¾èµ–äºæ˜“æ±¡æŸ“ä¸”å¯èƒ½éšæ—¶é—´é¥±å’Œçš„é™æ€æ•°æ®é›†ï¼Œæˆ–æ··æ·†æ¨ç†ä¸å…¶ä»–èƒ½åŠ›çš„äºŒå…ƒå®æ—¶äººç±»åé¦ˆã€‚GameArenaæ˜¯ä¸€ä¸ªåŠ¨æ€è¯„ä¼°å·¥å…·ï¼Œé€šè¿‡äººä¸LLMçš„äº¤äº’æ¸¸æˆæ¥è¯„ä¼°å…¶æ¨ç†èƒ½åŠ›ã€‚å®ƒåŒ…æ‹¬ä¸‰æ¬¾æ¸¸æˆï¼Œæ—¨åœ¨æµ‹è¯•ç‰¹å®šçš„æ¨ç†èƒ½åŠ›ï¼ˆå¦‚æ¼”ç»å’Œå½’çº³æ¨ç†ï¼‰ï¼ŒåŒæ—¶ä¿æŒå‚ä¸è€…çš„å¨±ä¹æ€§å’Œå‚ä¸åº¦ã€‚æˆ‘ä»¬åˆ†ææ¸¸æˆæ•°æ®ï¼Œä»¥å‘ç°LLMçš„æ¨ç†è¿‡ç¨‹å¹¶è¡¡é‡å…¶ç²¾ç»†æ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›è¯„ä¼°å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰è¯„ä¼°æ–¹æ³•å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>GameArenaæ˜¯ä¸€ä¸ªåŠ¨æ€è¯„ä¼°å·¥å…·ï¼Œé€šè¿‡äººä¸LLMçš„äº¤äº’æ¸¸æˆæ¥è¯„ä¼°å…¶æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¼”ç»å’Œå½’çº³æ¨ç†ç­‰ã€‚</li>
<li>GameArenaåŒ…å«ä¸‰æ¬¾æ—¨åœ¨æµ‹è¯•ç‰¹å®šæ¨ç†èƒ½åŠ›çš„æ¸¸æˆï¼ŒåŒæ—¶ä¿æŒç”¨æˆ·çš„å¨±ä¹æ€§å’Œå‚ä¸åº¦ã€‚</li>
<li>é€šè¿‡åˆ†ææ¸¸æˆæ•°æ®ï¼Œå¯ä»¥æ­ç¤ºLLMçš„æ¨ç†è¿‡ç¨‹å¹¶è¡¡é‡å…¶ç²¾ç»†æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GameArenaæ”¶é›†äº†è¶…è¿‡2000ä¸ªæ¸¸æˆä¼šè¯ï¼Œå¹¶å¯¹äº”ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å„é¡¹æ¨ç†èƒ½åŠ›è¿›è¡Œäº†è¯¦ç»†è¯„ä¼°ã€‚</li>
<li>ä¸Chatbot Arenaç›¸æ¯”ï¼ŒGameArenaèƒ½æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a770a519bdd904cac4e0f2e613afcc73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b40f950e65b30a27d81739b846a71d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cf3f77e54e7846fed69d31be378b787.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3df6b64a182f268758ac9e013f978d8d.jpg" align="middle">
</details>




<h2 id="Exploring-Memorization-and-Copyright-Violation-in-Frontier-LLMs-A-Study-of-the-New-York-Times-v-OpenAI-2023-Lawsuit"><a href="#Exploring-Memorization-and-Copyright-Violation-in-Frontier-LLMs-A-Study-of-the-New-York-Times-v-OpenAI-2023-Lawsuit" class="headerlink" title="Exploring Memorization and Copyright Violation in Frontier LLMs: A Study   of the New York Times v. OpenAI 2023 Lawsuit"></a>Exploring Memorization and Copyright Violation in Frontier LLMs: A Study   of the New York Times v. OpenAI 2023 Lawsuit</h2><p><strong>Authors:Joshua Freeman, Chloe Rippe, Edoardo Debenedetti, Maksym Andriushchenko</strong></p>
<p>Copyright infringement in frontier LLMs has received much attention recently due to the New York Times v. OpenAI lawsuit, filed in December 2023. The New York Times claims that GPT-4 has infringed its copyrights by reproducing articles for use in LLM training and by memorizing the inputs, thereby publicly displaying them in LLM outputs. Our work aims to measure the propensity of OpenAIâ€™s LLMs to exhibit verbatim memorization in its outputs relative to other LLMs, specifically focusing on news articles. We discover that both GPT and Claude models use refusal training and output filters to prevent verbatim output of the memorized articles. We apply a basic prompt template to bypass the refusal training and show that OpenAI models are currently less prone to memorization elicitation than models from Meta, Mistral, and Anthropic. We find that as models increase in size, especially beyond 100 billion parameters, they demonstrate significantly greater capacity for memorization. Our findings have practical implications for training: more attention must be placed on preventing verbatim memorization in very large models. Our findings also have legal significance: in assessing the relative memorization capacity of OpenAIâ€™s LLMs, we probe the strength of The New York Timesâ€™s copyright infringement claims and OpenAIâ€™s legal defenses, while underscoring issues at the intersection of generative AI, law, and policy. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç”±äºã€Šçº½çº¦æ—¶æŠ¥è¯‰OpenAIæ¡ˆã€‹çš„è¯‰è®¼ï¼Œå‰æ²¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰ˆæƒä¾µæƒé—®é¢˜å¤‡å—å…³æ³¨ã€‚è¯¥è¯‰è®¼äº2023å¹´12æœˆæèµ·ã€‚ã€Šçº½çº¦æ—¶æŠ¥ã€‹å£°ç§°GPT-4ä¾µçŠ¯äº†å…¶ç‰ˆæƒï¼Œé€šè¿‡å¤åˆ¶æ–‡ç« ç”¨äºLLMè®­ç»ƒï¼Œå¹¶è®°ä½äº†è¾“å…¥å†…å®¹ï¼Œä»è€Œåœ¨LLMè¾“å‡ºä¸­å…¬å¼€æ˜¾ç¤ºå®ƒä»¬ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¨åœ¨è¡¡é‡OpenAIçš„LLMç›¸å¯¹äºå…¶ä»–LLMï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é—»æ–‡ç« æ–¹é¢ï¼Œåœ¨å…¶è¾“å‡ºä¸­å±•ç°é€å­—è®°å¿†çš„å€¾å‘æ€§ã€‚æˆ‘ä»¬å‘ç°GPTå’ŒClaudeæ¨¡å‹éƒ½ä½¿ç”¨æ‹’ç»è®­ç»ƒå’Œè¾“å‡ºè¿‡æ»¤å™¨æ¥é˜²æ­¢å¯¹è®°å¿†æ–‡ç« çš„é€å­—è¾“å‡ºã€‚æˆ‘ä»¬åº”ç”¨äº†ä¸€ä¸ªåŸºæœ¬æç¤ºæ¨¡æ¿æ¥ç»•è¿‡æ‹’ç»è®­ç»ƒï¼Œå¹¶è¡¨æ˜ç›®å‰OpenAIæ¨¡å‹åœ¨è®°å¿†æ¿€å‘æ–¹é¢ç›¸å¯¹äºMetaã€Mistralå’ŒAnthropicçš„æ¨¡å‹ä¸é‚£ä¹ˆå®¹æ˜“å—åˆ°å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œéšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå°¤å…¶æ˜¯è¶…è¿‡100äº¿å‚æ•°åï¼Œå®ƒä»¬è¡¨ç°å‡ºæ›´å¤§çš„è®°å¿†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¯¹è®­ç»ƒå…·æœ‰å®é™…æ„ä¹‰ï¼šåœ¨éå¸¸å¤§çš„æ¨¡å‹ä¸­ï¼Œå¿…é¡»æ›´å¤šåœ°å…³æ³¨é˜²æ­¢é€å­—è®°å¿†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¹Ÿå…·æœ‰æ³•å¾‹æ„ä¹‰ï¼šåœ¨è¯„ä¼°OpenAIçš„LLMçš„ç›¸å¯¹è®°å¿†èƒ½åŠ›æ—¶ï¼Œæˆ‘ä»¬æ¢ç©¶äº†ã€Šçº½çº¦æ—¶æŠ¥ã€‹ç‰ˆæƒä¾µæƒæŒ‡æ§çš„å¼ºåº¦å’ŒOpenAIçš„æ³•å¾‹è¾©æŠ¤ï¼ŒåŒæ—¶å¼ºè°ƒäº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€æ³•å¾‹å’Œæ”¿ç­–ä¹‹é—´çš„äº¤å‰é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06370v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨çº½çº¦æ—¶æŠ¥è¯‰OpenAIç‰ˆæƒä¾µæƒæ¡ˆï¼Œç ”ç©¶OpenAIçš„LLMæ¨¡å‹ï¼ˆGPTå’ŒClaudeï¼‰åœ¨è¾“å‡ºä¸­å±•ç°åŸæ–‡è®°å¿†å€¾å‘çš„ç¨‹åº¦ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºMetaã€Mistralå’ŒAnthropicçš„æ¨¡å‹ï¼ŒOpenAIçš„æ¨¡å‹ç›®å‰æ›´ä¸å®¹æ˜“å¼•å‘è®°å¿†å”¤èµ·ã€‚ç„¶è€Œï¼Œéšç€æ¨¡å‹å‚æ•°è§„æ¨¡çš„æ‰©å¤§ï¼Œå°¤å…¶æ˜¯è¶…è¿‡100äº¿å‚æ•°åï¼Œå…¶è®°å¿†èƒ½åŠ›æ˜¾è‘—å¢å¼ºã€‚è¿™ä¸€å‘ç°å¯¹åŸ¹è®­å’Œæ³•å¾‹è¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ï¼Œæç¤ºåœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶éœ€æ›´å¤šå…³æ³¨é˜²æ­¢åŸæ–‡è®°å¿†çš„é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿåœ¨è¯„ä¼°OpenAIçš„LLMæ¨¡å‹è®°å¿†èƒ½åŠ›æ—¶ï¼Œåæ˜ äº†çº½çº¦æ—¶æŠ¥çš„ç‰ˆæƒä¾µæƒæŒ‡æ§å’ŒOpenAIçš„æ³•å¾‹é˜²å¾¡é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº½çº¦æ—¶æŠ¥å¯¹OpenAIçš„GPT-4æå‡ºç‰ˆæƒä¾µæƒæŒ‡æ§ï¼ŒæŒ‡æ§å…¶æœªç»æˆæƒä½¿ç”¨å¹¶å±•ç¤ºè¯¥æŠ¥çš„æ–‡ç« å†…å®¹ã€‚</li>
<li>ç ”ç©¶å‘ç°OpenAIçš„LLMæ¨¡å‹ï¼ˆGPTå’ŒClaudeï¼‰é‡‡ç”¨æ‹’ç»è®­ç»ƒå’Œè¾“å‡ºè¿‡æ»¤å™¨æ¥é˜²æ­¢ç›´æ¥è¾“å‡ºè®°å¿†ä¸­çš„æ–‡ç« ã€‚</li>
<li>é€šè¿‡åŸºæœ¬æç¤ºæ¨¡æ¿ç»•è¿‡æ‹’ç»è®­ç»ƒï¼Œå‘ç°ç›¸è¾ƒäºMetaã€Mistralå’ŒAnthropicçš„æ¨¡å‹ï¼ŒOpenAIçš„æ¨¡å‹ç›®å‰æ›´ä¸å®¹æ˜“å¼•å‘è®°å¿†å”¤èµ·ã€‚</li>
<li>æ¨¡å‹å‚æ•°è§„æ¨¡å¢å¤§åï¼Œå°¤å…¶æ˜¯è¶…è¿‡100äº¿å‚æ•°ï¼Œå…¶è®°å¿†èƒ½åŠ›æ˜¾è‘—å¢å¼ºã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹è®­ç»ƒå¤§å‹æ¨¡å‹å…·æœ‰å®é™…æŒ‡å¯¼æ„ä¹‰ï¼Œéœ€è¦æ›´å¤šå…³æ³¨é˜²æ­¢åŸæ–‡è®°å¿†çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹æ³•å¾‹è¯„ä¼°å…·æœ‰é‡è¦æ„ä¹‰ï¼Œåæ˜ äº†ç‰ˆæƒæ³•å’Œäººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„äº¤å‰é—®é¢˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b5f0ee19afe77f176e6be1f58cab33eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-217597176ba2ff06d54c7eff0c687ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfa7915ca7568eb0d5936e716c6426e3.jpg" align="middle">
</details>




<h2 id="S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity"><a href="#S-2-FT-Efficient-Scalable-and-Generalizable-LLM-Fine-tuning-by-Structured-Sparsity" class="headerlink" title="S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity"></a>S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by   Structured Sparsity</h2><p><strong>Authors:Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen</strong></p>
<p>Current PEFT methods for LLMs can achieve either high quality, efficient training, or scalable serving, but not all three simultaneously. To address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. Utilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S$^{2}$FT accomplishes this by â€œselecting sparsely and computing denselyâ€. It selects a few heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes weight matrices on both sides of the coupled structures in LLMs to connect the selected components in each layer into a dense submatrix. Finally, S$^{2}$FT performs in-place gradient updates on all submatrices. Through theoretical analysis and empirical results, our method prevents overfitting and forgetting, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$ compared to full FT, while delivering an average 10% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S$^{2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism for serving multiple fine-tuned models. </p>
<blockquote>
<p>å½“å‰é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„PEFTæ–¹æ³•å¯ä»¥å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒæˆ–å¯æ‰©å±•çš„æœåŠ¡ï¼Œä½†æ— æ³•åŒæ—¶å®ç°è¿™ä¸‰ä¸ªç›®æ ‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç¨€ç–å¾®è°ƒï¼Œå¹¶è§‚å¯Ÿåˆ°å…¶æ˜¾è‘—æé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨è¿™ä¸€å…³é”®è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢å‘å¤§å‹è¯­è¨€æ¨¡å‹çš„Structured Sparse Fine-Tuningï¼ˆS$^{2}$FTï¼‰æ–¹æ³•ç³»åˆ—ï¼Œè¯¥æ–¹æ³•åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§ã€‚S$^{2}$FTé€šè¿‡â€œç¨€ç–é€‰æ‹©ã€å¯†é›†è®¡ç®—â€æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å®ƒåˆ†åˆ«é€‰æ‹©MHAå’ŒFFNæ¨¡å—ä¸­æ¯ä¸ªTransformerå—çš„å°‘æ•°å¤´å’Œé€šé“ã€‚æ¥ä¸‹æ¥ï¼Œå®ƒå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è€¦åˆç»“æ„çš„ä¸¤ä¾§æƒé‡çŸ©é˜µè¿›è¡Œå…±ç½®æ¢ï¼Œä»¥å°†æ¯å±‚ä¸­é€‰æ‹©çš„ç»„ä»¶è¿æ¥æˆå¯†é›†å­çŸ©é˜µã€‚æœ€åï¼ŒS$^{2}$FTå¯¹æ‰€æœ‰å­çŸ©é˜µæ‰§è¡Œå°±åœ°æ¢¯åº¦æ›´æ–°ã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç»“æœï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜ï¼Œåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¸LoRAç›¸æ¯”å¹³å‡æé«˜äº†4.6%å’Œ1.3%ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´åè¿›è¡Œè·¨åŸŸæ³›åŒ–æ—¶æ¯”å…¨é‡å¾®è°ƒé«˜å‡º11.5%ã€‚ä½¿ç”¨æˆ‘ä»¬çš„éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTå¯èŠ‚çœé«˜è¾¾3å€çš„åŸ¹è®­å†…å­˜ï¼Œä¸å…¨é‡å¾®è°ƒç›¸æ¯”æé«˜äº†1.5-2.7å€çš„å»¶è¿Ÿï¼ŒåŒæ—¶åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šéƒ½å¹³å‡æ¯”LoRAæé«˜äº†10%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼ŒS$^{2}$FTä¸­çš„æƒé‡æ›´æ–°å¯ä»¥è¢«è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¸ºå®ç°å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„æœ‰æ•ˆèåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡ŒæœåŠ¡æä¾›äº†å¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06289v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆS$^{2}$FTï¼‰æ–¹æ³•ï¼Œè§£å†³äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¾®è°ƒæ—¶é¢ä¸´çš„é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯ä¼¸ç¼©æœåŠ¡ä¸èƒ½åŒæ—¶è¾¾æˆçš„é™åˆ¶ã€‚S$^{2}$FTé€šè¿‡â€œç¨€ç–é€‰æ‹©ï¼Œå¯†é›†è®¡ç®—â€çš„ç­–ç•¥ï¼Œå®ç°äº†åœ¨å¾®è°ƒæ€§èƒ½ã€è®­ç»ƒæ•ˆç‡å’Œæ¨ç†å¯æ‰©å±•æ€§ä¸Šçš„çªç ´ã€‚æœ¬æ–‡ç†è®ºåˆ†æå’Œå®è¯ç»“æœè¡¨æ˜ï¼ŒS$^{2}$FTå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜ï¼Œåœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢å–å¾—äº†å¹³å‡4.6%å’Œ1.3%çš„æ”¹è¿›ã€‚åŒæ—¶ï¼Œä½¿ç”¨éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTåœ¨è®­ç»ƒå†…å­˜ä¸ŠèŠ‚çœäº†é«˜è¾¾3å€ï¼Œå»¶è¿Ÿæ—¶é—´ç¼©çŸ­äº†1.5-2.7å€ï¼ŒåŒæ—¶åœ¨ä¸¤ä¸ªæŒ‡æ ‡ä¸Šå¹³å‡ä¼˜äºLoRA 10%ã€‚æ­¤å¤–ï¼ŒS$^{2}$FTçš„é‡é‡æ›´æ–°å¯ä»¥è¢«è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä¸ºæœåŠ¡å¤šä¸ªå¾®è°ƒæ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„èåˆã€å¿«é€Ÿåˆ‡æ¢å’Œé«˜æ•ˆå¹¶è¡Œæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„PEFTæ–¹æ³•ä¸èƒ½åŒæ—¶å®ç°é«˜è´¨é‡ã€é«˜æ•ˆè®­ç»ƒå’Œå¯ä¼¸ç¼©æœåŠ¡ã€‚</li>
<li>æå‡ºäº†ç»“æ„åŒ–ç¨€ç–å¾®è°ƒï¼ˆS$^{2}$FTï¼‰æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>S$^{2}$FTé€šè¿‡é€‰æ‹©MHAå’ŒFFNæ¨¡å—ä¸­çš„éƒ¨åˆ†å¤´éƒ¨å’Œé€šé“è¿›è¡Œå¾®è°ƒï¼Œå®ç°äº†çŠ¶æ€çš„ç²¾ç»†è°ƒæ•´æ€§èƒ½ã€‚</li>
<li>S$^{2}$FTé€šè¿‡ç†è®ºåˆ†æå’Œå®è¯ç»“æœè¯æ˜äº†å…¶é˜²æ­¢è¿‡æ‹Ÿåˆå’Œé—å¿˜çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨å¸¸è¯†å’Œç®—æœ¯æ¨ç†æ–¹é¢ï¼ŒS$^{2}$FTç›¸å¯¹äºLoRAå¹³å‡æ”¹è¿›äº†4.6%å’Œ1.3%ï¼Œå¹¶ä¸”åœ¨é€šç”¨é¢†åŸŸæŒ‡ä»¤è°ƒæ•´åçš„ä¸åŒé¢†åŸŸä¸Šè¶…è¶Šäº†å…¨é‡å¾®è°ƒï¼ˆFull FTï¼‰11.5%ã€‚</li>
<li>ä½¿ç”¨éƒ¨åˆ†åå‘ä¼ æ’­ç®—æ³•ï¼ŒS$^{2}$FTåœ¨è®­ç»ƒå†…å­˜ä¸Šå®ç°äº†æ˜¾è‘—çš„èŠ‚çœï¼Œå¹¶æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚</li>
<li>S$^{2}$FTçš„é‡é‡æ›´æ–°å¯ä»¥è¢«è§£è€¦ä¸ºé€‚é…å™¨ï¼Œä½¿å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„èåˆã€åˆ‡æ¢å’Œå¹¶è¡Œæ€§æ›´åŠ é«˜æ•ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-876cffd8411140fea998d654d3ee89e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93b5e879d74ec487afb8b61d72b63118.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-21063ae9675b71e79cc948b5f65b80ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5e3072eec965e7fd15b3335eaec48d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52615bada518b68dec445640650fa2f0.jpg" align="middle">
</details>




<h2 id="Methods-for-Legal-Citation-Prediction-in-the-Age-of-LLMs-An-Australian-Law-Case-Study"><a href="#Methods-for-Legal-Citation-Prediction-in-the-Age-of-LLMs-An-Australian-Law-Case-Study" class="headerlink" title="Methods for Legal Citation Prediction in the Age of LLMs: An Australian   Law Case Study"></a>Methods for Legal Citation Prediction in the Age of LLMs: An Australian   Law Case Study</h2><p><strong>Authors:Ehsan Shareghi, Jiuzhou Han, Paul Burgess</strong></p>
<p>In recent years, Large Language Models (LLMs) have shown great potential across a wide range of legal tasks. Despite these advances, mitigating hallucination remains a significant challenge, with state-of-the-art LLMs still frequently generating incorrect legal references. In this paper, we focus on the problem of legal citation prediction within the Australian law context, where correctly identifying and citing relevant legislations or precedents is critical. We compare several approaches: prompting general purpose and law-specialised LLMs, retrieval-only pipelines with both generic and domain-specific embeddings, task-specific instruction-tuning of LLMs, and hybrid strategies that combine LLMs with retrieval augmentation, query expansion, or voting ensembles. Our findings indicate that domain-specific pre-training alone is insufficient for achieving satisfactory citation accuracy even after law-specialised pre-training. In contrast, instruction tuning on our task-specific dataset dramatically boosts performance reaching the best results across all settings. We also highlight that database granularity along with the type of embeddings play a critical role in the performance of retrieval systems. Among retrieval-based approaches, hybrid methods consistently outperform retrieval-only setups, and among these, ensemble voting delivers the best result by combining the predictive quality of instruction-tuned LLMs with the retrieval system. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›çš„æ³•å¾‹ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å°½ç®¡æœ‰è¿™äº›è¿›å±•ï¼Œå‡è½»å¹»è§‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œæœ€å…ˆè¿›çš„LLMä»ç„¶ç»å¸¸äº§ç”Ÿä¸æ­£ç¡®çš„æ³•å¾‹å¼•ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨æ¾³å¤§åˆ©äºšæ³•å¾‹èƒŒæ™¯ä¸‹çš„æ³•å¾‹å¼•ç”¨é¢„æµ‹é—®é¢˜ï¼Œåœ¨é‚£é‡Œæ­£ç¡®è¯†åˆ«å’Œå¼•ç”¨ç›¸å…³çš„ç«‹æ³•æˆ–å…ˆä¾‹è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†å‡ ç§æ–¹æ³•ï¼šæç¤ºé€šç”¨å’Œæ³•å¾‹ä¸“ä¸šLLMçš„æ–¹æ³•ï¼Œä»…æ£€ç´¢ç®¡é“ä½¿ç”¨é€šç”¨å’Œé¢†åŸŸç‰¹å®šåµŒå…¥ï¼Œé’ˆå¯¹LLMè¿›è¡Œç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤è°ƒæ•´ï¼Œä»¥åŠç»“åˆLLMä¸æ£€ç´¢å¢å¼ºã€æŸ¥è¯¢æ‰©å±•æˆ–æŠ•ç¥¨é›†åˆçš„æ··åˆç­–ç•¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ¥å—æ³•å¾‹ä¸“ä¸šé¢„è®­ç»ƒä¹‹åï¼Œä»…è¿›è¡Œé¢†åŸŸç‰¹å®šçš„é¢„è®­ç»ƒä¹Ÿä¸è¶³ä»¥å®ç°ä»¤äººæ»¡æ„çš„å¼•ç”¨å‡†ç¡®æ€§ã€‚ç›¸åï¼Œåœ¨æˆ‘ä»¬ç‰¹å®šçš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ä¼šæå¤§åœ°æé«˜æ€§èƒ½ï¼Œåœ¨æ‰€æœ‰è®¾ç½®ä¸­è¾¾åˆ°æœ€ä½³ç»“æœã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒï¼Œæ•°æ®åº“ç²’åº¦ä»¥åŠåµŒå…¥çš„ç±»å‹å¯¹æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨åŸºäºæ£€ç´¢çš„æ–¹æ³•ä¸­ï¼Œæ··åˆæ–¹æ³•å§‹ç»ˆä¼˜äºä»…æ£€ç´¢è®¾ç½®ï¼Œå¹¶ä¸”åœ¨è¿™äº›æ–¹æ³•ä¹‹ä¸­ï¼Œé€šè¿‡ç»“åˆæŒ‡ä»¤è°ƒæ•´LLMçš„é¢„æµ‹è´¨é‡ä¸æ£€ç´¢ç³»ç»Ÿï¼Œé›†åˆæŠ•ç¥¨äº§ç”Ÿäº†æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06272v1">PDF</a> For code, data, and models see <a target="_blank" rel="noopener" href="https://auslawbench.github.io/">https://auslawbench.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¾³å¤§åˆ©äºšæ³•å¾‹ç¯å¢ƒä¸‹çš„æ³•å¾‹å¼•ç”¨é¢„æµ‹é—®é¢˜ã€‚æ–‡ç« æ¯”è¾ƒäº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬é€šç”¨å’Œæ³•åŸŸä¸“ä¸šåŒ–çš„LLMæç¤ºã€é€šç”¨å’Œæ³•åŸŸç‰¹å®šåµŒå…¥çš„æ£€ç´¢ç®¡é“ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„LLMæŒ‡ä»¤å¾®è°ƒä»¥åŠç»“åˆLLMsä¸æ£€ç´¢å¢å¼ºã€æŸ¥è¯¢æ‰©å±•æˆ–æŠ•ç¥¨é›†åˆçš„æ··åˆç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œä»…ä»…ä¾é æ³•åŸŸé¢„è®­ç»ƒå¹¶ä¸èƒ½è¾¾åˆ°ä»¤äººæ»¡æ„çš„å¼•ç”¨å‡†ç¡®æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé’ˆå¯¹ç‰¹å®šæ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ•°æ®åº“ç²’åº¦å’ŒåµŒå…¥ç±»å‹å¯¹æ£€ç´¢ç³»ç»Ÿçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚æ··åˆæ–¹æ³•é€šå¸¸ä¼˜äºä»…æ£€ç´¢è®¾ç½®ï¼Œå…¶ä¸­æŠ•ç¥¨é›†åˆæ–¹æ³•ç»“åˆäº†æŒ‡ä»¤è°ƒæ•´LLMsçš„é¢„æµ‹è´¨é‡å’Œæ£€ç´¢ç³»ç»Ÿï¼Œå–å¾—äº†æœ€ä½³æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šç§æ³•å¾‹ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†æŠ‘åˆ¶è™šæ„ä»æ˜¯é‡å¤§æŒ‘æˆ˜ï¼Œæœ€å…ˆè¿›çš„LLMsä»ä¼šé¢‘ç¹ç”Ÿæˆé”™è¯¯çš„æ³•å¾‹å¼•ç”¨ã€‚</li>
<li>æ–‡ç« èšç„¦äºæ¾³å¤§åˆ©äºšæ³•å¾‹ç¯å¢ƒä¸‹çš„æ³•å¾‹å¼•ç”¨é¢„æµ‹é—®é¢˜ã€‚</li>
<li>å¤šç§æ–¹æ³•è¢«æ¯”è¾ƒï¼ŒåŒ…æ‹¬é€šç”¨å’Œæ³•åŸŸä¸“ä¸šåŒ–çš„LLMæç¤ºã€æ£€ç´¢ç®¡é“ã€ä»»åŠ¡ç‰¹å®šæŒ‡ä»¤å¾®è°ƒä»¥åŠæ··åˆç­–ç•¥ã€‚</li>
<li>ä»…ä¾é æ³•åŸŸé¢„è®­ç»ƒå¹¶ä¸èƒ½è¾¾åˆ°æ»¡æ„çš„å¼•ç”¨å‡†ç¡®æ€§ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>æ•°æ®åº“ç²’åº¦å’ŒåµŒå…¥ç±»å‹å¯¹æ£€ç´¢ç³»ç»Ÿæ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f0547e75c6b7917fd53839199045f519.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a6d9baede0650ceb2d2b9d898588d74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d8e7163a7bb9a90ab75947546b3ca9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad3d8ffea164c00423f55f13b2392004.jpg" align="middle">
</details>




<h2 id="LLMs-as-Debate-Partners-Utilizing-Genetic-Algorithms-and-Adversarial-Search-for-Adaptive-Arguments"><a href="#LLMs-as-Debate-Partners-Utilizing-Genetic-Algorithms-and-Adversarial-Search-for-Adaptive-Arguments" class="headerlink" title="LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial   Search for Adaptive Arguments"></a>LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial   Search for Adaptive Arguments</h2><p><strong>Authors:Prakash Aryan</strong></p>
<p>This paper introduces DebateBrawl, an innovative AI-powered debate platform that integrates Large Language Models (LLMs), Genetic Algorithms (GA), and Adversarial Search (AS) to create an adaptive and engaging debating experience. DebateBrawl addresses the limitations of traditional LLMs in strategic planning by incorporating evolutionary optimization and game-theoretic techniques. The system demonstrates remarkable performance in generating coherent, contextually relevant arguments while adapting its strategy in real-time. Experimental results involving 23 debates show balanced outcomes between AI and human participants, with the AI system achieving an average score of 2.72 compared to the human average of 2.67 out of 10. User feedback indicates significant improvements in debating skills and a highly satisfactory learning experience, with 85% of users reporting improved debating abilities and 78% finding the AI opponent appropriately challenging. The systemâ€™s ability to maintain high factual accuracy (92% compared to 78% in human-only debates) while generating diverse arguments addresses critical concerns in AI-assisted discourse. DebateBrawl not only serves as an effective educational tool but also contributes to the broader goal of improving public discourse through AI-assisted argumentation. The paper discusses the ethical implications of AI in persuasive contexts and outlines the measures implemented to ensure responsible development and deployment of the system, including robust fact-checking mechanisms and transparency in decision-making processes. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DebateBrawlï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°çš„AIè¾©è®ºå¹³å°ï¼Œå®ƒé›†æˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€é—ä¼ ç®—æ³•ï¼ˆGAï¼‰å’Œå¯¹æŠ—æœç´¢ï¼ˆASï¼‰ï¼Œä»¥åˆ›é€ ä¸€ç§è‡ªé€‚åº”ä¸”å¼•äººå…¥èƒœçš„è¾©è®ºä½“éªŒã€‚DebateBrawlé€šè¿‡èå…¥è¿›åŒ–ä¼˜åŒ–å’Œåšå¼ˆè®ºæŠ€æœ¯ï¼Œè§£å†³äº†ä¼ ç»ŸLLMåœ¨æˆ˜ç•¥è§„åˆ’æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥ç³»ç»Ÿåœ¨ç”Ÿæˆè¿è´¯ã€è¯­å¢ƒç›¸å…³çš„è®ºç‚¹æ—¶è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶èƒ½å¤Ÿå®æ—¶è°ƒæ•´ç­–ç•¥ã€‚æ¶‰åŠ23åœºè¾©è®ºçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIå’Œäººç±»å‚ä¸è€…ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡çš„ç»“æœï¼ŒAIç³»ç»Ÿå¹³å‡å¾—åˆ†ä¸º2.72ï¼ˆæ»¡åˆ†10åˆ†ï¼‰ï¼Œä¸äººç±»å¹³å‡å¾—åˆ†2.67ç›¸å½“ã€‚ç”¨æˆ·åé¦ˆæ˜¾ç¤ºï¼Œè¾©è®ºæŠ€å·§æœ‰äº†æ˜¾è‘—æé«˜ï¼Œå­¦ä¹ ä½“éªŒé«˜åº¦æ»¡æ„ï¼Œ85%çš„ç”¨æˆ·è¡¨ç¤ºè¾©è®ºèƒ½åŠ›æœ‰æ‰€æé«˜ï¼Œ78%çš„ç”¨æˆ·è®¤ä¸ºAIå¯¹æ‰‹å…·æœ‰é€‚å½“çš„æŒ‘æˆ˜æ€§ã€‚ç³»ç»Ÿåœ¨ä¿æŒé«˜äº‹å®å‡†ç¡®æ€§ï¼ˆä¸åªæœ‰äººç±»çš„è¾©è®ºç›¸æ¯”ï¼ŒAIçš„å‡†ç¡®ç‡ä¸º92%ï¼Œè€Œäººç±»ä¸º78%ï¼‰çš„åŒæ—¶ï¼Œè¿˜èƒ½äº§ç”Ÿå¤šæ ·åŒ–çš„è®ºç‚¹ï¼Œè¿™è§£å†³äº†AIè¾…åŠ©å¯¹è¯ä¸­çš„å…³é”®æ‹…å¿§ã€‚DebateBrawlä¸ä»…æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•™è‚²å·¥å…·ï¼Œè€Œä¸”ä¸ºå®ç°æ›´å¹¿æ³›çš„ç›®æ ‡â€”â€”é€šè¿‡AIè¾…åŠ©è®ºè¯æé«˜å…¬ä¼—å¯¹è¯æ°´å¹³åšå‡ºäº†è´¡çŒ®ã€‚è®ºæ–‡è®¨è®ºäº†AIåœ¨è¯´æœæ€§è¯­å¢ƒä¸­çš„ä¼¦ç†å½±å“ï¼Œå¹¶æ¦‚è¿°äº†ä¸ºç¡®ä¿ç³»ç»Ÿè´Ÿè´£ä»»å¼€å‘å’Œéƒ¨ç½²æ‰€é‡‡å–çš„æªæ–½ï¼ŒåŒ…æ‹¬å¼ºå¤§çš„äº‹å®æ ¸æŸ¥æœºåˆ¶å’Œå†³ç­–è¿‡ç¨‹çš„é€æ˜æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06229v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¾©è®ºè¾¾äººï¼æ–°ä¸€ä»£äººå·¥æ™ºèƒ½è¾©è®ºå¹³å°DebateBrawlèåˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€é—ä¼ ç®—æ³•ï¼ˆGAï¼‰ä¸å¯¹æŠ—æ€§æœç´¢ï¼ˆASï¼‰ï¼Œçªç ´ä¼ ç»Ÿè¾©è®ºæ–¹å¼çš„å±€é™æ€§ï¼Œæ„å»ºæ²‰æµ¸å¼çš„è¾©è®ºä½“éªŒã€‚å®æ—¶é€‚åº”ç­–ç•¥ç”Ÿæˆè¿è´¯ä¸”ç›¸å…³è®ºç‚¹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºäººæœºè¾©è®ºåŒæ–¹ç»“æœå‡è¡¡ï¼Œäººå·¥æ™ºèƒ½å¹³å‡å¾—åˆ†æ¥è¿‘äººç±»ã€‚ç”¨æˆ·åé¦ˆæ˜¾ç¤ºè¾©è®ºæŠ€èƒ½æ˜¾è‘—æå‡ï¼Œå­¦ä¹ ä½“éªŒæ»¡æ„ï¼ŒæŒ‘æˆ˜åº¦é€‚ä¸­ã€‚DebateBrawlä¸ä»…æ˜¯é«˜æ•ˆçš„æ•™è‚²å·¥å…·ï¼Œè¿˜æ¨åŠ¨äººå·¥æ™ºèƒ½åŠ©åŠ›å…¬å…±è¾©è®ºçš„è¿›æ­¥ã€‚åŒæ—¶é‡è§†ä¼¦ç†ï¼Œä¿éšœé€æ˜æ€§ä¸äº‹å®æ ¸æŸ¥æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DebateBrawlæ˜¯ä¸€ä¸ªç»“åˆLLMã€GAå’ŒASçš„äººå·¥æ™ºèƒ½è¾©è®ºå¹³å°ã€‚</li>
<li>å¹³å°é€šè¿‡è¿›åŒ–ä¼˜åŒ–å’Œæ¸¸æˆç†è®ºæŠ€æœ¯è§£å†³äº†ä¼ ç»ŸLLMåœ¨æˆ˜ç•¥è§„åˆ’ä¸Šçš„å±€é™ã€‚</li>
<li>åœ¨å®æ—¶è¾©è®ºä¸­ï¼ŒDebateBrawlèƒ½ç”Ÿæˆè¿è´¯ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„è®ºç‚¹ï¼Œå¹¶è‡ªé€‚åº”è°ƒæ•´ç­–ç•¥ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒAIä¸äººç±»çš„è¾©è®ºè¡¨ç°ç›¸è¿‘ï¼Œå¹³å‡å¾—åˆ†ç›¸è¿‘ã€‚</li>
<li>ç”¨æˆ·åé¦ˆè¡¨æ˜ï¼Œä½¿ç”¨DebateBrawlåï¼Œå¤§éƒ¨åˆ†ç”¨æˆ·çš„è¾©è®ºæŠ€èƒ½æœ‰æ‰€æå‡ï¼Œå¹¶å¯¹æŒ‘æˆ˜åº¦è¡¨ç¤ºæ»¡æ„ã€‚</li>
<li>å¹³å°ä¿æŒé«˜äº‹å®å‡†ç¡®æ€§ï¼ŒåŒæ—¶ç”Ÿæˆå¤šæ ·åŒ–çš„è®ºç‚¹ã€‚</li>
<li>DebateBrawlé‡è§†ä¼¦ç†åº”ç”¨ï¼Œå…·å¤‡é€æ˜çš„å†³ç­–è¿‡ç¨‹å’Œäº‹å®æ ¸æŸ¥æœºåˆ¶ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02ae57abf169c5702ab5bc7df5e996fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3cc15367ed85c3894d52d8008c993a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1783ed3fdbd14f332f8803566cdc2dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73c547dc03c8d561a90f487e41069ad9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e05b66ca0ba2bbff64342c4ae008160f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c0b39dc11ff8ec83747ea1e1231f2b5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34b095366fd2e45fb40e9fa4602f5d8e.jpg" align="middle">
</details>




<h2 id="APOLLO-SGD-like-Memory-AdamW-level-Performance"><a href="#APOLLO-SGD-like-Memory-AdamW-level-Performance" class="headerlink" title="APOLLO: SGD-like Memory, AdamW-level Performance"></a>APOLLO: SGD-like Memory, AdamW-level Performance</h2><p><strong>Authors:Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z. Pan, Zhangyang Wang, Jinwon Lee</strong></p>
<p>Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance.   In this work, we identify that AdamWâ€™s learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs.   Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å‡ºäº†åçš„å†…å­˜å¯†é›†å‹ï¼Œç‰¹åˆ«æ˜¯ä¸æµè¡Œçš„AdamWä¼˜åŒ–å™¨ç»“åˆä½¿ç”¨æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚è¿™ç§å†…å­˜è´Ÿæ‹…éœ€è¦ä½¿ç”¨æ›´å¤šæˆ–æ›´é«˜ç«¯çš„GPUæˆ–å‡å°‘æ‰¹æ¬¡å¤§å°ï¼Œä»è€Œé™åˆ¶äº†è®­ç»ƒçš„æ‰©å±•æ€§å’Œååé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå·²ç»æå‡ºäº†å„ç§å†…å­˜é«˜æ•ˆçš„ä¼˜åŒ–å™¨æ¥å‡å°‘ä¼˜åŒ–å™¨çš„å†…å­˜ä½¿ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬é¢ä¸´ç€å…³é”®çš„æŒ‘æˆ˜ï¼šï¼ˆiï¼‰ä¾èµ–äºæ˜‚è´µçš„SVDæ“ä½œï¼›ï¼ˆiiï¼‰ä¸AdamWç›¸æ¯”ï¼Œæ€§èƒ½å­˜åœ¨é‡å¤§æƒè¡¡ï¼›ï¼ˆiiiï¼‰ä¸ºäº†ä¿æŒç«äº‰åŠ›ï¼Œä¼˜åŒ–å™¨ä»ç„¶å­˜åœ¨ç›¸å½“å¤§çš„å†…å­˜å¼€é”€ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°AdamWçš„å­¦ä¹ ç‡è‡ªé€‚åº”è§„åˆ™å¯ä»¥è¢«æœ‰æ•ˆåœ°ç²—åŒ–ä¸ºç»“æ„åŒ–å­¦ä¹ ç‡æ›´æ–°ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçº¯éšæœºæŠ•å½±çš„è¾…åŠ©ä½ç§©ä¼˜åŒ–å™¨çŠ¶æ€çš„è¿‘ä¼¼æ¢¯åº¦ç¼©æ”¾å†…å­˜é«˜æ•ˆLLMä¼˜åŒ–æ–¹æ³•ï¼ˆAPOLLOï¼‰ã€‚è¿™ç§ç»“æ„åŒ–å­¦ä¹ ç‡æ›´æ–°è§„åˆ™ä½¿å¾—APOLLOå¯¹è¿›ä¸€æ­¥çš„å†…å­˜å‡å°‘å…·æœ‰å¾ˆé«˜çš„å®¹å¿åº¦ï¼ŒåŒæ—¶æä¾›å¯ä¸AdamWç›¸å½“çš„é¢„è®­ç»ƒæ€§èƒ½ã€‚ç”šè‡³å…¶ç§©ä¸º1çš„å˜ä½“APOLLO-Miniä¹Ÿå®ç°äº†ä¼˜äºAdamWçš„é¢„è®­ç»ƒæ€§èƒ½ï¼ŒåŒæ—¶è¾¾åˆ°äº†SGDçº§åˆ«çš„å†…å­˜æˆæœ¬ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAPOLLOç³»åˆ—åœ¨æ€§èƒ½ä¸Šå¯ä»¥ä¸AdamWç›¸æå¹¶è®ºæˆ–æ›´å¥½ï¼ŒåŒæ—¶é€šè¿‡å‡ ä¹æ¶ˆé™¤AdamWçš„ä¼˜åŒ–çŠ¶æ€å®ç°äº†æ›´å¤§çš„å†…å­˜èŠ‚çœã€‚è¿™äº›èŠ‚çœæä¾›äº†é‡å¤§çš„ç³»ç»Ÿçº§å¥½å¤„ï¼šï¼ˆ1ï¼‰å¢å¼ºååé‡ï¼šåœ¨8xA100-80GBè®¾ç½®ä¸Šä¸AdamWç›¸æ¯”ï¼Œé€šè¿‡æ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼Œæé«˜äº†3å€çš„ååé‡ã€‚ï¼ˆ2ï¼‰æ”¹å–„æ¨¡å‹å¯æ‰©å±•æ€§ï¼šåœ¨æ— ç³»ç»Ÿçº§ä¼˜åŒ–çš„A100-80GB GPUä¸Šï¼Œä½¿ç”¨æœ´ç´ DDPå¯¹LLaMA-13Bè¿›è¡Œé¢„è®­ç»ƒã€‚ï¼ˆ3ï¼‰ä½ç«¯GPUå‹å¥½çš„é¢„è®­ç»ƒï¼šåœ¨å•ä¸ªGPUä¸Šä½¿ç”¨ä¸åˆ°12GBçš„å†…å­˜è¿›è¡Œæƒé‡é‡åŒ–ï¼Œå¯¹LLaMA-7Bè¿›è¡Œé¢„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05270v2">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜éœ€æ±‚å·¨å¤§ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨æµè¡Œçš„AdamWä¼˜åŒ–å™¨ã€‚è¿™å¯¼è‡´å¿…é¡»ä½¿ç”¨æ›´å¤šæˆ–æ›´é«˜ç«¯çš„GPUæˆ–å‡å°æ‰¹æ¬¡å¤§å°ï¼Œé™åˆ¶äº†è®­ç»ƒçš„å¯æ‰©å±•æ€§å’Œååé‡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†å„ç§å†…å­˜é«˜æ•ˆçš„ä¼˜åŒ–å™¨ä»¥å‡å°‘ä¼˜åŒ–å™¨çš„å†…å­˜ä½¿ç”¨ã€‚ä½†å®ƒä»¬é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä¾èµ–æ˜‚è´µçš„SVDæ“ä½œã€ä¸AdamWç›¸æ¯”æ€§èƒ½æ˜¾è‘—ä¸‹é™ä»¥åŠä»éœ€è¦è¾ƒå¤§çš„ä¼˜åŒ–å™¨å†…å­˜æ¥ä¿æŒç«äº‰åŠ›ã€‚æœ¬æ–‡å‘ç°AdamWçš„å­¦ä¹ ç‡é€‚åº”è§„åˆ™å¯ä»¥è¢«æœ‰æ•ˆåœ°ç²—ç•¥åŒ–ä¸ºç»“æ„åŒ–å­¦ä¹ ç‡æ›´æ–°ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåŸºäºè¾…åŠ©ä½ç§©ä¼˜åŒ–å™¨çŠ¶æ€çš„è¿‘ä¼¼æ¢¯åº¦ç¼©æ”¾ç”¨äºå†…å­˜é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–ï¼ˆAPOLLOï¼‰ï¼Œè¯¥è§„åˆ™ä½¿APOLLOåœ¨è¿›ä¸€æ­¥å‡å°‘å†…å­˜çš„åŒæ—¶ä»èƒ½ä¿æŒç›¸å½“çš„é¢„è®­ç»ƒæ€§èƒ½ã€‚å…¶ç§©ä¸º1çš„å˜ä½“APOLLO-Miniç”šè‡³è¾¾åˆ°äº†ä¸AdamWç›¸æ¯”çš„ä¼˜è¶Šé¢„è®­ç»ƒæ€§èƒ½ï¼ŒåŒæ—¶æ‹¥æœ‰SGDçº§åˆ«çš„å†…å­˜æˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒAPOLLOç³»åˆ—åœ¨æ€§èƒ½ä¸Šå¯ä¸AdamWæŒå¹³æˆ–æ›´å¥½ï¼ŒåŒæ—¶å®ç°äº†æ›´å¤§çš„å†…å­˜èŠ‚çœã€‚è¿™äº›èŠ‚çœæä¾›äº†æ˜¾è‘—çš„ç³»ç»Ÿçº§å¥½å¤„ï¼šå¢å¼ºååé‡ã€æé«˜æ¨¡å‹å¯æ‰©å±•æ€§å’Œä½ç«¯GPUå‹å¥½çš„é¢„è®­ç»ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMè®­ç»ƒè¿‡ç¨‹ä¸­å¯¹å†…å­˜çš„éœ€æ±‚å¾ˆå¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨AdamWä¼˜åŒ–å™¨æ—¶ã€‚</li>
<li>å½“å‰æå‡ºçš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨å­˜åœ¨ä¾èµ–æ˜‚è´µæ“ä½œã€æ€§èƒ½ä¸‹é™å’Œä»éœ€å¤§é‡å†…å­˜çš„é—®é¢˜ã€‚</li>
<li>å‘ç°å¹¶åº”ç”¨äº†AdamWå­¦ä¹ ç‡é€‚åº”è§„åˆ™çš„ç»“æ„åŒ–æ›´æ–°æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–å™¨APOLLOï¼Œå®ƒåŸºäºè¾…åŠ©ä½ç§©ä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œè¿‘ä¼¼æ¢¯åº¦ç¼©æ”¾ã€‚</li>
<li>APOLLOèƒ½åœ¨å‡å°‘å†…å­˜çš„åŒæ—¶ä¿æŒæˆ–æé«˜é¢„è®­ç»ƒæ€§èƒ½ã€‚</li>
<li>APOLLOç³»åˆ—åœ¨å†…å­˜èŠ‚çœæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ç³»ç»Ÿçº§ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å¢å¼ºååé‡ã€æé«˜æ¨¡å‹å¯æ‰©å±•æ€§å’Œæ”¯æŒä½ç«¯GPUé¢„è®­ç»ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ebdb88b2eb6f5b6aed31c21a869fcbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4753a3a86ec054f7f63a55dea895b926.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d02402b365fb2ea8c3d4e3938a8a570b.jpg" align="middle">
</details>




<h2 id="RARE-Retrieval-Augmented-Reasoning-Enhancement-for-Large-Language-Models"><a href="#RARE-Retrieval-Augmented-Reasoning-Enhancement-for-Large-Language-Models" class="headerlink" title="RARE: Retrieval-Augmented Reasoning Enhancement for Large Language   Models"></a>RARE: Retrieval-Augmented Reasoning Enhancement for Large Language   Models</h2><p><strong>Authors:Hieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang, Zhichao Yang, Hong Yu</strong></p>
<p>This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for complex, knowledge-intensive tasks such as commonsense and medical reasoning. RARE incorporates two innovative actions within the Monte Carlo Tree Search (MCTS) framework: A6, which generates search queries based on the initial problem statement, performs information retrieval using those queries, and augments reasoning with the retrieved data to formulate the final answer; and A7, which leverages information retrieval specifically for generated sub-questions and re-answers these sub-questions with the relevant contextual information. Additionally, a Retrieval-Augmented Factuality Scorer is proposed to replace the original discriminator, prioritizing reasoning paths that meet high standards of factuality. Experimental results with LLaMA 3.1 show that RARE enables open-source LLMs to achieve competitive performance with top open-source models like GPT-4 and GPT-4o. This research establishes RARE as a scalable solution for improving LLMs in domains where logical coherence and factual integrity are critical. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†RAREï¼ˆæ£€ç´¢å¢å¼ºæ¨ç†æ‰©å±•ï¼‰ï¼Œè¿™æ˜¯é’ˆå¯¹äº’æ¨ç†æ¡†æ¶ï¼ˆrStarï¼‰çš„ä¸€ä¸ªå¤šåŠŸèƒ½æ‰©å±•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¸¸è¯†å’ŒåŒ»å­¦æ¨ç†ç­‰å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„æ¨ç†å‡†ç¡®æ€§å’Œäº‹å®å®Œæ•´æ€§ã€‚RAREåœ¨è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶ä¸­èå…¥äº†ä¸¤ç§åˆ›æ–°è¡ŒåŠ¨ï¼šA6ï¼Œæ ¹æ®åˆå§‹é—®é¢˜é™ˆè¿°ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œä½¿ç”¨è¿™äº›æŸ¥è¯¢æ‰§è¡Œä¿¡æ¯æ£€ç´¢ï¼Œå¹¶ä½¿ç”¨æ£€ç´¢åˆ°çš„æ•°æ®å¢å¼ºæ¨ç†ä»¥å½¢æˆæœ€ç»ˆç­”æ¡ˆï¼›A7ï¼Œåˆ©ç”¨ä¿¡æ¯æ£€ç´¢ä¸“é—¨ç”Ÿæˆå­é—®é¢˜å¹¶é‡æ–°å›ç­”è¿™äº›å­é—®é¢˜ï¼ŒåŒæ—¶ç»“åˆç›¸å…³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºäº‹å®è¯„åˆ†å™¨æ¥æ›¿ä»£åŸå§‹é‰´åˆ«å™¨ï¼Œä¼˜å…ˆè€ƒè™‘ç¬¦åˆé«˜äº‹å®æ ‡å‡†çš„æ¨ç†è·¯å¾„ã€‚ä½¿ç”¨LLaMA 3.1çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRAREä½¿å¼€æºLLMèƒ½å¤Ÿå®ç°ä¸GPT-4å’ŒGPT-4oç­‰é¡¶å°–å¼€æºæ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚è¿™é¡¹ç ”ç©¶å°†RAREç¡®ç«‹ä¸ºä¸€ç§å¯åœ¨å…³é”®é¢†åŸŸæ”¹è¿›LLMçš„å¯æ‰©å±•è§£å†³æ–¹æ¡ˆï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œé€»è¾‘è¿è´¯æ€§å’Œäº‹å®å®Œæ•´æ€§è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02830v3">PDF</a> 24 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RAREï¼ˆæ£€ç´¢å¢å¼ºæ¨ç†å¢å¼ºï¼‰æŠ€æœ¯ï¼Œå®ƒæ˜¯é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€šç”¨æ‰©å±•ï¼Œæ—¨åœ¨æé«˜å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„æ¨ç†å‡†ç¡®æ€§å’Œäº‹å®å®Œæ•´æ€§ï¼Œå¦‚å¸¸è¯†å’ŒåŒ»å­¦æ¨ç†ã€‚RAREç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶ä¸­çš„ä¸¤ä¸ªåˆ›æ–°åŠ¨ä½œï¼šA6æ—¨åœ¨åŸºäºåˆå§‹é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢å¹¶æ‰§è¡Œä¿¡æ¯æ£€ç´¢ï¼Œç„¶åä½¿ç”¨æ£€ç´¢åˆ°çš„æ•°æ®å¢å¼ºæ¨ç†ä»¥å½¢æˆæœ€ç»ˆç­”æ¡ˆï¼›A7åˆ™ä¸“é—¨åˆ©ç”¨ä¿¡æ¯æ£€ç´¢ç”Ÿæˆå­é—®é¢˜å¹¶é‡æ–°å›ç­”è¿™äº›å­é—®é¢˜ä»¥è·å–ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæå‡ºäº†åŸºäºæ£€ç´¢çš„äº‹å®æ€§è¯„åˆ†å™¨æ¥æ›¿æ¢åŸå§‹é‰´åˆ«å™¨ï¼Œä»¥ä¼˜å…ˆé€‰å–ç¬¦åˆé«˜äº‹å®æ ‡å‡†çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAREæŠ€æœ¯ä½¿å¼€æºLLMçš„æ€§èƒ½è¾¾åˆ°ä¸GPT-4ç­‰é¡¶å°–æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚è¯¥ç ”ç©¶ç¡®ç«‹äº†RAREæŠ€æœ¯åœ¨é€»è¾‘è¿è´¯æ€§å’Œäº‹å®å®Œæ•´æ€§è‡³å…³é‡è¦çš„é¢†åŸŸæ”¹è¿›LLMçš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAREæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€šç”¨æ‰©å±•ï¼Œæ—¨åœ¨æé«˜å¤æ‚çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„æ¨ç†å‡†ç¡®æ€§å’Œäº‹å®å®Œæ•´æ€§ã€‚</li>
<li>RAREç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ¡†æ¶ä¸­çš„ä¸¤ä¸ªåˆ›æ–°åŠ¨ä½œï¼šA6å’ŒA7ï¼Œåˆ†åˆ«ç”¨äºç”Ÿæˆæœç´¢æŸ¥è¯¢å¹¶å¢å¼ºæ¨ç†ï¼Œä»¥åŠä¸“é—¨åˆ©ç”¨ä¿¡æ¯æ£€ç´¢å¤„ç†å­é—®é¢˜ã€‚</li>
<li>RAREå¼•å…¥äº†åŸºäºæ£€ç´¢çš„äº‹å®æ€§è¯„åˆ†å™¨ï¼Œä»¥è¯„ä¼°æ¨ç†è·¯å¾„çš„äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒRAREæŠ€æœ¯æ˜¾è‘—æé«˜äº†å¼€æºLLMçš„æ€§èƒ½ã€‚</li>
<li>RAREæŠ€æœ¯ä½¿LLMåœ¨é€»è¾‘è¿è´¯æ€§å’Œäº‹å®å®Œæ•´æ€§æ–¹é¢è¾¾åˆ°é¡¶å°–æ°´å¹³ã€‚</li>
<li>RAREæŠ€æœ¯çš„åº”ç”¨èŒƒå›´å¹¿æ³›ï¼Œå¯åº”ç”¨äºå¤šç§å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹çš„ä»»åŠ¡ï¼Œå¦‚å¸¸è¯†å’ŒåŒ»å­¦æ¨ç†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31ff625cb15aaf8044b825d65f420a9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a4d4fd7ab73e71a099981f430253149.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b41ceea6dd03d9c3d61cbb7fce091a6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1888b0aba03aab9b0f84a169148d8a3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78aee16d33c8ecf0d3a1d6604ce2bebd.jpg" align="middle">
</details>




<h2 id="FullStack-Bench-Evaluating-LLMs-as-Full-Stack-Coders"><a href="#FullStack-Bench-Evaluating-LLMs-as-Full-Stack-Coders" class="headerlink" title="FullStack Bench: Evaluating LLMs as Full Stack Coders"></a>FullStack Bench: Evaluating LLMs as Full Stack Coders</h2><p><strong>Authors:Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Jing Mai, Ge Zhang, Wenhao Huang, Kai Shen, Liang Xiang</strong></p>
<p>As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion. </p>
<blockquote>
<p>éšç€ä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ä¸æ–­æ‰©å±•ï¼Œå®ƒä»¬åœ¨å„ç§ä»£ç æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨ä¹Ÿåœ¨è¿…é€Ÿå¢åŠ ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ•°æ®é›†ä»…è¯„ä¼°æœ‰é™çš„åº”ç”¨é¢†åŸŸã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„ä»£ç è¯„ä¼°æ•°æ®é›†FullStack Benchï¼Œä¸“æ³¨äºå…¨æ ˆç¼–ç¨‹ï¼Œæ¶µç›–å¹¿æ³›çš„åº”ç”¨é¢†åŸŸï¼ˆå¦‚åŸºæœ¬ç¼–ç¨‹ã€æ•°æ®åˆ†æã€è½¯ä»¶å·¥ç¨‹ã€æ•°å­¦å’Œæœºå™¨å­¦ä¹ ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨FullStack Benchä¸­è®¾è®¡äº†æ¥è‡ª16ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€çš„ç°å®æŒ‡ä»¤å’Œç›¸åº”çš„å•å…ƒæµ‹è¯•æ¡ˆä¾‹ï¼Œä»¥åæ˜ ç°å®ä½¿ç”¨åœºæ™¯ï¼Œè€Œéç®€å•çš„ç¿»è¯‘ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªæœ‰æ•ˆçš„ä»£ç æ²™ç›’æ‰§è¡Œå·¥å…·ï¼ˆå³SandboxFusionï¼‰ï¼Œæ”¯æŒå„ç§ç¼–ç¨‹è¯­è¨€å’Œè½¯ä»¶åŒ…ï¼Œä»¥é«˜æ•ˆè¯„ä¼°æˆ‘ä»¬çš„FullStack Benchçš„æ€§èƒ½ã€‚åœ¨æˆ‘ä»¬FullStack Benchä¸Šçš„ç»¼åˆå®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬FullStack Benchå’ŒSandboxFusionçš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00535v3">PDF</a> 26 pages</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ä¸æ–­æ‰©å±•ï¼Œå…¶åœ¨å¤šç§ä»£ç æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ•°æ®é›†ä»…è¯„ä¼°æœ‰é™çš„åº”ç”¨é¢†åŸŸã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„ä»£ç è¯„ä¼°æ•°æ®é›†FullStack Benchï¼Œä¸“æ³¨äºå…¨æ ˆç¼–ç¨‹ï¼Œæ¶µç›–å¹¿æ³›çš„åº”ç”¨é¢†åŸŸï¼ˆå¦‚åŸºç¡€ç¼–ç¨‹ã€æ•°æ®åˆ†æã€è½¯ä»¶å·¥ç¨‹ã€æ•°å­¦å’Œæœºå™¨å­¦ä¹ ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›ï¼ŒFullStack BenchåŒ…å«æ¥è‡ª16ç§æµè¡Œç¼–ç¨‹è¯­è¨€çš„çœŸå®æŒ‡ä»¤å’Œç›¸åº”çš„å•å…ƒæµ‹è¯•æ¡ˆä¾‹ï¼Œä»¥åæ˜ çœŸå®ä½¿ç”¨åœºæ™¯è€Œéç®€å•ç¿»è¯‘ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªæœ‰æ•ˆçš„ä»£ç æ²™ç›’æ‰§è¡Œå·¥å…·SandboxFusionï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’ŒåŒ…ï¼Œä»¥æé«˜FullStack Benchçš„è¯„ä¼°æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»£ç æ™ºèƒ½é¢†åŸŸçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿå¢åŠ ï¼Œä½†ç°æœ‰æ•°æ®é›†ä»…è¯„ä¼°æœ‰é™çš„åº”ç”¨é¢†åŸŸã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„ä»£ç è¯„ä¼°æ•°æ®é›†FullStack Benchï¼Œæ¶µç›–å…¨æ ˆç¼–ç¨‹å’Œå¹¿æ³›çš„åº”ç”¨é¢†åŸŸã€‚</li>
<li>FullStack BenchåŒ…æ‹¬æ¥è‡ª16ç§æµè¡Œç¼–ç¨‹è¯­è¨€çš„çœŸå®æŒ‡ä»¤å’Œç›¸åº”çš„å•å…ƒæµ‹è¯•æ¡ˆä¾‹ï¼Œä»¥åæ˜ çœŸå®ä½¿ç”¨åœºæ™¯ã€‚</li>
<li>SandboxFusionæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ä»£ç æ²™ç›’æ‰§è¡Œå·¥å…·ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’ŒåŒ…ï¼Œæé«˜äº†FullStack Benchçš„è¯„ä¼°æ•ˆç‡ã€‚</li>
<li>FullStack Benchçš„è®¾è®¡æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å¤šè¯­è¨€ç¼–ç¨‹æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å…¨é¢çš„å®éªŒï¼Œè¯æ˜äº†FullStack Benchå’ŒSandboxFusionçš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>FullStack Benchçš„æ¨å‡ºå¯¹äºè¯„ä¼°LLMåœ¨å®é™…åº”ç”¨ä¸­çš„æ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-af01de07f27707906fbb94c4525b5dc8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7ef81e8fb4b6213168d2c961c0e5594.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ab7ae5ebaef61717a3dfa0ba1bba576.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca64fcec4d03f9af9d2c76a107968a9e.jpg" align="middle">
</details>




<h2 id="Puzzle-Distillation-Based-NAS-for-Inference-Optimized-LLMs"><a href="#Puzzle-Distillation-Based-NAS-for-Inference-Optimized-LLMs" class="headerlink" title="Puzzle: Distillation-Based NAS for Inference-Optimized LLMs"></a>Puzzle: Distillation-Based NAS for Inference-Optimized LLMs</h2><p><strong>Authors:Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, Ran El-Yaniv</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original modelâ€™s capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºæƒŠäººçš„èƒ½åŠ›ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†å…¶åº”ç”¨ã€‚è™½ç„¶å¢åŠ å‚æ•°æ•°é‡å¯ä»¥æé«˜å‡†ç¡®æ€§ï¼Œä½†ä¹Ÿæ‹‰å¤§äº†æœ€å‰æ²¿èƒ½åŠ›ä¸å®é™…éƒ¨ç½²èƒ½åŠ›ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†Puzzleæ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿç‰¹å®šç¡¬ä»¶ä¸Šçš„LLMæ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒå…¶èƒ½åŠ›ã€‚é€šè¿‡ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰çš„åˆ›æ–°åº”ç”¨ï¼ŒPuzzleåœ¨ç¡¬ä»¶çº¦æŸä¸‹ç³»ç»Ÿåœ°ä¼˜åŒ–äº†å…·æœ‰æ•°åäº¿å‚æ•°çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å—å±€éƒ¨çŸ¥è¯†è’¸é¦ï¼ˆBLDï¼‰è¿›è¡Œå¹¶è¡Œæ¶æ„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨æ··åˆæ•´æ•°ç¼–ç¨‹è¿›è¡Œç²¾ç¡®çº¦æŸä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡Llama-3.1-Nemotron-51B-Instructï¼ˆNemotron-51Bï¼‰å±•ç¤ºäº†æ¡†æ¶åœ¨ç°å®ä¸–ç•Œä¸­çš„å½±å“ï¼Œè¿™æ˜¯ä¸€ä¸ªæºäºLlama-3.1-70B-Instructçš„å…¬å¼€æ¨¡å‹ã€‚Nemotron-51Båœ¨å•ä¸ªNVIDIA H100 GPUä¸Šå®ç°äº†2.17å€çš„æ¨ç†ååé‡åŠ é€Ÿï¼ŒåŒæ—¶ä¿ç•™äº†åŸå§‹æ¨¡å‹98.4%çš„èƒ½åŠ›ã€‚Nemotron-51Bç›®å‰å·²æˆä¸ºèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šè¿›è¡Œå¤§è§„æ¨¡æ‰¹å¤„ç†çš„æœ€å‡†ç¡®çš„è¯­è¨€æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç§è½¬æ¢ä»…éœ€è¦45Bçš„è®­ç»ƒä»¤ç‰Œï¼Œè€Œä¸ä¹‹ç›¸å…³çš„70Bæ¨¡å‹åˆ™éœ€è¦è¶…è¿‡15Tçš„ä»¤ç‰Œã€‚è¿™å»ºç«‹äº†ä¸€ç§æ–°èŒƒå¼ï¼Œå…¶ä¸­å¼ºå¤§çš„æ¨¡å‹å¯ä»¥é’ˆå¯¹é«˜æ•ˆéƒ¨ç½²è¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶å‡ ä¹ä¸ä¼šç‰ºç‰²å…¶èƒ½åŠ›ï¼Œè¡¨æ˜åº”è¯¥æ ¹æ®æ¨ç†æ€§èƒ½ï¼ˆè€Œä¸ä»…ä»…æ˜¯å‚æ•°æ•°é‡ï¼‰æ¥é€‰æ‹©æ¨¡å‹ã€‚éšç€Nemotron-51Bçš„å‘å¸ƒå’ŒPuzzleæ¡†æ¶çš„å±•ç¤ºï¼Œæˆ‘ä»¬ä¸ºå®è·µè€…æä¾›äº†ä»¥æ˜¾è‘—é™ä½çš„è®¡ç®—æˆæœ¬ç«‹å³è®¿é—®æœ€æ–°è¯­è¨€å»ºæ¨¡åŠŸèƒ½çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19146v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠ é€Ÿæ¨ç†æ¡†æ¶Puzzleï¼Œå®ƒå¯ä»¥åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šä¼˜åŒ–æ¨¡å‹æ¨ç†ï¼ŒåŒæ—¶ä¿æŒå…¶åŸæœ‰åŠŸèƒ½ã€‚é€šè¿‡ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯ï¼ŒPuzzleèƒ½åœ¨ç¡¬ä»¶é™åˆ¶ä¸‹ä¼˜åŒ–æ•°åäº¿å‚æ•°çš„æ¨¡å‹ã€‚é€šè¿‡å—å¼å±€éƒ¨çŸ¥è¯†è’¸é¦ï¼ˆBLDï¼‰å’Œæ··åˆæ•´æ•°ç¼–ç¨‹æŠ€æœ¯ï¼ŒPuzzleæ¡†æ¶æå‡äº†æ¨¡å‹æ¨ç†é€Ÿåº¦å¹¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚ä»¥Nemotron-51Bä¸ºä¾‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒ98.4%åŸå§‹åŠŸèƒ½çš„åŒæ—¶ï¼Œå®ç°äº†å•ä¸ªNVIDIA H100 GPUä¸Šçš„æ¨ç†é€Ÿåº¦æå‡2.17å€ã€‚è¿™è¡¨æ˜åœ¨ä¼˜åŒ–éƒ¨ç½²æ—¶ï¼Œæ¨ç†æ€§èƒ½æ¯”å‚æ•°æ•°é‡æ›´é‡è¦ã€‚Puzzleæ¡†æ¶ä¸ºä»ä¸šè€…æä¾›äº†åœ¨æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œç«‹å³è®¿é—®æœ€æ–°è¯­è¨€å»ºæ¨¡åŠŸèƒ½çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Puzzleæ¡†æ¶æ—¨åœ¨åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šçš„æ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒå…¶åŸæœ‰åŠŸèƒ½ã€‚</li>
<li>é€šè¿‡ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯ï¼ŒPuzzleèƒ½å¤Ÿä¼˜åŒ–æ•°åäº¿å‚æ•°çš„æ¨¡å‹ã€‚</li>
<li>å—å¼å±€éƒ¨çŸ¥è¯†è’¸é¦ï¼ˆBLDï¼‰å’Œæ··åˆæ•´æ•°ç¼–ç¨‹æŠ€æœ¯è¢«ç”¨äºæå‡æ¨¡å‹æ¨ç†é€Ÿåº¦å’Œé™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>Nemotron-51Bæ¨¡å‹å±•ç¤ºäº†Puzzleæ¡†æ¶çš„å®é™…æ•ˆæœï¼Œå®ƒåœ¨ä¿æŒåŸå§‹åŠŸèƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æ¨ç†é€Ÿåº¦çš„æå‡ã€‚</li>
<li>ä¸ä¹‹å‰çš„æ¨¡å‹ç›¸æ¯”ï¼ŒNemotron-51Bçš„ä¼˜åŒ–è¿‡ç¨‹ä½¿ç”¨äº†æ˜¾è‘—æ›´å°‘çš„è®­ç»ƒä»¤ç‰Œã€‚</li>
<li>Puzzleæ¡†æ¶æä¾›äº†ä¸€ä¸ªæ–°çš„æ¨¡å¼ï¼Œå³å¼ºå¤§çš„æ¨¡å‹å¯ä»¥åœ¨ä¼˜åŒ–éƒ¨ç½²æ—¶ä»…å¯¹èƒ½åŠ›è¿›è¡Œå¾®å°å¦¥åã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e48b611026588994f2d50a40b3c75010.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-272dfbe515ea0c5bd8b33a386aae8c08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28b1b28d9b43315c027046cc726fc5d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a567bd3b8eb3423ecaf0638c2d553c0.jpg" align="middle">
</details>




<h2 id="Trustful-LLMs-Customizing-and-Grounding-Text-Generation-with-Knowledge-Bases-and-Dual-Decoders"><a href="#Trustful-LLMs-Customizing-and-Grounding-Text-Generation-with-Knowledge-Bases-and-Dual-Decoders" class="headerlink" title="Trustful LLMs: Customizing and Grounding Text Generation with Knowledge   Bases and Dual Decoders"></a>Trustful LLMs: Customizing and Grounding Text Generation with Knowledge   Bases and Dual Decoders</h2><p><strong>Authors:Xiaofeng Zhu, Jaya Krishna Mandivarapu</strong></p>
<p>Although people are impressed by the content generation skills of large language models, the use of LLMs, such as ChatGPT, is limited by the domain grounding of the content. The correctness and groundedness of the generated content need to be based on a verified context, such as results from Retrieval-Augmented Generation (RAG). One important issue when adapting LLMs to a customized domain is that the generated responses are often incomplete, or the additions are not verified and may even be hallucinated. Prior studies on hallucination detection have focused on evaluation metrics, which are not easily adaptable to dynamic domains and can be vulnerable to attacks like jail-breaking. In this work, we propose 1) a post-processing algorithm that leverages knowledge triplets in RAG context to correct hallucinations and 2) a dual-decoder model that fuses RAG context to guide the generation process. </p>
<blockquote>
<p>å°½ç®¡äººä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å®¹ç”Ÿæˆèƒ½åŠ›å°è±¡æ·±åˆ»ï¼Œä½†ChatGPTç­‰LLMçš„ä½¿ç”¨å—åˆ°å†…å®¹é¢†åŸŸç‰¹å®šæ€§çš„é™åˆ¶ã€‚ç”Ÿæˆå†…å®¹çš„æ­£ç¡®æ€§å’Œåˆç†æ€§éœ€è¦åŸºäºç»è¿‡éªŒè¯çš„ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰çš„ç»“æœã€‚å°†LLMé€‚åº”äºè‡ªå®šä¹‰é¢†åŸŸæ—¶çš„ä¸€ä¸ªé‡è¦é—®é¢˜æ˜¯ï¼Œç”Ÿæˆçš„å“åº”å¾€å¾€ä¸å®Œæ•´ï¼Œæˆ–è€…æ·»åŠ çš„å†…å®¹æœªç»éªŒè¯ï¼Œç”šè‡³å¯èƒ½æ˜¯è™šæ„çš„ã€‚å…ˆå‰å…³äºå¹»è§‰æ£€æµ‹çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œè¿™äº›æŒ‡æ ‡ä¸å®¹æ˜“é€‚åº”åŠ¨æ€é¢†åŸŸï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°å¦‚è¶Šç‹±æ”»å‡»ç­‰æ”»å‡»çš„å½±å“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†1ï¼‰ä¸€ç§åˆ©ç”¨RAGä¸Šä¸‹æ–‡ä¸­çš„çŸ¥è¯†ä¸‰å…ƒç»„æ ¡æ­£å¹»è§‰çš„åå¤„ç†ç®—æ³•ï¼›ä»¥åŠ2ï¼‰ä¸€ç§èåˆRAGä¸Šä¸‹æ–‡ä»¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹çš„åŒè§£ç å™¨æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07870v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å†…å®¹ç”Ÿæˆèƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å…¶åº”ç”¨å—é™äºå†…å®¹çš„é¢†åŸŸç›¸å…³æ€§ã€‚ç”Ÿæˆçš„æ­£ç¡®æ€§å’Œæ‰å®æ€§éœ€è¦åŸºäºéªŒè¯çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ç»“æœã€‚å°†LLMsé€‚åº”äºè‡ªå®šä¹‰é¢†åŸŸæ—¶ï¼Œç”Ÿæˆçš„å›å¤å¾€å¾€ä¸å®Œæ•´ï¼Œæ·»åŠ çš„å†…å®¹æœªç»éªŒè¯ï¼Œç”šè‡³å¯èƒ½æ˜¯è™šæ„çš„ã€‚æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨RAGä¸Šä¸‹æ–‡ä¸­çš„çŸ¥è¯†ä¸‰å…ƒç»„æ ¡æ­£è™šæ„å†…å®¹çš„åå¤„ç†ç®—æ³•ï¼Œä»¥åŠèåˆRAGä¸Šä¸‹æ–‡æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹çš„åŒè§£ç å™¨æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTçš„å†…å®¹ç”Ÿæˆèƒ½åŠ›å—é™äºå…¶é¢†åŸŸç›¸å…³æ€§ã€‚</li>
<li>ç”Ÿæˆçš„æ­£ç¡®æ€§éœ€åŸºäºéªŒè¯çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚RAGç»“æœã€‚</li>
<li>LLMsé€‚åº”è‡ªå®šä¹‰é¢†åŸŸæ—¶ï¼Œç”Ÿæˆçš„å›å¤å¯èƒ½ä¸å®Œæ•´ã€‚</li>
<li>æ·»åŠ çš„å†…å®¹å¯èƒ½æœªç»éªŒè¯ï¼Œå­˜åœ¨è™šæ„æƒ…å†µã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨è¯„ä¼°æŒ‡æ ‡ï¼Œéš¾ä»¥é€‚åº”åŠ¨æ€é¢†åŸŸä¸”æ˜“å—æ”»å‡»ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºåˆ©ç”¨çŸ¥è¯†ä¸‰å…ƒç»„æ ¡æ­£è™šæ„å†…å®¹çš„åå¤„ç†ç®—æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-27c46e913eae04950c1abec35227ed96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f60b98e8d7ca5629d386585e1c94f6a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2b45bc4de25a3e208bf7209f7030620.jpg" align="middle">
</details>




<h2 id="GPT-Semantic-Cache-Reducing-LLM-Costs-and-Latency-via-Semantic-Embedding-Caching"><a href="#GPT-Semantic-Cache-Reducing-LLM-Costs-and-Latency-via-Semantic-Embedding-Caching" class="headerlink" title="GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic   Embedding Caching"></a>GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic   Embedding Caching</h2><p><strong>Authors:Sajal Regmi, Chetan Phakami Pun</strong></p>
<p>Large Language Models (LLMs), such as GPT, have revolutionized artificial intelligence by enabling nuanced understanding and generation of human-like text across a wide range of applications. However, the high computational and financial costs associated with frequent API calls to these models present a substantial bottleneck, especially for applications like customer service chatbots that handle repetitive queries. In this paper, we introduce GPT Semantic Cache, a method that leverages semantic caching of query embeddings in in-memory storage (Redis). By storing embeddings of user queries, our approach efficiently identifies semantically similar questions, allowing for the retrieval of pre-generated responses without redundant API calls to the LLM. This technique achieves a notable reduction in operational costs while significantly enhancing response times, making it a robust solution for optimizing LLM-powered applications. Our experiments demonstrate that GPT Semantic Cache reduces API calls by up to 68.8% across various query categories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the system achieves high accuracy, with positive hit rates exceeding 97%, confirming the reliability of cached responses. This technique not only reduces operational costs, but also improves response times, enhancing the efficiency of LLM-powered applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚GPTï¼Œå·²ç»é€šè¿‡åœ¨ä¸€ç³»åˆ—å¹¿æ³›çš„åº”ç”¨ç¨‹åºä¸­å®ç°äººæ€§åŒ–çš„ç»†å¾®ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆï¼Œä»è€Œå½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½é¢†åŸŸã€‚ç„¶è€Œï¼Œä¸è¿™äº›æ¨¡å‹çš„é¢‘ç¹APIè°ƒç”¨ç›¸å…³çš„é«˜è®¡ç®—æˆæœ¬å’Œè´¢åŠ¡æˆæœ¬æˆä¸ºäº†ä¸€ä¸ªä¸»è¦çš„ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯å¯¹äºå¤„ç†é‡å¤æ€§æŸ¥è¯¢çš„å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äººç­‰åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GPTè¯­ä¹‰ç¼“å­˜æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å†…å­˜å­˜å‚¨ï¼ˆRedisï¼‰ä¸­çš„æŸ¥è¯¢åµŒå…¥è¯­ä¹‰ç¼“å­˜ã€‚é€šè¿‡å­˜å‚¨ç”¨æˆ·æŸ¥è¯¢çš„åµŒå…¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«è¯­ä¹‰ä¸Šç›¸ä¼¼çš„é—®é¢˜ï¼Œå…è®¸æ£€ç´¢é¢„å…ˆç”Ÿæˆçš„å“åº”ï¼Œè€Œæ— éœ€å¯¹LLMè¿›è¡Œå†—ä½™çš„APIè°ƒç”¨ã€‚æ­¤æŠ€æœ¯å®ç°äº†è¿è¥æˆæœ¬çš„å¯è§‚é™ä½ï¼ŒåŒæ—¶å¤§å¤§æé«˜äº†å“åº”æ—¶é—´ï¼Œæ˜¯ä¼˜åŒ–LLMé©±åŠ¨åº”ç”¨çš„ä¸€ä¸ªç¨³å¥è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGPTè¯­ä¹‰ç¼“å­˜å°†å„ç±»æŸ¥è¯¢çš„APIè°ƒç”¨å‡å°‘äº†é«˜è¾¾68.8%ï¼Œç¼“å­˜å‘½ä¸­ç‡åœ¨61.6%è‡³68.8%ä¹‹é—´ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿçš„å‡†ç¡®ç‡å¾ˆé«˜ï¼Œæ­£é¢å‘½ä¸­ç‡è¶…è¿‡97%ï¼Œè¯æ˜äº†ç¼“å­˜å“åº”çš„å¯é æ€§ã€‚è¿™é¡¹æŠ€æœ¯ä¸ä»…é™ä½äº†è¿è¥æˆæœ¬ï¼Œè€Œä¸”æé«˜äº†å“åº”æ—¶é—´ï¼Œå¢å¼ºäº†LLMé©±åŠ¨åº”ç”¨çš„å·¥ä½œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05276v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPTå¸¦æ¥äº†äººå·¥æ™ºèƒ½çš„é©æ–°ï¼Œå®ç°äº†è·¨å¤šä¸ªåº”ç”¨çš„ç±»ä¼¼äººç±»çš„æ–‡æœ¬ç†è§£å’Œç”Ÿæˆã€‚ç„¶è€Œï¼Œé¢‘ç¹è°ƒç”¨è¿™äº›æ¨¡å‹çš„APIäº§ç”Ÿäº†é«˜æ˜‚çš„è®¡ç®—å’Œè´¢åŠ¡æˆæœ¬ï¼Œæˆä¸ºäº†ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é‡å¤æ€§æŸ¥è¯¢çš„åº”ç”¨ï¼ˆå¦‚å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äººï¼‰ä¸­ã€‚æœ¬æ–‡æå‡ºGPTè¯­ä¹‰ç¼“å­˜æ–¹æ³•ï¼Œåˆ©ç”¨å†…å­˜å­˜å‚¨ï¼ˆRedisï¼‰ä¸­çš„æŸ¥è¯¢åµŒå…¥è¯­ä¹‰ç¼“å­˜ã€‚é€šè¿‡å­˜å‚¨ç”¨æˆ·æŸ¥è¯¢çš„åµŒå…¥ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°è¯†åˆ«è¯­ä¹‰ç›¸ä¼¼çš„æé—®ï¼Œæ£€ç´¢é¢„ç”Ÿæˆçš„ç­”æ¡ˆï¼Œæ— éœ€å†—ä½™è°ƒç”¨LLMçš„APIã€‚è¯¥æŠ€æœ¯æ˜¾è‘—é™ä½äº†è¿è¥æˆæœ¬ï¼Œæé«˜äº†å“åº”é€Ÿåº¦ï¼Œæ˜¯ä¼˜åŒ–LLMé©±åŠ¨åº”ç”¨çš„æœ‰åŠ›è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒGPTè¯­ä¹‰ç¼“å­˜å°†APIè°ƒç”¨å‡å°‘äº†é«˜è¾¾68.8%ï¼Œå„ç±»æŸ¥è¯¢çš„ç¼“å­˜å‘½ä¸­ç‡åœ¨61.6%è‡³68.8%ä¹‹é—´ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå‡†ç¡®ç‡é«˜ï¼Œç§¯æå‘½ä¸­ç‡è¶…è¿‡97%ï¼Œè¯æ˜äº†ç¼“å­˜ç­”æ¡ˆçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¦‚GPTå®ç°äº†äººç±»ç±»ä¼¼çš„æ–‡æœ¬ç†è§£å’Œç”Ÿæˆï¼Œä½†é«˜è®¡ç®—æˆæœ¬æˆä¸ºç“¶é¢ˆã€‚</li>
<li>GPTè¯­ä¹‰ç¼“å­˜æ–¹æ³•é€šè¿‡å†…å­˜å­˜å‚¨ä¸­çš„æŸ¥è¯¢åµŒå…¥è¯­ä¹‰ç¼“å­˜æ¥å‡å°‘APIè°ƒç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½é«˜æ•ˆè¯†åˆ«è¯­ä¹‰ç›¸ä¼¼çš„æé—®å¹¶æ£€ç´¢é¢„ç”Ÿæˆçš„ç­”æ¡ˆã€‚</li>
<li>GPTè¯­ä¹‰ç¼“å­˜æ˜¾è‘—é™ä½äº†è¿è¥æˆæœ¬å¹¶æé«˜äº†å“åº”é€Ÿåº¦ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºGPTè¯­ä¹‰ç¼“å­˜å°†APIè°ƒç”¨å‡å°‘äº†é«˜è¾¾68.8%ã€‚</li>
<li>å„ç±»æŸ¥è¯¢çš„ç¼“å­˜å‘½ä¸­ç‡åœ¨61.6%è‡³68.8%ä¹‹é—´ã€‚</li>
<li>ç³»ç»Ÿå‡†ç¡®ç‡é«˜ï¼Œç§¯æå‘½ä¸­ç‡è¶…è¿‡97%ï¼Œè¯æ˜ç¼“å­˜ç­”æ¡ˆçš„å¯é æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-41a72815ae3797294c2863652edd2fa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9e3db92edbd1fe6bbd8c336d573f304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-922d6220c4f337cf8453a6b5e70b571d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09d4f7c1c75ef0e859b65ce48ebf17fd.jpg" align="middle">
</details>




<h2 id="How-Transformers-Solve-Propositional-Logic-Problems-A-Mechanistic-Analysis"><a href="#How-Transformers-Solve-Propositional-Logic-Problems-A-Mechanistic-Analysis" class="headerlink" title="How Transformers Solve Propositional Logic Problems: A Mechanistic   Analysis"></a>How Transformers Solve Propositional Logic Problems: A Mechanistic   Analysis</h2><p><strong>Authors:Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy</strong></p>
<p>Large language models (LLMs) have shown amazing performance on tasks that require planning and reasoning. Motivated by this, we investigate the internal mechanisms that underpin a networkâ€™s ability to perform complex logical reasoning. We first construct a synthetic propositional logic problem that serves as a concrete test-bed for network training and evaluation. Crucially, this problem demands nontrivial planning to solve. We perform our study on two fronts. First, we pursue an understanding of precisely how a three-layer transformer, trained from scratch and attains perfect test accuracy, solves this problem. We are able to identify certain â€œplanningâ€ and â€œreasoningâ€ mechanisms in the network that necessitate cooperation between the attention blocks to implement the desired logic. Second, we study how pretrained LLMs, namely Mistral-7B and Gemma-2-9B, solve this problem. We characterize their reasoning circuits through causal intervention experiments, providing necessity and sufficiency evidence for the circuits. We find evidence suggesting that the two modelsâ€™ latent reasoning strategies are surprisingly similar, and human-like. Overall, our work systemically uncovers novel aspects of small and large transformers, and continues the study of how they plan and reason. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦è§„åˆ’å’Œæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„æ€§èƒ½ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶æ”¯æ’‘ç½‘ç»œæ‰§è¡Œå¤æ‚é€»è¾‘æ¨ç†èƒ½åŠ›çš„å†…éƒ¨æœºåˆ¶ã€‚æˆ‘ä»¬é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåˆæˆå‘½é¢˜é€»è¾‘é—®é¢˜ï¼Œä½œä¸ºç½‘ç»œè®­ç»ƒå’Œè¯„ä¼°çš„å…·ä½“æµ‹è¯•å¹³å°ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ä¸ªé—®é¢˜éœ€è¦éå¹³å‡¡çš„è§„åˆ’æ‰èƒ½è§£å†³ã€‚æˆ‘ä»¬çš„ç ”ç©¶åˆ†ä¸ºä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è‡´åŠ›äºç†è§£ä¸€ä¸ªä¸‰å±‚å˜å‹å™¨å¦‚ä½•ä»é›¶å¼€å§‹è®­ç»ƒï¼Œå¹¶è¾¾åˆ°å®Œç¾çš„æµ‹è¯•ç²¾åº¦ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç½‘ç»œä¸­è¯†åˆ«å‡ºæŸäº›â€œè§„åˆ’â€å’Œâ€œæ¨ç†â€æœºåˆ¶ï¼Œéœ€è¦æ³¨æ„åŠ›å—ä¹‹é—´çš„åˆä½œæ¥å®ç°æ‰€éœ€çš„é€»è¾‘ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå³Mistral-7Bå’ŒGemma-2-9Bï¼Œå¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å› æœå¹²é¢„å®éªŒå¯¹å…¶æ¨ç†ç”µè·¯è¿›è¡Œè¡¨å¾ï¼Œä¸ºç”µè·¯æä¾›å¿…è¦æ€§å’Œå……åˆ†æ€§è¯æ®ã€‚æˆ‘ä»¬å‘ç°è¯æ®è¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹çš„æ½œåœ¨æ¨ç†ç­–ç•¥å‡ºäººæ„æ–™åœ°ç›¸ä¼¼ï¼Œä¸”ç±»ä¼¼äºäººç±»ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œç³»ç»Ÿåœ°æ­ç¤ºäº†å°å‹å’Œå¤§å‹å˜å‹å™¨çš„å…¨æ–°æ–¹é¢ï¼Œå¹¶ç»§ç»­ç ”ç©¶å®ƒä»¬æ˜¯å¦‚ä½•è¿›è¡Œè§„åˆ’å’Œæ¨ç†çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04105v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éœ€è¦è§„åˆ’å’Œæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºæƒŠäººçš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶æ”¯æ’‘ç½‘ç»œæ‰§è¡Œå¤æ‚é€»è¾‘æ¨ç†çš„å†…éƒ¨æœºåˆ¶ã€‚é¦–å…ˆï¼Œæ„å»ºä¸€ä¸ªåˆæˆå‘½é¢˜é€»è¾‘é—®é¢˜ï¼Œä½œä¸ºç½‘ç»œè®­ç»ƒå’Œè¯„ä¼°çš„å…·ä½“æµ‹è¯•å¹³å°ã€‚æ­¤é—®é¢˜éœ€é€šè¿‡å¤æ‚çš„è§„åˆ’æ¥è§£å†³ã€‚ç ”ç©¶åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œäº†è§£ä¸€ä¸ªä¸‰å±‚å˜å‹å™¨å¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒå¹¶è¾¾åˆ°å®Œç¾æµ‹è¯•ç²¾åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç½‘ç»œä¸­è¯†åˆ«å‡ºæŸäº›éœ€è¦æ³¨æ„åŠ›å—åˆä½œçš„â€œè§„åˆ’â€å’Œâ€œæ¨ç†â€æœºåˆ¶æ¥å®ç°æ‰€éœ€çš„é€»è¾‘ã€‚å…¶æ¬¡ï¼Œç ”ç©¶é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Mistral-7Bå’ŒGemma-2-9Bï¼‰å¦‚ä½•è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å› æœå¹²é¢„å®éªŒåˆ»ç”»å…¶æ¨ç†ç”µè·¯ï¼Œä¸ºç”µè·¯æä¾›å¿…è¦æ€§å’Œå……åˆ†æ€§è¯æ®ã€‚æˆ‘ä»¬å‘ç°ä¸¤ä¸ªæ¨¡å‹çš„æ½œåœ¨æ¨ç†ç­–ç•¥å‡ºäººæ„æ–™åœ°ç›¸ä¼¼ä¸”ç±»ä¼¼äºäººç±»ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç³»ç»Ÿåœ°æ­ç¤ºäº†å°å‹å’Œå¤§å‹å˜å‹å™¨çš„å…¨æ–°æ–¹é¢ï¼Œå¹¶ç»§ç»­ç ”ç©¶å®ƒä»¬çš„è§„åˆ’å’Œæ¨ç†æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éœ€è¦è§„åˆ’å’Œæ¨ç†çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>åˆæˆå‘½é¢˜é€»è¾‘é—®é¢˜ä¸ºç½‘ç»œè®­ç»ƒå’Œè¯„ä¼°æä¾›äº†å…·ä½“æµ‹è¯•å¹³å°ã€‚</li>
<li>ä¸‰å±‚å˜å‹å™¨æ¨¡å‹å±•ç¤ºå®Œç¾çš„æµ‹è¯•ç²¾åº¦ï¼Œé€šè¿‡è¯†åˆ«è§„åˆ’åŠæ¨ç†æœºåˆ¶æ­ç¤ºå…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚Mistral-7Bå’ŒGemma-2-9Bå…·å¤‡å¼ºå¤§çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¿™äº›æ¨¡å‹çš„æ¨ç†ç­–ç•¥å…·æœ‰å‡ºäººæ„æ–™çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºç±»ä¼¼äºäººç±»çš„ç‰¹å¾ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å°å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’å’Œæ¨ç†æ–¹é¢çš„æ–°ç‰¹ç‚¹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-816d3cb83fb4c60415c8ab1e1c65cc32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c252410e42279575dfb0a5a77e48812b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c552637754b27b4571aaddd23cbf96b.jpg" align="middle">
</details>




<h2 id="From-Novice-to-Expert-LLM-Agent-Policy-Optimization-via-Step-wise-Reinforcement-Learning"><a href="#From-Novice-to-Expert-LLM-Agent-Policy-Optimization-via-Step-wise-Reinforcement-Learning" class="headerlink" title="From Novice to Expert: LLM Agent Policy Optimization via Step-wise   Reinforcement Learning"></a>From Novice to Expert: LLM Agent Policy Optimization via Step-wise   Reinforcement Learning</h2><p><strong>Authors:Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen</strong></p>
<p>The outstanding capabilities of large language models (LLMs) render them a crucial component in various autonomous agent systems. While traditional methods depend on the inherent knowledge of LLMs without fine-tuning, more recent approaches have shifted toward the reinforcement learning strategy to further enhance agentsâ€™ ability to solve complex interactive tasks with environments and tools. However, previous approaches are constrained by the sparse reward issue, where existing datasets solely provide a final scalar reward for each multi-step reasoning chain, potentially leading to ineffectiveness and inefficiency in policy learning. In this paper, we introduce StepAgent, which utilizes step-wise reward to optimize the agentâ€™s reinforcement learning process. Inheriting the spirit of novice-to-expert theory, we first compare the actions of the expert and the agent to automatically generate intermediate rewards for fine-grained optimization. Additionally, we propose implicit-reward and inverse reinforcement learning techniques to facilitate agent reflection and policy adjustment. Further theoretical analysis demonstrates that the action distribution of the agent can converge toward the expert action distribution over multiple training cycles. Experimental results across various datasets indicate that StepAgent outperforms existing baseline methods. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²èƒ½åŠ›ä½¿å®ƒä»¬æˆä¸ºå„ç§è‡ªä¸»ä»£ç†ç³»ç»Ÿä¸­çš„å…³é”®ç»„ä»¶ã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºLLMçš„å›ºæœ‰çŸ¥è¯†è€Œæ— éœ€å¾®è°ƒï¼Œä½†æ›´æ–°çš„æ–¹æ³•å·²è½¬å‘å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ä»£ç†è§£å†³ä¸ç¯å¢ƒå’Œå·¥å…·ç›¸å…³çš„å¤æ‚äº¤äº’ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä»¥å‰çš„æ–¹æ³•å—åˆ°ç¨€ç–å¥–åŠ±é—®é¢˜çš„é™åˆ¶ï¼Œç°æœ‰æ•°æ®é›†åªä¸ºæ¯ä¸ªå¤šæ­¥æ¨ç†é“¾æä¾›æœ€ç»ˆæ ‡é‡å¥–åŠ±ï¼Œè¿™å¯èƒ½å¯¼è‡´ç­–ç•¥å­¦ä¹ ä¸­çš„ä½æ•ˆå’Œæ— æ•ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†StepAgentï¼Œå®ƒåˆ©ç”¨åˆ†æ­¥å¥–åŠ±æ¥ä¼˜åŒ–ä»£ç†çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚ç§‰æ‰¿æ–°æ‰‹åˆ°ä¸“å®¶ç†è®ºçš„ç²¾ç¥ï¼Œæˆ‘ä»¬é¦–å…ˆå°†ä¸“å®¶ä¸ä»£ç†çš„è¡ŒåŠ¨è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆç”¨äºç²¾ç»†ä¼˜åŒ–çš„ä¸­é—´å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†éšå¼å¥–åŠ±å’Œé€†å‘å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›ä»£ç†åæ€å’Œç­–ç•¥è°ƒæ•´ã€‚è¿›ä¸€æ­¥çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œä»£ç†çš„åŠ¨ä½œåˆ†å¸ƒå¯ä»¥åœ¨å¤šä¸ªè®­ç»ƒå‘¨æœŸå†…æ”¶æ•›åˆ°ä¸“å®¶åŠ¨ä½œåˆ†å¸ƒã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒStepAgentä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03817v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªä¸»ä»£ç†ç³»ç»Ÿä¸­çš„ä½œç”¨æ—¥ç›Šé‡è¦ã€‚æœ€æ–°æ–¹æ³•é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜ä»£ç†è§£å†³å¤æ‚äº¤äº’ä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•å—åˆ°ç¨€ç–å¥–åŠ±é—®é¢˜çš„é™åˆ¶ï¼Œç°æœ‰æ•°æ®é›†åªä¸ºæ¯ä¸ªå¤šæ­¥æ¨ç†é“¾æä¾›æœ€ç»ˆæ ‡é‡å¥–åŠ±ï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥å­¦ä¹ çš„ä¸é«˜æ•ˆå’Œæ— æ•ˆã€‚æœ¬æ–‡æå‡ºStepAgentï¼Œåˆ©ç”¨æ­¥éª¤å¥–åŠ±ä¼˜åŒ–ä»£ç†çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ã€‚å€Ÿé‰´æ–°æ‰‹åˆ°ä¸“å®¶çš„ç†è®ºç²¾ç¥ï¼Œæˆ‘ä»¬æ¯”è¾ƒä¸“å®¶ä¸ä»£ç†çš„è¡ŒåŠ¨æ¥è‡ªåŠ¨ç”Ÿæˆä¸­é—´å¥–åŠ±ï¼Œå®ç°ç²¾ç»†ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†éšæ€§å¥–åŠ±å’Œé€†å‘å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œä»¥ä¿ƒè¿›ä»£ç†åæ€å’Œè°ƒæ•´ç­–ç•¥ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œä»£ç†çš„è¡ŒåŠ¨åˆ†å¸ƒå¯ä»¥åœ¨å¤šæ¬¡è®­ç»ƒå‘¨æœŸä¸­æ”¶æ•›åˆ°ä¸“å®¶è¡ŒåŠ¨åˆ†å¸ƒã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒStepAgentä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªä¸»ä»£ç†ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>æœ€è¿‘çš„ç­–ç•¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥æå‡ä»£ç†è§£å†³å¤æ‚äº¤äº’ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>ç¨€ç–å¥–åŠ±é—®é¢˜é™åˆ¶äº†ä¹‹å‰çš„æ–¹æ³•ï¼Œéœ€è¦ä¸ºæ¯ä¸ªå¤šæ­¥æ¨ç†é“¾æä¾›æ›´ç²¾ç»†çš„å¥–åŠ±ã€‚</li>
<li>StepAgenté€šè¿‡åˆ©ç”¨æ­¥éª¤å¥–åŠ±ä¼˜åŒ–ä»£ç†çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>StepAgentæ¯”è¾ƒä¸“å®¶ä¸ä»£ç†çš„è¡ŒåŠ¨æ¥è‡ªåŠ¨ç”Ÿæˆä¸­é—´å¥–åŠ±ï¼Œå®ç°ç²¾ç»†ä¼˜åŒ–ã€‚</li>
<li>å¼•å…¥éšæ€§å¥–åŠ±å’Œé€†å‘å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œä¿ƒè¿›ä»£ç†åæ€å’Œè°ƒæ•´ç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9e25e9bcec91952adadfd060eb0d2039.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bd8bc07dd42821138147139ee9956c4e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1822cf83418b3e85554f64faf9dcec51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8d2e4b6d67f29c2438e02a0b6a183db.jpg" align="middle">
</details>




<h2 id="Pangea-A-Fully-Open-Multilingual-Multimodal-LLM-for-39-Languages"><a href="#Pangea-A-Fully-Open-Multilingual-Multimodal-LLM-for-39-Languages" class="headerlink" title="Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages"></a>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</h2><p><strong>Authors:Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig</strong></p>
<p>Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the worldâ€™s languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess modelsâ€™ capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æœ‰æ‰€è¿›å±•ï¼Œä½†å®ƒä»¬çš„å‘å±•ä¸»è¦é›†ä¸­åœ¨è‹±è¯­å’Œè¥¿æ–¹ä¸ºä¸­å¿ƒçš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šï¼Œå¯¼è‡´ä¸–ç•Œä¸Šå¤§å¤šæ•°è¯­è¨€å’Œå¤šæ ·åŒ–çš„æ–‡åŒ–èƒŒæ™¯ä»£è¡¨æ€§ä¸è¶³ã€‚æœ¬æ–‡ä»‹ç»äº†Pangeaï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨PangeaInsä¸Šè®­ç»ƒçš„å¤šè¯­è¨€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒPangeaInsæ˜¯ä¸€ä¸ªåŒ…å«39ç§è¯­è¨€çš„å¤šæ ·åŒ–6ç™¾ä¸‡æŒ‡ä»¤æ•°æ®é›†ã€‚PangeaInsçš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š1ï¼‰é«˜è´¨é‡çš„è‹±è¯­æŒ‡ä»¤ï¼Œ2ï¼‰ç²¾å¿ƒæœºå™¨ç¿»è¯‘çš„æŒ‡ä»¤ï¼Œä»¥åŠ3ï¼‰ç¡®ä¿è·¨æ–‡åŒ–è¦†ç›–çš„æ–‡åŒ–ç›¸å…³å¤šæ¨¡æ€ä»»åŠ¡ã€‚ä¸ºäº†ä¸¥æ ¼è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PangeaBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å¥—ä»¶ï¼ŒåŒ…å«è¦†ç›–47ç§è¯­è¨€çš„14ä¸ªæ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§è¯­è¨€å’Œå¤šæ ·åŒ–çš„æ–‡åŒ–èƒŒæ™¯ä¸‹ï¼ŒPangeaæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºæ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†è‹±è¯­æ•°æ®æ¯”ä¾‹ã€è¯­è¨€æµè¡Œåº¦å’Œå¤šæ¨¡æ€è®­ç»ƒæ ·æœ¬æ•°é‡å¯¹æ•´ä½“æ€§èƒ½çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å®Œå…¨å¼€æºæˆ‘ä»¬çš„æ•°æ®ã€ä»£ç å’Œè®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›åŒ…å®¹æ€§å’Œç¨³å¥çš„å¤šè¯­è¨€MLLMsçš„å‘å±•ï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„è¯­è¨€å’Œæ–‡åŒ–é¢†åŸŸçš„å…¬å¹³å’Œå¯è®¿é—®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16153v2">PDF</a> 54 pages, 27 figures</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è™½æœ‰æ‰€è¿›å±•ï¼Œä½†ä¸»è¦èšç„¦äºè‹±è¯­å’Œè¥¿æ–¹è¯­å¢ƒçš„æ•°æ®é›†å’Œä»»åŠ¡ï¼Œå¿½è§†äº†ä¸–ç•Œä¸Šå…¶ä»–è¯­è¨€å’Œå¤šå…ƒæ–‡åŒ–èƒŒæ™¯çš„ä»£è¡¨æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªè·¨è¯­è¨€çš„MLLMâ€”â€”Pangeaï¼Œå®ƒåŸºäºä¸€ä¸ªåŒ…å«39ç§è¯­è¨€çš„å¤šå…ƒåŒ–æŒ‡ä»¤æ•°æ®é›†PangeaInsè¿›è¡Œè®­ç»ƒã€‚PangeaInsçš„ç‰¹ç‚¹åŒ…æ‹¬ï¼šé«˜è´¨é‡è‹±è¯­æŒ‡ä»¤ã€ç²¾å¿ƒæœºå™¨ç¿»è¯‘çš„æŒ‡ä»¤ï¼Œä»¥åŠç¡®ä¿è·¨æ–‡åŒ–è¦†ç›–çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚ä¸ºä¸¥æ ¼è¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†PangeaBenchè¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–47ç§è¯­è¨€çš„14ä¸ªæ•°æ®é›†ã€‚ç»“æœæ˜¾ç¤ºï¼ŒPangeaåœ¨å¤šè¯­è¨€èƒŒæ™¯å’Œå¤šå…ƒæ–‡åŒ–ç¯å¢ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥æ­ç¤ºäº†è‹±è¯­æ•°æ®æ¯”ä¾‹ã€è¯­è¨€æµè¡Œåº¦å’Œå¤šæ¨¡æ€è®­ç»ƒæ ·æœ¬æ•°é‡å¯¹æ•´ä½“æ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬å…¬å¼€æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œè®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›å¼€å‘å’ŒåŒ…å®¹æ€§å¼ºçš„å¤šè¯­è¨€MLLMsï¼Œæ¨åŠ¨æ›´å¹¿æ³›çš„è¯­è¨€å’Œæ–‡åŒ–çš„å…¬å¹³æ€§å’Œå¯è®¿é—®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsè™½ç„¶æœ‰æ‰€å‘å±•ï¼Œä½†ä¸»è¦èšç„¦äºè‹±è¯­å’Œè¥¿æ–¹è¯­å¢ƒï¼Œå¿½è§†äº†å…¶ä»–è¯­è¨€å’Œå¤šå…ƒæ–‡åŒ–èƒŒæ™¯çš„ä»£è¡¨æ€§ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªè·¨è¯­è¨€çš„MLLMâ€”â€”Pangeaï¼Œå…¶åŸºäºä¸€ä¸ªå¤šå…ƒåŒ–çš„æŒ‡ä»¤æ•°æ®é›†PangeaInsè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«39ç§è¯­è¨€ã€‚</li>
<li>PangeaInsæ•°æ®é›†åŒ…å«é«˜è´¨é‡è‹±è¯­æŒ‡ä»¤ã€æœºå™¨ç¿»è¯‘çš„æŒ‡ä»¤ä»¥åŠå¤šæ¨¡æ€ä»»åŠ¡ï¼Œä»¥ç¡®ä¿è·¨æ–‡åŒ–çš„è¦†ç›–ã€‚</li>
<li>è®ºæ–‡è¿˜ä»‹ç»äº†PangeaBenchè¯„ä¼°å¥—ä»¶ï¼Œç”¨äºä¸¥æ ¼è¯„ä¼°æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ¶µç›–14ä¸ªæ•°æ®é›†å’Œ47ç§è¯­è¨€ã€‚</li>
<li>Pangeaåœ¨å¤šè¯­è¨€èƒŒæ™¯å’Œå¤šå…ƒæ–‡åŒ–ç¯å¢ƒä¸­çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰å¼€æºæ¨¡å‹ã€‚</li>
<li>æ¶ˆèç ”ç©¶æ­ç¤ºäº†è‹±è¯­æ•°æ®æ¯”ä¾‹ã€è¯­è¨€æµè¡Œåº¦å’Œå¤šæ¨¡æ€è®­ç»ƒæ ·æœ¬æ•°é‡å¯¹æ¨¡å‹æ€§èƒ½çš„é‡è¦å½±å“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8d80f58a4f2dff18cf8e85db0ba9764c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63991c526f72b5f650ec81db11fd467b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37e971162bbd1fe73765edde3c2ade4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aebee328d6897770d7c5ee7696cc36fc.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-63b51490b75adc99918bc5dc29fd3d40.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  Multimodal Whole Slide Foundation Model for Pathology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-07/%E9%8D%96%E8%AF%B2%EE%84%9F%E9%8D%A5%E6%83%A7%E5%84%9A/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-537b2c67eed64b23a7a9b8bc6b7e1300.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-07  Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET   Image Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4004.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>













</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
