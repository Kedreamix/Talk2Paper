<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    29k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    118 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-11-更新"><a href="#2024-12-11-更新" class="headerlink" title="2024-12-11 更新"></a>2024-12-11 更新</h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners. </p>
<blockquote>
<p>文本转语音（TTS），也被称为语音合成，是一个显著的研究领域，旨在从文本生成听起来很自然的人类语音。最近，随着工业需求的增加，TTS技术已经发展超越了合成类似人类的语音，实现了可控的语音生成。这包括合成语音的各种属性的精细控制，如情感、语调、音色和持续时间。此外，深度学习中的扩散和大型语言模型等技术的进步，在过去的几年里极大地增强了可控TTS的性能。在本文中，我们对可控TTS进行了全面的调查，涵盖了从基本控制技术到利用自然语言提示的方法等多种方法，旨在为研究者提供一个清晰的研究现状理解。我们考察了可控TTS的一般流程、挑战、模型架构和控制策略，提供了现有方法的全面而清晰的分类。此外，我们还详细总结了数据集和评价指标，并对可控TTS的应用和未来发展方向进行了一些探讨。据我们所知，这篇综述论文首次全面回顾了新兴的可控TTS方法，对学术研究人员和行业从业者都具有参考价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v1">PDF</a> A comprehensive survey on controllable TTS, 23 pages, 6 tables, 4   figures, 280 references</p>
<p><strong>Summary</strong><br>文本转语音（TTS）是生成自然语音的重要研究领域。近年来，随着工业需求的增加，TTS技术已从单纯的模仿人类语音发展到了可实现可控的语音生成，能对语音的各种属性进行精细控制，如情感、语调、音质和时长等。深度学习的进步，如扩散模型和大型语言模型，为可控TTS带来了巨大的提升。本文全面回顾了可控TTS的研究现状，从基本控制技巧到利用自然语言提示的方法都有所涉及，旨在为学术界和工业界提供有益的参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS (文本转语音) 是生成自然语音的重要研究领域。</li>
<li>近期TTS技术已经从单纯的模仿人类语音发展到实现可控的语音生成。</li>
<li>精细控制语音的各种属性已成为可能，如情感、语调、音质和时长等。</li>
<li>深度学习的进步，如扩散模型和大型语言模型，显著提升了可控TTS的性能。</li>
<li>本文对可控TTS进行了全面的综述，涵盖了各种方法，从基本控制技巧到自然语言提示方法。</li>
<li>文章提供了关于可控TTS的当前研究状态的清晰理解，包括通用管道、挑战、模型架构和控制策略等。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a3255132bdca965fc85d69d43568f2f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d8fe56ee1b3d384d421df9dbf984646b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e826485c7661728072f0a00efebdb680.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78d17f9628b8ef660154504c576be31e.jpg" align="middle">
</details>




<h2 id="Not-All-Errors-Are-Equal-Investigation-of-Speech-Recognition-Errors-in-Alzheimer’s-Disease-Detection"><a href="#Not-All-Errors-Are-Equal-Investigation-of-Speech-Recognition-Errors-in-Alzheimer’s-Disease-Detection" class="headerlink" title="Not All Errors Are Equal: Investigation of Speech Recognition Errors in   Alzheimer’s Disease Detection"></a>Not All Errors Are Equal: Investigation of Speech Recognition Errors in   Alzheimer’s Disease Detection</h2><p><strong>Authors:Jiawen Kang, Junan Li, Jinchao Li, Xixin Wu, Helen Meng</strong></p>
<p>Automatic Speech Recognition (ASR) plays an important role in speech-based automatic detection of Alzheimer’s disease (AD). However, recognition errors could propagate downstream, potentially impacting the detection decisions. Recent studies have revealed a non-linear relationship between word error rates (WER) and AD detection performance, where ASR transcriptions with notable errors could still yield AD detection accuracy equivalent to that based on manual transcriptions. This work presents a series of analyses to explore the effect of ASR transcription errors in BERT-based AD detection systems. Our investigation reveals that not all ASR errors contribute equally to detection performance. Certain words, such as stopwords, despite constituting a large proportion of errors, are shown to play a limited role in distinguishing AD. In contrast, the keywords related to diagnosis tasks exhibit significantly greater importance relative to other words. These findings provide insights into the interplay between ASR errors and the downstream detection model. </p>
<blockquote>
<p>自动语音识别（ASR）在基于语音的阿尔茨海默病（AD）自动检测中扮演着重要角色。然而，识别错误可能会传播到下游，潜在地影响检测决策。最近的研究揭示了词错误率（WER）与AD检测性能之间的非线性关系，其中ASR转录的错误显著但仍然可以产生与手动转录相当的AD检测准确性。这项工作提供了一系列分析，以探索ASR转录错误在基于BERT的AD检测系统中的作用。我们的调查表明，并非所有的ASR错误对检测性能的影响都是均等的。某些词语，如停用词，虽然构成了错误的大部分，但在区分AD方面的作用有限。相比之下，与诊断任务相关的关键词相对于其他词语表现出极大的重要性。这些发现提供了ASR错误与下游检测模型之间相互作用的重要见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06332v1">PDF</a> Accepted by IEEE ISCSLP 2024</p>
<p><strong>Summary</strong>：语音识别技术在阿尔茨海默病（AD）的自动检测中扮演重要角色，但其识别错误可能影响下游检测决策。研究表明，词错误率（WER）与AD检测性能之间存在非线性关系，ASR转录中的错误仍可能获得与手动转录相当的AD检测准确性。本研究分析显示，并非所有ASR错误对检测性能的影响相同。某些词如停用词虽错误率高但对区分AD作用有限，而诊断任务相关的关键词则具有更大的重要性。这些发现揭示了ASR错误与下游检测模型之间的相互作用。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自动语音识别（ASR）在阿尔茨海默病（AD）的自动检测中扮演重要角色。</li>
<li>ASR的识别错误可能会影响下游的检测决策。</li>
<li>词错误率（WER）与AD检测性能之间存在非线性关系。</li>
<li>ASR转录中的错误仍可能获得与手动转录相当的AD检测准确性。</li>
<li>在ASR错误中，停用词对区分AD的作用有限。</li>
<li>诊断任务相关的关键词在AD检测中具有更大的重要性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-23a471c96cae17846ad62e941c0aba0f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4d12ba5e3292041f35109a44b48cb73.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f57551b370fdb18d073632be708ff9c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-925dec1f53b4bcceb4d0abda80f36c1e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f75f571cf0af433be00c20dac8b6560c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cb3a59fb4badf3ac761c2ddd70cb94c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8c4d4cd13dcae93d4cc276661413ad1e.jpg" align="middle">
</details>




<h2 id="Leveraging-Prompt-Learning-and-Pause-Encoding-for-Alzheimer’s-Disease-Detection"><a href="#Leveraging-Prompt-Learning-and-Pause-Encoding-for-Alzheimer’s-Disease-Detection" class="headerlink" title="Leveraging Prompt Learning and Pause Encoding for Alzheimer’s Disease   Detection"></a>Leveraging Prompt Learning and Pause Encoding for Alzheimer’s Disease   Detection</h2><p><strong>Authors:Yin-Long Liu, Rui Feng, Jia-Hong Yuan, Zhen-Hua Ling</strong></p>
<p>Compared to other clinical screening techniques, speech-and-language-based automated Alzheimer’s disease (AD) detection methods are characterized by their non-invasiveness, cost-effectiveness, and convenience. Previous studies have demonstrated the efficacy of fine-tuning pre-trained language models (PLMs) for AD detection. However, the objective of this traditional fine-tuning method, which involves inputting only transcripts, is inconsistent with the masked language modeling (MLM) task used during the pre-training phase of PLMs. In this paper, we investigate prompt-based fine-tuning of PLMs, converting the classification task into a MLM task by inserting prompt templates into the transcript inputs. We also explore the impact of incorporating pause information from forced alignment into manual transcripts. Additionally, we compare the performance of various automatic speech recognition (ASR) models and select the Whisper model to generate ASR-based transcripts for comparison with manual transcripts. Furthermore, majority voting and ensemble techniques are applied across different PLMs (BERT and RoBERTa) using different random seeds. Ultimately, we obtain maximum detection accuracy of 95.8% (with mean 87.9%, std 3.3%) using manual transcripts, achieving state-of-the-art performance for AD detection using only transcripts on the ADReSS test set. </p>
<blockquote>
<p>与其他临床筛查技术相比，基于语言和语音的自动化阿尔茨海默病（AD）检测方法具有非侵入性、成本效益和便捷性等特点。以往的研究已经证明了预训练语言模型（PLM）进行微调在AD检测中的有效性。然而，这种传统微调方法的目标仅涉及输入文本，这与预训练阶段中使用的语言模型的任务（即遮蔽语言建模）是不一致的。在本文中，我们研究了基于提示的PLM微调方法，通过在转录输入中插入提示模板，将分类任务转换为MLM任务。我们还探讨了将强制对齐中的停顿信息纳入手动转录的影响。此外，我们比较了不同自动语音识别（ASR）模型的性能，选择了Whisper模型生成基于ASR的转录本，以便与手动转录本进行比较。同时，我们还在不同的PLM（BERT和RoBERTa）上应用了多数投票和集成技术，使用不同的随机种子。最终，我们使用手动转录本获得了最高95.8%（平均值为87.9%，标准差为3.3%）的检测准确率，在ADReSS测试集上仅使用转录本实现了阿尔茨海默病检测的最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06259v1">PDF</a> Accepted by ISCSLP 2024</p>
<p><strong>Summary</strong><br>     语音与语言基础的自动化阿尔茨海默病（AD）检测方法与其他临床筛查技术相比具有无创性、经济性和便利性。本研究探索了基于提示微调预训练语言模型的方法，将分类任务转化为掩码语言建模任务，并在输入转录中加入提示模板。同时，研究将停顿信息融入手动转录本中，对比多种自动语音识别模型并选择Whisper模型进行对比分析。最终通过集成不同预训练语言模型技术实现最高检测准确率95.8%（平均87.9%，标准差3.3%），在ADReSS测试集上取得了顶尖表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音与语言基础的自动化阿尔茨海默病检测方法具有无创性、经济性和便利性，与传统临床筛查技术相比具有优势。</li>
<li>研究通过微调预训练语言模型，实现了有效阿尔茨海默病检测。</li>
<li>提示模板的引入提高了分类任务与预训练语言模型的掩码语言建模任务的一致性。</li>
<li>停顿信息融入手动转录本对阿尔茨海默病检测性能有积极影响。</li>
<li>对比多种自动语音识别模型后选择了Whisper模型进行性能对比。</li>
<li>集成不同预训练语言模型技术显著提高了阿尔茨海默病的检测准确率，达到最高95.8%。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3f6a0248bcc23e9d044282b14968a12e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e357f48ed6cd9be3ceccb8ce77daaa4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fca418a83e4822c28f2660e570106df.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5879fe12ad6fb1866ec60b59b5f0e7af.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-af720bb59b25f7c6d115ce1046db9e23.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f3350eb69bca3b8ab7c8f3ae43f3997.jpg" align="middle">
</details>




<h2 id="SQ-Whisper-Speaker-Querying-based-Whisper-Model-for-Target-Speaker-ASR"><a href="#SQ-Whisper-Speaker-Querying-based-Whisper-Model-for-Target-Speaker-ASR" class="headerlink" title="SQ-Whisper: Speaker-Querying based Whisper Model for Target-Speaker ASR"></a>SQ-Whisper: Speaker-Querying based Whisper Model for Target-Speaker ASR</h2><p><strong>Authors:Pengcheng Guo, Xuankai Chang, Hang Lv, Shinji Watanabe, Lei Xie</strong></p>
<p>Benefiting from massive and diverse data sources, speech foundation models exhibit strong generalization and knowledge transfer capabilities to a wide range of downstream tasks. However, a limitation arises from their exclusive handling of single-speaker speech input, making them ineffective in recognizing multi-speaker overlapped speech, a common occurrence in real-world scenarios. In this study, we delve into the adaptation of speech foundation models to eliminate interfering speakers from overlapping speech and perform target-speaker automatic speech recognition (TS-ASR). Initially, we utilize the Whisper model as the foundation for adaptation and conduct a thorough comparison of its integration with existing target-speaker adaptation techniques. We then propose an innovative model termed Speaker-Querying Whisper (SQ-Whisper), which employs a set number of trainable queries to capture speaker prompts from overlapping speech based on target-speaker enrollment. These prompts serve to steer the model in extracting speaker-specific features and accurately recognizing target-speaker transcriptions. Experimental results demonstrate that our approach effectively adapts the pre-trained speech foundation model to TS-ASR. Compared with the robust TS-HuBERT model, the proposed SQ-Whisper significantly improves performance, yielding up to 15% and 10% relative reductions in word error rates (WERs) on the Libri2Mix and WSJ0-2Mix datasets, respectively. With data augmentation, we establish new state-of-the-art WERs of 14.6% on the Libri2Mix Test set and 4.4% on the WSJ0-2Mix Test set. Furthermore, we evaluate our model on the real-world AMI meeting dataset, which shows consistent improvement over other adaptation methods. </p>
<blockquote>
<p>得益于大规模和多样化的数据来源，语音基础模型在广泛的下游任务中展现出强大的泛化和知识迁移能力。然而，它们处理单说话人语音输入的局限性导致它们在识别多说话人重叠语音时效果不佳，而在现实世界中，这种情况经常发生。在这项研究中，我们深入探讨了语音基础模型对消除重叠语音中的干扰说话者的适应性，并进行了目标说话人自动语音识别（TS-ASR）。首先，我们以whisper模型为基础进行适应，并对其与现有目标说话人适应技术的集成进行了全面的比较。然后，我们提出了一种创新的模型——Speaker-Querying Whisper（SQ-Whisper），该模型通过使用一组可训练查询来捕获目标说话人的说话提示。这些提示基于目标说话人的注册信息，用于指导模型提取说话人特定特征并准确识别目标说话人的转录内容。实验结果表明，我们的方法有效地适应了预训练的语音基础模型进行TS-ASR。与强大的TS-HuBERT模型相比，提出的SQ-Whisper显著提高了性能，在Libri2Mix和WSJ0-2Mix数据集上相对减少了15%和10%的词错误率（WER）。通过数据增强，我们在Libri2Mix测试集上建立了最新的最佳WER为14.6%，在WSJ0-2Mix测试集上为4.4%。此外，我们在真实世界的AMI会议数据集上评估了我们的模型，与其他适应方法相比，表现出持续的可改进之处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05589v1">PDF</a> Accepted by IEEE&#x2F;ACM TASLP</p>
<p><strong>摘要</strong></p>
<p>受益于大规模和多样化的数据源，语音基础模型展现出强大的泛化和知识迁移能力，能广泛应用于多种下游任务。然而，处理单讲者语音输入的局限性使得它们无法有效识别多讲者重叠语音，这在现实场景中很常见。本研究致力于适应语音基础模型，以消除重叠语音中的干扰讲者，并执行目标讲者自动语音识别（TS-ASR）。首先，我们使用whisper模型作为基础进行适应，并与其与现有的目标讲者适应技术进行了全面比较。接着，我们提出了一种创新的模型——Speaker-Querying Whisper（SQ-Whisper），它利用一系列可训练的查询来捕捉目标讲者的提示，基于这些提示从重叠语音中提取讲者特定的特征，并准确识别目标讲者的转录。实验结果表明，我们的方法有效地适应了预训练的语音基础模型进行TS-ASR。与强大的TS-HuBERT模型相比，提出的SQ-Whisper显著提高了性能，在Libri2Mix和WSJ0-2Mix数据集上分别降低了15%和10%的相对词错误率（WERs）。通过数据增强，我们在Libri2Mix测试集上建立了新的最佳WERs为14.6%，在WSJ0-2Mix测试集上为4.4%。此外，在真实世界的AMI会议数据集上，我们的模型也表现出比其他适应方法更持续的改进。</p>
<p><strong>要点</strong></p>
<ol>
<li>语音基础模型受益于大规模和多样化数据源，具有强大的泛化和知识迁移能力。</li>
<li>现有模型在处理多讲者重叠语音时存在局限性。</li>
<li>本研究旨在通过适应语音基础模型来识别重叠语音中的目标讲者。</li>
<li>使用whisper模型作为基础进行适应，并与其现有的目标讲者适应技术进行比较。</li>
<li>提出创新的SQ-Whisper模型，利用可训练查询捕捉目标讲者提示，以识别目标讲者的语音。</li>
<li>SQ-Whisper在多个数据集上显著提高了性能，包括Libri2Mix、WSJ0-2Mix和AMI会议数据集。</li>
<li>通过数据增强，建立了新的最佳WERs记录。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0269e0f784eb2942914fd6feace9c878.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-90fb044581dc8a0c15def3da08f167fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8cb941a770eeb8460c7d41edfacb8451.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d41ab9633b6b11d163018635dac73617.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-01c1a780650cab9e91577a713df41bd3.jpg" align="middle">
</details>




<h2 id="WavFusion-Towards-wav2vec-2-0-Multimodal-Speech-Emotion-Recognition"><a href="#WavFusion-Towards-wav2vec-2-0-Multimodal-Speech-Emotion-Recognition" class="headerlink" title="WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition"></a>WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition</h2><p><strong>Authors:Feng Li, Jiusong Luo, Wanjun Xia</strong></p>
<p>Speech emotion recognition (SER) remains a challenging yet crucial task due to the inherent complexity and diversity of human emotions. To address this problem, researchers attempt to fuse information from other modalities via multimodal learning. However, existing multimodal fusion techniques often overlook the intricacies of cross-modal interactions, resulting in suboptimal feature representations. In this paper, we propose WavFusion, a multimodal speech emotion recognition framework that addresses critical research problems in effective multimodal fusion, heterogeneity among modalities, and discriminative representation learning. By leveraging a gated cross-modal attention mechanism and multimodal homogeneous feature discrepancy learning, WavFusion demonstrates improved performance over existing state-of-the-art methods on benchmark datasets. Our work highlights the importance of capturing nuanced cross-modal interactions and learning discriminative representations for accurate multimodal SER. Experimental results on two benchmark datasets (IEMOCAP and MELD) demonstrate that WavFusion succeeds over the state-of-the-art strategies on emotion recognition. </p>
<blockquote>
<p>语音情感识别（SER）仍然是一项具有挑战但至关重要的任务，这是由于人类情绪的固有复杂性和多样性。为了解决这个问题，研究人员试图通过多模态学习融合其他模态的信息。然而，现有的多模态融合技术往往忽视了跨模态交互的复杂性，导致特征表示不佳。在本文中，我们提出了WavFusion，这是一个多模态语音情感识别框架，解决了有效多模态融合、模态之间的异质性和判别表示学习的关键研究问题。通过利用门控跨模态注意力机制和跨模态均匀特征差异学习，WavFusion在基准数据集上实现了对现有先进方法的性能提升。我们的工作强调了捕捉微妙的跨模态交互和学习判别性表示对于准确的多模态SER的重要性。在IEMOCAP和MELD两个基准数据集上的实验结果证明，WavFusion在情感识别方面超过了最新的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05558v1">PDF</a> Accepted by 31st International Conference on MultiMedia Modeling   (MMM2025)</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为WavFusion的多模态语音情感识别框架，解决了有效多模态融合、模态间异质性和判别表示学习等关键研究问题。通过利用门控跨模态注意力机制和模态内同质特征差异学习，WavFusion在基准数据集上的性能优于现有最先进的情绪识别方法。摘要总结了该文的主要贡献和实践结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）是一个具有挑战性和重要性的问题，涉及人类情绪的复杂性和多样性。</li>
<li>多模态融合是解决SER问题的一种方法，但现有技术常常忽略跨模态交互的细节，导致特征表示不佳。</li>
<li>WavFusion框架通过利用门控跨模态注意力机制和模态内同质特征差异学习来解决这些问题。</li>
<li>WavFusion在基准数据集上表现出优于现有最先进的情感识别方法的性能。</li>
<li>捕获微妙的跨模态交互和学习的判别表示对于准确的多模态SER至关重要。</li>
<li>实验结果表明，WavFusion在情绪识别方面超过了现有策略。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0df1a6a899775fd81b0de99c073f600d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4c4257506de3f0b461148c4cd84f8f27.jpg" align="middle">
</details>




<h2 id="Adaptive-Dropout-for-Pruning-Conformers"><a href="#Adaptive-Dropout-for-Pruning-Conformers" class="headerlink" title="Adaptive Dropout for Pruning Conformers"></a>Adaptive Dropout for Pruning Conformers</h2><p><strong>Authors:Yotaro Kubo, Xingyu Cai, Michiel Bacchiani</strong></p>
<p>This paper proposes a method to effectively perform joint training-and-pruning based on adaptive dropout layers with unit-wise retention probabilities. The proposed method is based on the estimation of a unit-wise retention probability in a dropout layer. A unit that is estimated to have a small retention probability can be considered to be prunable. The retention probability of the unit is estimated using back-propagation and the Gumbel-Softmax technique. This pruning method is applied at several application points in Conformers such that the effective number of parameters can be significantly reduced. Specifically, adaptive dropout layers are introduced in three locations in each Conformer block: (a) the hidden layer of the feed-forward-net component, (b) the query vectors and the value vectors of the self-attention component, and (c) the input vectors of the LConv component. The proposed method is evaluated by conducting a speech recognition experiment on the LibriSpeech task. It was shown that this approach could simultaneously achieve a parameter reduction and accuracy improvement. The word error rates improved by approx 1% while reducing the number of parameters by 54%. </p>
<blockquote>
<p>本文提出了一种基于自适应dropout层进行联合训练与剪枝的有效方法，该方法基于单位保留概率的估计。所提出的方法基于dropout层中单位保留概率的估计。估计保留概率较小的单位可以被视为可剪枝的。该单位的保留概率通过使用反向传播和Gumbel-Softmax技术进行估计。这种剪枝方法被应用于Conformers的几个应用点，从而可以显著减少有效参数的数量。具体来说，自适应dropout层在每个Conformer块的三个位置引入：（a）前馈网络组件的隐藏层，（b）自注意力组件的查询向量和值向量，以及（c）LConv组件的输入向量。该方法通过在LibriSpeech任务上进行语音识别实验进行评估。结果表明，该方法可以同时实现参数减少和准确性提高。词错误率提高了约1%，同时减少了54%的参数数量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04836v1">PDF</a> </p>
<p><strong>Summary</strong><br>本文提出一种基于自适应dropout层进行联合训练与剪枝的方法，使用单元级别的保持概率来估计剪枝决策。这种方法用于降低模型参数数量，应用于语音任务如语音识别，实现参数减少与准确率提升的双重效果。在LibriSpeech任务上的实验表明，该方法减少了约54%的参数数量，同时提高了约1%的单词错误率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该方法通过结合自适应dropout层和剪枝策略来实现模型的联合训练与参数优化。</li>
<li>利用单元级别的保持概率估计进行剪枝决策，预计剪枝单元较小的保持概率可进行删除操作。</li>
<li>采用后向传播和Gumbel-Softmax技术来估计单元保持概率。</li>
<li>该方法应用于Conformers中的多个点，包括前馈网络组件的隐藏层、自注意力组件的查询向量和价值向量以及LConv组件的输入向量。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-61de8512ccb50acd2cb7c5d8aea2d931.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6bf7da1d545bdb8cd2fb379b26d92a97.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ee648bf37e10f28cddd66bc5f5dde437.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-be176b09e1c8d3f699cfb2e4948f7438.jpg" align="middle">
</details>




<h2 id="Integrated-Minimum-Mean-Squared-Error-Algorithms-for-Combined-Acoustic-Echo-Cancellation-and-Noise-Reduction"><a href="#Integrated-Minimum-Mean-Squared-Error-Algorithms-for-Combined-Acoustic-Echo-Cancellation-and-Noise-Reduction" class="headerlink" title="Integrated Minimum Mean Squared Error Algorithms for Combined Acoustic   Echo Cancellation and Noise Reduction"></a>Integrated Minimum Mean Squared Error Algorithms for Combined Acoustic   Echo Cancellation and Noise Reduction</h2><p><strong>Authors:Arnout Roebben, Toon van Waterschoot, Jan Wouters, Marc Moonen</strong></p>
<p>In many speech recording applications, noise and acoustic echo corrupt the desired speech. Consequently, combined noise reduction (NR) and acoustic echo cancellation (AEC) is required. Generally, a cascade approach is followed, i.e., the AEC and NR are designed in isolation by selecting a separate signal model, formulating a separate cost function, and using a separate solution strategy. The AEC and NR are then cascaded one after the other, not accounting for their interaction. In this paper, however, an integrated approach is proposed to consider this interaction in a general multi-microphone&#x2F;multi-loudspeaker setup. Therefore, a single signal model of either the microphone signal vector or the extended signal vector, obtained by stacking microphone and loudspeaker signals, is selected, a single mean squared error cost function is formulated, and a common solution strategy is used. Using this microphone signal model, a multi channel Wiener filter (MWF) is derived. Using the extended signal model, an extended MWF (MWFext) is derived, and several equivalent expressions are found, which nevertheless are interpretable as cascade algorithms. Specifically, the MWFext is shown to be equivalent to algorithms where the AEC precedes the NR (AEC NR), the NR precedes the AEC (NR-AEC), and the extended NR (NRext) precedes the AEC and post-filter (PF) (NRext-AECPF). Under rank-deficiency conditions the MWFext is non-unique, such that this equivalence amounts to the expressions being specific, not necessarily minimum-norm solutions for this MWFext. The practical performances nonetheless differ due to non-stationarities and imperfect correlation matrix estimation, resulting in the AEC-NR and NRext-AEC-PF attaining best overall performance. </p>
<blockquote>
<p>在许多语音记录应用中，噪声和声音回声会干扰所需的语音。因此，需要联合降噪（NR）和声音回声消除（AEC）。通常采用的是级联方法，即独立设计AEC和NR，通过选择单独的信号模型、制定单独的代价函数和使用单独的解决方案策略来实现。然后将AEC和NR一个接一个地进行级联，没有考虑到它们之间的相互作用。然而，本文提出一种综合考虑方法，在一般的多个麦克风&#x2F;多个扬声器的设置中来考虑这种相互作用。因此，选择单个信号模型，该模型可以是麦克风信号向量也可以是通过对麦克风和扬声器信号进行堆叠而获得的扩展信号向量。制定一个单一的均方误差代价函数并使用一个通用的解决方案策略。基于这种麦克风信号模型，推导出了多通道维纳滤波器（MWF）。基于扩展的信号模型，推导出了扩展的MWF（MWFext），并找到了几个等效的表达式，这些表达式可以解释为级联算法。具体来说，MWFext相当于AEC先于NR（AEC NR）、NR先于AEC（NR-AEC）以及扩展NR（NRext）先于AEC和后置滤波器（PF）（NRext-AECPF）的算法。在秩不足的情况下，MWFext是非唯一的，因此这种等效表现为表达式特定，不一定是这个MWFext的最小范数解。但由于非平稳性和不完美的相关矩阵估计，实际性能仍然有所不同，使得AEC-NR和NRext-AEC-PF获得最佳的整体性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04267v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>在语音录制应用中，噪声和回声会影响所需的语音质量，因此需要结合降噪（NR）和声回消除（AEC）。通常采用级联方法，即独立设计AEC和NR，选择单独的信号模型、制定单独的成本函数和使用单独的解决方案策略。然后依次进行AEC和NR处理，而不考虑它们的相互作用。然而，本文提出了一种综合考虑这种相互作用的一般的多麦克风&#x2F;多扬声器设置中的集成方法。因此，选择了麦克风信号向量或通过将麦克风和扬声器信号叠加得到的扩展信号向量的单一信号模型，制定了单一均方误差成本函数，并使用通用解决方案策略。基于这种麦克风信号模型，推导出了多通道维纳滤波器（MWF）。基于扩展信号模型，推导出了扩展MWF（MWFext），并找到了几个等效表达式，它们可以解释为级联算法。特别是，MWFext与AEC先于NR（AEC NR）、NR先于AEC（NR-AEC）以及扩展NR（NRext）先于AEC和后滤波器（PF）（NRext-AECPF）的算法等效。在秩不足的情况下，MWFext是非唯一的，这意味着这种等效性表现为表达式的特定性，不一定是MWFext的最小范数解。然而，由于非平稳性和相关矩阵估计的不完美性，它们的实际性能有所不同，使得AEC-NR和NRext-AEC-PF获得最佳的整体性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文提出了一种集成方法，考虑噪声消除和声回消除之间的相互作用。</li>
<li>论文介绍了基于麦克风信号模型的MWF和多通道维纳滤波器扩展版本（MWFext）。</li>
<li>MWFext与不同的级联算法表现出等效性，包括AEC NR、NR-AEC、以及NRext-AECPF。</li>
<li>在秩不足的情况下，MWFext的非唯一性使得等效表达式的适用性受限。</li>
<li>实际性能差异源于非平稳性和相关矩阵估计的不完美性。</li>
<li>AEC-NR和NRext-AEC-PF在处理噪声和回声时表现出最佳的整体性能。</li>
<li>该论文提供了一种有效的方法来考虑噪声消除和声回消除的相互作用，从而提高了语音录制的质量。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b0bac0503dd0460a56d1af13474017f6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14123731d44298749609337b47a3bff9.jpg" align="middle">
</details>




<h2 id="Representation-Purification-for-End-to-End-Speech-Translation"><a href="#Representation-Purification-for-End-to-End-Speech-Translation" class="headerlink" title="Representation Purification for End-to-End Speech Translation"></a>Representation Purification for End-to-End Speech Translation</h2><p><strong>Authors:Chengwei Zhang, Yue Zhou, Rui Zhao, Yidong Chen, Xiaodong Shi</strong></p>
<p>Speech-to-text translation (ST) is a cross-modal task that involves converting spoken language into text in a different language. Previous research primarily focused on enhancing speech translation by facilitating knowledge transfer from machine translation, exploring various methods to bridge the gap between speech and text modalities. Despite substantial progress made, factors in speech that are not relevant to translation content, such as timbre and rhythm, often limit the efficiency of knowledge transfer. In this paper, we conceptualize speech representation as a combination of content-agnostic and content-relevant factors. We examine the impact of content-agnostic factors on translation performance through preliminary experiments and observe a significant performance deterioration when content-agnostic perturbations are introduced to speech signals. To address this issue, we propose a \textbf{S}peech \textbf{R}epresentation \textbf{P}urification with \textbf{S}upervision \textbf{E}nhancement (SRPSE) framework, which excludes the content-agnostic components within speech representations to mitigate their negative impact on ST. Experiments on MuST-C and CoVoST-2 datasets demonstrate that SRPSE significantly improves translation performance across all translation directions in three settings and achieves preeminent performance under a \textit{transcript-free} setting. </p>
<blockquote>
<p>语音到文本的翻译（ST）是一项涉及将口语转换为另一种语言的文本的多模式任务。以前的研究主要集中在通过促进机器翻译的知识转移来提高语音翻译的性能，探索各种方法来弥合语音和文本模式之间的差距。尽管取得了重大进展，但语音中与翻译内容不相关的因素，如音质和节奏，往往限制了知识转移的效率。在本文中，我们将语音表示概念化为与内容无关和与内容相关因素的结合。我们通过初步实验检查了与内容无关因素对翻译性能的影响，并观察到当向语音信号引入与内容无关的干扰时，性能会显著下降。为了解决这一问题，我们提出了一个名为SRPSE（有监督增强的语音表示净化）的框架，该框架排除语音表示中的与内容无关的成分，以减轻它们对ST的负面影响。在MuST-C和CoVoST-2数据集上的实验表明，SRPSE在三种设置的各个翻译方向上显著提高了翻译性能，并在无字幕设置下取得了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04266v1">PDF</a> Accepted by COLING 2025</p>
<p><strong>Summary</strong></p>
<p>语音到文本的翻译（ST）是一个跨模态任务，涉及将口语转化为另一种语言的文本。尽管取得了实质性进展，但语音中那些与翻译内容无关的因素（如音色和节奏）经常限制知识转移的效率。本文概念化语音表示为与内容无关和与内容相关因素的组合，并通过初步实验发现内容无关的干扰对翻译性能有重大影响。为解决这一问题，本文提出了一个名为SRPSE的语音表示净化与监督增强框架，它通过排除语音表示中的与内容无关的成分，减轻其对ST的负面影响。实验表明，SRPSE在三种设置下的所有翻译方向上均显著提高翻译性能，并在无字幕设置下实现卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音到文本的翻译是跨模态任务，涉及将口语转化为另一种语言的文本。</li>
<li>语音中与内容无关的因素（如音色和节奏）对翻译性能有重要影响。</li>
<li>初步实验表明，内容无关的干扰会导致翻译性能显著下降。</li>
<li>为解决此问题，提出了SRPSE框架，旨在排除语音表示中的与内容无关的成分。</li>
<li>SRPSE框架在多个数据集上的实验表明，它在各种翻译设置下显著提高翻译性能。</li>
<li>SRPSE框架在无字幕设置下实现卓越性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-39ac38b427489dc7a007a0d9ea84ab5d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-09646bac044a7c0900950d873f892997.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e328f7574b74ca68355294f5f501c073.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5af9f7e86ca76bf6d23a7f2ba9ab6956.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-45160a2d20550d9ac68b541a38d66491.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-17d59c5bf3e4fbfcccf9c644a1d5f1c3.jpg" align="middle">
</details>




<h2 id="Comprehensive-Audio-Query-Handling-System-with-Integrated-Expert-Models-and-Contextual-Understanding"><a href="#Comprehensive-Audio-Query-Handling-System-with-Integrated-Expert-Models-and-Contextual-Understanding" class="headerlink" title="Comprehensive Audio Query Handling System with Integrated Expert Models   and Contextual Understanding"></a>Comprehensive Audio Query Handling System with Integrated Expert Models   and Contextual Understanding</h2><p><strong>Authors:Vakada Naveen, Arvind Krishna Sridhar, Yinyi Guo, Erik Visser</strong></p>
<p>This paper presents a comprehensive chatbot system designed to handle a wide range of audio-related queries by integrating multiple specialized audio processing models. The proposed system uses an intent classifier, trained on a diverse audio query dataset, to route queries about audio content to expert models such as Automatic Speech Recognition (ASR), Speaker Diarization, Music Identification, and Text-to-Audio generation. A 3.8 B LLM model then takes inputs from an Audio Context Detection (ACD) module extracting audio event information from the audio and post processes text domain outputs from the expert models to compute the final response to the user. We evaluated the system on custom audio tasks and MMAU sound set benchmarks. The custom datasets were motivated by target use cases not covered in industry benchmarks and included ACD-timestamp-QA (Question Answering) as well as ACD-temporal-QA datasets to evaluate timestamp and temporal reasoning questions, respectively. First we determined that a BERT based Intent Classifier outperforms LLM-fewshot intent classifier in routing queries. Experiments further show that our approach significantly improves accuracy on some custom tasks compared to state-of-the-art Large Audio Language Models and outperforms models in the 7B parameter size range on the sound testset of the MMAU benchmark, thereby offering an attractive option for on device deployment. </p>
<blockquote>
<p>本文介绍了一个综合聊天机器人系统，该系统通过集成多个专业音频处理模型，能够处理各种音频相关查询。所提出的系统使用意图分类器（基于多样化的音频查询数据集进行训练），将关于音频内容的查询路由到专家模型，如自动语音识别（ASR）、说话人身份识别、音乐识别和文本到音频生成等。然后，一个3.8 BLLM模型从音频上下文检测（ACD）模块接收输入，该模块从音频中提取音频事件信息，并对专家模型的文本领域输出进行后处理，以计算最终的用户响应。我们在自定义的音频任务和MMAU声音集基准测试上对系统进行了评估。自定义数据集的目标用例是行业基准测试未涵盖的领域，包括ACD时间戳问答（问题回答）以及ACD时间推理问答数据集，以评估时间戳和时间推理问题。首先，我们确定基于BERT的意图分类器在路由查询方面优于LLM少量意图分类器。实验进一步表明，与最新的大型音频语言模型相比，我们的方法在一些自定义任务上的准确性有了显著提高，并且在MMAU基准测试的声音测试集上优于参数大小为7 B的模型，因此成为了设备部署的吸引力选择。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个综合聊天机器人系统，该系统通过集成多个专业音频处理模型，能够处理广泛的音频查询。该系统使用训练于多样化音频查询数据集的意图分类器，将关于音频内容的查询路由到专家模型，如语音识别、说话人划序、音乐识别和文本到音频生成等。利用大型语言模型处理音频上下文检测模块提取的音频事件信息，对专家模型的文本领域输出进行后处理，计算最终的用户响应。在自定义音频任务和MMAU声音集基准测试上对该系统进行了评估，证明了其在某些自定义任务上的准确性显著提高，且在MMAU基准测试的声音测试集上优于其他大型音频语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文介绍了一个聊天机器人系统，能处理广泛的音频查询。</li>
<li>系统通过集成多个专业音频处理模型来实现其功能。</li>
<li>意图分类器用于将音频查询路由到相应的专家模型。</li>
<li>大型语言模型用于处理音频上下文信息并计算最终响应。</li>
<li>在自定义音频任务和MMAU基准测试上进行了系统评估。</li>
<li>BERT基础的意图分类器在路由查询方面优于LLM少样本意图分类器。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6ea15997db5c8d3e6ea3d6c6aebc4bca.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fd5a5f62d5c97ab57d5457cf39c1bdef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-111b01e077285fcdd1c99c2c6fbc629e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-08280d82e3327f1edc7431017d601405.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78c6d808f1efd9d28a8876201792e70a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b352bb214a115912ede7ef7af5bd0e86.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e6a3c4fbff638e8c007aac6b000881c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9b5424f03cea9eaff3d8bff9afb8b2be.jpg" align="middle">
</details>




<h2 id="ASR-EC-Benchmark-Evaluating-Large-Language-Models-on-Chinese-ASR-Error-Correction"><a href="#ASR-EC-Benchmark-Evaluating-Large-Language-Models-on-Chinese-ASR-Error-Correction" class="headerlink" title="ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error   Correction"></a>ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error   Correction</h2><p><strong>Authors:Victor Junqiu Wei, Weicheng Wang, Di Jiang, Yuanfeng Song, Lu Wang</strong></p>
<p>Automatic speech Recognition (ASR) is a fundamental and important task in the field of speech and natural language processing. It is an inherent building block in many applications such as voice assistant, speech translation, etc. Despite the advancement of ASR technologies in recent years, it is still inevitable for modern ASR systems to have a substantial number of erroneous recognition due to environmental noise, ambiguity, etc. Therefore, the error correction in ASR is crucial.   Motivated by this, this paper studies ASR error correction in the Chinese language, which is one of the most popular languages and enjoys a large number of users in the world. We first create a benchmark dataset named \emph{ASR-EC} that contains a wide spectrum of ASR errors generated by industry-grade ASR systems. To the best of our knowledge, it is the first Chinese ASR error correction benchmark. Then, inspired by the recent advances in \emph{large language models (LLMs)}, we investigate how to harness the power of LLMs to correct ASR errors. We apply LLMs to ASR error correction in three paradigms. The first paradigm is prompting, which is further categorized as zero-shot, few-shot, and multi-step. The second paradigm is finetuning, which finetunes LLMs with ASR error correction data. The third paradigm is multi-modal augmentation, which collectively utilizes the audio and ASR transcripts for error correction. Extensive experiments reveal that prompting is not effective for ASR error correction. Finetuning is effective only for a portion of LLMs. Multi-modal augmentation is the most effective method for error correction and achieves state-of-the-art performance. </p>
<blockquote>
<p>自动语音识别（ASR）是语音和自然语言处理领域的一项基本且重要的任务。它是许多应用程序（如语音助手、语音识别等）中的固有组成部分。尽管近年来ASR技术取得了进展，但由于环境噪声、歧义等因素，现代ASR系统仍然存在大量错误识别。因此，ASR中的错误校正至关重要。本文受到此启发，研究中文ASR错误校正。中文是世界上最流行、用户数量最多的语言之一。首先，我们创建了一个名为ASR-EC的基准数据集，其中包含由工业级ASR系统生成的广泛ASR错误。据我们所知，这是第一个中文ASR错误校正基准。然后，受到大型语言模型（LLM）的最新进展的启发，我们研究如何利用LLM的力量来纠正ASR错误。我们将LLM应用于ASR错误校正的三种范式中。第一种范式是提示，它进一步分为零样本、少样本和多步提示。第二种范式是微调，它使用ASR错误校正数据对LLM进行微调。第三种范式是多模态增强，它结合音频和ASR转录进行错误校正。大量实验表明，提示对于ASR错误校正并不有效。微调只对部分LLM有效。多模态增强是最有效的错误校正方法，并达到了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要研究了基于大规模语言模型（LLMs）的中文语音识别（ASR）错误校正问题。创建了首个中文ASR错误校正基准数据集ASR-EC，并探索了三种利用LLMs进行ASR错误校正的方法：提示法、微调法和多模态增强法。实验表明，多模态增强法是最有效的错误校正方法，并达到了目前最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文研究了中文语音识别（ASR）错误校正问题，创建了首个相关基准数据集ASR-EC。</li>
<li>利用大规模语言模型（LLMs）进行ASR错误校正的方法被探索。</li>
<li>提出了三种利用LLMs进行ASR错误校正的方法：提示法、微调法和多模态增强法。</li>
<li>提示法对于ASR错误校正并不有效。</li>
<li>微调法仅对部分LLMs有效。</li>
<li>多模态增强法是最有效的ASR错误校正方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5cfdd3e4dfcb504e0d792d9aa4f1508a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2d16c81aced7163592b924926cbace76.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-57fa4ca173e8ebef5677b3d179071787.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d526ca774f9a372b3fa3b1ac4e729bc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e6691038694aab6dfdf31531c021015b.jpg" align="middle">
</details>




<h2 id="GLM-4-Voice-Towards-Intelligent-and-Human-Like-End-to-End-Spoken-Chatbot"><a href="#GLM-4-Voice-Towards-Intelligent-and-Human-Like-End-to-End-Spoken-Chatbot" class="headerlink" title="GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken   Chatbot"></a>GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken   Chatbot</h2><p><strong>Authors:Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, Jie Tang</strong></p>
<p>We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice">https://github.com/THUDM/GLM-4-Voice</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/glm-4-voice-9b">https://huggingface.co/THUDM/glm-4-voice-9b</a>. </p>
<blockquote>
<p>我们介绍了GLM-4-Voice，这是一个智能且人性化的端到端语音聊天机器人。它支持中文和英语，能进行实时语音对话，并根据用户指令变化情绪、语调、语速和方言等语音细微差别。GLM-4-Voice使用超低比特率（175bps）的单码本语音标记器，以12.5Hz的帧率从自动语音识别（ASR）模型中衍生出来，通过在编码器中加入向量量化瓶颈。为了有效地将知识从文本转移到语音模式，我们利用文本到令牌模型，通过现有文本预训练语料库合成语音文本交织数据。我们继续使用预训练的文本语言模型GLM-4-9B进行预训练，结合无监督语音数据、交织语音文本数据和监督语音文本数据，扩展到1万亿个令牌，在语音语言建模和语音问答方面都达到了最先进的性能。然后，我们使用高质量的对话语音数据对预训练模型进行微调，在对话能力和语音质量方面都达到了优于现有基准测试的性能。开放模型可通过<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice%E5%92%8Chttps://huggingface.co/THUDM/glm-4-voice-9b%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/THUDM/GLM-4-Voice和https://huggingface.co/THUDM/glm-4-voice-9b访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GLM-4-Voice是一款智能、人性化的端到端语音聊天机器人，支持中英文实时语音对话，可根据用户指令调整语音情感、语调、语速和方言。它采用超低比特率（175bps）和12.5Hz帧率的单编码本语音标记器，结合自动语音识别（ASR）模型，融入向量量化瓶颈到编码器。通过合成语音文本交织数据，从现有文本预训练语料库中高效转换知识。预训练模型结合无监督语音数据、交织语音文本数据和监督语音文本数据，扩展到1万亿个令牌，在语音语言建模和语音问答方面达到最新技术水平。通过高质量对话语音数据进行微调，达到对话能力和语音质量方面的卓越性能。公开模型可通过<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice%E5%92%8Chttps://huggingface.co/THUDM/glm-4-voice-9b%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/THUDM/GLM-4-Voice和https://huggingface.co/THUDM/glm-4-voice-9b访问。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLM-4-Voice是一个智能、人性化的端到端语音聊天机器人，支持中英文实时对话。</li>
<li>GLM-4-Voice能根据用户指令调整语音情感、语调、语速和方言。</li>
<li>它采用超低比特率和帧率的单编码本语音标记器技术结合ASR模型。</li>
<li>通过合成语音文本交织数据实现从文本到语音的高效知识转换。</li>
<li>预训练模型结合了多种数据，包括无监督、交织和监督的语音文本数据。</li>
<li>GLM-4-Voice在语音语言建模和语音问答方面达到了最新技术水平。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93d5efd16c01204f21a643086af317c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-908c87d006e309e0892b8537872b60db.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-77dae6055a3d14522e2162907a1c2c7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0b8d53c63f5d7628dd1b39b898e202ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f88916a03b6edfe48920863e0a168c23.jpg" align="middle">
</details>




<h2 id="It-Takes-Two-Real-time-Co-Speech-Two-person’s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model"><a href="#It-Takes-Two-Real-time-Co-Speech-Two-person’s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model" class="headerlink" title="It Takes Two: Real-time Co-Speech Two-person’s Interaction Generation   via Reactive Auto-regressive Diffusion Model"></a>It Takes Two: Real-time Co-Speech Two-person’s Interaction Generation   via Reactive Auto-regressive Diffusion Model</h2><p><strong>Authors:Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, Taku Komura</strong></p>
<p>Conversational scenarios are very common in real-world settings, yet existing co-speech motion synthesis approaches often fall short in these contexts, where one person’s audio and gestures will influence the other’s responses. Additionally, most existing methods rely on offline sequence-to-sequence frameworks, which are unsuitable for online applications. In this work, we introduce an audio-driven, auto-regressive system designed to synthesize dynamic movements for two characters during a conversation. At the core of our approach is a diffusion-based full-body motion synthesis model, which is conditioned on the past states of both characters, speech audio, and a task-oriented motion trajectory input, allowing for flexible spatial control. To enhance the model’s ability to learn diverse interactions, we have enriched existing two-person conversational motion datasets with more dynamic and interactive motions. We evaluate our system through multiple experiments to show it outperforms across a variety of tasks, including single and two-person co-speech motion generation, as well as interactive motion generation. To the best of our knowledge, this is the first system capable of generating interactive full-body motions for two characters from speech in an online manner. </p>
<blockquote>
<p>对话场景在真实世界环境中非常常见。然而，现有的协同语音运动合成方法在这种情况下往往表现不足，一个人的音频和手势会影响另一个人的反应。此外，大多数现有方法依赖于离线序列到序列框架，这不适用于在线应用。在这项工作中，我们引入了一个音频驱动的自动回归系统，旨在合成对话期间两个角色的动态动作。我们的方法的核心是一个基于扩散的全身运动合成模型，该模型受两个角色过去状态、语音音频和面向任务的运动轨迹输入的制约，可实现灵活的空间控制。为了提高模型学习各种交互的能力，我们丰富了现有的两人对话运动数据集，加入了更多动态和交互性的动作。我们通过多次实验评估了我们的系统，结果表明它在各种任务上的表现都优于其他系统，包括单人及两人协同语音运动生成以及交互运动生成。据我们所知，这是第一个能够以在线方式从语音为两个角色生成交互全身动作的系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02419v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于音频的、自回归系统，用于合成对话场景中的两个角色的动态动作。该系统采用扩散模型为基础的全身动作合成模型，根据对话双方过去的状态、语音音频和任务导向的动作轨迹输入进行条件控制，实现灵活的空间控制。通过丰富两人对话动作数据集，提高了模型学习各种互动的能力。实验表明，该系统在单人及双人共语动作生成、互动动作生成等任务上的表现均优于其他方法。这是首个能够在线生成基于语音的两人互动全身动作的系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该系统是一种基于音频的、自回归的在线对话动作合成方法。</li>
<li>系统采用扩散模型为基础的全身动作合成模型。</li>
<li>系统根据对话双方过去的状态、语音音频和任务导向的动作轨迹进行条件控制。</li>
<li>系统可实现灵活的空间控制，适用于对话场景的动作合成。</li>
<li>通过丰富两人对话动作数据集，提高模型学习各种互动的能力。</li>
<li>实验表明，该系统在多种任务上的表现均优于其他方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0f6255b72b8539d8aab13e42acd7a48.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-54ef72cfbbecac1eb389d95bcf9efdce.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-023425356e8d0b3df9cd25fa3d3bf131.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8cf361129e15b5b04e85bcc554daf426.jpg" align="middle">
</details>




<h2 id="Unveiling-Interpretability-in-Self-Supervised-Speech-Representations-for-Parkinson’s-Diagnosis"><a href="#Unveiling-Interpretability-in-Self-Supervised-Speech-Representations-for-Parkinson’s-Diagnosis" class="headerlink" title="Unveiling Interpretability in Self-Supervised Speech Representations for   Parkinson’s Diagnosis"></a>Unveiling Interpretability in Self-Supervised Speech Representations for   Parkinson’s Diagnosis</h2><p><strong>Authors:David Gimeno-Gómez, Catarina Botelho, Anna Pompili, Alberto Abad, Carlos-D. Martínez-Hinarejos</strong></p>
<p>Recent works in pathological speech analysis have increasingly relied on powerful self-supervised speech representations, leading to promising results. However, the complex, black-box nature of these embeddings and the limited research on their interpretability significantly restrict their adoption for clinical diagnosis. To address this gap, we propose a novel, interpretable framework specifically designed to support Parkinson’s Disease (PD) diagnosis. Through the design of simple yet effective cross-attention mechanisms for both embedding- and temporal-level analysis, the proposed framework offers interpretability from two distinct but complementary perspectives. Experimental findings across five well-established speech benchmarks for PD detection demonstrate the framework’s capability to identify meaningful speech patterns within self-supervised representations for a wide range of assessment tasks. Fine-grained temporal analyses further underscore its potential to enhance the interpretability of deep-learning pathological speech models, paving the way for the development of more transparent, trustworthy, and clinically applicable computer-assisted diagnosis systems in this domain. Moreover, in terms of classification accuracy, our method achieves results competitive with state-of-the-art approaches, while also demonstrating robustness in cross-lingual scenarios when applied to spontaneous speech production. </p>
<blockquote>
<p>近期病理语音分析的研究越来越依赖于强大的自监督语音表征，这带来了充满希望的结果。然而，这些嵌入的复杂性和黑箱性质以及对其解释性的研究有限，显著限制了它们在临床诊断中的应用。为了弥补这一空白，我们提出了一种新型的可解释框架，专门设计用于支持帕金森氏症（PD）的诊断。通过设计简单而有效的跨注意机制，用于嵌入级和时间级的分析，所提出的框架从两个独特但互补的角度提供了可解释性。在五个成熟的帕金森氏症检测语音基准测试上的实验结果表明，该框架能够在自监督表征中识别各种评估任务内的重要语音模式。精细的时间分析进一步强调了其在提高病理语音深度模型的解释潜力，为开发该领域更透明、更可靠的临床适用计算机辅助诊断系统铺平了道路。此外，在分类准确度方面，我们的方法实现了与国家前沿方法相竞争的结果，并且在应用于自然语音生产时表现出跨语言的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02006v1">PDF</a> Submitted to the Special Issue on “Modelling and Processing Language   and Speech in Neurodegenerative Disorders” published by Journal of Selected   Topics in Signal Processing (JSTSP)</p>
<p><strong>Summary</strong><br>帕金森病的病理语音分析新工作提出一种新型可解释的框架，旨在支持帕金森病诊断。该框架设计简单有效的跨注意力机制，从嵌入和时序两个角度进行分析，提高自监督语音表示的模型解释性。实验结果表明，该框架能够在多个公认的帕金森病语音基准测试中识别有意义的语音模式，且时序分析增强了深度学习的语音模型的解释性。同时，该方法分类准确度高，且在跨语言场景下表现出稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近年来的病理语音分析工作越来越依赖强大的自监督语音表示，取得了令人鼓舞的结果。</li>
<li>当前方法存在解释性不足的问题，限制了其在临床诊断中的应用。</li>
<li>提出了一种新型可解释的框架，旨在支持帕金森病诊断。</li>
<li>通过嵌入和时序两个角度的跨注意力机制分析，提高模型的解释性。</li>
<li>实验结果表明该框架在多个帕金森语音基准测试中表现优异。</li>
<li>时序分析增强了深度学习的语音模型的解释性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-46759d6ddcfa7c3e6216ac08066c4b10.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-511c3dae0a8e2843336f591b8c5bde94.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fe157559811021ad650d837179ade7fa.jpg" align="middle">
</details>




<h2 id="Late-fusion-ensembles-for-speech-recognition-on-diverse-input-audio-representations"><a href="#Late-fusion-ensembles-for-speech-recognition-on-diverse-input-audio-representations" class="headerlink" title="Late fusion ensembles for speech recognition on diverse input audio   representations"></a>Late fusion ensembles for speech recognition on diverse input audio   representations</h2><p><strong>Authors:Marin Jezidžić, Matej Mihelčić</strong></p>
<p>We explore diverse representations of speech audio, and their effect on a performance of late fusion ensemble of E-Branchformer models, applied to Automatic Speech Recognition (ASR) task. Although it is generally known that ensemble methods often improve the performance of the system even for speech recognition, it is very interesting to explore how ensembles of complex state-of-the-art models, such as medium-sized and large E-Branchformers, cope in this setting when their base models are trained on diverse representations of the input speech audio. The results are evaluated on four widely-used benchmark datasets: \textit{Librispeech, Aishell, Gigaspeech}, \textit{TEDLIUMv2} and show that improvements of $1% - 14%$ can still be achieved over the state-of-the-art models trained using comparable techniques on these datasets. A noteworthy observation is that such ensemble offers improvements even with the use of language models, although the gap is closing. </p>
<blockquote>
<p>我们探索了语音音频的多种表示形式，以及它们对E-Branchformer模型晚期融合集成在自动语音识别（ASR）任务性能的影响。虽然集成方法通常会提高系统的性能，即使在语音识别方面也是如此，但探索复杂的最先进模型的集成方法非常有趣，例如在中型和大型E-Branchformers中，当它们的基准模型在输入语音音频的多种表示上进行训练时，它们如何应对这种情况。在四个广泛使用的基准数据集：Librispeech、Aishell、Gigaspeech和TEDLIUMv2上的评估结果表明，与在这些数据集上使用类似技术训练的最先进模型相比，仍可实现1%~14%的改进。值得注意的是，即使使用语言模型，这种集成也能提供改进，尽管差距正在缩小。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01861v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文主要探讨了语音音频的多样化表示形式，以及它们对E-Branchformer模型的后期融合组合在自动语音识别（ASR）任务中的影响。虽然已知集合方法通常会提高系统的性能，但对于在复杂先进的模型（如中等规模和大型E-Branchformers）中如何应对训练有语音音频多样化表示的基准模型时，仍然非常有趣。在四个广泛使用的基准数据集上的评估结果表明，与在这些数据集上使用类似技术训练的最新模型相比，改进幅度可达1%-14%。值得注意的是，即使使用语言模型，这种组合也能实现改进，尽管差距正在缩小。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>探讨了语音音频的多样化表示形式对自动语音识别任务的影响。</li>
<li>研究了E-Branchformer模型的后期融合组合的效果。</li>
<li>集合方法可以提高系统的性能，特别是在复杂先进的模型中。</li>
<li>在四个广泛使用的基准数据集上进行了评估，包括Librispeech、Aishell、Gigaspeech和TEDLIUMv2。</li>
<li>与现有技术相比，改进范围可达1%-14%。</li>
<li>集合方法即使在使用语言模型的情况下也能提供改进。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4f334cb19a303c432f63b6b07a1be65.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-01160fccb4bccc09d32d1d30b9e5ec24.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-950a66d6dfbce047d79686b14b4713ab.jpg" align="middle">
</details>




<h2 id="Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data"><a href="#Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data" class="headerlink" title="Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data"></a>Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data</h2><p><strong>Authors:Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</strong></p>
<p>The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni%7D">https://huggingface.co/spaces/KE-Team/KE-Omni}</a>. </p>
<blockquote>
<p>GPT-4o代表了通过语音与大型语言模型（LLM）进行实时交互的一个重要里程碑。其显著的低延迟和高流利性不仅引起了关注，还刺激了该领域的研究兴趣。这种实时语音交互在需要快速反馈和即时响应的场景中尤其有价值，能极大地提升用户体验。然而，关于实时大型语音语言模型的研究相对较少，尤其是针对中文的研究。在这项工作中，我们推出了KE-Omni，这是一款无缝大型语音语言模型，基于Ke-SpeechChat构建。Ke-SpeechChat是一个大规模高质量合成语音交互数据集，包含700万中文和英文对话，涉及42,002名发言者，总计超过6万小时，为该领域的研究和发展做出了重大贡献。演示网址为：[<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni]%E3%80%82">https://huggingface.co/spaces/KE-Team/KE-Omni]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01078v2">PDF</a> KE-Omni, Ke-SpeechChat</p>
<p><strong>Summary</strong></p>
<p>GPT-4o实现了通过语音与大型语言模型（LLM）的实时交互，其低延迟和高流畅度引人注目，并刺激了相关领域的研究兴趣。特别是在需要快速反馈和即时响应的场景中，实时语音交互能大幅提升用户体验。本研究推出KE-Omni大型语音语言模型，基于包含7百万中英文对话的Ke-SpeechChat数据集，包含42,002位发言者的超过6万小时语音，为相关领域的研究和发展做出了重要贡献。演示可通过链接访问：[链接地址]。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4o实现了实时语音与大型语言模型的交互。</li>
<li>GPT-4o具有显著的低延迟和高流畅度。</li>
<li>实时语音交互在需要快速反馈和响应的场景中价值显著。</li>
<li>KE-Omni模型是基于Ke-SpeechChat数据集的的大型语音语言模型。</li>
<li>Ke-SpeechChat数据集包含7百万中英文对话，超过6万小时语音。</li>
<li>KE-Omni对相关领域的研究和发展做出了重要贡献。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-be63d03367d1646c10129ffa39a43920.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-40f6a84b1329d36c056c958d085b932a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e846567311c2df8847f41091a8724d2.jpg" align="middle">
</details>




<h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p>
<p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. </p>
<blockquote>
<p>随着基于扩散的生成模型的快速发展，肖像动画图像已经取得了显著的研究成果。然而，由于它的迭代采样特性，它在时序一致的视频生成和快速采样方面仍然面临挑战。本文提出了FLOAT，一种基于流匹配生成模型的音频驱动谈话肖像视频生成方法。我们将生成模型从基于像素的潜在空间转移到学习的运动潜在空间，实现了时序一致运动的有效设计。为此，我们引入了一个基于变压器的矢量场预测器，并设计了一个简单有效的帧条件机制。此外，我们的方法支持语音驱动的情绪增强，能够实现表达性运动的自然融合。大量实验表明，我们的方法在视觉质量、运动保真度和效率方面超越了最先进的音频驱动谈话肖像方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p>
<p><strong>Summary</strong></p>
<p>随着扩散生成模型的快速发展，肖像图像动画已取得了显著成果，但仍面临视频生成的时序一致性和快速采样方面的挑战。本文提出了基于流匹配生成模型的音频驱动肖像视频生成方法FLOAT。该方法将生成建模从像素级的潜在空间转移到学习到的运动潜在空间，实现了时序一致运动的有效设计。为此，引入了一个基于变换器的向量场预测器，并采用了简单有效的帧条件机制。此外，该方法还支持语音驱动的情感增强，能够自然地融入表达性动作。实验证明，该方法在视觉质量、运动保真度和效率方面优于现有音频驱动的肖像动画方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散生成模型的快速发展推动了肖像图像动画的显著进步。</li>
<li>肖像图像动画仍面临视频生成的时序一致性和快速采样挑战。</li>
<li>FLOAT方法是一种基于流匹配生成模型的音频驱动肖像视频生成方法。</li>
<li>FLOAT通过将生成建模转移到学习到的运动潜在空间，实现了时序一致运动的有效设计。</li>
<li>该方法使用基于变换器的向量场预测器，并引入帧条件机制以提高性能。</li>
<li>FLOAT支持语音驱动的情感增强，使动作表达更自然。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4c48114dadf2693ebec847e1f4161b7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-932be8d0b3a67e4d27e3b20089557ffb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8c0ae55682bf86c0002a9cdf127c986e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c66bfe10dfd878860a466abb92c19030.jpg" align="middle">
</details>




<h2 id="Automating-Feedback-Analysis-in-Surgical-Training-Detection-Categorization-and-Assessment"><a href="#Automating-Feedback-Analysis-in-Surgical-Training-Detection-Categorization-and-Assessment" class="headerlink" title="Automating Feedback Analysis in Surgical Training: Detection,   Categorization, and Assessment"></a>Automating Feedback Analysis in Surgical Training: Detection,   Categorization, and Assessment</h2><p><strong>Authors:Firdavs Nasriddinov, Rafal Kocielnik, Arushi Gupta, Cherine Yang, Elyssa Wong, Anima Anandkumar, Andrew Hung</strong></p>
<p>This work introduces the first framework for reconstructing surgical dialogue from unstructured real-world recordings, which is crucial for characterizing teaching tasks. In surgical training, the formative verbal feedback that trainers provide to trainees during live surgeries is crucial for ensuring safety, correcting behavior immediately, and facilitating long-term skill acquisition. However, analyzing and quantifying this feedback is challenging due to its unstructured and specialized nature. Automated systems are essential to manage these complexities at scale, allowing for the creation of structured datasets that enhance feedback analysis and improve surgical education. Our framework integrates voice activity detection, speaker diarization, and automated speech recaognition, with a novel enhancement that 1) removes hallucinations (non-existent utterances generated during speech recognition fueled by noise in the operating room) and 2) separates speech from trainers and trainees using few-shot voice samples. These aspects are vital for reconstructing accurate surgical dialogues and understanding the roles of operating room participants. Using data from 33 real-world surgeries, we demonstrated the system’s capability to reconstruct surgical teaching dialogues and detect feedback instances effectively (F1 score of 0.79+&#x2F;-0.07). Moreover, our hallucination removal step improves feedback detection performance by ~14%. Evaluation on downstream clinically relevant tasks of predicting Behavioral Adjustment of trainees and classifying Technical feedback, showed performances comparable to manual annotations with F1 scores of 0.82+&#x2F;0.03 and 0.81+&#x2F;0.03 respectively. These results highlight the effectiveness of our framework in supporting clinically relevant tasks and improving over manual methods. </p>
<blockquote>
<p>本文介绍了一个从非结构化的现实世界录音中重建手术对话的首个框架，这对于描述教学任务至关重要。在手术训练中，培训人员在现场手术中向受训人员提供的形成性口头反馈对于确保安全、立即纠正行为和促进长期技能获取至关重要。然而，由于其非结构化和专业化的特点，分析和量化这种反馈具有挑战性。自动系统在大规模管理这些复杂性方面至关重要，能够创建增强反馈分析和改善手术教育的结构化数据集。我们的框架集成了语音活动检测、说话人识别和自动语音识别，以及一种新颖的提升方法，即1）消除幻听（在手术室噪音驱动的语音识别过程中产生的不存在的讲话）和2）使用少量的语音样本将培训师和受训人员的语音分开。这些方面对于重建准确的手术对话和理解手术室参与者的角色至关重要。我们使用来自33场真实手术的数据，证明了该系统在重建手术教学对话和有效检测反馈实例方面的能力（F1分数为0.79+&#x2F;-0.07）。此外，我们的幻觉消除步骤提高了约14%的反馈检测性能。在对预测受训者行为调整的下游临床相关任务以及对技术反馈的分类评估中，与手动注释相比，F1分数分别为0.82+&#x2F;0.03和0.81+&#x2F;0.03。这些结果凸显了我们的框架在支持临床相关任务和改进手动方法方面的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00760v1">PDF</a> Accepted as a proceedings paper at Machine Learning for Health 2024</p>
<p><strong>Summary</strong></p>
<p>该文介绍了首个从现实世界的非结构化录音中重建手术对话的框架，这对于刻画教学任务至关重要。在手术培训中，培训师在现场手术中向受训人员提供的形成性口头反馈对于确保安全、立即纠正行为和促进长期技能获取至关重要。然而，由于反馈的非结构化和专业化特性，分析和量化这些反馈具有挑战性。自动化系统是大规模管理这些复杂性的关键，可以创建增强反馈分析和改善手术教育的结构化数据集。该框架集成了语音活动检测、说话人识别和自动语音识别，并有一种新颖的提升，即消除幻听（由手术室噪音引起的在语音识别过程中产生的非存在性发言）以及使用少数语音样本分离训练师和受训人员的语音。这些方面对于重建准确的手术对话和理解手术室参与者的角色至关重要。使用来自33场真实手术的数据，我们展示了该系统重建手术教学对话和检测反馈实例的能力（F1分数为0.79+&#x2F;-0.07）。此外，我们的幻听消除步骤提高了反馈检测性能约14%。对预测受训者行为调整和分类技术反馈等临床相关任务的评估显示，其性能与手动注释相当，F1分数分别为0.82+&#x2F;0.03和0.81+&#x2F;0.03。这些结果突显了我们的框架在支持临床相关任务和改善手动方法方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文介绍了首个针对教学任务的手术对话重建框架。</li>
<li>自动化系统是处理复杂的手术环境的关键，有助于分析和量化反馈。</li>
<li>该框架集成了多项技术，包括语音活动检测、说话人识别和自动语音识别。</li>
<li>系统具备消除幻听的能力，即在语音识别过程中由于手术室噪音产生的非存在性发言。</li>
<li>通过使用少数语音样本，能够分离训练师和受训人员的语音。</li>
<li>使用真实手术数据测试了系统的性能，在反馈实例检测方面表现出较高的准确性（F1分数为0.79+&#x2F;-0.07）。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-709cd260ad1b25b52129d6640b98474a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-51acc9169d22aa02a2423dd32aa7352c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-91d469f8293cca322e89e03c1eb1df77.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-84951b39a79afb3a33830614f3799e33.jpg" align="middle">
</details>




<h2 id="A-Comparative-Study-of-LLM-based-ASR-and-Whisper-in-Low-Resource-and-Code-Switching-Scenario"><a href="#A-Comparative-Study-of-LLM-based-ASR-and-Whisper-in-Low-Resource-and-Code-Switching-Scenario" class="headerlink" title="A Comparative Study of LLM-based ASR and Whisper in Low Resource and   Code Switching Scenario"></a>A Comparative Study of LLM-based ASR and Whisper in Low Resource and   Code Switching Scenario</h2><p><strong>Authors:Zheshu Song, Ziyang Ma, Yifan Yang, Jianheng Zhuo, Xie Chen</strong></p>
<p>Large Language Models (LLMs) have showcased exceptional performance across diverse NLP tasks, and their integration with speech encoder is rapidly emerging as a dominant trend in the Automatic Speech Recognition (ASR) field. Previous works mainly concentrated on leveraging LLMs for speech recognition in English and Chinese. However, their potential for addressing speech recognition challenges in low resource settings remains underexplored. Hence, in this work, we aim to explore the capability of LLMs in low resource ASR and Mandarin-English code switching ASR. We also evaluate and compare the recognition performance of LLM-based ASR systems against Whisper model. Extensive experiments demonstrate that LLM-based ASR yields a relative gain of 12.8% over the Whisper model in low resource ASR while Whisper performs better in Mandarin-English code switching ASR. We hope that this study could shed light on ASR for low resource scenarios. </p>
<blockquote>
<p>大型语言模型（LLMs）在多种NLP任务中表现出卓越的性能，它们与语音编码器的集成正在迅速成为自动语音识别（ASR）领域的主流趋势。之前的工作主要集中在利用LLMs进行英语和中文的语音识别。然而，它们在解决低资源设置中的语音识别挑战方面的潜力尚未得到充分探索。因此，在这项工作中，我们旨在探索LLMs在低资源ASR以及普通话-英语切换ASR中的能力。我们还评估和比较了基于LLM的ASR系统与Whisper模型的识别性能。大量实验表明，在低资源ASR中，基于LLM的ASR系统相对于Whisper模型有12.8%的相对增益，而在普通话-英语切换ASR中，Whisper的表现更好。我们希望这项研究能为低资源场景的ASR提供一些启示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00721v2">PDF</a> This work hasn’t been finished yet</p>
<p><strong>总结</strong></p>
<p>大型语言模型（LLMs）在多种自然语言处理任务中表现出卓越性能，其与语音编码器的集成正在迅速成为自动语音识别（ASR）领域的主流趋势。尽管之前的研究主要集中在利用LLMs进行英语和中文的语音识别，但它们在低资源环境下的语音识别挑战的潜力尚未得到充分探索。本研究旨在探索LLMs在低资源ASR和普通话-英语代码切换ASR中的能力，并评估其与Whisper模型的识别性能。实验表明，基于LLMs的ASR系统在低资源ASR上相对于Whisper模型有12.8%的相对增益，而在普通话-英语代码切换ASR中，Whisper表现更好。</p>
<p><strong>要点</strong></p>
<ol>
<li>大型语言模型（LLMs）在自动语音识别（ASR）领域具有显著优势，特别是在低资源环境中。</li>
<li>LLMs与语音编码器的集成是ASR领域的新兴趋势。</li>
<li>之前的研究主要集中在英语和中文的语音识别，但对低资源环境下的语音识别挑战的研究仍然不足。</li>
<li>基于LLMs的ASR系统在低资源环境中的性能相较于Whisper模型有显著提升。</li>
<li>在普通话-英语代码切换的ASR场景下，Whisper模型的性能表现较好。</li>
<li>本研究为低资源环境下的ASR提供了新的见解和研究方向。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fe645bf94c30ad3702114b19ac80b669.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b8d372efae3a6582931743a84ff27482.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5a01d49768b73600fe7df6b6baa34152.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-84166358babce76b74c378b975e3c84f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-538b5c6e21fb306653e7aa8db9cf3562.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-230a47d672931d3003c9b852dfe1d174.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c33af65f2b1027dfecbe2133fbb3e449.jpg" align="middle">
</details>




<h2 id="Empowering-the-Deaf-and-Hard-of-Hearing-Community-Enhancing-Video-Captions-Using-Large-Language-Models"><a href="#Empowering-the-Deaf-and-Hard-of-Hearing-Community-Enhancing-Video-Captions-Using-Large-Language-Models" class="headerlink" title="Empowering the Deaf and Hard of Hearing Community: Enhancing Video   Captions Using Large Language Models"></a>Empowering the Deaf and Hard of Hearing Community: Enhancing Video   Captions Using Large Language Models</h2><p><strong>Authors:Nadeen Fathallah, Monika Bhole, Steffen Staab</strong></p>
<p>In today’s digital age, video content is prevalent, serving as a primary source of information, education, and entertainment. However, the Deaf and Hard of Hearing (DHH) community often faces significant challenges in accessing video content due to the inadequacy of automatic speech recognition (ASR) systems in providing accurate and reliable captions. This paper addresses the urgent need to improve video caption quality by leveraging Large Language Models (LLMs). We present a comprehensive study that explores the integration of LLMs to enhance the accuracy and context-awareness of captions generated by ASR systems. Our methodology involves a novel pipeline that corrects ASR-generated captions using advanced LLMs. It explicitly focuses on models like GPT-3.5 and Llama2-13B due to their robust performance in language comprehension and generation tasks. We introduce a dataset representative of real-world challenges the DHH community faces to evaluate our proposed pipeline. Our results indicate that LLM-enhanced captions significantly improve accuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by ChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%), ChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the original ASR captions. </p>
<blockquote>
<p>在如今的数字化时代，视频内容普遍存在，成为信息、教育和娱乐的主要来源。然而，聋哑人群体在获取视频内容时常常面临重大挑战，这是因为自动语音识别（ASR）系统在提供准确可靠的字幕方面存在不足。本文针对利用大型语言模型（LLM）提高视频字幕质量的紧迫需求。我们进行了全面的研究，探索了将LLM集成到ASR系统中以提高生成字幕的准确性和上下文感知能力的方法。我们的方法包括使用先进的大型语言模型来校正ASR生成的字幕，重点关注GPT-3.5和Llama2-13B等模型，这些模型在理解和生成任务中表现出强大的性能。为了评估我们提出的管道，我们引入了一个代表真实世界挑战的数据集，该数据集反映了聋哑群体所面临的挑战。结果表明，大型语言模型增强的字幕显著提高准确性，以ChatGPT-3.5为例，其词错误率（WER）显著低于原始ASR字幕（WER：9.75%），相较于原始ASR字幕，ChatGPT-3.5的词错误率降低了约57.72%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在数字时代，视频内容广泛存在，作为信息、教育和娱乐的主要来源。然而，聋哑人群在获取视频内容时常常面临挑战，因为自动语音识别（ASR）系统在提供准确可靠的字幕方面存在不足。本文着重解决提高视频字幕质量的紧迫需求，通过利用大型语言模型（LLMs）。本文呈现了一项综合性研究，探讨了整合LLMs以提高ASR系统生成字幕的准确性和上下文意识。我们的方法涉及使用先进的LLMs来修正ASR生成的字幕，重点介绍了GPT-3.5和Llama2-13B等模型，它们在语言理解和生成任务中表现出强大的性能。我们引入了一个反映聋听群体面临现实挑战的数据集来评估我们提出的管道。结果表明，LLM增强的字幕显著提高了准确性，ChatGPT-3.5的单词错误率（WER）从原来的ASR字幕的23.07%降低到9.75%，显示出大约57.72%的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频内容在数字时代的重要性及其对聋哑人群的特别意义。</li>
<li>现有自动语音识别（ASR）系统在为聋哑人群提供准确可靠字幕方面的不足。</li>
<li>大型语言模型（LLMs）在提高视频字幕质量方面的潜力。</li>
<li>利用先进LLMs如GPT-3.5和Llama2-13B来增强ASR系统生成的字幕的准确性。</li>
<li>通过引入反映聋听群体现实挑战的数据集来评估新的字幕生成方法。</li>
<li>LLM增强的字幕显示出显著提高的准确性，特别是ChatGPT-3.5在单词错误率（WER）方面的显著改进。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7698f76dd914ce2e708dc0daf0cb56ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a7dcf77501eeca0e82ebcdf66c17123.jpg" align="middle">
</details>




<h2 id="OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation"><a href="#OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation" class="headerlink" title="OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation"></a>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation</h2><p><strong>Authors:Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu</strong></p>
<p>Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid">https://fudan-generative-vision.github.io/OpenHumanVid</a> </p>
<blockquote>
<p>随着视觉生成技术的最新进展，视频数据集的数量和可用性显著增加，这对于训练有效的视频生成模型至关重要。然而，高质量、以人为中心的视频数据集的缺乏为该领域的进步带来了挑战。为了弥补这一差距，我们推出了OpenHumanVid，这是一个大规模、高质量、以人为中心的视频数据集，其特点是具有精确和详细的字幕，涵盖了人类外观和运动状态，还包括额外的人类运动条件，如骨骼序列和语音音频。为了验证该数据集和相关训练策略的有效性，我们对现有的经典扩散变压器架构进行了扩展，并在所提出的数据集上对我们的模型进行了进一步的预训练。我们的研究发现两个关键见解：首先，使用大规模、高质量的数据集可以显著提高生成的人类视频的评价指标，同时保留了一般视频生成任务的性能。其次，文本与人类外观、人类运动和面部运动的有效对齐对于生成高质量视频输出至关重要。基于这些见解和相应的方法论，在所提出的数据集上训练的简单扩展网络在生成以人为中心的视频方面显示出明显的改进。项目页面<a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid%E3%80%82">https://fudan-generative-vision.github.io/OpenHumanVid。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00115v2">PDF</a> 11 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>OpenHumanVid是一个大规模、高质量的人机互动视频数据集，包含了精确详细的描述、人类动作状态以及额外的运动条件，如骨架序列和语音音频。利用该数据集进行预训练可以有效提升生成人类视频的评价指标，并保留了一般视频生成任务中的性能。该项目实现了基于深度学习的视频生成技术的重大突破。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenHumanVid是一个大规模、高质量的人机互动视频数据集，提供了详细的描述、人类动作状态以及额外的运动条件。</li>
<li>利用OpenHumanVid数据集进行预训练能有效提高生成人类视频的评价指标。</li>
<li>保留了一般视频生成任务中的性能。</li>
<li>经典扩散变压器架构的扩展和模型在OpenHumanVid数据集上的预训练对于生成高质量视频至关重要。</li>
<li>将文本与人类外观、动作和面部动作有效对齐是生成高质量视频的关键。</li>
<li>通过使用OpenHumanVid数据集进行训练，能够显著提高以人类为中心的视频生成能力。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e7a32979db120061c9814fbd3803ed20.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-af91df1cc921458f1fd25840d2007560.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-12fef6686db4ba0fa0c8fa9024adeb59.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bb7e2cea8efdf57502d07f18589af537.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-303bd37974641a07b143b5ccc769afc0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c950ea4205d5c525019e5a8026adcb1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-991b8dbd477d1cfa4f7db2f183f15261.jpg" align="middle">
</details>




<h2 id="EEG-Based-Analysis-of-Brain-Responses-in-Multi-Modal-Human-Robot-Interaction-Modulating-Engagement"><a href="#EEG-Based-Analysis-of-Brain-Responses-in-Multi-Modal-Human-Robot-Interaction-Modulating-Engagement" class="headerlink" title="EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot   Interaction: Modulating Engagement"></a>EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot   Interaction: Modulating Engagement</h2><p><strong>Authors:Suzanne Oliver, Tomoko Kitago, Adam Buchwald, S. Farokh Atashzar</strong></p>
<p>User engagement, cognitive participation, and motivation during task execution in physical human-robot interaction are crucial for motor learning. These factors are especially important in contexts like robotic rehabilitation, where neuroplasticity is targeted. However, traditional robotic rehabilitation systems often face challenges in maintaining user engagement, leading to unpredictable therapeutic outcomes. To address this issue, various techniques, such as assist-as-needed controllers, have been developed to prevent user slacking and encourage active participation. In this paper, we introduce a new direction through a novel multi-modal robotic interaction designed to enhance user engagement by synergistically integrating visual, motor, cognitive, and auditory (speech recognition) tasks into a single, comprehensive activity. To assess engagement quantitatively, we compared multiple electroencephalography (EEG) biomarkers between this multi-modal protocol and a traditional motor-only protocol. Fifteen healthy adult participants completed 100 trials of each task type. Our findings revealed that EEG biomarkers, particularly relative alpha power, showed statistically significant improvements in engagement during the multi-modal task compared to the motor-only task. Moreover, while engagement decreased over time in the motor-only task, the multi-modal protocol maintained consistent engagement, suggesting that users could remain engaged for longer therapy sessions. Our observations on neural responses during interaction indicate that the proposed multi-modal approach can effectively enhance user engagement, which is critical for improving outcomes. This is the first time that objective neural response highlights the benefit of a comprehensive robotic intervention combining motor, cognitive, and auditory functions in healthy subjects. </p>
<blockquote>
<p>在物理人机交互中的任务执行过程中，用户参与度、认知参与度和动机对于运动学习至关重要。在针对神经可塑性目标的领域（如机器人康复）中，这些因素尤为重要。然而，传统的机器人康复系统常常面临维持用户参与度方面的挑战，从而导致不可预测的治疗结果。为了解决这一问题，已经开发出了各种技术，如按需辅助控制器，以防止用户懈怠并鼓励积极参与。在这篇论文中，我们通过一种新型的多模式机器人交互介绍了一个新方向，该交互通过协同整合视觉、运动、认知和听觉（语音识别）任务到一个单一的综合活动中，从而增强用户参与度。为了定量评估参与度，我们比较了这种多模式协议和传统仅运动模式的协议之间的多个脑电图（EEG）生物标志物。十五名健康成年参与者完成了每种任务类型的一百次试验。我们的研究结果表明，与仅运动任务相比，脑电图生物标志物（尤其是相对阿尔法功率）在多模式任务期间的参与度有显著改善。此外，尽管参与度在仅运动任务中随时间下降，但多模式协议保持了持续的参与度，这表明用户可以在更长的治疗过程中保持参与。我们对交互过程中神经反应的观察表明，所提出的多模式方法可以有效地提高用户参与度，这对于提高治疗效果至关重要。这是第一次客观神经反应突显出在健康受试者中结合运动、认知和听觉功能的综合机器人干预的益处。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18587v1">PDF</a> 9 pages, 7 figures. Submitted to IEEE TNSRE</p>
<p><strong>摘要</strong></p>
<p>物理人机交互中的用户参与度、认知参与度和任务执行时的动机对于运动学习至关重要，特别是在机器人康复等针对神经可塑性目标的情境中。然而，传统的机器人康复系统往往面临维持用户参与度的挑战，从而导致治疗效果难以预测。为解决这一问题，研究者开发了按需辅助控制器等技术以防止用户懈怠并鼓励积极参与。本文介绍了一种通过新型多模式机器人交互增强用户参与度的方法，该方法通过协同整合视觉、运动、认知和听觉（语音识别）任务，将多种模式融入单一的全面活动中。为定量评估参与度，我们比较了多模态协议和传统运动协议下的脑电图（EEG）生物标志物，涉及15名健康成年参与者共完成每种任务类型各100次试验。结果显示，脑电图生物标志物即相对阿尔法功率在多模态任务中的改善具有统计学显著差异。此外，尽管参与者在仅进行运动任务的情景下随着时间推移其参与度下降，多模态协议则维持了一致的参与度水平，暗示用户在长时间治疗场景下也能保持高度参与。本研究观察到的神经反应表明，所提出的多模态方法可有效提升用户参与度，这对于改善治疗效果至关重要。这是首次在健康受试者中客观记录神经反应表明结合运动、认知和听觉功能的全面机器人干预的优势。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>用户参与度、认知参与度和动机在物理人机交互中的任务执行对运动学习非常重要，尤其在机器人康复领域。</li>
<li>传统机器人康复系统在维持用户参与度方面面临挑战，影响治疗效果的预测。</li>
<li>多模式机器人交互方法通过结合多种感官刺激（如视觉、运动、认知和听觉）来提高用户参与度。</li>
<li>研究比较了多模态任务与传统运动任务的脑电图生物标志物来衡量参与度。</li>
<li>相对阿尔法功率等EEG生物标志物在多模态任务中的改善显著，表明多模态方法能提高用户参与度。</li>
<li>多模态协议能维持稳定的参与度，即使随着时间推移，这在长期治疗中是关键。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9d7e0a9a026a5275bc42ea384ede41d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ab0b0df55a1a321846840dad36d70084.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6bbc17df22f2f5a0f43f3bb9b40f6e2d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0e4c3f2ebe2ad487dd1048fcf5118eee.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da6d465f9b9a30bf5aa2f8313f765ffd.jpg" align="middle">
</details>




<h2 id="Multiple-Choice-Learning-for-Efficient-Speech-Separation-with-Many-Speakers"><a href="#Multiple-Choice-Learning-for-Efficient-Speech-Separation-with-Many-Speakers" class="headerlink" title="Multiple Choice Learning for Efficient Speech Separation with Many   Speakers"></a>Multiple Choice Learning for Efficient Speech Separation with Many   Speakers</h2><p><strong>Authors:David Perera, François Derrida, Théo Mariotte, Gaël Richard, Slim Essid</strong></p>
<p>Training speech separation models in the supervised setting raises a permutation problem: finding the best assignation between the model predictions and the ground truth separated signals. This inherently ambiguous task is customarily solved using Permutation Invariant Training (PIT). In this article, we instead consider using the Multiple Choice Learning (MCL) framework, which was originally introduced to tackle ambiguous tasks. We demonstrate experimentally on the popular WSJ0-mix and LibriMix benchmarks that MCL matches the performances of PIT, while being computationally advantageous. This opens the door to a promising research direction, as MCL can be naturally extended to handle a variable number of speakers, or to tackle speech separation in the unsupervised setting. </p>
<blockquote>
<p>在监督环境下训练语音分离模型会引发排列问题：即如何为模型预测和真实分离信号找到最佳的分配方案。这个本质上是模糊的任务通常通过使用排列不变训练（PIT）来解决。在本文中，我们考虑使用最初为解决模糊任务而引入的多选学习（MCL）框架。我们在流行的WSJ0-mix和LibriMix基准测试上进行了实验，证明了MCL的性能与PIT相匹配，同时计算上更有优势。这为研究方向打开了大门，因为MCL可以自然地扩展到处理可变数量的发言人，或解决非监督环境下的语音分离问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18497v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了语音分离模型的训练问题，特别是在有监督的环境下。由于模型预测和真实分离信号之间的分配存在排列问题，通常使用排列不变训练（PIT）来解决此固有的模糊任务。本文考虑采用原始用于解决模糊任务的多项选择学习（MCL）框架。实验表明，在流行的WSJ0-mix和LibriMix基准测试中，MCL的性能与PIT相匹配，同时计算上更有优势。这为MCL在可变说话人数或多通道语音分离等方向的研究开辟了新的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音分离模型的训练面临排列问题，即模型预测与真实分离信号之间的最佳分配问题。</li>
<li>排列不变训练（PIT）是常规解决此模糊任务的方法。</li>
<li>本文引入多项选择学习（MCL）框架作为替代方案。</li>
<li>在WSJ0-mix和LibriMix基准测试中，MCL性能与PIT相当。</li>
<li>MCL具有计算优势，为处理可变说话人数或监督环境下的语音分离提供了研究潜力。</li>
<li>MCL可自然扩展到处理不同场景，如多通道语音分离等。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-393696b300164ff7504f20e5e027e363.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7d3ec7b6b0cd262e3e346266c8640b41.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a5f507b445890ce46695be7292e75aa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ed741ce28604f85f3c0a72f28aa2097f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fcc196dcd43a85c2097c41d444e529c2.jpg" align="middle">
</details>




<h2 id="AMPS-ASR-with-Multimodal-Paraphrase-Supervision"><a href="#AMPS-ASR-with-Multimodal-Paraphrase-Supervision" class="headerlink" title="AMPS: ASR with Multimodal Paraphrase Supervision"></a>AMPS: ASR with Multimodal Paraphrase Supervision</h2><p><strong>Authors:Amruta Parulekar, Abhishek Gupta, Sameep Chattopadhyay, Preethi Jyothi</strong></p>
<p>Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative reductions in word error rates (WERs) of up to 5%. We present detailed analyses of our system using both objective and human evaluation metrics. </p>
<blockquote>
<p>多语种自然口语为最先进的自动语音识别（ASR）系统带来了诸多挑战。在这项工作中，我们提出了一种新技术AMPS，该技术增强了一种多语种多媒体ASR系统，通过基于释义的监督改进了多种语言的对话ASR表现，包括印地语、马拉地语、马拉雅拉姆语、坎纳达语和尼扬贾语。我们在训练多媒体ASR模型时，会使用参考译文的释义作为额外的监督方式，并针对表现不佳的ASR表现选择性地启动这种释义目标。通过使用AMPS和最先进的多媒体模型无缝M4T的结合，我们实现了显著的相对减少，词错误率降低了高达5%。我们通过客观和人类评估指标两种方法对系统进行了详细分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18368v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动语音识别（ASR）系统在处理自发性或多语种对话语音时面临诸多挑战。本研究提出了一种新技术AMPS，该技术通过基于同义短语监督的方式增强多语种多媒体ASR系统，以提高对包括印地语、马拉地语、马拉雅拉姆语、坎纳达语和尼亚贾语在内的多种语言的对话语音识别。在训练多媒体ASR模型时，我们使用参考转录的同义短语作为额外的监督，并选择性地对表现不佳的ASR性能片段使用同义短语目标。通过将AMPS与先进的SeamlessM4T多媒体模型结合使用，我们获得了显著的相对词错误率（WER）降低，最高达5%。本研究通过客观和人类评估指标对系统进行了详细分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AMPS技术通过结合同义短语监督增强了多语种对话语音识别的性能。</li>
<li>AMPS能够应用于多种语言，包括印地语、马拉地语、马拉雅拉姆语、坎纳达语和尼亚贾语。</li>
<li>在训练多媒体ASR模型时，使用了参考转录的同义短语作为额外的监督信息。</li>
<li>对于ASR性能不佳的部分，会选择性使用同义短语目标进行优化。</li>
<li>结合AMPS技术和先进的SeamlessM4T多媒体模型，显著降低了词错误率（WER）。</li>
<li>该研究通过客观评估指标和人类评估指标对系统进行了详细分析。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e1e2fa50c2c86467a0cc5d314aa22720.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-336c7f5eda3569f0ee72b3c03b54fcca.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-92bf1dc1ed7c62fdbbc68a9b75f4b72a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a8791c7f0160546dbb86fb0c2f2d4f69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-29e1a6b0331cd203e0b35cc7bee2e4f1.jpg" align="middle">
</details>




<h2 id="SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation"><a href="#SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation" class="headerlink" title="SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and   Generation"></a>SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and   Generation</h2><p><strong>Authors:Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang</strong></p>
<p>Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a &#96;&#96;thinking’’ mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omni’s versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon. </p>
<blockquote>
<p>全双工多模态大型语言模型（LLM）为处理多样化的语音理解和生成任务提供了一个统一的框架，使人类与机器之间的对话更加自然和无缝。与传统的模块化对话AI系统不同，后者将语音识别、理解和文本到语音的生成过程划分为独立的不同组件，多模态LLM则以单一端到端的模型进行工作。这种流程化的设计消除了组件间错误传递，并充分利用输入语音信号中丰富的非语言信息。我们介绍了SALMONN-omni，这是一种无编解码器、全双工语音理解和生成模型，它能够在说话时同时监听自己的生成的语音和背景声音。为了支持这种能力，我们提出了一个新颖的双工对话框架，融入了一种“思考”机制，该机制依赖于嵌入而非编解码器（量化语音和音频令牌）来实现异步文本和语音生成。实验结果表明，SALMONN-omni在广泛的流式语音任务中表现出很强的通用性，包括语音识别、语音增强和语音问答。此外，SALMONN-omni在轮替发言、插话和回声消除等场景中表现出色，证明其作为全双工对话AI系统的稳健原型的潜力。据我们所知，SALMONN-omni是首个无编解码器的此类模型。我们将很快发布完整的技术报告和模型检查点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18138v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>基于全双工多模态大型语言模型（LLMs）的框架，可以统一解决多种语音理解和生成任务，促进更自然和无缝的人机对话。全双工模型SALMONN-omni无需编解码器，即可实现同时听取自身生成的语音和背景声音的同时进行语音输出。实验结果表明，SALMONN-omni在各种流式语音任务中表现优异，如语音识别、语音增强和语音问答等。其能妥善管理轮流发言、中断发言和回声消除等场景，为全双工对话式AI系统提供了稳健的原型。它是首个无需编解码器的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>全双工多模态大型语言模型（LLMs）提供了统一的框架来解决多种语音理解和生成任务。</li>
<li>LLMs促进了更自然和无缝的人机对话。</li>
<li>SALMONN-omni模型无需编解码器即可实现全双工语音理解和生成。</li>
<li>SALMONN-omni在各种流式语音任务中表现优异，包括语音识别、语音增强和语音问答等。</li>
<li>SALMONN-omni能妥善管理轮流发言、中断发言和回声消除等场景。</li>
<li>SALMONN-omni为全双工对话式AI系统提供了稳健的原型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-af83771b0fcf83031a32e1732ec9e749.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-380af3e54978f0e064a9d5c87187adef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da3211095949e31aeb53708715de1dd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e4211a8ca2e98b1f4a9df0ccfd0954b3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-59aaeeb020314e6bc7f9d8676410efd0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abcad34c82c8a99ad0b9dba70679e5b4.jpg" align="middle">
</details>




<h2 id="Speech-Separation-using-Neural-Audio-Codecs-with-Embedding-Loss"><a href="#Speech-Separation-using-Neural-Audio-Codecs-with-Embedding-Loss" class="headerlink" title="Speech Separation using Neural Audio Codecs with Embedding Loss"></a>Speech Separation using Neural Audio Codecs with Embedding Loss</h2><p><strong>Authors:Jia Qi Yip, Chin Yuen Kwok, Bin Ma, Eng Siong Chng</strong></p>
<p>Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs. </p>
<blockquote>
<p>神经网络音频编解码器通过允许在高度压缩的表示上执行语音任务，从而革新了音频处理。最近的研究表明，可以在这些压缩域内实现语音分离，从而提供更快的训练和推理成本。然而，当前的方法仍然依赖于基于波形的损失函数，在训练过程中需要进行不必要的解码步骤。我们提出了一种用于基于神经网络音频编解码器的语音分离的新型嵌入损失，它直接在压缩的音频表示上运行，从而在训练过程中消除了对解码的需求。为了验证我们的方法，我们使用客观指标和感知评估技术，包括侵入性和非侵入性方法，进行了全面的评估。结果表明，嵌入损失可用于训练基于编解码器的语音分离模型，在训练速度和计算成本方面实现2倍的提升，同时在WSJ0-2mix数据集上实现更好的DNSMOS和STOI性能，跨越3种不同的预训练编解码器。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17998v1">PDF</a> Accepted by APSIPA ASC 2024</p>
<p><strong>Summary</strong></p>
<p>神经网络音频编码器的出现，为音频处理带来了革命性的变革，它能够在高度压缩的表示上执行语音任务。最新研究表明，可在这些压缩域内实现语音分离，降低了训练和推理成本。然而，当前方法仍依赖于基于波形的损失函数，训练过程中需要进行不必要的解码步骤。我们提出了一种新颖的嵌入损失，用于基于神经网络音频编码器的语音分离，可直接在压缩的音频表示上操作，从而消除训练过程中的解码需求。通过客观指标和感知评估技术，包括侵入性和非侵入性方法，我们验证了该方法的有效性。结果表明，嵌入损失可用于训练基于编码器的语音分离模型，在WSJ0-2mix数据集上实现了DNSMOS和STOI性能的改善，同时训练速度和计算成本提高了两倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络音频编码器实现了在高度压缩的表示上进行语音任务的能力。</li>
<li>最新研究证明了在压缩域内进行语音分离的可行性。</li>
<li>当前方法仍依赖于波形损失函数，导致训练过程中需要额外的解码步骤。</li>
<li>提出了一种新颖的嵌入损失函数，可直接在压缩的音频表示上进行语音分离训练，无需解码步骤。</li>
<li>嵌入损失函数能够提高训练速度和计算效率。</li>
<li>通过客观指标和感知评估技术验证了新方法的有效性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-037d4ea3f14bb207b726149ac1e7206b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e175a11904a657b0d69385177ffa142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-36d3cfeae841cdca0819b94291e1ca39.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-21fd829a2496199545baa09ca8e3e9ea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ba3a0b7e3e24e3a0592b9811be509c24.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4962aaa7ad4dbf82990840ed00cd0a9c.jpg" align="middle">
</details>




<h2 id="Disentangled-Transformer-An-Explainable-End-to-End-Automatic-Speech-Recognition-Model-with-Speech-Content-Context-Separation"><a href="#Disentangled-Transformer-An-Explainable-End-to-End-Automatic-Speech-Recognition-Model-with-Speech-Content-Context-Separation" class="headerlink" title="Disentangled-Transformer: An Explainable End-to-End Automatic Speech   Recognition Model with Speech Content-Context Separation"></a>Disentangled-Transformer: An Explainable End-to-End Automatic Speech   Recognition Model with Speech Content-Context Separation</h2><p><strong>Authors:Pu Wang, Hugo Van hamme</strong></p>
<p>End-to-end transformer-based automatic speech recognition (ASR) systems often capture multiple speech traits in their learned representations that are highly entangled, leading to a lack of interpretability. In this study, we propose the explainable Disentangled-Transformer, which disentangles the internal representations into sub-embeddings with explicit content and speaker traits based on varying temporal resolutions. Experimental results show that the proposed Disentangled-Transformer produces a clear speaker identity, separated from the speech content, for speaker diarization while improving ASR performance. </p>
<blockquote>
<p>基于端到端的转换器自动语音识别（ASR）系统在其学习到的表示中通常会捕获多个高度纠缠的语音特征，导致缺乏可解释性。本研究提出了可解释的解纠缠转换器（Disentangled-Transformer），该转换器将内部表示解纠缠为具有明确内容和说话者特征的子嵌入，基于不同的时间分辨率。实验结果表明，所提出的解纠缠转换器在说话人身份辨析中能够清晰地产生与语音内容分离的说话人身份，同时提高ASR性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17846v1">PDF</a> Accepted by the 6th IEEE International Conference on Image Processing   Applications and Systems</p>
<p><strong>Summary</strong>：本研究提出了可解释的解纠缠转换器（Disentangled-Transformer），该转换器可将端到端的基于转换器的自动语音识别（ASR）系统的内部表示形式解纠缠为具有明确内容和说话者特征的子嵌入，基于不同的时间分辨率。实验结果表明，所提出的解纠缠转换器在说话人身份清晰分离的情况下，提高了语音内容的识别性能，并可用于说话人摘要化。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>端到端的基于转换器的ASR系统存在多个语音特征高度纠缠的问题，导致缺乏可解释性。</li>
<li>研究提出了可解释的解纠缠转换器（Disentangled-Transformer）来解决这一问题。</li>
<li>解纠缠转换器能够将内部表示形式转化为子嵌入，这些子嵌入具有明确的内容和说话者特征。</li>
<li>基于不同的时间分辨率，解纠缠转换器实现了这一转化。</li>
<li>实验证明，解纠缠转换器能够清晰地分离说话人的身份和语音内容。</li>
<li>解纠缠转换器在说话人身份清晰分离的情况下，提高了语音内容的识别性能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c5a64a7b6340a4ae69860300839e0602.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a07898c5b1dbcd24b430b54420413e69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ceab89681f51a5a67c3bf424da1c436.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-46714c7b5868231785880d41a952e8ce.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e476f9265139489394177a227ba0b802.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-16c9dd5e8adb471b59fe82d7c8c91097.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a4349e3f1ad538fd8155351d046c0bab.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6d3737d8f83e218d1de5bf750e161bb7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bbdeedec281955a4955850a8481701dd.jpg" align="middle">
</details>




<h2 id="BERT-or-FastText-A-Comparative-Analysis-of-Contextual-as-well-as-Non-Contextual-Embeddings"><a href="#BERT-or-FastText-A-Comparative-Analysis-of-Contextual-as-well-as-Non-Contextual-Embeddings" class="headerlink" title="BERT or FastText? A Comparative Analysis of Contextual as well as   Non-Contextual Embeddings"></a>BERT or FastText? A Comparative Analysis of Contextual as well as   Non-Contextual Embeddings</h2><p><strong>Authors:Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi</strong></p>
<p>Natural Language Processing (NLP) for low-resource languages presents significant challenges, particularly due to the scarcity of high-quality annotated data and linguistic resources. The choice of embeddings plays a critical role in enhancing the performance of NLP tasks, such as news classification, sentiment analysis, and hate speech detection, especially for low-resource languages like Marathi. In this study, we investigate the impact of various embedding techniques- Contextual BERT-based, Non-Contextual BERT-based, and FastText-based on NLP classification tasks specific to the Marathi language. Our research includes a thorough evaluation of both compressed and uncompressed embeddings, providing a comprehensive overview of how these embeddings perform across different scenarios. Specifically, we compare two BERT model embeddings, Muril and MahaBERT, as well as two FastText model embeddings, IndicFT and MahaFT. Our evaluation includes applying embeddings to a Multiple Logistic Regression (MLR) classifier for task performance assessment, as well as TSNE visualizations to observe the spatial distribution of these embeddings. The results demonstrate that contextual embeddings outperform non-contextual embeddings. Furthermore, BERT-based non-contextual embeddings extracted from the first BERT embedding layer yield better results than FastText-based embeddings, suggesting a potential alternative to FastText embeddings. </p>
<blockquote>
<p>自然语言处理（NLP）对于低资源语言来说存在重大挑战，尤其是因为高质量标注数据和语言资源的稀缺。嵌入层的选择在提高NLP任务性能中起着关键作用，例如新闻分类、情感分析和仇恨言论检测，特别是对于像马拉地语这样的低资源语言。在这项研究中，我们调查了多种嵌入技术的影响，包括基于上下文的BERT、非基于上下文的BERT和基于FastText的技术，这些技术对马拉地语特定的NLP分类任务具有重要影响。我们的研究包括对压缩和非压缩嵌入的彻底评估，全面概述了这些嵌入在不同场景中的表现。具体来说，我们比较了两种BERT模型嵌入（Muril和MahaBERT），以及两种FastText模型嵌入（IndicFT和MahaFT）。我们的评估包括将这些嵌入应用于多重逻辑回归（MLR）分类器进行任务性能评估，并使用TSNE可视化观察这些嵌入的空间分布。结果表明，上下文嵌入优于非上下文嵌入。此外，从BERT嵌入的第一层提取的基于BERT的非上下文嵌入比基于FastText的嵌入产生更好的结果，这可能成为FastText嵌入的一种潜在替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17661v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了自然语言处理（NLP）在低资源语言面临的挑战，特别是高质量标注数据和语言资源的稀缺性。文章聚焦于嵌入技术对提升NLP任务性能的重要性，如新闻分类、情感分析和仇恨言论检测，特别是针对低资源语言如马拉地语。本研究评估了多种嵌入技术的影响，包括基于上下文的BERT、非上下文的BERT和基于FastText的嵌入技术，在马拉地语NLP分类任务中的表现。文章全面评价了压缩和非压缩嵌入，并通过实例展示其在不同场景中的应用效果。结果表明，上下文嵌入技术优于非上下文嵌入技术，基于BERT的非上下文嵌入表现优于基于FastText的嵌入，成为潜在的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>低资源语言NLP面临的主要挑战是高质量标注数据和语言资源的稀缺性。</li>
<li>嵌入技术对于提升NLP任务性能至关重要，特别是在低资源语言环境下。</li>
<li>研究评估了基于上下文的BERT、非上下文的BERT和基于FastText的嵌入技术在马拉地语NLP分类任务中的应用。</li>
<li>上下文嵌入技术表现优于非上下文嵌入技术。</li>
<li>基于BERT的非上下文嵌入在新闻分类、情感分析和仇恨言论检测等任务中表现优异。</li>
<li>相比基于FastText的嵌入技术，BERT非上下文嵌入提供了一种潜在替代方案。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f68e9fed69772c17ffd74a79e382e4de.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f5049eb2f83c99fd34f3e75c92ba4a5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-89d024379be9f11c7f5c31f21d5473b8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-52ed655175e89865eed7024075a8158e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dade2b7e5a12b17fb07a068fb93f9325.jpg" align="middle">
</details>




<h2 id="Scaling-Speech-Text-Pre-training-with-Synthetic-Interleaved-Data"><a href="#Scaling-Speech-Text-Pre-training-with-Synthetic-Interleaved-Data" class="headerlink" title="Scaling Speech-Text Pre-training with Synthetic Interleaved Data"></a>Scaling Speech-Text Pre-training with Synthetic Interleaved Data</h2><p><strong>Authors:Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang</strong></p>
<p>Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain. </p>
<blockquote>
<p>语音语言模型（SpeechLMs）接受语音输入并产生语音输出，与基于文本的大型语言模型（LLMs）相比，实现了更自然的人机交互。传统开发SpeechLMs的方法受限于监督语音数据的有限可用性，以及与文本预训练数据相比，并行语音-文本数据明显不足，从而限制了其可扩展性。我们提出了一种利用从文本语料库衍生的大规模合成交织数据来扩展语音-文本预训练的新方法，从而消除了对并行语音-文本数据集的需求。我们的方法通过从现有文本语料库中采样文本片段并使用文本到标记模型合成相应的语音片段，有效地构建了语音-文本交织数据，从而无需生成实际语音。我们还通过使用自动语音识别（ASR）模型并融入向量量化瓶颈到编码器中来建立监督语音标记器。这种监督训练方法即使在较低帧率（例如12.5Hz）下也能产生具有强烈语义保留的离散语音标记，同时仍保持语音重建质量。从一个预训练的语言模型开始，我们将预训练扩展到1万亿个标记（使用600B合成交织语音-文本数据），在语音语言建模和口语问答方面达到最新技术水平，口语问答任务的性能从之前的最佳水平13%（莫希）提高到31%。我们进一步证明，通过对预训练模型使用语音对话数据进行微调，我们可以开发一种端到端的口语聊天机器人，即使在语音领域，其在对话能力和语音质量方面的表现也可与现有基线相媲美。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17607v2">PDF</a> </p>
<p><strong>Summary</strong><br>     该文本介绍了基于文本的语音语言模型的构建方法，该模型使用大规模合成交错数据对预训练模型进行扩展。使用这种方法可以避免对传统并行语音文本的依赖，更有效地构造语音文本交错数据，并提高性能表现。通过在端到端的语音聊天机器人中进行微调，该模型在对话能力和语音质量方面取得了显著的进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音语言模型（SpeechLMs）接受语音输入并产生语音输出，与传统基于文本的大型语言模型相比，更加自然和人性化。</li>
<li>传统语音语言模型的发展受限于缺乏监督的语音数据和并行语音文本数据。提出了一个新型预训练方法，即通过大规模合成交错数据来解决这个问题。无需实际语音数据的生成过程也能高效构建模型。</li>
<li>提出一种有效的数据构建方式，通过从现有文本语料库中采样文本片段并使用文本到标记模型合成相应的语音片段来合成语音数据。这种方法避免了生成实际语音数据的复杂性。</li>
<li>利用来自自动语音识别模型的矢量量化瓶颈信息创建有监督的语音分词器。此训练方法可以实现较低频率（如每秒仅产生一次）的离散语音标记，同时保持语义保留和语音重建质量。</li>
<li>通过扩展到大量预训练数据（如高达一千亿个标记），这种新型预训练方法提高了语音语言建模和口语问答的性能表现，达到业界领先水平。</li>
<li>通过微调预训练模型与语音对话数据，成功开发出具有竞争力的端到端口语聊天机器人，其对话能力和语音质量均表现良好。该机器人在口语任务上的表现优于现有基准测试水平。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-083a15304c8ff1744a4fb8ea3491090d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-883a8bdcbf286c4742aac394b97420a3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6d53c63a0e1389b63576111fd6941662.jpg" align="middle">
</details>




<h2 id="X-CrossNet-A-complex-spectral-mapping-approach-to-target-speaker-extraction-with-cross-attention-speaker-embedding-fusion"><a href="#X-CrossNet-A-complex-spectral-mapping-approach-to-target-speaker-extraction-with-cross-attention-speaker-embedding-fusion" class="headerlink" title="X-CrossNet: A complex spectral mapping approach to target speaker   extraction with cross attention speaker embedding fusion"></a>X-CrossNet: A complex spectral mapping approach to target speaker   extraction with cross attention speaker embedding fusion</h2><p><strong>Authors:Chang Sun, Bo Qin</strong></p>
<p>Target speaker extraction (TSE) is a technique for isolating a target speaker’s voice from mixed speech using auxiliary features associated with the target speaker. It is another attempt at addressing the cocktail party problem and is generally considered to have more practical application prospects than traditional speech separation methods. Although academic research in this area has achieved high performance and evaluation scores on public datasets, most models exhibit significantly reduced performance in real-world noisy or reverberant conditions. To address this limitation, we propose a novel TSE model, X-CrossNet, which leverages CrossNet as its backbone. CrossNet is a speech separation network specifically optimized for challenging noisy and reverberant environments, achieving state-of-the-art performance in tasks such as speaker separation under these conditions. Additionally, to enhance the network’s ability to capture and utilize auxiliary features of the target speaker, we integrate a Cross-Attention mechanism into the global multi-head self-attention (GMHSA) module within each CrossNet block. This facilitates more effective integration of target speaker features with mixed speech features. Experimental results show that our method performs superior separation on the WSJ0-2mix and WHAMR! datasets, demonstrating strong robustness and stability. </p>
<blockquote>
<p>目标说话人提取（TSE）是一种利用与目标说话人相关的辅助特征从混合语音中分离出目标说话人声音的技术。它是解决鸡尾酒会问题的另一种尝试，并且通常被认为比传统的语音分离方法具有更实际的应用前景。尽管该领域的学术研究在公共数据集上取得了高性能和评估分数，但大多数模型在现实世界中噪声大或混响条件下性能显著下降。为了解决这个问题，我们提出了一种新型的TSE模型X-CrossNet，它以CrossNet作为骨干网。CrossNet是一种针对噪声大和混响等挑战环境的语音分离网络，在这些条件下，它在说话人分离等任务上实现了最先进的性能。此外，为了提高网络捕获和利用目标说话人的辅助特征的能力，我们将交叉注意机制集成到每个CrossNet块内的全局多头自注意（GMHSA）模块中。这有助于更有效地将目标说话人的特征与混合语音特征结合起来。实验结果表明，我们的方法在WSJ0-2mix和WHAMR！数据集上实现了出色的分离效果，显示出强大的鲁棒性和稳定性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13811v2">PDF</a> </p>
<p><strong>摘要</strong><br>    目标说话人提取（TSE）技术通过利用目标说话人的辅助特征来从混合语音中分离出目标说话人的声音。尽管在公共数据集上的学术研究取得了高性能和评价分数，但在现实世界的嘈杂或混响环境中，大多数模型的性能会显著降低。为解决这一局限，我们提出一种新型TSE模型X-CrossNet，以CrossNet作为骨干。CrossNet是针对嘈杂和混响环境优化的语音分离网络，在这些条件下实现了最先进的性能。此外，为提高网络捕捉和利用目标说话人的辅助特征的能力，我们在每个CrossNet块的全球多头自注意力（GMHSA）模块中集成了交叉注意力机制。这有助于更有效地整合目标说话人的特征与混合语音特征。实验结果表明，我们的方法在WSJ0-2mix和WHAMR！数据集上实现了出色的分离性能，显示出强大的稳健性和稳定性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>目标说话人提取（TSE）旨在从混合语音中隔离目标说话人的声音，使用与目标说话人相关的辅助特征。</li>
<li>现有模型在嘈杂或混响的现实中环境中性能降低。</li>
<li>提出的X-CrossNet模型以CrossNet为骨干，针对挑战性的嘈杂和混响环境进行了优化。</li>
<li>在每个CrossNet块的GMHSA模块中集成了交叉注意力机制，以更有效地整合目标说话人的特征与混合语音特征。</li>
<li>X-CrossNet在WSJ0-2mix和WHAMR！数据集上实现了出色的分离性能。</li>
<li>该方法具有强大的稳健性和稳定性。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-965cc3523e6663bc205df591cfc75ba3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9ade30df9d05146f5999826e7ebf76d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc6fb05a44af3d0f9f9262b277d92ed8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2786a7819c3af3ab541f5d04da296319.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b17c122dab3ad34257948b7c60b66ef3.jpg" align="middle">
</details>




<h2 id="FabuLight-ASD-Unveiling-Speech-Activity-via-Body-Language"><a href="#FabuLight-ASD-Unveiling-Speech-Activity-via-Body-Language" class="headerlink" title="FabuLight-ASD: Unveiling Speech Activity via Body Language"></a>FabuLight-ASD: Unveiling Speech Activity via Body Language</h2><p><strong>Authors:Hugo Carneiro, Stefan Wermter</strong></p>
<p>Active speaker detection (ASD) in multimodal environments is crucial for various applications, from video conferencing to human-robot interaction. This paper introduces FabuLight-ASD, an advanced ASD model that integrates facial, audio, and body pose information to enhance detection accuracy and robustness. Our model builds upon the existing Light-ASD framework by incorporating human pose data, represented through skeleton graphs, which minimises computational overhead. Using the Wilder Active Speaker Detection (WASD) dataset, renowned for reliable face and body bounding box annotations, we demonstrate FabuLight-ASD’s effectiveness in real-world scenarios. Achieving an overall mean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD, which has an overall mAP of 93.7% across various challenging scenarios. The incorporation of body pose information shows a particularly advantageous impact, with notable improvements in mAP observed in scenarios with speech impairment, face occlusion, and human voice background noise. Furthermore, efficiency analysis indicates only a modest increase in parameter count (27.3%) and multiply-accumulate operations (up to 2.4%), underscoring the model’s efficiency and feasibility. These findings validate the efficacy of FabuLight-ASD in enhancing ASD performance through the integration of body pose data. FabuLight-ASD’s code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/knowledgetechnologyuhh/FabuLight-ASD">https://github.com/knowledgetechnologyuhh/FabuLight-ASD</a>. </p>
<blockquote>
<p>在多媒体环境中，主动说话者检测（ASD）对于从视频会议到人机交互的各种应用都至关重要。本文介绍了FabuLight-ASD，这是一种先进的ASD模型，它集成了面部、音频和身体姿势信息，以提高检测准确性和稳健性。我们的模型基于现有的Light-ASD框架，通过融入通过骨骼图表示的人体姿势数据，以最小化计算开销。我们使用以可靠的脸部和身体边界框注释而闻名的Wilder Active Speaker Detection（WASD）数据集，展示了FabuLight-ASD在真实场景中的有效性。FabuLight-ASD的总体平均精度（mAP）达到94.3%，优于Light-ASD的93.7%，在各种具有挑战性的场景中表现更佳。融入身体姿势信息产生了特别有利的影响，在言语障碍、面部遮挡和背景人声噪音等场景中，mAP的改进尤为显著。此外，效率分析显示，参数计数仅增加了27.3%，乘法累加运算增加了高达2.4%，这突显了模型的效率和可行性。这些发现验证了FabuLight-ASD通过整合身体姿势数据提高ASD性能的有效性。FabuLight-ASD的代码和模型权重可在<a target="_blank" rel="noopener" href="https://github.com/knowledgetechnologyuhh/FabuLight-ASD">https://github.com/knowledgetechnologyuhh/FabuLight-ASD</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13674v2">PDF</a> 23 pages, 8 figures, 3 tables, accepted for publication in Neural   Computing and Applications</p>
<p><strong>摘要</strong><br>基于人脸、音频和身体姿态的多模态主动说话人检测。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>主动说话者检测（ASD）在多模态环境中对于视频会议、人机交互等应用至关重要。</li>
<li>FabuLight-ASD是一个先进的ASD模型，集成了面部、音频和身体姿态信息，提高了检测准确性和稳健性。</li>
<li>该模型基于Light-ASD框架，通过引入人体姿态数据（通过骨骼图表示）来优化计算开销。</li>
<li>使用著名的Wilder Active Speaker Detection（WASD）数据集进行验证，该数据集具有可靠的人脸和人体边界框注释。</li>
<li>FabuLight-ASD在多种具有挑战性的场景中实现了平均精度（mAP）为94.3%，优于Light-ASD的93.7%。</li>
<li>引入身体姿态信息在多种场景中显著提高了mAP，如在有语音障碍、面部遮挡和背景噪音的情况下。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0a148d4bb7393a27530cba9f6b495c97.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2016159286e4a27b8c76c8ffca71529.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c1c7744841e9d792cd6c4d171ff79d9f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4311c8224609813d9e9fa781306a01c2.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79d8c530a223fa85974e2a7b72c453cc.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2024-12-11  Swap Path Network for Robust Person Search Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-aa0498f2de39d156ab924e917d63e25f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2024-12-11  Dynamic EventNeRF Reconstructing General Dynamic Scenes from Multi-view   Event Cameras
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">7753.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
