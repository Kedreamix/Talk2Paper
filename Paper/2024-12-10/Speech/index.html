<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    29k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    118 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ï¼Œä¹Ÿè¢«ç§°ä¸ºè¯­éŸ³åˆæˆï¼Œæ˜¯ä¸€ä¸ªæ˜¾è‘—çš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆå¬èµ·æ¥å¾ˆè‡ªç„¶çš„äººç±»è¯­éŸ³ã€‚æœ€è¿‘ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ç»å‘å±•è¶…è¶Šäº†åˆæˆç±»ä¼¼äººç±»çš„è¯­éŸ³ï¼Œå®ç°äº†å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™åŒ…æ‹¬åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è‰²å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ ä¸­çš„æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œåœ¨è¿‡å»çš„å‡ å¹´é‡Œæå¤§åœ°å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ç­‰å¤šç§æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…æä¾›ä¸€ä¸ªæ¸…æ™°çš„ç ”ç©¶ç°çŠ¶ç†è§£ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†å¯æ§TTSçš„ä¸€èˆ¬æµç¨‹ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œæä¾›äº†ç°æœ‰æ–¹æ³•çš„å…¨é¢è€Œæ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†æ€»ç»“äº†æ•°æ®é›†å’Œè¯„ä»·æŒ‡æ ‡ï¼Œå¹¶å¯¹å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘è¿›è¡Œäº†ä¸€äº›æ¢è®¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç¯‡ç»¼è¿°è®ºæ–‡é¦–æ¬¡å…¨é¢å›é¡¾äº†æ–°å…´çš„å¯æ§TTSæ–¹æ³•ï¼Œå¯¹å­¦æœ¯ç ”ç©¶äººå‘˜å’Œè¡Œä¸šä»ä¸šè€…éƒ½å…·æœ‰å‚è€ƒä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v1">PDF</a> A comprehensive survey on controllable TTS, 23 pages, 6 tables, 4   figures, 280 references</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ˜¯ç”Ÿæˆè‡ªç„¶è¯­éŸ³çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚è¿‘å¹´æ¥ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ä»å•çº¯çš„æ¨¡ä»¿äººç±»è¯­éŸ³å‘å±•åˆ°äº†å¯å®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆï¼Œèƒ½å¯¹è¯­éŸ³çš„å„ç§å±æ€§è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’Œæ—¶é•¿ç­‰ã€‚æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸ºå¯æ§TTSå¸¦æ¥äº†å·¨å¤§çš„æå‡ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†å¯æ§TTSçš„ç ”ç©¶ç°çŠ¶ï¼Œä»åŸºæœ¬æ§åˆ¶æŠ€å·§åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•éƒ½æœ‰æ‰€æ¶‰åŠï¼Œæ—¨åœ¨ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæä¾›æœ‰ç›Šçš„å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS (æ–‡æœ¬è½¬è¯­éŸ³) æ˜¯ç”Ÿæˆè‡ªç„¶è¯­éŸ³çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚</li>
<li>è¿‘æœŸTTSæŠ€æœ¯å·²ç»ä»å•çº¯çš„æ¨¡ä»¿äººç±»è¯­éŸ³å‘å±•åˆ°å®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>ç²¾ç»†æ§åˆ¶è¯­éŸ³çš„å„ç§å±æ€§å·²æˆä¸ºå¯èƒ½ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’Œæ—¶é•¿ç­‰ã€‚</li>
<li>æ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¯æ§TTSçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†å„ç§æ–¹æ³•ï¼Œä»åŸºæœ¬æ§åˆ¶æŠ€å·§åˆ°è‡ªç„¶è¯­è¨€æç¤ºæ–¹æ³•ã€‚</li>
<li>æ–‡ç« æä¾›äº†å…³äºå¯æ§TTSçš„å½“å‰ç ”ç©¶çŠ¶æ€çš„æ¸…æ™°ç†è§£ï¼ŒåŒ…æ‹¬é€šç”¨ç®¡é“ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a3255132bdca965fc85d69d43568f2f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d8fe56ee1b3d384d421df9dbf984646b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e826485c7661728072f0a00efebdb680.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78d17f9628b8ef660154504c576be31e.jpg" align="middle">
</details>




<h2 id="Not-All-Errors-Are-Equal-Investigation-of-Speech-Recognition-Errors-in-Alzheimerâ€™s-Disease-Detection"><a href="#Not-All-Errors-Are-Equal-Investigation-of-Speech-Recognition-Errors-in-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="Not All Errors Are Equal: Investigation of Speech Recognition Errors in   Alzheimerâ€™s Disease Detection"></a>Not All Errors Are Equal: Investigation of Speech Recognition Errors in   Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Jiawen Kang, Junan Li, Jinchao Li, Xixin Wu, Helen Meng</strong></p>
<p>Automatic Speech Recognition (ASR) plays an important role in speech-based automatic detection of Alzheimerâ€™s disease (AD). However, recognition errors could propagate downstream, potentially impacting the detection decisions. Recent studies have revealed a non-linear relationship between word error rates (WER) and AD detection performance, where ASR transcriptions with notable errors could still yield AD detection accuracy equivalent to that based on manual transcriptions. This work presents a series of analyses to explore the effect of ASR transcription errors in BERT-based AD detection systems. Our investigation reveals that not all ASR errors contribute equally to detection performance. Certain words, such as stopwords, despite constituting a large proportion of errors, are shown to play a limited role in distinguishing AD. In contrast, the keywords related to diagnosis tasks exhibit significantly greater importance relative to other words. These findings provide insights into the interplay between ASR errors and the downstream detection model. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨åŸºäºè¯­éŸ³çš„é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰è‡ªåŠ¨æ£€æµ‹ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œè¯†åˆ«é”™è¯¯å¯èƒ½ä¼šä¼ æ’­åˆ°ä¸‹æ¸¸ï¼Œæ½œåœ¨åœ°å½±å“æ£€æµ‹å†³ç­–ã€‚æœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸ADæ£€æµ‹æ€§èƒ½ä¹‹é—´çš„éçº¿æ€§å…³ç³»ï¼Œå…¶ä¸­ASRè½¬å½•çš„é”™è¯¯æ˜¾è‘—ä½†ä»ç„¶å¯ä»¥äº§ç”Ÿä¸æ‰‹åŠ¨è½¬å½•ç›¸å½“çš„ADæ£€æµ‹å‡†ç¡®æ€§ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ç³»åˆ—åˆ†æï¼Œä»¥æ¢ç´¢ASRè½¬å½•é”™è¯¯åœ¨åŸºäºBERTçš„ADæ£€æµ‹ç³»ç»Ÿä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰çš„ASRé”™è¯¯å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“éƒ½æ˜¯å‡ç­‰çš„ã€‚æŸäº›è¯è¯­ï¼Œå¦‚åœç”¨è¯ï¼Œè™½ç„¶æ„æˆäº†é”™è¯¯çš„å¤§éƒ¨åˆ†ï¼Œä½†åœ¨åŒºåˆ†ADæ–¹é¢çš„ä½œç”¨æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸è¯Šæ–­ä»»åŠ¡ç›¸å…³çš„å…³é”®è¯ç›¸å¯¹äºå…¶ä»–è¯è¯­è¡¨ç°å‡ºæå¤§çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°æä¾›äº†ASRé”™è¯¯ä¸ä¸‹æ¸¸æ£€æµ‹æ¨¡å‹ä¹‹é—´ç›¸äº’ä½œç”¨çš„é‡è¦è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06332v1">PDF</a> Accepted by IEEE ISCSLP 2024</p>
<p><strong>Summary</strong>ï¼šè¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è‡ªåŠ¨æ£€æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å…¶è¯†åˆ«é”™è¯¯å¯èƒ½å½±å“ä¸‹æ¸¸æ£€æµ‹å†³ç­–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸ADæ£€æµ‹æ€§èƒ½ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ï¼ŒASRè½¬å½•ä¸­çš„é”™è¯¯ä»å¯èƒ½è·å¾—ä¸æ‰‹åŠ¨è½¬å½•ç›¸å½“çš„ADæ£€æµ‹å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶åˆ†ææ˜¾ç¤ºï¼Œå¹¶éæ‰€æœ‰ASRé”™è¯¯å¯¹æ£€æµ‹æ€§èƒ½çš„å½±å“ç›¸åŒã€‚æŸäº›è¯å¦‚åœç”¨è¯è™½é”™è¯¯ç‡é«˜ä½†å¯¹åŒºåˆ†ADä½œç”¨æœ‰é™ï¼Œè€Œè¯Šæ–­ä»»åŠ¡ç›¸å…³çš„å…³é”®è¯åˆ™å…·æœ‰æ›´å¤§çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†ASRé”™è¯¯ä¸ä¸‹æ¸¸æ£€æµ‹æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è‡ªåŠ¨æ£€æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>ASRçš„è¯†åˆ«é”™è¯¯å¯èƒ½ä¼šå½±å“ä¸‹æ¸¸çš„æ£€æµ‹å†³ç­–ã€‚</li>
<li>è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸ADæ£€æµ‹æ€§èƒ½ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ã€‚</li>
<li>ASRè½¬å½•ä¸­çš„é”™è¯¯ä»å¯èƒ½è·å¾—ä¸æ‰‹åŠ¨è½¬å½•ç›¸å½“çš„ADæ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ASRé”™è¯¯ä¸­ï¼Œåœç”¨è¯å¯¹åŒºåˆ†ADçš„ä½œç”¨æœ‰é™ã€‚</li>
<li>è¯Šæ–­ä»»åŠ¡ç›¸å…³çš„å…³é”®è¯åœ¨ADæ£€æµ‹ä¸­å…·æœ‰æ›´å¤§çš„é‡è¦æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-23a471c96cae17846ad62e941c0aba0f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4d12ba5e3292041f35109a44b48cb73.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f57551b370fdb18d073632be708ff9c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-925dec1f53b4bcceb4d0abda80f36c1e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f75f571cf0af433be00c20dac8b6560c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cb3a59fb4badf3ac761c2ddd70cb94c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8c4d4cd13dcae93d4cc276661413ad1e.jpg" align="middle">
</details>




<h2 id="Leveraging-Prompt-Learning-and-Pause-Encoding-for-Alzheimerâ€™s-Disease-Detection"><a href="#Leveraging-Prompt-Learning-and-Pause-Encoding-for-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="Leveraging Prompt Learning and Pause Encoding for Alzheimerâ€™s Disease   Detection"></a>Leveraging Prompt Learning and Pause Encoding for Alzheimerâ€™s Disease   Detection</h2><p><strong>Authors:Yin-Long Liu, Rui Feng, Jia-Hong Yuan, Zhen-Hua Ling</strong></p>
<p>Compared to other clinical screening techniques, speech-and-language-based automated Alzheimerâ€™s disease (AD) detection methods are characterized by their non-invasiveness, cost-effectiveness, and convenience. Previous studies have demonstrated the efficacy of fine-tuning pre-trained language models (PLMs) for AD detection. However, the objective of this traditional fine-tuning method, which involves inputting only transcripts, is inconsistent with the masked language modeling (MLM) task used during the pre-training phase of PLMs. In this paper, we investigate prompt-based fine-tuning of PLMs, converting the classification task into a MLM task by inserting prompt templates into the transcript inputs. We also explore the impact of incorporating pause information from forced alignment into manual transcripts. Additionally, we compare the performance of various automatic speech recognition (ASR) models and select the Whisper model to generate ASR-based transcripts for comparison with manual transcripts. Furthermore, majority voting and ensemble techniques are applied across different PLMs (BERT and RoBERTa) using different random seeds. Ultimately, we obtain maximum detection accuracy of 95.8% (with mean 87.9%, std 3.3%) using manual transcripts, achieving state-of-the-art performance for AD detection using only transcripts on the ADReSS test set. </p>
<blockquote>
<p>ä¸å…¶ä»–ä¸´åºŠç­›æŸ¥æŠ€æœ¯ç›¸æ¯”ï¼ŒåŸºäºè¯­è¨€å’Œè¯­éŸ³çš„è‡ªåŠ¨åŒ–é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹æ–¹æ³•å…·æœ‰éä¾µå…¥æ€§ã€æˆæœ¬æ•ˆç›Šå’Œä¾¿æ·æ€§ç­‰ç‰¹ç‚¹ã€‚ä»¥å¾€çš„ç ”ç©¶å·²ç»è¯æ˜äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰è¿›è¡Œå¾®è°ƒåœ¨ADæ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œè¿™ç§ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„ç›®æ ‡ä»…æ¶‰åŠè¾“å…¥æ–‡æœ¬ï¼Œè¿™ä¸é¢„è®­ç»ƒé˜¶æ®µä¸­ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ï¼ˆå³é®è”½è¯­è¨€å»ºæ¨¡ï¼‰æ˜¯ä¸ä¸€è‡´çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºæç¤ºçš„PLMå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡åœ¨è½¬å½•è¾“å…¥ä¸­æ’å…¥æç¤ºæ¨¡æ¿ï¼Œå°†åˆ†ç±»ä»»åŠ¡è½¬æ¢ä¸ºMLMä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å°†å¼ºåˆ¶å¯¹é½ä¸­çš„åœé¡¿ä¿¡æ¯çº³å…¥æ‰‹åŠ¨è½¬å½•çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‰æ‹©äº†Whisperæ¨¡å‹ç”ŸæˆåŸºäºASRçš„è½¬å½•æœ¬ï¼Œä»¥ä¾¿ä¸æ‰‹åŠ¨è½¬å½•æœ¬è¿›è¡Œæ¯”è¾ƒã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜åœ¨ä¸åŒçš„PLMï¼ˆBERTå’ŒRoBERTaï¼‰ä¸Šåº”ç”¨äº†å¤šæ•°æŠ•ç¥¨å’Œé›†æˆæŠ€æœ¯ï¼Œä½¿ç”¨ä¸åŒçš„éšæœºç§å­ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰‹åŠ¨è½¬å½•æœ¬è·å¾—äº†æœ€é«˜95.8%ï¼ˆå¹³å‡å€¼ä¸º87.9%ï¼Œæ ‡å‡†å·®ä¸º3.3%ï¼‰çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œåœ¨ADReSSæµ‹è¯•é›†ä¸Šä»…ä½¿ç”¨è½¬å½•æœ¬å®ç°äº†é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹çš„æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06259v1">PDF</a> Accepted by ISCSLP 2024</p>
<p><strong>Summary</strong><br>     è¯­éŸ³ä¸è¯­è¨€åŸºç¡€çš„è‡ªåŠ¨åŒ–é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹æ–¹æ³•ä¸å…¶ä»–ä¸´åºŠç­›æŸ¥æŠ€æœ¯ç›¸æ¯”å…·æœ‰æ— åˆ›æ€§ã€ç»æµæ€§å’Œä¾¿åˆ©æ€§ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†åŸºäºæç¤ºå¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œå°†åˆ†ç±»ä»»åŠ¡è½¬åŒ–ä¸ºæ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œå¹¶åœ¨è¾“å…¥è½¬å½•ä¸­åŠ å…¥æç¤ºæ¨¡æ¿ã€‚åŒæ—¶ï¼Œç ”ç©¶å°†åœé¡¿ä¿¡æ¯èå…¥æ‰‹åŠ¨è½¬å½•æœ¬ä¸­ï¼Œå¯¹æ¯”å¤šç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹å¹¶é€‰æ‹©Whisperæ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æã€‚æœ€ç»ˆé€šè¿‡é›†æˆä¸åŒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æŠ€æœ¯å®ç°æœ€é«˜æ£€æµ‹å‡†ç¡®ç‡95.8%ï¼ˆå¹³å‡87.9%ï¼Œæ ‡å‡†å·®3.3%ï¼‰ï¼Œåœ¨ADReSSæµ‹è¯•é›†ä¸Šå–å¾—äº†é¡¶å°–è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ä¸è¯­è¨€åŸºç¡€çš„è‡ªåŠ¨åŒ–é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æ–¹æ³•å…·æœ‰æ— åˆ›æ€§ã€ç»æµæ€§å’Œä¾¿åˆ©æ€§ï¼Œä¸ä¼ ç»Ÿä¸´åºŠç­›æŸ¥æŠ€æœ¯ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†æœ‰æ•ˆé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ã€‚</li>
<li>æç¤ºæ¨¡æ¿çš„å¼•å…¥æé«˜äº†åˆ†ç±»ä»»åŠ¡ä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„ä¸€è‡´æ€§ã€‚</li>
<li>åœé¡¿ä¿¡æ¯èå…¥æ‰‹åŠ¨è½¬å½•æœ¬å¯¹é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æ€§èƒ½æœ‰ç§¯æå½±å“ã€‚</li>
<li>å¯¹æ¯”å¤šç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹åé€‰æ‹©äº†Whisperæ¨¡å‹è¿›è¡Œæ€§èƒ½å¯¹æ¯”ã€‚</li>
<li>é›†æˆä¸åŒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æŠ€æœ¯æ˜¾è‘—æé«˜äº†é˜¿å°”èŒ¨æµ·é»˜ç—…çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œè¾¾åˆ°æœ€é«˜95.8%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3f6a0248bcc23e9d044282b14968a12e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e357f48ed6cd9be3ceccb8ce77daaa4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fca418a83e4822c28f2660e570106df.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5879fe12ad6fb1866ec60b59b5f0e7af.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-af720bb59b25f7c6d115ce1046db9e23.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f3350eb69bca3b8ab7c8f3ae43f3997.jpg" align="middle">
</details>




<h2 id="SQ-Whisper-Speaker-Querying-based-Whisper-Model-for-Target-Speaker-ASR"><a href="#SQ-Whisper-Speaker-Querying-based-Whisper-Model-for-Target-Speaker-ASR" class="headerlink" title="SQ-Whisper: Speaker-Querying based Whisper Model for Target-Speaker ASR"></a>SQ-Whisper: Speaker-Querying based Whisper Model for Target-Speaker ASR</h2><p><strong>Authors:Pengcheng Guo, Xuankai Chang, Hang Lv, Shinji Watanabe, Lei Xie</strong></p>
<p>Benefiting from massive and diverse data sources, speech foundation models exhibit strong generalization and knowledge transfer capabilities to a wide range of downstream tasks. However, a limitation arises from their exclusive handling of single-speaker speech input, making them ineffective in recognizing multi-speaker overlapped speech, a common occurrence in real-world scenarios. In this study, we delve into the adaptation of speech foundation models to eliminate interfering speakers from overlapping speech and perform target-speaker automatic speech recognition (TS-ASR). Initially, we utilize the Whisper model as the foundation for adaptation and conduct a thorough comparison of its integration with existing target-speaker adaptation techniques. We then propose an innovative model termed Speaker-Querying Whisper (SQ-Whisper), which employs a set number of trainable queries to capture speaker prompts from overlapping speech based on target-speaker enrollment. These prompts serve to steer the model in extracting speaker-specific features and accurately recognizing target-speaker transcriptions. Experimental results demonstrate that our approach effectively adapts the pre-trained speech foundation model to TS-ASR. Compared with the robust TS-HuBERT model, the proposed SQ-Whisper significantly improves performance, yielding up to 15% and 10% relative reductions in word error rates (WERs) on the Libri2Mix and WSJ0-2Mix datasets, respectively. With data augmentation, we establish new state-of-the-art WERs of 14.6% on the Libri2Mix Test set and 4.4% on the WSJ0-2Mix Test set. Furthermore, we evaluate our model on the real-world AMI meeting dataset, which shows consistent improvement over other adaptation methods. </p>
<blockquote>
<p>å¾—ç›Šäºå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®æ¥æºï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨å¹¿æ³›çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’ŒçŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¤„ç†å•è¯´è¯äººè¯­éŸ³è¾“å…¥çš„å±€é™æ€§å¯¼è‡´å®ƒä»¬åœ¨è¯†åˆ«å¤šè¯´è¯äººé‡å è¯­éŸ³æ—¶æ•ˆæœä¸ä½³ï¼Œè€Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè¿™ç§æƒ…å†µç»å¸¸å‘ç”Ÿã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†è¯­éŸ³åŸºç¡€æ¨¡å‹å¯¹æ¶ˆé™¤é‡å è¯­éŸ³ä¸­çš„å¹²æ‰°è¯´è¯è€…çš„é€‚åº”æ€§ï¼Œå¹¶è¿›è¡Œäº†ç›®æ ‡è¯´è¯äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆTS-ASRï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»¥whisperæ¨¡å‹ä¸ºåŸºç¡€è¿›è¡Œé€‚åº”ï¼Œå¹¶å¯¹å…¶ä¸ç°æœ‰ç›®æ ‡è¯´è¯äººé€‚åº”æŠ€æœ¯çš„é›†æˆè¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¨¡å‹â€”â€”Speaker-Querying Whisperï¼ˆSQ-Whisperï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä½¿ç”¨ä¸€ç»„å¯è®­ç»ƒæŸ¥è¯¢æ¥æ•è·ç›®æ ‡è¯´è¯äººçš„è¯´è¯æç¤ºã€‚è¿™äº›æç¤ºåŸºäºç›®æ ‡è¯´è¯äººçš„æ³¨å†Œä¿¡æ¯ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹æå–è¯´è¯äººç‰¹å®šç‰¹å¾å¹¶å‡†ç¡®è¯†åˆ«ç›®æ ‡è¯´è¯äººçš„è½¬å½•å†…å®¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°é€‚åº”äº†é¢„è®­ç»ƒçš„è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡ŒTS-ASRã€‚ä¸å¼ºå¤§çš„TS-HuBERTæ¨¡å‹ç›¸æ¯”ï¼Œæå‡ºçš„SQ-Whisperæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œåœ¨Libri2Mixå’ŒWSJ0-2Mixæ•°æ®é›†ä¸Šç›¸å¯¹å‡å°‘äº†15%å’Œ10%çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚é€šè¿‡æ•°æ®å¢å¼ºï¼Œæˆ‘ä»¬åœ¨Libri2Mixæµ‹è¯•é›†ä¸Šå»ºç«‹äº†æœ€æ–°çš„æœ€ä½³WERä¸º14.6%ï¼Œåœ¨WSJ0-2Mixæµ‹è¯•é›†ä¸Šä¸º4.4%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œçš„AMIä¼šè®®æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä¸å…¶ä»–é€‚åº”æ–¹æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºæŒç»­çš„å¯æ”¹è¿›ä¹‹å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05589v1">PDF</a> Accepted by IEEE&#x2F;ACM TASLP</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å—ç›Šäºå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„æ•°æ®æºï¼Œè¯­éŸ³åŸºç¡€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’ŒçŸ¥è¯†è¿ç§»èƒ½åŠ›ï¼Œèƒ½å¹¿æ³›åº”ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤„ç†å•è®²è€…è¯­éŸ³è¾“å…¥çš„å±€é™æ€§ä½¿å¾—å®ƒä»¬æ— æ³•æœ‰æ•ˆè¯†åˆ«å¤šè®²è€…é‡å è¯­éŸ³ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­å¾ˆå¸¸è§ã€‚æœ¬ç ”ç©¶è‡´åŠ›äºé€‚åº”è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼Œä»¥æ¶ˆé™¤é‡å è¯­éŸ³ä¸­çš„å¹²æ‰°è®²è€…ï¼Œå¹¶æ‰§è¡Œç›®æ ‡è®²è€…è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆTS-ASRï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨whisperæ¨¡å‹ä½œä¸ºåŸºç¡€è¿›è¡Œé€‚åº”ï¼Œå¹¶ä¸å…¶ä¸ç°æœ‰çš„ç›®æ ‡è®²è€…é€‚åº”æŠ€æœ¯è¿›è¡Œäº†å…¨é¢æ¯”è¾ƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ¨¡å‹â€”â€”Speaker-Querying Whisperï¼ˆSQ-Whisperï¼‰ï¼Œå®ƒåˆ©ç”¨ä¸€ç³»åˆ—å¯è®­ç»ƒçš„æŸ¥è¯¢æ¥æ•æ‰ç›®æ ‡è®²è€…çš„æç¤ºï¼ŒåŸºäºè¿™äº›æç¤ºä»é‡å è¯­éŸ³ä¸­æå–è®²è€…ç‰¹å®šçš„ç‰¹å¾ï¼Œå¹¶å‡†ç¡®è¯†åˆ«ç›®æ ‡è®²è€…çš„è½¬å½•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°é€‚åº”äº†é¢„è®­ç»ƒçš„è¯­éŸ³åŸºç¡€æ¨¡å‹è¿›è¡ŒTS-ASRã€‚ä¸å¼ºå¤§çš„TS-HuBERTæ¨¡å‹ç›¸æ¯”ï¼Œæå‡ºçš„SQ-Whisperæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œåœ¨Libri2Mixå’ŒWSJ0-2Mixæ•°æ®é›†ä¸Šåˆ†åˆ«é™ä½äº†15%å’Œ10%çš„ç›¸å¯¹è¯é”™è¯¯ç‡ï¼ˆWERsï¼‰ã€‚é€šè¿‡æ•°æ®å¢å¼ºï¼Œæˆ‘ä»¬åœ¨Libri2Mixæµ‹è¯•é›†ä¸Šå»ºç«‹äº†æ–°çš„æœ€ä½³WERsä¸º14.6%ï¼Œåœ¨WSJ0-2Mixæµ‹è¯•é›†ä¸Šä¸º4.4%ã€‚æ­¤å¤–ï¼Œåœ¨çœŸå®ä¸–ç•Œçš„AMIä¼šè®®æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæ¯”å…¶ä»–é€‚åº”æ–¹æ³•æ›´æŒç»­çš„æ”¹è¿›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯­éŸ³åŸºç¡€æ¨¡å‹å—ç›Šäºå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–æ•°æ®æºï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–å’ŒçŸ¥è¯†è¿ç§»èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤šè®²è€…é‡å è¯­éŸ³æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡é€‚åº”è¯­éŸ³åŸºç¡€æ¨¡å‹æ¥è¯†åˆ«é‡å è¯­éŸ³ä¸­çš„ç›®æ ‡è®²è€…ã€‚</li>
<li>ä½¿ç”¨whisperæ¨¡å‹ä½œä¸ºåŸºç¡€è¿›è¡Œé€‚åº”ï¼Œå¹¶ä¸å…¶ç°æœ‰çš„ç›®æ ‡è®²è€…é€‚åº”æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>æå‡ºåˆ›æ–°çš„SQ-Whisperæ¨¡å‹ï¼Œåˆ©ç”¨å¯è®­ç»ƒæŸ¥è¯¢æ•æ‰ç›®æ ‡è®²è€…æç¤ºï¼Œä»¥è¯†åˆ«ç›®æ ‡è®²è€…çš„è¯­éŸ³ã€‚</li>
<li>SQ-Whisperåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼ŒåŒ…æ‹¬Libri2Mixã€WSJ0-2Mixå’ŒAMIä¼šè®®æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡æ•°æ®å¢å¼ºï¼Œå»ºç«‹äº†æ–°çš„æœ€ä½³WERsè®°å½•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0269e0f784eb2942914fd6feace9c878.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-90fb044581dc8a0c15def3da08f167fa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8cb941a770eeb8460c7d41edfacb8451.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d41ab9633b6b11d163018635dac73617.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-01c1a780650cab9e91577a713df41bd3.jpg" align="middle">
</details>




<h2 id="WavFusion-Towards-wav2vec-2-0-Multimodal-Speech-Emotion-Recognition"><a href="#WavFusion-Towards-wav2vec-2-0-Multimodal-Speech-Emotion-Recognition" class="headerlink" title="WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition"></a>WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition</h2><p><strong>Authors:Feng Li, Jiusong Luo, Wanjun Xia</strong></p>
<p>Speech emotion recognition (SER) remains a challenging yet crucial task due to the inherent complexity and diversity of human emotions. To address this problem, researchers attempt to fuse information from other modalities via multimodal learning. However, existing multimodal fusion techniques often overlook the intricacies of cross-modal interactions, resulting in suboptimal feature representations. In this paper, we propose WavFusion, a multimodal speech emotion recognition framework that addresses critical research problems in effective multimodal fusion, heterogeneity among modalities, and discriminative representation learning. By leveraging a gated cross-modal attention mechanism and multimodal homogeneous feature discrepancy learning, WavFusion demonstrates improved performance over existing state-of-the-art methods on benchmark datasets. Our work highlights the importance of capturing nuanced cross-modal interactions and learning discriminative representations for accurate multimodal SER. Experimental results on two benchmark datasets (IEMOCAP and MELD) demonstrate that WavFusion succeeds over the state-of-the-art strategies on emotion recognition. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜ä½†è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œè¿™æ˜¯ç”±äºäººç±»æƒ…ç»ªçš„å›ºæœ‰å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜è¯•å›¾é€šè¿‡å¤šæ¨¡æ€å­¦ä¹ èåˆå…¶ä»–æ¨¡æ€çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€èåˆæŠ€æœ¯å¾€å¾€å¿½è§†äº†è·¨æ¨¡æ€äº¤äº’çš„å¤æ‚æ€§ï¼Œå¯¼è‡´ç‰¹å¾è¡¨ç¤ºä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WavFusionï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ¡†æ¶ï¼Œè§£å†³äº†æœ‰æ•ˆå¤šæ¨¡æ€èåˆã€æ¨¡æ€ä¹‹é—´çš„å¼‚è´¨æ€§å’Œåˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ çš„å…³é”®ç ”ç©¶é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨é—¨æ§è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œè·¨æ¨¡æ€å‡åŒ€ç‰¹å¾å·®å¼‚å­¦ä¹ ï¼ŒWavFusionåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å¯¹ç°æœ‰å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†æ•æ‰å¾®å¦™çš„è·¨æ¨¡æ€äº¤äº’å’Œå­¦ä¹ åˆ¤åˆ«æ€§è¡¨ç¤ºå¯¹äºå‡†ç¡®çš„å¤šæ¨¡æ€SERçš„é‡è¦æ€§ã€‚åœ¨IEMOCAPå’ŒMELDä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼ŒWavFusionåœ¨æƒ…æ„Ÿè¯†åˆ«æ–¹é¢è¶…è¿‡äº†æœ€æ–°çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05558v1">PDF</a> Accepted by 31st International Conference on MultiMedia Modeling   (MMM2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºWavFusionçš„å¤šæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«æ¡†æ¶ï¼Œè§£å†³äº†æœ‰æ•ˆå¤šæ¨¡æ€èåˆã€æ¨¡æ€é—´å¼‚è´¨æ€§å’Œåˆ¤åˆ«è¡¨ç¤ºå­¦ä¹ ç­‰å…³é”®ç ”ç©¶é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨é—¨æ§è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œæ¨¡æ€å†…åŒè´¨ç‰¹å¾å·®å¼‚å­¦ä¹ ï¼ŒWavFusionåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æƒ…ç»ªè¯†åˆ«æ–¹æ³•ã€‚æ‘˜è¦æ€»ç»“äº†è¯¥æ–‡çš„ä¸»è¦è´¡çŒ®å’Œå®è·µç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œé‡è¦æ€§çš„é—®é¢˜ï¼Œæ¶‰åŠäººç±»æƒ…ç»ªçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>å¤šæ¨¡æ€èåˆæ˜¯è§£å†³SERé—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œä½†ç°æœ‰æŠ€æœ¯å¸¸å¸¸å¿½ç•¥è·¨æ¨¡æ€äº¤äº’çš„ç»†èŠ‚ï¼Œå¯¼è‡´ç‰¹å¾è¡¨ç¤ºä¸ä½³ã€‚</li>
<li>WavFusionæ¡†æ¶é€šè¿‡åˆ©ç”¨é—¨æ§è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œæ¨¡æ€å†…åŒè´¨ç‰¹å¾å·®å¼‚å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>WavFusionåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æƒ…æ„Ÿè¯†åˆ«æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>æ•è·å¾®å¦™çš„è·¨æ¨¡æ€äº¤äº’å’Œå­¦ä¹ çš„åˆ¤åˆ«è¡¨ç¤ºå¯¹äºå‡†ç¡®çš„å¤šæ¨¡æ€SERè‡³å…³é‡è¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒWavFusionåœ¨æƒ…ç»ªè¯†åˆ«æ–¹é¢è¶…è¿‡äº†ç°æœ‰ç­–ç•¥ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0df1a6a899775fd81b0de99c073f600d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4c4257506de3f0b461148c4cd84f8f27.jpg" align="middle">
</details>




<h2 id="Adaptive-Dropout-for-Pruning-Conformers"><a href="#Adaptive-Dropout-for-Pruning-Conformers" class="headerlink" title="Adaptive Dropout for Pruning Conformers"></a>Adaptive Dropout for Pruning Conformers</h2><p><strong>Authors:Yotaro Kubo, Xingyu Cai, Michiel Bacchiani</strong></p>
<p>This paper proposes a method to effectively perform joint training-and-pruning based on adaptive dropout layers with unit-wise retention probabilities. The proposed method is based on the estimation of a unit-wise retention probability in a dropout layer. A unit that is estimated to have a small retention probability can be considered to be prunable. The retention probability of the unit is estimated using back-propagation and the Gumbel-Softmax technique. This pruning method is applied at several application points in Conformers such that the effective number of parameters can be significantly reduced. Specifically, adaptive dropout layers are introduced in three locations in each Conformer block: (a) the hidden layer of the feed-forward-net component, (b) the query vectors and the value vectors of the self-attention component, and (c) the input vectors of the LConv component. The proposed method is evaluated by conducting a speech recognition experiment on the LibriSpeech task. It was shown that this approach could simultaneously achieve a parameter reduction and accuracy improvement. The word error rates improved by approx 1% while reducing the number of parameters by 54%. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”dropoutå±‚è¿›è¡Œè”åˆè®­ç»ƒä¸å‰ªæçš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºå•ä½ä¿ç•™æ¦‚ç‡çš„ä¼°è®¡ã€‚æ‰€æå‡ºçš„æ–¹æ³•åŸºäºdropoutå±‚ä¸­å•ä½ä¿ç•™æ¦‚ç‡çš„ä¼°è®¡ã€‚ä¼°è®¡ä¿ç•™æ¦‚ç‡è¾ƒå°çš„å•ä½å¯ä»¥è¢«è§†ä¸ºå¯å‰ªæçš„ã€‚è¯¥å•ä½çš„ä¿ç•™æ¦‚ç‡é€šè¿‡ä½¿ç”¨åå‘ä¼ æ’­å’ŒGumbel-SoftmaxæŠ€æœ¯è¿›è¡Œä¼°è®¡ã€‚è¿™ç§å‰ªææ–¹æ³•è¢«åº”ç”¨äºConformersçš„å‡ ä¸ªåº”ç”¨ç‚¹ï¼Œä»è€Œå¯ä»¥æ˜¾è‘—å‡å°‘æœ‰æ•ˆå‚æ•°çš„æ•°é‡ã€‚å…·ä½“æ¥è¯´ï¼Œè‡ªé€‚åº”dropoutå±‚åœ¨æ¯ä¸ªConformerå—çš„ä¸‰ä¸ªä½ç½®å¼•å…¥ï¼šï¼ˆaï¼‰å‰é¦ˆç½‘ç»œç»„ä»¶çš„éšè—å±‚ï¼Œï¼ˆbï¼‰è‡ªæ³¨æ„åŠ›ç»„ä»¶çš„æŸ¥è¯¢å‘é‡å’Œå€¼å‘é‡ï¼Œä»¥åŠï¼ˆcï¼‰LConvç»„ä»¶çš„è¾“å…¥å‘é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨LibriSpeechä»»åŠ¡ä¸Šè¿›è¡Œè¯­éŸ³è¯†åˆ«å®éªŒè¿›è¡Œè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŒæ—¶å®ç°å‚æ•°å‡å°‘å’Œå‡†ç¡®æ€§æé«˜ã€‚è¯é”™è¯¯ç‡æé«˜äº†çº¦1%ï¼ŒåŒæ—¶å‡å°‘äº†54%çš„å‚æ•°æ•°é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04836v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‡ªé€‚åº”dropoutå±‚è¿›è¡Œè”åˆè®­ç»ƒä¸å‰ªæçš„æ–¹æ³•ï¼Œä½¿ç”¨å•å…ƒçº§åˆ«çš„ä¿æŒæ¦‚ç‡æ¥ä¼°è®¡å‰ªæå†³ç­–ã€‚è¿™ç§æ–¹æ³•ç”¨äºé™ä½æ¨¡å‹å‚æ•°æ•°é‡ï¼Œåº”ç”¨äºè¯­éŸ³ä»»åŠ¡å¦‚è¯­éŸ³è¯†åˆ«ï¼Œå®ç°å‚æ•°å‡å°‘ä¸å‡†ç¡®ç‡æå‡çš„åŒé‡æ•ˆæœã€‚åœ¨LibriSpeechä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å‡å°‘äº†çº¦54%çš„å‚æ•°æ•°é‡ï¼ŒåŒæ—¶æé«˜äº†çº¦1%çš„å•è¯é”™è¯¯ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»“åˆè‡ªé€‚åº”dropoutå±‚å’Œå‰ªæç­–ç•¥æ¥å®ç°æ¨¡å‹çš„è”åˆè®­ç»ƒä¸å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>åˆ©ç”¨å•å…ƒçº§åˆ«çš„ä¿æŒæ¦‚ç‡ä¼°è®¡è¿›è¡Œå‰ªæå†³ç­–ï¼Œé¢„è®¡å‰ªæå•å…ƒè¾ƒå°çš„ä¿æŒæ¦‚ç‡å¯è¿›è¡Œåˆ é™¤æ“ä½œã€‚</li>
<li>é‡‡ç”¨åå‘ä¼ æ’­å’ŒGumbel-SoftmaxæŠ€æœ¯æ¥ä¼°è®¡å•å…ƒä¿æŒæ¦‚ç‡ã€‚</li>
<li>è¯¥æ–¹æ³•åº”ç”¨äºConformersä¸­çš„å¤šä¸ªç‚¹ï¼ŒåŒ…æ‹¬å‰é¦ˆç½‘ç»œç»„ä»¶çš„éšè—å±‚ã€è‡ªæ³¨æ„åŠ›ç»„ä»¶çš„æŸ¥è¯¢å‘é‡å’Œä»·å€¼å‘é‡ä»¥åŠLConvç»„ä»¶çš„è¾“å…¥å‘é‡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-61de8512ccb50acd2cb7c5d8aea2d931.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6bf7da1d545bdb8cd2fb379b26d92a97.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ee648bf37e10f28cddd66bc5f5dde437.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-be176b09e1c8d3f699cfb2e4948f7438.jpg" align="middle">
</details>




<h2 id="Integrated-Minimum-Mean-Squared-Error-Algorithms-for-Combined-Acoustic-Echo-Cancellation-and-Noise-Reduction"><a href="#Integrated-Minimum-Mean-Squared-Error-Algorithms-for-Combined-Acoustic-Echo-Cancellation-and-Noise-Reduction" class="headerlink" title="Integrated Minimum Mean Squared Error Algorithms for Combined Acoustic   Echo Cancellation and Noise Reduction"></a>Integrated Minimum Mean Squared Error Algorithms for Combined Acoustic   Echo Cancellation and Noise Reduction</h2><p><strong>Authors:Arnout Roebben, Toon van Waterschoot, Jan Wouters, Marc Moonen</strong></p>
<p>In many speech recording applications, noise and acoustic echo corrupt the desired speech. Consequently, combined noise reduction (NR) and acoustic echo cancellation (AEC) is required. Generally, a cascade approach is followed, i.e., the AEC and NR are designed in isolation by selecting a separate signal model, formulating a separate cost function, and using a separate solution strategy. The AEC and NR are then cascaded one after the other, not accounting for their interaction. In this paper, however, an integrated approach is proposed to consider this interaction in a general multi-microphone&#x2F;multi-loudspeaker setup. Therefore, a single signal model of either the microphone signal vector or the extended signal vector, obtained by stacking microphone and loudspeaker signals, is selected, a single mean squared error cost function is formulated, and a common solution strategy is used. Using this microphone signal model, a multi channel Wiener filter (MWF) is derived. Using the extended signal model, an extended MWF (MWFext) is derived, and several equivalent expressions are found, which nevertheless are interpretable as cascade algorithms. Specifically, the MWFext is shown to be equivalent to algorithms where the AEC precedes the NR (AEC NR), the NR precedes the AEC (NR-AEC), and the extended NR (NRext) precedes the AEC and post-filter (PF) (NRext-AECPF). Under rank-deficiency conditions the MWFext is non-unique, such that this equivalence amounts to the expressions being specific, not necessarily minimum-norm solutions for this MWFext. The practical performances nonetheless differ due to non-stationarities and imperfect correlation matrix estimation, resulting in the AEC-NR and NRext-AEC-PF attaining best overall performance. </p>
<blockquote>
<p>åœ¨è®¸å¤šè¯­éŸ³è®°å½•åº”ç”¨ä¸­ï¼Œå™ªå£°å’Œå£°éŸ³å›å£°ä¼šå¹²æ‰°æ‰€éœ€çš„è¯­éŸ³ã€‚å› æ­¤ï¼Œéœ€è¦è”åˆé™å™ªï¼ˆNRï¼‰å’Œå£°éŸ³å›å£°æ¶ˆé™¤ï¼ˆAECï¼‰ã€‚é€šå¸¸é‡‡ç”¨çš„æ˜¯çº§è”æ–¹æ³•ï¼Œå³ç‹¬ç«‹è®¾è®¡AECå’ŒNRï¼Œé€šè¿‡é€‰æ‹©å•ç‹¬çš„ä¿¡å·æ¨¡å‹ã€åˆ¶å®šå•ç‹¬çš„ä»£ä»·å‡½æ•°å’Œä½¿ç”¨å•ç‹¬çš„è§£å†³æ–¹æ¡ˆç­–ç•¥æ¥å®ç°ã€‚ç„¶åå°†AECå’ŒNRä¸€ä¸ªæ¥ä¸€ä¸ªåœ°è¿›è¡Œçº§è”ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æå‡ºä¸€ç§ç»¼åˆè€ƒè™‘æ–¹æ³•ï¼Œåœ¨ä¸€èˆ¬çš„å¤šä¸ªéº¦å…‹é£&#x2F;å¤šä¸ªæ‰¬å£°å™¨çš„è®¾ç½®ä¸­æ¥è€ƒè™‘è¿™ç§ç›¸äº’ä½œç”¨ã€‚å› æ­¤ï¼Œé€‰æ‹©å•ä¸ªä¿¡å·æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥æ˜¯éº¦å…‹é£ä¿¡å·å‘é‡ä¹Ÿå¯ä»¥æ˜¯é€šè¿‡å¯¹éº¦å…‹é£å’Œæ‰¬å£°å™¨ä¿¡å·è¿›è¡Œå †å è€Œè·å¾—çš„æ‰©å±•ä¿¡å·å‘é‡ã€‚åˆ¶å®šä¸€ä¸ªå•ä¸€çš„å‡æ–¹è¯¯å·®ä»£ä»·å‡½æ•°å¹¶ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚åŸºäºè¿™ç§éº¦å…‹é£ä¿¡å·æ¨¡å‹ï¼Œæ¨å¯¼å‡ºäº†å¤šé€šé“ç»´çº³æ»¤æ³¢å™¨ï¼ˆMWFï¼‰ã€‚åŸºäºæ‰©å±•çš„ä¿¡å·æ¨¡å‹ï¼Œæ¨å¯¼å‡ºäº†æ‰©å±•çš„MWFï¼ˆMWFextï¼‰ï¼Œå¹¶æ‰¾åˆ°äº†å‡ ä¸ªç­‰æ•ˆçš„è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼å¯ä»¥è§£é‡Šä¸ºçº§è”ç®—æ³•ã€‚å…·ä½“æ¥è¯´ï¼ŒMWFextç›¸å½“äºAECå…ˆäºNRï¼ˆAEC NRï¼‰ã€NRå…ˆäºAECï¼ˆNR-AECï¼‰ä»¥åŠæ‰©å±•NRï¼ˆNRextï¼‰å…ˆäºAECå’Œåç½®æ»¤æ³¢å™¨ï¼ˆPFï¼‰ï¼ˆNRext-AECPFï¼‰çš„ç®—æ³•ã€‚åœ¨ç§©ä¸è¶³çš„æƒ…å†µä¸‹ï¼ŒMWFextæ˜¯éå”¯ä¸€çš„ï¼Œå› æ­¤è¿™ç§ç­‰æ•ˆè¡¨ç°ä¸ºè¡¨è¾¾å¼ç‰¹å®šï¼Œä¸ä¸€å®šæ˜¯è¿™ä¸ªMWFextçš„æœ€å°èŒƒæ•°è§£ã€‚ä½†ç”±äºéå¹³ç¨³æ€§å’Œä¸å®Œç¾çš„ç›¸å…³çŸ©é˜µä¼°è®¡ï¼Œå®é™…æ€§èƒ½ä»ç„¶æœ‰æ‰€ä¸åŒï¼Œä½¿å¾—AEC-NRå’ŒNRext-AEC-PFè·å¾—æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04267v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨è¯­éŸ³å½•åˆ¶åº”ç”¨ä¸­ï¼Œå™ªå£°å’Œå›å£°ä¼šå½±å“æ‰€éœ€çš„è¯­éŸ³è´¨é‡ï¼Œå› æ­¤éœ€è¦ç»“åˆé™å™ªï¼ˆNRï¼‰å’Œå£°å›æ¶ˆé™¤ï¼ˆAECï¼‰ã€‚é€šå¸¸é‡‡ç”¨çº§è”æ–¹æ³•ï¼Œå³ç‹¬ç«‹è®¾è®¡AECå’ŒNRï¼Œé€‰æ‹©å•ç‹¬çš„ä¿¡å·æ¨¡å‹ã€åˆ¶å®šå•ç‹¬çš„æˆæœ¬å‡½æ•°å’Œä½¿ç”¨å•ç‹¬çš„è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚ç„¶åä¾æ¬¡è¿›è¡ŒAECå’ŒNRå¤„ç†ï¼Œè€Œä¸è€ƒè™‘å®ƒä»¬çš„ç›¸äº’ä½œç”¨ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»¼åˆè€ƒè™‘è¿™ç§ç›¸äº’ä½œç”¨çš„ä¸€èˆ¬çš„å¤šéº¦å…‹é£&#x2F;å¤šæ‰¬å£°å™¨è®¾ç½®ä¸­çš„é›†æˆæ–¹æ³•ã€‚å› æ­¤ï¼Œé€‰æ‹©äº†éº¦å…‹é£ä¿¡å·å‘é‡æˆ–é€šè¿‡å°†éº¦å…‹é£å’Œæ‰¬å£°å™¨ä¿¡å·å åŠ å¾—åˆ°çš„æ‰©å±•ä¿¡å·å‘é‡çš„å•ä¸€ä¿¡å·æ¨¡å‹ï¼Œåˆ¶å®šäº†å•ä¸€å‡æ–¹è¯¯å·®æˆæœ¬å‡½æ•°ï¼Œå¹¶ä½¿ç”¨é€šç”¨è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚åŸºäºè¿™ç§éº¦å…‹é£ä¿¡å·æ¨¡å‹ï¼Œæ¨å¯¼å‡ºäº†å¤šé€šé“ç»´çº³æ»¤æ³¢å™¨ï¼ˆMWFï¼‰ã€‚åŸºäºæ‰©å±•ä¿¡å·æ¨¡å‹ï¼Œæ¨å¯¼å‡ºäº†æ‰©å±•MWFï¼ˆMWFextï¼‰ï¼Œå¹¶æ‰¾åˆ°äº†å‡ ä¸ªç­‰æ•ˆè¡¨è¾¾å¼ï¼Œå®ƒä»¬å¯ä»¥è§£é‡Šä¸ºçº§è”ç®—æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒMWFextä¸AECå…ˆäºNRï¼ˆAEC NRï¼‰ã€NRå…ˆäºAECï¼ˆNR-AECï¼‰ä»¥åŠæ‰©å±•NRï¼ˆNRextï¼‰å…ˆäºAECå’Œåæ»¤æ³¢å™¨ï¼ˆPFï¼‰ï¼ˆNRext-AECPFï¼‰çš„ç®—æ³•ç­‰æ•ˆã€‚åœ¨ç§©ä¸è¶³çš„æƒ…å†µä¸‹ï¼ŒMWFextæ˜¯éå”¯ä¸€çš„ï¼Œè¿™æ„å‘³ç€è¿™ç§ç­‰æ•ˆæ€§è¡¨ç°ä¸ºè¡¨è¾¾å¼çš„ç‰¹å®šæ€§ï¼Œä¸ä¸€å®šæ˜¯MWFextçš„æœ€å°èŒƒæ•°è§£ã€‚ç„¶è€Œï¼Œç”±äºéå¹³ç¨³æ€§å’Œç›¸å…³çŸ©é˜µä¼°è®¡çš„ä¸å®Œç¾æ€§ï¼Œå®ƒä»¬çš„å®é™…æ€§èƒ½æœ‰æ‰€ä¸åŒï¼Œä½¿å¾—AEC-NRå’ŒNRext-AEC-PFè·å¾—æœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§é›†æˆæ–¹æ³•ï¼Œè€ƒè™‘å™ªå£°æ¶ˆé™¤å’Œå£°å›æ¶ˆé™¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†åŸºäºéº¦å…‹é£ä¿¡å·æ¨¡å‹çš„MWFå’Œå¤šé€šé“ç»´çº³æ»¤æ³¢å™¨æ‰©å±•ç‰ˆæœ¬ï¼ˆMWFextï¼‰ã€‚</li>
<li>MWFextä¸ä¸åŒçš„çº§è”ç®—æ³•è¡¨ç°å‡ºç­‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬AEC NRã€NR-AECã€ä»¥åŠNRext-AECPFã€‚</li>
<li>åœ¨ç§©ä¸è¶³çš„æƒ…å†µä¸‹ï¼ŒMWFextçš„éå”¯ä¸€æ€§ä½¿å¾—ç­‰æ•ˆè¡¨è¾¾å¼çš„é€‚ç”¨æ€§å—é™ã€‚</li>
<li>å®é™…æ€§èƒ½å·®å¼‚æºäºéå¹³ç¨³æ€§å’Œç›¸å…³çŸ©é˜µä¼°è®¡çš„ä¸å®Œç¾æ€§ã€‚</li>
<li>AEC-NRå’ŒNRext-AEC-PFåœ¨å¤„ç†å™ªå£°å’Œå›å£°æ—¶è¡¨ç°å‡ºæœ€ä½³çš„æ•´ä½“æ€§èƒ½ã€‚</li>
<li>è¯¥è®ºæ–‡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥è€ƒè™‘å™ªå£°æ¶ˆé™¤å’Œå£°å›æ¶ˆé™¤çš„ç›¸äº’ä½œç”¨ï¼Œä»è€Œæé«˜äº†è¯­éŸ³å½•åˆ¶çš„è´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b0bac0503dd0460a56d1af13474017f6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14123731d44298749609337b47a3bff9.jpg" align="middle">
</details>




<h2 id="Representation-Purification-for-End-to-End-Speech-Translation"><a href="#Representation-Purification-for-End-to-End-Speech-Translation" class="headerlink" title="Representation Purification for End-to-End Speech Translation"></a>Representation Purification for End-to-End Speech Translation</h2><p><strong>Authors:Chengwei Zhang, Yue Zhou, Rui Zhao, Yidong Chen, Xiaodong Shi</strong></p>
<p>Speech-to-text translation (ST) is a cross-modal task that involves converting spoken language into text in a different language. Previous research primarily focused on enhancing speech translation by facilitating knowledge transfer from machine translation, exploring various methods to bridge the gap between speech and text modalities. Despite substantial progress made, factors in speech that are not relevant to translation content, such as timbre and rhythm, often limit the efficiency of knowledge transfer. In this paper, we conceptualize speech representation as a combination of content-agnostic and content-relevant factors. We examine the impact of content-agnostic factors on translation performance through preliminary experiments and observe a significant performance deterioration when content-agnostic perturbations are introduced to speech signals. To address this issue, we propose a \textbf{S}peech \textbf{R}epresentation \textbf{P}urification with \textbf{S}upervision \textbf{E}nhancement (SRPSE) framework, which excludes the content-agnostic components within speech representations to mitigate their negative impact on ST. Experiments on MuST-C and CoVoST-2 datasets demonstrate that SRPSE significantly improves translation performance across all translation directions in three settings and achieves preeminent performance under a \textit{transcript-free} setting. </p>
<blockquote>
<p>è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘ï¼ˆSTï¼‰æ˜¯ä¸€é¡¹æ¶‰åŠå°†å£è¯­è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬çš„å¤šæ¨¡å¼ä»»åŠ¡ã€‚ä»¥å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é€šè¿‡ä¿ƒè¿›æœºå™¨ç¿»è¯‘çš„çŸ¥è¯†è½¬ç§»æ¥æé«˜è¯­éŸ³ç¿»è¯‘çš„æ€§èƒ½ï¼Œæ¢ç´¢å„ç§æ–¹æ³•æ¥å¼¥åˆè¯­éŸ³å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´çš„å·®è·ã€‚å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¯­éŸ³ä¸­ä¸ç¿»è¯‘å†…å®¹ä¸ç›¸å…³çš„å› ç´ ï¼Œå¦‚éŸ³è´¨å’ŒèŠ‚å¥ï¼Œå¾€å¾€é™åˆ¶äº†çŸ¥è¯†è½¬ç§»çš„æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¯­éŸ³è¡¨ç¤ºæ¦‚å¿µåŒ–ä¸ºä¸å†…å®¹æ— å…³å’Œä¸å†…å®¹ç›¸å…³å› ç´ çš„ç»“åˆã€‚æˆ‘ä»¬é€šè¿‡åˆæ­¥å®éªŒæ£€æŸ¥äº†ä¸å†…å®¹æ— å…³å› ç´ å¯¹ç¿»è¯‘æ€§èƒ½çš„å½±å“ï¼Œå¹¶è§‚å¯Ÿåˆ°å½“å‘è¯­éŸ³ä¿¡å·å¼•å…¥ä¸å†…å®¹æ— å…³çš„å¹²æ‰°æ—¶ï¼Œæ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºSRPSEï¼ˆæœ‰ç›‘ç£å¢å¼ºçš„è¯­éŸ³è¡¨ç¤ºå‡€åŒ–ï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ’é™¤è¯­éŸ³è¡¨ç¤ºä¸­çš„ä¸å†…å®¹æ— å…³çš„æˆåˆ†ï¼Œä»¥å‡è½»å®ƒä»¬å¯¹STçš„è´Ÿé¢å½±å“ã€‚åœ¨MuST-Cå’ŒCoVoST-2æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSRPSEåœ¨ä¸‰ç§è®¾ç½®çš„å„ä¸ªç¿»è¯‘æ–¹å‘ä¸Šæ˜¾è‘—æé«˜äº†ç¿»è¯‘æ€§èƒ½ï¼Œå¹¶åœ¨æ— å­—å¹•è®¾ç½®ä¸‹å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04266v1">PDF</a> Accepted by COLING 2025</p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘ï¼ˆSTï¼‰æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠå°†å£è¯­è½¬åŒ–ä¸ºå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ã€‚å°½ç®¡å–å¾—äº†å®è´¨æ€§è¿›å±•ï¼Œä½†è¯­éŸ³ä¸­é‚£äº›ä¸ç¿»è¯‘å†…å®¹æ— å…³çš„å› ç´ ï¼ˆå¦‚éŸ³è‰²å’ŒèŠ‚å¥ï¼‰ç»å¸¸é™åˆ¶çŸ¥è¯†è½¬ç§»çš„æ•ˆç‡ã€‚æœ¬æ–‡æ¦‚å¿µåŒ–è¯­éŸ³è¡¨ç¤ºä¸ºä¸å†…å®¹æ— å…³å’Œä¸å†…å®¹ç›¸å…³å› ç´ çš„ç»„åˆï¼Œå¹¶é€šè¿‡åˆæ­¥å®éªŒå‘ç°å†…å®¹æ— å…³çš„å¹²æ‰°å¯¹ç¿»è¯‘æ€§èƒ½æœ‰é‡å¤§å½±å“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSRPSEçš„è¯­éŸ³è¡¨ç¤ºå‡€åŒ–ä¸ç›‘ç£å¢å¼ºæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ’é™¤è¯­éŸ³è¡¨ç¤ºä¸­çš„ä¸å†…å®¹æ— å…³çš„æˆåˆ†ï¼Œå‡è½»å…¶å¯¹STçš„è´Ÿé¢å½±å“ã€‚å®éªŒè¡¨æ˜ï¼ŒSRPSEåœ¨ä¸‰ç§è®¾ç½®ä¸‹çš„æ‰€æœ‰ç¿»è¯‘æ–¹å‘ä¸Šå‡æ˜¾è‘—æé«˜ç¿»è¯‘æ€§èƒ½ï¼Œå¹¶åœ¨æ— å­—å¹•è®¾ç½®ä¸‹å®ç°å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘æ˜¯è·¨æ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠå°†å£è¯­è½¬åŒ–ä¸ºå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ã€‚</li>
<li>è¯­éŸ³ä¸­ä¸å†…å®¹æ— å…³çš„å› ç´ ï¼ˆå¦‚éŸ³è‰²å’ŒèŠ‚å¥ï¼‰å¯¹ç¿»è¯‘æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>åˆæ­¥å®éªŒè¡¨æ˜ï¼Œå†…å®¹æ— å…³çš„å¹²æ‰°ä¼šå¯¼è‡´ç¿»è¯‘æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†SRPSEæ¡†æ¶ï¼Œæ—¨åœ¨æ’é™¤è¯­éŸ³è¡¨ç¤ºä¸­çš„ä¸å†…å®¹æ— å…³çš„æˆåˆ†ã€‚</li>
<li>SRPSEæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨å„ç§ç¿»è¯‘è®¾ç½®ä¸‹æ˜¾è‘—æé«˜ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>SRPSEæ¡†æ¶åœ¨æ— å­—å¹•è®¾ç½®ä¸‹å®ç°å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-39ac38b427489dc7a007a0d9ea84ab5d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-09646bac044a7c0900950d873f892997.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e328f7574b74ca68355294f5f501c073.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5af9f7e86ca76bf6d23a7f2ba9ab6956.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-45160a2d20550d9ac68b541a38d66491.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-17d59c5bf3e4fbfcccf9c644a1d5f1c3.jpg" align="middle">
</details>




<h2 id="Comprehensive-Audio-Query-Handling-System-with-Integrated-Expert-Models-and-Contextual-Understanding"><a href="#Comprehensive-Audio-Query-Handling-System-with-Integrated-Expert-Models-and-Contextual-Understanding" class="headerlink" title="Comprehensive Audio Query Handling System with Integrated Expert Models   and Contextual Understanding"></a>Comprehensive Audio Query Handling System with Integrated Expert Models   and Contextual Understanding</h2><p><strong>Authors:Vakada Naveen, Arvind Krishna Sridhar, Yinyi Guo, Erik Visser</strong></p>
<p>This paper presents a comprehensive chatbot system designed to handle a wide range of audio-related queries by integrating multiple specialized audio processing models. The proposed system uses an intent classifier, trained on a diverse audio query dataset, to route queries about audio content to expert models such as Automatic Speech Recognition (ASR), Speaker Diarization, Music Identification, and Text-to-Audio generation. A 3.8 B LLM model then takes inputs from an Audio Context Detection (ACD) module extracting audio event information from the audio and post processes text domain outputs from the expert models to compute the final response to the user. We evaluated the system on custom audio tasks and MMAU sound set benchmarks. The custom datasets were motivated by target use cases not covered in industry benchmarks and included ACD-timestamp-QA (Question Answering) as well as ACD-temporal-QA datasets to evaluate timestamp and temporal reasoning questions, respectively. First we determined that a BERT based Intent Classifier outperforms LLM-fewshot intent classifier in routing queries. Experiments further show that our approach significantly improves accuracy on some custom tasks compared to state-of-the-art Large Audio Language Models and outperforms models in the 7B parameter size range on the sound testset of the MMAU benchmark, thereby offering an attractive option for on device deployment. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç»¼åˆèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡é›†æˆå¤šä¸ªä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§éŸ³é¢‘ç›¸å…³æŸ¥è¯¢ã€‚æ‰€æå‡ºçš„ç³»ç»Ÿä½¿ç”¨æ„å›¾åˆ†ç±»å™¨ï¼ˆåŸºäºå¤šæ ·åŒ–çš„éŸ³é¢‘æŸ¥è¯¢æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼‰ï¼Œå°†å…³äºéŸ³é¢‘å†…å®¹çš„æŸ¥è¯¢è·¯ç”±åˆ°ä¸“å®¶æ¨¡å‹ï¼Œå¦‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯´è¯äººèº«ä»½è¯†åˆ«ã€éŸ³ä¹è¯†åˆ«å’Œæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆç­‰ã€‚ç„¶åï¼Œä¸€ä¸ª3.8 BLLMæ¨¡å‹ä»éŸ³é¢‘ä¸Šä¸‹æ–‡æ£€æµ‹ï¼ˆACDï¼‰æ¨¡å—æ¥æ”¶è¾“å…¥ï¼Œè¯¥æ¨¡å—ä»éŸ³é¢‘ä¸­æå–éŸ³é¢‘äº‹ä»¶ä¿¡æ¯ï¼Œå¹¶å¯¹ä¸“å®¶æ¨¡å‹çš„æ–‡æœ¬é¢†åŸŸè¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œä»¥è®¡ç®—æœ€ç»ˆçš„ç”¨æˆ·å“åº”ã€‚æˆ‘ä»¬åœ¨è‡ªå®šä¹‰çš„éŸ³é¢‘ä»»åŠ¡å’ŒMMAUå£°éŸ³é›†åŸºå‡†æµ‹è¯•ä¸Šå¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚è‡ªå®šä¹‰æ•°æ®é›†çš„ç›®æ ‡ç”¨ä¾‹æ˜¯è¡Œä¸šåŸºå‡†æµ‹è¯•æœªæ¶µç›–çš„é¢†åŸŸï¼ŒåŒ…æ‹¬ACDæ—¶é—´æˆ³é—®ç­”ï¼ˆé—®é¢˜å›ç­”ï¼‰ä»¥åŠACDæ—¶é—´æ¨ç†é—®ç­”æ•°æ®é›†ï¼Œä»¥è¯„ä¼°æ—¶é—´æˆ³å’Œæ—¶é—´æ¨ç†é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šåŸºäºBERTçš„æ„å›¾åˆ†ç±»å™¨åœ¨è·¯ç”±æŸ¥è¯¢æ–¹é¢ä¼˜äºLLMå°‘é‡æ„å›¾åˆ†ç±»å™¨ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œä¸æœ€æ–°çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€äº›è‡ªå®šä¹‰ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§æœ‰äº†æ˜¾è‘—æé«˜ï¼Œå¹¶ä¸”åœ¨MMAUåŸºå‡†æµ‹è¯•çš„å£°éŸ³æµ‹è¯•é›†ä¸Šä¼˜äºå‚æ•°å¤§å°ä¸º7 Bçš„æ¨¡å‹ï¼Œå› æ­¤æˆä¸ºäº†è®¾å¤‡éƒ¨ç½²çš„å¸å¼•åŠ›é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03980v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªç»¼åˆèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡é›†æˆå¤šä¸ªä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¹¿æ³›çš„éŸ³é¢‘æŸ¥è¯¢ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨è®­ç»ƒäºå¤šæ ·åŒ–éŸ³é¢‘æŸ¥è¯¢æ•°æ®é›†çš„æ„å›¾åˆ†ç±»å™¨ï¼Œå°†å…³äºéŸ³é¢‘å†…å®¹çš„æŸ¥è¯¢è·¯ç”±åˆ°ä¸“å®¶æ¨¡å‹ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€è¯´è¯äººåˆ’åºã€éŸ³ä¹è¯†åˆ«å’Œæ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆç­‰ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†éŸ³é¢‘ä¸Šä¸‹æ–‡æ£€æµ‹æ¨¡å—æå–çš„éŸ³é¢‘äº‹ä»¶ä¿¡æ¯ï¼Œå¯¹ä¸“å®¶æ¨¡å‹çš„æ–‡æœ¬é¢†åŸŸè¾“å‡ºè¿›è¡Œåå¤„ç†ï¼Œè®¡ç®—æœ€ç»ˆçš„ç”¨æˆ·å“åº”ã€‚åœ¨è‡ªå®šä¹‰éŸ³é¢‘ä»»åŠ¡å’ŒMMAUå£°éŸ³é›†åŸºå‡†æµ‹è¯•ä¸Šå¯¹è¯¥ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨æŸäº›è‡ªå®šä¹‰ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§æ˜¾è‘—æé«˜ï¼Œä¸”åœ¨MMAUåŸºå‡†æµ‹è¯•çš„å£°éŸ³æµ‹è¯•é›†ä¸Šä¼˜äºå…¶ä»–å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªèŠå¤©æœºå™¨äººç³»ç»Ÿï¼Œèƒ½å¤„ç†å¹¿æ³›çš„éŸ³é¢‘æŸ¥è¯¢ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡é›†æˆå¤šä¸ªä¸“ä¸šéŸ³é¢‘å¤„ç†æ¨¡å‹æ¥å®ç°å…¶åŠŸèƒ½ã€‚</li>
<li>æ„å›¾åˆ†ç±»å™¨ç”¨äºå°†éŸ³é¢‘æŸ¥è¯¢è·¯ç”±åˆ°ç›¸åº”çš„ä¸“å®¶æ¨¡å‹ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç”¨äºå¤„ç†éŸ³é¢‘ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶è®¡ç®—æœ€ç»ˆå“åº”ã€‚</li>
<li>åœ¨è‡ªå®šä¹‰éŸ³é¢‘ä»»åŠ¡å’ŒMMAUåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>BERTåŸºç¡€çš„æ„å›¾åˆ†ç±»å™¨åœ¨è·¯ç”±æŸ¥è¯¢æ–¹é¢ä¼˜äºLLMå°‘æ ·æœ¬æ„å›¾åˆ†ç±»å™¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6ea15997db5c8d3e6ea3d6c6aebc4bca.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fd5a5f62d5c97ab57d5457cf39c1bdef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-111b01e077285fcdd1c99c2c6fbc629e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-08280d82e3327f1edc7431017d601405.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78c6d808f1efd9d28a8876201792e70a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b352bb214a115912ede7ef7af5bd0e86.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e6a3c4fbff638e8c007aac6b000881c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9b5424f03cea9eaff3d8bff9afb8b2be.jpg" align="middle">
</details>




<h2 id="ASR-EC-Benchmark-Evaluating-Large-Language-Models-on-Chinese-ASR-Error-Correction"><a href="#ASR-EC-Benchmark-Evaluating-Large-Language-Models-on-Chinese-ASR-Error-Correction" class="headerlink" title="ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error   Correction"></a>ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error   Correction</h2><p><strong>Authors:Victor Junqiu Wei, Weicheng Wang, Di Jiang, Yuanfeng Song, Lu Wang</strong></p>
<p>Automatic speech Recognition (ASR) is a fundamental and important task in the field of speech and natural language processing. It is an inherent building block in many applications such as voice assistant, speech translation, etc. Despite the advancement of ASR technologies in recent years, it is still inevitable for modern ASR systems to have a substantial number of erroneous recognition due to environmental noise, ambiguity, etc. Therefore, the error correction in ASR is crucial.   Motivated by this, this paper studies ASR error correction in the Chinese language, which is one of the most popular languages and enjoys a large number of users in the world. We first create a benchmark dataset named \emph{ASR-EC} that contains a wide spectrum of ASR errors generated by industry-grade ASR systems. To the best of our knowledge, it is the first Chinese ASR error correction benchmark. Then, inspired by the recent advances in \emph{large language models (LLMs)}, we investigate how to harness the power of LLMs to correct ASR errors. We apply LLMs to ASR error correction in three paradigms. The first paradigm is prompting, which is further categorized as zero-shot, few-shot, and multi-step. The second paradigm is finetuning, which finetunes LLMs with ASR error correction data. The third paradigm is multi-modal augmentation, which collectively utilizes the audio and ASR transcripts for error correction. Extensive experiments reveal that prompting is not effective for ASR error correction. Finetuning is effective only for a portion of LLMs. Multi-modal augmentation is the most effective method for error correction and achieves state-of-the-art performance. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ˜¯è¯­éŸ³å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬ä¸”é‡è¦çš„ä»»åŠ¡ã€‚å®ƒæ˜¯è®¸å¤šåº”ç”¨ç¨‹åºï¼ˆå¦‚è¯­éŸ³åŠ©æ‰‹ã€è¯­éŸ³è¯†åˆ«ç­‰ï¼‰ä¸­çš„å›ºæœ‰ç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡è¿‘å¹´æ¥ASRæŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºç¯å¢ƒå™ªå£°ã€æ­§ä¹‰ç­‰å› ç´ ï¼Œç°ä»£ASRç³»ç»Ÿä»ç„¶å­˜åœ¨å¤§é‡é”™è¯¯è¯†åˆ«ã€‚å› æ­¤ï¼ŒASRä¸­çš„é”™è¯¯æ ¡æ­£è‡³å…³é‡è¦ã€‚æœ¬æ–‡å—åˆ°æ­¤å¯å‘ï¼Œç ”ç©¶ä¸­æ–‡ASRé”™è¯¯æ ¡æ­£ã€‚ä¸­æ–‡æ˜¯ä¸–ç•Œä¸Šæœ€æµè¡Œã€ç”¨æˆ·æ•°é‡æœ€å¤šçš„è¯­è¨€ä¹‹ä¸€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºASR-ECçš„åŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ç”±å·¥ä¸šçº§ASRç³»ç»Ÿç”Ÿæˆçš„å¹¿æ³›ASRé”™è¯¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸­æ–‡ASRé”™è¯¯æ ¡æ­£åŸºå‡†ã€‚ç„¶åï¼Œå—åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•åˆ©ç”¨LLMçš„åŠ›é‡æ¥çº æ­£ASRé”™è¯¯ã€‚æˆ‘ä»¬å°†LLMåº”ç”¨äºASRé”™è¯¯æ ¡æ­£çš„ä¸‰ç§èŒƒå¼ä¸­ã€‚ç¬¬ä¸€ç§èŒƒå¼æ˜¯æç¤ºï¼Œå®ƒè¿›ä¸€æ­¥åˆ†ä¸ºé›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œå¤šæ­¥æç¤ºã€‚ç¬¬äºŒç§èŒƒå¼æ˜¯å¾®è°ƒï¼Œå®ƒä½¿ç”¨ASRé”™è¯¯æ ¡æ­£æ•°æ®å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚ç¬¬ä¸‰ç§èŒƒå¼æ˜¯å¤šæ¨¡æ€å¢å¼ºï¼Œå®ƒç»“åˆéŸ³é¢‘å’ŒASRè½¬å½•è¿›è¡Œé”™è¯¯æ ¡æ­£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæç¤ºå¯¹äºASRé”™è¯¯æ ¡æ­£å¹¶ä¸æœ‰æ•ˆã€‚å¾®è°ƒåªå¯¹éƒ¨åˆ†LLMæœ‰æ•ˆã€‚å¤šæ¨¡æ€å¢å¼ºæ˜¯æœ€æœ‰æ•ˆçš„é”™è¯¯æ ¡æ­£æ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03075v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ç ”ç©¶äº†åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é”™è¯¯æ ¡æ­£é—®é¢˜ã€‚åˆ›å»ºäº†é¦–ä¸ªä¸­æ–‡ASRé”™è¯¯æ ¡æ­£åŸºå‡†æ•°æ®é›†ASR-ECï¼Œå¹¶æ¢ç´¢äº†ä¸‰ç§åˆ©ç”¨LLMsè¿›è¡ŒASRé”™è¯¯æ ¡æ­£çš„æ–¹æ³•ï¼šæç¤ºæ³•ã€å¾®è°ƒæ³•å’Œå¤šæ¨¡æ€å¢å¼ºæ³•ã€‚å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€å¢å¼ºæ³•æ˜¯æœ€æœ‰æ•ˆçš„é”™è¯¯æ ¡æ­£æ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†ç›®å‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡ç ”ç©¶äº†ä¸­æ–‡è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é”™è¯¯æ ¡æ­£é—®é¢˜ï¼Œåˆ›å»ºäº†é¦–ä¸ªç›¸å…³åŸºå‡†æ•°æ®é›†ASR-ECã€‚</li>
<li>åˆ©ç”¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡ŒASRé”™è¯¯æ ¡æ­£çš„æ–¹æ³•è¢«æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸‰ç§åˆ©ç”¨LLMsè¿›è¡ŒASRé”™è¯¯æ ¡æ­£çš„æ–¹æ³•ï¼šæç¤ºæ³•ã€å¾®è°ƒæ³•å’Œå¤šæ¨¡æ€å¢å¼ºæ³•ã€‚</li>
<li>æç¤ºæ³•å¯¹äºASRé”™è¯¯æ ¡æ­£å¹¶ä¸æœ‰æ•ˆã€‚</li>
<li>å¾®è°ƒæ³•ä»…å¯¹éƒ¨åˆ†LLMsæœ‰æ•ˆã€‚</li>
<li>å¤šæ¨¡æ€å¢å¼ºæ³•æ˜¯æœ€æœ‰æ•ˆçš„ASRé”™è¯¯æ ¡æ­£æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5cfdd3e4dfcb504e0d792d9aa4f1508a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2d16c81aced7163592b924926cbace76.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-57fa4ca173e8ebef5677b3d179071787.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d526ca774f9a372b3fa3b1ac4e729bc1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e6691038694aab6dfdf31531c021015b.jpg" align="middle">
</details>




<h2 id="GLM-4-Voice-Towards-Intelligent-and-Human-Like-End-to-End-Spoken-Chatbot"><a href="#GLM-4-Voice-Towards-Intelligent-and-Human-Like-End-to-End-Spoken-Chatbot" class="headerlink" title="GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken   Chatbot"></a>GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken   Chatbot</h2><p><strong>Authors:Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, Jie Tang</strong></p>
<p>We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice">https://github.com/THUDM/GLM-4-Voice</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/glm-4-voice-9b">https://huggingface.co/THUDM/glm-4-voice-9b</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†GLM-4-Voiceï¼Œè¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¸”äººæ€§åŒ–çš„ç«¯åˆ°ç«¯è¯­éŸ³èŠå¤©æœºå™¨äººã€‚å®ƒæ”¯æŒä¸­æ–‡å’Œè‹±è¯­ï¼Œèƒ½è¿›è¡Œå®æ—¶è¯­éŸ³å¯¹è¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ·æŒ‡ä»¤å˜åŒ–æƒ…ç»ªã€è¯­è°ƒã€è¯­é€Ÿå’Œæ–¹è¨€ç­‰è¯­éŸ³ç»†å¾®å·®åˆ«ã€‚GLM-4-Voiceä½¿ç”¨è¶…ä½æ¯”ç‰¹ç‡ï¼ˆ175bpsï¼‰çš„å•ç æœ¬è¯­éŸ³æ ‡è®°å™¨ï¼Œä»¥12.5Hzçš„å¸§ç‡ä»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸­è¡ç”Ÿå‡ºæ¥ï¼Œé€šè¿‡åœ¨ç¼–ç å™¨ä¸­åŠ å…¥å‘é‡é‡åŒ–ç“¶é¢ˆã€‚ä¸ºäº†æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»æ–‡æœ¬è½¬ç§»åˆ°è¯­éŸ³æ¨¡å¼ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬åˆ°ä»¤ç‰Œæ¨¡å‹ï¼Œé€šè¿‡ç°æœ‰æ–‡æœ¬é¢„è®­ç»ƒè¯­æ–™åº“åˆæˆè¯­éŸ³æ–‡æœ¬äº¤ç»‡æ•°æ®ã€‚æˆ‘ä»¬ç»§ç»­ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬è¯­è¨€æ¨¡å‹GLM-4-9Bè¿›è¡Œé¢„è®­ç»ƒï¼Œç»“åˆæ— ç›‘ç£è¯­éŸ³æ•°æ®ã€äº¤ç»‡è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œç›‘ç£è¯­éŸ³æ–‡æœ¬æ•°æ®ï¼Œæ‰©å±•åˆ°1ä¸‡äº¿ä¸ªä»¤ç‰Œï¼Œåœ¨è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³é—®ç­”æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜è´¨é‡çš„å¯¹è¯è¯­éŸ³æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡æ–¹é¢éƒ½è¾¾åˆ°äº†ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚å¼€æ”¾æ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice%E5%92%8Chttps://huggingface.co/THUDM/glm-4-voice-9b%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/THUDM/GLM-4-Voiceå’Œhttps://huggingface.co/THUDM/glm-4-voice-9bè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GLM-4-Voiceæ˜¯ä¸€æ¬¾æ™ºèƒ½ã€äººæ€§åŒ–çš„ç«¯åˆ°ç«¯è¯­éŸ³èŠå¤©æœºå™¨äººï¼Œæ”¯æŒä¸­è‹±æ–‡å®æ—¶è¯­éŸ³å¯¹è¯ï¼Œå¯æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è°ƒæ•´è¯­éŸ³æƒ…æ„Ÿã€è¯­è°ƒã€è¯­é€Ÿå’Œæ–¹è¨€ã€‚å®ƒé‡‡ç”¨è¶…ä½æ¯”ç‰¹ç‡ï¼ˆ175bpsï¼‰å’Œ12.5Hzå¸§ç‡çš„å•ç¼–ç æœ¬è¯­éŸ³æ ‡è®°å™¨ï¼Œç»“åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œèå…¥å‘é‡é‡åŒ–ç“¶é¢ˆåˆ°ç¼–ç å™¨ã€‚é€šè¿‡åˆæˆè¯­éŸ³æ–‡æœ¬äº¤ç»‡æ•°æ®ï¼Œä»ç°æœ‰æ–‡æœ¬é¢„è®­ç»ƒè¯­æ–™åº“ä¸­é«˜æ•ˆè½¬æ¢çŸ¥è¯†ã€‚é¢„è®­ç»ƒæ¨¡å‹ç»“åˆæ— ç›‘ç£è¯­éŸ³æ•°æ®ã€äº¤ç»‡è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œç›‘ç£è¯­éŸ³æ–‡æœ¬æ•°æ®ï¼Œæ‰©å±•åˆ°1ä¸‡äº¿ä¸ªä»¤ç‰Œï¼Œåœ¨è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³é—®ç­”æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é€šè¿‡é«˜è´¨é‡å¯¹è¯è¯­éŸ³æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¾¾åˆ°å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡æ–¹é¢çš„å“è¶Šæ€§èƒ½ã€‚å…¬å¼€æ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice%E5%92%8Chttps://huggingface.co/THUDM/glm-4-voice-9b%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/THUDM/GLM-4-Voiceå’Œhttps://huggingface.co/THUDM/glm-4-voice-9bè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLM-4-Voiceæ˜¯ä¸€ä¸ªæ™ºèƒ½ã€äººæ€§åŒ–çš„ç«¯åˆ°ç«¯è¯­éŸ³èŠå¤©æœºå™¨äººï¼Œæ”¯æŒä¸­è‹±æ–‡å®æ—¶å¯¹è¯ã€‚</li>
<li>GLM-4-Voiceèƒ½æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è°ƒæ•´è¯­éŸ³æƒ…æ„Ÿã€è¯­è°ƒã€è¯­é€Ÿå’Œæ–¹è¨€ã€‚</li>
<li>å®ƒé‡‡ç”¨è¶…ä½æ¯”ç‰¹ç‡å’Œå¸§ç‡çš„å•ç¼–ç æœ¬è¯­éŸ³æ ‡è®°å™¨æŠ€æœ¯ç»“åˆASRæ¨¡å‹ã€‚</li>
<li>é€šè¿‡åˆæˆè¯­éŸ³æ–‡æœ¬äº¤ç»‡æ•°æ®å®ç°ä»æ–‡æœ¬åˆ°è¯­éŸ³çš„é«˜æ•ˆçŸ¥è¯†è½¬æ¢ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹ç»“åˆäº†å¤šç§æ•°æ®ï¼ŒåŒ…æ‹¬æ— ç›‘ç£ã€äº¤ç»‡å’Œç›‘ç£çš„è¯­éŸ³æ–‡æœ¬æ•°æ®ã€‚</li>
<li>GLM-4-Voiceåœ¨è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³é—®ç­”æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93d5efd16c01204f21a643086af317c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-908c87d006e309e0892b8537872b60db.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-77dae6055a3d14522e2162907a1c2c7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0b8d53c63f5d7628dd1b39b898e202ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f88916a03b6edfe48920863e0a168c23.jpg" align="middle">
</details>




<h2 id="It-Takes-Two-Real-time-Co-Speech-Two-personâ€™s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model"><a href="#It-Takes-Two-Real-time-Co-Speech-Two-personâ€™s-Interaction-Generation-via-Reactive-Auto-regressive-Diffusion-Model" class="headerlink" title="It Takes Two: Real-time Co-Speech Two-personâ€™s Interaction Generation   via Reactive Auto-regressive Diffusion Model"></a>It Takes Two: Real-time Co-Speech Two-personâ€™s Interaction Generation   via Reactive Auto-regressive Diffusion Model</h2><p><strong>Authors:Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, Taku Komura</strong></p>
<p>Conversational scenarios are very common in real-world settings, yet existing co-speech motion synthesis approaches often fall short in these contexts, where one personâ€™s audio and gestures will influence the otherâ€™s responses. Additionally, most existing methods rely on offline sequence-to-sequence frameworks, which are unsuitable for online applications. In this work, we introduce an audio-driven, auto-regressive system designed to synthesize dynamic movements for two characters during a conversation. At the core of our approach is a diffusion-based full-body motion synthesis model, which is conditioned on the past states of both characters, speech audio, and a task-oriented motion trajectory input, allowing for flexible spatial control. To enhance the modelâ€™s ability to learn diverse interactions, we have enriched existing two-person conversational motion datasets with more dynamic and interactive motions. We evaluate our system through multiple experiments to show it outperforms across a variety of tasks, including single and two-person co-speech motion generation, as well as interactive motion generation. To the best of our knowledge, this is the first system capable of generating interactive full-body motions for two characters from speech in an online manner. </p>
<blockquote>
<p>å¯¹è¯åœºæ™¯åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­éå¸¸å¸¸è§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ååŒè¯­éŸ³è¿åŠ¨åˆæˆæ–¹æ³•åœ¨è¿™ç§æƒ…å†µä¸‹å¾€å¾€è¡¨ç°ä¸è¶³ï¼Œä¸€ä¸ªäººçš„éŸ³é¢‘å’Œæ‰‹åŠ¿ä¼šå½±å“å¦ä¸€ä¸ªäººçš„ååº”ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºç¦»çº¿åºåˆ—åˆ°åºåˆ—æ¡†æ¶ï¼Œè¿™ä¸é€‚ç”¨äºåœ¨çº¿åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªéŸ³é¢‘é©±åŠ¨çš„è‡ªåŠ¨å›å½’ç³»ç»Ÿï¼Œæ—¨åœ¨åˆæˆå¯¹è¯æœŸé—´ä¸¤ä¸ªè§’è‰²çš„åŠ¨æ€åŠ¨ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å…¨èº«è¿åŠ¨åˆæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å—ä¸¤ä¸ªè§’è‰²è¿‡å»çŠ¶æ€ã€è¯­éŸ³éŸ³é¢‘å’Œé¢å‘ä»»åŠ¡çš„è¿åŠ¨è½¨è¿¹è¾“å…¥çš„åˆ¶çº¦ï¼Œå¯å®ç°çµæ´»çš„ç©ºé—´æ§åˆ¶ã€‚ä¸ºäº†æé«˜æ¨¡å‹å­¦ä¹ å„ç§äº¤äº’çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸°å¯Œäº†ç°æœ‰çš„ä¸¤äººå¯¹è¯è¿åŠ¨æ•°æ®é›†ï¼ŒåŠ å…¥äº†æ›´å¤šåŠ¨æ€å’Œäº¤äº’æ€§çš„åŠ¨ä½œã€‚æˆ‘ä»¬é€šè¿‡å¤šæ¬¡å®éªŒè¯„ä¼°äº†æˆ‘ä»¬çš„ç³»ç»Ÿï¼Œç»“æœè¡¨æ˜å®ƒåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°éƒ½ä¼˜äºå…¶ä»–ç³»ç»Ÿï¼ŒåŒ…æ‹¬å•äººåŠä¸¤äººååŒè¯­éŸ³è¿åŠ¨ç”Ÿæˆä»¥åŠäº¤äº’è¿åŠ¨ç”Ÿæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿä»¥åœ¨çº¿æ–¹å¼ä»è¯­éŸ³ä¸ºä¸¤ä¸ªè§’è‰²ç”Ÿæˆäº¤äº’å…¨èº«åŠ¨ä½œçš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02419v1">PDF</a> 15 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºéŸ³é¢‘çš„ã€è‡ªå›å½’ç³»ç»Ÿï¼Œç”¨äºåˆæˆå¯¹è¯åœºæ™¯ä¸­çš„ä¸¤ä¸ªè§’è‰²çš„åŠ¨æ€åŠ¨ä½œã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€çš„å…¨èº«åŠ¨ä½œåˆæˆæ¨¡å‹ï¼Œæ ¹æ®å¯¹è¯åŒæ–¹è¿‡å»çš„çŠ¶æ€ã€è¯­éŸ³éŸ³é¢‘å’Œä»»åŠ¡å¯¼å‘çš„åŠ¨ä½œè½¨è¿¹è¾“å…¥è¿›è¡Œæ¡ä»¶æ§åˆ¶ï¼Œå®ç°çµæ´»çš„ç©ºé—´æ§åˆ¶ã€‚é€šè¿‡ä¸°å¯Œä¸¤äººå¯¹è¯åŠ¨ä½œæ•°æ®é›†ï¼Œæé«˜äº†æ¨¡å‹å­¦ä¹ å„ç§äº’åŠ¨çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å•äººåŠåŒäººå…±è¯­åŠ¨ä½œç”Ÿæˆã€äº’åŠ¨åŠ¨ä½œç”Ÿæˆç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿåœ¨çº¿ç”ŸæˆåŸºäºè¯­éŸ³çš„ä¸¤äººäº’åŠ¨å…¨èº«åŠ¨ä½œçš„ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç³»ç»Ÿæ˜¯ä¸€ç§åŸºäºéŸ³é¢‘çš„ã€è‡ªå›å½’çš„åœ¨çº¿å¯¹è¯åŠ¨ä½œåˆæˆæ–¹æ³•ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨æ‰©æ•£æ¨¡å‹ä¸ºåŸºç¡€çš„å…¨èº«åŠ¨ä½œåˆæˆæ¨¡å‹ã€‚</li>
<li>ç³»ç»Ÿæ ¹æ®å¯¹è¯åŒæ–¹è¿‡å»çš„çŠ¶æ€ã€è¯­éŸ³éŸ³é¢‘å’Œä»»åŠ¡å¯¼å‘çš„åŠ¨ä½œè½¨è¿¹è¿›è¡Œæ¡ä»¶æ§åˆ¶ã€‚</li>
<li>ç³»ç»Ÿå¯å®ç°çµæ´»çš„ç©ºé—´æ§åˆ¶ï¼Œé€‚ç”¨äºå¯¹è¯åœºæ™¯çš„åŠ¨ä½œåˆæˆã€‚</li>
<li>é€šè¿‡ä¸°å¯Œä¸¤äººå¯¹è¯åŠ¨ä½œæ•°æ®é›†ï¼Œæé«˜æ¨¡å‹å­¦ä¹ å„ç§äº’åŠ¨çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d0f6255b72b8539d8aab13e42acd7a48.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-54ef72cfbbecac1eb389d95bcf9efdce.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-023425356e8d0b3df9cd25fa3d3bf131.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8cf361129e15b5b04e85bcc554daf426.jpg" align="middle">
</details>




<h2 id="Unveiling-Interpretability-in-Self-Supervised-Speech-Representations-for-Parkinsonâ€™s-Diagnosis"><a href="#Unveiling-Interpretability-in-Self-Supervised-Speech-Representations-for-Parkinsonâ€™s-Diagnosis" class="headerlink" title="Unveiling Interpretability in Self-Supervised Speech Representations for   Parkinsonâ€™s Diagnosis"></a>Unveiling Interpretability in Self-Supervised Speech Representations for   Parkinsonâ€™s Diagnosis</h2><p><strong>Authors:David Gimeno-GÃ³mez, Catarina Botelho, Anna Pompili, Alberto Abad, Carlos-D. MartÃ­nez-Hinarejos</strong></p>
<p>Recent works in pathological speech analysis have increasingly relied on powerful self-supervised speech representations, leading to promising results. However, the complex, black-box nature of these embeddings and the limited research on their interpretability significantly restrict their adoption for clinical diagnosis. To address this gap, we propose a novel, interpretable framework specifically designed to support Parkinsonâ€™s Disease (PD) diagnosis. Through the design of simple yet effective cross-attention mechanisms for both embedding- and temporal-level analysis, the proposed framework offers interpretability from two distinct but complementary perspectives. Experimental findings across five well-established speech benchmarks for PD detection demonstrate the frameworkâ€™s capability to identify meaningful speech patterns within self-supervised representations for a wide range of assessment tasks. Fine-grained temporal analyses further underscore its potential to enhance the interpretability of deep-learning pathological speech models, paving the way for the development of more transparent, trustworthy, and clinically applicable computer-assisted diagnosis systems in this domain. Moreover, in terms of classification accuracy, our method achieves results competitive with state-of-the-art approaches, while also demonstrating robustness in cross-lingual scenarios when applied to spontaneous speech production. </p>
<blockquote>
<p>è¿‘æœŸç—…ç†è¯­éŸ³åˆ†æçš„ç ”ç©¶è¶Šæ¥è¶Šä¾èµ–äºå¼ºå¤§çš„è‡ªç›‘ç£è¯­éŸ³è¡¨å¾ï¼Œè¿™å¸¦æ¥äº†å……æ»¡å¸Œæœ›çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›åµŒå…¥çš„å¤æ‚æ€§å’Œé»‘ç®±æ€§è´¨ä»¥åŠå¯¹å…¶è§£é‡Šæ€§çš„ç ”ç©¶æœ‰é™ï¼Œæ˜¾è‘—é™åˆ¶äº†å®ƒä»¬åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¯è§£é‡Šæ¡†æ¶ï¼Œä¸“é—¨è®¾è®¡ç”¨äºæ”¯æŒå¸•é‡‘æ£®æ°ç—‡ï¼ˆPDï¼‰çš„è¯Šæ–­ã€‚é€šè¿‡è®¾è®¡ç®€å•è€Œæœ‰æ•ˆçš„è·¨æ³¨æ„æœºåˆ¶ï¼Œç”¨äºåµŒå…¥çº§å’Œæ—¶é—´çº§çš„åˆ†æï¼Œæ‰€æå‡ºçš„æ¡†æ¶ä»ä¸¤ä¸ªç‹¬ç‰¹ä½†äº’è¡¥çš„è§’åº¦æä¾›äº†å¯è§£é‡Šæ€§ã€‚åœ¨äº”ä¸ªæˆç†Ÿçš„å¸•é‡‘æ£®æ°ç—‡æ£€æµ‹è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨è‡ªç›‘ç£è¡¨å¾ä¸­è¯†åˆ«å„ç§è¯„ä¼°ä»»åŠ¡å†…çš„é‡è¦è¯­éŸ³æ¨¡å¼ã€‚ç²¾ç»†çš„æ—¶é—´åˆ†æè¿›ä¸€æ­¥å¼ºè°ƒäº†å…¶åœ¨æé«˜ç—…ç†è¯­éŸ³æ·±åº¦æ¨¡å‹çš„è§£é‡Šæ½œåŠ›ï¼Œä¸ºå¼€å‘è¯¥é¢†åŸŸæ›´é€æ˜ã€æ›´å¯é çš„ä¸´åºŠé€‚ç”¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚æ­¤å¤–ï¼Œåœ¨åˆ†ç±»å‡†ç¡®åº¦æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä¸å›½å®¶å‰æ²¿æ–¹æ³•ç›¸ç«äº‰çš„ç»“æœï¼Œå¹¶ä¸”åœ¨åº”ç”¨äºè‡ªç„¶è¯­éŸ³ç”Ÿäº§æ—¶è¡¨ç°å‡ºè·¨è¯­è¨€çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02006v1">PDF</a> Submitted to the Special Issue on â€œModelling and Processing Language   and Speech in Neurodegenerative Disordersâ€ published by Journal of Selected   Topics in Signal Processing (JSTSP)</p>
<p><strong>Summary</strong><br>å¸•é‡‘æ£®ç—…çš„ç—…ç†è¯­éŸ³åˆ†ææ–°å·¥ä½œæå‡ºä¸€ç§æ–°å‹å¯è§£é‡Šçš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¸•é‡‘æ£®ç—…è¯Šæ–­ã€‚è¯¥æ¡†æ¶è®¾è®¡ç®€å•æœ‰æ•ˆçš„è·¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»åµŒå…¥å’Œæ—¶åºä¸¤ä¸ªè§’åº¦è¿›è¡Œåˆ†æï¼Œæé«˜è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºçš„æ¨¡å‹è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨å¤šä¸ªå…¬è®¤çš„å¸•é‡‘æ£®ç—…è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­è¯†åˆ«æœ‰æ„ä¹‰çš„è¯­éŸ³æ¨¡å¼ï¼Œä¸”æ—¶åºåˆ†æå¢å¼ºäº†æ·±åº¦å­¦ä¹ çš„è¯­éŸ³æ¨¡å‹çš„è§£é‡Šæ€§ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åˆ†ç±»å‡†ç¡®åº¦é«˜ï¼Œä¸”åœ¨è·¨è¯­è¨€åœºæ™¯ä¸‹è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘å¹´æ¥çš„ç—…ç†è¯­éŸ³åˆ†æå·¥ä½œè¶Šæ¥è¶Šä¾èµ–å¼ºå¤§çš„è‡ªç›‘ç£è¯­éŸ³è¡¨ç¤ºï¼Œå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠè¯Šæ–­ä¸­çš„åº”ç”¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¯è§£é‡Šçš„æ¡†æ¶ï¼Œæ—¨åœ¨æ”¯æŒå¸•é‡‘æ£®ç—…è¯Šæ–­ã€‚</li>
<li>é€šè¿‡åµŒå…¥å’Œæ—¶åºä¸¤ä¸ªè§’åº¦çš„è·¨æ³¨æ„åŠ›æœºåˆ¶åˆ†æï¼Œæé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶åœ¨å¤šä¸ªå¸•é‡‘æ£®è¯­éŸ³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ—¶åºåˆ†æå¢å¼ºäº†æ·±åº¦å­¦ä¹ çš„è¯­éŸ³æ¨¡å‹çš„è§£é‡Šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-46759d6ddcfa7c3e6216ac08066c4b10.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-511c3dae0a8e2843336f591b8c5bde94.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fe157559811021ad650d837179ade7fa.jpg" align="middle">
</details>




<h2 id="Late-fusion-ensembles-for-speech-recognition-on-diverse-input-audio-representations"><a href="#Late-fusion-ensembles-for-speech-recognition-on-diverse-input-audio-representations" class="headerlink" title="Late fusion ensembles for speech recognition on diverse input audio   representations"></a>Late fusion ensembles for speech recognition on diverse input audio   representations</h2><p><strong>Authors:Marin JezidÅ¾iÄ‡, Matej MihelÄiÄ‡</strong></p>
<p>We explore diverse representations of speech audio, and their effect on a performance of late fusion ensemble of E-Branchformer models, applied to Automatic Speech Recognition (ASR) task. Although it is generally known that ensemble methods often improve the performance of the system even for speech recognition, it is very interesting to explore how ensembles of complex state-of-the-art models, such as medium-sized and large E-Branchformers, cope in this setting when their base models are trained on diverse representations of the input speech audio. The results are evaluated on four widely-used benchmark datasets: \textit{Librispeech, Aishell, Gigaspeech}, \textit{TEDLIUMv2} and show that improvements of $1% - 14%$ can still be achieved over the state-of-the-art models trained using comparable techniques on these datasets. A noteworthy observation is that such ensemble offers improvements even with the use of language models, although the gap is closing. </p>
<blockquote>
<p>æˆ‘ä»¬æ¢ç´¢äº†è¯­éŸ³éŸ³é¢‘çš„å¤šç§è¡¨ç¤ºå½¢å¼ï¼Œä»¥åŠå®ƒä»¬å¯¹E-Branchformeræ¨¡å‹æ™šæœŸèåˆé›†æˆåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡æ€§èƒ½çš„å½±å“ã€‚è™½ç„¶é›†æˆæ–¹æ³•é€šå¸¸ä¼šæé«˜ç³»ç»Ÿçš„æ€§èƒ½ï¼Œå³ä½¿åœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä½†æ¢ç´¢å¤æ‚çš„æœ€å…ˆè¿›æ¨¡å‹çš„é›†æˆæ–¹æ³•éå¸¸æœ‰è¶£ï¼Œä¾‹å¦‚åœ¨ä¸­å‹å’Œå¤§å‹E-Branchformersä¸­ï¼Œå½“å®ƒä»¬çš„åŸºå‡†æ¨¡å‹åœ¨è¾“å…¥è¯­éŸ³éŸ³é¢‘çš„å¤šç§è¡¨ç¤ºä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒä»¬å¦‚ä½•åº”å¯¹è¿™ç§æƒ…å†µã€‚åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ï¼šLibrispeechã€Aishellã€Gigaspeechå’ŒTEDLIUMv2ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åœ¨è¿™äº›æ•°æ®é›†ä¸Šä½¿ç”¨ç±»ä¼¼æŠ€æœ¯è®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œä»å¯å®ç°1%~14%çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œè¿™ç§é›†æˆä¹Ÿèƒ½æä¾›æ”¹è¿›ï¼Œå°½ç®¡å·®è·æ­£åœ¨ç¼©å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01861v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è¯­éŸ³éŸ³é¢‘çš„å¤šæ ·åŒ–è¡¨ç¤ºå½¢å¼ï¼Œä»¥åŠå®ƒä»¬å¯¹E-Branchformeræ¨¡å‹çš„åæœŸèåˆç»„åˆåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä»»åŠ¡ä¸­çš„å½±å“ã€‚è™½ç„¶å·²çŸ¥é›†åˆæ–¹æ³•é€šå¸¸ä¼šæé«˜ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½†å¯¹äºåœ¨å¤æ‚å…ˆè¿›çš„æ¨¡å‹ï¼ˆå¦‚ä¸­ç­‰è§„æ¨¡å’Œå¤§å‹E-Branchformersï¼‰ä¸­å¦‚ä½•åº”å¯¹è®­ç»ƒæœ‰è¯­éŸ³éŸ³é¢‘å¤šæ ·åŒ–è¡¨ç¤ºçš„åŸºå‡†æ¨¡å‹æ—¶ï¼Œä»ç„¶éå¸¸æœ‰è¶£ã€‚åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åœ¨è¿™äº›æ•°æ®é›†ä¸Šä½¿ç”¨ç±»ä¼¼æŠ€æœ¯è®­ç»ƒçš„æœ€æ–°æ¨¡å‹ç›¸æ¯”ï¼Œæ”¹è¿›å¹…åº¦å¯è¾¾1%-14%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼Œè¿™ç§ç»„åˆä¹Ÿèƒ½å®ç°æ”¹è¿›ï¼Œå°½ç®¡å·®è·æ­£åœ¨ç¼©å°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ¢è®¨äº†è¯­éŸ³éŸ³é¢‘çš„å¤šæ ·åŒ–è¡¨ç¤ºå½¢å¼å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡çš„å½±å“ã€‚</li>
<li>ç ”ç©¶äº†E-Branchformeræ¨¡å‹çš„åæœŸèåˆç»„åˆçš„æ•ˆæœã€‚</li>
<li>é›†åˆæ–¹æ³•å¯ä»¥æé«˜ç³»ç»Ÿçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å…ˆè¿›çš„æ¨¡å‹ä¸­ã€‚</li>
<li>åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬Librispeechã€Aishellã€Gigaspeechå’ŒTEDLIUMv2ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ”¹è¿›èŒƒå›´å¯è¾¾1%-14%ã€‚</li>
<li>é›†åˆæ–¹æ³•å³ä½¿åœ¨ä½¿ç”¨è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æä¾›æ”¹è¿›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4f334cb19a303c432f63b6b07a1be65.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-01160fccb4bccc09d32d1d30b9e5ec24.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-950a66d6dfbce047d79686b14b4713ab.jpg" align="middle">
</details>




<h2 id="Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data"><a href="#Advancing-Speech-Language-Models-by-Scaling-Supervised-Fine-Tuning-with-Over-60-000-Hours-of-Synthetic-Speech-Dialogue-Data" class="headerlink" title="Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data"></a>Advancing Speech Language Models by Scaling Supervised Fine-Tuning with   Over 60,000 Hours of Synthetic Speech Dialogue Data</h2><p><strong>Authors:Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</strong></p>
<p>The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni%7D">https://huggingface.co/spaces/KE-Team/KE-Omni}</a>. </p>
<blockquote>
<p>GPT-4oä»£è¡¨äº†é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå®æ—¶äº¤äº’çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚å…¶æ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æµåˆ©æ€§ä¸ä»…å¼•èµ·äº†å…³æ³¨ï¼Œè¿˜åˆºæ¿€äº†è¯¥é¢†åŸŸçš„ç ”ç©¶å…´è¶£ã€‚è¿™ç§å®æ—¶è¯­éŸ³äº¤äº’åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå³æ—¶å“åº”çš„åœºæ™¯ä¸­å°¤å…¶æœ‰ä»·å€¼ï¼Œèƒ½æå¤§åœ°æå‡ç”¨æˆ·ä½“éªŒã€‚ç„¶è€Œï¼Œå…³äºå®æ—¶å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹ä¸­æ–‡çš„ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†KE-Omniï¼Œè¿™æ˜¯ä¸€æ¬¾æ— ç¼å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºKe-SpeechChatæ„å»ºã€‚Ke-SpeechChatæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡åˆæˆè¯­éŸ³äº¤äº’æ•°æ®é›†ï¼ŒåŒ…å«700ä¸‡ä¸­æ–‡å’Œè‹±æ–‡å¯¹è¯ï¼Œæ¶‰åŠ42,002åå‘è¨€è€…ï¼Œæ€»è®¡è¶…è¿‡6ä¸‡å°æ—¶ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æ¼”ç¤ºç½‘å€ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/KE-Team/KE-Omni]%E3%80%82">https://huggingface.co/spaces/KE-Team/KE-Omni]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01078v2">PDF</a> KE-Omni, Ke-SpeechChat</p>
<p><strong>Summary</strong></p>
<p>GPT-4oå®ç°äº†é€šè¿‡è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®æ—¶äº¤äº’ï¼Œå…¶ä½å»¶è¿Ÿå’Œé«˜æµç•…åº¦å¼•äººæ³¨ç›®ï¼Œå¹¶åˆºæ¿€äº†ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å…´è¶£ã€‚ç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå³æ—¶å“åº”çš„åœºæ™¯ä¸­ï¼Œå®æ—¶è¯­éŸ³äº¤äº’èƒ½å¤§å¹…æå‡ç”¨æˆ·ä½“éªŒã€‚æœ¬ç ”ç©¶æ¨å‡ºKE-Omniå¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºåŒ…å«7ç™¾ä¸‡ä¸­è‹±æ–‡å¯¹è¯çš„Ke-SpeechChatæ•°æ®é›†ï¼ŒåŒ…å«42,002ä½å‘è¨€è€…çš„è¶…è¿‡6ä¸‡å°æ—¶è¯­éŸ³ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚æ¼”ç¤ºå¯é€šè¿‡é“¾æ¥è®¿é—®ï¼š[é“¾æ¥åœ°å€]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oå®ç°äº†å®æ—¶è¯­éŸ³ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’ã€‚</li>
<li>GPT-4oå…·æœ‰æ˜¾è‘—çš„ä½å»¶è¿Ÿå’Œé«˜æµç•…åº¦ã€‚</li>
<li>å®æ—¶è¯­éŸ³äº¤äº’åœ¨éœ€è¦å¿«é€Ÿåé¦ˆå’Œå“åº”çš„åœºæ™¯ä¸­ä»·å€¼æ˜¾è‘—ã€‚</li>
<li>KE-Omniæ¨¡å‹æ˜¯åŸºäºKe-SpeechChatæ•°æ®é›†çš„çš„å¤§å‹è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚</li>
<li>Ke-SpeechChatæ•°æ®é›†åŒ…å«7ç™¾ä¸‡ä¸­è‹±æ–‡å¯¹è¯ï¼Œè¶…è¿‡6ä¸‡å°æ—¶è¯­éŸ³ã€‚</li>
<li>KE-Omniå¯¹ç›¸å…³é¢†åŸŸçš„ç ”ç©¶å’Œå‘å±•åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d3bd59d6dea70f81a2c6db13a417aaa8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-be63d03367d1646c10129ffa39a43920.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-40f6a84b1329d36c056c958d085b932a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e846567311c2df8847f41091a8724d2.jpg" align="middle">
</details>




<h2 id="FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait"><a href="#FLOAT-Generative-Motion-Latent-Flow-Matching-for-Audio-driven-Talking-Portrait" class="headerlink" title="FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait"></a>FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking   Portrait</h2><p><strong>Authors:Taekyung Ki, Dongchan Min, Gyeongsu Chae</strong></p>
<p>With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results. However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature. This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model. We shift the generative modeling from the pixel-based latent space to a learned motion latent space, enabling efficient design of temporally consistent motion. To achieve this, we introduce a transformer-based vector field predictor with a simple yet effective frame-wise conditioning mechanism. Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions. Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency. </p>
<blockquote>
<p>éšç€åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè‚–åƒåŠ¨ç”»å›¾åƒå·²ç»å–å¾—äº†æ˜¾è‘—çš„ç ”ç©¶æˆæœã€‚ç„¶è€Œï¼Œç”±äºå®ƒçš„è¿­ä»£é‡‡æ ·ç‰¹æ€§ï¼Œå®ƒåœ¨æ—¶åºä¸€è‡´çš„è§†é¢‘ç”Ÿæˆå’Œå¿«é€Ÿé‡‡æ ·æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†FLOATï¼Œä¸€ç§åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è°ˆè¯è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬å°†ç”Ÿæˆæ¨¡å‹ä»åŸºäºåƒç´ çš„æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°å­¦ä¹ çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†æ—¶åºä¸€è‡´è¿åŠ¨çš„æœ‰æ•ˆè®¾è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„çŸ¢é‡åœºé¢„æµ‹å™¨ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„å¸§æ¡ä»¶æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…ç»ªå¢å¼ºï¼Œèƒ½å¤Ÿå®ç°è¡¨è¾¾æ€§è¿åŠ¨çš„è‡ªç„¶èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„éŸ³é¢‘é©±åŠ¨è°ˆè¯è‚–åƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01064v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://deepbrainai-research.github.io/float/">https://deepbrainai-research.github.io/float/</a></p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œè‚–åƒå›¾åƒåŠ¨ç”»å·²å–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ä»é¢ä¸´è§†é¢‘ç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§å’Œå¿«é€Ÿé‡‡æ ·æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•FLOATã€‚è¯¥æ–¹æ³•å°†ç”Ÿæˆå»ºæ¨¡ä»åƒç´ çº§çš„æ½œåœ¨ç©ºé—´è½¬ç§»åˆ°å­¦ä¹ åˆ°çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†æ—¶åºä¸€è‡´è¿åŠ¨çš„æœ‰æ•ˆè®¾è®¡ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ä¸€ä¸ªåŸºäºå˜æ¢å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œå¹¶é‡‡ç”¨äº†ç®€å•æœ‰æ•ˆçš„å¸§æ¡ä»¶æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºï¼Œèƒ½å¤Ÿè‡ªç„¶åœ°èå…¥è¡¨è¾¾æ€§åŠ¨ä½œã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€è¿åŠ¨ä¿çœŸåº¦å’Œæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰éŸ³é¢‘é©±åŠ¨çš„è‚–åƒåŠ¨ç”»æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è‚–åƒå›¾åƒåŠ¨ç”»çš„æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>è‚–åƒå›¾åƒåŠ¨ç”»ä»é¢ä¸´è§†é¢‘ç”Ÿæˆçš„æ—¶åºä¸€è‡´æ€§å’Œå¿«é€Ÿé‡‡æ ·æŒ‘æˆ˜ã€‚</li>
<li>FLOATæ–¹æ³•æ˜¯ä¸€ç§åŸºäºæµåŒ¹é…ç”Ÿæˆæ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>FLOATé€šè¿‡å°†ç”Ÿæˆå»ºæ¨¡è½¬ç§»åˆ°å­¦ä¹ åˆ°çš„è¿åŠ¨æ½œåœ¨ç©ºé—´ï¼Œå®ç°äº†æ—¶åºä¸€è‡´è¿åŠ¨çš„æœ‰æ•ˆè®¾è®¡ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºå˜æ¢å™¨çš„å‘é‡åœºé¢„æµ‹å™¨ï¼Œå¹¶å¼•å…¥å¸§æ¡ä»¶æœºåˆ¶ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>FLOATæ”¯æŒè¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿå¢å¼ºï¼Œä½¿åŠ¨ä½œè¡¨è¾¾æ›´è‡ªç„¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4c48114dadf2693ebec847e1f4161b7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-932be8d0b3a67e4d27e3b20089557ffb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8c0ae55682bf86c0002a9cdf127c986e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c66bfe10dfd878860a466abb92c19030.jpg" align="middle">
</details>




<h2 id="Automating-Feedback-Analysis-in-Surgical-Training-Detection-Categorization-and-Assessment"><a href="#Automating-Feedback-Analysis-in-Surgical-Training-Detection-Categorization-and-Assessment" class="headerlink" title="Automating Feedback Analysis in Surgical Training: Detection,   Categorization, and Assessment"></a>Automating Feedback Analysis in Surgical Training: Detection,   Categorization, and Assessment</h2><p><strong>Authors:Firdavs Nasriddinov, Rafal Kocielnik, Arushi Gupta, Cherine Yang, Elyssa Wong, Anima Anandkumar, Andrew Hung</strong></p>
<p>This work introduces the first framework for reconstructing surgical dialogue from unstructured real-world recordings, which is crucial for characterizing teaching tasks. In surgical training, the formative verbal feedback that trainers provide to trainees during live surgeries is crucial for ensuring safety, correcting behavior immediately, and facilitating long-term skill acquisition. However, analyzing and quantifying this feedback is challenging due to its unstructured and specialized nature. Automated systems are essential to manage these complexities at scale, allowing for the creation of structured datasets that enhance feedback analysis and improve surgical education. Our framework integrates voice activity detection, speaker diarization, and automated speech recaognition, with a novel enhancement that 1) removes hallucinations (non-existent utterances generated during speech recognition fueled by noise in the operating room) and 2) separates speech from trainers and trainees using few-shot voice samples. These aspects are vital for reconstructing accurate surgical dialogues and understanding the roles of operating room participants. Using data from 33 real-world surgeries, we demonstrated the systemâ€™s capability to reconstruct surgical teaching dialogues and detect feedback instances effectively (F1 score of 0.79+&#x2F;-0.07). Moreover, our hallucination removal step improves feedback detection performance by ~14%. Evaluation on downstream clinically relevant tasks of predicting Behavioral Adjustment of trainees and classifying Technical feedback, showed performances comparable to manual annotations with F1 scores of 0.82+&#x2F;0.03 and 0.81+&#x2F;0.03 respectively. These results highlight the effectiveness of our framework in supporting clinically relevant tasks and improving over manual methods. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä»éç»“æ„åŒ–çš„ç°å®ä¸–ç•Œå½•éŸ³ä¸­é‡å»ºæ‰‹æœ¯å¯¹è¯çš„é¦–ä¸ªæ¡†æ¶ï¼Œè¿™å¯¹äºæè¿°æ•™å­¦ä»»åŠ¡è‡³å…³é‡è¦ã€‚åœ¨æ‰‹æœ¯è®­ç»ƒä¸­ï¼ŒåŸ¹è®­äººå‘˜åœ¨ç°åœºæ‰‹æœ¯ä¸­å‘å—è®­äººå‘˜æä¾›çš„å½¢æˆæ€§å£å¤´åé¦ˆå¯¹äºç¡®ä¿å®‰å…¨ã€ç«‹å³çº æ­£è¡Œä¸ºå’Œä¿ƒè¿›é•¿æœŸæŠ€èƒ½è·å–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå…¶éç»“æ„åŒ–å’Œä¸“ä¸šåŒ–çš„ç‰¹ç‚¹ï¼Œåˆ†æå’Œé‡åŒ–è¿™ç§åé¦ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è‡ªåŠ¨ç³»ç»Ÿåœ¨å¤§è§„æ¨¡ç®¡ç†è¿™äº›å¤æ‚æ€§æ–¹é¢è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿåˆ›å»ºå¢å¼ºåé¦ˆåˆ†æå’Œæ”¹å–„æ‰‹æœ¯æ•™è‚²çš„ç»“æ„åŒ–æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€è¯´è¯äººè¯†åˆ«å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œä»¥åŠä¸€ç§æ–°é¢–çš„æå‡æ–¹æ³•ï¼Œå³1ï¼‰æ¶ˆé™¤å¹»å¬ï¼ˆåœ¨æ‰‹æœ¯å®¤å™ªéŸ³é©±åŠ¨çš„è¯­éŸ³è¯†åˆ«è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ä¸å­˜åœ¨çš„è®²è¯ï¼‰å’Œ2ï¼‰ä½¿ç”¨å°‘é‡çš„è¯­éŸ³æ ·æœ¬å°†åŸ¹è®­å¸ˆå’Œå—è®­äººå‘˜çš„è¯­éŸ³åˆ†å¼€ã€‚è¿™äº›æ–¹é¢å¯¹äºé‡å»ºå‡†ç¡®çš„æ‰‹æœ¯å¯¹è¯å’Œç†è§£æ‰‹æœ¯å®¤å‚ä¸è€…çš„è§’è‰²è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª33åœºçœŸå®æ‰‹æœ¯çš„æ•°æ®ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿåœ¨é‡å»ºæ‰‹æœ¯æ•™å­¦å¯¹è¯å’Œæœ‰æ•ˆæ£€æµ‹åé¦ˆå®ä¾‹æ–¹é¢çš„èƒ½åŠ›ï¼ˆF1åˆ†æ•°ä¸º0.79+&#x2F;-0.07ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å¹»è§‰æ¶ˆé™¤æ­¥éª¤æé«˜äº†çº¦14%çš„åé¦ˆæ£€æµ‹æ€§èƒ½ã€‚åœ¨å¯¹é¢„æµ‹å—è®­è€…è¡Œä¸ºè°ƒæ•´çš„ä¸‹æ¸¸ä¸´åºŠç›¸å…³ä»»åŠ¡ä»¥åŠå¯¹æŠ€æœ¯åé¦ˆçš„åˆ†ç±»è¯„ä¼°ä¸­ï¼Œä¸æ‰‹åŠ¨æ³¨é‡Šç›¸æ¯”ï¼ŒF1åˆ†æ•°åˆ†åˆ«ä¸º0.82+&#x2F;0.03å’Œ0.81+&#x2F;0.03ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ”¯æŒä¸´åºŠç›¸å…³ä»»åŠ¡å’Œæ”¹è¿›æ‰‹åŠ¨æ–¹æ³•æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00760v1">PDF</a> Accepted as a proceedings paper at Machine Learning for Health 2024</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é¦–ä¸ªä»ç°å®ä¸–ç•Œçš„éç»“æ„åŒ–å½•éŸ³ä¸­é‡å»ºæ‰‹æœ¯å¯¹è¯çš„æ¡†æ¶ï¼Œè¿™å¯¹äºåˆ»ç”»æ•™å­¦ä»»åŠ¡è‡³å…³é‡è¦ã€‚åœ¨æ‰‹æœ¯åŸ¹è®­ä¸­ï¼ŒåŸ¹è®­å¸ˆåœ¨ç°åœºæ‰‹æœ¯ä¸­å‘å—è®­äººå‘˜æä¾›çš„å½¢æˆæ€§å£å¤´åé¦ˆå¯¹äºç¡®ä¿å®‰å…¨ã€ç«‹å³çº æ­£è¡Œä¸ºå’Œä¿ƒè¿›é•¿æœŸæŠ€èƒ½è·å–è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºåé¦ˆçš„éç»“æ„åŒ–å’Œä¸“ä¸šåŒ–ç‰¹æ€§ï¼Œåˆ†æå’Œé‡åŒ–è¿™äº›åé¦ˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è‡ªåŠ¨åŒ–ç³»ç»Ÿæ˜¯å¤§è§„æ¨¡ç®¡ç†è¿™äº›å¤æ‚æ€§çš„å…³é”®ï¼Œå¯ä»¥åˆ›å»ºå¢å¼ºåé¦ˆåˆ†æå’Œæ”¹å–„æ‰‹æœ¯æ•™è‚²çš„ç»“æ„åŒ–æ•°æ®é›†ã€‚è¯¥æ¡†æ¶é›†æˆäº†è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€è¯´è¯äººè¯†åˆ«å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œå¹¶æœ‰ä¸€ç§æ–°é¢–çš„æå‡ï¼Œå³æ¶ˆé™¤å¹»å¬ï¼ˆç”±æ‰‹æœ¯å®¤å™ªéŸ³å¼•èµ·çš„åœ¨è¯­éŸ³è¯†åˆ«è¿‡ç¨‹ä¸­äº§ç”Ÿçš„éå­˜åœ¨æ€§å‘è¨€ï¼‰ä»¥åŠä½¿ç”¨å°‘æ•°è¯­éŸ³æ ·æœ¬åˆ†ç¦»è®­ç»ƒå¸ˆå’Œå—è®­äººå‘˜çš„è¯­éŸ³ã€‚è¿™äº›æ–¹é¢å¯¹äºé‡å»ºå‡†ç¡®çš„æ‰‹æœ¯å¯¹è¯å’Œç†è§£æ‰‹æœ¯å®¤å‚ä¸è€…çš„è§’è‰²è‡³å…³é‡è¦ã€‚ä½¿ç”¨æ¥è‡ª33åœºçœŸå®æ‰‹æœ¯çš„æ•°æ®ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥ç³»ç»Ÿé‡å»ºæ‰‹æœ¯æ•™å­¦å¯¹è¯å’Œæ£€æµ‹åé¦ˆå®ä¾‹çš„èƒ½åŠ›ï¼ˆF1åˆ†æ•°ä¸º0.79+&#x2F;-0.07ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å¹»å¬æ¶ˆé™¤æ­¥éª¤æé«˜äº†åé¦ˆæ£€æµ‹æ€§èƒ½çº¦14%ã€‚å¯¹é¢„æµ‹å—è®­è€…è¡Œä¸ºè°ƒæ•´å’Œåˆ†ç±»æŠ€æœ¯åé¦ˆç­‰ä¸´åºŠç›¸å…³ä»»åŠ¡çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶æ€§èƒ½ä¸æ‰‹åŠ¨æ³¨é‡Šç›¸å½“ï¼ŒF1åˆ†æ•°åˆ†åˆ«ä¸º0.82+&#x2F;0.03å’Œ0.81+&#x2F;0.03ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ”¯æŒä¸´åºŠç›¸å…³ä»»åŠ¡å’Œæ”¹å–„æ‰‹åŠ¨æ–¹æ³•æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†é¦–ä¸ªé’ˆå¯¹æ•™å­¦ä»»åŠ¡çš„æ‰‹æœ¯å¯¹è¯é‡å»ºæ¡†æ¶ã€‚</li>
<li>è‡ªåŠ¨åŒ–ç³»ç»Ÿæ˜¯å¤„ç†å¤æ‚çš„æ‰‹æœ¯ç¯å¢ƒçš„å…³é”®ï¼Œæœ‰åŠ©äºåˆ†æå’Œé‡åŒ–åé¦ˆã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†å¤šé¡¹æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¯­éŸ³æ´»åŠ¨æ£€æµ‹ã€è¯´è¯äººè¯†åˆ«å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>ç³»ç»Ÿå…·å¤‡æ¶ˆé™¤å¹»å¬çš„èƒ½åŠ›ï¼Œå³åœ¨è¯­éŸ³è¯†åˆ«è¿‡ç¨‹ä¸­ç”±äºæ‰‹æœ¯å®¤å™ªéŸ³äº§ç”Ÿçš„éå­˜åœ¨æ€§å‘è¨€ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å°‘æ•°è¯­éŸ³æ ·æœ¬ï¼Œèƒ½å¤Ÿåˆ†ç¦»è®­ç»ƒå¸ˆå’Œå—è®­äººå‘˜çš„è¯­éŸ³ã€‚</li>
<li>ä½¿ç”¨çœŸå®æ‰‹æœ¯æ•°æ®æµ‹è¯•äº†ç³»ç»Ÿçš„æ€§èƒ½ï¼Œåœ¨åé¦ˆå®ä¾‹æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ˆF1åˆ†æ•°ä¸º0.79+&#x2F;-0.07ï¼‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-709cd260ad1b25b52129d6640b98474a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-51acc9169d22aa02a2423dd32aa7352c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-91d469f8293cca322e89e03c1eb1df77.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-84951b39a79afb3a33830614f3799e33.jpg" align="middle">
</details>




<h2 id="A-Comparative-Study-of-LLM-based-ASR-and-Whisper-in-Low-Resource-and-Code-Switching-Scenario"><a href="#A-Comparative-Study-of-LLM-based-ASR-and-Whisper-in-Low-Resource-and-Code-Switching-Scenario" class="headerlink" title="A Comparative Study of LLM-based ASR and Whisper in Low Resource and   Code Switching Scenario"></a>A Comparative Study of LLM-based ASR and Whisper in Low Resource and   Code Switching Scenario</h2><p><strong>Authors:Zheshu Song, Ziyang Ma, Yifan Yang, Jianheng Zhuo, Xie Chen</strong></p>
<p>Large Language Models (LLMs) have showcased exceptional performance across diverse NLP tasks, and their integration with speech encoder is rapidly emerging as a dominant trend in the Automatic Speech Recognition (ASR) field. Previous works mainly concentrated on leveraging LLMs for speech recognition in English and Chinese. However, their potential for addressing speech recognition challenges in low resource settings remains underexplored. Hence, in this work, we aim to explore the capability of LLMs in low resource ASR and Mandarin-English code switching ASR. We also evaluate and compare the recognition performance of LLM-based ASR systems against Whisper model. Extensive experiments demonstrate that LLM-based ASR yields a relative gain of 12.8% over the Whisper model in low resource ASR while Whisper performs better in Mandarin-English code switching ASR. We hope that this study could shed light on ASR for low resource scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå®ƒä»¬ä¸è¯­éŸ³ç¼–ç å™¨çš„é›†æˆæ­£åœ¨è¿…é€Ÿæˆä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢†åŸŸçš„ä¸»æµè¶‹åŠ¿ã€‚ä¹‹å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨LLMsè¿›è¡Œè‹±è¯­å’Œä¸­æ–‡çš„è¯­éŸ³è¯†åˆ«ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è§£å†³ä½èµ„æºè®¾ç½®ä¸­çš„è¯­éŸ³è¯†åˆ«æŒ‘æˆ˜æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¢ç´¢LLMsåœ¨ä½èµ„æºASRä»¥åŠæ™®é€šè¯-è‹±è¯­åˆ‡æ¢ASRä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°å’Œæ¯”è¾ƒäº†åŸºäºLLMçš„ASRç³»ç»Ÿä¸Whisperæ¨¡å‹çš„è¯†åˆ«æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨ä½èµ„æºASRä¸­ï¼ŒåŸºäºLLMçš„ASRç³»ç»Ÿç›¸å¯¹äºWhisperæ¨¡å‹æœ‰12.8%çš„ç›¸å¯¹å¢ç›Šï¼Œè€Œåœ¨æ™®é€šè¯-è‹±è¯­åˆ‡æ¢ASRä¸­ï¼ŒWhisperçš„è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹ç ”ç©¶èƒ½ä¸ºä½èµ„æºåœºæ™¯çš„ASRæä¾›ä¸€äº›å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00721v2">PDF</a> This work hasnâ€™t been finished yet</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå…¶ä¸è¯­éŸ³ç¼–ç å™¨çš„é›†æˆæ­£åœ¨è¿…é€Ÿæˆä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢†åŸŸçš„ä¸»æµè¶‹åŠ¿ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨LLMsè¿›è¡Œè‹±è¯­å’Œä¸­æ–‡çš„è¯­éŸ³è¯†åˆ«ï¼Œä½†å®ƒä»¬åœ¨ä½èµ„æºç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«æŒ‘æˆ˜çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢LLMsåœ¨ä½èµ„æºASRå’Œæ™®é€šè¯-è‹±è¯­ä»£ç åˆ‡æ¢ASRä¸­çš„èƒ½åŠ›ï¼Œå¹¶è¯„ä¼°å…¶ä¸Whisperæ¨¡å‹çš„è¯†åˆ«æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºLLMsçš„ASRç³»ç»Ÿåœ¨ä½èµ„æºASRä¸Šç›¸å¯¹äºWhisperæ¨¡å‹æœ‰12.8%çš„ç›¸å¯¹å¢ç›Šï¼Œè€Œåœ¨æ™®é€šè¯-è‹±è¯­ä»£ç åˆ‡æ¢ASRä¸­ï¼ŒWhisperè¡¨ç°æ›´å¥½ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é¢†åŸŸå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºç¯å¢ƒä¸­ã€‚</li>
<li>LLMsä¸è¯­éŸ³ç¼–ç å™¨çš„é›†æˆæ˜¯ASRé¢†åŸŸçš„æ–°å…´è¶‹åŠ¿ã€‚</li>
<li>ä¹‹å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è‹±è¯­å’Œä¸­æ–‡çš„è¯­éŸ³è¯†åˆ«ï¼Œä½†å¯¹ä½èµ„æºç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«æŒ‘æˆ˜çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>åŸºäºLLMsçš„ASRç³»ç»Ÿåœ¨ä½èµ„æºç¯å¢ƒä¸­çš„æ€§èƒ½ç›¸è¾ƒäºWhisperæ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>åœ¨æ™®é€šè¯-è‹±è¯­ä»£ç åˆ‡æ¢çš„ASRåœºæ™¯ä¸‹ï¼ŒWhisperæ¨¡å‹çš„æ€§èƒ½è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸ºä½èµ„æºç¯å¢ƒä¸‹çš„ASRæä¾›äº†æ–°çš„è§è§£å’Œç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fe645bf94c30ad3702114b19ac80b669.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b8d372efae3a6582931743a84ff27482.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5a01d49768b73600fe7df6b6baa34152.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-84166358babce76b74c378b975e3c84f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-538b5c6e21fb306653e7aa8db9cf3562.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-230a47d672931d3003c9b852dfe1d174.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c33af65f2b1027dfecbe2133fbb3e449.jpg" align="middle">
</details>




<h2 id="Empowering-the-Deaf-and-Hard-of-Hearing-Community-Enhancing-Video-Captions-Using-Large-Language-Models"><a href="#Empowering-the-Deaf-and-Hard-of-Hearing-Community-Enhancing-Video-Captions-Using-Large-Language-Models" class="headerlink" title="Empowering the Deaf and Hard of Hearing Community: Enhancing Video   Captions Using Large Language Models"></a>Empowering the Deaf and Hard of Hearing Community: Enhancing Video   Captions Using Large Language Models</h2><p><strong>Authors:Nadeen Fathallah, Monika Bhole, Steffen Staab</strong></p>
<p>In todayâ€™s digital age, video content is prevalent, serving as a primary source of information, education, and entertainment. However, the Deaf and Hard of Hearing (DHH) community often faces significant challenges in accessing video content due to the inadequacy of automatic speech recognition (ASR) systems in providing accurate and reliable captions. This paper addresses the urgent need to improve video caption quality by leveraging Large Language Models (LLMs). We present a comprehensive study that explores the integration of LLMs to enhance the accuracy and context-awareness of captions generated by ASR systems. Our methodology involves a novel pipeline that corrects ASR-generated captions using advanced LLMs. It explicitly focuses on models like GPT-3.5 and Llama2-13B due to their robust performance in language comprehension and generation tasks. We introduce a dataset representative of real-world challenges the DHH community faces to evaluate our proposed pipeline. Our results indicate that LLM-enhanced captions significantly improve accuracy, as evidenced by a notably lower Word Error Rate (WER) achieved by ChatGPT-3.5 (WER: 9.75%) compared to the original ASR captions (WER: 23.07%), ChatGPT-3.5 shows an approximate 57.72% improvement in WER compared to the original ASR captions. </p>
<blockquote>
<p>åœ¨å¦‚ä»Šçš„æ•°å­—åŒ–æ—¶ä»£ï¼Œè§†é¢‘å†…å®¹æ™®éå­˜åœ¨ï¼Œæˆä¸ºä¿¡æ¯ã€æ•™è‚²å’Œå¨±ä¹çš„ä¸»è¦æ¥æºã€‚ç„¶è€Œï¼Œè‹å“‘äººç¾¤ä½“åœ¨è·å–è§†é¢‘å†…å®¹æ—¶å¸¸å¸¸é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™æ˜¯å› ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨æä¾›å‡†ç¡®å¯é çš„å­—å¹•æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡é’ˆå¯¹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æé«˜è§†é¢‘å­—å¹•è´¨é‡çš„ç´§è¿«éœ€æ±‚ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œæ¢ç´¢äº†å°†LLMé›†æˆåˆ°ASRç³»ç»Ÿä¸­ä»¥æé«˜ç”Ÿæˆå­—å¹•çš„å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ ¡æ­£ASRç”Ÿæˆçš„å­—å¹•ï¼Œé‡ç‚¹å…³æ³¨GPT-3.5å’ŒLlama2-13Bç­‰æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æå‡ºçš„ç®¡é“ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä»£è¡¨çœŸå®ä¸–ç•ŒæŒ‘æˆ˜çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åæ˜ äº†è‹å“‘ç¾¤ä½“æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºçš„å­—å¹•æ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œä»¥ChatGPT-3.5ä¸ºä¾‹ï¼Œå…¶è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ˜¾è‘—ä½äºåŸå§‹ASRå­—å¹•ï¼ˆWERï¼š9.75%ï¼‰ï¼Œç›¸è¾ƒäºåŸå§‹ASRå­—å¹•ï¼ŒChatGPT-3.5çš„è¯é”™è¯¯ç‡é™ä½äº†çº¦57.72%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00342v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æ•°å­—æ—¶ä»£ï¼Œè§†é¢‘å†…å®¹å¹¿æ³›å­˜åœ¨ï¼Œä½œä¸ºä¿¡æ¯ã€æ•™è‚²å’Œå¨±ä¹çš„ä¸»è¦æ¥æºã€‚ç„¶è€Œï¼Œè‹å“‘äººç¾¤åœ¨è·å–è§†é¢‘å†…å®¹æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨æä¾›å‡†ç¡®å¯é çš„å­—å¹•æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æœ¬æ–‡ç€é‡è§£å†³æé«˜è§†é¢‘å­—å¹•è´¨é‡çš„ç´§è¿«éœ€æ±‚ï¼Œé€šè¿‡åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æœ¬æ–‡å‘ˆç°äº†ä¸€é¡¹ç»¼åˆæ€§ç ”ç©¶ï¼Œæ¢è®¨äº†æ•´åˆLLMsä»¥æé«˜ASRç³»ç»Ÿç”Ÿæˆå­—å¹•çš„å‡†ç¡®æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠä½¿ç”¨å…ˆè¿›çš„LLMsæ¥ä¿®æ­£ASRç”Ÿæˆçš„å­—å¹•ï¼Œé‡ç‚¹ä»‹ç»äº†GPT-3.5å’ŒLlama2-13Bç­‰æ¨¡å‹ï¼Œå®ƒä»¬åœ¨è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåæ˜ è‹å¬ç¾¤ä½“é¢ä¸´ç°å®æŒ‘æˆ˜çš„æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬æå‡ºçš„ç®¡é“ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMå¢å¼ºçš„å­—å¹•æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼ŒChatGPT-3.5çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä»åŸæ¥çš„ASRå­—å¹•çš„23.07%é™ä½åˆ°9.75%ï¼Œæ˜¾ç¤ºå‡ºå¤§çº¦57.72%çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å†…å®¹åœ¨æ•°å­—æ—¶ä»£çš„é‡è¦æ€§åŠå…¶å¯¹è‹å“‘äººç¾¤çš„ç‰¹åˆ«æ„ä¹‰ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨ä¸ºè‹å“‘äººç¾¤æä¾›å‡†ç¡®å¯é å­—å¹•æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æé«˜è§†é¢‘å­—å¹•è´¨é‡æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›LLMså¦‚GPT-3.5å’ŒLlama2-13Bæ¥å¢å¼ºASRç³»ç»Ÿç”Ÿæˆçš„å­—å¹•çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥åæ˜ è‹å¬ç¾¤ä½“ç°å®æŒ‘æˆ˜çš„æ•°æ®é›†æ¥è¯„ä¼°æ–°çš„å­—å¹•ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>LLMå¢å¼ºçš„å­—å¹•æ˜¾ç¤ºå‡ºæ˜¾è‘—æé«˜çš„å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯ChatGPT-3.5åœ¨å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7698f76dd914ce2e708dc0daf0cb56ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a7dcf77501eeca0e82ebcdf66c17123.jpg" align="middle">
</details>




<h2 id="OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation"><a href="#OpenHumanVid-A-Large-Scale-High-Quality-Dataset-for-Enhancing-Human-Centric-Video-Generation" class="headerlink" title="OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation"></a>OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing   Human-Centric Video Generation</h2><p><strong>Authors:Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, Siyu Zhu</strong></p>
<p>Recent advancements in visual generation technologies have markedly increased the scale and availability of video datasets, which are crucial for training effective video generation models. However, a significant lack of high-quality, human-centric video datasets presents a challenge to progress in this field. To bridge this gap, we introduce OpenHumanVid, a large-scale and high-quality human-centric video dataset characterized by precise and detailed captions that encompass both human appearance and motion states, along with supplementary human motion conditions, including skeleton sequences and speech audio. To validate the efficacy of this dataset and the associated training strategies, we propose an extension of existing classical diffusion transformer architectures and conduct further pretraining of our models on the proposed dataset. Our findings yield two critical insights: First, the incorporation of a large-scale, high-quality dataset substantially enhances evaluation metrics for generated human videos while preserving performance in general video generation tasks. Second, the effective alignment of text with human appearance, human motion, and facial motion is essential for producing high-quality video outputs. Based on these insights and corresponding methodologies, the straightforward extended network trained on the proposed dataset demonstrates an obvious improvement in the generation of human-centric videos. Project page <a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid">https://fudan-generative-vision.github.io/OpenHumanVid</a> </p>
<blockquote>
<p>éšç€è§†è§‰ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œè§†é¢‘æ•°æ®é›†çš„æ•°é‡å’Œå¯ç”¨æ€§æ˜¾è‘—å¢åŠ ï¼Œè¿™å¯¹äºè®­ç»ƒæœ‰æ•ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé«˜è´¨é‡ã€ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘æ•°æ®é›†çš„ç¼ºä¹ä¸ºè¯¥é¢†åŸŸçš„è¿›æ­¥å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenHumanVidï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡ã€ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘æ•°æ®é›†ï¼Œå…¶ç‰¹ç‚¹æ˜¯å…·æœ‰ç²¾ç¡®å’Œè¯¦ç»†çš„å­—å¹•ï¼Œæ¶µç›–äº†äººç±»å¤–è§‚å’Œè¿åŠ¨çŠ¶æ€ï¼Œè¿˜åŒ…æ‹¬é¢å¤–çš„äººç±»è¿åŠ¨æ¡ä»¶ï¼Œå¦‚éª¨éª¼åºåˆ—å’Œè¯­éŸ³éŸ³é¢‘ã€‚ä¸ºäº†éªŒè¯è¯¥æ•°æ®é›†å’Œç›¸å…³è®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰çš„ç»å…¸æ‰©æ•£å˜å‹å™¨æ¶æ„è¿›è¡Œäº†æ‰©å±•ï¼Œå¹¶åœ¨æ‰€æå‡ºçš„æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œäº†è¿›ä¸€æ­¥çš„é¢„è®­ç»ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ä¸¤ä¸ªå…³é”®è§è§£ï¼šé¦–å…ˆï¼Œä½¿ç”¨å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†å¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆçš„äººç±»è§†é¢‘çš„è¯„ä»·æŒ‡æ ‡ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸€èˆ¬è§†é¢‘ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œæ–‡æœ¬ä¸äººç±»å¤–è§‚ã€äººç±»è¿åŠ¨å’Œé¢éƒ¨è¿åŠ¨çš„æœ‰æ•ˆå¯¹é½å¯¹äºç”Ÿæˆé«˜è´¨é‡è§†é¢‘è¾“å‡ºè‡³å…³é‡è¦ã€‚åŸºäºè¿™äº›è§è§£å’Œç›¸åº”çš„æ–¹æ³•è®ºï¼Œåœ¨æ‰€æå‡ºçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç®€å•æ‰©å±•ç½‘ç»œåœ¨ç”Ÿæˆä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘æ–¹é¢æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ”¹è¿›ã€‚é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://fudan-generative-vision.github.io/OpenHumanVid%E3%80%82">https://fudan-generative-vision.github.io/OpenHumanVidã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00115v2">PDF</a> 11 pages, 8 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>OpenHumanVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„äººæœºäº’åŠ¨è§†é¢‘æ•°æ®é›†ï¼ŒåŒ…å«äº†ç²¾ç¡®è¯¦ç»†çš„æè¿°ã€äººç±»åŠ¨ä½œçŠ¶æ€ä»¥åŠé¢å¤–çš„è¿åŠ¨æ¡ä»¶ï¼Œå¦‚éª¨æ¶åºåˆ—å’Œè¯­éŸ³éŸ³é¢‘ã€‚åˆ©ç”¨è¯¥æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒå¯ä»¥æœ‰æ•ˆæå‡ç”Ÿæˆäººç±»è§†é¢‘çš„è¯„ä»·æŒ‡æ ‡ï¼Œå¹¶ä¿ç•™äº†ä¸€èˆ¬è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¯¥é¡¹ç›®å®ç°äº†åŸºäºæ·±åº¦å­¦ä¹ çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„é‡å¤§çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenHumanVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„äººæœºäº’åŠ¨è§†é¢‘æ•°æ®é›†ï¼Œæä¾›äº†è¯¦ç»†çš„æè¿°ã€äººç±»åŠ¨ä½œçŠ¶æ€ä»¥åŠé¢å¤–çš„è¿åŠ¨æ¡ä»¶ã€‚</li>
<li>åˆ©ç”¨OpenHumanVidæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒèƒ½æœ‰æ•ˆæé«˜ç”Ÿæˆäººç±»è§†é¢‘çš„è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>ä¿ç•™äº†ä¸€èˆ¬è§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç»å…¸æ‰©æ•£å˜å‹å™¨æ¶æ„çš„æ‰©å±•å’Œæ¨¡å‹åœ¨OpenHumanVidæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒå¯¹äºç”Ÿæˆé«˜è´¨é‡è§†é¢‘è‡³å…³é‡è¦ã€‚</li>
<li>å°†æ–‡æœ¬ä¸äººç±»å¤–è§‚ã€åŠ¨ä½œå’Œé¢éƒ¨åŠ¨ä½œæœ‰æ•ˆå¯¹é½æ˜¯ç”Ÿæˆé«˜è´¨é‡è§†é¢‘çš„å…³é”®ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨OpenHumanVidæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜ä»¥äººç±»ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e7a32979db120061c9814fbd3803ed20.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-af91df1cc921458f1fd25840d2007560.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-12fef6686db4ba0fa0c8fa9024adeb59.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bb7e2cea8efdf57502d07f18589af537.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-303bd37974641a07b143b5ccc769afc0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c950ea4205d5c525019e5a8026adcb1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-991b8dbd477d1cfa4f7db2f183f15261.jpg" align="middle">
</details>




<h2 id="EEG-Based-Analysis-of-Brain-Responses-in-Multi-Modal-Human-Robot-Interaction-Modulating-Engagement"><a href="#EEG-Based-Analysis-of-Brain-Responses-in-Multi-Modal-Human-Robot-Interaction-Modulating-Engagement" class="headerlink" title="EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot   Interaction: Modulating Engagement"></a>EEG-Based Analysis of Brain Responses in Multi-Modal Human-Robot   Interaction: Modulating Engagement</h2><p><strong>Authors:Suzanne Oliver, Tomoko Kitago, Adam Buchwald, S. Farokh Atashzar</strong></p>
<p>User engagement, cognitive participation, and motivation during task execution in physical human-robot interaction are crucial for motor learning. These factors are especially important in contexts like robotic rehabilitation, where neuroplasticity is targeted. However, traditional robotic rehabilitation systems often face challenges in maintaining user engagement, leading to unpredictable therapeutic outcomes. To address this issue, various techniques, such as assist-as-needed controllers, have been developed to prevent user slacking and encourage active participation. In this paper, we introduce a new direction through a novel multi-modal robotic interaction designed to enhance user engagement by synergistically integrating visual, motor, cognitive, and auditory (speech recognition) tasks into a single, comprehensive activity. To assess engagement quantitatively, we compared multiple electroencephalography (EEG) biomarkers between this multi-modal protocol and a traditional motor-only protocol. Fifteen healthy adult participants completed 100 trials of each task type. Our findings revealed that EEG biomarkers, particularly relative alpha power, showed statistically significant improvements in engagement during the multi-modal task compared to the motor-only task. Moreover, while engagement decreased over time in the motor-only task, the multi-modal protocol maintained consistent engagement, suggesting that users could remain engaged for longer therapy sessions. Our observations on neural responses during interaction indicate that the proposed multi-modal approach can effectively enhance user engagement, which is critical for improving outcomes. This is the first time that objective neural response highlights the benefit of a comprehensive robotic intervention combining motor, cognitive, and auditory functions in healthy subjects. </p>
<blockquote>
<p>åœ¨ç‰©ç†äººæœºäº¤äº’ä¸­çš„ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·å‚ä¸åº¦ã€è®¤çŸ¥å‚ä¸åº¦å’ŒåŠ¨æœºå¯¹äºè¿åŠ¨å­¦ä¹ è‡³å…³é‡è¦ã€‚åœ¨é’ˆå¯¹ç¥ç»å¯å¡‘æ€§ç›®æ ‡çš„é¢†åŸŸï¼ˆå¦‚æœºå™¨äººåº·å¤ï¼‰ä¸­ï¼Œè¿™äº›å› ç´ å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æœºå™¨äººåº·å¤ç³»ç»Ÿå¸¸å¸¸é¢ä¸´ç»´æŒç”¨æˆ·å‚ä¸åº¦æ–¹é¢çš„æŒ‘æˆ˜ï¼Œä»è€Œå¯¼è‡´ä¸å¯é¢„æµ‹çš„æ²»ç–—ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå·²ç»å¼€å‘å‡ºäº†å„ç§æŠ€æœ¯ï¼Œå¦‚æŒ‰éœ€è¾…åŠ©æ§åˆ¶å™¨ï¼Œä»¥é˜²æ­¢ç”¨æˆ·æ‡ˆæ€ å¹¶é¼“åŠ±ç§¯æå‚ä¸ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼æœºå™¨äººäº¤äº’ä»‹ç»äº†ä¸€ä¸ªæ–°æ–¹å‘ï¼Œè¯¥äº¤äº’é€šè¿‡ååŒæ•´åˆè§†è§‰ã€è¿åŠ¨ã€è®¤çŸ¥å’Œå¬è§‰ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰ä»»åŠ¡åˆ°ä¸€ä¸ªå•ä¸€çš„ç»¼åˆæ´»åŠ¨ä¸­ï¼Œä»è€Œå¢å¼ºç”¨æˆ·å‚ä¸åº¦ã€‚ä¸ºäº†å®šé‡è¯„ä¼°å‚ä¸åº¦ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¿™ç§å¤šæ¨¡å¼åè®®å’Œä¼ ç»Ÿä»…è¿åŠ¨æ¨¡å¼çš„åè®®ä¹‹é—´çš„å¤šä¸ªè„‘ç”µå›¾ï¼ˆEEGï¼‰ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚åäº”åå¥åº·æˆå¹´å‚ä¸è€…å®Œæˆäº†æ¯ç§ä»»åŠ¡ç±»å‹çš„ä¸€ç™¾æ¬¡è¯•éªŒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ä»…è¿åŠ¨ä»»åŠ¡ç›¸æ¯”ï¼Œè„‘ç”µå›¾ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆå°¤å…¶æ˜¯ç›¸å¯¹é˜¿å°”æ³•åŠŸç‡ï¼‰åœ¨å¤šæ¨¡å¼ä»»åŠ¡æœŸé—´çš„å‚ä¸åº¦æœ‰æ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼Œå°½ç®¡å‚ä¸åº¦åœ¨ä»…è¿åŠ¨ä»»åŠ¡ä¸­éšæ—¶é—´ä¸‹é™ï¼Œä½†å¤šæ¨¡å¼åè®®ä¿æŒäº†æŒç»­çš„å‚ä¸åº¦ï¼Œè¿™è¡¨æ˜ç”¨æˆ·å¯ä»¥åœ¨æ›´é•¿çš„æ²»ç–—è¿‡ç¨‹ä¸­ä¿æŒå‚ä¸ã€‚æˆ‘ä»¬å¯¹äº¤äº’è¿‡ç¨‹ä¸­ç¥ç»ååº”çš„è§‚å¯Ÿè¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¤šæ¨¡å¼æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æé«˜ç”¨æˆ·å‚ä¸åº¦ï¼Œè¿™å¯¹äºæé«˜æ²»ç–—æ•ˆæœè‡³å…³é‡è¦ã€‚è¿™æ˜¯ç¬¬ä¸€æ¬¡å®¢è§‚ç¥ç»ååº”çªæ˜¾å‡ºåœ¨å¥åº·å—è¯•è€…ä¸­ç»“åˆè¿åŠ¨ã€è®¤çŸ¥å’Œå¬è§‰åŠŸèƒ½çš„ç»¼åˆæœºå™¨äººå¹²é¢„çš„ç›Šå¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18587v1">PDF</a> 9 pages, 7 figures. Submitted to IEEE TNSRE</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç‰©ç†äººæœºäº¤äº’ä¸­çš„ç”¨æˆ·å‚ä¸åº¦ã€è®¤çŸ¥å‚ä¸åº¦å’Œä»»åŠ¡æ‰§è¡Œæ—¶çš„åŠ¨æœºå¯¹äºè¿åŠ¨å­¦ä¹ è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººåº·å¤ç­‰é’ˆå¯¹ç¥ç»å¯å¡‘æ€§ç›®æ ‡çš„æƒ…å¢ƒä¸­ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æœºå™¨äººåº·å¤ç³»ç»Ÿå¾€å¾€é¢ä¸´ç»´æŒç”¨æˆ·å‚ä¸åº¦çš„æŒ‘æˆ˜ï¼Œä»è€Œå¯¼è‡´æ²»ç–—æ•ˆæœéš¾ä»¥é¢„æµ‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†æŒ‰éœ€è¾…åŠ©æ§åˆ¶å™¨ç­‰æŠ€æœ¯ä»¥é˜²æ­¢ç”¨æˆ·æ‡ˆæ€ å¹¶é¼“åŠ±ç§¯æå‚ä¸ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€šè¿‡æ–°å‹å¤šæ¨¡å¼æœºå™¨äººäº¤äº’å¢å¼ºç”¨æˆ·å‚ä¸åº¦çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ååŒæ•´åˆè§†è§‰ã€è¿åŠ¨ã€è®¤çŸ¥å’Œå¬è§‰ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰ä»»åŠ¡ï¼Œå°†å¤šç§æ¨¡å¼èå…¥å•ä¸€çš„å…¨é¢æ´»åŠ¨ä¸­ã€‚ä¸ºå®šé‡è¯„ä¼°å‚ä¸åº¦ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†å¤šæ¨¡æ€åè®®å’Œä¼ ç»Ÿè¿åŠ¨åè®®ä¸‹çš„è„‘ç”µå›¾ï¼ˆEEGï¼‰ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œæ¶‰åŠ15åå¥åº·æˆå¹´å‚ä¸è€…å…±å®Œæˆæ¯ç§ä»»åŠ¡ç±»å‹å„100æ¬¡è¯•éªŒã€‚ç»“æœæ˜¾ç¤ºï¼Œè„‘ç”µå›¾ç”Ÿç‰©æ ‡å¿—ç‰©å³ç›¸å¯¹é˜¿å°”æ³•åŠŸç‡åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ”¹å–„å…·æœ‰ç»Ÿè®¡å­¦æ˜¾è‘—å·®å¼‚ã€‚æ­¤å¤–ï¼Œå°½ç®¡å‚ä¸è€…åœ¨ä»…è¿›è¡Œè¿åŠ¨ä»»åŠ¡çš„æƒ…æ™¯ä¸‹éšç€æ—¶é—´æ¨ç§»å…¶å‚ä¸åº¦ä¸‹é™ï¼Œå¤šæ¨¡æ€åè®®åˆ™ç»´æŒäº†ä¸€è‡´çš„å‚ä¸åº¦æ°´å¹³ï¼Œæš—ç¤ºç”¨æˆ·åœ¨é•¿æ—¶é—´æ²»ç–—åœºæ™¯ä¸‹ä¹Ÿèƒ½ä¿æŒé«˜åº¦å‚ä¸ã€‚æœ¬ç ”ç©¶è§‚å¯Ÿåˆ°çš„ç¥ç»ååº”è¡¨æ˜ï¼Œæ‰€æå‡ºçš„å¤šæ¨¡æ€æ–¹æ³•å¯æœ‰æ•ˆæå‡ç”¨æˆ·å‚ä¸åº¦ï¼Œè¿™å¯¹äºæ”¹å–„æ²»ç–—æ•ˆæœè‡³å…³é‡è¦ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨å¥åº·å—è¯•è€…ä¸­å®¢è§‚è®°å½•ç¥ç»ååº”è¡¨æ˜ç»“åˆè¿åŠ¨ã€è®¤çŸ¥å’Œå¬è§‰åŠŸèƒ½çš„å…¨é¢æœºå™¨äººå¹²é¢„çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç”¨æˆ·å‚ä¸åº¦ã€è®¤çŸ¥å‚ä¸åº¦å’ŒåŠ¨æœºåœ¨ç‰©ç†äººæœºäº¤äº’ä¸­çš„ä»»åŠ¡æ‰§è¡Œå¯¹è¿åŠ¨å­¦ä¹ éå¸¸é‡è¦ï¼Œå°¤å…¶åœ¨æœºå™¨äººåº·å¤é¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨äººåº·å¤ç³»ç»Ÿåœ¨ç»´æŒç”¨æˆ·å‚ä¸åº¦æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå½±å“æ²»ç–—æ•ˆæœçš„é¢„æµ‹ã€‚</li>
<li>å¤šæ¨¡å¼æœºå™¨äººäº¤äº’æ–¹æ³•é€šè¿‡ç»“åˆå¤šç§æ„Ÿå®˜åˆºæ¿€ï¼ˆå¦‚è§†è§‰ã€è¿åŠ¨ã€è®¤çŸ¥å’Œå¬è§‰ï¼‰æ¥æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†å¤šæ¨¡æ€ä»»åŠ¡ä¸ä¼ ç»Ÿè¿åŠ¨ä»»åŠ¡çš„è„‘ç”µå›¾ç”Ÿç‰©æ ‡å¿—ç‰©æ¥è¡¡é‡å‚ä¸åº¦ã€‚</li>
<li>ç›¸å¯¹é˜¿å°”æ³•åŠŸç‡ç­‰EEGç”Ÿç‰©æ ‡å¿—ç‰©åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„æ”¹å–„æ˜¾è‘—ï¼Œè¡¨æ˜å¤šæ¨¡æ€æ–¹æ³•èƒ½æé«˜ç”¨æˆ·å‚ä¸åº¦ã€‚</li>
<li>å¤šæ¨¡æ€åè®®èƒ½ç»´æŒç¨³å®šçš„å‚ä¸åº¦ï¼Œå³ä½¿éšç€æ—¶é—´æ¨ç§»ï¼Œè¿™åœ¨é•¿æœŸæ²»ç–—ä¸­æ˜¯å…³é”®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9d7e0a9a026a5275bc42ea384ede41d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ab0b0df55a1a321846840dad36d70084.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6bbc17df22f2f5a0f43f3bb9b40f6e2d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0e4c3f2ebe2ad487dd1048fcf5118eee.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da6d465f9b9a30bf5aa2f8313f765ffd.jpg" align="middle">
</details>




<h2 id="Multiple-Choice-Learning-for-Efficient-Speech-Separation-with-Many-Speakers"><a href="#Multiple-Choice-Learning-for-Efficient-Speech-Separation-with-Many-Speakers" class="headerlink" title="Multiple Choice Learning for Efficient Speech Separation with Many   Speakers"></a>Multiple Choice Learning for Efficient Speech Separation with Many   Speakers</h2><p><strong>Authors:David Perera, FranÃ§ois Derrida, ThÃ©o Mariotte, GaÃ«l Richard, Slim Essid</strong></p>
<p>Training speech separation models in the supervised setting raises a permutation problem: finding the best assignation between the model predictions and the ground truth separated signals. This inherently ambiguous task is customarily solved using Permutation Invariant Training (PIT). In this article, we instead consider using the Multiple Choice Learning (MCL) framework, which was originally introduced to tackle ambiguous tasks. We demonstrate experimentally on the popular WSJ0-mix and LibriMix benchmarks that MCL matches the performances of PIT, while being computationally advantageous. This opens the door to a promising research direction, as MCL can be naturally extended to handle a variable number of speakers, or to tackle speech separation in the unsupervised setting. </p>
<blockquote>
<p>åœ¨ç›‘ç£ç¯å¢ƒä¸‹è®­ç»ƒè¯­éŸ³åˆ†ç¦»æ¨¡å‹ä¼šå¼•å‘æ’åˆ—é—®é¢˜ï¼šå³å¦‚ä½•ä¸ºæ¨¡å‹é¢„æµ‹å’ŒçœŸå®åˆ†ç¦»ä¿¡å·æ‰¾åˆ°æœ€ä½³çš„åˆ†é…æ–¹æ¡ˆã€‚è¿™ä¸ªæœ¬è´¨ä¸Šæ˜¯æ¨¡ç³Šçš„ä»»åŠ¡é€šå¸¸é€šè¿‡ä½¿ç”¨æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰æ¥è§£å†³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨æœ€åˆä¸ºè§£å†³æ¨¡ç³Šä»»åŠ¡è€Œå¼•å…¥çš„å¤šé€‰å­¦ä¹ ï¼ˆMCLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„WSJ0-mixå’ŒLibriMixåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¯æ˜äº†MCLçš„æ€§èƒ½ä¸PITç›¸åŒ¹é…ï¼ŒåŒæ—¶è®¡ç®—ä¸Šæ›´æœ‰ä¼˜åŠ¿ã€‚è¿™ä¸ºç ”ç©¶æ–¹å‘æ‰“å¼€äº†å¤§é—¨ï¼Œå› ä¸ºMCLå¯ä»¥è‡ªç„¶åœ°æ‰©å±•åˆ°å¤„ç†å¯å˜æ•°é‡çš„å‘è¨€äººï¼Œæˆ–è§£å†³éç›‘ç£ç¯å¢ƒä¸‹çš„è¯­éŸ³åˆ†ç¦»é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18497v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­éŸ³åˆ†ç¦»æ¨¡å‹çš„è®­ç»ƒé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰ç›‘ç£çš„ç¯å¢ƒä¸‹ã€‚ç”±äºæ¨¡å‹é¢„æµ‹å’ŒçœŸå®åˆ†ç¦»ä¿¡å·ä¹‹é—´çš„åˆ†é…å­˜åœ¨æ’åˆ—é—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰æ¥è§£å†³æ­¤å›ºæœ‰çš„æ¨¡ç³Šä»»åŠ¡ã€‚æœ¬æ–‡è€ƒè™‘é‡‡ç”¨åŸå§‹ç”¨äºè§£å†³æ¨¡ç³Šä»»åŠ¡çš„å¤šé¡¹é€‰æ‹©å­¦ä¹ ï¼ˆMCLï¼‰æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æµè¡Œçš„WSJ0-mixå’ŒLibriMixåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMCLçš„æ€§èƒ½ä¸PITç›¸åŒ¹é…ï¼ŒåŒæ—¶è®¡ç®—ä¸Šæ›´æœ‰ä¼˜åŠ¿ã€‚è¿™ä¸ºMCLåœ¨å¯å˜è¯´è¯äººæ•°æˆ–å¤šé€šé“è¯­éŸ³åˆ†ç¦»ç­‰æ–¹å‘çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³åˆ†ç¦»æ¨¡å‹çš„è®­ç»ƒé¢ä¸´æ’åˆ—é—®é¢˜ï¼Œå³æ¨¡å‹é¢„æµ‹ä¸çœŸå®åˆ†ç¦»ä¿¡å·ä¹‹é—´çš„æœ€ä½³åˆ†é…é—®é¢˜ã€‚</li>
<li>æ’åˆ—ä¸å˜è®­ç»ƒï¼ˆPITï¼‰æ˜¯å¸¸è§„è§£å†³æ­¤æ¨¡ç³Šä»»åŠ¡çš„æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥å¤šé¡¹é€‰æ‹©å­¦ä¹ ï¼ˆMCLï¼‰æ¡†æ¶ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>åœ¨WSJ0-mixå’ŒLibriMixåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMCLæ€§èƒ½ä¸PITç›¸å½“ã€‚</li>
<li>MCLå…·æœ‰è®¡ç®—ä¼˜åŠ¿ï¼Œä¸ºå¤„ç†å¯å˜è¯´è¯äººæ•°æˆ–ç›‘ç£ç¯å¢ƒä¸‹çš„è¯­éŸ³åˆ†ç¦»æä¾›äº†ç ”ç©¶æ½œåŠ›ã€‚</li>
<li>MCLå¯è‡ªç„¶æ‰©å±•åˆ°å¤„ç†ä¸åŒåœºæ™¯ï¼Œå¦‚å¤šé€šé“è¯­éŸ³åˆ†ç¦»ç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-393696b300164ff7504f20e5e027e363.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7d3ec7b6b0cd262e3e346266c8640b41.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a5f507b445890ce46695be7292e75aa.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ed741ce28604f85f3c0a72f28aa2097f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fcc196dcd43a85c2097c41d444e529c2.jpg" align="middle">
</details>




<h2 id="AMPS-ASR-with-Multimodal-Paraphrase-Supervision"><a href="#AMPS-ASR-with-Multimodal-Paraphrase-Supervision" class="headerlink" title="AMPS: ASR with Multimodal Paraphrase Supervision"></a>AMPS: ASR with Multimodal Paraphrase Supervision</h2><p><strong>Authors:Amruta Parulekar, Abhishek Gupta, Sameep Chattopadhyay, Preethi Jyothi</strong></p>
<p>Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative reductions in word error rates (WERs) of up to 5%. We present detailed analyses of our system using both objective and human evaluation metrics. </p>
<blockquote>
<p>å¤šè¯­ç§è‡ªç„¶å£è¯­ä¸ºæœ€å…ˆè¿›çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå¸¦æ¥äº†è¯¸å¤šæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯AMPSï¼Œè¯¥æŠ€æœ¯å¢å¼ºäº†ä¸€ç§å¤šè¯­ç§å¤šåª’ä½“ASRç³»ç»Ÿï¼Œé€šè¿‡åŸºäºé‡Šä¹‰çš„ç›‘ç£æ”¹è¿›äº†å¤šç§è¯­è¨€çš„å¯¹è¯ASRè¡¨ç°ï¼ŒåŒ…æ‹¬å°åœ°è¯­ã€é©¬æ‹‰åœ°è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ã€åçº³è¾¾è¯­å’Œå°¼æ‰¬è´¾è¯­ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒå¤šåª’ä½“ASRæ¨¡å‹æ—¶ï¼Œä¼šä½¿ç”¨å‚è€ƒè¯‘æ–‡çš„é‡Šä¹‰ä½œä¸ºé¢å¤–çš„ç›‘ç£æ–¹å¼ï¼Œå¹¶é’ˆå¯¹è¡¨ç°ä¸ä½³çš„ASRè¡¨ç°é€‰æ‹©æ€§åœ°å¯åŠ¨è¿™ç§é‡Šä¹‰ç›®æ ‡ã€‚é€šè¿‡ä½¿ç”¨AMPSå’Œæœ€å…ˆè¿›çš„å¤šåª’ä½“æ¨¡å‹æ— ç¼M4Tçš„ç»“åˆï¼Œæˆ‘ä»¬å®ç°äº†æ˜¾è‘—çš„ç›¸å¯¹å‡å°‘ï¼Œè¯é”™è¯¯ç‡é™ä½äº†é«˜è¾¾5%ã€‚æˆ‘ä»¬é€šè¿‡å®¢è§‚å’Œäººç±»è¯„ä¼°æŒ‡æ ‡ä¸¤ç§æ–¹æ³•å¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18368v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å¤„ç†è‡ªå‘æ€§æˆ–å¤šè¯­ç§å¯¹è¯è¯­éŸ³æ—¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯AMPSï¼Œè¯¥æŠ€æœ¯é€šè¿‡åŸºäºåŒä¹‰çŸ­è¯­ç›‘ç£çš„æ–¹å¼å¢å¼ºå¤šè¯­ç§å¤šåª’ä½“ASRç³»ç»Ÿï¼Œä»¥æé«˜å¯¹åŒ…æ‹¬å°åœ°è¯­ã€é©¬æ‹‰åœ°è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ã€åçº³è¾¾è¯­å’Œå°¼äºšè´¾è¯­åœ¨å†…çš„å¤šç§è¯­è¨€çš„å¯¹è¯è¯­éŸ³è¯†åˆ«ã€‚åœ¨è®­ç»ƒå¤šåª’ä½“ASRæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚è€ƒè½¬å½•çš„åŒä¹‰çŸ­è¯­ä½œä¸ºé¢å¤–çš„ç›‘ç£ï¼Œå¹¶é€‰æ‹©æ€§åœ°å¯¹è¡¨ç°ä¸ä½³çš„ASRæ€§èƒ½ç‰‡æ®µä½¿ç”¨åŒä¹‰çŸ­è¯­ç›®æ ‡ã€‚é€šè¿‡å°†AMPSä¸å…ˆè¿›çš„SeamlessM4Tå¤šåª’ä½“æ¨¡å‹ç»“åˆä½¿ç”¨ï¼Œæˆ‘ä»¬è·å¾—äº†æ˜¾è‘—çš„ç›¸å¯¹è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ï¼Œæœ€é«˜è¾¾5%ã€‚æœ¬ç ”ç©¶é€šè¿‡å®¢è§‚å’Œäººç±»è¯„ä¼°æŒ‡æ ‡å¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AMPSæŠ€æœ¯é€šè¿‡ç»“åˆåŒä¹‰çŸ­è¯­ç›‘ç£å¢å¼ºäº†å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>AMPSèƒ½å¤Ÿåº”ç”¨äºå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬å°åœ°è¯­ã€é©¬æ‹‰åœ°è¯­ã€é©¬æ‹‰é›…æ‹‰å§†è¯­ã€åçº³è¾¾è¯­å’Œå°¼äºšè´¾è¯­ã€‚</li>
<li>åœ¨è®­ç»ƒå¤šåª’ä½“ASRæ¨¡å‹æ—¶ï¼Œä½¿ç”¨äº†å‚è€ƒè½¬å½•çš„åŒä¹‰çŸ­è¯­ä½œä¸ºé¢å¤–çš„ç›‘ç£ä¿¡æ¯ã€‚</li>
<li>å¯¹äºASRæ€§èƒ½ä¸ä½³çš„éƒ¨åˆ†ï¼Œä¼šé€‰æ‹©æ€§ä½¿ç”¨åŒä¹‰çŸ­è¯­ç›®æ ‡è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>ç»“åˆAMPSæŠ€æœ¯å’Œå…ˆè¿›çš„SeamlessM4Tå¤šåª’ä½“æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>è¯¥ç ”ç©¶é€šè¿‡å®¢è§‚è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°æŒ‡æ ‡å¯¹ç³»ç»Ÿè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e1e2fa50c2c86467a0cc5d314aa22720.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-336c7f5eda3569f0ee72b3c03b54fcca.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-92bf1dc1ed7c62fdbbc68a9b75f4b72a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a8791c7f0160546dbb86fb0c2f2d4f69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-29e1a6b0331cd203e0b35cc7bee2e4f1.jpg" align="middle">
</details>




<h2 id="SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation"><a href="#SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation" class="headerlink" title="SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and   Generation"></a>SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and   Generation</h2><p><strong>Authors:Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang</strong></p>
<p>Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a &#96;&#96;thinkingâ€™â€™ mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omniâ€™s versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon. </p>
<blockquote>
<p>å…¨åŒå·¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå¤„ç†å¤šæ ·åŒ–çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä½¿äººç±»ä¸æœºå™¨ä¹‹é—´çš„å¯¹è¯æ›´åŠ è‡ªç„¶å’Œæ— ç¼ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å—åŒ–å¯¹è¯AIç³»ç»Ÿä¸åŒï¼Œåè€…å°†è¯­éŸ³è¯†åˆ«ã€ç†è§£å’Œæ–‡æœ¬åˆ°è¯­éŸ³çš„ç”Ÿæˆè¿‡ç¨‹åˆ’åˆ†ä¸ºç‹¬ç«‹çš„ä¸åŒç»„ä»¶ï¼Œå¤šæ¨¡æ€LLMåˆ™ä»¥å•ä¸€ç«¯åˆ°ç«¯çš„æ¨¡å‹è¿›è¡Œå·¥ä½œã€‚è¿™ç§æµç¨‹åŒ–çš„è®¾è®¡æ¶ˆé™¤äº†ç»„ä»¶é—´é”™è¯¯ä¼ é€’ï¼Œå¹¶å……åˆ†åˆ©ç”¨è¾“å…¥è¯­éŸ³ä¿¡å·ä¸­ä¸°å¯Œçš„éè¯­è¨€ä¿¡æ¯ã€‚æˆ‘ä»¬ä»‹ç»äº†SALMONN-omniï¼Œè¿™æ˜¯ä¸€ç§æ— ç¼–è§£ç å™¨ã€å…¨åŒå·¥è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿåœ¨è¯´è¯æ—¶åŒæ—¶ç›‘å¬è‡ªå·±çš„ç”Ÿæˆçš„è¯­éŸ³å’ŒèƒŒæ™¯å£°éŸ³ã€‚ä¸ºäº†æ”¯æŒè¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„åŒå·¥å¯¹è¯æ¡†æ¶ï¼Œèå…¥äº†ä¸€ç§â€œæ€è€ƒâ€æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä¾èµ–äºåµŒå…¥è€Œéç¼–è§£ç å™¨ï¼ˆé‡åŒ–è¯­éŸ³å’ŒéŸ³é¢‘ä»¤ç‰Œï¼‰æ¥å®ç°å¼‚æ­¥æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSALMONN-omniåœ¨å¹¿æ³›çš„æµå¼è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¾ˆå¼ºçš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³é—®ç­”ã€‚æ­¤å¤–ï¼ŒSALMONN-omniåœ¨è½®æ›¿å‘è¨€ã€æ’è¯å’Œå›å£°æ¶ˆé™¤ç­‰åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜å…¶ä½œä¸ºå…¨åŒå·¥å¯¹è¯AIç³»ç»Ÿçš„ç¨³å¥åŸå‹çš„æ½œåŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSALMONN-omniæ˜¯é¦–ä¸ªæ— ç¼–è§£ç å™¨çš„æ­¤ç±»æ¨¡å‹ã€‚æˆ‘ä»¬å°†å¾ˆå¿«å‘å¸ƒå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šå’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18138v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¨åŒå·¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¡†æ¶ï¼Œå¯ä»¥ç»Ÿä¸€è§£å†³å¤šç§è¯­éŸ³ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œä¿ƒè¿›æ›´è‡ªç„¶å’Œæ— ç¼çš„äººæœºå¯¹è¯ã€‚å…¨åŒå·¥æ¨¡å‹SALMONN-omniæ— éœ€ç¼–è§£ç å™¨ï¼Œå³å¯å®ç°åŒæ—¶å¬å–è‡ªèº«ç”Ÿæˆçš„è¯­éŸ³å’ŒèƒŒæ™¯å£°éŸ³çš„åŒæ—¶è¿›è¡Œè¯­éŸ³è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSALMONN-omniåœ¨å„ç§æµå¼è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³é—®ç­”ç­‰ã€‚å…¶èƒ½å¦¥å–„ç®¡ç†è½®æµå‘è¨€ã€ä¸­æ–­å‘è¨€å’Œå›å£°æ¶ˆé™¤ç­‰åœºæ™¯ï¼Œä¸ºå…¨åŒå·¥å¯¹è¯å¼AIç³»ç»Ÿæä¾›äº†ç¨³å¥çš„åŸå‹ã€‚å®ƒæ˜¯é¦–ä¸ªæ— éœ€ç¼–è§£ç å™¨çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŒå·¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†ç»Ÿä¸€çš„æ¡†æ¶æ¥è§£å†³å¤šç§è¯­éŸ³ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>LLMsä¿ƒè¿›äº†æ›´è‡ªç„¶å’Œæ— ç¼çš„äººæœºå¯¹è¯ã€‚</li>
<li>SALMONN-omniæ¨¡å‹æ— éœ€ç¼–è§£ç å™¨å³å¯å®ç°å…¨åŒå·¥è¯­éŸ³ç†è§£å’Œç”Ÿæˆã€‚</li>
<li>SALMONN-omniåœ¨å„ç§æµå¼è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³é—®ç­”ç­‰ã€‚</li>
<li>SALMONN-omnièƒ½å¦¥å–„ç®¡ç†è½®æµå‘è¨€ã€ä¸­æ–­å‘è¨€å’Œå›å£°æ¶ˆé™¤ç­‰åœºæ™¯ã€‚</li>
<li>SALMONN-omniä¸ºå…¨åŒå·¥å¯¹è¯å¼AIç³»ç»Ÿæä¾›äº†ç¨³å¥çš„åŸå‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-af83771b0fcf83031a32e1732ec9e749.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-380af3e54978f0e064a9d5c87187adef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-da3211095949e31aeb53708715de1dd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e4211a8ca2e98b1f4a9df0ccfd0954b3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-59aaeeb020314e6bc7f9d8676410efd0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abcad34c82c8a99ad0b9dba70679e5b4.jpg" align="middle">
</details>




<h2 id="Speech-Separation-using-Neural-Audio-Codecs-with-Embedding-Loss"><a href="#Speech-Separation-using-Neural-Audio-Codecs-with-Embedding-Loss" class="headerlink" title="Speech Separation using Neural Audio Codecs with Embedding Loss"></a>Speech Separation using Neural Audio Codecs with Embedding Loss</h2><p><strong>Authors:Jia Qi Yip, Chin Yuen Kwok, Bin Ma, Eng Siong Chng</strong></p>
<p>Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨é€šè¿‡å…è®¸åœ¨é«˜åº¦å‹ç¼©çš„è¡¨ç¤ºä¸Šæ‰§è¡Œè¯­éŸ³ä»»åŠ¡ï¼Œä»è€Œé©æ–°äº†éŸ³é¢‘å¤„ç†ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥åœ¨è¿™äº›å‹ç¼©åŸŸå†…å®ç°è¯­éŸ³åˆ†ç¦»ï¼Œä»è€Œæä¾›æ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä»ç„¶ä¾èµ–äºåŸºäºæ³¢å½¢çš„æŸå¤±å‡½æ•°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦è¿›è¡Œä¸å¿…è¦çš„è§£ç æ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŸºäºç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–è§£ç å™¨çš„è¯­éŸ³åˆ†ç¦»çš„æ–°å‹åµŒå…¥æŸå¤±ï¼Œå®ƒç›´æ¥åœ¨å‹ç¼©çš„éŸ³é¢‘è¡¨ç¤ºä¸Šè¿è¡Œï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¶ˆé™¤äº†å¯¹è§£ç çš„éœ€æ±‚ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è¯„ä¼°æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¾µå…¥æ€§å’Œéä¾µå…¥æ€§æ–¹æ³•ï¼Œè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒåµŒå…¥æŸå¤±å¯ç”¨äºè®­ç»ƒåŸºäºç¼–è§£ç å™¨çš„è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œåœ¨è®­ç»ƒé€Ÿåº¦å’Œè®¡ç®—æˆæœ¬æ–¹é¢å®ç°2å€çš„æå‡ï¼ŒåŒæ—¶åœ¨WSJ0-2mixæ•°æ®é›†ä¸Šå®ç°æ›´å¥½çš„DNSMOSå’ŒSTOIæ€§èƒ½ï¼Œè·¨è¶Š3ç§ä¸åŒçš„é¢„è®­ç»ƒç¼–è§£ç å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17998v1">PDF</a> Accepted by APSIPA ASC 2024</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç å™¨çš„å‡ºç°ï¼Œä¸ºéŸ³é¢‘å¤„ç†å¸¦æ¥äº†é©å‘½æ€§çš„å˜é©ï¼Œå®ƒèƒ½å¤Ÿåœ¨é«˜åº¦å‹ç¼©çš„è¡¨ç¤ºä¸Šæ‰§è¡Œè¯­éŸ³ä»»åŠ¡ã€‚æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œå¯åœ¨è¿™äº›å‹ç¼©åŸŸå†…å®ç°è¯­éŸ³åˆ†ç¦»ï¼Œé™ä½äº†è®­ç»ƒå’Œæ¨ç†æˆæœ¬ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä»ä¾èµ–äºåŸºäºæ³¢å½¢çš„æŸå¤±å‡½æ•°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦è¿›è¡Œä¸å¿…è¦çš„è§£ç æ­¥éª¤ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åµŒå…¥æŸå¤±ï¼Œç”¨äºåŸºäºç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç å™¨çš„è¯­éŸ³åˆ†ç¦»ï¼Œå¯ç›´æ¥åœ¨å‹ç¼©çš„éŸ³é¢‘è¡¨ç¤ºä¸Šæ“ä½œï¼Œä»è€Œæ¶ˆé™¤è®­ç»ƒè¿‡ç¨‹ä¸­çš„è§£ç éœ€æ±‚ã€‚é€šè¿‡å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è¯„ä¼°æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¾µå…¥æ€§å’Œéä¾µå…¥æ€§æ–¹æ³•ï¼Œæˆ‘ä»¬éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒåµŒå…¥æŸå¤±å¯ç”¨äºè®­ç»ƒåŸºäºç¼–ç å™¨çš„è¯­éŸ³åˆ†ç¦»æ¨¡å‹ï¼Œåœ¨WSJ0-2mixæ•°æ®é›†ä¸Šå®ç°äº†DNSMOSå’ŒSTOIæ€§èƒ½çš„æ”¹å–„ï¼ŒåŒæ—¶è®­ç»ƒé€Ÿåº¦å’Œè®¡ç®—æˆæœ¬æé«˜äº†ä¸¤å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œéŸ³é¢‘ç¼–ç å™¨å®ç°äº†åœ¨é«˜åº¦å‹ç¼©çš„è¡¨ç¤ºä¸Šè¿›è¡Œè¯­éŸ³ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>æœ€æ–°ç ”ç©¶è¯æ˜äº†åœ¨å‹ç¼©åŸŸå†…è¿›è¡Œè¯­éŸ³åˆ†ç¦»çš„å¯è¡Œæ€§ã€‚</li>
<li>å½“å‰æ–¹æ³•ä»ä¾èµ–äºæ³¢å½¢æŸå¤±å‡½æ•°ï¼Œå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦é¢å¤–çš„è§£ç æ­¥éª¤ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„åµŒå…¥æŸå¤±å‡½æ•°ï¼Œå¯ç›´æ¥åœ¨å‹ç¼©çš„éŸ³é¢‘è¡¨ç¤ºä¸Šè¿›è¡Œè¯­éŸ³åˆ†ç¦»è®­ç»ƒï¼Œæ— éœ€è§£ç æ­¥éª¤ã€‚</li>
<li>åµŒå…¥æŸå¤±å‡½æ•°èƒ½å¤Ÿæé«˜è®­ç»ƒé€Ÿåº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>é€šè¿‡å®¢è§‚æŒ‡æ ‡å’Œæ„ŸçŸ¥è¯„ä¼°æŠ€æœ¯éªŒè¯äº†æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-037d4ea3f14bb207b726149ac1e7206b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8e175a11904a657b0d69385177ffa142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-36d3cfeae841cdca0819b94291e1ca39.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-21fd829a2496199545baa09ca8e3e9ea.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ba3a0b7e3e24e3a0592b9811be509c24.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4962aaa7ad4dbf82990840ed00cd0a9c.jpg" align="middle">
</details>




<h2 id="Disentangled-Transformer-An-Explainable-End-to-End-Automatic-Speech-Recognition-Model-with-Speech-Content-Context-Separation"><a href="#Disentangled-Transformer-An-Explainable-End-to-End-Automatic-Speech-Recognition-Model-with-Speech-Content-Context-Separation" class="headerlink" title="Disentangled-Transformer: An Explainable End-to-End Automatic Speech   Recognition Model with Speech Content-Context Separation"></a>Disentangled-Transformer: An Explainable End-to-End Automatic Speech   Recognition Model with Speech Content-Context Separation</h2><p><strong>Authors:Pu Wang, Hugo Van hamme</strong></p>
<p>End-to-end transformer-based automatic speech recognition (ASR) systems often capture multiple speech traits in their learned representations that are highly entangled, leading to a lack of interpretability. In this study, we propose the explainable Disentangled-Transformer, which disentangles the internal representations into sub-embeddings with explicit content and speaker traits based on varying temporal resolutions. Experimental results show that the proposed Disentangled-Transformer produces a clear speaker identity, separated from the speech content, for speaker diarization while improving ASR performance. </p>
<blockquote>
<p>åŸºäºç«¯åˆ°ç«¯çš„è½¬æ¢å™¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿåœ¨å…¶å­¦ä¹ åˆ°çš„è¡¨ç¤ºä¸­é€šå¸¸ä¼šæ•è·å¤šä¸ªé«˜åº¦çº ç¼ çš„è¯­éŸ³ç‰¹å¾ï¼Œå¯¼è‡´ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†å¯è§£é‡Šçš„è§£çº ç¼ è½¬æ¢å™¨ï¼ˆDisentangled-Transformerï¼‰ï¼Œè¯¥è½¬æ¢å™¨å°†å†…éƒ¨è¡¨ç¤ºè§£çº ç¼ ä¸ºå…·æœ‰æ˜ç¡®å†…å®¹å’Œè¯´è¯è€…ç‰¹å¾çš„å­åµŒå…¥ï¼ŒåŸºäºä¸åŒçš„æ—¶é—´åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£çº ç¼ è½¬æ¢å™¨åœ¨è¯´è¯äººèº«ä»½è¾¨æä¸­èƒ½å¤Ÿæ¸…æ™°åœ°äº§ç”Ÿä¸è¯­éŸ³å†…å®¹åˆ†ç¦»çš„è¯´è¯äººèº«ä»½ï¼ŒåŒæ—¶æé«˜ASRæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17846v1">PDF</a> Accepted by the 6th IEEE International Conference on Image Processing   Applications and Systems</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æå‡ºäº†å¯è§£é‡Šçš„è§£çº ç¼ è½¬æ¢å™¨ï¼ˆDisentangled-Transformerï¼‰ï¼Œè¯¥è½¬æ¢å™¨å¯å°†ç«¯åˆ°ç«¯çš„åŸºäºè½¬æ¢å™¨çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„å†…éƒ¨è¡¨ç¤ºå½¢å¼è§£çº ç¼ ä¸ºå…·æœ‰æ˜ç¡®å†…å®¹å’Œè¯´è¯è€…ç‰¹å¾çš„å­åµŒå…¥ï¼ŒåŸºäºä¸åŒçš„æ—¶é—´åˆ†è¾¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£çº ç¼ è½¬æ¢å™¨åœ¨è¯´è¯äººèº«ä»½æ¸…æ™°åˆ†ç¦»çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è¯­éŸ³å†…å®¹çš„è¯†åˆ«æ€§èƒ½ï¼Œå¹¶å¯ç”¨äºè¯´è¯äººæ‘˜è¦åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç«¯åˆ°ç«¯çš„åŸºäºè½¬æ¢å™¨çš„ASRç³»ç»Ÿå­˜åœ¨å¤šä¸ªè¯­éŸ³ç‰¹å¾é«˜åº¦çº ç¼ çš„é—®é¢˜ï¼Œå¯¼è‡´ç¼ºä¹å¯è§£é‡Šæ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†å¯è§£é‡Šçš„è§£çº ç¼ è½¬æ¢å™¨ï¼ˆDisentangled-Transformerï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è§£çº ç¼ è½¬æ¢å™¨èƒ½å¤Ÿå°†å†…éƒ¨è¡¨ç¤ºå½¢å¼è½¬åŒ–ä¸ºå­åµŒå…¥ï¼Œè¿™äº›å­åµŒå…¥å…·æœ‰æ˜ç¡®çš„å†…å®¹å’Œè¯´è¯è€…ç‰¹å¾ã€‚</li>
<li>åŸºäºä¸åŒçš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œè§£çº ç¼ è½¬æ¢å™¨å®ç°äº†è¿™ä¸€è½¬åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè§£çº ç¼ è½¬æ¢å™¨èƒ½å¤Ÿæ¸…æ™°åœ°åˆ†ç¦»è¯´è¯äººçš„èº«ä»½å’Œè¯­éŸ³å†…å®¹ã€‚</li>
<li>è§£çº ç¼ è½¬æ¢å™¨åœ¨è¯´è¯äººèº«ä»½æ¸…æ™°åˆ†ç¦»çš„æƒ…å†µä¸‹ï¼Œæé«˜äº†è¯­éŸ³å†…å®¹çš„è¯†åˆ«æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c5a64a7b6340a4ae69860300839e0602.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a07898c5b1dbcd24b430b54420413e69.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ceab89681f51a5a67c3bf424da1c436.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-46714c7b5868231785880d41a952e8ce.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e476f9265139489394177a227ba0b802.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-16c9dd5e8adb471b59fe82d7c8c91097.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a4349e3f1ad538fd8155351d046c0bab.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6d3737d8f83e218d1de5bf750e161bb7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-bbdeedec281955a4955850a8481701dd.jpg" align="middle">
</details>




<h2 id="BERT-or-FastText-A-Comparative-Analysis-of-Contextual-as-well-as-Non-Contextual-Embeddings"><a href="#BERT-or-FastText-A-Comparative-Analysis-of-Contextual-as-well-as-Non-Contextual-Embeddings" class="headerlink" title="BERT or FastText? A Comparative Analysis of Contextual as well as   Non-Contextual Embeddings"></a>BERT or FastText? A Comparative Analysis of Contextual as well as   Non-Contextual Embeddings</h2><p><strong>Authors:Abhay Shanbhag, Suramya Jadhav, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi</strong></p>
<p>Natural Language Processing (NLP) for low-resource languages presents significant challenges, particularly due to the scarcity of high-quality annotated data and linguistic resources. The choice of embeddings plays a critical role in enhancing the performance of NLP tasks, such as news classification, sentiment analysis, and hate speech detection, especially for low-resource languages like Marathi. In this study, we investigate the impact of various embedding techniques- Contextual BERT-based, Non-Contextual BERT-based, and FastText-based on NLP classification tasks specific to the Marathi language. Our research includes a thorough evaluation of both compressed and uncompressed embeddings, providing a comprehensive overview of how these embeddings perform across different scenarios. Specifically, we compare two BERT model embeddings, Muril and MahaBERT, as well as two FastText model embeddings, IndicFT and MahaFT. Our evaluation includes applying embeddings to a Multiple Logistic Regression (MLR) classifier for task performance assessment, as well as TSNE visualizations to observe the spatial distribution of these embeddings. The results demonstrate that contextual embeddings outperform non-contextual embeddings. Furthermore, BERT-based non-contextual embeddings extracted from the first BERT embedding layer yield better results than FastText-based embeddings, suggesting a potential alternative to FastText embeddings. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å¯¹äºä½èµ„æºè¯­è¨€æ¥è¯´å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºé«˜è´¨é‡æ ‡æ³¨æ•°æ®å’Œè¯­è¨€èµ„æºçš„ç¨€ç¼ºã€‚åµŒå…¥å±‚çš„é€‰æ‹©åœ¨æé«˜NLPä»»åŠ¡æ€§èƒ½ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä¾‹å¦‚æ–°é—»åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œä»‡æ¨è¨€è®ºæ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºåƒé©¬æ‹‰åœ°è¯­è¿™æ ·çš„ä½èµ„æºè¯­è¨€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¤šç§åµŒå…¥æŠ€æœ¯çš„å½±å“ï¼ŒåŒ…æ‹¬åŸºäºä¸Šä¸‹æ–‡çš„BERTã€éåŸºäºä¸Šä¸‹æ–‡çš„BERTå’ŒåŸºäºFastTextçš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯¹é©¬æ‹‰åœ°è¯­ç‰¹å®šçš„NLPåˆ†ç±»ä»»åŠ¡å…·æœ‰é‡è¦å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶åŒ…æ‹¬å¯¹å‹ç¼©å’Œéå‹ç¼©åµŒå…¥çš„å½»åº•è¯„ä¼°ï¼Œå…¨é¢æ¦‚è¿°äº†è¿™äº›åµŒå…¥åœ¨ä¸åŒåœºæ™¯ä¸­çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§BERTæ¨¡å‹åµŒå…¥ï¼ˆMurilå’ŒMahaBERTï¼‰ï¼Œä»¥åŠä¸¤ç§FastTextæ¨¡å‹åµŒå…¥ï¼ˆIndicFTå’ŒMahaFTï¼‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬å°†è¿™äº›åµŒå…¥åº”ç”¨äºå¤šé‡é€»è¾‘å›å½’ï¼ˆMLRï¼‰åˆ†ç±»å™¨è¿›è¡Œä»»åŠ¡æ€§èƒ½è¯„ä¼°ï¼Œå¹¶ä½¿ç”¨TSNEå¯è§†åŒ–è§‚å¯Ÿè¿™äº›åµŒå…¥çš„ç©ºé—´åˆ†å¸ƒã€‚ç»“æœè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡åµŒå…¥ä¼˜äºéä¸Šä¸‹æ–‡åµŒå…¥ã€‚æ­¤å¤–ï¼Œä»BERTåµŒå…¥çš„ç¬¬ä¸€å±‚æå–çš„åŸºäºBERTçš„éä¸Šä¸‹æ–‡åµŒå…¥æ¯”åŸºäºFastTextçš„åµŒå…¥äº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œè¿™å¯èƒ½æˆä¸ºFastTextåµŒå…¥çš„ä¸€ç§æ½œåœ¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17661v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åœ¨ä½èµ„æºè¯­è¨€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é«˜è´¨é‡æ ‡æ³¨æ•°æ®å’Œè¯­è¨€èµ„æºçš„ç¨€ç¼ºæ€§ã€‚æ–‡ç« èšç„¦äºåµŒå…¥æŠ€æœ¯å¯¹æå‡NLPä»»åŠ¡æ€§èƒ½çš„é‡è¦æ€§ï¼Œå¦‚æ–°é—»åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œä»‡æ¨è¨€è®ºæ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½èµ„æºè¯­è¨€å¦‚é©¬æ‹‰åœ°è¯­ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šç§åµŒå…¥æŠ€æœ¯çš„å½±å“ï¼ŒåŒ…æ‹¬åŸºäºä¸Šä¸‹æ–‡çš„BERTã€éä¸Šä¸‹æ–‡çš„BERTå’ŒåŸºäºFastTextçš„åµŒå…¥æŠ€æœ¯ï¼Œåœ¨é©¬æ‹‰åœ°è¯­NLPåˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æ–‡ç« å…¨é¢è¯„ä»·äº†å‹ç¼©å’Œéå‹ç¼©åµŒå…¥ï¼Œå¹¶é€šè¿‡å®ä¾‹å±•ç¤ºå…¶åœ¨ä¸åŒåœºæ™¯ä¸­çš„åº”ç”¨æ•ˆæœã€‚ç»“æœè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡åµŒå…¥æŠ€æœ¯ä¼˜äºéä¸Šä¸‹æ–‡åµŒå…¥æŠ€æœ¯ï¼ŒåŸºäºBERTçš„éä¸Šä¸‹æ–‡åµŒå…¥è¡¨ç°ä¼˜äºåŸºäºFastTextçš„åµŒå…¥ï¼Œæˆä¸ºæ½œåœ¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½èµ„æºè¯­è¨€NLPé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯é«˜è´¨é‡æ ‡æ³¨æ•°æ®å’Œè¯­è¨€èµ„æºçš„ç¨€ç¼ºæ€§ã€‚</li>
<li>åµŒå…¥æŠ€æœ¯å¯¹äºæå‡NLPä»»åŠ¡æ€§èƒ½è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€ç¯å¢ƒä¸‹ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†åŸºäºä¸Šä¸‹æ–‡çš„BERTã€éä¸Šä¸‹æ–‡çš„BERTå’ŒåŸºäºFastTextçš„åµŒå…¥æŠ€æœ¯åœ¨é©¬æ‹‰åœ°è¯­NLPåˆ†ç±»ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>ä¸Šä¸‹æ–‡åµŒå…¥æŠ€æœ¯è¡¨ç°ä¼˜äºéä¸Šä¸‹æ–‡åµŒå…¥æŠ€æœ¯ã€‚</li>
<li>åŸºäºBERTçš„éä¸Šä¸‹æ–‡åµŒå…¥åœ¨æ–°é—»åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œä»‡æ¨è¨€è®ºæ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç›¸æ¯”åŸºäºFastTextçš„åµŒå…¥æŠ€æœ¯ï¼ŒBERTéä¸Šä¸‹æ–‡åµŒå…¥æä¾›äº†ä¸€ç§æ½œåœ¨æ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f68e9fed69772c17ffd74a79e382e4de.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f5049eb2f83c99fd34f3e75c92ba4a5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-89d024379be9f11c7f5c31f21d5473b8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-52ed655175e89865eed7024075a8158e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dade2b7e5a12b17fb07a068fb93f9325.jpg" align="middle">
</details>




<h2 id="Scaling-Speech-Text-Pre-training-with-Synthetic-Interleaved-Data"><a href="#Scaling-Speech-Text-Pre-training-with-Synthetic-Interleaved-Data" class="headerlink" title="Scaling Speech-Text Pre-training with Synthetic Interleaved Data"></a>Scaling Speech-Text Pre-training with Synthetic Interleaved Data</h2><p><strong>Authors:Aohan Zeng, Zhengxiao Du, Mingdao Liu, Lei Zhang, Shengmin Jiang, Yuxiao Dong, Jie Tang</strong></p>
<p>Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain. </p>
<blockquote>
<p>è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰æ¥å—è¯­éŸ³è¾“å…¥å¹¶äº§ç”Ÿè¯­éŸ³è¾“å‡ºï¼Œä¸åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”ï¼Œå®ç°äº†æ›´è‡ªç„¶çš„äººæœºäº¤äº’ã€‚ä¼ ç»Ÿå¼€å‘SpeechLMsçš„æ–¹æ³•å—é™äºç›‘ç£è¯­éŸ³æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œä»¥åŠä¸æ–‡æœ¬é¢„è®­ç»ƒæ•°æ®ç›¸æ¯”ï¼Œå¹¶è¡Œè¯­éŸ³-æ–‡æœ¬æ•°æ®æ˜æ˜¾ä¸è¶³ï¼Œä»è€Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ä»æ–‡æœ¬è¯­æ–™åº“è¡ç”Ÿçš„å¤§è§„æ¨¡åˆæˆäº¤ç»‡æ•°æ®æ¥æ‰©å±•è¯­éŸ³-æ–‡æœ¬é¢„è®­ç»ƒçš„æ–°æ–¹æ³•ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹å¹¶è¡Œè¯­éŸ³-æ–‡æœ¬æ•°æ®é›†çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»ç°æœ‰æ–‡æœ¬è¯­æ–™åº“ä¸­é‡‡æ ·æ–‡æœ¬ç‰‡æ®µå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°æ ‡è®°æ¨¡å‹åˆæˆç›¸åº”çš„è¯­éŸ³ç‰‡æ®µï¼Œæœ‰æ•ˆåœ°æ„å»ºäº†è¯­éŸ³-æ–‡æœ¬äº¤ç»‡æ•°æ®ï¼Œä»è€Œæ— éœ€ç”Ÿæˆå®é™…è¯­éŸ³ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä½¿ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å¹¶èå…¥å‘é‡é‡åŒ–ç“¶é¢ˆåˆ°ç¼–ç å™¨ä¸­æ¥å»ºç«‹ç›‘ç£è¯­éŸ³æ ‡è®°å™¨ã€‚è¿™ç§ç›‘ç£è®­ç»ƒæ–¹æ³•å³ä½¿åœ¨è¾ƒä½å¸§ç‡ï¼ˆä¾‹å¦‚12.5Hzï¼‰ä¸‹ä¹Ÿèƒ½äº§ç”Ÿå…·æœ‰å¼ºçƒˆè¯­ä¹‰ä¿ç•™çš„ç¦»æ•£è¯­éŸ³æ ‡è®°ï¼ŒåŒæ—¶ä»ä¿æŒè¯­éŸ³é‡å»ºè´¨é‡ã€‚ä»ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å§‹ï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒæ‰©å±•åˆ°1ä¸‡äº¿ä¸ªæ ‡è®°ï¼ˆä½¿ç”¨600Båˆæˆäº¤ç»‡è¯­éŸ³-æ–‡æœ¬æ•°æ®ï¼‰ï¼Œåœ¨è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œå£è¯­é—®ç­”æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå£è¯­é—®ç­”ä»»åŠ¡çš„æ€§èƒ½ä»ä¹‹å‰çš„æœ€ä½³æ°´å¹³13%ï¼ˆè«å¸Œï¼‰æé«˜åˆ°31%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨è¯­éŸ³å¯¹è¯æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘ä¸€ç§ç«¯åˆ°ç«¯çš„å£è¯­èŠå¤©æœºå™¨äººï¼Œå³ä½¿åœ¨è¯­éŸ³é¢†åŸŸï¼Œå…¶åœ¨å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡æ–¹é¢çš„è¡¨ç°ä¹Ÿå¯ä¸ç°æœ‰åŸºçº¿ç›¸åª²ç¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17607v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºæ–‡æœ¬çš„è¯­éŸ³è¯­è¨€æ¨¡å‹çš„æ„å»ºæ–¹æ³•ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨å¤§è§„æ¨¡åˆæˆäº¤é”™æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ‰©å±•ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•å¯ä»¥é¿å…å¯¹ä¼ ç»Ÿå¹¶è¡Œè¯­éŸ³æ–‡æœ¬çš„ä¾èµ–ï¼Œæ›´æœ‰æ•ˆåœ°æ„é€ è¯­éŸ³æ–‡æœ¬äº¤é”™æ•°æ®ï¼Œå¹¶æé«˜æ€§èƒ½è¡¨ç°ã€‚é€šè¿‡åœ¨ç«¯åˆ°ç«¯çš„è¯­éŸ³èŠå¤©æœºå™¨äººä¸­è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹åœ¨å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰æ¥å—è¯­éŸ³è¾“å…¥å¹¶äº§ç”Ÿè¯­éŸ³è¾“å‡ºï¼Œä¸ä¼ ç»ŸåŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæ›´åŠ è‡ªç„¶å’Œäººæ€§åŒ–ã€‚</li>
<li>ä¼ ç»Ÿè¯­éŸ³è¯­è¨€æ¨¡å‹çš„å‘å±•å—é™äºç¼ºä¹ç›‘ç£çš„è¯­éŸ³æ•°æ®å’Œå¹¶è¡Œè¯­éŸ³æ–‡æœ¬æ•°æ®ã€‚æå‡ºäº†ä¸€ä¸ªæ–°å‹é¢„è®­ç»ƒæ–¹æ³•ï¼Œå³é€šè¿‡å¤§è§„æ¨¡åˆæˆäº¤é”™æ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æ— éœ€å®é™…è¯­éŸ³æ•°æ®çš„ç”Ÿæˆè¿‡ç¨‹ä¹Ÿèƒ½é«˜æ•ˆæ„å»ºæ¨¡å‹ã€‚</li>
<li>æå‡ºä¸€ç§æœ‰æ•ˆçš„æ•°æ®æ„å»ºæ–¹å¼ï¼Œé€šè¿‡ä»ç°æœ‰æ–‡æœ¬è¯­æ–™åº“ä¸­é‡‡æ ·æ–‡æœ¬ç‰‡æ®µå¹¶ä½¿ç”¨æ–‡æœ¬åˆ°æ ‡è®°æ¨¡å‹åˆæˆç›¸åº”çš„è¯­éŸ³ç‰‡æ®µæ¥åˆæˆè¯­éŸ³æ•°æ®ã€‚è¿™ç§æ–¹æ³•é¿å…äº†ç”Ÿæˆå®é™…è¯­éŸ³æ•°æ®çš„å¤æ‚æ€§ã€‚</li>
<li>åˆ©ç”¨æ¥è‡ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„çŸ¢é‡é‡åŒ–ç“¶é¢ˆä¿¡æ¯åˆ›å»ºæœ‰ç›‘ç£çš„è¯­éŸ³åˆ†è¯å™¨ã€‚æ­¤è®­ç»ƒæ–¹æ³•å¯ä»¥å®ç°è¾ƒä½é¢‘ç‡ï¼ˆå¦‚æ¯ç§’ä»…äº§ç”Ÿä¸€æ¬¡ï¼‰çš„ç¦»æ•£è¯­éŸ³æ ‡è®°ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¿ç•™å’Œè¯­éŸ³é‡å»ºè´¨é‡ã€‚</li>
<li>é€šè¿‡æ‰©å±•åˆ°å¤§é‡é¢„è®­ç»ƒæ•°æ®ï¼ˆå¦‚é«˜è¾¾ä¸€åƒäº¿ä¸ªæ ‡è®°ï¼‰ï¼Œè¿™ç§æ–°å‹é¢„è®­ç»ƒæ–¹æ³•æé«˜äº†è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œå£è¯­é—®ç­”çš„æ€§èƒ½è¡¨ç°ï¼Œè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä¸è¯­éŸ³å¯¹è¯æ•°æ®ï¼ŒæˆåŠŸå¼€å‘å‡ºå…·æœ‰ç«äº‰åŠ›çš„ç«¯åˆ°ç«¯å£è¯­èŠå¤©æœºå™¨äººï¼Œå…¶å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡å‡è¡¨ç°è‰¯å¥½ã€‚è¯¥æœºå™¨äººåœ¨å£è¯­ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-083a15304c8ff1744a4fb8ea3491090d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-883a8bdcbf286c4742aac394b97420a3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6d53c63a0e1389b63576111fd6941662.jpg" align="middle">
</details>




<h2 id="X-CrossNet-A-complex-spectral-mapping-approach-to-target-speaker-extraction-with-cross-attention-speaker-embedding-fusion"><a href="#X-CrossNet-A-complex-spectral-mapping-approach-to-target-speaker-extraction-with-cross-attention-speaker-embedding-fusion" class="headerlink" title="X-CrossNet: A complex spectral mapping approach to target speaker   extraction with cross attention speaker embedding fusion"></a>X-CrossNet: A complex spectral mapping approach to target speaker   extraction with cross attention speaker embedding fusion</h2><p><strong>Authors:Chang Sun, Bo Qin</strong></p>
<p>Target speaker extraction (TSE) is a technique for isolating a target speakerâ€™s voice from mixed speech using auxiliary features associated with the target speaker. It is another attempt at addressing the cocktail party problem and is generally considered to have more practical application prospects than traditional speech separation methods. Although academic research in this area has achieved high performance and evaluation scores on public datasets, most models exhibit significantly reduced performance in real-world noisy or reverberant conditions. To address this limitation, we propose a novel TSE model, X-CrossNet, which leverages CrossNet as its backbone. CrossNet is a speech separation network specifically optimized for challenging noisy and reverberant environments, achieving state-of-the-art performance in tasks such as speaker separation under these conditions. Additionally, to enhance the networkâ€™s ability to capture and utilize auxiliary features of the target speaker, we integrate a Cross-Attention mechanism into the global multi-head self-attention (GMHSA) module within each CrossNet block. This facilitates more effective integration of target speaker features with mixed speech features. Experimental results show that our method performs superior separation on the WSJ0-2mix and WHAMR! datasets, demonstrating strong robustness and stability. </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰æ˜¯ä¸€ç§åˆ©ç”¨ä¸ç›®æ ‡è¯´è¯äººç›¸å…³çš„è¾…åŠ©ç‰¹å¾ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººå£°éŸ³çš„æŠ€æœ¯ã€‚å®ƒæ˜¯è§£å†³é¸¡å°¾é…’ä¼šé—®é¢˜çš„å¦ä¸€ç§å°è¯•ï¼Œå¹¶ä¸”é€šå¸¸è¢«è®¤ä¸ºæ¯”ä¼ ç»Ÿçš„è¯­éŸ³åˆ†ç¦»æ–¹æ³•å…·æœ‰æ›´å®é™…çš„åº”ç”¨å‰æ™¯ã€‚å°½ç®¡è¯¥é¢†åŸŸçš„å­¦æœ¯ç ”ç©¶åœ¨å…¬å…±æ•°æ®é›†ä¸Šå–å¾—äº†é«˜æ€§èƒ½å’Œè¯„ä¼°åˆ†æ•°ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­å™ªå£°å¤§æˆ–æ··å“æ¡ä»¶ä¸‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„TSEæ¨¡å‹X-CrossNetï¼Œå®ƒä»¥CrossNetä½œä¸ºéª¨å¹²ç½‘ã€‚CrossNetæ˜¯ä¸€ç§é’ˆå¯¹å™ªå£°å¤§å’Œæ··å“ç­‰æŒ‘æˆ˜ç¯å¢ƒçš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œï¼Œåœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼Œå®ƒåœ¨è¯´è¯äººåˆ†ç¦»ç­‰ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜ç½‘ç»œæ•è·å’Œåˆ©ç”¨ç›®æ ‡è¯´è¯äººçš„è¾…åŠ©ç‰¹å¾çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å°†äº¤å‰æ³¨æ„æœºåˆ¶é›†æˆåˆ°æ¯ä¸ªCrossNetå—å†…çš„å…¨å±€å¤šå¤´è‡ªæ³¨æ„ï¼ˆGMHSAï¼‰æ¨¡å—ä¸­ã€‚è¿™æœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°å°†ç›®æ ‡è¯´è¯äººçš„ç‰¹å¾ä¸æ··åˆè¯­éŸ³ç‰¹å¾ç»“åˆèµ·æ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨WSJ0-2mixå’ŒWHAMRï¼æ•°æ®é›†ä¸Šå®ç°äº†å‡ºè‰²çš„åˆ†ç¦»æ•ˆæœï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„é²æ£’æ€§å’Œç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13811v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰æŠ€æœ¯é€šè¿‡åˆ©ç”¨ç›®æ ‡è¯´è¯äººçš„è¾…åŠ©ç‰¹å¾æ¥ä»æ··åˆè¯­éŸ³ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯äººçš„å£°éŸ³ã€‚å°½ç®¡åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å­¦æœ¯ç ”ç©¶å–å¾—äº†é«˜æ€§èƒ½å’Œè¯„ä»·åˆ†æ•°ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„å˜ˆæ‚æˆ–æ··å“ç¯å¢ƒä¸­ï¼Œå¤§å¤šæ•°æ¨¡å‹çš„æ€§èƒ½ä¼šæ˜¾è‘—é™ä½ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹TSEæ¨¡å‹X-CrossNetï¼Œä»¥CrossNetä½œä¸ºéª¨å¹²ã€‚CrossNetæ˜¯é’ˆå¯¹å˜ˆæ‚å’Œæ··å“ç¯å¢ƒä¼˜åŒ–çš„è¯­éŸ³åˆ†ç¦»ç½‘ç»œï¼Œåœ¨è¿™äº›æ¡ä»¶ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºæé«˜ç½‘ç»œæ•æ‰å’Œåˆ©ç”¨ç›®æ ‡è¯´è¯äººçš„è¾…åŠ©ç‰¹å¾çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªCrossNetå—çš„å…¨çƒå¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆGMHSAï¼‰æ¨¡å—ä¸­é›†æˆäº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™æœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°æ•´åˆç›®æ ‡è¯´è¯äººçš„ç‰¹å¾ä¸æ··åˆè¯­éŸ³ç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨WSJ0-2mixå’ŒWHAMRï¼æ•°æ®é›†ä¸Šå®ç°äº†å‡ºè‰²çš„åˆ†ç¦»æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç¨³å¥æ€§å’Œç¨³å®šæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰æ—¨åœ¨ä»æ··åˆè¯­éŸ³ä¸­éš”ç¦»ç›®æ ‡è¯´è¯äººçš„å£°éŸ³ï¼Œä½¿ç”¨ä¸ç›®æ ‡è¯´è¯äººç›¸å…³çš„è¾…åŠ©ç‰¹å¾ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å˜ˆæ‚æˆ–æ··å“çš„ç°å®ä¸­ç¯å¢ƒä¸­æ€§èƒ½é™ä½ã€‚</li>
<li>æå‡ºçš„X-CrossNetæ¨¡å‹ä»¥CrossNetä¸ºéª¨å¹²ï¼Œé’ˆå¯¹æŒ‘æˆ˜æ€§çš„å˜ˆæ‚å’Œæ··å“ç¯å¢ƒè¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>åœ¨æ¯ä¸ªCrossNetå—çš„GMHSAæ¨¡å—ä¸­é›†æˆäº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•´åˆç›®æ ‡è¯´è¯äººçš„ç‰¹å¾ä¸æ··åˆè¯­éŸ³ç‰¹å¾ã€‚</li>
<li>X-CrossNetåœ¨WSJ0-2mixå’ŒWHAMRï¼æ•°æ®é›†ä¸Šå®ç°äº†å‡ºè‰²çš„åˆ†ç¦»æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§å’Œç¨³å®šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-965cc3523e6663bc205df591cfc75ba3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9ade30df9d05146f5999826e7ebf76d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-fc6fb05a44af3d0f9f9262b277d92ed8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2786a7819c3af3ab541f5d04da296319.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b17c122dab3ad34257948b7c60b66ef3.jpg" align="middle">
</details>




<h2 id="FabuLight-ASD-Unveiling-Speech-Activity-via-Body-Language"><a href="#FabuLight-ASD-Unveiling-Speech-Activity-via-Body-Language" class="headerlink" title="FabuLight-ASD: Unveiling Speech Activity via Body Language"></a>FabuLight-ASD: Unveiling Speech Activity via Body Language</h2><p><strong>Authors:Hugo Carneiro, Stefan Wermter</strong></p>
<p>Active speaker detection (ASD) in multimodal environments is crucial for various applications, from video conferencing to human-robot interaction. This paper introduces FabuLight-ASD, an advanced ASD model that integrates facial, audio, and body pose information to enhance detection accuracy and robustness. Our model builds upon the existing Light-ASD framework by incorporating human pose data, represented through skeleton graphs, which minimises computational overhead. Using the Wilder Active Speaker Detection (WASD) dataset, renowned for reliable face and body bounding box annotations, we demonstrate FabuLight-ASDâ€™s effectiveness in real-world scenarios. Achieving an overall mean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD, which has an overall mAP of 93.7% across various challenging scenarios. The incorporation of body pose information shows a particularly advantageous impact, with notable improvements in mAP observed in scenarios with speech impairment, face occlusion, and human voice background noise. Furthermore, efficiency analysis indicates only a modest increase in parameter count (27.3%) and multiply-accumulate operations (up to 2.4%), underscoring the modelâ€™s efficiency and feasibility. These findings validate the efficacy of FabuLight-ASD in enhancing ASD performance through the integration of body pose data. FabuLight-ASDâ€™s code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/knowledgetechnologyuhh/FabuLight-ASD">https://github.com/knowledgetechnologyuhh/FabuLight-ASD</a>. </p>
<blockquote>
<p>åœ¨å¤šåª’ä½“ç¯å¢ƒä¸­ï¼Œä¸»åŠ¨è¯´è¯è€…æ£€æµ‹ï¼ˆASDï¼‰å¯¹äºä»è§†é¢‘ä¼šè®®åˆ°äººæœºäº¤äº’çš„å„ç§åº”ç”¨éƒ½è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†FabuLight-ASDï¼Œè¿™æ˜¯ä¸€ç§å…ˆè¿›çš„ASDæ¨¡å‹ï¼Œå®ƒé›†æˆäº†é¢éƒ¨ã€éŸ³é¢‘å’Œèº«ä½“å§¿åŠ¿ä¿¡æ¯ï¼Œä»¥æé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŸºäºç°æœ‰çš„Light-ASDæ¡†æ¶ï¼Œé€šè¿‡èå…¥é€šè¿‡éª¨éª¼å›¾è¡¨ç¤ºçš„äººä½“å§¿åŠ¿æ•°æ®ï¼Œä»¥æœ€å°åŒ–è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥å¯é çš„è„¸éƒ¨å’Œèº«ä½“è¾¹ç•Œæ¡†æ³¨é‡Šè€Œé—»åçš„Wilder Active Speaker Detectionï¼ˆWASDï¼‰æ•°æ®é›†ï¼Œå±•ç¤ºäº†FabuLight-ASDåœ¨çœŸå®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚FabuLight-ASDçš„æ€»ä½“å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰è¾¾åˆ°94.3%ï¼Œä¼˜äºLight-ASDçš„93.7%ï¼Œåœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°æ›´ä½³ã€‚èå…¥èº«ä½“å§¿åŠ¿ä¿¡æ¯äº§ç”Ÿäº†ç‰¹åˆ«æœ‰åˆ©çš„å½±å“ï¼Œåœ¨è¨€è¯­éšœç¢ã€é¢éƒ¨é®æŒ¡å’ŒèƒŒæ™¯äººå£°å™ªéŸ³ç­‰åœºæ™¯ä¸­ï¼ŒmAPçš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ã€‚æ­¤å¤–ï¼Œæ•ˆç‡åˆ†ææ˜¾ç¤ºï¼Œå‚æ•°è®¡æ•°ä»…å¢åŠ äº†27.3%ï¼Œä¹˜æ³•ç´¯åŠ è¿ç®—å¢åŠ äº†é«˜è¾¾2.4%ï¼Œè¿™çªæ˜¾äº†æ¨¡å‹çš„æ•ˆç‡å’Œå¯è¡Œæ€§ã€‚è¿™äº›å‘ç°éªŒè¯äº†FabuLight-ASDé€šè¿‡æ•´åˆèº«ä½“å§¿åŠ¿æ•°æ®æé«˜ASDæ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚FabuLight-ASDçš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/knowledgetechnologyuhh/FabuLight-ASD">https://github.com/knowledgetechnologyuhh/FabuLight-ASD</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13674v2">PDF</a> 23 pages, 8 figures, 3 tables, accepted for publication in Neural   Computing and Applications</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºäººè„¸ã€éŸ³é¢‘å’Œèº«ä½“å§¿æ€çš„å¤šæ¨¡æ€ä¸»åŠ¨è¯´è¯äººæ£€æµ‹ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>ä¸»åŠ¨è¯´è¯è€…æ£€æµ‹ï¼ˆASDï¼‰åœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­å¯¹äºè§†é¢‘ä¼šè®®ã€äººæœºäº¤äº’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>FabuLight-ASDæ˜¯ä¸€ä¸ªå…ˆè¿›çš„ASDæ¨¡å‹ï¼Œé›†æˆäº†é¢éƒ¨ã€éŸ³é¢‘å’Œèº«ä½“å§¿æ€ä¿¡æ¯ï¼Œæé«˜äº†æ£€æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹åŸºäºLight-ASDæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥äººä½“å§¿æ€æ•°æ®ï¼ˆé€šè¿‡éª¨éª¼å›¾è¡¨ç¤ºï¼‰æ¥ä¼˜åŒ–è®¡ç®—å¼€é”€ã€‚</li>
<li>ä½¿ç”¨è‘—åçš„Wilder Active Speaker Detectionï¼ˆWASDï¼‰æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œè¯¥æ•°æ®é›†å…·æœ‰å¯é çš„äººè„¸å’Œäººä½“è¾¹ç•Œæ¡†æ³¨é‡Šã€‚</li>
<li>FabuLight-ASDåœ¨å¤šç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å®ç°äº†å¹³å‡ç²¾åº¦ï¼ˆmAPï¼‰ä¸º94.3%ï¼Œä¼˜äºLight-ASDçš„93.7%ã€‚</li>
<li>å¼•å…¥èº«ä½“å§¿æ€ä¿¡æ¯åœ¨å¤šç§åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†mAPï¼Œå¦‚åœ¨æœ‰è¯­éŸ³éšœç¢ã€é¢éƒ¨é®æŒ¡å’ŒèƒŒæ™¯å™ªéŸ³çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0a148d4bb7393a27530cba9f6b495c97.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c2016159286e4a27b8c76c8ffca71529.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c1c7744841e9d792cd6c4d171ff79d9f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4311c8224609813d9e9fa781306a01c2.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79d8c530a223fa85974e2a7b72c453cc.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Swap Path Network for Robust Person Search Pre-training
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-aa0498f2de39d156ab924e917d63e25f.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Dynamic EventNeRF Reconstructing General Dynamic Scenes from Multi-view   Event Cameras
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">7753.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
