<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  MAVias Mitigate any Visual Bias">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1f0f0f4cf7d419c676d174ac15e31f5c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    28.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    116 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-10-æ›´æ–°"><a href="#2024-12-10-æ›´æ–°" class="headerlink" title="2024-12-10 æ›´æ–°"></a>2024-12-10 æ›´æ–°</h1><h2 id="MAVias-Mitigate-any-Visual-Bias"><a href="#MAVias-Mitigate-any-Visual-Bias" class="headerlink" title="MAVias: Mitigate any Visual Bias"></a>MAVias: Mitigate any Visual Bias</h2><p><strong>Authors:Ioannis Sarridis, Christos Koutlis, Symeon Papadopoulos, Christos Diou</strong></p>
<p>Mitigating biases in computer vision models is an essential step towards the trustworthiness of artificial intelligence models. Existing bias mitigation methods focus on a small set of predefined biases, limiting their applicability in visual datasets where multiple, possibly unknown biases exist. To address this limitation, we introduce MAVias, an open-set bias mitigation approach leveraging foundation models to discover spurious associations between visual attributes and target classes. MAVias first captures a wide variety of visual features in natural language via a foundation image tagging model, and then leverages a large language model to select those visual features defining the target class, resulting in a set of language-coded potential visual biases. We then translate this set of potential biases into vision-language embeddings and introduce an in-processing bias mitigation approach to prevent the model from encoding information related to them. Our experiments on diverse datasets, including CelebA, Waterbirds, ImageNet, and UrbanCars, show that MAVias effectively detects and mitigates a wide range of biases in visual recognition tasks outperforming current state-of-the-art. </p>
<blockquote>
<p>å‡è½»è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­çš„åè§æ˜¯å»ºç«‹äººå·¥æ™ºèƒ½æ¨¡å‹å¯ä¿¡åº¦çš„é‡è¦æ­¥éª¤ã€‚ç°æœ‰çš„åè§ç¼“è§£æ–¹æ³•ä¸»è¦å…³æ³¨ä¸€ç»„é¢„å®šä¹‰çš„åè§ï¼Œè¿™åœ¨å­˜åœ¨å¤šä¸ªå¯èƒ½æœªçŸ¥çš„åè§çš„è§†è§‰æ•°æ®é›†ä¸­é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº† MAViasï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹å‘ç°è§†è§‰å±æ€§ä¸ç›®æ ‡ç±»åˆ«ä¹‹é—´å¶ç„¶å…³è”çš„å¼€é›†åè§ç¼“è§£æ–¹æ³•ã€‚MAVias é¦–å…ˆé€šè¿‡åŸºç¡€å›¾åƒæ ‡è®°æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ä¸­æ•è·å„ç§è§†è§‰ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©å®šä¹‰ç›®æ ‡ç±»åˆ«çš„è§†è§‰ç‰¹å¾ï¼Œå½¢æˆä¸€ç»„è¯­è¨€ç¼–ç çš„æ½œåœ¨è§†è§‰åè§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™ç»„æ½œåœ¨çš„åè§è½¬åŒ–ä¸ºè§†è§‰è¯­è¨€åµŒå…¥ï¼Œå¹¶å¼•å…¥ä¸€ç§å¤„ç†è¿‡ç¨‹ä¸­çš„åè§ç¼“è§£æ–¹æ³•ï¼Œä»¥é˜²æ­¢æ¨¡å‹å¯¹ä¸åè§ç›¸å…³çš„ä¿¡æ¯ç¼–ç ã€‚æˆ‘ä»¬åœ¨åŒ…æ‹¬CelebAã€Waterbirdsã€ImageNetå’ŒUrbanCarsç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMAViasæœ‰æ•ˆåœ°æ£€æµ‹å’Œç¼“è§£äº†è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­çš„å¹¿æ³›åè§ï¼Œæ€§èƒ½ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06632v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­åè§é—®é¢˜çš„æ–°æ–¹æ³•â€”â€”MAViasã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸºç¡€æ¨¡å‹å‘ç°å¹¶æ¶ˆé™¤è§†è§‰å±æ€§ä¸ç›®æ ‡ç±»åˆ«ä¹‹é—´çš„æ½œåœ¨å…³è”ï¼Œä»è€Œå‡è½»åè§é—®é¢˜ã€‚é€šè¿‡å›¾åƒæ ‡æ³¨æ¨¡å‹æ•è·ä¸°å¯Œçš„è§†è§‰ç‰¹å¾ï¼Œå¹¶ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ç­›é€‰ä¸ç›®æ ‡ç±»åˆ«ç›¸å…³çš„ç‰¹å¾ï¼Œè¿›è€Œå½¢æˆä¸€ç³»åˆ—çš„è¯­è¨€ç¼–ç æ½œåœ¨åè§ã€‚éšåå°†è¿™äº›æ½œåœ¨åè§è½¬åŒ–ä¸ºè§†è§‰è¯­è¨€åµŒå…¥ï¼Œå¹¶æå‡ºä¸€ç§è¿‡ç¨‹æ€§åè§ç¼“è§£æ–¹æ³•æ¥é˜»æ­¢æ¨¡å‹å¯¹å…¶è¿›è¡Œç¼–ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šï¼ŒMAViasèƒ½å¤Ÿæœ‰æ•ˆæ£€æµ‹å¹¶ç¼“è§£è§†è§‰è¯†åˆ«ä»»åŠ¡çš„åè§é—®é¢˜ï¼Œå¹¶ä¼˜äºå½“å‰çš„ä¸»æµæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAViasæ˜¯ä¸€ç§ç”¨äºç¼“è§£è®¡ç®—æœºè§†è§‰æ¨¡å‹ä¸­åè§é—®é¢˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>MAViasåˆ©ç”¨åŸºç¡€æ¨¡å‹å‘ç°æ½œåœ¨åè§ï¼Œé€‚ç”¨äºå­˜åœ¨å¤šç§æœªçŸ¥åè§çš„æƒ…å†µã€‚</li>
<li>é€šè¿‡å›¾åƒæ ‡æ³¨æ¨¡å‹ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯†åˆ«è§†è§‰å±æ€§ä¸ç›®æ ‡ç±»åˆ«ä¹‹é—´çš„æ½œåœ¨å…³è”ã€‚</li>
<li>MAViaså°†æ½œåœ¨åè§è½¬åŒ–ä¸ºè§†è§‰è¯­è¨€åµŒå…¥ï¼Œä¾¿äºåç»­å¤„ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¿‡ç¨‹æ€§åè§ç¼“è§£æ–¹æ³•ï¼Œé˜²æ­¢æ¨¡å‹å¯¹æ½œåœ¨åè§è¿›è¡Œç¼–ç ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMAViasåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæœ‰æ•ˆæ£€æµ‹å¹¶ç¼“è§£åè§é—®é¢˜ï¼Œè¡¨ç°ä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea95fec25a4ad7a5aeb222adaf8a5ba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5fe53c2d07dcb80e845ab3f649f3b9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5f5c3269765ecd0ef8e554652a67fb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-102bdff6b88aaed5962a543f196838d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac371b250ba779b889fc419588d3dd17.jpg" align="middle">
</details>




<h2 id="Sound2Vision-Generating-Diverse-Visuals-from-Audio-through-Cross-Modal-Latent-Alignment"><a href="#Sound2Vision-Generating-Diverse-Visuals-from-Audio-through-Cross-Modal-Latent-Alignment" class="headerlink" title="Sound2Vision: Generating Diverse Visuals from Audio through Cross-Modal   Latent Alignment"></a>Sound2Vision: Generating Diverse Visuals from Audio through Cross-Modal   Latent Alignment</h2><p><strong>Authors:Kim Sung-Bin, Arda Senocak, Hyunwoo Ha, Tae-Hyun Oh</strong></p>
<p>How does audio describe the world around us? In this work, we propose a method for generating images of visual scenes from diverse in-the-wild sounds. This cross-modal generation task is challenging due to the significant information gap between auditory and visual signals. We address this challenge by designing a model that aligns audio-visual modalities by enriching audio features with visual information and translating them into the visual latent space. These features are then fed into the pre-trained image generator to produce images. To enhance image quality, we use sound source localization to select audio-visual pairs with strong cross-modal correlations. Our method achieves substantially better results on the VEGAS and VGGSound datasets compared to previous work and demonstrates control over the generation process through simple manipulations to the input waveform or latent space. Furthermore, we analyze the geometric properties of the learned embedding space and demonstrate that our learning approach effectively aligns audio-visual signals for cross-modal generation. Based on this analysis, we show that our method is agnostic to specific design choices, showing its generalizability by integrating various model architectures and different types of audio-visual data. </p>
<blockquote>
<p>éŸ³é¢‘æ˜¯å¦‚ä½•æè¿°æˆ‘ä»¬å‘¨å›´çš„ä¸–ç•Œçš„ï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å¤šæ ·åŒ–çš„é‡å¤–å£°éŸ³ç”Ÿæˆè§†è§‰åœºæ™¯å›¾åƒçš„æ–¹æ³•ã€‚ç”±äºå¬è§‰å’Œè§†è§‰ä¿¡å·ä¹‹é—´å­˜åœ¨å·¨å¤§çš„ä¿¡æ¯å·®è·ï¼Œå› æ­¤è¿™ç§è·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ä¸ªå°†éŸ³é¢‘è§†è§‰æ¨¡æ€å¯¹é½çš„æ¨¡å‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸°å¯ŒéŸ³é¢‘ç‰¹å¾å¹¶æ·»åŠ è§†è§‰ä¿¡æ¯ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºè§†è§‰æ½œåœ¨ç©ºé—´ã€‚è¿™äº›ç‰¹å¾éšåè¢«è¾“å…¥åˆ°é¢„è®­ç»ƒå›¾åƒç”Ÿæˆå™¨ä¸­ä»¥ç”Ÿæˆå›¾åƒã€‚ä¸ºäº†æé«˜å›¾åƒè´¨é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨å£°æºå®šä½æ¥é€‰æ‹©å…·æœ‰å¼ºè·¨æ¨¡æ€å…³è”çš„è§†å¬é…å¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨VEGASå’ŒVGGSoundæ•°æ®é›†ä¸Šå–å¾—äº†æ¯”ä»¥å‰çš„å·¥ä½œæ›´å¥½çš„ç»“æœï¼Œé€šè¿‡ç®€å•æ“ä½œè¾“å…¥æ³¢å½¢æˆ–æ½œåœ¨ç©ºé—´å®ç°å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†å­¦ä¹ åµŒå…¥ç©ºé—´çš„å‡ ä½•å±æ€§ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„å­¦ä¹ æœ‰æ•ˆåœ°å¯¹é½äº†éŸ³é¢‘è§†è§‰ä¿¡å·ä»¥å®ç°è·¨æ¨¡æ€ç”Ÿæˆã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•å¯¹ç‰¹å®šè®¾è®¡é€‰æ‹©çš„æ— å…³æ€§ï¼Œé€šè¿‡é›†æˆå„ç§æ¨¡å‹æ¶æ„å’Œä¸åŒç±»å‹çš„è§†å¬æ•°æ®æ¥è¯æ˜å…¶æ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06209v1">PDF</a> Under-review</p>
<p><strong>Summary</strong><br>éŸ³é¢‘å¦‚ä½•æè¿°æˆ‘ä»¬å‘¨å›´çš„ä¸–ç•Œï¼Ÿåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å„ç§è‡ªç„¶å£°éŸ³ç”Ÿæˆè§†è§‰åœºæ™¯å›¾åƒçš„æ–¹æ³•ã€‚ç”±äºå¬è§‰å’Œè§†è§‰ä¿¡å·ä¹‹é—´å­˜åœ¨å·¨å¤§çš„ä¿¡æ¯å·®è·ï¼Œè¿™ä¸€è·¨æ¨¡æ€ç”Ÿæˆä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ç§æ¨¡å‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ¨¡å‹é€šè¿‡ä¸°å¯ŒéŸ³é¢‘ç‰¹å¾å¹¶å¯¹å…¶è¿›è¡Œè§†è§‰ä¿¡æ¯å¤„ç†ï¼Œå°†å…¶ç¿»è¯‘åˆ°è§†è§‰æ½œåœ¨ç©ºé—´æ¥å¯¹é½éŸ³é¢‘-è§†è§‰æ¨¡æ€ã€‚ç„¶åï¼Œè¿™äº›ç‰¹æ€§è¢«è¾“å…¥åˆ°é¢„è®­ç»ƒå›¾åƒç”Ÿæˆå™¨ä¸­ä»¥äº§ç”Ÿå›¾åƒã€‚ä¸ºæé«˜å›¾åƒè´¨é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨å£°æºå®šä½æ¥é€‰æ‹©å…·æœ‰å¼ºè·¨æ¨¡æ€å…³è”çš„éŸ³é¢‘-è§†è§‰å¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨VEGASå’ŒVGGSoundæ•°æ®é›†ä¸Šå–å¾—äº†æ¯”ä»¥å¾€å·¥ä½œæ›´å¥½çš„ç»“æœï¼Œå¹¶é€šè¿‡ç®€å•æ“ä½œè¾“å…¥æ³¢å½¢æˆ–æ½œåœ¨ç©ºé—´æ¥è¯æ˜å¯¹ç”Ÿæˆè¿‡ç¨‹çš„æ§åˆ¶åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ†æäº†å­¦ä¹ åµŒå…¥ç©ºé—´çš„å‡ ä½•å±æ€§ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„å­¦ä¹ æ–¹æ³•æœ‰æ•ˆåœ°å¯¹é½éŸ³é¢‘-è§†è§‰ä¿¡å·è¿›è¡Œè·¨æ¨¡æ€ç”Ÿæˆã€‚åŸºäºæ­¤åˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•å¯¹ç‰¹å®šè®¾è®¡é€‰æ‹©çš„æ— è§†æ€§ï¼Œå¹¶é€šè¿‡é›†æˆå„ç§æ¨¡å‹æ¶æ„å’Œä¸åŒç±»å‹çš„éŸ³é¢‘-è§†è§‰æ•°æ®æ¥å±•ç¤ºå…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»è‡ªç„¶å£°éŸ³ç”Ÿæˆè§†è§‰åœºæ™¯å›¾åƒçš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡è®¾è®¡ä¸€ç§æ¨¡å‹æ¥è§£å†³è·¨æ¨¡æ€ç”ŸæˆæŒ‘æˆ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä¸°å¯ŒéŸ³é¢‘ç‰¹å¾å¹¶å¯¹å…¶è¿›è¡Œè§†è§‰ä¿¡æ¯å¤„ç†ã€‚</li>
<li>é€šè¿‡å£°æºå®šä½æŠ€æœ¯æé«˜å›¾åƒè´¨é‡ï¼Œé€‰æ‹©å…·æœ‰å¼ºè·¨æ¨¡æ€å…³è”çš„éŸ³é¢‘-è§†è§‰å¯¹ã€‚</li>
<li>åœ¨VEGASå’ŒVGGSoundæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ç®€å•çš„æ“ä½œæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œè¯æ˜å…¶å¯¹è¾“å…¥æ³¢å½¢æˆ–æ½œåœ¨ç©ºé—´çš„æ§åˆ¶åŠ›ã€‚</li>
<li>åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯¹é½éŸ³é¢‘-è§†è§‰ä¿¡å·çš„èƒ½åŠ›è¾ƒå¼ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-809559ef0950dbe21a0df1e424382506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c337aa13a287c30430eb0a5b4fd45f01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d824525f831f1a038809fe3ea3780a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576b69c7554e4c674ecc1f33d6069c91.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf38fdbd07bafe507fbd90cbf89b87cf.jpg" align="middle">
</details>




<h2 id="Annotations-for-Exploring-Food-Tweets-From-Multiple-Aspects"><a href="#Annotations-for-Exploring-Food-Tweets-From-Multiple-Aspects" class="headerlink" title="Annotations for Exploring Food Tweets From Multiple Aspects"></a>Annotations for Exploring Food Tweets From Multiple Aspects</h2><p><strong>Authors:MatÄ«ss Rikters, Edison Marrese-Taylor, Rinalds VÄ«ksna</strong></p>
<p>This research builds upon the Latvian Twitter Eater Corpus (LTEC), which is focused on the narrow domain of tweets related to food, drinks, eating and drinking. LTEC has been collected for more than 12 years and reaching almost 3 million tweets with the basic information as well as extended automatically and manually annotated metadata. In this paper we supplement the LTEC with manually annotated subsets of evaluation data for machine translation, named entity recognition, timeline-balanced sentiment analysis, and text-image relation classification. We experiment with each of the data sets using baseline models and highlight future challenges for various modelling approaches. </p>
<blockquote>
<p>è¿™ç¯‡ç ”ç©¶åŸºäºæ‹‰è„±ç»´äºšæ¨ç‰¹é¥®é£Ÿè¯­æ–™åº“ï¼ˆLTECï¼‰ï¼Œè¯¥è¯­æ–™åº“ä¸“æ³¨äºä¸é£Ÿç‰©ã€é¥®æ–™ã€é¥®é£Ÿå’Œé¥®æ°´ç›¸å…³çš„ç‹­çª„é¢†åŸŸçš„æ¨æ–‡ã€‚LTECå·²ç»æ”¶é›†äº†è¶…è¿‡12å¹´çš„æ—¶é—´ï¼Œå‡ ä¹åŒ…å«äº†300ä¸‡æ¡æ¨æ–‡çš„åŸºæœ¬ä¿¡æ¯ï¼Œä»¥åŠæ‰©å±•çš„è‡ªåŠ¨å’Œæ‰‹åŠ¨æ³¨é‡Šçš„å…ƒæ•°æ®ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»¥æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€æ—¶é—´å¹³è¡¡æƒ…æ„Ÿåˆ†æå’Œæ–‡æœ¬å›¾åƒå…³ç³»åˆ†ç±»çš„æ‰‹åŠ¨è¯„ä¼°æ•°æ®å­é›†æ¥è¡¥å……LTECã€‚æˆ‘ä»¬åˆ©ç”¨åŸºå‡†æ¨¡å‹å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œå®éªŒï¼Œå¹¶çªå‡ºå„ç§å»ºæ¨¡æ–¹æ³•æœªæ¥çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06179v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡åŸºäºæ‹‰è„±ç»´äºšTwitteré£Ÿè¯„è¯­æ–™åº“ï¼ˆLTECï¼‰ï¼Œå¯¹æ¶‰åŠé£Ÿå“å’Œé¥®æ–™ç›¸å…³çš„æ¨ç‰¹å†…å®¹è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚LTECå·²æ”¶é›†è¶…è¿‡12å¹´ï¼ŒåŒ…å«è¿‘3ç™¾ä¸‡æ¡æ¨ç‰¹ï¼Œæä¾›åŸºæœ¬ä¿¡æ¯å’Œæ‰©å±•çš„æ‰‹åŠ¨åŠè‡ªåŠ¨æ ‡æ³¨å…ƒæ•°æ®ã€‚æœ¬æ–‡è¡¥å……äº†LTECçš„æ‰‹åŠ¨æ ‡æ³¨å­é›†ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€æ—¶é—´å¹³è¡¡æƒ…æ„Ÿåˆ†æå’Œæ–‡æœ¬å›¾åƒå…³ç³»åˆ†ç±»çš„è¯„ä¼°æ•°æ®ã€‚é€šè¿‡åŸºçº¿æ¨¡å‹å®éªŒï¼Œå±•æœ›äº†æœªæ¥å„ç§å»ºæ¨¡æ–¹æ³•çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶åŸºäºæ‹‰è„±ç»´äºšTwitteré£Ÿè¯„è¯­æ–™åº“ï¼ˆLTECï¼‰ï¼Œä¸“æ³¨äºé£Ÿå“ã€é¥®æ–™ç›¸å…³çš„æ¨ç‰¹å†…å®¹ã€‚</li>
<li>LTECå·²æ”¶é›†è¶…è¿‡12å¹´ï¼ŒåŒ…å«è¿‘3ç™¾ä¸‡æ¡æ¨ç‰¹ï¼Œæä¾›åŸºæœ¬å’Œæ‰©å±•çš„æ ‡æ³¨å…ƒæ•°æ®ã€‚</li>
<li>ç ”ç©¶è¡¥å……äº†LTECçš„æ‰‹åŠ¨æ ‡æ³¨å­é›†ç”¨äºæœºå™¨ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ã€æ—¶é—´å¹³è¡¡æƒ…æ„Ÿåˆ†æå’Œæ–‡æœ¬å›¾åƒå…³ç³»åˆ†ç±»çš„è¯„ä¼°æ•°æ®ã€‚</li>
<li>é€šè¿‡åŸºçº¿æ¨¡å‹å®éªŒï¼ŒéªŒè¯äº†æ•°æ®é›†çš„å®ç”¨æ€§ã€‚</li>
<li>ç ”ç©¶å±•æœ›äº†æœªæ¥å„ç§å»ºæ¨¡æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>LTECæ•°æ®é›†çš„é•¿æœŸç§¯ç´¯ä¸ºé£Ÿå“ã€é¥®æ–™é¢†åŸŸçš„æ¨ç‰¹åˆ†ææä¾›äº†ä¸°å¯Œèµ„æºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d9eb1cd984c805125a36aeaf24bf67f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-917c12398661c40d7aa819f3931cb453.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f95ac118943441d37d2407e80b186176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20db27866049b81ad1afa4dbb3dd19f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1cb515aa571f0ae753ecb336960b2b1.jpg" align="middle">
</details>




<h2 id="Language-Guided-Image-Tokenization-for-Generation"><a href="#Language-Guided-Image-Tokenization-for-Generation" class="headerlink" title="Language-Guided Image Tokenization for Generation"></a>Language-Guided Image Tokenization for Generation</h2><p><strong>Authors:Kaiwen Zha, Lijun Yu, Alireza Fathi, David A. Ross, Cordelia Schmid, Dina Katabi, Xiuye Gu</strong></p>
<p>Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focus on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTokâ€™s superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization. </p>
<blockquote>
<p>å›¾åƒæ ‡è®°åŒ–æ˜¯å°†åŸå§‹å›¾åƒåƒç´ è½¬æ¢ä¸ºç´§å‡‘çš„ä½ç»´æ½œåœ¨è¡¨ç¤ºçš„è¿‡ç¨‹ï¼Œå·²è¢«è¯æ˜å¯¹äºå¯æ‰©å±•å’Œé«˜æ•ˆçš„å›¾åƒç”Ÿæˆè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¸»æµçš„å›¾åƒæ ‡è®°åŒ–æ–¹æ³•é€šå¸¸å‹ç¼©ç‡æœ‰é™ï¼Œä½¿å¾—é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆçš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è¯­è¨€è¿›è¡Œé«˜æ•ˆçš„å›¾åƒæ ‡è®°åŒ–ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºæ–‡æœ¬æ¡ä»¶å›¾åƒæ ‡è®°åŒ–ï¼ˆTexTokï¼‰ã€‚TexTokæ˜¯ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ ‡è®°åŒ–æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è¯­è¨€æä¾›é«˜çº§è¯­ä¹‰ã€‚é€šè¿‡ä»¥æè¿°æ€§æ–‡æœ¬å­—å¹•ä¸ºæ ‡è®°åŒ–è¿‡ç¨‹æä¾›æ¡ä»¶ï¼ŒTexTokå…è®¸æ ‡è®°åŒ–è¿‡ç¨‹ä¸“æ³¨äºå°†ç»†å¾®çš„è§†è§‰ç»†èŠ‚ç¼–ç ä¸ºæ½œåœ¨æ ‡è®°ï¼Œä»è€Œæé«˜é‡å»ºè´¨é‡å’Œå‹ç¼©ç‡ã€‚ä¸æ²¡æœ‰æ–‡æœ¬æ¡ä»¶çš„ä¼ ç»Ÿæ ‡è®°å™¨ç›¸æ¯”ï¼ŒTexTokåœ¨ImageNet-256å’Œ-512åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡é‡å»ºFIDåˆ†åˆ«æé«˜äº†29.2%å’Œ48.1%ï¼Œè·¨è¶Šä¸åŒæ•°é‡çš„æ ‡è®°ã€‚è¿™äº›æ ‡è®°åŒ–æ”¹è¿›å§‹ç»ˆè½¬åŒ–ä¸ºç”ŸæˆFIDçš„16.3%å’Œ34.3%çš„å¹³å‡æ”¹è¿›ã€‚é€šè¿‡ç®€å•åœ°ç”¨TexTokæ›¿æ¢æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ä¸­çš„æ ‡è®°å™¨ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨ä½¿ç”¨åªæœ‰32ä¸ªæ ‡è®°çš„ImageNet-512ä¸Šä»ç„¶ä¼˜äºåŸå§‹DiTï¼ŒåŒæ—¶å®ç°äº†93.5å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚TexTokä¸æ™®é€šçš„DiTç”Ÿæˆå™¨åœ¨ImageNet-256å’Œ-512ä¸Šåˆ†åˆ«è¾¾åˆ°äº†æœ€å…ˆè¿›çš„FIDåˆ†æ•°1.46å’Œ1.62ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå±•ç¤ºäº†TexTokçš„ä¼˜è¶Šæ€§ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨äº†ç°æˆçš„æ–‡æœ¬å­—å¹•è¿›è¡Œæ ‡è®°åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05796v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>    åˆ©ç”¨è¯­è¨€è¿›è¡Œå›¾åƒé«˜æ•ˆä»¤ç‰ŒåŒ–å¤‡å—å…³æ³¨ã€‚æ–°æ–¹æ³•Text-Conditioned Image Tokenizationï¼ˆTexTokï¼‰ç»“åˆæ–‡æœ¬æè¿°ï¼Œæ”¹å–„äº†å›¾åƒä»¤ç‰ŒåŒ–è¿‡ç¨‹ï¼Œå®ç°æ›´é«˜é‡å»ºè´¨é‡å’Œå‹ç¼©ç‡ã€‚é€šè¿‡æ›¿æ¢Diffusion Transformerä¸­çš„ä»¤ç‰ŒåŒ–å™¨ï¼ŒTexTokåŠ é€Ÿæ¨ç†å¹¶è¾¾åˆ°æœ€å…ˆè¿›çš„FIDåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TexTokæ˜¯ä¸€ä¸ªç»“åˆè¯­è¨€çš„ç®€å•æœ‰æ•ˆçš„å›¾åƒä»¤ç‰ŒåŒ–æ¡†æ¶ï¼Œåˆ©ç”¨è¯­è¨€æä¾›é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>TexToké€šè¿‡æ–‡æœ¬æè¿°æ¡ä»¶åŒ–ä»¤ç‰ŒåŒ–è¿‡ç¨‹ï¼Œä½¿ä»¤ç‰ŒåŒ–ä¸“æ³¨äºå°†ç»†å¾®çš„è§†è§‰ç»†èŠ‚ç¼–ç ä¸ºæ½œåœ¨ä»¤ç‰Œã€‚</li>
<li>ä¸æ²¡æœ‰æ–‡æœ¬æ¡ä»¶çš„å¸¸è§„ä»¤ç‰ŒåŒ–å™¨ç›¸æ¯”ï¼ŒTexTokåœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é‡å»ºFIDçš„æ˜¾è‘—æ”¹å–„ã€‚</li>
<li>TexTokèƒ½æé«˜ç”Ÿæˆè´¨é‡ï¼Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>TexTokåœ¨ä¸DiTç”Ÿæˆå™¨çš„ç»“åˆä¸‹è¾¾åˆ°å…ˆè¿›çš„FIDåˆ†æ•°ã€‚</li>
<li>TexTokåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæœ‰æ•ˆåˆ©ç”¨äº†ç°æˆçš„æ–‡æœ¬æè¿°è¿›è¡Œä»¤ç‰ŒåŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e46c6ca9e975dd20b4f2e922e9b277d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14f2bd55842df30ac820364582d553d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e0d72cf739b0c2ce9322bdca1b7430e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb581864e34de7061ded5b02c02499e3.jpg" align="middle">
</details>




<h2 id="Continuous-Video-Process-Modeling-Videos-as-Continuous-Multi-Dimensional-Processes-for-Video-Prediction"><a href="#Continuous-Video-Process-Modeling-Videos-as-Continuous-Multi-Dimensional-Processes-for-Video-Prediction" class="headerlink" title="Continuous Video Process: Modeling Videos as Continuous   Multi-Dimensional Processes for Video Prediction"></a>Continuous Video Process: Modeling Videos as Continuous   Multi-Dimensional Processes for Video Prediction</h2><p><strong>Authors:Gaurav Shrivastava, Abhinav Shrivastava</strong></p>
<p>Diffusion models have made significant strides in image generation, mastering tasks such as unconditional image synthesis, text-image translation, and image-to-image conversions. However, their capability falls short in the realm of video prediction, mainly because they treat videos as a collection of independent images, relying on external constraints such as temporal attention mechanisms to enforce temporal coherence. In our paper, we introduce a novel model class, that treats video as a continuous multi-dimensional process rather than a series of discrete frames. We also report a reduction of 75% sampling steps required to sample a new frame thus making our framework more efficient during the inference time. Through extensive experimentation, we establish state-of-the-art performance in video prediction, validated on benchmark datasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project page <a target="_blank" rel="noopener" href="https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html">https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html</a> for video results. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼ŒæŒæ¡äº†æ— æ¡ä»¶å›¾åƒåˆæˆã€å›¾æ–‡ç¿»è¯‘å’Œå›¾åƒè½¬æ¢ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è§†é¢‘é¢„æµ‹æ–¹é¢çš„èƒ½åŠ›è¿˜å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å°†è§†é¢‘è§†ä¸ºç‹¬ç«‹å›¾åƒçš„é›†åˆï¼Œä¾èµ–è¯¸å¦‚æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ç­‰å¤–éƒ¨çº¦æŸæ¥å¼ºåˆ¶æ‰§è¡Œæ—¶é—´è¿è´¯æ€§ã€‚åœ¨æˆ‘ä»¬çš„è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹ç±»åˆ«ï¼Œè¯¥æ¨¡å‹å°†è§†é¢‘è§†ä¸ºä¸€ä¸ªè¿ç»­çš„å¤šç»´è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä¸€ç³»åˆ—ç¦»æ•£å¸§ã€‚æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†é‡‡æ ·æ–°å¸§æ‰€éœ€çš„é‡‡æ ·æ­¥éª¤å‡å°‘äº†75%ï¼Œä»è€Œä½¿æˆ‘ä»¬çš„æ¡†æ¶åœ¨æ¨ç†æ—¶é—´æ›´åŠ é«˜æ•ˆã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬åœ¨è§†é¢‘é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨åŒ…æ‹¬KTHã€BAIRã€Human3.6Må’ŒUCF101ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚æœ‰å…³è§†é¢‘ç»“æœï¼Œè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://www.cs.umd.edu/%7Egauravsh/cvp/supp/website.html%E3%80%82">https://www.cs.umd.edu/~gauravsh&#x2F;cvp&#x2F;supp&#x2F;website.htmlã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04929v2">PDF</a> Navigate to the project page   <a target="_blank" rel="noopener" href="https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html">https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html</a> for video results.   Extended version of published CVPR paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨æ— æ¡ä»¶å›¾åƒåˆæˆã€æ–‡æœ¬å›¾åƒç¿»è¯‘å’Œå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°ã€‚ç„¶è€Œï¼Œåœ¨è§†é¢‘é¢„æµ‹æ–¹é¢ï¼Œç°æœ‰æ¨¡å‹çš„èƒ½åŠ›ä»æœ‰ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§æ–°çš„æ¨¡å‹ç±»åˆ«ï¼Œè¯¥æ¨¡å‹å°†è§†é¢‘è§†ä¸ºè¿ç»­çš„å¤šç»´è¿‡ç¨‹è€Œéä¸€ç³»åˆ—ç‹¬ç«‹çš„å¸§ï¼Œå¹¶å‡å°‘äº†75%çš„é‡‡æ ·æ­¥éª¤ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥æ¨¡å‹åœ¨è§†é¢‘é¢„æµ‹æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›æ°´å¹³ï¼Œå¹¶åœ¨KTHã€BAIRã€Human3.6Må’ŒUCF101ç­‰åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ›´å¤šè§†é¢‘ç»“æœè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://www.cs.umd.edu/~gauravsh/cvp/supp/website.html">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨æ— æ¡ä»¶å›¾åƒåˆæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤„ç†è§†é¢‘é¢„æµ‹æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦å› ä¸ºå°†è§†é¢‘è§†ä¸ºç‹¬ç«‹å›¾åƒé›†åˆã€‚</li>
<li>æœ¬æ–‡å¼•å…¥çš„æ–°å‹æ¨¡å‹å°†è§†é¢‘è§†ä¸ºè¿ç»­çš„å¤šç»´è¿‡ç¨‹ï¼Œæé«˜äº†è§†é¢‘é¢„æµ‹çš„æ•ˆæœã€‚</li>
<li>æ–°æ¨¡å‹å‡å°‘äº†75%çš„é‡‡æ ·æ­¥éª¤ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è§†é¢‘é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šå…³äºæ¨¡å‹æ€§èƒ½çš„è§†é¢‘ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a491540fbc3993ce8a48e87de08785d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4252ae1db338cc532291cd1e282561b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8409b56949ebd9352ea1ac5bf951b363.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c494e1f950c04186e36601d4a89589e2.jpg" align="middle">
</details>




<h2 id="DAug-Diffusion-based-Channel-Augmentation-for-Radiology-Image-Retrieval-and-Classification"><a href="#DAug-Diffusion-based-Channel-Augmentation-for-Radiology-Image-Retrieval-and-Classification" class="headerlink" title="DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval   and Classification"></a>DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval   and Classification</h2><p><strong>Authors:Ying Jin, Zhuoran Zhou, Haoquan Fang, Jenq-Neng Hwang</strong></p>
<p>Medical image understanding requires meticulous examination of fine visual details, with particular regions requiring additional attention. While radiologists build such expertise over years of experience, it is challenging for AI models to learn where to look with limited amounts of training data. This limitation results in unsatisfying robustness in medical image understanding. To address this issue, we propose Diffusion-based Feature Augmentation (DAug), a portable method that improves a perception modelâ€™s performance with a generative modelâ€™s output. Specifically, we extend a radiology image to multiple channels, with the additional channels being the heatmaps of regions where diseases tend to develop. A diffusion-based image-to-image translation model was used to generate such heatmaps conditioned on selected disease classes. Our method is motivated by the fact that generative models learn the distribution of normal and abnormal images, and such knowledge is complementary to image understanding tasks. In addition, we propose the Image-Text-Class Hybrid Contrastive learning to utilize both text and class labels. With two novel approaches combined, our method surpasses baseline models without changing the model architecture, and achieves state-of-the-art performance on both medical image retrieval and classification tasks. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒç†è§£éœ€è¦å¯¹ç»†å¾®çš„è§†è§‰ç»†èŠ‚è¿›è¡Œç»†è‡´çš„æ£€æŸ¥ï¼Œç‰¹å®šåŒºåŸŸéœ€è¦ç‰¹åˆ«æ³¨æ„ã€‚è™½ç„¶æ”¾å°„ç§‘åŒ»ç”Ÿç»è¿‡å¤šå¹´çš„ç»éªŒç§¯ç´¯è·å¾—äº†è¿™æ ·çš„ä¸“ä¸šçŸ¥è¯†ï¼Œä½†å¯¹äºäººå·¥æ™ºèƒ½æ¨¡å‹æ¥è¯´ï¼Œåœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹å­¦ä¹ åº”è¯¥å…³æ³¨çš„ä½ç½®å´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç§å±€é™æ€§å¯¼è‡´åŒ»å­¦å½±åƒç†è§£çš„ç¨³å¥æ€§ä»¤äººä¸æ»¡æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£çš„ç‰¹å¾å¢å¼ºï¼ˆDAugï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä¾¿æºçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºæ¥æé«˜æ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ”¾å°„å­¦å›¾åƒæ‰©å±•åˆ°å¤šä¸ªé€šé“ï¼Œé¢å¤–çš„é€šé“æ˜¯ç–¾ç—…å€¾å‘å‘å±•åŒºåŸŸçš„çƒ­å›¾ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºæ‰©æ•£çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹æ¥ç”Ÿæˆè¿™ç§çƒ­å›¾ï¼Œè¯¥çƒ­å›¾æ˜¯åŸºäºé€‰æ‹©çš„ç–¾ç—…ç±»åˆ«è¿›è¡Œæ¡ä»¶ç”Ÿæˆçš„ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°ä»¥ä¸‹äº‹å®çš„å¯å‘ï¼šç”Ÿæˆæ¨¡å‹å­¦ä¹ æ­£å¸¸å’Œå¼‚å¸¸å›¾åƒçš„åˆ†éƒ¨ï¼Œè¿™ç§çŸ¥è¯†ä¸å›¾åƒç†è§£ä»»åŠ¡æ˜¯äº’è¡¥çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å›¾åƒæ–‡æœ¬ç±»æ··åˆå¯¹æ¯”å­¦ä¹ ï¼Œä»¥åˆ©ç”¨æ–‡æœ¬å’Œç±»åˆ«æ ‡ç­¾ã€‚é€šè¿‡ç»“åˆè¿™ä¸¤ç§æ–°æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸éœ€è¦æ›´æ”¹æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨åŒ»å­¦å›¾åƒæ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„ç‰¹å¾å¢å¼ºï¼ˆDAugï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼Œæé«˜æ„ŸçŸ¥æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç†è§£ä¸Šçš„æ€§èƒ½ã€‚ä¸ºæé«˜æ¨¡å‹å¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨ï¼Œç”Ÿæˆç–¾ç—…æ˜“å‘åŒºåŸŸçš„çƒ­å›¾å¹¶å°†å…¶æ·»åŠ åˆ°åŒ»å­¦å›¾åƒä¸­ï¼Œä»¥å¢å¼ºç‰¹å¾ä¿¡æ¯ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹å°†å›¾åƒè½¬æ¢ä¸ºå¸¦æœ‰é€‰å®šç–¾ç—…ç±»åˆ«æ¡ä»¶çš„çƒ­å›¾ã€‚ç»“åˆå›¾åƒæ–‡æœ¬åˆ†ç±»æ··åˆå¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œç±»åˆ«æ ‡ç­¾æé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„å‰æä¸‹è¶…è¶ŠåŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨åŒ»å­¦å›¾åƒæ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒç†è§£éœ€è¦ç²¾ç»†çš„è§†è§‰åˆ†æï¼Œç‰¹å®šåŒºåŸŸéœ€è¦é‡ç‚¹å…³æ³¨ã€‚</li>
<li>AIæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç†è§£æ–¹é¢å› è®­ç»ƒæ•°æ®æœ‰é™é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„æ‰©æ•£ç‰¹å¾å¢å¼ºï¼ˆDAugï¼‰æ–¹æ³•é€šè¿‡ç”Ÿæˆæ¨¡å‹è¾“å‡ºæé«˜æ„ŸçŸ¥æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆç–¾ç—…æ˜“å‘åŒºåŸŸçƒ­å›¾å¹¶å°†å…¶æ·»åŠ åˆ°åŒ»å­¦å›¾åƒä¸­ï¼Œä»¥å¢å¼ºç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹å°†å›¾åƒè½¬æ¢ä¸ºçƒ­å›¾ï¼Œæ­¤è¿‡ç¨‹è€ƒè™‘é€‰å®šçš„ç–¾ç—…ç±»åˆ«æ¡ä»¶ã€‚</li>
<li>ç»“åˆå›¾åƒæ–‡æœ¬åˆ†ç±»æ··åˆå¯¹æ¯”å­¦ä¹ ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œç±»åˆ«æ ‡ç­¾æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52551bfa5714d144282c4a154e7f295d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2882a33c0f0434b8488ef31a8b82e736.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-171693de6bab2f5aa834a32f3252944f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eeb945673fadec31f755e92959f3429.jpg" align="middle">
</details>




<h2 id="Learning-to-Translate-Noise-for-Robust-Image-Denoising"><a href="#Learning-to-Translate-Noise-for-Robust-Image-Denoising" class="headerlink" title="Learning to Translate Noise for Robust Image Denoising"></a>Learning to Translate Noise for Robust Image Denoising</h2><p><strong>Authors:Inju Ha, Donghun Ryou, Seonguk Seo, Bohyung Han</strong></p>
<p>Deep learning-based image denoising techniques often struggle with poor generalization performance to out-of-distribution real-world noise. To tackle this challenge, we propose a novel noise translation framework that performs denoising on an image with translated noise rather than directly denoising an original noisy image. Specifically, our approach translates complex, unknown real-world noise into Gaussian noise, which is spatially uncorrelated and independent of image content, through a noise translation network. The translated noisy images are then processed by an image denoising network pretrained to effectively remove Gaussian noise, enabling robust and consistent denoising performance. We also design well-motivated loss functions and architectures for the noise translation network by leveraging the mathematical properties of Gaussian noise. Experimental results demonstrate that the proposed method substantially improves robustness and generalizability, outperforming state-of-the-art methods across diverse benchmarks. Visualized denoising results and the source code are available on our project page. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒå»å™ªæŠ€æœ¯é€šå¸¸å¯¹äºåˆ†å¸ƒå¤–çš„çœŸå®ä¸–ç•Œå™ªå£°çš„æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å™ªå£°ç¿»è¯‘æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹ç»è¿‡ç¿»è¯‘åçš„å™ªå£°å›¾åƒè¿›è¡Œå»å™ªï¼Œè€Œä¸æ˜¯ç›´æ¥å¯¹åŸå§‹å«å™ªå›¾åƒè¿›è¡Œå»å™ªã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸€ä¸ªå™ªå£°ç¿»è¯‘ç½‘ç»œï¼Œå°†å¤æ‚çš„ã€æœªçŸ¥çš„çœŸå®ä¸–ç•Œå™ªå£°ç¿»è¯‘æˆç©ºé—´ä¸Šä¸ç›¸å…³ä¸”ç‹¬ç«‹äºå›¾åƒå†…å®¹çš„é«˜æ–¯å™ªå£°ã€‚ç¿»è¯‘åçš„å«å™ªå›¾åƒç„¶åè¢«é€åˆ°ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„å›¾åƒå»å™ªç½‘ç»œä¸­è¿›è¡Œå¤„ç†ï¼Œè¯¥ç½‘ç»œå¯ä»¥æœ‰æ•ˆåœ°å»é™¤é«˜æ–¯å™ªå£°ï¼Œä»è€Œå®ç°ç¨³å¥å’Œä¸€è‡´çš„å»å™ªæ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡åˆ©ç”¨é«˜æ–¯å™ªå£°çš„æ•°å­¦å±æ€§ï¼Œä¸ºå™ªå£°ç¿»è¯‘ç½‘ç»œè®¾è®¡äº†åŠ¨æœºå……åˆ†çš„æŸå¤±å‡½æ•°å’Œæ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¤§å¤§æé«˜äº†ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å¯è§†åŒ–å»å™ªç»“æœå’Œé¡¹ç›®æºä»£ç å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04727v1">PDF</a> The project page is available at   <a target="_blank" rel="noopener" href="https://hij1112.github.io/learning-to-translate-noise/">https://hij1112.github.io/learning-to-translate-noise/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å™ªå£°ç¿»è¯‘æ¡†æ¶ï¼Œç”¨äºå¯¹å›¾åƒè¿›è¡Œå»å™ªå¤„ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡å™ªå£°ç¿»è¯‘ç½‘ç»œå°†å¤æ‚çš„ã€æœªçŸ¥çš„ç°å®ä¸–ç•Œå™ªå£°è½¬æ¢ä¸ºé«˜æ–¯å™ªå£°ï¼Œç„¶ååˆ©ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å›¾åƒå»å™ªç½‘ç»œå¯¹ç¿»è¯‘åçš„å™ªå£°å›¾åƒè¿›è¡Œå¤„ç†ï¼Œä»è€Œå®ç°ç¨³å¥å’Œä¸€è‡´çš„å»å™ªæ€§èƒ½ã€‚è¯¥æ–¹æ³•æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å™ªå£°ç¿»è¯‘æ¡†æ¶ï¼Œå°†å¤æ‚çš„ç°å®ä¸–ç•Œå™ªå£°è½¬æ¢ä¸ºé«˜æ–¯å™ªå£°ã€‚</li>
<li>é€šè¿‡å™ªå£°ç¿»è¯‘ç½‘ç»œå®ç°è¿™ä¸€è½¬æ¢ï¼Œè¯¥ç½‘ç»œåˆ©ç”¨æ•°å­¦é«˜æ–¯å™ªå£°å±æ€§è®¾è®¡æŸå¤±å‡½æ•°å’Œæ¶æ„ã€‚</li>
<li>ç¿»è¯‘åçš„å™ªå£°å›¾åƒé€šè¿‡é¢„è®­ç»ƒçš„å»å™ªç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä»¥æé«˜å»å™ªçš„ç¨³å¥æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æä¾›äº†å¯è§†åŒ–å»å™ªç»“æœå’Œæºä»£ç ã€‚</li>
<li>å™ªå£°ç¿»è¯‘ç½‘ç»œçš„è®¾è®¡åŸºäºé«˜æ–¯å™ªå£°çš„æ•°å­¦å±æ€§ï¼Œå¢å¼ºäº†å»å™ªç½‘ç»œçš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5ec9b08e8d94963f11df9ae2d01a5a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd040fbdca9d6e27642a00e9606c59fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0351a462ac21e54a1a7888a47e2b61d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fa70942dcc91098f40dec069824cbbe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff67803d2882715231f34c509c1a31a.jpg" align="middle">
</details>




<h2 id="Assessing-and-Learning-Alignment-of-Unimodal-Vision-and-Language-Models"><a href="#Assessing-and-Learning-Alignment-of-Unimodal-Vision-and-Language-Models" class="headerlink" title="Assessing and Learning Alignment of Unimodal Vision and Language Models"></a>Assessing and Learning Alignment of Unimodal Vision and Language Models</h2><p><strong>Authors:Le Zhang, Qian Yang, Aishwarya Agrawal</strong></p>
<p>How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this paper, we propose a direct assessment method, inspired by linear probing, to assess vision-language alignment. We identify that the degree of alignment of the SSL vision models depends on their SSL training objective, and we find that the clustering quality of SSL representations has a stronger impact on alignment performance than their linear separability. Next, we introduce Swift Alignment of Image and Language (SAIL), a efficient transfer learning framework that aligns pretrained unimodal vision and language models for downstream vision-language tasks. Since SAIL leverages the strengths of pretrained unimodal models, it requires significantly fewer (6%) paired image-text data for the multimodal alignment compared to models like CLIP which are trained from scratch. SAIL training only requires a single A100 GPU, 5 hours of training and can accommodate a batch size up to 32,768. SAIL achieves 73.4% zero-shot accuracy on ImageNet (vs. CLIPâ€™s 72.7%) and excels in zero-shot retrieval, complex reasoning, and semantic segmentation. Additionally, SAIL improves the language-compatibility of vision encoders that in turn enhance the performance of multimodal large language models. The entire codebase and model weights are open-source: <a target="_blank" rel="noopener" href="https://lezhang7.github.io/sail.github.io/">https://lezhang7.github.io/sail.github.io/</a> </p>
<blockquote>
<p>å•æ¨¡æ€è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„å¯¹é½ç¨‹åº¦å¦‚ä½•ï¼Ÿå°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»å°è¯•å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œä½†ä»–ä»¬çš„è¯„ä¼°æ–¹æ³•å¹¶ä¸èƒ½ç›´æ¥è½¬åŒ–ä¸ºè¿™äº›æ¨¡å‹åœ¨å®é™…è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­çš„ä½¿ç”¨æ–¹å¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—çº¿æ€§æ¢æµ‹å¯å‘çš„ç›´æ¥è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¯„ä¼°è§†è§‰è¯­è¨€å¯¹é½ç¨‹åº¦ã€‚æˆ‘ä»¬å‘ç°SSLè§†è§‰æ¨¡å‹çš„å¯¹é½ç¨‹åº¦å–å†³äºå…¶SSLè®­ç»ƒç›®æ ‡ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°SSLè¡¨ç¤ºçš„èšç±»è´¨é‡å¯¹å¯¹é½æ€§èƒ½çš„å½±å“å¼ºäºå…¶çº¿æ€§å¯åˆ†æ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†Swiftå›¾åƒå’Œè¯­è¨€å¯¹é½ï¼ˆSAILï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¯¹é½é¢„è®­ç»ƒçš„å•æ¨¡æ€è§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œä¸‹æ¸¸è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚ç”±äºSAILåˆ©ç”¨äº†é¢„è®­ç»ƒå•æ¨¡æ€æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå› æ­¤ä¸ä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰ç›¸æ¯”ï¼Œå®ƒè¿›è¡Œå¤šæ¨¡æ€å¯¹é½æ‰€éœ€çš„é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®å¤§å¤§å‡å°‘ï¼ˆå‡å°‘6%ï¼‰ã€‚SAILè®­ç»ƒä»…éœ€å•ä¸ªA100 GPUï¼Œè®­ç»ƒæ—¶é—´ä¸º5å°æ—¶ï¼Œå¹¶ä¸”å¯ä»¥å®¹çº³é«˜è¾¾32ï¼Œ768çš„æ‰¹æ¬¡å¤§å°ã€‚SAILåœ¨ImageNetä¸Šçš„é›¶æ ·æœ¬å‡†ç¡®ç‡è¾¾åˆ°äº†73.4%ï¼ˆç›¸è¾ƒäºCLIPçš„72.7%ï¼‰ï¼Œå¹¶åœ¨é›¶æ ·æœ¬æ£€ç´¢ã€å¤æ‚æ¨ç†å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼ŒSAILè¿˜æé«˜äº†è§†è§‰ç¼–ç å™¨çš„è¯­è¨€å…¼å®¹æ€§ï¼Œè¿›è€Œæé«˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡å‡ä¸ºå¼€æºï¼š<a target="_blank" rel="noopener" href="https://lezhang7.github.io/sail.github.io/">https://lezhang7.github.io/sail.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04616v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬è®ºæ–‡å…³æ³¨äºè¯„ä¼°è§†è§‰å’Œè¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½æ•ˆæœã€‚æ–‡ç« æå‡ºäº†åˆ©ç”¨çº¿æ€§æ¢æµ‹çš„è¯„ä¼°æ–¹æ³•ï¼Œæ·±å…¥ç ”ç©¶äº†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„è§†è§‰ä¸è¯­è¨€å¯¹é½æœºåˆ¶ï¼Œå¹¶æå‡ºäº†é«˜æ•ˆè¿ç§»å­¦ä¹ æ¡†æ¶Swift Alignment of Image and Languageï¼ˆSAILï¼‰ã€‚SAILåˆ©ç”¨é¢„è®­ç»ƒçš„å•æ¨¡æ€æ¨¡å‹ä¼˜åŠ¿ï¼Œåœ¨è·¨æ¨¡æ€å¯¹é½æ—¶å‡å°‘äº†é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®çš„éœ€æ±‚ï¼Œè®­ç»ƒæ•ˆç‡æé«˜ã€‚SAILåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œæé«˜äº†è§†è§‰ç¼–ç å™¨çš„è¯­è¨€å…¼å®¹æ€§å¹¶æå‡äº†å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ€§èƒ½ã€‚å…¶æºä»£ç å’Œæ¨¡å‹æƒé‡å‡å·²å¼€æºå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶æå‡ºä½¿ç”¨çº¿æ€§æ¢æµ‹æ¥ç›´æ¥è¯„ä¼°è§†è§‰ä¸è¯­è¨€æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½æ•ˆæœã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹çš„è§†è§‰ä¸è¯­è¨€å¯¹é½ç¨‹åº¦å—åˆ°è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒç›®æ ‡çš„å½±å“ã€‚</li>
<li>SSLè¡¨ç¤ºçš„èšç±»è´¨é‡æ¯”å¯¹é½æ€§èƒ½çš„å½±å“å¼ºäºå…¶çº¿æ€§å¯åˆ†æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é«˜æ•ˆè¿ç§»å­¦ä¹ æ¡†æ¶SAILï¼Œç”¨äºå¯¹é½é¢„è®­ç»ƒçš„å•æ¨¡æ€è§†è§‰å’Œè¯­è¨€æ¨¡å‹ã€‚</li>
<li>SAILç›¸è¾ƒäºä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹å¦‚CLIPï¼Œåœ¨å¯¹é½æ—¶å‡å°‘äº†é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>SAILåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå¦‚ImageNetçš„é›¶æ ·æœ¬å‡†ç¡®ç‡æå‡ï¼Œä»¥åŠåœ¨é›¶æ ·æœ¬æ£€ç´¢ã€å¤æ‚æ¨ç†å’Œè¯­ä¹‰åˆ†å‰²ä¸Šçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f00c06453dbe93741d8207aef04303b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a34d5434752100e7c05ca748842d1cfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a53e5e0d90906b6d503b4ee0b8446d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e42ec86e8ecf70ee5f8515b491012c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-841a9d4df2568e57bddb5b57672e2a60.jpg" align="middle">
</details>




<h2 id="DiffSign-AI-Assisted-Generation-of-Customizable-Sign-Language-Videos-With-Enhanced-Realism"><a href="#DiffSign-AI-Assisted-Generation-of-Customizable-Sign-Language-Videos-With-Enhanced-Realism" class="headerlink" title="DiffSign: AI-Assisted Generation of Customizable Sign Language Videos   With Enhanced Realism"></a>DiffSign: AI-Assisted Generation of Customizable Sign Language Videos   With Enhanced Realism</h2><p><strong>Authors:Sudha Krishnamurthy, Vimal Bhat, Abhinav Jain</strong></p>
<p>The proliferation of several streaming services in recent years has now made it possible for a diverse audience across the world to view the same media content, such as movies or TV shows. While translation and dubbing services are being added to make content accessible to the local audience, the support for making content accessible to people with different abilities, such as the Deaf and Hard of Hearing (DHH) community, is still lagging. Our goal is to make media content more accessible to the DHH community by generating sign language videos with synthetic signers that are realistic and expressive. Using the same signer for a given media content that is viewed globally may have limited appeal. Hence, our approach combines parametric modeling and generative modeling to generate realistic-looking synthetic signers and customize their appearance based on user preferences. We first retarget human sign language poses to 3D sign language avatars by optimizing a parametric model. The high-fidelity poses from the rendered avatars are then used to condition the poses of synthetic signers generated using a diffusion-based generative model. The appearance of the synthetic signer is controlled by an image prompt supplied through a visual adapter. Our results show that the sign language videos generated using our approach have better temporal consistency and realism than signing videos generated by a diffusion model conditioned only on text prompts. We also support multimodal prompts to allow users to further customize the appearance of the signer to accommodate diversity (e.g. skin tone, gender). Our approach is also useful for signer anonymization. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šç§æµåª’ä½“æœåŠ¡çš„æ™®åŠä½¿å¾—ä¸–ç•Œå„åœ°çš„è§‚ä¼—éƒ½èƒ½è§‚çœ‹ç›¸åŒçš„åª’ä½“å†…å®¹ï¼Œå¦‚ç”µå½±æˆ–ç”µè§†å‰§ã€‚è™½ç„¶å¢åŠ äº†ç¿»è¯‘å’Œé…éŸ³æœåŠ¡ï¼Œä½¿å†…å®¹èƒ½å¤Ÿé¢å‘å½“åœ°è§‚ä¼—ï¼Œä½†å¯¹äºä¸åŒèƒ½åŠ›äººç¾¤ï¼ˆå¦‚è‹å“‘å’Œå¬åŠ›å—æŸç¤¾åŒºï¼‰çš„å†…å®¹å¯è®¿é—®æ€§çš„æ”¯æŒä»ç„¶æ»åã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ç”Ÿæˆå…·æœ‰ç°å®æ„Ÿå’Œè¡¨ç°åŠ›çš„æ‰‹è¯­è§†é¢‘æ¥ä½¿åª’ä½“å†…å®¹æ›´æ˜“äºè‹å“‘å’Œå¬åŠ›å—æŸç¤¾åŒºè®¿é—®ã€‚å¯¹äºå…¨çƒèŒƒå›´å†…è§‚çœ‹çš„ç»™å®šåª’ä½“å†…å®¹ä½¿ç”¨åŒä¸€æ‰‹è¯­è€…å¯èƒ½å¸å¼•åŠ›æœ‰é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å‚æ•°å»ºæ¨¡å’Œç”Ÿæˆå»ºæ¨¡ï¼Œä»¥ç”Ÿæˆçœ‹ä¼¼é€¼çœŸçš„åˆæˆæ‰‹è¯­è€…ï¼Œå¹¶æ ¹æ®ç”¨æˆ·åå¥½å®šåˆ¶å…¶å¤–è§‚ã€‚æˆ‘ä»¬é¦–å…ˆå°†äººæ‰‹è¯­å§¿åŠ¿ç›®æ ‡è®¾å®šä¸ºä¸‰ç»´æ‰‹è¯­åŠ¨ç”»è§’è‰²ï¼Œé€šè¿‡ä¼˜åŒ–å‚æ•°æ¨¡å‹æ¥å®ç°ã€‚ä»æ¸²æŸ“çš„åŠ¨ç”»è§’è‰²ä¸­å¾—åˆ°çš„é«˜ä¿çœŸå§¿åŠ¿ç„¶åç”¨äºç¡®å®šä½¿ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„æ‰‹è¯­è€…çš„å§¿åŠ¿ã€‚åˆæˆæ‰‹è¯­è€…çš„å¤–è§‚ç”±é€šè¿‡è§†è§‰é€‚é…å™¨æä¾›çš„å›¾åƒæç¤ºè¿›è¡Œæ§åˆ¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„æ‰‹è¯­è§†é¢‘æ¯”ä»…é€šè¿‡æ–‡æœ¬æç¤ºè¿›è¡Œæ¡ä»¶å¤„ç†çš„æ‰‹è¯­è§†é¢‘å…·æœ‰æ›´å¥½çš„æ—¶é—´è¿è´¯æ€§å’Œé€¼çœŸæ€§ã€‚æˆ‘ä»¬è¿˜æ”¯æŒå¤šæ¨¡å¼æç¤ºï¼Œä»¥å…è®¸ç”¨æˆ·è¿›ä¸€æ­¥å®šåˆ¶æ‰‹è¯­è€…çš„å¤–è§‚ä»¥é€‚åº”å¤šæ ·æ€§ï¼ˆä¾‹å¦‚è‚¤è‰²ã€æ€§åˆ«ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹äºæ‰‹è¯­è€…çš„åŒ¿ååŒ–ä¹Ÿå¾ˆæœ‰ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03878v1">PDF</a> Published in Proceedings of ECCV, Workshop on Assistive Computer   Vision and Robotics, 2024</p>
<p><strong>Summary</strong><br>å†…å®¹æ‘˜è¦ï¼šè¿‘å¹´æ¥æµåª’ä½“æœåŠ¡çš„å¢å¤šä½¿å¾—å…¨çƒè§‚ä¼—èƒ½å¤Ÿè§‚çœ‹ç›¸åŒçš„åª’ä½“å†…å®¹ï¼Œå¦‚ç”µå½±æˆ–ç”µè§†å‰§ã€‚å°½ç®¡ç¿»è¯‘å’Œé…éŸ³æœåŠ¡æ­£åœ¨å¢åŠ ï¼Œä½¿å¾—å†…å®¹å¯¹å½“åœ°è§‚ä¼—æ›´å…·å¸å¼•åŠ›ï¼Œä½†å¯¹äºè‹å“‘å’Œå¬åŠ›å—æŸç¾¤ä½“çš„å†…å®¹æ— éšœç¢æ”¯æŒä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡ç”Ÿæˆå…·æœ‰ç°å®æ„Ÿå’Œè¡¨ç°åŠ›çš„æ‰‹è¯­è§†é¢‘åˆæˆç­¾åè€…ï¼Œä½¿åª’ä½“å†…å®¹æ›´æ˜“äºè‹å“‘ç¾¤ä½“è®¿é—®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å‚æ•°å»ºæ¨¡å’Œç”Ÿæˆå»ºæ¨¡ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„åˆæˆç­¾åè€…å¹¶æ ¹æ®ç”¨æˆ·åå¥½å®šåˆ¶å¤–è§‚ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–å‚æ•°æ¨¡å‹å°†äººç±»æ‰‹è¯­å§¿åŠ¿è½¬ç§»åˆ°3Dæ‰‹è¯­è™šæ‹Ÿè§’è‰²ä¸Šã€‚ç„¶åï¼Œä½¿ç”¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹æ¨¡æ‹Ÿè¿™äº›å§¿åŠ¿ï¼Œä»æ¸²æŸ“çš„è™šæ‹Ÿè§’è‰²è·å¾—çš„é«˜ç²¾åº¦å§¿åŠ¿è¢«ç”¨æ¥æ¨¡æ‹Ÿåˆæˆç­¾åè€…çš„å§¿åŠ¿ã€‚åˆæˆç­¾åè€…çš„å¤–è§‚ç”±è§†è§‰é€‚é…å™¨æä¾›çš„å›¾åƒæç¤ºæ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„æ‰‹è¯­è§†é¢‘åœ¨æ—¶åºä¸€è‡´æ€§å’Œé€¼çœŸåº¦æ–¹é¢ä¼˜äºä»…ç”±æ–‡æœ¬æç¤ºæ§åˆ¶çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„æ‰‹è¯­è§†é¢‘ã€‚æˆ‘ä»¬è¿˜æ”¯æŒå¤šæ¨¡å¼æç¤ºï¼Œå…è®¸ç”¨æˆ·è¿›ä¸€æ­¥å®šåˆ¶ç­¾åè€…çš„å¤–è§‚ä»¥é€‚åº”å¤šæ ·æ€§ï¼ˆå¦‚è‚¤è‰²ã€æ€§åˆ«ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æœ‰åŠ©äºå®ç°ç­¾åè€…çš„åŒ¿ååŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šç§æµåª’ä½“æœåŠ¡çš„å‘å±•ä½¿å¾—å…¨çƒè§‚ä¼—èƒ½å¤Ÿè§‚çœ‹ç›¸åŒçš„åª’ä½“å†…å®¹ï¼Œå¦‚ç”µå½±å’Œç”µè§†å‰§ã€‚</li>
<li>å½“å‰è™½ç„¶ç¿»è¯‘å’Œé…éŸ³æœåŠ¡åœ¨å¢åŠ ï¼Œä½†å¯¹è‹å“‘å’Œå¬åŠ›å—æŸç¾¤ä½“çš„å†…å®¹æ— éšœç¢æ”¯æŒä»ç„¶ä¸è¶³ã€‚</li>
<li>æœ¬æ–‡çš„ç›®æ ‡æ˜¯ä½¿åª’ä½“å†…å®¹æ›´æ˜“äºè‹å“‘ç¾¤ä½“è®¿é—®ï¼Œé€šè¿‡ç”Ÿæˆå…·æœ‰ç°å®æ„Ÿå’Œè¡¨ç°åŠ›çš„æ‰‹è¯­è§†é¢‘åˆæˆç­¾åè€…ã€‚</li>
<li>ç»“åˆå‚æ•°å»ºæ¨¡å’Œç”Ÿæˆå»ºæ¨¡çš„æ–¹æ³•ç”¨äºç”Ÿæˆåˆæˆç­¾åè€…ï¼Œä¿è¯äº†è§†é¢‘çš„é€¼çœŸåº¦å’Œå¤šæ ·æ€§ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡å›¾åƒæç¤ºå®šåˆ¶åˆæˆç­¾åè€…çš„å¤–è§‚ï¼ŒåŒ…æ‹¬è‚¤è‰²ã€æ€§åˆ«ç­‰ç‰¹å¾ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•ç”Ÿæˆçš„æ‰‹è¯­è§†é¢‘æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´åŠ é€¼çœŸä¸”å…·æœ‰æ›´å¥½çš„æ—¶åºä¸€è‡´æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-47b10c2f86e25c4ac140ab424551ad2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5e32c7e028f8e169b6490870d77a5a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44ae850e5512723aaafa7021339d7487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e7babfa38fb6366c18b3e931127f064.jpg" align="middle">
</details>




<h2 id="Distillation-of-Diffusion-Features-for-Semantic-Correspondence"><a href="#Distillation-of-Diffusion-Features-for-Semantic-Correspondence" class="headerlink" title="Distillation of Diffusion Features for Semantic Correspondence"></a>Distillation of Diffusion Features for Semantic Correspondence</h2><p><strong>Authors:Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, BjÃ¶rn Ommer</strong></p>
<p>Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results. Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency. In this work, we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences. Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence. Our code and weights are publicly available on our project page. </p>
<blockquote>
<p>è¯­ä¹‰å¯¹åº”æ˜¯ç¡®å®šå›¾åƒä¸åŒéƒ¨åˆ†ä¹‹é—´å…³ç³»çš„ä»»åŠ¡ï¼Œå®ƒæ˜¯å„ç§åº”ç”¨çš„åŸºç¡€ï¼ŒåŒ…æ‹¬3Dé‡å»ºã€å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€å¯¹è±¡è·Ÿè¸ªå’Œè§†è§‰åœºæ‰€è¯†åˆ«ã€‚æœ€è¿‘çš„ç ”ç©¶å¼€å§‹æ¢ç´¢åœ¨å¤§å‹ç”Ÿæˆå›¾åƒæ¨¡å‹ä¸­å­¦ä¹ çš„è¡¨ç¤ºç”¨äºè¯­ä¹‰å¯¹åº”ï¼Œå¹¶æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ä¾èµ–äºç»“åˆå¤šä¸ªå¤§å‹æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—éœ€æ±‚é«˜ã€æ•ˆç‡ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§æ›´é«˜æ•ˆçš„è®¡ç®—æ–¹æ³•æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„çŸ¥è¯†è’¸é¦æŠ€æœ¯æ¥è§£å†³æ•ˆç‡é™ä½çš„é—®é¢˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸¤ä¸ªå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¹¶å°†è¿™äº›äº’è¡¥æ¨¡å‹çš„èƒ½åŠ›æç‚¼åˆ°ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ä¸­ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡ç»“åˆ3Dæ•°æ®ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œè€Œæ— éœ€äººå·¥æ³¨é‡Šçš„å¯¹åº”å…³ç³»ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨3Dæ•°æ®å¢å¼ºçš„è’¸é¦æ¨¡å‹å®ç°äº†ä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®¡ç®—è´Ÿè½½ï¼Œæé«˜äº†å®é™…åº”ç”¨çš„å®ç”¨æ€§ï¼Œå¦‚è¯­ä¹‰è§†é¢‘å¯¹åº”ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæƒé‡å·²åœ¨é¡¹ç›®é¡µé¢ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03512v1">PDF</a> WACV 2025, Page: <a target="_blank" rel="noopener" href="https://compvis.github.io/distilldift">https://compvis.github.io/distilldift</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦æŠ€æœ¯çš„é«˜æ•ˆè¯­ä¹‰å¯¹åº”æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä¸¤ä¸ªå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå°†å…¶è’¸é¦åˆ°ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ä¸­ï¼Œå®ç°é«˜å‡†ç¡®æ€§ä¸ä½è®¡ç®—æˆæœ¬ã€‚åŒæ—¶ï¼Œç»“åˆ3Dæ•°æ®å¢å¼ºï¼Œæ— éœ€äººå·¥æ ‡æ³¨å¯¹åº”ç‰©ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥è’¸é¦æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½è®¡ç®—è´Ÿè½½ï¼Œå¢å¼ºç°å®åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰å¯¹åº”æ˜¯ç¡®å®šå›¾åƒä¸åŒéƒ¨åˆ†ä¹‹é—´å…³ç³»çš„åŸºç¡€ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>å½“å‰å…ˆè¿›æ–¹æ³•ä¾èµ–å¤šä¸ªå¤§å‹æ¨¡å‹ç»„åˆï¼Œå¯¼è‡´é«˜è®¡ç®—éœ€æ±‚å’Œä½æ•ˆç‡ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºçŸ¥è¯†è’¸é¦çš„æŠ€æœ¯æ¥å…‹æœæ•ˆç‡é™ä½çš„é—®é¢˜ï¼Œå°†ä¸¤ä¸ªå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹çš„èƒ½åŠ›è’¸é¦åˆ°ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ä¸­ã€‚</li>
<li>ç»“åˆ3Dæ•°æ®å¢å¼ºè¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å¯¹åº”ç‰©ã€‚</li>
<li>è’¸é¦æ¨¡å‹åœ¨æ€§èƒ½ä¸Šä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½è®¡ç®—è´Ÿè½½ã€‚</li>
<li>æå‡ºçš„æ¨¡å‹å…·æœ‰å®ç”¨æ€§ï¼Œå¯åº”ç”¨äºç°å®ä¸–ç•Œçš„è¯­ä¹‰è§†é¢‘å¯¹åº”ç­‰åº”ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60b4188d17364d2d5e91a0a2073c100a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e47e546cd0753fbca5bba3c9c25b0f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30b37db09954d9feb0f4b7d91eb2096.jpg" align="middle">
</details>




<h2 id="Survey-of-different-Large-Language-Model-Architectures-Trends-Benchmarks-and-Challenges"><a href="#Survey-of-different-Large-Language-Model-Architectures-Trends-Benchmarks-and-Challenges" class="headerlink" title="Survey of different Large Language Model Architectures: Trends,   Benchmarks, and Challenges"></a>Survey of different Large Language Model Architectures: Trends,   Benchmarks, and Challenges</h2><p><strong>Authors:Minghao Shao, Abdul Basit, Ramesh Karri, Muhammad Shafique</strong></p>
<p>Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£è¡¨äº†ä¸€ç±»æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ƒä»¬æ“…é•¿ç†è§£è‡ªç„¶è¯­è¨€å¹¶ä¸ºå„ç§æç¤ºæˆ–æŸ¥è¯¢ç”Ÿæˆè¿è´¯çš„å“åº”ã€‚è¿™äº›æ¨¡å‹çš„å¤æ‚æ€§è¿œè¿œè¶…è¿‡äº†ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œï¼Œé€šå¸¸åŒ…å«æ•°åä¸ªç¥ç»ç½‘ç»œå±‚ï¼Œå‚æ•°ä»æ•°åäº¿åˆ°æ•°ä¸‡äº¿ä¸ç­‰ã€‚å®ƒä»¬é€šå¸¸ä¼šåœ¨åºå¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨åŸºäºtransformerå—çš„æ¶æ„ã€‚ç°ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¤šåŠŸèƒ½æ€§ï¼Œèƒ½å¤Ÿæ‰§è¡Œä¸€ç³»åˆ—ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€è¯­è¨€ç¿»è¯‘ã€é—®ç­”ä»¥åŠä»£ç ç”Ÿæˆå’Œåˆ†æç­‰ã€‚è¿™äº›æ¨¡å‹çš„ä¸€ä¸ªé«˜çº§å­é›†è¢«ç§°ä¸ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æ‰©å±•åˆ°å¤„ç†å’Œè§£è¯‘å¤šç§æ•°æ®æ¨¡å¼ï¼ŒåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚è¿™ç§å¢å¼ºåŠŸèƒ½èµ‹äºˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è§†é¢‘ç¼–è¾‘ã€å›¾åƒç†è§£å’Œä¸ºè§†è§‰å†…å®¹æ·»åŠ å­—å¹•çš„èƒ½åŠ›ã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢æ¦‚è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¿½æº¯äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”å˜ï¼Œç„¶åæ·±å…¥æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…´èµ·å’Œç»†å¾®ä¹‹å¤„ã€‚æˆ‘ä»¬åˆ†æäº†æ–°å…´çš„æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ¢è®¨äº†å®ƒä»¬çš„æŠ€æœ¯ç‰¹ç‚¹ã€ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€æ½œåœ¨å±€é™æ€§å’Œæœªæ¥å‘å±•çš„å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03220v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸€ç±»æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€å¹¶ç”Ÿæˆè¿è´¯çš„å“åº”ã€‚å®ƒä»¬é€šå¸¸åŒ…å«æ•°åå±‚ç¥ç»ç½‘ç»œï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ°æ•°åäº¿è‡³æ•°åä¸‡äº¿ï¼Œè®­ç»ƒåœ¨åºå¤§çš„æ•°æ®é›†ä¸Šï¼ŒåŸºäºå˜å‹å™¨å—æ¶æ„ã€‚LLMså…·æœ‰å¤šç§åŠŸèƒ½ï¼Œèƒ½å¤Ÿæ‰§è¡Œæ–‡æœ¬ç”Ÿæˆã€è¯­è¨€ç¿»è¯‘ã€é—®ç­”ä»¥åŠä»£ç ç”Ÿæˆå’Œåˆ†æç­‰ä»»åŠ¡ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯LLMsçš„ä¸€ä¸ªé«˜çº§å­é›†ï¼Œèƒ½å¤Ÿå¤„ç†å¹¶è§£é‡Šå¤šç§æ•°æ®æ¨¡æ€ï¼ŒåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚å®ƒä»¬å…·æœ‰è§†é¢‘ç¼–è¾‘ã€å›¾åƒç†è§£å’Œä¸ºè§†è§‰å†…å®¹æ·»åŠ å­—å¹•ç­‰åŠŸèƒ½ã€‚æœ¬æ–‡æä¾›äº†å¯¹LLMsçš„æœ€æ–°è¿›å±•çš„å…¨é¢æ¦‚è¿°ï¼Œè¿½æº¯äº†LLMsçš„æ¼”å˜ï¼Œæ·±å…¥æ¢è®¨äº†MLLMsçš„å‡ºç°å’Œç‰¹ç‚¹ï¼Œåˆ†æäº†æœ€æ–°å…ˆè¿›çš„MLLMsçš„æŠ€æœ¯ç‰¹ç‚¹ã€ä¼˜åŠ¿å’Œå±€é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸€ç±»æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€ã€‚</li>
<li>LLMsåŒ…å«å¤šå±‚ç¥ç»ç½‘ç»œï¼Œå‚æ•°è§„æ¨¡åºå¤§ï¼Œé€šå¸¸åœ¨åºå¤§çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>LLMså…·æœ‰å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€è¯­è¨€ç¿»è¯‘ã€é—®ç­”å’Œä»£ç ç”Ÿæˆç­‰ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯LLMsçš„ä¸€ä¸ªå­é›†ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§æ•°æ®æ¨¡æ€ï¼Œå¦‚å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ã€‚</li>
<li>MLLMså…·æœ‰è§†é¢‘ç¼–è¾‘ã€å›¾åƒç†è§£å’Œä¸ºè§†è§‰å†…å®¹æ·»åŠ å­—å¹•ç­‰é«˜çº§åŠŸèƒ½ã€‚</li>
<li>æ–‡ä¸­æ¦‚è¿°äº†LLMsçš„æœ€æ–°è¿›å±•ï¼ŒåŒ…æ‹¬å…¶æ¼”å˜ã€MLLMsçš„å‡ºç°å’Œç‰¹ç‚¹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec62c0cbfec2386d9e21557f3a23e063.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e6ae535ee4b3b74e717803d7a466c44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f2849c2491b4aad58027f84948ed7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec56f92ae20fdf3a9c9320b63ed9907.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7b83e1cd29e887b436c129fe2c8354.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54c40382ad20041c4cb305f19746b5c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a1687363bdce1b258ef96c3b43b588e.jpg" align="middle">
</details>




<h2 id="MRNet-Multifaceted-Resilient-Networks-for-Medical-Image-to-Image-Translation"><a href="#MRNet-Multifaceted-Resilient-Networks-for-Medical-Image-to-Image-Translation" class="headerlink" title="MRNet: Multifaceted Resilient Networks for Medical Image-to-Image   Translation"></a>MRNet: Multifaceted Resilient Networks for Medical Image-to-Image   Translation</h2><p><strong>Authors:Hyojeong Lee, Youngwan Jo, Inpyo Hong, Sanghyun Park</strong></p>
<p>We propose a Multifaceted Resilient Network(MRNet), a novel architecture developed for medical image-to-image translation that outperforms state-of-the-art methods in MRI-to-CT and MRI-to-MRI conversion. MRNet leverages the Segment Anything Model (SAM) to exploit frequency-based features to build a powerful method for advanced medical image transformation. The architecture extracts comprehensive multiscale features from diverse datasets using a powerful SAM image encoder and performs resolution-aware feature fusion that consistently integrates U-Net encoder outputs with SAM-derived features. This fusion optimizes the traditional U-Net skip connection while leveraging transformer-based contextual analysis. The translation is complemented by an innovative dual-mask configuration incorporating dynamic attention patterns and a specialized loss function designed to address regional mapping mismatches, preserving both the gross anatomy and tissue details. Extensive validation studies have shown that MRNet outperforms state-of-the-art architectures, particularly in maintaining anatomical fidelity and minimizing translation artifacts. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé¢å¼¹æ€§ç½‘ç»œï¼ˆMRNetï¼‰è¿™ä¸€æ–°å‹æ¶æ„ï¼Œä¸“ä¸ºåŒ»å­¦å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢è®¾è®¡ï¼Œå…¶åœ¨MRIåˆ°CTå’ŒMRIåˆ°MRIçš„è½¬æ¢ä¸­è¡¨ç°å‡ºä¼˜äºæœ€æ–°æ–¹æ³•çš„æ•ˆæœã€‚MRNetåˆ©ç”¨åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰æ¥å¼€å‘åŸºäºé¢‘ç‡çš„ç‰¹å¾ï¼Œä»¥æ„å»ºä¸€ç§å…ˆè¿›çš„åŒ»å­¦å›¾åƒè½¬æ¢æ–¹æ³•ã€‚è¯¥æ¶æ„ä½¿ç”¨å¼ºå¤§çš„SAMå›¾åƒç¼–ç å™¨ä»å„ç§æ•°æ®é›†ä¸­æå–å…¨é¢çš„å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶æ‰§è¡Œå…·æœ‰åˆ†è¾¨ç‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆï¼Œè¯¥èåˆå§‹ç»ˆå°†U-Netç¼–ç å™¨çš„è¾“å‡ºä¸SAMæ´¾ç”Ÿçš„ç‰¹å¾é›†æˆåœ¨ä¸€èµ·ã€‚è¿™ç§èåˆä¼˜åŒ–äº†ä¼ ç»Ÿçš„U-Netè·³è¿‡è¿æ¥ï¼ŒåŒæ—¶åˆ©ç”¨åŸºäºå˜å‹å™¨çš„ä¸Šä¸‹æ–‡åˆ†æã€‚ç¿»è¯‘é€šè¿‡åˆ›æ–°çš„åŒæ©ç é…ç½®æ¥å®Œæˆï¼Œè¯¥é…ç½®ç»“åˆäº†åŠ¨æ€æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œä»¥è§£å†³åŒºåŸŸæ˜ å°„ä¸åŒ¹é…çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“è§£å‰–ç»“æ„å’Œç»„ç»‡ç»†èŠ‚ã€‚å¹¿æ³›çš„ç ”ç©¶éªŒè¯è¡¨æ˜ï¼ŒMRNetä¼˜äºæœ€æ–°çš„æ¶æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒè§£å‰–å­¦çš„ä¿çœŸåº¦å’Œæœ€å°åŒ–ç¿»è¯‘ä¼ªå½±æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03039v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong><br>MRNetæ˜¯ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„æ–°å‹æ¶æ„ï¼Œå…·æœ‰å¤šé¢æ¢å¤èƒ½åŠ›ã€‚å®ƒåˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰è¿›è¡ŒåŸºäºé¢‘ç‡çš„ç‰¹å¾æå–ï¼Œåœ¨MRIåˆ°CTå’ŒMRIåˆ°MRIè½¬æ¢ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é€šè¿‡å¼ºå¤§çš„SAMå›¾åƒç¼–ç å™¨å’Œå¤šå°ºåº¦ç‰¹å¾æå–ï¼Œç»“åˆU-Netç¼–ç å™¨çš„è¾“å‡ºå’ŒSAMè¡ç”Ÿçš„ç‰¹å¾è¿›è¡Œåˆ†è¾¨ç‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜é‡‡ç”¨åˆ›æ–°çš„åŒæ©è†œé…ç½®å’Œç‰¹æ®Šè®¾è®¡çš„æŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–åŒºåŸŸæ˜ å°„ä¸åŒ¹é…é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™å¤§ä½“è§£å‰–å’Œç»„ç»‡ç»†èŠ‚ã€‚MRNetåœ¨ä¿æŒè§£å‰–ä¿çœŸåº¦å’Œå‡å°‘ç¿»è¯‘ä¼ªå½±æ–¹é¢è¡¨ç°å‡ºä¼˜äºå…¶ä»–å…ˆè¿›æ¶æ„çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRNetæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„æ–°å‹æ¶æ„ã€‚</li>
<li>MRNetåˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰è¿›è¡Œé¢‘ç‡ç‰¹å¾æå–ã€‚</li>
<li>MRNetåœ¨MRIåˆ°CTå’ŒMRIåˆ°MRIè½¬æ¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>MRNeté€šè¿‡å¼ºå¤§çš„SAMå›¾åƒç¼–ç å™¨è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾æå–ã€‚</li>
<li>MRNetç»“åˆU-Netç¼–ç å™¨çš„è¾“å‡ºå’ŒSAMçš„ç‰¹å¾è¿›è¡Œåˆ†è¾¨ç‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆã€‚</li>
<li>MRNeté‡‡ç”¨åˆ›æ–°çš„åŒæ©è†œé…ç½®å’Œç‰¹æ®Šè®¾è®¡çš„æŸå¤±å‡½æ•°æ¥ä¼˜åŒ–åŒºåŸŸæ˜ å°„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>MRNetåœ¨ä¿æŒè§£å‰–ç»“æ„ä¿çœŸåº¦å’Œå‡å°‘ç¿»è¯‘ä¼ªå½±æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a78f48528f94fe0504ec02e6261b46f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2daa6d673d1bb2af40c304b1c38e9ec9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a991887d1d1f3721c7b454d670fb701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e8d4da0616e2836ed68b334fe091435.jpg" align="middle">
</details>




<h2 id="Motion-Prompting-Controlling-Video-Generation-with-Motion-Trajectories"><a href="#Motion-Prompting-Controlling-Video-Generation-with-Motion-Trajectories" class="headerlink" title="Motion Prompting: Controlling Video Generation with Motion Trajectories"></a>Motion Prompting: Controlling Video Generation with Motion Trajectories</h2><p><strong>Authors:Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, Deqing Sun</strong></p>
<p>Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, â€œinteractingâ€ with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: <a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a> </p>
<blockquote>
<p>åŠ¨ä½œæ§åˆ¶åœ¨ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›å’Œå¼•äººæ³¨ç›®çš„è§†é¢‘å†…å®¹æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸»è¦ä¾èµ–äºæ–‡æœ¬æç¤ºè¿›è¡ŒåŠ¨ä½œæ§åˆ¶ï¼Œè¿™åœ¨æ•æ‰åŠ¨æ€è¡Œä¸ºå’Œæ—¶åºç»„åˆç»†å¾®å·®åˆ«ä¸Šæœ‰æ‰€å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§åŸºäºæ—¶ç©ºç¨€ç–æˆ–å¯†é›†è¿åŠ¨è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼Œè¿™ç§çµæ´»çš„è¡¨ç¤ºå¯ä»¥ç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ã€ç‰¹å®šå¯¹è±¡çš„è¿åŠ¨æˆ–å…¨å±€åœºæ™¯çš„è¿åŠ¨ä»¥åŠæ—¶åºç¨€ç–è¿åŠ¨ï¼›ç”±äºå…¶çµæ´»æ€§ï¼Œæˆ‘ä»¬å°†è¿™ç§æ¡ä»¶ç§°ä¸ºåŠ¨ä½œæç¤ºã€‚ç”¨æˆ·å¯ä»¥ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•å°†é«˜çº§ç”¨æˆ·è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†ã€åŠå¯†é›†çš„åŠ¨ä½œæç¤ºï¼Œè¿™ä¸€è¿‡ç¨‹æˆ‘ä»¬ç§°ä¹‹ä¸ºåŠ¨ä½œæç¤ºæ‰©å±•ã€‚æˆ‘ä»¬é€šè¿‡å„ç§åº”ç”¨å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œå¯¹è±¡è¿åŠ¨æ§åˆ¶ã€â€œä¸å›¾åƒäº’åŠ¨â€ã€åŠ¨ä½œè½¬ç§»å’Œå›¾åƒç¼–è¾‘ç­‰ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†æ–°å…´è¡Œä¸ºï¼Œå¦‚é€¼çœŸçš„ç‰©ç†æ•ˆæœï¼Œè¿™è¡¨æ˜åŠ¨ä½œæç¤ºåœ¨æ¢ç´¢è§†é¢‘æ¨¡å‹å’Œä¸æœªæ¥ç”Ÿæˆä¸–ç•Œæ¨¡å‹äº’åŠ¨æ–¹é¢çš„æ½œåŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®šé‡è¯„ä¼°ã€äººç±»ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚è§†é¢‘ç»“æœå¯åœ¨æˆ‘ä»¬çš„ç½‘é¡µä¸ŠæŸ¥çœ‹ï¼š[ç½‘é¡µé“¾æ¥]ï¼ˆ<a target="_blank" rel="noopener" href="https://motion-prompting.github.io/%EF%BC%89%E3%80%82">https://motion-prompting.github.io/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02700v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a></p>
<p><strong>Summary</strong><br>è§†é¢‘å†…å®¹ç”Ÿæˆä¸­ï¼ŒåŠ¨ä½œæ§åˆ¶è‡³å…³é‡è¦ã€‚ç°æœ‰æ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬æç¤ºè¿›è¡ŒåŠ¨ä½œæ§åˆ¶ï¼Œéš¾ä»¥æ•æ‰åŠ¨æ€è¡Œä¸ºå’Œæ—¶åºç»„åˆçš„ç»†èŠ‚ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç§åŸºäºæ—¶ç©ºç¨€ç–æˆ–å¯†é›†è¿åŠ¨è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…¶çµæ´»è¡¨ç¤ºå¯ç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ã€ç‰¹å®šå¯¹è±¡æˆ–å…¨å±€åœºæ™¯è¿åŠ¨ä»¥åŠæ—¶åºç¨€ç–è¿åŠ¨ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºåŠ¨ä½œæç¤ºã€‚ç”¨æˆ·å¯ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼Œæˆ‘ä»¬ä¹Ÿå±•ç¤ºäº†å¦‚ä½•å°†é«˜çº§ç”¨æˆ·è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†çš„åŠå¯†é›†åŠ¨ä½œæç¤ºï¼Œç§°ä¸ºåŠ¨ä½œæç¤ºæ‰©å±•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§åº”ç”¨ä¸­è¡¨ç°å‡ºçµæ´»æ€§ï¼ŒåŒ…æ‹¬ç›¸æœºå’Œå¯¹è±¡è¿åŠ¨æ§åˆ¶ã€ä¸å›¾åƒçš„â€œäº¤äº’â€ã€è¿åŠ¨è½¬ç§»å’Œå›¾åƒç¼–è¾‘ç­‰ã€‚æˆ‘ä»¬çš„ç»“æœå±•ç¤ºäº†æ–°å…´è¡Œä¸ºï¼Œå¦‚çœŸå®ç‰©ç†ï¼Œæš—ç¤ºäº†åŠ¨ä½œæç¤ºåœ¨æ¢ç´¢è§†é¢‘æ¨¡å‹å’Œä¸æœªæ¥ç”Ÿæˆä¸–ç•Œæ¨¡å‹äº¤äº’ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡äº†å®šé‡è¯„ä¼°ã€äººç±»ç ”ç©¶ï¼Œå¹¶å±•ç¤ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚è§†é¢‘ç»“æœè¯·è®¿é—®æˆ‘ä»¬çš„ç½‘é¡µé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://motion-prompting.github.io/">https://motion-prompting.github.io/</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨ä½œæ§åˆ¶åœ¨è§†é¢‘å†…å®¹ç”Ÿæˆä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œèƒ½ç”Ÿæˆè¡¨è¾¾æ€§å¼ºä¸”å¸å¼•äººçš„è§†é¢‘å†…å®¹ã€‚</li>
<li>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸»è¦ä¾èµ–æ–‡æœ¬æç¤ºï¼Œéš¾ä»¥æ•æ‰åŠ¨æ€è¡Œä¸ºå’Œæ—¶åºç»„åˆçš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ—¶ç©ºç¨€ç–æˆ–å¯†é›†è¿åŠ¨è½¨è¿¹çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰çµæ´»è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å¯ç¼–ç ä»»æ„æ•°é‡çš„è½¨è¿¹ã€ç‰¹å®šå¯¹è±¡æˆ–å…¨å±€åœºæ™¯è¿åŠ¨ä»¥åŠæ—¶åºç¨€ç–è¿åŠ¨ï¼Œç§°ä¸ºåŠ¨ä½œæç¤ºã€‚</li>
<li>ç”¨æˆ·å¯ä»¥ç›´æ¥æŒ‡å®šç¨€ç–è½¨è¿¹ï¼Œä¹Ÿèƒ½å°†é«˜çº§ç”¨æˆ·è¯·æ±‚è½¬åŒ–ä¸ºè¯¦ç»†çš„åŠå¯†é›†åŠ¨ä½œæç¤ºã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§åº”ç”¨ä¸­è¡¨ç°å‡ºçµæ´»æ€§ï¼Œå¦‚ç›¸æœºå’Œå¯¹è±¡è¿åŠ¨æ§åˆ¶ã€ä¸å›¾åƒçš„äº¤äº’ã€è¿åŠ¨è½¬ç§»å’Œå›¾åƒç¼–è¾‘ç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af03877fc5d1e19e210533d6db49833d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f412082d9bdeceb09aafff53399ecf72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e8e03529298f81ab365b5d7d7091df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7c74d987a164c8037f6238e8c3ee5b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51ed5cc1a8e97cc59126fd4697d32f78.jpg" align="middle">
</details>




<h2 id="SimuScope-Realistic-Endoscopic-Synthetic-Dataset-Generation-through-Surgical-Simulation-and-Diffusion-Models"><a href="#SimuScope-Realistic-Endoscopic-Synthetic-Dataset-Generation-through-Surgical-Simulation-and-Diffusion-Models" class="headerlink" title="SimuScope: Realistic Endoscopic Synthetic Dataset Generation through   Surgical Simulation and Diffusion Models"></a>SimuScope: Realistic Endoscopic Synthetic Dataset Generation through   Surgical Simulation and Diffusion Models</h2><p><strong>Authors:Sabina Martyniak, Joanna Kaleta, Diego Dallâ€™Alba, MichaÅ‚ NaskrÄ™t, Szymon PÅ‚otka, PrzemysÅ‚aw Korzeniowski</strong></p>
<p>Computer-assisted surgical (CAS) systems enhance surgical execution and outcomes by providing advanced support to surgeons. These systems often rely on deep learning models trained on complex, challenging-to-annotate data. While synthetic data generation can address these challenges, enhancing the realism of such data is crucial. This work introduces a multi-stage pipeline for generating realistic synthetic data, featuring a fully-fledged surgical simulator that automatically produces all necessary annotations for modern CAS systems. This simulator generates a wide set of annotations that surpass those available in public synthetic datasets. Additionally, it offers a more complex and realistic simulation of surgical interactions, including the dynamics between surgical instruments and deformable anatomical environments, outperforming existing approaches. To further bridge the visual gap between synthetic and real data, we propose a lightweight and flexible image-to-image translation method based on Stable Diffusion (SD) and Low-Rank Adaptation (LoRA). This method leverages a limited amount of annotated data, enables efficient training, and maintains the integrity of annotations generated by our simulator. The proposed pipeline is experimentally validated and can translate synthetic images into images with real-world characteristics, which can generalize to real-world context, thereby improving both training and CAS guidance. The code and the dataset are available at <a target="_blank" rel="noopener" href="https://github.com/SanoScience/SimuScope">https://github.com/SanoScience/SimuScope</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ï¼ˆCASï¼‰ç³»ç»Ÿé€šè¿‡ä¸ºå¤–ç§‘åŒ»ç”Ÿæä¾›é«˜çº§æ”¯æŒï¼Œå¢å¼ºäº†æ‰‹æœ¯æ‰§è¡Œå’Œç»“æœã€‚è¿™äº›ç³»ç»Ÿé€šå¸¸ä¾èµ–äºåœ¨å¤æ‚ã€éš¾ä»¥æ ‡æ³¨çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è™½ç„¶åˆæˆæ•°æ®ç”Ÿæˆå¯ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½†æé«˜è¿™äº›æ•°æ®çš„çœŸå®æ€§è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªå¤šé˜¶æ®µç®¡é“ï¼Œç”¨äºç”Ÿæˆé€¼çœŸçš„åˆæˆæ•°æ®ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªå®Œå–„çš„æ‰‹æœ¯æ¨¡æ‹Ÿå™¨ï¼Œè¯¥æ¨¡æ‹Ÿå™¨å¯è‡ªåŠ¨ä¸ºç°ä»£CASç³»ç»Ÿç”Ÿæˆæ‰€æœ‰å¿…è¦çš„æ³¨é‡Šã€‚è¯¥æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„æ³¨é‡Šé›†å¹¿æ³›ï¼Œè¶…è¿‡äº†å…¬å…±åˆæˆæ•°æ®é›†ä¸­å¯ç”¨çš„æ³¨é‡Šã€‚æ­¤å¤–ï¼Œå®ƒæä¾›äº†æ›´å¤æ‚å’Œé€¼çœŸçš„æ‰‹æœ¯äº¤äº’æ¨¡æ‹Ÿï¼ŒåŒ…æ‹¬æ‰‹æœ¯å™¨æ¢°ä¸å¯å˜å½¢è§£å‰–ç¯å¢ƒä¹‹é—´çš„åŠ¨æ€å…³ç³»ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02332v1">PDF</a> Accepted to IEEE&#x2F;CVF Winter Conference on Applications of Computer   Vision (WACV) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šé˜¶æ®µç®¡é“ï¼Œç”¨äºç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿçš„åˆæˆæ•°æ®ã€‚é€šè¿‡æˆç†Ÿçš„æ‰‹æœ¯æ¨¡æ‹Ÿå™¨è‡ªåŠ¨ä¸ºç°ä»£è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ç³»ç»Ÿç”Ÿæˆæ‰€æœ‰å¿…è¦çš„æ³¨é‡Šï¼Œè§£å†³äº†æ•°æ®æ ‡æ³¨éš¾çš„é—®é¢˜ã€‚è¯¥æ¨¡æ‹Ÿå™¨äº§ç”Ÿçš„æ³¨é‡Šé›†è¶…è¿‡å…¬å…±åˆæˆæ•°æ®é›†ï¼Œå¯¹æ‰‹æœ¯å™¨æ¢°ä¸å¯å˜å½¢è§£å‰–ç¯å¢ƒä¹‹é—´çš„åŠ¨æ€å…³ç³»æä¾›äº†æ›´å¤æ‚ã€æ›´çœŸå®çš„æ¨¡æ‹Ÿã€‚ä¸ºç¼©å°åˆæˆæ•°æ®ä¸çœŸå®æ•°æ®ä¹‹é—´çš„è§†è§‰å·®è·ï¼Œæå‡ºäº†ä¸€ç§åŸºäºStable Diffusionå’ŒLow-Rank Adaptationçš„è½»é‡çº§ã€çµæ´»çš„å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆè®­ç»ƒï¼Œå¹¶ä¿æŒäº†æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„æ³¨é‡Šçš„å®Œæ•´æ€§ã€‚æ‰€æç®¡é“ç»è¿‡å®éªŒéªŒè¯ï¼Œå¯å°†åˆæˆå›¾åƒç¿»è¯‘æˆå…·æœ‰çœŸå®ä¸–ç•Œç‰¹å¾çš„å›¾åƒï¼Œå¯æ¨å¹¿è‡³çœŸå®åœºæ™¯ï¼Œæé«˜è®­ç»ƒå’Œè®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯çš„æŒ‡å¯¼æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯ç³»ç»Ÿé€šè¿‡æä¾›é«˜çº§æ”¯æŒæ¥æå‡æ‰‹æœ¯æ‰§è¡Œå’Œç»“æœã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤æ‚ã€éš¾ä»¥æ ‡æ³¨çš„æ•°æ®ä¸Šè®­ç»ƒå¯¹äºCASç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>åˆæˆæ•°æ®ç”Ÿæˆæ˜¯è§£å†³æ•°æ®æ ‡æ³¨éš¾çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å¢å¼ºæ•°æ®çš„çœŸå®æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå¤šé˜¶æ®µç®¡é“ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå…¨é¢çš„æ‰‹æœ¯æ¨¡æ‹Ÿå™¨ï¼Œèƒ½è‡ªåŠ¨ç”Ÿæˆæ‰€æœ‰å¿…è¦çš„æ³¨é‡Šã€‚</li>
<li>æ¨¡æ‹Ÿå™¨äº§ç”Ÿçš„æ³¨é‡Šé›†è¶…è¿‡å…¬å…±åˆæˆæ•°æ®é›†ï¼Œæä¾›æ›´ä¸ºå¤æ‚å’ŒçœŸå®çš„æ‰‹æœ¯æ¨¡æ‹Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºStable Diffusionå’ŒLow-Rank Adaptationçš„å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ–¹æ³•ï¼Œç¼©å°äº†åˆæˆä¸çœŸå®æ•°æ®é—´çš„è§†è§‰å·®è·ã€‚</li>
<li>æ‰€æç®¡é“ç»è¿‡å®éªŒéªŒè¯ï¼Œèƒ½æé«˜è®­ç»ƒå’Œè®¡ç®—æœºè¾…åŠ©æ‰‹æœ¯çš„æŒ‡å¯¼æ•ˆæœï¼Œç›¸å…³ä»£ç å’Œæ•°æ®é›†å·²å…¬å¼€ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ddb2427a515cc5138803fb4a354a087.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2b79f5205a53bc93dae99b8e35e8c19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd93ff1b0dfe86a503b204f9412c7f69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47685a5c1cdb67b474a08e0e13dba045.jpg" align="middle">
</details>




<h2 id="Uncertainty-Aware-Regularization-for-Image-to-Image-Translation"><a href="#Uncertainty-Aware-Regularization-for-Image-to-Image-Translation" class="headerlink" title="Uncertainty-Aware Regularization for Image-to-Image Translation"></a>Uncertainty-Aware Regularization for Image-to-Image Translation</h2><p><strong>Authors:Anuja Vats, Ivar Farup, Marius Pedersen, Kiran Raja</strong></p>
<p>The importance of quantifying uncertainty in deep networks has become paramount for reliable real-world applications. In this paper, we propose a method to improve uncertainty estimation in medical Image-to-Image (I2I) translation. Our model integrates aleatoric uncertainty and employs Uncertainty-Aware Regularization (UAR) inspired by simple priors to refine uncertainty estimates and enhance reconstruction quality. We show that by leveraging simple priors on parameters, our approach captures more robust uncertainty maps, effectively refining them to indicate precisely where the network encounters difficulties, while being less affected by noise. Our experiments demonstrate that UAR not only improves translation performance, but also provides better uncertainty estimations, particularly in the presence of noise and artifacts. We validate our approach using two medical imaging datasets, showcasing its effectiveness in maintaining high confidence in familiar regions while accurately identifying areas of uncertainty in novel&#x2F;ambiguous scenarios. </p>
<blockquote>
<p>åœ¨æ·±åº¦ç½‘ç»œä¸­å°†ä¸ç¡®å®šæ€§çš„é‡åŒ–åº”ç”¨äºå¯é çš„ç°å®ä¸–ç•Œåº”ç”¨å·²å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›åŒ»å­¦å›¾åƒå¯¹å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†å¶ç„¶æ€§ä¸ç¡®å®šæ€§ï¼Œå¹¶é‡‡ç”¨å—ç®€å•å…ˆéªŒå¯å‘çš„æ„è¯†ä¸ç¡®å®šæ€§æ­£åˆ™åŒ–ï¼ˆUARï¼‰æ¥ä¼˜åŒ–ä¸ç¡®å®šæ€§ä¼°è®¡å¹¶æé«˜é‡å»ºè´¨é‡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡åˆ©ç”¨å‚æ•°çš„ç®€å•å…ˆéªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ•è·äº†æ›´ç¨³å¥çš„ä¸ç¡®å®šæ€§æ˜ å°„ï¼Œæœ‰æ•ˆåœ°å¯¹å…¶è¿›è¡Œç»†åŒ–ï¼Œä»¥ç²¾ç¡®æŒ‡ç¤ºç½‘ç»œé‡åˆ°å›°éš¾çš„åŒºåŸŸï¼ŒåŒæ—¶å—åˆ°å™ªå£°çš„å½±å“è¾ƒå°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒUARä¸ä»…æé«˜äº†ç¿»è¯‘æ€§èƒ½ï¼Œè€Œä¸”æä¾›äº†æ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å™ªå£°å’Œä¼ªå½±çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç†Ÿæ‚‰åŒºåŸŸä¿æŒé«˜ç½®ä¿¡åº¦ä»¥åŠåœ¨æ–°å‹&#x2F;æ¨¡ç³Šåœºæ™¯ä¸­å‡†ç¡®è¯†åˆ«ä¸ç¡®å®šåŒºåŸŸçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01705v1">PDF</a> Accepted WACV 2025</p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›æ·±åº¦ç½‘ç»œä¸­ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ä¸­ã€‚é€šè¿‡æ•´åˆå¶ç„¶ä¸ç¡®å®šæ€§å¹¶é‡‡ç”¨å—ç®€å•å…ˆéªŒå¯å‘çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ­£åˆ™åŒ–ï¼ˆUARï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½æ›´ç¨³å¥åœ°æ•æ‰ä¸ç¡®å®šæ€§æ˜ å°„ï¼Œæé«˜é‡å»ºè´¨é‡ï¼Œå¹¶æœ‰æ•ˆæŒ‡ç¤ºç½‘ç»œé‡åˆ°å›°éš¾çš„åŒºåŸŸã€‚å®éªŒè¯æ˜ï¼ŒUARä¸ä»…æé«˜äº†ç¿»è¯‘æ€§èƒ½ï¼Œè€Œä¸”æä¾›äº†æ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å™ªå£°å’Œä¼ªå½±çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–ä¸ç¡®å®šæ€§åœ¨æ·±ç½‘ä¸­çš„å¯é æ€§å¯¹äºçœŸå®ä¸–ç•Œåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>è®ºæ–‡æå‡ºä¸€ç§æ”¹è¿›ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–¹æ³•ï¼Œåº”ç”¨äºåŒ»å­¦å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ã€‚</li>
<li>æ¨¡å‹æ•´åˆäº†å¶ç„¶ä¸ç¡®å®šæ€§å’Œå—ç®€å•å…ˆéªŒå¯å‘çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ­£åˆ™åŒ–ï¼ˆUARï¼‰ã€‚</li>
<li>UARèƒ½æ›´ç¨³å¥åœ°æ•æ‰ä¸ç¡®å®šæ€§æ˜ å°„ï¼Œæé«˜é‡å»ºè´¨é‡ã€‚</li>
<li>UARèƒ½æŒ‡ç¤ºç½‘ç»œé‡åˆ°å›°éš¾çš„åŒºåŸŸã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒUARæé«˜äº†ç¿»è¯‘æ€§èƒ½ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ab50e4c46323b9f82f31be90fcac4be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb103c2838223ebb5fbaa4210adaff42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d11e5b0821597f884d0844dc6702347b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14a45768a866c5b9f5580ebc0a80c4a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b16d3125c530ed354bb67516f09dd9db.jpg" align="middle">
</details>




<h2 id="RaD-A-Metric-for-Medical-Image-Distribution-Comparison-in-Out-of-Domain-Detection-and-Other-Applications"><a href="#RaD-A-Metric-for-Medical-Image-Distribution-Comparison-in-Out-of-Domain-Detection-and-Other-Applications" class="headerlink" title="RaD: A Metric for Medical Image Distribution Comparison in Out-of-Domain   Detection and Other Applications"></a>RaD: A Metric for Medical Image Distribution Comparison in Out-of-Domain   Detection and Other Applications</h2><p><strong>Authors:Nicholas Konz, Yuwen Chen, Hanxue Gu, Haoyu Dong, Yaqian Chen, Maciej A. Mazurowski</strong></p>
<p>Determining whether two sets of images belong to the same or different domain is a crucial task in modern medical image analysis and deep learning, where domain shift is a common problem that commonly results in decreased model performance. This determination is also important to evaluate the output quality of generative models, e.g., image-to-image translation models used to mitigate domain shift. Current metrics for this either rely on the (potentially biased) choice of some downstream task such as segmentation, or adopt task-independent perceptual metrics (e.g., FID) from natural imaging which insufficiently capture anatomical consistency and realism in medical images. We introduce a new perceptual metric tailored for medical images: Radiomic Feature Distance (RaD), which utilizes standardized, clinically meaningful and interpretable image features. We show that RaD is superior to other metrics for out-of-domain (OOD) detection in a variety of experiments. Furthermore, RaD outperforms previous perceptual metrics (FID, KID, etc.) for image-to-image translation by correlating more strongly with downstream task performance as well as anatomical consistency and realism, and shows similar utility for evaluating unconditional image generation. RaD also offers additional benefits such as interpretability, as well as stability and computational efficiency at low sample sizes. Our results are supported by broad experiments spanning four multi-domain medical image datasets, nine downstream tasks, six image translation models, and other factors, highlighting the broad potential of RaD for medical image analysis. </p>
<blockquote>
<p>ç¡®å®šä¸¤ç»„å›¾åƒæ˜¯å¦å±äºåŒä¸€é¢†åŸŸæˆ–ä¸åŒé¢†åŸŸï¼Œåœ¨ç°ä»£åŒ»å­¦å›¾åƒåˆ†æå’Œæ·±åº¦å­¦ä¹ ä¸­æ˜¯è‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚é¢†åŸŸåç§»æ˜¯ä¸€ä¸ªå¸¸è§çš„é—®é¢˜ï¼Œé€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€åˆ¤æ–­å¯¹äºè¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºè´¨é‡ä¹Ÿå¾ˆé‡è¦ï¼Œä¾‹å¦‚ç”¨äºå‡è½»é¢†åŸŸåç§»çš„å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘æ¨¡å‹ã€‚ç›®å‰çš„æŒ‡æ ‡è¦ä¹ˆä¾èµ–äºæŸäº›ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰çš„é€‰æ‹©ï¼ˆè¿™ç§é€‰æ‹©å¯èƒ½å­˜åœ¨åè§ï¼‰ï¼Œè¦ä¹ˆé‡‡ç”¨è‡ªç„¶æˆåƒä¸­çš„ä»»åŠ¡ç‹¬ç«‹æ„ŸçŸ¥æŒ‡æ ‡ï¼ˆå¦‚FIDï¼‰ï¼Œè€Œè¿™äº›æŒ‡æ ‡ä¸è¶³ä»¥æ•æ‰åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–ä¸€è‡´æ€§å’ŒçœŸå®æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹åŒ»å­¦å›¾åƒå¼•å…¥äº†ä¸€ç§æ–°çš„æ„ŸçŸ¥æŒ‡æ ‡ï¼šæ”¾å°„å­¦ç‰¹å¾è·ç¦»ï¼ˆRaDï¼‰ï¼Œå®ƒåˆ©ç”¨æ ‡å‡†åŒ–ã€ä¸´åºŠæ„ä¹‰é‡å¤§ä¸”å¯è§£é‡Šçš„å›¾åƒç‰¹å¾ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œåœ¨å„ç§å®éªŒä¸­ï¼ŒRaDåœ¨å…¶ä»–æŒ‡æ ‡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ›´æ“…é•¿æ£€æµ‹éé¢†åŸŸï¼ˆOODï¼‰ã€‚æ­¤å¤–ï¼ŒRaDåœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä»¥åŠè§£å‰–ä¸€è‡´æ€§å’ŒçœŸå®æ€§æ–¹é¢ä¸ä¹‹å‰çš„æ„ŸçŸ¥æŒ‡æ ‡ï¼ˆå¦‚FIDã€KIDç­‰ï¼‰ç›¸æ¯”è¡¨ç°å‡ºæ›´å¼ºçš„ç›¸å…³æ€§ï¼Œå¹¶ä¸”åœ¨è¯„ä¼°æ— æ¡ä»¶å›¾åƒç”Ÿæˆæ—¶æ˜¾ç¤ºå‡ºç±»ä¼¼çš„å®ç”¨æ€§ã€‚RaDè¿˜æä¾›äº†å¯è§£é‡Šæ€§ã€ç¨³å®šæ€§ä»¥åŠå°æ ·æœ¬è®¡ç®—æ•ˆç‡ç­‰é¢å¤–ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç»“æœå¾—åˆ°äº†å¹¿æ³›å®éªŒçš„æ”¯æŒï¼Œè¿™äº›å®éªŒæ¶‰åŠå››ä¸ªå¤šé¢†åŸŸåŒ»å­¦å›¾åƒæ•°æ®é›†ã€ä¹ä¸ªä¸‹æ¸¸ä»»åŠ¡ã€å…­ä¸ªå›¾åƒç¿»è¯‘æ¨¡å‹ä»¥åŠå…¶ä»–å› ç´ ï¼Œçªå‡ºäº†RaDåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¹¿é˜”æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01496v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒçš„æ–°å‹æ„ŸçŸ¥åº¦é‡æ–¹æ³•â€”â€”æ”¾å°„å­¦ç‰¹å¾è·ç¦»ï¼ˆRaDï¼‰ï¼Œç”¨äºåˆ¤æ–­å›¾åƒæ˜¯å¦å±äºåŒä¸€é¢†åŸŸã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ ‡å‡†åŒ–ã€å…·æœ‰ä¸´åºŠæ„ä¹‰ä¸”å¯è§£é‡Šçš„å›¾åƒç‰¹å¾ï¼Œåœ¨å¤šç§å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–åº¦é‡çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨æ£€æµ‹åŸŸå¤–æ•°æ®å’Œåº”ç”¨å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚RaDå…·æœ‰å¯è§£é‡Šæ€§ã€ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ç­‰ä¼˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œåˆ¤æ–­å›¾åƒæ˜¯å¦å±äºåŒä¸€é¢†åŸŸæ˜¯é‡è¦ä»»åŠ¡ï¼Œå…³ç³»åˆ°æ¨¡å‹æ€§èƒ½åŠç”Ÿæˆæ¨¡å‹è¾“å‡ºè´¨é‡è¯„ä¼°ã€‚</li>
<li>å½“å‰åº¦é‡æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œä¾èµ–äºä¸‹æ¸¸ä»»åŠ¡é€‰æ‹©æˆ–é‡‡ç”¨è‡ªç„¶æˆåƒçš„æ„ŸçŸ¥åº¦é‡ï¼Œä¸è¶³ä»¥æ•æ‰åŒ»å­¦å›¾åƒçš„è§£å‰–ä¸€è‡´æ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>å¼•å…¥æ–°å‹æ„ŸçŸ¥åº¦é‡æ–¹æ³•â€”â€”æ”¾å°„å­¦ç‰¹å¾è·ç¦»ï¼ˆRaDï¼‰ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒã€‚</li>
<li>RaDåˆ©ç”¨æ ‡å‡†åŒ–ã€å…·æœ‰ä¸´åºŠæ„ä¹‰ä¸”å¯è§£é‡Šçš„å›¾åƒç‰¹å¾ï¼Œåœ¨åŸŸå¤–æ£€æµ‹å®éªŒä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>RaDåœ¨å…¶ä»–æ„ŸçŸ¥åº¦é‡ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€è§£å‰–ä¸€è‡´æ€§å’ŒçœŸå®æ€§æœ‰æ›´å¼ºç›¸å…³æ€§ï¼Œé€‚ç”¨äºå›¾åƒè½¬æ¢ä»»åŠ¡ã€‚</li>
<li>RaDåœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆè¯„ä¼°ä¸­åŒæ ·å…·æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>RaDå…·æœ‰å¯è§£é‡Šæ€§ã€ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ç­‰é¢å¤–ä¼˜ç‚¹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8d1a1d66d5d318a5a9222abdf93232a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2a8b236c47f9d2e863d61d50575087f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d041cda8d50e86ab1e28b831b52b4919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3477e64f58d2624dd585fdfaf8d481f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f8034b2e25e1d441950312da18ce076.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ec6cda0e32f3010d4bde75364a27408.jpg" align="middle">
</details>




<h2 id="MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model"><a href="#MFTF-Mask-free-Training-free-Object-Level-Layout-Control-Diffusion-Model" class="headerlink" title="MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model"></a>MFTF: Mask-free Training-free Object Level Layout Control Diffusion   Model</h2><p><strong>Authors:Shan Yang</strong></p>
<p>Text-to-image generation models have become transformative tools. However, diffusion-based vision language models still lack the ability to precisely control the shape, appearance, and positional placement of objects in generated images using text guidance alone. Global image editing models typically achieve global layout control by relying on additional masks or images as guidance, which often require model training. Although local object-editing models enable modification of object shapes, they do not provide control over the positional placement of these objects. To address these limitations, we propose the MFTF model, which enables precise control over object positioning without requiring additional masks or images. The MFTF model supports both single-object and multi-object positional control (such as translation, rotation, etc.) and allows for concurrent layout control and object semantic editing. This is achieved by controlling the denoising process of the diffusion model through parallel denoising. Attention masks are dynamically generated from the cross-attention layers of the source diffusion model and applied to queries from the self-attention layers to isolate objects. These queries are then modified according to layout control parameters and injected back into the self-attention layers of the target diffusion model to enable precise positional control. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å·²æˆä¸ºå˜é©æ€§å·¥å…·ã€‚ç„¶è€Œï¼ŒåŸºäºæ‰©æ•£çš„è§†è¯­è¨€æ¨¡å‹ä»ç„¶ç¼ºä¹ä»…é€šè¿‡æ–‡æœ¬æŒ‡å¯¼ç²¾ç¡®æ§åˆ¶ç”Ÿæˆå›¾åƒä¸­ç‰©ä½“çš„å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ”¾ç½®çš„èƒ½åŠ›ã€‚å…¨å±€å›¾åƒç¼–è¾‘æ¨¡å‹é€šå¸¸é€šè¿‡ä¾èµ–é¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒä½œä¸ºæŒ‡å¯¼æ¥å®ç°å…¨å±€å¸ƒå±€æ§åˆ¶ï¼Œè¿™é€šå¸¸éœ€è¦æ¨¡å‹è®­ç»ƒã€‚è™½ç„¶å±€éƒ¨å¯¹è±¡ç¼–è¾‘æ¨¡å‹èƒ½å¤Ÿå®ç°å¯¹è±¡å½¢çŠ¶çš„ä¿®æ”¹ï¼Œä½†å®ƒå¹¶ä¸æä¾›å¯¹è±¡ä½ç½®æ”¾ç½®çš„æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†MFTFæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢å¤–è’™ç‰ˆæˆ–å›¾åƒçš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹ç‰©ä½“ä½ç½®çš„ç²¾ç¡®æ§åˆ¶ã€‚MFTFæ¨¡å‹æ”¯æŒå•ç‰©ä½“å’Œå¤šç‰©ä½“çš„ä½ç½®æ§åˆ¶ï¼ˆå¦‚å¹³ç§»ã€æ—‹è½¬ç­‰ï¼‰ï¼Œå¹¶å…è®¸åŒæ—¶è¿›è¡Œå¸ƒå±€æ§åˆ¶å’Œç‰©ä½“è¯­ä¹‰ç¼–è¾‘ã€‚è¿™æ˜¯é€šè¿‡æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹å®ç°çš„ï¼Œå…·ä½“é€šè¿‡å¹¶è¡Œå»å™ªæ¥å®ç°ã€‚æ³¨æ„åŠ›è’™ç‰ˆæ˜¯æ ¹æ®æºæ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å±‚åŠ¨æ€ç”Ÿæˆçš„ï¼Œå¹¶åº”ç”¨äºæ¥è‡ªè‡ªæ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢ä»¥éš”ç¦»ç‰©ä½“ã€‚ç„¶åï¼Œè¿™äº›æŸ¥è¯¢æ ¹æ®å¸ƒå±€æ§åˆ¶å‚æ•°è¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ³¨å…¥åˆ°ç›®æ ‡æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»¥å®ç°ç²¾ç¡®çš„ä½ç½®æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01284v1">PDF</a> 9 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹æˆä¸ºå˜é©æ€§å·¥å…·ï¼Œä½†åŸºäºæ‰©æ•£çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼æ—¶ï¼Œç¼ºä¹ç²¾ç¡®æ§åˆ¶ç”Ÿæˆå›¾åƒä¸­ç‰©ä½“çš„å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ”¾ç½®çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºMFTFæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ— éœ€é¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒï¼Œå°±èƒ½å®ç°å¯¹ç‰©ä½“ä½ç½®çš„ç²¾ç¡®æ§åˆ¶ï¼Œå¹¶æ”¯æŒå•ç‰©ä½“å’Œå¤šç‰©ä½“çš„ä½ç½®æ§åˆ¶ï¼ˆå¦‚å¹³ç§»ã€æ—‹è½¬ç­‰ï¼‰ï¼Œå¹¶å…è®¸åŒæ—¶è¿›è¡Œå¸ƒå±€æ§åˆ¶å’Œç‰©ä½“è¯­ä¹‰ç¼–è¾‘ã€‚è¿™æ˜¯é€šè¿‡æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹å®ç°çš„å¹¶è¡Œå»å™ªã€‚åŠ¨æ€ç”Ÿæˆæ³¨æ„åŠ›è’™ç‰ˆå¹¶åº”ç”¨äºæŸ¥è¯¢ï¼Œæ ¹æ®å¸ƒå±€æ§åˆ¶å‚æ•°ä¿®æ”¹æŸ¥è¯¢ï¼Œç„¶åæ³¨å…¥ç›®æ ‡æ‰©æ•£æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›å±‚ä»¥å®ç°ç²¾ç¡®çš„ä½ç½®æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å·²æˆä¸ºé‡è¦çš„å·¥å…·ï¼Œä½†æ‰©æ•£æ¨¡å‹åœ¨ä½¿ç”¨æ–‡æœ¬æŒ‡å¯¼æ—¶å­˜åœ¨å¯¹ç‰©ä½“å½¢çŠ¶ã€å¤–è§‚å’Œä½ç½®æ§åˆ¶çš„å±€é™ã€‚</li>
<li>MFTFæ¨¡å‹è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œå…è®¸å¯¹ç‰©ä½“çš„ä½ç½®è¿›è¡Œç²¾ç¡®æ§åˆ¶ï¼Œæ— éœ€é¢å¤–çš„è’™ç‰ˆæˆ–å›¾åƒã€‚</li>
<li>MFTFæ¨¡å‹æ”¯æŒå•ç‰©ä½“å’Œå¤šç‰©ä½“çš„ä½ç½®æ§åˆ¶ï¼Œå¦‚å¹³ç§»å’Œæ—‹è½¬ã€‚</li>
<li>MFTFæ¨¡å‹å¯ä»¥åŒæ—¶è¿›è¡Œå¸ƒå±€æ§åˆ¶å’Œç‰©ä½“è¯­ä¹‰ç¼–è¾‘ã€‚</li>
<li>MFTFæ¨¡å‹é€šè¿‡æ§åˆ¶æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹å®ç°ç²¾ç¡®ä½ç½®æ§åˆ¶ï¼Œé‡‡ç”¨å¹¶è¡Œå»å™ªæŠ€æœ¯ã€‚</li>
<li>æ³¨æ„åŠ›è’™ç‰ˆä»æºæ‰©æ•£æ¨¡å‹çš„äº¤å‰æ³¨æ„åŠ›å±‚åŠ¨æ€ç”Ÿæˆï¼Œå¹¶åº”ç”¨äºè‡ªæˆ‘æ³¨æ„åŠ›å±‚çš„æŸ¥è¯¢åˆ°ä»¥å®ç°ä½ç½®æ§åˆ¶ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f5307dede9c39a5e6238a4553cd509b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-92df01ad425ce71e6a3a7789281fc8d0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-31d932f273200085ffd85d5c55bc2acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb08341d38f99b43d2cb48d414f401d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-452bad3796827f922621781276fed702.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7024f4fd43f65c0109fc421d30a29356.jpg" align="middle">
</details>




<h2 id="Multi-resolution-Guided-3D-GANs-for-Medical-Image-Translation"><a href="#Multi-resolution-Guided-3D-GANs-for-Medical-Image-Translation" class="headerlink" title="Multi-resolution Guided 3D GANs for Medical Image Translation"></a>Multi-resolution Guided 3D GANs for Medical Image Translation</h2><p><strong>Authors:Juhyung Ha, Jong Sung Park, David Crandall, Eleftherios Garyfallidis, Xuhong Zhang</strong></p>
<p>Medical image translation is the process of converting from one imaging modality to another, in order to reduce the need for multiple image acquisitions from the same patient. This can enhance the efficiency of treatment by reducing the time, equipment, and labor needed. In this paper, we introduce a multi-resolution guided Generative Adversarial Network (GAN)-based framework for 3D medical image translation. Our framework uses a 3D multi-resolution Dense-Attention UNet (3D-mDAUNet) as the generator and a 3D multi-resolution UNet as the discriminator, optimized with a unique combination of loss functions including voxel-wise GAN loss and 2.5D perception loss. Our approach yields promising results in volumetric image quality assessment (IQA) across a variety of imaging modalities, body regions, and age groups, demonstrating its robustness. Furthermore, we propose a synthetic-to-real applicability assessment as an additional evaluation to assess the effectiveness of synthetic data in downstream applications such as segmentation. This comprehensive evaluation shows that our method produces synthetic medical images not only of high-quality but also potentially useful in clinical applications. Our code is available at github.com&#x2F;juhha&#x2F;3D-mADUNet. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè½¬æ¢æ˜¯å°†ä¸€ç§æˆåƒæ¨¡å¼è½¬æ¢ä¸ºå¦ä¸€ç§æˆåƒæ¨¡å¼çš„è¿‡ç¨‹ï¼Œä»¥å‡å°‘å¯¹åŒä¸€æ‚£è€…å¤šæ¬¡é‡‡é›†å›¾åƒçš„éœ€æ±‚ã€‚è¿™å¯ä»¥é€šè¿‡å‡å°‘æ‰€éœ€çš„æ—¶é—´ã€è®¾å¤‡å’ŒåŠ³åŠ¨åŠ›æ¥æé«˜æ²»ç–—æ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºå¤šåˆ†è¾¨ç‡å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„3DåŒ»å­¦å½±åƒè½¬æ¢æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨3Då¤šåˆ†è¾¨ç‡å¯†é›†æ³¨æ„åŠ›UNetï¼ˆ3D-mDAUNetï¼‰ä½œä¸ºç”Ÿæˆå™¨ï¼Œä½¿ç”¨3Då¤šåˆ†è¾¨ç‡UNetä½œä¸ºåˆ¤åˆ«å™¨ï¼Œå¹¶é€šè¿‡åŒ…æ‹¬åƒç´ çº§GANæŸå¤±å’Œ2.5Dæ„ŸçŸ¥æŸå¤±çš„ç‹¬ç‰¹ç»„åˆæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨å¤šç§æˆåƒæ¨¡å¼ã€èº«ä½“éƒ¨ä½å’Œå¹´é¾„ç»„çš„ä½“ç§¯å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰ä¸­å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆæˆåˆ°ç°å®çš„é€‚ç”¨æ€§è¯„ä¼°ä½œä¸ºé¢å¤–çš„è¯„ä¼°ï¼Œä»¥è¯„ä¼°åˆæˆæ•°æ®åœ¨åˆ†å‰²ç­‰ä¸‹æ¸¸åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚å…¨é¢çš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„åˆæˆåŒ»å­¦å½±åƒä¸ä»…è´¨é‡é«˜ï¼Œè€Œä¸”åœ¨ä¸´åºŠåº”ç”¨ä¸­ä¹Ÿå…·æœ‰æ½œåœ¨ç”¨å¤„ã€‚æˆ‘ä»¬çš„ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/juhha/3D-mADUNet">é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00575v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤šåˆ†è¾¨ç‡å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„3DåŒ»å­¦å›¾åƒè½¬æ¢æ¡†æ¶ï¼Œä½¿ç”¨3Då¤šåˆ†è¾¨ç‡Dense-Attention UNetä½œä¸ºç”Ÿæˆå™¨ï¼Œ3Då¤šåˆ†è¾¨ç‡UNetä½œä¸ºåˆ¤åˆ«å™¨ï¼Œé€šè¿‡ç»“åˆå¤šç§æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬ä½“ç´ çº§çš„GANæŸå¤±å’Œ2.5Dæ„ŸçŸ¥æŸå¤±ã€‚è¯¥æ¡†æ¶åœ¨å¤šç§æˆåƒæ¨¡æ€ã€èº«ä½“åŒºåŸŸå’Œå¹´é¾„ç»„çš„ä½“ç§¯å›¾åƒè´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç»“æœï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä»åˆæˆåˆ°å®é™…åº”ç”¨çš„é€‚ç”¨æ€§è¯„ä¼°ï¼Œä»¥è¯„ä¼°åˆæˆæ•°æ®åœ¨åˆ†å‰²ç­‰ä¸‹æ¸¸åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—å›¾åƒç¿»è¯‘æ˜¯é€šè¿‡ä¸€ç§æˆåƒæ–¹å¼è½¬æ¢ä¸ºå¦ä¸€ç§æˆåƒæ–¹å¼çš„è¿‡ç¨‹ï¼Œä»¥å‡å°‘å¯¹åŒä¸€ç—…äººè¿›è¡Œå¤šæ¬¡å›¾åƒé‡‡é›†çš„éœ€è¦ï¼Œæé«˜æ²»ç–—æ•ˆç‡ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†ä¸€ç§åŸºäºå¤šåˆ†è¾¨ç‡å¼•å¯¼ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„3DåŒ»å­¦å›¾åƒè½¬æ¢æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨3Då¤šåˆ†è¾¨ç‡Dense-Attention UNetä½œä¸ºç”Ÿæˆå™¨ï¼Œä»¥åŠä¸€ä¸ªä¼˜åŒ–çš„3Då¤šåˆ†è¾¨ç‡UNetä½œä¸ºåˆ¤åˆ«å™¨ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å¤šç§æŸå¤±å‡½æ•°ï¼ŒåŒ…æ‹¬ä½“ç´ çº§çš„GANæŸå¤±å’Œ2.5Dæ„ŸçŸ¥æŸå¤±ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§æˆåƒæ¨¡æ€ã€èº«ä½“åŒºåŸŸå’Œå¹´é¾„ç»„çš„ä½“ç§¯å›¾åƒè´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„ç»“æœï¼Œè¯æ˜äº†å…¶ç¨³å¥æ€§ã€‚</li>
<li>é™¤äº†å›¾åƒè´¨é‡è¯„ä¼°ï¼Œè¿˜è¿›è¡Œäº†åˆæˆåˆ°å®é™…åº”ç”¨çš„é€‚ç”¨æ€§è¯„ä¼°ï¼Œä»¥è¯„ä¼°åˆæˆæ•°æ®åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e709ee79d8f838575b8d877af7e59a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a771e2a8610d752cf67f48a7f32d7e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9765ed42d6c70a76a95b7898ddc9d5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf30a65bff9b16a9d474abd103adfdca.jpg" align="middle">
</details>




<h2 id="Homeostasis-and-Sparsity-in-Transformer"><a href="#Homeostasis-and-Sparsity-in-Transformer" class="headerlink" title="Homeostasis and Sparsity in Transformer"></a>Homeostasis and Sparsity in Transformer</h2><p><strong>Authors:Leonid Kotyuzanskiy, Artem Klimov</strong></p>
<p>The transformer architecture has become an integral part of the field of modern neural networks, playing a crucial role in a variety of tasks, such as text generation, machine translation, image and audio processing, among others. There is also an alternative approach to building intelligent systems, proposed by Jeff Hawkins and inspired by the processes occurring in the neocortex. In our article we want to combine some of these ideas and to propose the use of homeostasis mechanisms, such as RFB-kWTA and â€œSmartâ€ Inhibition, in the attention mechanism of the transformer and at the output of the transformer block, as well as conducting an experiment involving the introduction of sparse distributed representations of the transformer at various points. RFB-kWTA utilizes statistics of layer activations across time to adjust the entire layer, enhancing the values of rare activations while reducing those of frequent ones. â€œSmartâ€ Inhibition also uses activation statistics to sample sparsity masks, with rarer activation times are more likely to be activated. Our proposed mechanisms significantly outperform the classical transformer 0.2768 BLEU and a model that only makes use of dropout in the attention mechanism and output of the transformer block 0.3007 BLEU, achieving a score of 0.3062 on the Multi30K dataset. </p>
<blockquote>
<p>Transformeræ¶æ„å·²æˆä¸ºç°ä»£ç¥ç»ç½‘ç»œé¢†åŸŸä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€å›¾åƒå’ŒéŸ³é¢‘å¤„ç†ç­‰å„ç§ä»»åŠ¡ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚Jeff Hawkinsæå‡ºäº†ä¸€ç§å»ºç«‹æ™ºèƒ½ç³»ç»Ÿçš„æ›¿ä»£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å—åˆ°å¤§è„‘æ–°çš®å±‚ä¸­å‘ç”Ÿè¿‡ç¨‹çš„å¯å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æƒ³ç»“åˆè¿™äº›æ€æƒ³ï¼Œå¹¶æå‡ºåœ¨Transformerçš„æ³¨æ„æœºåˆ¶å’ŒTransformerå—è¾“å‡ºä¸­ä½¿ç”¨ç¨³æ€æœºåˆ¶ï¼Œå¦‚RFB-kWTAå’Œâ€œæ™ºèƒ½â€æŠ‘åˆ¶ï¼Œå¹¶è¿›è¡Œä¸€é¡¹å®éªŒï¼Œåœ¨Transformerçš„å„ä¸ªç‚¹å¼•å…¥ç¨€ç–åˆ†å¸ƒå¼è¡¨ç¤ºã€‚RFB-kWTAåˆ©ç”¨è·¨æ—¶é—´çš„å±‚æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯æ¥è°ƒæ•´æ•´ä¸ªå±‚ï¼Œå¢å¼ºç¨€æœ‰æ¿€æ´»çš„å€¼ï¼ŒåŒæ—¶å‡å°‘é¢‘ç¹å‡ºç°çš„æ¿€æ´»ã€‚â€œæ™ºèƒ½â€æŠ‘åˆ¶è¿˜ä½¿ç”¨æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯æ¥é‡‡æ ·ç¨€ç–æ©ç ï¼Œè¾ƒå°‘æ¿€æ´»çš„æ—¶é—´æ›´æœ‰å¯èƒ½è¢«æ¿€æ´»ã€‚æˆ‘ä»¬æå‡ºçš„æœºåˆ¶åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç»å…¸Transformerï¼ˆBLEUå¾—åˆ†ä¸º0.2768ï¼‰å’Œä»…ä½¿ç”¨dropoutçš„æ¨¡å‹ï¼ˆåœ¨Transformerçš„æ³¨æ„æœºåˆ¶å’Œè¾“å‡ºå—ä¸­çš„BLEUå¾—åˆ†ä¸º0.3007ï¼‰ï¼Œå–å¾—äº†0.3062çš„å¾—åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00503v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ç»“åˆæ™ºèƒ½ç³»ç»Ÿæ„å»ºæ–°æ€è·¯çš„å˜é©è€…æ¶æ„æ–¹æ³•ï¼Œåˆ©ç”¨å†…ç¨³æ€æœºåˆ¶ï¼Œå¦‚RFB-kWTAå’Œâ€œæ™ºèƒ½â€æŠ‘åˆ¶ï¼Œæ”¹è¿›äº†å˜é©è€…çš„æ³¨æ„åŠ›æœºåˆ¶å’Œè¾“å‡ºå—ã€‚å®éªŒè¯æ˜ï¼Œæ–°æ–¹æ³•åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå˜é©è€…å’Œä»…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œè¾“å‡ºå—dropoutçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜é©è€…æ¶æ„åœ¨ç°ä»£ç¥ç»ç½‘ç»œé¢†åŸŸæ‰®æ¼”é‡è¦è§’è‰²ï¼Œç”¨äºå¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€å›¾åƒå’ŒéŸ³é¢‘å¤„ç†ç­‰ã€‚</li>
<li>ä»‹ç»äº†ç»“åˆJeff Hawkinsçš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºæ€è·¯çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºåœ¨å˜é©è€…çš„æ³¨æ„åŠ›æœºåˆ¶å’Œè¾“å‡ºå—ä¸­ä½¿ç”¨å†…ç¨³æ€æœºåˆ¶ï¼Œå¦‚RFB-kWTAå’Œâ€œæ™ºèƒ½â€æŠ‘åˆ¶ã€‚</li>
<li>RFB-kWTAé€šè¿‡ç»Ÿè®¡å±‚æ¿€æ´»çš„æ—¶é—´å˜åŒ–æ¥è°ƒæ•´æ•´ä¸ªå±‚ï¼Œå¢å¼ºç¨€æœ‰æ¿€æ´»å€¼å¹¶å‡å°‘é¢‘ç¹å‡ºç°çš„æ¿€æ´»å€¼ã€‚</li>
<li>â€œæ™ºèƒ½â€æŠ‘åˆ¶åˆ©ç”¨æ¿€æ´»ç»Ÿè®¡æ¥é‡‡æ ·ç¨€ç–æ©ç ï¼Œç¨€æœ‰æ¿€æ´»æ—¶é—´æ›´æœ‰å¯èƒ½è¢«æ¿€æ´»ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ‰€ææœºåˆ¶åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå˜é©è€…å’Œä»…ä½¿ç”¨dropoutçš„æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-464b7bdc768c18ed0368f93c8a19d241.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb052c99b60485f9fd946a2267a6e26a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3481cb094858f38f2ba6a28f8fecc3a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98803d2e214ea4516794dc5599788f9d.jpg" align="middle">
</details>




<h2 id="T2Vid-Translating-Long-Text-into-Multi-Image-is-the-Catalyst-for-Video-LLMs"><a href="#T2Vid-Translating-Long-Text-into-Multi-Image-is-the-Catalyst-for-Video-LLMs" class="headerlink" title="T2Vid: Translating Long Text into Multi-Image is the Catalyst for   Video-LLMs"></a>T2Vid: Translating Long Text into Multi-Image is the Catalyst for   Video-LLMs</h2><p><strong>Authors:Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, Enhong Chen</strong></p>
<p>The success of Multimodal Large Language Models (MLLMs) in the image domain has garnered wide attention from the research community. Drawing on previous successful experiences, researchers have recently explored extending the success to the video understanding realms. Apart from training from scratch, an efficient way is to utilize the pre-trained image-LLMs, leading to two mainstream approaches, i.e. zero-shot inference and further fine-tuning with video data. In this work, our study of these approaches harvests an effective data augmentation method. We first make a deeper inspection of the zero-shot inference way and identify two limitations, i.e. limited generalization and lack of temporal understanding capabilities. Thus, we further investigate the fine-tuning approach and find a low learning efficiency when simply using all the video data samples, which can be attributed to a lack of instruction diversity. Aiming at this issue, we develop a method called T2Vid to synthesize video-like samples to enrich the instruction diversity in the training corpus. Integrating these data enables a simple and efficient training scheme, which achieves performance comparable to or even superior to using full video datasets by training with just 15% the sample size. Meanwhile, we find that the proposed scheme can boost the performance of long video understanding without training with long video samples. We hope our study will spark more thinking about using MLLMs for video understanding and curation of high-quality data. The code is released at <a target="_blank" rel="noopener" href="https://github.com/xjtupanda/T2Vid">https://github.com/xjtupanda/T2Vid</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒé¢†åŸŸçš„æˆåŠŸå¼•èµ·äº†ç ”ç©¶ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚åŸºäºä¹‹å‰çš„æˆåŠŸç»éªŒï¼Œç ”ç©¶äººå‘˜æœ€è¿‘å¼€å§‹æ¢ç´¢å°†å…¶æˆåŠŸæ‰©å±•åˆ°è§†é¢‘ç†è§£é¢†åŸŸã€‚é™¤äº†ä»å¤´å¼€å§‹è®­ç»ƒä¹‹å¤–ï¼Œä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ˜¯åˆ©ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„å›¾åƒLLMï¼Œä»è€Œäº§ç”Ÿäº†ä¸¤ç§ä¸»æµæ–¹æ³•ï¼Œå³é›¶æ ·æœ¬æ¨æ–­å’Œè¿›ä¸€æ­¥ä½¿ç”¨è§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•çš„ç ”ç©¶æ”¶è·äº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆå¯¹é›¶æ ·æœ¬æ¨æ–­æ–¹æ³•è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶å‘ç°äº†ä¸¤ä¸ªå±€é™æ€§ï¼Œå³æœ‰é™çš„æ³›åŒ–èƒ½åŠ›å’Œç¼ºä¹æ—¶é—´ç†è§£èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶äº†å¾®è°ƒæ–¹æ³•ï¼Œå¹¶å‘ç°ä»…ä½¿ç”¨æ‰€æœ‰è§†é¢‘æ•°æ®æ ·æœ¬æ—¶å­˜åœ¨å­¦ä¹ æ•ˆç‡ä½çš„é—®é¢˜ï¼Œè¿™å¯ä»¥å½’å› äºæŒ‡ä»¤å¤šæ ·æ€§çš„ç¼ºä¹ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºT2Vidçš„æ–¹æ³•æ¥åˆæˆç±»ä¼¼è§†é¢‘æ ·æœ¬ï¼Œä»¥ä¸°å¯Œè®­ç»ƒè¯­æ–™åº“ä¸­çš„æŒ‡ä»¤å¤šæ ·æ€§ã€‚æ•´åˆè¿™äº›æ•°æ®å®ç°äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä»…ä½¿ç”¨15%çš„æ ·æœ¬é‡å°±å¯ä»¥è¾¾åˆ°æˆ–è¶…è¿‡ä½¿ç”¨å®Œæ•´è§†é¢‘æ•°æ®é›†çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°æ‰€æå‡ºçš„æ–¹æ¡ˆå¯ä»¥åœ¨ä¸ä½¿ç”¨é•¿è§†é¢‘æ ·æœ¬çš„æƒ…å†µä¸‹æé«˜é•¿è§†é¢‘ç†è§£æ€§èƒ½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½å¤Ÿæ¿€å‘æ›´å¤šå…³äºä½¿ç”¨MLLMè¿›è¡Œè§†é¢‘ç†è§£å’Œé«˜è´¨é‡æ•°æ®ç­›é€‰çš„æ€è€ƒã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/xjtupanda/T2Vid%E3%80%82">https://github.com/xjtupanda/T2Vidã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19951v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/xjtupanda/T2Vid">https://github.com/xjtupanda/T2Vid</a></p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒé¢†åŸŸçš„æˆåŠŸå¼•èµ·äº†ç ”ç©¶ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚æœ€è¿‘ï¼Œç ”ç©¶è€…ä»¬å°è¯•å°†è¿™ç§æˆåŠŸæ‰©å±•åˆ°è§†é¢‘ç†è§£é¢†åŸŸã€‚æœ¬æ–‡ç ”ç©¶äº†ä¸¤ç§ä¸»æµæ–¹æ³•ï¼Œå³é›¶æ ·æœ¬æ¨ç†å’Œé€šè¿‡è§†é¢‘æ•°æ®è¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ï¼Œå¹¶æ®æ­¤æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚é’ˆå¯¹é›¶æ ·æœ¬æ¨ç†æ–¹æ³•çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºT2Vidçš„æ–¹æ³•ï¼Œé€šè¿‡åˆæˆç±»ä¼¼è§†é¢‘æ ·æœ¬æ¥ä¸°å¯Œè®­ç»ƒè¯­æ–™åº“ä¸­çš„æŒ‡ä»¤å¤šæ ·æ€§ã€‚æ­¤æ–¹æ³•èƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨15%æ ·æœ¬é‡çš„æƒ…å†µä¸‹å®ç°ä¸å…¨è§†é¢‘æ•°æ®é›†è®­ç»ƒç›¸å½“ç”šè‡³æ›´ä¼˜çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶å‘ç°åœ¨å¯¹é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ï¼Œå³ä½¿ä¸ä½¿ç”¨é•¿è§†é¢‘æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæ­¤æ–¹æ³•ä¹Ÿèƒ½æå‡æ€§èƒ½ã€‚æœ¬ç ”ç©¶å¸Œæœ›æ¿€å‘æ›´å¤šå…³äºåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè§†é¢‘ç†è§£å’Œé«˜è´¨é‡æ•°æ®æ”¶é›†çš„æ€è€ƒã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒé¢†åŸŸçš„æˆåŠŸå¸å¼•äº†ç ”ç©¶è€…çš„æ³¨æ„ï¼Œç›®å‰æ­£åœ¨æ¢ç´¢å°†å…¶åº”ç”¨äºè§†é¢‘ç†è§£é¢†åŸŸã€‚</li>
<li>ç ”ç©¶äººå‘˜ä¸»è¦å°è¯•ä¸¤ç§æ–¹æ³•ï¼šé›¶æ ·æœ¬æ¨ç†å’Œå¾®è°ƒæ–¹æ³•ï¼Œåè€…é‡‡ç”¨é¢„è®­ç»ƒçš„å›¾åƒLLMsè¿›è¡Œåº”ç”¨æ‹“å±•ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†é›¶æ ·æœ¬æ¨ç†æ–¹æ³•çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬æœ‰é™çš„æ³›åŒ–èƒ½åŠ›å’Œç¼ºä¹æ—¶é—´ç†è§£èƒ½åŠ›ã€‚</li>
<li>é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§åä¸ºT2Vidçš„æœ‰æ•ˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œé€šè¿‡åˆæˆç±»ä¼¼è§†é¢‘æ ·æœ¬ä¸°å¯Œè®­ç»ƒè¯­æ–™åº“ä¸­çš„æŒ‡ä»¤å¤šæ ·æ€§ã€‚</li>
<li>T2Vidæ–¹æ³•åœ¨å‡å°‘æ ·æœ¬ä½¿ç”¨çš„åŒæ—¶ï¼Œèƒ½è¾¾åˆ°ä¸ä½¿ç”¨å…¨è§†é¢‘æ•°æ®é›†ç›¸å½“çš„è®­ç»ƒæ•ˆæœï¼Œå±•ç°äº†å…¶åœ¨æå‡æ•ˆç‡å’Œæé«˜æ€§èƒ½æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶è¿˜è¡¨æ˜ï¼ŒT2Vidå¯¹äºæé«˜é•¿è§†é¢‘ç†è§£çš„æ€§èƒ½æœ‰æ˜æ˜¾å¸®åŠ©ï¼Œç”šè‡³åœ¨æœªä½¿ç”¨é•¿è§†é¢‘æ ·æœ¬è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fd3152a70941f31efd09ed5366f35602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c93eef8a0a964b2c3380e1418938d378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78191391371c0faec91497143685b687.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-388b3cda5d5f57293ab78e3bad60d168.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7749d77a7a9353f7b78694f70bb3dfa4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63cf77f9f85e6ccf7c70b16b2bdc7913.jpg" align="middle">
</details>




<h2 id="Latent-Schrodinger-Bridge-Prompting-Latent-Diffusion-for-Fast-Unpaired-Image-to-Image-Translation"><a href="#Latent-Schrodinger-Bridge-Prompting-Latent-Diffusion-for-Fast-Unpaired-Image-to-Image-Translation" class="headerlink" title="Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired   Image-to-Image Translation"></a>Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired   Image-to-Image Translation</h2><p><strong>Authors:Jeongsol Kim, Beomsu Kim, Jong Chul Ye</strong></p>
<p>Diffusion models (DMs), which enable both image generation from noise and inversion from data, have inspired powerful unpaired image-to-image (I2I) translation algorithms. However, they often require a larger number of neural function evaluations (NFEs), limiting their practical applicability. In this paper, we tackle this problem with Schrodinger Bridges (SBs), which are stochastic differential equations (SDEs) between distributions with minimal transport cost. We analyze the probability flow ordinary differential equation (ODE) formulation of SBs, and observe that we can decompose its vector field into a linear combination of source predictor, target predictor, and noise predictor. Inspired by this observation, we propose Latent Schrodinger Bridges (LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and develop appropriate prompt optimization and change of variables formula to match the training and inference between distributions. We demonstrate that our algorithm successfully conduct competitive I2I translation in unsupervised setting with only a fraction of computation cost required by previous DM-based I2I methods. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æ—¢èƒ½å¤Ÿç”Ÿæˆå›¾åƒï¼Œåˆèƒ½å¤Ÿå®ç°å¯¹æ•°æ®çš„åè½¬ï¼Œä¸ºå¼ºå¤§çš„æ— é…å¯¹å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢ç®—æ³•æä¾›äº†çµæ„Ÿã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰ï¼Œä»è€Œé™åˆ¶äº†å…¶å®é™…åº”ç”¨çš„å¯è¡Œæ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨è–›å®šè°”æ¡¥ï¼ˆSBsï¼‰æ¥è§£å†³ï¼Œå®ƒæ˜¯åˆ†å¸ƒä¹‹é—´çš„éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰ï¼Œå…·æœ‰æœ€å°çš„ä¼ è¾“æˆæœ¬ã€‚æˆ‘ä»¬åˆ†æäº†SBsçš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰å…¬å¼ï¼Œå¹¶è§‚å¯Ÿåˆ°å¯ä»¥å°†å…¶å‘é‡åœºåˆ†è§£ä¸ºæºé¢„æµ‹å™¨ã€ç›®æ ‡é¢„æµ‹å™¨å’Œå™ªå£°é¢„æµ‹å™¨çš„çº¿æ€§ç»„åˆã€‚å—æ­¤è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨è–›å®šè°”æ¡¥ï¼ˆLSBsï¼‰ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¥è¿‘ä¼¼SBçš„ODEï¼Œå¹¶å¼€å‘é€‚å½“çš„æç¤ºä¼˜åŒ–å’Œå˜é‡å˜æ¢å…¬å¼ï¼Œä»¥åŒ¹é…åˆ†å¸ƒä¹‹é—´çš„è®­ç»ƒå’Œæ¨ç†ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨æ— ç›‘ç£è®¾ç½®ä¸‹æˆåŠŸå®ç°äº†æœ‰ç«äº‰åŠ›çš„I2Iè½¬æ¢ï¼Œä¸”è®¡ç®—æˆæœ¬ä»…ä¸ºå…ˆå‰åŸºäºDMçš„I2Iæ–¹æ³•æ‰€éœ€çš„ä¸€å°éƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14863v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰åœ¨å›¾åƒç”Ÿæˆå’Œåè½¬ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºå…¶éœ€è¦æ›´å¤šçš„ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰é™åˆ¶äº†å®é™…åº”ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« å¼•å…¥Schrodinger Bridgesï¼ˆSBsï¼‰é€šè¿‡æœ€å°ä¼ è¾“æˆæœ¬çš„åˆ†å¸ƒé—´éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰ã€‚é€šè¿‡åˆ†æSBsçš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰å…¬å¼ï¼Œæ–‡ç« æå‡ºäº†Latent Schrodinger Bridgesï¼ˆLSBsï¼‰ï¼Œå®ƒé€šè¿‡é¢„è®­ç»ƒçš„Stable Diffusionè¿‘ä¼¼SB ODEï¼Œå¹¶é€šè¿‡é€‚å½“çš„æç¤ºä¼˜åŒ–å’Œå˜é‡å˜æ¢å…¬å¼åŒ¹é…åˆ†å¸ƒä¹‹é—´çš„è®­ç»ƒå’Œæ¨ç†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— ç›‘ç£è®¾ç½®ä¸‹çš„I2Iç¿»è¯‘ä»»åŠ¡è¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®¡ç®—æˆæœ¬ä»…ä¸ºä¹‹å‰DM-based I2Iæ–¹æ³•çš„ä¸€å°éƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å¯ç”¨äºå›¾åƒç”Ÿæˆå’Œåè½¬ä»»åŠ¡ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤šçš„ç¥ç»ç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆNFEsï¼‰ã€‚</li>
<li>Schrodinger Bridgesï¼ˆSBsï¼‰é€šè¿‡æœ€å°ä¼ è¾“æˆæœ¬çš„åˆ†å¸ƒé—´éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEsï¼‰è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>æ–‡ç« åˆ†æäº†SBsçš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰å…¬å¼ï¼Œå¹¶å°†å…¶å‘é‡åœºåˆ†è§£ä¸ºæºé¢„æµ‹å™¨ã€ç›®æ ‡é¢„æµ‹å™¨å’Œå™ªå£°é¢„æµ‹å™¨çš„çº¿æ€§ç»„åˆã€‚</li>
<li>åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæå‡ºäº†Latent Schrodinger Bridgesï¼ˆLSBsï¼‰ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„Stable Diffusionè¿‘ä¼¼SB ODEã€‚</li>
<li>LSBsé€šè¿‡é€‚å½“çš„æç¤ºä¼˜åŒ–å’Œå˜é‡å˜æ¢å…¬å¼åŒ¹é…åˆ†å¸ƒä¹‹é—´çš„è®­ç»ƒå’Œæ¨ç†ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ— ç›‘ç£è®¾ç½®ä¸‹çš„å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç¿»è¯‘ä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e349b0cea390d984f91163055694355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f841067017bfc663d4e2dbc677b797f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f0f0f4cf7d419c676d174ac15e31f5c.jpg" align="middle">
</details>




<h2 id="SAM-I2I-Unleash-the-Power-of-Segment-Anything-Model-for-Medical-Image-Translation"><a href="#SAM-I2I-Unleash-the-Power-of-Segment-Anything-Model-for-Medical-Image-Translation" class="headerlink" title="SAM-I2I: Unleash the Power of Segment Anything Model for Medical Image   Translation"></a>SAM-I2I: Unleash the Power of Segment Anything Model for Medical Image   Translation</h2><p><strong>Authors:Jiayu Huo, Sebastien Ourselin, Rachel Sparks</strong></p>
<p>Medical image translation is crucial for reducing the need for redundant and expensive multi-modal imaging in clinical field. However, current approaches based on Convolutional Neural Networks (CNNs) and Transformers often fail to capture fine-grain semantic features, resulting in suboptimal image quality. To address this challenge, we propose SAM-I2I, a novel image-to-image translation framework based on the Segment Anything Model 2 (SAM2). SAM-I2I utilizes a pre-trained image encoder to extract multiscale semantic features from the source image and a decoder, based on the mask unit attention module, to synthesize target modality images. Our experiments on multi-contrast MRI datasets demonstrate that SAM-I2I outperforms state-of-the-art methods, offering more efficient and accurate medical image translation. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè½¬æ¢å¯¹äºå‡å°‘ä¸´åºŠé¢†åŸŸä¸­å†—ä½™ä¸”æ˜‚è´µçš„å¤šæ¨¡æ€æˆåƒéœ€æ±‚è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›®å‰åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒTransformerçš„æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰ç²¾ç»†çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¯¼è‡´å›¾åƒè´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SAM-I2Iï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰çš„æ–°å‹å›¾åƒåˆ°å›¾åƒè½¬æ¢æ¡†æ¶ã€‚SAM-I2Iåˆ©ç”¨é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨ä»æºå›¾åƒä¸­æå–å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åŸºäºæ©è†œå•å…ƒæ³¨æ„åŠ›æ¨¡å—é‡‡ç”¨è§£ç å™¨æ¥åˆæˆç›®æ ‡æ¨¡æ€å›¾åƒã€‚æˆ‘ä»¬åœ¨å¤šå¯¹æ¯”åº¦MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-I2Iä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæä¾›äº†æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„åŒ»å­¦å½±åƒè½¬æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12755v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰çš„åŒ»å­¦å›¾åƒç¿»è¯‘æ¡†æ¶SAM-I2Iï¼Œç”¨äºå‡å°‘ä¸´åºŠé¢†åŸŸå†—ä½™æ˜‚è´µçš„å¤šæ¨¡æ€æˆåƒéœ€æ±‚ã€‚é€šè¿‡é‡‡ç”¨é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨æå–æºå›¾åƒçš„å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶ç»“åˆåŸºäºæ©è†œå•å…ƒæ³¨æ„åŠ›æ¨¡å—çš„è§£ç å™¨ï¼Œåˆæˆç›®æ ‡æ¨¡æ€å›¾åƒã€‚åœ¨å¯¹æ¯”MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-I2Iè¾ƒç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„åŒ»å­¦å›¾åƒç¿»è¯‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒç¿»è¯‘æœ‰åŠ©äºå‡å°‘ä¸´åºŠä¸­å†—ä½™å’Œæ˜‚è´µçš„å¤šæ¨¡æ€æˆåƒéœ€æ±‚ã€‚</li>
<li>å½“å‰åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒTransformerçš„æ–¹æ³•åœ¨æ•æ‰ç²¾ç»†è¯­ä¹‰ç‰¹å¾æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´å›¾åƒè´¨é‡ä¸ä½³ã€‚</li>
<li>SAM-I2Iæ˜¯ä¸€ç§æ–°å‹çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¡†æ¶ï¼ŒåŸºäºSegment Anything Model 2ï¼ˆSAM2ï¼‰ã€‚</li>
<li>SAM-I2Iä½¿ç”¨é¢„è®­ç»ƒçš„å›¾åƒç¼–ç å™¨ä»æºå›¾åƒä¸­æå–å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>SAM-I2Içš„è§£ç å™¨åŸºäºæ©è†œå•å…ƒæ³¨æ„åŠ›æ¨¡å—ï¼Œå¯ä»¥åˆæˆç›®æ ‡æ¨¡æ€å›¾åƒã€‚</li>
<li>åœ¨å¯¹æ¯”MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-I2Iè¾ƒç°æœ‰æ–¹æ³•è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7553228aab3e8f337c9a6e59725888a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f7416787b5e4a3346e2f0c08eae154c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-334ea76b9f93aac126e444bf0f9aba5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832f6d9a4adec09d258106d4ad60957e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b0f74b4966b705029814ae47db5e51f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3f789fde2cec820e6297e3019fd4a53.jpg" align="middle">
</details>




<h2 id="C-DiffSET-Leveraging-Latent-Diffusion-for-SAR-to-EO-Image-Translation-with-Confidence-Guided-Reliable-Object-Generation"><a href="#C-DiffSET-Leveraging-Latent-Diffusion-for-SAR-to-EO-Image-Translation-with-Confidence-Guided-Reliable-Object-Generation" class="headerlink" title="C-DiffSET: Leveraging Latent Diffusion for SAR-to-EO Image Translation   with Confidence-Guided Reliable Object Generation"></a>C-DiffSET: Leveraging Latent Diffusion for SAR-to-EO Image Translation   with Confidence-Guided Reliable Object Generation</h2><p><strong>Authors:Jeonghyeok Do, Jaehyup Lee, Munchurl Kim</strong></p>
<p>Synthetic Aperture Radar (SAR) imagery provides robust environmental and temporal coverage (e.g., during clouds, seasons, day-night cycles), yet its noise and unique structural patterns pose interpretation challenges, especially for non-experts. SAR-to-EO (Electro-Optical) image translation (SET) has emerged to make SAR images more perceptually interpretable. However, traditional approaches trained from scratch on limited SAR-EO datasets are prone to overfitting. To address these challenges, we introduce Confidence Diffusion for SAR-to-EO Translation, called C-DiffSET, a framework leveraging pretrained Latent Diffusion Model (LDM) extensively trained on natural images, thus enabling effective adaptation to the EO domain. Remarkably, we find that the pretrained VAE encoder aligns SAR and EO images in the same latent space, even with varying noise levels in SAR inputs. To further improve pixel-wise fidelity for SET, we propose a confidence-guided diffusion (C-Diff) loss that mitigates artifacts from temporal discrepancies, such as appearing or disappearing objects, thereby enhancing structural accuracy. C-DiffSET achieves state-of-the-art (SOTA) results on multiple datasets, significantly outperforming the very recent image-to-image translation methods and SET methods with large margins. </p>
<blockquote>
<p>é›·è¾¾åˆæˆå­”å¾„ï¼ˆSARï¼‰æˆåƒæä¾›äº†ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶é—´è¦†ç›–ï¼ˆä¾‹å¦‚åœ¨äº‘ã€å­£èŠ‚ã€æ˜¼å¤œå¾ªç¯æœŸé—´ï¼‰ï¼Œä½†å…¶å™ªå£°å’Œç‹¬ç‰¹çš„ç»“æ„æ¨¡å¼ç»™è§£é‡Šå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹äºéä¸“å®¶æ¥è¯´ã€‚SARåˆ°EOï¼ˆå…‰ç”µï¼‰å›¾åƒç¿»è¯‘ï¼ˆSETï¼‰çš„å‡ºç°ä½¿å¾—SARå›¾åƒæ›´åŠ æ˜“äºæ„ŸçŸ¥å’Œç†è§£ã€‚ç„¶è€Œï¼Œä»å¤´å¼€å§‹ä»¥æœ‰é™çš„SAR-EOæ•°æ®é›†è¿›è¡Œè®­ç»ƒçš„ä¼ ç»Ÿæ–¹æ³•å®¹æ˜“å‡ºç°è¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SARåˆ°EOç¿»è¯‘çš„ç½®ä¿¡æ‰©æ•£ï¼Œç§°ä¸ºC-DiffSETã€‚å®ƒåˆ©ç”¨åœ¨å¤§é‡è‡ªç„¶å›¾åƒä¸Šè®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œä»è€Œæœ‰æ•ˆåœ°é€‚åº”EOåŸŸã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é¢„è®­ç»ƒçš„VAEç¼–ç å™¨å³ä½¿åœ¨SARè¾“å…¥ä¸­å­˜åœ¨ä¸åŒå™ªå£°æ°´å¹³çš„æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½å°†SARå’ŒEOå›¾åƒå¯¹é½åˆ°åŒä¸€æ½œåœ¨ç©ºé—´ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜SETçš„åƒç´ çº§ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ç½®ä¿¡å¼•å¯¼æ‰©æ•£ï¼ˆC-Diffï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±å‡è½»äº†ç”±äºæ—¶é—´å·®å¼‚é€ æˆçš„ä¼ªå½±ï¼Œå¦‚å‡ºç°çš„ç‰©ä½“æˆ–æ¶ˆå¤±çš„ç‰©ä½“ï¼Œä»è€Œæé«˜äº†ç»“æ„å‡†ç¡®æ€§ã€‚C-DiffSETåœ¨å¤šæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—è¶…è¶Šäº†æœ€æ–°çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ–¹æ³•å’ŒSETæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10788v2">PDF</a> Please visit our project page   <a target="_blank" rel="noopener" href="https://kaist-viclab.github.io/C-DiffSET_site/">https://kaist-viclab.github.io/C-DiffSET_site/</a></p>
<p><strong>Summary</strong></p>
<p>SARå½±åƒå…·å¤‡ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶åºè¦†ç›–èƒ½åŠ›ï¼Œä½†å…¶å™ªå£°å’Œç‹¬ç‰¹ç»“æ„æ¨¡å¼ç»™è§£è¯»å¸¦æ¥æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéä¸“å®¶è€Œè¨€ã€‚ä¸ºæ­¤ï¼Œå‡ºç°äº†SAR-to-EOï¼ˆå…‰ç”µï¼‰å›¾åƒç¿»è¯‘ï¼ˆSETï¼‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨å°è§„æ¨¡SAR-EOæ•°æ®é›†ä¸Šè¿›è¡Œä»å¤´è®­ç»ƒå®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„SAR-to-EOç¿»è¯‘ç½®ä¿¡æ‰©æ•£æ¡†æ¶ï¼ˆC-DiffSETï¼‰ã€‚æ­¤æ¡†æ¶å€ŸåŠ©å¹¿æ³›è®­ç»ƒäºè‡ªç„¶å›¾åƒä¸Šçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œæœ‰æ•ˆé€‚åº”å…‰ç”µé¢†åŸŸã€‚ç ”ç©¶å‘ç°ï¼Œé¢„è®­ç»ƒçš„VAEç¼–ç å™¨å¯åœ¨ä¸åŒå™ªå£°æ°´å¹³çš„SARå’ŒEOå½±åƒä¹‹é—´å»ºç«‹åŒä¸€æ½œåœ¨ç©ºé—´ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜SETçš„åƒç´ çº§ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ç½®ä¿¡å¼•å¯¼æ‰©æ•£ï¼ˆC-Diffï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±å‡è½»äº†å› æ—¶é—´å·®å¼‚é€ æˆçš„ä¼ªå½±ï¼Œå¦‚ç‰©ä½“çš„å‡ºç°æˆ–æ¶ˆå¤±ï¼Œä»è€Œæé«˜ç»“æ„å‡†ç¡®æ€§ã€‚C-DiffSETåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³ç»“æœï¼Œå¤§å¹…è¶…è¶Šæœ€æ–°çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ–¹æ³•å’ŒSETæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SARå½±åƒæä¾›ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶åºè¦†ç›–èƒ½åŠ›ï¼Œä½†è§£è¯»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SAR-to-EOå›¾åƒç¿»è¯‘ï¼ˆSETï¼‰æŠ€æœ¯æ—¨åœ¨æé«˜SARå½±åƒçš„æ„ŸçŸ¥è§£è¯»æ€§ã€‚</li>
<li>ä¼ ç»ŸSETæ–¹æ³•åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆã€‚</li>
<li>å¼•å…¥åŸºäºé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„C-DiffSETæ¡†æ¶ï¼Œæœ‰æ•ˆé€‚åº”å…‰ç”µé¢†åŸŸã€‚</li>
<li>é¢„è®­ç»ƒçš„VAEç¼–ç å™¨å¯åœ¨ä¸åŒå™ªå£°æ°´å¹³çš„SARå’ŒEOå½±åƒä¹‹é—´å»ºç«‹åŒä¸€æ½œåœ¨ç©ºé—´ã€‚</li>
<li>æå‡ºç½®ä¿¡å¼•å¯¼æ‰©æ•£ï¼ˆC-Diffï¼‰æŸå¤±ï¼Œä»¥æé«˜SETçš„åƒç´ çº§ä¿çœŸåº¦å’Œç»“æ„å‡†ç¡®æ€§ã€‚</li>
<li>C-DiffSETåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³ç»“æœï¼Œæ˜¾è‘—è¶…è¶Šå…¶ä»–å›¾åƒç¿»è¯‘æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-502ffd3457b7c13c6a9f1051fcf94d1f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-918bb20a8277f4962d08874b4014a821.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ed1961e469daffe0cdfeb91943bff95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51678b149dbba80ed6cd688e2dc7e421.jpg" align="middle">
</details>




<h2 id="Mechanisms-of-Generative-Image-to-Image-Translation-Networks"><a href="#Mechanisms-of-Generative-Image-to-Image-Translation-Networks" class="headerlink" title="Mechanisms of Generative Image-to-Image Translation Networks"></a>Mechanisms of Generative Image-to-Image Translation Networks</h2><p><strong>Authors:Guangzong Chen, Mingui Sun, Zhi-Hong Mao, Kangni Liu, Wenyan Jia</strong></p>
<p>Generative Adversarial Networks (GANs) are a class of neural networks that have been widely used in the field of image-to-image translation. In this paper, we propose a streamlined image-to-image translation network with a simpler architecture compared to existing models. We investigate the relationship between GANs and autoencoders and provide an explanation for the efficacy of employing only the GAN component for tasks involving image translation. We show that adversarial for GAN models yields results comparable to those of existing methods without additional complex loss penalties. Subsequently, we elucidate the rationale behind this phenomenon. We also incorporate experimental results to demonstrate the validity of our findings. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æ˜¯ä¸€ç±»åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨çš„ç¥ç»ç½‘ç»œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€åŒ–çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œå…¶æ¶æ„æ¯”ç°æœ‰æ¨¡å‹æ›´ç®€å•ã€‚æˆ‘ä»¬ç ”ç©¶äº†GANså’Œè‡ªç¼–ç å™¨ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è§£é‡Šäº†ä»…ä½¿ç”¨GANç»„ä»¶è¿›è¡Œå›¾åƒç¿»è¯‘ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯¹æŠ—æ€§å¯¹äºGANæ¨¡å‹å¯ä»¥åœ¨æ²¡æœ‰é¢å¤–çš„å¤æ‚æŸå¤±æƒ©ç½šçš„æƒ…å†µä¸‹äº§ç”Ÿä¸ç°æœ‰æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚éšåï¼Œæˆ‘ä»¬é˜è¿°äº†è¿™ç§ç°è±¡èƒŒåçš„åŸç†ã€‚æˆ‘ä»¬è¿˜ç»“åˆäº†å®éªŒç»“æœæ¥è¯æ˜æˆ‘ä»¬çš„å‘ç°çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10368v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨å›¾åƒç¿»è¯‘é¢†åŸŸçš„åº”ç”¨ã€‚æå‡ºäº†ä¸€ç§ç®€åŒ–çš„å›¾åƒç¿»è¯‘ç½‘ç»œï¼Œå…¶æ¶æ„è¾ƒç°æœ‰æ¨¡å‹æ›´ä¸ºç®€æ´ã€‚æ–‡ç« æ¢è®¨äº†GANsä¸è‡ªç¼–ç å™¨çš„å…³ç³»ï¼Œå¹¶è§£é‡Šäº†ä»…ä½¿ç”¨GANç»„ä»¶è¿›è¡Œå›¾åƒç¿»è¯‘ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹æŠ—æ€§GANæ¨¡å‹çš„ç»“æœä¸ç°æœ‰æ–¹æ³•ç›¸å½“ï¼Œæ— éœ€é¢å¤–çš„å¤æ‚æŸå¤±æƒ©ç½šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç®€åŒ–çš„å›¾åƒç¿»è¯‘ç½‘ç»œæ¶æ„ã€‚</li>
<li>æ¢è®¨äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸è‡ªç¼–ç å™¨çš„å…³ç³»ã€‚</li>
<li>é˜é‡Šäº†ä»…ä½¿ç”¨GANç»„ä»¶è¿›è¡Œå›¾åƒç¿»è¯‘ä»»åŠ¡çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å¯¹æŠ—æ€§GANæ¨¡å‹ç»“æœå ªæ¯”ç°æœ‰æ–¹æ³•ï¼Œæ— éœ€å¤æ‚æŸå¤±æƒ©ç½šã€‚</li>
<li>æ–‡ä¸­ç»™å‡ºäº†å®éªŒéªŒè¯ç»“æœçš„è¯¦ç»†è§£é‡Šã€‚</li>
<li>ç ”ç©¶å¯¹å›¾åƒç¿»è¯‘é¢†åŸŸçš„è´¡çŒ®å’Œå¯ç¤ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3aa6747136d6c675aed4d50de72d1786.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64bdedd872adb3a5caafde3b7fb40822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55cd63b2ccf6110e0a419ed412f28533.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af5585db396fafcee3000826e7b9f768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59a340854fca1fa3f09b8c14b94853ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d3c154dd28dc3212155b32d67f7adec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0629546b0f0f5d9f270f84285b8f48d5.jpg" align="middle">
</details>




<h2 id="WeatherGFM-Learning-A-Weather-Generalist-Foundation-Model-via-In-context-Learning"><a href="#WeatherGFM-Learning-A-Weather-Generalist-Foundation-Model-via-In-context-Learning" class="headerlink" title="WeatherGFM: Learning A Weather Generalist Foundation Model via   In-context Learning"></a>WeatherGFM: Learning A Weather Generalist Foundation Model via   In-context Learning</h2><p><strong>Authors:Xiangyu Zhao, Zhiwang Zhou, Wenlong Zhang, Yihao Liu, Xiangyu Chen, Junchao Gong, Hao Chen, Ben Fei, Shiqi Chen, Wanli Ouyang, Xiao-Ming Wu, Lei Bai</strong></p>
<p>The Earthâ€™s weather system encompasses intricate weather data modalities and diverse weather understanding tasks, which hold significant value to human life. Existing data-driven models focus on single weather understanding tasks (e.g., weather forecasting). Although these models have achieved promising results, they fail to tackle various complex tasks within a single and unified model. Moreover, the paradigm that relies on limited real observations for a single scenario hinders the modelâ€™s performance upper bound. In response to these limitations, we draw inspiration from the in-context learning paradigm employed in state-of-the-art visual foundation models and large language models. In this paper, we introduce the first generalist weather foundation model (WeatherGFM), designed to address a wide spectrum of weather understanding tasks in a unified manner. More specifically, we initially unify the representation and definition of the diverse weather understanding tasks. Subsequently, we devised weather prompt formats to manage different weather data modalities, namely single, multiple, and temporal modalities. Finally, we adopt a visual prompting question-answering paradigm for the training of unified weather understanding tasks. Extensive experiments indicate that our WeatherGFM can effectively handle up to ten weather understanding tasks, including weather forecasting, super-resolution, weather image translation, and post-processing. Our method also showcases generalization ability on unseen tasks. </p>
<blockquote>
<p>åœ°çƒå¤©æ°”ç³»ç»ŸåŒ…å«äº†å¤æ‚çš„å¤©æ°”æ•°æ®æ¨¡å¼å’Œå¤šæ ·çš„å¤©æ°”ç†è§£ä»»åŠ¡ï¼Œå¯¹äººç±»ç”Ÿæ´»å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰çš„æ•°æ®é©±åŠ¨æ¨¡å‹ä¸»è¦å…³æ³¨å•ä¸ªå¤©æ°”ç†è§£ä»»åŠ¡ï¼ˆä¾‹å¦‚å¤©æ°”é¢„æŠ¥ï¼‰ã€‚è™½ç„¶è¿™äº›æ¨¡å‹å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬æ— æ³•åœ¨ä¸€ä¸ªå•ä¸€ç»Ÿä¸€çš„æ¨¡å‹ä¸­è§£å†³å„ç§å¤æ‚ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä¾èµ–äºå•ä¸€åœºæ™¯çš„æœ‰é™çœŸå®è§‚æµ‹ç»“æœçš„èŒƒå¼é˜»ç¢äº†æ¨¡å‹æ€§èƒ½çš„ä¸Šé™ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä»æœ€å…ˆè¿›è§†è§‰åŸºç¡€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹ä¸­é‡‡ç”¨çš„ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ä¸­æ±²å–çµæ„Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç¬¬ä¸€ä¸ªé€šç”¨å¤©æ°”åŸºç¡€æ¨¡å‹ï¼ˆWeatherGFMï¼‰ï¼Œæ—¨åœ¨ä»¥ç»Ÿä¸€çš„æ–¹å¼è§£å†³å¹¿æ³›çš„æ°”å€™ç†è§£ä»»åŠ¡ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æœ€åˆç»Ÿä¸€äº†å¤šç§å¤©æ°”ç†è§£ä»»åŠ¡çš„è¡¨ç¤ºå’Œå®šä¹‰ã€‚éšåï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤©æ°”æç¤ºæ ¼å¼æ¥ç®¡ç†ä¸åŒçš„å¤©æ°”æ•°æ®æ¨¡å¼ï¼Œå³å•æ¨¡æ€ã€å¤šæ¨¡æ€å’Œæ—¶é—´æ¨¡æ€ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨è§†è§‰æç¤ºé—®ç­”èŒƒå¼è¿›è¡Œç»Ÿä¸€å¤©æ°”ç†è§£ä»»åŠ¡çš„è®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„WeatherGFMå¯ä»¥æœ‰æ•ˆå¤„ç†å¤šè¾¾åç§å¤©æ°”ç†è§£ä»»åŠ¡ï¼ŒåŒ…æ‹¬å¤©æ°”é¢„æŠ¥ã€è¶…åˆ†è¾¨ç‡ã€å¤©æ°”å›¾åƒç¿»è¯‘å’ŒåæœŸå¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å±•ç¤ºäº†åœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05420v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºé¦–ä¸ªé€šç”¨å¤©æ°”åŸºç¡€æ¨¡å‹ï¼ˆWeatherGFMï¼‰ï¼Œæ—¨åœ¨ä»¥ç»Ÿä¸€çš„æ–¹å¼è§£å†³å¤šç§å¤©æ°”ç†è§£ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡ç»Ÿä¸€ä¸åŒå¤©æ°”ç†è§£ä»»åŠ¡çš„è¡¨ç¤ºå’Œå®šä¹‰ï¼Œé‡‡ç”¨å¤©æ°”æç¤ºæ ¼å¼ç®¡ç†ä¸åŒçš„å¤©æ°”æ•°æ®æ¨¡æ€ï¼Œå¹¶é‡‡ç”¨è§†è§‰æç¤ºé—®ç­”æ¨¡å¼è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒWeatherGFMèƒ½å¤„ç†å¤šè¾¾åç§å¤©æ°”ç†è§£ä»»åŠ¡ï¼Œå¹¶åœ¨æœªè§ä»»åŠ¡ä¸Šå±•ç°å‡ºæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤©æ°”ç³»ç»Ÿå¯¹äººç±»ç”Ÿæ´»å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç°æœ‰æ•°æ®é©±åŠ¨æ¨¡å‹ä¸»è¦å…³æ³¨å•ä¸€å¤©æ°”ç†è§£ä»»åŠ¡ï¼Œå¦‚å¤©æ°”é¢„æŠ¥ã€‚</li>
<li>å•ä¸€åœºæ™¯ä¾èµ–æœ‰é™çœŸå®è§‚å¯Ÿçš„èŒƒå¼é™åˆ¶äº†æ¨¡å‹æ€§èƒ½çš„ä¸Šé™ã€‚</li>
<li>å€Ÿé‰´è§†è§‰åŸºç¡€æ¨¡å‹å’Œå¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èŒƒå¼ï¼Œæå‡ºé¦–ä¸ªé€šç”¨å¤©æ°”åŸºç¡€æ¨¡å‹ï¼ˆWeatherGFMï¼‰ã€‚</li>
<li>WeatherGFMç»Ÿä¸€è¡¨ç¤ºå’Œå®šä¹‰å„ç§å¤©æ°”ç†è§£ä»»åŠ¡ï¼Œå¹¶è®¾è®¡å¤©æ°”æç¤ºæ ¼å¼æ¥ç®¡ç†ä¸åŒçš„å¤©æ°”æ•°æ®æ¨¡æ€ã€‚</li>
<li>é‡‡ç”¨è§†è§‰æç¤ºé—®ç­”æ¨¡å¼è¿›è¡Œè®­ç»ƒï¼Œä½¿WeatherGFMèƒ½å¤„ç†å¤šç§å¤©æ°”ç†è§£ä»»åŠ¡ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒWeatherGFMèƒ½æœ‰æ•ˆå¤„ç†åŒ…æ‹¬å¤©æ°”é¢„æŠ¥ã€è¶…åˆ†è¾¨ç‡ã€å¤©æ°”å›¾åƒç¿»è¯‘å’ŒåæœŸå¤„ç†åœ¨å†…çš„åç§å¤©æ°”ç†è§£ä»»åŠ¡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-41bfc2c14dae144a706962849f55a5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b9349f41767d61c6cc356764adb65e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ae72b00533d4ae59a820537f0c777c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ef44b4ddc275e31707bd4a159dace7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3fbeacc4b61a1acbc2f14494224b54b.jpg" align="middle">
</details>




<h2 id="A-Cycle-Ride-to-HDR-Semantics-Aware-Self-Supervised-Framework-for-Unpaired-LDR-to-HDR-Image-Translation"><a href="#A-Cycle-Ride-to-HDR-Semantics-Aware-Self-Supervised-Framework-for-Unpaired-LDR-to-HDR-Image-Translation" class="headerlink" title="A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for   Unpaired LDR-to-HDR Image Translation"></a>A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for   Unpaired LDR-to-HDR Image Translation</h2><p><strong>Authors:Hrishav Bakul Barua, Stefanov Kalin, Lemuel Lai En Che, Dhall Abhinav, Wong KokSheik, Krishnasamy Ganesh</strong></p>
<p>Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is an important computer vision problem. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task where the model learns a mapping between domains, i.e., LDR to HDR. To address limitations of current methods, such as the paired data constraint , as well as unwanted blurring and visual artifacts in the reconstructed HDR, we propose a method that uses a modified cycle-consistent adversarial architecture and utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. The method achieves state-of-the-art results across several benchmark datasets and reconstructs high-quality HDR images. </p>
<blockquote>
<p>ä»ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰åˆ°é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰çš„å›¾åƒè½¬æ¢æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚æœ‰å¤§é‡ç ”ç©¶é‡‡ç”¨ä¼ ç»Ÿçš„éå­¦ä¹ æ–¹æ³•å’Œç°ä»£çš„æ•°æ®é©±åŠ¨æ–¹æ³•ï¼Œé‡ç‚¹å…³æ³¨ä½¿ç”¨å•æ›å…‰å’Œå¤šæ›å…‰LDRè¿›è¡ŒHDRå›¾åƒé‡å»ºã€‚ç„¶è€Œï¼Œç›®å‰å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•éƒ½éœ€è¦é«˜è´¨é‡é…å¯¹{LDRï¼ŒHDR}æ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ­¤å¤–ï¼Œå…³äºä½¿ç”¨éé…å¯¹æ•°æ®é›†è¿›è¡Œæ­¤ä»»åŠ¡çš„ç ”ç©¶å¾ˆå°‘ï¼Œæ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­å­¦ä¹ åŸŸä¹‹é—´çš„æ˜ å°„ï¼Œå³LDRåˆ°HDRã€‚ä¸ºäº†è§£å†³å½“å‰æ–¹æ³•çš„å±€é™æ€§ï¼Œä¾‹å¦‚é…å¯¹æ•°æ®çš„çº¦æŸä»¥åŠé‡å»ºHDRä¸­çš„ä¸éœ€è¦çš„æ¨¡ç³Šå’Œè§†è§‰ä¼ªå½±ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¿®æ”¹åçš„å¾ªç¯ä¸€è‡´æ€§å¯¹æŠ—æ¶æ„çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨éé…å¯¹{LDRï¼ŒHDR}æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•å¼•å…¥æ–°å‹ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±å»é™¤é—®é¢˜ï¼Œä»¥åŠè§£å†³è¯­ä¹‰ä¸€è‡´æ€§è¿™ä¸€å°šæœªæ·±å…¥ç ”ç©¶çš„ä¸»é¢˜å¼•å…¥ç¼–ç å™¨å’ŒæŸå¤±å‡½æ•°ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°ç»“æœï¼Œå¹¶é‡å»ºäº†é«˜è´¨é‡HDRå›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v1">PDF</a> Submitted to IEEE</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä»ä½åŠ¨æ€èŒƒå›´ï¼ˆLDRï¼‰å›¾åƒåˆ°é«˜åŠ¨æ€èŒƒå›´ï¼ˆHDRï¼‰å›¾åƒçš„ç¿»è¯‘æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦é—®é¢˜ã€‚å°½ç®¡å­˜åœ¨ä½¿ç”¨ä¼ ç»Ÿéå­¦ä¹ æ–¹æ³•å’Œç°ä»£æ•°æ®é©±åŠ¨æ–¹æ³•çš„è®¸å¤šç ”ç©¶ï¼Œä½†å¤§å¤šæ•°æœ€å…ˆè¿›çš„æ–¹æ³•éƒ½éœ€è¦é«˜è´¨é‡é…å¯¹çš„LDRå’ŒHDRæ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¿®æ”¹åçš„å¾ªç¯ä¸€è‡´æ€§å¯¹æŠ—æ¶æ„çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æœªé…å¯¹çš„LDRå’ŒHDRæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶å¼•å…¥äº†æ–°çš„ç”Ÿæˆå™¨æ¥è§£å†³è§†è§‰ä¼ªå½±çš„å»é™¤ä»¥åŠè¯­ä¹‰ä¸€è‡´æ€§çš„ä¸è¶³ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³ç»“æœï¼Œå¹¶èƒ½é‡å»ºé«˜è´¨é‡çš„HDRå›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LDRåˆ°HDRçš„å›¾åƒç¿»è¯‘æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦é—®é¢˜ã€‚</li>
<li>å½“å‰çš„ç ”ç©¶æ–¹æ³•å¤§å¤šéœ€è¦é«˜è´¨é‡é…å¯¹çš„LDRå’ŒHDRæ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚</li>
<li>å­˜åœ¨ä¸€äº›ä½¿ç”¨æœªé…å¯¹æ•°æ®é›†çš„ç ”ç©¶ï¼Œæ¨¡å‹å­¦ä¹ åŸŸä¹‹é—´çš„æ˜ å°„ï¼Œå³LDRåˆ°HDRã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¿®æ”¹åçš„å¾ªç¯ä¸€è‡´æ€§å¯¹æŠ—æ¶æ„çš„æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥äº†æ–°çš„ç”Ÿæˆå™¨è§£å†³è§†è§‰ä¼ªå½±å»é™¤çš„é—®é¢˜ã€‚</li>
<li>æ–¹æ³•ä¸­è¿˜ä½¿ç”¨äº†ç¼–ç å™¨å’ŒæŸå¤±å‡½æ•°æ¥å¤„ç†è¯­ä¹‰ä¸€è‡´æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªå°šæœªè¢«å……åˆ†ç ”ç©¶çš„è¯é¢˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41cedee69afb96d964aaf6c59c8294cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0663eb3471a84ab8e7f238043e742fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1196eff6d982c2725dec057930510f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-081d9bf2268c736807c5b2be9e6e3143.jpg" align="middle">
</details>




<h2 id="FashionR2R-Texture-preserving-Rendered-to-Real-Image-Translation-with-Diffusion-Models"><a href="#FashionR2R-Texture-preserving-Rendered-to-Real-Image-Translation-with-Diffusion-Models" class="headerlink" title="FashionR2R: Texture-preserving Rendered-to-Real Image Translation with   Diffusion Models"></a>FashionR2R: Texture-preserving Rendered-to-Real Image Translation with   Diffusion Models</h2><p><strong>Authors:Rui Hu, Qian He, Gaofeng He, Jiedong Zhuang, Huang Chen, Huafeng Liu, Huamin Wang</strong></p>
<p>Modeling and producing lifelike clothed human images has attracted researchersâ€™ attention from different areas for decades, with the complexity from highly articulated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealism enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corresponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation. </p>
<blockquote>
<p>å»ºæ¨¡å’Œç”Ÿæˆé€¼çœŸçš„ç©¿è¡£äººä½“å›¾åƒå‡ åå¹´æ¥å¸å¼•äº†ä¸åŒé¢†åŸŸç ”ç©¶äººå‘˜çš„å…³æ³¨ï¼Œè¿™æ¶‰åŠåˆ°é«˜åº¦å¤æ‚å’Œç»“æ„åŒ–å†…å®¹ã€‚æ¸²æŸ“ç®—æ³•åˆ†è§£å¹¶æ¨¡æ‹Ÿç›¸æœºçš„æˆåƒè¿‡ç¨‹ï¼Œä½†å—é™äºå»ºæ¨¡å˜é‡çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚ç”Ÿæˆæ¨¡å‹å¯ä»¥äº§ç”Ÿä»¤äººå°è±¡æ·±åˆ»çš„ç”ŸåŠ¨äººä½“å›¾åƒï¼Œä½†åœ¨å¯æ§æ€§å’Œå¯ç¼–è¾‘æ€§æ–¹é¢ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡ç ”ç©¶äº†æ¸²æŸ“å›¾åƒçš„å…‰ç…§ç°å®ä¸»ä¹‰å¢å¼ºæŠ€æœ¯ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å¹¶åœ¨æ¸²æŸ“çš„åŸºç¡€ä¸Šè¿›è¡Œæ§åˆ¶ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå°†æ¸²æŸ“å›¾åƒç¿»è¯‘æˆé€¼çœŸå›¾åƒçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼ˆDKIï¼‰å’Œé€¼çœŸå›¾åƒç”Ÿæˆï¼ˆRIGï¼‰ã€‚åœ¨DKIé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡æ­£ï¼ˆçœŸå®ï¼‰é¢†åŸŸå¾®è°ƒä¸è´Ÿï¼ˆæ¸²æŸ“ï¼‰é¢†åŸŸåµŒå…¥å°†çŸ¥è¯†æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­ã€‚åœ¨RIGé˜¶æ®µï¼Œæˆ‘ä»¬æ ¹æ®è¾“å…¥çš„æ¸²æŸ“å›¾åƒç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œåˆ©ç”¨çº¹ç†ä¿ç•™æ³¨æ„åŠ›æ§åˆ¶ï¼ˆTACï¼‰æ¥ä¿ç•™ç²¾ç»†çš„è¡£ç‰©çº¹ç†ï¼Œå¹¶åˆ©ç”¨UNetç»“æ„ç¼–ç çš„è§£è€¦ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†SynFashionæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»¥é«˜è´¨é‡çš„æ•°å­—æœè£…å›¾åƒä¸ºç‰¹è‰²ï¼Œå…·æœ‰å¤šæ ·çš„çº¹ç†ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“åˆ°ç°å®å›¾åƒç¿»è¯‘ä¸­çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14429v1">PDF</a> Accepted by NeurIPS 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹æå‡æ¸²æŸ“å›¾åƒçš„çœŸå®æ„Ÿã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼ˆDKIï¼‰å’ŒçœŸå®å›¾åƒç”Ÿæˆï¼ˆRIGï¼‰ã€‚DKIé€šè¿‡æ­£å‘ï¼ˆçœŸå®ï¼‰é¢†åŸŸå¾®è°ƒä¸åå‘ï¼ˆæ¸²æŸ“ï¼‰é¢†åŸŸåµŒå…¥æ¥å°†çŸ¥è¯†æ³¨å…¥é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ã€‚åœ¨RIGé˜¶æ®µï¼Œç”Ÿæˆä¸è¾“å…¥æ¸²æŸ“å›¾åƒå¯¹åº”çš„çœŸå®å›¾åƒï¼Œåˆ©ç”¨çº¹ç†ä¿ç•™æ³¨æ„åŠ›æ§åˆ¶ï¼ˆTACï¼‰æ¥ä¿ç•™ç²¾ç»†çš„è¡£ç‰©çº¹ç†ï¼Œå¹¶åˆ©ç”¨UNetç»“æ„ç¼–ç çš„è§£è€¦ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†SynFashionæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„æ•°å­—æœè£…å›¾åƒï¼Œå…·æœ‰å¤šæ ·åŒ–çš„çº¹ç†ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ¸²æŸ“åˆ°çœŸå®å›¾åƒç¿»è¯‘ä¸­çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å…³æ³¨äºå°†æ¸²æŸ“å›¾åƒè½¬åŒ–ä¸ºæ›´çœŸå®çš„å›¾åƒï¼Œç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹æå‡æ¸²æŸ“å›¾åƒçš„çœŸå®æ„Ÿã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼ŒåŒ…æ‹¬é¢†åŸŸçŸ¥è¯†æ³¨å…¥ï¼ˆDKIï¼‰å’ŒçœŸå®å›¾åƒç”Ÿæˆï¼ˆRIGï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>DKIé˜¶æ®µé€šè¿‡æ­£å‘å’Œåå‘é¢†åŸŸåµŒå…¥æ¥å¢å¼ºé¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>RIGé˜¶æ®µåˆ©ç”¨çº¹ç†ä¿ç•™æ³¨æ„åŠ›æ§åˆ¶ï¼ˆTACï¼‰ç”Ÿæˆå…·æœ‰ç²¾ç»†è¡£ç‰©çº¹ç†çš„çœŸå®å›¾åƒã€‚</li>
<li>å¼•å…¥äº†SynFashionæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡ã€å¤šæ ·åŒ–çº¹ç†çš„æ•°å­—æœè£…å›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ¸²æŸ“åˆ°çœŸå®å›¾åƒç¿»è¯‘é¢†åŸŸçš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5904b839c947708bb063be1e1a62229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6b1c396ede500e3e8e6259a1b56b156.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b89c82f679d3d190d2b521f35ab03b00.jpg" align="middle">
</details>




<h2 id="From-Real-Artifacts-to-Virtual-Reference-A-Robust-Framework-for-Translating-Endoscopic-Images"><a href="#From-Real-Artifacts-to-Virtual-Reference-A-Robust-Framework-for-Translating-Endoscopic-Images" class="headerlink" title="From Real Artifacts to Virtual Reference: A Robust Framework for   Translating Endoscopic Images"></a>From Real Artifacts to Virtual Reference: A Robust Framework for   Translating Endoscopic Images</h2><p><strong>Authors:Junyang Wu, Fangfang Xie, Jiayuan Sun, Yun Gu, Guang-Zhong Yang</strong></p>
<p>Domain adaptation, which bridges the distributions across different modalities, plays a crucial role in multimodal medical image analysis. In endoscopic imaging, combining pre-operative data with intra-operative imaging is important for surgical planning and navigation. However, existing domain adaptation methods are hampered by distribution shift caused by in vivo artifacts, necessitating robust techniques for aligning noisy and artifact abundant patient endoscopic videos with clean virtual images reconstructed from pre-operative tomographic data for pose estimation during intraoperative guidance. This paper presents an artifact-resilient image translation method and an associated benchmark for this purpose. The method incorporates a novel &#96;&#96;local-globalâ€™â€™ translation framework and a noise-resilient feature extraction strategy. For the former, it decouples the image translation process into a local step for feature denoising, and a global step for global style transfer. For feature extraction, a new contrastive learning strategy is proposed, which can extract noise-resilient features for establishing robust correspondence across domains. Detailed validation on both public and in-house clinical datasets has been conducted, demonstrating significantly improved performance compared to the current state-of-the-art. </p>
<blockquote>
<p>é¢†åŸŸé€‚åº”ï¼ˆdomain adaptationï¼‰åœ¨å¤šæ¨¡æ€åŒ»ç–—å›¾åƒåˆ†æä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒæ¶èµ·äº†ä¸åŒåˆ†å¸ƒä¹‹é—´çš„æ¡¥æ¢ã€‚åœ¨å†…çª¥æˆåƒä¸­ï¼Œå°†æœ¯å‰æ•°æ®ä¸æœ¯ä¸­æˆåƒç›¸ç»“åˆå¯¹äºæ‰‹æœ¯è§„åˆ’å’Œå¯¼èˆªè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¢†åŸŸé€‚åº”æ–¹æ³•å—åˆ°ä½“å†…ä¼ªå½±å¼•èµ·çš„åˆ†å¸ƒå˜åŒ–çš„å½±å“ã€‚å› æ­¤ï¼Œéœ€è¦å¼ºå¤§çš„æŠ€æœ¯æ¥å¯¹é½å……æ»¡å™ªå£°å’Œä¼ªå½±çš„æ‚£è€…å†…çª¥é•œè§†é¢‘ä¸é€šè¿‡æœ¯å‰æ–­å±‚æ‰«ææ•°æ®é‡å»ºçš„å¹²å‡€è™šæ‹Ÿå›¾åƒï¼Œä»¥ç”¨äºæœ¯ä¸­æŒ‡å¯¼çš„å§¿åŠ¿ä¼°è®¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ­¤ç›®çš„çš„æŠ—ä¼ªå½±å›¾åƒç¿»è¯‘æ–¹æ³•å’Œç›¸å…³åŸºå‡†æµ‹è¯•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸€ç§æ–°å‹çš„â€œå±€éƒ¨-å…¨å±€â€ç¿»è¯‘æ¡†æ¶å’ŒæŠ—å™ªå£°ç‰¹å¾æå–ç­–ç•¥ã€‚å¯¹äºå‰è€…ï¼Œå®ƒå°†å›¾åƒç¿»è¯‘è¿‡ç¨‹åˆ†è§£ä¸ºå±€éƒ¨ç‰¹å¾å»å™ªå’Œå…¨å±€é£æ ¼è½¬æ¢ä¸¤ä¸ªæ­¥éª¤ã€‚å¯¹äºç‰¹å¾æå–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå¯ä»¥æå–å‡ºç¨³å¥çš„ç‰¹å¾ï¼Œä»¥åœ¨å„ä¸ªé¢†åŸŸä¹‹é—´å»ºç«‹ç¨³å¥çš„å¯¹åº”å…³ç³»ã€‚åœ¨å…¬å…±å’Œå†…éƒ¨ä¸´åºŠæ•°æ®é›†ä¸Šçš„è¯¦ç»†éªŒè¯æ˜¾ç¤ºï¼Œä¸å½“å‰å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13896v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼ŒåŸŸé€‚åº”æŠ€æœ¯å¯¹äºå¼¥ä¸åŒæ¨¡æ€é—´çš„æ•°æ®åˆ†å¸ƒå·®å¼‚è‡³å…³é‡è¦ã€‚åœ¨å†…çª¥æˆåƒé¢†åŸŸï¼Œç»“åˆæœ¯å‰æ•°æ®ä¸æœ¯ä¸­å½±åƒå¯¹æ‰‹æœ¯è§„åˆ’ä¸å¯¼èˆªå…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰åŸŸé€‚åº”æ–¹æ³•å—åˆ°ä½“å†…ä¼ªå½±å¯¼è‡´çš„åˆ†å¸ƒåç§»çš„åˆ¶çº¦ï¼Œéœ€è¦ç¨³å¥æŠ€æœ¯æ¥å¯¹å«å™ªå£°å’Œä¸°å¯Œä¼ªå½±çš„æ‚£è€…å†…çª¥è§†é¢‘ä¸ä»æœ¯å‰æ–­å±‚æ‰«ææ•°æ®ä¸­é‡å»ºçš„å¹²å‡€è™šæ‹Ÿå›¾åƒè¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°æœ¯ä¸­æŒ‡å¯¼çš„å§¿æ€ä¼°è®¡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è€ä¼ªå½±çš„å›¾åƒç¿»è¯‘æ–¹æ³•å’Œç›¸å…³åŸºå‡†æµ‹è¯•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸€ç§æ–°é¢–çš„â€œå±€éƒ¨-å…¨å±€â€ç¿»è¯‘æ¡†æ¶å’Œä¸€ç§è€å™ªå£°çš„ç‰¹å¾æå–ç­–ç•¥ã€‚å‰è€…å°†å›¾åƒç¿»è¯‘è¿‡ç¨‹åˆ†è§£ä¸ºå±€éƒ¨ç‰¹å¾å»å™ªå’Œå…¨å±€é£æ ¼è½¬æ¢ä¸¤ä¸ªæ­¥éª¤ã€‚å¯¹äºç‰¹å¾æå–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå¯ä»¥æå–è·¨åŸŸå»ºç«‹ç¨³å¥å¯¹åº”çš„å™ªå£°é²æ£’ç‰¹å¾ã€‚åœ¨å…¬å…±å’Œé™¢å†…ä¸´åºŠæ•°æ®é›†ä¸Šçš„è¯¦ç»†éªŒè¯æ˜¾ç¤ºï¼Œä¸å½“å‰æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸŸé€‚åº”åœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†æä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œå°¤å…¶åœ¨è¿æ¥ä¸åŒæ¨¡æ€æ•°æ®åˆ†å¸ƒæ–¹é¢ã€‚</li>
<li>å†…çª¥æˆåƒä¸­ç»“åˆæœ¯å‰ä¸æœ¯ä¸­å½±åƒå¯¹æ‰‹æœ¯è§„åˆ’å’Œå¯¼èˆªå¾ˆé‡è¦ã€‚</li>
<li>ç°æœ‰åŸŸé€‚åº”æ–¹æ³•å—åˆ°ä½“å†…ä¼ªå½±å¯¼è‡´çš„åˆ†å¸ƒåç§»çš„åˆ¶çº¦ã€‚</li>
<li>éœ€è¦ç¨³å¥çš„æŠ€æœ¯æ¥å¯¹é½å«å™ªå£°å’Œä¼ªå½±çš„å†…çª¥è§†é¢‘ä¸è™šæ‹Ÿå›¾åƒã€‚</li>
<li>æå‡ºçš„å›¾åƒç¿»è¯‘æ–¹æ³•ç»“åˆâ€œå±€éƒ¨-å…¨å±€â€ç¿»è¯‘æ¡†æ¶å’Œå™ªå£°é²æ£’ç‰¹å¾æå–ç­–ç•¥ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç­–ç•¥ç”¨äºæå–è·¨åŸŸçš„å™ªå£°é²æ£’ç‰¹å¾ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cb07b92b845c50557ac8800ed1781da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd4d529d3a007f3600b623c1d6d5252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4414f559653b4cf26e5c56572d88a8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e47219295e7ac9d1998ab30e8656ebbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e169d1a48568214aeeaaca0c6808c81.jpg" align="middle">
</details>




<h2 id="A-Unified-Framework-for-Forward-and-Inverse-Problems-in-Subsurface-Imaging-using-Latent-Space-Translations"><a href="#A-Unified-Framework-for-Forward-and-Inverse-Problems-in-Subsurface-Imaging-using-Latent-Space-Translations" class="headerlink" title="A Unified Framework for Forward and Inverse Problems in Subsurface   Imaging using Latent Space Translations"></a>A Unified Framework for Forward and Inverse Problems in Subsurface   Imaging using Latent Space Translations</h2><p><strong>Authors:Naveen Gupta, Medha Sawhney, Arka Daw, Youzuo Lin, Anuj Karpatne</strong></p>
<p>In subsurface imaging, learning the mapping from velocity maps to seismic waveforms (forward problem) and waveforms to velocity (inverse problem) is important for several applications. While traditional techniques for solving forward and inverse problems are computationally prohibitive, there is a growing interest in leveraging recent advances in deep learning to learn the mapping between velocity maps and seismic waveform images directly from data. Despite the variety of architectures explored in previous works, several open questions still remain unanswered such as the effect of latent space sizes, the importance of manifold learning, the complexity of translation models, and the value of jointly solving forward and inverse problems. We propose a unified framework to systematically characterize prior research in this area termed the Generalized Forward-Inverse (GFI) framework, building on the assumption of manifolds and latent space translations. We show that GFI encompasses previous works in deep learning for subsurface imaging, which can be viewed as specific instantiations of GFI. We also propose two new model architectures within the framework of GFI: Latent U-Net and Invertible X-Net, leveraging the power of U-Nets for domain translation and the ability of IU-Nets to simultaneously learn forward and inverse translations, respectively. We show that our proposed models achieve state-of-the-art (SOTA) performance for forward and inverse problems on a wide range of synthetic datasets, and also investigate their zero-shot effectiveness on two real-world-like datasets. </p>
<blockquote>
<p>åœ¨åœ°ä¸‹æˆåƒä¸­ï¼Œå­¦ä¹ ä»é€Ÿåº¦å›¾åˆ°åœ°éœ‡æ³¢å½¢çš„æ˜ å°„ï¼ˆæ­£é—®é¢˜ï¼‰ä»¥åŠä»æ³¢å½¢åˆ°é€Ÿåº¦ï¼ˆåé—®é¢˜ï¼‰å¯¹äºå¤šç§åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶ä¼ ç»Ÿçš„è§£å†³æ­£é—®é¢˜å’Œåé—®é¢˜çš„æŠ€æœ¯åœ¨è®¡ç®—ä¸Šå—åˆ°å¾ˆå¤§é™åˆ¶ï¼Œä½†äººä»¬è¶Šæ¥è¶Šæœ‰å…´è¶£åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ é€Ÿåº¦å›¾ä¸åœ°éœ‡æ³¢å½¢å›¾åƒä¹‹é—´çš„æ˜ å°„ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·¥ä½œæ¢ç´¢äº†å„ç§æ¶æ„ï¼Œä½†ä»æœ‰è®¸å¤šå¼€æ”¾é—®é¢˜å°šæœªè§£å†³ï¼Œä¾‹å¦‚æ½œåœ¨ç©ºé—´å¤§å°çš„å½±å“ã€æµå½¢å­¦ä¹ çš„é‡è¦æ€§ã€ç¿»è¯‘æ¨¡å‹çš„å¤æ‚æ€§ä»¥åŠè”åˆè§£å†³æ­£é—®é¢˜å’Œåé—®é¢˜çš„ä»·å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥ç³»ç»Ÿåœ°åˆ»ç”»è¿™ä¸€é¢†åŸŸçš„å…ˆå‰ç ”ç©¶ï¼Œç§°ä¸ºå¹¿ä¹‰æ­£é€†ï¼ˆGFIï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å»ºç«‹åœ¨æµå½¢å’Œæ½œåœ¨ç©ºé—´ç¿»è¯‘å‡è®¾çš„åŸºç¡€ä¸Šã€‚æˆ‘ä»¬å±•ç¤ºäº†GFIæ¶µç›–äº†åœ°ä¸‹æˆåƒæ·±åº¦å­¦ä¹ é¢†åŸŸçš„å…ˆå‰ç ”ç©¶ï¼Œè¿™äº›ç ”ç©¶å¯ä»¥è¢«è§†ä¸ºGFIçš„ç‰¹å®šå®ä¾‹ã€‚åœ¨GFIæ¡†æ¶å†…ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼šæ½œåœ¨U-Netå’Œå¯é€†X-Netï¼Œå®ƒä»¬åˆ©ç”¨U-Netåœ¨é¢†åŸŸç¿»è¯‘æ–¹é¢çš„ä¼˜åŠ¿ä»¥åŠIU-NetåŒæ—¶å­¦ä¹ æ­£å‘å’Œé€†å‘ç¿»è¯‘çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨å¤šç§åˆæˆæ•°æ®é›†ä¸Šçš„æ­£å‘å’Œåå‘é—®é¢˜ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œå¹¶è°ƒæŸ¥äº†å®ƒä»¬åœ¨ä¸¤ä¸ªç°å®ä¸–ç•Œç±»ä¼¼æ•°æ®é›†çš„é›¶æ ·æœ¬æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.11247v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è§£å†³åœ°ä¸‹æˆåƒä¸­çš„æ­£é—®é¢˜å’Œåé—®é¢˜çš„é‡è¦æ€§åŠæ–¹æ³•ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯è®¡ç®—é‡å¤§ã€æ¨¡å‹å¤æ‚ç­‰é—®é¢˜ï¼Œæå‡ºäº†å¹¿ä¹‰æ­£é€†ï¼ˆGFIï¼‰æ¡†æ¶ï¼Œå¹¶å¯¹æ½œç©ºé—´å¤§å°ã€æµå½¢å­¦ä¹ çš„é‡è¦æ€§ç­‰å¼€æ”¾æ€§é—®é¢˜è¿›è¡Œäº†ç³»ç»Ÿæ¢è®¨ã€‚åŒæ—¶ï¼Œæ–‡ç« æå‡ºäº†ä¸¤ç§æ–°æ¨¡å‹æ¶æ„â€”â€”Latent U-Netå’ŒInvertible X-Netï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†è¿™äº›æ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ä»¥åŠå¯¹ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„é›¶æ ·æœ¬æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åœ°ä¸‹æˆåƒä¸­ï¼Œæ­£é—®é¢˜å’Œåé—®é¢˜çš„ç ”ç©¶å¯¹äºå¤šç§åº”ç”¨è‡³å…³é‡è¦ã€‚ä¼ ç»ŸæŠ€æœ¯è®¡ç®—é‡å¤§ï¼Œè€Œæ·±åº¦å­¦ä¹ åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>å¹¿ä¹‰æ­£é€†ï¼ˆGFIï¼‰æ¡†æ¶ç”¨äºç³»ç»Ÿåœ°æè¿°å’Œç ”ç©¶è¯¥é¢†åŸŸçš„å…ˆå‰ç ”ç©¶ã€‚</li>
<li>GFIæ¡†æ¶åŒ…å«æ½œç©ºé—´å¤§å°å’Œæµå½¢å­¦ä¹ ç­‰å…³é”®è¦ç´ çš„ç ”ç©¶ã€‚</li>
<li>Latent U-Netå’ŒInvertible X-Netæ¨¡å‹åœ¨åˆæˆæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ–°æ¨¡å‹æ¶æ„åˆ©ç”¨U-Netè¿›è¡Œé¢†åŸŸè½¬æ¢ï¼Œå¹¶èƒ½åŒæ—¶å­¦ä¹ æ­£é€†è½¬æ¢ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†æ–°æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d188a2fe124f802ab6fa6cc22cd70a11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad5f89a3c32852778f2acb5ad8c5fc14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db56d42981883ff56d938b315cbb92e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5518de99d8be941d7625e46667809dd3.jpg" align="middle">
</details>




<h2 id="EBDM-Exemplar-guided-Image-Translation-with-Brownian-bridge-Diffusion-Models"><a href="#EBDM-Exemplar-guided-Image-Translation-with-Brownian-bridge-Diffusion-Models" class="headerlink" title="EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion   Models"></a>EBDM: Exemplar-guided Image Translation with Brownian-bridge Diffusion   Models</h2><p><strong>Authors:Eungbean Lee, Somi Jeong, Kwanghoon Sohn</strong></p>
<p>Exemplar-guided image translation, synthesizing photo-realistic images that conform to both structural control and style exemplars, is attracting attention due to its ability to enhance user control over style manipulation. Previous methodologies have predominantly depended on establishing dense correspondences across cross-domain inputs. Despite these efforts, they incur quadratic memory and computational costs for establishing dense correspondence, resulting in limited versatility and performance degradation. In this paper, we propose a novel approach termed Exemplar-guided Image Translation with Brownian-Bridge Diffusion Models (EBDM). Our method formulates the task as a stochastic Brownian bridge process, a diffusion process with a fixed initial point as structure control and translates into the corresponding photo-realistic image while being conditioned solely on the given exemplar image. To efficiently guide the diffusion process toward the style of exemplar, we delineate three pivotal components: the Global Encoder, the Exemplar Network, and the Exemplar Attention Module to incorporate global and detailed texture information from exemplar images. Leveraging Bridge diffusion, the network can translate images from structure control while exclusively conditioned on the exemplar style, leading to more robust training and inference processes. We illustrate the superiority of our method over competing approaches through comprehensive benchmark evaluations and visual results. </p>
<blockquote>
<p>èŒƒä¾‹å¼•å¯¼çš„å›¾åƒç¿»è¯‘æŠ€æœ¯æ­£åœ¨å¸å¼•äººä»¬çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåˆæˆæ—¢ç¬¦åˆç»“æ„æ§åˆ¶åˆç¬¦åˆæ ·å¼èŒƒä¾‹çš„å…‰æ …å›¾åƒã€‚ä¹‹å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå»ºç«‹è·¨åŸŸè¾“å…¥çš„å¯†é›†å¯¹åº”å…³ç³»ã€‚å°½ç®¡ä»˜å‡ºäº†è¿™äº›åŠªåŠ›ï¼Œä½†ç”±äºå»ºç«‹å¯†é›†å¯¹åº”å…³ç³»éœ€è¦äºŒæ¬¡å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œå¯¼è‡´çµæ´»æ€§æœ‰é™å’Œæ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºå¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹çš„èŒƒä¾‹å¼•å¯¼å›¾åƒç¿»è¯‘ï¼ˆEBDMï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ä»»åŠ¡è¡¨è¿°ä¸ºéšæœºå¸ƒæœ—æ¡¥è¿‡ç¨‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å›ºå®šåˆå§‹ç‚¹ä¸ºç»“æ„æ§åˆ¶çš„æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶ä»…æ ¹æ®ç»™å®šçš„èŒƒä¾‹å›¾åƒç¿»è¯‘æˆç›¸åº”çš„å…‰æ …å›¾åƒã€‚ä¸ºäº†æœ‰æ•ˆåœ°å°†æ‰©æ•£è¿‡ç¨‹å¼•å‘èŒƒä¾‹çš„é£æ ¼ï¼Œæˆ‘ä»¬åˆ’å®šäº†ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå…¨å±€ç¼–ç å™¨ã€èŒƒä¾‹ç½‘ç»œå’ŒèŒƒä¾‹æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥èå…¥èŒƒä¾‹å›¾åƒä¸­çš„å…¨å±€å’Œè¯¦ç»†çº¹ç†ä¿¡æ¯ã€‚å€ŸåŠ©æ¡¥æ¢æ‰©æ•£ï¼Œç½‘ç»œå¯ä»¥åœ¨ä»…å—èŒƒä¾‹é£æ ¼åˆ¶çº¦çš„åŒæ—¶ï¼Œä»ç»“æ„æ§åˆ¶ç¿»è¯‘å›¾åƒï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬é€šè¿‡ç»¼åˆåŸºå‡†è¯„ä¼°å’Œäº§å“è§†è§‰ç»“æœï¼Œè¯´æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09802v1">PDF</a> ECCV 2024</p>
<p><strong>Summary</strong>:<br>å›¾åƒç¿»è¯‘æŠ€æœ¯åœ¨ç»“æ„å’Œé£æ ¼å…¸èŒƒæŒ‡å¯¼ä¸‹åˆæˆé€¼çœŸçš„å›¾åƒæ–¹é¢å¼•èµ·äº†å…³æ³¨ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•ä¸»è¦é€šè¿‡å»ºç«‹è·¨åŸŸè¾“å…¥çš„å¯†é›†å¯¹åº”å…³ç³»æ¥å®ç°ï¼Œä½†å®ƒä»¬éœ€è¦äºŒæ¬¡å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œé™åˆ¶äº†çµæ´»æ€§å’Œæ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºåŸºäºå¸ƒæœ—æ¡¥æ‰©æ•£æ¨¡å‹çš„èŒƒä¾‹å¼•å¯¼å›¾åƒç¿»è¯‘ï¼ˆEBDMï¼‰ï¼Œå®ƒå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºä¸€ä¸ªéšæœºå¸ƒæœ—æ¡¥è¿‡ç¨‹ï¼Œä»…ä¾èµ–äºç»™å®šçš„èŒƒä¾‹å›¾åƒè¿›è¡Œç¿»è¯‘ã€‚é€šè¿‡å…¨çƒç¼–ç å™¨ã€èŒƒä¾‹ç½‘ç»œå’ŒèŒƒä¾‹æ³¨æ„åŠ›æ¨¡å—ä¸‰ä¸ªå…³é”®ç»„ä»¶æœ‰æ•ˆåœ°å¼•å¯¼æ‰©æ•£è¿‡ç¨‹å‘èŒƒä¾‹é£æ ¼å‘å±•ã€‚åˆ©ç”¨æ¡¥æ¢æ‰©æ•£ç½‘ç»œå¯ä»¥åœ¨éµå¾ªç»“æ„æ§åˆ¶çš„åŒæ—¶ï¼Œä»…æ ¹æ®èŒƒä¾‹é£æ ¼è¿›è¡Œå›¾åƒç¿»è¯‘ï¼Œå®ç°æ›´ç¨³å¥çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å›¾åƒç¿»è¯‘æŠ€æœ¯åœ¨åˆæˆé€¼çœŸå›¾åƒæ–¹é¢å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„å’Œé£æ ¼å…¸èŒƒçš„æŒ‡å¯¼ä¸‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å»ºç«‹è·¨åŸŸè¾“å…¥çš„å¯†é›†å¯¹åº”å…³ç³»ï¼Œä½†å­˜åœ¨å†…å­˜å’Œè®¡ç®—æˆæœ¬é«˜çš„ç¼ºç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒç¿»è¯‘æ–¹æ³•EBDMï¼Œå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºéšæœºå¸ƒæœ—æ¡¥è¿‡ç¨‹ã€‚</li>
<li>EBDMæ–¹æ³•ä»…ä¾èµ–äºç»™å®šçš„èŒƒä¾‹å›¾åƒè¿›è¡Œç¿»è¯‘ï¼Œæé«˜äº†çµæ´»æ€§å’Œæ€§èƒ½ã€‚</li>
<li>é€šè¿‡å…¨çƒç¼–ç å™¨ã€èŒƒä¾‹ç½‘ç»œå’ŒèŒƒä¾‹æ³¨æ„åŠ›æ¨¡å—ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼•å¯¼æ‰©æ•£è¿‡ç¨‹å‘èŒƒä¾‹é£æ ¼å‘å±•ã€‚</li>
<li>åˆ©ç”¨æ¡¥æ¢æ‰©æ•£ç½‘ç»œå¯ä»¥åœ¨éµå¾ªç»“æ„æ§åˆ¶çš„åŒæ—¶ï¼Œä»…æ ¹æ®èŒƒä¾‹é£æ ¼è¿›è¡Œå›¾åƒç¿»è¯‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab91b5c330defcfdfa0870cc5e5197f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d008eed3b22b7c1e4ec9db9d619764f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb453b1539f213ad516adbacf0ed6e9.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-48451e82ae24963df895b403b137806d.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  Multimodal Whole Slide Foundation Model for Pathology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-39029a31cbacc01d5860058af630a47f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  Training Large Language Models to Reason in a Continuous Latent Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17196.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
