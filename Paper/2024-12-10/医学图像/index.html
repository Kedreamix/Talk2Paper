<!DOCTYPE HTML><html lang="zh-CN"><head><meta charset="utf-8"><meta name="keywords" content="åŒ»å­¦å›¾åƒ"><meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  [MASK] is All You Need  In generative models, two paradigms have gained attraction in various applications"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="renderer" content="webkit|ie-stand|ie-comp"><meta name="mobile-web-app-capable" content="yes"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="referrer" content="no-referrer-when-downgrade"><title>åŒ»å­¦å›¾åƒ | Talk2Paper</title><link rel="icon" type="image/png" href="/Talk2Paper/favicon.png"><style>body{background-image:url(/Talk2Paper/background.jpg);background-repeat:no-repeat;background-size:100% 100%;background-attachment:fixed}</style><link rel="stylesheet" href="/Talk2Paper/libs/awesome/css/all.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/materialize/materialize.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/aos/aos.css"><link rel="stylesheet" href="/Talk2Paper/libs/animate/animate.min.css"><link rel="stylesheet" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css"><link rel="stylesheet" href="/Talk2Paper/css/matery.css"><link rel="stylesheet" href="/Talk2Paper/css/my.css"><link rel="stylesheet" href="/Talk2Paper/css/dark.css" media="none" onload='"all"!=media&&(media="all")'><link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css"><link rel="stylesheet" href="/Talk2Paper/css/post.css"><script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header class="navbar-fixed"><nav id="headNav" class="bg-color nav-transparent"><div id="navContainer" class="nav-wrapper container"><div class="brand-logo"><a href="/Talk2Paper/" class="waves-effect waves-light"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO"> <span class="logo-span">Talk2Paper</span></a></div><a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a><ul class="right nav-menu"><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fas fa-home" style="zoom:0.6"></i> <span>é¦–é¡µ</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fas fa-tags" style="zoom:0.6"></i> <span>æ ‡ç­¾</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fas fa-bookmark" style="zoom:0.6"></i> <span>åˆ†ç±»</span></a></li><li class="hide-on-med-and-down nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fas fa-archive" style="zoom:0.6"></i> <span>å½’æ¡£</span></a></li><li><a href="#searchModal" class="modal-trigger waves-effect waves-light"><i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom:0.85"></i></a></li><li><a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼"><i id="sum-moon-icon" class="fas fa-sun" style="zoom:0.85"></i></a></li></ul><div id="mobile-nav" class="side-nav sidenav"><div class="mobile-head bg-color"><img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img"><div class="logo-name">Talk2Paper</div><div class="logo-desc">Never really desperate, only the lost of the soul.</div></div><ul class="menu-list mobile-menu-list"><li class="m-nav-item"><a href="/Talk2Paper/" class="waves-effect waves-light"><i class="fa-fw fas fa-home"></i> é¦–é¡µ</a></li><li class="m-nav-item"><a href="/Talk2Paper/tags" class="waves-effect waves-light"><i class="fa-fw fas fa-tags"></i> æ ‡ç­¾</a></li><li class="m-nav-item"><a href="/Talk2Paper/categories" class="waves-effect waves-light"><i class="fa-fw fas fa-bookmark"></i> åˆ†ç±»</a></li><li class="m-nav-item"><a href="/Talk2Paper/archives" class="waves-effect waves-light"><i class="fa-fw fas fa-archive"></i> å½’æ¡£</a></li><li><div class="divider"></div></li><li><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank"><i class="fab fa-github-square fa-fw"></i> Fork Me</a></li></ul></div></div><style>.nav-transparent .github-corner{display:none!important}.github-corner{position:absolute;z-index:10;top:0;right:0;border:0;transform:scale(1.1)}.github-corner svg{color:#0f9d58;fill:#fff;height:64px;width:64px}.github-corner:hover .octo-arm{animation:a .56s ease-in-out}.github-corner .octo-arm{animation:none}@keyframes a{0%,to{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}</style><a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50"><svg viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a></nav></header><div class="bg-cover pd-header post-cover" style="background-image:url('https://pica.zhimg.com/v2-ea0ffd3a67649ccdf1c36e0b999ac684.jpg')"><div class="container" style="right:0;left:0"><div class="row"><div class="col s12 m12 l12"><div class="brand"><h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1></div></div></div></div></div><main class="post-container content"><div class="row"><div id="main-content" class="col s12 m12 l9"><div id="artDetail"><div class="card"><div class="card-content article-info"><div class="row tag-cate"><div class="col s7"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"><span class="chip bg-color">åŒ»å­¦å›¾åƒ</span></a></div></div><div class="col s5 right-align"><div class="post-cate"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">åŒ»å­¦å›¾åƒ</a></div></div></div><div class="post-info"><div class="post-date info-break-policy"><i class="far fa-calendar-minus fa-fw"></i> å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp; 2024-12-11</div><div class="post-date info-break-policy"><i class="far fa-calendar-check fa-fw"></i> æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp; 2024-12-11</div><div class="info-break-policy"><i class="far fa-file-word fa-fw"></i> æ–‡ç« å­—æ•°:&nbsp;&nbsp; 34.1k</div><div class="info-break-policy"><i class="far fa-clock fa-fw"></i> é˜…è¯»æ—¶é•¿:&nbsp;&nbsp; 140 åˆ†</div><div id="busuanzi_container_page_pv" class="info-break-policy"><i class="far fa-eye fa-fw"></i> é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;<span id="busuanzi_value_page_pv"></span></div></div></div><hr class="clearfix"><div class="card-content article-card-content"><div id="articleContent"><blockquote><p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p></blockquote><h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="MASK-is-All-You-Need"><a href="#MASK-is-All-You-Need" class="headerlink" title="[MASK] is All You Need"></a>[MASK] is All You Need</h2><p><strong>Authors:Vincent Tao Hu, BjÃ¶rn Ommer</strong></p><p>In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK]tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks.</p><blockquote><p>åœ¨ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä¸¤ç§èŒƒå¼åœ¨å„ç§åº”ç”¨ä¸­å—åˆ°äº†å…³æ³¨ï¼šåŸºäºä¸‹ä¸€é›†é¢„æµ‹çš„æ©ç ç”Ÿæˆæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼Œä¾‹å¦‚æ‰©æ•£æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹æ¥è¿æ¥å®ƒä»¬ï¼Œå¹¶æ¢ç´¢å…¶åœ¨è§†è§‰é¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€çš„è®¾è®¡ç©ºé—´å†…ä»¥åˆ†é˜¶æ®µçš„æ–¹å¼å¯¹è¿™ä¸¤ç§æ¨¡å‹è¿›è¡Œåˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°è°ƒåº¦ã€æ¸©åº¦ã€å¼•å¯¼å¼ºåº¦ç­‰ï¼Œå¹¶ä»¥å¯æ‰©å±•çš„æ–¹å¼å¤„ç†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰é‡æ–°å®šä¹‰ä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„[MASK]æ ‡è®°çš„å»æ©ç è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ‰§è¡Œå„ç§é‡‡æ ·è¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡ä»…ä¸€æ¬¡è®­ç»ƒå¯¹è”åˆåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡æ¥å®ç°çµæ´»çš„æ¡ä»¶é‡‡æ ·ã€‚æ‰€æœ‰ä¸Šè¿°æ¢ç´¢éƒ½å¼•é¢†æˆ‘ä»¬æ„å»ºäº†åä¸ºâ€œç¦»æ•£æ’å€¼â€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æˆ–ä¿æŒä¸ä»¥å‰åŸºäºç¦»æ•£çŠ¶æ€çš„æ–¹æ³•ç›¸æ¯”çš„å…ˆè¿›æˆ–ç«äº‰æ€§èƒ½ï¼Œå¦‚ImageNet256ã€MS COCOå’Œè§†é¢‘æ•°æ®é›†FaceForensicsã€‚æ€»ä¹‹ï¼Œé€šè¿‡åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œæˆ‘ä»¬å¯ä»¥æ¶èµ·æ©ç ç”Ÿæˆå’Œéè‡ªå›å½’æ‰©æ•£æ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œä»¥åŠç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06787v1">PDF</a> Technical Report (WIP), Project Page(code, model, dataset): <a target="_blank" rel="noopener" href="https://compvis.github.io/mask/">https://compvis.github.io/mask/</a></p><p><strong>Summary</strong></p><p>åŸºäºç¦»æ•£çŠ¶æ€æ¨¡å‹çš„ç”Ÿæˆæ¨¡å‹ç ”ç©¶ï¼Œé€šè¿‡è¿æ¥Masked Generativeæ¨¡å‹å’ŒNon-Autoregressiveæ¨¡å‹ï¼ˆå¦‚Diffusion Modelsï¼‰ï¼Œæ¢ç´¢å…¶åœ¨è§†è§‰é¢†åŸŸçš„å¯æ‰©å±•æ€§ã€‚ç ”ç©¶åŒ…æ‹¬ç»Ÿä¸€è®¾è®¡ç©ºé—´å†…çš„é€æ­¥åˆ†æã€å…¸å‹åˆ¤åˆ«ä»»åŠ¡çš„é‡æ–°æ„å»ºä»¥åŠæ–°çš„æ¡†æ¶Discrete Interpolantsçš„æå‡ºï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ç”Ÿæˆæ¨¡å‹ä¸­çš„ä¸¤ä¸ªèŒƒå¼ï¼šåŸºäºä¸‹ä¸€ç»„é¢„æµ‹çš„Masked Generativeæ¨¡å‹å’ŒåŸºäºä¸‹ä¸€å™ªå£°é¢„æµ‹çš„éè‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚Diffusion Modelsï¼‰å—åˆ°å…³æ³¨ã€‚</li><li>ç¦»æ•£çŠ¶æ€æ¨¡å‹è¢«ç”¨æ¥è¿æ¥è¿™ä¸¤ç§æ¨¡å‹ï¼Œå¹¶åœ¨è§†è§‰é¢†åŸŸæ¢ç´¢å…¶å¯æ‰©å±•æ€§ã€‚</li><li>åœ¨ç»Ÿä¸€è®¾è®¡ç©ºé—´å†…ï¼Œå¯¹ä¸¤ç§æ¨¡å‹è¿›è¡Œäº†é€æ­¥åˆ†æï¼ŒåŒ…æ‹¬æ—¶é—´æ­¥ç‹¬ç«‹æ€§ã€å™ªå£°æ—¶é—´è¡¨ã€æ¸©åº¦ã€æŒ‡å¯¼å¼ºåº¦ç­‰ã€‚</li><li>å°†å…¸å‹çš„åˆ¤åˆ«ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†å‰²ï¼‰é‡æ–°æ„å»ºä¸ºç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸Šçš„å»é®æ©è¿‡ç¨‹ã€‚</li><li>æå‡ºäº†åä¸ºDiscrete Interpolantsçš„æ¡†æ¶ï¼Œå®ç°äº†ä¸ä¹‹å‰çš„ç¦»æ•£çŠ¶æ€æ–¹æ³•ç›¸æ¯”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å“è¶Šæˆ–ç«äº‰æ€§èƒ½ã€‚</li><li>åˆ©ç”¨ç¦»æ•£çŠ¶æ€æ¨¡å‹ä¸­çš„[MASK]ï¼Œèƒ½å¤Ÿæ¶èµ·Masked Generativeå’ŒNon-autoregressive Diffusionæ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f2afa4aa8c3941337d72089d68866b6a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7bd916bdace6c59e807a319127cb89ee.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9c904c0be1bc653fb977f8f885bccd97.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7f6b3fb5bea1254bf5ba9645d84e0087.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9b8af394d33e303a111b6fbe5b092658.jpg" align="middle"></details><h2 id="Visual-Lexicon-Rich-Image-Features-in-Language-Space"><a href="#Visual-Lexicon-Rich-Image-Features-in-Language-Space" class="headerlink" title="Visual Lexicon: Rich Image Features in Language Space"></a>Visual Lexicon: Rich Image Features in Language Space</h2><p><strong>Authors:XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, Cordelia Schmid</strong></p><p>We present Visual Lexicon, a novel visual language that encodes rich image information into the text space of vocabulary tokens while retaining intricate visual details that are often challenging to convey in natural language. Unlike traditional methods that prioritize either high-level semantics (e.g., CLIP) or pixel-level reconstruction (e.g., VAE), ViLex simultaneously captures rich semantic content and fine visual details, enabling high-quality image generation and comprehensive visual scene understanding. Through a self-supervised learning pipeline, ViLex generates tokens optimized for reconstructing input images using a frozen text-to-image (T2I) diffusion model, preserving the detailed information necessary for high-fidelity semantic-level reconstruction. As an image embedding in the language space, ViLex tokens leverage the compositionality of natural languages, allowing them to be used independently as â€œtext tokensâ€ or combined with natural language tokens to prompt pretrained T2I models with both visual and textual inputs, mirroring how we interact with vision-language models (VLMs). Experiments demonstrate that ViLex achieves higher fidelity in image reconstruction compared to text embeddingsâ€“even with a single ViLex token. Moreover, ViLex successfully performs various DreamBooth tasks in a zero-shot, unsupervised manner without fine-tuning T2I models. Additionally, ViLex serves as a powerful vision encoder, consistently improving vision-language model performance across 15 benchmarks relative to a strong SigLIP baseline.</p><blockquote><p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºVisual Lexiconçš„æ–°å‹è§†è§‰è¯­è¨€ã€‚å®ƒèƒ½å¤Ÿå°†ä¸°å¯Œçš„å›¾åƒä¿¡æ¯ç¼–ç æˆè¯æ±‡ä»¤ç‰Œçš„æ–‡æœ¬ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤æ‚çš„è§†è§‰ç»†èŠ‚ï¼Œè¿™äº›è§†è§‰ç»†èŠ‚åœ¨è‡ªç„¶è¯­è¨€ä¼ è¾¾æ—¶å¸¸å¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ä¼ ç»Ÿçš„ä¼˜å…ˆå…³æ³¨é«˜çº§è¯­ä¹‰ï¼ˆä¾‹å¦‚CLIPï¼‰æˆ–åƒç´ çº§é‡å»ºï¼ˆä¾‹å¦‚VAEï¼‰çš„æ–¹æ³•ä¸åŒï¼ŒViLexèƒ½å¤ŸåŒæ—¶æ•è·ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ï¼Œä»è€Œå®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œå…¨é¢çš„è§†è§‰åœºæ™¯ç†è§£ã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ç®¡é“ï¼ŒViLexç”Ÿæˆäº†é’ˆå¯¹è¾“å…¥å›¾åƒé‡å»ºè€Œä¼˜åŒ–çš„ä»¤ç‰Œï¼Œä½¿ç”¨å†»ç»“çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œä¿ç•™ç”¨äºé«˜ä¿çœŸè¯­ä¹‰çº§é‡å»ºçš„è¯¦ç»†ä¿¡æ¯ã€‚ä½œä¸ºè¯­è¨€ç©ºé—´ä¸­çš„å›¾åƒåµŒå…¥ï¼ŒViLexä»¤ç‰Œåˆ©ç”¨è‡ªç„¶è¯­è¨€çš„ç»„åˆæ€§ï¼Œå¯ä»¥ç‹¬ç«‹ç”¨ä½œâ€œæ–‡æœ¬ä»¤ç‰Œâ€ï¼Œä¹Ÿå¯ä»¥ä¸è‡ªç„¶è¯­è¨€ä»¤ç‰Œç»“åˆï¼Œæç¤ºé¢„è®­ç»ƒçš„T2Iæ¨¡å‹è¿›è¡Œè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„äº¤äº’æ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å•ä¸ªViLexä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒViLexåœ¨å›¾åƒé‡å»ºæ–¹é¢ä¹Ÿæ¯”æ–‡æœ¬åµŒå…¥å®ç°äº†æ›´é«˜çš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼ŒViLexæˆåŠŸåœ°åœ¨é›¶æ ·æœ¬ã€æ— ç›‘ç£çš„æ–¹å¼ä¸‹æ‰§è¡Œå„ç§DreamBoothä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒT2Iæ¨¡å‹ã€‚å¦å¤–ï¼ŒViLexä½œä¸ºä¸€ç§å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨ï¼Œç›¸å¯¹äºå¼ºå¤§çš„SigLIPåŸºå‡†æµ‹è¯•ï¼Œåœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸æ–­æ”¹è¿›è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06774v1">PDF</a> Tech report. 16 pages, 10 figures</p><p><strong>Summary</strong></p><p>è§†è§‰è¯æ±‡æ˜¯ä¸€ç§æ–°å‹è§†è§‰è¯­è¨€ï¼Œèƒ½å°†ä¸°å¯Œçš„å›¾åƒä¿¡æ¯ç¼–ç æˆè¯æ±‡ç¬¦å·ï¼ŒåŒæ—¶ä¿ç•™å¤æ‚çš„è§†è§‰ç»†èŠ‚ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒViLexå¯æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ï¼Œå®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œå…¨é¢çš„è§†è§‰åœºæ™¯ç†è§£ã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ ç”Ÿæˆä¼˜åŒ–çš„ç¬¦å·ï¼Œç”¨äºé‡å»ºå›¾åƒï¼Œå¹¶å¯åœ¨è¯­è¨€ç©ºé—´ä¸­ä½œä¸ºç‹¬ç«‹çš„â€œæ–‡æœ¬ç¬¦å·â€ä½¿ç”¨æˆ–ç»“åˆè‡ªç„¶è¯­è¨€ç¬¦å·ã€‚å®éªŒè¯æ˜ï¼ŒViLexåœ¨å›¾åƒé‡å»ºæ–¹é¢å…·æœ‰æ›´é«˜çš„ä¿çœŸåº¦ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ã€‚åŒæ—¶ä½œä¸ºå¼ºå¤§çš„è§†è§‰ç¼–ç å™¨ï¼Œå®ƒæ”¹è¿›äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ViLexæ˜¯ä¸€ç§è§†è§‰è¯­è¨€ï¼Œèƒ½å°†å›¾åƒä¿¡æ¯ç¼–ç æˆè¯æ±‡ç¬¦å·ã€‚</li><li>ViLexå¯æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚</li><li>ViLexé€šè¿‡è‡ªç›‘ç£å­¦ä¹ ç”Ÿæˆä¼˜åŒ–çš„ç¬¦å·ç”¨äºé‡å»ºå›¾åƒã€‚</li><li>ViLexå¯å®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œå…¨é¢çš„è§†è§‰åœºæ™¯ç†è§£ã€‚</li><li>ViLexç¬¦å·å¯åœ¨è¯­è¨€ç©ºé—´ä¸­ç‹¬ç«‹ä½¿ç”¨æˆ–ç»“åˆè‡ªç„¶è¯­è¨€ç¬¦å·ä½¿ç”¨ã€‚</li><li>ViLexåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­å…·æœ‰é«˜ä¿çœŸåº¦ï¼Œè¡¨ç°å‡ºä¼˜ç§€æ€§èƒ½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-72df5042ef65a43a6123414e25877c7d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8dae4020d6c37c4e42ce041d1175f272.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b9b5d9c577f64f8f50b0e1edf057459d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a1a0b5381ccc18c183bc192b5120ecdb.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-902ce2c0af4b1bb6c3476a019175d349.jpg" align="middle"></details><h2 id="3D-Graph-Attention-Networks-for-High-Fidelity-Pediatric-Glioma-Segmentation"><a href="#3D-Graph-Attention-Networks-for-High-Fidelity-Pediatric-Glioma-Segmentation" class="headerlink" title="3D Graph Attention Networks for High Fidelity Pediatric Glioma   Segmentation"></a>3D Graph Attention Networks for High Fidelity Pediatric Glioma Segmentation</h2><p><strong>Authors:Harish Thangaraj, Diya Katariya, Eshaan Joshi, Sangeetha N</strong></p><p>Pediatric brain tumors, particularly gliomas, represent a significant cause of cancer related mortality in children with complex infiltrative growth patterns that complicate treatment. Early, accurate segmentation of these tumors in neuroimaging data is crucial for effective diagnosis and intervention planning. This study presents a novel 3D UNet architecture with a spatial attention mechanism tailored for automated segmentation of pediatric gliomas. Using the BraTS pediatric glioma dataset with multiparametric MRI data, the proposed model captures multi-scale features and selectively attends to tumor relevant regions, enhancing segmentation precision and reducing interference from surrounding tissue. The modelâ€™s performance is quantitatively evaluated using the Dice similarity coefficient and HD95, demonstrating improved delineation of complex glioma structured. This approach offers a promising advancement in automating pediatric glioma segmentation, with the potential to improve clinical decision making and outcomes.</p><blockquote><p>å„¿ç«¥è„‘è‚¿ç˜¤ï¼Œç‰¹åˆ«æ˜¯èƒ¶è´¨ç˜¤ï¼Œæ˜¯å¯¼è‡´å„¿ç«¥ç™Œç—‡ç›¸å…³æ­»äº¡çš„é‡è¦åŸå› ï¼Œå…¶å¤æ‚çš„æµ¸æ¶¦æ€§ç”Ÿé•¿æ¨¡å¼ä½¿æ²»ç–—å˜å¾—å¤æ‚ã€‚åœ¨ç¥ç»å½±åƒæ•°æ®ä¸­æ—©æœŸã€å‡†ç¡®åœ°åˆ†å‰²è¿™äº›è‚¿ç˜¤å¯¹äºæœ‰æ•ˆè¯Šæ–­å’Œæ²»ç–—å¹²é¢„è®¡åˆ’è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„3D UNetæ¶æ„ï¼Œç»“åˆç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸“é—¨ç”¨äºå„¿ç«¥èƒ¶è´¨ç˜¤çš„è‡ªåŠ¨åˆ†å‰²ã€‚è¯¥ç ”ç©¶ä½¿ç”¨BraTSå„¿ç«¥èƒ¶è´¨ç˜¤æ•°æ®é›†å’Œå¤šå‚æ•°MRIæ•°æ®ï¼Œæ‰€æå‡ºæ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶é€‰æ‹©æ€§å…³æ³¨è‚¿ç˜¤ç›¸å…³åŒºåŸŸï¼Œæé«˜åˆ†å‰²ç²¾åº¦ï¼Œå‡å°‘å‘¨å›´ç»„ç»‡çš„å¹²æ‰°ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½é€šè¿‡Diceç›¸ä¼¼ç³»æ•°å’ŒHD95è¿›è¡Œå®šé‡è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚èƒ¶è´¨ç˜¤ç»“æ„æç»˜ä¸Šçš„æ”¹è¿›ã€‚è¯¥æ–¹æ³•åœ¨å°å„¿èƒ¶è´¨ç˜¤åˆ†å‰²è‡ªåŠ¨åŒ–æ–¹é¢å–å¾—äº†æœ‰å¸Œæœ›çš„è¿›å±•ï¼Œæœ‰æœ›æ”¹å–„ä¸´åºŠå†³ç­–å’Œæ²»ç–—æ•ˆæœã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06743v1">PDF</a> 8 pages, 9 figures</p><p><strong>Summary</strong><br>ç ”ç©¶é‡‡ç”¨æ–°å‹3D UNetæ¶æ„ç»“åˆç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å„¿ç«¥èƒ¶è´¨ç˜¤çš„è‡ªåŠ¨åˆ†å‰²ã€‚åˆ©ç”¨BraTSå„¿ç«¥èƒ¶è´¨ç˜¤æ•°æ®é›†å’Œå¤šå‚æ•°MRIæ•°æ®ï¼Œæ¨¡å‹æ•æ‰å¤šå°ºåº¦ç‰¹å¾ï¼Œé€‰æ‹©æ€§å…³æ³¨è‚¿ç˜¤ç›¸å…³åŒºåŸŸï¼Œæé«˜åˆ†å‰²ç²¾åº¦ï¼Œå‡å°‘å‘¨å›´ç»„ç»‡çš„å¹²æ‰°ã€‚æ­¤ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–å„¿ç«¥èƒ¶è´¨ç˜¤åˆ†å‰²æä¾›äº†æœ‰å‰æ™¯çš„è¿›å±•ï¼Œæœ‰æœ›æ”¹å–„ä¸´åºŠå†³ç­–å’Œç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å„¿ç«¥è„‘è‚¿ç˜¤ï¼ˆç‰¹åˆ«æ˜¯èƒ¶è´¨ç˜¤ï¼‰æ˜¯å„¿ç«¥ç™Œç—‡æ­»äº¡çš„é‡è¦åŸå› ï¼Œå…¶å¤æ‚æµ¸æ¶¦æ€§ç”Ÿé•¿æ¨¡å¼ä½¿æ²»ç–—å˜å¾—å¤æ‚ã€‚</li><li>æ—©æœŸã€å‡†ç¡®çš„è„‘è‚¿ç˜¤ç¥ç»å½±åƒæ•°æ®åˆ†å‰²å¯¹æœ‰æ•ˆè¯Šæ–­å’Œæ²»ç–—å¹²é¢„è‡³å…³é‡è¦ã€‚</li><li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„3D UNetæ¶æ„ï¼Œç»“åˆç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºè‡ªåŠ¨åˆ†å‰²å„¿ç«¥èƒ¶è´¨ç˜¤ã€‚</li><li>ä½¿ç”¨BraTSå„¿ç«¥èƒ¶è´¨ç˜¤æ•°æ®é›†å’Œå¤šå‚æ•°MRIæ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚</li><li>æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶é€‰æ‹©æ€§å…³æ³¨è‚¿ç˜¤ç›¸å…³åŒºåŸŸï¼Œæé«˜åˆ†å‰²ç²¾åº¦ã€‚</li><li>æ¨¡å‹é€šè¿‡Diceç›¸ä¼¼ç³»æ•°å’ŒHD95è¿›è¡Œäº†å®šé‡è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤æ‚èƒ¶è´¨ç˜¤ç»“æ„ä¸Šçš„æ”¹è¿›ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6aac17379dc04ed205a6bf79336ec15f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-84843ef6f2fa6703bc87e982f470a1a7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d6d1187e985b6d62336f920c6c696915.jpg" align="middle"></details><h2 id="Toward-Non-Invasive-Diagnosis-of-Bankart-Lesions-with-Deep-Learning"><a href="#Toward-Non-Invasive-Diagnosis-of-Bankart-Lesions-with-Deep-Learning" class="headerlink" title="Toward Non-Invasive Diagnosis of Bankart Lesions with Deep Learning"></a>Toward Non-Invasive Diagnosis of Bankart Lesions with Deep Learning</h2><p><strong>Authors:Sahil Sethi, Sai Reddy, Mansi Sakarvadia, Jordan Serotte, Darlington Nwaudo, Nicholas Maassen, Lewis Shi</strong></p><p>Bankart lesions, or anterior-inferior glenoid labral tears, are diagnostically challenging on standard MRIs due to their subtle imaging features-often necessitating invasive MRI arthrograms (MRAs). This study develops deep learning (DL) models to detect Bankart lesions on both standard MRIs and MRAs, aiming to improve diagnostic accuracy and reduce reliance on MRAs. We curated a dataset of 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for Bankart lesion diagnosis. Separate DL models for MRAs and standard MRIs were trained using the Swin Transformer architecture, pre-trained on a public knee MRI dataset. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). Bankart lesions were identified in 31.9% of MRAs and 8.6% of standard MRIs. The models achieved AUCs of 0.87 (86% accuracy, 83% sensitivity, 86% specificity) and 0.90 (85% accuracy, 82% sensitivity, 86% specificity) on standard MRIs and MRAs, respectively. These results match or surpass radiologist performance on our dataset and reported literature metrics. Notably, our modelâ€™s performance on non-invasive standard MRIs matched or surpassed the radiologists interpreting MRAs. This study demonstrates the feasibility of using DL to address the diagnostic challenges posed by subtle pathologies like Bankart lesions. Our models demonstrate potential to improve diagnostic confidence, reduce reliance on invasive imaging, and enhance accessibility to care.</p><blockquote><p>BankartæŸä¼¤ï¼Œä¹Ÿç§°ä¸ºè‚©å…³èŠ‚å‰ä¸‹ç›‚å”‡æ’•è£‚ï¼Œåœ¨æ ‡å‡†MRIä¸Šçš„è¯Šæ–­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå…¶ç»†å¾®çš„æˆåƒç‰¹å¾é€šå¸¸éœ€è¦ä¾µå…¥æ€§çš„MRIå…³èŠ‚é€ å½±æœ¯ï¼ˆMRAsï¼‰ã€‚æœ¬ç ”ç©¶å¼€å‘äº†æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ï¼Œå¯åœ¨æ ‡å‡†MRIå’ŒMRAsä¸Šæ£€æµ‹BankartæŸä¼¤ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å¹¶å‡å°‘å¯¹MRAsçš„ä¾èµ–ã€‚æˆ‘ä»¬ä»æ¥å—å…³èŠ‚é•œæ£€æŸ¥çš„558ä¾‹æ‚£è€…ä¸­ç²¾å¿ƒæŒ‘é€‰äº†ä¸€ä¸ªåŒ…å«586ä¸ªè‚©å…³èŠ‚MRIï¼ˆ335ä¸ªæ ‡å‡†MRIï¼Œ251ä¸ªMRAsï¼‰çš„æ•°æ®é›†ã€‚çœŸå®æ ‡ç­¾æ¥è‡ªæœ¯ä¸­æ£€æŸ¥ç»“æœï¼Œè¿™æ˜¯è¯Šæ–­BankartæŸä¼¤çš„é‡‘æ ‡å‡†ã€‚é’ˆå¯¹MRAså’Œæ ‡å‡†MRIçš„å•ç‹¬DLæ¨¡å‹ä½¿ç”¨Swin Transformeræ¶æ„è¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¶æ„åœ¨å…¬å…±è†å…³èŠ‚MRIæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚å°†ä»çŸ¢çŠ¶é¢ã€è½´é¢å’Œå† çŠ¶é¢å¾—åˆ°çš„é¢„æµ‹ç»“æœè¿›è¡Œç»„åˆï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚æ¨¡å‹åœ¨20%çš„ä¿ç•™æµ‹è¯•é›†ï¼ˆ117ä¸ªMRIï¼š46ä¸ªMRAsï¼Œ71ä¸ªæ ‡å‡†MRIï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚åœ¨MRAsä¸­æ£€æµ‹å‡ºBankartæŸä¼¤çš„å 31.9ï¼…ï¼Œåœ¨æ ‡å‡†MRIä¸­æ£€æµ‹å‡ºBankartæŸä¼¤çš„å 8.6ï¼…ã€‚æ¨¡å‹åœ¨æ ‡å‡†MRIå’ŒMRAsä¸Šçš„AUCå€¼åˆ†åˆ«ä¸º0.87ï¼ˆå‡†ç¡®ç‡86ï¼…ï¼Œçµæ•åº¦83ï¼…ï¼Œç‰¹å¼‚åº¦86ï¼…ï¼‰å’Œ0.90ï¼ˆå‡†ç¡®ç‡85ï¼…ï¼Œçµæ•åº¦82ï¼…ï¼Œç‰¹å¼‚åº¦86ï¼…ï¼‰ã€‚è¿™äº›ç»“æœåœ¨æˆ‘ä»¬çš„æ•°æ®é›†å’ŒæŠ¥å‘Šçš„æ–‡çŒ®æŒ‡æ ‡ä¸Šä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„æ€§èƒ½ç›¸åŒ¹é…æˆ–æ›´é«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨éä¾µå…¥æ€§æ ‡å‡†MRIä¸Šçš„æ€§èƒ½ä¸è§£è¯»MRAsçš„æ”¾å°„ç§‘åŒ»ç”Ÿç›¸åŒ¹é…æˆ–æ›´é«˜ã€‚è¯¥ç ”ç©¶è¯æ˜äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ è§£å†³ç”±ç»†å¾®ç—…ç†å˜åŒ–å¼•èµ·çš„è¯Šæ–­æŒ‘æˆ˜ï¼ˆå¦‚BankartæŸä¼¤ï¼‰çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ½œåŠ›æé«˜è¯Šæ–­ä¿¡å¿ƒï¼Œå‡å°‘å¯¹ä¾µå…¥æ€§æˆåƒçš„ä¾èµ–ï¼Œå¹¶æé«˜æŠ¤ç†å¯åŠæ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06717v1">PDF</a> Accepted for presentation at SPIE Medical Imaging 2025: Computer-Aided Diagnosis. The manuscript is expected to appear in the conference proceedings</p><p><strong>Summary</strong><br>æœ¬ç ”ç©¶åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ£€æµ‹Bankartç—…å˜çš„æ ‡å‡†MRIå’ŒMRIå…³èŠ‚é€ å½±æœ¯ï¼ˆMRAsï¼‰å›¾åƒï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å¹¶å‡å°‘å¯¹MRAsçš„ä¾èµ–ã€‚ç ”ç©¶ä½¿ç”¨è‚©éƒ¨çš„MRIæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹é‡‡ç”¨Swin Transformeræ¶æ„ï¼Œå¹¶åœ¨å…¬å…±è†å…³èŠ‚MRIæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œå¯¹æ ‡å‡†MRIå’ŒMRAsçš„è¯†åˆ«èƒ½åŠ›å‡è¾¾åˆ°è¾ƒé«˜æ°´å¹³ã€‚ç ”ç©¶è¯æ˜äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ è§£å†³ç»†å¾®ç—…å˜è¯Šæ–­æŒ‘æˆ˜çš„å¯è¡Œæ€§ï¼Œæœ‰æœ›æå‡è¯Šæ–­ä¿¡å¿ƒã€å‡å°‘ä¾µå…¥æ€§æˆåƒçš„ä¾èµ–ï¼Œå¹¶æé«˜åŒ»ç–—æœåŠ¡å¯åŠæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>Bankartç—…å˜åœ¨æ ‡å‡†MRIä¸Šçš„è¯Šæ–­å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¸¸éœ€å€ŸåŠ©ä¾µå…¥æ€§çš„MRIå…³èŠ‚é€ å½±æœ¯ï¼ˆMRAsï¼‰ã€‚</li><li>æœ¬ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥æ£€æµ‹Bankartç—…å˜ï¼Œæ—¨åœ¨æé«˜è¯Šæ–­å‡†ç¡®æ€§å¹¶å‡å°‘MRAsçš„ä¾èµ–ã€‚</li><li>ç ”ç©¶é‡‡ç”¨Swin Transformeræ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶åœ¨å…¬å…±è†å…³èŠ‚MRIæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li><li>æ¨¡å‹ç»“åˆäº†çŸ¢çŠ¶é¢ã€è½´é¢å’Œå† çŠ¶é¢çš„é¢„æµ‹æ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li><li>æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œå¯¹Bankartç—…å˜çš„è¯†åˆ«èƒ½åŠ›è¾ƒé«˜ã€‚</li><li>æ¨¡å‹æ€§èƒ½ä¸æ”¾å°„ç§‘åŒ»ç”Ÿçš„è¡¨ç°ç›¸å½“ï¼Œç”šè‡³åœ¨éä¾µå…¥æ€§çš„æ ‡å‡†MRIä¸Šè¡¨ç°æ›´ä½³ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4111dc444f6bacb0399826a43871c1fb.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-badf7ac16f2c48803c403159958f3d9f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9aac63a96ceb739401e6e3ee8ea5f0b6.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4f76c7a360ad1cde534ce47499aa363c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c1b6299d3ab26b9ede0fb75ac7a2fd2c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2876f4763b06b3a550af5a2e9bdc447b.jpg" align="middle"></details><h2 id="Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation"><a href="#Knowledge-Transfer-and-Domain-Adaptation-for-Fine-Grained-Remote-Sensing-Image-Segmentation" class="headerlink" title="Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing   Image Segmentation"></a>Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing Image Segmentation</h2><p><strong>Authors:Shun Zhang, Xuechao Zou, Kai Li, Congyan Lang, Shiying Wang, Pin Tao, Tengfei Cao</strong></p><p>Fine-grained remote sensing image segmentation is essential for accurately identifying detailed objects in remote sensing images. Recently, vision transformer models (VTM) pretrained on large-scale datasets have shown strong zero-shot generalization, indicating that they have learned the general knowledge of object understanding. We introduce a novel end-to-end learning paradigm combining knowledge guidance with domain refinement to enhance performance. We present two key components: the Feature Alignment Module (FAM) and the Feature Modulation Module (FMM). FAM aligns features from a CNN-based backbone with those from the pretrained VTMâ€™s encoder using channel transformation and spatial interpolation, and transfers knowledge via KL divergence and L2 normalization constraint. FMM further adapts the knowledge to the specific domain to address domain shift. We also introduce a fine-grained grass segmentation dataset and demonstrate, through experiments on two datasets, that our method achieves a significant improvement of 2.57 mIoU on the grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the potential of combining knowledge transfer and domain adaptation to overcome domain-related challenges and data limitations. The project page is available at <a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/">https://xavierjiezou.github.io/KTDA/</a>.</p><blockquote><p>ç²¾ç»†é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†å¯¹è±¡è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œåœ¨å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨æ¨¡å‹ï¼ˆVTMï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¿™è¡¨æ˜å®ƒä»¬å·²ç»å­¦ä¹ äº†å¯¹è±¡ç†è§£çš„ä¸€èˆ¬çŸ¥è¯†ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’Œé¢†åŸŸç²¾åŒ–çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡å¯¹CNNéª¨å¹²ç½‘ç»œä¸é¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾è¿›è¡Œé€šé“å˜æ¢å’Œç©ºé—´æ’å€¼æ¥å¯¹é½ç‰¹å¾ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚FMMè¿›ä¸€æ­¥å°†çŸ¥è¯†é€‚åº”åˆ°ç‰¹å®šé¢†åŸŸä»¥è§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç²¾ç»†çš„è‰åˆ†å‰²æ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä¸¤ä¸ªæ•°æ®é›†çš„å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‰æ•°æ®é›†ä¸Šå®ç°äº†2.57 mIoUçš„æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šå®ç°äº†3.73 mIoUçš„æ”¹è¿›ã€‚ç»“æœçªå‡ºäº†ç»“åˆçŸ¥è¯†è½¬ç§»å’Œé¢†åŸŸé€‚åº”ä»¥å…‹æœé¢†åŸŸç›¸å…³æŒ‘æˆ˜å’Œæ•°æ®é™åˆ¶çš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://xavierjiezou.github.io/KTDA/">https://xavierjiezou.github.io/KTDA/</a>è®¿é—®ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06664v1">PDF</a> 6 pages, 4 figures, 6 tables</p><p><strong>Summary</strong></p><p>åŸºäºé¥æ„Ÿå›¾åƒçš„ç²¾ç»†ç²’åº¦åˆ†å‰²å¯¹äºå‡†ç¡®è¯†åˆ«é¥æ„Ÿå›¾åƒä¸­çš„è¯¦ç»†å¯¹è±¡è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’ŒåŸŸç²¾åŒ–çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ï¼Œå¹¶è®¾è®¡äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ã€‚FAMé€šè¿‡å¯¹é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼ï¼Œå°†å¯¹æ¥è‡ªCNNä¸»å¹²ç½‘çš„ç‰¹å¾ä¸é¢„è®­ç»ƒVTMç¼–ç å™¨çš„ç‰¹å¾å¯¹é½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚FMMè¿›ä¸€æ­¥è°ƒæ•´çŸ¥è¯†ä»¥é€‚åº”ç‰¹å®šé¢†åŸŸï¼Œä»¥è§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‰ç±»æ•°æ®é›†ä¸Šæé«˜äº†2.57 mIoUï¼Œåœ¨äº‘æ•°æ®é›†ä¸Šæé«˜äº†3.73 mIoUã€‚ç»“æœçªæ˜¾äº†ç»“åˆçŸ¥è¯†è½¬ç§»å’Œé¢†åŸŸé€‚åº”ä»¥å…‹æœé¢†åŸŸç›¸å…³æŒ‘æˆ˜å’Œæ•°æ®é™åˆ¶çš„æ½œåŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ç²¾ç»†ç²’åº¦é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºè¯†åˆ«å›¾åƒä¸­çš„è¯¦ç»†å¯¹è±¡éå¸¸é‡è¦ã€‚</li><li>å¼•å…¥äº†ä¸€ç§ç»“åˆçŸ¥è¯†å¼•å¯¼å’ŒåŸŸç²¾åŒ–çš„æ–°å‹ç«¯åˆ°ç«¯å­¦ä¹ èŒƒå¼ã€‚</li><li>è®¾è®¡äº†ç‰¹å¾å¯¹é½æ¨¡å—ï¼ˆFAMï¼‰å’Œç‰¹å¾è°ƒåˆ¶æ¨¡å—ï¼ˆFMMï¼‰ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li><li>FAMé€šè¿‡å¯¹é€šé“å˜æ¢å’Œç©ºé—´æ’å€¼å®ç°ç‰¹å¾å¯¹é½ï¼Œå¹¶é€šè¿‡KLæ•£åº¦å’ŒL2å½’ä¸€åŒ–çº¦æŸè¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚</li><li>FMMèƒ½å¤Ÿè°ƒæ•´çŸ¥è¯†ä»¥é€‚åº”ç‰¹å®šé¢†åŸŸï¼Œè§£å†³é¢†åŸŸåç§»é—®é¢˜ã€‚</li><li>åœ¨è‰ç±»æ•°æ®é›†å’Œäº‘æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œåˆ†åˆ«æé«˜äº†2.57 mIoUå’Œ3.73 mIoUã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ff65a562adccd86e47010082ac3ea98b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-648c9797142b2b0b90510e7a47dad073.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-1220716c3f50fdc79d093a08a45d52aa.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f47fcfe914936041d50d22178ec6fa11.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e3181e9b388b3c1d2759b30c558ab4a3.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bb34410517c74bfa63ec36e02e16b1e2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bc5e284a6a54e28601378336ee575de9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6f16860972f0060ae21b43295a991d4e.jpg" align="middle"></details><h2 id="HES-UNet-A-U-Net-for-Hepatic-Echinococcosis-Lesion-Segmentation"><a href="#HES-UNet-A-U-Net-for-Hepatic-Echinococcosis-Lesion-Segmentation" class="headerlink" title="HES-UNet: A U-Net for Hepatic Echinococcosis Lesion Segmentation"></a>HES-UNet: A U-Net for Hepatic Echinococcosis Lesion Segmentation</h2><p><strong>Authors:Jiayan Chen, Kai Li, Zhanjin Wang, Zhan Wang, Jianqiang Huang</strong></p><p>Hepatic echinococcosis (HE) is a prevalent disease in economically underdeveloped pastoral areas, where adequate medical resources are usually lacking. Existing methods often ignore multi-scale feature fusion or focus only on feature fusion between adjacent levels, which may lead to insufficient feature fusion. To address these issues, we propose HES-UNet, an efficient and accurate model for HE lesion segmentation. This model combines convolutional layers and attention modules to capture local and global features. During downsampling, the multi-directional downsampling block (MDB) is employed to integrate high-frequency and low-frequency features, effectively extracting image details. The multi-scale aggregation block (MAB) aggregates multi-scale feature information. In contrast, the multi-scale upsampling Block (MUB) learns highly abstract features and supplies this information to the skip connection module to fuse multi-scale features. Due to the distinct regional characteristics of HE, there is currently no publicly available high-quality dataset for training our model. We collected CT slice data from 268 patients at a certain hospital to train and evaluate the model. The experimental results show that HES-UNet achieves state-of-the-art performance on our dataset, achieving an overall Dice Similarity Coefficient (DSC) of 89.21%, which is 1.09% higher than that of TransUNet. The project page is available at <a target="_blank" rel="noopener" href="https://chenjiayan-qhu.github.io/HES-UNet-page">https://chenjiayan-qhu.github.io/HES-UNet-page</a>.</p><blockquote><p>è‚æ£˜çƒè™«ç—…ï¼ˆHEï¼‰åœ¨ç»æµæ¬ å‘è¾¾çš„ç‰§åŒºæ˜¯ä¸€ç§å¸¸è§ç–¾ç—…ï¼Œè¿™äº›åœ°åŒºé€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„åŒ»ç–—èµ„æºã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å¿½ç•¥å¤šå°ºåº¦ç‰¹å¾èåˆï¼Œæˆ–è€…åªå…³æ³¨ç›¸é‚»çº§åˆ«ä¹‹é—´çš„ç‰¹å¾èåˆï¼Œè¿™å¯èƒ½å¯¼è‡´ç‰¹å¾èåˆä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HES-UNetï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å‡†ç¡®çš„HEç—…å˜åˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç»“åˆå·ç§¯å±‚å’Œæ³¨æ„åŠ›æ¨¡å—æ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚åœ¨ä¸‹é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å¤šæ–¹å‘ä¸‹é‡‡æ ·å—ï¼ˆMDBï¼‰æ¥èåˆé«˜é¢‘å’Œä½é¢‘ç‰¹å¾ï¼Œæœ‰æ•ˆåœ°æå–å›¾åƒç»†èŠ‚ã€‚å¤šå°ºåº¦èšåˆå—ï¼ˆMABï¼‰èšåˆå¤šå°ºåº¦ç‰¹å¾ä¿¡æ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤šå°ºåº¦ä¸Šé‡‡æ ·å—ï¼ˆMUBï¼‰å­¦ä¹ é«˜åº¦æŠ½è±¡çš„ç‰¹å¾ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯ä¾›åº”åˆ°è·³è¿‡è¿æ¥æ¨¡å—ä»¥èåˆå¤šå°ºåº¦ç‰¹å¾ã€‚ç”±äºHEå…·æœ‰ç‹¬ç‰¹çš„åŒºåŸŸç‰¹å¾ï¼Œç›®å‰å°šæ— å…¬å¼€çš„é«˜è´¨é‡æ•°æ®é›†å¯ä¾›è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬ä»æŸåŒ»é™¢çš„268åæ‚£è€…é‚£é‡Œæ”¶é›†äº†CTåˆ‡ç‰‡æ•°æ®æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHES-UNetåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ä¸º89.21%ï¼Œæ¯”TransUNeté«˜å‡º1.09%ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://chenjiayan-qhu.github.io/HES-UNet-page%E8%AE%BF%E9%97%AE%E3%80%82">https://chenjiayan-qhu.github.io/HES-UNet-pageè®¿é—®ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06530v1">PDF</a> 6 pages, 3 figures</p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è‚æ£˜çƒèš´ç—…ï¼ˆHEï¼‰ç—…å˜åˆ†å‰²çš„é«˜æ•ˆä¸”å‡†ç¡®çš„æ¨¡å‹HES-UNetã€‚è¯¥æ¨¡å‹ç»“åˆå·ç§¯å±‚å’Œæ³¨æ„åŠ›æ¨¡å—æ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨å¤šæ–¹å‘ä¸‹é‡‡æ ·å—ï¼ˆMDBï¼‰ã€å¤šå°ºåº¦èšåˆå—ï¼ˆMABï¼‰å’Œå¤šå°ºåº¦ä¸Šé‡‡æ ·å—ï¼ˆMUBï¼‰è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾èåˆã€‚ä½¿ç”¨æ¥è‡ª268åæ‚£è€…çš„CTåˆ‡ç‰‡æ•°æ®è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œå®éªŒç»“æœæ˜¾ç¤ºHES-UNetåœ¨æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ€»ä½“Diceç›¸ä¼¼åº¦ç³»æ•°ä¸º89.21%ï¼Œæ¯”TransUNeté«˜å‡º1.09%ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>HES-UNetæ¨¡å‹è¢«æå‡ºç”¨äºè§£å†³è‚æ£˜çƒèš´ç—…ï¼ˆHEï¼‰çš„ç—…å˜åˆ†å‰²é—®é¢˜ã€‚</li><li>æ¨¡å‹ç»“åˆäº†å·ç§¯å±‚å’Œæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚</li><li>é‡‡ç”¨å¤šæ–¹å‘ä¸‹é‡‡æ ·å—ï¼ˆMDBï¼‰è¿›è¡Œå›¾åƒç»†èŠ‚çš„æœ‰æ•ˆæå–ã€‚</li><li>å¤šå°ºåº¦èšåˆå—ï¼ˆMABï¼‰ç”¨äºèšåˆå¤šå°ºåº¦ç‰¹å¾ä¿¡æ¯ã€‚</li><li>å¤šå°ºåº¦ä¸Šé‡‡æ ·å—ï¼ˆMUBï¼‰å­¦ä¹ é«˜åº¦æŠ½è±¡çš„ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¸å¤šå°ºåº¦ç‰¹å¾èåˆã€‚</li><li>ç”±äºHEçš„ç‹¬ç‰¹åŒºåŸŸç‰¹æ€§ï¼Œæ¨¡å‹ä½¿ç”¨çš„æ˜¯è‡ªè¡Œæ”¶é›†çš„é«˜è´¨é‡çš„CTåˆ‡ç‰‡æ•°æ®ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a14e018c625a2fc83c6a62ab7041ba69.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a40eb33f9ce343028ec7a0ba9c2cd6a3.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f44f65e3ff5269f8bb02b67998ea0c6c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8dc65d5f5d17e657976c65885a582faa.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b17d8ddd0f4b2bac484b8369f8ecd369.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5387f69d8bd0acbaa4142d69d589282f.jpg" align="middle"></details><h2 id="Improving-text-conditioned-latent-diffusion-for-cancer-pathology"><a href="#Improving-text-conditioned-latent-diffusion-for-cancer-pathology" class="headerlink" title="Improving text-conditioned latent diffusion for cancer pathology"></a>Improving text-conditioned latent diffusion for cancer pathology</h2><p><strong>Authors:Aakash Madhav Rao, Debayan Gupta</strong></p><p>The development of generative models in the past decade has allowed for hyperrealistic data synthesis. While potentially beneficial, this synthetic data generation process has been relatively underexplored in cancer histopathology. One algorithm for synthesising a realistic image is diffusion; it iteratively converts an image to noise and learns the recovery process from this noise [Wang and Vastola, 2023]. While effective, it is highly computationally expensive for high-resolution images, rendering it infeasible for histopathology. The development of Variational Autoencoders (VAEs) has allowed us to learn the representation of complex high-resolution images in a latent space. A vital by-product of this is the ability to compress high-resolution images to space and recover them lossless. The marriage of diffusion and VAEs allows us to carry out diffusion in the latent space of an autoencoder, enabling us to leverage the realistic generative capabilities of diffusion while maintaining reasonable computational requirements. Rombach et al. [2021b] and Yellapragada et al. [2023] build foundational models for this task, paving the way to generate realistic histopathology images. In this paper, we discuss the pitfalls of current methods, namely [Yellapragada et al., 2023] and resolve critical errors while proposing improvements along the way. Our methods achieve an FID score of 21.11, beating its SOTA counterparts in [Yellapragada et al., 2023] by 1.2 FID, while presenting a train-time GPU memory usage reduction of 7%.</p><blockquote><p>è¿‡å»åå¹´ç”Ÿæˆæ¨¡å‹çš„å‘å±•ä½¿å¾—è¶…çœŸå®æ•°æ®åˆæˆæˆä¸ºå¯èƒ½ã€‚å°½ç®¡å…·æœ‰æ½œåœ¨ä¼˜åŠ¿ï¼Œä½†è¿™ç§åˆæˆæ•°æ®ç”Ÿæˆè¿‡ç¨‹åœ¨ç™Œç—‡ç»„ç»‡ç—…ç†å­¦æ–¹é¢ç›¸å¯¹è¢«ç ”ç©¶å¾—è¾ƒå°‘ã€‚åˆæˆé€¼çœŸå›¾åƒçš„ä¸€ç§ç®—æ³•æ˜¯æ‰©æ•£æ³•ï¼Œå®ƒå°†å›¾åƒé€æ­¥è½¬æ¢ä¸ºå™ªå£°ï¼Œå¹¶ä»è¿™ç§å™ªå£°ä¸­å­¦ä¹ æ¢å¤è¿‡ç¨‹ï¼ˆWang and Vastola, 2023ï¼‰ã€‚è™½ç„¶æœ‰æ•ˆï¼Œä½†å¯¹äºé«˜åˆ†è¾¨ç‡å›¾åƒæ¥è¯´ï¼Œå®ƒçš„è®¡ç®—æˆæœ¬éå¸¸é«˜ï¼Œä½¿å¾—å®ƒåœ¨ç»„ç»‡ç—…ç†å­¦ä¸­ä¸å¯è¡Œã€‚å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰çš„å‘å±•ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­å­¦ä¹ å¤æ‚é«˜åˆ†è¾¨ç‡å›¾åƒçš„ä»£è¡¨ã€‚è¿™çš„ä¸€ä¸ªé‡è¦å‰¯äº§å“æ˜¯èƒ½å¤Ÿå‹ç¼©é«˜åˆ†è¾¨ç‡å›¾åƒå¹¶æ— æŸæ¢å¤å®ƒä»¬ã€‚æ‰©æ•£å’ŒVAEsçš„ç»“åˆä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨è‡ªåŠ¨ç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ‰©æ•£ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨æ‰©æ•£çš„é€¼çœŸç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒåˆç†çš„è®¡ç®—è¦æ±‚ã€‚Rombachç­‰äººï¼ˆ2021bï¼‰å’ŒYellapragadaç­‰äººï¼ˆ2023ï¼‰ä¸ºè¿™é¡¹ä»»åŠ¡å»ºç«‹äº†åŸºç¡€æ¨¡å‹ï¼Œä¸ºç”Ÿæˆé€¼çœŸçš„ç»„ç»‡ç—…ç†å›¾åƒé“ºå¹³äº†é“è·¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰æ–¹æ³•çš„ä¸è¶³ï¼ˆå°¤å…¶æ˜¯Yellapragadaç­‰äººï¼ˆ2023ï¼‰çš„å·¥ä½œï¼‰ï¼Œå¹¶æå‡ºäº†è§£å†³å…³é”®é”™è¯¯çš„æ–¹æ³•ä»¥åŠåœ¨è¿‡ç¨‹ä¸­æå‡ºæ”¹è¿›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†FIDåˆ†æ•°ä¸º21.11çš„æˆç»©ï¼Œæ¯”Yellapragadaç­‰äººï¼ˆ2023ï¼‰çš„æœ€ä½³åŒè¡Œé«˜å‡º1.2 FIDï¼ŒåŒæ—¶å®ç°äº†è®­ç»ƒæ—¶GPUå†…å­˜ä½¿ç”¨å‡å°‘7%ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06487v1">PDF</a></p><p><strong>Summary</strong><br>ç”Ÿæˆæ¨¡å‹çš„å‘å±•ä½¿å¾—è¶…çœŸå®æ•°æ®åˆæˆæˆä¸ºå¯èƒ½ã€‚åœ¨ç™Œç—‡ç»„ç»‡ç—…ç†å­¦é¢†åŸŸï¼Œæ‰©æ•£ç®—æ³•ï¼ˆå¦‚è¿­ä»£å¼å™ªå£°å›¾åƒå¤åŸï¼‰æ˜¯åˆæˆç°å®å›¾åƒçš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†å…¶å¯¹é«˜åˆ†è¾¨ç‡å›¾åƒçš„è®¡ç®—éœ€æ±‚è¿‡é«˜ï¼Œä½¿å¾—å®é™…åº”ç”¨å—é™ã€‚å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰çš„æå‡ºï¼Œå¯ä»¥åœ¨æ½œåœ¨ç©ºé—´å†…å­¦ä¹ å¤æ‚é«˜åˆ†è¾¨ç‡å›¾åƒçš„è¡¨ç°ï¼Œå¹¶å¯è¿›è¡Œå›¾åƒæ— æŸå‹ç¼©ã€‚ç»“åˆæ‰©æ•£å’ŒVAEsï¼Œå¯åœ¨æ½œåœ¨ç©ºé—´å†…å®ç°æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥åˆ©ç”¨æ‰©æ•£çš„çœŸå®ç”Ÿæˆèƒ½åŠ›åŒæ—¶ä¿æŒåˆç†çš„è®¡ç®—éœ€æ±‚ã€‚æœ¬æ–‡è®¨è®ºäº†å½“å‰æ–¹æ³•çš„ç¼ºé™·å¹¶æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆï¼Œå®ç°äº†FIDåˆ†æ•°ä¸º21.11çš„æ•ˆæœï¼Œè¶…è¿‡äº†ä¹‹å‰çš„ç ”ç©¶æˆæœã€‚é€šè¿‡æœ¬æ–‡ç ”ç©¶å¯è¿›ä¸€æ­¥æ¨è¿›åŒ»å­¦å›¾åƒå¤„ç†é¢†åŸŸçš„æŠ€æœ¯å‘å±•ä¸åº”ç”¨å®è·µã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>ç”Ÿæˆæ¨¡å‹çš„å‘å±•æ¨åŠ¨äº†è¶…çœŸå®æ•°æ®åˆæˆæŠ€æœ¯çš„è¿›æ­¥ã€‚</li><li>æ‰©æ•£ç®—æ³•æ˜¯ä¸€ç§æœ‰æ•ˆçš„åˆæˆç°å®å›¾åƒçš„æ–¹æ³•ï¼Œä½†å¯¹è®¡ç®—èµ„æºçš„éœ€æ±‚è¾ƒé«˜ï¼Œé™åˆ¶äº†å…¶åœ¨é«˜åˆ†è¾¨ç‡å›¾åƒä¸­çš„åº”ç”¨ã€‚</li><li>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´å†…å­¦ä¹ å¤æ‚é«˜åˆ†è¾¨ç‡å›¾åƒçš„è¡¨ç°ï¼Œå¹¶æ”¯æŒå›¾åƒæ— æŸå‹ç¼©ã€‚</li><li>ç»“åˆæ‰©æ•£å’ŒVAEsæŠ€æœ¯ï¼Œå¯ä»¥åœ¨æ½œåœ¨ç©ºé—´å†…å®ç°æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡å¹¶ä¿ç•™ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ad05302611f2722837c9a8f307669d9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-8ca345e5021d7ecf10f380cd8a447cb4.jpg" align="middle"></details><h2 id="How-Certain-are-Uncertainty-Estimates-Three-Novel-Earth-Observation-Datasets-for-Benchmarking-Uncertainty-Quantification-in-Machine-Learning"><a href="#How-Certain-are-Uncertainty-Estimates-Three-Novel-Earth-Observation-Datasets-for-Benchmarking-Uncertainty-Quantification-in-Machine-Learning" class="headerlink" title="How Certain are Uncertainty Estimates? Three Novel Earth Observation   Datasets for Benchmarking Uncertainty Quantification in Machine Learning"></a>How Certain are Uncertainty Estimates? Three Novel Earth Observation Datasets for Benchmarking Uncertainty Quantification in Machine Learning</h2><p><strong>Authors:Yuanyuan Wang, Qian Song, Dawood Wasif, Muhammad Shahzad, Christoph Koller, Jonathan Bamber, Xiao Xiang Zhu</strong></p><p>Uncertainty quantification (UQ) is essential for assessing the reliability of Earth observation (EO) products. However, the extensive use of machine learning models in EO introduces an additional layer of complexity, as those models themselves are inherently uncertain. While various UQ methods do exist for machine learning models, their performance on EO datasets remains largely unevaluated. A key challenge in the community is the absence of the ground truth for uncertainty, i.e. how certain the uncertainty estimates are, apart from the labels for the image&#x2F;signal. This article fills this gap by introducing three benchmark datasets specifically designed for UQ in EO machine learning models. These datasets address three common problem types in EO: regression, image segmentation, and scene classification. They enable a transparent comparison of different UQ methods for EO machine learning models. We describe the creation and characteristics of each dataset, including data sources, preprocessing steps, and label generation, with a particular focus on calculating the reference uncertainty. We also showcase baseline performance of several machine learning models on each dataset, highlighting the utility of these benchmarks for model development and comparison. Overall, this article offers a valuable resource for researchers and practitioners working in artificial intelligence for EO, promoting a more accurate and reliable quality measure of the outputs of machine learning models. The dataset and code are accessible via <a target="_blank" rel="noopener" href="https://gitlab.lrz.de/ai4eo/WG_Uncertainty">https://gitlab.lrz.de/ai4eo/WG_Uncertainty</a>.</p><blockquote><p>ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰å¯¹äºè¯„ä¼°åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰äº§å“çš„å¯é æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œåœ¨åœ°çƒè§‚æµ‹ä¸­å¹¿æ³›ä½¿ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªé¢å¤–çš„å¤æ‚æ€§å±‚é¢ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æœ¬èº«å…·æœ‰å›ºæœ‰çš„ä¸ç¡®å®šæ€§ã€‚è™½ç„¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹å­˜åœ¨å„ç§ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œä½†å®ƒä»¬åœ¨åœ°çƒè§‚æµ‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªå¾—åˆ°è¯„ä¼°ã€‚ç¤¾åŒºé¢ä¸´çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ç¼ºä¹ä¸ç¡®å®šæ€§çš„çœŸå®åŸºå‡†ï¼Œå³é™¤äº†å›¾åƒ&#x2F;ä¿¡å·çš„æ ‡ç­¾å¤–ï¼Œä¸ç¡®å®šæ€§çš„ä¼°è®¡æœ‰å¤šç¡®å®šã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥ä¸‰ä¸ªä¸“é—¨ç”¨äºåœ°çƒè§‚æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„åŸºå‡†æ•°æ®é›†æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚è¿™äº›æ•°æ®é›†æ¶µç›–äº†åœ°çƒè§‚æµ‹ä¸­çš„ä¸‰ç§å¸¸è§ç±»å‹é—®é¢˜ï¼šå›å½’ã€å›¾åƒåˆ†å‰²å’Œåœºæ™¯åˆ†ç±»ã€‚å®ƒä»¬èƒ½å¤Ÿé€æ˜åœ°æ¯”è¾ƒä¸åŒçš„åœ°çƒè§‚æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬æè¿°äº†æ¯ä¸ªæ•°æ®é›†çš„åˆ›å»ºå’Œç‰¹å¾ï¼ŒåŒ…æ‹¬æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œæ ‡ç­¾ç”Ÿæˆï¼Œç‰¹åˆ«ä¾§é‡äºè®¡ç®—å‚è€ƒä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å‡ ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ¯ä¸ªæ•°æ®é›†ä¸­çš„åŸºå‡†æ€§èƒ½ï¼Œçªå‡ºäº†è¿™äº›åŸºå‡†å¯¹äºæ¨¡å‹å¼€å‘å’Œæ¯”è¾ƒçš„æœ‰ç”¨æ€§ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡ä¸ºä»äº‹åœ°çƒè§‚æµ‹äººå·¥æ™ºèƒ½çš„ç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å®è´µçš„èµ„æºï¼Œä¿ƒè¿›äº†æœºå™¨å­¦ä¹ æ¨¡å‹è¾“å‡ºè´¨é‡çš„æ›´å‡†ç¡®å’Œå¯é åº¦é‡ã€‚æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://gitlab.lrz.de/ai4eo/WG_Uncertainty%E8%AE%BF%E9%97%AE%E3%80%82">https://gitlab.lrz.de/ai4eo/WG_Uncertaintyè®¿é—®ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06451v1">PDF</a> Submitted to IEEE Geoscience and Remote Sensing Magazine</p><p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸ç¡®å®šæ€§é‡åŒ–åœ¨åœ°çƒè§‚æµ‹äº§å“è¯„ä¼°ä¸­çš„é‡è¦æ€§ï¼Œå¹¶é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åœ°çƒè§‚æµ‹ä¸­çš„åº”ç”¨æ‰€å¸¦æ¥çš„ä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸‰ä¸ªä¸“é—¨ç”¨äºåœ°çƒè§‚æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ç¡®å®šæ€§é‡åŒ–çš„åŸºå‡†æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†æ¶µç›–äº†å›å½’ã€å›¾åƒåˆ†å‰²å’Œåœºæ™¯åˆ†ç±»ä¸‰ç§å¸¸è§é—®é¢˜ç±»å‹ï¼Œä¸ºä¸åŒä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•åœ¨åœ°çƒè§‚æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„é€æ˜æ¯”è¾ƒæä¾›äº†å¯èƒ½ã€‚æ–‡ç« è¿˜ä»‹ç»äº†æ¯ä¸ªæ•°æ®é›†çš„åˆ›å»ºå’Œç‰¹æ€§ï¼ŒåŒ…æ‹¬æ•°æ®æ¥æºã€é¢„å¤„ç†æ­¥éª¤å’Œæ ‡ç­¾ç”Ÿæˆï¼Œå¹¶ç‰¹åˆ«å¼ºè°ƒäº†å‚è€ƒä¸ç¡®å®šæ€§çš„è®¡ç®—ã€‚æ­¤å¤–ï¼Œæ–‡ç« å±•ç¤ºäº†å‡ ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è¿™äº›åŸºå‡†æ•°æ®é›†ä¸Šçš„åŸºå‡†æ€§èƒ½ï¼Œå¼ºè°ƒäº†è¿™äº›åŸºå‡†å¯¹äºæ¨¡å‹å¼€å‘å’Œæ¯”è¾ƒçš„å®ç”¨æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ä¸ç¡®å®šæ€§é‡åŒ–åœ¨è¯„ä¼°åœ°çƒè§‚æµ‹äº§å“å¯é æ€§æ–¹é¢è‡³å…³é‡è¦ã€‚</li><li>æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åœ°çƒè§‚æµ‹ä¸­çš„ä½¿ç”¨å¼•å…¥äº†ä¸€å±‚é¢å¤–çš„ä¸ç¡®å®šæ€§ã€‚</li><li>ç°æœ‰ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•åœ¨åœ°çƒè§‚æµ‹æ•°æ®é›†ä¸Šçš„æ€§èƒ½å°šæœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚</li><li>æ–‡ä¸­ä»‹ç»äº†ä¸‰ä¸ªä¸“é—¨ç”¨äºåœ°çƒè§‚æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ç¡®å®šæ€§é‡åŒ–çš„åŸºå‡†æ•°æ®é›†ã€‚</li><li>è¿™äº›æ•°æ®é›†æ¶µç›–å›å½’ã€å›¾åƒåˆ†å‰²å’Œåœºæ™¯åˆ†ç±»ä¸‰ç§å¸¸è§é—®é¢˜ç±»å‹ã€‚</li><li>æ–‡ç« å¼ºè°ƒäº†å‚è€ƒä¸ç¡®å®šæ€§çš„è®¡ç®—å’Œæ•°æ®é›†çš„åˆ›å»ºç‰¹æ€§ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ac957adebfdb0c828e952841d833267d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2895556a36d5453fcf00f2109f44cba8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f92e8318486d413883be1b7ca68ad901.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c5fa07cb20635158ef894db1e42b3e17.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89c5f6d8a8172f27ddd5a8247f5a9bb2.jpg" align="middle"></details><h2 id="High-throughput-computational-screening-of-small-eco-friendly-molecular-crystals-for-sustainable-piezoelectric-materials"><a href="#High-throughput-computational-screening-of-small-eco-friendly-molecular-crystals-for-sustainable-piezoelectric-materials" class="headerlink" title="High-throughput computational screening of small, eco-friendly,   molecular crystals for sustainable piezoelectric materials"></a>High-throughput computational screening of small, eco-friendly, molecular crystals for sustainable piezoelectric materials</h2><p><strong>Authors:Shubham Vishnoi, Geetu Kumari, Robert Guest, Pierre-AndrÃ© Cazade, Sarah Guerin</strong></p><p>Organic molecular crystals are ideally placed to become next-generation piezoelectric materials due to their diverse chemistries that can be used to engineer tailor-made solid-state assemblies. Using crystal engineering principles, and techniques such as co-crystallisation, these materials can be engineered to have a wide range of electromechanical properties. For materials that have been structurally characterised by methods such as X-Ray Diffraction, computational chemistry is an effective tool to predict their electromechanical properties, allowing researchers to screen these molecular crystals and identify materials best suited to their chosen application. Here we present our database of small molecular crystals, and their Density Functional Theory (DFT) predicted electromechanical properties, CrystalDFT (<a target="_blank" rel="noopener" href="https://actuatelab.ie/CrystalDFT">https://actuatelab.ie/CrystalDFT</a>). We highlight the broad range of electromechanical properties amongst this primary dataset, and in particular, the high number of crystals that have a naturally occurring longitudinal d33 constant. This longitudinal electromechanical coupling is a prerequisite for several conventional sensing and energy harvesting applications, the presence of which is notably rare amongst the literature on biomolecular crystal piezoelectricity to date.</p><blockquote><p>æœ‰æœºåˆ†å­æ™¶ä½“å› å…¶ä¸°å¯Œçš„åŒ–å­¦ç‰¹æ€§ï¼Œå¯å®šåˆ¶å·¥ç¨‹åŒ–å›ºæ€ç»„è£…ç»“æ„ï¼Œæœ‰æœ›æˆä¸ºä¸‹ä¸€ä»£å‹ç”µææ–™ã€‚åˆ©ç”¨æ™¶ä½“å·¥ç¨‹åŸç†å’Œå…±ç»“æ™¶ç­‰æŠ€æœ¯ï¼Œå¯ä»¥è®¾è®¡è¿™äº›ææ–™å…·æœ‰å¹¿æ³›çš„æœºç”µæ€§èƒ½ã€‚å¯¹äºé€šè¿‡Xå°„çº¿è¡å°„ç­‰æ–¹æ³•è¿›è¡Œç»“æ„è¡¨å¾çš„ææ–™ï¼Œè®¡ç®—åŒ–å­¦æ˜¯é¢„æµ‹å…¶æœºç”µæ€§èƒ½çš„æœ‰æ•ˆå·¥å…·ï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿç­›é€‰è¿™äº›åˆ†å­æ™¶ä½“ï¼Œå¹¶ç¡®å®šæœ€é€‚åˆå…¶é€‰æ‹©åº”ç”¨çš„ææ–™ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬ä»‹ç»äº†å°åˆ†å­æ™¶ä½“çš„æ•°æ®åº“åŠå…¶åŸºäºå¯†åº¦æ³›å‡½ç†è®ºï¼ˆDFTï¼‰é¢„æµ‹çš„æœºç”µæ€§èƒ½CrystalDFTï¼ˆ<a target="_blank" rel="noopener" href="https://actuatelab.ie/CrystalDFT%EF%BC%89%E3%80%82%E6%88%91%E4%BB%AC%E5%BC%BA%E8%B0%83%E4%BA%86%E6%AD%A4%E4%B8%BB%E8%A6%81%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E6%9C%BA%E7%94%B5%E6%80%A7%E8%83%BD%E7%9A%84%E5%B9%BF%E6%B3%9B%E8%8C%83%E5%9B%B4%EF%BC%8C%E5%B0%A4%E5%85%B6%E6%98%AF%E5%85%B7%E6%9C%89%E9%AB%98%E8%87%AA%E7%84%B6%E7%BA%B5%E5%90%91d33%E5%B8%B8%E6%95%B0%E7%9A%84%E6%99%B6%E4%BD%93%E6%95%B0%E9%87%8F%E4%B9%8B%E5%A4%9A%E3%80%82%E8%BF%99%E7%A7%8D%E7%BA%B5%E5%90%91%E6%9C%BA%E7%94%B5%E8%80%A6%E5%90%88%E6%98%AF%E5%A4%9A%E7%A7%8D%E4%BC%A0%E7%BB%9F%E4%BC%A0%E6%84%9F%E5%92%8C%E8%83%BD%E9%87%8F%E6%94%B6%E9%9B%86%E5%BA%94%E7%94%A8%E7%9A%84%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6%EF%BC%8C%E5%9C%A8%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E7%9A%84%E7%94%9F%E7%89%A9%E5%88%86%E5%AD%90%E6%99%B6%E4%BD%93%E5%8E%8B%E7%94%B5%E6%80%A7%E8%83%BD%E7%9A%84%E6%96%87%E7%8C%AE%E4%B8%AD%E6%9E%81%E4%B8%BA%E7%BD%95%E8%A7%81%E3%80%82">https://actuatelab.ie/CrystalDFTï¼‰ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æ­¤ä¸»è¦æ•°æ®é›†ä¸­æœºç”µæ€§èƒ½çš„å¹¿æ³›èŒƒå›´ï¼Œå°¤å…¶æ˜¯å…·æœ‰é«˜è‡ªç„¶çºµå‘d33å¸¸æ•°çš„æ™¶ä½“æ•°é‡ä¹‹å¤šã€‚è¿™ç§çºµå‘æœºç”µè€¦åˆæ˜¯å¤šç§ä¼ ç»Ÿä¼ æ„Ÿå’Œèƒ½é‡æ”¶é›†åº”ç”¨çš„å‰ææ¡ä»¶ï¼Œåœ¨è¿„ä»Šä¸ºæ­¢çš„ç”Ÿç‰©åˆ†å­æ™¶ä½“å‹ç”µæ€§èƒ½çš„æ–‡çŒ®ä¸­æä¸ºç½•è§ã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06449v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ‰æœºåˆ†å­æ™¶ä½“å› å…¶ä¸°å¯Œçš„åŒ–å­¦ç‰¹æ€§ï¼Œæˆä¸ºä¸‹ä¸€ä»£å‹ç”µææ–™çš„ç†æƒ³é€‰æ‹©ã€‚é€šè¿‡æ™¶ä½“å·¥ç¨‹åŸç†å’Œå…±ç»“æ™¶ç­‰æŠ€æœ¯ï¼Œå¯ä»¥å®šåˆ¶å›ºæ€ç»„è£…ï¼Œä½¿å…¶å…·æœ‰å¹¿æ³›çš„æœºæ¢°ç”µå­ç‰¹æ€§ã€‚ç»“æ„ç‰¹å¾æ˜ç¡®çš„ææ–™ï¼Œé€šè¿‡å¦‚Xå°„çº¿è¡å°„ç­‰æ–¹æ³•è¿›è¡Œç ”ç©¶ï¼Œè®¡ç®—åŒ–å­¦æˆä¸ºé¢„æµ‹å…¶æœºæ¢°ç”µå­ç‰¹æ€§çš„æœ‰æ•ˆå·¥å…·ã€‚æœ¬æ–‡ä»‹ç»äº†å°åˆ†å­æ™¶ä½“æ•°æ®åº“åŠå…¶å¯†åº¦æ³›å‡½ç†è®ºé¢„æµ‹çš„æœºæ¢°ç”µå­ç‰¹æ€§â€”â€”CrystalDFTã€‚ç‰¹åˆ«å¼ºè°ƒäº†è¿™äº›æ™¶ä½“å¹¿æ³›çš„æœºæ¢°ç”µå­ç‰¹æ€§èŒƒå›´ï¼Œç‰¹åˆ«æ˜¯è‡ªç„¶çŠ¶æ€ä¸‹å…·æœ‰é«˜çºµå‘d33å¸¸æ•°çš„æ™¶ä½“æ•°é‡ä¼—å¤šã€‚è¿™ç§çºµå‘æœºæ¢°ç”µå­è€¦åˆæ˜¯è®¸å¤šä¼ ç»Ÿä¼ æ„Ÿå’Œèƒ½é‡é‡‡é›†åº”ç”¨çš„å‰ææ¡ä»¶ï¼Œè¿™åœ¨ç”Ÿç‰©åˆ†å­æ™¶ä½“å‹ç”µæ€§çš„æ–‡çŒ®ä¸­æä¸ºç½•è§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æœ‰æœºåˆ†å­æ™¶ä½“å› å…¶ä¸°å¯Œçš„åŒ–å­¦ç‰¹æ€§æˆä¸ºä¸‹ä¸€ä»£å‹ç”µææ–™çš„ç†æƒ³é€‰æ‹©ã€‚</li><li>é€šè¿‡æ™¶ä½“å·¥ç¨‹åŸç†å’Œå…±ç»“æ™¶æŠ€æœ¯ï¼Œå¯ä»¥å®šåˆ¶å›ºæ€ç»„è£…ä»¥å…·æœ‰å¹¿æ³›çš„æœºæ¢°ç”µå­ç‰¹æ€§ã€‚</li><li>ç»“æ„ç‰¹å¾æ˜ç¡®çš„ææ–™å¯ä»¥é€šè¿‡Xå°„çº¿è¡å°„æ–¹æ³•è¿›è¡Œç ”ç©¶ã€‚</li><li>è®¡ç®—åŒ–å­¦æ˜¯é¢„æµ‹ææ–™æœºæ¢°ç”µå­ç‰¹æ€§çš„æœ‰æ•ˆå·¥å…·ã€‚</li><li>æœ¬æ–‡ä»‹ç»äº†å°åˆ†å­æ™¶ä½“æ•°æ®åº“â€”â€”CrystalDFTã€‚</li><li>è¿™äº›æ™¶ä½“å…·æœ‰å¹¿æ³›çš„æœºæ¢°ç”µå­ç‰¹æ€§èŒƒå›´ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6e6cc0c2f66db4e825b9e482289d8af7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-18335fd034c07fda3cc552131bccdd00.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c6ef937d984634f2bf59d19f6f66ce06.jpg" align="middle"></details><h2 id="Not-All-Errors-Are-Equal-Investigation-of-Speech-Recognition-Errors-in-Alzheimerâ€™s-Disease-Detection"><a href="#Not-All-Errors-Are-Equal-Investigation-of-Speech-Recognition-Errors-in-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="Not All Errors Are Equal: Investigation of Speech Recognition Errors in   Alzheimerâ€™s Disease Detection"></a>Not All Errors Are Equal: Investigation of Speech Recognition Errors in Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Jiawen Kang, Junan Li, Jinchao Li, Xixin Wu, Helen Meng</strong></p><p>Automatic Speech Recognition (ASR) plays an important role in speech-based automatic detection of Alzheimerâ€™s disease (AD). However, recognition errors could propagate downstream, potentially impacting the detection decisions. Recent studies have revealed a non-linear relationship between word error rates (WER) and AD detection performance, where ASR transcriptions with notable errors could still yield AD detection accuracy equivalent to that based on manual transcriptions. This work presents a series of analyses to explore the effect of ASR transcription errors in BERT-based AD detection systems. Our investigation reveals that not all ASR errors contribute equally to detection performance. Certain words, such as stopwords, despite constituting a large proportion of errors, are shown to play a limited role in distinguishing AD. In contrast, the keywords related to diagnosis tasks exhibit significantly greater importance relative to other words. These findings provide insights into the interplay between ASR errors and the downstream detection model.</p><blockquote><p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨åŸºäºè¯­éŸ³çš„è‡ªåŠ¨æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç„¶è€Œï¼Œè¯†åˆ«é”™è¯¯å¯èƒ½ä¼šä¼ æ’­è‡³ä¸‹æ¸¸ï¼Œå¯èƒ½å½±å“æ£€æµ‹å†³ç­–ã€‚æœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸ADæ£€æµ‹æ€§èƒ½ä¹‹é—´çš„éçº¿æ€§å…³ç³»ï¼Œå…¶ä¸­ASRè½¬å½•çš„æ˜¾è‘—é”™è¯¯ä»ç„¶å¯èƒ½äº§ç”Ÿä¸æ‰‹åŠ¨è½¬å½•ç›¸å½“çš„ADæ£€æµ‹ç²¾åº¦ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ç³»åˆ—åˆ†æï¼Œä»¥æ¢è®¨ASRè½¬å½•é”™è¯¯å¯¹åŸºäºBERTçš„ADæ£€æµ‹ç³»ç»Ÿçš„å½±å“ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œå¹¶éæ‰€æœ‰çš„ASRé”™è¯¯éƒ½å¯¹æ£€æµ‹æ€§èƒ½äº§ç”ŸåŒç­‰å½±å“ã€‚æŸäº›è¯è¯­ï¼Œå¦‚åœç”¨è¯ï¼Œè™½ç„¶æ„æˆäº†å¤§é‡çš„é”™è¯¯ï¼Œä½†åœ¨åŒºåˆ†ADæ–¹é¢çš„ä½œç”¨æœ‰é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸è¯Šæ–­ä»»åŠ¡ç›¸å…³çš„å…³é”®è¯ç›¸å¯¹äºå…¶ä»–è¯è¯­è¡¨ç°å‡ºæ˜¾è‘—çš„é‡è¦æ€§ã€‚è¿™äº›å‘ç°æä¾›äº†ASRé”™è¯¯ä¸ä¸‹æ¸¸æ£€æµ‹æ¨¡å‹ä¹‹é—´ç›¸äº’ä½œç”¨çš„é‡è¦è§è§£ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06332v1">PDF</a> Accepted by IEEE ISCSLP 2024</p><p><strong>Summary</strong><br>è¯­éŸ³è¯†åˆ«åœ¨åŸºäºè¯­éŸ³çš„è‡ªåŠ¨æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†è¯†åˆ«é”™è¯¯å¯èƒ½å½±å“ä¸‹æ¸¸æ£€æµ‹å†³ç­–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯é”™è¯¯ç‡ä¸é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹æ€§èƒ½ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«è½¬å½•é”™è¯¯å¯¹åŸºäºBERTçš„é˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹ç³»ç»Ÿçš„æ€§èƒ½å½±å“ï¼Œå‘ç°å¹¶éæ‰€æœ‰ASRé”™è¯¯å¯¹æ£€æµ‹æ€§èƒ½äº§ç”ŸåŒæ ·çš„å½±å“ã€‚å¯¹äºä»»åŠ¡æ— å…³åœç”¨è¯äº§ç”Ÿå¤§é‡é”™è¯¯å¯¹æ£€æµ‹åŒºåˆ†åº¦å½±å“æœ‰é™ï¼Œè€Œè¯Šæ–­å…³é”®è¯çš„é‡è¦æ€§æ˜¾è‘—å¤§äºå…¶ä»–è¯æ±‡ã€‚è¿™ä¸ºASRé”™è¯¯ä¸ä¸‹æ¸¸æ£€æµ‹æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨æä¾›äº†è§è§£ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨åŸºäºè¯­éŸ³çš„é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚</li><li>ASRè¯†åˆ«é”™è¯¯å¯èƒ½å½±å“ä¸‹æ¸¸æ£€æµ‹å†³ç­–ã€‚</li><li>è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸ADæ£€æµ‹æ€§èƒ½ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ã€‚</li><li>åœ¨BERTä¸ºåŸºç¡€çš„ADæ£€æµ‹ç³»ç»Ÿä¸­ï¼Œå¹¶éæ‰€æœ‰ASRè½¬å½•é”™è¯¯å‡å¯¹æ£€æµ‹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“ã€‚</li><li>è¯Šæ–­ä»»åŠ¡æ— å…³åœç”¨è¯è™½ç„¶äº§ç”Ÿå¤§é‡é”™è¯¯ä½†å¯¹ADæ£€æµ‹åŒºåˆ†åº¦å½±å“æœ‰é™ã€‚</li><li>ä¸è¯Šæ–­ç›¸å…³çš„å…³é”®è¯åœ¨åŒºåˆ†é˜¿å°”èŒ¨æµ·é»˜ç—…æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„é‡è¦æ€§ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0ffb64c90a2b24839a4cdc0be52bc93a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-23a471c96cae17846ad62e941c0aba0f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d4d12ba5e3292041f35109a44b48cb73.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f57551b370fdb18d073632be708ff9c0.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-925dec1f53b4bcceb4d0abda80f36c1e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f75f571cf0af433be00c20dac8b6560c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cb3a59fb4badf3ac761c2ddd70cb94c3.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8c4d4cd13dcae93d4cc276661413ad1e.jpg" align="middle"></details><h2 id="CAD-Unet-A-Capsule-Network-Enhanced-Unet-Architecture-for-Accurate-Segmentation-of-COVID-19-Lung-Infections-from-CT-Images"><a href="#CAD-Unet-A-Capsule-Network-Enhanced-Unet-Architecture-for-Accurate-Segmentation-of-COVID-19-Lung-Infections-from-CT-Images" class="headerlink" title="CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate   Segmentation of COVID-19 Lung Infections from CT Images"></a>CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images</h2><p><strong>Authors:Yijie Dang, Weijun Ma, Xiaohu Luo</strong></p><p>Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: <a target="_blank" rel="noopener" href="https://github.com/AmanoTooko-jie/CAD-Unet">https://github.com/AmanoTooko-jie/CAD-Unet</a>.</p><blockquote><p>è‡ª2019å¹´COVID-19ç–«æƒ…çˆ†å‘ä»¥æ¥ï¼ŒåŒ»å­¦æˆåƒå·²æˆä¸ºè¯Šæ–­COVID-19è‚ºç‚çš„ä¸»è¦æ–¹å¼ã€‚åœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œä»è®¡ç®—æœºæ–­å±‚æ‰«æå›¾åƒä¸­å¯¹è‚ºéƒ¨æ„ŸæŸ“çš„åˆ†å‰²èƒ½å¤Ÿå®ç°COVID-19çš„å¿«é€Ÿå’Œå‡†ç¡®é‡åŒ–åŠè¯Šæ–­ã€‚å¯¹è‚ºéƒ¨COVID-19æ„ŸæŸ“çš„åˆ†å‰²æ„æˆäº†ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºç£¨ç»ç’ƒæ ·æµ‘æµŠè¡¨ç°æ‰€å‘ˆç°çš„è¾¹ç•Œæ¨¡ç³Šå’Œå¯¹æ¯”åº¦æœ‰é™ã€‚æ­¤å¤–ï¼Œæµ¸æ¶¦ã€è‚ºç»„ç»‡å’Œè‚ºå£ä¹‹é—´çš„æ··æ·†ç›¸ä¼¼æ€§è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ä¸€åˆ†å‰²ä»»åŠ¡çš„å¤æ‚æ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06314v1">PDF</a></p><p><strong>Summary</strong><br>æ–°å† ç–«æƒ…çˆ†å‘ä»¥æ¥ï¼ŒåŒ»å­¦æˆåƒæˆä¸ºè¯Šæ–­æ–°å† è‚ºç‚çš„ä¸»è¦æ‰‹æ®µä¹‹ä¸€ã€‚é’ˆå¯¹è‚ºéƒ¨CTå½±åƒä¸­æ–°å† è‚ºç‚æ„ŸæŸ“ç—…ç¶åˆ†å‰²çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ·±åº¦ç½‘ç»œæ¶æ„â€”â€”CAD-Unetã€‚è¯¥æ¶æ„ç»“åˆäº†èƒ¶å›Šç½‘ç»œä¸ç°æœ‰çš„Unetæ¡†æ¶ï¼Œé€šè¿‡çŸ¢é‡ä¼ é€’ä¿¡æ¯çš„æ–¹å¼ï¼Œæ›´ç²¾ç¡®åœ°æå–ç—…ç¶çš„ç©ºé—´ä¿¡æ¯ã€‚ç»è¿‡å››é¡¹å…¬å¼€æ•°æ®é›†çš„å®éªŒéªŒè¯ï¼Œè¯¥æ¨¡å‹åœ¨äºŒå…ƒåŠå¤šå…ƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>åŒ»å­¦æˆåƒæˆä¸ºè¯Šæ–­æ–°å† è‚ºç‚çš„ä¸»è¦æ‰‹æ®µä¹‹ä¸€ã€‚</li><li>è‚ºéƒ¨CTå½±åƒä¸­æ–°å† è‚ºç‚æ„ŸæŸ“ç—…ç¶åˆ†å‰²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li><li>CAD-Unetæ˜¯ä¸€ç§ç»“åˆäº†èƒ¶å›Šç½‘ç»œä¸Unetçš„æ–°å‹æ·±åº¦ç½‘ç»œæ¶æ„ã€‚</li><li>èƒ¶å›Šç½‘ç»œé€šè¿‡çŸ¢é‡ä¼ é€’ä¿¡æ¯ï¼Œæ›´ç²¾ç¡®åœ°æå–ç—…ç¶çš„ç©ºé—´ä¿¡æ¯ã€‚</li><li>CAD-Unetæ¨¡å‹åœ¨äºŒå…ƒåŠå¤šå…ƒåˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li><li>è¯¥æ¨¡å‹é€šè¿‡èåˆä¸¤ç§ç½‘ç»œç»“æ„çš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ•ˆçš„ä¿¡æ¯èåˆã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d2a1d8209923645ff9c452f729686751.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ae7321c06444552c7a663e4d37084177.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2cb3d35aeefb80741f45770c371c556d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e640c724c73a932c5c12baa69479f58e.jpg" align="middle"></details><h2 id="A-Lightweight-U-like-Network-Utilizing-Neural-Memory-Ordinary-Differential-Equations-for-Slimming-the-Decoder"><a href="#A-Lightweight-U-like-Network-Utilizing-Neural-Memory-Ordinary-Differential-Equations-for-Slimming-the-Decoder" class="headerlink" title="A Lightweight U-like Network Utilizing Neural Memory Ordinary   Differential Equations for Slimming the Decoder"></a>A Lightweight U-like Network Utilizing Neural Memory Ordinary Differential Equations for Slimming the Decoder</h2><p><strong>Authors:Quansong He, Xiaojun Yao, Jun Wu, Zhang Yi, Tao He</strong></p><p>In recent years, advanced U-like networks have demonstrated remarkable performance in medical image segmentation tasks. However, their drawbacks, including excessive parameters, high computational complexity, and slow inference speed, pose challenges for practical implementation in scenarios with limited computational resources. Existing lightweight U-like networks have alleviated some of these problems, but they often have pre-designed structures and consist of inseparable modules, limiting their application scenarios. In this paper, we propose three plug-and-play decoders by employing different discretization methods of the neural memory Ordinary Differential Equations (nmODEs). These decoders integrate features at various levels of abstraction by processing information from skip connections and performing numerical operations on upward path. Through experiments on the PH2, ISIC2017, and ISIC2018 datasets, we embed these decoders into different U-like networks, demonstrating their effectiveness in significantly reducing the number of parameters and FLOPs while maintaining performance. In summary, the proposed discretized nmODEs decoders are capable of reducing the number of parameters by about 20% ~ 50% and FLOPs by up to 74%, while possessing the potential to adapt to all U-like networks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks">https://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks</a>.</p><blockquote><p>è¿‘å¹´æ¥ï¼Œå…ˆè¿›çš„Uå‹ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„ç¼ºç‚¹ï¼ŒåŒ…æ‹¬å‚æ•°è¿‡å¤šã€è®¡ç®—å¤æ‚æ€§é«˜å’Œæ¨ç†é€Ÿåº¦æ…¢ï¼Œä¸ºåœ¨æœ‰é™è®¡ç®—èµ„æºçš„åœºæ™¯ä¸­å®é™…åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰çš„è½»é‡çº§Uå‹ç½‘ç»œå·²ç»ç¼“è§£äº†ä¸€äº›è¿™äº›é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸å…·æœ‰é¢„å…ˆè®¾è®¡çš„ç»“æ„å’Œä¸å¯åˆ†ç¦»çš„æ¨¡å—ï¼Œé™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é‡‡ç”¨ç¥ç»è®°å¿†æ™®é€šå¾®åˆ†æ–¹ç¨‹ï¼ˆnmODEsï¼‰çš„ä¸åŒç¦»æ•£åŒ–æ–¹æ³•ï¼Œæå‡ºäº†ä¸‰ç§å³æ’å³ç”¨çš„è§£ç å™¨ã€‚è¿™äº›è§£ç å™¨é€šè¿‡å¤„ç†æ¥è‡ªè·³è¿‡è¿æ¥çš„ä¿¡æ¯å¹¶åœ¨å‘ä¸Šè·¯å¾„ä¸Šè¿›è¡Œæ•°å€¼è¿ç®—ï¼Œæ¥æ•´åˆä¸åŒæŠ½è±¡å±‚æ¬¡çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨PH2ã€ISIC2017å’ŒISIC2018æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†è¿™äº›è§£ç å™¨åµŒå…¥ä¸åŒçš„Uå‹ç½‘ç»œï¼Œè¯æ˜äº†å®ƒä»¬åœ¨æ˜¾è‘—å‡å°‘å‚æ•°å’ŒFLOPsçš„åŒæ—¶ä¿æŒæ€§èƒ½çš„æœ‰æ•ˆæ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæ‰€æå‡ºçš„ç¦»æ•£åŒ–nmODEsè§£ç å™¨èƒ½å¤Ÿå‡å°‘çº¦20%~50%çš„å‚æ•°å’Œæœ€å¤šè¾¾74%çš„FLOPsï¼ŒåŒæ—¶æœ‰æ½œåŠ›é€‚åº”æ‰€æœ‰Uå‹ç½‘ç»œã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networks%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/nayutayuki/Lightweight-nmODE-Decoders-For-U-like-networksæ‰¾åˆ°ã€‚]</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06262v1">PDF</a></p><p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºåˆ©ç”¨ç¥ç»è®°å¿†å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆnmODEsï¼‰çš„ä¸åŒç¦»æ•£åŒ–æ–¹æ³•è®¾è®¡ä¸‰æ¬¾å³æ’å³ç”¨è§£ç å™¨ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¿™äº›è§£ç å™¨èƒ½å¤Ÿé›†æˆä¸åŒå±‚æ¬¡çš„ç‰¹å¾ï¼Œå‡å°‘å‚æ•°å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›è§£ç å™¨å¯åµŒå…¥å„ç§Uå½¢ç½‘ç»œï¼Œå‡å°‘çº¦20%-50%çš„å‚æ•°å’Œæœ€å¤š74%çš„è®¡ç®—é‡ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>Uå½¢ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶å‚æ•°è¿‡å¤šã€è®¡ç®—å¤æ‚åº¦é«˜å’Œæ¨ç†é€Ÿåº¦æ…¢ç­‰ç¼ºç‚¹é™åˆ¶äº†å®é™…åº”ç”¨ã€‚</li><li>ç°æœ‰è½»é‡åŒ–Uå½¢ç½‘ç»œè™½ç¼“è§£äº†éƒ¨åˆ†é—®é¢˜ï¼Œä½†é¢„è®¾è®¡ç»“æ„å’Œä¸å¯åˆ†ç¦»æ¨¡å—é™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚</li><li>æœ¬æ–‡æå‡ºåˆ©ç”¨nmODEsçš„ä¸åŒç¦»æ•£åŒ–æ–¹æ³•è®¾è®¡å³æ’å³ç”¨è§£ç å™¨ï¼Œä»¥é›†æˆä¸åŒå±‚æ¬¡çš„ç‰¹å¾ã€‚</li><li>è¿™äº›è§£ç å™¨é€šè¿‡å¤„ç†è·³è·ƒè¿æ¥çš„ä¿¡æ¯å’Œå‘ä¸Šè·¯å¾„ä¸Šçš„æ•°å€¼è¿ç®—ï¼Œå®ç°äº†å‚æ•°å’Œè®¡ç®—é‡çš„æ˜¾è‘—å‡å°‘ã€‚</li><li>å®éªŒè¯æ˜ï¼Œè¿™äº›è§£ç å™¨å¯åµŒå…¥ä¸åŒçš„Uå½¢ç½‘ç»œï¼Œå¹¶åœ¨PH2ã€ISIC2017å’ŒISIC2018æ•°æ®é›†ä¸Šå±•ç¤ºäº†æ•ˆæœã€‚</li><li>è§£ç å™¨å¯å‡å°‘çº¦20%-50%çš„å‚æ•°å’Œæœ€å¤š74%çš„è®¡ç®—é‡ï¼ŒåŒæ—¶ä¿æŒæ€§èƒ½ã€‚</li><li>ä»£ç å·²å…¬å¼€ï¼Œå¯é€‚åº”æ‰€æœ‰Uå½¢ç½‘ç»œã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-967cab9f9e243dcdc9afd7aa8fb2eaf0.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c6f77f53e3717845b82da9c6b994ec6d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a1c1f9fd8c5409fbdd732ec3fd38c92d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0afd9f96fce403f6d2816f89e4be8fef.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-aa3f8acfa6b1441bb896ab8d2b87408a.jpg" align="middle"></details><h2 id="Leveraging-Prompt-Learning-and-Pause-Encoding-for-Alzheimerâ€™s-Disease-Detection"><a href="#Leveraging-Prompt-Learning-and-Pause-Encoding-for-Alzheimerâ€™s-Disease-Detection" class="headerlink" title="Leveraging Prompt Learning and Pause Encoding for Alzheimerâ€™s Disease   Detection"></a>Leveraging Prompt Learning and Pause Encoding for Alzheimerâ€™s Disease Detection</h2><p><strong>Authors:Yin-Long Liu, Rui Feng, Jia-Hong Yuan, Zhen-Hua Ling</strong></p><p>Compared to other clinical screening techniques, speech-and-language-based automated Alzheimerâ€™s disease (AD) detection methods are characterized by their non-invasiveness, cost-effectiveness, and convenience. Previous studies have demonstrated the efficacy of fine-tuning pre-trained language models (PLMs) for AD detection. However, the objective of this traditional fine-tuning method, which involves inputting only transcripts, is inconsistent with the masked language modeling (MLM) task used during the pre-training phase of PLMs. In this paper, we investigate prompt-based fine-tuning of PLMs, converting the classification task into a MLM task by inserting prompt templates into the transcript inputs. We also explore the impact of incorporating pause information from forced alignment into manual transcripts. Additionally, we compare the performance of various automatic speech recognition (ASR) models and select the Whisper model to generate ASR-based transcripts for comparison with manual transcripts. Furthermore, majority voting and ensemble techniques are applied across different PLMs (BERT and RoBERTa) using different random seeds. Ultimately, we obtain maximum detection accuracy of 95.8% (with mean 87.9%, std 3.3%) using manual transcripts, achieving state-of-the-art performance for AD detection using only transcripts on the ADReSS test set.</p><blockquote><p>ä¸å…¶ä»–ä¸´åºŠç­›æŸ¥æŠ€æœ¯ç›¸æ¯”ï¼ŒåŸºäºè¯­éŸ³å’Œè¯­è¨€çš„è‡ªåŠ¨åŒ–é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹æ–¹æ³•å…·æœ‰éä¾µå…¥æ€§ã€æˆæœ¬æ•ˆç›Šå’Œä¾¿åˆ©æ€§ç­‰ç‰¹ç‚¹ã€‚ä»¥å¾€çš„ç ”ç©¶å·²ç»è¯æ˜äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰è¿›è¡Œå¾®è°ƒå¯¹ADæ£€æµ‹çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œè¿™ç§ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•çš„ç›®æ ‡ä»…æ¶‰åŠè¾“å…¥æ–‡æœ¬ï¼Œä¸é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨çš„è¯­è¨€æ¨¡å‹ä¸­çš„æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ä»»åŠ¡ä¸ä¸€è‡´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºæç¤ºçš„PLMå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å°†æç¤ºæ¨¡æ¿æ’å…¥åˆ°æ–‡æœ¬è¾“å…¥ä¸­ï¼Œå°†åˆ†ç±»ä»»åŠ¡è½¬æ¢ä¸ºMLMä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å°†å¼ºåˆ¶å¯¹é½ä¸­çš„åœé¡¿ä¿¡æ¯èå…¥æ‰‹åŠ¨æ–‡æœ¬çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸åŒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‰æ‹©äº†Whisperæ¨¡å‹æ¥ç”ŸæˆåŸºäºASRçš„æ–‡æœ¬ï¼Œä»¥ä¾¿ä¸æ‰‹åŠ¨æ–‡æœ¬è¿›è¡Œæ¯”è¾ƒã€‚åŒæ—¶ï¼Œé€šè¿‡åº”ç”¨å¤šæ•°æŠ•ç¥¨å’Œé›†æˆæŠ€æœ¯ï¼Œç»“åˆä½¿ç”¨ä¸åŒéšæœºç§å­çš„ä¸åŒPLMï¼ˆBERTå’ŒRoBERTaï¼‰ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰‹åŠ¨æ–‡æœ¬è·å¾—äº†æœ€é«˜95.8%ï¼ˆå¹³å‡87.9%ï¼Œæ ‡å‡†å·®3.3%ï¼‰çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œåœ¨ä»…ä½¿ç”¨æ–‡æœ¬è¿›è¡ŒADæ£€æµ‹çš„ADReSSæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06259v1">PDF</a> Accepted by ISCSLP 2024</p><p><strong>Summary</strong><br>æœ¬ç ”ç©¶æ¢ç´¢äº†åŸºäºè¯­éŸ³å’Œè¯­è¨€çš„è‡ªåŠ¨åŒ–é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ£€æµ‹æ–¹æ³•çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬å…¶éä¾µå…¥æ€§ã€æˆæœ¬æ•ˆç›Šå’Œä¾¿åˆ©æ€§ã€‚ç ”ç©¶é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰è¿›è¡ŒADæ£€æµ‹ï¼Œæå‡ºåŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•ï¼Œå°†åˆ†ç±»ä»»åŠ¡è½¬åŒ–ä¸ºæ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼Œå¹¶åœ¨è½¬å½•è¾“å…¥ä¸­æ’å…¥æç¤ºæ¨¡æ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜çº³å…¥äº†æ¥è‡ªå¼ºåˆ¶å¯¹é½çš„åœé¡¿ä¿¡æ¯ï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œä½¿ç”¨æ‰‹åŠ¨è½¬å½•æœ¬è·å¾—æœ€é«˜95.8%çš„æ£€æµ‹å‡†ç¡®ç‡ï¼Œè¾¾åˆ°ä»…ä½¿ç”¨è½¬å½•æœ¬è¿›è¡ŒADæ£€æµ‹çš„é¢†å…ˆæ°´å¹³ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>è¯­éŸ³å’ŒåŸºäºè¯­è¨€çš„è‡ªåŠ¨åŒ–æ–¹æ³•ç”¨äºé˜¿å°”èŒ¨æµ·é»˜ç—…æ£€æµ‹å…·æœ‰éä¾µå…¥æ€§ã€æˆæœ¬æ•ˆç›Šå’Œä¾¿åˆ©æ€§çš„ç‰¹ç‚¹ã€‚</li><li>ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ä¸“æ³¨äºè¾“å…¥è½¬å½•ï¼Œä¸é¢„è®­ç»ƒé˜¶æ®µçš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸ä¸€è‡´ã€‚</li><li>ç ”ç©¶æå‡ºäº†åŸºäºæç¤ºçš„å¾®è°ƒæ–¹æ³•ï¼Œå°†åˆ†ç±»ä»»åŠ¡è½¬åŒ–ä¸ºæ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚</li><li>åœé¡¿ä¿¡æ¯è¢«çº³å…¥ç ”ç©¶ï¼Œæ¥è‡ªå¼ºåˆ¶å¯¹é½æŠ€æœ¯ï¼Œå¢å¼ºäº†æ‰‹åŠ¨è½¬å½•çš„ä¿¡æ¯ä»·å€¼ã€‚</li><li>å¯¹æ¯”äº†å¤šç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‰æ‹©äº†Whisperæ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚</li><li>ä½¿ç”¨æ‰‹åŠ¨è½¬å½•æœ¬è·å¾—æœ€é«˜æ£€æµ‹å‡†ç¡®ç‡95.8%ï¼Œè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3f6a0248bcc23e9d044282b14968a12e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e357f48ed6cd9be3ceccb8ce77daaa4.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9fca418a83e4822c28f2660e570106df.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5879fe12ad6fb1866ec60b59b5f0e7af.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-af720bb59b25f7c6d115ce1046db9e23.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4f3350eb69bca3b8ab7c8f3ae43f3997.jpg" align="middle"></details><h2 id="DenseVLM-A-Retrieval-and-Decoupled-Alignment-Framework-for-Open-Vocabulary-Dense-Prediction"><a href="#DenseVLM-A-Retrieval-and-Decoupled-Alignment-Framework-for-Open-Vocabulary-Dense-Prediction" class="headerlink" title="DenseVLM: A Retrieval and Decoupled Alignment Framework for   Open-Vocabulary Dense Prediction"></a>DenseVLM: A Retrieval and Decoupled Alignment Framework for Open-Vocabulary Dense Prediction</h2><p><strong>Authors:Yunheng Li, Yuxuan Li, Quansheng Zeng, Wenhai Wang, Qibin Hou, Ming-Ming Cheng</strong></p><p>Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive zero-shot recognition capability, but still underperform in dense prediction tasks. Self-distillation recently is emerging as a promising approach for fine-tuning VLMs to better adapt to local regions without requiring extensive annotations. However, previous state-of-the-art approaches often suffer from significant &#96;foreground biasâ€™, where models tend to wrongly identify background regions as foreground objects. To alleviate this issue, we propose DenseVLM, a framework designed to learn unbiased region-language alignment from powerful pre-trained VLM representations. By leveraging the pre-trained VLM to retrieve categories for unlabeled regions, DenseVLM effectively decouples the interference between foreground and background region features, ensuring that each region is accurately aligned with its corresponding category. We show that DenseVLM can be seamlessly integrated into open-vocabulary object detection and image segmentation tasks, leading to notable performance improvements. Furthermore, it exhibits promising zero-shot scalability when training on more extensive and diverse datasets.</p><blockquote><p>é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å·²å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä»ç„¶ä¸ä½³ã€‚æœ€è¿‘ï¼Œè‡ªè’¸é¦æ³•ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•å‡ºç°ï¼Œå¯ä»¥å¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”å±€éƒ¨åŒºåŸŸï¼Œè€Œæ— éœ€å¤§é‡æ³¨é‡Šã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•å¾€å¾€å—åˆ°ä¸¥é‡çš„â€œå‰æ™¯åè§â€çš„å½±å“ï¼Œæ¨¡å‹å¾€å¾€é”™è¯¯åœ°å°†èƒŒæ™¯åŒºåŸŸè¯†åˆ«ä¸ºå‰æ™¯å¯¹è±¡ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DenseVLMæ¡†æ¶ï¼Œæ—¨åœ¨ä»å¼ºå¤§çš„é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹è¡¨ç¤ºä¸­å­¦ä¹ æ— åè§åŒºåŸŸ-è¯­è¨€å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ä¸ºæœªæ ‡è®°åŒºåŸŸæ£€ç´¢ç±»åˆ«ï¼ŒDenseVLMæœ‰æ•ˆåœ°æ¶ˆé™¤äº†å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å¹²æ‰°ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½èƒ½ä¸å…¶ç›¸åº”çš„ç±»åˆ«å‡†ç¡®å¯¹é½ã€‚æˆ‘ä»¬å±•ç¤ºäº†DenseVLMå¯ä»¥æ— ç¼é›†æˆåˆ°å¼€æ”¾è¯æ±‡è¡¨çš„ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œä»è€Œå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æ›´å¤§ã€æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œè¡¨ç°å‡ºæœ‰å¸Œæœ›çš„é›¶æ ·æœ¬å¯æ‰©å±•æ€§ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06244v1">PDF</a></p><p><strong>Summary</strong></p><p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPå…·æœ‰å‡ºè‰²çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è¿‘æœŸè‡ªè’¸é¦æ³•æˆä¸ºäº†ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥æ›´å¥½åœ°å¾®è°ƒVLMsä»¥é€‚åº”å±€éƒ¨åŒºåŸŸè€Œæ— éœ€å¤§é‡æ³¨é‡Šã€‚ç„¶è€Œï¼Œå…ˆå‰çš„æ–¹æ³•å¸¸å¸¸å—åˆ°å‰æ™¯åè§çš„å½±å“ï¼Œæ¨¡å‹å€¾å‘äºé”™è¯¯åœ°å°†èƒŒæ™¯åŒºåŸŸè¯†åˆ«ä¸ºå‰æ™¯ç‰©ä½“ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DenseVLMæ¡†æ¶ï¼Œæ—¨åœ¨ä»å¼ºå¤§çš„é¢„è®­ç»ƒVLMè¡¨ç¤ºä¸­å­¦ä¹ æ— ååŒºåŸŸè¯­è¨€å¯¹é½ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„VLMæ£€ç´¢æœªæ ‡è®°åŒºåŸŸçš„ç±»åˆ«ï¼ŒDenseVLMæœ‰æ•ˆåœ°è§£é™¤äº†å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å¹²æ‰°ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½èƒ½ä¸å…¶å¯¹åº”çš„ç±»åˆ«å‡†ç¡®å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼ŒDenseVLMå¯ä»¥æ— ç¼é›†æˆå¼€æ”¾å¼è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œåœ¨æ›´å¤§å’Œæ›´å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒè¡¨ç°å‡ºè‰¯å¥½çš„é›¶æ ·æœ¬å¯æ‰©å±•æ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¸è¶³ã€‚</li><li>è‡ªè’¸é¦æ³•æ˜¯ä¸€ç§å¾®è°ƒVLMsä»¥æ›´å¥½åœ°é€‚åº”å±€éƒ¨åŒºåŸŸçš„æ–¹æ³•ï¼Œä¸”ä¸éœ€è¦å¤§é‡æ³¨é‡Šã€‚</li><li>ç°æœ‰æ–¹æ³•å¸¸å¸¸å—åˆ°å‰æ™¯åè§çš„å½±å“ï¼Œå³æ¨¡å‹å¯èƒ½é”™è¯¯åœ°å°†èƒŒæ™¯åŒºåŸŸè¯†åˆ«ä¸ºå‰æ™¯ç‰©ä½“ã€‚</li><li>DenseVLMæ¡†æ¶æ—¨åœ¨å­¦ä¹ æ— ååŒºåŸŸè¯­è¨€å¯¹é½ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„VLMæ£€ç´¢æœªæ ‡è®°åŒºåŸŸçš„ç±»åˆ«æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li><li>DenseVLMèƒ½æœ‰æ•ˆè§£é™¤å‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸç‰¹å¾ä¹‹é—´çš„å¹²æ‰°ï¼Œç¡®ä¿æ¯ä¸ªåŒºåŸŸéƒ½èƒ½å‡†ç¡®å¯¹é½å…¶å¯¹åº”çš„ç±»åˆ«ã€‚</li><li>DenseVLMå¯ä»¥é›†æˆå¼€æ”¾å¼è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå¸¦æ¥æ€§èƒ½æå‡ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4ae6aa7b410417312c1489e0f00d675f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e5871640fb622b6d506b1f3a9ae28ade.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bf432ba07bebc679fd527e0099db29dd.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-396efeb49c970863e3651e3b514a3149.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0dcea2d4202b56fd1d80b80a905d3a6b.jpg" align="middle"></details><h2 id="In-Silico-Pharmacokinetic-and-Molecular-Docking-Studies-of-Natural-Plants-against-Essential-Protein-KRAS-for-Treatment-of-Pancreatic-Cancer"><a href="#In-Silico-Pharmacokinetic-and-Molecular-Docking-Studies-of-Natural-Plants-against-Essential-Protein-KRAS-for-Treatment-of-Pancreatic-Cancer" class="headerlink" title="In Silico Pharmacokinetic and Molecular Docking Studies of Natural   Plants against Essential Protein KRAS for Treatment of Pancreatic Cancer"></a>In Silico Pharmacokinetic and Molecular Docking Studies of Natural Plants against Essential Protein KRAS for Treatment of Pancreatic Cancer</h2><p><strong>Authors:Marsha Mariya Kappan, Joby George</strong></p><p>A kind of pancreatic cancer called Pancreatic Ductal Adenocarcinoma (PDAC) is anticipated to be one of the main causes of mortality during past years. Evidence from several researches supported the concept that the oncogenic KRAS (Ki-ras2 Kirsten rat sarcoma viral oncogene) mutation is the major cause of pancreatic cancer. KRAS acts as an on-off switch that promotes cell growth. But when the KRAS gene is mutated, it will be in one position, allowing the cell growth uncontrollably. This uncontrollable multiplication of cells causes cancer growth. Therefore, KRAS was selected as the target protein in the study. Fifty plant-derived compounds are selected for the study. To determine whether the examined drugs could bind to the KRAS complexâ€™s binding pocket, molecular docking was performed. Computational analyses were used to assess the possible ability of tested substances to pass the Blood Brain Barrier (BBB). To predict the bioactivity of ligands a machine learning model was created. Five machine learning models were created and have chosen the best one among them for analyzing the bioactivity of each ligand. From the fifty plant-derived compounds the compounds with the least binding energies are selected. Then bioactivity of these six compounds is analyzed using Random Forest Regression model. Adsorption, Distribution, Metabolism, Excretion (ADME) properties of compounds are analyzed. The results showed that borneol has powerful effects and acts as a promising agent for the treatment of pancreatic cancer. This suggests that borneol found in plants like mint, ginger, rosemary, etc., is a successful compound for the treatment of pancreatic cancer.</p><blockquote><p>ä¸€ç§åä¸ºèƒ°è…ºå¯¼ç®¡è…ºç™Œï¼ˆPDACï¼‰çš„èƒ°è…ºç™Œé¢„è®¡æ˜¯è¿‡å»å‡ å¹´é‡Œå¯¼è‡´æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚æ¥è‡ªå¤šé¡¹ç ”ç©¶è¯æ®è¡¨æ˜ï¼Œè‡´ç™ŒKRASï¼ˆKi-ras2 Kirsteé¼ è‚‰ç˜¤ç—…æ¯’è‡´ç™ŒåŸºå› ï¼‰çªå˜æ˜¯èƒ°è…ºç™Œçš„ä¸»è¦åŸå› ã€‚KRASå……å½“å¼€å…³ï¼Œä¿ƒè¿›ç»†èƒç”Ÿé•¿ã€‚ä½†å½“KRASåŸºå› å‘ç”Ÿçªå˜æ—¶ï¼Œå®ƒå°†å¤„äºæŸä¸€ä½ç½®ï¼Œä½¿ç»†èƒæ— æ³•æ§åˆ¶åœ°å¢é•¿ã€‚è¿™ç§æ— æ³•æ§åˆ¶çš„ç»†èƒå¢æ®–å¯¼è‡´ç™Œç—‡å¢é•¿ã€‚å› æ­¤ï¼ŒKRASè¢«é€‰ä¸ºè¯¥ç ”ç©¶ä¸­çš„ç›®æ ‡è›‹ç™½ã€‚è¯¥ç ”ç©¶é€‰æ‹©äº†50ç§æ¤ç‰©è¡ç”Ÿçš„åŒ–åˆç‰©è¿›è¡Œç ”ç©¶ã€‚ä¸ºäº†ç¡®å®šæ‰€æµ‹è¯•çš„è¯ç‰©æ˜¯å¦èƒ½å¤Ÿç»“åˆåˆ°KRASå¤åˆç‰©çš„ç»“åˆå£è¢‹ï¼Œè¿›è¡Œäº†åˆ†å­å¯¹æ¥ã€‚ä½¿ç”¨è®¡ç®—åˆ†æè¯„ä¼°äº†æµ‹è¯•ç‰©è´¨é€šè¿‡è¡€è„‘å±éšœï¼ˆBBBï¼‰çš„å¯èƒ½èƒ½åŠ›ã€‚åˆ›å»ºäº†ä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹é…ä½“çš„ç”Ÿç‰©æ´»æ€§ã€‚åˆ›å»ºäº†äº”ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä»ä¸­é€‰æ‹©äº†æœ€ä½³æ¨¡å‹æ¥åˆ†ææ¯ä¸ªé…ä½“çš„ç”Ÿç‰©æ´»æ€§ã€‚ä»äº”åç§æ¤ç‰©è¡ç”Ÿçš„åŒ–åˆç‰©ä¸­é€‰æ‹©äº†ç»“åˆèƒ½æœ€ä½çš„åŒ–åˆç‰©ã€‚ç„¶åä½¿ç”¨éšæœºæ£®æ—å›å½’æ¨¡å‹åˆ†æè¿™äº›åŒ–åˆç‰©çš„ç”Ÿç‰©æ´»æ€§ã€‚è¿˜åˆ†æäº†åŒ–åˆç‰©çš„å¸é™„ã€åˆ†å¸ƒã€ä»£è°¢ã€æ’æ³„ï¼ˆADMEï¼‰ç‰¹æ€§ã€‚ç»“æœè¡¨æ˜ï¼Œå†°ç‰‡å…·æœ‰å¼ºå¤§çš„åŠŸæ•ˆï¼Œå¹¶ä¸”ä½œä¸ºæ²»ç–—èƒ°è…ºç™Œçš„æœ‰å‰é€”çš„è¯ç‰©ã€‚è¿™è¡¨æ˜å†°ç‰‡å­˜åœ¨äºè–„è·ã€å§œã€è¿·è¿­é¦™ç­‰æ¤ç‰©ä¸­ï¼Œæ˜¯æ²»ç–—èƒ°è…ºç™Œçš„æˆåŠŸåŒ–åˆç‰©ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06237v1">PDF</a></p><p><strong>Summary</strong><br>èƒ°è…ºç™Œä¸­çš„ä¸€ç§ç±»å‹â€”â€”èƒ°ç®¡è…ºç™Œï¼ˆPDACï¼‰é¢„è®¡å°†æˆä¸ºè¿‡å»å‡ å¹´å†…ä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‡´ç™ŒKRASåŸºå› çªå˜æ˜¯èƒ°è…ºç™Œçš„ä¸»è¦åŸå› ã€‚ç ”ç©¶é€‰æ‹©äº†äº”åç§æ¤ç‰©è¡ç”Ÿçš„åŒ–åˆç‰©è¿›è¡Œç ”ç©¶ï¼Œå¹¶é€šè¿‡åˆ†å­å¯¹æ¥ã€è®¡ç®—åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹ç­‰æ–¹æ³•ç­›é€‰å‡ºäº†å…·æœ‰æ½œåœ¨æ´»æ€§çš„åŒ–åˆç‰©ã€‚æœ€ç»ˆå‘ç°è–„è·é†‡ç­‰æ¤ç‰©ä¸­çš„æˆåˆ†å…·æœ‰æ²»ç–—èƒ°è…ºç™Œçš„æ½œåŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>èƒ°è…ºç™Œæ˜¯ä¸€ç§ä¸¥é‡çš„è‡´æ­»æ€§ç–¾ç—…ï¼Œå…¶ä¸­èƒ°ç®¡è…ºç™Œï¼ˆPDACï¼‰æ˜¯ä¸»è¦çš„ç±»å‹ã€‚</li><li>KRASåŸºå› çªå˜æ˜¯èƒ°è…ºç™Œçš„ä¸»è¦è¯±å› ï¼Œå®ƒæ§åˆ¶ç€ç»†èƒçš„ç”Ÿé•¿ï¼Œå½“å…¶çªå˜æ—¶ï¼Œä¼šå¯¼è‡´ç»†èƒæ— æ§åˆ¶åœ°å¢æ®–ï¼Œè¿›è€Œå¼•å‘ç™Œç—‡ã€‚</li><li>ç ”ç©¶é‡‡ç”¨äº†äº”åç§æ¤ç‰©è¡ç”Ÿçš„åŒ–åˆç‰©ä½œä¸ºæ½œåœ¨çš„æ²»ç–—è¯ç‰©ã€‚</li><li>é€šè¿‡åˆ†å­å¯¹æ¥ã€è®¡ç®—åˆ†æå’Œæœºå™¨å­¦ä¹ æ¨¡å‹ç­‰æ–¹æ³•ï¼Œç­›é€‰å‡ºèƒ½å¤Ÿä¸KRASå¤åˆä½“ç»“åˆå¹¶å…·æœ‰æ½œåœ¨æ´»æ€§çš„åŒ–åˆç‰©ã€‚</li><li>è–„è·é†‡ç­‰æ¤ç‰©æˆåˆ†è¢«ç­›é€‰å‡ºå…·æœ‰å¼ºå¤§çš„æŠ—ç™Œæ•ˆæœã€‚</li><li>ç­›é€‰å‡ºçš„åŒ–åˆç‰©è¿˜éœ€è¦è¿›ä¸€æ­¥åˆ†æå…¶åœ¨ç”Ÿç‰©ä½“å†…çš„å¸é™„ã€åˆ†å¸ƒã€ä»£è°¢å’Œæ’æ³„ï¼ˆADMEï¼‰ç­‰ç‰¹æ€§ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5ca75b2760b65792a55c5ea157f69fac.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4b4f8a5cfd22ff6c64298fcc7cc9a100.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1b09d320e870f7aab5e2b316bef19687.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2294c23128aadd29bd1d8931fb22ceee.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-45d35e3824bfc2ee3d0bd754e7f0bdd2.jpg" align="middle"></details><h2 id="MSCrackMamba-Leveraging-Vision-Mamba-for-Crack-Detection-in-Fused-Multispectral-Imagery"><a href="#MSCrackMamba-Leveraging-Vision-Mamba-for-Crack-Detection-in-Fused-Multispectral-Imagery" class="headerlink" title="MSCrackMamba: Leveraging Vision Mamba for Crack Detection in Fused   Multispectral Imagery"></a>MSCrackMamba: Leveraging Vision Mamba for Crack Detection in Fused Multispectral Imagery</h2><p><strong>Authors:Qinfeng Zhu, Yuan Fang, Lei Fan</strong></p><p>Crack detection is a critical task in structural health monitoring, aimed at assessing the structural integrity of bridges, buildings, and roads to prevent potential failures. Vision-based crack detection has become the mainstream approach due to its ease of implementation and effectiveness. Fusing infrared (IR) channels with red, green and blue (RGB) channels can enhance feature representation and thus improve crack detection. However, IR and RGB channels often differ in resolution. To align them, higher-resolution RGB images typically need to be downsampled to match the IR image resolution, which leads to the loss of fine details. Moreover, crack detection performance is restricted by the limited receptive fields and high computational complexity of traditional image segmentation networks. Inspired by the recently proposed Mamba neural architecture, this study introduces a two-stage paradigm called MSCrackMamba, which leverages Vision Mamba along with a super-resolution network to address these challenges. Specifically, to align IR and RGB channels, we first apply super-resolution to IR channels to match the resolution of RGB channels for data fusion. Vision Mamba is then adopted as the backbone network, while UperNet is employed as the decoder for crack detection. Our approach is validated on the large-scale Crack Detection dataset Crack900, demonstrating an improvement of 3.55% in mIoU compared to the best-performing baseline methods.</p><blockquote><p>è£‚ç¼æ£€æµ‹æ˜¯ç»“æ„å¥åº·ç›‘æµ‹ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¡¥æ¢ã€å»ºç­‘å’Œé“è·¯çš„ç»“æ„å®Œæ•´æ€§ï¼Œä»¥é˜²æ­¢æ½œåœ¨æ•…éšœã€‚åŸºäºè§†è§‰çš„è£‚ç¼æ£€æµ‹å·²æˆä¸ºä¸»æµæ–¹æ³•ï¼Œå› å…¶æ˜“äºå®æ–½ä¸”æ•ˆæœæ˜¾è‘—ã€‚å°†çº¢å¤–ï¼ˆIRï¼‰é€šé“ä¸çº¢ç»¿è“ï¼ˆRGBï¼‰é€šé“èåˆå¯ä»¥å¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜è£‚ç¼æ£€æµ‹æ•ˆæœã€‚ç„¶è€Œï¼ŒIRå’ŒRGBé€šé“åœ¨åˆ†è¾¨ç‡ä¸Šç»å¸¸å­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†å¯¹é½å®ƒä»¬ï¼Œé€šå¸¸éœ€è¦å°†å†›ç”¨çº§RGBå›¾åƒçš„åˆ†è¾¨ç‡é™ä½ä»¥åŒ¹é…IRå›¾åƒï¼Œè¿™å¯¼è‡´äº†ç²¾ç»†ç»†èŠ‚çš„ä¸¢å¤±ã€‚æ­¤å¤–ï¼Œè£‚ç¼æ£€æµ‹æ€§èƒ½è¿˜å—åˆ°ä¼ ç»Ÿå›¾åƒåˆ†å‰²ç½‘ç»œå—é™çš„æ„Ÿå—é‡å’Œé«˜è®¡ç®—å¤æ‚åº¦çš„é™åˆ¶ã€‚æœ¬ç ”ç©¶å—åˆ°æœ€è¿‘æå‡ºçš„Mambaç¥ç»ç½‘ç»œæ¶æ„çš„å¯å‘ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºMSCrackMambaçš„ä¸¤é˜¶æ®µèŒƒå¼ï¼Œè¯¥èŒƒå¼åˆ©ç”¨Vision Mambaå’Œè¶…åˆ†è¾¨ç‡ç½‘ç»œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¯¹é½IRå’ŒRGBé€šé“ï¼Œæˆ‘ä»¬é¦–å…ˆåº”ç”¨è¶…åˆ†è¾¨ç‡æŠ€æœ¯æ¥æé«˜IRé€šé“çš„åˆ†è¾¨ç‡ï¼Œä»¥åŒ¹é…RGBé€šé“è¿›è¡Œæ•°æ®èåˆã€‚ç„¶åé‡‡ç”¨Vision Mambaä½œä¸ºéª¨å¹²ç½‘ç»œï¼ŒåŒæ—¶é‡‡ç”¨UperNetä½œä¸ºè£‚ç¼æ£€æµ‹çš„è§£ç å™¨ã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡è£‚ç¼æ£€æµ‹æ•°æ®é›†Crack900ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æ–¹æ³•ï¼Œåœ¨mIoUæ–¹é¢æé«˜äº†3.55%ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06211v1">PDF</a></p><p><strong>æ‘˜è¦</strong><br>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºçº¢å¤–ï¼ˆIRï¼‰å’Œçº¢è‰²ã€ç»¿è‰²å’Œè“è‰²ï¼ˆRGBï¼‰åŒé€šé“èåˆçš„è£‚ç¼æ£€æµ‹æ–°æ–¹æ³•MSCrackMambaã€‚ä¸ºè§£å†³IRå’ŒRGBé€šé“åˆ†è¾¨ç‡ä¸åŒ¹é…é—®é¢˜ï¼Œé‡‡ç”¨è¶…åˆ†è¾¨ç‡æŠ€æœ¯å¯¹é½IRé€šé“åˆ†è¾¨ç‡ã€‚åˆ©ç”¨Vision Mambaä½œä¸ºä¸»å¹²ç½‘ç»œï¼ŒUperNetä½œä¸ºè§£ç å™¨è¿›è¡Œè£‚ç¼æ£€æµ‹ã€‚åœ¨å¤§å‹è£‚ç¼æ£€æµ‹æ•°æ®é›†Crack900ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡äº¤å¹¶æ¯”æé«˜äº†3.55%ã€‚</p><p><strong>å…³é”®è¦ç‚¹</strong></p><ol><li>è£‚ç¼æ£€æµ‹æ˜¯ç»“æ„å¥åº·ç›‘æµ‹ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¡¥æ¢ã€å»ºç­‘å’Œé“è·¯çš„ç»“æ„å®Œæ•´æ€§ï¼Œé¢„é˜²æ½œåœ¨æ•…éšœã€‚</li><li>è§†è§‰åŸºäºçš„è£‚ç¼æ£€æµ‹å·²æˆä¸ºä¸»æµæ–¹æ³•ï¼Œå› å…¶æ˜“äºå®æ–½ä¸”æœ‰æ•ˆã€‚</li><li>çº¢å¤–ï¼ˆIRï¼‰ä¸çº¢ã€ç»¿ã€è“ï¼ˆRGBï¼‰é€šé“çš„èåˆèƒ½å¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜è£‚ç¼æ£€æµ‹æ€§èƒ½ã€‚</li><li>IRå’ŒRGBé€šé“åˆ†è¾¨ç‡çš„ä¸åŒ¹é…é—®é¢˜é€šè¿‡è¶…åˆ†è¾¨ç‡æŠ€æœ¯è§£å†³ï¼Œä»¥å®ç°æ•°æ®èåˆã€‚</li><li>é‡‡ç”¨Vision Mambaä½œä¸ºä¸»å¹²ç½‘ç»œï¼ŒUperNetä½œä¸ºè§£ç å™¨è¿›è¡Œè£‚ç¼æ£€æµ‹ã€‚</li><li>æ–¹æ³•åœ¨å¤§å‹è£‚ç¼æ£€æµ‹æ•°æ®é›†Crack900ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰æœ‰æ‰€æé«˜ã€‚</li><li>ä¸æœ€ä½³åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒMSCrackMambaæ–¹æ³•åœ¨mIoUä¸Šæé«˜äº†3.55%ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ffa1fd055912dc68d4501f1df430371.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-223060ed2966e2f108205ea755e15e36.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e08ac74a93d157f4a1b8357f3e9af589.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-898447a03c062e7d92601c740f976630.jpg" align="middle"></details><h2 id="A4-Unet-Deformable-Multi-Scale-Attention-Network-for-Brain-Tumor-Segmentation"><a href="#A4-Unet-Deformable-Multi-Scale-Attention-Network-for-Brain-Tumor-Segmentation" class="headerlink" title="A4-Unet: Deformable Multi-Scale Attention Network for Brain Tumor   Segmentation"></a>A4-Unet: Deformable Multi-Scale Attention Network for Brain Tumor Segmentation</h2><p><strong>Authors:Ruoxin Wang, Tianyi Tang, Haiming Du, Yuxuan Cheng, Yu Wang, Lingjie Yang, Xiaohui Duan, Yunfang Yu, Yu Zhou, Donglong Chen</strong></p><p>Brain tumor segmentation models have aided diagnosis in recent years. However, they face MRI complexity and variability challenges, including irregular shapes and unclear boundaries, leading to noise, misclassification, and incomplete segmentation, thereby limiting accuracy. To address these issues, we adhere to an outstanding Convolutional Neural Networks (CNNs) design paradigm and propose a novel network named A4-Unet. In A4-Unet, Deformable Large Kernel Attention (DLKA) is incorporated in the encoder, allowing for improved capture of multi-scale tumors. Swin Spatial Pyramid Pooling (SSPP) with cross-channel attention is employed in a bottleneck further to study long-distance dependencies within images and channel relationships. To enhance accuracy, a Combined Attention Module (CAM) with Discrete Cosine Transform (DCT) orthogonality for channel weighting and convolutional element-wise multiplication is introduced for spatial weighting in the decoder. Attention gates (AG) are added in the skip connection to highlight the foreground while suppressing irrelevant background information. The proposed network is evaluated on three authoritative MRI brain tumor benchmarks and a proprietary dataset, and it achieves a 94.4% Dice score on the BraTS 2020 dataset, thereby establishing multiple new state-of-the-art benchmarks. The code is available here: <a target="_blank" rel="noopener" href="https://github.com/WendyWAAAAANG/A4-Unet">https://github.com/WendyWAAAAANG/A4-Unet</a>.</p><blockquote><p>è¿‘å¹´æ¥ï¼Œè„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹åœ¨è¯Šæ–­ä¸­èµ·åˆ°äº†è¾…åŠ©ä½œç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬é¢ä¸´ç€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„å¤æ‚æ€§å’Œå˜å¼‚æ€§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å½¢çŠ¶ä¸è§„åˆ™å’Œè¾¹ç•Œä¸æ¸…ï¼Œå¯¼è‡´å™ªå£°ã€è¯¯åˆ†ç±»å’Œåˆ†å‰²ä¸å®Œå…¨ï¼Œä»è€Œé™åˆ¶äº†å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬éµå¾ªå“è¶Šçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è®¾è®¡èŒƒå¼ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œï¼Œåä¸ºA4-Unetã€‚åœ¨A4-Unetä¸­ï¼Œæˆ‘ä»¬åœ¨ç¼–ç å™¨ä¸­åŠ å…¥äº†å¯å˜å½¢å¤§å†…æ ¸æ³¨æ„åŠ›ï¼ˆDLKAï¼‰ï¼Œèƒ½å¤Ÿæ”¹è¿›å¤šå°ºåº¦è‚¿ç˜¤çš„æ•è·ã€‚æˆ‘ä»¬è¿˜åœ¨ç“¶é¢ˆå¤„é‡‡ç”¨äº†å¸¦æœ‰è·¨é€šé“æ³¨æ„åŠ›çš„Swinç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆSSPPï¼‰ï¼Œè¿›ä¸€æ­¥ç ”ç©¶å›¾åƒå†…çš„é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œé€šé“å…³ç³»ã€‚ä¸ºäº†æé«˜å‡†ç¡®æ€§ï¼Œæˆ‘ä»¬åœ¨è§£ç å™¨ä¸­å¼•å…¥äº†ä¸€ä¸ªç»“åˆæ³¨æ„åŠ›æ¨¡å—ï¼ˆCAMï¼‰ï¼Œè¯¥æ¨¡å—å…·æœ‰ç¦»æ•£ä½™å¼¦å˜æ¢ï¼ˆDCTï¼‰æ­£äº¤æ€§ç”¨äºé€šé“åŠ æƒå’Œå·ç§¯å…ƒç´ çº§ä¹˜æ³•ä»¥å®ç°ç©ºé—´åŠ æƒã€‚æˆ‘ä»¬è¿˜åœ¨è·³è·ƒè¿æ¥ä¸­æ·»åŠ äº†æ³¨æ„åŠ›é—¨ï¼ˆAGï¼‰ï¼Œä»¥çªå‡ºå‰æ™¯å¹¶æŠ‘åˆ¶æ— å…³çš„èƒŒæ™¯ä¿¡æ¯ã€‚è¯¥ç½‘ç»œåœ¨ä¸‰ä¸ªæƒå¨çš„MRIè„‘è‚¿ç˜¤åŸºå‡†æµ‹è¯•å’Œä¸€ä¸ªä¸“æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨BraTS 2020æ•°æ®é›†ä¸Šå®ç°äº†94.4%çš„Diceå¾—åˆ†ï¼Œä»è€Œå»ºç«‹äº†å¤šä¸ªæ–°çš„æœ€æ–°æŠ€æœ¯åŸºå‡†ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WendyWAAAAANG/A4-Unet">https://github.com/WendyWAAAAANG/A4-Unet</a>è·å¾—ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06088v1">PDF</a> 8 pages, 14 figures, IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2024</p><p><strong>Summary</strong></p><p>è¿‘å¹´æ¥ï¼Œè„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹é€šè¿‡é‡‡ç”¨å·ç§¯ç¥ç»ç½‘ç»œè®¾è®¡èŒƒä¾‹æ¥è¾…åŠ©è¯Šæ–­ã€‚ä¸ºäº†è§£å†³MRIçš„å¤æ‚æ€§å’Œå·®å¼‚æ€§æ‰€å¸¦æ¥çš„å™ªéŸ³ã€è¯¯åˆ†ç±»å’Œä¸å®Œæ•´åˆ†å‰²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºA4-Unetçš„æ–°å‹ç½‘ç»œã€‚è¯¥ç½‘ç»œåœ¨ç¼–ç å™¨ä¸­åŠ å…¥äº†å¤§å†…æ ¸å˜å½¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¤šå°ºåº¦è‚¿ç˜¤ï¼›åœ¨ç“¶é¢ˆéƒ¨åˆ†é‡‡ç”¨äº†å¸¦æœ‰è·¨é€šé“æ³¨æ„åŠ›çš„ç©ºé—´é‡‘å­—å¡”æ± åŒ–æ¨¡å—ï¼Œç ”ç©¶å›¾åƒå†…çš„é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œé€šé“å…³ç³»ï¼›å¹¶ç»“åˆäº†æ³¨æ„åŠ›æ¨¡å—å’Œç¦»æ•£ä½™å¼¦å˜æ¢ä»¥å¢å¼ºå‡†ç¡®æ€§ã€‚åœ¨ä¸‰ä¸ªæƒå¨çš„MRIè„‘è‚¿ç˜¤åŸºå‡†æµ‹è¯•é›†å’Œä¸€ä¸ªä¸“æœ‰æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒA4-Unetåœ¨BraTS 2020æ•°æ®é›†ä¸Šå®ç°äº†94.4%çš„Diceå¾—åˆ†ï¼Œæ ‘ç«‹äº†æ–°çš„åŸºå‡†çº¿ã€‚ç›¸å…³ä»£ç å¯åœ¨é“¾æ¥ä¸­è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/WendyWAAAAANG/A4-Unet">https://github.com/WendyWAAAAANG/A4-Unet</a>ã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>è„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹è¾…åŠ©è¯Šæ–­ï¼Œé¢ä¸´MRIå¤æ‚æ€§å’Œå·®å¼‚æ€§æŒ‘æˆ˜ã€‚</li><li>A4-Unetç½‘ç»œé€šè¿‡åŠ å…¥å¤§å†…æ ¸å˜å½¢æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å¤šå°ºåº¦è‚¿ç˜¤ã€‚</li><li>ç“¶é¢ˆéƒ¨åˆ†é‡‡ç”¨ç©ºé—´é‡‘å­—å¡”æ± åŒ–å’Œè·¨é€šé“æ³¨æ„åŠ›æœºåˆ¶ç ”ç©¶å›¾åƒçš„é•¿è·ç¦»ä¾èµ–å…³ç³»å’Œé€šé“å…³ç³»ã€‚</li><li>å¼•å…¥æ³¨æ„åŠ›æ¨¡å—å’Œç¦»æ•£ä½™å¼¦å˜æ¢ä»¥å¢å¼ºå‡†ç¡®æ€§ã€‚</li><li>A4-Unetåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨BraTS 2020æ•°æ®é›†ä¸Šå®ç°äº†é«˜Diceå¾—åˆ†ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ea0ffd3a67649ccdf1c36e0b999ac684.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2257e665d32b0f7e7f31610e9a23a68e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3d7ee5e8a9259574ad3fd127a17688bb.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-472a94d970be7e5213c6a338fe13c18d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7ad9a00c0474f5a92606f90060c7d0b8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-59f16204e808e7f0bebff31b618d87b1.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4733a0bef938a35d087334b13a033fd5.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-280d6f05efa872adc8836da5bc0ab674.jpg" align="middle"></details><h2 id="Dilated-Balanced-Cross-Entropy-Loss-for-Medical-Image-Segmentation"><a href="#Dilated-Balanced-Cross-Entropy-Loss-for-Medical-Image-Segmentation" class="headerlink" title="Dilated Balanced Cross Entropy Loss for Medical Image Segmentation"></a>Dilated Balanced Cross Entropy Loss for Medical Image Segmentation</h2><p><strong>Authors:Seyed Mohsen Hosseini, Mahdieh Soleymani Baghshah</strong></p><p>A novel method for tackling the problem of imbalanced data in medical image segmentation is proposed in this work. In balanced cross entropy (CE) loss, which is a type of weighted CE loss, the weight assigned to each class is the in-verse of the class frequency. These balancing weights are expected to equalize the effect of each class on the overall loss and prevent the model from being biased towards the majority class. But, as it has been shown in previous studies, this method degrades the performance by a large margin. Therefore, balanced CE is not a popular loss in medical segmentation tasks, and usually a region-based loss, like the Dice loss, is used to address the class imbalance problem. In the pro-posed method, the weighting of cross entropy loss for each class is based on a dilated area of each class mask, and balancing weights are assigned to each class together with its surrounding pixels. The goal of this study is to show that the performance of balanced CE loss can be greatly improved my modifying its weighting strategy. Experiments on different datasets show that the proposed dilated balanced CE (DBCE) loss outperforms the balanced CE loss by a large margin and produces superior results compared to CE loss, and its performance is similar to the performance of the combination of Dice and CE loss. This means that a weighted cross entropy loss with the right weighing strategy can be as effective as a region-based loss in handling the problem of class imbalance in medical segmentation tasks.</p><blockquote><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ•°æ®ä¸å¹³è¡¡é—®é¢˜çš„æ–°æ–¹æ³•ã€‚åœ¨å¹³è¡¡äº¤å‰ç†µï¼ˆCEï¼‰æŸå¤±ä¸­ï¼Œå®ƒæ˜¯ä¸€ç§åŠ æƒCEæŸå¤±ï¼Œåˆ†é…ç»™æ¯ä¸ªç±»çš„æƒé‡æ˜¯ç±»é¢‘ç‡çš„å€’æ•°ã€‚è¿™äº›å¹³è¡¡æƒé‡é¢„è®¡ä¼šå¯¹æ¯ä¸ªç±»å¯¹æ€»ä½“æŸå¤±çš„å½±å“è¿›è¡Œå¹³è¡¡ï¼Œé˜²æ­¢æ¨¡å‹åå‘äºå¤šæ•°ç±»ã€‚ä½†æ˜¯ï¼Œå¦‚å…ˆå‰ç ”ç©¶æ‰€ç¤ºï¼Œæ­¤æ–¹æ³•ä¼šé™ä½æ€§èƒ½ï¼Œå¹…åº¦è¾ƒå¤§ã€‚å› æ­¤ï¼Œå¹³è¡¡CEåœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­å¹¶ä¸æµè¡Œï¼Œé€šå¸¸ä½¿ç”¨åŒºåŸŸæ€§çš„æŸå¤±ï¼Œå¦‚DiceæŸå¤±ï¼Œæ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨æå‡ºçš„æ–¹æ³•ä¸­ï¼Œæ¯ä¸ªç±»çš„äº¤å‰ç†µæŸå¤±çš„æƒé‡åŸºäºæ¯ä¸ªç±»æ©ç çš„è†¨èƒ€åŒºåŸŸï¼Œå¹¶ä¸”ä¸ºæ¯ä¸ªç±»åŠå…¶å‘¨å›´åƒç´ åˆ†é…å¹³è¡¡æƒé‡ã€‚æœ¬ç ”ç©¶çš„ç›®æ ‡è¡¨æ˜ï¼Œé€šè¿‡ä¿®æ”¹å…¶åŠ æƒç­–ç•¥ï¼Œå¯ä»¥å¤§å¤§æé«˜å¹³è¡¡CEæŸå¤±çš„æ€§èƒ½ã€‚åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è†¨èƒ€å¹³è¡¡CEï¼ˆDBCEï¼‰æŸå¤±åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºå¹³è¡¡CEæŸå¤±ï¼Œå¹¶ä¸”ä¸CEæŸå¤±ç›¸æ¯”äº§ç”Ÿäº†æ›´å¥½çš„ç»“æœï¼Œå…¶æ€§èƒ½ä¸Diceå’ŒCEæŸå¤±ç»„åˆçš„æ€§èƒ½ç›¸ä¼¼ã€‚è¿™æ„å‘³ç€é‡‡ç”¨æ­£ç¡®åŠ æƒç­–ç•¥çš„åŠ æƒäº¤å‰ç†µæŸå¤±åœ¨å¤„ç†åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ—¶ï¼Œå¯ä»¥åƒåŸºäºåŒºåŸŸçš„æŸå¤±ä¸€æ ·æœ‰æ•ˆã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06045v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºè†¨èƒ€å¹³è¡¡äº¤å‰ç†µï¼ˆDBCEï¼‰æŸå¤±ï¼Œé€šè¿‡ä¿®æ”¹æƒé‡ç­–ç•¥ï¼Œæ”¹è¿›äº†å¹³è¡¡äº¤å‰ç†µæŸå¤±çš„æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒDBCEæŸå¤±åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå¹³è¡¡äº¤å‰ç†µæŸå¤±å’Œä¼ ç»Ÿçš„äº¤å‰ç†µæŸå¤±ï¼Œå…¶æ€§èƒ½ä¸ç»“åˆDiceå’Œäº¤å‰ç†µæŸå¤±çš„æ–¹æ¡ˆç›¸å½“ã€‚è¿™è¡¨æ˜é‡‡ç”¨é€‚å½“æƒé‡ç­–ç•¥çš„åŠ æƒäº¤å‰ç†µæŸå¤±åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æ—¶ï¼Œå¯ä»¥åƒåŸºäºåŒºåŸŸçš„æŸå¤±ä¸€æ ·æœ‰æ•ˆã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li><li>å¹³è¡¡äº¤å‰ç†µæŸå¤±é€šè¿‡ä¸ºæ¯ä¸ªç±»åˆ«åˆ†é…æƒé‡æ¥è§£å†³æ­¤é—®é¢˜ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li><li>æ–°æå‡ºçš„è†¨èƒ€å¹³è¡¡äº¤å‰ç†µï¼ˆDBCEï¼‰æŸå¤±è€ƒè™‘åˆ°äº†æ¯ä¸ªç±»åˆ«çš„è†¨èƒ€åŒºåŸŸåŠå…¶å‘¨å›´åƒç´ æ¥åˆ†é…æƒé‡ã€‚</li><li>DBCEæŸå¤±åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¹³è¡¡äº¤å‰ç†µæŸå¤±å’Œä¼ ç»Ÿçš„äº¤å‰ç†µæŸå¤±ã€‚</li><li>DBCEæŸå¤±çš„æ€§èƒ½ä¸ç»“åˆDiceå’Œäº¤å‰ç†µæŸå¤±çš„æ–¹æ¡ˆç›¸å½“ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-43df8d70e44e622f238ea4c4c9a72fa9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-085fa897c3389b831d097565eedaf4a2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9b08c8c39be328ce614bd708e1f8ae04.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-389bb12c52a72ba798dc01b0c1abad5c.jpg" align="middle"></details><h2 id="Paddy-Disease-Detection-and-Classification-Using-Computer-Vision-Techniques-A-Mobile-Application-to-Detect-Paddy-Disease"><a href="#Paddy-Disease-Detection-and-Classification-Using-Computer-Vision-Techniques-A-Mobile-Application-to-Detect-Paddy-Disease" class="headerlink" title="Paddy Disease Detection and Classification Using Computer Vision   Techniques: A Mobile Application to Detect Paddy Disease"></a>Paddy Disease Detection and Classification Using Computer Vision Techniques: A Mobile Application to Detect Paddy Disease</h2><p><strong>Authors:Bimarsha Khanal, Paras Poudel, Anish Chapagai, Bijan Regmi, Sitaram Pokhrel, Salik Ram Khanal</strong></p><p>Plant diseases significantly impact our food supply, causing problems for farmers, economies reliant on agriculture, and global food security. Accurate and timely plant disease diagnosis is crucial for effective treatment and minimizing yield losses. Despite advancements in agricultural technology, a precise and early diagnosis remains a challenge, especially in underdeveloped regions where agriculture is crucial and agricultural experts are scarce. However, adopting Deep Learning applications can assist in accurately identifying diseases without needing plant pathologists. In this study, the effectiveness of various computer vision models for detecting paddy diseases is evaluated and proposed the best deep learning-based disease detection system. Both classification and detection using the Paddy Doctor dataset, which contains over 20,000 annotated images of paddy leaves for disease diagnosis are tested and evaluated. For detection, we utilized the YOLOv8 model-based model were used for paddy disease detection and CNN models and the Vision Transformer were used for disease classification. The average mAP50 of 69% for detection tasks was achieved and the Vision Transformer classification accuracy was 99.38%. It was found that detection models are effective at identifying multiple diseases simultaneously with less computing power, whereas classification models, though computationally expensive, exhibit better performance for classifying single diseases. Additionally, a mobile application was developed to enable farmers to identify paddy diseases instantly. Experiments with the app showed encouraging results in utilizing the trained models for both disease classification and treatment guidance.</p><blockquote><p>æ¤ç‰©ç–¾ç—…æ˜¾è‘—å½±å“æˆ‘ä»¬çš„ç²®é£Ÿä¾›åº”ï¼Œç»™å†œæ°‘ã€ä¾èµ–å†œä¸šçš„ç»æµå’Œå…¨çƒç²®é£Ÿå®‰å…¨å¸¦æ¥é—®é¢˜ã€‚å‡†ç¡®åŠæ—¶çš„æ¤ç‰©ç–¾ç—…è¯Šæ–­å¯¹äºæœ‰æ•ˆæ²»ç–—å’Œå‡å°‘äº§é‡æŸå¤±è‡³å…³é‡è¦ã€‚å°½ç®¡å†œä¸šæŠ€æœ¯å–å¾—äº†è¿›å±•ï¼Œä½†ç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å†œä¸šè‡³å…³é‡è¦ä½†å†œä¸šä¸“å®¶ç¨€ç¼ºçš„æ¬ å‘è¾¾åœ°åŒºã€‚ç„¶è€Œï¼Œé‡‡ç”¨æ·±åº¦å­¦ä¹ åº”ç”¨å¯ä»¥åœ¨ä¸éœ€è¦æ¤ç‰©ç—…ç†å­¦å®¶çš„æƒ…å†µä¸‹ï¼Œå¸®åŠ©å‡†ç¡®è¯†åˆ«ç–¾ç—…ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å¤šç§è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨æ£€æµ‹æ°´ç¨»ç–¾ç—…æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†æœ€ä½³çš„åŸºäºæ·±åº¦å­¦ä¹ çš„ç–¾ç—…æ£€æµ‹ç³»ç»Ÿã€‚ä½¿ç”¨Paddy Doctoræ•°æ®é›†å¯¹åˆ†ç±»å’Œæ£€æµ‹è¿›è¡Œäº†æµ‹è¯•å’Œè¯„ä¼°ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡2ä¸‡å¼ ç”¨äºç–¾ç—…è¯Šæ–­çš„æ°´ç¨»å¶ç‰‡æ³¨é‡Šå›¾åƒã€‚å¯¹äºæ£€æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†YOLOv8æ¨¡å‹è¿›è¡Œæ°´ç¨»ç–¾ç—…æ£€æµ‹ï¼Œå¹¶ä½¿ç”¨CNNæ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨è¿›è¡Œç–¾ç—…åˆ†ç±»ã€‚æ£€æµ‹ä»»åŠ¡çš„å¹³å‡mAP50è¾¾åˆ°69%ï¼Œè§†è§‰è½¬æ¢å™¨åˆ†ç±»å‡†ç¡®ç‡ä¸º99.38%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ£€æµ‹æ¨¡å‹åœ¨è¯†åˆ«å¤šç§ç–¾ç—…æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä¸”è®¡ç®—åŠŸç‡è¾ƒä½ï¼Œè€Œåˆ†ç±»æ¨¡å‹è™½ç„¶è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä½†åœ¨å•ä¸€ç–¾ç—…åˆ†ç±»æ–¹é¢è¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªç§»åŠ¨åº”ç”¨ç¨‹åºï¼Œä½¿å†œæ°‘èƒ½å¤Ÿç«‹å³è¯†åˆ«æ°´ç¨»ç–¾ç—…ã€‚è¯¥åº”ç”¨ç¨‹åºçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç–¾ç—…åˆ†ç±»å’Œæ²»ç–—æŒ‡å¯¼æ–¹é¢åˆ©ç”¨è®­ç»ƒæ¨¡å‹å…·æœ‰ä»¤äººé¼“èˆçš„æ•ˆæœã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05996v1">PDF</a> 21 pages,12 figures and 2 tables</p><p><strong>Summary</strong></p><p>æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨ç¨»è°·ç–¾ç—…è¯Šæ–­ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶è¯„ä¼°äº†å¤šç§è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨æ£€æµ‹æ°´ç¨»ç–¾ç—…æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†æœ€ä½³çš„æ°´ç¨»ç–¾ç—…æ£€æµ‹ä½“ç³»ã€‚é€šè¿‡ä½¿ç”¨YOLOv8æ¨¡å‹ã€CNNæ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨è¿›è¡Œè¯•éªŒå’Œè¯„ä¼°ï¼Œå‘ç°æ£€æµ‹æ¨¡å‹å¯æœ‰æ•ˆè¯†åˆ«å¤šç§ç–¾ç—…ä¸”è®¡ç®—åŠŸç‡è¾ƒä½ï¼Œè€Œåˆ†ç±»æ¨¡å‹è™½è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä½†åœ¨å•ä¸€ç–¾ç—…åˆ†ç±»æ–¹é¢è¡¨ç°è¾ƒå¥½ã€‚æ­¤å¤–ï¼Œè¿˜å¼€å‘äº†ä¸€æ¬¾ç§»åŠ¨åº”ç”¨ç¨‹åºï¼Œä½¿å†œæ°‘èƒ½å¤Ÿå³æ—¶è¯†åˆ«æ°´ç¨»ç–¾ç—…å¹¶è¿›è¡Œæ²»ç–—æŒ‡å¯¼ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æ¤ç‰©ç–¾ç—…å¯¹å…¨çƒç²®é£Ÿä¾›åº”å®‰å…¨æ„æˆä¸¥é‡å½±å“ï¼Œæ—©æœŸå‡†ç¡®è¯Šæ–­å¯¹äºæœ‰æ•ˆæ²»ç–—å’Œå‡å°‘äº§é‡æŸå¤±è‡³å…³é‡è¦ã€‚</li><li>åœ¨å†œä¸šæŠ€æœ¯ä¸æ–­è¿›æ­¥çš„èƒŒæ™¯ä¸‹ï¼Œç²¾ç¡®çš„æ—©æœŸè¯Šæ–­ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å†œä¸šè‡³å…³é‡è¦è€Œå†œä¸šä¸“å®¶ç¨€ç¼ºçš„åœ°åŒºã€‚</li><li>æ·±åº¦å­¦ä¹ åœ¨æ¤ç‰©ç–¾ç—…è¯Šæ–­ä¸­å…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤ŸååŠ©å‡†ç¡®è¯†åˆ«ç–¾ç—…è€Œæ— éœ€æ¤ç‰©ç—…ç†å­¦å®¶çš„å‚ä¸ã€‚</li><li>ç ”ç©¶è¯„ä¼°äº†å¤šç§è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨æ£€æµ‹æ°´ç¨»ç–¾ç—…æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç§»åŠ¨åº”ç”¨ç¨‹åºç”¨äºå³æ—¶è¯†åˆ«æ°´ç¨»ç–¾ç—…å¹¶è¿›è¡Œæ²»ç–—æŒ‡å¯¼ã€‚</li><li>æ£€æµ‹æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¤šç§ç–¾ç—…ä¸”è®¡ç®—åŠŸç‡è¾ƒä½ï¼Œè€Œåˆ†ç±»æ¨¡å‹åˆ™åœ¨å•ä¸€ç–¾ç—…åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚</li><li>YOLOv8æ¨¡å‹è¢«ç”¨äºæ°´ç¨»ç–¾ç—…çš„æ£€æµ‹ï¼Œè€ŒCNNæ¨¡å‹å’Œè§†è§‰è½¬æ¢å™¨åˆ™ç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7ba9e738a8e08337b06a08b6c11521c7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e684e8ef65f7b7d1afbabd3374e98105.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bc12d620d23f8bf2ab2f9c6ab2d993f3.jpg" align="middle"></details><h2 id="LVS-Net-A-Lightweight-Vessels-Segmentation-Network-for-Retinal-Image-Analysis"><a href="#LVS-Net-A-Lightweight-Vessels-Segmentation-Network-for-Retinal-Image-Analysis" class="headerlink" title="LVS-Net: A Lightweight Vessels Segmentation Network for Retinal Image   Analysis"></a>LVS-Net: A Lightweight Vessels Segmentation Network for Retinal Image Analysis</h2><p><strong>Authors:Mehwish Mehmood, Shahzaib Iqbal, Tariq Mahmood Khan, Ivor Spence, Muhammad Fahim</strong></p><p>The analysis of retinal images for the diagnosis of various diseases is one of the emerging areas of research. Recently, the research direction has been inclined towards investigating several changes in retinal blood vessels in subjects with many neurological disorders, including dementia. This research focuses on detecting diseases early by improving the performance of models for segmentation of retinal vessels with fewer parameters, which reduces computational costs and supports faster processing. This paper presents a novel lightweight encoder-decoder model that segments retinal vessels to improve the efficiency of disease detection. It incorporates multi-scale convolutional blocks in the encoder to accurately identify vessels of various sizes and thicknesses. The bottleneck of the model integrates the Focal Modulation Attention and Spatial Feature Refinement Blocks to refine and enhance essential features for efficient segmentation. The decoder upsamples features and integrates them with the corresponding feature in the encoder using skip connections and the spatial feature refinement block at every upsampling stage to enhance feature representation at various scales. The estimated computation complexity of our proposed model is around 29.60 GFLOP with 0.71 million parameters and 2.74 MB of memory size, and it is evaluated using public datasets, that is, DRIVE, CHASE_DB, and STARE. It outperforms existing models with dice scores of 86.44%, 84.22%, and 87.88%, respectively.</p><blockquote><p>è§†ç½‘è†œå›¾åƒåˆ†æä»¥è¯Šæ–­å„ç§ç–¾ç—…æ˜¯æ–°å…´ç ”ç©¶é¢†åŸŸä¹‹ä¸€ã€‚æœ€è¿‘ï¼Œç ”ç©¶æ–¹å‘å€¾å‘äºç ”ç©¶åŒ…æ‹¬ç—´å‘†åœ¨å†…çš„å¤šç§ç¥ç»éšœç¢æ‚£è€…çš„è§†ç½‘è†œè¡€ç®¡å˜åŒ–ã€‚è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡æé«˜è§†ç½‘è†œè¡€ç®¡åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½æ¥æ—©æœŸæ£€æµ‹ç–¾ç—…ï¼Œä½¿ç”¨è¾ƒå°‘çš„å‚æ•°ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬å¹¶æ”¯æŒæ›´å¿«å¤„ç†é€Ÿåº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œç”¨äºåˆ†å‰²è§†ç½‘è†œè¡€ç®¡ï¼Œä»¥æé«˜ç–¾ç—…æ£€æµ‹çš„æ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨ç¼–ç å™¨ä¸­å¼•å…¥äº†å¤šå°ºåº¦å·ç§¯å—ï¼Œä»¥å‡†ç¡®è¯†åˆ«å„ç§å¤§å°å’Œåšåº¦çš„è¡€ç®¡ã€‚è¯¥æ¨¡å‹çš„ç“¶é¢ˆç»“åˆäº†ç„¦ç‚¹è°ƒåˆ¶æ³¨æ„åŠ›å’Œç©ºé—´ç‰¹å¾ç»†åŒ–å—ï¼Œä»¥ç²¾ç‚¼å’Œå¢å¼ºé‡è¦ç‰¹å¾ï¼Œå®ç°é«˜æ•ˆåˆ†å‰²ã€‚è§£ç å™¨å¯¹ç‰¹å¾è¿›è¡Œä¸Šé‡‡æ ·ï¼Œå¹¶åœ¨æ¯ä¸ªä¸Šé‡‡æ ·é˜¶æ®µä½¿ç”¨è·³è·ƒè¿æ¥å’Œç©ºé—´ç‰¹å¾ç»†åŒ–å—å°†å…¶ä¸ç¼–ç å™¨ä¸­çš„å¯¹åº”ç‰¹å¾ç›¸ç»“åˆï¼Œä»¥å¢å¼ºä¸åŒå°ºåº¦çš„ç‰¹å¾è¡¨ç¤ºã€‚æ‰€æå‡ºæ¨¡å‹çš„ä¼°ç®—è®¡ç®—å¤æ‚åº¦çº¦ä¸º29.60 GFLOPï¼Œå…·æœ‰0.71ç™¾ä¸‡ä¸ªå‚æ•°å’Œ2.74 MBçš„å†…å­˜å¤§å°ã€‚å®ƒä½¿ç”¨å…¬å…±æ•°æ®é›†ï¼ˆå³DRIVEã€CHASE_DBå’ŒSTAREï¼‰è¿›è¡Œè¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰æ¨¡å‹çš„è¡¨ç°ï¼Œå…¶Diceå¾—åˆ†åˆ†åˆ«ä¸º86.44ï¼…ã€84.22ï¼…å’Œ87.88ï¼…ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05968v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†è§†ç½‘è†œå›¾åƒåˆ†æåœ¨ç–¾ç—…è¯Šæ–­ä¸­çš„æ–°å…´ç ”ç©¶è¶‹åŠ¿ã€‚é’ˆå¯¹ç¥ç»ç–¾ç—…å¦‚ç—´å‘†ç—‡æ‚£è€…è§†ç½‘è†œè¡€ç®¡å˜åŒ–çš„è°ƒæŸ¥å·²æˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚ä¸ºæé«˜ç–¾ç—…æ£€æµ‹æ•ˆç‡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§ç¼–ç è§£ç æ¨¡å‹ï¼Œç”¨äºè§†ç½‘è†œè¡€ç®¡åˆ†å‰²ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥å¤šå°ºåº¦å·ç§¯å—ã€ç„¦ç‚¹è°ƒåˆ¶æ³¨æ„åŠ›å’Œç©ºé—´ç‰¹å¾ç»†åŒ–å—ç­‰ç»“æ„ï¼Œæé«˜äº†å¯¹ä¸åŒå¤§å°å’Œåšåº¦è¡€ç®¡çš„å‡†ç¡®è¯†åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è®¡ç®—å¤æ‚åº¦ä½ï¼Œå‚æ•°å’Œå†…å­˜å ç”¨è¾ƒå°ï¼Œä¸”åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>è§†ç½‘è†œå›¾åƒåˆ†æåœ¨ç–¾ç—…è¯Šæ–­ä¸­æ˜¯æ–°å…´ç ”ç©¶é¢†åŸŸã€‚</li><li>ç ”ç©¶é‡ç‚¹æ˜¯é€šè¿‡æ”¹è¿›æ¨¡å‹æ€§èƒ½æ¥æ—©æœŸæ£€æµ‹ç–¾ç—…ï¼Œç‰¹åˆ«å…³æ³¨è§†ç½‘è†œè¡€ç®¡å˜åŒ–åœ¨ç¥ç»ç–¾ç—…ä¸­çš„åº”ç”¨ã€‚</li><li>æå‡ºäº†ä¸€ç§æ–°å‹çš„è½»é‡çº§ç¼–ç è§£ç æ¨¡å‹ï¼Œç”¨äºè§†ç½‘è†œè¡€ç®¡åˆ†å‰²ï¼Œä»¥æé«˜ç–¾ç—…æ£€æµ‹æ•ˆç‡ã€‚</li><li>æ¨¡å‹é€šè¿‡å¼•å…¥å¤šå°ºåº¦å·ç§¯å—ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ä¸åŒå¤§å°å’Œåšåº¦çš„è¡€ç®¡ã€‚</li><li>æ¨¡å‹é›†æˆäº†ç„¦ç‚¹è°ƒåˆ¶æ³¨æ„åŠ›å’Œç©ºé—´ç‰¹å¾ç»†åŒ–å—ï¼Œä»¥ä¼˜åŒ–å’Œæ”¹è¿›é‡è¦ç‰¹å¾ï¼Œæé«˜åˆ†å‰²æ•ˆç‡ã€‚</li><li>æ¨¡å‹è®¡ç®—å¤æ‚åº¦ä½ï¼Œå‚æ•°å’Œå†…å­˜å ç”¨è¾ƒå°ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-9c9c178dd9b6c067ce74c3bcd5e1d787.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f90bfd6010008d50d10980a61ac1069b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5dc192f4a28bb1458d654e6269ad91b6.jpg" align="middle"></details><h2 id="MCP-MedSAM-A-Powerful-Lightweight-Medical-Segment-Anything-Model-Trained-with-a-Single-GPU-in-Just-One-Day"><a href="#MCP-MedSAM-A-Powerful-Lightweight-Medical-Segment-Anything-Model-Trained-with-a-Single-GPU-in-Just-One-Day" class="headerlink" title="MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model   Trained with a Single GPU in Just One Day"></a>MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model Trained with a Single GPU in Just One Day</h2><p><strong>Authors:Donghang Lyu, Ruochen Gao, Marius Staring</strong></p><p>Medical image segmentation involves partitioning medical images into meaningful regions, with a focus on identifying anatomical structures or abnormalities. It has broad applications in healthcare, and deep learning methods have enabled significant advancements in automating this process. Recently, the introduction of the Segmentation Anything Model (SAM), the first foundation model for segmentation task, has prompted researchers to adapt it for the medical domain to improve performance across various tasks. However, SAMâ€™s large model size and high GPU requirements hinder its scalability and development in the medical domain. To address these challenges, research has increasingly focused on lightweight adaptations of SAM to reduce its parameter count, enabling training with limited GPU resources while maintaining competitive segmentation performance. In this work, we propose MCP-MedSAM, a powerful and lightweight medical SAM model designed to be trainable on a single GPU within one day while delivering superior segmentation performance. Our method was trained and evaluated using a large-scale challenge dataset\footnote{\url{<a target="_blank" rel="noopener" href="https://www.codabench.org/competitions/1847%7D/label%7Bcomp%7D%7D">https://www.codabench.org/competitions/1847}\label{comp}}</a>, compared to top-ranking methods on the challenge leaderboard, MCP-MedSAM achieved superior performance while requiring only one day of training on a single GPU. The code is publicly available at \url{<a target="_blank" rel="noopener" href="https://github.com/dong845/MCP-MedSAM%7D">https://github.com/dong845/MCP-MedSAM}</a>.</p><blockquote><p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯å°†åŒ»å­¦å›¾åƒåˆ†å‰²æˆæœ‰æ„ä¹‰çš„åŒºåŸŸï¼Œé‡ç‚¹å…³æ³¨è§£å‰–ç»“æ„æˆ–å¼‚å¸¸çš„è¯†åˆ«ã€‚å®ƒåœ¨åŒ»ç–—ä¿å¥é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œè€Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•å·²ç»èƒ½å¤Ÿå®ç°è¿™ä¸ªè¿‡ç¨‹çš„è‡ªåŠ¨åŒ–ï¼Œå¹¶æ¨åŠ¨äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æœ€è¿‘ï¼Œåˆ†å‰²ä»»åŠ¡é¦–ä¸ªåŸºç¡€æ¨¡å‹â€”â€”Segmentation Anything Modelï¼ˆSAMï¼‰çš„å¼•å…¥ï¼Œä¿ƒä½¿ç ”ç©¶è€…å°†å…¶é€‚åº”åŒ»å­¦é¢†åŸŸï¼Œä»¥æé«˜å„ç§ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒSAMçš„å¤§å‹æ¨¡å‹å°ºå¯¸å’Œé«˜GPUè¦æ±‚é˜»ç¢äº†å…¶åœ¨åŒ»å­¦é¢†åŸŸçš„å¯æ‰©å±•æ€§å’Œå¼€å‘ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è¶Šæ¥è¶Šå¤šåœ°å…³æ³¨SAMçš„è½»å‹é€‚é…ï¼Œä»¥å‡å°‘å…¶å‚æ•°æ•°é‡ï¼Œåœ¨æœ‰é™çš„GPUèµ„æºä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ä¿æŒç«äº‰æ€§çš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MCP-MedSAMï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œè½»å‹çš„åŒ»å­¦SAMæ¨¡å‹ï¼Œæ—¨åœ¨èƒ½å¤Ÿåœ¨å•GPUä¸Šä¸€å¤©å†…å®Œæˆè®­ç»ƒï¼ŒåŒæ—¶æä¾›å“è¶Šçš„åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¤§è§„æ¨¡æŒ‘æˆ˜èµ›æ•°æ®é›†è¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°^[æ•°æ®é›†é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://www.codabench.org/competitions/1847]^%EF%BC%8C%E4%B8%8E%E6%8E%92%E8%A1%8C%E6%A6%9C%E4%B8%8A%E7%9A%84%E9%A1%B6%E5%B0%96%E6%96%B9%E6%B3%95%E7%9B%B8%E6%AF%94%EF%BC%8CMCP-MedSAM%E5%9C%A8%E5%8D%95%E4%B8%AAGPU%E4%B8%8A%E4%BB%85%E9%9C%80%E8%A6%81%E4%B8%80%E5%A4%A9%E7%9A%84%E8%AE%AD%E7%BB%83%E6%97%B6%E9%97%B4%E5%B0%B1%E5%8F%96%E5%BE%97%E4%BA%86%E4%BC%98%E8%B6%8A%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82%E4%BB%A3%E7%A0%81%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E5%9C%A8https://github.com/dong845/MCP-MedSAM%E3%80%82">https://www.codabench.org/competitions/1847]^ï¼Œä¸æ’è¡Œæ¦œä¸Šçš„é¡¶å°–æ–¹æ³•ç›¸æ¯”ï¼ŒMCP-MedSAMåœ¨å•ä¸ªGPUä¸Šä»…éœ€è¦ä¸€å¤©çš„è®­ç»ƒæ—¶é—´å°±å–å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä»£ç å…¬å¼€å¯ç”¨åœ¨https://github.com/dong845/MCP-MedSAMã€‚</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05888v1">PDF</a></p><p><strong>Summary</strong></p><p>åŒ»ç–—å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒçš„é‡è¦åº”ç”¨é¢†åŸŸï¼Œé€šè¿‡è¯†åˆ«è§£å‰–ç»“æ„æˆ–å¼‚å¸¸æ¥å°†å›¾åƒåˆ†å‰²æˆæœ‰æ„ä¹‰çš„åŒºåŸŸã€‚æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚æœ€è¿‘å¼•å…¥çš„åˆ†å‰²æ¨¡å‹SAMä¸ºåŒ»å­¦é¢†åŸŸå¸¦æ¥äº†æœºä¼šï¼Œä½†å…¶åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²é¢†åŸŸçš„å¤§è§„æ¨¡åº”ç”¨ä»é¢ä¸´æ¨¡å‹ä½“ç§¯å¤§ã€GPUéœ€æ±‚é«˜ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è€…è‡´åŠ›äºå¼€å‘SAMçš„è½»é‡çº§ç‰ˆæœ¬ï¼Œä»¥é™ä½å…¶å‚æ•°æ•°é‡ï¼Œå®ç°åœ¨æœ‰é™GPUèµ„æºä¸‹çš„è®­ç»ƒï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„åˆ†å‰²æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¼ºå¤§çš„è½»é‡çº§åŒ»å­¦SAMæ¨¡å‹MCP-MedSAMï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šè®­ç»ƒä¸€å¤©æ—¶é—´å³å¯è·å¾—ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ã€‚è¯¥æ–¹æ³•å·²ç»åœ¨å¤§å‹æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œå¹¶ä¸”ä¸æ’è¡Œæ¦œä¸Šçš„é¡¶å°–æ–¹æ³•ç›¸æ¯”è¡¨ç°å‡ºäº†ä¼˜è¶Šæ€§ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>åŒ»ç–—å›¾åƒåˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒå¤„ç†çš„é‡è¦åˆ†æ”¯ï¼Œæ¶‰åŠå°†å›¾åƒåˆ’åˆ†ä¸ºæœ‰æ„ä¹‰çš„åŒºåŸŸä»¥è¯†åˆ«è§£å‰–ç»“æ„æˆ–å¼‚å¸¸ã€‚</li><li>æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—å›¾åƒåˆ†å‰²ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œä¿ƒè¿›äº†è‡ªåŠ¨åŒ–è¿›ç¨‹çš„å‘å±•ã€‚</li><li>Segmentation Anything Model (SAM) çš„å¼•å…¥ä¸ºåŒ»ç–—å›¾åƒåˆ†å‰²å¸¦æ¥äº†æ–°çš„æœºä¼šï¼Œä½†å…¶åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨é¢ä¸´æ¨¡å‹ä½“ç§¯å¤§å’ŒGPUéœ€æ±‚é«˜çš„æŒ‘æˆ˜ã€‚</li><li>ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æ­£åœ¨å¼€å‘SAMçš„è½»é‡çº§ç‰ˆæœ¬ï¼Œä»¥é™ä½å‚æ•°æ•°é‡å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li><li>MCP-MedSAMæ˜¯ä¸€ä¸ªå¼ºå¤§çš„è½»é‡çº§åŒ»å­¦SAMæ¨¡å‹ï¼Œèƒ½åœ¨å•ä¸ªGPUä¸Šä¸€å¤©å†…å®Œæˆè®­ç»ƒå¹¶è¡¨ç°å‡ºå“è¶Šçš„åˆ†å‰²æ€§èƒ½ã€‚</li><li>MCP-MedSAMå·²ç»åœ¨å¤§å‹æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶ä¸æ’è¡Œæ¦œä¸Šçš„é¡¶å°–æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-816a3a68fde2a292a6487291549399fd.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-658fd8081a976908b0d1781361f33b94.jpg" align="middle"></details><h2 id="RefSAM3D-Adapting-SAM-with-Cross-modal-Reference-for-3D-Medical-Image-Segmentation"><a href="#RefSAM3D-Adapting-SAM-with-Cross-modal-Reference-for-3D-Medical-Image-Segmentation" class="headerlink" title="RefSAM3D: Adapting SAM with Cross-modal Reference for 3D Medical Image   Segmentation"></a>RefSAM3D: Adapting SAM with Cross-modal Reference for 3D Medical Image Segmentation</h2><p><strong>Authors:Xiang Gao, Kai Lu</strong></p><p>The Segment Anything Model (SAM), originally built on a 2D Vision Transformer (ViT), excels at capturing global patterns in 2D natural images but struggles with 3D medical imaging modalities like CT and MRI. These modalities require capturing spatial information in volumetric space for tasks such as organ segmentation and tumor quantification. To address this challenge, we introduce RefSAM3D, which adapts SAM for 3D medical imaging by incorporating a 3D image adapter and cross-modal reference prompt generation. Our approach modifies the visual encoder to handle 3D inputs and enhances the mask decoder for direct 3D mask generation. We also integrate textual prompts to improve segmentation accuracy and consistency in complex anatomical scenarios. By employing a hierarchical attention mechanism, our model effectively captures and integrates information across different scales. Extensive evaluations on multiple medical imaging datasets demonstrate the superior performance of RefSAM3D over state-of-the-art methods. Our contributions advance the application of SAM in accurately segmenting complex anatomical structures in medical imaging.</p><blockquote><p>Segment Anything Modelï¼ˆSAMï¼‰æœ€åˆæ˜¯å»ºç«‹åœ¨2D Vision Transformerï¼ˆViTï¼‰ä¹‹ä¸Šï¼Œæ“…é•¿æ•æ‰2Dè‡ªç„¶å›¾åƒä¸­çš„å…¨å±€æ¨¡å¼ï¼Œä½†åœ¨å¤„ç†å¦‚CTå’ŒMRIç­‰3DåŒ»å­¦æˆåƒæ¨¡å¼æ—¶å´é¢ä¸´å›°éš¾ã€‚è¿™äº›æ¨¡å¼éœ€è¦æ•è·ä½“ç§¯ç©ºé—´ä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œä»¥å®Œæˆå™¨å®˜åˆ†å‰²å’Œè‚¿ç˜¤é‡åŒ–ç­‰ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefSAM3Dï¼Œå®ƒé€šè¿‡èå…¥3Då›¾åƒé€‚é…å™¨å’Œè·¨æ¨¡æ€å‚è€ƒæç¤ºç”Ÿæˆï¼Œå°†SAMæ”¹ç¼–ä¸ºé€‚ç”¨äº3DåŒ»å­¦å½±åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿®æ”¹äº†è§†è§‰ç¼–ç å™¨ä»¥å¤„ç†3Dè¾“å…¥ï¼Œå¹¶å¢å¼ºäº†æ©è†œè§£ç å™¨ä»¥è¿›è¡Œç›´æ¥çš„3Dæ©è†œç”Ÿæˆã€‚æˆ‘ä»¬è¿˜æ•´åˆäº†æ–‡æœ¬æç¤ºï¼Œä»¥æé«˜å¤æ‚è§£å‰–åœºæ™¯ä¸­çš„åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚é€šè¿‡é‡‡ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å¹¶æ•´åˆä¸åŒå°ºåº¦çš„ä¿¡æ¯ã€‚åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒRefSAM3Dçš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ¨åŠ¨äº†SAMåœ¨åŒ»å­¦æˆåƒä¸­å‡†ç¡®åˆ†å‰²å¤æ‚è§£å‰–ç»“æ„çš„åº”ç”¨ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05605v1">PDF</a></p><p><strong>Summary</strong></p><p>SAMæ¨¡å‹åœ¨äºŒç»´è‡ªç„¶å›¾åƒä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ä¸‰ç»´åŒ»å­¦å½±åƒå¦‚CTå’ŒMRIä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºRefSAM3Dï¼Œå®ƒé€šè¿‡èå…¥ä¸‰ç»´å›¾åƒé€‚é…å™¨å’Œè·¨æ¨¡æ€å‚è€ƒæç¤ºç”Ÿæˆï¼Œè®©SAMé€‚åº”ä¸‰ç»´åŒ»å­¦å½±åƒã€‚è¯¥æ–¹æ³•è°ƒæ•´è§†è§‰ç¼–ç å™¨ä»¥å¤„ç†ä¸‰ç»´è¾“å…¥ï¼Œå¢å¼ºæ©è†œè§£ç å™¨ä»¥å®ç°ç›´æ¥çš„ä¸‰ç»´æ©è†œç”Ÿæˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ç»“åˆäº†æ–‡æœ¬æç¤ºä»¥æé«˜å¤æ‚è§£å‰–åœºæ™¯çš„åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚é€šè¿‡åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸åŒå°ºåº¦ä¸Šæœ‰æ•ˆåœ°æ•æ‰å¹¶æ•´åˆä¿¡æ¯ã€‚åœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒRefSAM3Dçš„æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>SAMæ¨¡å‹æ“…é•¿æ•æ‰äºŒç»´è‡ªç„¶å›¾åƒä¸­çš„å…¨å±€æ¨¡å¼ï¼Œä½†åœ¨å¤„ç†ä¸‰ç»´åŒ»å­¦å½±åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li><li>RefSAM3Dæ—¨åœ¨è§£å†³SAMåœ¨ä¸‰ç»´åŒ»å­¦å½±åƒä¸Šçš„ä¸è¶³ï¼Œé€šè¿‡èå…¥ä¸‰ç»´å›¾åƒé€‚é…å™¨ç­‰æ–¹æ³•ä½¿å…¶é€‚åº”åŒ»å­¦å½±åƒåˆ†æã€‚</li><li>RefSAM3Dèƒ½å¤Ÿç›´æ¥ç”Ÿæˆä¸‰ç»´æ©è†œï¼Œæé«˜äº†åŒ»å­¦å½±åƒçš„åˆ†å‰²èƒ½åŠ›ã€‚</li><li>é€šè¿‡ç»“åˆæ–‡æœ¬æç¤ºï¼ŒRefSAM3Dåœ¨å¤æ‚è§£å‰–åœºæ™¯çš„å½±åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºæ›´é«˜çš„ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚</li><li>é‡‡ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹åœ¨ä¸åŒå°ºåº¦ä¸Šæ›´æœ‰æ•ˆåœ°æ•æ‰å’Œæ•´åˆä¿¡æ¯ã€‚</li><li>åœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRefSAM3Dæ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li><li>æ­¤ç ”ç©¶ä¸ºSAMæ¨¡å‹åœ¨åŒ»å­¦å½±åƒåˆ†å‰²é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ–°çš„å‘å±•æ–¹å‘ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6be6466209c0efbc9db2b74113c2dcb8.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a4067c1698755c1039699a5ba4d58ff9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b4d16b114ee6e065761f3d5666735f4b.jpg" align="middle"></details><h2 id="UNet-and-LSTM-combined-approach-for-Breast-Ultrasound-Image-Segmentation"><a href="#UNet-and-LSTM-combined-approach-for-Breast-Ultrasound-Image-Segmentation" class="headerlink" title="UNet++ and LSTM combined approach for Breast Ultrasound Image   Segmentation"></a>UNet++ and LSTM combined approach for Breast Ultrasound Image Segmentation</h2><p><strong>Authors:Saba Hesaraki, Morteza Akbari, Ramin Mousa</strong></p><p>Breast cancer stands as a prevalent cause of fatality among females on a global scale, with prompt detection playing a pivotal role in diminishing mortality rates. The utilization of ultrasound scans in the BUSI dataset for medical imagery pertaining to breast cancer has exhibited commendable segmentation outcomes through the application of UNet and UNet++ networks. Nevertheless, a notable drawback of these models resides in their inattention towards the temporal aspects embedded within the images. This research endeavors to enrich the UNet++ architecture by integrating LSTM layers and self-attention mechanisms to exploit temporal characteristics for segmentation purposes. Furthermore, the incorporation of a Multiscale Feature Extraction Module aims to grasp varied scale features within the UNet++. Through the amalgamation of our proposed methodology with data augmentation on the BUSI with GT dataset, an accuracy rate of 98.88%, specificity of 99.53%, precision of 95.34%, sensitivity of 91.20%, F1-score of 93.74, and Dice coefficient of 92.74% are achieved. These findings demonstrate competitiveness with cutting-edge techniques outlined in existing literature.</p><blockquote><p>ä¹³è…ºç™Œæ˜¯å…¨çƒå¥³æ€§æ­»äº¡çš„å¸¸è§åŸå› ä¹‹ä¸€ï¼Œè€ŒåŠæ—©å‘ç°å¯¹äºé™ä½æ­»äº¡ç‡èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨åŒ»ç–—å›¾åƒç›¸å…³çš„ä¹³è…ºç™Œè¶…å£°æ‰«ææ•°æ®é›†ï¼ˆBUSIï¼‰ä¸­ï¼Œé€šè¿‡åº”ç”¨UNetå’ŒUNet++ç½‘ç»œï¼Œå±•ç°å‡ºäº†ä»¤äººç§°èµçš„åˆ†å‰²æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„ä¸€ä¸ªæ˜æ˜¾ç¼ºç‚¹æ˜¯å¯¹å›¾åƒä¸­åµŒå…¥çš„æš‚æ—¶æ–¹é¢çš„å¿½è§†ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡é›†æˆLSTMå±‚å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¸°å¯ŒUNet++æ¶æ„ï¼Œä»¥åˆ©ç”¨æ—¶é—´ç‰¹å¾è¿›è¡Œåˆ†å‰²ã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—çš„ç›®çš„æ˜¯ä¸ºäº†æ•è·UNet++å†…çš„ä¸åŒå°ºåº¦ç‰¹å¾ã€‚é€šè¿‡å°†æœ¬ç ”ç©¶æå‡ºçš„æ–¹æ³•ä¸BUSI-GTæ•°æ®é›†ä¸Šçš„æ•°æ®å¢å¼ºç›¸ç»“åˆï¼Œå®ç°äº†å‡†ç¡®ç‡98.88%ï¼Œç‰¹å¼‚æ€§99.53%ï¼Œç²¾ç¡®åº¦95.34%ï¼Œæ•æ„Ÿåº¦91.20%ï¼ŒF1åˆ†æ•°93.74%ï¼Œä»¥åŠDiceç³»æ•°92.74%ã€‚è¿™äº›å‘ç°è¡¨æ˜ä¸ç°æœ‰æ–‡çŒ®ä¸­å‰æ²¿æŠ€æœ¯çš„ç«äº‰åŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05585v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ç ”ç©¶äº†ä¹³è…ºç™ŒåŒ»å­¦å½±åƒçš„åˆ†å‰²é—®é¢˜ï¼Œåˆ©ç”¨è¶…å£°æ‰«ææ•°æ®é›†çš„åŒ»å­¦å›¾åƒæ•°æ®ï¼Œæ¢è®¨äº†ä½¿ç”¨æ”¹è¿›åçš„UNet++ç½‘ç»œç»“åˆLSTMå±‚å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå›¾åƒåˆ†å‰²çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥å¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®ç‡å’Œè‰¯å¥½çš„æ€§èƒ½æŒ‡æ ‡ã€‚è¿™äº›å‘ç°ä¸ç°æœ‰æ–‡çŒ®ä¸­çš„å…ˆè¿›æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ä¹³è…ºç™Œæ˜¯å…¨çƒå¥³æ€§å¸¸è§çš„è‡´å‘½ç–¾ç—…ï¼ŒåŠæ—¶æ£€æµ‹å¯¹é™ä½æ­»äº¡ç‡è‡³å…³é‡è¦ã€‚</li><li>UNetå’ŒUNet++ç½‘ç»œåœ¨BUSIæ•°æ®é›†ä¸Šçš„åŒ»å­¦å›¾åƒåˆ†å‰²è¡¨ç°å‡ºè‰¯å¥½çš„ç»“æœã€‚</li><li>ç°æœ‰æ¨¡å‹å¿½ç•¥å›¾åƒä¸­çš„æ—¶é—´ç‰¹æ€§æ˜¯ä¸€ä¸ªæ˜¾è‘—ç¼ºç‚¹ã€‚</li><li>ç ”ç©¶é€šè¿‡æ•´åˆLSTMå±‚å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥ä¸°å¯ŒUNet++æ¶æ„ï¼Œä»¥åˆ©ç”¨æ—¶é—´ç‰¹æ€§è¿›è¡Œåˆ†å‰²ã€‚</li><li>å¼•å…¥çš„å¤šå°ºåº¦ç‰¹å¾æå–æ¨¡å—æ—¨åœ¨æ•è·ä¸åŒå°ºåº¦çš„ç‰¹å¾ã€‚</li><li>ç»“åˆæå‡ºçš„æ–¹æ³•å’Œæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œåœ¨BUSI with GTæ•°æ®é›†ä¸Šå–å¾—äº†é«˜å‡†ç¡®ç‡ï¼ˆ98.88%ï¼‰ã€é«˜ç‰¹å¼‚æ€§ï¼ˆ99.53%ï¼‰ã€ç²¾å‡†åº¦ï¼ˆ95.34%ï¼‰ã€æ•æ„Ÿæ€§ï¼ˆ91.2%ï¼‰å’ŒF1åˆ†æ•°ï¼ˆ93.74%ï¼‰ä»¥åŠDiceç³»æ•°ï¼ˆ92.74%ï¼‰ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b8782b804c1a2c0df6c0b89baa9b9fb2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-de0212116fddb8d9b0dc23fae0891a0c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-13f69da6d252b45847695820a8ec0b6b.jpg" align="middle"></details><h2 id="From-Deterministic-to-Probabilistic-A-Novel-Perspective-on-Domain-Generalization-for-Medical-Image-Segmentation"><a href="#From-Deterministic-to-Probabilistic-A-Novel-Perspective-on-Domain-Generalization-for-Medical-Image-Segmentation" class="headerlink" title="From Deterministic to Probabilistic: A Novel Perspective on Domain   Generalization for Medical Image Segmentation"></a>From Deterministic to Probabilistic: A Novel Perspective on Domain Generalization for Medical Image Segmentation</h2><p><strong>Authors:Yuheng Xu, Taiping Zhang</strong></p><p>Traditional domain generalization methods often rely on domain alignment to reduce inter-domain distribution differences and learn domain-invariant representations. However, domain shifts are inherently difficult to eliminate, which limits model generalization. To address this, we propose an innovative framework that enhances data representation quality through probabilistic modeling and contrastive learning, reducing dependence on domain alignment and improving robustness under domain variations. Specifically, we combine deterministic features with uncertainty modeling to capture comprehensive feature distributions. Contrastive learning enforces distribution-level alignment by aligning the mean and covariance of feature distributions, enabling the model to dynamically adapt to domain variations and mitigate distribution shifts. Additionally, we design a frequency-domain-based structural enhancement strategy using discrete wavelet transforms to preserve critical structural details and reduce visual distortions caused by style variations. Experimental results demonstrate that the proposed framework significantly improves segmentation performance, providing a robust solution to domain generalization challenges in medical image segmentation.</p><blockquote><p>ä¼ ç»Ÿé¢†åŸŸæ³›åŒ–æ–¹æ³•é€šå¸¸ä¾èµ–äºé¢†åŸŸå¯¹é½æ¥å‡å°‘è·¨åŸŸåˆ†å¸ƒå·®å¼‚å¹¶å­¦ä¹ é¢†åŸŸä¸å˜çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œé¢†åŸŸåç§»æœ¬è´¨ä¸Šéš¾ä»¥æ¶ˆé™¤ï¼Œè¿™é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°æ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ æé«˜æ•°æ®è¡¨ç¤ºè´¨é‡ï¼Œå‡å°‘å¯¹é¢†åŸŸå¯¹é½çš„ä¾èµ–ï¼Œå¹¶åœ¨é¢†åŸŸå˜åŒ–ä¸‹æé«˜ç¨³å¥æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç¡®å®šæ€§ç‰¹å¾ä¸ä¸ç¡®å®šæ€§å»ºæ¨¡ç›¸ç»“åˆï¼Œä»¥æ•æ‰å…¨é¢çš„ç‰¹å¾åˆ†å¸ƒã€‚å¯¹æ¯”å­¦ä¹ é€šè¿‡å¯¹ç‰¹å¾åˆ†å¸ƒçš„å‡å€¼å’Œåæ–¹å·®è¿›è¡Œå¯¹é½ï¼Œå®ç°åˆ†å¸ƒçº§åˆ«çš„å¯¹é½ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€é€‚åº”é¢†åŸŸå˜åŒ–å¹¶ç¼“è§£åˆ†å¸ƒåç§»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ç¦»æ•£å°æ³¢å˜æ¢è®¾è®¡äº†ä¸€ç§åŸºäºé¢‘åŸŸçš„ç»“æ„å¢å¼ºç­–ç•¥ï¼Œä»¥ä¿ç•™å…³é”®çš„ç»“æ„ç»†èŠ‚ï¼Œå¹¶å‡å°‘ç”±é£æ ¼å˜åŒ–å¼•èµ·çš„è§†è§‰å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ³›åŒ–æŒ‘æˆ˜æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05572v1">PDF</a> 6 pages, 3 figures</p><p><strong>Summary</strong></p><p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¦‚ç‡å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ æå‡æ•°æ®è¡¨ç°è´¨é‡ï¼Œå‡å°‘äº†å¯¹é¢†åŸŸå¯¹é½çš„ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸä¸‹çš„ç¨³å¥æ€§ã€‚ç»“åˆç¡®å®šæ€§ç‰¹å¾å’Œä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰å…¨é¢çš„ç‰¹å¾åˆ†å¸ƒã€‚å¯¹æ¯”å­¦ä¹ é€šè¿‡å¯¹ç‰¹å¾åˆ†å¸ƒçš„å‡å€¼å’Œåæ–¹å·®è¿›è¡Œå¯¹é½ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŠ¨æ€é€‚åº”é¢†åŸŸå˜åŒ–å¹¶ç¼“è§£åˆ†å¸ƒåç§»é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜è®¾è®¡äº†ä¸€ç§åŸºäºé¢‘åŸŸçš„ç»“æ„å¢å¼ºç­–ç•¥ï¼Œä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢ä¿ç•™å…³é”®ç»“æ„ç»†èŠ‚ï¼Œå‡å°‘ç”±é£æ ¼å˜åŒ–å¼•èµ·çš„è§†è§‰å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ³›åŒ–æŒ‘æˆ˜æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ•°æ®è¡¨ç°è´¨é‡ä»¥å¢å¼ºæ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li><li>è¯¥æ¡†æ¶ç»“åˆæ¦‚ç‡å»ºæ¨¡å’Œå¯¹æ¯”å­¦ä¹ ï¼Œå‡å°‘äº†å¯¹é¢†åŸŸå¯¹é½çš„ä¾èµ–ã€‚</li><li>é€šè¿‡ç»“åˆç¡®å®šæ€§ç‰¹å¾å’Œä¸ç¡®å®šæ€§å»ºæ¨¡ï¼Œå…¨é¢æ•æ‰ç‰¹å¾åˆ†å¸ƒã€‚</li><li>å¯¹æ¯”å­¦ä¹ é€šè¿‡åˆ†å¸ƒå±‚é¢çš„å¯¹é½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”é¢†åŸŸå˜åŒ–å¹¶ç¼“è§£åˆ†å¸ƒåç§»ã€‚</li><li>è®¾è®¡äº†ä¸€ç§åŸºäºé¢‘åŸŸçš„ç»“æ„å¢å¼ºç­–ç•¥ï¼Œä½¿ç”¨ç¦»æ•£å°æ³¢å˜æ¢ä¿ç•™å…³é”®ç»“æ„ç»†èŠ‚ã€‚</li><li>æ¡†æ¶èƒ½å¤Ÿå‡å°‘ç”±é£æ ¼å˜åŒ–å¼•èµ·çš„è§†è§‰å¤±çœŸã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-70c2959e89ce9e49496ee104788a361e.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c7af8c853eb4320da9d9eaecb4aafc98.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8f239bc1132942adec3016e58c31f0df.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-269e457e89074236b9423c4fc7848d22.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-18a312578f8909e44df5799262d21298.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a1fe7aaf577946781875ec2f6a5ac146.jpg" align="middle"></details><h2 id="Automated-Dynamic-Image-Analysis-for-Particle-Size-and-Shape-Classification-in-Three-Dimensions"><a href="#Automated-Dynamic-Image-Analysis-for-Particle-Size-and-Shape-Classification-in-Three-Dimensions" class="headerlink" title="Automated Dynamic Image Analysis for Particle Size and Shape   Classification in Three Dimensions"></a>Automated Dynamic Image Analysis for Particle Size and Shape Classification in Three Dimensions</h2><p><strong>Authors:Sadegh Nadimi, Vasileios Angelidakis, Sadaf Maramizonouz, Chao Zhang</strong></p><p>We introduce OCULAR, an innovative hardware and software solution for three-dimensional dynamic image analysis of fine particles. Current state-of-the art instruments for dynamic image analysis are largely limited to two-dimensional imaging. However, extensive literature has demonstrated that relying on a single two-dimensional projection for particle characterisation can lead to inaccuracies in many applications. Existing three-dimensional imaging technologies, such as computed tomography, laser scanning, and orthophotography, are limited to static objects. These methods are often not statistically representative and come with significant post-processing requirements, as well as the need for specialised imaging and computing resources. OCULAR addresses these challenges by providing a cost-effective solution for imaging continuous particle streams using a synchronised array of optical cameras. Particle shape characterisation is achieved through the reconstruction of their three-dimensional surfaces. This paper details the OCULAR methodology, evaluates its reproducibility, and compares its results against X-ray micro computed tomography, highlighting its potential for efficient and reliable particle analysis.</p><blockquote><p>æˆ‘ä»¬ä»‹ç»äº†OCULARï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¾®ç²’ä¸‰ç»´åŠ¨æ€å›¾åƒåˆ†æçš„åˆ›æ–°è½¯ç¡¬ä»¶è§£å†³æ–¹æ¡ˆã€‚å½“å‰åŠ¨æ€å›¾åƒåˆ†æçš„æœ€å…ˆè¿›ä»ªå™¨å¤§å¤šä»…é™äºäºŒç»´æˆåƒã€‚ç„¶è€Œï¼Œå¤§é‡æ–‡çŒ®è¡¨æ˜ï¼Œä¾èµ–å•ä¸€äºŒç»´æŠ•å½±è¿›è¡Œé¢—ç²’è¡¨å¾ä¼šå¯¼è‡´è®¸å¤šåº”ç”¨ä¸­çš„ä¸å‡†ç¡®ã€‚ç°æœ‰çš„ä¸‰ç»´æˆåƒæŠ€æœ¯ï¼Œå¦‚è®¡ç®—æœºæ–­å±‚æ‰«æã€æ¿€å…‰æ‰«æå’Œæ­£å°„æ‘„å½±ï¼Œä»…é™äºé™æ€ç‰©ä½“ã€‚è¿™äº›æ–¹æ³•å¾€å¾€ä¸å…·æœ‰ç»Ÿè®¡ä»£è¡¨æ€§ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„åæœŸå¤„ç†è¦æ±‚ï¼Œä»¥åŠéœ€è¦ä¸“é—¨çš„æˆåƒå’Œè®¡ç®—èµ„æºã€‚OCULARé€šè¿‡æä¾›ä½¿ç”¨åŒæ­¥å…‰å­¦ç›¸æœºé˜µåˆ—å¯¹è¿ç»­é¢—ç²’æµè¿›è¡Œæˆåƒçš„æˆæœ¬æ•ˆç›Šè§£å†³æ–¹æ¡ˆæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é€šè¿‡é‡å»ºé¢—ç²’çš„ä¸‰ç»´è¡¨é¢æ¥å®ç°é¢—ç²’å½¢çŠ¶è¡¨å¾ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†OCULARçš„æ–¹æ³•è®ºï¼Œè¯„ä¼°äº†å…¶å¯é‡å¤æ€§ï¼Œå¹¶ä¸Xå°„çº¿æ˜¾å¾®è®¡ç®—æœºæ–­å±‚æ‰«æè¿›è¡Œäº†æ¯”è¾ƒï¼Œçªå‡ºäº†å…¶é«˜æ•ˆå¯é çš„é¢—ç²’åˆ†ææ½œåŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05347v1">PDF</a> 11 pages, 5 figures</p><p><strong>Summary</strong><br>OCULARæ˜¯ä¸€ç§åˆ›æ–°çš„ç¡¬ä»¶å’Œè½¯ä»¶è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå¯¹å¾®ç²’è¿›è¡Œä¸‰ç»´åŠ¨æ€å›¾åƒåˆ†æã€‚ç›¸è¾ƒäºå½“å‰äºŒç»´æˆåƒçš„æŠ€æœ¯ï¼ŒOCULARæ›´èƒ½å‡†ç¡®åˆ†æç²’å­ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è¿ç»­ç²’å­æµè¿›è¡Œæˆåƒæ—¶å…·æœ‰ä¼˜åŠ¿ã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŒæ­¥çš„å…‰å­¦ç›¸æœºé˜µåˆ—è¿›è¡Œæ‹æ‘„ï¼Œå¹¶é€šè¿‡é‡å»ºç²’å­ä¸‰ç»´è¡¨é¢å®ç°å½¢çŠ¶ç‰¹å¾æè¿°ã€‚æ­¤æ–¹æ³•å…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œä¸”ä¸Xå°„çº¿å¾®è®¡ç®—æœºæ–­å±‚æ‰«æç›¸æ¯”ï¼Œå…·æœ‰é«˜æ•ˆå¯é çš„ç²’å­åˆ†ææ½œåŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>OCULARæ˜¯ä¸€ç§ç”¨äºå¾®ç²’ä¸‰ç»´åŠ¨æ€å›¾åƒåˆ†æçš„åˆ›æ–°ç¡¬ä»¶å’Œè½¯ä»¶è§£å†³æ–¹æ¡ˆã€‚</li><li>å½“å‰äºŒç»´æˆåƒæŠ€æœ¯åœ¨ç²’å­ç‰¹æ€§åˆ†ææ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li><li>OCULARé€šè¿‡é‡‡ç”¨åŒæ­¥å…‰å­¦ç›¸æœºé˜µåˆ—å¯¹è¿ç»­ç²’å­æµè¿›è¡Œæˆåƒã€‚</li><li>è¯¥æŠ€æœ¯é€šè¿‡é‡å»ºç²’å­çš„ä¸‰ç»´è¡¨é¢æ¥å®ç°å½¢çŠ¶ç‰¹å¾æè¿°ã€‚</li><li>OCULARå…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œæ— éœ€ç‰¹æ®Šçš„æˆåƒå’Œè®¡ç®—èµ„æºã€‚</li><li>OCULARä¸Xå°„çº¿å¾®è®¡ç®—æœºæ–­å±‚æ‰«æç›¸æ¯”ï¼Œå…·æœ‰é«˜æ•ˆå¯é çš„ç²’å­åˆ†ææ½œåŠ›ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4f770ba9484b2d2ae06891667076d829.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4a3ed5deee94d2492506c12c57558a0b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d2a93486765b34fb9a744707ff7852d0.jpg" align="middle"></details><h2 id="Gla-AI4BioMed-at-RRG24-Visual-Instruction-tuned-Adaptation-for-Radiology-Report-Generation"><a href="#Gla-AI4BioMed-at-RRG24-Visual-Instruction-tuned-Adaptation-for-Radiology-Report-Generation" class="headerlink" title="Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for   Radiology Report Generation"></a>Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for Radiology Report Generation</h2><p><strong>Authors:Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</strong></p><p>We introduce a radiology-focused visual language model designed to generate radiology reports from chest X-rays. Building on previous findings that large language models (LLMs) can acquire multimodal capabilities when aligned with pretrained vision encoders, we demonstrate similar potential with chest X-ray images. This integration enhances the ability of model to understand and describe chest X-ray images. Our model combines an image encoder with a fine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate different sections of a radiology report with notable accuracy. The training process involves a two-stage approach: (i) initial alignment of chest X-ray features with the LLM (ii) followed by fine-tuning for radiology report generation.</p><blockquote><p>æˆ‘ä»¬å¼•å…¥äº†ä¸€æ¬¾ä¸“æ³¨äºæ”¾å°„å­¦çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¨åœ¨ä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥ä¸é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å¯¹é½ä»è€Œè·å–å¤šæ¨¡æ€èƒ½åŠ›çš„å‰æœŸç ”ç©¶åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒå±•ç¤ºäº†ç±»ä¼¼çš„æ½œåŠ›ã€‚è¿™ç§èåˆå¢å¼ºäº†æ¨¡å‹ç†è§£å’Œæè¿°èƒ¸éƒ¨Xå°„çº¿å›¾åƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆäº†å›¾åƒç¼–ç å™¨å’ŒåŸºäºVicuna-7Bæ¶æ„çš„å¾®è°ƒLLMï¼Œä½¿å…¶èƒ½å¤Ÿå‡†ç¡®ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„ä¸åŒéƒ¨åˆ†ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆiï¼‰èƒ¸éƒ¨Xå°„çº¿ç‰¹å¾ä¸LLMçš„åˆæ­¥å¯¹é½ï¼Œï¼ˆiiï¼‰éšåè¿›è¡Œé’ˆå¯¹æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å¾®è°ƒã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04954v1">PDF</a> Accepted by BioNLP@ACL 2024</p><p><strong>Summary</strong><br>åŸºäºå…ˆå‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ç›¸ç»“åˆçš„å¤šæ¨¡æ€èƒ½åŠ›ç ”ç©¶æˆæœï¼Œæˆ‘ä»¬é’ˆå¯¹æ”¾å°„å­¦é¢†åŸŸå¼€å‘äº†ä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šã€‚é€šè¿‡ç»“åˆå›¾åƒç¼–ç å™¨å’ŒåŸºäºVicuna-7Bæ¶æ„çš„å¾®è°ƒLLMï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ç”Ÿæˆä¸åŒéƒ¨åˆ†çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚å…¶è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šä¸€æ˜¯åˆæ­¥å¯¹é½èƒ¸éƒ¨Xå…‰ç‰¹å¾ä¸LLMï¼ŒäºŒæ˜¯é’ˆå¯¹æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆè¿›è¡Œå¾®è°ƒã€‚</p><p><strong>Key Takeaways</strong></p><ul><li>å¼•å…¥äº†ä¸€ç§é’ˆå¯¹æ”¾å°„å­¦çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯ä»èƒ¸éƒ¨Xå…‰ç‰‡ä¸­ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šã€‚</li><li>æ¨¡å‹ç»“åˆå›¾åƒç¼–ç å™¨å’ŒåŸºäºVicuna-7Bæ¶æ„çš„LLMï¼Œå±•ç¤ºå¤šæ¨¡æ€æ½œåŠ›ã€‚</li><li>è®­ç»ƒè¿‡ç¨‹é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ï¼šåˆæœŸå¯¹é½èƒ¸éƒ¨Xå…‰ç‰¹å¾åˆ°LLMï¼Œéšåé’ˆå¯¹æŠ¥å‘Šç”Ÿæˆè¿›è¡Œå¾®è°ƒã€‚</li><li>æ¨¡å‹èƒ½å¤Ÿç†è§£å¹¶æè¿°èƒ¸éƒ¨Xå…‰å›¾åƒï¼Œç”ŸæˆæŠ¥å‘Šçš„ä¸åŒéƒ¨åˆ†å…·æœ‰æ˜¾è‘—å‡†ç¡®æ€§ã€‚</li><li>æ­¤æ¨¡å‹ä¸ºæ”¾å°„å­¦æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œå¯èƒ½æ”¹å˜æ”¾å°„å­¦æŠ¥å‘Šçš„ç”Ÿæˆæ–¹å¼ã€‚</li><li>è¯¥æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•å’Œæ€§èƒ½ä¸ºåŒ»å­¦å½±åƒé¢†åŸŸçš„AIåº”ç”¨æä¾›äº†æ–°çš„æ€è·¯ã€‚</li></ul><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d861eb0d6474da5bbe41edb46a24dd4.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-32d950f7971be5ac37ff0f64aa7f42e1.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-0d0d309097ba931addf3eafa4c3ca75b.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cd86e80f6a7a6f5f910056145a7a7957.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8ac9fed378d0a6eeea3004656ed7e00.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-42cc48d7bd46c96bcb582db6974bd98a.jpg" align="middle"></details><h2 id="DAug-Diffusion-based-Channel-Augmentation-for-Radiology-Image-Retrieval-and-Classification"><a href="#DAug-Diffusion-based-Channel-Augmentation-for-Radiology-Image-Retrieval-and-Classification" class="headerlink" title="DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval   and Classification"></a>DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval and Classification</h2><p><strong>Authors:Ying Jin, Zhuoran Zhou, Haoquan Fang, Jenq-Neng Hwang</strong></p><p>Medical image understanding requires meticulous examination of fine visual details, with particular regions requiring additional attention. While radiologists build such expertise over years of experience, it is challenging for AI models to learn where to look with limited amounts of training data. This limitation results in unsatisfying robustness in medical image understanding. To address this issue, we propose Diffusion-based Feature Augmentation (DAug), a portable method that improves a perception modelâ€™s performance with a generative modelâ€™s output. Specifically, we extend a radiology image to multiple channels, with the additional channels being the heatmaps of regions where diseases tend to develop. A diffusion-based image-to-image translation model was used to generate such heatmaps conditioned on selected disease classes. Our method is motivated by the fact that generative models learn the distribution of normal and abnormal images, and such knowledge is complementary to image understanding tasks. In addition, we propose the Image-Text-Class Hybrid Contrastive learning to utilize both text and class labels. With two novel approaches combined, our method surpasses baseline models without changing the model architecture, and achieves state-of-the-art performance on both medical image retrieval and classification tasks.</p><blockquote><p>åŒ»å­¦å›¾åƒç†è§£éœ€è¦å¯¹ç»†å¾®çš„è§†è§‰ç»†èŠ‚è¿›è¡Œç»†è‡´çš„æ£€æŸ¥ï¼Œç‰¹å®šåŒºåŸŸéœ€è¦é¢å¤–çš„å…³æ³¨ã€‚è™½ç„¶æ”¾å°„ç§‘åŒ»ç”Ÿé€šè¿‡å¤šå¹´çš„ç»éªŒç§¯ç´¯è·å¾—äº†è¿™æ ·çš„ä¸“ä¸šçŸ¥è¯†ï¼Œä½†å¯¹äºäººå·¥æ™ºèƒ½æ¨¡å‹æ¥è¯´ï¼Œåœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸‹å­¦ä¹ åº”è¯¥å…³æ³¨çš„åœ°æ–¹å´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚è¿™ç§å±€é™æ€§å¯¼è‡´äº†åŒ»å­¦å›¾åƒç†è§£çš„ç¨³å¥æ€§ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£çš„ç‰¹å¾å¢å¼ºï¼ˆDAugï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºæé«˜æ„ŸçŸ¥æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ”¾å°„å­¦å›¾åƒæ‰©å±•åˆ°å¤šä¸ªé€šé“ï¼Œé¢å¤–çš„é€šé“æ˜¯ç–¾ç—…å€¾å‘å‘å±•åŒºåŸŸçš„çƒ­å›¾ã€‚æˆ‘ä»¬ä½¿ç”¨åŸºäºæ‰©æ•£çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹æ¥æ ¹æ®é€‰å®šçš„ç–¾ç—…ç±»åˆ«ç”Ÿæˆè¿™æ ·çš„çƒ­å›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°ä»¥ä¸‹äº‹å®çš„å¯å‘ï¼šç”Ÿæˆæ¨¡å‹å­¦ä¹ æ­£å¸¸å’Œå¼‚å¸¸å›¾åƒçš„åˆ†å¸ƒï¼Œè¿™äº›çŸ¥è¯†å¯¹äºå›¾åƒç†è§£ä»»åŠ¡æ˜¯äº’è¡¥çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å›¾åƒæ–‡æœ¬ç±»åˆ«æ··åˆå¯¹æ¯”å­¦ä¹ ï¼Œä»¥åˆ©ç”¨æ–‡æœ¬å’Œç±»åˆ«æ ‡ç­¾ã€‚é€šè¿‡ç»“åˆä¸¤ç§æ–°æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸éœ€è¦æ”¹å˜æ¨¡å‹æ¶æ„çš„æƒ…å†µä¸‹è¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨åŒ»å­¦å›¾åƒæ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04828v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„ç‰¹å¾å¢å¼ºï¼ˆDAugï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºï¼Œæé«˜æ„ŸçŸ¥æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç†è§£æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡æ‰©å±•æ”¾å°„å›¾åƒåˆ°å¤šä¸ªé€šé“ï¼Œå¹¶ç”Ÿæˆç–¾ç—…çš„æ½œåœ¨çƒ­å›¾ï¼Œå¢å¼ºæ¨¡å‹å¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨ã€‚ç»“åˆå›¾åƒæ–‡æœ¬åˆ†ç±»æ··åˆå¯¹æ¯”å­¦ä¹ ï¼Œè¯¥æ–¹æ³•åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„çš„å‰æä¸‹ï¼Œè¶…è¶Šäº†åŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†åŒ»å­¦å›¾åƒæ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>åŒ»å­¦å›¾åƒç†è§£éœ€è¦ç²¾ç»†çš„è§†è§‰ç»†èŠ‚åˆ†æï¼Œç‰¹å®šåŒºåŸŸéœ€é‡ç‚¹å…³æ³¨ã€‚</li><li>AIæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒç†è§£ä¸­é¢ä¸´è®­ç»ƒæ•°æ®æœ‰é™ã€ç¨³å¥æ€§ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li><li>æå‡ºæ‰©æ•£ç‰¹å¾å¢å¼ºï¼ˆDAugï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºæé«˜æ„ŸçŸ¥æ¨¡å‹æ€§èƒ½ã€‚</li><li>å°†åŒ»å­¦å›¾åƒæ‰©å±•åˆ°å¤šä¸ªé€šé“ï¼ŒåŒ…æ‹¬ç–¾ç—…çš„æ½œåœ¨çƒ­å›¾ã€‚</li><li>é‡‡ç”¨åŸºäºæ‰©æ•£çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ï¼Œæ ¹æ®é€‰å®šç–¾ç—…ç±»åˆ«ç”Ÿæˆçƒ­å›¾ã€‚</li><li>å€ŸåŠ©ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ­£å¸¸å’Œå¼‚å¸¸å›¾åƒåˆ†å¸ƒçš„çŸ¥è¯†ï¼Œè¡¥å……å›¾åƒç†è§£ä»»åŠ¡ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-52551bfa5714d144282c4a154e7f295d.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2882a33c0f0434b8488ef31a8b82e736.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-171693de6bab2f5aa834a32f3252944f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5eeb945673fadec31f755e92959f3429.jpg" align="middle"></details><h2 id="Multi-class-heart-disease-Detection-Classification-and-Prediction-using-Machine-Learning-Models"><a href="#Multi-class-heart-disease-Detection-Classification-and-Prediction-using-Machine-Learning-Models" class="headerlink" title="Multi-class heart disease Detection, Classification, and Prediction   using Machine Learning Models"></a>Multi-class heart disease Detection, Classification, and Prediction using Machine Learning Models</h2><p><strong>Authors:Mahfuzul Haque, Abu Saleh Musa Miah, Debashish Gupta, Md. Maruf Al Hossain Prince, Tanzina Alam, Nusrat Sharmin, Mohammed Sowket Ali, Jungpil Shin</strong></p><p>Heart disease is a leading cause of premature death worldwide, particularly among middle-aged and older adults, with men experiencing a higher prevalence. According to the World Health Organization (WHO), non-communicable diseases, including heart disease, account for 25% (17.9 million) of global deaths, with over 43,204 annual fatalities in Bangladesh. However, the development of heart disease detection (HDD) systems tailored to the Bangladeshi population remains underexplored due to the lack of benchmark datasets and reliance on manual or limited-data approaches. This study addresses these challenges by introducing new, ethically sourced HDD dataset, BIG-Dataset and CD dataset which incorporates comprehensive data on symptoms, examination techniques, and risk factors. Using advanced machine learning techniques, including Logistic Regression and Random Forest, we achieved a remarkable testing accuracy of up to 96.6% with Random Forest. The proposed AI-driven system integrates these models and datasets to provide real-time, accurate diagnostics and personalized healthcare recommendations. By leveraging structured datasets and state-of-the-art machine learning algorithms, this research offers an innovative solution for scalable and effective heart disease detection, with the potential to reduce mortality rates and improve clinical outcomes.</p><blockquote><p>å¿ƒè„ç—…æ˜¯å…¨çƒèŒƒå›´å†…ï¼Œå°¤å…¶æ˜¯ä¸­è€å¹´ç”·æ€§ç¾¤ä½“ä¸­çš„ä¸»è¦æ—©é€åŸå› ã€‚æ ¹æ®ä¸–ç•Œå«ç”Ÿç»„ç»‡ï¼ˆWHOï¼‰çš„æ•°æ®ï¼ŒåŒ…æ‹¬å¿ƒè„ç—…åœ¨å†…çš„éä¼ æŸ“æ€§ç–¾ç—…å å…¨çƒæ­»äº¡äººæ•°çš„25%ï¼ˆ1790ä¸‡äººï¼‰ï¼Œå…¶ä¸­å­ŸåŠ æ‹‰å›½æ¯å¹´æœ‰è¶…è¿‡43,204äººå› æ­¤ä¸§ç”Ÿã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åŸºå‡†æ•°æ®é›†ï¼Œä»¥åŠè¿‡åº¦ä¾èµ–æ‰‹åŠ¨æˆ–æœ‰é™æ•°æ®çš„æ–¹æ³•ï¼Œé’ˆå¯¹å­ŸåŠ æ‹‰å›½äººå£å®šåˆ¶çš„å¿ƒè„ç—…æ£€æµ‹ï¼ˆHDDï¼‰ç³»ç»Ÿçš„å¼€å‘ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ–°çš„ã€é“å¾·æ¥æºçš„HDDæ•°æ®é›†â€”â€”BIGæ•°æ®é›†å’ŒCDæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†ç—‡çŠ¶ã€æ£€æŸ¥æŠ€æœ¯å’Œé£é™©å› ç´ çš„å…¨é¢æ•°æ®ï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚ä½¿ç”¨å…ˆè¿›çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ŒåŒ…æ‹¬é€»è¾‘å›å½’å’Œéšæœºæ£®æ—ï¼Œæˆ‘ä»¬åˆ©ç”¨éšæœºæ£®æ—è¾¾åˆ°äº†é«˜è¾¾96.6%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚æ‰€æå‡ºçš„AIé©±åŠ¨ç³»ç»Ÿé›†æˆäº†è¿™äº›æ¨¡å‹å’Œæ•°æ®é›†ï¼Œä»¥æä¾›å®æ—¶ã€å‡†ç¡®çš„è¯Šæ–­å’Œä¸ªæ€§åŒ–çš„åŒ»ç–—å»ºè®®ã€‚é€šè¿‡åˆ©ç”¨ç»“æ„åŒ–æ•°æ®é›†å’Œæœ€æ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œè¿™é¡¹ç ”ç©¶ä¸ºå¯ä¼¸ç¼©å’Œæœ‰æ•ˆçš„å¿ƒè„ç—…æ£€æµ‹æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰é™ä½æ­»äº¡ç‡ã€æ”¹å–„ä¸´åºŠæ²»ç–—æ•ˆæœçš„æ½œåŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04792v1">PDF</a></p><p><strong>Summary</strong><br>å¿ƒè„ç—…æ˜¯å…¨çƒå°¤å…¶æ˜¯ä¸­è€å¹´ç”·æ€§æ—©é€çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚ä¸–ç•Œå«ç”Ÿç»„ç»‡è¡¨ç¤ºï¼Œéä¼ æŸ“æ€§ç–¾ç—…ï¼ˆåŒ…æ‹¬å¿ƒè„ç—…ï¼‰å å…¨çƒæ­»äº¡äººæ•°çš„å››åˆ†ä¹‹ä¸€ï¼Œå­ŸåŠ æ‹‰å›½æ¯å¹´æœ‰è¶…è¿‡å››ä¸‡ä¸‰åƒäººæ­»äº¡äºæ­¤ã€‚ç„¶è€Œï¼Œé’ˆå¯¹å­ŸåŠ æ‹‰å›½äººå£çš„å¿ƒè„ç—…æ£€æµ‹ï¼ˆHDDï¼‰ç³»ç»Ÿå‘å±•ä»ç„¶æ¬ ç¼ºç ”ç©¶ï¼Œç”±äºç¼ºä¹åŸºå‡†æ•°æ®é›†ä»¥åŠä¾èµ–äºæ‰‹åŠ¨æˆ–æœ‰é™æ•°æ®æ–¹æ³•ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼•å…¥æ–°çš„åˆæ³•è·å–HDDæ•°æ®é›†å’ŒCDæ•°æ®é›†æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†ç—‡çŠ¶ã€æ£€æŸ¥æŠ€æœ¯å’Œé£é™©å› ç´ çš„ç»¼åˆæ•°æ®ã€‚é€šè¿‡é€»è¾‘å›å½’å’Œéšæœºæ£®æ—ç­‰å…ˆè¿›æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæˆ‘ä»¬å–å¾—äº†é«˜è¾¾96.6%çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚æ‰€æå‡ºçš„AIé©±åŠ¨ç³»ç»Ÿé›†æˆäº†è¿™äº›æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå¯å®æ—¶å‡†ç¡®åœ°è¯Šæ–­å’Œä¸ªæ€§åŒ–åŒ»ç–—ä¿å¥å»ºè®®ã€‚è¯¥ç ”ç©¶æä¾›äº†ä¸€ä¸ªå¯è§„æ¨¡æ‰©å±•ä¸”æœ‰æ•ˆçš„åˆ›æ–°å¿ƒè„ç—…æ£€æµ‹è§£å†³æ–¹æ¡ˆï¼Œæœ‰æœ›é™ä½æ­»äº¡ç‡å¹¶æ”¹å–„ä¸´åºŠç»“æœã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>å¿ƒè„ç—…æ˜¯å…¨çƒæ€§çš„ä¸»è¦å¥åº·é—®é¢˜ï¼Œå°¤å…¶åœ¨ä¸­å¹´å’Œè€å¹´äººä¸­æ›´ä¸ºå¸¸è§ï¼Œå¯¹ç”·æ€§å½±å“æ›´å¤§ã€‚</li><li>éä¼ æŸ“æ€§ç–¾ç—…ï¼ŒåŒ…æ‹¬å¿ƒè„ç—…ï¼Œå å…¨çƒæ­»äº¡äººæ•°çš„å››åˆ†ä¹‹ä¸€ï¼Œå­ŸåŠ æ‹‰å›½çš„å¹´åº¦æ­»äº¡äººæ•°è¶…è¿‡å››ä¸‡ä¸‰åƒäººã€‚</li><li>é’ˆå¯¹å­ŸåŠ æ‹‰å›½äººå£çš„å¿ƒè„ç—…æ£€æµ‹ç³»ç»Ÿå‘å±•ä»ç„¶ä¸è¶³ï¼Œç¼ºä¹åŸºå‡†æ•°æ®é›†å’Œä¾èµ–æ‰‹åŠ¨æˆ–æœ‰é™æ•°æ®æ–¹æ³•çš„é—®é¢˜çªå‡ºã€‚</li><li>æœ¬ç ”ç©¶å¼•å…¥äº†æ–°çš„åˆæ³•è·å–HDDæ•°æ®é›†å’ŒCDæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç—‡çŠ¶ã€æ£€æŸ¥æŠ€æœ¯å’Œé£é™©å› ç´ çš„ç»¼åˆæ•°æ®ã€‚</li><li>é€šè¿‡é€»è¾‘å›å½’å’Œéšæœºæ£®æ—ç­‰æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæµ‹è¯•å‡†ç¡®ç‡é«˜è¾¾96.6%ã€‚</li><li>AIé©±åŠ¨çš„ç³»ç»Ÿé›†æˆäº†è¿™äº›æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå¯å®æ—¶æä¾›å‡†ç¡®çš„è¯Šæ–­å’Œä¸ªæ€§åŒ–åŒ»ç–—å»ºè®®ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-80530e3f37d869cf4f9228121cf541bf.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1986c885cd38c6705d5077f46758fec9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ab7efddf023e80fdc617ef3599ece3fc.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-780096739c3c15d58efa76caf4237aec.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-92b50d5b3eabd212ac79bb9b67bd6f34.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7f0427904498597e77f2670897bf6d84.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-881e3f34301961332cfce6b54c9f07f2.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-316e8846e0791f9053c11f1bcda4867a.jpg" align="middle"></details><h2 id="Unsupervised-Segmentation-by-Diffusing-Walking-and-Cutting"><a href="#Unsupervised-Segmentation-by-Diffusing-Walking-and-Cutting" class="headerlink" title="Unsupervised Segmentation by Diffusing, Walking and Cutting"></a>Unsupervised Segmentation by Diffusing, Walking and Cutting</h2><p><strong>Authors:Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson</strong></p><p>We propose an unsupervised image segmentation method using features from pre-trained text-to-image diffusion models. Inspired by classic spectral clustering approaches, we construct adjacency matrices from self-attention layers between image patches and recursively partition using Normalised Cuts. A key insight is that self-attention probability distributions, which capture semantic relations between patches, can be interpreted as a transition matrix for random walks across the image. We leverage this by first using Random Walk Normalized Cuts directly on these self-attention activations to partition the image, minimizing transition probabilities between clusters while maximizing coherence within clusters. Applied recursively, this yields a hierarchical segmentation that reflects the rich semantics in the pre-trained attention layers, without any additional training. Next, we explore other ways to build the NCuts adjacency matrix from features, and how we can use the random walk interpretation of self-attention to capture long-range relationships. Finally, we propose an approach to automatically determine the NCut cost criterion, avoiding the need to tune this manually. We quantitatively analyse the effect incorporating different features, a constant versus dynamic NCut threshold, and incorporating multi-node paths when constructing the NCuts adjacency matrix. We show that our approach surpasses all existing methods for zero-shot unsupervised segmentation, achieving state-of-the-art results on COCO-Stuff-27 and Cityscapes.</p><blockquote><p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç‰¹æ€§è¿›è¡Œæ— ç›‘ç£å›¾åƒåˆ†å‰²çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å—åˆ°ç»å…¸è°±èšç±»æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬æ„å»ºäº†å›¾åƒè¡¥ä¸é—´çš„è‡ªæ³¨æ„åŠ›å±‚çš„é‚»æ¥çŸ©é˜µï¼Œå¹¶ä½¿ç”¨å½’ä¸€åŒ–åˆ‡å‰²è¿›è¡Œé€’å½’åˆ’åˆ†ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè‡ªæ³¨æ„åŠ›æ¦‚ç‡åˆ†å¸ƒèƒ½å¤Ÿæ•æ‰è¡¥ä¸ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œå¯ä»¥è¢«è§£é‡Šä¸ºå›¾åƒä¸Šéšæœºæ¸¸èµ°çš„è½¬ç§»çŸ©é˜µã€‚æˆ‘ä»¬é¦–æ¬¡åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›æ¿€æ´»å€¼ä¸Šä½¿ç”¨éšæœºæ¸¸èµ°å½’ä¸€åŒ–åˆ‡å‰²æ¥åˆ’åˆ†å›¾åƒï¼Œä»¥æœ€å°åŒ–é›†ç¾¤ä¹‹é—´çš„è¿‡æ¸¡æ¦‚ç‡ï¼ŒåŒæ—¶æœ€å¤§åŒ–é›†ç¾¤å†…çš„è¿è´¯æ€§ã€‚é€’å½’åº”ç”¨è¿™ç§æ–¹æ³•ä¼šäº§ç”Ÿåæ˜ é¢„è®­ç»ƒæ³¨æ„åŠ›å±‚ä¸°å¯Œè¯­ä¹‰çš„å±‚æ¬¡åˆ†å‰²ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»ç‰¹å¾æ„å»ºNCutsé‚»æ¥çŸ©é˜µçš„å…¶ä»–æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨è‡ªæ³¨æ„åŠ›çš„éšæœºæ¸¸èµ°è§£é‡Šæ¥æ•æ‰é•¿è·ç¦»å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨ç¡®å®šNCutæˆæœ¬æ ‡å‡†çš„æ–¹æ³•ï¼Œé¿å…äº†éœ€è¦æ‰‹åŠ¨è°ƒæ•´è¿™ä¸€å‚æ•°ã€‚æˆ‘ä»¬å®šé‡åˆ†æäº†èå…¥ä¸åŒç‰¹æ€§ã€å¸¸æ•°ä¸åŠ¨æ€NCuté˜ˆå€¼ã€åœ¨æ„å»ºNCutsé‚»æ¥çŸ©é˜µæ—¶èå…¥å¤šèŠ‚ç‚¹è·¯å¾„çš„å½±å“ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é›¶æ ·æœ¬æ— ç›‘ç£åˆ†å‰²ä¸Šè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨COCO-Stuff-27å’ŒCityscapesä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04678v1">PDF</a></p><p><strong>Summary</strong><br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç‰¹å¾è¿›è¡Œæ— ç›‘ç£å›¾åƒåˆ†å‰²çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆç»å…¸è°±èšç±»æ€æƒ³ï¼Œæ„å»ºå›¾åƒè¡¥ä¸é—´çš„è‡ªæ³¨æ„åŠ›å±‚é‚»æ¥çŸ©é˜µï¼Œé‡‡ç”¨å½’ä¸€åŒ–åˆ‡å‰²è¿›è¡Œé€’å½’åˆ†å‰²ã€‚ç ”ç©¶å…³é”®è§è§£æ˜¯è‡ªæ³¨æ„åŠ›æ¦‚ç‡åˆ†å¸ƒèƒ½å¤Ÿè§£é‡Šä¸ºå›¾åƒå†…éšæœºæ¸¸èµ°çš„è½¬ç§»çŸ©é˜µï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€ç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›æ¿€æ´»ä¸Šåº”ç”¨éšæœºæ¸¸èµ°å½’ä¸€åŒ–åˆ‡å‰²ï¼Œå®ç°å›¾åƒåˆ†å‰²ï¼Œåæ˜ é¢„è®­ç»ƒæ³¨æ„å±‚ä¸­çš„ä¸°å¯Œè¯­ä¹‰ï¼Œæ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ„å»ºNCutsé‚»æ¥çŸ©é˜µçš„æ–¹æ³•å’Œè‡ªåŠ¨ç¡®å®šNCutæˆæœ¬æ ‡å‡†çš„æ–¹å¼ï¼Œå¹¶è¿›è¡Œå®šé‡åˆ†æã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨COCO-Stuff-27å’ŒCityscapesä¸Šè¶…è¶Šç°æœ‰é›¶ç›‘ç£åˆ†å‰²æ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ç‰¹å¾çš„æ— ç›‘ç£å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚</li><li>ç»“åˆè°±èšç±»æ€æƒ³ï¼Œåˆ©ç”¨è‡ªæ³¨æ„åŠ›å±‚æ„å»ºé‚»æ¥çŸ©é˜µã€‚</li><li>é€šè¿‡å½’ä¸€åŒ–åˆ‡å‰²é€’å½’åˆ†å‰²å›¾åƒï¼ŒåŸºäºè‡ªæ³¨æ„åŠ›æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œéšæœºæ¸¸èµ°è§£é‡Šã€‚</li><li>æå‡ºç›´æ¥åœ¨è‡ªæ³¨æ„åŠ›æ¿€æ´»ä¸Šåº”ç”¨éšæœºæ¸¸èµ°å½’ä¸€åŒ–åˆ‡å‰²çš„æ–¹æ³•ï¼Œåæ˜ é¢„è®­ç»ƒæ³¨æ„å±‚ä¸­çš„ä¸°å¯Œè¯­ä¹‰ã€‚</li><li>æ¢è®¨æ„å»ºNCutsé‚»æ¥çŸ©é˜µçš„å¤šç§æ–¹æ³•å’Œåˆ©ç”¨éšæœºæ¸¸èµ°è§£é‡Šè‡ªæ³¨æ„åŠ›çš„æ–¹å¼ä»¥æ•æ‰é•¿æœŸå…³ç³»ã€‚</li><li>å®ç°è‡ªåŠ¨ç¡®å®šNCutæˆæœ¬æ ‡å‡†çš„æ–¹æ³•ï¼Œé¿å…æ‰‹åŠ¨è°ƒæ•´ã€‚</li><li>åœ¨COCO-Stuff-27å’ŒCityscapesç­‰æ•°æ®é›†ä¸Šè¾¾åˆ°é›¶ç›‘ç£åˆ†å‰²çš„é¢†å…ˆæ°´å¹³ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-208532a2279f31dd63653ecce25d273a.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ad7a540c47f32b0be0cd496cdf6c7d8f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-273cfa0f411edc437a8b63b5e96901e8.jpg" align="middle"></details><h2 id="Using-Diffusion-Priors-for-Video-Amodal-Segmentation"><a href="#Using-Diffusion-Priors-for-Video-Amodal-Segmentation" class="headerlink" title="Using Diffusion Priors for Video Amodal Segmentation"></a>Using Diffusion Priors for Video Amodal Segmentation</h2><p><strong>Authors:Kaihua Chen, Deva Ramanan, Tarasha Khurana</strong></p><p>Object permanence in humans is a fundamental cue that helps in understanding persistence of objects, even when they are fully occluded in the scene. Present day methods in object segmentation do not account for this amodal nature of the world, and only work for segmentation of visible or modal objects. Few amodal methods exist; single-image segmentation methods cannot handle high-levels of occlusions which are better inferred using temporal information, and multi-frame methods have focused solely on segmenting rigid objects. To this end, we propose to tackle video amodal segmentation by formulating it as a conditional generation task, capitalizing on the foundational knowledge in video generative models. Our method is simple; we repurpose these models to condition on a sequence of modal mask frames of an object along with contextual pseudo-depth maps, to learn which object boundary may be occluded and therefore, extended to hallucinate the complete extent of an object. This is followed by a content completion stage which is able to inpaint the occluded regions of an object. We benchmark our approach alongside a wide array of state-of-the-art methods on four datasets and show a dramatic improvement of upto 13% for amodal segmentation in an objectâ€™s occluded region.</p><blockquote><p>äººç±»çš„å¯¹è±¡æ’å®šæ€§æ˜¯ä¸€ä¸ªåŸºæœ¬çº¿ç´¢ï¼Œæœ‰åŠ©äºç†è§£å¯¹è±¡åœ¨å®Œå…¨é®æŒ¡æ—¶çš„æŒä¹…æ€§ã€‚ç°æœ‰çš„å¯¹è±¡åˆ†å‰²æ–¹æ³•å¹¶æ²¡æœ‰è€ƒè™‘åˆ°ä¸–ç•Œçš„éæ¨¡æ€æ€§è´¨ï¼Œåªèƒ½å¯¹å¯è§æˆ–æ¨¡æ€å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚è™½ç„¶å­˜åœ¨ä¸€äº›éæ¨¡æ€æ–¹æ³•ï¼Œä½†å•å›¾åƒåˆ†å‰²æ–¹æ³•æ— æ³•å¤„ç†é«˜æ°´å¹³çš„é®æŒ¡ï¼Œè€Œè¿™äº›é®æŒ¡æ›´é€‚åˆé€šè¿‡æ—¶é—´ä¿¡æ¯è¿›è¡Œæ¨æ–­ï¼Œè€Œå¤šå¸§æ–¹æ³•åˆ™åªä¸“æ³¨äºå¯¹åˆšæ€§å¯¹è±¡çš„åˆ†å‰²ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºå°†è§†é¢‘éæ¨¡æ€åˆ†å‰²ä½œä¸ºæ¡ä»¶ç”Ÿæˆä»»åŠ¡æ¥è§£å†³ï¼Œåˆ©ç”¨è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åŸºç¡€çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¾ˆç®€å•ï¼šæˆ‘ä»¬é‡æ–°åˆ©ç”¨è¿™äº›æ¨¡å‹ï¼Œä»¥å¯¹è±¡çš„æ¨¡æ€æ©è†œå¸§åºåˆ—å’Œä¸Šä¸‹æ–‡ä¼ªæ·±åº¦å›¾ä½œä¸ºæ¡ä»¶ï¼Œå­¦ä¹ å“ªäº›å¯¹è±¡è¾¹ç•Œå¯èƒ½è¢«é®æŒ¡ï¼Œå› æ­¤å¯ä»¥æ‰©å±•åˆ°æƒ³è±¡å¯¹è±¡çš„å®Œæ•´ç¨‹åº¦ã€‚æ¥ä¸‹æ¥æ˜¯å†…å®¹å®Œæˆé˜¶æ®µï¼Œèƒ½å¤Ÿå¡«å……å¯¹è±¡è¢«é®æŒ¡çš„åŒºåŸŸã€‚æˆ‘ä»¬åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¸€ç³»åˆ—æœ€æ–°æ–¹æ³•çš„è¡¨ç°ï¼Œå¹¶åœ¨å¯¹è±¡çš„é®æŒ¡åŒºåŸŸè¿›è¡Œéæ¨¡æ€åˆ†å‰²æ—¶æ˜¾ç¤ºå‡ºé«˜è¾¾13%çš„æ˜¾è‘—æ”¹å–„ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04623v1">PDF</a> project page: <a target="_blank" rel="noopener" href="https://diffusion-vas.github.io/">https://diffusion-vas.github.io</a></p><p><strong>Summary</strong><br>æœ¬æ‘˜è¦æå‡ºä¸€ç§è§†é¢‘çš„æ— æ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡æ¡ä»¶ç”Ÿæˆä»»åŠ¡å¤„ç†è§†é¢‘çš„æ— æ¨¡æ€åˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åºåˆ—çš„æ¨¡æ€æ©è†œå¸§å’Œä¸Šä¸‹æ–‡ä¼ªæ·±åº¦å›¾æ¥é¢„æµ‹å¯¹è±¡çš„è¾¹ç•Œæ˜¯å¦è¢«é®æŒ¡ï¼Œä»è€Œæ¨æ–­å‡ºå¯¹è±¡çš„å®Œæ•´èŒƒå›´ï¼Œå¹¶å¡«å……é®æŒ¡åŒºåŸŸã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å¯¹è±¡é®æŒ¡åŒºåŸŸçš„æ— æ¨¡æ€åˆ†å‰²æ–¹é¢æé«˜äº†é«˜è¾¾13%ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>äººç±»ç‰©ä½“æ°¸ä¹…æ€§çš„æ¦‚å¿µæ˜¯ç†è§£ç‰©ä½“æŒä¹…æ€§çš„åŸºæœ¬çº¿ç´¢ï¼Œå°¤å…¶åœ¨ç‰©ä½“è¢«å®Œå…¨é®æŒ¡æ—¶ã€‚</li><li>ç°æœ‰ç‰©ä½“åˆ†å‰²æ–¹æ³•å¿½ç•¥äº†ä¸–ç•Œæ— æ¨¡æ€ï¼ˆéå±€éƒ¨é®æŒ¡ï¼‰ç‰¹æ€§ï¼Œåªèƒ½å¤„ç†å¯è§æˆ–æ¨¡æ€ç‰©ä½“çš„åˆ†å‰²ã€‚</li><li>æå‡ºä¸€ç§åŸºäºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ¡ä»¶ç”Ÿæˆä»»åŠ¡æ¥å¤„ç†è§†é¢‘çš„æ— æ¨¡æ€åˆ†å‰²é—®é¢˜ã€‚</li><li>æ–¹æ³•é€šè¿‡åºåˆ—çš„æ¨¡æ€æ©è†œå¸§å’Œä¸Šä¸‹æ–‡ä¼ªæ·±åº¦å›¾æ¥é¢„æµ‹å¯¹è±¡è¾¹ç•Œæ˜¯å¦è¢«é®æŒ¡ã€‚</li><li>é€šè¿‡å­¦ä¹ è¿™ç§å…ˆéªŒçŸ¥è¯†ï¼Œæ–¹æ³•å¯ä»¥æ¨æ–­å‡ºå¯¹è±¡çš„å®Œæ•´èŒƒå›´å¹¶å¡«å……é®æŒ¡åŒºåŸŸã€‚</li><li>æ–¹æ³•åŒ…æ‹¬å†…å®¹å®Œæˆé˜¶æ®µï¼Œèƒ½å¤Ÿå¡«å……å¯¹è±¡çš„é®æŒ¡åŒºåŸŸã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-bb3c96c35570a05920c9a51aac979a26.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ec9c6dc962d44f951be469b0cb7c4e55.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b123b987356a2e35fd162aa46119352f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f517d550c68a423aee883cd09bdd045f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89defab41ae644acb97e333d2cbadd54.jpg" align="middle"></details><h2 id="Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging"><a href="#Biologically-inspired-Semi-supervised-Semantic-Segmentation-for-Biomedical-Imaging" class="headerlink" title="Biologically-inspired Semi-supervised Semantic Segmentation for   Biomedical Imaging"></a>Biologically-inspired Semi-supervised Semantic Segmentation for Biomedical Imaging</h2><p><strong>Authors:Luca Ciampi, Gabriele Lagani, Giuseppe Amato, Fabrizio Falchi</strong></p><p>We propose a novel two-stage semi-supervised learning approach for training downsampling-upsampling semantic segmentation architectures. The first stage does not use backpropagation. Rather, it exploits the bio-inspired Hebbian principle â€œfire together, wire togetherâ€ as a local learning rule for updating the weights of both convolutional and transpose-convolutional layers, allowing unsupervised discovery of data features. In the second stage, the model is fine-tuned with standard backpropagation on a small subset of labeled data. We evaluate our methodology through experiments conducted on several widely used biomedical datasets, deeming that this domain is paramount in computer vision and is notably impacted by data scarcity. Results show that our proposed method outperforms SOTA approaches across different levels of label availability. Furthermore, we show that using our unsupervised stage to initialize the SOTA approaches leads to performance improvements. The code to replicate our experiments can be found at: <a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-medical-image-segmentation">https://github.com/ciampluca/hebbian-medical-image-segmentation</a></p><blockquote><p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µåŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸‹é‡‡æ ·-ä¸Šé‡‡æ ·è¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚ç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨åå‘ä¼ æ’­ã€‚ç›¸åï¼Œå®ƒåˆ©ç”¨ç”Ÿç‰©å¯å‘çš„èµ«å¸ƒåŸåˆ™â€œä¸€èµ·å¼€ç«ï¼Œä¸€èµ·è¿çº¿â€ä½œä¸ºå±€éƒ¨å­¦ä¹ è§„åˆ™ï¼Œä»¥æ›´æ–°å·ç§¯å±‚å’Œè½¬ç½®å·ç§¯å±‚çš„æƒé‡ï¼Œä»è€Œå®ç°æ•°æ®ç‰¹å¾çš„æ— ç›‘ç£å‘ç°ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹åœ¨å°‘é‡æ ‡è®°æ•°æ®å­é›†ä¸Šåˆ©ç”¨æ ‡å‡†åå‘ä¼ æ’­è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒæ˜¯åœ¨å‡ ä¸ªå¹¿æ³›ä½¿ç”¨çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªé¢†åŸŸåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­è‡³å…³é‡è¦ï¼Œè€Œä¸”å—åˆ°æ•°æ®ç¨€ç¼ºçš„æ˜¾è‘—å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æ ‡ç­¾å¯ç”¨æ€§ä¸åŒçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå…¶ä»–å‰æ²¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä½¿ç”¨æˆ‘ä»¬çš„æ— ç›‘ç£é˜¶æ®µæ¥åˆå§‹åŒ–å‰æ²¿æ–¹æ³•å¯ä»¥æé«˜æ€§èƒ½ã€‚å¤åˆ¶æˆ‘ä»¬å®éªŒçš„ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ciampluca/hebbian-medical-image-segmentation">https://github.com/ciampluca/hebbian-medical-image-segmentation</a></p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03192v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µåŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸‹é‡‡æ ·ä¸Šé‡‡æ ·è¯­ä¹‰åˆ†å‰²æ¶æ„ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨ç”Ÿç‰©å¯å‘çš„å­¦ä¹ è§„åˆ™ï¼Œä¸ä¾èµ–åå‘ä¼ æ’­ï¼Œåˆ©ç”¨èµ«å¸ƒåŸåˆ™æ›´æ–°æƒé‡ï¼Œè¿›è¡Œç‰¹å¾çš„æ— ç›‘ç£å‘ç°ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ ‡å‡†åå‘ä¼ æ’­å¯¹å°é‡æ ‡è®°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µåŠç›‘ç£å­¦ä¹ æ–¹æ³•è¿›è¡Œè¯­ä¹‰åˆ†å‰²æ¶æ„çš„è®­ç»ƒã€‚</li><li>ç¬¬ä¸€é˜¶æ®µä¸ä¾èµ–åå‘ä¼ æ’­ï¼Œåˆ©ç”¨èµ«å¸ƒåŸåˆ™è¿›è¡Œæƒé‡çš„æ— ç›‘ç£æ›´æ–°ã€‚</li><li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ ‡å‡†åå‘ä¼ æ’­å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li><li>æ–¹æ³•åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li><li>æ–¹æ³•åœ¨æ ‡ç­¾ç¨€ç¼ºçš„æƒ…å†µä¸‹è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li><li>ä½¿ç”¨æ— ç›‘ç£é˜¶æ®µåˆå§‹åŒ–ç°æœ‰æŠ€æœ¯å¯æé«˜æ€§èƒ½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8ae1ef65d6d78f5a3b2ced3f03e4787c.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c309be12fe46d7372170491747a3a3d4.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b1c8f4854d7245b4ec92c20c712c0362.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4cc32163d8904c981f8e26dcd1bcaece.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0f7129008c6dc7771d668465564eff26.jpg" align="middle"></details><h2 id="MRNet-Multifaceted-Resilient-Networks-for-Medical-Image-to-Image-Translation"><a href="#MRNet-Multifaceted-Resilient-Networks-for-Medical-Image-to-Image-Translation" class="headerlink" title="MRNet: Multifaceted Resilient Networks for Medical Image-to-Image   Translation"></a>MRNet: Multifaceted Resilient Networks for Medical Image-to-Image Translation</h2><p><strong>Authors:Hyojeong Lee, Youngwan Jo, Inpyo Hong, Sanghyun Park</strong></p><p>We propose a Multifaceted Resilient Network(MRNet), a novel architecture developed for medical image-to-image translation that outperforms state-of-the-art methods in MRI-to-CT and MRI-to-MRI conversion. MRNet leverages the Segment Anything Model (SAM) to exploit frequency-based features to build a powerful method for advanced medical image transformation. The architecture extracts comprehensive multiscale features from diverse datasets using a powerful SAM image encoder and performs resolution-aware feature fusion that consistently integrates U-Net encoder outputs with SAM-derived features. This fusion optimizes the traditional U-Net skip connection while leveraging transformer-based contextual analysis. The translation is complemented by an innovative dual-mask configuration incorporating dynamic attention patterns and a specialized loss function designed to address regional mapping mismatches, preserving both the gross anatomy and tissue details. Extensive validation studies have shown that MRNet outperforms state-of-the-art architectures, particularly in maintaining anatomical fidelity and minimizing translation artifacts.</p><blockquote><p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé¢å¼¹æ€§ç½‘ç»œï¼ˆMRNetï¼‰çš„æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡è€Œå¼€å‘ï¼Œå¹¶åœ¨MRIåˆ°CTå’ŒMRIåˆ°MRIçš„è½¬æ¢ä¸­è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ–¹æ³•çš„æ•ˆæœã€‚MRNetåˆ©ç”¨åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰æ¥å¼€å‘åŸºäºé¢‘ç‡çš„ç‰¹å¾ï¼Œä»¥å»ºç«‹å…ˆè¿›çš„åŒ»å­¦å›¾åƒè½¬æ¢æ–¹æ³•ã€‚è¯¥æ¶æ„ä½¿ç”¨å¼ºå¤§çš„SAMå›¾åƒç¼–ç å™¨ä»å„ç§æ•°æ®é›†ä¸­æå–å…¨é¢çš„å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶æ‰§è¡Œå…·æœ‰åˆ†è¾¨ç‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆï¼Œè¯¥èåˆå§‹ç»ˆå°†U-Netç¼–ç å™¨çš„è¾“å‡ºä¸SAMè¡ç”Ÿçš„ç‰¹å¾ç»“åˆåœ¨ä¸€èµ·ã€‚è¿™ç§èåˆä¼˜åŒ–äº†ä¼ ç»Ÿçš„U-Netè·³è¿‡è¿æ¥ï¼ŒåŒæ—¶åˆ©ç”¨åŸºäºå˜å‹å™¨çš„ä¸Šä¸‹æ–‡åˆ†æã€‚ç¿»è¯‘ç”±åˆ›æ–°çš„åŒæ©è†œé…ç½®è¡¥å……ï¼Œè¯¥é…ç½®ç»“åˆäº†åŠ¨æ€æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œä»¥è§£å†³åŒºåŸŸæ˜ å°„ä¸åŒ¹é…çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æ•´ä½“è§£å‰–ç»“æ„å’Œç»„ç»‡ç»†èŠ‚ã€‚å¹¿æ³›çš„éªŒè¯ç ”ç©¶è¡¨æ˜ï¼ŒMRNetä¼˜äºæœ€å…ˆè¿›çš„æ¶æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒè§£å‰–ä¿çœŸåº¦å’Œæœ€å°åŒ–ç¿»è¯‘ä¼ªå½±æ–¹é¢ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03039v1">PDF</a> This work has been submitted to the IEEE for possible publication</p><p><strong>æ‘˜è¦</strong></p><p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šé¢æŠ—ç½‘ç»œï¼ˆMRNetï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºåŒ»å­¦å›¾åƒåˆ°å›¾åƒè½¬æ¢å¼€å‘çš„æ–°å‹æ¶æ„ï¼Œå®ƒåœ¨MRIåˆ°CTå’ŒMRIåˆ°MRIè½¬æ¢ä¸­è¡¨ç°å‡ºè¶…è¶Šæœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚MRNetåˆ©ç”¨åˆ†å‰²ä»»ä½•ä¸œè¥¿æ¨¡å‹ï¼ˆSAMï¼‰æŒ–æ˜åŸºäºé¢‘ç‡çš„ç‰¹å¾ï¼Œå»ºç«‹äº†ä¸€ç§å…ˆè¿›çš„åŒ»å­¦å›¾åƒè½¬æ¢æ–¹æ³•ã€‚è¯¥æ¶æ„ä½¿ç”¨å¼ºå¤§çš„SAMå›¾åƒç¼–ç å™¨ä»å„ç§æ•°æ®é›†ä¸­æå–å…¨é¢çš„å¤šå°ºåº¦ç‰¹å¾ï¼Œå¹¶æ‰§è¡Œå…·æœ‰åˆ†è¾¨ç‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆï¼Œè¯¥èåˆå§‹ç»ˆå°†U-Netç¼–ç å™¨çš„è¾“å‡ºä¸SAMè¡ç”Ÿçš„ç‰¹å¾ç›¸ç»“åˆã€‚è¿™ç§èåˆä¼˜åŒ–äº†ä¼ ç»Ÿçš„U-Netè·³è·ƒè¿æ¥ï¼ŒåŒæ—¶åˆ©ç”¨åŸºäºå˜å‹å™¨çš„ä¸Šä¸‹æ–‡åˆ†æã€‚ç¿»è¯‘é€šè¿‡åˆ›æ–°çš„åŒæ©æ¨¡é…ç½®è¿›è¡Œè¡¥å……ï¼Œè¯¥é…ç½®ç»“åˆäº†åŠ¨æ€æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªä¸“é—¨çš„æŸå¤±å‡½æ•°ï¼Œä»¥è§£å†³åŒºåŸŸæ˜ å°„ä¸åŒ¹é…çš„é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æ€»ä½“è§£å‰–ç»“æ„å’Œç»„ç»‡ç»†èŠ‚ã€‚å¹¿æ³›çš„éªŒè¯ç ”ç©¶è¡¨æ˜ï¼ŒMRNetåœ¨ä¿æŒè§£å‰–ä¿çœŸåº¦å’Œæœ€å°åŒ–ç¿»è¯‘ä¼ªå½±æ–¹é¢ä¼˜äºæœ€æ–°çš„æ¶æ„ã€‚</p><p><strong>è¦ç‚¹</strong></p><ol><li>MRNetæ˜¯ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ°å›¾åƒè½¬æ¢çš„æ–°å‹æ¶æ„ã€‚</li><li>å®ƒåˆ©ç”¨Segment Anything Model (SAM) æå–é¢‘ç‡ç‰¹å¾ä¸ºåŸºç¡€è¿›è¡ŒåŒ»å­¦å›¾åƒè½¬æ¢ã€‚</li><li>MRNetèƒ½å¤Ÿå…¨é¢æå–å¤šå°ºåº¦ç‰¹å¾å¹¶è¿›è¡Œåˆ†è¾¨ç‡æ„ŸçŸ¥çš„ç‰¹å¾èåˆã€‚</li><li>MRNetå°†SAMå’ŒU-Netç»“åˆï¼Œä¼˜åŒ–äº†ä¼ ç»Ÿçš„è·³è·ƒè¿æ¥å¹¶ä½¿ç”¨åŸºäºå˜å‹å™¨çš„ä¸Šä¸‹æ–‡åˆ†æã€‚</li><li>MRNetå…·æœ‰åˆ›æ–°çš„åŒæ©æ¨¡é…ç½®ï¼Œç»“åˆåŠ¨æ€æ³¨æ„åŠ›æ¨¡å¼æ¥å¤„ç†åŒºåŸŸæ˜ å°„ä¸åŒ¹é…çš„é—®é¢˜ã€‚</li><li>MRNetè®¾è®¡äº†ä¸€ç§ä¸“é—¨çš„æŸå¤±å‡½æ•°æ¥ä¿æŒè§£å‰–ç»“æ„çš„å®Œæ•´æ€§å’Œç»„ç»‡ç»†èŠ‚ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7a78f48528f94fe0504ec02e6261b46f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2daa6d673d1bb2af40c304b1c38e9ec9.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-1a991887d1d1f3721c7b454d670fb701.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9e8d4da0616e2836ed68b334fe091435.jpg" align="middle"></details><h2 id="Is-Foreground-Prototype-Sufficient-Few-Shot-Medical-Image-Segmentation-with-Background-Fused-Prototype"><a href="#Is-Foreground-Prototype-Sufficient-Few-Shot-Medical-Image-Segmentation-with-Background-Fused-Prototype" class="headerlink" title="Is Foreground Prototype Sufficient? Few-Shot Medical Image Segmentation   with Background-Fused Prototype"></a>Is Foreground Prototype Sufficient? Few-Shot Medical Image Segmentation with Background-Fused Prototype</h2><p><strong>Authors:Song Tang, Chunxiao Zu, Wenxin Su, Yuan Dong, Mao Ye, Yan Gan, Xiatian Zhu</strong></p><p>Few-shot Semantic Segmentation(FSS)aim to adapt a pre-trained model to new classes with as few as a single labeled training sample per class. The existing prototypical work used in natural image scenarios biasedly focus on capturing foregroundâ€™s discrimination while employing a simplistic representation for background, grounded on the inherent observation separation between foreground and background. However, this paradigm is not applicable to medical images where the foreground and background share numerous visual features, necessitating a more detailed description for background. In this paper, we present a new pluggable Background-fused prototype(Bro)approach for FSS in medical images. Instead of finding a commonality of background subjects in support image, Bro incorporates this background with two pivot designs. Specifically, Feature Similarity Calibration(FeaC)initially reduces noise in the support image by employing feature cross-attention with the query image. Subsequently, Hierarchical Channel Adversarial Attention(HiCA)merges the background into comprehensive prototypes. We achieve this by a channel groups-based attention mechanism, where an adversarial Mean-Offset structure encourages a coarse-to-fine fusion. Extensive experiments show that previous state-of-the-art methods, when paired with Bro, experience significant performance improvements. This demonstrates a more integrated way to represent backgrounds specifically for medical image.</p><blockquote><p>å°‘é‡è¯­ä¹‰åˆ†å‰²ï¼ˆFSSï¼‰æ—¨åœ¨å°†é¢„è®­ç»ƒæ¨¡å‹é€‚åº”äºæ–°ç±»åˆ«ï¼Œæ¯ç±»åªéœ€ä¸€ä¸ªæ ‡è®°çš„è®­ç»ƒæ ·æœ¬ã€‚ç°æœ‰çš„åŸå‹å·¥ä½œåœ¨è‡ªç„¶å›¾åƒåœºæ™¯ä¸­åé‡äºæ•è·å‰æ™¯çš„åŒºåˆ†åº¦ï¼ŒåŒæ—¶å¯¹èƒŒæ™¯é‡‡ç”¨ç®€å•çš„è¡¨ç¤ºæ–¹å¼ï¼ŒåŸºäºå‰æ™¯å’ŒèƒŒæ™¯ä¹‹é—´çš„å›ºæœ‰è§‚å¯Ÿåˆ†ç¦»ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨¡å¼å¹¶ä¸é€‚ç”¨äºåŒ»å­¦å›¾åƒï¼Œå› ä¸ºå‰æ™¯å’ŒèƒŒæ™¯å…±äº«è®¸å¤šè§†è§‰ç‰¹å¾ï¼Œéœ€è¦å¯¹èƒŒæ™¯è¿›è¡Œæ›´è¯¦ç»†çš„æè¿°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯æ’å…¥å¼èƒŒæ™¯èåˆåŸå‹ï¼ˆBroï¼‰æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒçš„FSSã€‚Broä¸éœ€è¦åœ¨æ”¯æŒå›¾åƒä¸­å¯»æ‰¾èƒŒæ™¯ä¸»é¢˜å…±æ€§ï¼Œè€Œæ˜¯ä¸ä¸¤ä¸ªæ¢è½´è®¾è®¡ç›¸ç»“åˆèå…¥èƒŒæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œç‰¹å¾ç›¸ä¼¼æ€§æ ¡å‡†ï¼ˆFeaCï¼‰é¦–å…ˆé€šè¿‡é‡‡ç”¨ç‰¹å¾äº¤å‰æ³¨æ„åŠ›ä¸æŸ¥è¯¢å›¾åƒç›¸ç»“åˆæ¥å‡å°‘æ”¯æŒå›¾åƒä¸­çš„å™ªå£°ã€‚éšåï¼Œå±‚æ¬¡åŒ–é€šé“å¯¹æŠ—æ³¨æ„åŠ›ï¼ˆHiCAï¼‰å°†èƒŒæ™¯åˆå¹¶åˆ°ç»¼åˆåŸå‹ä¸­ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºé€šé“ç»„çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­å¯¹æŠ—æ€§å‡å€¼åç§»ç»“æ„é¼“åŠ±ä»ç²—ç³™åˆ°ç²¾ç»†çš„èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“ä¸Broé…å¯¹æ—¶ï¼Œä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ”¹å–„ã€‚è¿™è¯æ˜äº†ä¸€ç§æ›´ä¸ºæ•´åˆçš„æ–¹å¼æ¥è¡¨ç¤ºåŒ»å­¦å›¾åƒä¸­çš„èƒŒæ™¯ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02983v1">PDF</a></p><p><strong>Summary</strong></p><p>è¯¥è®ºæ–‡é’ˆå¯¹åŒ»å­¦å›¾åƒä¸­çš„å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¯æ’å…¥èƒŒæ™¯èåˆåŸå‹ï¼ˆBroï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§æ ¡å‡†ï¼ˆFeaCï¼‰å’Œå±‚æ¬¡åŒ–é€šé“å¯¹æŠ—æ³¨æ„åŠ›ï¼ˆHiCAï¼‰æœºåˆ¶ï¼Œå°†èƒŒæ™¯ä¿¡æ¯èå…¥åŸå‹è¡¨ç¤ºä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBroæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ€§èƒ½ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›æ›´å…¨é¢çš„èƒŒæ™¯è¡¨ç¤ºæ–¹å¼ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>è¯¥è®ºæ–‡å…³æ³¨åŒ»å­¦å›¾åƒä¸­çš„å°æ ·æœ¬è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œå³ä½¿ç”¨æå°‘é‡æœ‰æ ‡ç­¾çš„è®­ç»ƒæ ·æœ¬å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé€‚åº”ã€‚</li><li>ç°æœ‰æ–¹æ³•åœ¨è‡ªç„¶å›¾åƒåœºæ™¯ä¸­ä½¿ç”¨çš„åŸå‹å·¥ä½œåå‘äºæ•æ‰å‰æ™¯çš„åˆ¤åˆ«ä¿¡æ¯ï¼Œè€Œå¯¹èƒŒæ™¯é‡‡ç”¨ç®€å•çš„è¡¨ç¤ºæ–¹å¼ã€‚</li><li>åŒ»å­¦å›¾åƒä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯å…·æœ‰è®¸å¤šå…±äº«çš„è§†è§‰ç‰¹å¾ï¼Œéœ€è¦æ›´è¯¦ç»†çš„èƒŒæ™¯æè¿°ã€‚</li><li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„èƒŒæ™¯èåˆåŸå‹ï¼ˆBroï¼‰æ–¹æ³•ï¼Œé€‚ç”¨äºåŒ»å­¦å›¾åƒçš„FSSã€‚</li><li>Broæ–¹æ³•é€šè¿‡ç‰¹å¾ç›¸ä¼¼æ€§æ ¡å‡†ï¼ˆFeaCï¼‰å‡å°‘æ”¯æŒå›¾åƒä¸­çš„å™ªå£°ï¼Œé€šè¿‡å±‚æ¬¡åŒ–é€šé“å¯¹æŠ—æ³¨æ„åŠ›ï¼ˆHiCAï¼‰å°†èƒŒæ™¯èå…¥ç»¼åˆåŸå‹è¡¨ç¤ºã€‚</li><li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBroæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b105ad8594cdba1e46ebc883da774072.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0646dd7d9384d26b2709d8c5fa130373.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fcf9bb34de6b9698da850ad881be9c29.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2c28c7af3739ac15d99b9ebedb5a8a9f.jpg" align="middle"></details><h2 id="RaD-A-Metric-for-Medical-Image-Distribution-Comparison-in-Out-of-Domain-Detection-and-Other-Applications"><a href="#RaD-A-Metric-for-Medical-Image-Distribution-Comparison-in-Out-of-Domain-Detection-and-Other-Applications" class="headerlink" title="RaD: A Metric for Medical Image Distribution Comparison in Out-of-Domain   Detection and Other Applications"></a>RaD: A Metric for Medical Image Distribution Comparison in Out-of-Domain Detection and Other Applications</h2><p><strong>Authors:Nicholas Konz, Yuwen Chen, Hanxue Gu, Haoyu Dong, Yaqian Chen, Maciej A. Mazurowski</strong></p><p>Determining whether two sets of images belong to the same or different domain is a crucial task in modern medical image analysis and deep learning, where domain shift is a common problem that commonly results in decreased model performance. This determination is also important to evaluate the output quality of generative models, e.g., image-to-image translation models used to mitigate domain shift. Current metrics for this either rely on the (potentially biased) choice of some downstream task such as segmentation, or adopt task-independent perceptual metrics (e.g., FID) from natural imaging which insufficiently capture anatomical consistency and realism in medical images. We introduce a new perceptual metric tailored for medical images: Radiomic Feature Distance (RaD), which utilizes standardized, clinically meaningful and interpretable image features. We show that RaD is superior to other metrics for out-of-domain (OOD) detection in a variety of experiments. Furthermore, RaD outperforms previous perceptual metrics (FID, KID, etc.) for image-to-image translation by correlating more strongly with downstream task performance as well as anatomical consistency and realism, and shows similar utility for evaluating unconditional image generation. RaD also offers additional benefits such as interpretability, as well as stability and computational efficiency at low sample sizes. Our results are supported by broad experiments spanning four multi-domain medical image datasets, nine downstream tasks, six image translation models, and other factors, highlighting the broad potential of RaD for medical image analysis.</p><blockquote><p>ç¡®å®šä¸¤ç»„å›¾åƒæ˜¯å¦å±äºåŒä¸€é¢†åŸŸæˆ–ä¸åŒé¢†åŸŸæ˜¯ç°ä»£åŒ»å­¦å›¾åƒåˆ†æå’Œæ·±åº¦å­¦ä¹ ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œé¢†åŸŸåç§»æ˜¯ä¸€ä¸ªå¸¸è§çš„é—®é¢˜ï¼Œé€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸ªå†³å®šå¯¹äºè¯„ä¼°ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºè´¨é‡ä¹Ÿå¾ˆé‡è¦ï¼Œä¾‹å¦‚ç”¨äºç¼“è§£é¢†åŸŸåç§»çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ¨¡å‹ã€‚å½“å‰çš„æŒ‡æ ‡è¦ä¹ˆä¾èµ–äºæŸäº›ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰çš„é€‰æ‹©ï¼ˆå¯èƒ½å­˜åœ¨åè§ï¼‰ï¼Œè¦ä¹ˆé‡‡ç”¨è‡ªç„¶æˆåƒçš„ä»»åŠ¡ç‹¬ç«‹æ„ŸçŸ¥æŒ‡æ ‡ï¼ˆå¦‚FIDï¼‰ï¼Œè¿™äº›æŒ‡æ ‡ä¸è¶³ä»¥æ•æ‰åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–ä¸€è‡´æ€§å’ŒçœŸå®æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹åŒ»å­¦å›¾åƒå¼•å…¥äº†ä¸€ç§æ–°çš„æ„ŸçŸ¥æŒ‡æ ‡ï¼šæ”¾å°„å­¦ç‰¹å¾è·ç¦»ï¼ˆRaDï¼‰ï¼Œå®ƒåˆ©ç”¨æ ‡å‡†åŒ–ã€ä¸´åºŠæœ‰æ„ä¹‰å’Œå¯è§£é‡Šçš„å›¾åƒç‰¹å¾ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å„ç§å®éªŒä¸­ï¼ŒRaDåœ¨å…¶ä»–æŒ‡æ ‡ä¸­è¡¨ç°å‡ºå“è¶Šçš„è·¨é¢†åŸŸï¼ˆOODï¼‰æ£€æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒRaDåœ¨ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä»¥åŠä¸è§£å‰–ä¸€è‡´æ€§å’ŒçœŸå®æ€§çš„ç›¸å…³æ€§æ–¹é¢ï¼Œè¡¨ç°ä¼˜äºä¹‹å‰çš„æ„ŸçŸ¥æŒ‡æ ‡ï¼ˆå¦‚FIDã€KIDç­‰ï¼‰ï¼Œåœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¸ºå‡ºè‰²ï¼Œå¹¶åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆè¯„ä¼°ä¸­æ˜¾ç¤ºå‡ºç±»ä¼¼çš„å®ç”¨æ€§ã€‚RaDè¿˜æä¾›äº†å¯è§£é‡Šæ€§ã€ç¨³å®šæ€§ä»¥åŠå°æ ·æœ¬è®¡ç®—æ•ˆç‡ç­‰é¢å¤–ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„ç»“æœå¾—åˆ°äº†å¹¿æ³›çš„å®éªŒæ”¯æŒï¼Œè¿™äº›å®éªŒæ¶µç›–äº†å››ä¸ªå¤šé¢†åŸŸåŒ»å­¦å›¾åƒæ•°æ®é›†ã€ä¹ä¸ªä¸‹æ¸¸ä»»åŠ¡ã€å…­ä¸ªå›¾åƒç¿»è¯‘æ¨¡å‹ä»¥åŠå…¶ä»–å› ç´ ï¼Œçªå‡ºäº†RaDåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¹¿é˜”æ½œåŠ›ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01496v1">PDF</a></p><p><strong>Summary</strong></p><p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œæ·±åº¦å­¦ä¹ é¢†åŸŸä¸­ï¼Œåˆ¤æ–­ä¸¤ç»„å›¾åƒæ˜¯å¦å±äºåŒä¸€åŸŸçš„é‡è¦æ€§ã€‚é’ˆå¯¹ç°æœ‰åº¦é‡æŒ‡æ ‡çš„ä¸è¶³ï¼Œå¦‚å€¾å‘äºä¸‹æ¸¸ä»»åŠ¡çš„æ½œåœ¨åè§æˆ–è‡ªç„¶æˆåƒä¸­çš„æ„ŸçŸ¥æŒ‡æ ‡ä¸è¶³ä»¥æ•æ‰åŒ»å­¦å›¾åƒçš„è§£å‰–ä¸€è‡´æ€§å’Œé€¼çœŸåº¦ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ„ŸçŸ¥æŒ‡æ ‡â€”â€”æ”¾å°„å­¦ç‰¹å¾è·ç¦»ï¼ˆRaDï¼‰ã€‚è¯¥æŒ‡æ ‡åˆ©ç”¨æ ‡å‡†åŒ–ã€ä¸´åºŠæ„ä¹‰é‡å¤§ä¸”å¯è§£é‡Šçš„åŒ»å­¦å›¾åƒç‰¹å¾è¿›è¡Œè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒRaDåœ¨å¤šç§å®éªŒä¸­çš„ç¦»ç¾¤æ£€æµ‹è¡¨ç°ä¼˜äºå…¶ä»–æŒ‡æ ‡ï¼Œå¯¹äºå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡èƒ½å¤Ÿæ›´å¼ºçƒˆåœ°ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½å’Œè§£å‰–ä¸€è‡´æ€§ä¸é€¼çœŸåº¦ç›¸å…³ï¼Œå¹¶å±•ç¤ºäº†åœ¨æ— æ¡ä»¶å›¾åƒç”Ÿæˆä¸­çš„è¯„ä»·èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒRaDè¿˜å…·æœ‰å¯è§£é‡Šæ€§ã€ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ç­‰é¢å¤–ä¼˜åŠ¿ã€‚å®éªŒç»“æœå¹¿æ³›æ”¯æŒäº†RaDåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ç¡®å®šåŒ»å­¦å›¾åƒæ˜¯å¦å±äºåŒä¸€åŸŸå¯¹äºåŒ»å­¦å›¾åƒåˆ†æå’Œæ·±åº¦å­¦ä¹ è‡³å…³é‡è¦ï¼Œå› ä¸ºåŸŸåç§»æ˜¯ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li><li>å½“å‰åº¦é‡æŒ‡æ ‡å­˜åœ¨ç¼ºé™·ï¼Œå¯èƒ½å€¾å‘äºä¸‹æ¸¸ä»»åŠ¡çš„åè§æˆ–ä¸è¶³ä»¥æ•æ‰åŒ»å­¦å›¾åƒçš„è§£å‰–ä¸€è‡´æ€§å’Œé€¼çœŸåº¦ã€‚</li><li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ„ŸçŸ¥æŒ‡æ ‡â€”â€”æ”¾å°„å­¦ç‰¹å¾è·ç¦»ï¼ˆRaDï¼‰ï¼Œç”¨äºè¯„ä¼°åŒ»å­¦å›¾åƒã€‚</li><li>RaDåˆ©ç”¨æ ‡å‡†åŒ–ã€ä¸´åºŠæ„ä¹‰é‡å¤§ä¸”å¯è§£é‡Šçš„å›¾åƒç‰¹å¾è¿›è¡Œè¯„ä¼°ã€‚</li><li>RaDåœ¨å¤šç§å®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„ç¦»ç¾¤æ£€æµ‹æ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–æŒ‡æ ‡æ›´å…·ä¼˜åŠ¿ã€‚</li><li>RaDåœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºçƒˆçš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ç›¸å…³æ€§ï¼ŒåŒæ—¶å…·å¤‡è‰¯å¥½çš„è§£å‰–ä¸€è‡´æ€§ä¸é€¼çœŸåº¦è¯„ä¼°èƒ½åŠ›ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8d1a1d66d5d318a5a9222abdf93232a3.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a2a8b236c47f9d2e863d61d50575087f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d041cda8d50e86ab1e28b831b52b4919.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b3477e64f58d2624dd585fdfaf8d481f.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1f8034b2e25e1d441950312da18ce076.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ec6cda0e32f3010d4bde75364a27408.jpg" align="middle"></details><h2 id="Exploiting-Precision-Mapping-and-Component-Specific-Feature-Enhancement-for-Breast-Cancer-Segmentation-and-Identification"><a href="#Exploiting-Precision-Mapping-and-Component-Specific-Feature-Enhancement-for-Breast-Cancer-Segmentation-and-Identification" class="headerlink" title="Exploiting Precision Mapping and Component-Specific Feature Enhancement   for Breast Cancer Segmentation and Identification"></a>Exploiting Precision Mapping and Component-Specific Feature Enhancement for Breast Cancer Segmentation and Identification</h2><p><strong>Authors:Pandiyaraju V, Shravan Venkatraman, Pavan Kumar S, Santhosh Malarvannan, Kannan A</strong></p><p>Breast cancer is one of the leading causes of death globally, and thus there is an urgent need for early and accurate diagnostic techniques. Although ultrasound imaging is a widely used technique for breast cancer screening, it faces challenges such as poor boundary delineation caused by variations in tumor morphology and reduced diagnostic accuracy due to inconsistent image quality. To address these challenges, we propose novel Deep Learning (DL) frameworks for breast lesion segmentation and classification. We introduce a precision mapping mechanism (PMM) for a precision mapping and attention-driven LinkNet (PMAD-LinkNet) segmentation framework that dynamically adapts spatial mappings through morphological variation analysis, enabling precise pixel-level refinement of tumor boundaries. Subsequently, we introduce a component-specific feature enhancement module (CSFEM) for a component-specific feature-enhanced classifier (CSFEC-Net). Through a multi-level attention approach, the CSFEM magnifies distinguishing features of benign, malignant, and normal tissues. The proposed frameworks are evaluated against existing literature and a diverse set of state-of-the-art Convolutional Neural Network (CNN) architectures. The obtained results show that our segmentation model achieves an accuracy of 98.1%, an IoU of 96.9%, and a Dice Coefficient of 97.2%. For the classification model, an accuracy of 99.2% is achieved with F1-score, precision, and recall values of 99.1%, 99.3%, and 99.1%, respectively.</p><blockquote><p>ä¹³è…ºç™Œæ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œå› æ­¤è¿«åˆ‡éœ€è¦æ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­æŠ€æœ¯ã€‚è¶…å£°æˆåƒè™½ç„¶æ˜¯ä¹³è…ºç™Œç­›æŸ¥ä¸­å¹¿æ³›ä½¿ç”¨çš„ä¸€ç§æŠ€æœ¯ï¼Œä½†å®ƒé¢ä¸´ç€å› è‚¿ç˜¤å½¢æ€å˜åŒ–å¯¼è‡´çš„è¾¹ç•Œåˆ’åˆ†ä¸è‰¯ä»¥åŠå›¾åƒè´¨é‡ä¸ä¸€è‡´å¯¼è‡´çš„è¯Šæ–­å‡†ç¡®æ€§é™ä½ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä¹³è…ºç—…ç¶åˆ†å‰²å’Œåˆ†ç±»çš„æ–°å‹æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬ä¸ºç²¾ç¡®æ˜ å°„å’Œæ³¨æ„åŠ›é©±åŠ¨çš„LinkNetï¼ˆPMAD-LinkNetï¼‰åˆ†å‰²æ¡†æ¶å¼•å…¥äº†ä¸€ç§ç²¾ç¡®æ˜ å°„æœºåˆ¶ï¼ˆPMMï¼‰ï¼Œè¯¥æœºåˆ¶é€šè¿‡å½¢æ€å˜åŒ–åˆ†æåŠ¨æ€é€‚åº”ç©ºé—´æ˜ å°„ï¼Œå®ç°å¯¹è‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§ç»†åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä¸ºç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºåˆ†ç±»å™¨ï¼ˆCSFEC-Netï¼‰å¼•å…¥äº†ä¸€ä¸ªç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆCSFEMï¼‰ã€‚é€šè¿‡å¤šçº§æ³¨æ„åŠ›æ–¹æ³•ï¼ŒCSFEMæ”¾å¤§äº†è‰¯æ€§ã€æ¶æ€§å’Œæ­£å¸¸ç»„ç»‡çš„åŒºåˆ†ç‰¹å¾ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ç°æœ‰æ–‡çŒ®å’Œä¸€ç³»åˆ—å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„è¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒã€‚è·å¾—çš„ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„åˆ†å‰²æ¨¡å‹è¾¾åˆ°äº†98.1%çš„å‡†ç¡®ç‡ï¼Œ96.9%çš„IoUå’Œ97.2%çš„Diceç³»æ•°ã€‚åˆ†ç±»æ¨¡å‹åˆ™å®ç°äº†99.2%çš„å‡†ç¡®ç‡ï¼ŒF1åˆ†æ•°ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«ä¸º99.1%ã€99.3%å’Œ99.1%ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02844v5">PDF</a> 27 pages, 18 figures, 6 tables</p><p><strong>Summary</strong></p><p>æœ¬ç ”ç©¶é’ˆå¯¹ä¹³è…ºç™Œè¶…å£°æˆåƒä¸­å­˜åœ¨çš„è¾¹ç•Œæ¨¡ç³Šå’Œå›¾åƒè´¨é‡ä¸ä¸€å¯¼è‡´çš„è¯Šæ–­å‡†ç¡®æ€§é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ çš„ä¹³è…ºç—…ç¶åˆ†å‰²å’Œåˆ†ç±»æ–°æ¡†æ¶ã€‚é€šè¿‡ç²¾ç¡®æ˜ å°„æœºåˆ¶å’Œç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼Œå®ç°å¯¹è‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§ç»†åŒ–å’Œè‰¯ã€æ¶æ€§åŠæ­£å¸¸ç»„ç»‡çš„ç‰¹å¾å¢å¼ºåˆ†ç±»ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåˆ†å‰²æ¨¡å‹å‡†ç¡®ç‡ä¸º98.1%ï¼ŒIoUå’ŒDiceç³»æ•°åˆ†åˆ«ä¸º96.9%å’Œ97.2%ï¼›åˆ†ç±»æ¨¡å‹å‡†ç¡®ç‡ä¸º99.2%ï¼ŒF1åˆ†æ•°ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«ä¸º99.1%ã€99.3%å’Œ99.1%ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>ä¹³è…ºç™Œæ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œéœ€è¦æ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­æŠ€æœ¯ã€‚</li><li>è¶…å£°æˆåƒåœ¨ä¹³è…ºç™Œç­›æŸ¥ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨è¾¹ç•Œæ¨¡ç³Šå’Œå›¾åƒè´¨é‡ä¸ä¸€çš„æŒ‘æˆ˜ã€‚</li><li>å¼•å…¥æ·±åº¦å­¦ä¹ æ–¹æ³•è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç²¾ç¡®æ˜ å°„æœºåˆ¶å’Œç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ã€‚</li><li>ç²¾ç¡®æ˜ å°„æœºåˆ¶èƒ½å¤Ÿå®ç°åŠ¨æ€é€‚åº”ç©ºé—´æ˜ å°„ï¼Œè¿›è¡Œè‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§ç»†åŒ–ã€‚</li><li>ç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—é€šè¿‡å¤šçº§åˆ«æ³¨æ„åŠ›æ–¹æ³•æ”¾å¤§è‰¯ã€æ¶æ€§åŠæ­£å¸¸ç»„ç»‡ä¹‹é—´çš„ç‰¹å¾å·®å¼‚ã€‚</li><li>åˆ†å‰²æ¨¡å‹è¯„ä¼°ç»“æœæ˜¾ç¤ºé«˜å‡†ç¡®ç‡ã€IoUå’ŒDiceç³»æ•°ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-16b8efc7decc6275f611191f2d7f1b92.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e73a377ac493de01906c5aad1842257f.jpg" align="middle"></details><h2 id="DDGS-CT-Direction-Disentangled-Gaussian-Splatting-for-Realistic-Volume-Rendering"><a href="#DDGS-CT-Direction-Disentangled-Gaussian-Splatting-for-Realistic-Volume-Rendering" class="headerlink" title="DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume   Rendering"></a>DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering</h2><p><strong>Authors:Zhongpai Gao, Benjamin Planche, Meng Zheng, Xiao Chen, Terrence Chen, Ziyan Wu</strong></p><p>Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks, especially for accurate but heavy physics-based Monte Carlo methods. While analytical DRR renderers offer greater efficiency, they overlook anisotropic X-ray image formation phenomena, such as Compton scattering. We present a novel approach that marries realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method separates the radiosity contribution into isotropic and direction-dependent components, approximating complex anisotropic interactions without intricate runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency. Our method outperforms state-of-the-art techniques in image accuracy. Furthermore, our DDGS shows promise for intraoperative applications and inverse problems such as pose registration, delivering superior registration accuracy and runtime performance compared to analytical DRR methods.</p><blockquote><p>æ•°å­—é‡å»ºæ”¾å°„å›¾åƒï¼ˆDRRsï¼‰æ˜¯ä»ä¸‰ç»´CTä½“ç§¯ä¸­ç”Ÿæˆçš„æ¨¡æ‹ŸäºŒç»´Xå°„çº¿å›¾åƒï¼Œå¹¿æ³›åº”ç”¨äºæœ¯å‰ç¯å¢ƒï¼Œä½†ç”±äºè®¡ç®—ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ç²¾ç¡®ä½†åŸºäºç‰©ç†çš„è’™ç‰¹å¡æ´›æ–¹æ³•æ–¹é¢ï¼Œå…¶åœ¨æœ¯ä¸­åº”ç”¨å—åˆ°é™åˆ¶ã€‚è™½ç„¶åˆ†æå‹DRRæ¸²æŸ“å™¨æä¾›äº†æ›´é«˜çš„æ•ˆç‡ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å„å‘å¼‚æ€§çš„Xå°„çº¿å›¾åƒå½¢æˆç°è±¡ï¼Œå¦‚åº·æ™®é¡¿æ•£å°„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒå°†åŸºäºç°å®ç‰©ç†çš„Xå°„çº¿æ¨¡æ‹Ÿä¸é«˜æ•ˆã€å¯å¾®åˆ†çš„DRRç”Ÿæˆç›¸ç»“åˆï¼Œä½¿ç”¨ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ã€‚æˆ‘ä»¬çš„æ–¹å‘åˆ†ç¦»çš„ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆDDGSï¼‰æ–¹æ³•å°†è¾å°„åº¦è´¡çŒ®åˆ†ä¸ºå„å‘åŒæ€§å’Œæ–¹å‘ä¾èµ–æ€§æˆåˆ†ï¼Œåœ¨ä¸éœ€è¦å¤æ‚è¿è¡Œæ—¶æ¨¡æ‹Ÿçš„æƒ…å†µä¸‹è¿‘ä¼¼å¤æ‚çš„å„å‘å¼‚æ€§ç›¸äº’ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€‚åº”äº†3DGSåˆå§‹åŒ–ä»¥è€ƒè™‘å±‚ææ•°æ®å±æ€§ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„DDGSå¯¹äºæœ¯ä¸­å’Œé€†å‘é—®é¢˜ï¼ˆå¦‚å§¿åŠ¿æ³¨å†Œï¼‰æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä¸è§£æDRRæ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´é«˜çš„æ³¨å†Œå‡†ç¡®æ€§å’Œè¿è¡Œæ—¶æ€§èƒ½ã€‚</p></blockquote><p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p><p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02518v2">PDF</a> Accepted by NeurIPS2024</p><p><strong>Summary</strong></p><p>åŸºäºä¸‰ç»´CTä½“ç§¯æ•°æ®çš„æ•°å­—åŒ–é‡å»ºæ”¾å°„å›¾åƒï¼ˆDRRsï¼‰æ˜¯æ¨¡æ‹Ÿçš„äºŒç»´Xå°„çº¿å›¾åƒï¼Œå¹¿æ³›åº”ç”¨äºæœ¯å‰è®¾ç½®ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å‡†ç¡®çš„ç‰©ç†è’™ç‰¹å¡æ´›æ–¹æ³•æ–¹é¢ï¼Œå…¶åœ¨æœ¯ä¸­åº”ç”¨å—é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆçœŸå®ç‰©ç†é©±åŠ¨çš„Xå°„çº¿æ¨¡æ‹Ÿä¸é«˜æ•ˆå¯å¾®åˆ†çš„DRRç”Ÿæˆæ–°æ–¹æ³•ï¼Œä½¿ç”¨ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ã€‚æˆ‘ä»¬çš„æ–¹å‘åˆ†ç¦»çš„ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆDDGSï¼‰æ–¹æ³•å°†è¾å°„è´¡çŒ®åˆ†ä¸ºå„å‘åŒæ€§å’Œæ–¹å‘ä¾èµ–æ€§æˆåˆ†ï¼Œè¿‘ä¼¼å¤æ‚çš„å„å‘å¼‚æ€§ç›¸äº’ä½œç”¨ï¼Œæ— éœ€å¤æ‚çš„è¿è¡Œæ—¶æ¨¡æ‹Ÿã€‚è¯¥æ–¹æ³•åœ¨å›¾åƒå‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”åœ¨æœ¯ä¸­åº”ç”¨å’Œåå‘é—®é¢˜å¦‚å§¿æ€æ³¨å†Œä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä¸è§£æDRRæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ³¨å†Œå‡†ç¡®æ€§å’Œè¿è¡Œæ€§èƒ½ã€‚</p><p><strong>Key Takeaways</strong></p><ol><li>DRRsæ˜¯ä»ä¸‰ç»´CTä½“ç§¯æ•°æ®ç”Ÿæˆçš„æ¨¡æ‹ŸäºŒç»´Xå°„çº¿å›¾åƒï¼Œå¹¿æ³›åº”ç”¨äºæœ¯å‰è®¾ç½®ã€‚</li><li>ç°æœ‰DRRæ–¹æ³•åœ¨æœ¯ä¸­åº”ç”¨å—é™ï¼Œä¸»è¦ç”±äºè®¡ç®—ç“¶é¢ˆå’Œå¤æ‚çš„ç‰©ç†æ¨¡æ‹Ÿã€‚</li><li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆçœŸå®ç‰©ç†é©±åŠ¨çš„Xå°„çº¿æ¨¡æ‹Ÿä¸é«˜æ•ˆå¯å¾®åˆ†çš„DRRç”Ÿæˆæ–°æ–¹æ³•â€”â€”ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆDDGSï¼‰ã€‚</li><li>DDGSé€šè¿‡å°†è¾å°„è´¡çŒ®åˆ†ä¸ºå„å‘åŒæ€§å’Œæ–¹å‘ä¾èµ–æ€§æˆåˆ†ï¼Œæ¥è¿‘ä¼¼å¤æ‚çš„å„å‘å¼‚æ€§ç›¸äº’ä½œç”¨ã€‚</li><li>DDGSæ–¹æ³•åœ¨ä¸éœ€å¤æ‚è¿è¡Œæ—¶æ¨¡æ‹Ÿçš„æƒ…å†µä¸‹ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li><li>DDGSåœ¨å›¾åƒå‡†ç¡®æ€§å’Œæœ¯ä¸­åº”ç”¨æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li></ol><details><summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary><img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8a63aefe41b12cf774dad39344713166.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-107fc1bcea7e331e681b9f163de5ec22.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43c08d71ca7404702f8277c07086f6d7.jpg" align="middle"> <img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7749305bd8a5ba3d0a222a758a12221e.jpg" align="middle"></details></div><hr><div class="reprint" id="reprint-statement"><div class="reprint__author"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-user">æ–‡ç« ä½œè€…:</i></span> <span class="reprint-info"><a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a></span></div><div class="reprint__type"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-link">æ–‡ç« é“¾æ¥:</i></span> <span class="reprint-info"><a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a></span></div><div class="reprint__notice"><span class="reprint-meta" style="font-weight:700"><i class="fas fa-copyright">ç‰ˆæƒå£°æ˜:</i></span> <span class="reprint-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> !</span></div></div><script async defer>function navToReprintStatement(){$("html, body").animate({scrollTop:$("#reprint-statement").offset().top-80},800)}document.addEventListener("copy",(function(t){M.toast({html:'<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>'})}))</script><div class="tag_share" style="display:block"><div class="post-meta__tag-list" style="display:inline-block"><div class="article-tag"><a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/"><span class="chip bg-color">åŒ»å­¦å›¾åƒ</span></a></div></div><div class="post_share" style="zoom:80%;width:fit-content;display:inline-block;float:right;margin:-.15rem 0"><link rel="stylesheet" href="/Talk2Paper/libs/share/css/share.min.css"><div id="article-share"><div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div><script src="/Talk2Paper/libs/share/js/social-share.min.js"></script></div></div></div></div></div><article id="prenext-posts" class="prev-next articles"><div class="row article-row"><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge left-badge text-color"><i class="fas fa-chevron-left"></i> &nbsp;ä¸Šä¸€ç¯‡</div><div class="card"><a href="/Talk2Paper/Paper/2024-12-10/TTS/"><div class="card-image"><img src="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" class="responsive-img" alt="TTS"> <span class="card-title">TTS</span></div></a><div class="card-content article-content"><div class="summary block-with-text">TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11 Towards Controllable Speech Synthesis in the Era of Large Language Models A Survey</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-11</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/TTS/" class="post-category">TTS</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/TTS/"><span class="chip bg-color">TTS</span></a></div></div></div><div class="article col s12 m6" data-aos="fade-up"><div class="article-badge right-badge text-color">ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i></div><div class="card"><a href="/Talk2Paper/Paper/2024-12-10/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/"><div class="card-image"><img src="https://picx.zhimg.com/v2-4a454aa61f7ec76e9cf1c776368ea2ca.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤"> <span class="card-title">ç‰™é½¿ä¿®å¤</span></div></a><div class="card-content article-content"><div class="summary block-with-text">ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11 VerA Versatile Anonymization Applicable to Clinical Facial Photographs</div><div class="publish-info"><span class="publish-date"><i class="far fa-clock fa-fw icon-date"></i> 2024-12-11</span><span class="publish-author"><i class="fas fa-bookmark fa-fw icon-category"></i> <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">ç‰™é½¿ä¿®å¤</a></span></div></div><div class="card-action article-tags"><a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/"><span class="chip bg-color">ç‰™é½¿ä¿®å¤</span></a></div></div></div></div></article></div><script src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script><script src="/Talk2Paper/libs/codeBlock/codeLang.js"></script><script src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script><script src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script></div><div id="toc-aside" class="expanded col l3 hide-on-med-and-down"><div class="toc-widget card" style="background-color:#fff"><div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div><div id="toc-content"></div></div></div></div><div id="floating-toc-btn" class="hide-on-med-and-down"><a class="btn-floating btn-large bg-color"><i class="fas fa-list-ul"></i></a></div><script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script><script>$((function(){tocbot.init({tocSelector:"#toc-content",contentSelector:"#articleContent",headingsOffset:-(.4*$(window).height()-45),collapseDepth:Number("0"),headingSelector:"h2, h3, h4"});let t=parseInt(.4*$(window).height()-64),e=$(".toc-widget");$(window).scroll((function(){$(window).scrollTop()>t?e.addClass("toc-fixed"):e.removeClass("toc-fixed")}));const o="expanded";let n=$("#toc-aside"),i=$("#main-content");$("#floating-toc-btn .btn-floating").click((function(){n.hasClass(o)?(n.removeClass(o).hide(),i.removeClass("l9")):(n.addClass(o).show(),i.addClass("l9")),function(t,e){let o=$("#"+t);if(0===o.length)return;let n=o.width();n+=n>=450?21:n>=350&&n<450?18:n>=300&&n<350?16:14,$("#"+e).width(n)}("artDetail","prenext-posts")}))}))</script></main><footer class="page-footer bg-color"><link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css"><style>.aplayer .aplayer-lrc p{display:none;font-size:12px;font-weight:700;line-height:16px!important}.aplayer .aplayer-lrc p.aplayer-lrc-current{display:none;font-size:15px;color:#42b983}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body{left:-66px!important}.aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover{left:0!important}</style><div><div class="row"><meting-js class="col l8 offset-l2 m10 offset-m1 s12" server="netease" type="playlist" id="503838841" fixed="true" autoplay theme="#42b983" loop order="random" preload="auto" volume="0.7" list-folded="true"></meting-js></div></div><script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script><script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script><div class="container row center-align" style="margin-bottom:15px!important"><div class="col s12 m8 l8 copy-right">Copyright&nbsp;&copy; <span id="year">2024</span> <a href="/Talk2Paper/about" target="_blank">Kedreamix</a> |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a> |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a><br>&nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span class="white-color">5506.2k</span> <span id="busuanzi_container_site_pv">&nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;<span id="busuanzi_value_site_pv" class="white-color"></span></span> <span id="busuanzi_container_site_uv">&nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;<span id="busuanzi_value_site_uv" class="white-color"></span></span><br><span id="sitetime">Loading ...</span><script>var calcSiteTime=function(){var e=864e5,t=new Date,n="2024",i=t.getFullYear(),a=t.getMonth()+1,r=t.getDate(),s=t.getHours(),o=t.getMinutes(),g=t.getSeconds(),d=Date.UTC(n,"1","1","0","0","0"),m=Date.UTC(i,a,r,s,o,g)-d,l=Math.floor(m/31536e6),c=Math.floor(m/e-365*l);if(n===String(i)){document.getElementById("year").innerHTML=i;var u="This site has been running for "+c+" days";u="æœ¬ç«™å·²è¿è¡Œ "+c+" å¤©",document.getElementById("sitetime").innerHTML=u}else{document.getElementById("year").innerHTML=n+" - "+i;var T="This site has been running for "+l+" years and "+c+" days";T="æœ¬ç«™å·²è¿è¡Œ "+l+" å¹´ "+c+" å¤©",document.getElementById("sitetime").innerHTML=T}};calcSiteTime()</script><br></div><div class="col s12 m4 l4 social-link social-statis"><a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50"><i class="fab fa-github"></i></a><a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50"><i class="fas fa-envelope-open"></i></a> <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50"><i class="fab fa-zhihu1">çŸ¥</i></a></div></div></footer><div class="progress-bar"></div><div id="searchModal" class="modal"><div class="modal-content"><div class="search-header"><span class="title"><i class="fas fa-search"></i> &nbsp;&nbsp;æœç´¢</span> <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—" class="search-input"></div><div id="searchResult"></div></div></div><script>$((function(){!function(t,e,r){"use strict";$.ajax({url:t,dataType:"xml",success:function(t){var n=$("entry",t).map((function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}})).get(),a=document.getElementById(e),s=document.getElementById(r);a.addEventListener("input",(function(){var t='<ul class="search-result-list">',e=this.value.trim().toLowerCase().split(/[\s\-]+/);s.innerHTML="",this.value.trim().length<=0||(n.forEach((function(r){var n=!0,a=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),i=r.url;i=0===i.indexOf("/")?r.url:"/"+i;var l=-1,c=-1,u=-1;if(""!==a&&""!==s&&e.forEach((function(t,e){l=a.indexOf(t),c=s.indexOf(t),l<0&&c<0?n=!1:(c<0&&(c=0),0===e&&(u=c))})),n){t+="<li><a href='"+i+"' class='search-result-title'>"+a+"</a>";var o=r.content.trim().replace(/<[^>]+>/g,"");if(u>=0){var h=u-20,f=u+80;h<0&&(h=0),0===h&&(f=100),f>o.length&&(f=o.length);var m=o.substr(h,f);e.forEach((function(t){var e=new RegExp(t,"gi");m=m.replace(e,'<em class="search-keyword">'+t+"</em>")})),t+='<p class="search-result">'+m+"...</p>"}t+="</li>"}})),t+="</ul>",s.innerHTML=t)}))}})}("/Talk2Paper/search.xml","searchInput","searchResult")}))</script><div class="stars-con"><div id="stars"></div><div id="stars2"></div><div id="stars3"></div></div><script>function switchNightMode(){$('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($("body")),setTimeout((function(){$("body").hasClass("DarkMode")?($("body").removeClass("DarkMode"),localStorage.setItem("isDark","0"),$("#sum-moon-icon").removeClass("fa-sun").addClass("fa-moon")):($("body").addClass("DarkMode"),localStorage.setItem("isDark","1"),$("#sum-moon-icon").addClass("fa-sun").removeClass("fa-moon")),setTimeout((function(){$(".Cuteen_DarkSky").fadeOut(1e3,(function(){$(this).remove()}))}),2e3)}))}</script><div id="backTop" class="top-scroll"><a class="btn-floating btn-large waves-effect waves-light" href="#!"><i class="fas fa-arrow-up"></i></a></div><script src="/Talk2Paper/libs/materialize/materialize.min.js"></script><script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script><script src="/Talk2Paper/libs/aos/aos.js"></script><script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script><script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script><script src="/Talk2Paper/js/matery.js"></script><script>var windowWidth=$(window).width();windowWidth>768&&document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>')</script><script src="https://ssl.captcha.qq.com/TCaptcha.js"></script><script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script><button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script src="/Talk2Paper/libs/others/clicklove.js" async></script><script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script><script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:3,processImages:null}</script><script>window.addEventListener("load",(function(){var a=/\.(gif|jpg|jpeg|tiff|png)$/i,e=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach((function(t){var r=t.parentNode;"A"===r.tagName&&(a.test(r.href)||e.test(r.href))&&(r.href=t.dataset.original)}))}))</script><script>(t=>{t.imageLazyLoadSetting.processImages=n;var e=t.imageLazyLoadSetting.isSPA,a=t.imageLazyLoadSetting.preloadRatio||1,o=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function n(n){(e||n)&&(o=i());for(var r,d=0;d<o.length;d++)0<=(r=(r=o[d]).getBoundingClientRect()).bottom&&0<=r.left&&r.top<=(t.innerHeight*a||document.documentElement.clientHeight*a)&&(()=>{var e,a,i,n,r=o[d];a=function(){o=o.filter((function(t){return r!==t})),t.imageLazyLoadSetting.onImageLoaded&&t.imageLazyLoadSetting.onImageLoaded(r)},(e=r).dataset.loaded||(e.hasAttribute("bg-lazy")?(e.removeAttribute("bg-lazy"),a&&a()):(i=new Image,n=e.getAttribute("data-original"),i.onload=function(){e.src=n,e.removeAttribute("data-original"),e.setAttribute("data-loaded",!0),a&&a()},i.onerror=function(){e.removeAttribute("data-original"),e.setAttribute("data-loaded",!1),e.src=n},e.src!==n&&(i.src=n)))})()}function r(){clearTimeout(n.tId),n.tId=setTimeout(n,500)}n(),document.addEventListener("scroll",r),t.addEventListener("resize",r),t.addEventListener("orientationchange",r)})(this)</script></body></html><script>var st,OriginTitile=document.title;document.addEventListener("visibilitychange",(function(){document.hidden?(document.title="Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ",clearTimeout(st)):(document.title="Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼",st=setTimeout((function(){document.title=OriginTitile}),3e3))}))</script>