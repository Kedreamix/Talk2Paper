<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="元宇宙/虚拟人">
    <meta name="description" content="元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-12-11  MixedGaussianAvatar Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>元宇宙/虚拟人 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ba1148f524c897fed52125ae4e1dc003.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">元宇宙/虚拟人</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                <span class="chip bg-color">元宇宙/虚拟人</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                元宇宙/虚拟人
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-11-更新"><a href="#2024-12-11-更新" class="headerlink" title="2024-12-11 更新"></a>2024-12-11 更新</h1><h2 id="MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting"><a href="#MixedGaussianAvatar-Realistically-and-Geometrically-Accurate-Head-Avatar-via-Mixed-2D-3D-Gaussian-Splatting" class="headerlink" title="MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting"></a>MixedGaussianAvatar: Realistically and Geometrically Accurate Head   Avatar via Mixed 2D-3D Gaussian Splatting</h2><p><strong>Authors:Peng Chen, Xiaobao Wei, Qingpo Wuwu, Xinyi Wang, Xingyu Xiao, Ming Lu</strong></p>
<p>Reconstructing high-fidelity 3D head avatars is crucial in various applications such as virtual reality. The pioneering methods reconstruct realistic head avatars with Neural Radiance Fields (NeRF), which have been limited by training and rendering speed. Recent methods based on 3D Gaussian Splatting (3DGS) significantly improve the efficiency of training and rendering. However, the surface inconsistency of 3DGS results in subpar geometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy at the expense of rendering fidelity. To leverage the benefits of both 2DGS and 3DGS, we propose a novel method named MixedGaussianAvatar for realistically and geometrically accurate head avatar reconstruction. Our main idea is to utilize 2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric accuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model and connect additional 3D Gaussians to those 2D Gaussians where the rendering quality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation. These 2D-3D Gaussians can then be animated using FLAME parameters. We further introduce a progressive training strategy that first trains the 2D Gaussians and then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority of MixedGaussianAvatar through comprehensive experiments. The code will be released at: <a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>. </p>
<blockquote>
<p>重建高保真3D头像在虚拟现实等各种应用中至关重要。前沿方法使用神经辐射场（NeRF）重建逼真的头像，但受限于训练和渲染速度。基于3D高斯拼贴（3DGS）的近期方法显著提高了训练和渲染的效率。然而，3DGS的表面不一致导致几何精度不佳；后来的2DGS使用2D表面元素以提高几何精度，但牺牲了渲染保真度。为了结合2DGS和3DGS的优点，我们提出了一种名为MixedGaussianAvatar的新方法，用于进行真实且几何准确的头像重建。我们的主要思想是使用2D高斯重建3D头像的表面，以确保几何精度。我们将2D高斯附加到FLAME模型的三角网格上，并在2DGS的渲染质量不足的地方连接到额外的3D高斯，创建混合的2D-3D高斯表示。这些2D-3D高斯可以使用FLAME参数进行动画设置。我们还引入了一种逐步训练策略，首先训练2D高斯，然后对混合的2D-3D高斯进行微调。我们通过全面的实验证明了MixedGaussianAvatar的优势。代码将在<a target="_blank" rel="noopener" href="https://github.com/ChenVoid/MGA/">https://github.com/ChenVoid/MGA/</a>发布。</p>
</blockquote>
<p><strong>简化解释</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04955v1">PDF</a> Project: <a target="_blank" rel="noopener" href="https://chenvoid.github.io/MGA/">https://chenvoid.github.io/MGA/</a></p>
<p><strong>Summary</strong></p>
<p>该文本介绍了在虚拟现实中重建高保真3D头像的重要性，以及采用神经网络辐射场（NeRF）和基于三维高斯展开（3DGS）的方法在重建真实头像方面的进展。然而，这些方法存在训练与渲染速度的限制以及几何精度问题。为此，提出了一种名为MixedGaussianAvatar的新方法，利用二维高斯重建三维头部的表面，保证了几何精度，同时引入了混合的二维-三维高斯表示，并可以通过FLAME参数进行动画渲染。通过渐进式训练策略进行模型训练和优化。最终通过实验证明了MixedGaussianAvatar的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建高保真3D头像在虚拟现实等应用中至关重要。</li>
<li>当前方法如NeRF和3DGS在重建真实头像方面存在局限性，如训练与渲染速度、几何精度问题。</li>
<li>MixedGaussianAvatar方法结合了2DGS和3DGS的优点，旨在实现真实且几何准确的头像重建。</li>
<li>该方法利用二维高斯重建三维头部表面，同时引入混合的二维-三维高斯表示。</li>
<li>通过连接FLAME模型的三角网格和附加的三维高斯，提高了渲染质量。</li>
<li>采用渐进式训练策略进行模型训练和优化。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-34f42332d014b46369069fd2d1d3a994.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0ef5787956810f1e111d21adf0bdcf5c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ce4f964cf25207a6db5a28f7f85bd755.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a7e93cc4f1cccfe010d043da886dc390.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5d0573a7ab1441e50500057923302b87.jpg" align="middle">
</details>




<h2 id="PBDyG-Position-Based-Dynamic-Gaussians-for-Motion-Aware-Clothed-Human-Avatars"><a href="#PBDyG-Position-Based-Dynamic-Gaussians-for-Motion-Aware-Clothed-Human-Avatars" class="headerlink" title="PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human   Avatars"></a>PBDyG: Position Based Dynamic Gaussians for Motion-Aware Clothed Human   Avatars</h2><p><strong>Authors:Shota Sasaki, Jane Wu, Ko Nishino</strong></p>
<p>This paper introduces a novel clothed human model that can be learned from multiview RGB videos, with a particular emphasis on recovering physically accurate body and cloth movements. Our method, Position Based Dynamic Gaussians (PBDyG), realizes <code>movement-dependent&#39;&#39; cloth deformation via physical simulation, rather than merely relying on </code>pose-dependent’’ rigid transformations. We model the clothed human holistically but with two distinct physical entities in contact: clothing modeled as 3D Gaussians, which are attached to a skinned SMPL body that follows the movement of the person in the input videos. The articulation of the SMPL body also drives physically-based simulation of the clothes’ Gaussians to transform the avatar to novel poses. In order to run position based dynamics simulation, physical properties including mass and material stiffness are estimated from the RGB videos through Dynamic 3D Gaussian Splatting. Experiments demonstrate that our method not only accurately reproduces appearance but also enables the reconstruction of avatars wearing highly deformable garments, such as skirts or coats, which have been challenging to reconstruct using existing methods. </p>
<blockquote>
<p>本文介绍了一种可以从多视角RGB视频中学习的新型穿衣人体模型，特别侧重于恢复物理上准确的身体和衣物运动。我们的方法，基于位置动态高斯（PBDyG），通过物理模拟实现“运动相关”的衣物变形，而不是仅仅依赖“姿态相关”的刚性变换。我们对穿衣人体进行整体建模，但接触的两个物理实体是独特的：将衣物建模为三维高斯，附着在跟随输入视频人物动作的带皮肤SMPL身体上。SMPL身体的关节活动还驱动衣物高斯基于物理的模拟，将角色变形为新的姿势。为了运行基于位置的动态模拟，物理特性包括质量和材料刚度是通过动态三维高斯喷绘从RGB视频中估计出来的。实验表明，我们的方法不仅准确地再现了外观，还实现了对穿着高度可变形服装的角色重建，如裙子或外套，这在以前的方法中一直是一个挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04433v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型穿衣人体模型，该模型可从多角度RGB视频中学习，并特别重视恢复物理准确的身体和衣物动作。通过基于位置的动态高斯方法，实现了通过物理模拟的“动作相关”衣物变形，而不是仅依赖于“姿势相关”的刚性变换。该模型将穿衣的人体视为一个整体，但分为两个接触的物理实体：衣物被视为三维高斯分布，附着在随输入视频人物动作而变化的皮肤化SMPL身体上。SMPL身体的关节活动也驱动衣物的基于物理的高斯模拟，将角色转换为新的姿势。为了运行基于位置的动态模拟，通过动态三维高斯模板从RGB视频中估计质量、材料刚度等物理属性。实验表明，该方法不仅准确还原外观，还能重建穿着高度可变形服装的角色，如裙子或外套，这在现有方法中是一个挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了一种新的穿衣人体模型学习方法，可从多角度RGB视频中学习并建立物理准确的动作模型。</li>
<li>利用基于位置的动态高斯（PBDyG）方法实现物理模拟的衣物变形。</li>
<li>将穿衣的人体视为整体，分为两个接触的物理实体：衣物和皮肤化SMPL身体。</li>
<li>SMPL身体的关节活动驱动衣物的物理模拟。</li>
<li>通过动态三维高斯模板从RGB视频中估计物理属性，如质量和材料刚度。</li>
<li>方法能准确还原外观并重建高度可变形服装的角色。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2d1711cfeb4aa545688bd82288fe4ba5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-325bb7409947b2356cc510d3fabf325b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-082105f475afd440dabb10a54eb43e99.jpg" align="middle">
</details>




<h2 id="A-multidimensional-measurement-of-photorealistic-avatar-quality-of-experience"><a href="#A-multidimensional-measurement-of-photorealistic-avatar-quality-of-experience" class="headerlink" title="A multidimensional measurement of photorealistic avatar quality of   experience"></a>A multidimensional measurement of photorealistic avatar quality of   experience</h2><p><strong>Authors:Ross Cutler, Babak Naderi, Vishak Gopal, Dharmendar Palle</strong></p>
<p>Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. We show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. This means that avatars that are not as realistic as real video will have lower trust, comfortableness using, comfortableness interacting with, appropriateness for work, formality, and affinity, and higher creepiness compared to real video. In addition, because there is a strong linear relationship between avatar affinity and realism, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We provide several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/P.910">https://github.com/microsoft/P.910</a>. </p>
<blockquote>
<p>超写实虚拟人是看起来像、移动和说话都像真实人类的虚拟人。基于PSNR、SSIM、LPIPS、FID和FVD等客观指标，超写实虚拟人的性能最近得到了显著提高。然而，最近的超写实虚拟人出版物并没有提供主观测试，以测量虚拟人的可用性因子。我们提供了一个开源测试框架，主观地测量超写实虚拟人在十个维度上的性能：真实性、信任度、使用舒适度、交互舒适度、工作适宜性、怪异感、正式程度、亲和力、与人的相似性以及情绪准确性。我们展示这些主观指标中的九个与PSNR、SSIM、LPIPS、FID和FVD的关联很微弱，情绪准确度的关联则适中。与专家小组相比，众包主观测试框架高度可复制且准确。我们分析了从超写实到卡通般的各种虚拟人，并显示一些超写实虚拟人在这些维度上已接近真实视频的性能。我们还发现，对于达到一定现实水平的虚拟人，这八个测量维度之间存在强烈关联。这意味着与现实视频相比，不那么真实的虚拟人将在信任度、使用舒适度、交互舒适度、工作适宜性、正式程度和亲和力方面较低，并且会有更高的怪异感。此外，由于虚拟人亲和力和现实感之间存在强烈的线性关系，因此在电信场景中，超写实虚拟人不会出现诡异谷效应。我们为这个测试框架提供了几个未来工作的扩展，并讨论了电信系统设计的影响。测试框架可在<a target="_blank" rel="noopener" href="https://github.com/microsoft/P.910">https://github.com/microsoft/P.910</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09066v2">PDF</a> arXiv admin note: text overlap with arXiv:2204.06784</p>
<p><strong>Summary</strong><br>     该文本介绍了逼真虚拟人的概念及其性能评估。虽然客观指标如PSNR、SSIM等有所提升，但主观测试对于衡量虚拟人在人机交互中的可用性至关重要。因此，文章提供了一个开源测试框架来主观测量虚拟人在十个维度上的性能，并发现某些高度逼真的虚拟人在这些维度上的表现已接近真实视频。此外，对于达到一定真实水平的虚拟人，八个测量维度之间存在强烈的相关性。最后，文章讨论了该测试框架的扩展和未来工作设计的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>逼真虚拟人的定义和最新进展。</li>
<li>文章强调了主观测试在评估虚拟人性能中的重要性，因为客观指标无法全面反映人类在使用和交互过程中的感受。</li>
<li>文章提供了一个开源测试框架，用于主观测量虚拟人在多个维度（如真实感、信任度、舒适性等）的性能。</li>
<li>高度逼真的虚拟人在这些维度上的表现已接近真实视频。</li>
<li>对于达到一定真实水平的虚拟人，多个测量维度之间存在强烈的相关性。</li>
<li>文章讨论了测试框架的扩展性以及对未来电信系统设计的潜在影响。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ba1148f524c897fed52125ae4e1dc003.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2598fc35edfa3777fff0b4a86bbb508c.jpg" align="middle">
</details>




<h2 id="Topology-aware-Human-Avatars-with-Semantically-guided-Gaussian-Splatting"><a href="#Topology-aware-Human-Avatars-with-Semantically-guided-Gaussian-Splatting" class="headerlink" title="Topology-aware Human Avatars with Semantically-guided Gaussian Splatting"></a>Topology-aware Human Avatars with Semantically-guided Gaussian Splatting</h2><p><strong>Authors:Haoyu Zhao, Chen Yang, Hao Wang, Xingyue Zhao, Wei Shen</strong></p>
<p>Reconstructing photo-realistic and topology-aware animatable human avatars from monocular videos remains challenging in computer vision and graphics. Recently, methods using 3D Gaussians to represent the human body have emerged, offering faster optimization and real-time rendering. However, due to ignoring the crucial role of human body semantic information which represents the explicit topological and intrinsic structure within human body, they fail to achieve fine-detail reconstruction of human avatars. To address this issue, we propose SG-GS, which uses semantics-embedded 3D Gaussians, skeleton-driven rigid deformation, and non-rigid cloth dynamics deformation to create photo-realistic human avatars. We then design a Semantic Human-Body Annotator (SHA) which utilizes SMPL’s semantic prior for efficient body part semantic labeling. The generated labels are used to guide the optimization of semantic attributes of Gaussian. To capture the explicit topological structure of the human body, we employ a 3D network that integrates both topological and geometric associations for human avatar deformation. We further implement three key strategies to enhance the semantic accuracy of 3D Gaussians and rendering quality: semantic projection with 2D regularization, semantic-guided density regularization and semantic-aware regularization with neighborhood consistency. Extensive experiments demonstrate that SG-GS achieves state-of-the-art geometry and appearance reconstruction performance. </p>
<blockquote>
<p>从单目视频中重建真实感且具备拓扑感知能力的可动画人类化身，在计算机视觉和图形学中仍然是一个挑战。近期，使用3D高斯表示人体的方法已经出现，它们提供了更快的优化和实时渲染。然而，由于忽略了人体语义信息在表示人体内部明确的拓扑结构和内在结构中的关键作用，这些方法无法实现人类化身的精细细节重建。为了解决这一问题，我们提出了SG-GS方法，它使用嵌入语义的3D高斯、骨架驱动的刚性变形和非刚性布料动态变形来创建真实感的人类化身。然后，我们设计了一个语义人体标注器（SHA），它利用SMPL的语义先验进行高效的身体部位语义标注。生成的标签用于引导高斯语义属性的优化。为了捕捉人体的明确拓扑结构，我们采用了一个3D网络，该网络结合了拓扑和几何关联，用于人类化身的变形。我们还实施了三种关键策略，以提高3D高斯和渲染质量的语义准确性：带有二维正则化的语义投影、语义引导密度正则化和具有邻域一致性的语义感知正则化。大量实验表明，SG-GS在几何和外观重建方面达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09665v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种利用语义嵌入的三维高斯模型、骨架驱动的刚性变形和非刚性布料动力学变形技术，创建逼真人类虚拟形象的方法。为解决忽略人体语义信息导致的精细重建问题，提出了SG-GS方法，并结合语义人体标注器（SHA）和一系列优化策略，实现了先进的人形几何和外观重建效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>重建基于单目视频的光学逼真且具有拓扑感知的可动画人类虚拟形象在计算机视觉和图形学中仍然具有挑战性。</li>
<li>现有使用三维高斯模型的方法虽然能加快优化和实时渲染，但忽略了人体语义信息的重要性，无法实现精细重建。</li>
<li>SG-GS方法利用语义嵌入的三维高斯模型、骨架驱动的刚性变形和非刚性布料动力学变形技术，以创建逼真的虚拟人类形象。</li>
<li>SHA（语义人体标注器）利用SMPL的语义先验进行高效的身体部位语义标注，用于指导高斯语义属性的优化。</li>
<li>通过结合拓扑和几何关联的三维网络，捕捉人体的显式拓扑结构，实现人类虚拟形象的变形。</li>
<li>实施三个关键策略以提高三维高斯模型的语义准确性和渲染质量，包括语义投影与二维正则化、语义引导密度正则化和语义感知的邻域一致性正则化。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6ff77e21bec81067b0e2966ed6634bd6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fdb456e5809bd0fa45ea185f6d20687a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-93be2bb8d66c02543723f8dae3fae9b4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-fa45cbf4b6d66afe5756817c7c32afc9.jpg" align="middle">
</details>




<h2 id="3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning"><a href="#3D-Consistent-Human-Avatars-with-Sparse-Inputs-via-Gaussian-Splatting-and-Contrastive-Learning" class="headerlink" title="3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting   and Contrastive Learning"></a>3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting   and Contrastive Learning</h2><p><strong>Authors:Haoyu Zhao, Hao Wang, Chen Yang, Wei Shen</strong></p>
<p>Existing approaches for human avatar generation–both NeRF-based and 3D Gaussian Splatting (3DGS) based–struggle with maintaining 3D consistency and exhibit degraded detail reconstruction, particularly when training with sparse inputs. To address this challenge, we propose CHASE, a novel framework that achieves dense-input-level performance using only sparse inputs through two key innovations: cross-pose intrinsic 3D consistency supervision and 3D geometry contrastive learning. Building upon prior skeleton-driven approaches that combine rigid deformation with non-rigid cloth dynamics, we first establish baseline avatars with fundamental 3D consistency. To enhance 3D consistency under sparse inputs, we introduce a Dynamic Avatar Adjustment (DAA) module, which refines deformed Gaussians by leveraging similar poses from the training set. By minimizing the rendering discrepancy between adjusted Gaussians and reference poses, DAA provides additional supervision for avatar reconstruction. We further maintain global 3D consistency through a novel geometry-aware contrastive learning strategy. While designed for sparse inputs, CHASE surpasses state-of-the-art methods across both full and sparse settings on ZJU-MoCap and H36M datasets, demonstrating that our enhanced 3D consistency leads to superior rendering quality. </p>
<blockquote>
<p>当前的人形化身生成方法，无论是基于NeRF的方法还是基于3D高斯平板印刷（3DGS）的方法，在保持3D一致性方面都面临困难，并且在细节重建方面表现出退化，特别是在使用稀疏输入进行训练时更为明显。为了应对这一挑战，我们提出了CHASE，这是一个新型框架，仅通过稀疏输入就能实现密集输入级别的性能，主要通过两个关键创新点：跨姿势内在3D一致性监督和3D几何对比学习。我们建立在先前的骨架驱动方法的基础上，结合刚体变形和非刚性布料动力学，首先建立具有基本3D一致性的基准化身。为了提高稀疏输入下的3D一致性，我们引入了动态化身调整（DAA）模块，该模块通过利用训练集中的相似姿势来优化变形后的高斯函数。通过最小化调整后的高斯函数与参考姿势之间的渲染差异，DAA为化身重建提供了额外的监督。我们还通过一种新型几何感知对比学习策略来保持全局3D一致性。虽然是为稀疏输入而设计的，但CHASE在ZJU-MoCap和H36M数据集上的全面和稀疏设置方面都超越了最先进的方法，表明我们增强的3D一致性导致了更高的渲染质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09663v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型框架CHASE，用于解决人类角色生成的问题。该框架能够在稀疏输入下实现密集输入级别的性能，主要通过两个关键创新点：跨姿态的3D一致性监督和3D几何对比学习。通过引入动态角色调整模块，增强稀疏输入下的3D一致性，并通过几何感知对比学习策略维持全局3D一致性。在ZJU-MoCap和H36M数据集上，CHASE在稀疏和完整输入场景下均超越现有方法，展现出卓越的三维渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CHASE框架解决了现有方法在生成人类角色时的挑战，实现了密集输入级别的性能。</li>
<li>CHASE通过跨姿态的3D一致性监督和3D几何对比学习两个关键创新点实现高性能。</li>
<li>动态角色调整模块增强了稀疏输入下的渲染性能，减少重建差异并提升了性能表现。</li>
<li>动态角色调整模块采用对类似姿态的数据集进行训练，为重建提供了额外的监督。</li>
<li>CHASE框架维持全局的3D一致性，采用新颖的几何感知对比学习策略。</li>
<li>在多个数据集上，CHASE在稀疏和完整输入场景下均超越现有方法。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e3d818183426e4d07c1d657130bfb4e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-172ade8d51614a60d51ade7f48d5d7e7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c52b7469d9dca60a05047d6d38dfd9d8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-87a3da9c75038d560f603c2afb23bbb0.jpg" align="middle">
</details>




<h2 id="AniFaceDiff-Animating-Stylized-Avatars-via-Parametric-Conditioned-Diffusion-Models"><a href="#AniFaceDiff-Animating-Stylized-Avatars-via-Parametric-Conditioned-Diffusion-Models" class="headerlink" title="AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned   Diffusion Models"></a>AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned   Diffusion Models</h2><p><strong>Authors:Ken Chen, Sachith Seneviratne, Wei Wang, Dongting Hu, Sanjay Saha, Md. Tarek Hasan, Sanka Rasnayaka, Tamasha Malepathirana, Mingming Gong, Saman Halgamuge</strong></p>
<p>Animating stylized avatars with dynamic poses and expressions has attracted increasing attention for its broad range of applications. Previous research has made significant progress by training controllable generative models to synthesize animations based on reference characteristics, pose, and expression conditions. However, the mechanisms used in these methods to control pose and expression often inadvertently introduce unintended features from the target motion, while also causing a loss of expression-related details, particularly when applied to stylized animation. This paper proposes a new method based on Stable Diffusion, called AniFaceDiff, incorporating a new conditioning module for animating stylized avatars. First, we propose a refined spatial conditioning approach by Facial Alignment to prevent the inclusion of identity characteristics from the target motion. Then, we introduce an Expression Adapter that incorporates additional cross-attention layers to address the potential loss of expression-related information. Our approach effectively preserves pose and expression from the target video while maintaining input image consistency. Extensive experiments demonstrate that our method achieves state-of-the-art results, showcasing superior image quality, preservation of reference features, and expression accuracy, particularly for out-of-domain animation across diverse styles, highlighting its versatility and strong generalization capabilities. This work aims to enhance the quality of virtual stylized animation for positive applications. To promote responsible use in virtual environments, we contribute to the advancement of detection for generative content by evaluating state-of-the-art detectors, highlighting potential areas for improvement, and suggesting solutions. </p>
<blockquote>
<p>使用动态姿势和表情制作动画风格的化身已经因其广泛的应用领域而越来越受到关注。之前的研究通过训练可控生成模型来合成基于参考特征、姿势和表情条件的动画，已经取得了重大进展。然而，这些方法在控制姿势和表情的机制时，常常会无意中引入目标动作的额外特征，同时也会失去与表情相关的细节，特别是在应用于动画风格化时。本文提出了一种基于Stable Diffusion的新方法，称为AniFaceDiff，并融入了一个用于动画风格化化身的新型条件模块。首先，我们通过面部对齐提出了一种精细的空间条件方法，以防止目标运动中的身份特征被包含进来。然后，我们引入了一个表情适配器，它结合了额外的交叉注意力层，以解决可能丢失与表情相关的信息的问题。我们的方法有效地保留了目标视频中的姿势和表情，同时保持了输入图像的一致性。大量实验表明，我们的方法达到了最先进的水平，展现了卓越的图片质量、对参考特征的保留和表情的准确性，尤其是在不同风格的域外动画中，凸显了其通用性和强大的泛化能力。本工作的目标是提高虚拟风格化动画的质量，用于积极的应用。为了促进虚拟环境中负责任的使用，我们通过评估最先进的检测器，来推动生成内容的检测发展，突出潜在的可改进领域，并提出解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13272v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Stable Diffusion的新方法AniFaceDiff，应用于动态调整个性化动画人物形象的姿态和表情。通过面部对齐技术改进空间调节方式，防止目标运动的身份特征被引入，同时引入表情适配器，通过额外的交叉注意力层解决表情相关信息可能丢失的问题。该方法能很好地保留目标视频的姿态和表情，同时保持输入图像的一致性。实验结果达到领先水平，表现出优越的图像质量、对参考特征的保留以及准确的表情表达，特别是适用于跨不同风格的动画创作，展现了其通用性和强大的泛化能力。该工作旨在提升虚拟个性化动画的质量，并对虚拟环境中的负责任使用做出贡献。同时评估了现有的检测生成内容的技术，指出潜在的改进领域并给出解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>利用Stable Diffusion的AniFaceDiff方法提升个性化动画人物形象的姿态和表情控制效果。</li>
<li>通过面部对齐技术避免目标运动身份特征的干扰。</li>
<li>采用新设计的表情适配器以改善表情相关信息的保留情况。</li>
<li>方法保留了目标视频的姿态和表情，同时维持输入图像的一致性。</li>
<li>实验结果领先，图像质量高、保留参考特征、准确表达表情，尤其适用于跨风格动画创作。</li>
<li>方法展现出强大的通用性和泛化能力。</li>
<li>对虚拟个性化动画质量的提升做出贡献，并关注虚拟环境中的负责任使用。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-5e06353e6f9171d1ad537a75cdfe4fb6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-256ed1f5cc1de0cf023ed84d1a0cf1cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-97626fb58568884ecbf48c451dad9de3.jpg" align="middle">
</details>




<h2 id="A-philosophical-and-ontological-perspective-on-Artificial-General-Intelligence-and-the-Metaverse"><a href="#A-philosophical-and-ontological-perspective-on-Artificial-General-Intelligence-and-the-Metaverse" class="headerlink" title="A philosophical and ontological perspective on Artificial General   Intelligence and the Metaverse"></a>A philosophical and ontological perspective on Artificial General   Intelligence and the Metaverse</h2><p><strong>Authors:Martin Schmalzried</strong></p>
<p>This paper leverages various philosophical and ontological frameworks to explore the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. Several theoretical frameworks underpin this exploration, such as embodied cognition, Michael Levin’s computational boundary of a “Self,” Donald D. Hoffman’s Interface Theory of Perception, and Bernardo Kastrup’s analytical idealism, which lead to considering our perceived outer reality as a symbolic representation of alternate inner states of being, and where AGI could embody a different form of consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a feedback loop between AGI and human users in metaverse spaces as a tool for AGI calibration, as well as the role of local homeostasis and decentralized governance as preconditions for achieving a stable embodied AGI. The paper concludes by emphasizing the importance of achieving a certain degree of harmony in human relations and recognizing the interconnectedness of humanity at a global level, as key prerequisites for the emergence of a stable embodied AGI. </p>
<blockquote>
<p>本文利用多种哲学和本体论框架，探讨具身通用人工智能（AGI）的概念、其与人类意识的关系，以及元宇宙在促进这种关系中的关键作用。本文的探讨基于多个理论框架，如具身认知、迈克尔·莱文的“自我”计算边界、唐纳德·霍夫曼的界面感知理论以及贝尔纳多·卡斯特鲁的分析理想主义，这些理论引导我们考虑我们所感知的外部现实是存在不同内在状态的象征性表现，而AGI可能体现了一种具有更大计算边界的不同形式的意识。论文还进一步讨论了AGI的发展阶段、具身AGI出现的要求、为AGI提供校准符号界面的重要性，以及元宇宙、去中心化系统、开源区块链技术以及开源人工智能研究的关键作用。此外，它还探索了元空间内AGI和人类用户之间的反馈循环作为AGI校准工具的理念，以及实现稳定具身AGI的本地稳态和去中心化治理的先决条件。本文最后强调，实现一定程度的和谐人际关系和认识到人类在全球层面的相互关联性，是实现稳定具身AGI的关键先决条件。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06660v3">PDF</a> Presented at the conference second international conference on   human-centred AI ethics: seeing the human in the artificial (HCAIE 2023):   <a target="_blank" rel="noopener" href="https://ethics-ai.eu/hcaie2023/">https://ethics-ai.eu/hcaie2023/</a></p>
<p><strong>Summary</strong>：<br>此论文探讨实体化人工智能一般智能（AGI）的概念及其在元宇宙中的角色。它利用多种哲学和本体论框架探讨AGI与人类意识的关联，提出计算边界与自我意识、感知接口理论及虚拟现实的重要性等观点。强调开发阶段的需求和象征性界面的校准。文章探讨AGI和用户的元宇宙空间的反馈循环及其在AI校准中的价值。</p>
<p><strong>Key Takeaways</strong>：</p>
<p>以下是该文本的关键见解：</p>
<ul>
<li>论文探讨了实体化人工智能一般智能（AGI）的概念及其在元宇宙中的角色。</li>
<li>通过哲学和本体论框架探索AGI与人类意识的关联。</li>
<li>计算边界、自我意识与感知接口理论在探讨AGI时非常重要。</li>
<li>AGI发展需要达到特定的成熟度阶段，并需要校准象征性界面。</li>
<li>元宇宙和开放源代码区块链技术对于实现实体化AGI至关重要。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-36597c1cc685a5a862f5f7c1ed93ebb4.jpg" align="middle">
</details>




<h2 id="Deepfake-for-the-Good-Generating-Avatars-through-Face-Swapping-with-Implicit-Deepfake-Generation"><a href="#Deepfake-for-the-Good-Generating-Avatars-through-Face-Swapping-with-Implicit-Deepfake-Generation" class="headerlink" title="Deepfake for the Good: Generating Avatars through Face-Swapping with   Implicit Deepfake Generation"></a>Deepfake for the Good: Generating Avatars through Face-Swapping with   Implicit Deepfake Generation</h2><p><strong>Authors:Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</strong></p>
<p>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the object’s shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the object’s characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Deepfakes refers to artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such simple strategies can produce plausible 3D deepfake-based avatars. </p>
<blockquote>
<p>大量新兴的深度学习技术已经对计算机图形产生了重大影响。其中最有前途的突破之一是神经辐射场（NeRF）和高斯拼贴（GS）的兴起。NeRF使用神经网络权重编码物体的形状和颜色，利用少量已知相机位置的图片来生成新视角。相比之下，GS通过在一组高斯分布中编码物体特性，从而实现了加速训练和推理，同时不降低渲染质量。这两种技术在空间计算和其他领域找到了许多用例。另一方面，深度伪造方法的出现引起了相当大的争议。深度伪造是指使用人工智能生成的视频，这些视频紧密模仿真实镜头。使用生成模型，它们可以修改面部特征，能够创建具有惊人逼真度的身份或表情改变，看起来就像一个真实的人。尽管存在这些争议，但深度伪造在质量足够高的情况下，可以为化身创建和游戏提供下一代解决方案。为此，我们展示了如何结合所有这些新兴技术来获得更可信的结果。我们的ImplicitDeepfake使用经典的深度伪造算法分别修改所有训练图像，然后在修改后的面部上训练NeRF和GS。这种简单的策略可以产生基于深度伪造的逼真3D化身。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.06390v2">PDF</a> </p>
<p><strong>Summary</strong><br>新一代深度学习技术如Neural Radiance Fields（NeRF）和Gaussian Splatting（GS）对计算机图形产生了重大影响。它们被广泛用于空间计算等领域。同时，深度伪造技术的出现引发了争议。深度伪造技术可以生成逼真视频，但也可以被用于创建虚假身份或表情。研究人员尝试结合这些新兴技术，如使用经典深度伪造算法修改训练图像，然后利用NeRF和GS在修改后的面部上训练，以产生更逼真的三维深度伪造虚拟人。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>深度学习技术对计算机图形产生了重大影响，特别是Neural Radiance Fields（NeRF）和Gaussian Splatting（GS）的突破。</li>
<li>NeRF使用神经网络权重编码物体形状和颜色，能够从已知相机位置的少量图像生成新视角。</li>
<li>GS通过高斯分布集合编码物体特性，加速训练和推理，同时不降低渲染质量。</li>
<li>深度伪造技术引发了争议，但可以用于创建高质量的虚拟人和游戏角色。</li>
<li>结合深度学习技术和深度伪造算法可以产生更逼真的三维深度伪造虚拟人。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3aa0c5bd5d55fab6b3456408bcfa7ba8.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7c6dd62daeb59781f75d66815146fbb5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-59bdfd70e3985fd56be81d718f9a9c8e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a0eb9d0ad2fd4b43afa5f960c84523bf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9e6dceac7784672ecd9f51fb468ecff5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89629fe07055252ace8bfce59c0bd9b0.jpg" align="middle">
</details>




<h2 id="HHAvatar-Gaussian-Head-Avatar-with-Dynamic-Hairs"><a href="#HHAvatar-Gaussian-Head-Avatar-with-Dynamic-Hairs" class="headerlink" title="HHAvatar: Gaussian Head Avatar with Dynamic Hairs"></a>HHAvatar: Gaussian Head Avatar with Dynamic Hairs</h2><p><strong>Authors:Zhanfeng Liao, Yuelang Xu, Zhe Li, Qijing Li, Boyao Zhou, Ruifeng Bai, Di Xu, Hongwen Zhang, Yebin Liu</strong></p>
<p>Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head </p>
<blockquote>
<p>创建高保真3D头部化身一直是研究的热点，但在轻量级稀疏视图设置下，这仍然是一个巨大的挑战。在本文中，我们提出了HHAvatar，采用可控的3D高斯表示高保真头部化身，具有动态头发建模。我们首先使用3D高斯来表示头部的外观，然后通过联合优化中性3D高斯和完全学习的MLP基变形场来捕捉复杂的表情。这两部分相互受益，因此我们的方法可以在保证表情准确性的同时，对细微的动态细节进行建模。此外，我们基于隐式SDF和深度四面体网格设计了一种良好的几何引导初始化策略，以提高训练过程的稳定性和收敛性。为了解决动态头发建模的问题，我们将混合头部模型引入我们的化身表示基于高斯头部化身，并开发了一种考虑时间信息和遮挡感知模块的训练方法，以模拟头发的非刚性运动。实验表明，我们的方法在稀疏视图设置下优于其他最新技术，即使在夸张表情下也能达到超高的渲染质量（2K分辨率），并且能够合理驱动头发随着头部的运动而运动。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03029v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://liaozhanfeng.github.io/HHAvatar">https://liaozhanfeng.github.io/HHAvatar</a></p>
<p><strong>Summary</strong><br>基于可控的3D高斯分布的HHAvatar方法被提出，用于创建高保真度的头部虚拟人，具有动态头发建模。该方法使用中性高斯分布和基于MLP的变形场联合优化，以捕捉复杂的表情。此外，还设计了一种基于隐式SDF和Deep Marching Tetrahedra的几何引导初始化策略，以提高训练过程的稳定性和收敛性。针对动态头发建模问题，引入了混合头部模型和考虑时序信息的训练方法，以及遮挡感知模块来模拟头发的非刚性运动。该方法在稀疏视图下实现了超高保真度的渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了HHAvatar方法，使用可控的3D高斯分布创建高保真度的头部虚拟人。</li>
<li>通过中性高斯分布和基于MLP的变形场联合优化，捕捉复杂表情。</li>
<li>几何引导初始化策略提高了训练过程的稳定性和收敛性。</li>
<li>引入了混合头部模型和考虑时序信息的训练方法，以处理动态头发建模问题。</li>
<li>引入了遮挡感知模块来模拟头发的非刚性运动。</li>
<li>该方法在稀疏视图下实现了超高保真度的渲染质量，甚至在夸张表情下也能保持效果。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1e03c5fd26cb0fae72d9cb8044e27f6d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-5ca7dc70600b12c58c1eb6adf4366924.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-afcf7270946e8ceac83fe49e265e6607.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8446030a4bec80425558ebe681b11f37.jpg" align="middle">
</details>




<h2 id="VAST-Vivify-Your-Talking-Avatar-via-Zero-Shot-Expressive-Facial-Style-Transfer"><a href="#VAST-Vivify-Your-Talking-Avatar-via-Zero-Shot-Expressive-Facial-Style-Transfer" class="headerlink" title="VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style   Transfer"></a>VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style   Transfer</h2><p><strong>Authors:Liyang Chen, Zhiyong Wu, Runnan Li, Weihong Bao, Jun Ling, Xu Tan, Sheng Zhao</strong></p>
<p>Current talking face generation methods mainly focus on speech-lip synchronization. However, insufficient investigation on the facial talking style leads to a lifeless and monotonous avatar. Most previous works fail to imitate expressive styles from arbitrary video prompts and ensure the authenticity of the generated video. This paper proposes an unsupervised variational style transfer model (VAST) to vivify the neutral photo-realistic avatars. Our model consists of three key components: a style encoder that extracts facial style representations from the given video prompts; a hybrid facial expression decoder to model accurate speech-related movements; a variational style enhancer that enhances the style space to be highly expressive and meaningful. With our essential designs on facial style learning, our model is able to flexibly capture the expressive facial style from arbitrary video prompts and transfer it onto a personalized image renderer in a zero-shot manner. Experimental results demonstrate the proposed approach contributes to a more vivid talking avatar with higher authenticity and richer expressiveness. </p>
<blockquote>
<p>当前的人脸生成方法主要聚焦于语音与嘴唇的同步。然而，对于面部说话风格的调查不足导致了生成的虚拟形象缺乏生命力和单调性。之前的大部分工作都无法模仿来自任意视频提示的表达风格，也无法确保生成视频的真实性。本文提出了一种无监督的变风格转移模型（VAST），以赋予中性写实风格的虚拟形象生命力。我们的模型由三个关键部分组成：一个风格编码器，用于从给定的视频提示中提取面部风格表示；一个混合面部表情解码器，用于模拟准确的语音相关动作；一个变风格增强器，用于增强风格空间，使其具有高度表达力和意义。我们对面部风格学习进行了关键设计，使得模型能够灵活地捕捉来自任意视频提示的表达性面部风格，并将其以零样本的方式转移到个性化图像渲染器中。实验结果表明，所提出的方法有助于生成更加生动、更真实的说话虚拟形象，并具备更丰富的表现力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04830v3">PDF</a> Accepted by ICCV2023</p>
<p><strong>Summary</strong><br>文章提出了一个无监督的变分风格迁移模型（VAST），用于生动化中性逼真的头像。该模型包含三个关键组件：从给定视频提示中提取面部风格表示的风格编码器；对精确语音相关动作进行建模的混合面部表情解码器；增强风格空间以使其高度表达和意义丰富的变分风格增强器。该模型能够灵活地捕捉任意视频提示中的表达性面部风格，并将其零样本转移到个性化图像渲染器上。实验结果证明，该方法有助于提高说话头像的生动性、真实性和表达性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前面部生成方法主要关注语音-唇部同步，但对面部讲话风格的研究不足，导致生成的头像缺乏生命力和单调。</li>
<li>论文提出了一种无监督的变分风格迁移模型（VAST），旨在生动化中性逼真的头像。</li>
<li>VAST模型包含三个关键组件：风格编码器、混合面部表情解码器和变分风格增强器。</li>
<li>风格编码器能够从给定的视频提示中提取面部风格表示。</li>
<li>混合面部表情解码器能够准确建模语音相关的动作。</li>
<li>变分风格增强器能够增强风格空间，使生成的面部表情更丰富、有意义。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4e689f3d922179f5f39d7a2882859cb4.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2a819020fd662736bf40be61db67f133.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1a66b31dd4d8b90a061ab8f1b7532e83.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b019b9e44fa8336d68249df48a51c5f1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-73bf539f51040af219e1a9529abc6acf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-44add23e767fc59670cf6c5f95c16891.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                                    <span class="chip bg-color">元宇宙/虚拟人</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c66bfe10dfd878860a466abb92c19030.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-12-11  It Takes Two Real-time Co-Speech Two-person's Interaction Generation   via Reactive Auto-regressive Diffusion Model
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/GAN/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5237f15bcbbce4298b010ed16cb47cca.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2024-12-11  Take Fake as Real Realistic-like Robust Black-box Adversarial Attack to   Evade AIGC Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">7369.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
