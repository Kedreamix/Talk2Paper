<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Controllable Speech Synthesis in the Era of Large Language   Models A Survey">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    29.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    119 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. Besides, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this paper, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industry practitioners. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ï¼Œä¹Ÿç§°ä¸ºè¯­éŸ³åˆæˆï¼Œæ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆå¬èµ·æ¥å¾ˆè‡ªç„¶çš„äººç±»è¯­éŸ³çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚æœ€è¿‘ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ç»è¶…è¶Šäº†åˆæˆäººç±»è¯­éŸ³çš„èŒƒå›´ï¼Œå®ç°äº†å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™åŒ…æ‹¬åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…ç»ªã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¿‡å»çš„å‡ å¹´é‡Œæ˜¾è‘—å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ç­‰å¤šç§æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›å¯¹ç ”ç©¶ç°çŠ¶çš„æ¸…æ™°ç†è§£ã€‚æˆ‘ä»¬ç ”ç©¶äº†å¯æ§TTSçš„ä¸€èˆ¬æµç¨‹ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œæä¾›äº†ç°æœ‰æ–¹æ³•çš„å…¨é¢è€Œæ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†æ€»ç»“äº†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç¯‡ç»¼è¿°è®ºæ–‡å¯¹æ–°å…´çš„å¯æ§TTSæ–¹æ³•è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„å›é¡¾ï¼Œå¯¹å­¦æœ¯ç ”ç©¶äººå‘˜å’Œå·¥ä¸šä»ä¸šè€…éƒ½å¤§æœ‰è£¨ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v1">PDF</a> A comprehensive survey on controllable TTS, 23 pages, 6 tables, 4   figures, 280 references</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ä¹Ÿç§°ä¸ºè¯­éŸ³åˆæˆï¼Œæ˜¯ä¸€ä¸ªæ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆè‡ªç„¶å£°éŸ³çš„äººç±»è¯­éŸ³çš„ç ”ç©¶é¢†åŸŸã€‚éšç€å·¥ä¸šéœ€æ±‚çš„å¢é•¿ï¼ŒTTSæŠ€æœ¯å·²ç»è¶…è¶Šäº†åˆæˆç±»ä¼¼äººç±»è¯­éŸ³çš„é˜¶æ®µï¼Œå®ç°äº†å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™åŒ…æ‹¬å¯¹åˆæˆè¯­éŸ³çš„å„ç§å±æ€§è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ çš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚æœ¬æ–‡å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›å¯¹å½“å‰ç ”ç©¶çŠ¶æ€çš„æ¸…æ™°ç†è§£ã€‚æœ¬æ–‡è¯¦ç»†æ¢è®¨äº†å¯æ§TTSçš„é€šç”¨ç®¡é“ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œä¸ºç°æœ‰æ–¹æ³•æä¾›äº†æ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ€»ç»“äº†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¯¹å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥æ–¹å‘è¿›è¡Œäº†é˜è¿°ã€‚æœ¬æ–‡æ˜¯å¯¹æ–°å…´çš„å¯æ§TTSæ–¹æ³•çš„é¦–æ¬¡å…¨é¢ç»¼è¿°ï¼Œå¯¹å­¦æœ¯ç ”ç©¶äººå‘˜å’Œè¡Œä¸šä»ä¸šè€…éƒ½å…·æœ‰å‚è€ƒä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSæŠ€æœ¯å·²ç»ä»å•çº¯çš„æ¨¡ä»¿äººç±»è¯­éŸ³è¿›åŒ–åˆ°å¯æ§çš„è¯­éŸ³ç”Ÿæˆé˜¶æ®µã€‚</li>
<li>TTSèƒ½å¤Ÿå®ç°ç²¾ç»†æ§åˆ¶åˆæˆè¯­éŸ³çš„å„ç§å±æ€§ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚</li>
<li>æ·±åº¦å­¦ä¹ çš„è¿›æ­¥å¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹æ˜¾è‘—æå‡äº†å¯æ§TTSçš„æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†å„ç§æ–¹æ³•å¹¶æä¾›äº†æ¸…æ™°çš„åˆ†ç±»ã€‚</li>
<li>è®ºæ–‡è¯¦ç»†æ¢è®¨äº†å¯æ§TTSçš„é€šç”¨ç®¡é“ã€æŒ‘æˆ˜å’Œæ¨¡å‹æ¶æ„ã€‚</li>
<li>è®ºæ–‡æ€»ç»“äº†ç°æœ‰çš„æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œä¸ºç ”ç©¶è€…æä¾›äº†é‡è¦çš„å‚è€ƒä¿¡æ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a3255132bdca965fc85d69d43568f2f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d8fe56ee1b3d384d421df9dbf984646b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e826485c7661728072f0a00efebdb680.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-78d17f9628b8ef660154504c576be31e.jpg" align="middle">
</details>




<h2 id="Text-Is-Not-All-You-Need-Multimodal-Prompting-Helps-LLMs-Understand-Humor"><a href="#Text-Is-Not-All-You-Need-Multimodal-Prompting-Helps-LLMs-Understand-Humor" class="headerlink" title="Text Is Not All You Need: Multimodal Prompting Helps LLMs Understand   Humor"></a>Text Is Not All You Need: Multimodal Prompting Helps LLMs Understand   Humor</h2><p><strong>Authors:Ashwin Baluja</strong></p>
<p>While Large Language Models (LLMs) have demonstrated impressive natural language understanding capabilities across various text-based tasks, understanding humor has remained a persistent challenge. Humor is frequently multimodal, relying on phonetic ambiguity, rhythm and timing to convey meaning. In this study, we explore a simple multimodal prompting approach to humor understanding and explanation. We present an LLM with both the text and the spoken form of a joke, generated using an off-the-shelf text-to-speech (TTS) system. Using multimodal cues improves the explanations of humor compared to textual prompts across all tested datasets. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è‡ªç„¶è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œä½†ç†è§£å¹½é»˜ä»ç„¶æ˜¯ä¸€ä¸ªæŒä¹…çš„æŒ‘æˆ˜ã€‚å¹½é»˜é€šå¸¸æ˜¯å¤šæ¨¡å¼çš„ï¼Œä¾èµ–äºè¯­éŸ³çš„æ¨¡ç³Šæ€§ã€èŠ‚å¥å’Œæ—¶æœºæ¥ä¼ è¾¾æ„ä¹‰ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§ç®€å•çš„å¤šæ¨¡å¼æç¤ºæ–¹æ³•æ¥ç†è§£å¹¶è§£é‡Šå¹½é»˜ã€‚æˆ‘ä»¬å‘LLMå±•ç¤ºäº†ä¸€ä¸ªç¬‘è¯çš„æ–‡æœ¬å’Œè¯­éŸ³å½¢å¼ï¼Œè¯¥è¯­éŸ³å½¢å¼ä½¿ç”¨ç°æˆçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿç”Ÿæˆã€‚ä½¿ç”¨å¤šæ¨¡å¼çº¿ç´¢ä¸ä»…ä½¿ç”¨æ–‡æœ¬æç¤ºç›¸æ¯”ï¼Œåœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šéƒ½èƒ½æ›´å¥½åœ°è§£é‡Šå¹½é»˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05315v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å¹½é»˜æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„å¤šæ¨¡æ€æç¤ºæ–¹æ³•ã€‚é€šè¿‡å‘LLMæä¾›ç¬‘è¯çš„æ–‡æœ¬å’Œè¯­éŸ³å½¢å¼ï¼Œä½¿ç”¨å¤šæ¨¡æ€çº¿ç´¢å¯ä»¥æé«˜å¹½é»˜è§£é‡Šçš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å¹½é»˜æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¹½é»˜æ˜¯å¤šæ¨¡æ€çš„ï¼Œä¾èµ–äºè¯­éŸ³çš„éŸµå¾‹ã€èŠ‚å¥å’Œæ—¶æœºæ¥ä¼ è¾¾æ„ä¹‰ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ç®€å•çš„å¤šæ¨¡æ€æç¤ºæ–¹æ³•ï¼Œå°†æ–‡æœ¬å’Œè¯­éŸ³å½¢å¼çš„ç¬‘è¯å‘ˆç°ç»™LLMã€‚</li>
<li>ä½¿ç”¨å¤šæ¨¡æ€çº¿ç´¢æé«˜äº†åœ¨æ‰€æœ‰æµ‹è¯•æ•°æ®é›†ä¸Šçš„å¹½é»˜è§£é‡Šæ•ˆæœã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨äº†æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„ç”Ÿæˆè¯­éŸ³å½¢å¼ã€‚</li>
<li>é€šè¿‡ç»“åˆæ–‡æœ¬å’Œè¯­éŸ³ä¿¡æ¯ï¼ŒLLMèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œè§£é‡Šå¹½é»˜ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6e32f163daebfaff349844dd7e422c50.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-34d118ce5088eebd9522a6610a3e753b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2d06752067091bf19a682f99164fdc59.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b79293bdc8f2260ba4609edbd6c30e2d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-ed5d6b77985b73979ef7adb01b6de38d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b5e0659bcb7cbc434efb03e0aee44bed.jpg" align="middle">
</details>




<h2 id="DiffStyleTTS-Diffusion-based-Hierarchical-Prosody-Modeling-for-Text-to-Speech-with-Diverse-and-Controllable-Styles"><a href="#DiffStyleTTS-Diffusion-based-Hierarchical-Prosody-Modeling-for-Text-to-Speech-with-Diverse-and-Controllable-Styles" class="headerlink" title="DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for   Text-to-Speech with Diverse and Controllable Styles"></a>DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for   Text-to-Speech with Diverse and Controllable Styles</h2><p><strong>Authors:Jiaxuan Liu, Zhaoci Liu, Yajun Hu, Yingying Gao, Shilei Zhang, Zhenhua Ling</strong></p>
<p>Human speech exhibits rich and flexible prosodic variations. To address the one-to-many mapping problem from text to prosody in a reasonable and flexible manner, we propose DiffStyleTTS, a multi-speaker acoustic model based on a conditional diffusion module and an improved classifier-free guidance, which hierarchically models speech prosodic features, and controls different prosodic styles to guide prosody prediction. Experiments show that our method outperforms all baselines in naturalness and achieves superior synthesis speed compared to three diffusion-based baselines. Additionally, by adjusting the guiding scale, DiffStyleTTS effectively controls the guidance intensity of the synthetic prosody. </p>
<blockquote>
<p>äººç±»è¯­éŸ³å±•ç°å‡ºä¸°å¯Œçµæ´»çš„éŸµå¾‹å˜åŒ–ã€‚ä¸ºäº†è§£å†³æ–‡æœ¬åˆ°éŸµå¾‹çš„ä¸€å¯¹å¤šæ˜ å°„é—®é¢˜ï¼Œå¹¶å®ç°åˆç†çµæ´»çš„æ–¹å¼ï¼Œæˆ‘ä»¬æå‡ºäº†DiffStyleTTSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å—å’Œæ”¹è¿›çš„æ— åˆ†ç±»å™¨å¼•å¯¼çš„å¤šäººå£°éŸ³å­¦æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ†å±‚å»ºæ¨¡è¯­éŸ³éŸµå¾‹ç‰¹å¾ï¼Œå¹¶é€šè¿‡æ§åˆ¶ä¸åŒçš„éŸµå¾‹é£æ ¼æ¥å¼•å¯¼éŸµå¾‹é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªç„¶åº¦æ–¹é¢è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œä¸ä¸‰ç§åŸºäºæ‰©æ•£çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œåˆæˆé€Ÿåº¦æ›´ä¼˜è¶Šã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´å¼•å¯¼å°ºåº¦ï¼ŒDiffStyleTTSå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶åˆæˆéŸµå¾‹çš„å¼•å¯¼å¼ºåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03388v1">PDF</a> COLING 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸­æå‡ºäº†DiffStyleTTSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å—å’Œæ”¹è¿›çš„æ— åˆ†ç±»å™¨å¼•å¯¼çš„å¤šè¯´è¯è€…å£°å­¦æ¨¡å‹ã€‚å®ƒé€šè¿‡åˆ†å±‚å»ºæ¨¡è¯­éŸ³éŸµå¾‹ç‰¹å¾å’Œæ§åˆ¶ä¸åŒçš„éŸµå¾‹é£æ ¼æ¥æŒ‡å¯¼éŸµå¾‹é¢„æµ‹ï¼Œè§£å†³äº†æ–‡æœ¬åˆ°è¯­éŸ³çš„çµæ´»å¤šå˜é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªç„¶æ€§æ–¹é¢ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨åˆæˆé€Ÿåº¦æ–¹é¢è¶…è¿‡äº†ä¸‰ä¸ªåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡è°ƒæ•´å¼•å¯¼å°ºåº¦ï¼ŒDiffStyleTTSå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶åˆæˆéŸµå¾‹çš„å¼•å¯¼å¼ºåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DiffStyleTTSæ˜¯ä¸€ç§å¤šè¯´è¯è€…å£°å­¦æ¨¡å‹ï¼ŒåŸºäºæ¡ä»¶æ‰©æ•£æ¨¡å—å’Œæ”¹è¿›çš„æ— åˆ†ç±»å™¨å¼•å¯¼æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡åˆ†å±‚å»ºæ¨¡è¯­éŸ³éŸµå¾‹ç‰¹å¾ï¼Œè§£å†³æ–‡æœ¬åˆ°è¯­éŸ³çš„çµæ´»å¤šå˜é—®é¢˜ã€‚</li>
<li>DiffStyleTTSèƒ½æ§åˆ¶ä¸åŒçš„éŸµå¾‹é£æ ¼ï¼Œä»¥æŒ‡å¯¼éŸµå¾‹é¢„æµ‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiffStyleTTSåœ¨è‡ªç„¶æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œåˆæˆé€Ÿåº¦ä¹Ÿæ›´å¿«ã€‚</li>
<li>é€šè¿‡è°ƒæ•´å¼•å¯¼å°ºåº¦ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ§åˆ¶åˆæˆéŸµå¾‹çš„å¼•å¯¼å¼ºåº¦ã€‚</li>
<li>è¯¥æ¨¡å‹çš„åº”ç”¨åœºæ™¯å¯èƒ½åŒ…æ‹¬è¯­éŸ³åˆæˆã€æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€è¯­éŸ³äº¤äº’ç­‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-49242e69a1833cedecad1ab0cf4abe3f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e50155feccc65370d03420980916132a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-3d26aded4f628a2880e7fe859c5fbe67.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-e435f24c06d4e4ce130769dc21d42dab.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f8fc7555b4f9cdee9da78b44e722af2f.jpg" align="middle">
</details>




<h2 id="Analytic-Study-of-Text-Free-Speech-Synthesis-for-Raw-Audio-using-a-Self-Supervised-Learning-Model"><a href="#Analytic-Study-of-Text-Free-Speech-Synthesis-for-Raw-Audio-using-a-Self-Supervised-Learning-Model" class="headerlink" title="Analytic Study of Text-Free Speech Synthesis for Raw Audio using a   Self-Supervised Learning Model"></a>Analytic Study of Text-Free Speech Synthesis for Raw Audio using a   Self-Supervised Learning Model</h2><p><strong>Authors:Joonyong Park, Daisuke Saito, Nobuaki Minematsu</strong></p>
<p>We examine the text-free speech representations of raw audio obtained from a self-supervised learning (SSL) model by analyzing the synthesized speech using the SSL representations instead of conventional text representations. Since raw audio does not have paired speech representations as transcribed texts do, obtaining speech representations from unpaired speech is crucial for augmenting available datasets for speech synthesis. Specifically, the proposed speech synthesis is conducted using discrete symbol representations from the SSL model in comparison with text representations, and analytical examinations of the synthesized speech have been carried out. The results empirically show that using text representations is advantageous for preserving semantic information, while using discrete symbol representations is superior for preserving acoustic content, including prosodic and intonational information. </p>
<blockquote>
<p>æˆ‘ä»¬é€šè¿‡åˆ†æä½¿ç”¨SSLè¡¨ç¤ºæ³•åˆæˆçš„è¯­éŸ³ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ–‡æœ¬è¡¨ç¤ºæ³•ï¼Œæ¥æ£€æŸ¥ä»è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¨¡å‹ä¸­è·å¾—çš„åŸå§‹éŸ³é¢‘çš„éæ–‡æœ¬è¯­éŸ³è¡¨ç¤ºã€‚ç”±äºåŸå§‹éŸ³é¢‘æ²¡æœ‰åƒè½¬å½•æ–‡æœ¬é‚£æ ·çš„é…å¯¹è¯­éŸ³è¡¨ç¤ºï¼Œå› æ­¤ä»éé…å¯¹è¯­éŸ³ä¸­è·å¾—è¯­éŸ³è¡¨ç¤ºå¯¹äºå¢å¼ºç°æœ‰è¯­éŸ³åˆæˆæ•°æ®é›†è‡³å…³é‡è¦ã€‚å…·ä½“è€Œè¨€ï¼Œä¸æ–‡æœ¬è¡¨ç¤ºç›¸æ¯”ï¼Œæ‰€æå‡ºçš„è¯­éŸ³åˆæˆæ˜¯ä½¿ç”¨SSLæ¨¡å‹çš„ç¦»æ•£ç¬¦å·è¡¨ç¤ºæ¥å®Œæˆçš„ï¼Œå¹¶å¯¹åˆæˆçš„è¯­éŸ³è¿›è¡Œäº†åˆ†ææ£€æŸ¥ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºæ³•æœ‰åˆ©äºä¿ç•™è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œä½¿ç”¨ç¦»æ•£ç¬¦å·è¡¨ç¤ºæ³•åˆ™åœ¨ä¿ç•™å£°å­¦å†…å®¹ï¼ˆåŒ…æ‹¬éŸµå¾‹å’Œè¯­è°ƒä¿¡æ¯ï¼‰æ–¹é¢æ›´ä¸ºä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03074v1">PDF</a> APSIPA ASC 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ä»åŸå§‹éŸ³é¢‘ä¸­è·å–éæ–‡æœ¬è¯­éŸ³è¡¨ç¤ºçš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡åˆæˆè¯­éŸ³è¿›è¡Œåˆ†æã€‚ç”±äºåŸå§‹éŸ³é¢‘æ²¡æœ‰åƒè½¬å½•æ–‡æœ¬é‚£æ ·çš„é…å¯¹è¯­éŸ³è¡¨ç¤ºï¼Œå› æ­¤ä»éé…å¯¹è¯­éŸ³ä¸­è·å¾—è¯­éŸ³è¡¨ç¤ºå¯¹äºå¢å¼ºç°æœ‰è¯­éŸ³åˆæˆæ•°æ®é›†è‡³å…³é‡è¦ã€‚æœ¬æ–‡é‡‡ç”¨SSLæ¨¡å‹çš„ç¦»æ•£ç¬¦å·è¡¨ç¤ºä¸æ–‡æœ¬è¡¨ç¤ºè¿›è¡Œå¯¹æ¯”ï¼Œå¹¶å¯¹åˆæˆè¯­éŸ³è¿›è¡Œäº†åˆ†æã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ–‡æœ¬è¡¨ç¤ºæœ‰åŠ©äºä¿ç•™è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œä½¿ç”¨ç¦»æ•£ç¬¦å·è¡¨ç¤ºåˆ™æ›´æ“…é•¿ä¿ç•™å£°éŸ³å†…å®¹ï¼ŒåŒ…æ‹¬éŸµå¾‹å’Œè¯­è°ƒä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹å¯ä»åŸå§‹éŸ³é¢‘è·å–éæ–‡æœ¬è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>åˆæˆè¯­éŸ³åˆ†ææ˜¯ç”¨äºè¯„ä¼°è¿™ç§è¡¨ç¤ºæ–¹æ³•çš„å…³é”®æ‰‹æ®µã€‚</li>
<li>ç”±äºåŸå§‹éŸ³é¢‘æ²¡æœ‰é…å¥—çš„æ–‡æœ¬è¡¨ç¤ºï¼Œå› æ­¤éœ€è¦åˆ©ç”¨éé…å¯¹è¯­éŸ³æ¥è·å–è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨SSLæ¨¡å‹çš„ç¦»æ•£ç¬¦å·è¡¨ç¤ºä¸æ–‡æœ¬è¡¨ç¤ºåœ¨è¯­éŸ³åˆæˆä¸­å„æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ–‡æœ¬è¡¨ç¤ºæœ‰åŠ©äºä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ç¦»æ•£ç¬¦å·è¡¨ç¤ºæ›´æ“…é•¿ä¿ç•™å£°éŸ³å†…å®¹ï¼Œå¦‚éŸµå¾‹å’Œè¯­è°ƒä¿¡æ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-0b8b17647909fca6594246f9a72ec1f1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-75a811749c8ccf972d425d5d3f8835f6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a03e9055dc22cb8ad850d5cbd8932142.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7fcb49b76618db9a8092e22156ca308d.jpg" align="middle">
</details>




<h2 id="GLM-4-Voice-Towards-Intelligent-and-Human-Like-End-to-End-Spoken-Chatbot"><a href="#GLM-4-Voice-Towards-Intelligent-and-Human-Like-End-to-End-Spoken-Chatbot" class="headerlink" title="GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken   Chatbot"></a>GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken   Chatbot</h2><p><strong>Authors:Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, Jie Tang</strong></p>
<p>We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice">https://github.com/THUDM/GLM-4-Voice</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/glm-4-voice-9b">https://huggingface.co/THUDM/glm-4-voice-9b</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºGLM-4-Voiceï¼Œè¿™æ˜¯ä¸€æ¬¾æ™ºèƒ½ä¸”äººæ€§åŒ–çš„ç«¯åˆ°ç«¯è¯­éŸ³èŠå¤©æœºå™¨äººã€‚å®ƒæ”¯æŒä¸­æ–‡å’Œè‹±è¯­ï¼Œèƒ½è¿›è¡Œå®æ—¶è¯­éŸ³å¯¹è¯ï¼Œå¹¶æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è°ƒæ•´è¯­éŸ³çš„è¯­è°ƒã€è¯­è°ƒã€è¯­é€Ÿå’Œæ–¹è¨€ç­‰ã€‚GLM-4-Voiceé‡‡ç”¨è¶…ä½æ¯”ç‰¹ç‡ï¼ˆ175bpsï¼‰çš„å•ç¼–ç æœ¬è¯­éŸ³æ ‡è®°å™¨ï¼Œå¸§ç‡ä¸º12.5Hzï¼Œæºäºèå…¥å‘é‡é‡åŒ–ç“¶é¢ˆçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ã€‚ä¸ºäº†æœ‰æ•ˆåœ°å°†çŸ¥è¯†ä»æ–‡æœ¬è½¬ç§»åˆ°è¯­éŸ³æ¨¡å¼ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬åˆ°ä»¤ç‰Œæ¨¡å‹ï¼Œä»¥åˆæˆæ¥è‡ªç°æœ‰æ–‡æœ¬é¢„è®­ç»ƒè¯­æ–™åº“çš„è¯­éŸ³æ–‡æœ¬äº¤ç»‡æ•°æ®ã€‚æˆ‘ä»¬ç»§ç»­åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬è¯­è¨€æ¨¡å‹GLM-4-9Bè¿›è¡Œé¢„è®­ç»ƒï¼Œç»“åˆæ— ç›‘ç£è¯­éŸ³æ•°æ®ã€äº¤ç»‡è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œç›‘ç£è¯­éŸ³æ–‡æœ¬æ•°æ®ï¼Œæ‰©å±•è‡³é«˜è¾¾1ä¸‡äº¿ä»¤ç‰Œï¼Œåœ¨è¯­éŸ³è¯­è¨€å»ºæ¨¡å’Œè¯­éŸ³é—®ç­”æ–¹é¢è¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ€§èƒ½ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜è´¨é‡çš„å¯¹è¯è¯­éŸ³æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡æ–¹é¢å‡è¾¾åˆ°äº†ä¼˜äºç°æœ‰åŸºå‡†çš„æ€§èƒ½ã€‚å¼€æ”¾æ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice%E5%92%8Chttps://huggingface.co/THUDM/glm-4-voice-9b%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/THUDM/GLM-4-Voiceå’Œhttps://huggingface.co/THUDM/glm-4-voice-9bè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02612v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GLM-4-Voiceæ˜¯ä¸€æ¬¾æ™ºèƒ½ã€äººæ€§åŒ–çš„ç«¯åˆ°ç«¯è¯­éŸ³èŠå¤©æœºå™¨äººï¼Œæ”¯æŒä¸­è‹±æ–‡å®æ—¶è¯­éŸ³å¯¹è¯ï¼Œå¯æ ¹æ®ç”¨æˆ·æŒ‡ä»¤è°ƒæ•´è¯­éŸ³çš„æƒ…æ„Ÿã€è¯­è°ƒã€è¯­é€Ÿå’Œæ–¹è¨€ã€‚å®ƒé‡‡ç”¨è¶…ä½æ¯”ç‰¹ç‡ï¼ˆ175bpsï¼‰å’Œ12.5Hzå¸§ç‡çš„å•ç¼–ç æœ¬è¯­éŸ³æ ‡è®°å™¨ï¼Œå¹¶ç»“åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ä¸­çš„å‘é‡é‡åŒ–ç“¶é¢ˆç¼–ç å™¨å®ç°é«˜æ•ˆè¯­éŸ³è½¬æ–‡æœ¬ã€‚é€šè¿‡åˆæˆç°æœ‰æ–‡æœ¬é¢„è®­ç»ƒè¯­æ–™åº“ä¸­çš„è¯­éŸ³æ–‡æœ¬äº¤ç»‡æ•°æ®ï¼Œå®ç°æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡æ€çš„çŸ¥è¯†é«˜æ•ˆè½¬ç§»ã€‚é€šè¿‡é¢„è®­ç»ƒæ–‡æœ¬è¯­è¨€æ¨¡å‹GLM-4-9Bï¼Œç»“åˆæ— ç›‘ç£è¯­éŸ³æ•°æ®ã€äº¤ç»‡è¯­éŸ³æ–‡æœ¬æ•°æ®å’Œç›‘ç£è¯­éŸ³æ–‡æœ¬æ•°æ®ï¼Œæ‰©å±•è‡³1ä¸‡äº¿ä¸ªä»¤ç‰Œï¼Œå®ç°è¯­éŸ³è¯­è¨€å’Œå£è¯­é—®ç­”çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚ä½¿ç”¨é«˜è´¨é‡å¯¹è¯è¯­éŸ³æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¾¾åˆ°å¯¹è¯èƒ½åŠ›å’Œè¯­éŸ³è´¨é‡çš„æœ€ä½³æ€§èƒ½ã€‚å…¬å¼€æ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4-Voice%E5%92%8Chttps://huggingface.co/THUDM/glm-4-voice-9b%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/THUDM/GLM-4-Voiceå’Œhttps://huggingface.co/THUDM/glm-4-voice-9bè®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GLM-4-Voiceæ˜¯ä¸€ä¸ªæ™ºèƒ½ã€äººæ€§åŒ–çš„ç«¯åˆ°ç«¯è¯­éŸ³èŠå¤©æœºå™¨äººï¼Œæ”¯æŒä¸­è‹±æ–‡å®æ—¶å¯¹è¯ã€‚</li>
<li>å®ƒèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·æŒ‡ä»¤è°ƒæ•´è¯­éŸ³çš„æƒ…æ„Ÿã€è¯­è°ƒã€è¯­é€Ÿå’Œæ–¹è¨€ã€‚</li>
<li>GLM-4-Voiceé‡‡ç”¨äº†å…ˆè¿›çš„è¯­éŸ³å¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¶…ä½æ¯”ç‰¹ç‡ç¼–ç å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚</li>
<li>é€šè¿‡åˆæˆè¯­éŸ³æ–‡æœ¬äº¤ç»‡æ•°æ®ï¼Œå®ç°äº†æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡æ€çš„çŸ¥è¯†é«˜æ•ˆè½¬ç§»ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨é¢„è®­ç»ƒçš„åŸºç¡€ä¸Šç»“åˆäº†å¤šç§æ•°æ®ï¼ŒåŒ…æ‹¬æ— ç›‘ç£ã€äº¤ç»‡å’Œç›‘ç£è¯­éŸ³æ–‡æœ¬æ•°æ®ã€‚</li>
<li>GLM-4-Voiceåœ¨è¯­éŸ³è¯­è¨€å’Œå£è¯­é—®ç­”æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-93d5efd16c01204f21a643086af317c9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-908c87d006e309e0892b8537872b60db.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-77dae6055a3d14522e2162907a1c2c7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0b8d53c63f5d7628dd1b39b898e202ec.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f88916a03b6edfe48920863e0a168c23.jpg" align="middle">
</details>




<h2 id="Characterizing-Information-Shared-by-Participants-to-Coding-Challenges-The-Case-of-Advent-of-Code"><a href="#Characterizing-Information-Shared-by-Participants-to-Coding-Challenges-The-Case-of-Advent-of-Code" class="headerlink" title="Characterizing Information Shared by Participants to Coding Challenges:   The Case of Advent of Code"></a>Characterizing Information Shared by Participants to Coding Challenges:   The Case of Advent of Code</h2><p><strong>Authors:Francesco Cauteruccio, Enrico Corradini, Luca Virgili</strong></p>
<p>Advent of Code (AoC from now on) is a popular coding challenge requiring to solve programming puzzles for a variety of skill sets and levels. AoC follows the advent calendar, therefore it is an annual challenge that lasts for 25 days. AoC participants usually post their solutions on social networks and discuss them online. These challenges are interesting to study since they could highlight the adoption of new tools, the evolution of the developer community, or the technological requirements of well-known companies. For these reasons, we first create a dataset of the 2019-2021 AoC editions containing the discussion threads made on the subreddit {\tt &#x2F;r&#x2F;adventofcode}. Then, we propose a model based on stream graphs to best study this context, where we represent its most important actors through time: participants, comments, and programming languages. Thanks to our model, we investigate user participation, adoption of new programming languages during a challenge and between two of them, and resiliency of programming languages based on a Stack Overflow survey. We find that the top-used programming languages are almost the same in the three years, pointing out their importance. Moreover, participants tend to keep the same programming language for the whole challenge, while the ones attending two AoCs usually change it in the next one. Finally, we observe interesting results about the programming languages that are <code>Popular&#39;&#39; or </code>Lovedâ€™â€™ according to the Stack Overflow survey. Firstly, these are the ones adopted for the longest time in an AoC edition, thanks to which users have a high chance of reaching the end of the challenge. Secondly, they are the most chosen when a participant decides to change programming language during the same challenge. </p>
<blockquote>
<p>â€œAdvent of Codeâ€ï¼ˆç®€ç§°AoCï¼‰æ˜¯ä¸€é¡¹å¹¿å—æ¬¢è¿çš„ç¼–ç æŒ‘æˆ˜ï¼Œè¦æ±‚é’ˆå¯¹ä¸åŒæŠ€èƒ½å’Œæ°´å¹³çš„å‚ä¸è€…è§£å†³ç¼–ç¨‹è°œé¢˜ã€‚AoCéµå¾ªå†æ³•ï¼Œå› æ­¤æ˜¯ä¸€é¡¹ä¸ºæœŸ25å¤©çš„å¹´åº¦æŒ‘æˆ˜ã€‚AoCå‚ä¸è€…é€šå¸¸ä¼šåœ¨ç¤¾äº¤ç½‘ç»œä¸Šå‘å¸ƒè‡ªå·±çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨çº¿è¿›è¡Œè®¨è®ºã€‚è¿™äº›æŒ‘æˆ˜å¾ˆæœ‰è¶£ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½åæ˜ å‡ºæ–°å·¥å…·çš„ä½¿ç”¨æƒ…å†µã€å¼€å‘è€…ç¤¾åŒºçš„æ¼”å˜ï¼Œæˆ–çŸ¥åå…¬å¸çš„æŠ€æœ¯éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«2019-2021å¹´AoCç‰ˆæœ¬è®¨è®ºå¸–çš„æ•°æ®é›†ï¼Œè¿™äº›è®¨è®ºå¸–æ¥è‡ªRedditä¸Šçš„{&#x2F;r&#x2F;adventofcode}ç¤¾åŒºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæµå›¾çš„æ¨¡å‹æ¥æ›´å¥½åœ°ç ”ç©¶è¿™ä¸€ç¯å¢ƒï¼Œé€šè¿‡æ—¶é—´å±•ç°æœ€é‡è¦çš„å‚ä¸è€…ï¼šå‚ä¸è€…ã€è¯„è®ºå’Œç¼–ç¨‹è¯­è¨€ã€‚å€ŸåŠ©æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨æˆ·å‚ä¸åº¦ã€æ–°ç¼–ç¨‹è¯­è¨€çš„é‡‡ç”¨æƒ…å†µä»¥åŠåœ¨ä¸¤æ¬¡æŒ‘æˆ˜ä¹‹é—´çš„é‡‡ç”¨æƒ…å†µï¼Œä»¥åŠåŸºäºStack Overflowè°ƒæŸ¥çš„ç¼–ç¨‹è¯­è¨€çš„éŸ§æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä¸‰å¹´å†…ä½¿ç”¨çš„æœ€æµè¡Œçš„ç¼–ç¨‹è¯­è¨€å‡ ä¹ç›¸åŒï¼Œå‡¸æ˜¾äº†å®ƒä»¬çš„é‡è¦æ€§ã€‚æ­¤å¤–ï¼Œå‚ä¸è€…å¾€å¾€åœ¨æ•´ä¸ªæŒ‘æˆ˜ä¸­åšæŒä½¿ç”¨åŒä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œè€Œå‚åŠ ä¸¤æ¬¡AoCçš„å‚ä¸è€…é€šå¸¸ä¼šåœ¨ä¸‹ä¸€æ¬¡æŒ‘æˆ˜ä¸­æ›´æ¢è¯­è¨€ã€‚æœ€åï¼Œå…³äºStack Overflowè°ƒæŸ¥ä¸­æ ‡è®°ä¸ºâ€œæµè¡Œâ€æˆ–â€œå—å–œçˆ±â€çš„ç¼–ç¨‹è¯­è¨€ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€äº›æœ‰è¶£çš„ç»“æœã€‚é¦–å…ˆï¼Œè¿™äº›è¯­è¨€æ˜¯AoCç‰ˆæœ¬ä¸­é‡‡ç”¨æ—¶é—´æœ€é•¿çš„è¯­è¨€ï¼Œå€ŸåŠ©è¿™äº›è¯­è¨€ï¼Œç”¨æˆ·æœ‰å¾ˆå¤§æœºä¼šå®Œæˆæ•´ä¸ªæŒ‘æˆ˜ã€‚å…¶æ¬¡ï¼Œå½“å‚ä¸è€…å†³å®šåœ¨åŒä¸€æŒ‘æˆ˜ä¸­æ›´æ¢ç¼–ç¨‹è¯­è¨€æ—¶ï¼Œå®ƒä»¬æ˜¯æœ€å—æ¬¢è¿çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02290v1">PDF</a> 10 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Advent of Codeï¼ˆAoCï¼‰çš„æŒ‘æˆ˜ã€‚è¯¥æŒ‘æˆ˜æŒç»­é•¿è¾¾25å¤©ï¼Œæ¯å¹´ä¸¾è¡Œä¸€æ¬¡ï¼Œå‚ä¸è€…ä¼šåœ¨çº¿ä¸Šåˆ†äº«å’Œè®¨è®ºè§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨æµå›¾æ¨¡å‹ï¼Œé€šè¿‡å¯¹ç¤¾åŒºå‚ä¸è€…ã€è¯„è®ºå’Œç¼–ç¨‹è¯­è¨€çš„åˆ†æï¼Œç ”ç©¶äº†ç”¨æˆ·å‚ä¸åº¦ã€æ–°ç¼–ç¨‹è¯­è¨€çš„é‡‡çº³æƒ…å†µä»¥åŠç¼–ç¨‹è¯­è¨€çš„æŒä¹…æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶ç¼–ç¨‹è¯­è¨€çš„æµè¡Œè¶‹åŠ¿ç•¥æœ‰å˜åŒ–ï¼Œä½†è¿‡å»ä¸‰å¹´ä¸­æœ€å—æ¬¢è¿çš„ç¼–ç¨‹è¯­è¨€å‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚åŒæ—¶ï¼Œå‚ä¸è€…æ›´å€¾å‘äºåœ¨æ•´ä¸ªæŒ‘æˆ˜æœŸé—´ä½¿ç”¨åŒä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œè€Œè¿ç»­å‚åŠ ä¸¤æ¬¡æŒ‘æˆ˜çš„å‚ä¸è€…å¾€å¾€ä¼šæ›´æ¢ç¼–ç¨‹è¯­è¨€ã€‚æ­¤å¤–ï¼Œæ ¹æ®Stack Overflowè°ƒæŸ¥çš„ç»“æœï¼Œæœ€å—æ¬¢è¿æˆ–æœ€å—å–œçˆ±çš„ç¼–ç¨‹è¯­è¨€æ›´æœ‰å¯èƒ½å¸®åŠ©ç”¨æˆ·å®ŒæˆæŒ‘æˆ˜å¹¶æŒç»­è¾ƒé•¿æ—¶é—´ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹ç ”ç©¶æ­ç¤ºäº†AoCæŒ‘æˆ˜ä¸­ç¼–ç¨‹è¯­è¨€å’Œç¤¾åŒºçš„åŠ¨æ€å˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³é”®è§è§£ï¼š</p>
<ol>
<li>Advent of Codeæ˜¯ä¸€é¡¹å¹´åº¦æŒ‘æˆ˜ï¼ŒæŒç»­25å¤©ï¼Œå‚ä¸è€…ä¼šåœ¨ç¤¾äº¤åª’ä½“ä¸Šåˆ†äº«å’Œè®¨è®ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>åˆ©ç”¨æµå›¾æ¨¡å‹å¯¹ç¤¾åŒºå‚ä¸è€…ã€è¯„è®ºå’Œç¼–ç¨‹è¯­è¨€è¿›è¡Œç»¼åˆåˆ†æï¼Œå¯ä»¥æ›´å…¨é¢åœ°ç ”ç©¶ç”¨æˆ·å‚ä¸åº¦å’ŒæŠ€æœ¯è¶‹åŠ¿ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œåœ¨è¿‡å»ä¸‰å¹´ä¸­ï¼Œä¸»æµçš„ç¼–ç¨‹è¯­è¨€å˜åŒ–ä¸å¤§ã€‚è¯´æ˜å­˜åœ¨ä¸€äº›å¸¸ç”¨çš„è¯­è¨€èƒ½å¤Ÿåœ¨ä¸åŒæ—¶æœŸéƒ½è¢«å¹¿æ³›åº”ç”¨ã€‚è¿™å¯èƒ½æ­ç¤ºè¿™äº›è¯­è¨€å…·æœ‰ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-402632388026de6f691af7dd077a3f8e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8b6f87d706fdd32648f57e883e8809d9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dc5e8a7faa16c915fb356fe2bcefd455.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-699c6480000ed8b55623c0060831c0d3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-78b24fa4564f11026fad88ac45f7d4b1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5762f5077df8c97ff4527ff63e2c1ef2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-699ef6bdf91435c20d8fd56d902bf4f5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-4ade94a5d23e89636d488ec06150931e.jpg" align="middle">
</details>




<h2 id="The-Codec-Language-Model-based-Zero-Shot-Spontaneous-Style-TTS-System-for-CoVoC-Challenge-2024"><a href="#The-Codec-Language-Model-based-Zero-Shot-Spontaneous-Style-TTS-System-for-CoVoC-Challenge-2024" class="headerlink" title="The Codec Language Model-based Zero-Shot Spontaneous Style TTS System   for CoVoC Challenge 2024"></a>The Codec Language Model-based Zero-Shot Spontaneous Style TTS System   for CoVoC Challenge 2024</h2><p><strong>Authors:Shuoyi Zhou, Yixuan Zhou, Weiqing Li, Jun Chen, Runchuan Ye, Weihao Wu, Zijian Lin, Shun Lei, Zhiyong Wu</strong></p>
<p>This paper describes the zero-shot spontaneous style TTS system for the ISCSLP 2024 Conversational Voice Clone Challenge (CoVoC). We propose a LLaMA-based codec language model with a delay pattern to achieve spontaneous style voice cloning. To improve speech intelligibility, we introduce the Classifier-Free Guidance (CFG) strategy in the language model to strengthen conditional guidance on token prediction. To generate high-quality utterances, we adopt effective data preprocessing operations and fine-tune our model with selected high-quality spontaneous speech data. The official evaluations in the CoVoC constrained track show that our system achieves the best speech naturalness MOS of 3.80 and obtains considerable speech quality and speaker similarity results. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ISCSLP 2024å¯¹è¯å¼è¯­éŸ³å…‹éš†æŒ‘æˆ˜èµ›ï¼ˆCoVoCï¼‰çš„é›¶æ ·æœ¬è‡ªå‘é£æ ¼TTSç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLaMAçš„ç¼–è§£ç è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨å»¶è¿Ÿæ¨¡å¼æ¥å®ç°è‡ªå‘é£æ ¼çš„å£°éŸ³å…‹éš†ã€‚ä¸ºäº†æé«˜è¯­éŸ³æ¸…æ™°åº¦ï¼Œæˆ‘ä»¬åœ¨è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥äº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰ç­–ç•¥ï¼Œä»¥åŠ å¼ºä»¤ç‰Œé¢„æµ‹çš„æ¡ä»¶å¼•å¯¼ã€‚ä¸ºäº†ç”Ÿæˆé«˜è´¨é‡çš„çŸ­è¯­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ‰æ•ˆçš„æ•°æ®é¢„å¤„ç†æ“ä½œï¼Œå¹¶ä½¿ç”¨ç²¾é€‰çš„é«˜è´¨é‡è‡ªå‘è¯­éŸ³æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨CoVoCçº¦æŸè½¨é“çš„å®˜æ–¹è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿè¾¾åˆ°äº†æœ€ä½³è¯­éŸ³è‡ªç„¶åº¦MOS 3.80ï¼Œå¹¶è·å¾—äº†ç›¸å½“ä¸é”™çš„è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01100v1">PDF</a> Accepted by ISCSLP 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ISCSLP 2024å¯¹è¯å¼è¯­éŸ³å…‹éš†æŒ‘æˆ˜ï¼ˆCoVoCï¼‰çš„é›¶æ ·æœ¬è‡ªå‘å¼è¯­éŸ³TTSç³»ç»Ÿã€‚è¯¥ç ”ç©¶æå‡ºåŸºäºLLaMAç¼–ç è¯­è¨€æ¨¡å‹çš„è‡ªå‘å¼è¯­éŸ³å…‹éš†æ–¹æ³•ï¼Œå¹¶ç»“åˆå»¶è¿Ÿæ¨¡å¼å’Œåˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ç­–ç•¥æ”¹å–„è¯­éŸ³æ¸…æ™°åº¦ã€‚é€šè¿‡æœ‰æ•ˆæ•°æ®é¢„å¤„ç†å’Œç²¾ç»†æ¨¡å‹è°ƒä¼˜ï¼Œä½¿ç”¨é«˜è´¨é‡è‡ªå‘è¯­éŸ³æ•°æ®ç”Ÿæˆé«˜è´¨é‡çš„è¯è¯­ã€‚å®˜æ–¹è¯„ä»·æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨CoVoCçº¦æŸèµ›é“ä¸­è·å¾—äº†æœ€ä½³è¯­éŸ³è‡ªç„¶åº¦MOS 3.80åˆ†ï¼Œå¹¶åœ¨è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†é’ˆå¯¹ISCSLP 2024å¯¹è¯å¼è¯­éŸ³å…‹éš†æŒ‘æˆ˜çš„é›¶æ ·æœ¬è‡ªå‘å¼TTSç³»ç»Ÿã€‚</li>
<li>é‡‡ç”¨åŸºäºLLaMAç¼–ç çš„è¯­è¨€æ¨¡å‹å®ç°è‡ªå‘å¼è¯­éŸ³å…‹éš†ï¼Œå¹¶å¼•å…¥å»¶è¿Ÿæ¨¡å¼ä»¥å¢å¼ºè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚</li>
<li>ä½¿ç”¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ç­–ç•¥æ”¹å–„è¯­éŸ³æ¸…æ™°åº¦ï¼Œæé«˜è¯­éŸ³çš„è¾¨è¯†åº¦ã€‚</li>
<li>é€šè¿‡æœ‰æ•ˆæ•°æ®é¢„å¤„ç†å’Œç²¾ç»†æ¨¡å‹è°ƒä¼˜ï¼Œä½¿ç”¨é«˜è´¨é‡è‡ªå‘è¯­éŸ³æ•°æ®æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¯è¯­ã€‚</li>
<li>ç³»ç»Ÿåœ¨å®˜æ–¹è¯„ä»·ä¸­è¡¨ç°å‡ºæœ€ä½³è¯­éŸ³è‡ªç„¶åº¦ã€‚</li>
<li>ç³»ç»Ÿè·å¾—äº†è¾ƒé«˜çš„è¯­éŸ³è´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0ec8360b57ff3c4d1e2a02d10b2e36c0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f581a1d063454d7033c124b64413c7d0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-32c9a8f6fc5a15522f19118db94bca34.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-e758ed5dd1bd12bd80297980afc6fe8d.jpg" align="middle">
</details>




<h2 id="Compressed-â€˜CMB-liteâ€™-Likelihoods-Using-Automatic-Differentiation"><a href="#Compressed-â€˜CMB-liteâ€™-Likelihoods-Using-Automatic-Differentiation" class="headerlink" title="Compressed â€˜CMB-liteâ€™ Likelihoods Using Automatic Differentiation"></a>Compressed â€˜CMB-liteâ€™ Likelihoods Using Automatic Differentiation</h2><p><strong>Authors:L. Balkenhol</strong></p>
<p>The compression of multi-frequency cosmic microwave background (CMB) power spectrum measurements into a series of foreground-marginalised CMB-only band powers allows for the construction of faster and more easily interpretable â€˜liteâ€™ likelihoods. However, obtaining the compressed data vector is computationally expensive and yields a covariance matrix with sampling noise. In this work, we present an implementation of the CMB-lite framework relying on automatic differentiation. The technique presented reduces the computational cost of the lite likelihood construction to one minimisation and one Hessian evaluation, which run on a personal computer in about a minute. We demonstrate the efficiency and accuracy of this procedure by applying it to the differentiable SPT-3G 2018 TT&#x2F;TE&#x2F;EE likelihood from the candl library. We find good agreement between the marginalised posteriors of cosmological parameters yielded by the resulting lite likelihood and the reference multi-frequency version for all cosmological models tested; the best-fit values shift by $&lt;0.1,\sigma$, where $\sigma$ is the width of the multi-frequency posterior, and the inferred parameter error bars match to within $&lt;10%$. We publicly release the SPT-3G 2018 TT&#x2F;TE&#x2F;EE lite likelihood and a python notebook showing its construction at <a target="_blank" rel="noopener" href="https://github.com/Lbalkenhol/candl">https://github.com/Lbalkenhol/candl</a> . </p>
<blockquote>
<p>å°†å¤šé¢‘å®‡å®™å¾®æ³¢èƒŒæ™¯ï¼ˆCMBï¼‰åŠŸç‡è°±æµ‹é‡å€¼å‹ç¼©æˆä¸€ç³»åˆ—å‰æ™¯è¾¹ç¼˜åŒ–çš„ä»…åŒ…å«CMBçš„æ³¢æ®µåŠŸç‡ï¼Œä»è€Œå¯ä»¥æ„å»ºæ›´å¿«ã€æ›´æ˜“è§£è¯»çš„â€œè½»é‡çº§â€ä¼¼ç„¶å‡½æ•°ã€‚ç„¶è€Œï¼Œè·å–å‹ç¼©æ•°æ®å‘é‡åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µï¼Œå¹¶ä¸”äº§ç”Ÿçš„åæ–¹å·®çŸ©é˜µå…·æœ‰é‡‡æ ·å™ªå£°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¾èµ–è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯çš„CMB-liteæ¡†æ¶çš„å®ç°ã€‚æ‰€æå‡ºçš„æŠ€æœ¯å°†è½»é‡çº§ä¼¼ç„¶å‡½æ•°æ„å»ºçš„è®¡ç®—æˆæœ¬é™ä½åˆ°ä¸€æ¬¡æœ€å°åŒ–å’Œä¸€æ¬¡Hessianè¯„ä¼°ï¼Œå¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šå¤§çº¦ä¸€åˆ†é’Ÿå†…è¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶åº”ç”¨äºå¯å¾®åˆ†çš„SPT-3G 2018 TT&#x2F;TE&#x2F;EEä¼¼ç„¶å‡½æ•°ï¼ˆæ¥è‡ªcandlåº“ï¼‰æ¥å±•ç¤ºè¯¥ç¨‹åºçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºæµ‹è¯•çš„æ‰€æœ‰å®‡å®™å­¦æ¨¡å‹ï¼Œç”±æ‰€å¾—è½»é‡çº§ä¼¼ç„¶å‡½æ•°äº§ç”Ÿçš„è¾¹ç¼˜åŒ–å®‡å®™å­¦å‚æ•°åéªŒä¸å‚è€ƒå¤šé¢‘ç‰ˆæœ¬å…·æœ‰è‰¯å¥½çš„ä¸€è‡´æ€§ï¼›æœ€ä½³æ‹Ÿåˆå€¼åœ¨$&lt;0.1\sigma$å†…ç§»åŠ¨ï¼Œå…¶ä¸­$\sigma$æ˜¯å¤šé¢‘åéªŒçš„å®½åº¦ï¼Œæ¨æ–­çš„å‚æ•°è¯¯å·®èŒƒå›´åŒ¹é…åº¦åœ¨$&lt;10%$ä»¥å†…ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lbalkenhol/candl%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E4%BA%86SPT-3G">https://github.com/Lbalkenhol/candlå…¬å¼€å‘å¸ƒäº†SPT-3G</a> 2018 TT&#x2F;TE&#x2F;EEè½»é‡çº§ä¼¼ç„¶å‡½æ•°ä»¥åŠå±•ç¤ºå…¶æ„å»ºçš„Pythonç¬”è®°æœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00826v1">PDF</a> 8 pages, 4 figures, 1 table, prepared for OJA submission</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†å¤šé¢‘å®‡å®™å¾®æ³¢èƒŒæ™¯ï¼ˆCMBï¼‰åŠŸç‡è°±çš„æµ‹é‡å€¼å‹ç¼©æˆä¸€ç³»åˆ—ä»…åŒ…å«CMBä¿¡å·çš„è½»é‡çº§è°±å¸¦åŠŸç‡ï¼Œä»¥æ„å»ºæ›´å¿«ã€æ›´æ˜“è§£è¯»çš„è½»é‡çº§æ¦‚ç‡åˆ†å¸ƒã€‚å°½ç®¡å‹ç¼©æ•°æ®å‘é‡å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œé‡‡æ ·å™ªå£°çš„é—®é¢˜ï¼Œä½†æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¾èµ–è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯çš„CMBè½»é‡çº§æ¡†æ¶å®ç°æ–¹æ¡ˆã€‚è¯¥æŠ€æœ¯å°†è½»é‡çº§æ¦‚ç‡åˆ†å¸ƒçš„è®¡ç®—æˆæœ¬é™ä½åˆ°ä»…ä¸€æ¬¡æœ€å°åŒ–å’Œä¸€æ¬¡Hessianè¯„ä¼°ï¼Œå¯åœ¨ä¸ªäººè®¡ç®—æœºä¸Šå¤§çº¦ä¸€åˆ†é’Ÿå†…è¿è¡Œã€‚é€šè¿‡åº”ç”¨è¯¥æŠ€æœ¯åˆ°å¯å¾®åˆ†çš„SPT-3G 2018 TT&#x2F;TE&#x2F;EEæ¦‚ç‡åˆ†å¸ƒåº“ï¼ŒéªŒè¯äº†å…¶æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å‘ç°å¯¹äºæ‰€æœ‰æµ‹è¯•çš„å®‡å®™å­¦æ¨¡å‹ï¼Œç”±æ­¤äº§ç”Ÿçš„è½»é‡çº§æ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿçš„è¾¹é™…åéªŒå®‡å®™å­¦å‚æ•°ä¸å¤šé¢‘ç‰ˆæœ¬çš„åéªŒå‚æ•°å…·æœ‰è‰¯å¥½çš„ä¸€è‡´æ€§ï¼Œæœ€ä½³æ‹Ÿåˆå€¼åœ¨$&lt; 0.1Ïƒ$ä»¥å†…å˜åŒ–ï¼Œå…¶ä¸­Ïƒä¸ºå¤šé¢‘åéªŒçš„å®½åº¦ï¼Œæ¨æ–­çš„å‚æ•°è¯¯å·®èŒƒå›´åŒ¹é…åº¦åœ¨$&lt; 10%$ä»¥å†…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CMBåŠŸç‡è°±æµ‹é‡å€¼å¯å‹ç¼©æˆä¸€ç³»åˆ—ä»…åŒ…å«CMBä¿¡å·çš„è½»é‡çº§è°±å¸¦åŠŸç‡ï¼Œæ„å»ºæ›´å¿«ã€æ›´æ˜“è§£è¯»çš„è½»é‡çº§æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li>è®¡ç®—å‹ç¼©æ•°æ®å‘é‡å­˜åœ¨è®¡ç®—æˆæœ¬é«˜å’Œé‡‡æ ·å™ªå£°çš„é—®é¢˜ã€‚</li>
<li>æå‡ºä¾èµ–è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯çš„CMBè½»é‡çº§æ¡†æ¶å®ç°æ–¹æ¡ˆï¼Œé™ä½è®¡ç®—æˆæœ¬ã€‚</li>
<li>åº”ç”¨æŠ€æœ¯åˆ°SPT-3G 2018 TT&#x2F;TE&#x2F;EEæ¦‚ç‡åˆ†å¸ƒåº“éªŒè¯å…¶æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è½»é‡çº§æ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿçš„è¾¹é™…åéªŒå®‡å®™å­¦å‚æ•°ä¸å¤šé¢‘ç‰ˆæœ¬çš„åéªŒå‚æ•°å…·æœ‰è‰¯å¥½ä¸€è‡´æ€§ã€‚</li>
<li>æœ€ä½³æ‹Ÿåˆå€¼åœ¨$&lt; 0.1Ïƒ$ä»¥å†…å˜åŒ–ï¼Œå…¶ä¸­Ïƒä¸ºå¤šé¢‘åéªŒçš„å®½åº¦ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a88b174cea8af3442d4aa11d8da8475a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ef5d73572dc2a498e90c4384ba8a3750.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b666480f7056e8e631d6966f935a7ccb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a1ed6ff85999f2b434352b40337c5004.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eaf7e302b7915e119cc3bb3e913c9e6f.jpg" align="middle">
</details>




<h2 id="A-Context-Based-Numerical-Format-Prediction-for-a-Text-To-Speech-System"><a href="#A-Context-Based-Numerical-Format-Prediction-for-a-Text-To-Speech-System" class="headerlink" title="A Context-Based Numerical Format Prediction for a Text-To-Speech System"></a>A Context-Based Numerical Format Prediction for a Text-To-Speech System</h2><p><strong>Authors:Yaser Darwesh, Lit Wei Wern, Mumtaz Begum Mustafa</strong></p>
<p>Many of the existing TTS systems cannot accurately synthesize text containing a variety of numerical formats, resulting in reduced intelligibility of the synthesized speech. This research aims to develop a numerical format classifier that can classify six types of numeric contexts. Experiments were carried out using the proposed context-based feature extraction technique, which is focused on extracting keywords, punctuation marks, and symbols as the features of the numbers. Support Vector Machine, K-Nearest Neighbors Linear Discriminant Analysis, and Decision Tree were used as classifiers. We have used the 10-fold cross-validation technique to determine the classification accuracy in terms of recall and precision. It can be found that the proposed solution is better than the existing feature extraction technique with improvement to the classification accuracy by 30% to 37%. The use of the number format classification can increase the intelligibility of the TTS systems. </p>
<blockquote>
<p>ç°æœ‰çš„è®¸å¤šTTSç³»ç»Ÿæ— æ³•å‡†ç¡®åˆæˆåŒ…å«å¤šç§æ•°å­—æ ¼å¼çš„æ–‡æœ¬ï¼Œå¯¼è‡´åˆæˆè¯­éŸ³çš„æ¸…æ™°åº¦é™ä½ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ•°å­—æ ¼å¼åˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨èƒ½å¤Ÿåˆ†ç±»å…­ç§æ•°å­—ä¸Šä¸‹æ–‡ç±»å‹ã€‚å®éªŒé‡‡ç”¨äº†åŸºäºä¸Šä¸‹æ–‡çš„ç‰¹å¾æå–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä¾§é‡äºæå–å…³é”®è¯ã€æ ‡ç‚¹ç¬¦å·å’Œç¬¦å·ä½œä¸ºæ•°å­—çš„ç‰¹å¾ã€‚æ”¯æŒå‘é‡æœºã€Kè¿‘é‚»çº¿æ€§åˆ¤åˆ«åˆ†æå’Œå†³ç­–æ ‘è¢«ç”¨ä½œåˆ†ç±»å™¨ã€‚æˆ‘ä»¬é‡‡ç”¨äº†10å€äº¤å‰éªŒè¯æŠ€æœ¯æ¥ç¡®å®šå¬å›ç‡å’Œç²¾ç¡®ç‡æ–¹é¢çš„åˆ†ç±»ç²¾åº¦ã€‚å¯ä»¥å‘ç°ï¼Œæ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆä¼˜äºç°æœ‰çš„ç‰¹å¾æå–æŠ€æœ¯ï¼Œåˆ†ç±»ç²¾åº¦æé«˜äº†30%è‡³37%ã€‚ä½¿ç”¨æ•°å­—æ ¼å¼åˆ†ç±»å¯ä»¥æé«˜TTSç³»ç»Ÿçš„æ¸…æ™°åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00028v1">PDF</a> 21 pages, 6 tables, 1 figure</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ•°å€¼æ ¼å¼åˆ†ç±»å™¨ï¼Œèƒ½å¤Ÿåˆ†ç±»å…­ç§æ•°å€¼ä¸Šä¸‹æ–‡ã€‚é‡‡ç”¨åŸºäºä¸Šä¸‹æ–‡çš„ç‰¹å¾æå–æŠ€æœ¯ï¼Œå…³æ³¨æ•°å­—çš„ç‰¹å¾æå–ï¼Œå¦‚å…³é”®è¯ã€æ ‡ç‚¹ç¬¦å·å’Œç¬¦å·ã€‚ä½¿ç”¨æ”¯æŒå‘é‡æœºã€Kè¿‘é‚»çº¿æ€§åˆ¤åˆ«åˆ†æå’Œå†³ç­–æ ‘ä½œä¸ºåˆ†ç±»å™¨ã€‚ä½¿ç”¨10å€äº¤å‰éªŒè¯æŠ€æœ¯ç¡®å®šå¬å›ç‡å’Œç²¾ç¡®åº¦çš„åˆ†ç±»ç²¾åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„è§£å†³æ–¹æ¡ˆä¼˜äºç°æœ‰çš„ç‰¹å¾æå–æŠ€æœ¯ï¼Œåˆ†ç±»ç²¾åº¦æé«˜äº†30%è‡³37%ã€‚æ•°å€¼æ ¼å¼åˆ†ç±»çš„ä½¿ç”¨å¯ä»¥æé«˜TTSç³»ç»Ÿçš„æ¸…æ™°åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§èƒ½å¤Ÿåˆ†ç±»å…­ç§æ•°å€¼ä¸Šä¸‹æ–‡çš„æ•°å€¼æ ¼å¼åˆ†ç±»å™¨ã€‚</li>
<li>é‡‡ç”¨åŸºäºä¸Šä¸‹æ–‡çš„ç‰¹å¾æå–æŠ€æœ¯ï¼Œå…³æ³¨æ•°å­—çš„ç‰¹å¾ï¼Œå¦‚å…³é”®è¯ã€æ ‡ç‚¹ç¬¦å·å’Œç¬¦å·ã€‚</li>
<li>ä½¿ç”¨æ”¯æŒå‘é‡æœºã€Kè¿‘é‚»çº¿æ€§åˆ¤åˆ«åˆ†æå’Œå†³ç­–æ ‘ä½œä¸ºåˆ†ç±»å™¨ã€‚</li>
<li>ä½¿ç”¨10å€äº¤å‰éªŒè¯æŠ€æœ¯è¯„ä¼°åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæ–°æ–¹æ³•çš„åˆ†ç±»ç²¾åº¦æé«˜äº†30%è‡³37%ã€‚</li>
<li>æ•°å€¼æ ¼å¼åˆ†ç±»çš„åº”ç”¨å¯ä»¥å¢åŠ TTSç³»ç»Ÿçš„æ¸…æ™°åº¦ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹äºæé«˜TTSç³»ç»Ÿåœ¨å¤„ç†åŒ…å«å¤šç§æ•°å€¼æ ¼å¼çš„æ–‡æœ¬æ—¶çš„å‡†ç¡®æ€§å…·æœ‰æ½œåœ¨æ„ä¹‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8ea168121fc84c0c13e102f01afc42c7.jpg" align="middle">
</details>




<h2 id="Continual-Learning-in-Machine-Speech-Chain-Using-Gradient-Episodic-Memory"><a href="#Continual-Learning-in-Machine-Speech-Chain-Using-Gradient-Episodic-Memory" class="headerlink" title="Continual Learning in Machine Speech Chain Using Gradient Episodic   Memory"></a>Continual Learning in Machine Speech Chain Using Gradient Episodic   Memory</h2><p><strong>Authors:Geoffrey Tyndall, Kurniawati Azizah, Dipta Tanaya, Ayu Purwarianti, Dessi Puji Lestari, Sakriani Sakti</strong></p>
<p>Continual learning for automatic speech recognition (ASR) systems poses a challenge, especially with the need to avoid catastrophic forgetting while maintaining performance on previously learned tasks. This paper introduces a novel approach leveraging the machine speech chain framework to enable continual learning in ASR using gradient episodic memory (GEM). By incorporating a text-to-speech (TTS) component within the machine speech chain, we support the replay mechanism essential for GEM, allowing the ASR model to learn new tasks sequentially without significant performance degradation on earlier tasks. Our experiments, conducted on the LJ Speech dataset, demonstrate that our method outperforms traditional fine-tuning and multitask learning approaches, achieving a substantial error rate reduction while maintaining high performance across varying noise conditions. We showed the potential of our semi-supervised machine speech chain approach for effective and efficient continual learning in speech recognition. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æŒç»­å­¦ä¹ æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é¿å…ç¾éš¾æ€§é—å¿˜çš„åŒæ—¶ï¼Œè¿˜è¦ä¿æŒå¯¹å…ˆå‰å­¦ä¹ ä»»åŠ¡çš„æ€§èƒ½ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ©ç”¨æœºå™¨è¯­éŸ³é“¾æ¡†æ¶å®ç°ASRä¸­æŒç»­å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨æ¢¯åº¦ç‰‡æ®µè®°å¿†ï¼ˆGEMï¼‰ã€‚é€šè¿‡åœ¨æœºå™¨è¯­éŸ³é“¾ä¸­èå…¥æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç»„ä»¶ï¼Œæˆ‘ä»¬æ”¯æŒå¯¹GEMè‡³å…³é‡è¦çš„å›æ”¾æœºåˆ¶ï¼Œä½¿ASRæ¨¡å‹èƒ½å¤ŸæŒ‰é¡ºåºå­¦ä¹ æ–°ä»»åŠ¡ï¼Œè€Œä¸ä¼šæ˜¾è‘—é™ä½æ—©æœŸä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨LJ Speechæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒå’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å¤§å¹…é™ä½é”™è¯¯ç‡çš„åŒæ—¶ï¼Œåœ¨ä¸åŒå™ªå£°æ¡ä»¶ä¸‹ä¿æŒé«˜æ€§èƒ½ã€‚æˆ‘ä»¬å±•ç¤ºäº†åŠç›‘ç£æœºå™¨è¯­éŸ³é“¾æ–¹æ³•åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ•ˆå’Œé«˜æ•ˆæŒç»­å­¦ä¹ çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18320v1">PDF</a> Published as a conference paper at O-COCOSDA 2024. 6 pages; 2 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæœºå™¨è¯­éŸ³é“¾æ¡†æ¶çš„åˆ©ç”¨æ¢¯åº¦ç»éªŒè®°å¿†ï¼ˆGEMï¼‰å®ç°è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æŒç»­å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç»„ä»¶ï¼Œæ”¯æŒGEMæ‰€éœ€çš„å›æ”¾æœºåˆ¶ï¼Œä½¿ASRæ¨¡å‹èƒ½å¤Ÿé¡ºåºå­¦ä¹ æ–°ä»»åŠ¡ï¼Œè€Œä¸ä¼šæ˜¾è‘—å½±å“æ—©æœŸä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨LJ Speechæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä¼ ç»Ÿçš„å¾®è°ƒå’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†é”™è¯¯ç‡çš„å¤§å¹…é™ä½ï¼Œå¹¶åœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹ä¿æŒäº†é«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºæœºå™¨è¯­éŸ³é“¾æ¡†æ¶çš„æ–°æ–¹æ³•ï¼Œå®ç°ASRç³»ç»Ÿçš„æŒç»­å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¼•å…¥TTSç»„ä»¶ï¼Œæ”¯æŒäº†å›æ”¾æœºåˆ¶ï¼Œè¿™æ˜¯GEMçš„å…³é”®éƒ¨åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸å½±å“æ—©æœŸä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œä½¿ASRæ¨¡å‹é¡ºåºå­¦ä¹ æ–°ä»»åŠ¡ã€‚</li>
<li>åœ¨LJ Speechæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†é”™è¯¯ç‡çš„å¤§å¹…é™ä½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹ä¿æŒäº†é«˜æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6537342d5857e0665f15da7f721d587b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d043e9a805c7b49bfa49a921a6792c17.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8890de26f456535ab2c5fb70fe1f6816.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4d62ba7c700244aa506b654aa215ed6d.jpg" align="middle">
</details>




<h2 id="SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation"><a href="#SALMONN-omni-A-Codec-free-LLM-for-Full-duplex-Speech-Understanding-and-Generation" class="headerlink" title="SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and   Generation"></a>SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and   Generation</h2><p><strong>Authors:Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang</strong></p>
<p>Full-duplex multimodal large language models (LLMs) provide a unified framework for addressing diverse speech understanding and generation tasks, enabling more natural and seamless human-machine conversations. Unlike traditional modularised conversational AI systems, which separate speech recognition, understanding, and text-to-speech generation into distinct components, multimodal LLMs operate as single end-to-end models. This streamlined design eliminates error propagation across components and fully leverages the rich non-verbal information embedded in input speech signals. We introduce SALMONN-omni, a codec-free, full-duplex speech understanding and generation model capable of simultaneously listening to its own generated speech and background sounds while speaking. To support this capability, we propose a novel duplex spoken dialogue framework incorporating a &#96;&#96;thinkingâ€™â€™ mechanism that facilitates asynchronous text and speech generation relying on embeddings instead of codecs (quantized speech and audio tokens). Experimental results demonstrate SALMONN-omniâ€™s versatility across a broad range of streaming speech tasks, including speech recognition, speech enhancement, and spoken question answering. Additionally, SALMONN-omni excels at managing turn-taking, barge-in, and echo cancellation scenarios, establishing its potential as a robust prototype for full-duplex conversational AI systems. To the best of our knowledge, SALMONN-omni is the first codec-free model of its kind. A full technical report along with model checkpoints will be released soon. </p>
<blockquote>
<p>å…¨åŒå·¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºå¤„ç†å„ç§è¯­éŸ³ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä½¿äººä¸æœºå™¨ä¹‹é—´çš„å¯¹è¯æ›´åŠ è‡ªç„¶å’Œæ— ç¼ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å—åŒ–å¯¹è¯AIç³»ç»Ÿä¸åŒï¼Œåè€…å°†è¯­éŸ³è¯†åˆ«ã€ç†è§£å’Œæ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆåˆ’åˆ†ä¸ºä¸åŒçš„ç»„ä»¶ï¼Œå¤šæ¨¡æ€LLMåˆ™ä½œä¸ºå•ä¸ªç«¯åˆ°ç«¯çš„æ¨¡å‹è¿›è¡Œæ“ä½œã€‚è¿™ç§ç®€åŒ–çš„è®¾è®¡æ¶ˆé™¤äº†ç»„ä»¶é—´é”™è¯¯ä¼ æ’­ï¼Œå¹¶å……åˆ†åˆ©ç”¨äº†è¾“å…¥è¯­éŸ³ä¿¡å·ä¸­åµŒå…¥çš„ä¸°å¯Œçš„éè¯­éŸ³ä¿¡æ¯ã€‚æˆ‘ä»¬ä»‹ç»äº†SALMONN-omniï¼Œè¿™æ˜¯ä¸€ç§æ— ç¼–è§£ç å™¨çš„å…¨åŒå·¥è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è¯´è¯çš„åŒæ—¶è†å¬è‡ªå·±ç”Ÿæˆçš„è¯­éŸ³å’ŒèƒŒæ™¯å£°éŸ³ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€åŠŸèƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŒå·¥å£è¯­å¯¹è¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸€ç§â€œæ€è€ƒâ€æœºåˆ¶ï¼Œèƒ½å¤Ÿä¾èµ–åµŒå…¥è€Œä¸æ˜¯ç¼–è§£ç å™¨å®ç°å¼‚æ­¥æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆï¼ˆé‡åŒ–è¯­éŸ³å’ŒéŸ³é¢‘æ ‡è®°ï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSALMONN-omniåœ¨å¹¿æ³›çš„æµå¼è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œå£è¯­é—®ç­”ã€‚æ­¤å¤–ï¼ŒSALMONN-omniåœ¨è½®æµå‘è¨€ã€æŠ¢è¯å’Œå›å£°æ¶ˆé™¤ç­‰åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶ä½œä¸ºå…¨åŒå·¥å¯¹è¯AIç³»ç»Ÿçš„ç¨³å¥åŸå‹çš„æ½œåŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSALMONN-omniæ˜¯é¦–ä¸ªæ— ç¼–è§£ç å™¨çš„æ­¤ç±»æ¨¡å‹ã€‚æˆ‘ä»¬å°†å¾ˆå¿«å‘å¸ƒå®Œæ•´çš„æŠ€æœ¯æŠ¥å‘Šå’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18138v1">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¨åŒå·¥æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºå¤„ç†å¤šæ ·çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œå®ç°äº†æ›´è‡ªç„¶ã€æ— ç¼çš„äººæœºå¯¹è¯ã€‚SALMONN-omniæ˜¯ä¸€ä¸ªæ— ç¼–è§£ç å™¨ã€å…¨åŒå·¥è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶è†å¬å…¶è‡ªèº«ç”Ÿæˆçš„è¯­éŸ³å’ŒèƒŒæ™¯å£°éŸ³ï¼Œå¹¶å¯åœ¨è¯´è¯çš„åŒæ—¶è¿›è¡Œæ€è€ƒã€‚è¯¥æ¨¡å‹å¼•å…¥äº†æ–°å‹çš„åŒå·¥å¯¹è¯æ¡†æ¶ï¼Œå€ŸåŠ©åµŒå…¥æŠ€æœ¯å®ç°å¼‚æ­¥æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆï¼Œæ— éœ€ç¼–è§£ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSALMONN-omniåœ¨å¤šç§æµå¼è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³é—®ç­”ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†å¯¹è¯ä¸­çš„è½®æ›¿å‘è¨€ã€å³æ—¶ä»‹å…¥å’Œå›å£°æ¶ˆé™¤ç­‰åœºæ™¯æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œæˆä¸ºäº†å…¨åŒå·¥å¯¹è¯AIç³»ç»Ÿçš„ç¨³å¥åŸå‹ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSALMONN-omniæ˜¯é¦–ä¸ªæ— éœ€ç¼–è§£ç å™¨çš„æ­¤ç±»æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨åŒå·¥æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ç”¨äºå¤šæ ·çš„è¯­éŸ³ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>SALMONN-omniæ¨¡å‹å¯ä»¥æ— ç¼–è§£ç å™¨åœ°è¿›è¡Œè¯­éŸ³ç”Ÿæˆå’Œç†è§£ï¼Œå®ç°å…¨åŒå·¥å¯¹è¯ã€‚</li>
<li>SALMONN-omniå¯ä»¥åŒæ—¶è†å¬è‡ªèº«ç”Ÿæˆçš„è¯­éŸ³å’ŒèƒŒæ™¯å£°éŸ³ï¼Œæå‡è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†æ–°å‹çš„åŒå·¥å¯¹è¯æ¡†æ¶ï¼Œå€ŸåŠ©åµŒå…¥æŠ€æœ¯å®ç°å¼‚æ­¥æ–‡æœ¬å’Œè¯­éŸ³ç”Ÿæˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSALMONN-omniåœ¨å¤šç§æµå¼è¯­éŸ³ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SALMONN-omniåœ¨å¤„ç†å¯¹è¯ä¸­çš„è½®æ›¿å‘è¨€ã€å³æ—¶ä»‹å…¥å’Œå›å£°æ¶ˆé™¤ç­‰åœºæ™¯å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-af83771b0fcf83031a32e1732ec9e749.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-380af3e54978f0e064a9d5c87187adef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-da3211095949e31aeb53708715de1dd2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e4211a8ca2e98b1f4a9df0ccfd0954b3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-59aaeeb020314e6bc7f9d8676410efd0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-abcad34c82c8a99ad0b9dba70679e5b4.jpg" align="middle">
</details>




<h2 id="Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis"><a href="#Visatronic-A-Multimodal-Decoder-Only-Model-for-Speech-Synthesis" class="headerlink" title="Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis"></a>Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis</h2><p><strong>Authors:Akshita Gupta, Tatiana Likhomanenko, Karren Dai Yang, Richard He Bai, Zakaria Aldeneh, Navdeep Jaitly</strong></p>
<p>In this paper, we propose a new task â€“ generating speech from videos of people and their transcripts (VTTS) â€“ to motivate new techniques for multimodal speech generation. This task generalizes the task of generating speech from cropped lip videos, and is also more complicated than the task of generating generic audio clips (e.g., dog barking) from videos and text. Multilingual versions of the task could lead to new techniques for cross-lingual dubbing. We also present a decoder-only multimodal model for this task, which we call Visatronic. This model embeds vision, text and speech directly into the common subspace of a transformer model and uses an autoregressive loss to learn a generative model of discretized mel-spectrograms conditioned on speaker videos and transcripts of their speech. By embedding all modalities into a common subspace, Visatronic can achieve improved results over models that use only text or video as input. Further, it presents a much simpler approach for multimodal speech generation compared to prevailing approaches which rely on lip-detectors and complicated architectures to fuse modalities while producing better results. Since the model is flexible enough to accommodate different ways of ordering inputs as a sequence, we carefully explore different strategies to better understand the best way to propagate information to the generative steps. To facilitate further research on VTTS, we will release (i) our code, (ii) clean transcriptions for the large-scale VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS incorporating both objective and subjective metrics. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡â€”â€”ä»äººç‰©è§†é¢‘åŠå…¶å­—å¹•ç”Ÿæˆè¯­éŸ³ï¼ˆVideo-based Text-to-Speechï¼ŒVTTSï¼‰ï¼Œä»¥æ¿€åŠ±å¤šæ¨¡æ€è¯­éŸ³ç”Ÿæˆçš„æ–°æŠ€æœ¯ã€‚è¿™ä¸ªä»»åŠ¡æ³›åŒ–äº†ä»è£å‰ªçš„å”‡è¯­è§†é¢‘ä¸­ç”Ÿæˆè¯­éŸ³çš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ¯”ä»è§†é¢‘å’Œæ–‡æœ¬ä¸­ç”Ÿæˆé€šç”¨éŸ³é¢‘ç‰‡æ®µï¼ˆä¾‹å¦‚ç‹—å«ï¼‰çš„ä»»åŠ¡æ›´åŠ å¤æ‚ã€‚è¯¥ä»»åŠ¡çš„å¤šè¯­è¨€ç‰ˆæœ¬å¯èƒ½ä¼šå¯¼è‡´è·¨è¯­è¨€é…éŸ³çš„æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬è¿˜ä¸ºæ­¤ä»»åŠ¡æå‡ºäº†ä¸€ç§ä»…è§£ç å™¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºVisatronicã€‚æ­¤æ¨¡å‹å°†è§†è§‰ã€æ–‡æœ¬å’Œè¯­éŸ³ç›´æ¥åµŒå…¥åˆ°å˜å‹å™¨çš„å…¬å…±å­ç©ºé—´ä¸­ï¼Œå¹¶ä½¿ç”¨è‡ªå›å½’æŸå¤±æ¥å­¦ä¹ åŸºäºè¯´è¯äººè§†é¢‘å’Œä»–ä»¬çš„è¯­éŸ³å­—å¹•çš„ç¦»æ•£æ¢…å°”é¢‘è°±å›¾çš„ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å°†æ‰€æœ‰æ¨¡æ€åµŒå…¥å…¬å…±å­ç©ºé—´ä¸­ï¼ŒVisatronicå¯ä»¥å®ç°åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æˆ–è§†é¢‘ä½œä¸ºè¾“å…¥çš„æ¨¡å‹ä¸Šè·å¾—æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„ä¾èµ–äºå”‡è¯­æ£€æµ‹å™¨å’Œå¤æ‚çš„æ¶æ„æ¥èåˆæ¨¡æ€çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´ç®€å•çš„æ–¹æ³•æ¥å®ç°å¤šæ¨¡æ€è¯­éŸ³ç”ŸæˆåŒæ—¶äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚ç”±äºè¯¥æ¨¡å‹è¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥å®¹çº³ä¸åŒçš„è¾“å…¥åºåˆ—é¡ºåºæ–¹å¼ï¼Œå› æ­¤æˆ‘ä»¬ä»”ç»†æ¢ç´¢äº†ä¸åŒçš„ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°äº†è§£å¦‚ä½•å°†ä¿¡æ¯ä¼ æ’­åˆ°ç”Ÿæˆæ­¥éª¤çš„æœ€ä½³æ–¹å¼ã€‚ä¸ºäº†ä¿ƒè¿›å¯¹VTTSçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å‘å¸ƒï¼ˆiï¼‰æˆ‘ä»¬çš„ä»£ç ï¼Œï¼ˆiiï¼‰å¤§è§„æ¨¡VoxCeleb2æ•°æ®é›†çš„å¹²å‡€è½¬å½•ï¼Œï¼ˆiiiï¼‰åŒ…å«å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡çš„VTTSæ ‡å‡†åŒ–è¯„ä¼°åè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17690v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”ä»äººçš„è§†é¢‘å’Œä»–ä»¬çš„æ–‡å­—è„šæœ¬ç”Ÿæˆè¯­éŸ³ï¼ˆVTTSï¼‰ï¼Œä»¥æ¿€å‘å¤šæ¨¡æ€è¯­éŸ³ç”Ÿæˆçš„æ–°æŠ€æœ¯ã€‚æ­¤ä»»åŠ¡ä¸ä»…æ¶µç›–äº†ä»è£å‰ªçš„å”‡éƒ¨è§†é¢‘ç”Ÿæˆè¯­éŸ³çš„ä»»åŠ¡ï¼Œè€Œä¸”ç›¸è¾ƒäºä»è§†é¢‘å’Œæ–‡å­—ç”Ÿæˆé€šç”¨éŸ³é¢‘å‰ªè¾‘ï¼ˆä¾‹å¦‚ç‹—å«ï¼‰çš„ä»»åŠ¡æ›´ä¸ºå¤æ‚ã€‚å¤šè¯­è¨€ç‰ˆæœ¬çš„ä»»åŠ¡å¯èƒ½ä¼šå¯¼è‡´è·¨è¯­è¨€é…éŸ³çš„æ–°æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæ–‡ä¸­è¿˜ä»‹ç»äº†ä¸€ç§é’ˆå¯¹è¯¥ä»»åŠ¡çš„ä»…è§£ç å™¨å¤šæ¨¡æ€æ¨¡å‹Visatronicã€‚å®ƒå°†è§†è§‰ã€æ–‡æœ¬å’Œè¯­éŸ³ç›´æ¥åµŒå…¥åˆ°transformeræ¨¡å‹çš„å…¬å…±å­ç©ºé—´ä¸­ï¼Œå¹¶ä½¿ç”¨è‡ªå›å½’æŸå¤±æ¥å­¦ä¹ ä»¥å‘è¨€äººè§†é¢‘å’Œä»–ä»¬çš„è¯­éŸ³æ–‡æœ¬ä¸ºæ¡ä»¶çš„ç¦»æ•£æ¢…å°”é¢‘è°±å›¾çš„ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡å°†æ‰€æœ‰æ¨¡æ€åµŒå…¥å…¬å…±å­ç©ºé—´ï¼ŒVisatronicå¯ä»¥åœ¨ä»…ä½¿ç”¨æ–‡æœ¬æˆ–è§†é¢‘ä½œä¸ºè¾“å…¥çš„æ¨¡å‹ä¸Šå®ç°æ›´å¥½çš„ç»“æœã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºä¾èµ–å”‡æ£€æµ‹å™¨å’Œå¤æ‚æ¶æ„æ¥èåˆæ¨¡æ€çš„ä¸»æµæ–¹æ³•ï¼Œå®ƒä¸ºå¤šæ¨¡æ€è¯­éŸ³ç”Ÿæˆæä¾›äº†ä¸€ç§æ›´ä¸ºç®€å•çš„æ–¹æ³•ã€‚æœ€åï¼Œä¸ºä¾¿äºè¿›ä¸€æ­¥è¿›è¡ŒVTTSç ”ç©¶ï¼Œå°†å…¬å¼€ä»£ç ã€å¤§è§„æ¨¡VoxCeleb2æ•°æ®é›†çš„æ ‡å‡†è½¬å½•ä»¥åŠVTTSçš„æ ‡å‡†è¯„ä¼°åè®®ï¼Œè¯¥åè®®åŒ…æ‹¬å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡â€”â€”ä»äººçš„è§†é¢‘å’Œä»–ä»¬çš„æ–‡å­—è„šæœ¬ç”Ÿæˆè¯­éŸ³ï¼ˆVTTSï¼‰ï¼Œæ—¨åœ¨æ¨åŠ¨å¤šæ¨¡æ€è¯­éŸ³ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºVisatronicçš„ä»…è§£ç å™¨å¤šæ¨¡æ€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è§†è§‰ã€æ–‡æœ¬å’Œè¯­éŸ³åµŒå…¥åˆ°å…¬å…±å­ç©ºé—´ä¸­ï¼Œå¹¶åŸºäºå‘è¨€äººè§†é¢‘å’Œè¯­éŸ³æ–‡æœ¬è¿›è¡Œç”Ÿæˆå­¦ä¹ ã€‚</li>
<li>Visatronicåœ¨èåˆå¤šç§è¾“å…¥æ¨¡æ€æ—¶å–å¾—äº†è‰¯å¥½æ•ˆæœï¼Œä¼˜äºä»…ä½¿ç”¨æ–‡æœ¬æˆ–è§†é¢‘çš„æ¨¡å‹ã€‚</li>
<li>Visatronicä¸ºå¤šæ¨¡æ€è¯­éŸ³ç”Ÿæˆæä¾›äº†ä¸€ç§ç›¸å¯¹ç®€å•çš„æ–¹æ³•ï¼Œä¸ä¸»æµæ–¹æ³•ç›¸æ¯”ï¼Œæ— éœ€ä¾èµ–å¤æ‚çš„å”‡æ£€æµ‹å™¨å’Œæ¶æ„ã€‚</li>
<li>æ¨¡å‹å…·æœ‰çµæ´»æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„è¾“å…¥åºåˆ—é¡ºåºç­–ç•¥ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•ä¼ æ’­ä¿¡æ¯åˆ°ç”Ÿæˆæ­¥éª¤ã€‚</li>
<li>ä¸ºäº†æ¨åŠ¨VTTSçš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå…¬å¼€äº†ä»£ç ã€å¤§è§„æ¨¡VoxCeleb2æ•°æ®é›†çš„æ ‡å‡†è½¬å½•ä»¥åŠVTTSçš„æ ‡å‡†è¯„ä¼°åè®®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-86e176bb75f7cdeb0fea130e9e678024.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f9f91afbb7c194282ebeaef09e8afd7a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a599e0d812fed874672b602f3fc5484d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9c099e33ca4f1e5a7932d23c0948e6f5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f209cd86f47fe75db76e773d0560feab.jpg" align="middle">
</details>




<h2 id="VQalAttent-a-Transparent-Speech-Generation-Pipeline-based-on-Transformer-learned-VQ-VAE-Latent-Space"><a href="#VQalAttent-a-Transparent-Speech-Generation-Pipeline-based-on-Transformer-learned-VQ-VAE-Latent-Space" class="headerlink" title="VQalAttent: a Transparent Speech Generation Pipeline based on   Transformer-learned VQ-VAE Latent Space"></a>VQalAttent: a Transparent Speech Generation Pipeline based on   Transformer-learned VQ-VAE Latent Space</h2><p><strong>Authors:Armani Rodriguez, Silvija Kokalj-Filipovic</strong></p>
<p>Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttentâ€™s capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models. </p>
<blockquote>
<p>ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³çš„æ•ˆç‡ä»ç„¶æ˜¯è¯­éŸ³åˆæˆä¸­ç”Ÿæˆæ¨¡å‹çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†VQalAttentï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¨¡å‹ï¼Œæ—¨åœ¨ä»¥å¯è°ƒçš„æ€§èƒ½å’Œå¯è§£é‡Šæ€§ç”Ÿæˆè™šå‡çš„è¯­éŸ³ã€‚æˆ‘ä»¬åˆ©ç”¨ç”±äººæ‰‹å‘éŸ³çš„åè¿›åˆ¶æ•°å­—ï¼ˆ0-9ï¼‰ç»„æˆçš„AudioMNISTæ•°æ®é›†ï¼Œé‡‡ç”¨ä¸¤æ­¥æ¶æ„ï¼šé¦–å…ˆï¼Œä¸€ä¸ªå¯æ‰©å±•çš„å‘é‡é‡åŒ–è‡ªç¼–ç å™¨ï¼ˆVQ-VAEï¼‰ï¼Œå®ƒå°†éŸ³é¢‘è°±å›¾å‹ç¼©æˆç¦»æ½œåœ¨è¡¨ç¤ºï¼›å…¶æ¬¡ï¼Œä»…è§£ç å™¨ç«¯çš„å˜å‹å™¨ï¼Œå­¦ä¹ è¿™äº›æ½œåœ¨æ¦‚ç‡æ¨¡å‹ã€‚è®­ç»ƒè¿‡çš„å˜å‹å™¨ç”Ÿæˆç±»ä¼¼çš„æ½œåœ¨åºåˆ—ï¼Œå¯ä»¥é€šè¿‡VQ-VAEè§£ç å™¨è½¬æ¢ä¸ºéŸ³é¢‘è°±å›¾ï¼Œä»è€Œç”Ÿæˆè™šå‡çš„è¯­éŸ³ã€‚æ ¹æ®æ½œåœ¨ç©ºé—´çš„ç»´åº¦å’Œå¤–åœ¨ä¿¡æ¯æ¥è§£é‡Šè™šå‡è¯­éŸ³çš„ç»Ÿè®¡å’Œæ„ŸçŸ¥è´¨é‡ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤§çš„å•†ä¸šç”Ÿæˆæ¨¡å‹ä¸­å®ç°æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›ã€‚ä½œä¸ºç†è§£å’Œæ”¹è¿›éŸ³é¢‘åˆæˆçš„æœ‰ä»·å€¼çš„å·¥å…·ï¼Œæˆ‘ä»¬çš„ç»“æœè¯æ˜äº†VQalAttentåœ¨æœ‰é™è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹ç”Ÿæˆå¯ç†è§£çš„è¯­éŸ³æ ·æœ¬çš„èƒ½åŠ›ï¼Œè€ŒåŸ¹è®­ç®¡é“æ¨¡å—åŒ–å’Œé€æ˜åº¦çš„ç‰¹ç‚¹æœ‰åŠ©äºè½»æ¾åœ°å°†åˆ†æä¸æ¨¡å—åŒ–ä¿®æ”¹ç›¸å…³è”ï¼Œä»è€Œä¸ºæ›´å¤æ‚çš„æ¨¡å‹æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14642v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºVQalAttentçš„è½»é‡çº§æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ã€‚è¯¥æ¨¡å‹åˆ©ç”¨AudioMNISTæ•°æ®é›†ï¼Œé‡‡ç”¨ä¸¤æ­¥æ¶æ„ï¼šé¦–å…ˆä½¿ç”¨å¯ä¼¸ç¼©å‘é‡é‡åŒ–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVQ-VAEï¼‰å°†éŸ³é¢‘å…‰è°±å›¾å‹ç¼©æˆç¦»æ•£æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ä»…è§£ç å™¨è½¬æ¢å™¨å­¦ä¹ è¿™äº›æ½œåœ¨æ¦‚ç‡æ¨¡å‹ã€‚é€šè¿‡è®­ç»ƒï¼Œç”Ÿæˆç±»ä¼¼æ½œåœ¨åºåˆ—çš„å‡å‘å£°ï¼Œå¯é€šè¿‡VQ-VAEè§£ç å™¨è½¬æ¢ä¸ºéŸ³é¢‘å…‰è°±å›¾ã€‚æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ•ˆç‡ä½¿å…¶æˆä¸ºå¤§å‹å•†ä¸šç”Ÿæˆæ¨¡å‹çš„æœ‰ä»·å€¼çš„å·¥å…·ï¼Œå¯ç”Ÿæˆå…·æœ‰æœ‰é™è®¡ç®—èµ„æºçš„å¯ç†è§£è¯­éŸ³æ ·æœ¬ã€‚å…¶æ¨¡å—åŒ–è®­ç»ƒç®¡é“æœ‰åŠ©äºè½»æ¾å…³è”åˆ†æå¹¶æ¨¡å—åŒ–æ”¹è¿›ï¼Œä¸ºæ›´å¤æ‚çš„æ¨¡å‹æä¾›è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VQalAttentæ¨¡å‹æ—¨åœ¨è§£å†³è¯­éŸ³åˆæˆä¸­ç”Ÿæˆé«˜è´¨é‡è¯­éŸ³çš„æ•ˆç‡é—®é¢˜ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨AudioMNISTæ•°æ®é›†å¹¶é‡‡ç”¨ä¸¤æ­¥æ¶æ„ï¼šVQ-VAEå‹ç¼©éŸ³é¢‘å…‰è°±å›¾åˆ°ç¦»æ•£æ½œåœ¨è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨è§£ç å™¨è½¬æ¢å™¨è¿›è¡Œæ¦‚ç‡å»ºæ¨¡ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆç±»ä¼¼æ½œåœ¨åºåˆ—çš„å‡å‘å£°ï¼Œå†è½¬æ¢æˆéŸ³é¢‘å…‰è°±å›¾ç”Ÿæˆå‡å‘å£°ã€‚</li>
<li>VQalAttentå…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼Œæœ‰åŠ©äºç†è§£å¹¶æ”¹è¿›éŸ³é¢‘åˆæˆè¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹å…·æœ‰æœ‰é™çš„è®¡ç®—èµ„æºç”Ÿæˆå¯ç†è§£è¯­éŸ³æ ·æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>VQalAttentçš„æ¨¡å—åŒ–è®­ç»ƒç®¡é“æœ‰åŠ©äºä¸æ¨¡å—åŒ–ä¿®æ”¹è½»æ¾å…³è”åˆ†æï¼Œä¸ºå¤æ‚æ¨¡å‹æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5aeda9ac5677e5491dad6f04581ac07b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a3059f43b337fc6d4aec8afd12305cac.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f8a2ebd3e456f6cff3ebcfe6b6a40a76.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-65719e1e9058c005b23af7d54dedd05a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c66b54627b673b5daabdfb7c4901a7c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6bc55b9d18a1d672c78598d153b12834.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-437cd8ff24f7bb4756c5187bc2825b98.jpg" align="middle">
</details>




<h2 id="WavChat-A-Survey-of-Spoken-Dialogue-Models"><a href="#WavChat-A-Survey-of-Spoken-Dialogue-Models" class="headerlink" title="WavChat: A Survey of Spoken Dialogue Models"></a>WavChat: A Survey of Spoken Dialogue Models</h2><p><strong>Authors:Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, Xiaoda Yang, Zehan Wang, Qian Yang, Jian Li, Yidi Jiang, Jingzhen He, Yunfei Chu, Jin Xu, Zhou Zhao</strong></p>
<p>Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at <a target="_blank" rel="noopener" href="https://github.com/jishengpeng/WavChat">https://github.com/jishengpeng/WavChat</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä»¥GPT-4oç­‰ç³»ç»Ÿä¸ºä»£è¡¨ï¼Œå£è¯­å¯¹è¯æ¨¡å‹çš„å‘å±•åœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚ä¸ä¼ ç»Ÿçš„ä¸‰çº§çº§è”å£è¯­å¯¹è¯æ¨¡å‹ç›¸æ¯”ï¼Œç°ä»£å£è¯­å¯¹è¯æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„æ™ºèƒ½æ°´å¹³ï¼Œè¿™äº›æ¨¡å‹åŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ã€‚å…ˆè¿›çš„å£è¯­å¯¹è¯æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£éŸ³é¢‘ã€éŸ³ä¹å’Œå…¶ä»–è¯­éŸ³ç›¸å…³ç‰¹å¾ï¼Œè¿˜èƒ½æ•æ‰è¯­éŸ³ä¸­çš„é£æ ¼å’ŒéŸ³è‰²ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå®ƒä»¬èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å¤šè½®è¯­éŸ³å“åº”ï¼Œå…·æœ‰ä½å»¶è¿Ÿæ€§ï¼Œé€šè¿‡åŒæ—¶çš„å¬å’Œè¯´èƒ½åŠ›å®ç°å®æ—¶äº¤äº’ã€‚å°½ç®¡å£è¯­å¯¹è¯ç³»ç»Ÿçš„è¿›å±•æ˜¾è‘—ï¼Œä½†ç¼ºä¹ç³»ç»Ÿç»„ç»‡å’Œåˆ†æè¿™äº›ç³»ç»Ÿå’Œåº•å±‚æŠ€æœ¯çš„å…¨é¢ç»¼è¿°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæŒ‰æ—¶é—´é¡ºåºæ•´ç†äº†ç°æœ‰çš„å£è¯­å¯¹è¯ç³»ç»Ÿï¼Œå¹¶å°†å…¶åˆ†ä¸ºçº§è”å’Œç«¯åˆ°ç«¯èŒƒå¼ã€‚ç„¶åï¼Œæˆ‘ä»¬æ·±å…¥æ¦‚è¿°äº†å£è¯­å¯¹è¯æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¶µç›–äº†è¯­éŸ³è¡¨ç¤ºã€è®­ç»ƒèŒƒå¼ã€æµåª’ä½“ã€åŒå‘å¯¹è¯å’Œäº¤äº’èƒ½åŠ›ç­‰æ–¹é¢ã€‚æ¯ä¸ªéƒ¨åˆ†éƒ½è®¨è®ºäº†è¿™äº›æŠ€æœ¯çš„å±€é™æ€§ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„è€ƒè™‘å› ç´ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»è®­ç»ƒå’Œè¯„ä¼°å£è¯­å¯¹è¯ç³»ç»Ÿçš„è§’åº¦ï¼Œå¯¹ç›¸å…³æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•è¿›è¡Œäº†å…¨é¢çš„å›é¡¾ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä»½ç»¼è¿°èƒ½ä¸ºå£è¯­å¯¹è¯ç³»ç»Ÿçš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨åšå‡ºè´¡çŒ®ã€‚ç›¸å…³èµ„æ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jishengpeng/WavChat">https://github.com/jishengpeng/WavChat</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13577v2">PDF</a> 60 papes, working in progress</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå¦‚GPT-4oç­‰å…ˆè¿›å¯¹è¯æ¨¡å‹åœ¨è¯­éŸ³é¢†åŸŸå¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ä¸ä¼ ç»Ÿä¸‰çº§çº§è”å¯¹è¯æ¨¡å‹ç›¸æ¯”ï¼Œç°ä»£å¯¹è¯æ¨¡å‹å±•ç°å‡ºæ›´é«˜æ™ºèƒ½ï¼Œä¸ä»…èƒ½ç†è§£éŸ³é¢‘ã€éŸ³ä¹ç­‰è¯­éŸ³ç›¸å…³ç‰¹å¾ï¼Œè¿˜èƒ½æ•æ‰è¯­éŸ³çš„é£æ ¼å’ŒéŸ³è‰²ç‰¹ç‚¹ã€‚å®ƒä»¬èƒ½ç”Ÿæˆé«˜è´¨é‡ã€å¤šè½®æ¬¡çš„è¯­éŸ³å›åº”ï¼Œå®ç°å®æ—¶äº’åŠ¨ã€‚ç„¶è€Œï¼Œç¼ºä¹ç³»ç»Ÿåˆ†æå’Œæ€»ç»“è¿™äº›ç³»ç»Ÿå’Œåº•å±‚æŠ€æœ¯çš„ç»¼è¿°æ–‡ç« ã€‚æœ¬æ–‡é¦–å…ˆæŒ‰æ—¶é—´é¡ºåºæ•´ç†ç°æœ‰å¯¹è¯ç³»ç»Ÿï¼Œå°†å…¶åˆ†ä¸ºçº§è”å’Œç«¯åˆ°ç«¯èŒƒå¼ã€‚æ¥ç€æ·±å…¥æ¦‚è¿°å¯¹è¯æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå¦‚è¯­éŸ³è¡¨ç¤ºã€è®­ç»ƒèŒƒå¼ã€æµå¼å¤„ç†ã€åŒå‘å¯¹è¯å’Œäº¤äº’èƒ½åŠ›ç­‰ã€‚åŒæ—¶ï¼Œè®¨è®ºå„æŠ€æœ¯çš„å±€é™æ€§å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¹¶ä»è®­ç»ƒå’Œè¯„ä¼°å¯¹è¯ç³»ç»Ÿçš„è§’åº¦å…¨é¢å›é¡¾ç›¸å…³æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæ¨è¿›å¯¹è¯ç³»ç»Ÿçš„å‘å±•åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å¯¹è¯æ¨¡å‹å¦‚GPT-4oå±•ç°å‡ºé«˜æ™ºèƒ½ï¼Œèƒ½ç†è§£å¤šç§è¯­éŸ³ç›¸å…³ç‰¹å¾å¹¶æ•æ‰è¯­éŸ³é£æ ¼åŠéŸ³è‰²ã€‚<br>2.ä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”ï¼Œå…ˆè¿›å¯¹è¯æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡ã€å¤šè½®æ¬¡çš„è¯­éŸ³å›åº”ï¼Œå®ç°å®æ—¶äº’åŠ¨ã€‚</li>
<li>å½“å‰ç¼ºä¹ç³»ç»Ÿåˆ†æå’Œæ€»ç»“ç°ä»£å¯¹è¯ç³»ç»Ÿå’Œåº•å±‚æŠ€æœ¯çš„ç»¼è¿°æ–‡ç« ã€‚</li>
<li>æœ¬æ–‡æŒ‰æ—¶é—´é¡ºåºæ•´ç†å¯¹è¯ç³»ç»Ÿï¼Œå¹¶æ·±å…¥è®¨è®ºå…¶æ ¸å¿ƒæŠ€æœ¯çš„å±€é™æ€§å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
<li>æ–‡ç« æ¦‚è¿°äº†è¯­éŸ³è¡¨ç¤ºã€è®­ç»ƒèŒƒå¼ã€æµå¼å¤„ç†ç­‰æ–¹é¢çš„æŠ€æœ¯ç»†èŠ‚ã€‚</li>
<li>ä»è®­ç»ƒå’Œè¯„ä¼°è§’åº¦å…¨é¢å›é¡¾ç›¸å…³æ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-24311c930f434a24b73a7089966a7771.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7b1a7d15d280d8147e1ad804ca6acd7e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-84ca884eb29428e79b1af023706dc177.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8fbf6065f595ad151c6f1ff724d1c45b.jpg" align="middle">
</details>




<h2 id="I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception"><a href="#I2TTS-Image-indicated-Immersive-Text-to-speech-Synthesis-with-Spatial-Perception" class="headerlink" title="I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception"></a>I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial   Perception</h2><p><strong>Authors:Jiawei Zhang, Tian-Hao Zhang, Jun Wang, Jiaran Gao, Xinyuan Qian, Xu-Cheng Yin</strong></p>
<p>Controlling the style and characteristics of speech synthesis is crucial for adapting the output to specific contexts and user requirements. Previous Text-to-speech (TTS) works have focused primarily on the technical aspects of producing natural-sounding speech, such as intonation, rhythm, and clarity. However, they overlook the fact that there is a growing emphasis on spatial perception of synthesized speech, which may provide immersive experience in gaming and virtual reality. To solve this issue, in this paper, we present a novel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech Synthesis (I2TTS). Specifically, we introduce a scene prompt encoder that integrates visual scene prompts directly into the synthesis pipeline to control the speech generation process. Additionally, we propose a reverberation classification and refinement technique that adjusts the synthesized mel-spectrogram to enhance the immersive experience, ensuring that the involved reverberation condition matches the scene accurately. Experimental results demonstrate that our model achieves high-quality scene and spatial matching without compromising speech naturalness, marking a significant advancement in the field of context-aware speech synthesis. Project demo page: <a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> Index Terms-Speech synthesis, scene prompt, spatial perception </p>
<blockquote>
<p>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹æ€§å¯¹äºé€‚åº”ç‰¹å®šçš„ä¸Šä¸‹æ–‡å’Œç”¨æˆ·è¦æ±‚è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨äº§ç”Ÿè‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œä»–ä»¬å¿½ç•¥äº†ä¸€ä¸ªäº‹å®ï¼Œé‚£å°±æ˜¯å¯¹äºåˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥çš„é‡è§†ç¨‹åº¦æ­£åœ¨å¢é•¿ï¼Œè¿™å¯èƒ½åœ¨æ¸¸æˆå’Œè™šæ‹Ÿç°å®æä¾›æ²‰æµ¸å¼ä½“éªŒã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼TTSæ–¹æ³•ï¼Œå³å›¾åƒæŒ‡ç¤ºæ²‰æµ¸å¼æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆï¼ˆI2TTSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå®ƒå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œä»¥æ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è°ƒèŠ‚åˆæˆçš„æ¢…å°”é¢‘è°±å›¾ä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ‰€æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸å½±å“è¯­éŸ³è‡ªç„¶æ€§çš„æƒ…å†µä¸‹å®ç°äº†é«˜è´¨é‡çš„åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œæ ‡å¿—ç€ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¯­éŸ³åˆæˆé¢†åŸŸçš„ä¸€é¡¹é‡å¤§è¿›å±•ã€‚é¡¹ç›®æ¼”ç¤ºé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://spatialtts.github.io/">https://spatialTTS.github.io/</a> ç´¢å¼•æœ¯è¯­-è¯­éŸ³åˆæˆã€åœºæ™¯æç¤ºã€ç©ºé—´æ„ŸçŸ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13314v2">PDF</a> The paper is missing some information</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºI2TTSçš„æ–°å‹å¤šæ¨¡æ€TTSæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨å°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°åˆæˆç®¡é“ä¸­ï¼Œæ§åˆ¶è¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚åŒæ—¶æå‡ºä¸€ç§æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œè°ƒæ•´åˆæˆmelé¢‘è°±å›¾ä»¥å¢å¼ºæ²‰æµ¸å¼ä½“éªŒï¼Œç¡®ä¿æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚è¯¥æ–¹æ³•å®ç°äº†é«˜è´¨é‡åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼Œä¸æŸå¤±è¯­éŸ³è‡ªç„¶æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ§åˆ¶è¯­éŸ³åˆæˆçš„é£æ ¼å’Œç‰¹ç‚¹æ˜¯é€‚åº”ç‰¹å®šä¸Šä¸‹æ–‡å’Œç”¨æˆ·éœ€æ±‚çš„å…³é”®ã€‚</li>
<li>ä¼ ç»Ÿçš„TTSæŠ€æœ¯ä¸»è¦å…³æ³¨è‡ªç„¶è¯­éŸ³çš„æŠ€æœ¯æ–¹é¢ï¼Œå¦‚è¯­è°ƒã€èŠ‚å¥å’Œæ¸…æ™°åº¦ã€‚</li>
<li>ç›®å‰å¯¹åˆæˆè¯­éŸ³çš„ç©ºé—´æ„ŸçŸ¥æœ‰è¶Šæ¥è¶Šå¼ºçš„å…³æ³¨ï¼Œè¿™åœ¨æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç­‰é¢†åŸŸæä¾›æ²‰æµ¸å¼ä½“éªŒæ–¹é¢éå¸¸é‡è¦ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€TTSæ–¹æ³•I2TTSï¼Œå¼•å…¥åœºæ™¯æç¤ºç¼–ç å™¨ï¼Œå°†è§†è§‰åœºæ™¯æç¤ºç›´æ¥é›†æˆåˆ°è¯­éŸ³åˆæˆè¿‡ç¨‹ä¸­ã€‚</li>
<li>I2TTSèƒ½æé«˜åˆæˆè¯­éŸ³çš„æ²‰æµ¸å¼ä½“éªŒï¼Œé€šè¿‡è°ƒæ•´melé¢‘è°±å›¾å®ç°é«˜è´¨é‡åœºæ™¯å’Œç©ºé—´åŒ¹é…ï¼ŒåŒæ—¶ä¿æŒè¯­éŸ³çš„è‡ªç„¶æ€§ã€‚</li>
<li>è®ºæ–‡è¿˜ä»‹ç»äº†æ··å“åˆ†ç±»å’Œç»†åŒ–æŠ€æœ¯ï¼Œç¡®ä¿æ¶‰åŠçš„æ··å“æ¡ä»¶ä¸åœºæ™¯å‡†ç¡®åŒ¹é…ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-66e6ccdd517a50e9ee5dddf9637b47e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f4689ec2cdbacd48202667ffe499ad66.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-cf7fa3b4564f4f1f3a720aedb6cba728.jpg" align="middle">
</details>




<h2 id="Hard-Synth-Synthesizing-Diverse-Hard-Samples-for-ASR-using-Zero-Shot-TTS-and-LLM"><a href="#Hard-Synth-Synthesizing-Diverse-Hard-Samples-for-ASR-using-Zero-Shot-TTS-and-LLM" class="headerlink" title="Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot   TTS and LLM"></a>Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot   TTS and LLM</h2><p><strong>Authors:Jiawei Yu, Yuang Li, Xiaosong Qiao, Huan Zhao, Xiaofeng Zhao, Wei Tang, Min Zhang, Hao Yang, Jinsong Su</strong></p>
<p>Text-to-speech (TTS) models have been widely adopted to enhance automatic speech recognition (ASR) systems using text-only corpora, thereby reducing the cost of labeling real speech data. Existing research primarily utilizes additional text data and predefined speech styles supported by TTS models. In this paper, we propose Hard-Synth, a novel ASR data augmentation method that leverages large language models (LLMs) and advanced zero-shot TTS. Our approach employs LLMs to generate diverse in-domain text through rewriting, without relying on additional text data. Rather than using predefined speech styles, we introduce a hard prompt selection method with zero-shot TTS to clone speech styles that the ASR model finds challenging to recognize. Experiments demonstrate that Hard-Synth significantly enhances the Conformer model, achieving relative word error rate (WER) reductions of 6.5%&#x2F;4.4% on LibriSpeech dev&#x2F;test-other subsets. Additionally, we show that Hard-Synth is data-efficient and capable of reducing bias in ASR. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨äºä»…ä½¿ç”¨æ–‡æœ¬è¯­æ–™åº“å¢å¼ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿï¼Œä»è€Œé™ä½æ ‡è®°çœŸå®è¯­éŸ³æ•°æ®çš„æˆæœ¬ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦åˆ©ç”¨é¢å¤–çš„æ–‡æœ¬æ•°æ®å’ŒTTSæ¨¡å‹æ”¯æŒçš„é¢„å®šä¹‰è¯­éŸ³é£æ ¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Hard-Synthï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ASRæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå…ˆè¿›çš„é›¶æ ·æœ¬TTSã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡å†™çš„æ–¹å¼ï¼Œé‡‡ç”¨LLMsç”Ÿæˆå¤šæ ·åŒ–çš„é¢†åŸŸå†…æ–‡æœ¬ï¼Œè€Œä¸ä¾èµ–é¢å¤–çš„æ–‡æœ¬æ•°æ®ã€‚æˆ‘ä»¬æ‘’å¼ƒä½¿ç”¨é¢„å®šä¹‰çš„è¯­éŸ³é£æ ¼ï¼Œè€Œé‡‡ç”¨ç¡¬æ€§æç¤ºé€‰æ‹©æ–¹æ³•å’Œé›¶æ ·æœ¬TTSæ¥å¤åˆ¶ASRæ¨¡å‹å‘ç°éš¾ä»¥è¯†åˆ«çš„è¯­éŸ³é£æ ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒHard-Synthå¯ä»¥æ˜¾è‘—å¢å¼ºConformeræ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨LibriSpeechçš„dev&#x2F;test-otherå­é›†ä¸Šç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†6.5%&#x2F;4.4%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†Hard-Synthæ•°æ®æ•ˆç‡è¾ƒé«˜ä¸”èƒ½é™ä½ASRçš„åè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13159v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œé›¶æ ·æœ¬TTSè¿›è¡ŒASRæ•°æ®å¢å¼ºçš„æ–°æ–¹æ³•â€”â€”Hard-Synthã€‚è¯¥æ–¹æ³•é€šè¿‡LLMsæ”¹å†™ç”Ÿæˆå¤šæ ·åŒ–å’Œç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ï¼Œæ— éœ€é¢å¤–æ–‡æœ¬æ•°æ®ï¼Œå¹¶é‡‡ç”¨ç¡¬æç¤ºé€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨é›¶æ ·æœ¬TTSå¤åˆ¶ASRæ¨¡å‹éš¾ä»¥è¯†åˆ«çš„è¯­éŸ³é£æ ¼ã€‚å®éªŒè¡¨æ˜ï¼ŒHard-Synthå¯æ˜¾è‘—æé«˜Conformeræ¨¡å‹çš„æ€§èƒ½ï¼Œåœ¨LibriSpeechçš„dev&#x2F;test-otherå­é›†ä¸Šç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰åˆ†åˆ«é™ä½6.5%\textbackslashå’Œ4.4%ã€‚æ­¤å¤–ï¼ŒHard-Synthå…·æœ‰æ•°æ®é«˜æ•ˆæ€§å’Œå‡å°‘ASRåè§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Hard-Synthæ˜¯ä¸€ç§æ–°å‹çš„ASRæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œç»“åˆäº†LLMså’Œé›¶æ ·æœ¬TTSæŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡LLMsç”Ÿæˆå¤šæ ·åŒ–å’Œç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬ï¼Œæ— éœ€é¢å¤–æ–‡æœ¬æ•°æ®ã€‚</li>
<li>é‡‡ç”¨ç¡¬æç¤ºé€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨é›¶æ ·æœ¬TTSå¤åˆ¶ASRæ¨¡å‹éš¾ä»¥è¯†åˆ«çš„è¯­éŸ³é£æ ¼ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHard-Synthèƒ½æ˜¾è‘—æé«˜Conformeræ¨¡å‹çš„æ€§èƒ½ï¼Œé™ä½å­—è¯é”™è¯¯ç‡ã€‚</li>
<li>Hard-Synthæ–¹æ³•å…·æœ‰æ•°æ®é«˜æ•ˆæ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„æ•°æ®èµ„æºã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå‡å°‘ASRæ¨¡å‹çš„åè§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-76b8d04c6d6a79c4a38ae6387b600246.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-97dbf588dcd245e3630d92591335f976.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8197c3a2116ee45eb6589389f8c1692f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-70b4f5057248cb65d357ec7774755a01.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46791e9d759964744ef68ac9ea49be97.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-629fdd20073bd21179971138c92d5da0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9147f23b8315c6a62541e5aa9d895ba6.jpg" align="middle">
</details>




<h2 id="Rethinking-MUSHRA-Addressing-Modern-Challenges-in-Text-to-Speech-Evaluation"><a href="#Rethinking-MUSHRA-Addressing-Modern-Challenges-in-Text-to-Speech-Evaluation" class="headerlink" title="Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech   Evaluation"></a>Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech   Evaluation</h2><p><strong>Authors:Praveen Srinivasa Varadhan, Amogh Gulati, Ashwin Sankar, Srija Anand, Anirudh Gupta, Anirudh Mukherjee, Shiva Kumar Marepally, Ankur Bhatia, Saloni Jaju, Suvrat Bhooshan, Mitesh M. Khapra</strong></p>
<p>Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOSâ€™s pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 471 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 47,100 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems. </p>
<blockquote>
<p>å°½ç®¡TTSæ¨¡å‹è¿…é€Ÿè¿›æ­¥ï¼Œä½†ä»ç¼ºä¹ä¸€è‡´ä¸”ç¨³å¥çš„äººç±»è¯„ä¼°æ¡†æ¶ã€‚ä¾‹å¦‚ï¼ŒMOSæµ‹è¯•æ— æ³•åŒºåˆ†ç›¸ä¼¼çš„æ¨¡å‹ï¼ŒCMOSçš„é…å¯¹æ¯”è¾ƒåˆ™éå¸¸è€—æ—¶ã€‚MUSHRAæµ‹è¯•æ˜¯è¯„ä¼°å¤šä¸ªTTSç³»ç»Ÿçš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•ï¼Œä½†åœ¨è¿™é¡¹å·¥ä½œä¸­æˆ‘ä»¬è¡¨æ˜ï¼Œå…¶å¯¹åŒ¹é…äººç±»å‚è€ƒè¯­éŸ³çš„ä¾èµ–ä¼šè¿‡åº¦æƒ©ç½šé‚£äº›å·²ç»è¶…è¿‡äººç±»è¯­éŸ³è´¨é‡çš„ç°ä»£TTSç³»ç»Ÿçš„åˆ†æ•°ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¯¹MUSHRAæµ‹è¯•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨å…¶å¯¹è¯„åˆ†è€…å˜å¼‚æ€§ã€å¬è€…ç–²åŠ³å’Œå‚è€ƒåè§ç­‰å› ç´ çš„æ•æ„Ÿæ€§ã€‚åŸºäºæˆ‘ä»¬æ¶‰åŠå°åœ°è¯­å’Œæ³°ç±³å°”è¯­471åäººç±»å¬è€…çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šï¼ˆiï¼‰å‚è€ƒåŒ¹é…åè§ï¼Œå³è¯„åˆ†è€…å—åˆ°ä¸å¿…è¦çš„äººç±»å‚è€ƒå½±å“ï¼›ï¼ˆiiï¼‰ç”±äºç¼ºä¹æ˜ç¡®çš„ç²¾ç»†æŒ‡å—è€Œäº§ç”Ÿçš„åˆ¤æ–­æ¨¡ç³Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MUSHRAæµ‹è¯•çš„ä¸¤ç§æ”¹è¿›å˜ä½“ã€‚ç¬¬ä¸€ç§å˜ä½“ä½¿åˆæˆæ ·æœ¬çš„è¯„åˆ†æ›´åŠ å…¬å¹³ï¼Œå³ä½¿å…¶è´¨é‡è¶…è¿‡äººç±»å‚è€ƒè´¨é‡ã€‚ç¬¬äºŒç§å˜ä½“å‡å°‘äº†æ¨¡ç³Šæ€§ï¼Œå¦‚è¯„åˆ†è€…ä¹‹é—´çš„æ–¹å·®ç›¸å¯¹è¾ƒä½æ‰€ç¤ºã€‚é€šè¿‡ç»“åˆè¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬å®ç°äº†æ›´å¯é å’Œæ›´ç²¾ç»†çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†MANGOæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªäººç±»è¯„åˆ†çš„å·¨å¤§æ•°æ®é›†ï¼ŒåŒ…å«47,100é¡¹è¯„åˆ†ï¼Œæ˜¯å°åº¦è¯­è¨€çš„é¦–ä¸ªæ­¤ç±»é›†åˆï¼Œæœ‰åŠ©äºåˆ†æäººç±»åå¥½å¹¶å¼€å‘ç”¨äºè¯„ä¼°TTSç³»ç»Ÿçš„è‡ªåŠ¨åº¦é‡æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12719v1">PDF</a> 19 pages, 12 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†TTSæ¨¡å‹è¯„ä¼°ä¸­å­˜åœ¨çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå°½ç®¡TTSæŠ€æœ¯å¿«é€Ÿå‘å±•ï¼Œä½†ä»ç¼ºä¹ä¸€è‡´ä¸”ç¨³å¥çš„äººè¯„æ¡†æ¶ã€‚ä½œè€…é‡ç‚¹å¯¹MUSHRAæµ‹è¯•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°å…¶å­˜åœ¨çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šä¸€æ˜¯å—å‚è€ƒåŒ¹é…åè§å½±å“ï¼ŒäºŒæ˜¯åˆ¤æ–­æ¨¡ç³Šã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§æ”¹è¿›çš„MUSHRAæµ‹è¯•æ–¹æ³•ï¼Œå¹¶å‘å¸ƒäº†é’ˆå¯¹å°åº¦è¯­è¨€çš„å¤§è§„æ¨¡æ•°æ®é›†MANGOï¼Œæœ‰åŠ©äºåˆ†æäººç±»åå¥½å¹¶å¼€å‘è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSé¢†åŸŸç¼ºä¹ç»Ÿä¸€ç¨³å¥çš„äººè¯„æ¡†æ¶ï¼Œç°æœ‰æµ‹è¯•æ–¹æ³•å¦‚MOSå’ŒCMOSå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MUSHRAæµ‹è¯•æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„TTSè¯„ä¼°æ–¹æ³•ï¼Œä½†å­˜åœ¨å‚è€ƒåŒ¹é…åè§å’Œåˆ¤æ–­æ¨¡ç³Šçš„é—®é¢˜ã€‚</li>
<li>ä½œè€…æå‡ºäº†ä¸¤ç§æ”¹è¿›çš„MUSHRAæµ‹è¯•æ–¹æ³•ï¼Œä»¥å…¬å¹³è¯„ä»·è¶…è¿‡äººç±»å‚è€ƒè´¨é‡çš„åˆæˆæ ·æœ¬å¹¶å‡å°‘åˆ¤æ–­æ¨¡ç³Šã€‚</li>
<li>é€šè¿‡ç»“åˆè¿™ä¸¤ç§æ–¹æ³•ï¼Œå¯ä»¥å®ç°æ›´å¯é å’Œæ›´ç²¾ç»†çš„è¯„ä¼°ã€‚</li>
<li>å‘å¸ƒäº†é’ˆå¯¹å°åº¦è¯­è¨€çš„å¤§è§„æ¨¡æ•°æ®é›†MANGOï¼Œæœ‰åŠ©äºåˆ†æäººç±»åå¥½å’Œå¼€å‘è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠäº†471åäººç±»å¬ä¼—çš„å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬å°åº¦è¯­è¨€ï¼ˆå¦‚å°åœ°è¯­å’Œæ³°ç±³å°”è¯­ï¼‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8f73fb771363c57f9607e03ed2ee8648.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8877484b6e27a2729903e5d49c4a28e0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9d8f786b0d3b297627d4cd40128142f5.jpg" align="middle">
</details>




<h2 id="ESTVocoder-An-Excitation-Spectral-Transformed-Neural-Vocoder-Conditioned-on-Mel-Spectrogram"><a href="#ESTVocoder-An-Excitation-Spectral-Transformed-Neural-Vocoder-Conditioned-on-Mel-Spectrogram" class="headerlink" title="ESTVocoder: An Excitation-Spectral-Transformed Neural Vocoder   Conditioned on Mel Spectrogram"></a>ESTVocoder: An Excitation-Spectral-Transformed Neural Vocoder   Conditioned on Mel Spectrogram</h2><p><strong>Authors:Xiao-Hang Jiang, Hui-Peng Du, Yang Ai, Ye-Xin Lu, Zhen-Hua Ling</strong></p>
<p>This paper proposes ESTVocoder, a novel excitation-spectral-transformed neural vocoder within the framework of source-filter theory. The ESTVocoder transforms the amplitude and phase spectra of the excitation into the corresponding speech amplitude and phase spectra using a neural filter whose backbone is ConvNeXt v2 blocks. Finally, the speech waveform is reconstructed through the inverse short-time Fourier transform (ISTFT). The excitation is constructed based on the F0: for voiced segments, it contains full harmonic information, while for unvoiced segments, it is represented by noise. The excitation provides the filter with prior knowledge of the amplitude and phase patterns, expecting to reduce the modeling difficulty compared to conventional neural vocoders. To ensure the fidelity of the synthesized speech, an adversarial training strategy is applied to ESTVocoder with multi-scale and multi-resolution discriminators. Analysis-synthesis and text-to-speech experiments both confirm that our proposed ESTVocoder outperforms or is comparable to other baseline neural vocoders, e.g., HiFi-GAN, SiFi-GAN, and Vocos, in terms of synthesized speech quality, with a reasonable model complexity and generation speed. Additional analysis experiments also demonstrate that the introduced excitation effectively accelerates the modelâ€™s convergence process, thanks to the speech spectral prior information contained in the excitation. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ESTVocoderï¼Œè¿™æ˜¯ä¸€ç§åœ¨æºæ»¤æ³¢å™¨ç†è®ºæ¡†æ¶ä¸‹ï¼Œé‡‡ç”¨æ¿€åŠ±è°±å˜æ¢æŠ€æœ¯çš„æ–°å‹ç¥ç»vocoderã€‚ESTVocoderä½¿ç”¨ç¥ç»æ»¤æ³¢å™¨å°†æ¿€åŠ±çš„å¹…åº¦å’Œç›¸ä½è°±è½¬æ¢ä¸ºç›¸åº”çš„è¯­éŸ³å¹…åº¦å’Œç›¸ä½è°±ï¼Œè¯¥æ»¤æ³¢å™¨çš„éª¨å¹²æ˜¯ConvNeXt v2å—ã€‚æœ€åï¼Œé€šè¿‡é€†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆISTFTï¼‰é‡å»ºè¯­éŸ³æ³¢å½¢ã€‚æ¿€åŠ±çš„æ„é€ åŸºäºF0ï¼šå¯¹äºæœ‰å£°æ®µï¼Œå®ƒåŒ…å«å®Œæ•´çš„è°æ³¢ä¿¡æ¯ï¼Œè€Œå¯¹äºæ— å£°æ®µï¼Œåˆ™ç”¨å™ªå£°è¡¨ç¤ºã€‚æ¿€åŠ±ä¸ºæ»¤æ³¢å™¨æä¾›å¹…åº¦å’Œç›¸ä½æ¨¡å¼çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥æœŸä¸å¸¸è§„ç¥ç»vocoderç›¸æ¯”é™ä½å»ºæ¨¡éš¾åº¦ã€‚ä¸ºäº†ä¿è¯åˆæˆè¯­éŸ³çš„ä¿çœŸåº¦ï¼ŒESTVocoderé‡‡ç”¨äº†å¯¹æŠ—æ€§è®­ç»ƒç­–ç•¥ï¼Œå¹¶é…å¤‡äº†å¤šå°ºåº¦ã€å¤šåˆ†è¾¨ç‡é‰´åˆ«å™¨ã€‚åˆ†æå’Œæ–‡æœ¬è½¬è¯­éŸ³å®éªŒå‡è¯å®ï¼Œä¸å…¶ä»–åŸºçº¿ç¥ç»vocoderï¼ˆå¦‚HiFi-GANã€SiFi-GANå’ŒVocosï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„ESTVocoderåœ¨åˆæˆè¯­éŸ³è´¨é‡æ–¹é¢è¡¨ç°ä¼˜å¼‚æˆ–ç›¸å½“ï¼Œå…·æœ‰åˆç†çš„æ¨¡å‹å¤æ‚åº¦å’Œç”Ÿæˆé€Ÿåº¦ã€‚å¦å¤–çš„åˆ†æå®éªŒè¿˜è¡¨æ˜ï¼Œå¼•å…¥çš„æ¿€åŠ±æœ‰æ•ˆåœ°åŠ é€Ÿäº†æ¨¡å‹çš„æ”¶æ•›è¿‡ç¨‹ï¼Œè¿™å¾—ç›Šäºæ¿€åŠ±ä¸­æ‰€åŒ…å«çš„è¯­éŸ³è°±å…ˆéªŒä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11258v1">PDF</a> Accepted by NCMMSC2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºESTVocoderçš„æ–°å‹æ¿€åŠ±-é¢‘è°±è½¬æ¢ç¥ç»ç½‘ç»œç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨åœ¨æºæ»¤æ³¢å™¨ç†è®ºæ¡†æ¶ä¸‹è¿è¡Œã€‚å®ƒåˆ©ç”¨ç¥ç»ç½‘ç»œè¿‡æ»¤å™¨å°†æ¿€åŠ±çš„å¹…åº¦å’Œç›¸ä½è°±è½¬æ¢ä¸ºç›¸åº”çš„è¯­éŸ³å¹…åº¦å’Œç›¸ä½è°±ï¼Œå¹¶é€šè¿‡é€†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆISTFTï¼‰é‡å»ºè¯­éŸ³æ³¢å½¢ã€‚æ¿€åŠ±åŸºäºF0æ„å»ºï¼Œå¯¹äºå‘éŸ³æ®µï¼ŒåŒ…å«å…¨éƒ¨è°æ³¢ä¿¡æ¯ï¼Œè€Œå¯¹äºéå‘éŸ³æ®µï¼Œåˆ™é€šè¿‡å™ªå£°è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œé‡‡ç”¨å¯¹æŠ—è®­ç»ƒç­–ç•¥ç¡®ä¿åˆæˆè¯­éŸ³çš„ä¿çœŸåº¦ï¼Œå¹¶åˆ©ç”¨å¤šå°ºåº¦ã€å¤šåˆ†è¾¨ç‡é‰´åˆ«å™¨å¯¹ESTVocoderè¿›è¡Œé‰´åˆ«ã€‚å®éªŒè¯å®ï¼ŒESTVocoderåœ¨åˆæˆè¯­éŸ³è´¨é‡æ–¹é¢ä¼˜äºæˆ–ä¸å…¶ä»–åŸºçº¿ç¥ç»ç½‘ç»œç¼–ç å™¨ç›¸å½“ï¼Œå¦‚HiFi-GANã€SiFi-GANå’ŒVocosç­‰ï¼ŒåŒæ—¶æ¨¡å‹å¤æ‚åº¦åˆç†ï¼Œç”Ÿæˆé€Ÿåº¦å¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ESTVocoderæ˜¯ä¸€ç§åŸºäºæºæ»¤æ³¢å™¨ç†è®ºçš„æ¿€åŠ±-é¢‘è°±è½¬æ¢ç¥ç»ç½‘ç»œç¼–ç å™¨ã€‚</li>
<li>é€šè¿‡ç¥ç»ç½‘ç»œè¿‡æ»¤å™¨å°†æ¿€åŠ±çš„å¹…åº¦å’Œç›¸ä½è°±è½¬æ¢ä¸ºè¯­éŸ³å¹…åº¦å’Œç›¸ä½è°±ã€‚</li>
<li>é‡‡ç”¨é€†çŸ­æ—¶å‚…é‡Œå¶å˜æ¢ï¼ˆISTFTï¼‰é‡å»ºè¯­éŸ³æ³¢å½¢ã€‚</li>
<li>æ¿€åŠ±ç”±F0æ„å»ºï¼ŒåŒ…å«å‘éŸ³æ®µçš„å…¨éƒ¨è°æ³¢ä¿¡æ¯å’Œéå‘éŸ³æ®µçš„å™ªå£°è¡¨ç¤ºã€‚</li>
<li>å¯¹æŠ—è®­ç»ƒç­–ç•¥ç”¨äºç¡®ä¿åˆæˆè¯­éŸ³çš„ä¿çœŸåº¦ã€‚</li>
<li>ESTVocoderåœ¨åˆæˆè¯­éŸ³è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸å…¶ä»–åŸºçº¿ç¥ç»ç½‘ç»œç¼–ç å™¨ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-96ca06934bfc8b505585e2ce2a575f0d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b101cfb513e859fefa38c0ddb5406348.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bd64f48634a8a9442dd331eaf917737d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c4f9825b07cfe65163935cc3ee24b0ce.jpg" align="middle">
</details>




<h2 id="SAMOS-A-Neural-MOS-Prediction-Model-Leveraging-Semantic-Representations-and-Acoustic-Features"><a href="#SAMOS-A-Neural-MOS-Prediction-Model-Leveraging-Semantic-Representations-and-Acoustic-Features" class="headerlink" title="SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations   and Acoustic Features"></a>SAMOS: A Neural MOS Prediction Model Leveraging Semantic Representations   and Acoustic Features</h2><p><strong>Authors:Yu-Fei Shi, Yang Ai, Ye-Xin Lu, Hui-Peng Du, Zhen-Hua Ling</strong></p>
<p>Assessing the naturalness of speech using mean opinion score (MOS) prediction models has positive implications for the automatic evaluation of speech synthesis systems. Early MOS prediction models took the raw waveform or amplitude spectrum of speech as input, whereas more advanced methods employed self-supervised-learning (SSL) based models to extract semantic representations from speech for MOS prediction. These methods utilized limited aspects of speech information for MOS prediction, resulting in restricted prediction accuracy. Therefore, in this paper, we propose SAMOS, a MOS prediction model that leverages both Semantic and Acoustic information of speech to be assessed. Specifically, the proposed SAMOS leverages a pretrained wav2vec2 to extract semantic representations and uses the feature extractor of a pretrained BiVocoder to extract acoustic features. These two types of features are then fed into the prediction network, which includes multi-task heads and an aggregation layer, to obtain the final MOS score. Experimental results demonstrate that the proposed SAMOS outperforms current state-of-the-art MOS prediction models on the BVCC dataset and performs comparable performance on the BC2019 dataset, according to the results of system-level evaluation metrics. </p>
<blockquote>
<p>ä½¿ç”¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹æ¨¡å‹è¯„ä¼°è¯­éŸ³çš„è‡ªç„¶åº¦ï¼Œå¯¹äºè‡ªåŠ¨è¯„ä¼°è¯­éŸ³åˆæˆç³»ç»Ÿå…·æœ‰ç§¯æçš„å½±å“ã€‚æ—©æœŸçš„MOSé¢„æµ‹æ¨¡å‹ä»¥è¯­éŸ³çš„åŸå§‹æ³¢å½¢æˆ–å¹…åº¦è°±ä½œä¸ºè¾“å…¥ï¼Œè€Œæ›´å…ˆè¿›çš„æ–¹æ³•åˆ™ä½¿ç”¨åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ¨¡å‹æ¥ä»è¯­éŸ³ä¸­æå–è¯­ä¹‰è¡¨ç¤ºä»¥è¿›è¡ŒMOSé¢„æµ‹ã€‚è¿™äº›æ–¹æ³•ä»…åˆ©ç”¨è¯­éŸ³çš„æœ‰é™æ–¹é¢æ¥è¿›è¡ŒMOSé¢„æµ‹ï¼Œå¯¼è‡´é¢„æµ‹ç²¾åº¦å—åˆ°é™åˆ¶ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSAMOSçš„MOSé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨è¯­éŸ³çš„è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯è¿›è¡Œè¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œæå‡ºçš„SAMOSåˆ©ç”¨é¢„è®­ç»ƒçš„wav2vec2æå–è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„BiVocoderçš„ç‰¹å¾æå–å™¨æå–å£°éŸ³ç‰¹å¾ã€‚ç„¶åï¼Œè¿™ä¸¤ç§ç±»å‹çš„ç‰¹å¾è¢«è¾“å…¥åˆ°é¢„æµ‹ç½‘ç»œä¸­ï¼Œè¯¥ç½‘ç»œåŒ…æ‹¬å¤šä»»åŠ¡å¤´å’Œèšåˆå±‚ï¼Œä»¥è·å¾—æœ€ç»ˆçš„MOSå¾—åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨BVCCæ•°æ®é›†ä¸Šï¼Œæ‰€æå‡ºçš„SAMOSåœ¨æ€§èƒ½ä¸Šä¼˜äºå½“å‰æœ€å…ˆè¿›çš„MOSé¢„æµ‹æ¨¡å‹ï¼Œå¹¶åœ¨BC2019æ•°æ®é›†ä¸Šçš„ç³»ç»Ÿçº§è¯„ä¼°æŒ‡æ ‡ä¸­è¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11232v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬ä¸­çš„ä¿¡æ¯ï¼Œä½¿ç”¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹æ¨¡å‹è¯„ä¼°è¯­éŸ³çš„è‡ªç„¶åº¦å¯¹è‡ªåŠ¨è¯„ä¼°è¯­éŸ³åˆæˆç³»ç»Ÿå…·æœ‰ç§¯æå½±å“ã€‚æ—©æœŸçš„MOSé¢„æµ‹æ¨¡å‹é‡‡ç”¨è¯­éŸ³çš„åŸå§‹æ³¢å½¢æˆ–å¹…åº¦è°±ä½œä¸ºè¾“å…¥ï¼Œè€Œæ›´å…ˆè¿›çš„æ–¹æ³•ä½¿ç”¨åŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ¨¡å‹ä»è¯­éŸ³ä¸­æå–è¯­ä¹‰è¡¨ç¤ºä»¥è¿›è¡ŒMOSé¢„æµ‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»…ä½¿ç”¨æœ‰é™çš„è¯­éŸ³ä¿¡æ¯æ¥è¿›è¡ŒMOSé¢„æµ‹ï¼Œå¯¼è‡´é¢„æµ‹å‡†ç¡®æ€§å—é™ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„MOSé¢„æµ‹æ¨¡å‹SAMOSï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è¯­éŸ³çš„è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMOSåœ¨BVCCæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„MOSé¢„æµ‹æ¨¡å‹ï¼Œåœ¨BC2019æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸ä¹‹ç›¸å½“ã€‚ç³»ç»Ÿçº§è¯„ä¼°æŒ‡æ ‡æ˜¾ç¤ºå…¶å“è¶Šè¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰é¢„æµ‹æ¨¡å‹è¯„ä¼°è¯­éŸ³è‡ªç„¶åº¦å¯¹è‡ªåŠ¨è¯„ä¼°è¯­éŸ³åˆæˆç³»ç»Ÿæœ‰ç§¯æå½±å“ã€‚</li>
<li>æ—©æœŸçš„MOSé¢„æµ‹æ¨¡å‹ä¸»è¦åŸºäºåŸå§‹æ³¢å½¢æˆ–å¹…åº¦è°±è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>æ›´å…ˆè¿›çš„MOSé¢„æµ‹æ–¹æ³•é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ä»è¯­éŸ³ä¸­æå–è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»…ä½¿ç”¨æœ‰é™çš„è¯­éŸ³ä¿¡æ¯ï¼Œå¯¼è‡´é¢„æµ‹å‡†ç¡®æ€§å—é™ã€‚</li>
<li>SAMOSæ¨¡å‹ç»“åˆäº†è¯­ä¹‰å’Œå£°éŸ³ä¿¡æ¯æ¥è¿›è¡Œæ›´å‡†ç¡®çš„MOSé¢„æµ‹ã€‚</li>
<li>SAMOSåœ¨BVCCæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å…ˆè¿›æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-eb59f164f3e5ffa211d5126a473e857f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b9eaee1b7ea7b970f0f91a410705323e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-12eb99938b12b9e8727f893807ab6dd9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-50054df8d24d19f595eeb60c0a92eba5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-701375ba6af4aff3ff05d57114213651.jpg" align="middle">
</details>




<h2 id="SmoothCache-A-Universal-Inference-Acceleration-Technique-for-Diffusion-Transformers"><a href="#SmoothCache-A-Universal-Inference-Acceleration-Technique-for-Diffusion-Transformers" class="headerlink" title="SmoothCache: A Universal Inference Acceleration Technique for Diffusion   Transformers"></a>SmoothCache: A Universal Inference Acceleration Technique for Diffusion   Transformers</h2><p><strong>Authors:Joseph Liu, Joshua Geddes, Ziyu Guo, Haomiao Jiang, Mahesh Kumar Nandwana</strong></p>
<p>Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models. </p>
<blockquote>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²ç»æˆä¸ºåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³åˆæˆåœ¨å†…çš„å„ç§ä»»åŠ¡çš„å¼ºå¤§ç”Ÿæˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦é‡å¤è¯„ä¼°èµ„æºå¯†é›†å‹çš„æ³¨æ„åŠ›å’Œå‰é¦ˆæ¨¡å—ï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹è®¡ç®—æˆæœ¬ä»ç„¶å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SmoothCacheï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºDiTæ¶æ„çš„æ¨¡å‹æ— å…³æ¨ç†åŠ é€ŸæŠ€æœ¯ã€‚SmoothCacheåˆ©ç”¨ç›¸é‚»æ‰©æ•£æ—¶é—´æ­¥é•¿ä¹‹é—´å±‚è¾“å‡ºçš„é«˜ç›¸ä¼¼æ€§ã€‚é€šè¿‡åˆ†ææ¥è‡ªå°å‹æ ¡å‡†é›†çš„å±‚è¡¨ç¤ºè¯¯å·®ï¼ŒSmoothCacheåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°ç¼“å­˜å’Œé‡ç”¨å…³é”®ç‰¹å¾ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSmoothCacheåœ¨ä¿æŒæˆ–ç”šè‡³æé«˜ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†8%è‡³71%çš„åŠ é€Ÿã€‚æˆ‘ä»¬åœ¨å›¾åƒç”Ÿæˆçš„DiT-XLã€æ–‡æœ¬åˆ°è§†é¢‘çš„Open-Soraä»¥åŠæ–‡æœ¬åˆ°éŸ³é¢‘çš„Stable Audio Openä¸Šå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ï¼Œçªå‡ºäº†å…¶å®ç°å®æ—¶åº”ç”¨å’Œæ‰©å¤§å¼ºå¤§DiTæ¨¡å‹å¯åŠæ€§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10510v1">PDF</a> Code can be found at <a target="_blank" rel="noopener" href="https://github.com/Roblox/SmoothCache">https://github.com/Roblox/SmoothCache</a></p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£Transformerï¼ˆDiTï¼‰å·²æˆä¸ºå›¾åƒã€è§†é¢‘å’Œè¯­éŸ³åˆæˆç­‰ä»»åŠ¡çš„å¼ºå¤§ç”Ÿæˆæ¨¡å‹ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SmoothCacheï¼Œè¿™æ˜¯ä¸€ç§é€‚ç”¨äºDiTæ¶æ„çš„æ¨¡å‹æ— å…³æ¨ç†åŠ é€ŸæŠ€æœ¯ã€‚SmoothCacheåˆ©ç”¨ç›¸é‚»æ‰©æ•£æ—¶åºæ­¥éª¤é—´å±‚è¾“å‡ºä¹‹é—´çš„é«˜ç›¸ä¼¼æ€§ï¼Œé€šè¿‡åˆ†ææ ¡å‡†é›†ä¸Šçš„å±‚è¡¨ç¤ºè¯¯å·®æ¥è‡ªé€‚åº”ç¼“å­˜å’Œé‡ç”¨å…³é”®ç‰¹å¾ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒSmoothCacheåœ¨ä¿æŒæˆ–æé«˜ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†8%è‡³71%çš„æé€Ÿï¼Œåœ¨å›¾åƒç”Ÿæˆã€æ–‡æœ¬åˆ°è§†é¢‘ä»¥åŠæ–‡æœ¬åˆ°éŸ³é¢‘ç­‰å¤šä¸ªæ¨¡æ€ä¸­å‡å±•ç°å‡ºå…¶æœ‰æ•ˆæ€§ï¼Œå…·æœ‰å®ç°å®æ—¶åº”ç”¨å’Œæ‰©å¤§å¼ºå¤§DiTæ¨¡å‹è®¿é—®æ½œåŠ›çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£Transformerï¼ˆDiTï¼‰æ˜¯å›¾åƒã€è§†é¢‘å’Œè¯­éŸ³åˆæˆç­‰ä»»åŠ¡çš„å¼ºå¤§ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>DiTçš„æ¨ç†è¿‡ç¨‹è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦åŠ é€Ÿã€‚</li>
<li>SmoothCacheæ˜¯ä¸€ç§é€‚ç”¨äºDiTæ¶æ„çš„æ¨¡å‹æ— å…³æ¨ç†åŠ é€ŸæŠ€æœ¯ã€‚</li>
<li>SmoothCacheåˆ©ç”¨ç›¸é‚»æ‰©æ•£æ—¶åºæ­¥éª¤é—´å±‚è¾“å‡ºçš„é«˜ç›¸ä¼¼æ€§æ¥åŠ é€Ÿæ¨ç†ã€‚</li>
<li>SmoothCacheé€šè¿‡è‡ªé€‚åº”ç¼“å­˜å’Œé‡ç”¨å…³é”®ç‰¹å¾æ¥å®ç°åŠ é€Ÿã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSmoothCacheå®ç°äº†8%è‡³71%çš„æé€Ÿï¼ŒåŒæ—¶ä¿æŒæˆ–æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2c27f98ccee829f120642c601f9d589b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e6ecb1bc38f45e2ca953ad1ce857a9ef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-366926ae68d0d3eb3c4a0056eec25d8d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0b26614d2a07242f196bbe9b3c09a0e3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-45ea0b8969ac050a88c8c2de854a3a1d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-dffd2301e9f028e035113a224a93fbf1.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4b8714da103f0a02244337860fc6f841.jpg" align="middle">
</details>




<h2 id="Improving-Grapheme-to-Phoneme-Conversion-through-In-Context-Knowledge-Retrieval-with-Large-Language-Models"><a href="#Improving-Grapheme-to-Phoneme-Conversion-through-In-Context-Knowledge-Retrieval-with-Large-Language-Models" class="headerlink" title="Improving Grapheme-to-Phoneme Conversion through In-Context Knowledge   Retrieval with Large Language Models"></a>Improving Grapheme-to-Phoneme Conversion through In-Context Knowledge   Retrieval with Large Language Models</h2><p><strong>Authors:Dongrui Han, Mingyu Cui, Jiawen Kang, Xixin Wu, Xunying Liu, Helen Meng</strong></p>
<p>Grapheme-to-phoneme (G2P) conversion is a crucial step in Text-to-Speech (TTS) systems, responsible for mapping grapheme to corresponding phonetic representations. However, it faces ambiguities problems where the same grapheme can represent multiple phonemes depending on contexts, posing a challenge for G2P conversion. Inspired by the remarkable success of Large Language Models (LLMs) in handling context-aware scenarios, contextual G2P conversion systems with LLMsâ€™ in-context knowledge retrieval (ICKR) capabilities are proposed to promote disambiguation capability. The efficacy of incorporating ICKR into G2P conversion systems is demonstrated thoroughly on the Librig2p dataset. In particular, the best contextual G2P conversion system using ICKR outperforms the baseline with weighted average phoneme error rate (PER) reductions of 2.0% absolute (28.9% relative). Using GPT-4 in the ICKR system can increase of 3.5% absolute (3.8% relative) on the Librig2p dataset. </p>
<blockquote>
<p>å­—å½¢åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰è½¬æ¢æ˜¯æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­çš„å…³é”®æ­¥éª¤ï¼Œè´Ÿè´£å°†å­—å½¢æ˜ å°„åˆ°ç›¸åº”çš„è¯­éŸ³è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå®ƒé¢ä¸´ç€æ­§ä¹‰é—®é¢˜ï¼Œå³åŒä¸€ä¸ªå­—å½¢å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡ä»£è¡¨å¤šä¸ªéŸ³ç´ ï¼Œè¿™ç»™G2Pè½¬æ¢å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ä¸Šä¸‹æ–‡æ„ŸçŸ¥åœºæ™¯æ—¶å–å¾—æ˜¾è‘—æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†å…·æœ‰LLMä¸Šä¸‹æ–‡çŸ¥è¯†æ£€ç´¢ï¼ˆICKRï¼‰èƒ½åŠ›çš„ä¸Šä¸‹æ–‡G2Pè½¬æ¢ç³»ç»Ÿï¼Œä»¥æé«˜è§£æ­§ä¹‰èƒ½åŠ›ã€‚å°†ICKRçº³å…¥G2Pè½¬æ¢ç³»ç»Ÿçš„æœ‰æ•ˆæ€§åœ¨Librig2pæ•°æ®é›†ä¸Šå¾—åˆ°äº†å……åˆ†è¯æ˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨ICKRçš„æœ€ä½³ä¸Šä¸‹æ–‡G2Pè½¬æ¢ç³»ç»Ÿæ¯”åŸºçº¿ç³»ç»Ÿçš„å¹³å‡éŸ³ç´ é”™è¯¯ç‡ï¼ˆPERï¼‰ç»å¯¹é™ä½äº†2.0%ï¼ˆç›¸å¯¹é™ä½äº†28.9%ï¼‰ã€‚åœ¨ICKRç³»ç»Ÿä¸­ä½¿ç”¨GPT-4å¯ä»¥åœ¨Librig2pæ•°æ®é›†ä¸Šç»å¯¹æé«˜3.5%ï¼ˆç›¸å¯¹æé«˜3.8%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.07563v1">PDF</a> accepted by ISCSLP 2024</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬æåˆ°ï¼Œåœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­ï¼Œå­—æ¯éŸ³è½¬æ¢ï¼ˆG2Pï¼‰æ˜¯å…³é”®æ­¥éª¤ä¹‹ä¸€ï¼Œè´Ÿè´£å°†å­—æ¯æ˜ å°„åˆ°ç›¸åº”çš„è¯­éŸ³è¡¨ç¤ºã€‚ç„¶è€Œï¼Œç”±äºåŒä¸€å­—æ¯åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­å¯èƒ½ä»£è¡¨ä¸åŒçš„éŸ³ç´ ï¼Œå› æ­¤å­˜åœ¨æ­§ä¹‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡çŸ¥è¯†æ£€ç´¢ï¼ˆICKRï¼‰èƒ½åŠ›æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„G2Pè½¬æ¢ç³»ç»Ÿã€‚åœ¨Librig2pæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŠ å…¥ICKRçš„G2Pè½¬æ¢ç³»ç»Ÿçš„æœ‰æ•ˆæ€§å¾—åˆ°äº†éªŒè¯ã€‚ç‰¹åˆ«æ˜¯ä½¿ç”¨GPT-4çš„ICKRç³»ç»Ÿï¼Œåœ¨Librig2pæ•°æ®é›†ä¸Šçš„è¡¨ç°æœ€ä½³ï¼Œç»å¯¹å¹³å‡éŸ³ç´ é”™è¯¯ç‡é™ä½äº†2.0%ï¼ˆç›¸å¯¹é™ä½28.9%ï¼‰ï¼Œè¿›ä¸€æ­¥èå…¥GPT-4èƒ½é¢å¤–é™ä½å¹³å‡éŸ³ç´ é”™è¯¯ç‡3.5%ï¼ˆç›¸å¯¹é™ä½ä»…å¢åŠ äº†é¢å¤–çš„æ•ˆç‡ï¼‰ï¼Œå±•ç¤ºå‡ºè‰¯å¥½åº”ç”¨å‰æ™¯ã€‚<br>    <strong>Key Takeaways</strong></p>
<pre><code>ä»¥ä¸‹æ˜¯ä»æ–‡æœ¬ä¸­æå–çš„å…³é”®è§è§£ï¼š
- G2Pè½¬æ¢åœ¨TTSç³»ç»Ÿä¸­å…·æœ‰é‡è¦æ€§ï¼Œå®ƒå°†å­—æ¯æ˜ å°„åˆ°å¯¹åº”çš„è¯­éŸ³è¡¨ç¤ºå½¢å¼ã€‚ä½†åœ¨å¤„ç†ä¸åŒä¸Šä¸‹æ–‡æ—¶å­˜åœ¨æ­§ä¹‰é—®é¢˜ã€‚
- å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å­—æ¯éŸ³è½¬æ¢ï¼ˆG2Pï¼‰ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚
- ä½¿ç”¨LLMçš„ICKRèƒ½åŠ›æœ‰åŠ©äºæé«˜G2Pè½¬æ¢ç³»ç»Ÿçš„æ€§èƒ½ã€‚
- åœ¨Librig2pæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæœ€ä½³ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„G2Pè½¬æ¢ç³»ç»Ÿç›¸æ¯”åŸºçº¿æ¨¡å‹ç»å¯¹å¹³å‡éŸ³ç´ é”™è¯¯ç‡é™ä½äº†2.0%ï¼ˆç›¸å¯¹é™ä½äº†28.9%ï¼‰ã€‚ èå…¥GPT-4è¿›ä¸€æ­¥æå‡æ•ˆæœæ˜¾è‘—æ•ˆæœè‰¯å¥½è¡¨ç°ï¼Œè¡¨ç°å‡ºå·¨å¤§çš„åº”ç”¨æ½œåŠ›ã€‚
</code></pre>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-526e6052a85bc37714b10b64c6610f3c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f165d4ab78630d3bdd9463caa19aa6f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bc48725463866327e31787cae0508890.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-14d3d8519ddedd4c3562dedb86138972.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-f8e79f80066bf9f9ff399b7f19354319.jpg" align="middle">
</details>




<h2 id="Debatts-Zero-Shot-Debating-Text-to-Speech-Synthesis"><a href="#Debatts-Zero-Shot-Debating-Text-to-Speech-Synthesis" class="headerlink" title="Debatts: Zero-Shot Debating Text-to-Speech Synthesis"></a>Debatts: Zero-Shot Debating Text-to-Speech Synthesis</h2><p><strong>Authors:Yiqiao Huang, Yuancheng Wang, Jiaqi Li, Haotian Guo, Haorui He, Shunsi Zhang, Zhizheng Wu</strong></p>
<p>In debating, rebuttal is one of the most critical stages, where a speaker addresses the arguments presented by the opposing side. During this process, the speaker synthesizes their own persuasive articulation given the context from the opposing side. This work proposes a novel zero-shot text-to-speech synthesis system for rebuttal, namely Debatts. Debatts takes two speech prompts, one from the opposing side (i.e. opponent) and one from the speaker. The prompt from the opponent is supposed to provide debating style prosody, and the prompt from the speaker provides identity information. In particular, we pretrain the Debatts system from in-the-wild dataset, and integrate an additional reference encoder to take debating prompt for style. In addition, we also create a debating dataset to develop Debatts. In this setting, Debatts can generate a debating-style speech in rebuttal for any voices. Experimental results confirm the effectiveness of the proposed system in comparison with the classic zero-shot TTS systems. </p>
<blockquote>
<p>åœ¨è¾©è®ºä¸­ï¼Œåé©³æ˜¯æœ€å…³é”®çš„é˜¶æ®µä¹‹ä¸€ï¼Œè¾©æ‰‹éœ€è¦é’ˆå¯¹å¯¹æ–¹æå‡ºçš„è§‚ç‚¹è¿›è¡Œå›åº”ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œè¾©æ‰‹éœ€è¦åŸºäºå¯¹æ–¹çš„è¯­å¢ƒï¼Œç»“åˆè‡ªèº«çš„è®ºç‚¹è¿›è¡Œæœ‰è¯´æœåŠ›çš„è¡¨è¿°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œç”¨äºè¿›è¡Œåé©³ï¼Œç§°ä¸ºDebattsã€‚Debattsæ¥å—ä¸¤ä¸ªè¯­éŸ³æç¤ºï¼Œä¸€ä¸ªæ¥è‡ªå¯¹æ–¹ï¼ˆå³å¯¹æ‰‹ï¼‰ï¼Œå¦ä¸€ä¸ªæ¥è‡ªè¾©æ‰‹è‡ªèº«ã€‚å¯¹æ‰‹çš„æç¤ºæ—¨åœ¨æä¾›è¾©è®ºé£æ ¼çš„éŸµå¾‹ï¼Œè€Œè¾©æ‰‹è‡ªèº«çš„æç¤ºåˆ™æä¾›èº«ä»½ä¿¡æ¯ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬ä»é‡ç”Ÿæ•°æ®é›†å¯¹Debattsç³»ç»Ÿè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é›†æˆä¸€ä¸ªé¢å¤–çš„å‚è€ƒç¼–ç å™¨æ¥æå–è¾©è®ºæç¤ºçš„é£æ ¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªè¾©è®ºæ•°æ®é›†æ¥å¼€å‘Debattsã€‚åœ¨æ­¤è®¾å®šä¸‹ï¼ŒDebattså¯ä»¥ç”Ÿæˆä»»ä½•å£°éŸ³çš„è¾©è®ºå¼åé©³è¯­éŸ³ã€‚å®éªŒç»“æœè¯å®ï¼Œä¸ç»å…¸çš„é›¶æ ·æœ¬TTSç³»ç»Ÿç›¸æ¯”ï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿå…·æœ‰æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.06540v2">PDF</a> </p>
<p><strong>Summary</strong><br>è¾©è®ºä¸­çš„åé©³ç¯èŠ‚è‡³å…³é‡è¦ï¼Œå‘è¨€è€…éœ€é’ˆå¯¹å¯¹æ–¹è§‚ç‚¹è¿›è¡Œå›åº”ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œåä¸ºDebattsï¼Œç”¨äºåé©³ç¯èŠ‚ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨ä¸¤ä¸ªè¯­éŸ³æç¤ºï¼Œä¸€ä¸ªæ¥è‡ªå¯¹æ–¹ï¼Œä¸€ä¸ªæ¥è‡ªå‘è¨€è€…ã€‚å¯¹æ–¹æç¤ºæä¾›è¾©è®ºé£æ ¼ï¼Œå‘è¨€è€…æç¤ºæä¾›èº«ä»½ä¿¡æ¯ã€‚Debattsé€šè¿‡é‡å¤–æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é›†æˆå‚è€ƒç¼–ç å™¨ä»¥è·å–è¾©è®ºæç¤ºé£æ ¼ã€‚æ­¤å¤–ï¼Œè¿˜åˆ›å»ºäº†è¾©è®ºæ•°æ®é›†æ¥å¼€å‘Debattsï¼Œå¯ç”Ÿæˆå…·æœ‰åé©³æ€§çš„è¾©è®ºè¯­éŸ³ï¼Œé€‚ç”¨äºä»»ä½•å£°éŸ³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç»å…¸é›¶æ ·æœ¬TTSç³»ç»Ÿç›¸æ¯”ï¼ŒDebattsç³»ç»Ÿæ›´æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åé©³åœ¨è¾©è®ºä¸­æ˜¯å…³é”®é˜¶æ®µï¼Œéœ€è¦å‘è¨€è€…é’ˆå¯¹å¯¹æ–¹è§‚ç‚¹è¿›è¡Œå›åº”ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆç³»ç»ŸDebattsï¼Œä¸“é—¨ç”¨äºç”Ÿæˆè¾©è®ºä¸­çš„åé©³ç¯èŠ‚ã€‚</li>
<li>Debattsé‡‡ç”¨ä¸¤ä¸ªè¯­éŸ³æç¤ºï¼Œåˆ†åˆ«æ¥è‡ªå¯¹æ–¹å’Œå‘è¨€è€…ï¼Œä»¥æä¾›è¾©è®ºé£æ ¼å’Œèº«ä»½ä¿¡æ¯ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡é‡å¤–æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é›†æˆå‚è€ƒç¼–ç å™¨ä»¥è·å–è¾©è®ºæç¤ºé£æ ¼ã€‚</li>
<li>åˆ›å»ºäº†ä¸“é—¨çš„è¾©è®ºæ•°æ®é›†ä»¥å¼€å‘Debattsç³»ç»Ÿã€‚</li>
<li>Debattså¯ä»¥ç”Ÿæˆå…·æœ‰åé©³æ€§çš„è¾©è®ºè¯­éŸ³ï¼Œé€‚ç”¨äºä»»ä½•å£°éŸ³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-17385833a2b97c6f15017584b50bb45f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9c6664588b783daa01fc6f2c67dd8b5a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-505ebf9aa4bf6bdb8d5e9f9023a117e2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-77cfc31b96495233f610573509a4fd18.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6348b0eb42c29d88b39ffc9c0f4099bb.jpg" align="middle">
</details>




<h2 id="CUIfy-the-XR-An-Open-Source-Package-to-Embed-LLM-powered-Conversational-Agents-in-XR"><a href="#CUIfy-the-XR-An-Open-Source-Package-to-Embed-LLM-powered-Conversational-Agents-in-XR" class="headerlink" title="CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational   Agents in XR"></a>CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational   Agents in XR</h2><p><strong>Authors:Kadir Burak Buldu, SÃ¼leyman Ã–zdel, Ka Hei Carrie Lau, Mengdi Wang, Daniel Saad, Sofie SchÃ¶nborn, Auxane Boch, Enkelejda Kasneci, Efe Bozkir</strong></p>
<p>Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering consumer-grade head-mounted displays (HMDs) in an affordable way, it is likely that XR will become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)â€“powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. In this paper, we provide the community with an open-source, customizable, extensible, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with various LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes the latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: <a target="_blank" rel="noopener" href="https://gitlab.lrz.de/hctl/cuify">https://gitlab.lrz.de/hctl/cuify</a> </p>
<blockquote>
<p>æœ€è¿‘çš„è®¡ç®—æœºå›¾å½¢å­¦ã€æœºå™¨å­¦ä¹ å’Œä¼ æ„Ÿå™¨æŠ€æœ¯çš„å‘å±•ä¸ºæ‰©å±•ç°å®ï¼ˆXRï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„è®¾ç½®æä¾›äº†ä¼—å¤šæœºä¼šï¼Œä»æŠ€èƒ½åŸ¹è®­åˆ°å¨±ä¹ã€‚éšç€å¤§å‹å…¬å¸ä»¥è´Ÿæ‹…å¾—èµ·çš„æ–¹å¼æä¾›é¢å‘æ¶ˆè´¹è€…çš„å¤´æˆ´å¼æ˜¾ç¤ºå™¨ï¼ˆHMDï¼‰ï¼ŒXRå¾ˆå¯èƒ½å˜å¾—æ™®åŠï¼ŒHMDå°†åƒæ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘ä¸€æ ·å‘å±•æˆä¸ºä¸ªäººè®¾å¤‡ã€‚ç„¶è€Œï¼Œæ‹¥æœ‰æ™ºèƒ½ç©ºé—´å’Œè‡ªç„¶äº¤äº’åŒæ ·é‡è¦ï¼Œè¿™æ ·ç”¨æˆ·æ‰èƒ½å¢åŠ ä»–ä»¬åœ¨è™šæ‹Ÿå’Œå¢å¼ºç©ºé—´ä¸­çš„å‚ä¸åº¦ã€‚ä¸ºæ­¤ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„éç©å®¶è§’è‰²ï¼ˆNPCï¼‰ä½¿ç”¨è¯­éŸ³è½¬æ–‡æœ¬ï¼ˆSTTï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æˆ–é¢„å…ˆè®¾å®šçš„NPCï¼Œä¸ºXRä¸­ä¿ƒè¿›æ›´è‡ªç„¶çš„å¯¹è¯å¼ç”¨æˆ·ç•Œé¢ï¼ˆCUIï¼‰å¸¦æ¥äº†å·¨å¤§ä¼˜åŠ¿ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºç¤¾åŒºæä¾›äº†ä¸€ä¸ªå¼€æºçš„ã€å¯å®šåˆ¶çš„ã€å¯æ‰©å±•çš„ã€å…·æœ‰éšç§æ„è¯†çš„UnityåŒ…ï¼Œåä¸ºCUIfyï¼Œå®ƒä¿ƒè¿›äº†åŸºäºè¯­éŸ³çš„NPC-ç”¨æˆ·ä¸å„ç§LLMã€STTå’ŒTTSæ¨¡å‹çš„äº¤äº’ã€‚æˆ‘ä»¬çš„åŒ…è¿˜æ”¯æŒæ¯ä¸ªç¯å¢ƒå¤šä¸ªLLMé©±åŠ¨çš„NPCï¼Œå¹¶é€šè¿‡æµæŠ€æœ¯æœ€å°åŒ–ä¸åŒè®¡ç®—æ¨¡å‹ä¹‹é—´çš„å»¶è¿Ÿï¼Œä»¥å®ç°ç”¨æˆ·å’ŒNPCä¹‹é—´çš„å¯ç”¨äº¤äº’ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ä»“åº“ä¸­å‘å¸ƒæºä»£ç ï¼š<a target="_blank" rel="noopener" href="https://gitlab.lrz.de/hctl/cuify">https://gitlab.lrz.de/hctl/cuify</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04671v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong><br>     è¿‘æœŸè®¡ç®—æœºå›¾å½¢å­¦ã€æœºå™¨å­¦ä¹ å’Œä¼ æ„Ÿå™¨æŠ€æœ¯çš„å‘å±•ä¸ºæ‰©å±•ç°å®ï¼ˆXRï¼‰æŠ€æœ¯åœ¨ç”Ÿæ´»ä¸­çš„åº”ç”¨æä¾›äº†æœºä¼šï¼Œå¦‚æŠ€èƒ½åŸ¹è®­å’Œå¨±ä¹ç­‰ã€‚éšç€å¤§å‹ä¼ä¸šæä¾›è´Ÿæ‹…å¾—èµ·çš„å¤´æˆ´å¼æ˜¾ç¤ºå™¨ï¼ˆHMDsï¼‰ï¼ŒXRå¯èƒ½å˜å¾—æ— å¤„ä¸åœ¨ï¼Œå¹¶æˆä¸ºåƒæ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘ä¸€æ ·çš„ä¸ªäººè®¾å¤‡ã€‚ä¸ºäº†å¢åŠ ç”¨æˆ·å¯¹è™šæ‹Ÿå’Œå¢å¼ºç©ºé—´çš„å‚ä¸åº¦ï¼Œæ™ºèƒ½ç©ºé—´å’Œè‡ªç„¶äº¤äº’å˜å¾—ä¸æŠ€æœ¯è¿›æ­¥åŒæ ·é‡è¦ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„éç©å®¶è§’è‰²ï¼ˆNPCsï¼‰ç»“åˆè¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰å’Œæ–‡æœ¬åˆæˆï¼ˆTTSï¼‰æ¨¡å‹ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæˆ–é¢„è®¾çš„NPCsï¼Œå¯ä¸ºXRä¸­æ„å»ºæ›´è‡ªç„¶çš„å¯¹è¯ç”¨æˆ·ç•Œé¢ï¼ˆCUIï¼‰å¸¦æ¥æ˜¾è‘—ä¼˜åŠ¿ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¼€æºã€å¯å®šåˆ¶ã€å¯æ‰©å±•ä¸”æ³¨é‡éšç§çš„UnityåŒ…â€œCUIfyâ€ï¼Œå®ƒä¿ƒè¿›äº†åŸºäºè¯­éŸ³çš„NPC-ç”¨æˆ·äº’åŠ¨ï¼Œæ”¯æŒå¤šç§LLMã€STTå’ŒTTSæ¨¡å‹ã€‚æˆ‘ä»¬çš„è½¯ä»¶åŒ…è¿˜æ”¯æŒæ¯ä¸ªç¯å¢ƒå¤šä¸ªLLMé©±åŠ¨NPCsï¼Œå¹¶é€šè¿‡æµæŠ€æœ¯æœ€å°åŒ–ä¸åŒè®¡ç®—æ¨¡å‹ä¹‹é—´çš„å»¶è¿Ÿï¼Œä»¥å®ç°ç”¨æˆ·å’ŒNPCsä¹‹é—´çš„å¯ç”¨äº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘æŠ€è¿›æ­¥ä½¿å¾—æ‰©å±•ç°å®ï¼ˆXRï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„è¿ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå°¤å…¶æ˜¯æŠ€èƒ½åŸ¹è®­å’Œå¨±ä¹é¢†åŸŸã€‚</li>
<li>å¤´æˆ´å¼æ˜¾ç¤ºå™¨ï¼ˆHMDsï¼‰å¯èƒ½å˜å¾—åƒæ™ºèƒ½æ‰‹æœºå’Œå¹³æ¿ç”µè„‘ä¸€æ ·æ™®åŠã€‚</li>
<li>è‡ªç„¶äº¤äº’å’Œæ™ºèƒ½ç©ºé—´å¯¹äºæé«˜ç”¨æˆ·åœ¨è™šæ‹Ÿå’Œå¢å¼ºç©ºé—´çš„å‚ä¸åº¦è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„éç©å®¶è§’è‰²ï¼ˆNPCsï¼‰ç»“åˆè¯­éŸ³è¯†åˆ«ï¼ˆSTTï¼‰å’Œæ–‡æœ¬åˆæˆï¼ˆTTSï¼‰æ¨¡å‹èƒ½æ„å»ºæ›´è‡ªç„¶çš„å¯¹è¯ç”¨æˆ·ç•Œé¢ï¼ˆCUIï¼‰ã€‚</li>
<li>â€œCUIfyâ€æ˜¯ä¸€æ¬¾å¼€æºçš„UnityåŒ…ï¼Œæ”¯æŒåŸºäºè¯­éŸ³çš„NPC-ç”¨æˆ·äº’åŠ¨ï¼Œå¹¶é‡‡ç”¨å¤šç§LLMã€STTå’ŒTTSæ¨¡å‹ã€‚</li>
<li>è¯¥è½¯ä»¶åŒ…æ”¯æŒå¤šä¸ªLLMé©±åŠ¨çš„NPCsåœ¨åŒä¸€ç¯å¢ƒä¸­ï¼Œå¹¶å®ç°ä¸ç”¨æˆ·çš„å¯ç”¨äº¤äº’ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-01c93d0f5eff6e163f1c503e6fca3557.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-f3669de0ea6c0b83666c25ce0f4b4363.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2e3bdc36395a0239cbdcd8e30c67a281.jpg" align="middle">
</details>




<h2 id="Complete-reconstruction-of-the-tongue-contour-through-acoustic-to-articulatory-inversion-using-real-time-MRI-data"><a href="#Complete-reconstruction-of-the-tongue-contour-through-acoustic-to-articulatory-inversion-using-real-time-MRI-data" class="headerlink" title="Complete reconstruction of the tongue contour through acoustic to   articulatory inversion using real-time MRI data"></a>Complete reconstruction of the tongue contour through acoustic to   articulatory inversion using real-time MRI data</h2><p><strong>Authors:Sofiane Azzouz, Pierre-AndrÃ© Vuissoz, Yves Laprie</strong></p>
<p>Acoustic articulatory inversion is a major processing challenge, with a wide range of applications from speech synthesis to feedback systems for language learning and rehabilitation. In recent years, deep learning methods have been applied to the inversion of less than a dozen geometrical positions corresponding to sensors glued to easily accessible articulators. It is therefore impossible to know the shape of the whole tongue from root to tip. In this work, we use high-quality real-time MRI data to track the contour of the tongue. The data used to drive the inversion are therefore the unstructured speech signal and the tongue contours. Several architectures relying on a Bi-MSTM including or not an autoencoder to reduce the dimensionality of the latent space, using or not the phonetic segmentation have been explored. The results show that the tongue contour can be recovered with a median accuracy of 2.21 mm (or 1.37 pixel) taking a context of 1 MFCC frame (static, delta and double-delta cepstral features). </p>
<blockquote>
<p>å£°å­¦å‘éŸ³å€’ç½®æ˜¯ä¸€ä¸ªä¸»è¦çš„å¤„ç†æŒ‘æˆ˜ï¼Œå…¶åº”ç”¨èŒƒå›´å¹¿æ³›ï¼Œä»è¯­éŸ³åˆæˆåˆ°è¯­è¨€å­¦ä¹ å’Œåº·å¤çš„åé¦ˆç³»ç»Ÿã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•è¢«åº”ç”¨äºå€’ç½®çš„å‡ ä½•ä½ç½®å°‘äºåå‡ ä¸ªï¼Œè¿™äº›ä½ç½®å¯¹åº”äºç²˜è´´åœ¨å®¹æ˜“æ¥è§¦çš„å‘éŸ³å™¨å®˜ä¸Šçš„ä¼ æ„Ÿå™¨ã€‚å› æ­¤ï¼Œæ— æ³•çŸ¥é“æ•´ä¸ªèˆŒå¤´çš„å½¢çŠ¶ï¼Œä»æ ¹éƒ¨åˆ°å°–ç«¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜è´¨é‡çš„å®æ—¶MRIæ•°æ®æ¥è·Ÿè¸ªèˆŒå¤´çš„è½®å»“ã€‚ç”¨äºé©±åŠ¨å€’ç½®çš„æ•°æ®å› æ­¤æ˜¯æ— ç»“æ„åŒ–çš„è¯­éŸ³ä¿¡å·å’ŒèˆŒå¤´è½®å»“ã€‚æˆ‘ä»¬æ¢ç´¢äº†å‡ ç§ä¾èµ–äºåŒå‘è®°å¿†å¢å¼ºç½‘ç»œï¼ˆBi-MSTMï¼‰çš„æ¶æ„ï¼ŒåŒ…æ‹¬ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œé™ç»´å’Œä¸ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨çš„æƒ…å†µï¼Œä»¥åŠæ˜¯å¦ä½¿ç”¨è¯­éŸ³åˆ†æ®µã€‚ç»“æœè¡¨æ˜ï¼ŒèˆŒè½®å»“çš„æ¢å¤ç²¾åº¦ä¸­ä½æ•°ä¸º2.21æ¯«ç±³ï¼ˆæˆ–1.37åƒç´ ï¼‰ï¼Œåœ¨1ä¸ªMFCCå¸§ï¼ˆé™æ€ã€deltaå’ŒåŒdeltaå€’è°±ç‰¹å¾ï¼‰çš„ä¸Šä¸‹æ–‡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02037v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†å£°å­¦å‘éŸ³åè½¬æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠæœ€æ–°è¿›å±•ã€‚é‡‡ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹èˆŒä½“è½®å»“è¿›è¡Œå®æ—¶è¿½è¸ªï¼Œä»¥çœŸå®MRIæ•°æ®ä¸ºé©±åŠ¨ï¼Œæé«˜å£°å­¦å‘éŸ³åè½¬çš„å‡†ç¡®æ€§ã€‚ç»“æœè¯æ˜ä½¿ç”¨MFCCæ¡†æ¶èƒ½æœ‰æ•ˆæé«˜é¢„æµ‹ç²¾åº¦ï¼Œé‡‡ç”¨è‡ªé€‚åº”ç¼–ç å™¨å¯ä»¥å‡å°‘æ½œåœ¨ç©ºé—´çš„ç»´åº¦ï¼Œå‡å°‘è¯­å¢ƒå˜åŒ–çš„è¯¯å·®å½±å“ã€‚ç ”ç©¶æˆæœå°†å½±å“å¤šä¸ªé¢†åŸŸï¼Œå¦‚è¯­éŸ³åˆæˆã€è¯­è¨€å­¦ä¹ å’Œåº·å¤ç­‰ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å£°å­¦å‘éŸ³åè½¬æ˜¯è¯­éŸ³åˆæˆç­‰é¢†åŸŸçš„é‡è¦å¤„ç†æŒ‘æˆ˜ã€‚è¿‘å¹´æ¥é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯è§£å†³äº†ä¸€äº›ç‰¹å®šé—®é¢˜ï¼Œä½†éš¾ä»¥å…¨é¢é¢„æµ‹æ•´ä¸ªèˆŒå¤´çš„å½¢çŠ¶ã€‚</li>
<li>æœ¬ç ”ç©¶åˆ©ç”¨é«˜è´¨é‡çš„å®æ—¶MRIæ•°æ®è¿½è¸ªèˆŒä½“è½®å»“ï¼Œä½¿å¾—å¯¹èˆŒä½“è½®å»“çš„é¢„æµ‹æ›´ä¸ºå‡†ç¡®ã€‚æ•°æ®é©±åŠ¨æ–¹æ³•é‡‡ç”¨éç»“æ„åŒ–è¯­éŸ³ä¿¡å·å’ŒèˆŒè½®å»“ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†å¤šç§æ¶æ„ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŒå‘å¤šæ¨¡æ€çŠ¶æ€æœºï¼ˆBi-MSTMï¼‰å’Œè‡ªé€‚åº”ç¼–ç å™¨ç­‰æŠ€æœ¯æ¥æé«˜é¢„æµ‹ç²¾åº¦å’Œé™ä½ç»´åº¦ç©ºé—´ã€‚åŒæ—¶æ¢ç´¢äº†ä½¿ç”¨è¯­éŸ³åˆ†å‰²æŠ€æœ¯çš„å½±å“ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨MFCCæ¡†æ¶èƒ½æé«˜é¢„æµ‹ç²¾åº¦ï¼Œé€šè¿‡è‡ªé€‚åº”ç¼–ç å™¨å‡å°‘æ½œåœ¨ç©ºé—´çš„ç»´åº¦ï¼Œè¿›ä¸€æ­¥æé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚åŒæ—¶æŒ‡å‡ºï¼Œé‡‡ç”¨MFCCæ¡†æ¶éœ€è¦è€ƒè™‘é™æ€ã€deltaå’ŒåŒdeltaé¢‘è°±ç‰¹å¾ç­‰å› ç´ ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7fb74e83742403c317574d24e9545782.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-393b50d09e491c1e398f5406e62bcdf5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-49c683b51101c495964a2baca9faba3a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-05eba35427b652975bf39f2eb047b6aa.jpg" align="middle">
</details>




<h2 id="Fish-Speech-Leveraging-Large-Language-Models-for-Advanced-Multilingual-Text-to-Speech-Synthesis"><a href="#Fish-Speech-Leveraging-Large-Language-Models-for-Advanced-Multilingual-Text-to-Speech-Synthesis" class="headerlink" title="Fish-Speech: Leveraging Large Language Models for Advanced Multilingual   Text-to-Speech Synthesis"></a>Fish-Speech: Leveraging Large Language Models for Advanced Multilingual   Text-to-Speech Synthesis</h2><p><strong>Authors:Shijia Liao, Yuxuan Wang, Tianyu Li, Yifan Cheng, Ruoyi Zhang, Rongzhi Zhou, Yijin Xing</strong></p>
<p>Text-to-Speech (TTS) systems face ongoing challenges in processing complex linguistic features, handling polyphonic expressions, and producing natural-sounding multilingual speech - capabilities that are crucial for future AI applications. In this paper, we present Fish-Speech, a novel framework that implements a serial fast-slow Dual Autoregressive (Dual-AR) architecture to enhance the stability of Grouped Finite Scalar Vector Quantization (GFSQ) in sequence generation tasks. This architecture improves codebook processing efficiency while maintaining high-fidelity outputs, making it particularly effective for AI interactions and voice cloning.   Fish-Speech leverages Large Language Models (LLMs) for linguistic feature extraction, eliminating the need for traditional grapheme-to-phoneme (G2P) conversion and thereby streamlining the synthesis pipeline and enhancing multilingual support. Additionally, we developed FF-GAN through GFSQ to achieve superior compression ratios and near 100% codebook utilization.   Our approach addresses key limitations of current TTS systems while providing a foundation for more sophisticated, context-aware speech synthesis. Experimental results show that Fish-Speech significantly outperforms baseline models in handling complex linguistic scenarios and voice cloning tasks, demonstrating its potential to advance TTS technology in AI applications. The implementation is open source at \href{<a target="_blank" rel="noopener" href="https://github.com/fishaudio/fish-speech%7D%7Bhttps://github.com/fishaudio/fish-speech%7D">https://github.com/fishaudio/fish-speech}{https://github.com/fishaudio/fish-speech}</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚çš„è¯­è¨€ç‰¹æ€§ã€å¤„ç†å¤šéŸ³è¡¨è¾¾å’Œäº§ç”Ÿè‡ªç„¶çš„å¤šè¯­è¨€è¯­éŸ³æ–¹é¢æŒç»­é¢ä¸´æŒ‘æˆ˜â€”â€”è¿™äº›èƒ½åŠ›æ˜¯æœªæ¥äººå·¥æ™ºèƒ½åº”ç”¨çš„å…³é”®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Fish-Speechï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå®ç°äº†ä¸²è¡Œå¿«é€Ÿæ…¢é€ŸåŒè‡ªå›å½’ï¼ˆDual-ARï¼‰æ¶æ„ï¼Œä»¥æé«˜åˆ†ç»„æœ‰é™æ ‡é‡çŸ¢é‡é‡åŒ–ï¼ˆGFSQï¼‰åœ¨åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­çš„ç¨³å®šæ€§ã€‚è¯¥æ¶æ„æé«˜äº†ä»£ç æœ¬å¤„ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸè¾“å‡ºï¼Œä½¿å…¶æˆä¸ºäººå·¥æ™ºèƒ½äº¤äº’å’Œè¯­éŸ³å…‹éš†çš„ç‰¹åˆ«æœ‰æ•ˆå·¥å…·ã€‚</p>
</blockquote>
<p>Fish-Speechåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè¯­è¨€ç‰¹æ€§æå–ï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿå­—æ¯åˆ°éŸ³èŠ‚ï¼ˆG2Pï¼‰è½¬æ¢çš„éœ€è¦ï¼Œä»è€Œç®€åŒ–äº†åˆæˆç®¡é“å¹¶å¢å¼ºäº†å¤šè¯­è¨€æ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡GFSQå¼€å‘äº†FF-GANï¼Œå®ç°äº†è¾ƒé«˜çš„å‹ç¼©ç‡å’Œè¿‘100%çš„ä»£ç æœ¬åˆ©ç”¨ç‡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.01156v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºFish-Speechçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨ä¸²è¡Œå¿«é€Ÿæ…¢é€ŸåŒè‡ªå›å½’æ¶æ„ï¼Œæé«˜åˆ†ç»„æœ‰é™æ ‡é‡çŸ¢é‡é‡åŒ–åœ¨åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­çš„ç¨³å®šæ€§ï¼Œå¢å¼ºä»£ç æœ¬å¤„ç†æ•ˆç‡å¹¶ä¿æŒé«˜ä¿çœŸè¾“å‡ºã€‚Fish-Speechåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­è¨€ç‰¹å¾æå–ï¼Œæ— éœ€ä¼ ç»Ÿçš„å­—æ¯åˆ°éŸ³ç´ è½¬æ¢ï¼Œä»è€Œç®€åŒ–äº†åˆæˆç®¡é“å¹¶å¢å¼ºäº†å¤šè¯­è¨€æ”¯æŒã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†ç»„æœ‰é™æ ‡é‡çŸ¢é‡é‡åŒ–å¼€å‘FF-GANå®ç°è¾ƒé«˜çš„å‹ç¼©ç‡å’Œè¿‘ç™¾åˆ†ä¹‹ç™¾çš„ä»£ç æœ¬åˆ©ç”¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFish-Speechåœ¨å¤„ç†å¤æ‚è¯­è¨€åœºæ™¯å’Œè¯­éŸ³å…‹éš†ä»»åŠ¡æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œä¸ºAIåº”ç”¨çš„æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯æä¾›äº†å‘å±•æ½œåŠ›ã€‚è¯¥æ¡†æ¶å·²å¼€æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Fish-Speechæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé‡‡ç”¨åŒè‡ªå›å½’æ¶æ„å¢å¼ºç¨³å®šæ€§ï¼Œé€‚ç”¨äºå¤æ‚çš„è¯­éŸ³åˆæˆä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶æé«˜äº†ä»£ç æœ¬å¤„ç†æ•ˆç‡å¹¶ä¿æŒé«˜ä¿çœŸè¾“å‡ºï¼Œç‰¹åˆ«é€‚ç”¨äºAIäº¤äº’å’Œè¯­éŸ³å…‹éš†ã€‚</li>
<li>Fish-Speechåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­è¨€ç‰¹å¾æå–ï¼Œæ— éœ€ä¼ ç»Ÿçš„å­—æ¯åˆ°éŸ³ç´ è½¬æ¢ï¼Œç®€åŒ–äº†åˆæˆæµç¨‹å¹¶å¢å¼ºäº†å¤šè¯­è¨€æ”¯æŒã€‚</li>
<li>é€šè¿‡åˆ†ç»„æœ‰é™æ ‡é‡çŸ¢é‡é‡åŒ–å¼€å‘FF-GANæŠ€æœ¯å®ç°é«˜æ•ˆå‹ç¼©å’Œä»£ç æœ¬åˆ©ç”¨ç‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºFish-Speechåœ¨å¤„ç†å¤æ‚è¯­è¨€åœºæ™¯å’Œè¯­éŸ³å…‹éš†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Fish-Speechæ¡†æ¶å…·æœ‰å¼€æ”¾æ€§æºä»£ç ï¼Œæœ‰åŠ©äºæ¨åŠ¨æ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯çš„å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a5403638a87984196e186e5fab898abc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f6d7bde981856b6596e25d43cb7b6c3b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-47f6136d6f52b9c9fdbe563201b22b82.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2d68c5a3331be5b973b1416928f218cc.jpg" align="middle">
</details>




<h2 id="Lina-Speech-Gated-Linear-Attention-is-a-Fast-and-Parameter-Efficient-Learner-for-text-to-speech-synthesis"><a href="#Lina-Speech-Gated-Linear-Attention-is-a-Fast-and-Parameter-Efficient-Learner-for-text-to-speech-synthesis" class="headerlink" title="Lina-Speech: Gated Linear Attention is a Fast and Parameter-Efficient   Learner for text-to-speech synthesis"></a>Lina-Speech: Gated Linear Attention is a Fast and Parameter-Efficient   Learner for text-to-speech synthesis</h2><p><strong>Authors:ThÃ©odor Lemerle, Harrison Vanderbyl, Vaibhav Srivastav, Nicolas Obin, Axel Roebel</strong></p>
<p>Neural codec language models have achieved state-of-the-art performance in text-to-speech (TTS) synthesis, leveraging scalable architectures like autoregressive transformers and large-scale speech datasets. By framing voice cloning as a prompt continuation task, these models excel at cloning voices from short audio samples. However, this approach is limited in its ability to handle numerous or lengthy speech excerpts, since the concatenation of source and target speech must fall within the maximum context length which is determined during training. In this work, we introduce Lina-Speech, a model that replaces traditional self-attention mechanisms with emerging recurrent architectures like Gated Linear Attention (GLA). Building on the success of initial-state tuning on RWKV, we extend this technique to voice cloning, enabling the use of multiple speech samples and full utilization of the context window in synthesis. This approach is fast, easy to deploy, and achieves performance comparable to fine-tuned baselines when the dataset size ranges from 3 to 15 minutes. Notably, Lina-Speech matches or outperforms state-of-the-art baseline models, including some with a parameter count up to four times higher or trained in an end-to-end style. We release our code and checkpoints. Audio samples are available at <a target="_blank" rel="noopener" href="https://theodorblackbird.github.io/blog/demo_lina/">https://theodorblackbird.github.io/blog/demo_lina/</a>. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ƒåˆ©ç”¨å¯æ‰©å±•çš„æ¶æ„ï¼ˆå¦‚è‡ªå›å½’å˜å‹å™¨ï¼‰å’Œå¤§è§„æ¨¡çš„è¯­éŸ³æ•°æ®é›†ã€‚é€šè¿‡å°†ä»¥å£°éŸ³å…‹éš†ä½œä¸ºæç¤ºå»¶ç»­ä»»åŠ¡ï¼Œè¿™äº›æ¨¡å‹åœ¨å…‹éš†çŸ­éŸ³é¢‘æ ·æœ¬çš„å£°éŸ³æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤§é‡æˆ–å†—é•¿çš„è¯­éŸ³ç‰‡æ®µæ—¶å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºæºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³çš„æ‹¼æ¥å¿…é¡»åœ¨è®­ç»ƒæœŸé—´ç¡®å®šçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ä¹‹å†…ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Lina-Speechæ¨¡å‹ï¼Œå®ƒç”¨æ–°å…´çš„å¾ªç¯æ¶æ„ï¼ˆå¦‚é—¨æ§çº¿æ€§æ³¨æ„åŠ›ï¼‰å–ä»£äº†ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬åœ¨RWKVåˆå§‹çŠ¶æ€è°ƒæ•´çš„æˆåŠŸåŸºç¡€ä¸Šï¼Œå°†å…¶æŠ€æœ¯æ‰©å±•åˆ°å£°éŸ³å…‹éš†ï¼Œèƒ½å¤Ÿä½¿ç”¨å¤šä¸ªè¯­éŸ³æ ·æœ¬å¹¶åœ¨åˆæˆä¸­å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çª—å£ã€‚è¿™ç§æ–¹æ³•å¿«é€Ÿä¸”æ˜“äºéƒ¨ç½²ï¼Œå½“æ•°æ®é›†å¤§å°åœ¨3åˆ°15åˆ†é’Ÿä¹‹é—´æ—¶ï¼Œå…¶æ€§èƒ½å¯ä¸å¾®è°ƒåŸºçº¿ç›¸å½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLina-Speechä¸æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ï¼ŒåŒ…æ‹¬ä¸€äº›å‚æ•°è®¡æ•°é«˜è¾¾å››å€æˆ–é‡‡ç”¨ç«¯åˆ°ç«¯é£æ ¼è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://theodorblackbird.github.io/blog/demo_lina/%E6%89%BE%E5%88%B0%E3%80%82">https://theodorblackbird.github.io/blog/demo_lina&#x2F;æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23320v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>     ç¥ç»ç½‘ç»œç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆä¸­å–å¾—äº†æœ€æ–°æ€§èƒ½ï¼Œä½¿ç”¨è‡ªå›å½’å˜å‹å™¨ç­‰å¯æ‰©å±•æ¶æ„å’Œå¤§è§„æ¨¡è¯­éŸ³æ•°æ®é›†ã€‚é€šè¿‡å°†è¢«å£°éŸ³å…‹éš†è§†ä½œæç¤ºå»¶ç»­ä»»åŠ¡ï¼Œè¿™äº›æ¨¡å‹åœ¨å…‹éš†çŸ­éŸ³é¢‘æ ·æœ¬çš„å£°éŸ³æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç”±äºåˆæˆä¸­æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³çš„æ‹¼æ¥å¿…é¡»åœ¨è®­ç»ƒæœŸé—´ç¡®å®šçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦å†…ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤§é‡æˆ–å†—é•¿çš„è¯­éŸ³ç‰‡æ®µæ—¶å­˜åœ¨å±€é™æ€§ã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Lina-Speechæ¨¡å‹ï¼Œå®ƒé‡‡ç”¨æ–°å…´å¾ªç¯æ¶æ„ï¼ˆå¦‚é—¨æ§çº¿æ€§æ³¨æ„åŠ›ï¼‰æ›¿ä»£ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨RWKVåˆå§‹çŠ¶æ€è°ƒæ•´æˆåŠŸçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†è¯¥æŠ€æœ¯æ‰©å±•åˆ°å£°éŸ³å…‹éš†ï¼Œå¯ä½¿ç”¨å¤šä¸ªè¯­éŸ³æ ·æœ¬å¹¶å……åˆ†åˆ©ç”¨åˆæˆä¸­çš„ä¸Šä¸‹æ–‡çª—å£ã€‚è¯¥æ–¹æ³•å¿«é€Ÿä¸”æ˜“äºéƒ¨ç½²ï¼Œå½“æ•°æ®é›†å¤§å°åœ¨3è‡³15åˆ†é’Ÿä¹‹é—´æ—¶ï¼Œå…¶æ€§èƒ½ä¸ç²¾ç»†è°ƒæ•´çš„åŸºçº¿ç›¸å½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLina-Speechä¸åŒ…æ‹¬ä¸€äº›å‚æ•°è®¡æ•°é«˜è¾¾å››å€æˆ–é‡‡ç”¨ç«¯åˆ°ç«¯è®­ç»ƒçš„å…ˆè¿›åŸºçº¿æ¨¡å‹ç›¸æ¯”å…·æœ‰åŒç­‰æˆ–æ›´å¥½çš„è¡¨ç°ã€‚æˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚<a target="_blank" rel="noopener" href="https://theodorblackbird.github.io/blog/demo_lina/">éŸ³é¢‘æ ·æœ¬é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œç¼–è§£ç å™¨è¯­è¨€æ¨¡å‹åœ¨TTSé¢†åŸŸè¡¨ç°å“è¶Šï¼Œåˆ©ç”¨è‡ªå›å½’å˜å‹å™¨ç­‰å¯æ‰©å±•æ¶æ„å’Œå¤§è§„æ¨¡è¯­éŸ³æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å°†è¢«å£°éŸ³å…‹éš†è§†ä½œæç¤ºå»¶ç»­ä»»åŠ¡ï¼Œè¿™äº›æ¨¡å‹åœ¨å…‹éš†çŸ­éŸ³é¢‘æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿è¯­éŸ³ç‰‡æ®µæ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ‹¼æ¥æºå’Œç›®æ ‡è¯­éŸ³ä¸”éœ€åœ¨è®­ç»ƒçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦å†…ã€‚</li>
<li>Lina-Speechæ¨¡å‹é‡‡ç”¨æ–°å…´å¾ªç¯æ¶æ„ï¼ˆå¦‚GLAï¼‰æ›¿ä»£ä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚</li>
<li>Lina-Speechæ‰©å±•åˆ°å£°éŸ³å…‹éš†ï¼Œèƒ½ä½¿ç”¨å¤šä¸ªè¯­éŸ³æ ·æœ¬å¹¶å……åˆ†åˆ©ç”¨åˆæˆä¸­çš„ä¸Šä¸‹æ–‡çª—å£ã€‚</li>
<li>Lina-Speechæ€§èƒ½ä¸ç²¾ç»†è°ƒæ•´çš„åŸºçº¿ç›¸å½“ï¼Œå°¤å…¶åœ¨æ•°æ®é›†å¤§å°é€‚ä¸­çš„æƒ…å†µä¸‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-c3ec2d81d77e5a8c9bedf192f66d67e6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-829a58c546ef46dfd7335ed96bdb42c2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-43505008ef2183e9853dcd6a11a84cc1.jpg" align="middle">
</details>




<h2 id="Augmenting-Polish-Automatic-Speech-Recognition-System-With-Synthetic-Data"><a href="#Augmenting-Polish-Automatic-Speech-Recognition-System-With-Synthetic-Data" class="headerlink" title="Augmenting Polish Automatic Speech Recognition System With Synthetic   Data"></a>Augmenting Polish Automatic Speech Recognition System With Synthetic   Data</h2><p><strong>Authors:Åukasz Bondaruk, Jakub Kubiak, Mateusz CzyÅ¼nikiewicz</strong></p>
<p>This paper presents a system developed for submission to Poleval 2024, Task 3: Polish Automatic Speech Recognition Challenge. We describe Voicebox-based speech synthesis pipeline and utilize it to augment Conformer and Whisper speech recognition models with synthetic data. We show that addition of synthetic speech to training improves achieved results significantly. We also present final results achieved by our models in the competition. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸ºPoleval 2024çš„Task 3ï¼šPolishè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŒ‘æˆ˜èµ›è€Œå¼€å‘çš„ç³»ç»Ÿã€‚æˆ‘ä»¬æè¿°äº†åŸºäºVoiceboxçš„è¯­éŸ³åˆæˆæµç¨‹ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºConformerå’ŒWhisperè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„åˆæˆæ•°æ®ã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨è®­ç»ƒä¸­åŠ å…¥åˆæˆè¯­éŸ³å¯ä»¥æ˜¾è‘—æé«˜æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¯”èµ›ä¸­çš„æœ€ç»ˆæˆç»©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22903v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªä¸ºPoleval 2024çš„Task 3ï¼šPolish Automatic Speech Recognition Challengeè€Œå¼€å‘çš„ç³»ç»Ÿã€‚æ–‡ä¸­æè¿°äº†åŸºäºVoiceboxçš„è¯­éŸ³åˆæˆæµç¨‹ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºConformerå’ŒWhisperè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒæ•°æ®ï¼ŒåŠ å…¥åˆæˆè¯­éŸ³èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†åœ¨æ¯”èµ›ä¸­çš„æœ€ç»ˆæˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡ä¸ºPoleval 2024çš„Task 3æŒ‘æˆ˜å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿã€‚</li>
<li>è®ºæ–‡æè¿°äº†åŸºäºVoiceboxçš„è¯­éŸ³åˆæˆæµç¨‹ã€‚</li>
<li>åˆæˆè¯­éŸ³æ•°æ®è¢«ç”¨äºå¢å¼ºConformerå’ŒWhisperè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒã€‚</li>
<li>åŠ å…¥åˆæˆè¯­éŸ³èƒ½æ˜¾è‘—æé«˜è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è®ºæ–‡å±•ç¤ºäº†åœ¨æ¯”èµ›ä¸­çš„æœ€ç»ˆæˆç»©ã€‚</li>
<li>è¯¥ç³»ç»Ÿåˆ©ç”¨åˆæˆæ•°æ®å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9d72bec1505ca4571213b48cafbf2e3e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c0a1a92a6a41d30b26845226fa66c85d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d900580161e83bdd93e1ed490c2a326b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ab756f9dbd8046d3e36c3ee2de087ba5.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-27275a059155a952ca08cfb0e002eb30.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-00adfdd07f45ecdbd1d6b769740aa458.jpg" align="middle">
</details>




<h2 id="Very-Attentive-Tacotron-Robust-and-Unbounded-Length-Generalization-in-Autoregressive-Transformer-Based-Text-to-Speech"><a href="#Very-Attentive-Tacotron-Robust-and-Unbounded-Length-Generalization-in-Autoregressive-Transformer-Based-Text-to-Speech" class="headerlink" title="Very Attentive Tacotron: Robust and Unbounded Length Generalization in   Autoregressive Transformer-Based Text-to-Speech"></a>Very Attentive Tacotron: Robust and Unbounded Length Generalization in   Autoregressive Transformer-Based Text-to-Speech</h2><p><strong>Authors:Eric Battenberg, RJ Skerry-Ryan, Daisy Stanton, Soroosh Mariooryad, Matt Shannon, Julian Salazar, David Kao</strong></p>
<p>Autoregressive (AR) Transformer-based sequence models are known to have difficulty generalizing to sequences longer than those seen during training. When applied to text-to-speech (TTS), these models tend to drop or repeat words or produce erratic output, especially for longer utterances. In this paper, we introduce enhancements aimed at AR Transformer-based encoder-decoder TTS systems that address these robustness and length generalization issues. Our approach uses an alignment mechanism to provide cross-attention operations with relative location information. The associated alignment position is learned as a latent property of the model via backprop and requires no external alignment information during training. While the approach is tailored to the monotonic nature of TTS input-output alignment, it is still able to benefit from the flexible modeling power of interleaved multi-head self- and cross-attention operations. A system incorporating these improvements, which we call Very Attentive Tacotron, matches the naturalness and expressiveness of a baseline T5-based TTS system, while eliminating problems with repeated or dropped words and enabling generalization to any practical utterance length. </p>
<blockquote>
<p>åŸºäºè‡ªå›å½’ï¼ˆARï¼‰çš„Transformeråºåˆ—æ¨¡å‹åœ¨æ¨å¹¿åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§çš„é•¿åºåˆ—æ—¶ï¼Œå­˜åœ¨æ™®éåŒ–å›°éš¾çš„é—®é¢˜ã€‚å½“åº”ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ—¶ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šå‡ºç°é—æ¼æˆ–é‡å¤å•è¯çš„æƒ…å†µï¼Œæˆ–è€…äº§ç”Ÿä¸è§„åˆ™çš„è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒé•¿çš„è¿ç»­å‘è¨€ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é’ˆå¯¹åŸºäºAR Transformerçš„ç¼–ç å™¨-è§£ç å™¨TTSç³»ç»Ÿçš„æ”¹è¿›æ–¹æ¡ˆï¼Œä»¥è§£å†³è¿™äº›é²æ£’æ€§å’Œé•¿åº¦æ³›åŒ–é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¯¹é½æœºåˆ¶æ¥æä¾›å¸¦æœ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„äº¤å‰æ³¨æ„æ“ä½œã€‚ç›¸å…³çš„å¯¹é½ä½ç½®æ˜¯é€šè¿‡åå‘ä¼ æ’­ä½œä¸ºæ¨¡å‹çš„æ½œåœ¨å±æ€§æ¥å­¦ä¹ çš„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸éœ€è¦å¤–éƒ¨çš„å¯¹é½ä¿¡æ¯ã€‚è™½ç„¶è¯¥æ–¹æ³•é’ˆå¯¹TTSè¾“å…¥è¾“å‡ºå¯¹é½çš„å•è°ƒæ€§è´¨è¿›è¡Œäº†å®šåˆ¶ï¼Œä½†å®ƒä»ç„¶èƒ½å¤Ÿä»äº¤æ›¿çš„å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„æ“ä½œä¸­è·å¾—çµæ´»çš„å»ºæ¨¡èƒ½åŠ›ã€‚ç»“åˆäº†è¿™äº›æ”¹è¿›çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºéå¸¸ä¸“æ³¨çš„Tacotronï¼Œå®ƒä¸åŸºäºT5çš„åŸºçº¿TTSç³»ç»Ÿçš„è‡ªç„¶åº¦å’Œè¡¨è¾¾åŠ›ç›¸åŒ¹é…ï¼ŒåŒæ—¶è§£å†³äº†é‡å¤æˆ–é—æ¼å•è¯çš„é—®é¢˜ï¼Œå¹¶å®ç°äº†å¯¹ä»»ä½•å®é™…å‘è¨€é•¿åº¦çš„æ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22179v1">PDF</a> Submitted to NAACL</p>
<p><strong>æ€»ç»“</strong></p>
<p>åŸºäºè‡ªå›å½’ï¼ˆARï¼‰Transformerçš„åºåˆ—æ¨¡å‹åœ¨åº”ç”¨äºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰è½¬æ¢æ—¶ï¼Œå¯¹äºè®­ç»ƒæœŸé—´æœªæ¥è§¦åˆ°çš„è¾ƒé•¿åºåˆ—ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å­˜åœ¨é—®é¢˜ã€‚è¿™ç±»æ¨¡å‹åœ¨é•¿è¯­å¥è½¬æ¢ä¸­å®¹æ˜“å‡ºç°é—æ¼ã€é‡å¤è¯æ±‡æˆ–äº§ç”Ÿä¸ç¨³å®šçš„è¾“å‡ºã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹åŸºäºAR Transformerçš„ç¼–ç å™¨-è§£ç å™¨TTSç³»ç»Ÿçš„å¢å¼ºæ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜å¹¶æ”¹å–„å…¶åœ¨é•¿åº¦ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å¯¹é½æœºåˆ¶ï¼Œè¯¥æ–¹æ¡ˆä¸ºäº¤å‰æ³¨æ„åŠ›æ“ä½œæä¾›ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ç›¸å…³çš„å¯¹é½ä½ç½®æ˜¯ä½œä¸ºæ¨¡å‹çš„æ½œåœ¨å±æ€§é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ çš„ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€å¤–éƒ¨å¯¹é½ä¿¡æ¯ã€‚å°½ç®¡è¿™ä¸€æ–¹æ¡ˆæ˜¯é’ˆå¯¹TTSè¾“å…¥è¾“å‡ºå¯¹é½çš„å•è°ƒæ€§è´¨çš„ï¼Œä½†å®ƒä»èƒ½ä»äº¤ç»‡çš„å¤šå¤´è‡ªæˆ‘å’Œäº¤å‰æ³¨æ„åŠ›æ“ä½œä¸­è·ç›Šï¼Œæä¾›çµæ´»çš„å»ºæ¨¡èƒ½åŠ›ã€‚é›†æˆäº†è¿™äº›æ”¹è¿›çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œéå¸¸ä¸“æ³¨çš„Tacotronâ€ï¼Œå®ƒä¸åŸºäºT5çš„TTSç³»ç»Ÿçš„è‡ªç„¶åº¦å’Œè¡¨ç°åŠ›ç›¸åŒ¹é…ï¼ŒåŒæ—¶è§£å†³äº†è¯æ±‡é‡å¤æˆ–é—æ¼çš„é—®é¢˜ï¼Œå¹¶å®ç°äº†å¯¹ä»»ä½•å®é™…è¯­å¥é•¿åº¦çš„æ³›åŒ–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºè‡ªå›å½’ï¼ˆARï¼‰Transformerçš„åºåˆ—æ¨¡å‹åœ¨TTSä¸­é¢ä¸´æ³›åŒ–é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿è¯­å¥æ—¶ã€‚</li>
<li>å¼•å…¥å¯¹é½æœºåˆ¶æ¥æ”¹å–„AR Transformeræ¨¡å‹çš„æ€§èƒ½ï¼Œä¸ºäº¤å‰æ³¨æ„åŠ›æ“ä½œæä¾›ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚</li>
<li>å¯¹é½ä½ç½®æ˜¯æ¨¡å‹çš„æ½œåœ¨å±æ€§ï¼Œé€šè¿‡åå‘ä¼ æ’­å­¦ä¹ ï¼Œæ— éœ€å¤–éƒ¨å¯¹é½ä¿¡æ¯ã€‚</li>
<li>å¢å¼ºæ–¹æ¡ˆé€‚ç”¨äºTTSçš„è¾“å…¥è¾“å‡ºå¯¹é½çš„å•è°ƒæ€§è´¨ï¼ŒåŒæ—¶ä¿ç•™å¤šå¤´è‡ªæˆ‘å’Œäº¤å‰æ³¨æ„åŠ›æ“ä½œçš„çµæ´»å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>â€œéå¸¸ä¸“æ³¨çš„Tacotronâ€ç³»ç»ŸåŒ¹é…äº†T5-based TTSç³»ç»Ÿçš„è‡ªç„¶åº¦å’Œè¡¨ç°åŠ›ã€‚</li>
<li>è¯¥ç³»ç»Ÿè§£å†³äº†è¯æ±‡é‡å¤æˆ–é—æ¼çš„é—®é¢˜ã€‚</li>
<li>ç³»ç»Ÿå®ç°äº†å¯¹ä»»ä½•å®é™…è¯­å¥é•¿åº¦çš„æ³›åŒ–ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-d17fe4be3803b164dfeca6bf926af9b6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-3086c67c1062dd472b75b3037e6e444f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d665209ab8f2a6a30ff6759ebac62700.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-ea9eefa43c61920a2ae21c33f9f32d9c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-dacd28ed4d79554799d8978b69e19dfb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c34569f10757919dc723a6570d704081.jpg" align="middle">
</details>




<h2 id="Fast-and-High-Quality-Auto-Regressive-Speech-Synthesis-via-Speculative-Decoding"><a href="#Fast-and-High-Quality-Auto-Regressive-Speech-Synthesis-via-Speculative-Decoding" class="headerlink" title="Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative   Decoding"></a>Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative   Decoding</h2><p><strong>Authors:Bohan Li, Hankun Wang, Situo Zhang, Yiwei Guo, Kai Yu</strong></p>
<p>The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens. </p>
<blockquote>
<p>è‡ªå›å½’æ¶æ„ï¼Œå¦‚GPTï¼Œåœ¨ç°ä»£æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå®ƒäº§ç”Ÿäº†å¤§é‡çš„æ¨ç†æ—¶é—´ï¼Œç‰¹åˆ«æ˜¯ç”±äºè¯­éŸ³ä»¤ç‰Œåºåˆ—é•¿åº¦æ‰€å¸¦æ¥çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†VADUSAï¼Œè¿™æ˜¯é€šè¿‡é¢„æµ‹è§£ç åŠ é€Ÿè‡ªå›å½’TTSçš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒVADUSAä¸ä»…æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè€Œä¸”é€šè¿‡èå…¥è‰ç¨¿å¤´ä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹æœªæ¥è¯­éŸ³å†…å®¹ï¼Œæé«˜äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé‡‡æ ·è¿‡ç¨‹ä¸­å®¹é”™æœºåˆ¶çš„åŠ å…¥åœ¨ä¸å½±å“è´¨é‡çš„æƒ…å†µä¸‹åŠ é€Ÿäº†æ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†å’Œå„ç§è¯­éŸ³ä»¤ç‰Œç±»å‹ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21951v1">PDF</a> 5 pages, 3 figures, 3 tables. Submitted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿä¸­å¹¿æ³›ä½¿ç”¨çš„è‡ªå›å½’æ¶æ„ï¼ˆå¦‚GPTï¼‰å­˜åœ¨çš„æ¨ç†æ—¶é—´è¾ƒé•¿çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†VADUSAæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é¢„æµ‹æœªæ¥è¯­éŸ³å†…å®¹æ¥åŠ é€Ÿè‡ªå›å½’TTSçš„æ¨æµ‹è§£ç æ–¹æ³•ã€‚VADUSAä¸ä»…æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼Œè¿˜é€šè¿‡å¼•å…¥è‰æ¡ˆå¤´æ¥æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé‡‡æ ·è¿‡ç¨‹ä¸­çš„å®¹å¿æœºåˆ¶å¯ä»¥åŠ é€Ÿæ¨ç†è€Œä¸å½±å“è´¨é‡ã€‚è¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†å’Œå„ç§è¯­éŸ³æ ‡è®°ç±»å‹ä¸Šå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªå›å½’æ¶æ„ï¼ˆå¦‚GPTï¼‰åœ¨TTSç³»ç»Ÿä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨æ¨ç†æ—¶é—´é•¿çš„é—®é¢˜ã€‚</li>
<li>VADUSAæ˜¯é¦–ä¸ªé€šè¿‡æ¨æµ‹è§£ç æ¥åŠ é€Ÿè‡ªå›å½’TTSçš„æ–¹æ³•ã€‚</li>
<li>VADUSAé€šè¿‡å¼•å…¥è‰æ¡ˆå¤´é¢„æµ‹æœªæ¥è¯­éŸ³å†…å®¹ï¼Œæé«˜æ¨ç†é€Ÿåº¦å’Œæ€§èƒ½ã€‚</li>
<li>é‡‡æ ·è¿‡ç¨‹ä¸­çš„å®¹å¿æœºåˆ¶å¯ä»¥åŠ é€Ÿæ¨ç†ï¼ŒåŒæ—¶ä¸æŸå®³è´¨é‡ã€‚</li>
<li>VADUSAæ–¹æ³•å¯¹å¤§è§„æ¨¡æ•°æ®é›†å’Œå„ç§è¯­éŸ³æ ‡è®°ç±»å‹å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
<li>VADUSAçš„å¼•å…¥æ˜¯å¯¹ç°æœ‰TTSç³»ç»Ÿæ€§èƒ½æå‡çš„é‡è¦çªç ´ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-be0e7ec057e787292d165846a81b7bf0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-9e680ac807c2813ca3622539239fee16.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-f1775d3f578ff2e5ed68674627bec7cc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-d5fce1e312956bf61d88aafb350a6c6f.jpg" align="middle">
</details>




<h2 id="Enhancing-TTS-Stability-in-Hebrew-using-Discrete-Semantic-Units"><a href="#Enhancing-TTS-Stability-in-Hebrew-using-Discrete-Semantic-Units" class="headerlink" title="Enhancing TTS Stability in Hebrew using Discrete Semantic Units"></a>Enhancing TTS Stability in Hebrew using Discrete Semantic Units</h2><p><strong>Authors:Ella Zeldes, Or Tal, Yossi Adi</strong></p>
<p>This study introduces a refined approach to Text-to-Speech (TTS) generation that significantly enhances sampling stability across languages, with a particular focus on Hebrew. By leveraging discrete semantic units with higher phonetic correlation obtained from a self-supervised model, our method addresses the inherent instability often encountered in TTS systems, especially those dealing with non-diacriticized scripts like Hebrew. Utilizing HuBERT codes, our model generates discrete representations that are optimized for TTS tasks, thereby reducing the dependency on diacritic-based text processing. This advancement not only simplifies the language modeling process but also improves the robustness and shows controllability of the speech output due to disentenglement properties of the semantic units. The inclusion of a speaker embedding in the vocoder further aids in capturing the unique vocal characteristics of the speaker, contributing to the naturalness of the synthesized speech. Our experimental results demonstrate that this approach not only maintains high performance in Hebrew but also shows adaptability to English, underscoring its effectiveness in enhancing stability in TTS systems universally. Our method, named LOTHM (Language of The Hebrew Man), outperforms existing methods in terms of stability while achieving naturalness and speaker similarity on par with previous methods, making it a compelling choice for future speech synthesis applications. Samples can be found in our page pages.cs.huji.ac.il&#x2F;adiyoss-lab&#x2F;LoTHM . </p>
<blockquote>
<p>æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§ç²¾ç»†çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è·¨è¯­è¨€çš„é‡‡æ ·ç¨³å®šæ€§ï¼Œç‰¹åˆ«ä¸“æ³¨äºå¸Œä¼¯æ¥è¯­ã€‚é€šè¿‡åˆ©ç”¨ä»è‡ªç›‘ç£æ¨¡å‹ä¸­è·å¾—çš„é«˜è¯­éŸ³ç›¸å…³ç¦»æ•£è¯­ä¹‰å•å…ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†TTSç³»ç»Ÿä¸­ç»å¸¸é‡åˆ°çš„å›ºæœ‰ä¸ç¨³å®šé—®é¢˜ï¼Œå°¤å…¶æ˜¯å¤„ç†å¸Œä¼¯æ¥è¯­ç­‰éå˜éŸ³ç¬¦å·è„šæœ¬æ—¶ã€‚åˆ©ç”¨HuBERTä»£ç ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆé’ˆå¯¹TTSä»»åŠ¡ä¼˜åŒ–çš„ç¦»æ•£è¡¨ç¤ºï¼Œä»è€Œå‡å°‘äº†åŸºäºå˜éŸ³ç¬¦å·çš„æ–‡æœ¬å¤„ç†çš„ä¾èµ–æ€§ã€‚è¿™ä¸€è¿›å±•ä¸ä»…ç®€åŒ–äº†è¯­è¨€å»ºæ¨¡è¿‡ç¨‹ï¼Œè€Œä¸”é€šè¿‡è¯­ä¹‰å•å…ƒçš„è§£çº ç¼ å±æ€§æé«˜äº†é²æ£’æ€§ï¼Œå¹¶æ˜¾ç¤ºå‡ºè¯­éŸ³è¾“å‡ºçš„å¯æ§æ€§ã€‚åœ¨vocoderä¸­åŠ å…¥è¯´è¯äººåµŒå…¥è¿˜æœ‰åŠ©äºæ•æ‰è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ï¼Œä¸ºåˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦åšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸€æ–¹æ³•ä¸ä»…åœ¨å¸Œä¼¯æ¥è¯­ä¸­ä¿æŒé«˜æ€§èƒ½ï¼Œè€Œä¸”æ˜¾ç¤ºå‡ºå¯¹è‹±è¯­çš„é€‚åº”æ€§ï¼Œå¼ºè°ƒäº†å…¶åœ¨æ™®éæé«˜TTSç³»ç»Ÿç¨³å®šæ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åä¸ºLOTHMï¼ˆå¸Œä¼¯æ¥äººçš„è¯­è¨€ï¼‰ï¼Œåœ¨ç¨³å®šæ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶åœ¨è‡ªç„¶åº¦å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢ä¸ä»¥å‰çš„æ–¹æ³•ç›¸å½“ï¼Œå› æ­¤å®ƒæ˜¯æœªæ¥è¯­éŸ³åˆæˆåº”ç”¨çš„æœ‰åŠ›é€‰æ‹©ã€‚æ ·æœ¬å¯åœ¨æˆ‘ä»¬çš„é¡µé¢æ‰¾åˆ°pages.cs.huji.ac.il&#x2F;adiyoss-lab&#x2F;LoTHMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ”¹è¿›çš„æ–‡ç”Ÿè¯­éŸ³ï¼ˆTTSï¼‰ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨è‡ªç›‘ç£æ¨¡å‹è·å¾—çš„æ›´é«˜è¯­éŸ³ç›¸å…³æ€§çš„ç¦»æ•£è¯­ä¹‰å•å…ƒï¼Œæ˜¾è‘—æé«˜äº†è·¨è¯­è¨€çš„é‡‡æ ·ç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸Œä¼¯æ¥è¯­ç­‰éå˜éŸ³è„šæœ¬çš„å¤„ç†ä¸Šã€‚å€ŸåŠ©HuBERTä»£ç ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆäº†é’ˆå¯¹TTSä»»åŠ¡ä¼˜åŒ–çš„ç¦»æ•£è¡¨ç¤ºå½¢å¼ï¼Œå‡å°‘äº†åŸºäºå˜éŸ³çš„æ–‡æœ¬å¤„ç†çš„ä¾èµ–æ€§ã€‚è¯¥ç ”ç©¶ä¸ä»…ç®€åŒ–äº†è¯­è¨€å»ºæ¨¡è¿‡ç¨‹ï¼Œè¿˜æé«˜äº†è¯­éŸ³è¾“å‡ºçš„ç¨³å¥æ€§å’Œå¯æ§æ€§ã€‚æ­¤å¤–ï¼Œå°†è¯´è¯äººåµŒå…¥åˆ°vocoderä¸­è¿˜æœ‰åŠ©äºæ•æ‰è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ï¼Œä¸ºåˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦åšå‡ºè´¡çŒ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…åœ¨å¸Œä¼¯æ¥è¯­ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè€Œä¸”é€‚åº”äºè‹±è¯­ï¼Œå‡¸æ˜¾å…¶åœ¨æé«˜TTSç³»ç»Ÿç¨³å®šæ€§æ–¹é¢çš„æ™®éæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„TTSç”Ÿæˆæ–¹æ³•ï¼Œæé«˜äº†è·¨è¯­è¨€çš„é‡‡æ ·ç¨³å®šæ€§ã€‚</li>
<li>æ–¹æ³•é‡ç‚¹å…³æ³¨å¸Œä¼¯æ¥è¯­ç­‰éå˜éŸ³è„šæœ¬çš„å¤„ç†ï¼Œè§£å†³äº†TTSç³»ç»Ÿå¸¸è§çš„å†…åœ¨ä¸ç¨³å®šé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨è‡ªç›‘ç£æ¨¡å‹çš„ç¦»æ•£è¯­ä¹‰å•å…ƒï¼Œé€šè¿‡HuBERTä»£ç ä¼˜åŒ–TTSä»»åŠ¡ã€‚</li>
<li>æ–¹æ³•ç®€åŒ–äº†è¯­è¨€å»ºæ¨¡è¿‡ç¨‹ï¼Œæé«˜äº†è¯­éŸ³è¾“å‡ºçš„ç¨³å¥æ€§å’Œå¯æ§æ€§ã€‚</li>
<li>Vocoderä¸­åµŒå…¥çš„è¯´è¯äººç‰¹å¾æœ‰åŠ©äºæ•æ‰è¯´è¯äººçš„ç‹¬ç‰¹è¯­éŸ³ç‰¹å¾ï¼Œå¢å¼ºäº†åˆæˆè¯­éŸ³çš„è‡ªç„¶åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºå¸Œä¼¯æ¥è¯­ï¼Œä¹Ÿé€‚åº”äºè‹±è¯­ï¼Œå…·æœ‰æ™®éæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8eea4b54410dbb61e2332701db986fda.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-001c0d78ac4dd0b95d158355608c06a3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6bbcd167cd026ffb890765d98e44ee34.jpg" align="middle">
</details>




<h2 id="Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming"><a href="#Mini-Omni-Language-Models-Can-Hear-Talk-While-Thinking-in-Streaming" class="headerlink" title="Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming"></a>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</h2><p><strong>Authors:Zhifei Xie, Changqiao Wu</strong></p>
<p>Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original modelâ€™s language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method â€œAny Model Can Talkâ€. We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research. </p>
<blockquote>
<p>è¿‘æœŸè¯­è¨€æ¨¡å‹é¢†åŸŸçš„è¿›å±•å–å¾—äº†æ˜¾è‘—æˆå°±ã€‚GPT-4oä½œä¸ºä¸€ä¸ªæ–°é‡Œç¨‹ç¢‘ï¼Œå®ç°äº†ä¸äººç±»å®æ—¶å¯¹è¯çš„èƒ½åŠ›ï¼Œå±•ç°å‡ºè¿‘ä¼¼äººç±»çš„è‡ªç„¶æµç•…åº¦ã€‚è¿™ç§äººæœºäº¤äº’éœ€è¦æ¨¡å‹å…·å¤‡ç›´æ¥ä»¥éŸ³é¢‘æ¨¡å¼è¿›è¡Œæ¨ç†å¹¶æµå¼ç”Ÿæˆè¾“å‡ºçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰å­¦æœ¯æ¨¡å‹å°šæ— æ³•å®ç°è¿™ä¸€åŠŸèƒ½ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¾èµ–äºé¢å¤–çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè¿›è¡Œè¯­éŸ³åˆæˆï¼Œå¯¼è‡´ä¸ç†æƒ³çš„å»¶è¿Ÿã€‚æœ¬æ–‡ä»‹ç»äº†Mini-Omniï¼Œä¸€ä¸ªåŸºäºéŸ³é¢‘çš„ç«¯åˆ°ç«¯å¯¹è¯æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚ä¸ºå®ç°æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–‡æœ¬æŒ‡å¯¼çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡‡ç”¨æ‰¹é‡å¹¶è¡Œç­–ç•¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜æœ‰åŠ©äºä¿æŒåŸå§‹æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›è½»å¾®é€€åŒ–ï¼Œä½¿å…¶ä»–å·¥ä½œèƒ½å¤Ÿå»ºç«‹å®æ—¶äº¤äº’èƒ½åŠ›ã€‚æˆ‘ä»¬å°†è¿™ç§è®­ç»ƒæ–¹æ³•ç§°ä¸ºâ€œä»»ä½•æ¨¡å‹éƒ½èƒ½è¯´è¯â€ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†VoiceAssistant-400Kæ•°æ®é›†ï¼Œç”¨äºå¯¹ä¼˜åŒ–è¯­éŸ³è¾“å‡ºçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMini-Omniæ˜¯é¦–ä¸ªå®Œå…¨ç«¯åˆ°ç«¯ã€å¼€æºçš„å®æ—¶è¯­éŸ³äº¤äº’æ¨¡å‹ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†å®è´µçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16725v3">PDF</a> Technical report, work in progress. Demo and code:   <a target="_blank" rel="noopener" href="https://github.com/gpt-omni/mini-omni">https://github.com/gpt-omni/mini-omni</a></p>
<p><strong>Summary</strong><br>     æœ€æ–°è¯­è¨€æ¨¡å‹è¿›å±•æ˜¾è‘—ï¼ŒGPT-4oå®ç°äº†å®æ—¶å¯¹è¯ã€‚å½“å‰æ¨¡å‹éœ€å€ŸåŠ©é¢å¤–æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿè¿›è¡Œè¯­éŸ³åˆæˆï¼Œå¯¼è‡´å»¶è¿Ÿã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§éŸ³é¢‘ç«¯åˆ°ç«¯çš„å¯¹è¯æ¨¡å‹Mini-Omniï¼Œå¯å®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚é‡‡ç”¨æ–‡æœ¬æŒ‡å¯¼çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•å’Œæ‰¹é‡å¹¶è¡Œæ¨ç†ç­–ç•¥æå‡æ€§èƒ½ï¼Œä¿æŒåŸæœ‰è¯­è¨€åŠŸèƒ½ï¼Œå¼•å…¥VoiceAssistant-400Kæ•°æ®é›†ä¼˜åŒ–è¯­éŸ³è¾“å‡ºæ¨¡å‹ã€‚Mini-Omniæ˜¯é¦–ä¸ªå…¨ç«¯ã€å¼€æºçš„å®æ—¶è¯­éŸ³äº¤äº’æ¨¡å‹ï¼Œå…·æœ‰æ½œåœ¨çš„ç ”ç©¶ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4oå®ç°äº†å®æ—¶å¯¹è¯ï¼Œå±•ç°äº†è¿‘ä¹äººç±»è‡ªç„¶æµç•…åº¦ã€‚</li>
<li>å½“å‰æ¨¡å‹ä¾èµ–äºé¢å¤–çš„TTSç³»ç»Ÿè¿›è¡Œè¯­éŸ³åˆæˆï¼Œé€ æˆå»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>Mini-Omniæ˜¯ä¸€ç§éŸ³é¢‘ç«¯åˆ°ç«¯çš„å¯¹è¯æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å®æ—¶è¯­éŸ³äº¤äº’ã€‚</li>
<li>Mini-Omnié‡‡ç”¨æ–‡æœ¬æŒ‡å¯¼çš„è¯­éŸ³ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>æ‰¹é‡å¹¶è¡Œç­–ç•¥è¢«ç”¨äºè¿›ä¸€æ­¥æå‡Mini-Omniçš„æ€§èƒ½ã€‚</li>
<li>Mini-Omniä¿æŒåŸæœ‰è¯­è¨€åŠŸèƒ½çš„åŒæ—¶å®ç°äº†å®æ—¶äº¤äº’èƒ½åŠ›çš„æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-2bc5a1cc9e49bdeb2bb93e564870560f.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-65205ae6b15cfac1ebb1b53671bdf6bd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-16a663ab63b6a0b7ea62a7c36d45cbf6.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6f6edc5df9a1a7deaa89927cf545f98c.jpg" align="middle">
</details>




<h2 id="AudioMarkBench-Benchmarking-Robustness-of-Audio-Watermarking"><a href="#AudioMarkBench-Benchmarking-Robustness-of-Audio-Watermarking" class="headerlink" title="AudioMarkBench: Benchmarking Robustness of Audio Watermarking"></a>AudioMarkBench: Benchmarking Robustness of Audio Watermarking</h2><p><strong>Authors:Hongbin Liu, Moyang Guo, Zhengyuan Jiang, Lun Wang, Neil Zhenqiang Gong</strong></p>
<p>The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common&#x2F;adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/moyangkuo/AudioMarkBench">https://github.com/moyangkuo/AudioMarkBench</a>. </p>
<blockquote>
<p>éšç€æ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹çš„è¿›æ­¥ï¼Œåˆæˆè¯­éŸ³çš„é€¼çœŸåº¦è¶Šæ¥è¶Šé«˜ï¼Œè¿™å¼•å‘äº†å…³äºå†’å……å’Œè™šå‡ä¿¡æ¯çš„é“å¾·æ‹…å¿§ã€‚éŸ³é¢‘æ°´å°æŠ€æœ¯é€šè¿‡åœ¨äººå·¥æ™ºèƒ½ç”Ÿæˆçš„éŸ³é¢‘ä¸­åµŒå…¥äººç±»æ— æ³•å¯Ÿè§‰çš„æ°´å°ï¼Œæä¾›äº†ä¸€ä¸ªå¾ˆæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ°´å°å¯¹å¸¸è§&#x2F;å¯¹æŠ—æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†AudioMarkBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç³»ç»ŸåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘æ°´å°å¯¹æ°´å°ç§»é™¤å’Œä¼ªé€ æ“ä½œçš„ç¨³å¥æ€§ã€‚AudioMarkBenchåŒ…æ‹¬ä½¿ç”¨Common-Voiceåˆ›å»ºçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§è¯­è¨€ã€ç”Ÿç‰©æ€§åˆ«å’Œå¹´é¾„ï¼ŒåŒ…å«ä¸‰ç§æœ€å…ˆè¿›çš„æ°´å°æ–¹æ³•ï¼Œä»¥åŠ1 5ç§æ‰°åŠ¨ç±»å‹ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•åœ¨æ— æ¡†ã€é»‘ç®±å’Œç™½ç®±è®¾ç½®ä¸‹çš„æ‰°åŠ¨ç¨³å¥æ€§è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†å½“å‰æ°´å°æŠ€æœ¯çš„æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥å’Œå…¬å¹³çš„éŸ³é¢‘æ°´å°è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/moyangkuo/AudioMarkBench%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/moyangkuo/AudioMarkBenchå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.06979v2">PDF</a> To appear in NeurIPS Datasets and Benchmarks, 2024</p>
<p><strong>Summary</strong><br>     éšç€æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹çš„è¿›æ­¥ï¼Œåˆæˆè¯­éŸ³çš„é€¼çœŸåº¦ä¸æ–­æé«˜ï¼Œå¼•å‘äº†å…³äºèº«ä»½ä¼ªè£…å’Œè™šå‡ä¿¡æ¯çš„ä¼¦ç†é—®é¢˜ã€‚éŸ³é¢‘æ°´å°æŠ€æœ¯ä¸ºåœ¨AIç”Ÿæˆçš„éŸ³é¢‘ä¸­åµŒå…¥äººç±»æ— æ³•å¯Ÿè§‰çš„æ°´å°æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒéŸ³é¢‘æ°´å°å¯¹å¸¸è§&#x2F;å¯¹æŠ—æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§å°šå¾…ç ”ç©¶ã€‚æˆ‘ä»¬æ¨å‡ºäº†AudioMarkBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°éŸ³é¢‘æ°´å°ç¨³å¥æ€§çš„ç³»ç»ŸåŸºå‡†æµ‹è¯•ï¼Œå®ƒå¯¹æŠ—æ°´å°ç§»é™¤å’Œæ°´å°ä¼ªé€ è¿›è¡Œäº†æµ‹è¯•ã€‚AudioMarkBenchåŒ…æ‹¬ä½¿ç”¨Common-Voiceåˆ›å»ºçš„æ–°æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§è¯­è¨€ã€ç”Ÿç‰©æ€§åˆ«å’Œå¹´é¾„ï¼Œè¿˜æœ‰ä¸‰ç§æœ€å…ˆè¿›çš„æ°´å°æ–¹æ³•å’Œåäº”ç§æ‰°åŠ¨ç±»å‹ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•åœ¨æ— ç›’ã€é»‘ç›’å’Œç™½ç›’è®¾ç½®ä¸­çš„æ‰°åŠ¨è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å½“å‰æ°´å°æŠ€æœ¯çš„æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†éœ€è¦æ›´ç¨³å¥å’Œå…¬å¹³çš„éŸ³é¢‘æ°´å°è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/moyangkuo/AudioMarkBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/moyangkuo/AudioMarkBenchè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè¯­éŸ³é€¼çœŸåº¦çš„æé«˜å¼•å‘äº†å…³äºèº«ä»½ä¼ªè£…å’Œè™šå‡ä¿¡æ¯çš„ä¼¦ç†é—®é¢˜ã€‚</li>
<li>éŸ³é¢‘æ°´å°æŠ€æœ¯ä¸ºAIç”Ÿæˆçš„éŸ³é¢‘æä¾›äº†åµŒå…¥ä¸å¯å¯Ÿè§‰æ°´å°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>éŸ³é¢‘æ°´å°å¯¹å¸¸è§å’Œå¯¹æŠ—æ€§çš„æ‰°åŠ¨çš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>æ¨å‡ºäº†AudioMarkBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘æ°´å°çš„ç¨³å¥æ€§ã€‚</li>
<li>AudioMarkBenchåŒ…å«å¤šè¯­è¨€ã€æ€§åˆ«å’Œå¹´é¾„çš„æ–°æ•°æ®é›†ã€‚</li>
<li>å½“å‰æ°´å°æŠ€æœ¯å­˜åœ¨æ¼æ´ï¼Œéœ€è¦æ›´ç¨³å¥å’Œå…¬å¹³çš„éŸ³é¢‘æ°´å°è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-08915683a4c758690683b2f6e7a55794.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-162e3e837d0021b08f938cabddea0622.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6c52264182cf4eb9c617de2585a2f4cf.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-bd1ab33af7d9e984de40e97c71a6eb18.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-11-27/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3e9141a208090f243e5f350c29e32c4d.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-12  Diffusion Features for Zero-Shot 6DoF Object Pose Estimation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ea0ffd3a67649ccdf1c36e0b999ac684.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  [MASK] is All You Need  In generative models, two paradigms have gained attraction in various applications
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">4610.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
