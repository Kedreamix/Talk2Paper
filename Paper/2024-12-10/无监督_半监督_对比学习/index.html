<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Delve into Visual Contrastive Decoding for Hallucination Mitigation of   Large Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f7e8a03334c7b295d75eec44a9ef4132.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    26.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    106 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Delve-into-Visual-Contrastive-Decoding-for-Hallucination-Mitigation-of-Large-Vision-Language-Models"><a href="#Delve-into-Visual-Contrastive-Decoding-for-Hallucination-Mitigation-of-Large-Vision-Language-Models" class="headerlink" title="Delve into Visual Contrastive Decoding for Hallucination Mitigation of   Large Vision-Language Models"></a>Delve into Visual Contrastive Decoding for Hallucination Mitigation of   Large Vision-Language Models</h2><p><strong>Authors:Yi-Lun Lee, Yi-Hsuan Tsai, Wei-Chen Chiu</strong></p>
<p>While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the modelâ€™s response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨ç”Ÿæˆä¸è¾“å…¥è§†è§‰å†…å®¹ç›¸å…³çš„åˆç†å“åº”æ–¹é¢å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°å¹»è§‰çš„å½±å“ï¼Œå³ç”Ÿæˆçš„æ–‡æœ¬ä¸èƒ½å‡†ç¡®åœ°åæ˜ è§†è§‰å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨å¯¹æ¯”è§£ç ï¼Œé€šè¿‡å¯¹æ¯”è¾“å‡ºåˆ†å¸ƒä¸åŸå§‹å’Œè§†è§‰å¤±çœŸæ ·æœ¬ï¼Œä»¥æ— éœ€è®­ç»ƒçš„æ–¹å¼å±•ç¤ºå‡ºäº†æœ‰å‰æ™¯çš„å¹»è§‰å‡è½»æ•ˆæœã€‚ç„¶è€Œï¼Œè§†è§‰è¾“å…¥ä¸­æ”¹å˜ä¿¡æ¯çš„æ½œåŠ›å°šæœªå¾—åˆ°å¾ˆå¥½çš„æ¢ç´¢ï¼Œå› æ­¤æ·±å…¥æ¢è®¨è§†è§‰å¯¹æ¯”è§£ç çš„è¡Œä¸ºå…·æœ‰é‡è¦æ„ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢äº†å„ç§å¯¹æ¯”è§£ç æ–¹æ³•æ¥æ”¹å˜è§†è§‰å†…å®¹ï¼ŒåŒ…æ‹¬å›¾åƒä¸‹é‡‡æ ·å’Œç¼–è¾‘ã€‚ä¸‹é‡‡æ ·å›¾åƒä¼šå‡å°‘è¯¦ç»†çš„æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œç¼–è¾‘åˆ™ä¼šåœ¨å›¾åƒä¸­äº§ç”Ÿæ–°çš„å†…å®¹ï¼Œä½œä¸ºè§†è§‰å¯¹æ¯”æ ·æœ¬æä¾›äº†æ–°çš„è§†è§’ã€‚ä¸ºäº†è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ä¸åŒå¯¹æ¯”æ ·æœ¬çš„å¥½å¤„ï¼Œæˆ‘ä»¬åˆ†æäº†æ¦‚ç‡çº§åˆ«çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç†µå’Œåˆ†å¸ƒè·ç¦»ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™äº›æ ·æœ¬åœ¨å‡è½»å¹»è§‰æ–¹é¢çš„æ•ˆæœåœ¨LVLMså’ŒåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å·®å¼‚å¾ˆå¤§ã€‚åŸºäºæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç»“åˆå¯¹æ¯”æ ·æœ¬çš„æ–¹æ³•ï¼Œä¸ºåœ¨å„ç§åœºæ™¯ä¸‹åº”ç”¨å¯¹æ¯”è§£ç æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¿›è¡Œäº†å¤§é‡å®éªŒæ¥éªŒè¯åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­æå‡ºçš„èåˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06775v1">PDF</a> Under review. Project pages: <a target="_blank" rel="noopener" href="https://github.com/YiLunLee/VCD_Analysis">https://github.com/YiLunLee/VCD_Analysis</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†å¯¹æ¯”è§£ç åœ¨è§†è§‰å†…å®¹æ”¹å˜æ–¹é¢çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å›¾åƒä¸‹é‡‡æ ·å’Œç¼–è¾‘ã€‚é€šè¿‡å¯¹æ¯”è§£ç ï¼Œä½¿ç”¨åŸå§‹å’Œè§†è§‰æ‰­æ›²çš„æ ·æœ¬å¯¹æ¯”è¾“å‡ºåˆ†å¸ƒï¼Œä»¥å‡è½»è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„ç”Ÿæˆæ–‡æœ¬çš„ä¸å‡†ç¡®æ€§é—®é¢˜ã€‚ç ”ç©¶æ·±å…¥åˆ†æäº†å¯¹æ¯”æ ·æœ¬çš„ä¸åŒè¡¨ç°å½¢å¼åŠå…¶å½±å“ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç»“åˆå¯¹æ¯”æ ·æœ¬çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å„ç§åœºæ™¯ä¸­åº”ç”¨å¯¹æ¯”è§£ç ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”è§£ç ç”¨äºé€šè¿‡å¯¹æ¯”è¾“å‡ºåˆ†å¸ƒä¸åŸå§‹å’Œè§†è§‰æ‰­æ›²çš„æ ·æœ¬æ¥æ ¡å‡†æ¨¡å‹çš„å“åº”ï¼Œæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ— è®­ç»ƒå‡è½»å¹»è§‰çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶äº†æ”¹å˜è§†è§‰è¾“å…¥ä¿¡æ¯çš„æ½œåŠ›ï¼Œæ¢ç´¢äº†å¯¹æ¯”è§£ç åœ¨è§†è§‰å†…å®¹æ”¹å˜æ–¹é¢çš„ä¸åŒæ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒä¸‹é‡‡æ ·å’Œç¼–è¾‘ã€‚</li>
<li>å›¾åƒä¸‹é‡‡æ ·å‡å°‘äº†æ–‡æœ¬ç»†èŠ‚ä¿¡æ¯ï¼Œè€Œç¼–è¾‘åˆ™äº§ç”Ÿæ–°çš„å›¾åƒå†…å®¹ï¼Œä¸ºå¯¹æ¯”è§£ç æä¾›äº†æ–°è§†è§’çš„æ ·æœ¬ã€‚</li>
<li>é€šè¿‡åˆ†ææ¦‚ç‡åº¦é‡ï¼ˆå¦‚ç†µå’Œåˆ†å¸ƒè·ç¦»ï¼‰ï¼Œæ¢è®¨äº†ä¸åŒå¯¹æ¯”æ ·æœ¬çš„åˆ©ç›Šã€‚å¯¹æ¯”æ ·æœ¬åœ¨å‡è½»å¹»è§‰æ–¹é¢çš„æ•ˆæœåœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•ä¸­å·®å¼‚å¾ˆå¤§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç»“åˆå¯¹æ¯”æ ·æœ¬çš„æ–¹æ³•ï¼Œé€‚ç”¨äºå„ç§åœºæ™¯ä¸­çš„å¯¹æ¯”è§£ç åº”ç”¨ã€‚</li>
<li>å¹¿æ³›çš„å®éªŒéªŒè¯äº†æ‰€æèåˆæ–¹æ³•åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91c022b3de43f4f47b14bf74a9c28a89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18d0f0293e70cc71c6220b9d9c7eca61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df894fc968bad2b79aa299412b1dfdc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c109a1292cce51b5797f57a82454e0a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a320cc11e78cfecf0bc276cf17e94ca.jpg" align="middle">
</details>




<h2 id="Compositional-Image-Retrieval-via-Instruction-Aware-Contrastive-Learning"><a href="#Compositional-Image-Retrieval-via-Instruction-Aware-Contrastive-Learning" class="headerlink" title="Compositional Image Retrieval via Instruction-Aware Contrastive Learning"></a>Compositional Image Retrieval via Instruction-Aware Contrastive Learning</h2><p><strong>Authors:Wenliang Zhong, Weizhi An, Feng Jiang, Hehuan Ma, Yuzhi Guo, Junzhou Huang</strong></p>
<p>Composed Image Retrieval (CIR) involves retrieving a target image based on a composed query of an image paired with text that specifies modifications or changes to the visual reference. CIR is inherently an instruction-following task, as the model needs to interpret and apply modifications to the image. In practice, due to the scarcity of annotated data in downstream tasks, Zero-Shot CIR (ZS-CIR) is desirable. While existing ZS-CIR models based on CLIP have shown promising results, their capability in interpreting and following modification instructions remains limited. Some research attempts to address this by incorporating Large Language Models (LLMs). However, these approaches still face challenges in effectively integrating multimodal information and instruction understanding. To tackle above challenges, we propose a novel embedding method utilizing an instruction-tuned Multimodal LLM (MLLM) to generate composed representation, which significantly enhance the instruction following capability for a comprehensive integration between images and instructions. Nevertheless, directly applying MLLMs introduces a new challenge since MLLMs are primarily designed for text generation rather than embedding extraction as required in CIR. To address this, we introduce a two-stage training strategy to efficiently learn a joint multimodal embedding space and further refining the ability to follow modification instructions by tuning the model in a triplet dataset similar to the CIR format. Extensive experiments on four public datasets: FashionIQ, CIRR, GeneCIS, and CIRCO demonstrates the superior performance of our model, outperforming state-of-the-art baselines by a significant margin. Codes are available at the GitHub repository. </p>
<blockquote>
<p>ç»„æˆå›¾åƒæ£€ç´¢ï¼ˆCIRï¼‰æ˜¯æ ¹æ®ç”±å›¾åƒå’Œæ–‡æœ¬ç»„æˆçš„æŸ¥è¯¢æ¥æ£€ç´¢ç›®æ ‡å›¾åƒï¼Œè¯¥æ–‡æœ¬æŒ‡å®šå¯¹è§†è§‰å‚è€ƒçš„ä¿®æ”¹æˆ–æ›´æ”¹ã€‚CIRæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªéµå¾ªæŒ‡ä»¤çš„ä»»åŠ¡ï¼Œå› ä¸ºæ¨¡å‹éœ€è¦è§£é‡Šå¹¶åº”ç”¨å¯¹å›¾åƒçš„ä¿®æ”¹ã€‚åœ¨å®è·µä¸­ï¼Œç”±äºä¸‹æ¸¸ä»»åŠ¡ä¸­æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§ï¼Œé›¶æ ·æœ¬ç»„æˆå›¾åƒæ£€ç´¢ï¼ˆZS-CIRï¼‰æ˜¯ç†æƒ³çš„è§£å†³æ–¹æ¡ˆã€‚è™½ç„¶åŸºäºCLIPçš„ç°æœ‰ZS-CIRæ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨è§£é‡Šå’Œéµå¾ªä¿®æ”¹æŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ä¸€äº›ç ”ç©¶è¯•å›¾é€šè¿‡èå…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æœ‰æ•ˆæ•´åˆå¤šæ¨¡å¼ä¿¡æ¯å’ŒæŒ‡ä»¤ç†è§£æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åµŒå…¥æ–¹æ³•ï¼Œåˆ©ç”¨ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„å¤šåª’ä½“LLMï¼ˆMLLMï¼‰ç”Ÿæˆç»„æˆè¡¨ç¤ºï¼Œè¿™æ˜¾è‘—æé«˜äº†éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå®ç°äº†å›¾åƒå’ŒæŒ‡ä»¤ä¹‹é—´çš„å…¨é¢é›†æˆã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨MLLMså¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå› ä¸ºMLLMsä¸»è¦è®¾è®¡ç”¨äºæ–‡æœ¬ç”Ÿæˆï¼Œè€Œä¸æ˜¯å¦‚CIRæ‰€éœ€çš„åµŒå…¥æå–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°å­¦ä¹ è”åˆå¤šæ¨¡å¼åµŒå…¥ç©ºé—´ï¼Œå¹¶é€šè¿‡åœ¨ç±»ä¼¼CIRæ ¼å¼çš„ä¸‰å…ƒç»„æ•°æ®é›†ä¸Šè°ƒæ•´æ¨¡å‹ï¼Œè¿›ä¸€æ­¥æ”¹å–„éµå¾ªä¿®æ”¹æŒ‡ä»¤çš„èƒ½åŠ›ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†FashionIQã€CIRRã€GeneCISå’ŒCIRCOä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½å“è¶Šï¼Œå¤§å¤§è¶…è¶Šäº†æœ€æ–°åŸºå‡†æ¨¡å‹ã€‚ä»£ç å¯åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05756v1">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åˆ©ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ç”Ÿæˆç»„åˆè¡¨ç¤ºçš„æ–°åµŒå…¥æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æå‡æ¨¡å‹éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå®ç°äº†å›¾åƒå’ŒæŒ‡ä»¤ä¹‹é—´çš„å…¨é¢æ•´åˆã€‚ä¸ºåº”å¯¹ç›´æ¥ä½¿ç”¨MLLMçš„æŒ‘æˆ˜ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé«˜æ•ˆå­¦ä¹ è”åˆå¤šæ¨¡æ€åµŒå…¥ç©ºé—´ï¼Œå¹¶é€šè¿‡åœ¨ç±»ä¼¼CIRæ ¼å¼çš„ä¸‰å…ƒç»„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œè¿›ä¸€æ­¥æå‡éµå¾ªä¿®æ”¹æŒ‡ä»¤çš„èƒ½åŠ›ã€‚åœ¨å››ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Composed Image Retrieval (CIR)å…è®¸åŸºäºå›¾åƒä¸æŒ‡å®šä¿®æ”¹æˆ–å˜åŒ–æ–‡æœ¬çš„é…å¯¹ç»„åˆæŸ¥è¯¢æ¥æ£€ç´¢ç›®æ ‡å›¾åƒã€‚</li>
<li>CIRæœ¬è´¨ä¸Šæ˜¯éµå¾ªæŒ‡ä»¤çš„ä»»åŠ¡ï¼Œéœ€è¦æ¨¡å‹è§£é‡Šå¹¶åº”ç”¨å¯¹å›¾åƒçš„ä¿®æ”¹ã€‚</li>
<li>é›¶æ ·æœ¬CIRï¼ˆZS-CIRï¼‰å› ä¸‹æ¸¸ä»»åŠ¡ä¸­æ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºæ€§è€Œå…·æœ‰å¸å¼•åŠ›ã€‚</li>
<li>ç°æœ‰åŸºäºCLIPçš„ZS-CIRæ¨¡å‹åœ¨è§£é‡Šå’Œéµå¾ªä¿®æ”¹æŒ‡ä»¤æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ä»é¢ä¸´æœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯å’ŒæŒ‡ä»¤ç†è§£æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºæ–°å‹åµŒå…¥æ–¹æ³•ï¼Œåˆ©ç”¨æŒ‡ä»¤è°ƒæ•™çš„MLLMç”Ÿæˆç»„åˆè¡¨ç¤ºï¼Œæ˜¾è‘—å¢å¼ºæ¨¡å‹éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-82ccc31f2f48183e8efffb22de5bb726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb77f33baaf7c2e41916eaca51148d7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4260ab20168b9d90366e4a7d1c5e9c7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-032ef64d1c0ba72f63ab33c4c49559e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f8b9876ac1c26049d277d9e4d15f108.jpg" align="middle">
</details>




<h2 id="SimC3D-A-Simple-Contrastive-3D-Pretraining-Framework-Using-RGB-Images"><a href="#SimC3D-A-Simple-Contrastive-3D-Pretraining-Framework-Using-RGB-Images" class="headerlink" title="SimC3D: A Simple Contrastive 3D Pretraining Framework Using RGB Images"></a>SimC3D: A Simple Contrastive 3D Pretraining Framework Using RGB Images</h2><p><strong>Authors:Jiahua Dong, Tong Wu, Rui Qian, Jiaqi Wang</strong></p>
<p>The 3D contrastive learning paradigm has demonstrated remarkable performance in downstream tasks through pretraining on point cloud data. Recent advances involve additional 2D image priors associated with 3D point clouds for further improvement. Nonetheless, these existing frameworks are constrained by the restricted range of available point cloud datasets, primarily due to the high costs of obtaining point cloud data. To this end, we propose SimC3D, a simple but effective 3D contrastive learning framework, for the first time, pretraining 3D backbones from pure RGB image data. SimC3D performs contrastive 3D pretraining with three appealing properties. (1) Pure image data: SimC3D simplifies the dependency of costly 3D point clouds and pretrains 3D backbones using solely RBG images. By employing depth estimation and suitable data processing, the monocular synthesized point cloud shows great potential for 3D pretraining. (2) Simple framework: Traditional multi-modal frameworks facilitate 3D pretraining with 2D priors by utilizing an additional 2D backbone, thereby increasing computational expense. In this paper, we empirically demonstrate that the primary benefit of the 2D modality stems from the incorporation of locality information. Inspired by this insightful observation, SimC3D directly employs 2D positional embeddings as a stronger contrastive objective, eliminating the necessity for 2D backbones and leading to considerable performance improvements. (3) Strong performance: SimC3D outperforms previous approaches that leverage ground-truth point cloud data for pretraining in various downstream tasks. Furthermore, the performance of SimC3D can be further enhanced by combining multiple image datasets, showcasing its significant potential for scalability. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/Dongjiahua/SimC3D">https://github.com/Dongjiahua/SimC3D</a>. </p>
<blockquote>
<p>3Då¯¹æ¯”å­¦ä¹ èŒƒå¼é€šè¿‡åœ¨ç‚¹äº‘æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚æœ€è¿‘çš„è¿›å±•æ¶‰åŠä¸3Dç‚¹äº‘ç›¸å…³çš„é¢å¤–2Då›¾åƒå…ˆéªŒï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›ç°æœ‰æ¡†æ¶å—åˆ°å¯ç”¨ç‚¹äº‘æ•°æ®é›†èŒƒå›´æœ‰é™çš„é™åˆ¶ï¼Œä¸»è¦æ˜¯ç”±äºè·å–ç‚¹äº‘æ•°æ®çš„æˆæœ¬å¾ˆé«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SimC3Dï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„3Då¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œé¦–æ¬¡ä»çº¯RGBå›¾åƒæ•°æ®å¯¹3Dä¸»å¹²ç½‘è¿›è¡Œé¢„è®­ç»ƒã€‚SimC3Dè¿›è¡Œ3Då¯¹æ¯”é¢„è®­ç»ƒæ—¶å…·æœ‰ä¸‰ä¸ªå¸å¼•äººçš„ç‰¹ç‚¹ã€‚ï¼ˆ1ï¼‰çº¯å›¾åƒæ•°æ®ï¼šSimC3Dç®€åŒ–äº†å¯¹æ˜‚è´µçš„3Dç‚¹äº‘çš„ä¾èµ–ï¼Œä»…ä½¿ç”¨RGBå›¾åƒå¯¹3Dä¸»å¹²ç½‘è¿›è¡Œé¢„è®­ç»ƒã€‚é€šè¿‡æ·±åº¦ä¼°è®¡å’Œåˆé€‚çš„æ•°æ®å¤„ç†ï¼Œå•ç›®åˆæˆç‚¹äº‘åœ¨3Dé¢„è®­ç»ƒä¸­å…·æœ‰å¾ˆå¤§æ½œåŠ›ã€‚ï¼ˆ2ï¼‰ç®€å•çš„æ¡†æ¶ï¼šä¼ ç»Ÿçš„å¤šæ¨¡æ€æ¡†æ¶é€šè¿‡åˆ©ç”¨é¢å¤–çš„2Då…ˆéªŒçŸ¥è¯†æ¥ä¿ƒè¿›3Dé¢„è®­ç»ƒï¼Œä»è€Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶è¯æ˜ï¼Œ2Dæ¨¡æ€çš„ä¸»è¦ä¼˜åŠ¿æ¥è‡ªäºå±€éƒ¨ä¿¡æ¯çš„èåˆã€‚å—è¿™ä¸€è§è§£çš„å¯å‘ï¼ŒSimC3Dç›´æ¥é‡‡ç”¨2Dä½ç½®åµŒå…¥ä½œä¸ºæ›´å¼ºçš„å¯¹æ¯”ç›®æ ‡ï¼Œæ¶ˆé™¤äº†å¯¹2Dä¸»å¹²çš„ä¾èµ–ï¼Œå¹¶å¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚ï¼ˆ3ï¼‰å“è¶Šæ€§èƒ½ï¼šåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒSimC3Dè¶…è¶Šäº†ä»¥å‰é‚£äº›åˆ©ç”¨çœŸå®ç‚¹äº‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªå›¾åƒæ•°æ®é›†ï¼ŒSimC3Dçš„æ€§èƒ½å¯ä»¥å¾—åˆ°è¿›ä¸€æ­¥æå‡ï¼Œå±•ç¤ºäº†å…¶å¯æ‰©å±•æ€§çš„æ˜¾è‘—æ½œåŠ›ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Dongjiahua/SimC3D%E4%B8%8A%E6%8F%9B%E4%BA%86%E3%80%82">https://github.com/Dongjiahua/SimC3Dä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05274v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSimC3Dçš„ç®€å•çš„3Då¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿä»çº¯ç²¹çš„RGBå›¾åƒæ•°æ®ä¸­é¢„è®­ç»ƒ3Déª¨æ¶ã€‚è¯¥æ¡†æ¶å…·æœ‰ä¸‰ä¸ªå¸å¼•äººçš„ç‰¹æ€§ï¼šçº¯å›¾åƒæ•°æ®ã€ç®€å•çš„æ¡†æ¶å’Œå¼ºå¤§çš„æ€§èƒ½ã€‚SimC3Dç®€åŒ–äº†å¯¹æ˜‚è´µ3Dç‚¹äº‘æ•°æ®çš„ä¾èµ–ï¼Œåˆ©ç”¨æ·±åº¦ä¼°è®¡å’Œé€‚å½“çš„æ•°æ®å¤„ç†ï¼Œä»å•ç›®åˆæˆç‚¹äº‘ä¸­é¢„è®­ç»ƒ3Déª¨æ¶ã€‚æ­¤å¤–ï¼ŒSimC3Dç›´æ¥é‡‡ç”¨2Dä½ç½®åµŒå…¥ä½œä¸ºæ›´å¼ºçš„å¯¹æ¯”ç›®æ ‡ï¼Œä¸éœ€è¦é¢å¤–çš„2Déª¨æ¶ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚åœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼ŒSimC3Dè¶…è¶Šäº†åˆ©ç”¨çœŸå®ç‚¹äº‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„å‰ç»æ–¹æ³•ã€‚ç»“åˆå¤šä¸ªå›¾åƒæ•°æ®é›†ï¼ŒSimC3Dçš„æ€§èƒ½å¯ä»¥å¾—åˆ°è¿›ä¸€æ­¥æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SimC3Dæ˜¯ä¸€ä¸ªæ–°çš„3Då¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥ä»çº¯ç²¹çš„RGBå›¾åƒæ•°æ®ä¸­é¢„è®­ç»ƒ3Déª¨æ¶ã€‚</li>
<li>SimC3Dç®€åŒ–äº†å¯¹é«˜æˆæœ¬ç‚¹äº‘æ•°æ®çš„ä¾èµ–ï¼Œé€šè¿‡æ·±åº¦ä¼°è®¡å’Œæ•°æ®å¤„ç†ä»å•ç›®åˆæˆç‚¹äº‘è¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>SimC3Dåˆ©ç”¨ç®€å•çš„æ¡†æ¶è®¾è®¡ç›´æ¥é‡‡ç”¨2Dä½ç½®åµŒå…¥ä½œä¸ºå¯¹æ¯”ç›®æ ‡ï¼Œæé«˜äº†æ€§èƒ½ä¸”ä¸éœ€è¦é¢å¤–çš„2Déª¨æ¶ã€‚</li>
<li>SimC3Dåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä½¿ç”¨çœŸå®ç‚¹äº‘æ•°æ®è¿›è¡Œé¢„è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>ç»“åˆå¤šä¸ªå›¾åƒæ•°æ®é›†å¯ä»¥è¿›ä¸€æ­¥æå‡SimC3Dçš„æ€§èƒ½ï¼Œæ˜¾ç¤ºå‡ºå…¶å¯æ‰©å±•æ€§çš„æ½œåŠ›ã€‚</li>
<li>SimC3Dçš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Dongjiahua/SimC3D%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82%E8%BF%99%E5%AF%B9%E4%BA%8E%E7%A0%94%E7%A9%B6%E8%80%85%E5%92%8C%E5%BC%80%E5%8F%91%E8%80%85%E8%BF%9B%E4%B8%80%E6%AD%A5%E4%BA%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%A5%E6%96%B9%E6%B3%95%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E3%80%82">https://github.com/Dongjiahua/SimC3Dä¸Šæä¾›ã€‚è¿™å¯¹äºç ”ç©¶è€…å’Œå¼€å‘è€…è¿›ä¸€æ­¥äº†è§£å’Œä½¿ç”¨è¯¥æ–¹æ³•è‡³å…³é‡è¦ã€‚</a></li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b1c553ecaa28649e48747b724eaeca1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d11a3700f9dac8c63e80fa37f0fdd511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98575957d242a6ef0e8a23603c44b2a7.jpg" align="middle">
</details>




<h2 id="Enhancing-Whole-Slide-Image-Classification-through-Supervised-Contrastive-Domain-Adaptation"><a href="#Enhancing-Whole-Slide-Image-Classification-through-Supervised-Contrastive-Domain-Adaptation" class="headerlink" title="Enhancing Whole Slide Image Classification through Supervised   Contrastive Domain Adaptation"></a>Enhancing Whole Slide Image Classification through Supervised   Contrastive Domain Adaptation</h2><p><strong>Authors:IlÃ¡n Carretero, Pablo Meseguer, RocÃ­o del Amor, Valery Naranjo</strong></p>
<p>Domain shift in the field of histopathological imaging is a common phenomenon due to the intra- and inter-hospital variability of staining and digitization protocols. The implementation of robust models, capable of creating generalized domains, represents a need to be solved. In this work, a new domain adaptation method to deal with the variability between histopathological images from multiple centers is presented. In particular, our method adds a training constraint to the supervised contrastive learning approach to achieve domain adaptation and improve inter-class separability. Experiments performed on domain adaptation and classification of whole-slide images of six skin cancer subtypes from two centers demonstrate the methodâ€™s usefulness. The results reflect superior performance compared to not using domain adaptation after feature extraction or staining normalization. </p>
<blockquote>
<p>ç—…ç†æˆåƒé¢†åŸŸä¸­çš„åŸŸåç§»ç°è±¡å¾ˆå¸¸è§ï¼Œè¿™æ˜¯ç”±äºå„åŒ»é™¢å†…éƒ¨å’Œä¹‹é—´çš„æŸ“è‰²å’Œæ•°å­—åŒ–åè®®å­˜åœ¨å·®å¼‚æ€§ã€‚å®æ–½èƒ½å¤Ÿåˆ›å»ºé€šç”¨åŸŸçš„å¼ºå¤§æ¨¡å‹ä»£è¡¨äº†ä¸€ç§éœ€è¦è§£å†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸŸé€‚åº”æ–¹æ³•æ¥å¤„ç†æ¥è‡ªå¤šä¸ªä¸­å¿ƒçš„ç—…ç†å›¾åƒä¹‹é—´çš„å·®å¼‚ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ºç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¢åŠ äº†è®­ç»ƒçº¦æŸï¼Œä»¥å®ç°åŸŸé€‚åº”å¹¶æ”¹å–„ç±»é—´å¯åˆ†æ€§ã€‚å¯¹æ¥è‡ªä¸¤ä¸ªä¸­å¿ƒçš„å…­ç§çš®è‚¤ç™Œäºšå‹çš„å…¨å¹»ç¯ç‰‡å›¾åƒè¿›è¡Œçš„åŸŸé€‚åº”å’Œåˆ†ç±»å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä¸åœ¨ç‰¹å¾æå–æˆ–æŸ“è‰²å½’ä¸€åŒ–åä¸ä½¿ç”¨åŸŸé€‚åº”ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04260v1">PDF</a> Accepted in CASEIB 2024</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šä¸­å¿ƒç—…ç†å›¾åƒé¢†åŸŸåç§»é—®é¢˜çš„ä¸€ç§æ–°çš„åŸŸé€‚åº”æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ·»åŠ è®­ç»ƒçº¦æŸåˆ°ç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥å®ç°åŸŸé€‚åº”ï¼Œæé«˜ç±»é—´å¯åˆ†æ€§ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨è·¨ä¸­å¿ƒçš®è‚¤ç™Œå…¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºç‰¹å¾æå–åä¸ä½¿ç”¨åŸŸé€‚åº”æˆ–ä»…è¿›è¡ŒæŸ“è‰²æ ‡å‡†åŒ–å¤„ç†ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å›¾åƒé¢†åŸŸå­˜åœ¨ç”±äºä¸åŒåŒ»é™¢æŸ“è‰²å’Œæ•°å­—åŒ–åè®®å¯¼è‡´çš„åŸŸåç§»é—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„åŸŸé€‚åº”æ–¹æ³•ï¼Œç”¨äºå¤„ç†æ¥è‡ªå¤šä¸ªä¸­å¿ƒçš„ç—…ç†å›¾åƒé—´çš„å·®å¼‚ã€‚</li>
<li>æ–¹æ³•åŸºäºç›‘ç£å¯¹æ¯”å­¦ä¹ ï¼Œé€šè¿‡æ·»åŠ è®­ç»ƒçº¦æŸå®ç°åŸŸé€‚åº”å’Œç±»é—´å¯åˆ†æ€§çš„æé«˜ã€‚</li>
<li>å®éªŒåœ¨è·¨ä¸­å¿ƒçš®è‚¤ç™Œå…¨åˆ‡ç‰‡å›¾åƒåˆ†ç±»ä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸å…¶ä»–å¤„ç†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºåˆ›å»ºé€šç”¨åŒ–çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒæ¥æºçš„ç—…ç†å›¾åƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e9268dfd7456d174d984112a4e35623.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-548ec31eda17ea3b6f92a5d1c08d7d71.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e9630fb3272eb44a0f501ef9bd35e08.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ec7bf05995dad9fd6d3b9fa0b63e230.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a57da176ce8cd90571b354af8bc94720.jpg" align="middle">
</details>




<h2 id="A-Framework-For-Image-Synthesis-Using-Supervised-Contrastive-Learning"><a href="#A-Framework-For-Image-Synthesis-Using-Supervised-Contrastive-Learning" class="headerlink" title="A Framework For Image Synthesis Using Supervised Contrastive Learning"></a>A Framework For Image Synthesis Using Supervised Contrastive Learning</h2><p><strong>Authors:Yibin Liu, Jianyu Zhang, Li Zhang, Shijian Li, Gang Pan</strong></p>
<p>Text-to-image (T2I) generation aims at producing realistic images corresponding to text descriptions. Generative Adversarial Network (GAN) has proven to be successful in this task. Typical T2I GANs are 2 phase methods that first pretrain an inter-modal representation from aligned image-text pairs and then use GAN to train image generator on that basis. However, such representation ignores the inner-modal semantic correspondence, e.g. the images with same label. The semantic label in priory describes the inherent distribution pattern with underlying cross-image relationships, which is supplement to the text description for understanding the full characteristics of image. In this paper, we propose a framework leveraging both inter- and inner-modal correspondence by label guided supervised contrastive learning. We extend the T2I GANs to two parameter-sharing contrast branches in both pretraining and generation phases. This integration effectively clusters the semantically similar image-text pair representations, thereby fostering the generation of higher-quality images. We demonstrate our framework on four novel T2I GANs by both single-object dataset CUB and multi-object dataset COCO, achieving significant improvements in the Inception Score (IS) and Frechet Inception Distance (FID) metrics of imagegeneration evaluation. Notably, on more complex multi-object COCO, our framework improves FID by 30.1%, 27.3%, 16.2% and 17.1% for AttnGAN, DM-GAN, SSA-GAN and GALIP, respectively. We also validate our superiority by comparing with other label guided T2I GANs. The results affirm the effectiveness and competitiveness of our approach in advancing the state-of-the-art GAN for T2I generation </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­å·²è¯æ˜æ˜¯æˆåŠŸçš„ã€‚å…¸å‹çš„T2I GANsæ˜¯ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆæ ¹æ®å¯¹é½çš„å›¾åƒ-æ–‡æœ¬å¯¹é¢„è®­ç»ƒä¸€ä¸ªè·¨æ¨¡æ€è¡¨ç¤ºï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šä½¿ç”¨GANè®­ç»ƒå›¾åƒç”Ÿæˆå™¨ã€‚ç„¶è€Œï¼Œè¿™ç§è¡¨ç¤ºå¿½ç•¥äº†å†…æ¨¡æ€è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œä¾‹å¦‚å…·æœ‰ç›¸åŒæ ‡ç­¾çš„å›¾åƒã€‚å…ˆéªŒçš„è¯­ä¹‰æ ‡ç­¾æè¿°äº†å›¾åƒä¹‹é—´å…³ç³»çš„å†…åœ¨åˆ†å¸ƒæ¨¡å¼ï¼Œè¿™æ˜¯å¯¹æ–‡æœ¬æè¿°ç†è§£å›¾åƒå®Œæ•´ç‰¹å¾çš„è¡¥å……ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨è·¨æ¨¡æ€å’Œå†…æ¨¡æ€å¯¹åº”å…³ç³»çš„æ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬å°†T2I GANsæ‰©å±•åˆ°é¢„è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µçš„ä¸¤ä¸ªå…±äº«å‚æ•°çš„å¯¹æ¯”åˆ†æ”¯ã€‚è¿™ç§é›†æˆæœ‰æ•ˆåœ°èšç±»äº†è¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒ-æ–‡æœ¬å¯¹è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›äº†æ›´é«˜è´¨é‡å›¾åƒçš„ç”Ÿæˆã€‚æˆ‘ä»¬åœ¨å•å¯¹è±¡æ•°æ®é›†CUBå’Œå¤šå¯¹è±¡æ•°æ®é›†COCOä¸Šçš„å››ä¸ªæ–°å‹T2I GANsä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œåœ¨å›¾åƒç”Ÿæˆçš„è¯„ä¼°æŒ‡æ ‡Inception Scoreï¼ˆISï¼‰å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ›´å¤æ‚çš„å¤šå¯¹è±¡COCOä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶åˆ†åˆ«å°†AttnGANã€DM-GANã€SSA-GANå’ŒGALIPçš„FIDæé«˜äº†30.1%ã€27.3%ã€16.2%å’Œ17.1%ã€‚æˆ‘ä»¬è¿˜é€šè¿‡ä¸å…¶ä»–æ ‡ç­¾å¼•å¯¼çš„T2I GANsè¿›è¡Œæ¯”è¾ƒæ¥éªŒè¯æˆ‘ä»¬çš„ä¼˜è¶Šæ€§ã€‚ç»“æœè¯å®äº†æˆ‘ä»¬æ–¹æ³•åœ¨æ¨è¿›å…ˆè¿›GANè¿›è¡ŒT2Iç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§å’Œç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03957v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆT2Iï¼‰æ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé€¼çœŸçš„å›¾åƒã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸã€‚å…¸å‹çš„T2I GANsæ˜¯åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆæ ¹æ®å¯¹é½çš„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè·¨æ¨¡æ€è¡¨ç¤ºé¢„è®­ç»ƒï¼Œç„¶ååœ¨è¯¥åŸºç¡€ä¸Šä½¿ç”¨GANè®­ç»ƒå›¾åƒç”Ÿæˆå™¨ã€‚ç„¶è€Œï¼Œè¿™ç§è¡¨ç¤ºæ–¹æ³•å¿½ç•¥äº†åŒä¸€æ¨¡æ€å†…éƒ¨è¯­ä¹‰çš„å¯¹åº”å…³ç³»ï¼Œä¾‹å¦‚å…·æœ‰ç›¸åŒæ ‡ç­¾çš„å›¾åƒä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚è¿™ç§è¯­ä¹‰æ ‡ç­¾èƒ½å¤Ÿæè¿°å›¾åƒå†…åœ¨çš„åˆ†å¸ƒæ¨¡å¼ä»¥åŠæ½œåœ¨çš„è·¨å›¾åƒå…³ç³»ï¼Œè¿™å¯¹äºç†è§£å›¾åƒçš„å…¨è²Œæ˜¯å¯¹æ–‡æœ¬æè¿°çš„è¡¥å……ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨è·¨æ¨¡æ€å’Œæ¨¡æ€å†…éƒ¨å¯¹åº”å…³ç³»çš„æ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ å®ç°è¿™ä¸€ç‚¹ã€‚æœ¬æ–‡å°†T2I GANsæ‰©å±•åˆ°é¢„è®­ç»ƒå’Œç”Ÿæˆé˜¶æ®µçš„ä¸¤ä¸ªå…±äº«å‚æ•°å¯¹æ¯”åˆ†æ”¯ã€‚è¿™ç§é›†æˆæœ‰æ•ˆåœ°èšç±»äº†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„å›¾åƒæ–‡æœ¬å¯¹è¡¨ç¤ºï¼Œä»è€Œä¿ƒè¿›ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚åœ¨å•ç›®æ ‡æ•°æ®é›†CUBå’Œå¤šç›®æ ‡æ•°æ®é›†COCOä¸Šçš„å››ç§æ–°å‹T2I GANsä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œåœ¨å›¾åƒç”Ÿæˆçš„Inception Scoreï¼ˆISï¼‰å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯åœ¨æ›´å¤æ‚çš„å¤šç›®æ ‡COCOæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨AttnGANã€DM-GANã€SSA-GANå’ŒGALIPä¸Šçš„FIDåˆ†åˆ«æé«˜äº†30.1%ã€27.3%ã€16.2%å’Œ17.1%ã€‚æˆ‘ä»¬ä¹Ÿé€šè¿‡ä¸å…¶ä»–çš„æ ‡ç­¾å¼•å¯¼çš„T2I GANsè¿›è¡Œæ¯”è¾ƒéªŒè¯äº†æˆ‘ä»¬çš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨è¿›æ–‡æœ¬ç”Ÿæˆå›¾åƒé¢†åŸŸçš„GANæŠ€æœ¯æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬ç”Ÿæˆå›¾åƒï¼ˆT2Iï¼‰ä»»åŠ¡æ—¨åœ¨åŸºäºæ–‡æœ¬æè¿°ç”ŸæˆçœŸå®å›¾åƒï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰åœ¨æ­¤é¢†åŸŸè¡¨ç°å“è¶Šã€‚</li>
<li>ç°æœ‰T2I GANsæ–¹æ³•ä¸»è¦ä¾èµ–è·¨æ¨¡æ€è¡¨ç¤ºï¼Œä½†å¿½ç•¥äº†åŒä¸€æ¨¡æ€å†…éƒ¨è¯­ä¹‰çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>è¯­ä¹‰æ ‡ç­¾å¯¹äºæè¿°å›¾åƒçš„å†…åœ¨åˆ†å¸ƒæ¨¡å¼å’Œæ½œåœ¨è·¨å›¾åƒå…³ç³»å¾ˆé‡è¦ï¼Œæ˜¯å¯¹æ–‡æœ¬æè¿°çš„è¡¥å……ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆè·¨æ¨¡æ€å’Œæ¨¡æ€å†…éƒ¨å¯¹åº”å…³ç³»çš„æ¡†æ¶ï¼Œé€šè¿‡æ ‡ç­¾å¼•å¯¼çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æå‡T2I GANsçš„æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šç§T2I GANsä¸Šè¿›è¡ŒéªŒè¯ï¼Œå¹¶åœ¨Inception Scoreï¼ˆISï¼‰å’ŒFrechet Inception Distanceï¼ˆFIDï¼‰æŒ‡æ ‡ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>åœ¨æ›´å¤æ‚çš„å¤šç›®æ ‡æ•°æ®é›†COCOä¸Šï¼Œæ¡†æ¶è¡¨ç°å‡ºè¾ƒå¼ºçš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e4fa8e5002aff79fa0f993cd8f600d8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c977d0c995d40a5daed4d2bf1c843d5.jpg" align="middle">
</details>




<h2 id="LoCo-Low-Contrast-Enhanced-Contrastive-Learning-for-Semi-Supervised-Endoscopic-Image-Segmentation"><a href="#LoCo-Low-Contrast-Enhanced-Contrastive-Learning-for-Semi-Supervised-Endoscopic-Image-Segmentation" class="headerlink" title="LoCo: Low-Contrast-Enhanced Contrastive Learning for Semi-Supervised   Endoscopic Image Segmentation"></a>LoCo: Low-Contrast-Enhanced Contrastive Learning for Semi-Supervised   Endoscopic Image Segmentation</h2><p><strong>Authors:Lingcong Cai, Yun Li, Xiaomao Fan, Kaixuan Song, Yongcheng Li, Yixuan Yuan, Ruxin Wang, Wenbin Lei</strong></p>
<p>The segmentation of endoscopic images plays a vital role in computer-aided diagnosis and treatment. The advancements in deep learning have led to the employment of numerous models for endoscopic tumor segmentation, achieving promising segmentation performance. Despite recent advancements, precise segmentation remains challenging due to limited annotations and the issue of low contrast. To address these issues, we propose a novel semi-supervised segmentation framework termed LoCo via low-contrast-enhanced contrastive learning (LCC). This innovative approach effectively harnesses the vast amounts of unlabeled data available for endoscopic image segmentation, improving both accuracy and robustness in the segmentation process. Specifically, LCC incorporates two advanced strategies to enhance the distinctiveness of low-contrast pixels: inter-class contrast enhancement (ICE) and boundary contrast enhancement (BCE), enabling models to segment low-contrast pixels among malignant tumors, benign tumors, and normal tissues. Additionally, a confidence-based dynamic filter (CDF) is designed for pseudo-label selection, enhancing the utilization of generated pseudo-labels for unlabeled data with a specific focus on minority classes. Extensive experiments conducted on two public datasets, as well as a large proprietary dataset collected over three years, demonstrate that LoCo achieves state-of-the-art results, significantly outperforming previous methods. The source code of LoCo is available at the URL of <a target="_blank" rel="noopener" href="https://github.com/AnoK3111/LoCo">https://github.com/AnoK3111/LoCo</a>. </p>
<blockquote>
<p>å†…é•œå›¾åƒçš„åˆ†å‰²åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ·±åº¦å­¦ä¹ çš„è¿›æ­¥å¯¼è‡´äº†å¤šç§å†…é•œè‚¿ç˜¤åˆ†å‰²æ¨¡å‹çš„åº”ç”¨ï¼Œå¹¶å®ç°äº†æœ‰å‰æ™¯çš„åˆ†å‰²æ€§èƒ½ã€‚å°½ç®¡æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºæ ‡æ³¨æœ‰é™å’Œå¯¹æ¯”åº¦ä½çš„é—®é¢˜ï¼Œç²¾ç¡®åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŠç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œåä¸ºLoCoï¼Œé€šè¿‡ä½å¯¹æ¯”åº¦å¢å¼ºå¯¹æ¯”å­¦ä¹ ï¼ˆLCCï¼‰å®ç°ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•æœ‰æ•ˆåœ°åˆ©ç”¨äº†å†…é•œå›¾åƒåˆ†å‰²ä¸­å¤§é‡å¯ç”¨çš„æœªæ ‡è®°æ•°æ®ï¼Œæé«˜äº†åˆ†å‰²è¿‡ç¨‹çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚å…·ä½“è€Œè¨€ï¼ŒLCCç»“åˆäº†ä¸¤ç§å…ˆè¿›ç­–ç•¥ï¼Œä»¥æé«˜ä½å¯¹æ¯”åº¦åƒç´ çš„åŒºåˆ†åº¦ï¼šç±»é—´å¯¹æ¯”åº¦å¢å¼ºï¼ˆICEï¼‰å’Œè¾¹ç•Œå¯¹æ¯”åº¦å¢å¼ºï¼ˆBCEï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¶æ€§è‚¿ç˜¤ã€è‰¯æ€§è‚¿ç˜¤å’Œæ­£å¸¸ç»„ç»‡ä¹‹é—´åˆ†å‰²ä½å¯¹æ¯”åº¦åƒç´ ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ç§åŸºäºç½®ä¿¡åº¦çš„åŠ¨æ€æ»¤æ³¢å™¨ï¼ˆCDFï¼‰è¿›è¡Œä¼ªæ ‡ç­¾é€‰æ‹©ï¼Œä»¥æé«˜ç”Ÿæˆçš„ä¼ªæ ‡ç­¾åœ¨æœªæ ‡è®°æ•°æ®ä¸Šçš„åˆ©ç”¨ç‡ï¼Œé‡ç‚¹å…³æ³¨å°‘æ•°ç±»ã€‚åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä»¥åŠä¸€ä¸ªæ”¶é›†è¶…è¿‡ä¸‰å¹´çš„ä¸“æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLoCoè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚LoCoçš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AnoK3111/LoCo%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AnoK3111/LoCoè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02314v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLoCoçš„åŠç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä½å¯¹æ¯”å¢å¼ºå¯¹æ¯”å­¦ä¹ ï¼ˆLCCï¼‰æŠ€æœ¯ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®è¿›è¡Œå†…çª¥é•œå›¾åƒåˆ†å‰²ã€‚LoCoé‡‡ç”¨ä¸¤ç§ç­–ç•¥å¢å¼ºä½å¯¹æ¯”åƒç´ çš„è¾¨åˆ«åŠ›ï¼Œå¹¶å®ç°é«˜ç²¾åº¦å’Œç¨³å¥çš„åˆ†å‰²ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºä¿¡å¿ƒçš„åŠ¨æ€è¿‡æ»¤å™¨ï¼ˆCDFï¼‰ç”¨äºä¼ªæ ‡ç­¾é€‰æ‹©ï¼Œæé«˜ç”Ÿæˆçš„ä¼ªæ ‡ç­¾å¯¹æœªæ ‡è®°æ•°æ®çš„åˆ©ç”¨ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å°‘æ•°ç±»åˆ«ä¸Šã€‚åœ¨å¤šä¸ªå…¬å…±å’Œä¸“æœ‰æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLoCoå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoCoæ˜¯ä¸€ä¸ªåŠç›‘ç£åˆ†å‰²æ¡†æ¶ï¼Œç”¨äºå†…çª¥é•œå›¾åƒåˆ†å‰²ã€‚</li>
<li>åˆ©ç”¨ä½å¯¹æ¯”å¢å¼ºå¯¹æ¯”å­¦ä¹ ï¼ˆLCCï¼‰æŠ€æœ¯ï¼Œæœ‰æ•ˆä½¿ç”¨å¤§é‡æ— æ ‡ç­¾æ•°æ®ã€‚</li>
<li>é‡‡ç”¨ä¸¤ç§ç­–ç•¥ï¼šICEå’ŒBCEï¼Œå¢å¼ºä½å¯¹æ¯”åƒç´ çš„è¾¨åˆ«åŠ›ã€‚</li>
<li>åŸºäºä¿¡å¿ƒçš„åŠ¨æ€è¿‡æ»¤å™¨ï¼ˆCDFï¼‰ç”¨äºä¼ªæ ‡ç­¾é€‰æ‹©ï¼Œæé«˜æœªæ ‡è®°æ•°æ®çš„åˆ©ç”¨ç‡ã€‚</li>
<li>LoCoåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>LoCoæºä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-450b741f172f604f789980e4d6c603b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e6cc253aac04ff0f7b44e27ae28430f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d80303ae044efaeae6dc791866393bdc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efed40a04ab08f017e2e4cbbf65420d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ff503905c1080efde1a90939e5a1be4.jpg" align="middle">
</details>




<h2 id="CLERF-Contrastive-LEaRning-for-Full-Range-Head-Pose-Estimation"><a href="#CLERF-Contrastive-LEaRning-for-Full-Range-Head-Pose-Estimation" class="headerlink" title="CLERF: Contrastive LEaRning for Full Range Head Pose Estimation"></a>CLERF: Contrastive LEaRning for Full Range Head Pose Estimation</h2><p><strong>Authors:Ting-Ruen Wei, Haowei Liu, Huei-Chung Hu, Xuyang Wu, Yi Fang, Hsin-Tai Wu</strong></p>
<p>We introduce a novel framework for representation learning in head pose estimation (HPE). Previously such a scheme was difficult due to head pose data sparsity, making triplet sampling infeasible. Recent progress in 3D generative adversarial networks (3D-aware GAN) has opened the door for easily sampling triplets (anchor, positive, negative). We perform contrastive learning on extensively augmented data including geometric transformations and demonstrate that contrastive learning allows networks to learn genuine features that contribute to accurate HPE. On the other hand, we observe that existing HPE works struggle to predict head poses as accurately when test image rotation matrices are slightly out of the training dataset distribution. Experiments show that our methodology performs on par with state-of-the-art models on standard test datasets and outperforms them when images are slightly rotated&#x2F; flipped or full range head pose. To the best of our knowledge, we are the first to deliver a true full range HPE model capable of accurately predicting any head pose including upside-down pose. Furthermore, we compared with other existing full-yaw range models and demonstrated superior results. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç”¨äºå¤´éƒ¨å§¿æ€ä¼°è®¡ï¼ˆHPEï¼‰è¡¨ç¤ºå­¦ä¹ çš„æ–°å‹æ¡†æ¶ã€‚ä¹‹å‰ç”±äºå¤´éƒ¨å§¿æ€æ•°æ®ç¨€ç–ï¼Œä½¿å¾—ä¸‰å…ƒç»„é‡‡æ ·å˜å¾—ä¸å¯è¡Œï¼Œå› æ­¤éš¾ä»¥å®ç°è¿™ç§æ–¹æ¡ˆã€‚åŸºäºæœ€è¿‘çš„ä¸‰ç»´ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆ3Dæ„ŸçŸ¥GANï¼‰çš„è¿›æ­¥ï¼Œå¯ä»¥è½»æ¾é‡‡æ ·ä¸‰å…ƒç»„ï¼ˆé”šç‚¹ã€æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬ï¼‰ã€‚æˆ‘ä»¬å¯¹å¤§é‡æ‰©å……çš„æ•°æ®æ‰§è¡Œå¯¹æ¯”å­¦ä¹ ï¼ŒåŒ…æ‹¬å‡ ä½•å˜æ¢ï¼Œå¹¶è¯æ˜å¯¹æ¯”å­¦ä¹ å¯ä»¥ä½¿ç½‘ç»œå­¦ä¹ çœŸå®çš„ç‰¹å¾ï¼Œæœ‰åŠ©äºå®ç°å‡†ç¡®çš„HPEã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰çš„HPEå·¥ä½œåœ¨æµ‹è¯•å›¾åƒæ—‹è½¬çŸ©é˜µç•¥è¶…å‡ºè®­ç»ƒæ•°æ®é›†åˆ†å¸ƒæ—¶ï¼Œéš¾ä»¥å‡†ç¡®é¢„æµ‹å¤´éƒ¨å§¿æ€ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†æµ‹è¯•æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ï¼Œåœ¨å›¾åƒç¨å¾®æ—‹è½¬ã€ç¿»è½¬æˆ–å…¨èŒƒå›´å¤´éƒ¨å§¿æ€çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºå®ƒä»¬ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæä¾›èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹åŒ…æ‹¬å€’ç«‹å§¿æ€åœ¨å†…çš„ä»»ä½•å¤´éƒ¨å§¿æ€çš„çœŸæ­£å…¨èŒƒå›´HPEæ¨¡å‹çš„å›¢é˜Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸å…¶ä»–ç°æœ‰çš„å…¨åèˆªèŒƒå›´æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†æ›´ä¼˜è¶Šçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02066v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤´éƒ¨å§¿æ€ä¼°è®¡ï¼ˆHPEï¼‰çš„è¡¨ç¤ºå­¦ä¹ æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºå¯¹æ¯”å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨å¤§é‡æ‰©å……çš„æ•°æ®ä¸Šè¿›è¡Œå‡ ä½•å˜æ¢ï¼Œä»è€Œå­¦ä¹ çœŸå®ç‰¹å¾ä»¥å®ç°å‡†ç¡®çš„HPEã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåº”å¯¹æµ‹è¯•å›¾åƒæ—‹è½¬çŸ©é˜µç•¥è¶…å‡ºè®­ç»ƒæ•°æ®é›†åˆ†å¸ƒçš„æƒ…å†µï¼Œå¹¶åœ¨æ ‡å‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç¨å¾®æ—‹è½¬ã€ç¿»è½¬æˆ–å…¨èŒƒå›´å¤´éƒ¨å§¿æ€é¢„æµ‹æ—¶è¡¨ç°æ›´å‡ºè‰²ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæä¾›èƒ½å¤Ÿå‡†ç¡®é¢„æµ‹ä»»ä½•å¤´éƒ¨å§¿æ€ï¼ˆåŒ…æ‹¬å€’ç«‹å§¿æ€ï¼‰çš„å…¨èŒƒå›´HPEæ¨¡å‹ï¼Œå¹¶åœ¨ä¸å…¶ä»–å…¨åèˆªèŒƒå›´æ¨¡å‹çš„æ¯”è¾ƒä¸­å±•ç°å‡ºå“è¶Šç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„å¤´éƒ¨å§¿æ€ä¼°è®¡ï¼ˆHPEï¼‰è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼ŒåŸºäºå¯¹æ¯”å­¦ä¹ è¿›è¡Œç‰¹å¾å­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨3Dç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆ3D-aware GANï¼‰è½»æ¾é‡‡æ ·ä¸‰å…ƒç»„ï¼ˆé”šç‚¹ã€æ­£ä¾‹ã€è´Ÿä¾‹ï¼‰ï¼Œè§£å†³äº†ä¹‹å‰å› å¤´éƒ¨å§¿æ€æ•°æ®ç¨€ç–å¯¼è‡´çš„é‡‡æ ·å›°éš¾é—®é¢˜ã€‚</li>
<li>æ¡†æ¶åœ¨å¤§é‡æ‰©å……çš„æ•°æ®ä¸Šè¿›è¡Œäº†å‡ ä½•å˜æ¢ï¼Œå¢å¼ºäº†ç½‘ç»œçš„æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æœ‰åŠ©äºç½‘ç»œå­¦ä¹ çœŸå®ç‰¹å¾ï¼Œä»è€Œæé«˜å¤´éƒ¨å§¿æ€ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåº”å¯¹æµ‹è¯•å›¾åƒæ—‹è½¬çŸ©é˜µç•¥è¶…å‡ºè®­ç»ƒæ•°æ®é›†åˆ†å¸ƒçš„æƒ…å†µã€‚</li>
<li>åœ¨æ ‡å‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒç¨å¾®æ—‹è½¬ã€ç¿»è½¬æˆ–å…¨èŒƒå›´å¤´éƒ¨å§¿æ€é¢„æµ‹æ—¶è¡¨ç°æ›´å‡ºè‰²ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ae8cd046d5eebe9f67f1272bb96291b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c520c5d3e68202129f2729b8239c004.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a626ebfd766afbc67cd86730cec0fd7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-128d6765a6e9c2212ec173d001df8b0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f230b8fdc921ec2858ec99aa113bf3c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5177209accec95eaba96e4368da228cd.jpg" align="middle">
</details>




<h2 id="MPBD-LSTM-A-Predictive-Model-for-Colorectal-Liver-Metastases-Using-Time-Series-Multi-phase-Contrast-Enhanced-CT-Scans"><a href="#MPBD-LSTM-A-Predictive-Model-for-Colorectal-Liver-Metastases-Using-Time-Series-Multi-phase-Contrast-Enhanced-CT-Scans" class="headerlink" title="MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time   Series Multi-phase Contrast-Enhanced CT Scans"></a>MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time   Series Multi-phase Contrast-Enhanced CT Scans</h2><p><strong>Authors:Xueyang Li, Han Xiao, Weixiang Weng, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Colorectal cancer is a prevalent form of cancer, and many patients develop colorectal cancer liver metastasis (CRLM) as a result. Early detection of CRLM is critical for improving survival rates. Radiologists usually rely on a series of multi-phase contrast-enhanced computed tomography (CECT) scans done during follow-up visits to perform early detection of the potential CRLM. These scans form unique five-dimensional data (time, phase, and axial, sagittal, and coronal planes in 3D CT). Most of the existing deep learning models can readily handle four-dimensional data (e.g., time-series 3D CT images) and it is not clear how well they can be extended to handle the additional dimension of phase. In this paper, we build a dataset of time-series CECT scans to aid in the early diagnosis of CRLM, and build upon state-of-the-art deep learning techniques to evaluate how to best predict CRLM. Our experimental results show that a multi-plane architecture based on 3D bi-directional LSTM, which we call MPBD-LSTM, works best, achieving an area under curve (AUC) of 0.79. On the other hand, analysis of the results shows that there is still great room for further improvement. </p>
<blockquote>
<p>ç»“è‚ ç™Œæ˜¯ä¸€ç§å¸¸è§çš„ç™Œç—‡ï¼Œè®¸å¤šæ‚£è€…ä¼šå‘å±•ä¸ºç»“è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰ã€‚æ—©æœŸå‘ç°CRLMå¯¹äºæé«˜å­˜æ´»ç‡è‡³å…³é‡è¦ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿé€šå¸¸ä¾èµ–äºä¸€ç³»åˆ—å¤šæœŸå¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰æ£€æŸ¥æ¥è¿›è¡Œæ—©æœŸæ£€æµ‹æ½œåœ¨çš„CRLMã€‚è¿™äº›æ‰«æå½¢æˆç‹¬ç‰¹çš„äº”ç»´æ•°æ®ï¼ˆæ—¶é—´ã€é˜¶æ®µå’Œè½´å‘ã€çŸ¢çŠ¶é¢å’Œå† çŠ¶é¢åœ¨ä¸‰ç»´CTä¸­ï¼‰ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥è½»æ¾å¤„ç†å››ç»´æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ—¶é—´åºåˆ—ä¸‰ç»´CTå›¾åƒï¼‰ï¼Œå°šä¸æ¸…æ¥šå®ƒä»¬æ‰©å±•åˆ°å¤„ç†é¢å¤–çš„é˜¶æ®µç»´åº¦çš„å¥½åã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ—¶é—´åºåˆ—CECTæ‰«ææ•°æ®é›†ï¼Œä»¥è¾…åŠ©CRLMçš„æ—©æœŸè¯Šæ–­ï¼Œå¹¶åŸºäºæœ€æ–°æ·±åº¦å­¦ä¹ æŠ€æœ¯è¯„ä¼°å¦‚ä½•æœ€ä½³é¢„æµ‹CRLMã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä¸‰ç»´åŒå‘LSTMçš„å¤šå¹³é¢æ¶æ„è¡¨ç°æœ€ä½³ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºMPBD-LSTMï¼Œå…¶æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰è¾¾åˆ°0.79ã€‚å¦ä¸€æ–¹é¢ï¼Œå¯¹ç»“æœçš„åˆ†æè¡¨æ˜ï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01973v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶ç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰çš„æ—©æœŸè¯Šæ–­ï¼Œåˆ©ç”¨æ—¶é—´åºåˆ—çš„å¯¹æ¯”å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰æ•°æ®æ„å»ºæ•°æ®é›†ï¼Œå¹¶åŸºäºå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯è¯„ä¼°æœ€ä½³çš„é¢„æµ‹æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä¸‰ç»´åŒå‘LSTMçš„å¤šå¹³é¢æ¶æ„ï¼ˆMPBD-LSTMï¼‰è¡¨ç°æœ€ä½³ï¼Œæ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰è¾¾åˆ°0.79ï¼Œä½†ä»å­˜åœ¨æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç»“ç›´è‚ ç™Œæ˜¯ä¸€ç§å¸¸è§çš„ç™Œç—‡ï¼Œå…¶ä¸­è®¸å¤šæ‚£è€…ä¼šå‘å±•ä¸ºç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰ã€‚</li>
<li>æ—©æœŸæ£€æµ‹CRLMå¯¹äºæé«˜å­˜æ´»ç‡è‡³å…³é‡è¦ã€‚</li>
<li>æ”¾å°„ç§‘åŒ»ç”Ÿé€šå¸¸ä¾èµ–äºå¤šé˜¶æ®µå¯¹æ¯”å¢å¼ºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCECTï¼‰è¿›è¡Œæ—©æœŸæ£€æµ‹ã€‚</li>
<li>CECTæ‰«æå½¢æˆç‹¬ç‰¹çš„æ•°æ®ç»´åº¦ï¼ŒåŒ…æ‹¬æ—¶é—´ã€é˜¶æ®µä»¥åŠä¸‰ç»´CTçš„ä¸‰ä¸ªå¹³é¢ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥å¤„ç†å››ç»´æ•°æ®ï¼Œä½†å¯¹äºåŒ…å«é¢å¤–ç»´åº¦çš„æ•°æ®ï¼ˆå¦‚é˜¶æ®µï¼‰çš„å¤„ç†èƒ½åŠ›å°šä¸æ¸…æ¥šã€‚</li>
<li>ç ”ç©¶è€…æ„å»ºäº†ä¸€ä¸ªåŸºäºæ—¶é—´åºåˆ—CECTæ‰«æçš„æ•°æ®é›†æ¥è¾…åŠ©æ—©æœŸCRLMè¯Šæ–­ï¼Œå¹¶é‡‡ç”¨åŸºäºä¸‰ç»´åŒå‘LSTMçš„å¤šå¹³é¢æ¶æ„ï¼ˆMPBD-LSTMï¼‰è¿›è¡Œé¢„æµ‹ï¼Œå–å¾—äº†æœ€ä½³æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-35b2599e45f57426cebc3364c22f6ad6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be471c00272acb59d7aafd11cb79a97e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-26b98b82f527ee30cfc63d2207179479.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-545aac6e710fa6fa276f139bb27270f6.jpg" align="middle">
</details>




<h2 id="Advancing-Myopia-To-Holism-Fully-Contrastive-Language-Image-Pre-training"><a href="#Advancing-Myopia-To-Holism-Fully-Contrastive-Language-Image-Pre-training" class="headerlink" title="Advancing Myopia To Holism: Fully Contrastive Language-Image   Pre-training"></a>Advancing Myopia To Holism: Fully Contrastive Language-Image   Pre-training</h2><p><strong>Authors:Haicheng Wang, Chen Ju, Weixiong Lin, Shuai Xiao, Mengting Chen, Yixuan Huang, Chang Liu, Mingshuai Yao, Jinsong Lan, Ying Chen, Qingwen Liu, Yanfeng Wang</strong></p>
<p>In rapidly evolving field of vision-language models (VLMs), contrastive language-image pre-training (CLIP) has made significant strides, becoming foundation for various downstream tasks. However, relying on one-to-one (image, text) contrastive paradigm to learn alignment from large-scale messy web data, CLIP faces a serious myopic dilemma, resulting in biases towards monotonous short texts and shallow visual expressivity. To overcome these issues, this paper advances CLIP into one novel holistic paradigm, by updating both diverse data and alignment optimization. To obtain colorful data with low cost, we use image-to-text captioning to generate multi-texts for each image, from multiple perspectives, granularities, and hierarchies. Two gadgets are proposed to encourage textual diversity. To match such (image, multi-texts) pairs, we modify the CLIP image encoder into multi-branch, and propose multi-to-multi contrastive optimization for image-text part-to-part matching. As a result, diverse visual embeddings are learned for each image, bringing good interpretability and generalization. Extensive experiments and ablations across over ten benchmarks indicate that our holistic CLIP significantly outperforms existing myopic CLIP, including image-text retrieval, open-vocabulary classification, and dense visual tasks. </p>
<blockquote>
<p>åœ¨å¿«é€Ÿå‘å±•çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢†åŸŸï¼Œå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å–å¾—äº†é‡å¤§è¿›å±•ï¼Œæˆä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€ã€‚ç„¶è€Œï¼ŒCLIPä¾èµ–äºä¸€å¯¹ä¸€ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰å¯¹æ¯”æ¨¡å¼æ¥å­¦ä¹ å¤§è§„æ¨¡æ‚ä¹±ç½‘ç»œæ•°æ®çš„å¯¹é½æ–¹å¼ï¼Œé¢ä¸´ç€ä¸¥é‡çš„è¿‘è§†å›°å¢ƒï¼Œå¯¼è‡´å¯¹å•è°ƒçŸ­æ–‡æœ¬çš„åè§å’Œè§†è§‰è¡¨è¾¾çš„è‚¤æµ…ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡é€šè¿‡å°†CLIPæ¨è¿›åˆ°ä¸€ä¸ªæ–°çš„æ•´ä½“æ¨¡å¼ï¼Œæ›´æ–°å„ç§æ•°æ®å’Œå¯¹é½ä¼˜åŒ–æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸ºäº†è·å¾—ä½æˆæœ¬çš„å¤šå½©æ•°æ®ï¼Œæˆ‘ä»¬ä»å¤šä¸ªè§’åº¦ã€ç²’åº¦å’Œå±‚æ¬¡ç»“æ„å‡ºå‘ï¼Œåˆ©ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„æ ‡é¢˜ç”Ÿæˆæ¥ä¸ºæ¯å¼ å›¾åƒç”Ÿæˆå¤šæ–‡æœ¬ã€‚æå‡ºäº†ä¸¤ç§å°å·¥å…·æ¥é¼“åŠ±æ–‡æœ¬å¤šæ ·æ€§ã€‚ä¸ºäº†åŒ¹é…è¿™æ ·çš„ï¼ˆå›¾åƒã€å¤šæ–‡æœ¬ï¼‰å¯¹ï¼Œæˆ‘ä»¬å°†CLIPå›¾åƒç¼–ç å™¨æ”¹ä¸ºå¤šåˆ†æ”¯ï¼Œå¹¶æå‡ºå¤šå¯¹å¤šçš„å¯¹æ¯”ä¼˜åŒ–æ¥å®ç°å›¾åƒæ–‡æœ¬éƒ¨åˆ†ä¹‹é—´çš„åŒ¹é…ã€‚å› æ­¤ï¼Œæ¯å¼ å›¾åƒéƒ½å­¦ä¼šäº†å¤šæ ·åŒ–çš„è§†è§‰åµŒå…¥ï¼Œå¸¦æ¥äº†è‰¯å¥½çš„å¯è§£é‡Šæ€§å’Œæ³›åŒ–æ€§ã€‚åœ¨è¶…è¿‡åä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ•´ä½“CLIPæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¿‘è§†CLIPï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬æ£€ç´¢ã€å¼€æ”¾è¯æ±‡åˆ†ç±»å’Œå¯†é›†è§†è§‰ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00440v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢†åŸŸå­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†å°†CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰å‡çº§ä¸ºä¸€ç§æ–°å‹æ•´ä½“èŒƒå¼çš„æ–¹æ³•ã€‚é€šè¿‡æ›´æ–°å¤šæ ·åŒ–çš„æ•°æ®å’Œä¼˜åŒ–å¯¹é½æ–¹å¼ï¼Œè§£å†³äº†CLIPåœ¨é¢ä¸´å¤§è§„æ¨¡æ··æ‚ç½‘ç»œæ•°æ®æ—¶å‡ºç°çš„çŸ­è§†å›°å¢ƒã€‚è®ºæ–‡ä½¿ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„æè¿°ç”Ÿæˆå¤šè§’åº¦ã€å¤šå±‚æ¬¡å’Œå¤šç²’åº¦çš„å¤šæ–‡æœ¬æ•°æ®ï¼Œå¹¶æå‡ºä¸¤ç§æ–¹æ³•æ¥é¼“åŠ±æ–‡æœ¬å¤šæ ·æ€§ã€‚é€šè¿‡ä¿®æ”¹CLIPå›¾åƒç¼–ç å™¨ä¸ºå¤šåˆ†æ”¯ï¼Œå¹¶æå‡ºå¤šå¯¹å¤šçš„å¯¹æ¯”ä¼˜åŒ–ï¼Œå®ç°äº†å›¾åƒä¸æ–‡æœ¬çš„éƒ¨åˆ†åŒ¹é…ã€‚å®éªŒè¡¨æ˜ï¼Œè¿™ç§æ•´ä½“æ–¹æ³•æ˜¾è‘—æé«˜äº†CLIPçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬æ£€ç´¢ã€å¼€æ”¾è¯æ±‡åˆ†ç±»å’Œå¯†é›†è§†è§‰ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨è§†è§‰è¯­è¨€æ¨¡å‹é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæˆä¸ºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„åŸºç¡€ã€‚</li>
<li>CLIPé¢ä¸´ä¾èµ–ä¸€å¯¹ä¸€ï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹æ¯”èŒƒå¼çš„å›°å¢ƒï¼Œå¯¼è‡´å¯¹å•è°ƒçŸ­æ–‡æœ¬çš„åè§å’Œè§†è§‰è¡¨è¾¾åŠ›æµ…ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„æ•´ä½“èŒƒå¼ï¼Œé€šè¿‡æ›´æ–°å¤šæ ·åŒ–çš„æ•°æ®å’Œä¼˜åŒ–å¯¹é½æ–¹å¼ã€‚</li>
<li>ä½¿ç”¨å›¾åƒåˆ°æ–‡æœ¬çš„æè¿°ç”Ÿæˆå¤šæ–‡æœ¬æ•°æ®ï¼Œä»å¤šä¸ªè§’åº¦ã€ç²’åº¦å’Œå±‚æ¬¡å¯¹å›¾åƒè¿›è¡Œæè¿°ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥é¼“åŠ±æ–‡æœ¬å¤šæ ·æ€§ï¼Œå¹¶ä¿®æ”¹CLIPå›¾åƒç¼–ç å™¨ä¸ºå¤šåˆ†æ”¯ã€‚</li>
<li>é€šè¿‡å¤šå¯¹å¤šçš„å¯¹æ¯”ä¼˜åŒ–ï¼Œå®ç°äº†å›¾åƒä¸æ–‡æœ¬çš„éƒ¨åˆ†åŒ¹é…ï¼Œæé«˜äº†æ¨¡å‹çš„è§£é‡Šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ba0f81f111945b5624625fef2d5fe8ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03da32261b44cb26dc7ce7c6408f5290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d76b6e2bc30b2c915dca9ec7dad6a44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c26622940573aebcdce2cef1ce3ac39b.jpg" align="middle">
</details>




<h2 id="CAMLD-Contrast-Agnostic-Medical-Landmark-Detection-with-Consistency-Based-Regularization"><a href="#CAMLD-Contrast-Agnostic-Medical-Landmark-Detection-with-Consistency-Based-Regularization" class="headerlink" title="CAMLD: Contrast-Agnostic Medical Landmark Detection with   Consistency-Based Regularization"></a>CAMLD: Contrast-Agnostic Medical Landmark Detection with   Consistency-Based Regularization</h2><p><strong>Authors:Soorena Salari, Arash Harirpoush, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Anatomical landmark detection in medical images is essential for various clinical and research applications, including disease diagnosis and surgical planning. However, manual landmark annotation is time-consuming and requires significant expertise. Existing deep learning (DL) methods often require large amounts of well-annotated data, which are costly to acquire. In this paper, we introduce CAMLD, a novel self-supervised DL framework for anatomical landmark detection in unlabeled scans with varying contrasts by using only a single reference example. To achieve this, we employed an inter-subject landmark consistency loss with an image registration loss while introducing a 3D convolution-based contrast augmentation strategy to promote model generalization to new contrasts. Additionally, we utilize an adaptive mixed loss function to schedule the contributions of different sub-tasks for optimal outcomes. We demonstrate the proposed method with the intricate task of MRI-based 3D brain landmark detection. With comprehensive experiments on four diverse clinical and public datasets, including both T1w and T2w MRI scans at different MRI field strengths, we demonstrate that CAMLD outperforms the state-of-the-art methods in terms of mean radial errors (MREs) and success detection rates (SDRs). Our framework provides a robust and accurate solution for anatomical landmark detection, reducing the need for extensively annotated datasets and generalizing well across different imaging contrasts. Our code will be publicly available at: <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/CAMLD">https://github.com/HealthX-Lab/CAMLD</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–æ ‡å¿—ç‚¹æ£€æµ‹å¯¹äºå„ç§ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ï¼ˆåŒ…æ‹¬ç–¾ç—…è¯Šæ–­å’Œæ‰‹æœ¯è®¡åˆ’ï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ ‡æ³¨æ ‡å¿—ç‚¹æ—¢è€—æ—¶åˆéœ€è¦ä¸°å¯Œçš„ä¸“ä¸šçŸ¥è¯†ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨è‰¯å¥½çš„æ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®çš„è·å–æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAMLDï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨å…·æœ‰ä¸åŒå¯¹æ¯”åº¦çš„æœªæ ‡è®°æ‰«æä¸­ä»…ä½¿ç”¨ä¸€ä¸ªå‚è€ƒæ ·æœ¬è¿›è¡Œè§£å‰–æ ‡å¿—ç‚¹æ£€æµ‹ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸»ä½“é—´æ ‡å¿—ç‚¹ä¸€è‡´æ€§æŸå¤±å’Œå›¾åƒé…å‡†æŸå¤±ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§åŸºäº3Då·ç§¯çš„å¯¹æ¯”åº¦å¢å¼ºç­–ç•¥ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹å¯¹æ–°å¯¹æ¯”åº¦çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨è‡ªé€‚åº”æ··åˆæŸå¤±å‡½æ•°æ¥è°ƒåº¦ä¸åŒå­ä»»åŠ¡çš„è´¡çŒ®ï¼Œä»¥è·å–æœ€ä½³ç»“æœã€‚æˆ‘ä»¬é€šè¿‡åŸºäºMRIçš„3Dè„‘æ ‡å¿—ç‚¹æ£€æµ‹è¿™ä¸€å¤æ‚ä»»åŠ¡æ¥å±•ç¤ºæ‰€æå‡ºçš„æ–¹æ³•ã€‚åœ¨å››ä¸ªå¤šæ ·åŒ–å’Œå…¬å…±çš„ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸åŒMRIåœºå¼ºçš„T1wå’ŒT2w MRIæ‰«æã€‚æˆ‘ä»¬è¯æ˜CAMLDåœ¨å¹³å‡å¾„å‘è¯¯å·®ï¼ˆMREï¼‰å’ŒæˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºè§£å‰–æ ‡å¿—ç‚¹æ£€æµ‹æä¾›äº†ç¨³å¥å’Œå‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ï¼Œå¹¶åœ¨ä¸åŒçš„æˆåƒå¯¹æ¯”åº¦ä¸Šå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/CAMLD">https://github.com/HealthX-Lab/CAMLD</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17845v1">PDF</a> 14 pages, 6 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCAMLDçš„æ–°å‹è‡ªç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨æ— æ ‡ç­¾æ‰«æä¸­å®ç°è§£å‰–å­¦æ ‡è®°æ£€æµ‹ã€‚è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨å•ä¸€å‚è€ƒæ ·æœ¬å’Œä¸åŒæ•°æ®é›†çš„æ··åˆæŸå¤±å‡½æ•°ï¼Œå®ç°äº†åœ¨ä¸åŒå¯¹æ¯”åº¦ä¸‹çš„è§£å‰–å­¦æ ‡è®°æ£€æµ‹ã€‚åœ¨MRIè„‘éƒ¨æ ‡è®°æ£€æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶åœ¨å¤šä¸ªä¸´åºŠå’Œå…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå‡å°‘äº†å¤§é‡æ ‡æ³¨æ•°æ®é›†çš„éœ€æ±‚ï¼Œä¸”èƒ½å¤Ÿåœ¨ä¸åŒæˆåƒå¯¹æ¯”åº¦ä¹‹é—´è‰¯å¥½åœ°æ¨å¹¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶CAMLDï¼Œç”¨äºåœ¨åŒ»å­¦å›¾åƒä¸­è¿›è¡Œè§£å‰–å­¦æ ‡è®°æ£€æµ‹ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å•ä¸€å‚è€ƒæ ·æœ¬ï¼Œå®ç°äº†å¯¹æ— æ ‡ç­¾æ‰«æçš„è§£å‰–å­¦æ ‡è®°æ£€æµ‹ã€‚</li>
<li>é‡‡ç”¨äº†è·¨ä¸»ä½“æ ‡è®°ä¸€è‡´æ€§æŸå¤±å’Œå›¾åƒæ³¨å†ŒæŸå¤±ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨åŸºäº3Då·ç§¯çš„å¯¹æ¯”å¢å¼ºç­–ç•¥ï¼Œä¿ƒè¿›äº†æ¨¡å‹å¯¹æ–°å¯¹æ¯”åº¦æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨äº†è‡ªé€‚åº”æ··åˆæŸå¤±å‡½æ•°ï¼Œä»¥ä¼˜åŒ–ä¸åŒå­ä»»åŠ¡çš„è´¡çŒ®ã€‚</li>
<li>åœ¨MRIè„‘éƒ¨æ ‡è®°æ£€æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬T1wå’ŒT2w MRIæ‰«æå’Œä¸åŒMRIåœºå¼ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb853e7cf58b9a5952fd87653d126772.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7c59c8c8e0fe77add764ec053fc7244.jpg" align="middle">
</details>




<h2 id="CoCoNO-Attention-Contrast-and-Complete-for-Initial-Noise-Optimization-in-Text-to-Image-Synthesis"><a href="#CoCoNO-Attention-Contrast-and-Complete-for-Initial-Noise-Optimization-in-Text-to-Image-Synthesis" class="headerlink" title="CoCoNO: Attention Contrast-and-Complete for Initial Noise Optimization   in Text-to-Image Synthesis"></a>CoCoNO: Attention Contrast-and-Complete for Initial Noise Optimization   in Text-to-Image Synthesis</h2><p><strong>Authors:Aravindan Sundaram, Ujjayan Pal, Abhimanyu Chauhan, Aishwarya Agarwal, Srikrishna Karanam</strong></p>
<p>Despite recent advancements in text-to-image models, achieving semantically accurate images in text-to-image diffusion models is a persistent challenge. While existing initial latent optimization methods have demonstrated impressive performance, we identify two key limitations: (a) attention neglect, where the synthesized image omits certain subjects from the input prompt because they do not have a designated segment in the self-attention map despite despite having a high-response cross-attention, and (b) attention interference, where the generated image has mixed-up properties of multiple subjects because of a conflicting overlap between cross- and self-attention maps of different subjects.   To address these limitations, we introduce CoCoNO, a new algorithm that optimizes the initial latent by leveraging the complementary information within self-attention and cross-attention maps. Our method introduces two new loss functions: the attention contrast loss, which minimizes undesirable overlap by ensuring each self-attention segment is exclusively linked to a specific subjectâ€™s cross attention map, and the attention complete loss, which maximizes the activation within these segments to guarantee that each subject is fully and distinctly represented. Our approach operates within a noise optimization framework, avoiding the need to retrain base models. Through extensive experiments on multiple benchmarks, we demonstrate that CoCoNO significantly improves text-image alignment and outperforms the current state of the art. </p>
<blockquote>
<p>å°½ç®¡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­å®ç°è¯­ä¹‰å‡†ç¡®çš„å›¾åƒä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰çš„åˆå§‹æ½œåœ¨ä¼˜åŒ–æ–¹æ³•å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå…³é”®å±€é™ï¼š<br>ï¼ˆaï¼‰æ³¨æ„åŠ›å¿½è§†ï¼Œå³åˆæˆå›¾åƒå¿½ç•¥äº†è¾“å…¥æç¤ºä¸­çš„æŸäº›ä¸»é¢˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨è‡ªæ³¨æ„åŠ›å›¾ä¸­æ²¡æœ‰æŒ‡å®šæ®µè½ï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰è¾ƒé«˜çš„å“åº”äº¤å‰æ³¨æ„åŠ›ï¼›<br>ï¼ˆbï¼‰æ³¨æ„åŠ›å¹²æ‰°ï¼Œå³ç”±äºä¸åŒä¸»é¢˜çš„äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å›¾ä¹‹é—´çš„å†²çªé‡å ï¼Œç”Ÿæˆå›¾åƒæ··åˆäº†å¤šä¸ªä¸»é¢˜çš„æ€§è´¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16783v1">PDF</a> 15 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­è¯­ä¹‰å‡†ç¡®å›¾åƒç”Ÿæˆçš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰åˆå§‹æ½œåœ¨ä¼˜åŒ–æ–¹æ³•çš„ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šæ³¨æ„åŠ›å¿½è§†å’Œæ³¨æ„åŠ›å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†CoCoNOç®—æ³•ï¼Œé€šè¿‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›å›¾å’Œäº¤å‰æ³¨æ„åŠ›å›¾ä¸­çš„äº’è¡¥ä¿¡æ¯æ¥ä¼˜åŒ–åˆå§‹æ½œåœ¨çŠ¶æ€ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼šæ³¨æ„åŠ›å¯¹æ¯”æŸå¤±å’Œæ³¨æ„åŠ›å®Œæ•´æ€§æŸå¤±ï¼Œä»¥æœ€å°åŒ–ä¸å¿…è¦çš„é‡å å¹¶ä¿è¯æ¯ä¸ªä¸»é¢˜éƒ½å¾—åˆ°å®Œæ•´ä¸”ç‹¬ç‰¹çš„è¡¨ç¤ºã€‚æ­¤æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ–‡æœ¬ä¸å›¾åƒçš„åŒ¹é…åº¦ï¼Œå¹¶è¶…è¶Šäº†å½“å‰çš„æœ€ä½³æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨è¯­ä¹‰å‡†ç¡®å›¾åƒç”Ÿæˆæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åˆå§‹æ½œåœ¨ä¼˜åŒ–æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šæ³¨æ„åŠ›å¿½è§†å’Œæ³¨æ„åŠ›å¹²æ‰°ã€‚</li>
<li>CoCoNOç®—æ³•é€šè¿‡åˆ©ç”¨è‡ªæ³¨æ„åŠ›å›¾å’Œäº¤å‰æ³¨æ„åŠ›å›¾ä¸­çš„äº’è¡¥ä¿¡æ¯æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>CoCoNOå¼•å…¥äº†ä¸¤ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼šæ³¨æ„åŠ›å¯¹æ¯”æŸå¤±å’Œæ³¨æ„åŠ›å®Œæ•´æ€§æŸå¤±ã€‚</li>
<li>æ³¨æ„åŠ›å¯¹æ¯”æŸå¤±æœ€å°åŒ–ä¸å¿…è¦çš„é‡å ï¼Œç¡®ä¿æ¯ä¸ªè‡ªæˆ‘æ³¨æ„æ®µåªä¸ç‰¹å®šä¸»é¢˜çš„äº¤å‰æ³¨æ„åŠ›å›¾ç›¸å…³è”ã€‚</li>
<li>æ³¨æ„åŠ›å®Œæ•´æ€§æŸå¤±ç¡®ä¿æ¯ä¸ªä¸»é¢˜åœ¨å›¾åƒä¸­å¾—åˆ°å®Œæ•´ä¸”ç‹¬ç‰¹çš„è¡¨ç¤ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7e19800364f083bf540f26aa4c80809.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0eb58a62a22f185fe1f609309ec397.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35fe0a1752e159c6873c3b1179afee8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c7e48da343ea633d7bb32f9e52b8173.jpg" align="middle">
</details>




<h2 id="ENCLIP-Ensembling-and-Clustering-Based-Contrastive-Language-Image-Pretraining-for-Fashion-Multimodal-Search-with-Limited-Data-and-Low-Quality-Images"><a href="#ENCLIP-Ensembling-and-Clustering-Based-Contrastive-Language-Image-Pretraining-for-Fashion-Multimodal-Search-with-Limited-Data-and-Low-Quality-Images" class="headerlink" title="ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image   Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality   Images"></a>ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image   Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality   Images</h2><p><strong>Authors:Prithviraj Purushottam Naik, Rohit Agarwal</strong></p>
<p>Multimodal search has revolutionized the fashion industry, providing a seamless and intuitive way for users to discover and explore fashion items. Based on their preferences, style, or specific attributes, users can search for products by combining text and image information. Text-to-image searches enable users to find visually similar items or describe products using natural language. This paper presents an innovative approach called ENCLIP, for enhancing the performance of the Contrastive Language-Image Pretraining (CLIP) model, specifically in Multimodal Search targeted towards the domain of fashion intelligence. This method focuses on addressing the challenges posed by limited data availability and low-quality images. This paper proposes an algorithm that involves training and ensembling multiple instances of the CLIP model, and leveraging clustering techniques to group similar images together. The experimental findings presented in this study provide evidence of the effectiveness of the methodology. This approach unlocks the potential of CLIP in the domain of fashion intelligence, where data scarcity and image quality issues are prevalent. Overall, the ENCLIP method represents a valuable contribution to the field of fashion intelligence and provides a practical solution for optimizing the CLIP model in scenarios with limited data and low-quality images. </p>
<blockquote>
<p>å¤šæ¨¡æ€æœç´¢å·²ç»å½»åº•æ”¹å˜äº†æ—¶å°šäº§ä¸šï¼Œä¸ºç”¨æˆ·æä¾›äº†ä¸€ç§æ— ç¼ã€ç›´è§‚çš„æ–¹å¼æ¥å‘ç°å’Œæ¢ç´¢æ—¶å°šäº§å“ã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®ä»–ä»¬çš„åå¥½ã€é£æ ¼æˆ–ç‰¹å®šå±æ€§ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯è¿›è¡Œäº§å“æœç´¢ã€‚æ–‡æœ¬åˆ°å›¾åƒçš„æœç´¢ä½¿ç”¨æˆ·èƒ½å¤Ÿæ‰¾åˆ°è§†è§‰ä¸Šç›¸ä¼¼çš„é¡¹ç›®æˆ–ä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°äº§å“ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºENCLIPçš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆCLIPæ¨¡å‹ï¼‰åœ¨é¢å‘æ—¶å°šæ™ºèƒ½çš„å¤šæ¨¡æ€æœç´¢ä¸­çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•ä¾§é‡äºè§£å†³æ•°æ®æœ‰é™å’Œå›¾åƒè´¨é‡ä½ä¸‹æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç®—æ³•ï¼ŒåŒ…æ‹¬è®­ç»ƒCLIPæ¨¡å‹çš„å¤šä¸ªå®ä¾‹å¹¶è¿›è¡Œé›†æˆï¼Œä»¥åŠåˆ©ç”¨èšç±»æŠ€æœ¯å°†ç›¸ä¼¼çš„å›¾åƒåˆ†ç»„åœ¨ä¸€èµ·ã€‚æœ¬ç ”ç©¶æä¾›çš„å®éªŒç»“æœä¸ºè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§æä¾›äº†è¯æ®ã€‚è¿™ç§æ–¹æ³•é‡Šæ”¾äº†CLIPåœ¨æ—¶å°šæ™ºèƒ½é¢†åŸŸçš„æ½œåŠ›ï¼Œå…¶ä¸­æ•°æ®ç¨€ç¼ºå’Œå›¾åƒè´¨é‡é—®é¢˜æ™®éå­˜åœ¨ã€‚æ€»ä½“è€Œè¨€ï¼ŒENCLIPæ–¹æ³•ä¸ºæ—¶å°šæ™ºèƒ½é¢†åŸŸæä¾›äº†å®è´µçš„è´¡çŒ®ï¼Œå¹¶ä¸ºåœ¨æ•°æ®æœ‰é™å’Œå›¾åƒè´¨é‡è¾ƒä½çš„æƒ…å†µä¸‹ä¼˜åŒ–CLIPæ¨¡å‹æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16096v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€æœç´¢å·²é©æ–°æ—¶å°šäº§ä¸šï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›æ— ç¼ä¸”ç›´è§‚çš„æ–¹å¼å‘ç°å’Œæ¢ç´¢æ—¶å°šäº§å“ã€‚æœ¬æ–‡æå‡ºä¸€ç§å¢å¼ºContrastive Language-Image Pretraining (CLIP)æ¨¡å‹æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•ENCLIPï¼Œç‰¹åˆ«é’ˆå¯¹æ—¶å°šæ™ºèƒ½é¢†åŸŸçš„å¤šæ¨¡æ€æœç´¢ã€‚è¯¥æ–¹æ³•è§£å†³æ•°æ®æœ‰é™å’Œå›¾åƒè´¨é‡ä½çš„é—®é¢˜ï¼Œé€šè¿‡è®­ç»ƒå’Œé›†æˆå¤šä¸ªCLIPæ¨¡å‹å®ä¾‹ï¼Œå¹¶åˆ©ç”¨èšç±»æŠ€æœ¯å°†ç›¸ä¼¼å›¾åƒåˆ†ç»„ã€‚å®éªŒç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œé‡Šæ”¾CLIPåœ¨æ—¶å°šæ™ºèƒ½é¢†åŸŸçš„æ½œåŠ›ï¼Œä¸ºåœ¨æ•°æ®ç¨€ç¼ºå’Œå›¾åƒè´¨é‡é—®é¢˜æ™®éçš„æƒ…å†µä¸‹ä¼˜åŒ–CLIPæ¨¡å‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æœç´¢åœ¨æ—¶å°šäº§ä¸šä¸­å®ç°äº†æ— ç¼ä¸”ç›´è§‚çš„äº§å“æœç´¢æ–¹å¼ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒä¿¡æ¯æ»¡è¶³ç”¨æˆ·åå¥½ã€é£æ ¼å’Œç‰¹å®šå±æ€§éœ€æ±‚ã€‚</li>
<li>ENCLIPæ–¹æ³•æ—¨åœ¨å¢å¼ºCLIPæ¨¡å‹åœ¨æ—¶å°šæ™ºèƒ½é¢†åŸŸçš„å¤šæ¨¡æ€æœç´¢æ€§èƒ½ï¼Œè§£å†³æ•°æ®æœ‰é™å’Œå›¾åƒè´¨é‡ä½çš„é—®é¢˜ã€‚</li>
<li>ENCLIPæ–¹æ³•é€šè¿‡è®­ç»ƒå’Œé›†æˆå¤šä¸ªCLIPæ¨¡å‹å®ä¾‹ï¼Œåˆ©ç”¨èšç±»æŠ€æœ¯åˆ†ç»„ç›¸ä¼¼å›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†ENCLIPæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ENCLIPåœ¨CLIPæ¨¡å‹ä¼˜åŒ–æ–¹é¢æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºå’Œå›¾åƒè´¨é‡é—®é¢˜å¸¸è§çš„æƒ…å†µä¸‹ã€‚</li>
<li>ENCLIPæ–¹æ³•å¯¹æ—¶å°šæ™ºèƒ½é¢†åŸŸå…·æœ‰é‡å¤§è´¡çŒ®ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cb0014e62614405a4f1b09f6221b95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09a3d13d138bfa43b6052fb9761c05f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de6b139128612b6578db0840e8fab72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4b104f2b0f76983d445991e23344cfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e8a03334c7b295d75eec44a9ef4132.jpg" align="middle">
</details>




<h2 id="CLIC-Contrastive-Learning-Framework-for-Unsupervised-Image-Complexity-Representation"><a href="#CLIC-Contrastive-Learning-Framework-for-Unsupervised-Image-Complexity-Representation" class="headerlink" title="CLIC: Contrastive Learning Framework for Unsupervised Image Complexity   Representation"></a>CLIC: Contrastive Learning Framework for Unsupervised Image Complexity   Representation</h2><p><strong>Authors:Shipeng Liu, Liang Zhao, Dengfeng Chen</strong></p>
<p>As an essential visual attribute, image complexity affects human image comprehension and directly influences the performance of computer vision tasks. However, accurately assessing and quantifying image complexity faces significant challenges. Previous works needed more generalization capabilities and well-labeled datasets to learn image complexity features. However, creating such datasets requires expensive manual labeling costs, and the models inevitably learn about human subjective biases. To address the above problems, we propose CLIC, an unsupervised framework based on contrastive learning, for learning image complexity representations. The method learns image complexity features on unlabeled data, avoiding the high labeling cost. Specifically, we propose a unique positive and negative sample selection strategy to reinforce the differences in complexity features. At the same time, we introduce an image prior-based Complexity-Aware Loss to constrain the learning process of the model. We conducted extensive experiments for verification, and the results show that CLIC can effectively learn the image complexity representation. CLIC obtained competitive results with supervised methods by fine-tuning on IC9600. In addition, CLIC applied to downstream tasks shows significant performance improvements, demonstrating the potential for application in various real-world scenarios. \href{<a target="_blank" rel="noopener" href="https://github.com/xauat-liushipeng/CLIC%7D%7Bcode%7D">https://github.com/xauat-liushipeng/CLIC}{code}</a> </p>
<blockquote>
<p>å›¾åƒå¤æ‚åº¦ä½œä¸ºä¸€ä¸ªé‡è¦çš„è§†è§‰å±æ€§ï¼Œå½±å“äººç±»å›¾åƒç†è§£ï¼Œå¹¶ç›´æ¥å…³è”è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå‡†ç¡®è¯„ä¼°å’Œé‡åŒ–å›¾åƒå¤æ‚åº¦é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä»¥å¾€çš„ç ”ç©¶éœ€è¦æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œæ ‡æ³¨å¥½çš„æ•°æ®é›†æ¥å­¦ä¹ å›¾åƒå¤æ‚åº¦ç‰¹å¾ã€‚ä½†æ˜¯ï¼Œåˆ›å»ºè¿™æ ·çš„æ•°æ®é›†éœ€è¦æ˜‚è´µçš„æ‰‹åŠ¨æ ‡æ³¨æˆæœ¬ï¼Œè€Œä¸”æ¨¡å‹ä¸å¯é¿å…åœ°ä¼šå­¦ä¹ åˆ°äººç±»çš„ä¸»è§‚åè§ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ— ç›‘ç£æ¡†æ¶CLICï¼Œç”¨äºå­¦ä¹ å›¾åƒå¤æ‚åº¦è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨æ— éœ€æ ‡æ³¨çš„æ•°æ®ä¸Šå­¦ä¹ å›¾åƒå¤æ‚åº¦ç‰¹å¾ï¼Œä»è€Œé¿å…äº†é«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„æ­£è´Ÿæ ·æœ¬é€‰æ‹©ç­–ç•¥ï¼Œä»¥å¼ºåŒ–å¤æ‚åº¦ç‰¹å¾çš„å·®å¼‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºå›¾åƒå…ˆéªŒçš„å¤æ‚åº¦æ„ŸçŸ¥æŸå¤±ï¼Œä»¥çº¦æŸæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡çš„å®éªŒè¿›è¡ŒéªŒè¯ï¼Œç»“æœè¡¨æ˜CLICå¯ä»¥æœ‰æ•ˆåœ°å­¦ä¹ å›¾åƒå¤æ‚åº¦è¡¨ç¤ºã€‚CLICåœ¨IC9600ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œå…¶è¡¨ç°ä¸ç›‘ç£æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼ŒCLICåº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡æ—¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚<a target="_blank" rel="noopener" href="https://github.com/xauat-liushipeng/CLIC">ä»£ç </a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12792v1">PDF</a> </p>
<p><strong>Summary</strong><br>å›¾åƒå¤æ‚åº¦æ˜¯å½±å“äººç±»å›¾åƒç†è§£å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä¸ºè§£å†³è¯„ä¼°å›¾åƒå¤æ‚åº¦é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹é€šç”¨æ€§å’Œæ ‡æ³¨æ•°æ®é›†ï¼Œä»¥åŠé«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬å’Œæ¨¡å‹å­¦ä¹ çš„ä¸»è§‚åè§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ— ç›‘ç£æ¡†æ¶CLICï¼Œç”¨äºå­¦ä¹ å›¾åƒå¤æ‚åº¦è¡¨å¾ã€‚CLICèƒ½å¤Ÿåœ¨æ— æ ‡æ³¨æ•°æ®ä¸Šå­¦ä¹ å›¾åƒå¤æ‚åº¦ç‰¹å¾ï¼Œé¿å…äº†é«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬ï¼Œå¹¶æå‡ºç‹¬ç‰¹çš„æ­£è´Ÿæ ·æœ¬é€‰æ‹©ç­–ç•¥æ¥å¼ºåŒ–å¤æ‚åº¦ç‰¹å¾çš„å·®å¼‚ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå›¾åƒå…ˆéªŒçš„å¤æ‚åº¦æ„ŸçŸ¥æŸå¤±æ¥çº¦æŸæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLICèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å›¾åƒå¤æ‚åº¦è¡¨å¾ï¼Œå¹¶åœ¨IC9600ä¸Šè¿›è¡Œå¾®è°ƒåè·å¾—ä¸ç›‘ç£æ–¹æ³•ç›¸å½“çš„ç»“æœã€‚æ­¤å¤–ï¼ŒCLICåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå±•ç°å‡ºåœ¨å„ç§å®é™…åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒå¤æ‚åº¦å¯¹å›¾åƒç†è§£å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å‡†ç¡®è¯„ä¼°å’Œé‡åŒ–å›¾åƒå¤æ‚åº¦å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹é€šç”¨æ€§å’Œæ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ— ç›‘ç£æ¡†æ¶CLICã€‚</li>
<li>CLICèƒ½å¤Ÿåœ¨æ— æ ‡æ³¨æ•°æ®ä¸Šå­¦ä¹ å›¾åƒå¤æ‚åº¦ç‰¹å¾ï¼Œé¿å…é«˜æ˜‚çš„æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>CLICé‡‡ç”¨ç‹¬ç‰¹çš„æ­£è´Ÿæ ·æœ¬é€‰æ‹©ç­–ç•¥æ¥å¼ºåŒ–å¤æ‚åº¦ç‰¹å¾çš„å·®å¼‚ã€‚</li>
<li>å¼•å…¥åŸºäºå›¾åƒå…ˆéªŒçš„å¤æ‚åº¦æ„ŸçŸ¥æŸå¤±ä»¥çº¦æŸæ¨¡å‹å­¦ä¹ ã€‚</li>
<li>CLICåœ¨å®éªŒä¸­è¡¨ç°å‡ºæœ‰æ•ˆå­¦ä¹ å›¾åƒå¤æ‚åº¦è¡¨å¾çš„èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-19c7cb107d7989946844c6d0e94fefc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a63ffd0c9698ed7f0f0c65ee10665fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0af1ab7a31a70765d5df2a56d5f08727.jpg" align="middle">
</details>




<h2 id="Debias-your-Large-Multi-Modal-Model-at-Test-Time-with-Non-Contrastive-Visual-Attribute-Steering"><a href="#Debias-your-Large-Multi-Modal-Model-at-Test-Time-with-Non-Contrastive-Visual-Attribute-Steering" class="headerlink" title="Debias your Large Multi-Modal Model at Test-Time with Non-Contrastive   Visual Attribute Steering"></a>Debias your Large Multi-Modal Model at Test-Time with Non-Contrastive   Visual Attribute Steering</h2><p><strong>Authors:Neale Ratzlaff, Matthew Lyle Olson, Musashi Hinck, Estelle Aflalo, Shao-Yen Tseng, Vasudev Lal, Phillip Howard</strong></p>
<p>Large Multi-Modal Models (LMMs) have demonstrated impressive capabilities as general-purpose chatbots that can engage in conversations about a provided input, such as an image. However, their responses are influenced by societal biases present in their training datasets, leading to undesirable differences in how the model responds when presented with images depicting people of different demographics. In this work, we propose a novel debiasing framework for LMMs that directly removes biased representations during text generation to decrease outputs related to protected attributes, or even representing them internally. Our proposed method is training-free; given a single image and a list of target attributes, we can ablate the corresponding representations with just one step of gradient descent on the image itself. Our experiments show that not only can we can minimize the propensity of LMMs to generate text related to protected attributes, but we can improve sentiment and even simply use synthetic data to inform the ablation while retaining language modeling capabilities on real data such as COCO or FACET. Furthermore, we find the resulting generations from a debiased LMM exhibit similar accuracy as a baseline biased model, showing that debiasing effects can be achieved without sacrificing model performance. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ä½œä¸ºé€šç”¨èŠå¤©æœºå™¨äººæ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå®ƒä»¬å¯ä»¥è¿›è¡Œå…³äºæ‰€æä¾›è¾“å…¥ï¼ˆå¦‚å›¾åƒï¼‰çš„å¯¹è¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„å›åº”å—åˆ°å…¶è®­ç»ƒæ•°æ®é›†ä¸­å­˜åœ¨çš„ç¤¾ä¼šåè§çš„å½±å“ï¼Œå¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æç»˜ä¸åŒäººå£ç»Ÿè®¡å›¾åƒçš„è¾“å…¥æ—¶ï¼Œå›åº”æ–¹å¼å­˜åœ¨ä¸å¸Œæœ›å‡ºç°çš„å·®å¼‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºLMMsçš„æ–°å‹å»åæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ç›´æ¥å»é™¤æœ‰åè¡¨ç¤ºï¼Œä»¥å‡å°‘ä¸å—ä¿æŠ¤å±æ€§ç›¸å…³çš„è¾“å‡ºï¼Œç”šè‡³ä¸å†…éƒ¨è¡¨ç¤ºå®ƒä»¬ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¯å…åŸ¹è®­çš„ï¼›ç»™å®šä¸€ä¸ªå›¾åƒå’Œä¸€ç³»åˆ—ç›®æ ‡å±æ€§ï¼Œæˆ‘ä»¬åªéœ€åœ¨å›¾åƒæœ¬èº«ä¸Šè¿›è¡Œä¸€æ­¥æ¢¯åº¦ä¸‹é™ï¼Œå°±å¯ä»¥æ¶ˆé™¤ç›¸åº”çš„è¡¨ç¤ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬ä¸ä»…èƒ½å¤Ÿæœ€å°åŒ–LMMsç”Ÿæˆä¸å—ä¿æŠ¤å±æ€§ç›¸å…³æ–‡æœ¬çš„å€¾å‘ï¼Œè€Œä¸”æˆ‘ä»¬è¿˜å¯ä»¥æ”¹å–„æƒ…æ„Ÿï¼Œç”šè‡³åªéœ€ä½¿ç”¨åˆæˆæ•°æ®æ¥æŒ‡å¯¼æ¶ˆé™¤è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿ç•™åœ¨çœŸå®æ•°æ®ï¼ˆå¦‚COCOæˆ–FACETï¼‰ä¸Šçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å»åLMMç”Ÿæˆçš„æˆæœä¸åŸºçº¿åè§æ¨¡å‹çš„å‡†ç¡®æ€§ç›¸ä¼¼ï¼Œè¿™è¡¨æ˜å¯ä»¥åœ¨ä¸ç‰ºç‰²æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹å®ç°å»åæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12590v1">PDF</a> 10 pages, 3 Figures, 3 Tables. arXiv admin note: text overlap with   arXiv:2410.13976</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ä½œä¸ºé€šç”¨èŠå¤©æœºå™¨äººæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å—è®­ç»ƒæ•°æ®é›†çš„ç¤¾ä¼šåè§å½±å“ï¼Œå¯¼è‡´å¯¹ä¸åŒäººå£ç‰¹å¾çš„å›¾åƒçš„ååº”å­˜åœ¨ä¸å¿…è¦çš„å·®å¼‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹LMMsçš„æ–°å‹å»åæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ç›´æ¥æ¶ˆé™¤åè§è¡¨ç¤ºï¼Œå‡å°‘ä¸å—ä¿æŠ¤å±æ€§ç›¸å…³çš„è¾“å‡ºï¼Œç”šè‡³ä¸å†…éƒ¨è¡¨ç¤ºå®ƒä»¬ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ— éœ€è®­ç»ƒï¼›åªéœ€ç»™å®šå•ä¸ªå›¾åƒå’Œç›®æ ‡å±æ€§åˆ—è¡¨ï¼Œæˆ‘ä»¬å°±å¯ä»¥é€šè¿‡å›¾åƒæœ¬èº«çš„æ¢¯åº¦ä¸‹é™ä¸€æ­¥æ¶ˆé™¤ç›¸åº”çš„è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥å‡å°‘LMMsç”Ÿæˆä¸å—ä¿æŠ¤å±æ€§ç›¸å…³æ–‡æœ¬çš„è¶‹åŠ¿ï¼Œè¿˜å¯ä»¥æ”¹å–„æƒ…æ„Ÿï¼Œç”šè‡³ä½¿ç”¨åˆæˆæ•°æ®æ¥æŒ‡å¯¼æ¶ˆé™¤è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿ç•™åœ¨çœŸå®æ•°æ®ï¼ˆå¦‚COCOæˆ–FACETï¼‰ä¸Šçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°å»ååçš„LMMç”Ÿæˆçš„å‡†ç¡®æ€§ç±»ä¼¼äºåŸºçº¿åè§æ¨¡å‹ï¼Œè¡¨æ˜å¯ä»¥åœ¨ä¸ç‰ºç‰²æ¨¡å‹æ€§èƒ½çš„æƒ…å†µä¸‹å®ç°å»åæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMsè™½ç„¶ä½œä¸ºèŠå¤©æœºå™¨äººè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å—è®­ç»ƒæ•°æ®ä¸­çš„ç¤¾ä¼šåè§å½±å“ï¼Œå¯¹ä¸åŒäººå£ç‰¹å¾çš„å›¾åƒååº”å­˜åœ¨åå·®ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹å»åæ¡†æ¶ï¼Œå¯ä»¥åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æ¶ˆé™¤åè§è¡¨ç¤ºã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œåªéœ€ç»™å®šå›¾åƒå’Œç›®æ ‡å±æ€§åˆ—è¡¨å³å¯æ¶ˆé™¤ç›¸å…³è¡¨ç¤ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘ä¸å—ä¿æŠ¤å±æ€§ç›¸å…³çš„æ–‡æœ¬è¾“å‡ºï¼Œè¿˜èƒ½æ”¹å–„è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å»ååçš„LMMç”Ÿæˆçš„å‡†ç¡®æ€§ä¸é€Šè‰²äºåŸºçº¿åè§æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥åˆ©ç”¨åˆæˆæ•°æ®æ¥æŒ‡å¯¼æ¶ˆé™¤è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿ç•™åœ¨çœŸå®æ•°æ®ä¸Šçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9fe3e0db4346e7f248e09667adb41d8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6fd545dc8615cd30a620f42cde517be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d061645edbc0ce1014e343ceb7dcdd39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae260ac80570dc27d5f90bfabf067aea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0b9616aa428482a96f4b3129d6908fd.jpg" align="middle">
</details>




<h2 id="Cross-Patient-Pseudo-Bags-Generation-and-Curriculum-Contrastive-Learning-for-Imbalanced-Multiclassification-of-Whole-Slide-Image"><a href="#Cross-Patient-Pseudo-Bags-Generation-and-Curriculum-Contrastive-Learning-for-Imbalanced-Multiclassification-of-Whole-Slide-Image" class="headerlink" title="Cross-Patient Pseudo Bags Generation and Curriculum Contrastive Learning   for Imbalanced Multiclassification of Whole Slide Image"></a>Cross-Patient Pseudo Bags Generation and Curriculum Contrastive Learning   for Imbalanced Multiclassification of Whole Slide Image</h2><p><strong>Authors:Yonghuang Wu, Xuan Xie, Xinyuan Niu, Chengqian Zhao, Jinhua Yu</strong></p>
<p>Pathology computing has dramatically improved pathologistsâ€™ workflow and diagnostic decision-making processes. Although computer-aided diagnostic systems have shown considerable value in whole slide image (WSI) analysis, the problem of multi-classification under sample imbalance remains an intractable challenge. To address this, we propose learning fine-grained information by generating sub-bags with feature distributions similar to the original WSIs. Additionally, we utilize a pseudo-bag generation algorithm to further leverage the abundant and redundant information in WSIs, allowing efficient training in unbalanced-sample multi-classification tasks. Furthermore, we introduce an affinity-based sample selection and curriculum contrastive learning strategy to enhance the stability of model representation learning. Unlike previous approaches, our framework transitions from learning bag-level representations to understanding and exploiting the feature distribution of multi-instance bags. Our method demonstrates significant performance improvements on three datasets, including tumor classification and lymph node metastasis. On average, it achieves a 4.39-point improvement in F1 score compared to the second-best method across the three tasks, underscoring its superior performance. </p>
<blockquote>
<p>ç—…ç†å­¦è®¡ç®—æ˜¾è‘—æ”¹å–„äº†ç—…ç†å­¦å®¶çš„å·¥ä½œæµç¨‹å’Œè¯Šæ–­å†³ç­–è¿‡ç¨‹ã€‚è™½ç„¶è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­æ˜¾ç¤ºå‡ºç›¸å½“å¤§çš„ä»·å€¼ï¼Œä½†åœ¨æ ·æœ¬ä¸å¹³è¡¡ä¸‹çš„å¤šåˆ†ç±»é—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªéš¾ä»¥è§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡ç”Ÿæˆå…·æœ‰ä¸åŸå§‹WSIç›¸ä¼¼çš„ç‰¹å¾åˆ†å¸ƒçš„å­åŒ…æ¥å­¦ä¹ ç²¾ç»†ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¼ªè¢‹ç”Ÿæˆç®—æ³•è¿›ä¸€æ­¥åˆ©ç”¨WSIä¸­ä¸°å¯Œä¸”å¤šä½™çš„ä¿¡æ¯ï¼Œå®ç°åœ¨ä¸å¹³è¡¡æ ·æœ¬å¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºäº²å’ŒåŠ›çš„æ ·æœ¬é€‰æ‹©å’Œè¯¾ç¨‹å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹è¡¨ç¤ºå­¦ä¹ çš„ç¨³å®šæ€§ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä»å­¦ä¹ è¢‹çº§è¡¨ç¤ºè¿‡æ¸¡åˆ°ç†è§£å’Œåˆ©ç”¨å¤šå®ä¾‹è¢‹çš„ç‰¹å¾åˆ†å¸ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬è‚¿ç˜¤åˆ†ç±»å’Œæ·‹å·´ç»“è½¬ç§»ã€‚å¹³å‡è€Œè¨€ï¼Œä¸ä¸‰é¡¹ä»»åŠ¡ä¸­ç¬¬äºŒå¥½çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨F1åˆ†æ•°ä¸Šæé«˜äº†4.39åˆ†ï¼Œçªæ˜¾äº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11262v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç—…ç†å›¾åƒåˆ†æä¸­çš„æ ·æœ¬ä¸å¹³è¡¡å¤šåˆ†ç±»é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºé€šè¿‡ç”Ÿæˆå…·æœ‰ä¸åŸå§‹WSIç›¸ä¼¼ç‰¹å¾åˆ†å¸ƒçš„ç»†ç²’åº¦ä¿¡æ¯çš„å­åŒ…è¿›è¡Œå­¦ä¹ ã€‚åŒæ—¶åˆ©ç”¨ä¼ªåŒ…ç”Ÿæˆç®—æ³•ï¼Œè¿›ä¸€æ­¥æŒ–æ˜WSIä¸­çš„ä¸°å¯Œå†—ä½™ä¿¡æ¯ï¼Œå®ç°åœ¨ä¸å¹³è¡¡æ ·æœ¬å¤šåˆ†ç±»ä»»åŠ¡ä¸­çš„é«˜æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œå¼•å…¥åŸºäºäº²å’ŒåŠ›çš„æ ·æœ¬é€‰æ‹©å’Œè¯¾ç¨‹å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæé«˜æ¨¡å‹è¡¨ç¤ºå­¦ä¹ çš„ç¨³å®šæ€§ã€‚ç›¸è¾ƒäºä»¥å¾€æ–¹æ³•ï¼Œæœ¬æ–‡æ¡†æ¶ä»å­¦ä¹ åŒ…çº§è¡¨ç¤ºè½¬å‘ç†è§£å’Œåˆ©ç”¨å¤šå®ä¾‹åŒ…çš„ç‰¹å¾åˆ†å¸ƒï¼Œæ˜¾è‘—æé«˜äº†è‚¿ç˜¤åˆ†ç±»å’Œæ·‹å·´ç»“è½¬ç§»ç­‰ä¸‰ä¸ªæ•°æ®é›†çš„æ€§èƒ½ï¼Œå¹³å‡F1åˆ†æ•°è¾ƒç¬¬äºŒå¥½çš„æ–¹æ³•æé«˜äº†4.39ä¸ªç™¾åˆ†ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†å­¦è®¡ç®—æ˜¾è‘—æ”¹è¿›äº†ç—…ç†å­¦å®¶çš„å·¥ä½œæµç¨‹å’Œè¯Šæ–­å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­å±•ç°å‡ºæ˜¾è‘—ä»·å€¼ã€‚</li>
<li>æ ·æœ¬ä¸å¹³è¡¡çš„å¤šåˆ†ç±»é—®é¢˜æ˜¯WSIåˆ†æä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡ç”Ÿæˆå…·æœ‰ä¸åŸå§‹WSIç›¸ä¼¼ç‰¹å¾åˆ†å¸ƒçš„ç»†ç²’åº¦ä¿¡æ¯çš„å­åŒ…æ¥å­¦ä¹ ã€‚</li>
<li>åˆ©ç”¨ä¼ªåŒ…ç”Ÿæˆç®—æ³•æŒ–æ˜WSIsä¸­çš„ä¸°å¯Œå†—ä½™ä¿¡æ¯ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>å¼•å…¥åŸºäºäº²å’ŒåŠ›çš„æ ·æœ¬é€‰æ‹©å’Œè¯¾ç¨‹å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89c8495b05d0fca44294e18cd44642a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efc22958c742c5bfd133246aacfff93f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26a9b46e93a1aca0d37629992feea178.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8729cda508ca4d2ce3c9dcbb759e8ed3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9053c4181aea6f880aaaf81b14e0e61.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c7cebe5f20c328cc02a30b3343e4435.jpg" align="middle">
</details>




<h2 id="EXCON-Extreme-Instance-based-Contrastive-Representation-Learning-of-Severely-Imbalanced-Multivariate-Time-Series-for-Solar-Flare-Prediction"><a href="#EXCON-Extreme-Instance-based-Contrastive-Representation-Learning-of-Severely-Imbalanced-Multivariate-Time-Series-for-Solar-Flare-Prediction" class="headerlink" title="EXCON: Extreme Instance-based Contrastive Representation Learning of   Severely Imbalanced Multivariate Time Series for Solar Flare Prediction"></a>EXCON: Extreme Instance-based Contrastive Representation Learning of   Severely Imbalanced Multivariate Time Series for Solar Flare Prediction</h2><p><strong>Authors:Onur Vural, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</strong></p>
<p>In heliophysics research, predicting solar flares is crucial due to their potential to impact both space-based systems and Earthâ€™s infrastructure substantially. Magnetic field data from solar active regions, recorded by solar imaging observatories, are transformed into multivariate time series to enable solar flare prediction using temporal window-based analysis. In the realm of multivariate time series-driven solar flare prediction, addressing severe class imbalance with effective strategies for multivariate time series representation learning is key to developing robust predictive models. Traditional methods often struggle with overfitting to the majority class in prediction tasks where major solar flares are infrequent. This work presents EXCON, a contrastive representation learning framework designed to enhance classification performance amidst such imbalances. EXCON operates through four stages: obtaining core features from multivariate time series data; selecting distinctive contrastive representations for each class to maximize inter-class separation; training a temporal feature embedding module with a custom extreme reconstruction loss to minimize intra-class variation; and applying a classifier to the learned embeddings for robust classification. The proposed method leverages contrastive learning principles to map similar instances closer in the feature space while distancing dissimilar ones, a strategy not extensively explored in solar flare prediction tasks. This approach not only addresses class imbalance but also offers a versatile solution applicable to univariate and multivariate time series across binary and multiclass classification problems. Experimental results, including evaluations on the benchmark solar flare dataset and multiple time series archive datasets with binary and multiclass labels, demonstrate EXCONâ€™s efficacy in enhancing classification performance. </p>
<blockquote>
<p>åœ¨æ—¥å†•ç‰©ç†å­¦ç ”ç©¶ä¸­ï¼Œé¢„æµ‹å¤ªé˜³è€€æ–‘è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å¯¹å¤©åŸºç³»ç»Ÿå’Œåœ°çƒåŸºç¡€è®¾æ–½äº§ç”Ÿé‡å¤§å½±å“ã€‚æ¥è‡ªå¤ªé˜³æ´»åŠ¨åŒºåŸŸçš„ç£åœºæ•°æ®ï¼Œç”±å¤ªé˜³æˆåƒè§‚æµ‹ç«™è®°å½•ï¼Œè¢«è½¬åŒ–ä¸ºå¤šå…ƒæ—¶é—´åºåˆ—ï¼Œä»¥é€šè¿‡åŸºäºæ—¶é—´çª—å£çš„åˆ†æè¿›è¡Œå¤ªé˜³è€€æ–‘é¢„æµ‹ã€‚åœ¨å¤šå…ƒæ—¶é—´åºåˆ—é©±åŠ¨çš„å¤ªé˜³è€€æ–‘é¢„æµ‹é¢†åŸŸï¼Œé‡‡ç”¨æœ‰æ•ˆç­–ç•¥è§£å†³ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜å¯¹äºå¼€å‘ç¨³å¥çš„é¢„æµ‹æ¨¡å‹è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨é¢„æµ‹ä»»åŠ¡ä¸­å¾€å¾€ä¼šå¯¹å¤šæ•°ç±»è¿‡åº¦æ‹Ÿåˆï¼Œè€Œå¤§å‹å¤ªé˜³è€€æ–‘çš„å‘ç”Ÿé¢‘ç‡è¾ƒä½ã€‚è¿™é¡¹å·¥ä½œæå‡ºäº†EXCONï¼Œä¸€ä¸ªå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨æ­¤ç±»ä¸å¹³è¡¡æƒ…å†µä¸‹æé«˜åˆ†ç±»æ€§èƒ½ã€‚EXCONé€šè¿‡å››ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œï¼šä»å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®ä¸­è·å–æ ¸å¿ƒç‰¹å¾ï¼›ä¸ºæ¯ä¸ªç±»åˆ«é€‰æ‹©æœ‰ç‰¹è‰²çš„å¯¹æ¯”è¡¨ç¤ºï¼Œä»¥æœ€å¤§åŒ–ç±»é—´åˆ†ç¦»ï¼›è®­ç»ƒå…·æœ‰è‡ªå®šä¹‰æç«¯é‡å»ºæŸå¤±çš„ä¸´æ—¶ç‰¹å¾åµŒå…¥æ¨¡å—ï¼Œä»¥æœ€å°åŒ–ç±»å†…å˜åŒ–ï¼›å¯¹å­¦ä¹ çš„åµŒå…¥åº”ç”¨åˆ†ç±»å™¨ä»¥å®ç°ç¨³å¥åˆ†ç±»ã€‚æ‰€æå‡ºçš„æ–¹æ³•åˆ©ç”¨å¯¹æ¯”å­¦ä¹ åŸç†å°†ç›¸ä¼¼çš„å®ä¾‹æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ä¸­çš„è¾ƒè¿‘è·ç¦»ï¼ŒåŒæ—¶ä½¿ä¸ç›¸ä¼¼çš„å®ä¾‹è¿œç¦»ï¼Œè¿™ä¸€ç­–ç•¥åœ¨å¤ªé˜³è€€æ–‘é¢„æµ‹ä»»åŠ¡ä¸­å°šæœªè¢«å¹¿æ³›æ¢ç´¢ã€‚è¿™ç§æ–¹æ³•ä¸ä»…è§£å†³äº†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œè€Œä¸”æä¾›äº†ä¸€ä¸ªé€šç”¨è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºäºŒå…ƒå’Œå¤šå…ƒåˆ†ç±»é—®é¢˜çš„å•å…ƒå’Œå¤šå…ƒæ—¶é—´åºåˆ—ã€‚å®éªŒç»“æœåŒ…æ‹¬åœ¨åŸºå‡†å¤ªé˜³è€€æ–‘æ•°æ®é›†å’Œå…·æœ‰äºŒå…ƒå’Œå¤šå…ƒæ ‡ç­¾çš„å¤šä¸ªæ—¶é—´åºåˆ—å½’æ¡£æ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼Œè¯æ˜äº†EXCONåœ¨æé«˜åˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11249v1">PDF</a> This work has been accepted at the 2024 IEEE International Conference   on Big Data (IEEE BigData 2024) on October 27, 2024, as a main conference   paper</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤ªé˜³æ´»åŠ¨åŒºåŸŸè®°å½•çš„ç£åœºæ•°æ®è¿›è¡Œå¤šå…ƒæ—¶é—´åºåˆ—è½¬æ¢ï¼Œç”¨ä»¥é¢„æµ‹å¤ªé˜³è€€æ–‘ã€‚ä¸ºè§£å†³é¢„æµ‹ä»»åŠ¡ä¸­çš„ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œæå‡ºEXCONå¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ ¸å¿ƒç‰¹å¾æå–ã€å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ã€æ—¶é—´ç‰¹å¾åµŒå…¥å’Œåˆ†ç±»å™¨åº”ç”¨å››ä¸ªé˜¶æ®µï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä¸ä»…è§£å†³äº†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿˜é€‚ç”¨äºå•å˜é‡å’Œå¤šå˜é‡æ—¶é—´åºåˆ—çš„äºŒå…ƒå’Œå¤šç±»åˆ†ç±»é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEXCONåœ¨å¢å¼ºåˆ†ç±»æ€§èƒ½æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„æµ‹å¤ªé˜³è€€æ–‘å¯¹ç©ºé—´ç³»ç»Ÿå’Œåœ°çƒåŸºç¡€è®¾æ–½çš„å½±å“è‡³å…³é‡è¦ã€‚</li>
<li>å¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®åœ¨å¤ªé˜³è€€æ–‘é¢„æµ‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨é¢„æµ‹ä»»åŠ¡ä¸­é¢ä¸´ç±»åˆ«ä¸å¹³è¡¡å’Œè¿‡åº¦æ‹Ÿåˆçš„é—®é¢˜ã€‚</li>
<li>EXCONå¯¹æ¯”å­¦ä¹ æ¡†æ¶é€šè¿‡å››ä¸ªæ­¥éª¤æé«˜åˆ†ç±»æ€§èƒ½ï¼šæ ¸å¿ƒç‰¹å¾æå–ã€å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ã€æ—¶é—´ç‰¹å¾åµŒå…¥å’Œåˆ†ç±»å™¨åº”ç”¨ã€‚</li>
<li>EXCONåˆ©ç”¨å¯¹æ¯”å­¦ä¹ åŸåˆ™ï¼Œå°†ç›¸ä¼¼å®ä¾‹æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ä¸­çš„è¿‘è·ç¦»ï¼ŒåŒæ—¶ä½¿ä¸åŒå®ä¾‹ç›¸è·è¾ƒè¿œã€‚</li>
<li>EXCONæ¡†æ¶å¯åº”ç”¨äºäºŒå…ƒå’Œå¤šç±»åˆ†ç±»é—®é¢˜çš„å•å˜é‡å’Œå¤šå˜é‡æ—¶é—´åºåˆ—ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ebdbb551023fac6d5aeb86e0f4ffc07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-300ff700090c75c6bf185e37fcbc6169.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bff33d1bf80bf82861824bebd1252355.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce253fa0162ffda10a0b899414891552.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90444d12b1dfb84b4be7629c4fc59c7b.jpg" align="middle">
</details>




<h2 id="Relational-Contrastive-Learning-and-Masked-Image-Modeling-for-Scene-Text-Recognition"><a href="#Relational-Contrastive-Learning-and-Masked-Image-Modeling-for-Scene-Text-Recognition" class="headerlink" title="Relational Contrastive Learning and Masked Image Modeling for Scene Text   Recognition"></a>Relational Contrastive Learning and Masked Image Modeling for Scene Text   Recognition</h2><p><strong>Authors:Tiancheng Lin, Jinglei Zhang, Yi Xu, Kai Chen, Rui Zhang, Chang-Wen Chen</strong></p>
<p>Context-aware methods have achieved remarkable advancements in supervised scene text recognition by leveraging semantic priors from words. Considering the heterogeneity of text and background in STR, we propose that such contextual priors can be reinterpreted as the relations between textual elements, serving as effective self-supervised labels for representation learning. However, textual relations are restricted to the finite size of the dataset due to lexical dependencies, which causes over-fitting problem, thus compromising the representation quality. To address this, our work introduces a unified framework of Relational Contrastive Learning and Masked Image Modeling for STR (RCMSTR), which explicitly models the enriched textual relations. For the RCL branch, we first introduce the relational rearrangement module to cultivate new relations on the fly. Based on this, we further conduct relational contrastive learning to model the intra- and inter-hierarchical relations for frames, sub-words and words. On the other hand, MIM can naturally boost the context information via masking, where we find that the block masking strategy is more effective for STR. For the effective integration of RCL and MIM, we also introduce a novel decoupling design aimed at mitigating the impact of masked images on contrastive learning. Additionally, to enhance the compatibility of MIM with CNNs, we propose the adoption of sparse convolutions and directly sharing the weights with dense convolutions in training. The proposed RCMSTR demonstrates superior performance in various evaluation protocols for different STR-related downstream tasks, outperforming the existing state-of-the-art self-supervised STR techniques. Ablation studies and qualitative experimental results further validate the effectiveness of our method. The code and pre-trained models will be available at <a target="_blank" rel="noopener" href="https://github.com/ThunderVVV/RCMSTR">https://github.com/ThunderVVV/RCMSTR</a> . </p>
<blockquote>
<p>é’ˆå¯¹æœ‰ç›‘ç£çš„åœºæ™¯æ–‡æœ¬è¯†åˆ«ï¼Œç»“åˆè¯çº§çš„è¯­ä¹‰å…ˆéªŒçŸ¥è¯†ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚è€ƒè™‘åˆ°åœºæ™¯æ–‡æœ¬ï¼ˆSTRï¼‰ä¸­çš„æ–‡æœ¬å’ŒèƒŒæ™¯å¤šæ ·æ€§ï¼Œæˆ‘ä»¬æå‡ºå¯ä»¥å°†æ­¤ç±»ä¸Šä¸‹æ–‡å…ˆéªŒçŸ¥è¯†é‡æ–°è§£é‡Šä¸ºæ–‡æœ¬å…ƒç´ ä¹‹é—´çš„å…³ç³»ï¼Œä½œä¸ºè¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆè‡ªç›‘ç£æ ‡ç­¾ã€‚ç„¶è€Œï¼Œç”±äºè¯æ±‡ä¾èµ–å…³ç³»ï¼Œæ–‡æœ¬å…³ç³»å—é™äºæ•°æ®é›†çš„å¤§å°ï¼Œè¿™ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä»è€Œå½±å“è¡¨ç¤ºè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼•å…¥äº†é’ˆå¯¹STRçš„Relational Contrastive Learningå’ŒMasked Image Modelingçš„ç»Ÿä¸€æ¡†æ¶ï¼ˆRCMSTRï¼‰ï¼Œè¯¥æ¡†æ¶å¯¹ä¸°å¯Œçš„æ–‡æœ¬å…³ç³»è¿›è¡Œæ˜¾å¼å»ºæ¨¡ã€‚åœ¨RCLåˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥å…³ç³»é‡æ„æ¨¡å—æ¥å®æ—¶åŸ¹å…»æ–°å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œå…³ç³»å¯¹æ¯”å­¦ä¹ ï¼Œå¯¹å¸§ã€å­è¯å’Œè¯çš„å†…éƒ¨å’Œå±‚æ¬¡é—´å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚å¦ä¸€æ–¹é¢ï¼ŒMIMå¯ä»¥é€šè¿‡æ©ç è‡ªç„¶åœ°æå‡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ‘ä»¬å‘ç°å¯¹äºSTRè€Œè¨€ï¼Œå—æ©ç ç­–ç•¥æ›´ä¸ºæœ‰æ•ˆã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ•´åˆRCLå’ŒMIMï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹è§£è€¦è®¾è®¡ï¼Œæ—¨åœ¨å‡è½»é®æŒ¡å›¾åƒå¯¹å¯¹æ¯”å­¦ä¹ çš„å½±å“ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜MIMä¸å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å…¼å®¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é‡‡ç”¨ç¨€ç–å·ç§¯çš„æ–¹æ³•ï¼Œå¹¶åœ¨è®­ç»ƒä¸­ç›´æ¥å…±äº«ä¸å¯†é›†å·ç§¯çš„æƒé‡ã€‚æ‰€æå‡ºçš„RCMSTRåœ¨å„ç§è¯„ä¼°åè®®ä¸‹è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨å¤šç§ä¸STRç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›çš„è‡ªç›‘ç£STRæŠ€æœ¯ã€‚æ¶ˆèç ”ç©¶å’Œå®šæ€§å®éªŒç»“æœè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ThunderVVV/RCMSTR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ThunderVVV/RCMSTRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11219v2">PDF</a> arXiv admin note: text overlap with arXiv:2308.00508</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºåœºæ™¯æ–‡æœ¬è¯†åˆ«çš„è¯­å¢ƒæ„è¯†æ–¹æ³•ï¼Œå€ŸåŠ©è¯æ±‡è¯­ä¹‰å…ˆéªŒä¿¡æ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¸ºåº”å¯¹åœºæ™¯æ–‡æœ¬è¯†åˆ«ï¼ˆSTRï¼‰ä¸­æ–‡å­—å’ŒèƒŒæ™¯çš„å¼‚è´¨æ€§ï¼Œæå‡ºå°†æ–‡æœ¬å…ƒç´ é—´çš„å…³ç³»è§£é‡Šä¸ºä¸Šä¸‹æ–‡å…ˆéªŒï¼Œä½œä¸ºè¡¨ç¤ºå­¦ä¹ çš„æœ‰æ•ˆè‡ªç›‘ç£æ ‡ç­¾ã€‚ç„¶è€Œï¼Œç”±äºè¯æ±‡ä¾èµ–ï¼Œæ–‡æœ¬å…³ç³»å—é™äºæ•°æ®é›†çš„å¤§å°ï¼Œä¼šå¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå½±å“è¡¨ç¤ºè´¨é‡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆå…³ç³»å¯¹æ¯”å­¦ä¹ å’Œé®ç½©å›¾åƒå»ºæ¨¡çš„ç»Ÿä¸€æ¡†æ¶ï¼ˆRCMSTRï¼‰ï¼Œç”¨äºæ˜¾å¼å»ºæ¨¡ä¸°å¯Œçš„æ–‡æœ¬å…³ç³»ã€‚é€šè¿‡å…³ç³»é‡ç»„æ¨¡å—ç”Ÿæˆæ–°å…³ç³»ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå…³ç³»å¯¹æ¯”å­¦ä¹ ï¼Œä»¥å»ºæ¨¡å¸§ã€å­è¯å’Œè¯ä¹‹é—´çš„å†…éƒ¨å’Œå±‚æ¬¡é—´å…³ç³»ã€‚é®ç½©å›¾åƒå»ºæ¨¡è‡ªç„¶æå‡äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæˆ‘ä»¬å‘ç°å¯¹äºSTRæ¥è¯´ï¼Œå—é®ç½©ç­–ç•¥æ›´ä¸ºæœ‰æ•ˆã€‚ä¸ºæœ‰æ•ˆæ•´åˆå…³ç³»å¯¹æ¯”å­¦ä¹ å’Œé®ç½©å›¾åƒå»ºæ¨¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¼“è§£é®ç½©å›¾åƒå¯¹å¯¹æ¯”å­¦ä¹ å½±å“çš„è§£è€¦è®¾è®¡ã€‚æ­¤å¤–ï¼Œä¸ºæé«˜MIMä¸CNNçš„å…¼å®¹æ€§ï¼Œæˆ‘ä»¬æè®®é‡‡ç”¨ç¨€ç–å·ç§¯å¹¶åœ¨è®­ç»ƒä¸­ç›´æ¥å…±äº«æƒé‡ã€‚RCMSTRåœ¨å„ç§è¯„ä¼°åè®®ä¸‹çš„ä¸åŒSTRç›¸å…³ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è‡ªç›‘ç£STRæŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡æœ¬å…ƒç´ é—´çš„å…³ç³»å¯ä½œä¸ºä¸Šä¸‹æ–‡å…ˆéªŒï¼Œç”¨äºåœºæ™¯æ–‡æœ¬è¯†åˆ«çš„è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>æ–‡æœ¬å…³ç³»å—é™äºæ•°æ®é›†å¤§å°ï¼Œä¼šå¯¼è‡´è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>å¼•å…¥å…³ç³»å¯¹æ¯”å­¦ä¹ ï¼ˆRCLï¼‰å’Œé®ç½©å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰çš„ç»Ÿä¸€æ¡†æ¶RCMSTRã€‚</li>
<li>RCMSTRé€šè¿‡å…³ç³»é‡ç»„æ¨¡å—ç”Ÿæˆæ–°å…³ç³»ï¼Œå¹¶è¿›è¡Œå…³ç³»å¯¹æ¯”å­¦ä¹ ã€‚</li>
<li>é®ç½©å›¾åƒå»ºæ¨¡èƒ½æå‡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå—é®ç½©ç­–ç•¥å¯¹STRæ›´æœ‰æ•ˆã€‚</li>
<li>å¼•å…¥è§£è€¦è®¾è®¡ä»¥ç¼“è§£é®ç½©å›¾åƒå¯¹å¯¹æ¯”å­¦ä¹ çš„å½±å“ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8018c0cb51b4f028ea8c5d341249976b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa231862cb8921f9ad662fc446e4f2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c51e1dd7947480a9251f431982f98bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cadd833c74735ad63bdfbc37b2a2fd57.jpg" align="middle">
</details>




<h2 id="Multi-perspective-Contrastive-Logit-Distillation"><a href="#Multi-perspective-Contrastive-Logit-Distillation" class="headerlink" title="Multi-perspective Contrastive Logit Distillation"></a>Multi-perspective Contrastive Logit Distillation</h2><p><strong>Authors:Qi Wang, Jinjia Zhou</strong></p>
<p>We propose a novel and efficient logit distillation method, Multi-perspective Contrastive Logit Distillation (MCLD), which leverages contrastive learning to distill logits from multiple perspectives in knowledge distillation. Recent research on logit distillation has primarily focused on maximizing the information learned from the teacher modelâ€™s logits to enhance the performance of the student model. To this end, we propose MCLD, which consists of three key components: Instance-wise CLD, Sample-wise CLD, and Category-wise CLD. These components are designed to facilitate the transfer of more information from the teacherâ€™s logits to the student model. Comprehensive evaluations on image classification tasks using CIFAR-100 and ImageNet, alongside representation transferability assessments on STL-10 and Tiny-ImageNet, highlight the significant advantages of our method. The knowledge distillation with our MCLD, surpasses existing state-of-the-art methods. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆlogitè’¸é¦æ–¹æ³•â€”â€”å¤šè§’åº¦å¯¹æ¯”logitè’¸é¦ï¼ˆMCLDï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä»å¤šä¸ªè§’åº¦è¿›è¡ŒçŸ¥è¯†è’¸é¦ä¸­çš„logitè’¸é¦ã€‚å…³äºlogitè’¸é¦çš„æœ€æ–°ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æœ€å¤§åŒ–ä»æ•™å¸ˆæ¨¡å‹çš„logitä¸­å­¦åˆ°çš„ä¿¡æ¯ï¼Œä»¥æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MCLDï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå®ä¾‹çº§åˆ«çš„CLDã€æ ·æœ¬çº§åˆ«çš„CLDå’Œç±»åˆ«çº§åˆ«çš„CLDã€‚è¿™äº›ç»„ä»¶çš„è®¾è®¡æ—¨åœ¨ä¾¿äºå°†æ›´å¤šä¿¡æ¯ä»æ•™å¸ˆæ¨¡å‹çš„logitè½¬ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬åœ¨CIFAR-100å’ŒImageNetå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶åœ¨STL-10å’ŒTiny-ImageNetä¸Šè¿›è¡Œäº†è¡¨ç¤ºè¿ç§»æ€§è¯„ä¼°ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„é‡è¦ä¼˜åŠ¿ã€‚ä½¿ç”¨æˆ‘ä»¬çš„MCLDè¿›è¡ŒçŸ¥è¯†è’¸é¦è¶…è¶Šäº†ç°æœ‰çš„æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10693v1">PDF</a> 10 pages, 6 figures, 11 tabels, 9 formulas, including pseudo-code</p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§æ–°å‹é«˜æ•ˆçš„logitè’¸é¦æ–¹æ³•â€”â€”å¤šè§’åº¦å¯¹æ¯”logitè’¸é¦ï¼ˆMCLDï¼‰ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ ä»å¤šä¸ªè§’åº¦å¯¹çŸ¥è¯†è¿›è¡Œè’¸é¦ã€‚MCLDåŒ…æ‹¬å®ä¾‹çº§ã€æ ·æœ¬çº§å’Œç±»åˆ«çº§çš„å¯¹æ¯”å­¦ä¹ ï¼Œæ—¨åœ¨ä»æ•™å¸ˆæ¨¡å‹çš„logitä¸­å‘å­¦ç”Ÿæ¨¡å‹è½¬ç§»æ›´å¤šä¿¡æ¯ã€‚åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MCLDçš„çŸ¥è¯†è’¸é¦è¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„logitè’¸é¦æ–¹æ³•â€”â€”å¤šè§’åº¦å¯¹æ¯”logitè’¸é¦ï¼ˆMCLDï¼‰ã€‚</li>
<li>åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œä»å¤šä¸ªè§’åº¦è¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚</li>
<li>MCLDåŒ…æ‹¬å®ä¾‹çº§ã€æ ·æœ¬çº§å’Œç±»åˆ«çº§çš„å¯¹æ¯”å­¦ä¹ ç»„ä»¶ã€‚</li>
<li>è¯¥æ–¹æ³•æ—¨åœ¨ä»æ•™å¸ˆæ¨¡å‹çš„logitä¸­å‘å­¦ç”Ÿæ¨¡å‹è½¬ç§»æ›´å¤šä¿¡æ¯ã€‚</li>
<li>åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬CIFAR-100å’ŒImageNetï¼Œæ˜¾ç¤ºäº†MCLDçš„ä¼˜åŠ¿ã€‚</li>
<li>åœ¨çŸ¥è¯†è’¸é¦æ–¹é¢çš„åº”ç”¨ï¼ŒMCLDè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-faf6f1c153a63696868bc404503cbf95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb1007eba7caf03623ccf3ac4064888a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d60f22985ecef5283fa2cf88760b83e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2368a30da21bc4fe3c3939496f36895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d200860b4178bc3d8ebf805a0a7310c8.jpg" align="middle">
</details>




<h2 id="Underwater-Image-Enhancement-with-Cascaded-Contrastive-Learning"><a href="#Underwater-Image-Enhancement-with-Cascaded-Contrastive-Learning" class="headerlink" title="Underwater Image Enhancement with Cascaded Contrastive Learning"></a>Underwater Image Enhancement with Cascaded Contrastive Learning</h2><p><strong>Authors:Yi Liu, Qiuping Jiang, Xinyi Wang, Ting Luo, Jingchun Zhou</strong></p>
<p>Underwater image enhancement (UIE) is a highly challenging task due to the complexity of underwater environment and the diversity of underwater image degradation. Due to the application of deep learning, current UIE methods have made significant progress. Most of the existing deep learning-based UIE methods follow a single-stage network which cannot effectively address the diverse degradations simultaneously. In this paper, we propose to address this issue by designing a two-stage deep learning framework and taking advantage of cascaded contrastive learning to guide the network training of each stage. The proposed method is called CCL-Net in short. Specifically, the proposed CCL-Net involves two cascaded stages, i.e., a color correction stage tailored to the color deviation issue and a haze removal stage tailored to improve the visibility and contrast of underwater images. To guarantee the underwater image can be progressively enhanced, we also apply contrastive loss as an additional constraint to guide the training of each stage. In the first stage, the raw underwater images are used as negative samples for building the first contrastive loss, ensuring the enhanced results of the first color correction stage are better than the original inputs. While in the second stage, the enhanced results rather than the raw underwater images of the first color correction stage are used as the negative samples for building the second contrastive loss, thus ensuring the final enhanced results of the second haze removal stage are better than the intermediate color corrected results. Extensive experiments on multiple benchmark datasets demonstrate that our CCL-Net can achieve superior performance compared to many state-of-the-art methods. The source code of CCL-Net will be released at <a target="_blank" rel="noopener" href="https://github.com/lewis081/CCL-Net">https://github.com/lewis081/CCL-Net</a>. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒå¢å¼ºï¼ˆUIEï¼‰æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæ°´ä¸‹ç¯å¢ƒçš„å¤æ‚æ€§å’Œæ°´ä¸‹å›¾åƒé€€åŒ–çš„å¤šæ ·æ€§ã€‚ç”±äºæ·±åº¦å­¦ä¹ çš„åº”ç”¨ï¼Œå½“å‰çš„UIEæ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„UIEæ–¹æ³•éµå¾ªå•é˜¶æ®µç½‘ç»œï¼Œæ— æ³•åŒæ—¶æœ‰æ•ˆåœ°è§£å†³å¤šç§é€€åŒ–é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¹¶åˆ©ç”¨çº§è”å¯¹æ¯”å­¦ä¹ æ¥æŒ‡å¯¼æ¯ä¸ªé˜¶æ®µçš„ç½‘ç»œè®­ç»ƒã€‚æ‰€ææ–¹æ³•ç®€ç§°ä¸ºCCL-Netã€‚å…·ä½“æ¥è¯´ï¼Œæ‰€æå‡ºçš„CCL-NetåŒ…æ‹¬ä¸¤ä¸ªçº§è”é˜¶æ®µï¼Œå³é’ˆå¯¹è‰²å½©åå·®é—®é¢˜çš„è‰²å½©æ ¡æ­£é˜¶æ®µå’Œæ—¨åœ¨æé«˜æ°´ä¸‹å›¾åƒçš„å¯è§åº¦å’Œå¯¹æ¯”åº¦çš„é™¤é›¾é˜¶æ®µã€‚ä¸ºäº†ä¿è¯æ°´ä¸‹å›¾åƒèƒ½å¤Ÿé€æ­¥å¢å¼ºï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨å¯¹æ¯”æŸå¤±ä½œä¸ºé¢å¤–çš„çº¦æŸæ¥æŒ‡å¯¼æ¯ä¸ªé˜¶æ®µçš„è®­ç»ƒã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå°†åŸå§‹æ°´ä¸‹å›¾åƒç”¨ä½œæ„å»ºç¬¬ä¸€ä¸ªå¯¹æ¯”æŸå¤±çš„è´Ÿæ ·æœ¬ï¼Œç¡®ä¿ç¬¬ä¸€é˜¶æ®µè‰²å½©æ ¡æ­£çš„å¢å¼ºç»“æœä¼˜äºåŸå§‹è¾“å…¥ã€‚è€Œåœ¨ç¬¬äºŒé˜¶æ®µï¼Œå°†ç¬¬ä¸€é˜¶æ®µè‰²å½©æ ¡æ­£çš„å¢å¼ºç»“æœè€ŒéåŸå§‹æ°´ä¸‹å›¾åƒç”¨ä½œæ„å»ºç¬¬äºŒä¸ªå¯¹æ¯”æŸå¤±çš„è´Ÿæ ·æœ¬ï¼Œä»è€Œç¡®ä¿ç¬¬äºŒé˜¶æ®µé™¤é›¾çš„æœ€ç»ˆå¢å¼ºç»“æœä¼˜äºä¸­é—´è‰²å½©æ ¡æ­£çš„ç»“æœã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„CCL-Netç›¸è¾ƒäºè®¸å¤šæœ€å…ˆè¿›çš„æ–¹æ³•å¯ä»¥å®ç°å“è¶Šçš„æ€§èƒ½ã€‚CCL-Netçš„æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/lewis030/CCL-Net%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/lewis030/CCL-Netä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10682v1">PDF</a> Accepted by IEEE Transacitons on MultiMedia</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸¤é˜¶æ®µæ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œåä¸ºCCL-Netã€‚è¯¥æ–¹æ³•é€šè¿‡é¢œè‰²æ ¡æ­£é˜¶æ®µå’ŒçƒŸé›¾å»é™¤é˜¶æ®µæ¥è§£å†³æ°´ä¸‹å›¾åƒå­˜åœ¨çš„å¤šæ ·åŒ–å’Œå¤æ‚æ€§é—®é¢˜ã€‚åœ¨æ¯ä¸ªé˜¶æ®µï¼Œä½¿ç”¨çº§è”å¯¹æ¯”å­¦ä¹ ä½œä¸ºç½‘ç»œè®­ç»ƒçš„æŒ‡å¯¼ï¼Œä»¥ç¡®ä¿å›¾åƒé€æ¸æ”¹è¿›ã€‚é€šè¿‡å¹¿æ³›çš„åŸºå‡†æµ‹è¯•é›†å®éªŒï¼Œè¯æ˜äº†CCL-Netç›¸æ¯”å…¶ä»–å…ˆè¿›çš„æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰çš„æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ°´ä¸‹ç¯å¢ƒçš„å¤æ‚æ€§å’Œæ°´ä¸‹å›¾åƒé€€åŒ–çš„å¤šæ ·æ€§ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„æ°´ä¸‹å›¾åƒå¢å¼ºæ–¹æ³•é‡‡ç”¨å•é˜¶æ®µç½‘ç»œï¼Œæ— æ³•åŒæ—¶è§£å†³å¤šæ ·åŒ–çš„é€€åŒ–é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ·±åº¦å­¦ä¹ æ–¹æ³•æ¡†æ¶ï¼Œé€šè¿‡é¢œè‰²æ ¡æ­£é˜¶æ®µè§£å†³é¢œè‰²åå·®é—®é¢˜ï¼Œå¹¶é€šè¿‡çƒŸé›¾å»é™¤é˜¶æ®µæé«˜æ°´ä¸‹å›¾åƒçš„å¯è§åº¦å’Œå¯¹æ¯”åº¦ã€‚</li>
<li>å¯¹æ¯”æŸå¤±è¢«ç”¨ä½œæ¯ä¸ªé˜¶æ®µçš„é™„åŠ çº¦æŸï¼Œä»¥ç¡®ä¿å›¾åƒé€æ¸å¢å¼ºã€‚åœ¨ç¬¬ä¸€é˜¶æ®µä¸­ï¼ŒåŸå§‹æ°´ä¸‹å›¾åƒè¢«ç”¨ä½œè´Ÿæ ·æœ¬æ„å»ºå¯¹æ¯”æŸå¤±ã€‚åœ¨ç¬¬äºŒé˜¶æ®µä¸­ï¼Œç¬¬ä¸€é˜¶æ®µçš„é¢œè‰²æ ¡æ­£ç»“æœç”¨äºæ„å»ºç¬¬äºŒä¸ªå¯¹æ¯”æŸå¤±ï¼Œç¡®ä¿æœ€ç»ˆçš„å¢å¼ºç»“æœä¼˜äºä¸­é—´ç»“æœã€‚</li>
<li>é€šè¿‡å¤šä¸ªåŸºå‡†æ•°æ®é›†çš„å®éªŒè¯æ˜ï¼Œæœ¬æ–‡æå‡ºçš„CCL-Netç›¸è¾ƒäºå…¶ä»–å‰æ²¿æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de707b05bb0253206f583f882ae31f3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-883ba4f29a0d0beff9be2c7ffcbe3932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1de3c288b316c860c3dbc095680d02b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb984ad9700a7e346da48981d59db2e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24491f78e4ede2682f6ddfc061a1f608.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3f566ff1c1ef1adf770ee29107a2a97.jpg" align="middle">
</details>




<h2 id="Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning"><a href="#Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning" class="headerlink" title="Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning"></a>Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning</h2><p><strong>Authors:Jun-En Ding, Chien-Chin Hsu, Feng Liu</strong></p>
<p>The classification of medical images is a pivotal aspect of disease diagnosis, often enhanced by deep learning techniques. However, traditional approaches typically focus on unimodal medical image data, neglecting the integration of diverse non-image patient data. This paper proposes a novel Cross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal medical image classification. The model effectively integrates both image and non-image data by constructing cross-modality graphs and leveraging contrastive learning to align multimodal features in a shared latent space. An inter-modality feature scaling module further optimizes the representation learning process by reducing the gap between heterogeneous modalities. The proposed approach is evaluated on two datasets: a Parkinsonâ€™s disease (PD) dataset and a public melanoma dataset. Results demonstrate that CGMCL outperforms conventional unimodal methods in accuracy, interpretability, and early disease prediction. Additionally, the method shows superior performance in multi-class melanoma classification. The CGMCL framework provides valuable insights into medical image classification while offering improved disease interpretability and predictive capabilities. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„å…³é”®ç¯èŠ‚ï¼Œå¾€å¾€å¯ä»¥é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯å¾—åˆ°å¢å¼ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¸“æ³¨äºå•æ¨¡æ€åŒ»ç–—å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†ä¸åŒéå›¾åƒæ‚£è€…æ•°æ®çš„æ•´åˆã€‚æœ¬æ–‡é’ˆå¯¹å¤šæ¨¡æ€åŒ»ç–—å›¾åƒåˆ†ç±»ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰æ¡†æ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¥å¯¹é½å¤šæ¨¡æ€ç‰¹å¾åœ¨å…±äº«æ½œåœ¨ç©ºé—´ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–äº†è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šå¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ•°æ®é›†å’Œå…¬å…±é»‘è‰²ç´ ç˜¤æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ï¼ŒCGMCLä¼˜äºä¼ ç»Ÿå•æ¨¡æ€æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚CGMCLæ¡†æ¶ä¸ºåŒ»ç–—å›¾åƒåˆ†ç±»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼ŒåŒæ—¶æé«˜äº†ç–¾ç—…å¯è§£é‡Šæ€§å’Œé¢„æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17494v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè·¨æ¨¡æ€å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰çš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†ç±»æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°æ•´åˆäº†å›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡ä¼˜åŒ–ä¸åŒæ¨¡æ€é—´ç‰¹å¾çš„æ¯”ä¾‹ï¼Œå‡å°‘ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œè¿›ä¸€æ­¥æé«˜ç‰¹å¾å­¦ä¹ çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¸•é‡‘æ£®ç—…å’Œé»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šï¼ŒCGmclæ¡†æ¶çš„åˆ†ç±»æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ï¼Œåœ¨å‡†ç¡®æ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„å…³é”®ç¯èŠ‚ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†éå›¾åƒæ•°æ®çš„æ•´åˆã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è·¨æ¨¡æ€å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆCGmclï¼‰æ¡†æ¶ï¼Œå®ç°äº†å¤šæ¨¡æ€åŒ»å­¦å›¾åƒçš„æœ‰æ•ˆåˆ†ç±»ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æŠ€æœ¯å’Œæ„å»ºè·¨æ¨¡æ€å›¾ï¼ŒæˆåŠŸèåˆäº†å›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¸åŒæ¨¡æ€ç‰¹å¾çš„æ¯”ä¾‹ï¼Œè¯¥æ¡†æ¶å‡å°‘äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œæé«˜äº†ç‰¹å¾å­¦ä¹ çš„æ•ˆæœã€‚</li>
<li>åœ¨å¸•é‡‘æ£®ç—…å’Œé»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCGmclæ¡†æ¶åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>CGMCLæ¡†æ¶åœ¨æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-772c9b8505fc70e5f0855bdb249c334f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a57e599a9ad9af9d08558934e581b32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b4ba0360bb361782143510eeae891e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82c5f47f24fe9d5f2e300cb82c6b076b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2376ff4beaffa5e235af8d72213fe3e8.jpg" align="middle">
</details>




<h2 id="SiamSeg-Self-Training-with-Contrastive-Learning-for-Unsupervised-Domain-Adaptation-Semantic-Segmentation-in-Remote-Sensing"><a href="#SiamSeg-Self-Training-with-Contrastive-Learning-for-Unsupervised-Domain-Adaptation-Semantic-Segmentation-in-Remote-Sensing" class="headerlink" title="SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain   Adaptation Semantic Segmentation in Remote Sensing"></a>SiamSeg: Self-Training with Contrastive Learning for Unsupervised Domain   Adaptation Semantic Segmentation in Remote Sensing</h2><p><strong>Authors:Bin Wang, Fei Deng, Shuang Wang, Wen Luo, Zhixuan Zhang, Peifan Jiang</strong></p>
<p>Semantic segmentation of remote sensing (RS) images is a challenging yet essential task with broad applications. While deep learning, particularly supervised learning with large-scale labeled datasets, has significantly advanced this field, the acquisition of high-quality labeled data remains costly and time-intensive. Unsupervised domain adaptation (UDA) provides a promising alternative by enabling models to learn from unlabeled target domain data while leveraging labeled source domain data. Recent self-training (ST) approaches employing pseudo-label generation have shown potential in mitigating domain discrepancies. However, the application of ST to RS image segmentation remains underexplored. Factors such as variations in ground sampling distance, imaging equipment, and geographic diversity exacerbate domain shifts, limiting model performance across domains. In that case, existing ST methods, due to significant domain shifts in cross-domain RS images, often underperform. To address these challenges, we propose integrating contrastive learning into UDA, enhancing the modelâ€™s ability to capture semantic information in the target domain by maximizing the similarity between augmented views of the same image. This additional supervision improves the modelâ€™s representational capacity and segmentation performance in the target domain. Extensive experiments conducted on RS datasets, including Potsdam, Vaihingen, and LoveDA, demonstrate that our method, SimSeg, outperforms existing approaches, achieving state-of-the-art results. Visualization and quantitative analyses further validate SimSegâ€™s superior ability to learn from the target domain. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg">https://github.com/woldier/SiamSeg</a>. </p>
<blockquote>
<p>é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§ä½†è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œå·²ç»å¤§å¤§æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œä½†æ˜¯è·å–é«˜è´¨é‡æ ‡è®°æ•°æ®ä»ç„¶æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ç›®æ ‡åŸŸçš„æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ ï¼ŒåŒæ—¶åˆ©ç”¨æºåŸŸçš„æœ‰æ ‡ç­¾æ•°æ®ã€‚æœ€è¿‘é‡‡ç”¨ä¼ªæ ‡ç­¾ç”Ÿæˆè¿›è¡Œè‡ªæˆ‘è®­ç»ƒï¼ˆSTï¼‰çš„æ–¹æ³•åœ¨å‡å°‘åŸŸå·®å¼‚æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†STåº”ç”¨äºRSå›¾åƒåˆ†å‰²ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ°é¢é‡‡æ ·è·ç¦»ã€æˆåƒè®¾å¤‡å’Œåœ°ç†å¤šæ ·æ€§çš„å˜åŒ–ç­‰å› ç´ åŠ å‰§äº†åŸŸåç§»ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸åŒåŸŸä¹‹é—´çš„æ€§èƒ½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè·¨åŸŸRSå›¾åƒä¸­çš„åŸŸåç§»è¾ƒå¤§ï¼Œç°æœ‰çš„STæ–¹æ³•å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå°†å¯¹æ¯”å­¦ä¹ æ•´åˆåˆ°UDAä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„å¢å¼ºè§†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¢å¼ºæ¨¡å‹åœ¨ç›®æ ‡åŸŸæ•è·è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¿™ç§é¢å¤–çš„ç›‘ç£æé«˜äº†æ¨¡å‹åœ¨ç›®æ ‡åŸŸä¸­çš„è¡¨ç¤ºèƒ½åŠ›å’Œåˆ†å‰²æ€§èƒ½ã€‚åœ¨åŒ…æ‹¬Potsdamã€Vaihingenå’ŒLoveDAçš„RSæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SimSegæ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å¯è§†åŒ–åŠå®šé‡åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†SimSegä»ç›®æ ‡åŸŸå­¦ä¹ çš„é«˜çº§èƒ½åŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg%E3%80%82">https://github.com/woldier/SiamSegã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13471v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é‡è¦ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚æ·±åº¦å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å¤§è§„æ¨¡æ ‡è®°æ•°æ®é›†è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œå·²åœ¨æ­¤é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè·å–é«˜è´¨é‡æ ‡è®°æ•°æ®æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»æ— æ ‡ç­¾çš„ç›®æ ‡åŸŸæ•°æ®ä¸­å­¦ä¹ ï¼ŒåŒæ—¶åˆ©ç”¨æœ‰æ ‡ç­¾çš„æºåŸŸæ•°æ®ã€‚æœ€è¿‘é‡‡ç”¨ä¼ªæ ‡ç­¾ç”Ÿæˆçš„è‡ªè®­ç»ƒï¼ˆSTï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºå‡å°‘åŸŸå·®å¼‚æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†STåº”ç”¨äºé¥æ„Ÿå›¾åƒåˆ†å‰²ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚ç”±äºåœ°é¢é‡‡æ ·è·ç¦»ã€æˆåƒè®¾å¤‡å’Œåœ°ç†å¤šæ ·æ€§çš„å˜åŒ–å¯¼è‡´çš„åŸŸåç§»å› ç´ åŠ å‰§äº†æ¨¡å‹è·¨åŸŸçš„æ€§å·®è·ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºé¥æ„Ÿå›¾åƒè·¨åŸŸçš„å·¨å¤§åŸŸåç§»ï¼Œç°æœ‰çš„STæ–¹æ³•å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå°†å¯¹æ¯”å­¦ä¹ æ•´åˆåˆ°UDAä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–åŒä¸€å›¾åƒå¢å¼ºè§†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¢å¼ºæ¨¡å‹åœ¨ç›®æ ‡åŸŸæ•è·è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚è¿™ç§é¢å¤–çš„ç›‘ç£æé«˜äº†æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›å’Œç›®æ ‡åŸŸçš„åˆ†å‰²æ€§èƒ½ã€‚åœ¨åŒ…æ‹¬Potsdamã€Vaihingenå’ŒLoveDAåœ¨å†…çš„é¥æ„Ÿæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SimSegæ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ°´å¹³ã€‚å¯è§†åŒ–å®šé‡åˆ†æäº†SimSegä»ç›®æ ‡åŸŸå­¦ä¹ çš„é«˜çº§èƒ½åŠ›ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/woldier/SiamSeg%E3%80%82">https://github.com/woldier/SiamSegã€‚</a> </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>é¥æ„Ÿå›¾åƒè¯­ä¹‰åˆ†å‰²æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¹¿æ³›åº”ç”¨çš„ä»»åŠ¡ã€‚</li>
<li>æ·±åº¦å­¦ä¹ å’Œç›‘ç£å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†æ ‡è®°æ•°æ®è·å–æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰ä¸ºä»ç›®æ ‡åŸŸæ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è‡ªè®­ç»ƒï¼ˆSTï¼‰æ–¹æ³•é€šè¿‡ä¼ªæ ‡ç­¾ç”Ÿæˆæ˜¾ç¤ºå‡ºå‡å°‘åŸŸå·®å¼‚æ½œåŠ›ï¼Œä½†åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­åº”ç”¨æœ‰é™ã€‚</li>
<li>åŸŸåç§»å› ç´ å¦‚åœ°é¢é‡‡æ ·è·ç¦»ã€æˆåƒè®¾å¤‡å’Œåœ°ç†å¤šæ ·æ€§åŠ å‰§äº†è·¨åŸŸæ¨¡å‹çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ è¢«æ•´åˆåˆ°UDAä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„å¢å¼ºè§†å›¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨ç›®æ ‡åŸŸçš„è¯­ä¹‰æ•è·èƒ½åŠ›ã€‚</li>
<li>SimSegæ–¹æ³•é€šè¿‡ç»“åˆæ— ç›‘ç£å­¦ä¹ å’Œå¯¹æ¯”å­¦ä¹ åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸Šå–å¾—äº†å…ˆè¿›çš„ç»“æœã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-16b9c605aaae9f9eba57c4095c57a82a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0441e5239e8a25f01953cf4967e11891.jpg" align="middle">
</details>




<h2 id="Bridging-Text-and-Image-for-Artist-Style-Transfer-via-Contrastive-Learning"><a href="#Bridging-Text-and-Image-for-Artist-Style-Transfer-via-Contrastive-Learning" class="headerlink" title="Bridging Text and Image for Artist Style Transfer via Contrastive   Learning"></a>Bridging Text and Image for Artist Style Transfer via Contrastive   Learning</h2><p><strong>Authors:Zhi-Song Liu, Li-Wen Wang, Jun Xiao, Vicky Kalogeiton</strong></p>
<p>Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a Contrastive Learning for Artistic Style Transfer (CLAST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a supervised contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient adaLN based state space models that explore style-content fusion. Finally, we achieve a text-driven image style transfer. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in artistic style transfer. More importantly, it does not require online fine-tuning and can render a 512x512 image in 0.03s. </p>
<blockquote>
<p>å›¾åƒé£æ ¼è½¬æ¢åœ¨è¿‘å‡ å¹´å†…å¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ã€‚å°½ç®¡å…¶å–å¾—äº†æ˜¾è‘—çš„æ•ˆæœï¼Œä½†å®ƒéœ€è¦é¢å¤–çš„é£æ ¼å›¾åƒä½œä¸ºå‚è€ƒï¼Œè¿™ä½¿å¾—å…¶çµæ´»æ€§è¾ƒå·®ï¼Œä½¿ç”¨ä¸ä¾¿ã€‚ä½¿ç”¨æ–‡æœ¬æ˜¯æœ€è‡ªç„¶åœ°æè¿°é£æ ¼çš„æ–¹å¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ–‡æœ¬å¯ä»¥æè¿°éšå«çš„æŠ½è±¡é£æ ¼ï¼Œå¦‚ç‰¹å®šè‰ºæœ¯å®¶çš„é£æ ¼æˆ–è‰ºæœ¯è¿åŠ¨é£æ ¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‰ºæœ¯é£æ ¼è½¬æ¢çš„å¯¹æ¯”å­¦ä¹ ï¼ˆCLASTï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å…ˆè¿›çš„å›¾åƒæ–‡æœ¬ç¼–ç å™¨æ¥æ§åˆ¶ä»»æ„é£æ ¼è½¬æ¢ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰ç›‘ç£çš„å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°ä»å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰ä¸­æå–é£æ ¼æè¿°ï¼Œä½¿é£æ ¼åŒ–ä¸æ–‡æœ¬æè¿°ä¿æŒä¸€è‡´ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºadaLNçš„æ–°å‹é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œæ¢ç´¢é£æ ¼å†…å®¹çš„èåˆã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å®ç°äº†æ–‡æœ¬é©±åŠ¨å›¾åƒé£æ ¼è½¬æ¢ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‰ºæœ¯é£æ ¼è½¬æ¢æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒä¸éœ€è¦åœ¨çº¿å¾®è°ƒï¼Œå¯ä»¥åœ¨0.03ç§’å†…æ¸²æŸ“ä¸€ä¸ª512x512çš„å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.09566v2">PDF</a> 18 pages, 8 figures. arXiv admin note: substantial text overlap with   arXiv:2202.13562</p>
<p><strong>Summary</strong><br>åŸºäºæ–‡æœ¬æè¿°çš„è‰ºæœ¯é£æ ¼è¿ç§»å¯¹æ¯”å­¦ä¹ ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å…ˆè¿›çš„å›¾åƒæ–‡æœ¬ç¼–ç å™¨è¿›è¡Œä»»æ„é£æ ¼è¿ç§»ï¼Œé‡‡ç”¨ç›‘ç£å¯¹æ¯”è®­ç»ƒç­–ç•¥ä»å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸­æå–é£æ ¼æè¿°ï¼Œå¹¶ä¸æ–‡æœ¬æè¿°å¯¹é½ã€‚æå‡ºé«˜æ•ˆçš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå®ç°æ–‡æœ¬é©±åŠ¨å›¾åƒé£æ ¼è¿ç§»ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”æ— éœ€åœ¨çº¿å¾®è°ƒï¼Œå¯å¿«é€Ÿæ¸²æŸ“å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨å›¾åƒé£æ ¼è¿ç§»é¢†åŸŸï¼Œå°¤å…¶æ˜¯å¦‚ä½•åˆ©ç”¨æ–‡æœ¬æè¿°è¿›è¡Œé£æ ¼è¿ç§»ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„è‰ºæœ¯é£æ ¼è¿ç§»æ–¹æ³•ï¼ˆCLASTï¼‰ï¼Œåˆ©ç”¨å›¾åƒæ–‡æœ¬ç¼–ç å™¨è¿›è¡Œæ§åˆ¶ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£å¯¹æ¯”è®­ç»ƒç­–ç•¥ï¼Œä»å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰ä¸­æå–é£æ ¼æè¿°ï¼Œå®ç°ä¸æ–‡æœ¬æè¿°çš„å¯¹é½ã€‚</li>
<li>å¼•å…¥ä¸€ç§æ–°çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆadaLNï¼‰ï¼Œæ¢ç´¢é£æ ¼ä¸å†…å®¹èåˆã€‚</li>
<li>å®ç°æ–‡æœ¬é©±åŠ¨çš„å›¾åƒé£æ ¼è¿ç§»ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ— éœ€åœ¨çº¿å¾®è°ƒï¼Œå¯å¿«é€Ÿæ¸²æŸ“é«˜åˆ†è¾¨ç‡å›¾åƒï¼ˆå¦‚512x512åƒç´ ï¼‰ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-83ff0cba145c3c67d4425e8164c8928e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09054d694e956a4469b8ab4c879be1bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aece100e392e50ffa3169a21f3bd770.jpg" align="middle">
</details>




<h2 id="Adaptive-Patch-Contrast-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Adaptive-Patch-Contrast-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation"></a>Adaptive Patch Contrast for Weakly Supervised Semantic Segmentation</h2><p><strong>Authors:Wangyu Wu, Tianhong Dai, Zhenhong Chen, Xiaowei Huang, Jimin Xiao, Fei Ma, Renrong Ouyang</strong></p>
<p>Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to its cost-effectiveness. The typical framework involves using image-level labels as training data to generate pixel-level pseudo-labels with refinements. Recently, methods based on Vision Transformers (ViT) have demonstrated superior capabilities in generating reliable pseudo-labels, particularly in recognizing complete object regions, compared to CNN methods. However, current ViT-based approaches have some limitations in the use of patch embeddings, being prone to being dominated by certain abnormal patches, as well as many multi-stage methods being time-consuming and lengthy in training, thus lacking efficiency. Therefore, in this paper, we introduce a novel ViT-based WSSS method named \textit{Adaptive Patch Contrast} (APC) that significantly enhances patch embedding learning for improved segmentation effectiveness. APC utilizes an Adaptive-K Pooling (AKP) layer to address the limitations of previous max pooling selection methods. Additionally, we propose a Patch Contrastive Learning (PCL) to enhance patch embeddings, thereby further improving the final results. Furthermore, we improve upon the existing multi-stage training framework without CAM by transforming it into an end-to-end single-stage training approach, thereby enhancing training efficiency. The experimental results show that our approach is effective and efficient, outperforming other state-of-the-art WSSS methods on the PASCAL VOC 2012 and MS COCO 2014 dataset within a shorter training duration. </p>
<blockquote>
<p>ä½¿ç”¨ä»…å›¾åƒçº§æ ‡ç­¾çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰å› å…¶æˆæœ¬æ•ˆç›Šè€Œå¤‡å—å…³æ³¨ã€‚å…¸å‹çš„æ¡†æ¶æ˜¯ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œé€šè¿‡æ”¹è¿›ç”Ÿæˆåƒç´ çº§ä¼ªæ ‡ç­¾ã€‚æœ€è¿‘ï¼ŒåŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„æ–¹æ³•åœ¨ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å®Œæ•´çš„å¯¹è±¡åŒºåŸŸæ–¹é¢ä¼˜äºCNNæ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“å‰çš„ViTæ–¹æ³•åœ¨ä½¿ç”¨è¡¥ä¸åµŒå…¥æ—¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œå®¹æ˜“å—åˆ°æŸäº›å¼‚å¸¸è¡¥ä¸çš„ä¸»å¯¼ï¼Œè€Œä¸”è®¸å¤šå¤šé˜¶æ®µæ–¹æ³•è®­ç»ƒè€—æ—¶ä¸”å†—é•¿ï¼Œå› æ­¤ç¼ºä¹æ•ˆç‡ã€‚å› æ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºViTçš„WSSSæ–¹æ³•ï¼Œåä¸ºè‡ªé€‚åº”è¡¥ä¸å¯¹æ¯”ï¼ˆAPCï¼‰ï¼Œå®ƒæ˜¾è‘—å¢å¼ºäº†è¡¥ä¸åµŒå…¥å­¦ä¹ ï¼Œæé«˜äº†åˆ†å‰²æ•ˆæœã€‚APCåˆ©ç”¨è‡ªé€‚åº”Kæ± åŒ–ï¼ˆAKPï¼‰å±‚è§£å†³äº†ä»¥å‰æœ€å¤§æ± åŒ–é€‰æ‹©æ–¹æ³•çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è¡¥ä¸å¯¹æ¯”å­¦ä¹ ï¼ˆPCLï¼‰æ¥å¢å¼ºè¡¥ä¸åµŒå…¥ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æœ€ç»ˆç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¹è¿›äº†ç°æœ‰çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå°†å…¶è½¬å˜ä¸ºç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é«˜æ•ˆä¸”æœ‰æ•ˆï¼Œåœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›çš„WSSSæ–¹æ³•ï¼Œä¸”è®­ç»ƒæ—¶é—´æ›´çŸ­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.10649v2">PDF</a> Accepted by the EAAI Journal</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºVision Transformerï¼ˆViTï¼‰çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰æ–°æ–¹æ³•â€”â€”è‡ªé€‚åº”è¡¥ä¸å¯¹æ¯”ï¼ˆAPCï¼‰ï¼Œç”¨äºæ”¹è¿›è¡¥ä¸åµŒå…¥å­¦ä¹ ä»¥æé«˜åˆ†å‰²æ•ˆæœã€‚APCé€šè¿‡è‡ªé€‚åº”Kæ± åŒ–å±‚è§£å†³å…ˆå‰æœ€å¤§æ± åŒ–é€‰æ‹©æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºè¡¥ä¸å¯¹æ¯”å­¦ä¹ ï¼ˆPCLï¼‰å¢å¼ºè¡¥ä¸åµŒå…¥ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜æœ€ç»ˆç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ç°æœ‰çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶æ”¹è¿›ä¸ºç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨PASCAL VOC 2012å’ŒMS COCO 2014æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæ—¶é—´çŸ­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WSSSåˆ©ç”¨å›¾åƒçº§æ ‡ç­¾ä½œä¸ºè®­ç»ƒæ•°æ®ç”Ÿæˆåƒç´ çº§ä¼ªæ ‡ç­¾ï¼Œå…·æœ‰æˆæœ¬æ•ˆç›Šã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰åœ¨ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å®Œæ•´å¯¹è±¡åŒºåŸŸæ–¹é¢ä¼˜äºCNNæ–¹æ³•ã€‚</li>
<li>å½“å‰ViTæ–¹æ³•åœ¨ä½¿ç”¨è¡¥ä¸åµŒå…¥æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå®¹æ˜“å—åˆ°å¼‚å¸¸è¡¥ä¸çš„ä¸»å¯¼ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ViT-based WSSSæ–¹æ³•â€”â€”è‡ªé€‚åº”è¡¥ä¸å¯¹æ¯”ï¼ˆAPCï¼‰ï¼Œå¢å¼ºäº†è¡¥ä¸åµŒå…¥å­¦ä¹ ä»¥æé«˜åˆ†å‰²æ•ˆæœã€‚</li>
<li>APCé€šè¿‡è‡ªé€‚åº”Kæ± åŒ–å±‚å’Œè¡¥ä¸å¯¹æ¯”å­¦ä¹ ï¼ˆPCLï¼‰è§£å†³å…ˆå‰æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿›ä¸€æ­¥æé«˜åˆ†å‰²ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æ”¹è¿›äº†ç°æœ‰çš„å¤šé˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼Œå°†å…¶è½¬æ¢ä¸ºç«¯åˆ°ç«¯çš„å•é˜¶æ®µè®­ç»ƒï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-37c3ec3384e6ddd1aeaa3c8b2c22510e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4b55aacee3db09fe4f73359438a7ced.jpg" align="middle">
</details>




<h2 id="CLCE-An-Approach-to-Refining-Cross-Entropy-and-Contrastive-Learning-for-Optimized-Learning-Fusion"><a href="#CLCE-An-Approach-to-Refining-Cross-Entropy-and-Contrastive-Learning-for-Optimized-Learning-Fusion" class="headerlink" title="CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for   Optimized Learning Fusion"></a>CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for   Optimized Learning Fusion</h2><p><strong>Authors:Zijun Long, George Killick, Lipeng Zhuang, Gerardo Aragon-Camarasa, Zaiqiao Meng, Richard Mccreadie</strong></p>
<p>State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks, achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in transfer learning settings with the BEiT-3 model. Importantly, our proposed CLCE approach effectively mitigates the dependency of contrastive learning on large batch sizes such as 4096 samples per batch, a limitation that has previously constrained the application of contrastive learning in budget-limited hardware environments. </p>
<blockquote>
<p>å½“å‰å…ˆè¿›çš„é¢„è®­ç»ƒå›¾åƒæ¨¡å‹ä¸»è¦é‡‡å–ä¸¤é˜¶æ®µæ–¹æ³•ï¼šé¦–å…ˆåœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œåˆå§‹çš„æ— ç›‘ç£é¢„è®­ç»ƒï¼Œç„¶åä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼ˆCEï¼‰è¿›è¡Œç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚ç„¶è€Œï¼Œå·²ç»è¯æ˜CEå¯èƒ½æŸå®³æ¨¡å‹çš„æ³›åŒ–å’Œç¨³å®šæ€§ã€‚è™½ç„¶æœ€è¿‘é‡‡ç”¨å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•é€šè¿‡æé«˜åµŒå…¥è´¨é‡å’Œäº§ç”Ÿæ›´å¥½çš„å†³ç­–è¾¹ç•Œæ¥è§£å†³è¿™äº›é™åˆ¶ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜çš„é‡è¦æ€§ï¼Œå¹¶ä¾èµ–äºèµ„æºå¯†é›†å‹å’Œç¼“æ…¢çš„å¤§æ ·æœ¬æ‰¹æ¬¡è®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCLCEçš„æ–°æ–¹æ³•ï¼Œå®ƒå°†æ ‡ç­¾æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ä¸CEç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¿æŒäº†ä¸¤ç§æŸå¤±å‡½æ•°çš„ä¼˜ç‚¹ï¼Œè€Œä¸”ä»¥ååŒæ–¹å¼åˆ©ç”¨ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æ¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLCEåœ¨åäºŒä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºCEï¼Œåœ¨å°‘é•œå¤´å­¦ä¹ åœºæ™¯ä¸­å®ç°äº†é«˜è¾¾3.52%çš„å¢ç›Šï¼Œåœ¨è¿ç§»å­¦ä¹ è®¾ç½®ä¸­å®ç°äº†3.41%çš„å¢ç›Šï¼Œä½¿ç”¨BEiT-3æ¨¡å‹ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„CLCEæ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†å¯¹æ¯”å­¦ä¹ å¯¹å¤§é‡æ‰¹æ¬¡å¤§å°çš„ä¾èµ–ï¼Œä¾‹å¦‚æ¯ä¸ªæ‰¹æ¬¡4096ä¸ªæ ·æœ¬ï¼Œè¿™ä¸€é™åˆ¶ä»¥å‰åœ¨æœ‰é¢„ç®—é™åˆ¶ç¡¬ä»¶ç¯å¢ƒä¸­é™åˆ¶äº†å¯¹æ¯”å­¦ä¹ çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.14551v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCLCEçš„æ–°æ–¹æ³•ï¼Œå®ƒå°†æ ‡ç­¾æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ä¸äº¤å‰ç†µï¼ˆCEï¼‰ç›¸ç»“åˆï¼Œæ—¨åœ¨è§£å†³å½“å‰å›¾åƒæ¨¡å‹ä½¿ç”¨CEæŸå¤±å¸¦æ¥çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§é—®é¢˜ã€‚CLCEä¸ä»…ç»“åˆäº†ä¸¤ç§æŸå¤±å‡½æ•°çš„ä¼˜åŠ¿ï¼Œè¿˜é€šè¿‡ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æé«˜äº†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLCEåœ¨åäºŒä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºCEï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­æé«˜äº†é«˜è¾¾3.52%çš„Top-1å‡†ç¡®ç‡ï¼Œåœ¨è¿ç§»å­¦ä¹ è®¾ç½®ä¸­æé«˜äº†3.41%ã€‚æ­¤å¤–ï¼ŒCLCEè¿˜è§£å†³äº†å¯¹æ¯”å­¦ä¹ å¯¹å¤§æ‰¹æ¬¡å¤§å°çš„ä¾èµ–é—®é¢˜ï¼Œä½¿å…¶æ›´é€‚åˆé¢„ç®—æœ‰é™çš„ç¡¬ä»¶ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å›¾åƒæ¨¡å‹ä¸»è¦é‡‡ç”¨çš„é¢„è®­ç»ƒæ–¹æ³•æ˜¯ä¸¤é˜¶æ®µæ³•ï¼šé¦–å…ˆæ˜¯åŸºäºå¤§è§„æ¨¡æ•°æ®é›†çš„æ— ç›‘ç£é¢„è®­ç»ƒï¼Œç„¶åæ˜¯ä½¿ç”¨äº¤å‰ç†µï¼ˆCEï¼‰æŸå¤±çš„ç‰¹å®šä»»åŠ¡å¾®è°ƒã€‚</li>
<li>äº¤å‰ç†µæŸå¤±å¯èƒ½æŸå®³æ¨¡å‹çš„æ³›åŒ–å’Œç¨³å®šæ€§ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ–¹æ³•å·²ç”¨äºå¢å¼ºåµŒå…¥è´¨é‡å’Œå†³ç­–è¾¹ç•Œï¼Œä½†å¸¸å¸¸å¿½è§†ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ï¼Œä¸”ä¾èµ–äºèµ„æºå¯†é›†å‹çš„å¤§æ ·æœ¬æ‰¹æ¬¡è®­ç»ƒã€‚</li>
<li>æå‡ºçš„CLCEæ–¹æ³•ç»“åˆäº†æ ‡ç­¾æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ä¸äº¤å‰ç†µçš„ä¼˜åŠ¿ï¼Œå¹¶é€šè¿‡ååŒæ–¹å¼åˆ©ç”¨ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>CLCEåœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œè¿ç§»å­¦ä¹ åœºæ™¯ä¸‹æ˜¾è‘—ä¼˜äºäº¤å‰ç†µï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>CLCEæœ‰æ•ˆå‡è½»äº†å¯¹æ¯”å­¦ä¹ å¯¹å¤§æ‰¹æ¬¡å¤§å°çš„ä¾èµ–ï¼Œä½¿å…¶æ›´é€‚ç”¨äºç¡¬ä»¶èµ„æºæœ‰é™çš„ç¯å¢ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3a0a9b13b66666ec31f6038e31458cf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0621a0bf54ff1dfefbfcdbae2232a0ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e63de98f035722581e1b55bb3b18cd93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfe98c79c8631c91896479090b70c285.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9cbc5243909df5e44fb3353761da2a8.jpg" align="middle">
</details>




<h2 id="Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music"><a href="#Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music" class="headerlink" title="Emotion-Aligned Contrastive Learning Between Images and Music"></a>Emotion-Aligned Contrastive Learning Between Images and Music</h2><p><strong>Authors:Shanti Stewart, Kleanthis Avramidis, Tiantian Feng, Shrikanth Narayanan</strong></p>
<p>Traditional music search engines rely on retrieval methods that match natural language queries with music metadata. There have been increasing efforts to expand retrieval methods to consider the audio characteristics of music itself, using queries of various modalities including text, video, and speech. While most approaches aim to match general music semantics to the input queries, only a few focus on affective qualities. In this work, we address the task of retrieving emotionally-relevant music from image queries by learning an affective alignment between images and music audio. Our approach focuses on learning an emotion-aligned joint embedding space between images and music. This embedding space is learned via emotion-supervised contrastive learning, using an adapted cross-modal version of the SupCon loss. We evaluate the joint embeddings through cross-modal retrieval tasks (image-to-music and music-to-image) based on emotion labels. Furthermore, we investigate the generalizability of the learned music embeddings via automatic music tagging. Our experiments show that the proposed approach successfully aligns images and music, and that the learned embedding space is effective for cross-modal retrieval applications. </p>
<blockquote>
<p>ä¼ ç»ŸéŸ³ä¹æœç´¢å¼•æ“ä¾èµ–äºåŒ¹é…è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’ŒéŸ³ä¹å…ƒæ•°æ®çš„æ–¹æ³•æ¥è¿›è¡Œæ£€ç´¢ã€‚ç›®å‰ï¼Œäººä»¬æ­£è¶Šæ¥è¶Šå¤šåœ°åŠªåŠ›æ‰©å±•æ£€ç´¢æ–¹æ³•ï¼Œä»¥è€ƒè™‘éŸ³ä¹æœ¬èº«çš„éŸ³é¢‘ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨åŒ…æ‹¬æ–‡æœ¬ã€è§†é¢‘å’Œè¯­éŸ³åœ¨å†…çš„å„ç§æ¨¡æ€æŸ¥è¯¢ã€‚è™½ç„¶å¤§å¤šæ•°æ–¹æ³•æ—¨åœ¨å°†ä¸€èˆ¬éŸ³ä¹è¯­ä¹‰ä¸è¾“å…¥æŸ¥è¯¢ç›¸åŒ¹é…ï¼Œä½†åªæœ‰å°‘æ•°æ–¹æ³•å…³æ³¨æƒ…æ„Ÿå“è´¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å­¦ä¹ å›¾åƒå’ŒéŸ³ä¹éŸ³é¢‘ä¹‹é—´çš„æƒ…æ„Ÿå¯¹é½æ¥è§£å†³ä»å›¾åƒæŸ¥è¯¢ä¸­æ£€ç´¢æƒ…æ„Ÿç›¸å…³éŸ³ä¹çš„ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾§é‡äºåœ¨å›¾åƒå’ŒéŸ³ä¹ä¹‹é—´å­¦ä¹ ä¸€ä¸ªæƒ…æ„Ÿå¯¹é½çš„è”åˆåµŒå…¥ç©ºé—´ã€‚è¿™ä¸ªåµŒå…¥ç©ºé—´æ˜¯é€šè¿‡æƒ…æ„Ÿç›‘ç£å¯¹æ¯”å­¦ä¹ æ¥å­¦ä¹ çš„ï¼Œä½¿ç”¨é€‚åº”çš„è·¨æ¨¡æ€ç‰ˆæœ¬çš„SupConæŸå¤±ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºæƒ…æ„Ÿæ ‡ç­¾çš„è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ï¼ˆå›¾åƒåˆ°éŸ³ä¹å’ŒéŸ³ä¹åˆ°å›¾åƒï¼‰æ¥è¯„ä¼°è”åˆåµŒå…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡è‡ªåŠ¨éŸ³ä¹æ ‡è®°æ¥æ¢è®¨å­¦ä¹ åˆ°çš„éŸ³ä¹åµŒå…¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æˆåŠŸåœ°å¯¹é½äº†å›¾åƒå’ŒéŸ³ä¹ï¼Œå¹¶ä¸”å­¦ä¹ åˆ°çš„åµŒå…¥ç©ºé—´å¯¹äºè·¨æ¨¡æ€æ£€ç´¢åº”ç”¨æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12610v3">PDF</a> Published at ICASSP 2024. Code:   <a target="_blank" rel="noopener" href="https://github.com/shantistewart/Emo-CLIM">https://github.com/shantistewart/Emo-CLIM</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨å›¾åƒä¸éŸ³ä¹ä¹‹é—´çš„æƒ…æ„Ÿå¯¹é½é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡æƒ…ç»ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•æ¥å®ç°å›¾åƒå’ŒéŸ³ä¹ä¹‹é—´çš„è”åˆåµŒå…¥ç©ºé—´å­¦ä¹ ã€‚è¯¥ç ”ç©¶é‡‡ç”¨è·¨æ¨¡æ€ç‰ˆæœ¬çš„SupConæŸå¤±è¿›è¡Œé€‚åº”è°ƒæ•´ï¼Œé€šè¿‡æƒ…æ„Ÿæ ‡ç­¾è¿›è¡Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡è¯„ä¼°ï¼Œå¹¶æ¢ç©¶äº†éŸ³ä¹åµŒå…¥çš„å¯æ³›åŒ–æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå®ç°äº†å›¾åƒå’ŒéŸ³ä¹çš„å¯¹é½ï¼Œå¹¶æœ‰æ•ˆåº”ç”¨äºè·¨æ¨¡æ€æ£€ç´¢åº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä¼ ç»ŸéŸ³ä¹æœç´¢å¼•æ“ä¸»è¦ä¾èµ–è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸éŸ³ä¹å…ƒæ•°æ®åŒ¹é…è¿›è¡Œæ£€ç´¢ã€‚</li>
<li>ç›®å‰å·²æœ‰ç ”ç©¶æ‰©å±•æ£€ç´¢æ–¹æ³•ï¼Œè€ƒè™‘éŸ³ä¹çš„éŸ³é¢‘ç‰¹æ€§åŠå¤šç§æŸ¥è¯¢æ¨¡å¼ï¼Œå¦‚æ–‡æœ¬ã€è§†é¢‘å’Œè¯­éŸ³ã€‚</li>
<li>å¤§å¤šæ•°æ–¹æ³•æ—¨åœ¨åŒ¹é…ä¸€èˆ¬éŸ³ä¹è¯­ä¹‰åˆ°è¾“å…¥æŸ¥è¯¢ï¼Œè€Œåªæœ‰å°‘æ•°å…³æ³¨æƒ…æ„Ÿç‰¹è´¨ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡æƒ…æ„Ÿç›‘ç£å¯¹æ¯”å­¦ä¹ å®ç°å›¾åƒå’ŒéŸ³ä¹ä¹‹é—´çš„æƒ…æ„Ÿå¯¹é½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æƒ…ç»ªå¯¹é½çš„è·¨æ¨¡æ€è”åˆåµŒå…¥ç©ºé—´å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æƒ…æ„Ÿæ ‡ç­¾è¿›è¡Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡è¯„ä¼°ï¼Œè¯å®æ‰€å­¦åµŒå…¥ç©ºé—´çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-203876447fd802ae8a09d8f2f8cf6c1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da624c3aa03432c57a1cfb4f3b497a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bee2480ca3a701bec6fcd466efce05d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f45bff78f72670eddb6449c7f9273228.jpg" align="middle">
</details>




<h2 id="Cross-View-Prediction-Exploring-Contrastive-Feature-for-Hyperspectral-Image-Classification"><a href="#Cross-View-Prediction-Exploring-Contrastive-Feature-for-Hyperspectral-Image-Classification" class="headerlink" title="Cross-View-Prediction: Exploring Contrastive Feature for Hyperspectral   Image Classification"></a>Cross-View-Prediction: Exploring Contrastive Feature for Hyperspectral   Image Classification</h2><p><strong>Authors:Anyu Zhang, Haotian Wu, Zeyu Cao</strong></p>
<p>This paper presents a self-supervised feature learning method for hyperspectral image classification. Our method tries to construct two different views of the raw hyperspectral image through a cross-representation learning method. And then to learn semantically consistent representation over the created views by contrastive learning method. Specifically, four cross-channel-prediction based augmentation methods are naturally designed to utilize the high dimension characteristic of hyperspectral data for the view construction. And the better representative features are learned by maximizing mutual information and minimizing conditional entropy across different views from our contrastive network. This â€˜Cross-View-Predictonâ€™ style is straightforward and gets the state-of-the-art performance of unsupervised classification with a simple SVM classifier. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé«˜å…‰è°±å›¾åƒåˆ†ç±»çš„è‡ªç›‘ç£ç‰¹å¾å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¯•å›¾é€šè¿‡è·¨è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ„å»ºé«˜å…‰è°±åŸå§‹å›¾åƒçš„ä¸¤ä¸ªä¸åŒè§†å›¾ã€‚ç„¶åï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ æ–¹æ³•å­¦ä¹ åˆ›å»ºçš„è§†å›¾ä¸Šè¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºå››é€šé“é¢„æµ‹çš„å¢å¼ºæ–¹æ³•è‡ªç„¶åœ°åˆ©ç”¨é«˜å…‰è°±æ•°æ®çš„é«˜ç»´ç‰¹æ€§æ¥æ„å»ºè§†å›¾ã€‚é€šè¿‡æœ€å¤§åŒ–äº’ä¿¡æ¯å’Œæœ€å°åŒ–æ¥è‡ªå¯¹æ¯”ç½‘ç»œä¸åŒè§†å›¾ä¹‹é—´çš„æ¡ä»¶ç†µï¼Œå­¦ä¹ æ›´å…·ä»£è¡¨æ€§çš„ç‰¹å¾ã€‚è¿™ç§â€œè·¨è§†å›¾é¢„æµ‹â€é£æ ¼ç®€å•ç›´æ¥ï¼Œä½¿ç”¨ç®€å•çš„SVMåˆ†ç±»å™¨å³å¯å®ç°æœ€å…ˆè¿›çš„æ— ç›‘ç£åˆ†ç±»æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2203.07000v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºè‡ªç›‘ç£ç‰¹å¾å­¦ä¹ çš„è¶…å…‰è°±å›¾åƒåˆ†ç±»æ–¹æ³•ã€‚é€šè¿‡è·¨è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ„å»ºè¶…å…‰è°±å›¾åƒçš„ä¸åŒè§†å›¾ï¼Œç„¶åé‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•å­¦ä¹ è¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºã€‚è®¾è®¡å››ç§åŸºäºè·¨é€šé“é¢„æµ‹çš„å¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨è¶…å…‰è°±æ•°æ®çš„é«˜ç»´ç‰¹æ€§è¿›è¡Œè§†å›¾æ„å»ºã€‚é€šè¿‡æœ€å¤§åŒ–äº’ä¿¡æ¯å’Œæœ€å°åŒ–æ¡ä»¶ç†µï¼Œä»å¯¹æ¯”ç½‘ç»œä¸­å­¦ä¹ æ›´å…·ä»£è¡¨æ€§çš„ç‰¹å¾ã€‚è¿™ç§â€œè·¨è§†å›¾é¢„æµ‹â€é£æ ¼ç®€å•ç›´æ¥ï¼Œé‡‡ç”¨ç®€å•çš„SVMåˆ†ç±»å™¨ä¾¿è·å¾—æ— ç›‘ç£åˆ†ç±»çš„æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§è‡ªç›‘ç£ç‰¹å¾å­¦ä¹ æ–¹æ³•ç”¨äºè¶…å…‰è°±å›¾åƒåˆ†ç±»ã€‚</li>
<li>é€šè¿‡è·¨è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ„å»ºå›¾åƒçš„ä¸åŒè§†å›¾ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•å­¦ä¹ è¯­ä¹‰ä¸€è‡´çš„è¡¨ç¤ºã€‚</li>
<li>è®¾è®¡äº†å››ç§åŸºäºè·¨é€šé“é¢„æµ‹çš„å¢å¼ºæ–¹æ³•ï¼Œåˆ©ç”¨è¶…å…‰è°±æ•°æ®çš„é«˜ç»´ç‰¹æ€§ã€‚</li>
<li>é€šè¿‡å¯¹æ¯”ç½‘ç»œæœ€å¤§åŒ–äº’ä¿¡æ¯å’Œæœ€å°åŒ–æ¡ä»¶ç†µï¼Œå­¦ä¹ æ›´å…·ä»£è¡¨æ€§çš„ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ— ç›‘ç£åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0b405e623c7be5e501556cd18f2caaf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d221a480e0c7b4abc85b233da9e3edcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83f7f70ba3e7101c734381c817517420.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-501cc80ed7e3fa43822d377af44a7c4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccd078693dab1b1720ad15ef272c2a8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee7e307b66c6e2b57bd20244435bd23.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d460005db73d4069a7ca154e972bd229.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  UNet++ and LSTM combined approach for Breast Ultrasound Image   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-46824d2f6082475e2c89c230472a59e3.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Exploring Depth Information for Detecting Manipulated Face Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
