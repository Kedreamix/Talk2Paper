<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="MMT">
    <meta name="description" content="MMT 方向最新论文已更新，请持续关注 Update in 2024-12-10  Multimodal Whole Slide Foundation Model for Pathology">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>MMT | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-48451e82ae24963df895b403b137806d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">MMT</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/MMT/">
                                <span class="chip bg-color">MMT</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                MMT
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-10-更新"><a href="#2024-12-10-更新" class="headerlink" title="2024-12-10 更新"></a>2024-12-10 更新</h1><h2 id="Multimodal-Whole-Slide-Foundation-Model-for-Pathology"><a href="#Multimodal-Whole-Slide-Foundation-Model-for-Pathology" class="headerlink" title="Multimodal Whole Slide Foundation Model for Pathology"></a>Multimodal Whole Slide Foundation Model for Pathology</h2><p><strong>Authors:Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Bowen Chen, Cristina Almagro-Perez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Daisuke Komura, Akihiro Kawabe, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood</strong></p>
<p>The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL). However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose TITAN, a multimodal whole slide foundation model pretrained using 335,645 WSIs via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any finetuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that TITAN outperforms both ROI and slide foundation models across machine learning settings such as linear probing, few-shot and zero-shot classification, rare cancer retrieval and cross-modal retrieval, and pathology report generation. </p>
<blockquote>
<p>计算病理学领域已经发生了变革，得益于基础模型的最新进展，这些模型通过自我监督学习（SSL）将组织病理学感兴趣区域（ROI）编码成通用和可转移的特征表示。然而，将这些进展转化为针对患者和切片级别的复杂临床挑战仍然受到特定疾病队列中临床数据有限的制约，尤其是对于罕见的临床状况。我们提出了TITAN，这是一个多模式全切片基础模型，使用335,645张全切片图像（WSI）通过视觉自我监督学习和视觉语言对齐进行预训练，并辅以相应的病理报告和由多模式生成式AI病理助手生成的423,122个合成标题。TITAN无需微调或要求临床标签，即可提取通用切片表示并生成适用于资源有限的临床场景的病理报告，如罕见疾病检索和癌症预后。我们在多种临床任务上评估了TITAN的表现，发现无论是在线性探测、少样本和零样本分类、罕见癌症检索和跨模态检索，还是病理报告生成等机器学习环境中，TITAN的表现都优于ROI和切片基础模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19666v1">PDF</a> The code is accessible at <a target="_blank" rel="noopener" href="https://github.com/mahmoodlab/TITAN">https://github.com/mahmoodlab/TITAN</a></p>
<p><strong>摘要</strong><br>    基于自监督学习（SSL）的最新进展，计算病理学领域已经能够将组织病理学感兴趣区域（ROI）编码为通用和可转移的特征表示，从而推动了该领域的发展。然而，在患者和切片层面应对复杂的临床挑战时，尤其是在罕见临床条件下，由于特定疾病队列的临床数据有限，将这些进步转化为实际应用仍面临挑战。本文提出TITAN模型，这是一种多模态全切片基础模型，通过使用335,645张全切片图像（WSIs）进行视觉自监督学习和视觉语言对齐进行预训练，并融合了相应的病理报告和由多模态生成AI辅助生成的423,122张合成幻灯片字幕。TITAN无需微调且不需要临床标签，就能够提取通用的幻灯片表示并生成病理报告，可推广到资源有限的临床场景中，如罕见疾病检索和癌症预后。我们对TITAN进行了多样化的临床任务评估，发现其在机器学习设置（如线性探测、小样本和零样本分类、罕见癌症检索和跨模态检索以及病理报告生成）中表现优于ROI和幻灯片基础模型。</p>
<p><strong>要点</strong></p>
<ol>
<li>计算病理学领域因基础模型的最新进展而变革，这些模型通过自监督学习将组织病理学感兴趣区域编码为通用和可转移的特征表示。</li>
<li>将这些进步应用于患者和切片级别的临床挑战时仍面临限制，特别是缺乏特定疾病的临床数据和罕见临床条件下的数据。</li>
<li>提出了TITAN多模态全切片基础模型，利用大量的全切片图像、病理报告和合成字幕进行预训练。</li>
<li>TITAN模型无需微调或临床标签，能在多种临床任务中表现优异，包括罕见疾病检索、癌症预后预测等。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-09b704fe2ed5f189a2b9657caebba907.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b27bef05fc8ad7bbe9b913695d43c90f.jpg" align="middle">
</details>




<h2 id="Constructing-Multimodal-Datasets-from-Scratch-for-Rapid-Development-of-a-Japanese-Visual-Language-Model"><a href="#Constructing-Multimodal-Datasets-from-Scratch-for-Rapid-Development-of-a-Japanese-Visual-Language-Model" class="headerlink" title="Constructing Multimodal Datasets from Scratch for Rapid Development of a   Japanese Visual Language Model"></a>Constructing Multimodal Datasets from Scratch for Rapid Development of a   Japanese Visual Language Model</h2><p><strong>Authors:Keito Sasagawa, Koki Maeda, Issa Sugiura, Shuhei Kurita, Naoaki Okazaki, Daisuke Kawahara</strong></p>
<p>To develop high-performing Visual Language Models (VLMs), it is essential to prepare multimodal resources, such as image-text pairs, interleaved data, and instruction data. While multimodal resources for English are abundant, there is a significant lack of corresponding resources for non-English languages, such as Japanese. To address this problem, we take Japanese as a non-English language and propose a method for rapidly creating Japanese multimodal datasets from scratch. We collect Japanese image-text pairs and interleaved data from web archives and generate Japanese instruction data directly from images using an existing VLM. Our experimental results show that a VLM trained on these native datasets outperforms those relying on machine-translated content. </p>
<blockquote>
<p>为了开发高性能的视觉语言模型（VLMs），准备多模态资源至关重要，例如图像文本对、交错数据和指令数据。虽然英语的多模态资源非常丰富，但非英语语言（如日语）的相应资源却严重匮乏。为了解决这一问题，我们以日语为非英语语言为例，提出了一种从零开始快速创建日语多模态数据集的方法。我们从网络档案中收集日语图像文本对和交错数据，并使用现有的VLM直接从图像生成日语指令数据。我们的实验结果表明，在这些本地数据集上训练的VLM的性能优于那些依赖于机器翻译内容的模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22736v1">PDF</a> 15 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了为开发高性能视觉语言模型（VLMs），准备多模态资源如图像文本对、交错数据等的重要性。由于非英语语言的相应资源严重匮乏，例如日语，该研究提出一种从零开始快速创建日语多模态数据集的方法。通过从网络档案收集日语图像文本对和交错数据，并利用现有VLM直接从图像生成日语指令数据，实验结果显示在本地数据集上训练的VLM表现优于依赖机器翻译内容的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态资源对开发高性能视觉语言模型至关重要，包括图像文本对、交错数据等。</li>
<li>非英语语言的相应资源如日语等存在严重匮乏的问题。</li>
<li>研究提出了一种快速创建日语多模态数据集的方法。</li>
<li>通过从网络档案收集日语图像文本对和交错数据。</li>
<li>利用现有视觉语言模型直接从图像生成日语指令数据。</li>
<li>在本地数据集上训练的视觉语言模型表现优于依赖机器翻译内容的模型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8f82472c0a0686f0e8081734d173c5a9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-782c67fb2a82e0aedece2da117b69002.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a2df80fb6af1a6b14d220af852804929.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8bb935fac506969cba358917ccb2bf96.jpg" align="middle">
</details>




<h2 id="OSCAR-Operating-System-Control-via-State-Aware-Reasoning-and-Re-Planning"><a href="#OSCAR-Operating-System-Control-via-State-Aware-Reasoning-and-Re-Planning" class="headerlink" title="OSCAR: Operating System Control via State-Aware Reasoning and   Re-Planning"></a>OSCAR: Operating System Control via State-Aware Reasoning and   Re-Planning</h2><p><strong>Authors:Xiaoqiang Wang, Bang Liu</strong></p>
<p>Large language models (LLMs) and large multimodal models (LMMs) have shown great potential in automating complex tasks like web browsing and gaming. However, their ability to generalize across diverse applications remains limited, hindering broader utility. To address this challenge, we present OSCAR: Operating System Control via state-Aware reasoning and Re-planning. OSCAR is a generalist agent designed to autonomously navigate and interact with various desktop and mobile applications through standardized controls, such as mouse and keyboard inputs, while processing screen images to fulfill user commands. OSCAR translates human instructions into executable Python code, enabling precise control over graphical user interfaces (GUIs). To enhance stability and adaptability, OSCAR operates as a state machine, equipped with error-handling mechanisms and dynamic task re-planning, allowing it to efficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR’s effectiveness through extensive experiments on diverse benchmarks across desktop and mobile platforms, where it transforms complex workflows into simple natural language commands, significantly boosting user productivity. Our code will be open-source upon publication. </p>
<blockquote>
<p>大型语言模型（LLM）和多模态大模型（LMM）在自动化浏览网页和玩游戏等复杂任务方面显示出巨大潜力。然而，它们在跨不同应用程序方面的泛化能力仍然有限，阻碍了其更广泛的应用。为了应对这一挑战，我们推出了OSCAR：基于状态感知推理和重新规划的操作系统控制。OSCAR是一个通用智能体，旨在通过标准化的控制（如鼠标和键盘输入）自主浏览和与各种桌面和移动应用程序进行交互，同时处理屏幕图像以执行用户命令。OSCAR将人类指令翻译成可执行的Python代码，从而实现对图形用户界面（GUI）的精确控制。为了提高稳定性和适应性，OSCAR作为一个状态机运行，配备了错误处理机制和动态任务重新规划，使其能够高效地适应实时反馈和异常情况。我们通过桌面和移动平台上各种基准测试的广泛实验证明了OSCAR的有效性，它可以将复杂的工作流程转化为简单的自然语言命令，从而极大地提高了用户的工作效率。我们的代码将在发表时开源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18963v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>OSCAR是一个通用代理，可通过标准化控制自主导航和与各种桌面和移动应用程序进行交互，如鼠标和键盘输入，同时处理屏幕图像以执行用户命令。它能够将人类指令翻译成可执行的Python代码，实现对图形用户界面（GUI）的精确控制。OSCAR采用状态机工作方式，具有错误处理机制和动态任务规划，可高效适应实时反馈和异常。在桌面和移动平台的多样化基准测试中，OSCAR通过自然语言命令执行复杂的工作流程，显著提高用户生产率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OSCAR是LLM和LMM领域的进步，可以自动化执行复杂的任务如网页浏览和游戏。</li>
<li>OSCAR设计用于自主导航和与各种应用程序交互。</li>
<li>OSCAR通过标准化控制（如鼠标和键盘）以及处理屏幕图像来执行用户命令。</li>
<li>OSCAR能将人类指令翻译成Python代码，实现GUI的精确控制。</li>
<li>OSCAR采用状态机工作方式，具有稳定性和适应性。</li>
<li>OSCAR具有错误处理机制和动态任务规划功能。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-12d9883a143c335bf96a660999838ed7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-48451e82ae24963df895b403b137806d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-61c4d3cb6fc0f9b2922a6199c1cbd534.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6dbceca50a30dff23e5e3bc527401d4e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-4a7f750eb0b5dad1cb03804c5c1c0df3.jpg" align="middle">
</details>




<h2 id="Pangea-A-Fully-Open-Multilingual-Multimodal-LLM-for-39-Languages"><a href="#Pangea-A-Fully-Open-Multilingual-Multimodal-LLM-for-39-Languages" class="headerlink" title="Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages"></a>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</h2><p><strong>Authors:Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig</strong></p>
<p>Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world’s languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models’ capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum. </p>
<blockquote>
<p>尽管最近多模态大型语言模型（MLLMs）取得了进展，但它们的开发主要集中在英语和西方为中心的数据集和任务上，导致世界上大多数语言和多样化的文化背景代表性不足。本文介绍了Pangea，这是一个在PangeaIns上训练的多语言多模态大型语言模型。PangeaIns是一个包含39种语言的多样化600万指令数据集。PangeaIns的特点包括：1）高质量的英语指令，2）精心机器翻译的指令，以及3）文化相关的多模态任务，以确保跨文化覆盖。为了严格评估模型的能力，我们推出了PangeaBench，这是一个包含14个数据集、覆盖47种语言的全面评估套件。结果表明，在多种语言和多样化的文化背景环境中，Pangea显著优于现有开源模型。消融研究进一步揭示了英语数据比例、语言流行度和多模态训练样本数量对整体性能的重要性。我们完全开源我们的数据、代码和训练检查点，以促进包容性和稳健的多语言MLLMs的发展，推动更广泛的语言和文化领域的公平和可访问性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16153v2">PDF</a> 54 pages, 27 figures</p>
<p><strong>Summary</strong></p>
<p>一篇论文介绍了Pangea，这是一个基于PangeaIns数据集的多语言多模态大型语言模型。该数据集包含高质量的英文指令、经过机器翻译后的指令以及跨文化相关的多模态任务，覆盖39种语言。论文还介绍了用于评估模型能力的PangeaBench评估套件。研究表明，Pangea在多语言环境和不同文化背景下显著优于现有开源模型。论文公开了数据集、代码和训练检查点，以促进开发包容性强、稳健的多语言大型语言模型，推动更广泛的语言和文化领域的公平性和可访问性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pangea是一个多语言多模态的大型语言模型，旨在解决现有模型主要集中在英语和西方中心数据集和任务上的问题，涵盖39种语言。</li>
<li>PangeaIns数据集包含高质量的英文指令、机器翻译后的指令以及跨文化相关的多模态任务，确保跨文化的覆盖。</li>
<li>论文介绍了PangeaBench评估套件，用于全面评估模型能力，涵盖47种语言。</li>
<li>Pangea在多语言环境和不同文化背景下的性能显著优于现有开源模型。</li>
<li>论文进行的消减研究揭示了英语数据比例、语言流行度和多模态训练样本数量对模型性能的整体影响。</li>
<li>论文公开了数据集、代码和训练检查点，以促进开发更包容、更稳健的多语言大型语言模型。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-8d80f58a4f2dff18cf8e85db0ba9764c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-63991c526f72b5f650ec81db11fd467b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-37e971162bbd1fe73765edde3c2ade4b.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-aebee328d6897770d7c5ee7696cc36fc.jpg" align="middle">
</details>




<h2 id="SwaQuAD-24-QA-Benchmark-Dataset-in-Swahili"><a href="#SwaQuAD-24-QA-Benchmark-Dataset-in-Swahili" class="headerlink" title="SwaQuAD-24: QA Benchmark Dataset in Swahili"></a>SwaQuAD-24: QA Benchmark Dataset in Swahili</h2><p><strong>Authors:Alfred Malengo Kondoro</strong></p>
<p>This paper proposes the creation of a Swahili Question Answering (QA) benchmark dataset, aimed at addressing the underrepresentation of Swahili in natural language processing (NLP). Drawing from established benchmarks like SQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing high-quality, annotated question-answer pairs that capture the linguistic diversity and complexity of Swahili. The dataset is designed to support a variety of applications, including machine translation, information retrieval, and social services like healthcare chatbots. Ethical considerations, such as data privacy, bias mitigation, and inclusivity, are central to the dataset development. Additionally, the paper outlines future expansion plans to include domain-specific content, multimodal integration, and broader crowdsourcing efforts. The Swahili QA dataset aims to foster technological innovation in East Africa and provide an essential resource for NLP research and applications in low-resource languages. </p>
<blockquote>
<p>本文提议创建一个斯瓦希里语问答（QA）基准数据集，旨在解决自然语言处理（NLP）中斯瓦希里语的代表性不足问题。该数据集借鉴了SQuAD、GLUE、KenSwQuAD和KLUE等现有的基准测试，将重点放在提供高质量、注释过的问题答案对上，这些对捕捉斯瓦希里语的语言多样性和复杂性至关重要。该数据集旨在支持多种应用，包括机器翻译、信息检索以及医疗保健聊天机器人等社会服务。在数据集开发过程中，道德考量（如数据隐私、偏见缓解和包容性）占据核心地位。此外，本文还概述了未来扩展计划，包括包含特定领域的内容、多模式集成和更广泛的众包工作。斯瓦希里语QA数据集旨在促进东非的技术创新，并为低资源语言中的NLP研究与应用提供重要资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14289v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>针对斯瓦希里语在自然语言处理（NLP）中的代表性不足问题，本文提出了创建斯瓦希里语问答（QA）基准数据集的建议。该数据集致力于提供高质量的注释问答对，体现斯瓦希里语的语言多样性和复杂性。该数据集的设计旨在支持多种应用，包括机器翻译、信息检索和社会服务（如医疗聊天机器人）。伦理考量，如数据隐私、偏见缓解和包容性，是数据集开发的核心。此外，本文还介绍了未来扩展计划，包括纳入领域特定内容、多模式集成和更广泛的众包努力。斯瓦希里语QA数据集旨在促进东非的技术创新，并为低资源语言中的NLP研究与应用提供重要资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出创建斯瓦希里语问答（QA）基准数据集，以解决NLP中斯瓦希里语的代表性不足问题。</li>
<li>数据集将借鉴现有基准测试，如SQuAD、GLUE、KenSwQuAD和KLUE，并提供高质量的注释问答对。</li>
<li>数据集设计支持多种应用，包括机器翻译、信息检索和社会服务。</li>
<li>伦理考量（如数据隐私、偏见缓解和包容性）是数据集开发的核心。</li>
<li>计划未来纳入领域特定内容、多模式集成和众包努力以扩展数据集。</li>
<li>斯瓦希里语QA数据集旨在促进东非的技术创新。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-1b95d2649b285287fca22ee08275fa11.jpg" align="middle">
</details>




<h2 id="Plug-Play-and-Fuse-Zero-Shot-Joint-Decoding-via-Word-Level-Re-ranking-Across-Diverse-Vocabularies"><a href="#Plug-Play-and-Fuse-Zero-Shot-Joint-Decoding-via-Word-Level-Re-ranking-Across-Diverse-Vocabularies" class="headerlink" title="Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking   Across Diverse Vocabularies"></a>Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking   Across Diverse Vocabularies</h2><p><strong>Authors:Sai Koneru, Matthias Huck, Miriam Exel, Jan Niehues</strong></p>
<p>Recent advancements in NLP have resulted in models with specialized strengths, such as processing multimodal inputs or excelling in specific domains. However, real-world tasks, like multimodal translation, often require a combination of these strengths, such as handling both translation and image processing. While individual translation and vision models are powerful, they typically lack the ability to perform both tasks in a single system. Combining these models poses challenges, particularly due to differences in their vocabularies, which limit the effectiveness of traditional ensemble methods to post-generation techniques like N-best list re-ranking. In this work, we propose a novel zero-shot ensembling strategy that allows for the integration of different models during the decoding phase without the need for additional training. Our approach re-ranks beams during decoding by combining scores at the word level, using heuristics to predict when a word is completed. We demonstrate the effectiveness of this method in machine translation scenarios, showing that it enables the generation of translations that are both speech- and image-aware while also improving overall translation quality (We will release the code upon paper acceptance.). </p>
<blockquote>
<p>近年来，自然语言处理领域的进步导致了具有专门优势的模型的出现，如处理多模式输入或在特定领域表现卓越。然而，现实世界中的任务，如多模式翻译，通常需要这些优势的组合，如同时处理翻译和图像处理。虽然单独的翻译和视觉模型功能强大，但它们通常缺乏在一个系统中执行这两个任务的能力。将这些模型组合起来带来了挑战，尤其是由于它们词汇表的差异，这限制了传统集成方法采用后生成技术的有效性，如N-best列表重排。在这项工作中，我们提出了一种新型的零样本集成策略，允许在解码阶段集成不同的模型，无需额外的训练。我们的方法通过在单词层面结合分数对光束进行重新排名，并使用启发式方法来预测单词何时完成。我们证明了该方法在机器翻译场景中的有效性，展示它能够在生成翻译时兼顾语音和图像感知能力，同时提高整体翻译质量。（论文被接受后我们将发布代码。）</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11327v2">PDF</a> WMT 2024</p>
<p><strong>Summary</strong></p>
<p>近期自然语言处理（NLP）的进展带来了具有专业优势的模型，如在处理多模式输入或在特定领域表现卓越。然而，如多模态翻译等现实任务需要同时处理翻译和图像处理等多种任务，单一模型难以满足。结合这些模型面临挑战，特别是词汇差异问题，传统集成方法如N-best列表重排等方法在后期生成技术中效果不佳。本研究提出了一种新型零样本集成策略，可在解码阶段集成不同模型，无需额外训练。该方法通过结合词汇级分数进行解码阶段的重新排名，并利用启发式方法来预测词汇完成度。研究表明，此方法在机器翻译场景中能有效生成既兼顾语音又考虑图像的翻译，并提高了整体翻译质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NLP领域的最新进展带来了具有专业优势的模型，这些模型在特定任务上表现卓越。</li>
<li>现实任务如多模态翻译需要同时处理多种任务，单一模型难以满足这些复杂需求。</li>
<li>结合多个模型面临挑战，特别是词汇差异问题。</li>
<li>传统集成方法在后期生成技术中效果不佳。</li>
<li>本研究提出了一种新型的零样本集成策略，可以在解码阶段集成不同的模型。</li>
<li>该策略通过结合词汇级分数进行解码阶段的重新排名，以提高翻译质量。</li>
<li>研究表明，此方法能生成既考虑语音又考虑图像的翻译，提高了整体翻译质量。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-043f625f48edc359aaf8d372776e8907.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-63b51490b75adc99918bc5dc29fd3d40.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-46b6799669c236763c37edee98e28c1c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-778b06fc9cde11330bd308106a36d2e1.jpg" align="middle">
</details>




<h2 id="MTVQA-Benchmarking-Multilingual-Text-Centric-Visual-Question-Answering"><a href="#MTVQA-Benchmarking-Multilingual-Text-Centric-Visual-Question-Answering" class="headerlink" title="MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering"></a>MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering</h2><p><strong>Authors:Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, Can Huang</strong></p>
<p>Text-Centric Visual Question Answering (TEC-VQA) in its proper format not only facilitates human-machine interaction in text-centric visual environments but also serves as a de facto gold proxy to evaluate AI models in the domain of text-centric scene understanding. Nonetheless, most existing TEC-VQA benchmarks have focused on high-resource languages like English and Chinese. Despite pioneering works to expand multilingual QA pairs in non-text-centric VQA datasets through translation engines, the translation-based protocol encounters a substantial “visual-textual misalignment” problem when applied to TEC-VQA. Specifically, it prioritizes the text in question-answer pairs while disregarding the visual text present in images. Moreover, it fails to address complexities related to nuanced meaning, contextual distortion, language bias, and question-type diversity. In this work, we tackle multilingual TEC-VQA by introducing MTVQA, the first benchmark featuring high-quality human expert annotations across 9 diverse languages, consisting of 6,778 question-answer pairs across 2,116 images. Further, by comprehensively evaluating numerous state-of-the-art Multimodal Large Language Models~(MLLMs), including Qwen2-VL, GPT-4o, GPT-4V, Claude3, and Gemini, on the MTVQA benchmark, it is evident that there is still a large room for performance improvement (Qwen2-VL scoring 30.9 versus 79.7 for human performance), underscoring the value of MTVQA. Additionally, we supply multilingual training data within the MTVQA dataset, demonstrating that straightforward fine-tuning with this data can substantially enhance multilingual TEC-VQA performance. We aspire that MTVQA will offer the research community fresh insights and stimulate further exploration in multilingual visual text comprehension. The project homepage is available at <a target="_blank" rel="noopener" href="https://bytedance.github.io/MTVQA/">https://bytedance.github.io/MTVQA/</a>. </p>
<blockquote>
<p>文本中心化的视觉问答（TEC-VQA）在适当的形式下，不仅促进了文本中心化视觉环境中的人机交互，还作为评估文本中心化场景理解领域中AI模型的实际黄金标准。然而，现有的TEC-VQA基准测试主要集中在英语和中文等资源丰富的语言上。尽管有开创性的工作通过翻译引擎在非文本中心化的VQA数据集中扩展了多种语言的问答对，但基于翻译的方法在应用于TEC-VQA时遇到了重大的“视觉文本不对齐”问题。具体来说，它优先考虑问答对中的文本，而忽视图像中的视觉文本。此外，它无法处理与细微意义、上下文失真、语言偏见和问题类型多样性相关的复杂性。在这项工作中，我们通过引入MTVQA来解决多语言TEC-VQA问题，MTVQA是第一个在9种不同语言上具有高质量人类专家注释的基准测试，包含2,116张图片中的6,778个问答对。此外，通过全面评估众多最先进的多媒体大型语言模型（MLLMs），包括Qwen2-VL、GPT-4o、GPT-4V、Claude3和Gemini在MTVQA基准上的表现，显然还有很大的性能提升空间（Qwen2-VL得分为30.9，而人类表现得分为79.7），这凸显了MTVQA的价值。另外，我们在MTVQA数据集中提供了多语言训练数据，证明使用此数据进行简单微调可以显著增强多语言TEC-VQA的性能。我们期望MTVQA能为研究界提供新的见解，并激发多语言视觉文本理解方面的进一步探索。项目主页可在[<a target="_blank" rel="noopener" href="https://bytedance.github.io/MTVQA/]%E8%AE%BF%E9%97%AE%E3%80%82">https://bytedance.github.io/MTVQA/]访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.11985v3">PDF</a> </p>
<p><strong>摘要</strong><br>文本中心的视觉问答（TEC-VQA）不仅有利于文本为中心的视觉环境中的人机交互，而且是评估文本中心场景理解领域AI模型的黄金标准。然而，现有的TEC-VQA基准测试主要集中在英语和中文等资源丰富的语言上。尽管有先驱工作通过翻译引擎扩展了非文本中心VQA数据集的多语言问答对，但翻译协议应用于TEC-VQA时遇到了“视觉文本不对齐”的问题。它优先考虑问答对中的文本，而忽视图像中的视觉文本。此外，它未能解决与微妙意义、上下文扭曲、语言偏见和问题类型多样性相关的复杂性。在这项工作中，我们通过引入MTVQA来解决多语言TEC-VQA问题，MTVQA是第一个在9种不同语言中具有高质量人类专家注释的基准测试，包含6778个问答对和2116张图像。此外，通过对多种最先进的多媒体大型语言模型（MLLMs）进行全面评估，包括Qwen2-VL、GPT-4o、GPT-4V、Claude3和Gemini在MTVQA基准上的评估表现仍然有很大的提升空间（Qwen2-VL得分为30.9分，人类表现得分为79.7分），这凸显了MTVQA的价值。此外，我们还在MTVQA数据集中提供了多语言训练数据，证明使用此数据进行微调可以大大提高多语言TEC-VQA的性能。我们希望MTVQA能为研究社区提供新的见解，并激发多语言视觉文本理解领域的进一步探索。项目主页位于<a target="_blank" rel="noopener" href="https://bytedance.github.io/MTVQA/%E3%80%82">https://bytedance.github.io/MTVQA/。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本中心的视觉问答（TEC-VQA）对于评估AI模型在文本中心场景理解领域的性能具有重要意义。</li>
<li>现有的TEC-VQA基准测试主要集中在高资源语言上，如英语和中文。</li>
<li>翻译协议应用于TEC-VQA时存在“视觉文本不对齐”的问题。</li>
<li>MTVQA是第一个包含9种不同语言高质量人类专家注释的基准测试，具有多种语言问答对和图像数据。</li>
<li>当前最先进的多媒体大型语言模型在MTVQA基准测试上的表现仍有待提升。</li>
<li>MTVQA提供了多语言训练数据，证明通过微调可以提高多语言TEC-VQA的性能。</li>
<li>MTVQA为研究领域提供了宝贵资源，有望激发多语言视觉文本理解领域的进一步探索。</li>
</ol>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-7f74895ab773ee4d6feed9acaabd014e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-b36490fe0242aedca726bde2480ceee0.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7981bb4f2eb8cbbb37f8735a45b271f2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-dfdbe37bb8b147833c58e3a09cb5851c.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2d37f8587b59629d61a6a3bf11687340.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6369311c3376d7e6162b7041df3bbc52.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/MMT/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/MMT/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/MMT/">
                                    <span class="chip bg-color">MMT</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-102376073a70ff046fee9027769501d3.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2024-12-10  JAPAGEN Efficient Few/Zero-shot Learning via Japanese Training Dataset   Generation with LLM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-39029a31cbacc01d5860058af630a47f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2024-12-10  Training Large Language Models to Reason in a Continuous Latent Space
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">7652.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
