<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Long Video Understanding via Fine-detailed Video Story   Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-21e5621df29b631d04b41f2f4008ecb3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    29.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    118 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Towards-Long-Video-Understanding-via-Fine-detailed-Video-Story-Generation"><a href="#Towards-Long-Video-Understanding-via-Fine-detailed-Video-Story-Generation" class="headerlink" title="Towards Long Video Understanding via Fine-detailed Video Story   Generation"></a>Towards Long Video Understanding via Fine-detailed Video Story   Generation</h2><p><strong>Authors:Zeng You, Zhiquan Wen, Yaofo Chen, Xin Li, Runhao Zeng, Yaowei Wang, Mingkui Tan</strong></p>
<p>Long video understanding has become a critical task in computer vision, driving advancements across numerous applications from surveillance to content retrieval. Existing video understanding methods suffer from two challenges when dealing with long video understanding: intricate long-context relationship modeling and interference from redundancy. To tackle these challenges, we introduce Fine-Detailed Video Story generation (FDVS), which interprets long videos into detailed textual representations. Specifically, to achieve fine-grained modeling of long-temporal content, we propose a Bottom-up Video Interpretation Mechanism that progressively interprets video content from clips to video. To avoid interference from redundant information in videos, we introduce a Semantic Redundancy Reduction mechanism that removes redundancy at both the visual and textual levels. Our method transforms long videos into hierarchical textual representations that contain multi-granularity information of the video. With these representations, FDVS is applicable to various tasks without any fine-tuning. We evaluate the proposed method across eight datasets spanning three tasks. The performance demonstrates the effectiveness and versatility of our method. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£å·²æˆä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œæ¨åŠ¨äº†ä»ç›‘æ§åˆ°å†…å®¹æ£€ç´¢ç­‰å¤šä¸ªåº”ç”¨çš„å‘å±•ã€‚ç°æœ‰çš„è§†é¢‘ç†è§£æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘ç†è§£æ—¶é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šå¤æ‚çš„é•¿ä¸Šä¸‹æ–‡å…³ç³»å»ºæ¨¡å’Œå†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç²¾ç»†è¯¦ç»†è§†é¢‘æ•…äº‹ç”Ÿæˆï¼ˆFDVSï¼‰æ–¹æ³•ï¼Œå®ƒå°†é•¿è§†é¢‘è§£é‡Šä¸ºè¯¦ç»†çš„æ–‡æœ¬è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å®ç°é•¿æ—¶å†…å®¹çš„ç²¾ç»†å»ºæ¨¡ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªä¸‹è€Œä¸Šçš„è§†é¢‘è§£é‡Šæœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€æ­¥ä»ç‰‡æ®µåˆ°è§†é¢‘è§£é‡Šè§†é¢‘å†…å®¹ã€‚ä¸ºäº†é¿å…è§†é¢‘ä¸­çš„å†—ä½™ä¿¡æ¯é€ æˆçš„å¹²æ‰°ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¯­ä¹‰å†—ä½™å‡å°‘æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨è§†è§‰å’Œæ–‡æœ¬å±‚é¢æ¶ˆé™¤äº†å†—ä½™ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†é•¿è§†é¢‘è½¬æ¢ä¸ºå±‚æ¬¡åŒ–çš„æ–‡æœ¬è¡¨ç¤ºå½¢å¼ï¼ŒåŒ…å«è§†é¢‘çš„å¤šç²’åº¦ä¿¡æ¯ã€‚åˆ©ç”¨è¿™äº›è¡¨ç¤ºå½¢å¼ï¼ŒFDVSå¯å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œæ— éœ€è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»»åŠ¡çš„å…«ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ã€‚æ€§èƒ½ç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06182v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é•¿è§†é¢‘ç†è§£å·²æˆä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ¨åŠ¨äº†ä»ç›‘æ§åˆ°å†…å®¹æ£€ç´¢ç­‰å¤šä¸ªåº”ç”¨çš„å‘å±•ã€‚é’ˆå¯¹é•¿è§†é¢‘ç†è§£ï¼Œç°æœ‰æ–¹æ³•é¢ä¸´å¤æ‚çš„é•¿ä¸Šä¸‹æ–‡å…³ç³»å»ºæ¨¡å’Œå†—ä½™ä¿¡æ¯çš„å¹²æ‰°ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç²¾ç»†è¯¦ç»†è§†é¢‘æ•…äº‹ç”Ÿæˆï¼ˆFDVSï¼‰æ–¹æ³•ï¼Œå°†é•¿è§†é¢‘è½¬åŒ–ä¸ºè¯¦ç»†çš„æ–‡æœ¬è¡¨ç¤ºã€‚é€šè¿‡è‡ªä¸‹è€Œä¸Šçš„è§†é¢‘è§£é‡Šæœºåˆ¶å’Œè¯­ä¹‰å†—ä½™å‡å°‘æœºåˆ¶ï¼Œå®ç°äº†å¯¹é•¿è§†é¢‘å†…å®¹çš„ç²¾ç»†é¢—ç²’åº¦å»ºæ¨¡å’Œå†—ä½™ä¿¡æ¯çš„é¿å…ã€‚è¯¥æ–¹æ³•å°†é•¿è§†é¢‘è½¬åŒ–ä¸ºå±‚æ¬¡åŒ–çš„æ–‡æœ¬è¡¨ç¤ºï¼ŒåŒ…å«è§†é¢‘çš„å¤šç²’åº¦ä¿¡æ¯ï¼Œå¹¶é€‚ç”¨äºå„ç§ä»»åŠ¡è€Œæ— éœ€å¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é•¿è§†é¢‘ç†è§£åœ¨è®¡ç®—æœºè§†è§‰ä¸­è‡³å…³é‡è¦ï¼Œæ¨åŠ¨äº†å¤šç§åº”ç”¨çš„å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é•¿è§†é¢‘ç†è§£ä¸­é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¤æ‚çš„é•¿ä¸Šä¸‹æ–‡å…³ç³»å»ºæ¨¡å’Œå†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚</li>
<li>æå‡ºçš„Fine-Detailed Video Story generationï¼ˆFDVSï¼‰æ–¹æ³•å°†é•¿è§†é¢‘è½¬åŒ–ä¸ºè¯¦ç»†çš„æ–‡æœ¬è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡è‡ªä¸‹è€Œä¸Šçš„è§†é¢‘è§£é‡Šæœºåˆ¶å®ç°é•¿è§†é¢‘å†…å®¹çš„ç²¾ç»†é¢—ç²’åº¦å»ºæ¨¡ã€‚</li>
<li>è¯­ä¹‰å†—ä½™å‡å°‘æœºåˆ¶é¿å…äº†å†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚</li>
<li>è¯¥æ–¹æ³•å°†é•¿è§†é¢‘è½¬åŒ–ä¸ºå±‚æ¬¡åŒ–çš„æ–‡æœ¬è¡¨ç¤ºï¼ŒåŒ…å«è§†é¢‘çš„å¤šç²’åº¦ä¿¡æ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-497bc55568893062d8f99077b5f7cd5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8758f2b167af8e48872e8b70b6998b79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f34aef7266080bbe907dc4bfcb253c0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47b6308c74b1c065442f49b4d984e132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-953e79a1c31dbfc4ca126a8304ceeadc.jpg" align="middle">
</details>




<h2 id="Holmes-VAU-Towards-Long-term-Video-Anomaly-Understanding-at-Any-Granularity"><a href="#Holmes-VAU-Towards-Long-term-Video-Anomaly-Understanding-at-Any-Granularity" class="headerlink" title="Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any   Granularity"></a>Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any   Granularity</h2><p><strong>Authors:Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, Nong Sang</strong></p>
<p>How can we enable models to comprehend video anomalies occurring over varying temporal scales and contexts? Traditional Video Anomaly Understanding (VAU) methods focus on frame-level anomaly prediction, often missing the interpretability of complex and diverse real-world anomalies. Recent multimodal approaches leverage visual and textual data but lack hierarchical annotations that capture both short-term and long-term anomalies. To address this challenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical video anomaly understanding across any granularity. We develop a semi-automated annotation engine that efficiently scales high-quality annotations by combining manual video segmentation with recursive free-text annotation using large language models (LLMs). This results in over 70,000 multi-granular annotations organized at clip-level, event-level, and video-level segments. For efficient anomaly detection in long videos, we propose the Anomaly-focused Temporal Sampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to adaptively select frames based on anomaly scores, ensuring that the multimodal LLM concentrates on anomaly-rich regions, which significantly enhances both efficiency and accuracy. Extensive experiments demonstrate that our hierarchical instruction data markedly improves anomaly comprehension. The integrated ATS and visual-language model outperform traditional methods in processing long videos. Our benchmark and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/pipixin321/HolmesVAU">https://github.com/pipixin321/HolmesVAU</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¦‚ä½•æ‰èƒ½ä½¿æ¨¡å‹ç†è§£å‘ç”Ÿåœ¨ä¸åŒæ—¶é—´å°ºåº¦å’Œä¸Šä¸‹æ–‡ä¸­çš„è§†é¢‘å¼‚å¸¸ï¼Ÿä¼ ç»Ÿçš„è§†é¢‘å¼‚å¸¸ç†è§£ï¼ˆVAUï¼‰æ–¹æ³•ä¸»è¦å…³æ³¨å¸§çº§çš„å¼‚å¸¸é¢„æµ‹ï¼Œå¾€å¾€å¿½ç•¥äº†å¤æ‚å’Œå¤šæ ·åŒ–çš„çœŸå®ä¸–ç•Œå¼‚å¸¸çš„è§£è¯»ã€‚æœ€è¿‘çš„å¤šæ¨¡å¼æ–¹æ³•åˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œä½†ç¼ºä¹èƒ½å¤Ÿæ•æ‰çŸ­æœŸå’Œé•¿æœŸå¼‚å¸¸çš„å±‚æ¬¡åŒ–æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†HIVAU-70kï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºä»»ä½•ç²’åº¦å±‚æ¬¡åŒ–è§†é¢‘å¼‚å¸¸ç†è§£çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŠè‡ªåŠ¨æ³¨é‡Šå¼•æ“ï¼Œé€šè¿‡ç»“åˆæ‰‹åŠ¨è§†é¢‘åˆ†å‰²å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€’å½’æ–‡æœ¬æ³¨é‡Šï¼Œæœ‰æ•ˆåœ°æ‰©å±•äº†é«˜è´¨é‡æ³¨é‡Šçš„è§„æ¨¡ã€‚è¿™äº§ç”Ÿäº†è¶…è¿‡7ä¸‡å¤šä¸ªå¤šç²’åº¦æ³¨é‡Šï¼ŒæŒ‰å‰ªè¾‘çº§åˆ«ã€äº‹ä»¶çº§åˆ«å’Œè§†é¢‘çº§åˆ«åˆ†æ®µç»„ç»‡ã€‚ä¸ºäº†åœ¨é•¿è§†é¢‘ä¸­è¿›è¡Œé«˜æ•ˆçš„å¼‚å¸¸æ£€æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥å¼‚å¸¸ä¸ºé‡ç‚¹çš„ä¸´æ—¶é‡‡æ ·å™¨ï¼ˆATSï¼‰ã€‚ATSå°†å¼‚å¸¸è¯„åˆ†è€…ä¸å¯†åº¦æ„ŸçŸ¥é‡‡æ ·å™¨ç›¸ç»“åˆï¼Œæ ¹æ®å¼‚å¸¸è¯„åˆ†è‡ªé€‚åº”åœ°é€‰æ‹©å¸§ï¼Œç¡®ä¿å¤šæ¨¡å¼LLMä¸“æ³¨äºå¼‚å¸¸ä¸°å¯Œçš„åŒºåŸŸï¼Œè¿™æ˜¾è‘—æé«˜äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å±‚æ¬¡åŒ–æŒ‡ä»¤æ•°æ®æ˜¾è‘—æé«˜äº†å¼‚å¸¸ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬é›†æˆçš„ATSå’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹åœ¨<a target="_blank" rel="noopener" href="https://github.com/pipixin321/HolmesVAU%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/pipixin321/HolmesVAUå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06171v1">PDF</a> 21 pages</p>
<p><strong>Summary</strong><br>     ä¸ºåº”å¯¹æ¨¡å‹åœ¨ä¸åŒæ—¶é—´å°ºåº¦å’Œä¸Šä¸‹æ–‡ç¯å¢ƒä¸­ç†è§£è§†é¢‘å¼‚å¸¸çš„é—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†HIVAU-70kå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†å’ŒåŠè‡ªåŠ¨æ ‡æ³¨å¼•æ“ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†é’ˆå¯¹é•¿è§†é¢‘ä¸­å¼‚å¸¸æ£€æµ‹é—®é¢˜çš„Anomaly-focused Temporal Samplerï¼ˆATSï¼‰ã€‚ç ”ç©¶æˆæœåŒ…æ‹¬å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•é›†ã€åŠè‡ªåŠ¨æ ‡æ³¨å¼•æ“å’Œé«˜æ•ˆçš„å¼‚å¸¸æ£€æµ‹é‡‡æ ·æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹çš„å¼‚å¸¸ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥HIVAU-70kåŸºå‡†æµ‹è¯•é›†ï¼šé’ˆå¯¹è§†é¢‘å¼‚å¸¸ç†è§£ï¼ˆVAUï¼‰çš„æŒ‘æˆ˜ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒæ—¶é—´å°ºåº¦å’Œä¸Šä¸‹æ–‡ä¸­çš„å¼‚å¸¸ç†è§£èƒ½åŠ›ã€‚</li>
<li>åŠè‡ªåŠ¨æ ‡æ³¨å¼•æ“ï¼šç»“åˆæ‰‹åŠ¨è§†é¢‘åˆ†å‰²å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é€’å½’è‡ªç”±æ–‡æœ¬æ ‡æ³¨ï¼Œå®ç°äº†é«˜æ•ˆçš„æ ‡æ³¨æ‰©å±•ã€‚</li>
<li>å¤šå±‚æ¬¡æ ‡æ³¨ï¼šæä¾›äº†è¶…è¿‡70,000ä¸ªå¤šå±‚æ¬¡æ ‡æ³¨ï¼ŒåŒ…æ‹¬å‰ªè¾‘çº§åˆ«ã€äº‹ä»¶çº§åˆ«å’Œè§†é¢‘çº§åˆ«åˆ†æ®µã€‚</li>
<li>Anomaly-focused Temporal Samplerï¼ˆATSï¼‰ï¼šæå‡ºäº†ä¸€ç§é’ˆå¯¹é•¿è§†é¢‘ä¸­å¼‚å¸¸æ£€æµ‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¼‚å¸¸è¯„åˆ†è€…å’Œå¯†åº¦æ„ŸçŸ¥é‡‡æ ·å™¨ï¼Œè‡ªé€‚åº”åœ°é€‰æ‹©å…³é”®å¸§è¿›è¡Œé›†ä¸­å¤„ç†ã€‚</li>
<li>æé«˜äº†å¼‚å¸¸ç†è§£èƒ½åŠ›ï¼šé€šè¿‡å¤§é‡å®éªŒè¯æ˜ï¼Œåˆ©ç”¨å±‚æ¬¡ç»“æ„æŒ‡ä»¤æ•°æ®å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„å¼‚å¸¸ç†è§£èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€LLMçš„åº”ç”¨ï¼šç»“åˆå¼‚å¸¸è¯„åˆ†è€…å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæé«˜äº†å¤„ç†é•¿è§†é¢‘çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å…¬å¼€å¯ç”¨ï¼šåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹å·²å…¬å¼€åœ¨GitHubä¸Šä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9123f0e5c4d35c048ac8bb2535406da2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72b8eaba4ae0d69d56ac276e24d2c2dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e6f53e9d21122d3d2c1380ff1ea3de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-450404bc9819b899bd5d7810ef993494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-955558c653f1558773088633c62b1a26.jpg" align="middle">
</details>




<h2 id="LinVT-Empower-Your-Image-level-Large-Language-Model-to-Understand-Videos"><a href="#LinVT-Empower-Your-Image-level-Large-Language-Model-to-Understand-Videos" class="headerlink" title="LinVT: Empower Your Image-level Large Language Model to Understand   Videos"></a>LinVT: Empower Your Image-level Large Language Model to Understand   Videos</h2><p><strong>Authors:Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, Zheng Zhao</strong></p>
<p>Large Language Models (LLMs) have been widely used in various tasks, motivating us to develop an LLM-based assistant for videos. Instead of training from scratch, we propose a module to transform arbitrary well-trained image-based LLMs into video-LLMs (after being trained on video data). To better adapt image-LLMs for processing videos, we introduce two design principles: linear transformation to preserve the original visual-language alignment and representative information condensation from redundant video content. Guided by these principles, we propose a plug-and-play Linear Video Tokenizer(LinVT), which enables existing image-LLMs to understand videos. We benchmark LinVT with six recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL, showcasing the high compatibility of LinVT. LinVT-based LLMs achieve state-of-the-art performance across various video benchmarks, illustrating the effectiveness of LinVT in multi-modal video understanding. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²åœ¨å„ç§ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬å¼€å‘åŸºäºLLMçš„è§†é¢‘åŠ©ç†ã€‚æˆ‘ä»¬ä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè€Œæ˜¯æå‡ºä¸€ä¸ªæ¨¡å—ï¼Œå°†ä»»æ„çš„ã€åŸºäºå›¾åƒçš„è‰¯å¥½è®­ç»ƒçš„LLMsè½¬åŒ–ä¸ºåœ¨è§†é¢‘æ•°æ®è®­ç»ƒåçš„è§†é¢‘LLMsã€‚ä¸ºäº†æ›´å¥½åœ°é€‚åº”å›¾åƒLLMså¤„ç†è§†é¢‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªè®¾è®¡åŸåˆ™ï¼šçº¿æ€§å˜æ¢ä»¥ä¿ç•™åŸå§‹è§†è§‰è¯­è¨€å¯¹é½å’Œä»å†—ä½™è§†é¢‘å†…å®¹ä¸­å‡ç»ƒä»£è¡¨æ€§ä¿¡æ¯ã€‚åœ¨è¿™äº›åŸåˆ™çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†å³æ’å³ç”¨çš„çº¿æ€§è§†é¢‘ä»¤ç‰ŒåŒ–å™¨ï¼ˆLinVTï¼‰ï¼Œä½¿ç°æœ‰çš„å›¾åƒLLMsèƒ½å¤Ÿç†è§£è§†é¢‘ã€‚æˆ‘ä»¬ä½¿ç”¨LinVTå¯¹å…­ç§æœ€æ–°çš„è§†è§‰LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼šAquilaã€Blip-3ã€InternVL2ã€Miphaã€Molmoå’ŒQwen2-VLï¼Œå±•ç¤ºäº†LinVTçš„é«˜å…¼å®¹æ€§ã€‚åŸºäºLinVTçš„LLMsåœ¨å„é¡¹è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†LinVTåœ¨å¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05185v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„è§†é¢‘åŠ©ç†ã€‚ä¸åŒäºä»å¤´å¼€å§‹è®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å—ï¼Œå¯å°†ä»»æ„å·²è®­ç»ƒå¥½çš„å›¾åƒLLMè½¬åŒ–ä¸ºè§†é¢‘LLMï¼ˆç»è¿‡è§†é¢‘æ•°æ®è®­ç»ƒåï¼‰ã€‚ä¸ºäº†æ›´å¥½åœ°é€‚åº”è§†é¢‘å¤„ç†çš„å›¾åƒLLMï¼Œæœ¬æ–‡å¼•å…¥äº†çº¿æ€§å˜æ¢ä»¥ä¿ç•™åŸå§‹è§†è§‰è¯­è¨€å¯¹é½å’Œä»£è¡¨æ€§ä¿¡æ¯å†·å‡çš„è®¾è®¡åŸåˆ™ã€‚åŸºäºæ­¤åŸåˆ™ï¼Œæˆ‘ä»¬æå‡ºäº†å³æ’å³ç”¨çš„çº¿æ€§è§†é¢‘ä»¤ç‰Œå™¨ï¼ˆLinVTï¼‰ï¼Œä½¿ç°æœ‰å›¾åƒLLMèƒ½å¤Ÿç†è§£è§†é¢‘ã€‚æˆ‘ä»¬å¯¹LinVTè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä¸æœ€è¿‘çš„å…­ç§è§†è§‰LLMè¿›è¡Œæ¯”å¯¹ï¼šAquilaã€Blip-3ã€InternVL2ã€Miphaã€Molmoå’ŒQwen2-VLï¼Œè¯æ˜äº†LinVTçš„é«˜åº¦å…¼å®¹æ€§ã€‚åŸºäºLinVTçš„LLMåœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ€§èƒ½ï¼Œè¯æ˜äº†LinVTåœ¨å¤šæ¨¡æ€è§†é¢‘ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¹¿æ³›ç”¨äºå„ç§ä»»åŠ¡ï¼Œæ¿€åŠ±æˆ‘ä»¬å¼€å‘åŸºäºLLMçš„è§†é¢‘åŠ©ç†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ¨¡å—åŒ–çš„æ–¹æ³•ï¼Œå°†å›¾åƒLLMè½¬åŒ–ä¸ºè§†é¢‘LLMï¼Œé€šè¿‡è§†é¢‘æ•°æ®è®­ç»ƒã€‚</li>
<li>å¼•å…¥çº¿æ€§å˜æ¢å’Œä»£è¡¨æ€§ä¿¡æ¯å†·å‡çš„è®¾è®¡åŸåˆ™ï¼Œä½¿å›¾åƒLLMæ›´å¥½åœ°é€‚åº”è§†é¢‘å¤„ç†ã€‚</li>
<li>æå‡ºäº†å³æ’å³ç”¨çš„Linear Video Tokenizerï¼ˆLinVTï¼‰ï¼Œä½¿ç°æœ‰å›¾åƒLLMèƒ½ç†è§£è§†é¢‘ã€‚</li>
<li>LinVTä¸å…­ç§è§†è§‰LLMåŸºå‡†æµ‹è¯•è¡¨ç°å‡ºé«˜å…¼å®¹æ€§ã€‚</li>
<li>åŸºäºLinVTçš„LLMåœ¨å¤šç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbcc4a459a0e6c4fcbc213e1284d8de8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd2a5665dc68e084753c477e1615d57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0f186b1a9be80c23891ff009151837e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ed5620fff1c3e00928878fa6dad48d7.jpg" align="middle">
</details>




<h2 id="VidHalluc-Evaluating-Temporal-Hallucinations-in-Multimodal-Large-Language-Models-for-Video-Understanding"><a href="#VidHalluc-Evaluating-Temporal-Hallucinations-in-Multimodal-Large-Language-Models-for-Video-Understanding" class="headerlink" title="VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large   Language Models for Video Understanding"></a>VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large   Language Models for Video Understanding</h2><p><strong>Authors:Chaoyu Li, Eun Woo Im, Pooyan Fazli</strong></p>
<p>Multimodal large language models (MLLMs) have recently shown significant advancements in video understanding, excelling in content reasoning and instruction-following tasks. However, the problem of hallucination, where models generate inaccurate or misleading content, remains underexplored in the video domain. Building on the observation that the visual encoder of MLLMs often struggles to differentiate between video pairs that are visually distinct but semantically similar, we introduce VidHalluc, the largest benchmark designed to examine hallucinations in MLLMs for video understanding tasks. VidHalluc assesses hallucinations across three critical dimensions: (1) action, (2) temporal sequence, and (3) scene transition. VidHalluc consists of 5,002 videos, paired based on semantic similarity and visual differences, focusing on cases where hallucinations are most likely to occur. Through comprehensive testing, our experiments show that most MLLMs are vulnerable to hallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a training-free method that reduces hallucinations by incorporating spatial saliency information from DINOv2 to reweight visual features during inference. Our results demonstrate that DINO-HEAL consistently improves performance on VidHalluc, achieving an average improvement of 3.02% in mitigating hallucinations among all tasks. Both the VidHalluc benchmark and DINO-HEAL code can be accessed via $\href{<a target="_blank" rel="noopener" href="https://vid-halluc.github.io/%7D%7B/text%7Bthis">https://vid-halluc.github.io/}{\text{this</a> link}}$. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢æœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨å†…å®¹æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæ¨¡å‹ç”Ÿæˆä¸å‡†ç¡®æˆ–è¯¯å¯¼æ€§å†…å®¹çš„é—®é¢˜â€”â€”å³å¹»è§‰é—®é¢˜ï¼Œåœ¨è§†é¢‘é¢†åŸŸä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°MLLMçš„è§†è§‰ç¼–ç å™¨åœ¨åŒºåˆ†è§†è§‰ä¸Šä¸åŒä½†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„è§†é¢‘å¯¹æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œäºæ˜¯æˆ‘ä»¬å¼•å…¥äº†VidHallucï¼Œè¿™æ˜¯ä¸ºè§†é¢‘ç†è§£ä»»åŠ¡ä¸­MLLMçš„å¹»è§‰é—®é¢˜è®¾è®¡çš„æœ€å¤§åŸºå‡†æµ‹è¯•ã€‚VidHallucè¯„ä¼°å¹»è§‰çš„ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šï¼ˆ1ï¼‰åŠ¨ä½œã€ï¼ˆ2ï¼‰æ—¶é—´åºåˆ—ã€ï¼ˆ3ï¼‰åœºæ™¯è¿‡æ¸¡ã€‚VidHallucåŒ…å«5002ä¸ªè§†é¢‘ï¼Œè¿™äº›è§†é¢‘åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§å’Œè§†è§‰å·®å¼‚è¿›è¡Œé…å¯¹ï¼Œé‡ç‚¹å…³æ³¨å¹»è§‰æœ€å¯èƒ½å‘ç”Ÿçš„æƒ…å†µã€‚é€šè¿‡å…¨é¢çš„æµ‹è¯•ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå¤§å¤šæ•°MLLMåœ¨è¿™äº›ç»´åº¦ä¸Šéƒ½å®¹æ˜“å—åˆ°å¹»è§‰çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†DINO-HEALï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆDINOv2çš„ç©ºé—´æ˜¾è‘—æ€§ä¿¡æ¯æ¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡æ–°åŠ æƒè§†è§‰ç‰¹å¾ï¼Œä»è€Œå‡å°‘å¹»è§‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDINO-HEALåœ¨VidHallucä¸ŠæŒç»­æé«˜äº†æ€§èƒ½ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å‡è½»å¹»è§‰çš„å¹³å‡æ”¹è¿›ç‡ä¸º3.02%ã€‚å¯ä»¥é€šè¿‡æ­¤é“¾æ¥è®¿é—®VidHallucåŸºå‡†æµ‹è¯•å’ŒDINO-HEALä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…å®¹æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ã€‚ç„¶è€Œï¼Œæ¨¡å‹ç”Ÿæˆä¸å‡†ç¡®æˆ–è¯¯å¯¼æ€§å†…å®¹çš„é—®é¢˜â€”â€”å³å¹»è§†ç°è±¡ï¼Œåœ¨è§†é¢‘é¢†åŸŸä»è¢«å¿½è§†ã€‚é’ˆå¯¹MLLMsçš„è§†è§‰ç¼–ç å™¨éš¾ä»¥åŒºåˆ†è§†è§‰ä¸Šæœ‰å·®å¼‚ä½†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„è§†é¢‘å¯¹çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†VidHallucåŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯ä¸“ä¸ºè§†é¢‘ç†è§£ä»»åŠ¡ä¸­æ£€æŸ¥MLLMsçš„å¹»è§†ç°è±¡è€Œè®¾è®¡çš„å¤§å‹åŸºå‡†æµ‹è¯•ã€‚VidHallucä»åŠ¨ä½œã€æ—¶é—´é¡ºåºå’Œåœºæ™¯è¿‡æ¸¡ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°å¹»è§†ç°è±¡ã€‚å®ƒåŒ…å«5002ä¸ªè§†é¢‘ï¼ŒåŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§å’Œè§†è§‰å·®å¼‚è¿›è¡Œé…å¯¹ï¼Œé‡ç‚¹å…³æ³¨æœ€å¯èƒ½å‘ç”Ÿå¹»è§†çš„æƒ…å†µã€‚å®éªŒè¡¨æ˜ï¼Œå¤§å¤šæ•°MLLMsåœ¨è¿™äº›ç»´åº¦ä¸Šéƒ½å®¹æ˜“å—åˆ°å¹»è§†çš„å½±å“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†DINO-HEALæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å‡å°‘å¹»è§†çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆDINOv2çš„ç©ºé—´æ˜¾è‘—æ€§ä¿¡æ¯åœ¨æ¨ç†æ—¶é‡æ–°åŠ æƒè§†è§‰ç‰¹å¾ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDINO-HEALåœ¨VidHallucä¸Šè¡¨ç°ä¸€è‡´ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†3.02%çš„å¹»è§†å‡è½»æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£é¢†åŸŸæœ‰æ˜¾è‘—çš„è¿›å±•ï¼Œå°¤å…¶åœ¨å†…å®¹æ¨ç†å’ŒæŒ‡ä»¤éµå¾ªä»»åŠ¡æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>å¹»è§†ç°è±¡åœ¨è§†é¢‘é¢†åŸŸçš„MLLMsä¸­ä»ç„¶æ˜¯ä¸€ä¸ªè¢«å¿½è§†çš„é—®é¢˜ï¼Œè¿™æŒ‡çš„æ˜¯æ¨¡å‹ç”Ÿæˆä¸å‡†ç¡®æˆ–è¯¯å¯¼æ€§çš„å†…å®¹ã€‚</li>
<li>VidHallucåŸºå‡†æµ‹è¯•æ˜¯ä¸ºäº†æ£€æŸ¥è§†é¢‘ç†è§£ä»»åŠ¡ä¸­MLLMsçš„å¹»è§†ç°è±¡è€Œè®¾è®¡çš„å¤§å‹åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«5002ä¸ªè§†é¢‘ï¼Œä¸“æ³¨äºè¯„ä¼°åŠ¨ä½œã€æ—¶é—´é¡ºåºå’Œåœºæ™¯è¿‡æ¸¡ä¸‰ä¸ªå…³é”®ç»´åº¦çš„å¹»è§†æƒ…å†µã€‚</li>
<li>å¤§å¤šæ•°MLLMsåœ¨è§†è§‰ä¸Šæœ‰å·®å¼‚ä½†è¯­ä¹‰ä¸Šç›¸ä¼¼çš„è§†é¢‘å¯¹ä¸Šçš„è¡¨ç°å­˜åœ¨é—®é¢˜ï¼Œå®¹æ˜“å—åˆ°å¹»è§†çš„å½±å“ã€‚</li>
<li>DINO-HEALæ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆç©ºé—´æ˜¾è‘—æ€§ä¿¡æ¯æ¥å‡å°‘å¹»è§†ç°è±¡ï¼Œè¿™åœ¨æ¨ç†æ—¶èƒ½å¤Ÿæ”¹å–„è§†é¢‘ç†è§£çš„æ•ˆæœã€‚</li>
<li>DINO-HEALåœ¨VidHallucåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å¹³å‡æé«˜äº†å¹»è§†å‡è½»çš„æ•ˆæœè¾¾åˆ°3.02%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39eb369642272767f41a6a437ddd5d1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4c6df2e346122db81fe8fd3610ac5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9db8cd69b38df1b802a44904db62ccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-127a4d1cb18f3ce892fd0a4839fec22e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f9b037348dfb17a86c69ed580f1c8c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb3c917de78415a1a9e518a3dda157ee.jpg" align="middle">
</details>




<h2 id="Mimir-Improving-Video-Diffusion-Models-for-Precise-Text-Understanding"><a href="#Mimir-Improving-Video-Diffusion-Models-for-Precise-Text-Understanding" class="headerlink" title="Mimir: Improving Video Diffusion Models for Precise Text Understanding"></a>Mimir: Improving Video Diffusion Models for Precise Text Understanding</h2><p><strong>Authors:Shuai Tan, Biao Gong, Yutong Feng, Kecheng Zheng, Dandan Zheng, Shuwei Shi, Yujun Shen, Jingdong Chen, Ming Yang</strong></p>
<p>Text serves as the key control signal in video generation due to its narrative nature. To render text descriptions into video clips, current video diffusion models borrow features from text encoders yet struggle with limited text comprehension. The recent success of large language models (LLMs) showcases the power of decoder-only transformers, which offers three clear benefits for text-to-video (T2V) generation, namely, precise text understanding resulting from the superior scalability, imagination beyond the input text enabled by next token prediction, and flexibility to prioritize user interests through instruction tuning. Nevertheless, the feature distribution gap emerging from the two different text modeling paradigms hinders the direct use of LLMs in established T2V models. This work addresses this challenge with Mimir, an end-to-end training framework featuring a carefully tailored token fuser to harmonize the outputs from text encoders and LLMs. Such a design allows the T2V model to fully leverage learned video priors while capitalizing on the text-related capability of LLMs. Extensive quantitative and qualitative results demonstrate the effectiveness of Mimir in generating high-quality videos with excellent text comprehension, especially when processing short captions and managing shifting motions. Project page: <a target="_blank" rel="noopener" href="https://lucaria-academy.github.io/Mimir/">https://lucaria-academy.github.io/Mimir/</a> </p>
<blockquote>
<p>æ–‡æœ¬åœ¨è§†é¢‘ç”Ÿæˆä¸­æ‰®æ¼”ç€å…³é”®æ§åˆ¶ä¿¡å·çš„è§’è‰²ï¼Œè¿™æ˜¯ç”±äºå®ƒçš„å™äº‹æ€§è´¨ã€‚ä¸ºäº†å°†æ–‡æœ¬æè¿°è½¬åŒ–ä¸ºè§†é¢‘ç‰‡æ®µï¼Œå½“å‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å€Ÿé‰´äº†æ–‡æœ¬ç¼–ç å™¨çš„ç‰¹å¾ï¼Œä½†åœ¨æ–‡æœ¬ç†è§£æ–¹é¢å­˜åœ¨å±€é™ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆåŠŸå±•ç¤ºäº†åªå«è§£ç å™¨çš„å˜å‹å™¨çš„åŠ›é‡ï¼Œå®ƒä¸ºæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆæä¾›äº†ä¸‰å¤§æ˜ç¡®ä¼˜åŠ¿ï¼Œåˆ†åˆ«æ˜¯ï¼šç”±äºå‡ºè‰²çš„å¯æ‰©å±•æ€§è€Œå®ç°çš„ç²¾ç¡®æ–‡æœ¬ç†è§£ã€é€šè¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å®ç°çš„è¶…è¶Šè¾“å…¥æ–‡æœ¬çš„æƒ³è±¡åŠ›ï¼Œä»¥åŠé€šè¿‡æŒ‡ä»¤è°ƒæ•´æ¥ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·å…´è¶£çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œä¸¤ç§ä¸åŒæ–‡æœ¬å»ºæ¨¡èŒƒå¼äº§ç”Ÿçš„ç‰¹å¾åˆ†å¸ƒå·®è·é˜»ç¢äº†LLMsåœ¨ç°æœ‰T2Væ¨¡å‹ä¸­çš„ç›´æ¥ä½¿ç”¨ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡Mimirè§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ï¼ŒMimiræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶ï¼Œå…·æœ‰ç²¾å¿ƒå®šåˆ¶çš„ä»¤ç‰Œèåˆå™¨ï¼Œä»¥åè°ƒæ–‡æœ¬ç¼–ç å™¨å’ŒLLMsçš„è¾“å‡ºã€‚è¿™æ ·çš„è®¾è®¡å…è®¸T2Væ¨¡å‹å……åˆ†åˆ©ç”¨å­¦ä¹ çš„è§†é¢‘å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶åˆ©ç”¨LLMsçš„æ–‡æœ¬ç›¸å…³åŠŸèƒ½ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼ŒMimiråœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œå…·æœ‰è‰¯å¥½çš„æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†çŸ­æ ‡é¢˜å’Œç®¡ç†åŠ¨æ€åœºæ™¯è½¬æ¢æ—¶ã€‚é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://lucaria-academy.github.io/Mimir/]">https://lucaria-academy.github.io/Mimir/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03085v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åœ¨è§†é¢‘ç”Ÿæˆä¸­æ‰®æ¼”ç€å…³é”®çš„æ§åˆ¶ä¿¡å·è§’è‰²ï¼Œå¾—ç›Šäºå…¶å™äº‹æ€§è´¨ã€‚å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹å€Ÿé‰´æ–‡æœ¬ç¼–ç å™¨çš„ç‰¹æ€§ï¼Œä½†åœ¨æ–‡æœ¬ç†è§£æ–¹é¢å­˜åœ¨å±€é™ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿‘æœŸæˆåŠŸå±•ç¤ºäº†ä»…è§£ç å™¨è½¬æ¢å™¨æ¨¡å‹çš„å¨åŠ›ï¼Œä¸ºæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆæä¾›äº†ä¸‰å¤§ä¼˜åŠ¿ï¼šæºäºå“è¶Šå¯æ‰©å±•æ€§çš„ç²¾ç¡®æ–‡æœ¬ç†è§£ã€ç”±ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æ¿€å‘çš„è¶…è¶Šè¾“å…¥æ–‡æœ¬çš„æƒ³è±¡åŠ›ä»¥åŠé€šè¿‡æŒ‡ä»¤è°ƒæ•´ä¼˜å…ˆç”¨æˆ·å…´è¶£çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼Œä¸¤ç§æ–‡æœ¬å»ºæ¨¡èŒƒå¼äº§ç”Ÿçš„ç‰¹å¾åˆ†å¸ƒå·®è·é˜»ç¢äº†LLMsåœ¨ç°æœ‰T2Væ¨¡å‹ä¸­çš„ç›´æ¥ä½¿ç”¨ã€‚æœ¬ç ”ç©¶é€šè¿‡Mimirè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶ï¼Œé…å¤‡ç²¾å¿ƒå®šåˆ¶çš„åˆ†éš”å™¨æ¥è°ƒå’Œæ–‡æœ¬ç¼–ç å™¨å’ŒLLMsçš„è¾“å‡ºã€‚è¿™ç§è®¾è®¡å…è®¸T2Væ¨¡å‹å……åˆ†åˆ©ç”¨å­¦ä¹ åˆ°çš„è§†é¢‘å…ˆéªŒçŸ¥è¯†ï¼ŒåŒæ—¶åˆ©ç”¨LLMsçš„æ–‡æœ¬ç›¸å…³åŠŸèƒ½ã€‚å¤§é‡å®šé‡å’Œå®šæ€§ç»“æœè¡¨æ˜ï¼ŒMimiråœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†çŸ­å­—å¹•å’Œç®¡ç†åŠ¨æ€åœºæ™¯æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åœ¨è§†é¢‘ç”Ÿæˆä¸­èµ·å…³é”®ä½œç”¨ï¼Œå¾—ç›Šäºå…¶å™äº‹æ€§è´¨ã€‚</li>
<li>å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬ç†è§£æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆæä¾›äº†ä¸‰å¤§ä¼˜åŠ¿ã€‚</li>
<li>æ–‡æœ¬å»ºæ¨¡çš„ä¸¤ç§èŒƒå¼ä¹‹é—´å­˜åœ¨ç‰¹å¾åˆ†å¸ƒå·®è·ã€‚</li>
<li>Mimiræ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è®­ç»ƒæ¡†æ¶ï¼Œå¯ä»¥è°ƒå’Œæ–‡æœ¬ç¼–ç å™¨å’ŒLLMsçš„è¾“å‡ºã€‚</li>
<li>Mimirå…è®¸T2Væ¨¡å‹åˆ©ç”¨è§†é¢‘å…ˆéªŒçŸ¥è¯†å’ŒLLMsçš„æ–‡æœ¬ç›¸å…³åŠŸèƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-120b41a9ee6d788499ca9f1d482ece7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62d4596e1da8c0b91589f756deb93d76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2eabe88075af39292564819808258248.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f1653fe6d5137d7639290926050fffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5130d03432cf31fe8bb0bd92245a1f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7c75503b99a2c0095c4fded7599e734.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-522338e81d0f893ce9562c4b304f32a3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b1a20a76cb085b276e1d0e67c846383.jpg" align="middle">
</details>




<h2 id="VideoICL-Confidence-based-Iterative-In-context-Learning-for-Out-of-Distribution-Video-Understanding"><a href="#VideoICL-Confidence-based-Iterative-In-context-Learning-for-Out-of-Distribution-Video-Understanding" class="headerlink" title="VideoICL: Confidence-based Iterative In-context Learning for   Out-of-Distribution Video Understanding"></a>VideoICL: Confidence-based Iterative In-context Learning for   Out-of-Distribution Video Understanding</h2><p><strong>Authors:Kangsan Kim, Geon Park, Youngwan Lee, Woongyeong Yeo, Sung Ju Hwang</strong></p>
<p>Recent advancements in video large multimodal models (LMMs) have significantly improved their video understanding and reasoning capabilities. However, their performance drops on out-of-distribution (OOD) tasks that are underrepresented in training data. Traditional methods like fine-tuning on OOD datasets are impractical due to high computational costs. While In-context learning (ICL) with demonstration examples has shown promising generalization performance in language tasks and image-language tasks without fine-tuning, applying ICL to video-language tasks faces challenges due to the limited context length in Video LMMs, as videos require longer token lengths. To address these issues, we propose VideoICL, a novel video in-context learning framework for OOD tasks that introduces a similarity-based relevant example selection strategy and a confidence-based iterative inference approach. This allows to select the most relevant examples and rank them based on similarity, to be used for inference. If the generated response has low confidence, our framework selects new examples and performs inference again, iteratively refining the results until a high-confidence response is obtained. This approach improves OOD video understanding performance by extending effective context length without incurring high costs. The experimental results on multiple benchmarks demonstrate significant performance gains, especially in domain-specific scenarios, laying the groundwork for broader video comprehension applications. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/KangsanKim07/VideoICL">https://github.com/KangsanKim07/VideoICL</a> </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›å±•æ˜¾è‘—æé«˜äº†å…¶è§†é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è®­ç»ƒæ•°æ®ä»£è¡¨æ€§ä¸è¶³çš„è·¨åˆ†å¸ƒï¼ˆOODï¼‰ä»»åŠ¡ä¸Šçš„è¡¨ç°æœ‰æ‰€ä¸‹é™ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚åœ¨è·¨åˆ†å¸ƒæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¹¶ä¸å®ç”¨ï¼Œå› ä¸ºè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚å°½ç®¡æ— éœ€å¾®è°ƒçš„åœ¨è¯­å¢ƒä¸­å­¦ä¹ ï¼ˆICLï¼‰æ–¹æ³•é€šè¿‡ç¤ºä¾‹å±•ç¤ºåœ¨è¯­è¨€å’Œå›¾åƒè¯­è¨€ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ³›åŒ–æ€§èƒ½ï¼Œä½†å°†å…¶åº”ç”¨äºè§†é¢‘è¯­è¨€ä»»åŠ¡æ—¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºè§†é¢‘LMMä¸­çš„è¯­å¢ƒé•¿åº¦æœ‰é™ï¼Œè€Œè§†é¢‘éœ€è¦æ›´é•¿çš„ä»¤ç‰Œé•¿åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoICLï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é¢å‘è·¨åˆ†å¸ƒä»»åŠ¡çš„è§†é¢‘è¯­å¢ƒå­¦ä¹ æ¡†æ¶ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åŸºäºç›¸ä¼¼åº¦çš„ç›¸å…³ç¤ºä¾‹é€‰æ‹©ç­–ç•¥å’Œä¸€ç§åŸºäºä¿¡å¿ƒçš„è¿­ä»£æ¨ç†æ–¹æ³•ã€‚è¿™å…è®¸é€‰æ‹©æœ€ç›¸å…³çš„ç¤ºä¾‹å¹¶æ ¹æ®ç›¸ä¼¼æ€§å¯¹å…¶è¿›è¡Œæ’åï¼Œä»¥ä¾›æ¨ç†ä½¿ç”¨ã€‚å¦‚æœç”Ÿæˆçš„å“åº”ç¼ºä¹ä¿¡å¿ƒï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼šé€‰æ‹©æ–°çš„ç¤ºä¾‹å¹¶å†æ¬¡è¿›è¡Œæ¨ç†ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç»“æœï¼Œç›´è‡³è·å¾—é«˜ä¿¡å¿ƒçš„å“åº”ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ‰©å±•æœ‰æ•ˆçš„è¯­å¢ƒé•¿åº¦è€Œæ— éœ€äº§ç”Ÿé«˜æ˜‚æˆæœ¬ï¼Œæé«˜äº†è·¨åˆ†å¸ƒè§†é¢‘ç†è§£æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºæ›´å¹¿æ³›çš„è§†é¢‘ç†è§£åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/KangsanKim07/VideoICL%E3%80%82">https://github.com/KangsanKim07/VideoICLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02186v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›å±•æ˜¾è‘—æé«˜äº†è§†é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨è®­ç»ƒæ•°æ®ç¼ºä¹ä»£è¡¨æ€§ï¼ˆOODï¼‰çš„ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼šä¸‹é™ã€‚ä¼ ç»Ÿæ–¹æ³•å¦‚ç²¾ç»†è°ƒæ•´OODæ•°æ®é›†å› é«˜è®¡ç®—æˆæœ¬è€Œä¸å®ç”¨ã€‚å°½ç®¡é€šè¿‡å±•ç¤ºèŒƒä¾‹è¿›è¡Œä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼ˆICLï¼‰å·²åœ¨è¯­è¨€ä»»åŠ¡å’Œå›¾åƒè¯­è¨€ä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººé¼“èˆçš„æ³›åŒ–æ€§èƒ½ï¼Œä½†å°†å…¶åº”ç”¨äºè§†é¢‘è¯­è¨€ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºè§†é¢‘éœ€è¦æ›´é•¿çš„ä»¤ç‰Œé•¿åº¦ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoICLï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºOODä»»åŠ¡çš„æ–°å‹è§†é¢‘ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå¼•å…¥äº†åŸºäºç›¸ä¼¼åº¦çš„ç›¸å…³ç¤ºä¾‹é€‰æ‹©ç­–ç•¥å’ŒåŸºäºä¿¡å¿ƒçš„è¿­ä»£æ¨ç†æ–¹æ³•ã€‚è¿™å…è®¸é€‰æ‹©æœ€ç›¸å…³çš„ç¤ºä¾‹å¹¶æ ¹æ®ç›¸ä¼¼æ€§å¯¹å…¶è¿›è¡Œæ’åï¼Œç”¨äºæ¨ç†ã€‚å¦‚æœç”Ÿæˆçš„å“åº”ç¼ºä¹ä¿¡å¿ƒï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼šé€‰æ‹©æ–°çš„ç¤ºä¾‹å¹¶å†æ¬¡è¿›è¡Œæ¨ç†ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–ç»“æœç›´è‡³è·å¾—é«˜ä¿¡å¿ƒçš„å“åº”ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸äº§ç”Ÿé«˜æ˜‚æˆæœ¬çš„æƒ…å†µä¸‹æ‰©å±•äº†æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæé«˜äº†OODè§†é¢‘ç†è§£æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­ï¼Œä¸ºæ›´å¹¿æ³›çš„è§†é¢‘ç†è§£åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†é¢‘ç†è§£å’Œæ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>åœ¨è®­ç»ƒæ•°æ®ç¼ºä¹ä»£è¡¨æ€§çš„ä»»åŠ¡ï¼ˆOODä»»åŠ¡ï¼‰ä¸Šï¼ŒLMMsæ€§èƒ½æœ‰æ‰€ä¸‹é™ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨OODä»»åŠ¡ä¸Šå› é«˜è®¡ç®—æˆæœ¬è€Œä¸å®ç”¨ã€‚</li>
<li>ä¸Šä¸‹æ–‡å†…å­¦ä¹ ï¼ˆICLï¼‰åœ¨è§†é¢‘è¯­è¨€ä»»åŠ¡ä¸­åº”ç”¨æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è§†é¢‘éœ€è¦æ›´é•¿çš„ä»¤ç‰Œé•¿åº¦ã€‚</li>
<li>VideoICLæ˜¯ä¸€ç§æ–°å‹è§†é¢‘ä¸Šä¸‹æ–‡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå¤„ç†OODä»»åŠ¡ã€‚</li>
<li>VideoICLé€šè¿‡å¼•å…¥åŸºäºç›¸ä¼¼åº¦çš„ç›¸å…³ç¤ºä¾‹é€‰æ‹©ç­–ç•¥å’ŒåŸºäºä¿¡å¿ƒçš„è¿­ä»£æ¨ç†æ–¹æ³•ï¼Œæé«˜äº†OODè§†é¢‘ç†è§£æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d941f9e1503ce6eb1af50cf931fd4314.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea4e99ff512b66bbc4151d15382aacd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abb763db24f29e527b807ddc1848510e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a71d26d0c9c03d58b6b5694b81b72a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8460c2465af4faa05d890dba4cfc847a.jpg" align="middle">
</details>




<h2 id="Understanding-Particles-From-Video-Property-Estimation-of-Granular-Materials-via-Visuo-Haptic-Learning"><a href="#Understanding-Particles-From-Video-Property-Estimation-of-Granular-Materials-via-Visuo-Haptic-Learning" class="headerlink" title="Understanding Particles From Video: Property Estimation of Granular   Materials via Visuo-Haptic Learning"></a>Understanding Particles From Video: Property Estimation of Granular   Materials via Visuo-Haptic Learning</h2><p><strong>Authors:Zeqing Zhang, Guangze Zheng, Xuebo Ji, Guanqi Chen, Ruixing Jia, Wentao Chen, Guanhua Chen, Liangjun Zhang, Jia Pan</strong></p>
<p>Granular materials (GMs) are ubiquitous in daily life. Understanding their properties is also important, especially in agriculture and industry. However, existing works require dedicated measurement equipment and also need large human efforts to handle a large number of particles. In this paper, we introduce a method for estimating the relative values of particle size and density from the video of the interaction with GMs. It is trained on a visuo-haptic learning framework inspired by a contact model, which reveals the strong correlation between GM properties and the visual-haptic data during the probe-dragging in the GMs. After training, the network can map the visual modality well to the haptic signal and implicitly characterize the relative distribution of particle properties in its latent embeddings, as interpreted in that contact model. Therefore, we can analyze GM properties using the trained encoder, and only visual information is needed without extra sensory modalities and human efforts for labeling. The presented GM property estimator has been extensively validated via comparison and ablation experiments. The generalization capability has also been evaluated and a real-world application on the beach is also demonstrated. Experiment videos are available at \url{<a target="_blank" rel="noopener" href="https://sites.google.com/view/gmwork/vhlearning%7D">https://sites.google.com/view/gmwork/vhlearning}</a> . </p>
<blockquote>
<p>é¢—ç²’ææ–™ï¼ˆGMsï¼‰åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­æ— å¤„ä¸åœ¨ã€‚äº†è§£å®ƒä»¬çš„ç‰¹æ€§ä¹Ÿéå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å†œä¸šå’Œå·¥ä¸šä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰å·¥ä½œè¿˜éœ€è¦ä¸“é—¨çš„æµ‹é‡è®¾å¤‡ï¼Œå¹¶éœ€è¦å¤§é‡çš„äººåŠ›æ¥å¤„ç†å¤§é‡çš„é¢—ç²’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡ä¸é¢—ç²’ææ–™çš„äº¤äº’è§†é¢‘æ¥ä¼°è®¡é¢—ç²’å°ºå¯¸å’Œå¯†åº¦çš„ç›¸å¯¹å€¼ã€‚å®ƒæ˜¯åŸºäºè§†è§‰è§¦è§‰å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒçš„ï¼Œè¯¥æ¡†æ¶å—åˆ°æ¥è§¦æ¨¡å‹çš„å¯å‘ï¼Œæ­ç¤ºäº†é¢—ç²’ææ–™å±æ€§ä¸æ¢é’ˆæ‹–åŠ¨è¿‡ç¨‹ä¸­çš„è§†è§‰è§¦è§‰æ•°æ®ä¹‹é—´çš„å¼ºçƒˆç›¸å…³æ€§ã€‚è®­ç»ƒåï¼Œç½‘ç»œå¯ä»¥å°†è§†è§‰æ¨¡å¼å¾ˆå¥½åœ°æ˜ å°„åˆ°è§¦è§‰ä¿¡å·ï¼Œå¹¶éšå«åœ°åœ¨å…¶æ½œåœ¨åµŒå…¥ä¸­æç»˜é¢—ç²’å±æ€§çš„ç›¸å¯¹åˆ†å¸ƒï¼Œæ­£å¦‚æ¥è§¦æ¨¡å‹æ‰€è§£é‡Šçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„ç¼–ç å™¨æ¥åˆ†æé¢—ç²’ææ–™çš„å±æ€§ï¼Œä»…éœ€è¦è§†è§‰ä¿¡æ¯ï¼Œè€Œæ— éœ€é¢å¤–çš„æ„Ÿå®˜æ¨¡å¼å’ŒäººåŠ›æ ‡æ³¨ã€‚æ‰€å‘ˆç°çš„é¢—ç²’ææ–™å±æ€§ä¼°è®¡å™¨å·²é€šè¿‡æ¯”è¾ƒå’Œæ¶ˆèå®éªŒè¿›è¡Œäº†å¹¿æ³›éªŒè¯ã€‚è¿˜è¯„ä¼°äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æ²™æ»©ä¸Šè¿›è¡Œäº†å®é™…åº”ç”¨æ¼”ç¤ºã€‚å®éªŒè§†é¢‘å¯åœ¨[<a target="_blank" rel="noopener" href="https://sites.google.com/view/gmwork/vhlearning]%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://sites.google.com/view/gmwork/vhlearning]ä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02119v1">PDF</a> IEEE Robotics and Automation Letters, with ICRA 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§é€šè¿‡è§†é¢‘äº’åŠ¨ä¼°è®¡é¢—ç²’ææ–™ï¼ˆGMï¼‰é¢—ç²’å¤§å°å’Œå¯†åº¦ç›¸å¯¹å€¼çš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶é‡‡ç”¨è§†è§‰è§¦è§‰å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆæ¥è§¦æ¨¡å‹ï¼Œæ­ç¤ºé¢—ç²’å±æ€§ä¸è§†é¢‘è§¦è§‰æ•°æ®ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚è®­ç»ƒåçš„ç½‘ç»œèƒ½å¤Ÿä»è§†è§‰ä¿¡æ¯ä¸­æ˜ å°„å‡ºè§¦è§‰ä¿¡å·ï¼Œå¹¶éšå«åœ°æç»˜å‡ºé¢—ç²’å±æ€§çš„ç›¸å¯¹åˆ†å¸ƒã€‚å› æ­¤ï¼Œåªéœ€è§†è§‰ä¿¡æ¯å³å¯åˆ†æGMå±æ€§ï¼Œæ— éœ€é¢å¤–çš„æ„Ÿå®˜æ¨¡å¼å’Œäººå·¥æ ‡æ³¨ã€‚è¯¥ç ”ç©¶å·²é€šè¿‡å¯¹æ¯”å’Œæ¶ˆèå®éªŒè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œå¹¶è¯„ä¼°äº†å…¶æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å±•ç¤ºäº†åœ¨æ²™æ»©ç­‰å®é™…åœºæ™¯çš„åº”ç”¨ã€‚ç›¸å…³å®éªŒè§†é¢‘å¯åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/gmwork/vhlearning">é“¾æ¥</a>è§‚çœ‹ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§é€šè¿‡è§†é¢‘ä¼°è®¡é¢—ç²’ææ–™ï¼ˆGMï¼‰é¢—ç²’å¤§å°å’Œå¯†åº¦ç›¸å¯¹å€¼çš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨è§†è§‰è§¦è§‰å­¦ä¹ æ¡†æ¶å’Œæ¥è§¦æ¨¡å‹ï¼Œå±•ç¤ºé¢—ç²’ææ–™ä¸è§†è§‰è§¦è§‰æ•°æ®ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚</li>
<li>è®­ç»ƒåçš„ç½‘ç»œèƒ½å¤Ÿä»è§†è§‰ä¿¡æ¯æ˜ å°„å‡ºè§¦è§‰ä¿¡å·ï¼Œéšå«æç»˜é¢—ç²’å±æ€§çš„ç›¸å¯¹åˆ†å¸ƒã€‚</li>
<li>æ— éœ€é¢å¤–çš„æ„Ÿå®˜æ¨¡å¼å’Œäººå·¥æ ‡æ³¨å³å¯åˆ†æGMå±æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å·²é€šè¿‡å¯¹æ¯”å’Œæ¶ˆèå®éªŒè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å±•ç¤ºäº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å±•ç¤ºäº†åœ¨æ²™æ»©ç­‰å®é™…åœºæ™¯çš„åº”ç”¨æ•ˆæœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c77a333eec3d4b3bdcb39f401dfd16ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9466606e357a8bd963601bda3124a15b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4eac801621d1318e3b55d47774e17cd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d1e1fe4b200c17eccb7b7095c406234.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c97ab8a7f521cdc35649076abeecc726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d584b755bc4b22150a8d216b4613d9e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d946f6914587c480cba26226dbc9f345.jpg" align="middle">
</details>




<h2 id="Towards-Universal-Soccer-Video-Understanding"><a href="#Towards-Universal-Soccer-Video-Understanding" class="headerlink" title="Towards Universal Soccer Video Understanding"></a>Towards Universal Soccer Video Understanding</h2><p><strong>Authors:Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research. </p>
<blockquote>
<p>ä½œä¸ºä¸€é¡¹å…¨çƒå¹¿å—æ¬¢è¿çš„è¿åŠ¨ï¼Œè¶³çƒå¸å¼•äº†æ¥è‡ªä¸–ç•Œå„åœ°çƒè¿·çš„å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªç”¨äºè¶³çƒè§†é¢‘ç†è§£çš„å…¨é¢å¤šæ¨¡å¼æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡åšå‡ºäº†ä»¥ä¸‹è´¡çŒ®ï¼šï¼ˆiï¼‰æˆ‘ä»¬ä»‹ç»äº†SoccerReplay-1988ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡å¼è¶³çƒæ•°æ®é›†ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“å¯¹æ¥è‡ª1988åœºå®Œæ•´æ¯”èµ›çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šè¿›è¡Œäº†ä»‹ç»ï¼›ï¼ˆiiï¼‰æˆ‘ä»¬å±•ç¤ºäº†ç¬¬ä¸€ä¸ªåœ¨è¶³çƒé¢†åŸŸçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹MatchVisionï¼Œè¯¥æ¨¡å‹åˆ©ç”¨è¶³çƒè§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼›ï¼ˆiiiï¼‰æˆ‘ä»¬åœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«æ–¹é¢è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œæ¶ˆå»ç ”ç©¶ã€‚MatchVisionåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¤§å¹…ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå‡¸æ˜¾äº†æˆ‘ä»¬æå‡ºçš„æ•°æ®å’Œæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†ä¸ºä½“è‚²ç†è§£ç ”ç©¶æä¾›ä¸€ä¸ªæ ‡å‡†èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01820v2">PDF</a> Technical Report; Project Page: <a target="_blank" rel="noopener" href="https://jyrao.github.io/UniSoccer/">https://jyrao.github.io/UniSoccer/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œç”¨äºè¶³çƒè§†é¢‘ç†è§£ã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šå¼•å…¥è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡å¼è¶³çƒæ•°æ®é›†SoccerReplay-1988ï¼ŒåŒ…å«1988åœºå®Œæ•´æ¯”èµ›çš„è‡ªåŠ¨æ³¨é‡Šè§†é¢‘ï¼›æ¨å‡ºé¦–ä¸ªè¶³çƒé¢†åŸŸçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹MatchVisionï¼Œå…¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼›é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œå¯¹ç…§ç ”ç©¶éªŒè¯äº†åœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«ç­‰æ–¹é¢çš„å…ˆè¿›æ€§èƒ½ã€‚æœ¬æ–‡æœ‰æœ›ä¸ºä½“è‚²ç†è§£ç ”ç©¶æä¾›æ ‡å‡†èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥å¤šæ¨¡å¼è¶³çƒæ•°æ®é›†SoccerReplay-1988ï¼ŒåŒ…å«å¤§é‡è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šã€‚</li>
<li>æå‡ºé¦–ä¸ªé’ˆå¯¹è¶³çƒé¢†åŸŸçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹MatchVisionã€‚</li>
<li>MatchVisionæ¨¡å‹åˆ©ç”¨æ—¶ç©ºä¿¡æ¯ï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>é€šè¿‡å®éªŒå’Œå¯¹ç…§ç ”ç©¶éªŒè¯äº†æ¨¡å‹åœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>è¯¥å·¥ä½œæä¾›äº†å…¨é¢çš„è¶³çƒè§†é¢‘ç†è§£æ¡†æ¶ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>SoccerReplay-1988æ•°æ®é›†çš„è‡ªåŠ¨æ³¨é‡Šç®¡é“æ˜¯ä¸€ä¸ªé‡è¦çš„æŠ€æœ¯äº®ç‚¹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76b6294123377f3fda30823244478a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4fd34ce6d77497c2a5a5dc4e4b6dc33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b3d1fe495d9db6619df4b926c35e3b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-baac5d052e5ca7bfee8aa1846d7c275e.jpg" align="middle">
</details>




<h2 id="VISTA-Enhancing-Long-Duration-and-High-Resolution-Video-Understanding-by-Video-Spatiotemporal-Augmentation"><a href="#VISTA-Enhancing-Long-Duration-and-High-Resolution-Video-Understanding-by-Video-Spatiotemporal-Augmentation" class="headerlink" title="VISTA: Enhancing Long-Duration and High-Resolution Video Understanding   by Video Spatiotemporal Augmentation"></a>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding   by Video Spatiotemporal Augmentation</h2><p><strong>Authors:Weiming Ren, Huan Yang, Jie Min, Cong Wei, Wenhu Chen</strong></p>
<p>Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective Video Spatiotemporal Augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨å¤„ç†å’Œç†è§£é•¿æ—¶é•¿æˆ–é«˜åˆ†è¾¨ç‡è§†é¢‘æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ã€‚ä¸ºäº†ä»æ•°æ®ä¸­å¿ƒçš„è§†è§’è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VISTAï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„è§†é¢‘æ—¶ç©ºå¢å¼ºæ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»ç°æœ‰çš„è§†é¢‘å­—å¹•æ•°æ®é›†ä¸­åˆæˆé•¿æ—¶é•¿å’Œé«˜åˆ†è¾¨ç‡çš„è§†é¢‘æŒ‡ä»¤å¯¹ã€‚VISTAåœ¨ç©ºé—´å’Œæ—¶é—´ä¸Šç»“åˆè§†é¢‘ï¼Œä»¥åˆ›å»ºå…·æœ‰æ‰©å±•æ—¶é•¿å’Œå¢å¼ºåˆ†è¾¨ç‡çš„æ–°åˆæˆè§†é¢‘ï¼Œå¹¶éšåç”Ÿæˆä¸è¿™äº›æ–°åˆæˆè§†é¢‘ç›¸å…³çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚åŸºäºæ­¤èŒƒå¼ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸ƒç§è§†é¢‘å¢å¼ºæ–¹æ³•ï¼Œå¹¶ç­–åˆ’äº†VISTA-400Kï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºé•¿æ—¶é•¿å’Œé«˜åˆ†è¾¨ç‡è§†é¢‘ç†è§£èƒ½åŠ›çš„è§†é¢‘æŒ‡ä»¤éµå¾ªæ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒçš„å„ç§è§†é¢‘LMMåœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†3.3%çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†é¦–ä¸ªå…¨é¢é«˜åˆ†è¾¨ç‡è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•HRVideoBenchï¼Œæˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹åœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†6.5%çš„æ€§èƒ½æå‡ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00927v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/VISTA/">https://tiger-ai-lab.github.io/VISTA/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºVISTAçš„è§†é¢‘æ—¶ç©ºå¢å¼ºæ¡†æ¶ï¼Œç”¨äºä»ç°æœ‰è§†é¢‘å­—å¹•æ•°æ®é›†ä¸­åˆæˆé•¿æ—¶å’Œé«˜æ¸…è§†é¢‘æŒ‡ä»¤å¯¹ã€‚é€šè¿‡ç©ºé—´å’Œæ—¶é—´ä¸Šçš„è§†é¢‘ç»„åˆï¼Œåˆ›å»ºæ–°çš„åˆæˆè§†é¢‘ï¼Œå¹¶ç”Ÿæˆä¸ä¹‹ç›¸å…³çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚åŸºäºè¿™ä¸€æ¡†æ¶ï¼Œå¼€å‘äº†ä¸ƒç§è§†é¢‘å¢å¼ºæ–¹æ³•ï¼Œå¹¶åˆ›å»ºäº†VISTA-400Kè§†é¢‘æŒ‡ä»¤éµå¾ªæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶å’Œé«˜æ¸…è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VISTAæ¡†æ¶è§£å†³äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿æ—¶æˆ–é«˜æ¸…è§†é¢‘æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸»è¦åŸå› æ˜¯ç¼ºä¹é«˜è´¨é‡çš„æ•°æ®é›†ã€‚</li>
<li>VISTAé€šè¿‡ç©ºé—´å’Œæ—¶é—´ä¸Šçš„è§†é¢‘ç»„åˆï¼Œåˆæˆé•¿æ—¶å’Œé«˜æ¸…è§†é¢‘ã€‚</li>
<li>VISTAèƒ½å¤Ÿç”Ÿæˆä¸åˆæˆè§†é¢‘ç›¸å…³çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>å¼€å‘äº†ä¸ƒç§è§†é¢‘å¢å¼ºæ–¹æ³•ï¼Œä»¥æé«˜è§†é¢‘æ•°æ®çš„è´¨é‡ã€‚</li>
<li>åˆ›å»ºäº†VISTA-400Kè§†é¢‘æŒ‡ä»¤éµå¾ªæ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶å’Œé«˜æ¸…è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åœ¨VISTA-400Kæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†å¹³å‡3.3%çš„æ”¹è¿›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd783f3e7058d604d5d13d3d370d75de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76160473592578252cb7a1cba1b69c5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d5c69ee92aa25301208793e250f274c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2db9cf98c195354a03b13eebcae5e9f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a579182ce9bcfbd32503a805e6750651.jpg" align="middle">
</details>




<h2 id="Video-3D-LLM-Learning-Position-Aware-Video-Representation-for-3D-Scene-Understanding"><a href="#Video-3D-LLM-Learning-Position-Aware-Video-Representation-for-3D-Scene-Understanding" class="headerlink" title="Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene   Understanding"></a>Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene   Understanding</h2><p><strong>Authors:Duo Zheng, Shijia Huang, Liwei Wang</strong></p>
<p>The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the modelsâ€™ learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•å¯¹å¤šç§å¤šæ¨¡æ€ä»»åŠ¡äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†éœ€è¦åœ¨3Dç¯å¢ƒä¸­è¿›è¡Œç©ºé—´ç†è§£çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å¢å¼ºMLLMsçš„åŠŸèƒ½ï¼Œå·²ç»åšå‡ºäº†åŠªåŠ›ï¼Œå¦‚èå…¥ç‚¹äº‘ç‰¹å¾ï¼Œä½†æ¨¡å‹æ‰€å­¦ä¹ åˆ°çš„è¡¨ç¤ºä¸3Dåœºæ™¯å›ºæœ‰å¤æ‚æ€§ä¹‹é—´ä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚è¿™ç§å·®å¼‚ä¸»è¦æºäºMLLMsä¸»è¦åœ¨2Dæ•°æ®ä¸Šçš„è®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç†è§£3Dç©ºé—´æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é€šç”¨æ¨¡å‹ï¼Œå³Video-3D LLMï¼Œç”¨äº3Dåœºæ™¯ç†è§£ã€‚æˆ‘ä»¬å°†3Dåœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘ï¼Œå¹¶å°†3Dä½ç½®ç¼–ç èå…¥è¿™äº›è¡¨ç¤ºä¸­ï¼Œæˆ‘ä»¬çš„Video-3D LLMèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä½¿è§†é¢‘è¡¨ç¤ºä¸çœŸå®ä¸–ç•Œç©ºé—´ä¸Šä¸‹æ–‡å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†ä¸€ç§æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬ä¸æ€§èƒ½æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ª3Dåœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒ…æ‹¬ScanReferã€Multi3DReferã€Scan2Capã€ScanQAå’ŒSQA3Dã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00493v1">PDF</a> 14 pages, 4 figures</p>
<p><strong>Summary</strong><br>å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­æœ‰æ˜¾è‘—å½±å“ï¼Œä½†åœ¨éœ€è¦ç©ºé—´ç†è§£çš„3Dç¯å¢ƒä¸­é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰æ¨¡å‹çš„ä¸è¶³ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é€šç”¨æ¨¡å‹â€”â€”Video-3D LLMï¼Œé€šè¿‡åŠ¨æ€è§†é¢‘ç†è§£ç»“åˆçœŸå®ç©ºé—´è¯­å¢ƒå®ç°å‡†ç¡®é«˜æ•ˆçš„3Dåœºæ™¯ç†è§£ã€‚é‡‡ç”¨æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ª3Dåœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­æœ‰æ˜¾è‘—å½±å“ï¼Œä½†åœ¨å¤„ç†éœ€è¦ç©ºé—´ç†è§£çš„3Dç¯å¢ƒä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>MLLMsä¸»è¦åŸºäºäºŒç»´æ•°æ®è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨ç†è§£ä¸‰ç»´ç©ºé—´ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹é€šç”¨æ¨¡å‹Video-3D LLMç”¨äºç†è§£ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>Video-3D LLMé€šè¿‡å°†ä¸‰ç»´åœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘å¹¶èå…¥ä¸‰ç»´ä½ç½®ç¼–ç ï¼Œæ›´å‡†ç¡®åœ°å°†è§†é¢‘è¡¨ç°ä¸çœŸå®ç©ºé—´è¯­å¢ƒå¯¹é½ã€‚</li>
<li>Video-3D LLMé‡‡ç”¨æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>å®éªŒè¯æ˜Video-3D LLMåœ¨å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fc9c3e9e263e2b773922ee67e89b2f8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8c1c8a2185ee63c1f4a8104413e1954.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1229421e6519c070ef7dde5f8496b766.jpg" align="middle">
</details>




<h2 id="Look-Every-Frame-All-at-Once-Video-Ma-2-mba-for-Efficient-Long-form-Video-Understanding-with-Multi-Axis-Gradient-Checkpointing"><a href="#Look-Every-Frame-All-at-Once-Video-Ma-2-mba-for-Efficient-Long-form-Video-Understanding-with-Multi-Axis-Gradient-Checkpointing" class="headerlink" title="Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form   Video Understanding with Multi-Axis Gradient Checkpointing"></a>Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form   Video Understanding with Multi-Axis Gradient Checkpointing</h2><p><strong>Authors:Hosu Lee, Junho Kim, Hyunjun Kim, Yong Man Ro</strong></p>
<p>With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma$^2$mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma$^2$mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks. </p>
<blockquote>
<p>éšç€è§†é¢‘æ•°æ®çš„è§„æ¨¡å’Œå¤æ‚æ€§ä¸æ–­å¢é•¿ï¼Œç”±äºç°æœ‰åŸºäºTransformerçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å†…å­˜å’Œè®¡ç®—éœ€æ±‚ä¸Šçš„äºŒæ¬¡å¢é•¿ï¼Œæœ‰æ•ˆåœ°å¤„ç†é•¿è§†é¢‘åºåˆ—å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Video-Ma$^2$mbaè¿™ä¸€æ–°å‹æ¶æ„ï¼Œå®ƒç»“åˆäº†Mamba-2æ¡†æ¶ä¸­çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œæ›¿ä»£äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ä½¿å¾—LMMåœ¨æ—¶é—´å’Œå†…å­˜éœ€æ±‚æ–¹é¢å®ç°çº¿æ€§æ‰©å±•ï¼Œä»è€Œèƒ½å¤Ÿå¤„ç†é•¿æ—¶é—´çš„è§†é¢‘å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥å¤šè½´æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆMA-GCï¼‰æ–¹æ³•æé«˜äº†å†…å­˜æ•ˆç‡ï¼Œè¯¥æ–¹æ³•é€šè¿‡ä¿ç•™å¤šä¸ªè®¡ç®—è½´ä¸Šçš„é‡è¦æ¿€æ´»æ¥ç­–ç•¥æ€§åœ°ç®¡ç†å†…å­˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”æ ‡å‡†æ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯å¤§å¤§é™ä½äº†å†…å­˜å ç”¨ã€‚ç»éªŒåˆ†æè¡¨æ˜ï¼ŒVideo-Ma$^2$mbaå¯ä»¥åœ¨å•ä¸ªGPUä¸Šä»¥æ¯ç§’ä¸€å¸§çš„é€Ÿåº¦å¤„ç†å¤§é‡ç­‰æ•ˆäºæ•°ç™¾ä¸‡ä¸ªæ ‡è®°æˆ–è¶…è¿‡ä¸¤å°æ—¶çš„è¿ç»­åºåˆ—çš„è§†é¢‘åºåˆ—ã€‚é€šè¿‡ä¿æŒå¯¹æ—¶é—´åŠ¨æ€çš„è¯¦ç»†æ•æ‰ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æé«˜äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå“åº”çš„å®ç”¨æ€§ï¼Œæ˜¾ç¤ºå‡ºç›¸å¯¹äºç°æœ‰æ¡†æ¶çš„å·¨å¤§ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19460v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://ivy-lvlm.github.io/Video-MA2MBA/">https://ivy-lvlm.github.io/Video-MA2MBA/</a></p>
<p><strong>Summary</strong><br>è§†é¢‘æ•°æ®çš„è§„æ¨¡å’Œå¤æ‚æ€§ä¸æ–­å¢é•¿ï¼Œç°æœ‰åŸºäºtransformerçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘åºåˆ—æ—¶é¢ä¸´å†…å­˜å’Œè®¡ç®—éœ€æ±‚å‘ˆäºŒæ¬¡æ–¹å¢é•¿çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥Video-Ma$^2$mbaè¿™ä¸€æ–°å‹æ¶æ„ï¼Œé€šè¿‡çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨Mamba-2æ¡†æ¶å†…æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å¾—å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ—¶é—´éœ€æ±‚ä¸Šå®ç°çº¿æ€§æ‰©å±•ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥å¤šè½´æ¢¯åº¦æ£€æŸ¥ç‚¹æ³•ï¼ˆMA-GCï¼‰ï¼Œåœ¨å¤šä¸ªè®¡ç®—è½´ä¸Šä¿ç•™å…³é”®æ¿€æ´»ä¿¡æ¯ï¼Œæé«˜å†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚è¯¥æ–¹æ³•çš„å†…å­˜å ç”¨ä¸æ ‡å‡†æ¢¯åº¦æ£€æŸ¥ç‚¹æ³•ç›¸æ¯”å¤§å¹…å‡å°‘ã€‚å®éªŒåˆ†æè¡¨æ˜ï¼ŒVideo-Ma$^2$mbaèƒ½åœ¨å•ä¸ªGPUä¸Šå¤„ç†ç›¸å½“äºæ•°ç™¾ä¸‡ä»¤ç‰Œæˆ–è¶…è¿‡ä¸¤å°æ—¶è¿ç»­åºåˆ—çš„è§†é¢‘æ•°æ®ã€‚é€šè¿‡ç²¾ç¡®æ•æ‰æ—¶é—´åŠ¨æ€ï¼Œè¯¥æ¨¡å‹æé«˜äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå“åº”ç›¸å…³æ€§ï¼Œå±•ç°å‡ºå¯¹ç°è¡Œæ¡†æ¶çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹è§†é¢‘æ•°æ®è§„æ¨¡å¢é•¿å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç°æœ‰åŸºäºtransformerçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘åºåˆ—æ—¶å­˜åœ¨å†…å­˜å’Œè®¡ç®—éœ€æ±‚çš„é—®é¢˜ã€‚</li>
<li>Video-Ma$^2$mbaæ¶æ„é€šè¿‡å¼•å…¥çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨Mamba-2æ¡†æ¶å†…æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°çº¿æ€§æ‰©å±•ã€‚</li>
<li>å¤šè½´æ¢¯åº¦æ£€æŸ¥ç‚¹æ³•ï¼ˆMA-GCï¼‰æé«˜äº†å†…å­˜ä½¿ç”¨æ•ˆç‡ï¼Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨ã€‚</li>
<li>Video-Ma$^2$mbaèƒ½åœ¨å•ä¸ªGPUä¸Šå¤„ç†å¤§é‡è§†é¢‘æ•°æ®ï¼Œç›¸å½“äºæ•°ç™¾ä¸‡ä»¤ç‰Œæˆ–è¶…è¿‡ä¸¤å°æ—¶è¿ç»­åºåˆ—ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿç²¾ç¡®æ•æ‰è§†é¢‘çš„æ—¶é—´åŠ¨æ€ã€‚</li>
<li>Video-Ma$^2$mbaæé«˜äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œå“åº”ç›¸å…³æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5fad1fd512888509a7fa233c8aeb8755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d72d88af637e011f428f702cc18a0aca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d78c00a1dd2d84bb1e36a33d8939da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3c054ce51dda3ecf51d6d306025b2d3.jpg" align="middle">
</details>




<h2 id="ObjectRelator-Enabling-Cross-View-Object-Relation-Understanding-in-Ego-Centric-and-Exo-Centric-Videos"><a href="#ObjectRelator-Enabling-Cross-View-Object-Relation-Understanding-in-Ego-Centric-and-Exo-Centric-Videos" class="headerlink" title="ObjectRelator: Enabling Cross-View Object Relation Understanding in   Ego-Centric and Exo-Centric Videos"></a>ObjectRelator: Enabling Cross-View Object Relation Understanding in   Ego-Centric and Exo-Centric Videos</h2><p><strong>Authors:Yuqian Fu, Runze Wang, Yanwei Fu, Danda Pani Paudel, Xuanjing Huang, Luc Van Gool</strong></p>
<p>In this paper, we focus on the Ego-Exo Object Correspondence task, an emerging challenge in the field of computer vision that aims to map objects across ego-centric and exo-centric views. We introduce ObjectRelator, a novel method designed to tackle this task, featuring two new modules: Multimodal Condition Fusion (MCFuse) and SSL-based Cross-View Object Alignment (XObjAlign). MCFuse effectively fuses language and visual conditions to enhance target object localization, while XObjAlign enforces consistency in object representations across views through a self-supervised alignment strategy. Extensive experiments demonstrate the effectiveness of ObjectRelator, achieving state-of-the-art performance on Ego2Exo and Exo2Ego tasks with minimal additional parameters. This work provides a foundation for future research in comprehensive cross-view object relation understanding highlighting the potential of leveraging multimodal guidance and cross-view alignment. Codes and models will be released to advance further research in this direction. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹ç ”ç©¶Ego-Exoå¯¹è±¡å¯¹åº”å…³ç³»ä»»åŠ¡ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ–°å…´æŒ‘æˆ˜ï¼Œæ—¨åœ¨å®ç°è·¨è‡ªæˆ‘ä¸­å¿ƒè§†è§’å’Œå¤–åœ¨è§†è§’çš„å¯¹è±¡æ˜ å°„ã€‚æˆ‘ä»¬æå‡ºäº†ObjectRelatorè¿™ä¸€æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œå…¶åŒ…å«ä¸¤ä¸ªæ–°æ¨¡å—ï¼šå¤šæ¨¡æ€æ¡ä»¶èåˆï¼ˆMCFuseï¼‰å’ŒåŸºäºSSLçš„è·¨è§†å›¾å¯¹è±¡å¯¹é½ï¼ˆXObjAlignï¼‰ã€‚MCFuseæœ‰æ•ˆåœ°èåˆäº†è¯­è¨€å’Œè§†è§‰æ¡ä»¶ï¼Œå¢å¼ºäº†ç›®æ ‡å¯¹è±¡çš„å®šä½ï¼Œè€ŒXObjAlignåˆ™é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„å¯¹é½ç­–ç•¥ï¼Œå®ç°äº†è·¨è§†å›¾å¯¹è±¡è¡¨ç¤ºçš„ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¯æ˜äº†ObjectRelatorçš„æœ‰æ•ˆæ€§ï¼Œåœ¨Ego2Exoå’ŒExo2Egoä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œä¸”ä»…å¢åŠ äº†æå°‘é‡çš„å‚æ•°ã€‚è¿™é¡¹å·¥ä½œä¸ºå…¨é¢è·¨è§†å›¾å¯¹è±¡å…³ç³»ç†è§£çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ï¼Œçªå‡ºäº†åˆ©ç”¨å¤šæ¨¡æ€æŒ‡å¯¼å’Œè·¨è§†å›¾å¯¹é½çš„æ½œåŠ›ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€æ–¹å‘çš„ç ”ç©¶è¿›å±•ï¼Œæˆ‘ä»¬å°†å‘å¸ƒç›¸å…³ä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19083v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡èšç„¦äºEgo-Exoå¯¹è±¡å¯¹åº”å…³ç³»ä»»åŠ¡ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ–°å…´æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ˜ å°„ä¸åŒè§†ç‚¹ä¸‹çš„å¯¹è±¡ã€‚å¼•å…¥äº†ä¸€ç§æ–°æ–¹æ³•ObjectRelatoræ¥è§£å†³æ­¤ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°æ¨¡å—ï¼šå¤šæ¨¡æ€æ¡ä»¶èåˆï¼ˆMCFuseï¼‰å’ŒåŸºäºSSLçš„è·¨è§†å›¾å¯¹è±¡å¯¹é½ï¼ˆXObjAlignï¼‰ã€‚MCFuseæœ‰æ•ˆåœ°èåˆäº†è¯­è¨€å’Œè§†è§‰æ¡ä»¶ï¼Œå¢å¼ºäº†ç›®æ ‡å¯¹è±¡çš„å®šä½èƒ½åŠ›ï¼›è€ŒXObjAlignåˆ™é€šè¿‡ä¸€ç§è‡ªç›‘ç£çš„å¯¹é½ç­–ç•¥ï¼Œç¡®ä¿è·¨è§†å›¾çš„å¯¹è±¡è¡¨ç¤ºçš„ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ObjectRelatorçš„æœ‰æ•ˆæ€§ï¼Œåœ¨Ego2Exoå’ŒExo2Egoä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶å±•ç¤ºäº†åœ¨æœªæ¥å¯¹å…¨é¢è·¨è§†å›¾å¯¹è±¡å…³ç³»ç†è§£çš„æ½œåœ¨è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªå…³äºè¯¥æ–‡æœ¬çš„å…³é”®è§è§£ï¼š</p>
<ul>
<li>æœ¬æ–‡ä¸»è¦è®¨è®ºEgo-Exoå¯¹è±¡å¯¹åº”å…³ç³»ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å…´æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ˜ å°„ä¸åŒè§†ç‚¹ä¸‹çš„å¯¹è±¡ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ObjectRelatoræ¥è§£å†³è¿™ä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°æ¨¡å—ï¼šå¤šæ¨¡æ€æ¡ä»¶èåˆï¼ˆMCFuseï¼‰å’ŒåŸºäºSSLçš„è·¨è§†å›¾å¯¹è±¡å¯¹é½ï¼ˆXObjAlignï¼‰ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b876b676f58411c5403e8efa9cca829a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce767fd403a01bffecab2c95a3aed3d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6912743830ba8b50b4198476d1760c10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96ebd3d9149513636d3007f82ca735bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4056c046e68ef6ff9aa4868115f7a8.jpg" align="middle">
</details>




<h2 id="TimeMarker-A-Versatile-Video-LLM-for-Long-and-Short-Video-Understanding-with-Superior-Temporal-Localization-Ability"><a href="#TimeMarker-A-Versatile-Video-LLM-for-Long-and-Short-Video-Understanding-with-Superior-Temporal-Localization-Ability" class="headerlink" title="TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding   with Superior Temporal Localization Ability"></a>TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding   with Superior Temporal Localization Ability</h2><p><strong>Authors:Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, Lin Ma</strong></p>
<p>Rapid development of large language models (LLMs) has significantly advanced multimodal large language models (LMMs), particularly in vision-language tasks. However, existing video-language models often overlook precise temporal localization and struggle with videos of varying lengths. We introduce TimeMarker, a versatile Video-LLM designed for high-quality dialogue based on video content, emphasizing temporal localization. TimeMarker integrates Temporal Separator Tokens to enhance temporal awareness, accurately marking specific moments within videos. It employs the AnyLength mechanism for dynamic frame sampling and adaptive token merging, enabling effective handling of both short and long videos. Additionally, TimeMarker utilizes diverse datasets, including further transformed temporal-related video QA datasets, to bolster its temporal understanding capabilities. Image and interleaved data are also employed to further enhance the modelâ€™s semantic perception ability. Evaluations demonstrate that TimeMarker achieves state-of-the-art performance across multiple benchmarks, excelling in both short and long video categories. Our project page is at \url{<a target="_blank" rel="noopener" href="https://github.com/TimeMarker-LLM/TimeMarker/%7D">https://github.com/TimeMarker-LLM/TimeMarker/}</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMMï¼‰ä¹Ÿå–å¾—äº†é‡å¤§è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†é¢‘è¯­è¨€æ¨¡å‹å¾€å¾€å¿½è§†äº†ç²¾ç¡®çš„æ—¶é—´å®šä½ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä¸åŒé•¿åº¦çš„è§†é¢‘æ—¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†TimeMarkerï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºè§†é¢‘å†…å®¹çš„é«˜å“è´¨å¯¹è¯é€šç”¨è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¼ºè°ƒæ—¶é—´å®šä½ã€‚TimeMarkeré›†æˆäº†æ—¶é—´åˆ†éš”ç¬¦ä»¤ç‰Œï¼Œä»¥æé«˜æ—¶é—´æ„è¯†ï¼Œå‡†ç¡®æ ‡è®°è§†é¢‘ä¸­çš„ç‰¹å®šæ—¶åˆ»ã€‚å®ƒé‡‡ç”¨AnyLengthæœºåˆ¶è¿›è¡ŒåŠ¨æ€å¸§é‡‡æ ·å’Œè‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†é•¿çŸ­è§†é¢‘ã€‚æ­¤å¤–ï¼ŒTimeMarkeråˆ©ç”¨å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç»è¿‡è¿›ä¸€æ­¥æ—¶é—´å˜æ¢çš„è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œä»¥å¢å¼ºå…¶æ—¶é—´ç†è§£èƒ½åŠ›ã€‚å›¾åƒå’Œäº¤é”™æ•°æ®ä¹Ÿè¢«ç”¨æ¥è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒTimeMarkeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨é•¿çŸ­è§†é¢‘ç±»åˆ«ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯\url{<a target="_blank" rel="noopener" href="https://github.com/TimeMarker-LLM/TimeMarker/%7D%E3%80%82">https://github.com/TimeMarker-LLM/TimeMarker/}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18211v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„è¿›æ­¥ï¼Œä½†ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨ç²¾ç¡®æ—¶é—´å®šä½å’Œä¸åŒé•¿åº¦è§†é¢‘çš„åº”å¯¹ä¸Šå­˜åœ¨å±€é™ã€‚æˆ‘ä»¬æ¨å‡ºTimeMarkerï¼Œä¸€æ¬¾é’ˆå¯¹è§†é¢‘å†…å®¹çš„å¯¹è¯è¿›è¡Œè®¾è®¡çš„å¤šåŠŸèƒ½è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¼ºè°ƒæ—¶é—´å®šä½çš„é‡è¦æ€§ã€‚TimeMarkeré€šè¿‡å¼•å…¥æ—¶é—´åˆ†éš”ç¬¦ä»¤ç‰Œå¢å¼ºæ—¶é—´æ„ŸçŸ¥ï¼Œå‡†ç¡®æ ‡è®°è§†é¢‘ä¸­çš„ç‰¹å®šæ—¶åˆ»ã€‚å®ƒé‡‡ç”¨ä»»æ„é•¿åº¦æœºåˆ¶è¿›è¡ŒåŠ¨æ€å¸§é‡‡æ ·å’Œè‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ï¼Œæœ‰æ•ˆå¤„ç†é•¿çŸ­ä¸ä¸€çš„è§†é¢‘ã€‚æ­¤å¤–ï¼ŒTimeMarkeråˆ©ç”¨å¤šæ ·åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç»è¿‡æ—¶é—´è½¬åŒ–çš„è§†é¢‘é—®ç­”æ•°æ®é›†ï¼Œå¢å¼ºæ—¶é—´ç†è§£çš„èƒ½åŠ›ã€‚è¯„ä¼°å’Œå®éªŒè¡¨æ˜ï¼ŒTimeMarkeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­é•¿å’Œé•¿è§†é¢‘ç±»åˆ«ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•æ¨åŠ¨äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„è¿›æ­¥ã€‚</li>
<li>ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´å®šä½ä¸Šå­˜åœ¨é—®é¢˜ï¼Œæ— æ³•å¾ˆå¥½åœ°å¤„ç†ä¸åŒé•¿åº¦çš„è§†é¢‘ã€‚</li>
<li>TimeMarkeræ˜¯ä¸€æ¬¾é’ˆå¯¹è§†é¢‘å†…å®¹çš„å¯¹è¯è®¾è®¡çš„å¤šåŠŸèƒ½è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¼ºè°ƒæ—¶é—´å®šä½çš„é‡è¦æ€§ã€‚</li>
<li>TimeMarkeré€šè¿‡å¼•å…¥æ—¶é—´åˆ†éš”ç¬¦ä»¤ç‰Œå’Œä»»æ„é•¿åº¦æœºåˆ¶æ¥å¤„ç†é•¿çŸ­ä¸ä¸€çš„è§†é¢‘ã€‚</li>
<li>TimeMarkeråˆ©ç”¨å¤šæ ·åŒ–çš„æ•°æ®é›†å¢å¼ºæ—¶é—´ç†è§£èƒ½åŠ›ã€‚</li>
<li>TimeMarkeré‡‡ç”¨å›¾åƒå’Œäº¤é”™æ•°æ®æ¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„è¯­ä¹‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>TimeMarkeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œç‰¹åˆ«æ˜¯åœ¨çŸ­é•¿å’Œé•¿è§†é¢‘ç±»åˆ«ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c4b312ec3310743cabacd41d31082851.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d29af6d6c853f4e0d8cd0b5f7524bdce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b09b2ca0bd05b78dcb73190d932a775c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-500b3f22272c9748018247cbed84bf0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da0da08b07e7485c4c7b2c8760f44813.jpg" align="middle">
</details>




<h2 id="SAVEn-Vid-Synergistic-Audio-Visual-Integration-for-Enhanced-Understanding-in-Long-Video-Context"><a href="#SAVEn-Vid-Synergistic-Audio-Visual-Integration-for-Enhanced-Understanding-in-Long-Video-Context" class="headerlink" title="SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced   Understanding in Long Video Context"></a>SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced   Understanding in Long Video Context</h2><p><strong>Authors:Jungang Li, Sicheng Tao, Yibo Yan, Xiaojie Gu, Haodong Xu, Xu Zheng, Yuanhuiyi Lyu, Linfeng Zhang, Xuming Hu</strong></p>
<p>Endeavors have been made to explore Large Language Models for video analysis (Video-LLMs), particularly in understanding and interpreting long videos. However, existing Video-LLMs still face challenges in effectively integrating the rich and diverse audio-visual information inherent in long videos, which is crucial for comprehensive understanding. This raises the question: how can we leverage embedded audio-visual information to enhance long video understanding? Therefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video dataset comprising over 58k audio-visual instructions. (ii) From the model perspective, we propose a time-aware Audio-Visual Large Language Model (AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present AVBench, a benchmark containing 2,500 QAs designed to evaluate models on enhanced audio-visual comprehension tasks within long video, challenging their ability to handle intricate audio-visual interactions. Experiments on AVBench reveal the limitations of current AV-LLMs. Experiments also demonstrate that SAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video task (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the zero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter scale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and code will be released at <a target="_blank" rel="noopener" href="https://ljungang.github.io/SAVEn-Vid/">https://ljungang.github.io/SAVEn-Vid/</a> upon acceptance. </p>
<blockquote>
<p>ç›®å‰æ­£åŠªåŠ›æ¢ç´¢ç”¨äºè§†é¢‘åˆ†æçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œè§£é‡Šé•¿è§†é¢‘æ–¹é¢ã€‚ç„¶è€Œï¼Œç°æœ‰çš„Video-LLMsä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå³å¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆé•¿è§†é¢‘ä¸­ä¸°å¯Œçš„å„ç§è§†å¬ä¿¡æ¯ï¼Œè¿™å¯¹äºå…¨é¢ç†è§£è‡³å…³é‡è¦ã€‚è¿™å¼•å‘äº†ä»¥ä¸‹é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨åµŒå…¥çš„è§†å¬ä¿¡æ¯æ¥æé«˜é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Ÿå› æ­¤ï¼Œ(i)æˆ‘ä»¬æ¨å‡ºäº†SAVEn-Vidï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«è¶…è¿‡58,000æ¡è§†å¬æŒ‡ä»¤çš„é•¿æ—¶é—´è§†å¬è§†é¢‘æ•°æ®é›†ã€‚(ii)ä»æ¨¡å‹è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´æ„ŸçŸ¥è§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆAV-LLMï¼‰ï¼Œå³SAVEnVideoï¼Œå¹¶åœ¨SAVEn-Vidä¸Šè¿›è¡Œå¾®è°ƒã€‚(iii)æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†AVBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2500ä¸ªé—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘ä¸­çš„å¢å¼ºè§†å¬ç†è§£ä»»åŠ¡çš„èƒ½åŠ›ï¼ŒæŒ‘æˆ˜å®ƒä»¬å¤„ç†å¤æ‚è§†å¬äº¤äº’çš„èƒ½åŠ›ã€‚åœ¨AVBenchä¸Šçš„å®éªŒæ­ç¤ºäº†å½“å‰AV-LLMsçš„å±€é™æ€§ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬é•¿è§†é¢‘ä»»åŠ¡ï¼ˆVideo-MMEï¼‰ä¸Šï¼ŒSAVEnVideoçš„è¡¨ç°ä¼˜äºæœ€ä½³Video-LLMè¾¾3.61%ï¼Œåœ¨é›¶æ ·æœ¬è§†å¬ä»»åŠ¡ï¼ˆMusic-AVQAï¼‰ä¸Šï¼ŒSAVEnVideoçš„è¡¨ç°ä¹Ÿè¶…è¿‡äº†é¢†å…ˆçš„è§†å¬LLMè¾¾1.29%ã€‚å› æ­¤ï¼Œåœ¨7Bå‚æ•°è§„æ¨¡ä¸‹ï¼ŒSAVEnVideoå¯ä»¥è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://ljungang.github.io/SAVEn-Vid/%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E5%BE%85%E5%AE%A1%E6%A0%B8%E9%80%9A%E8%BF%87%E5%90%8E%E5%8D%B3%E5%8F%AF%E4%BD%BF%E7%94%A8%E3%80%82">https://ljungang.github.io/SAVEn-Vid/ä¸Šå‘å¸ƒï¼Œå¾…å®¡æ ¸é€šè¿‡åå³å¯ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16213v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘åˆ†æä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é•¿è§†é¢‘çš„ç†è§£å’Œè§£é‡Šã€‚ç ”ç©¶é¢ä¸´å¦‚ä½•æœ‰æ•ˆæ•´åˆé•¿è§†é¢‘ä¸­çš„ä¸°å¯Œè§†å¬ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº†SAVEn-Vidæ•°æ®é›†å’ŒSAVEnVideoæ—¶é—´æ„ŸçŸ¥è§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶è®¾ç«‹äº†AVBenchåŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼ŒSAVEnVideoåœ¨é›¶æ ·æœ¬é•¿è§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰è§†å¬è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘åˆ†æä¸­çš„åº”ç”¨æ­£åœ¨å—åˆ°å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œè§£é‡Šé•¿è§†é¢‘æ–¹é¢ã€‚</li>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•´åˆé•¿è§†é¢‘çš„ä¸°å¯Œè§†å¬ä¿¡æ¯æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>SAVEn-Vidæ•°æ®é›†çš„æ¨å‡ºï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒ…å«è¶…è¿‡5.8ä¸‡çš„è§†å¬æŒ‡ä»¤ã€‚</li>
<li>SAVEnVideoæ˜¯ä¸€ç§æ—¶é—´æ„ŸçŸ¥çš„è§†å¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»è¿‡SAVEn-Vidæ•°æ®é›†çš„å¾®è°ƒã€‚</li>
<li>AVBenchåŸºå‡†æµ‹è¯•çš„è®¾ç«‹ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘ä¸­çš„å¤æ‚è§†å¬äº¤äº’ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSAVEnVideoåœ¨é›¶æ ·æœ¬é•¿è§†é¢‘ä»»åŠ¡å’Œè§†å¬ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c377cfa6694d981a6da59847bad6c17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae4b65d8cc7d7e26e19bbe2ee348bdc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4290c8e22ee321990e9c9af2085f06a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed4e6f694ea2f795c573ca32380556c.jpg" align="middle">
</details>




<h2 id="ReWind-Understanding-Long-Videos-with-Instructed-Learnable-Memory"><a href="#ReWind-Understanding-Long-Videos-with-Instructed-Learnable-Memory" class="headerlink" title="ReWind: Understanding Long Videos with Instructed Learnable Memory"></a>ReWind: Understanding Long Videos with Instructed Learnable Memory</h2><p><strong>Authors:Anxhelo Diko, Tinghuai Wang, Wassim Swaileh, Shiyan Sun, Ioannis Patras</strong></p>
<p>Vision-Language Models (VLMs) are crucial for applications requiring integrated understanding textual and visual information. However, existing VLMs struggle with long videos due to computational inefficiency, memory limitations, and difficulties in maintaining coherent understanding across extended sequences. To address these challenges, we introduce ReWind, a novel memory-based VLM designed for efficient long video understanding while preserving temporal fidelity. ReWind operates in a two-stage framework. In the first stage, ReWind maintains a dynamic learnable memory module with a novel \textbf{read-perceive-write} cycle that stores and updates instruction-relevant visual information as the video unfolds. This module utilizes learnable queries and cross-attentions between memory contents and the input stream, ensuring low memory requirements by scaling linearly with the number of tokens. In the second stage, we propose an adaptive frame selection mechanism guided by the memory content to identify instruction-relevant key moments. It enriches the memory representations with detailed spatial information by selecting a few high-resolution frames, which are then combined with the memory contents and fed into a Large Language Model (LLM) to generate the final answer. We empirically demonstrate ReWindâ€™s superior performance in visual question answering (VQA) and temporal grounding tasks, surpassing previous methods on long video benchmarks. Notably, ReWind achieves a +13% score gain and a +12% accuracy improvement on the MovieChat-1K VQA dataset and an +8% mIoU increase on Charades-STA for temporal grounding. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹äºéœ€è¦ç»¼åˆç†è§£æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—æ•ˆç‡ä½ä¸‹ã€å†…å­˜é™åˆ¶ä»¥åŠç»´æŒæ‰©å±•åºåˆ—ä¸­è¿è´¯ç†è§£çš„å›°éš¾ï¼Œç°æœ‰çš„VLMsåœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Rewindï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ–°å‹è®°å¿†è®¾è®¡çš„VLMï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„é•¿è§†é¢‘ç†è§£ï¼ŒåŒæ—¶ä¿æŒæ—¶é—´ä¿çœŸåº¦ã€‚Rewindé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶è¿›è¡Œæ“ä½œã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒRewindç»´æŠ¤ä¸€ä¸ªåŠ¨æ€çš„å¯å­¦ä¹ è®°å¿†æ¨¡å—ï¼Œè¯¥æ¨¡å—å…·æœ‰æ–°é¢–çš„â€œè¯»-æ„ŸçŸ¥-å†™â€å¾ªç¯ï¼Œéšç€è§†é¢‘çš„å±•å¼€ï¼Œå­˜å‚¨å¹¶æ›´æ–°ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚è¯¥æ¨¡å—åˆ©ç”¨å¯å­¦ä¹ æŸ¥è¯¢å’Œå†…å­˜å†…å®¹ä¸è¾“å…¥æµä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›ï¼Œé€šè¿‡ä»¤ç‰Œæ•°é‡è¿›è¡Œçº¿æ€§ç¼©æ”¾ï¼Œç¡®ä¿ä½å†…å­˜è¦æ±‚ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±å†…å­˜å†…å®¹å¼•å¯¼çš„è‡ªé€‚åº”å¸§é€‰æ‹©æœºåˆ¶ï¼Œä»¥è¯†åˆ«ä¸æŒ‡ä»¤ç›¸å…³çš„å…³é”®æ—¶åˆ»ã€‚å®ƒé€šè¿‡é€‰æ‹©ä¸€äº›é«˜åˆ†è¾¨ç‡å¸§æ¥ä¸°å¯Œå†…å­˜è¡¨ç¤ºä¸­çš„è¯¦ç»†ç©ºé—´ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›å¸§ä¸å†…å­˜å†…å®¹ç›¸ç»“åˆå¹¶è¾“å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚æˆ‘ä»¬åœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸­è¿›è¡Œäº†å®è¯æ¼”ç¤ºï¼ŒRewindåœ¨é•¿é£è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRewindåœ¨MovieChat-1K VQAæ•°æ®é›†ä¸Šçš„å¾—åˆ†æé«˜äº†+13%ï¼Œå‡†ç¡®ç‡æé«˜äº†+12%ï¼Œåœ¨æ—¶é—´å®šä½ä»»åŠ¡Charades-STAä¸Šçš„mIoUæé«˜äº†+8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15556v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ReWindæ˜¯ä¸€ç§åŸºäºè®°å¿†çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆçš„é•¿è§†é¢‘ç†è§£å¹¶ä¿ç•™æ—¶é—´çœŸå®æ€§ã€‚å®ƒé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å­¦ä¹ è®°å¿†æ¨¡å—å’Œè‡ªé€‚åº”å¸§é€‰æ‹©æœºåˆ¶æ¥å¤„ç†é•¿è§†é¢‘ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è§†é¢‘å±•å¼€æ—¶å­˜å‚¨å’Œæ›´æ–°ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œå†…å­˜å†…å®¹ä¸è¾“å…¥æµä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›æ¥ç¡®ä¿ä½å†…å­˜è¦æ±‚ã€‚è‡ªé€‚åº”å¸§é€‰æ‹©æœºåˆ¶èƒ½å¤Ÿè¯†åˆ«ä¸æŒ‡ä»¤ç›¸å…³çš„å…³é”®æ—¶åˆ»ï¼Œä¸°å¯Œå†…å­˜è¡¨ç¤ºå¹¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚ReWindåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œåœ¨é•¿è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReWindæ˜¯ä¸€ä¸ªé’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„åŸºäºè®°å¿†çš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>ReWindé‡‡ç”¨ä¸¤é˜¶æ®µæ¡†æ¶å¤„ç†é•¿è§†é¢‘ï¼ŒåŒ…æ‹¬åŠ¨æ€å­¦ä¹ è®°å¿†æ¨¡å—å’Œè‡ªé€‚åº”å¸§é€‰æ‹©æœºåˆ¶ã€‚</li>
<li>åŠ¨æ€å­¦ä¹ è®°å¿†æ¨¡å—é€šè¿‡â€œè¯»-æ„ŸçŸ¥-å†™â€å¾ªç¯å­˜å‚¨å’Œæ›´æ–°ä¸æŒ‡ä»¤ç›¸å…³çš„è§†è§‰ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œå†…å­˜å†…å®¹ä¸è¾“å…¥æµä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›æ¥ç¡®ä¿ä½å†…å­˜è¦æ±‚ã€‚</li>
<li>è‡ªé€‚åº”å¸§é€‰æ‹©æœºåˆ¶èƒ½å¤Ÿè¯†åˆ«ä¸æŒ‡ä»¤ç›¸å…³çš„å…³é”®æ—¶åˆ»ï¼Œå¹¶ä¸°å¯Œå†…å­˜è¡¨ç¤ºã€‚</li>
<li>ReWindåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ—¶é—´å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e5aaac72fe0e4cc94083fef731ca70f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef67b504538f27f33dd71bf2084380d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07df5f433a3cb6ea8006bd39979762c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d539941970717e5bcaaa2dea8764a53.jpg" align="middle">
</details>




<h2 id="Beyond-Training-Dynamic-Token-Merging-for-Zero-Shot-Video-Understanding"><a href="#Beyond-Training-Dynamic-Token-Merging-for-Zero-Shot-Video-Understanding" class="headerlink" title="Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding"></a>Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding</h2><p><strong>Authors:Yiming Zhang, Zhuokai Zhao, Zhaorun Chen, Zenghui Ding, Xianjun Yang, Yining Sun</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have opened new avenues for video understanding. However, achieving high fidelity in zero-shot video tasks remains challenging. Traditional video processing methods rely heavily on fine-tuning to capture nuanced spatial-temporal details, which incurs significant data and computation costs. In contrast, training-free approaches, though efficient, often lack robustness in preserving context-rich features across complex video content. To this end, we propose DYTO, a novel dynamic token merging framework for zero-shot video understanding that adaptively optimizes token efficiency while preserving crucial scene details. DYTO integrates a hierarchical frame selection and a bipartite token merging strategy to dynamically cluster key frames and selectively compress token sequences, striking a balance between computational efficiency with semantic richness. Extensive experiments across multiple benchmarks demonstrate the effectiveness of DYTO, achieving superior performance compared to both fine-tuned and training-free methods and setting a new state-of-the-art for zero-shot video understanding. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•ä¸ºè§†é¢‘ç†è§£å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œåœ¨é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡ä¸­å®ç°é«˜ä¿çœŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿè§†é¢‘å¤„ç†æ–¹æ³•ä¾èµ–äºå¾®è°ƒæ¥æ•æ‰å¾®å¦™çš„æ—¶ç©ºç»†èŠ‚ï¼Œè¿™äº§ç”Ÿäº†å¤§é‡çš„æ•°æ®å’Œè®¡ç®—æˆæœ¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ— è®­ç»ƒçš„æ–¹æ³•è™½ç„¶æ•ˆç‡é«˜ï¼Œä½†åœ¨ä¿ç•™å¤æ‚è§†é¢‘å†…å®¹çš„ä¸°å¯Œä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢å¾€å¾€ç¼ºä¹ç¨³å¥æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DYTOï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé›¶æ ·æœ¬è§†é¢‘ç†è§£çš„æ–°å‹åŠ¨æ€ä»¤ç‰Œåˆå¹¶æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨ä¿ç•™å…³é”®åœºæ™¯ç»†èŠ‚çš„åŒæ—¶è‡ªé€‚åº”åœ°ä¼˜åŒ–ä»¤ç‰Œæ•ˆç‡ã€‚DYTOç»“åˆäº†åˆ†å±‚å¸§é€‰æ‹©å’ŒäºŒåˆ†ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œä»¥åŠ¨æ€èšç±»å…³é”®å¸§å¹¶é€‰æ‹©æ€§å‹ç¼©ä»¤ç‰Œåºåˆ—ï¼Œåœ¨è®¡ç®—æ•ˆç‡å’Œè¯­ä¹‰ä¸°å¯Œæ€§ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†DYTOçš„æœ‰æ•ˆæ€§ï¼Œä¸å¾®è°ƒå’Œæ— è®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼Œå®ƒå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶ä¸ºé›¶æ ·æœ¬è§†é¢‘ç†è§£è®¾å®šäº†æ–°çš„æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.14401v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘ç†è§£é¢†åŸŸçš„æ–°è¿›å±•ä¸­ï¼ŒåŠ¨æ€ä»¤ç‰Œåˆå¹¶æ¡†æ¶DYTOèƒ½æœ‰æ•ˆè§£å†³é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡çš„é«˜ä¿çœŸåº¦æŒ‘æˆ˜ã€‚DYTOé€šè¿‡åˆ†å±‚å¸§é€‰æ‹©å’ŒäºŒåˆ†ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œå®ç°å…³é”®å¸§çš„åŠ¨æ€èšç±»å’Œä»¤ç‰Œåºåˆ—çš„é€‰æ‹©æ€§å‹ç¼©ï¼Œå¹³è¡¡è®¡ç®—æ•ˆç‡å’Œè¯­ä¹‰ä¸°å¯Œæ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä¼ ç»Ÿå¾®è°ƒå’Œæ— è®­ç»ƒæ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œä¸ºé›¶æ ·æœ¬è§†é¢‘ç†è§£è®¾å®šäº†æ–°çš„æŠ€æœ¯é«˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸ºè§†é¢‘ç†è§£å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</li>
<li>é›¶æ ·æœ¬è§†é¢‘ä»»åŠ¡çš„é«˜ä¿çœŸåº¦å®ç°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¼ ç»Ÿè§†é¢‘å¤„ç†æ–¹æ³•ä¾èµ–å¾®è°ƒæ•æ‰ç»†å¾®æ—¶ç©ºç»†èŠ‚ï¼Œä½†æ•°æ®è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>è®­ç»ƒå…è´¹çš„æ–¹æ³•è™½ç„¶é«˜æ•ˆï¼Œä½†åœ¨å¤æ‚è§†é¢‘å†…å®¹ä¸­ç¼ºä¹ä¿æŒä¸°å¯Œä¸Šä¸‹æ–‡ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>DYTOæ˜¯ä¸€ä¸ªåŠ¨æ€ä»¤ç‰Œåˆå¹¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é›¶æ ·æœ¬è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ã€‚</li>
<li>DYTOé€šè¿‡åˆ†å±‚å¸§é€‰æ‹©å’ŒäºŒåˆ†ä»¤ç‰Œåˆå¹¶ç­–ç•¥ï¼Œå®ç°å…³é”®å¸§çš„åŠ¨æ€èšç±»å’Œä»¤ç‰Œåºåˆ—çš„é€‰æ‹©æ€§å‹ç¼©ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72c9a29c780dec6726b7a8a24badd503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e5621df29b631d04b41f2f4008ecb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fbe76c7f259c835d2bcc73917f5a53c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b9d9e6941ec357a74364aa23cb731e6.jpg" align="middle">
</details>




<h2 id="Principles-of-Visual-Tokens-for-Efficient-Video-Understanding"><a href="#Principles-of-Visual-Tokens-for-Efficient-Video-Understanding" class="headerlink" title="Principles of Visual Tokens for Efficient Video Understanding"></a>Principles of Visual Tokens for Efficient Video Understanding</h2><p><strong>Authors:Xinyue Hao, Gen Li, Shreyank N Gowda, Robert B Fisher, Jonathan Huang, Anurag Arnab, Laura Sevilla-Lara</strong></p>
<p>Video understanding has made huge strides in recent years, relying largely on the power of the transformer architecture. As this architecture is notoriously expensive and video is highly redundant, research into improving efficiency has become particularly relevant. This has led to many creative solutions, including token merging and token selection. While most methods succeed in reducing the cost of the model and maintaining accuracy, an interesting pattern arises: most methods do not outperform the random sampling baseline. In this paper we take a closer look at this phenomenon and make several observations. First, we develop an oracle for the value of tokens which exposes a clear Pareto distribution where most tokens have remarkably low value, and just a few carry most of the perceptual information. Second, we analyze why this oracle is extremely hard to learn, as it does not consistently coincide with visual cues. Third, we observe that easy videos need fewer tokens to maintain accuracy. We build on these and further insights to propose a lightweight video model we call LITE that can select a small number of tokens effectively, outperforming state-of-the-art and existing baselines across datasets (Kinetics400 and Something-Something-V2) in the challenging trade-off of computation (GFLOPs) vs accuracy. </p>
<blockquote>
<p>è§†é¢‘ç†è§£æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œè¿™ä¸»è¦å¾—ç›ŠäºTransformeræ¶æ„çš„å¼ºå¤§æ€§èƒ½ã€‚ç”±äºè¯¥æ¶æ„éå¸¸æ˜‚è´µä¸”è§†é¢‘å…·æœ‰é«˜åº¦å†—ä½™æ€§ï¼Œå…³äºæé«˜æ•ˆç‡çš„ç ”ç©¶å˜å¾—å°¤ä¸ºé‡è¦ã€‚è¿™å¯¼è‡´äº†è®¸å¤šåˆ›é€ æ€§çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä»¤ç‰Œåˆå¹¶å’Œä»¤ç‰Œé€‰æ‹©ã€‚è™½ç„¶å¤§å¤šæ•°æ–¹æ³•åœ¨é™ä½æ¨¡å‹æˆæœ¬çš„åŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ï¼Œä½†å‡ºç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šå¤§å¤šæ•°æ–¹æ³•å¹¶æ²¡æœ‰è¶…è¶ŠéšæœºæŠ½æ ·åŸºçº¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸€ç°è±¡è¿›è¡Œäº†æ·±å…¥ç ”ç©¶å¹¶è¿›è¡Œäº†å¤šæ¬¡è§‚å¯Ÿã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºä»¤ç‰Œçš„ä»·å€¼å¼€å‘äº†ä¸€ä¸ªoracleï¼Œå®ƒæš´éœ²äº†ä¸€ä¸ªæ¸…æ™°çš„å¸•ç´¯æ‰˜åˆ†å¸ƒï¼Œå…¶ä¸­å¤§å¤šæ•°ä»¤ç‰Œçš„ä»·å€¼ç›¸å½“ä½ï¼Œåªæœ‰å°‘æ•°ä»¤ç‰Œæºå¸¦äº†å¤§éƒ¨åˆ†çš„æ„ŸçŸ¥ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸ºä»€ä¹ˆè¿™ä¸ªoracleå¾ˆéš¾å­¦ä¹ ï¼Œå› ä¸ºå®ƒå¹¶ä¸æ€»æ˜¯ä¸è§†è§‰çº¿ç´¢ç›¸å»åˆã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç®€å•çš„è§†é¢‘åªéœ€è¦æ›´å°‘çš„ä»¤ç‰Œå°±èƒ½ç»´æŒå‡†ç¡®æ€§ã€‚æˆ‘ä»¬åŸºäºè¿™äº›è§‚å¯Ÿä»¥åŠå…¶ä»–è¿›ä¸€æ­¥çš„è§è§£ï¼Œæå‡ºäº†ä¸€ä¸ªè½»é‡çº§çš„è§†é¢‘æ¨¡å‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºLITEã€‚è¯¥æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é€‰æ‹©ä¸€å°éƒ¨åˆ†ä»¤ç‰Œï¼Œåœ¨æ•°æ®é›†ï¼ˆKinetics400å’ŒSomething-Something-V2ï¼‰çš„è®¡ç®—ï¼ˆGFLOPsï¼‰ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸”è¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯å’Œç°æœ‰åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13626v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘ç†è§£é¢†åŸŸè¿‘å¹´æ¥å–å¾—äº†å·¨å¤§è¿›å±•ï¼Œä¸»è¦ä¾èµ–äºTransformeræ¶æ„çš„åŠ›é‡ã€‚ç”±äºè¯¥æ¶æ„æˆæœ¬é«˜æ˜‚ä¸”è§†é¢‘å†—ä½™åº¦é«˜ï¼Œæé«˜æ•ˆç‡çš„ç ”ç©¶å˜å¾—å°¤ä¸ºé‡è¦ã€‚æœ¬æ–‡è§‚å¯Ÿåˆ°ä¸€ä¸ªç°è±¡ï¼šå°½ç®¡è®¸å¤šæ–¹æ³•æˆåŠŸé™ä½äº†æ¨¡å‹æˆæœ¬å¹¶ä¿æŒäº†å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¹¶ä¸ä¼˜äºéšæœºæŠ½æ ·åŸºçº¿ã€‚æœ¬æ–‡å¯¹æ­¤ç°è±¡è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¹¶å‘ç°å¤§å¤šæ•°ä»¤ç‰Œçš„ä»·å€¼æä½ï¼Œåªæœ‰å°‘æ•°æºå¸¦å¤§éƒ¨åˆ†æ„ŸçŸ¥ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åˆ†æäº†å­¦ä¹ è¿™ç§ä»¤ç‰Œä»·å€¼å›°éš¾çš„åŸå› ï¼Œå‘ç°å…¶å¹¶ä¸ä¸€è‡´åœ°ä¸è§†è§‰çº¿ç´¢ç›¸å»åˆã€‚åŒæ—¶ï¼Œæœ¬æ–‡è§‚å¯Ÿåˆ°ç®€å•çš„è§†é¢‘ç»´æŒå‡†ç¡®æ€§çš„ä»¤ç‰Œæ•°é‡æ›´å°‘ã€‚åŸºäºè¿™äº›å’Œå…¶ä»–è§è§£ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§çš„è§†é¢‘æ¨¡å‹LITEï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‰æ‹©å°‘é‡ä»¤ç‰Œï¼Œåœ¨æ•°æ®é›†ï¼ˆKinetics400å’ŒSomething-Something-V2ï¼‰ä¸Šå®ç°äº†è®¡ç®—ä¸å‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œä¼˜äºç°æœ‰åŸºçº¿å’Œæœ€æ–°æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£é¢†åŸŸä¾èµ–Transformeræ¶æ„å–å¾—è¿›å±•ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”è§†é¢‘å†—ä½™åº¦é«˜ã€‚</li>
<li>å¤§å¤šæ•°ä»¤ç‰Œçš„ä»·å€¼æä½ï¼Œåªæœ‰å°‘æ•°ä»¤ç‰Œæºå¸¦å¤§éƒ¨åˆ†æ„ŸçŸ¥ä¿¡æ¯ã€‚</li>
<li>å­¦ä¹ ä»¤ç‰Œä»·å€¼å›°éš¾çš„åŸå› åœ¨äºå…¶ä¸è§†è§‰çº¿ç´¢çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ç®€å•è§†é¢‘ç»´æŒå‡†ç¡®æ€§çš„ä»¤ç‰Œæ•°é‡æ›´å°‘ã€‚</li>
<li>LITEæ¨¡å‹èƒ½æœ‰æ•ˆé€‰æ‹©å°‘é‡ä»¤ç‰Œã€‚</li>
<li>LITEæ¨¡å‹åœ¨è®¡ç®—ä¸å‡†ç¡®æ€§ä¹‹é—´å®ç°äº†æƒè¡¡ï¼Œä¼˜äºç°æœ‰åŸºçº¿å’Œæœ€æ–°æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f040272feb5176758d01f1150eb284d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c7fa02576b76a49a2ae892b47ee73b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7ad48d3b11e8bb7ea6b84870431b030.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf98153d2ad7664c123895361d54c9cb.jpg" align="middle">
</details>




<h2 id="AdaCM-2-On-Understanding-Extremely-Long-Term-Video-with-Adaptive-Cross-Modality-Memory-Reduction"><a href="#AdaCM-2-On-Understanding-Extremely-Long-Term-Video-with-Adaptive-Cross-Modality-Memory-Reduction" class="headerlink" title="AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive   Cross-Modality Memory Reduction"></a>AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive   Cross-Modality Memory Reduction</h2><p><strong>Authors:Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin</strong></p>
<p>The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM$^2$, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œé€šè¿‡å°†LLMä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆï¼Œæ¨åŠ¨äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŸºäºLLMçš„æ¨¡å‹ï¼ˆä¾‹å¦‚VideoLLaMAã€VideoChatï¼‰ä»…é™äºå¤„ç†çŸ­æ—¶è§†é¢‘ã€‚æœ€è¿‘æœ‰äººå°è¯•é€šè¿‡æå–å’Œå‹ç¼©è§†è§‰ç‰¹å¾åˆ°å›ºå®šå†…å­˜å¤§å°æ¥ç†è§£é•¿è§†é¢‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åªåˆ©ç”¨è§†è§‰æ¨¡å¼æ¥åˆå¹¶è§†é¢‘ä»¤ç‰Œï¼Œå¿½è§†äº†è§†è§‰å’Œæ–‡æœ¬æŸ¥è¯¢ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¯¼è‡´éš¾ä»¥æœ‰æ•ˆå¤„ç†å¤æ‚çš„é—®ç­”ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³é•¿è§†é¢‘å’Œå¤æ‚æç¤ºçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdaCM$^2$ï¼Œå®ƒé¦–æ¬¡å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”è·¨æ¨¡æ€å†…å­˜ç¼©å‡æ–¹æ³•ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼åœ¨è§†é¢‘æµä¸Šè¿›è¡Œè§†é¢‘æ–‡æœ¬å¯¹é½ã€‚æˆ‘ä»¬åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œå¦‚è§†é¢‘æè¿°ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘åˆ†ç±»ï¼Œç»“æœè¡¨æ˜AdaCM$^2$åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å†…å­˜ä½¿ç”¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨LVUæ•°æ®é›†çš„å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°äº†4.5%çš„æ”¹è¿›ï¼ŒGPUå†…å­˜æ¶ˆè€—æœ€å¤šå‡å°‘äº†65%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12593v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ¨åŠ¨äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„è¿›æ­¥ï¼Œé€šè¿‡å°†LLMä¸è§†è§‰æ¨¡å‹ç»“åˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„LLMæ¨¡å‹ä¸»è¦ç”¨äºå¤„ç†çŸ­è§†é¢‘ã€‚å¯¹äºé•¿è§†é¢‘çš„è®¤çŸ¥ï¼Œç°æœ‰æ–¹æ³•ä»…åˆ©ç”¨è§†è§‰æ¨¡å¼åˆå¹¶è§†é¢‘ç¬¦å·ï¼Œå¿½ç•¥äº†è§†é¢‘ä¸æ–‡æœ¬æŸ¥è¯¢ä¹‹é—´çš„å…³è”ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚é—®ç­”ä»»åŠ¡ã€‚ä¸ºè§£å†³é•¿è§†é¢‘å’Œå¤æ‚æç¤ºçš„æŒ‘æˆ˜ï¼Œé¦–æ¬¡æå‡ºAdaCM$^2$æ¨¡å‹ï¼Œé‡‡ç”¨è‡ªé€‚åº”è·¨æ¨¡æ€è®°å¿†ç¼©å‡æ–¹æ³•ï¼Œä»¥è‡ªé€‚åº”æ–¹å¼å¯¹é½è§†é¢‘æ–‡æœ¬ã€‚å®éªŒè¯æ˜ï¼ŒAdaCM$^2$åœ¨å¤šä¸ªè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå¦‚è§†é¢‘æè¿°ã€è§†é¢‘é—®ç­”å’Œè§†é¢‘åˆ†ç±»ç­‰ï¼Œå¹¶åœ¨LVUæ•°æ®é›†ä¸Šå®ç°äº†é«˜è¾¾4.5%çš„æå‡ï¼ŒåŒæ—¶GPUå†…å­˜æ¶ˆè€—é™ä½äº†65%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¿ƒè¿›äº†è§†é¢‘ç†è§£ä»»åŠ¡çš„å‘å±•ã€‚</li>
<li>å½“å‰æ¨¡å‹ä¸»è¦å¤„ç†çŸ­è§†é¢‘ï¼Œå¯¹é•¿è§†é¢‘çš„è®¤çŸ¥å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿è§†é¢‘æ—¶ä»…åˆ©ç”¨è§†è§‰æ¨¡å¼ï¼Œå¿½ç•¥äº†è§†é¢‘ä¸æ–‡æœ¬æŸ¥è¯¢çš„å…³è”ã€‚</li>
<li>AdaCM$^2$æ¨¡å‹é¦–æ¬¡é‡‡ç”¨è‡ªé€‚åº”è·¨æ¨¡æ€è®°å¿†ç¼©å‡æ–¹æ³•å¤„ç†è§†é¢‘æ–‡æœ¬å¯¹é½ã€‚</li>
<li>AdaCM$^2$åœ¨å¤šä¸ªè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>AdaCM$^2$åœ¨LVUæ•°æ®é›†ä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f0513f4fdc11ae37c8faebc5d5107660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc24960bf530eefc9336f3b0bcc74bd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a4596d8c20d674c631830971b7f2215.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd31b8751dc54027d2cc91c17fbe87f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-212d066d1836dbd26fd50e84b5de62c1.jpg" align="middle">
</details>




<h2 id="DynFocus-Dynamic-Cooperative-Network-Empowers-LLMs-with-Video-Understanding"><a href="#DynFocus-Dynamic-Cooperative-Network-Empowers-LLMs-with-Video-Understanding" class="headerlink" title="DynFocus: Dynamic Cooperative Network Empowers LLMs with Video   Understanding"></a>DynFocus: Dynamic Cooperative Network Empowers LLMs with Video   Understanding</h2><p><strong>Authors:Yudong Han, Qingpei Guo, Liyuan Pan, Liu Liu, Yu Guan, Ming Yang</strong></p>
<p>The challenge in LLM-based video understanding lies in preserving visual and semantic information in long videos while maintaining a memory-affordable token count. However, redundancy and correspondence in videos have hindered the performance potential of existing methods. Through statistical learning on current datasets, we observe that redundancy occurs in both repeated and answer-irrelevant frames, and the corresponding frames vary with different questions. This suggests the possibility of adopting dynamic encoding to balance detailed video information preservation with token budget reduction. To this end, we propose a dynamic cooperative network, DynFocus, for memory-efficient video encoding in this paper. Specifically, i) a Dynamic Event Prototype Estimation (DPE) module to dynamically select meaningful frames for question answering; (ii) a Compact Cooperative Encoding (CCE) module that encodes meaningful frames with detailed visual appearance and the remaining frames with sketchy perception separately. We evaluate our method on five publicly available benchmarks, and experimental results consistently demonstrate that our method achieves competitive performance. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘ç†è§£æŒ‘æˆ˜åœ¨äºåœ¨é•¿è§†é¢‘ä¸­ä¿ç•™è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œä¿æŒå¯æ‰¿å—çš„ä»¤ç‰Œè®¡æ•°ã€‚ç„¶è€Œï¼Œè§†é¢‘ä¸­çš„å†—ä½™å’Œå¯¹åº”å…³ç³»é˜»ç¢äº†ç°æœ‰æ–¹æ³•çš„æ€§èƒ½æ½œåŠ›ã€‚é€šè¿‡å¯¹å½“å‰æ•°æ®é›†è¿›è¡Œç»Ÿè®¡å­¦ä¹ ï¼Œæˆ‘ä»¬å‘ç°å†—ä½™ç°è±¡æ—¢å‡ºç°åœ¨é‡å¤å¸§ä¸­ä¹Ÿå‡ºç°åœ¨ä¸ç­”æ¡ˆæ— å…³çš„å¸§ä¸­ï¼Œå¹¶ä¸”å¯¹åº”å¸§ä¼šå› ä¸åŒé—®é¢˜è€Œå˜åŒ–ã€‚è¿™æç¤ºæˆ‘ä»¬æœ‰å¯èƒ½é‡‡ç”¨åŠ¨æ€ç¼–ç æ¥å¹³è¡¡ä¿ç•™è¯¦ç»†çš„è§†é¢‘ä¿¡æ¯ä¸å‡å°‘ä»¤ç‰Œé¢„ç®—ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºå†…å­˜é«˜æ•ˆè§†é¢‘ç¼–ç çš„åŠ¨æ€ååŒç½‘ç»œDynFocusã€‚å…·ä½“æ¥è¯´ï¼Œä¸€æ˜¯å¯¹åŠ¨æ€äº‹ä»¶åŸå‹ä¼°è®¡ï¼ˆDPEï¼‰æ¨¡å—è¿›è¡ŒåŠ¨æ€é€‰æ‹©æœ‰æ„ä¹‰çš„å¸§ç”¨äºé—®ç­”ï¼›äºŒæ˜¯ç´§å‡‘ååŒç¼–ç ï¼ˆCCEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯¹æœ‰æ„ä¹‰çš„å¸§è¿›è¡Œè¯¦ç»†è§†è§‰å¤–è§‚ç¼–ç ï¼Œå¯¹å…¶ä½™å¸§è¿›è¡Œè‰å›¾æ„ŸçŸ¥ç¼–ç ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå…¬å¼€å¯ç”¨åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.12355v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘ç†è§£çš„æŒ‘æˆ˜åœ¨äºå¦‚ä½•åœ¨ä¿æŒè§†é¢‘è§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œæ§åˆ¶è®°å¿†è´Ÿæ‹…ã€‚ç°æœ‰æ–¹æ³•å—åˆ°å†—ä½™å’Œå¯¹åº”é—®é¢˜çš„å›°æ‰°ï¼Œæœ¬æ–‡é€šè¿‡å½“å‰æ•°æ®é›†è¿›è¡Œç»Ÿè®¡å­¦ä¹ ï¼Œå‘ç°å†—ä½™å­˜åœ¨äºé‡å¤å’Œä¸ç­”æ¡ˆæ— å…³çš„å¸§ä¸­ï¼Œå¯¹åº”å¸§å› ä¸åŒé—®é¢˜è€Œå¼‚ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºé‡‡ç”¨åŠ¨æ€ç¼–ç ä»¥å¹³è¡¡è§†é¢‘ä¿¡æ¯çš„è¯¦ç»†ä¿å­˜ä¸ä»¤ç‰Œé¢„ç®—çš„å‡å°‘ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€åä½œç½‘ç»œDynFocusç”¨äºå†…å­˜é«˜æ•ˆçš„è§†é¢‘ç¼–ç ã€‚å…·ä½“è€Œè¨€ï¼ŒåŒ…æ‹¬åŠ¨æ€äº‹ä»¶åŸå‹ä¼°è®¡æ¨¡å—ï¼ˆDPEï¼‰é€‰æ‹©å¯¹é—®ç­”æœ‰æ„ä¹‰çš„å¸§ï¼›ç´§å‡‘åˆä½œç¼–ç æ¨¡å—ï¼ˆCCEï¼‰åˆ†åˆ«å¯¹æœ‰æ„ä¹‰çš„å¸§è¿›è¡Œè¯¦ç»†çš„è§†è§‰å¤–è§‚ç¼–ç ï¼Œå¯¹å…¶ä½™å¸§è¿›è¡Œè‰å›¾æ„ŸçŸ¥ç¼–ç ã€‚åœ¨äº”ä¸ªå…¬å¼€åŸºå‡†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§†é¢‘ç†è§£çš„æŒ‘æˆ˜åœ¨äºä¿æŒè§†è§‰å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶æ§åˆ¶å†…å­˜è´Ÿæ‹…ã€‚</li>
<li>å†—ä½™å­˜åœ¨äºé‡å¤å’Œä¸ç­”æ¡ˆæ— å…³çš„å¸§ä¸­ï¼Œå½±å“ç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>ç»Ÿè®¡å­¦ä¹ å‘ç°ï¼Œä¸åŒé—®é¢˜å¯¹åº”çš„å¸§ä¸åŒã€‚</li>
<li>æå‡ºåŠ¨æ€ç¼–ç ä»¥å¹³è¡¡è§†é¢‘ä¿¡æ¯ä¿å­˜ä¸ä»¤ç‰Œé¢„ç®—å‡å°‘ã€‚</li>
<li>å¼•å…¥åŠ¨æ€åä½œç½‘ç»œDynFocusï¼ŒåŒ…æ‹¬åŠ¨æ€äº‹ä»¶åŸå‹ä¼°è®¡æ¨¡å—å’Œç´§å‡‘åˆä½œç¼–ç æ¨¡å—ã€‚</li>
<li>æ–¹æ³•åœ¨äº”ä¸ªå…¬å¼€åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¡¨ç°å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0d0dfcec92cb9464d775e6d057a6df6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bdab505c9cc3c004a5651e5cfb3fbaf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cef5e47be8b9d35e03f334aaa621af79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e648dccf97ee8adc54590c8808b02464.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b33ba376e29a5cf8588ef3b2c93df609.jpg" align="middle">
</details>




<h2 id="Motion-Grounded-Video-Reasoning-Understanding-and-Perceiving-Motion-at-Pixel-Level"><a href="#Motion-Grounded-Video-Reasoning-Understanding-and-Perceiving-Motion-at-Pixel-Level" class="headerlink" title="Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at   Pixel Level"></a>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at   Pixel Level</h2><p><strong>Authors:Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen</strong></p>
<p>In this paper, we introduce Motion-Grounded Video Reasoning, a new motion understanding task that requires generating visual answers (video segmentation masks) according to the input question, and hence needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal grounding work focusing on explicit action&#x2F;motion grounding, to a more general format by enabling implicit reasoning via questions. To facilitate the development of the new task, we collect a large-scale dataset called GROUNDMORE, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE uniquely requires models to generate visual answers, providing a more concrete and visually interpretable response than plain texts. It evaluates models on both spatiotemporal grounding and reasoning, fostering to address complex challenges in motion-related video reasoning, temporal perception, and pixel-level understanding. Furthermore, we introduce a novel baseline model named Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head. MORA achieves respectable performance on GROUNDMORE outperforming the best existing visual grounding baseline model by an average of 21.5% relatively. We hope this novel and challenging task will pave the way for future advancements in robust and general motion understanding via video reasoning segmentation </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºåŠ¨ä½œçš„è§†é¢‘æ¨ç†ï¼ˆMotion-Grounded Video Reasoningï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŠ¨ä½œç†è§£ä»»åŠ¡ã€‚å®ƒè¦æ±‚æ ¹æ®è¾“å…¥é—®é¢˜ç”Ÿæˆè§†è§‰ç­”æ¡ˆï¼ˆè§†é¢‘åˆ†å‰²æ©ç ï¼‰ï¼Œå› æ­¤éœ€è¦éšå¼çš„æ—¶ç©ºæ¨ç†å’Œå®šä½ã€‚è¿™ä¸ªä»»åŠ¡æ‰©å±•äº†ç°æœ‰çš„æ—¶ç©ºå®šä½å·¥ä½œï¼Œä»ä¾§é‡äºæ˜¾å¼åŠ¨ä½œ&#x2F;è¿åŠ¨å®šä½ï¼Œå‘å±•åˆ°äº†é€šè¿‡é—®é¢˜è¿›è¡Œéšå¼æ¨ç†çš„æ›´é€šç”¨æ ¼å¼ã€‚ä¸ºäº†ä¿ƒè¿›æ–°ä»»åŠ¡çš„å‘å±•ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†ï¼Œåä¸ºGROUNDMOREã€‚å®ƒåŒ…å«1715ä¸ªè§†é¢‘ç‰‡æ®µå’Œ24ä¸‡å¤šä¸ªå¯¹è±¡æ©ç ï¼Œè¿™äº›æ©ç æ˜¯ä¸“é—¨è®¾è®¡çš„ï¼ŒåŒ…æ‹¬å› æœã€é¡ºåºã€åäº‹å®å’Œæè¿°å››ç§ç±»å‹çš„é—®é¢˜ï¼Œç”¨äºè¯„ä¼°æ·±åº¦å’Œç»¼åˆè¿åŠ¨æ¨ç†èƒ½åŠ›ã€‚GROUNDMOREè¦æ±‚æ¨¡å‹ç”Ÿæˆè§†è§‰ç­”æ¡ˆï¼Œæä¾›äº†ä¸€ä¸ªæ¯”çº¯æ–‡æœ¬æ›´å…·ä½“å’Œè§†è§‰å¯è§£é‡Šçš„å“åº”ã€‚å®ƒè¯„ä¼°äº†æ¨¡å‹çš„æ—¶ç©ºå®šä½å’Œæ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ©äºè§£å†³ä¸è¿åŠ¨ç›¸å…³çš„è§†é¢‘æ¨ç†ã€æ—¶é—´æ„ŸçŸ¥å’Œåƒç´ çº§ç†è§£çš„å¤æ‚æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºå‡†æ¨¡å‹ï¼Œåä¸ºMotion-Grounded Video Reasoning Assistantï¼ˆMORAï¼‰ã€‚MORAç»“åˆäº†å¤šæ¨¡æ€LLMçš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€å®šä½æ¨¡å‹ï¼ˆSAMï¼‰çš„åƒç´ çº§æ„ŸçŸ¥èƒ½åŠ›å’Œè½»é‡åŒ–å®šä½å¤´çš„æ—¶åºæ„ŸçŸ¥èƒ½åŠ›ã€‚åœ¨GROUNDMOREä¸Šï¼ŒMORAå–å¾—äº†ä»¤äººå°Šæ•¬çš„ä¸šç»©ï¼Œå¹³å‡æ¯”ç°æœ‰çš„æœ€ä½³è§†è§‰å®šä½åŸºå‡†æ¨¡å‹é«˜å‡º21.5%ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ–°é¢–è€Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡å°†ä¸ºæœªæ¥é€šè¿‡è§†é¢‘æ¨ç†åˆ†å‰²è¿›è¡Œç¨³å¥å’Œé€šç”¨è¿åŠ¨ç†è§£çš„ç ”ç©¶é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09921v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Motion-Grounded Video Reasoningä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡éœ€è¦ä¾æ®è¾“å…¥é—®é¢˜ç”Ÿæˆè§†è§‰ç­”æ¡ˆï¼ˆè§†é¢‘åˆ†å‰²æ©è†œï¼‰ï¼Œéœ€è¦è¿›è¡Œéšå¼æ—¶ç©ºæ¨ç†å’Œå®šä½ã€‚ä¸ºæ¨è¿›æ­¤æ–°ä»»åŠ¡çš„å‘å±•ï¼Œä½œè€…æ”¶é›†äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†GROUNDMOREï¼ŒåŒ…å«1715ä¸ªè§†é¢‘ç‰‡æ®µå’Œ24.9ä¸‡ä¸ªå¯¹è±¡æ©è†œï¼Œç‰¹æ„è®¾è®¡äº†å››ç§é—®é¢˜ç±»å‹ï¼ˆå› æœã€é¡ºåºã€åäº‹å®å’Œæè¿°ï¼‰ä»¥è¯„ä¼°æ·±åº¦å’Œç»¼åˆè¿åŠ¨æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºçº¿æ¨¡å‹MORAï¼Œè¯¥æ¨¡å‹å…·å¤‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€åƒç´ çº§æ„ŸçŸ¥èƒ½åŠ›å’Œæ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œåœ¨GROUNDMOREä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºç°æœ‰æœ€ä½³è§†è§‰å®šä½åŸºçº¿æ¨¡å‹ï¼Œå¹³å‡æé«˜äº†21.5%çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†Motion-Grounded Video Reasoningä»»åŠ¡ï¼Œè¦æ±‚æ ¹æ®è¾“å…¥é—®é¢˜ç”Ÿæˆè§†è§‰ç­”æ¡ˆï¼Œéœ€è¦éšå¼æ—¶ç©ºæ¨ç†å’Œå®šä½ã€‚</li>
<li>æ¨å‡ºå¤§è§„æ¨¡æ•°æ®é›†GROUNDMOREï¼ŒåŒ…å«å¤šæ ·åŒ–è§†é¢‘ç‰‡æ®µå’Œå¯¹è±¡æ©è†œï¼Œç”¨äºè¯„ä¼°æ·±åº¦å’Œç»¼åˆè¿åŠ¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GROUNDMOREæ•°æ®é›†è¦æ±‚æ¨¡å‹ç”Ÿæˆè§†è§‰ç­”æ¡ˆï¼Œæä¾›æ¯”çº¯æ–‡æœ¬æ›´å…·ä½“å’Œå¯è§£é‡Šçš„ååº”ã€‚</li>
<li>MORAæ¨¡å‹å…·å¤‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€åƒç´ çº§æ„ŸçŸ¥èƒ½åŠ›å’Œæ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>MORAåœ¨GROUNDMOREæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>MORAç›¸è¾ƒäºç°æœ‰æœ€ä½³è§†è§‰å®šä½åŸºçº¿æ¨¡å‹ï¼Œå¹³å‡æé«˜äº†21.5%çš„æ€§èƒ½ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e921f3add5730f92f23ada93f3fb6180.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3833e65d0c0fe30b952a19b30d76334.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-724a29e16d554aa6e7bdf9b618f17b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e15da7c6ffbb290ee5c01476a07113a.jpg" align="middle">
</details>




<h2 id="HourVideo-1-Hour-Video-Language-Understanding"><a href="#HourVideo-1-Hour-Video-Language-Understanding" class="headerlink" title="HourVideo: 1-Hour Video-Language Understanding"></a>HourVideo: 1-Hour Video-Language Understanding</h2><p><strong>Authors:Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, CristÃ³bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei</strong></p>
<p>We present HourVideo, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (recall, tracking), visual reasoning (spatial, temporal, predictive, causal, counterfactual), and navigation (room-to-room, object retrieval) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features 12,976 high-quality, five-way multiple-choice questions. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at <a target="_blank" rel="noopener" href="https://hourvideo.stanford.edu/">https://hourvideo.stanford.edu</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºHourVideoï¼Œè¿™æ˜¯ä¸€é¡¹é’ˆå¯¹é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘è¯­è¨€ç†è§£çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä¸€ç»„æ–°é¢–çš„ä»»åŠ¡ç»„åˆï¼ŒåŒ…æ‹¬æ‘˜è¦ã€æ„ŸçŸ¥ï¼ˆå›å¿†ã€è·Ÿè¸ªï¼‰ã€è§†è§‰æ¨ç†ï¼ˆç©ºé—´ã€æ—¶é—´ã€é¢„æµ‹ã€å› æœã€åäº‹å®ï¼‰å’Œå¯¼èˆªï¼ˆæˆ¿é—´åˆ°æˆ¿é—´ã€å¯¹è±¡æ£€ç´¢ï¼‰ä»»åŠ¡ã€‚HourVideoåŒ…å«æ¥è‡ªEgo4Dæ•°æ®é›†çš„500ä¸ªæ‰‹åŠ¨ç­–åˆ’çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ï¼Œæ—¶é•¿ä»20åˆ†é’Ÿåˆ°120åˆ†é’Ÿä¸ç­‰ï¼Œå¹¶è®¾æœ‰é«˜è´¨é‡çš„äº”é€‰ä¸€çš„å¤šé¡¹é€‰æ‹©é¢˜å…±è®¡æœ‰ç‰¹è‰²æ–¹å¼ç»„å»ºæˆå¯¹å¥—è£…ä¸€ä¸‡äºŒåƒä¹ç™¾ä¸ƒåå…­ä»½è¯„ä¼°æ¨¡å‹å‘ç°å„ç§æ¨¡å‹åŸºçº¿æ–¹æ¡ˆéšç€å…¶ä»–å¤‡é€‰è·¯çº¿å¢å¼ºå¸¦æ¥äº†ç›¸è¾ƒäºéšæœºæŠ½å–æ”¹å–„çš„è½»å¾®æ”¹å˜å…¶ä¸­æœ€æ˜¾è‘—çš„ä¾‹å­å³ä¸ºåœ¨æ„å»ºç®€å•é€»è¾‘æ¨ç†ç”µè·¯è¿™ä¸ªä»»åŠ¡åœºæ™¯ä¸­åŒ…å æ˜Ÿæ§å‘æŒ¥äº†è‡ªå·±å¼ºåŠ¿çš„ä¸€é¢å¯¹æ‰‹æ——èˆ°å‹å·Gemini Proåœ¨å¹³å‡æ­£ç¡®ç‡ä¸Šçš„ç»“æœè¾ƒå·®è¾¾åˆ°å°†è¿‘å·®äº”ç‚¹äº”å€æ•ˆèƒ½ç›¸æ¯”è¾ƒç»“æœæƒ¨æ·¡çš„å±€é¢å±•ç°äº†AIäº§å“åœ¨æ¨¡æ€é¢†åŸŸä»ç„¶æœ‰ç€å·¨å¤§å·®è·æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¯„ä¼°å·¥å…·æç¤ºå’Œæ–‡æ¡£å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://hourvideo.stanford.eduæ‰¾åˆ°./">https://hourvideo.stanford.eduæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04998v1">PDF</a> NeurIPS 2024 Datasets and Benchmarks Track; 28 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†HourVideoæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ˜¯ç”¨äºä¸€å°æ—¶è§†é¢‘è¯­è¨€ç†è§£çš„åŸºå‡†æ•°æ®é›†ã€‚å®ƒåŒ…å«å¤šä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬æ‘˜è¦ã€æ„ŸçŸ¥ï¼ˆå›å¿†ã€è·Ÿè¸ªï¼‰ã€è§†è§‰æ¨ç†ï¼ˆç©ºé—´ã€æ—¶é—´ã€é¢„æµ‹ã€å› æœã€åäº‹å®ï¼‰å’Œå¯¼èˆªï¼ˆæˆ¿é—´å¯¹æˆ¿é—´ã€å¯¹è±¡æ£€ç´¢ï¼‰ã€‚HourVideoåŒ…æ‹¬æ¥è‡ªEgo4Dæ•°æ®é›†çš„500ä¸ªæ‰‹åŠ¨ç­–åˆ’çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ï¼Œæ—¶é•¿åœ¨20è‡³120åˆ†é’Ÿä¹‹é—´ï¼Œå«æœ‰é«˜è´¨é‡äº”é€‰ä¸€é€‰æ‹©é¢˜å…±ä¸€ä¸‡äºŒåƒä¹ç™¾ä¸ƒåå…­é¢˜ã€‚åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ˜ï¼ŒåŒ…æ‹¬GPT-4å’ŒLLaVA-NeXTåœ¨å†…çš„å¤šæ¨¡å¼æ¨¡å‹è¾ƒéšæœºç»“æœä»…ç¨æœ‰æ”¹å–„ã€‚ç›¸è¾ƒæœ€å…ˆè¿›çš„é•¿æœŸä¸Šä¸‹æ–‡å¤šæ¨¡å¼æ¨¡å‹åŒå­åº§Pro 1.5è€Œè¨€ï¼Œäººç±»ä¸“å®¶è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚çš„ä¼˜å¼‚æ€§èƒ½ï¼ˆå‡†ç¡®ç‡ç”±85%å¯¹æ¯”è‡³ä»…çº¦ä¸‰åˆ†ä¹‹ä¸€ï¼‰ï¼Œæ­ç¤ºäº†å¤šæ¨¡å¼èƒ½åŠ›å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ã€è¯„ä¼°å·¥å…·åŒ…ã€æç¤ºä»¥åŠæ–‡æ¡£å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="http://hourvideo.stanford.eduæŸ¥é˜…./">http://hourvideo.stanford.eduæŸ¥é˜…ã€‚</a></p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡æ‰€åˆ—çš„ä¸ƒç‚¹ä¸»è¦æ´è§ï¼š</p>
<ul>
<li>ä»‹ç»HourVideoæ•°æ®é›†ï¼Œå®ƒæ˜¯ç”¨äºä¸€å°æ—¶è§†é¢‘è¯­è¨€ç†è§£çš„åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>HourVideoæ•°æ®é›†åŒ…å«å¤šç§ä»»åŠ¡ï¼Œæ¶µç›–æ‘˜è¦æ’°å†™ã€æ„ŸçŸ¥ç†è§£ç­‰å¤šå…ƒæŠ€èƒ½æŒ‘æˆ˜ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f63e86d42ef48f3a16751d7995f917b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de52d4d7a2a44f9bdddf9bf6342a6f9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b176b2d35355a677f04660df8484dcac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a5e668a7fd69f897d2adbbd06588e3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b50ad4787cc10848d069911879728997.jpg" align="middle">
</details>




<h2 id="StreamingBench-Assessing-the-Gap-for-MLLMs-to-Achieve-Streaming-Video-Understanding"><a href="#StreamingBench-Assessing-the-Gap-for-MLLMs-to-Achieve-Streaming-Video-Understanding" class="headerlink" title="StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video   Understanding"></a>StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video   Understanding</h2><p><strong>Authors:Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, Maosong Sun</strong></p>
<p>The rapid development of Multimodal Large Language Models (MLLMs) has expanded their capabilities from image comprehension to video understanding. However, most of these MLLMs focus primarily on offline video comprehension, necessitating extensive processing of all video frames before any queries can be made. This presents a significant gap compared to the human ability to watch, listen, think, and respond to streaming inputs in real time, highlighting the limitations of current MLLMs. In this paper, we introduce StreamingBench, the first comprehensive benchmark designed to evaluate the streaming video understanding capabilities of MLLMs. StreamingBench assesses three core aspects of streaming video understanding: (1) real-time visual understanding, (2) omni-source understanding, and (3) contextual understanding. The benchmark consists of 18 tasks, featuring 900 videos and 4,500 human-curated QA pairs. Each video features five questions presented at different time points to simulate a continuous streaming scenario. We conduct experiments on StreamingBench with 13 open-source and proprietary MLLMs and find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and GPT-4o perform significantly below human-level streaming video understanding capabilities. We hope our work can facilitate further advancements for MLLMs, empowering them to approach human-level video comprehension and interaction in more realistic scenarios. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶èƒ½åŠ›å·²ä»å›¾åƒç†è§£æ‰©å±•åˆ°è§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MLLMä¸»è¦å…³æ³¨ç¦»çº¿è§†é¢‘ç†è§£ï¼Œéœ€è¦åœ¨æå‡ºä»»ä½•æŸ¥è¯¢ä¹‹å‰å¯¹æ‰€æœ‰è§†é¢‘å¸§è¿›è¡Œå¤§é‡å¤„ç†ã€‚è¿™ä¸äººç±»å®æ—¶è§‚çœ‹ã€è†å¬ã€æ€è€ƒå’Œå›åº”æµåª’ä½“è¾“å…¥çš„èƒ½åŠ›ç›¸æ¯”å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œçªæ˜¾äº†å½“å‰MLLMçš„å±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†StreamingBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMçš„æµåª’ä½“è§†é¢‘ç†è§£èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚StreamingBenchè¯„ä¼°æµåª’ä½“ç†è§£çš„ä¸‰ä¸ªæ ¸å¿ƒæ–¹é¢ï¼šï¼ˆ1ï¼‰å®æ—¶è§†è§‰ç†è§£ã€ï¼ˆ2ï¼‰å…¨æºç†è§£å’Œï¼ˆ3ï¼‰ä¸Šä¸‹æ–‡ç†è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«18é¡¹ä»»åŠ¡ï¼Œä»¥900ä¸ªè§†é¢‘å’Œ4500ä¸ªäººå·¥ç­–åˆ’çš„é—®ç­”å¯¹ä¸ºç‰¹è‰²ã€‚æ¯ä¸ªè§†é¢‘éƒ½åœ¨ä¸åŒçš„æ—¶é—´ç‚¹å‘ˆç°äº”ä¸ªé—®é¢˜ï¼Œä»¥æ¨¡æ‹Ÿè¿ç»­çš„æµåª’ä½“åœºæ™¯ã€‚æˆ‘ä»¬åœ¨StreamingBenchä¸Šè¿›è¡Œäº†å®éªŒï¼Œä½¿ç”¨äº†åŒ…æ‹¬å¼€æºå’Œä¸“æœ‰åœ¨å†…çš„å…±13ç§MLLMï¼Œå‘ç°æœ€å…ˆè¿›çš„ä¸“æœ‰MLLMå¦‚Gemini 1.5 Proå’ŒGPT-4oåœ¨æµåª’ä½“è§†é¢‘ç†è§£èƒ½åŠ›æ–¹é¢ä¸äººç±»æ°´å¹³å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿä¿ƒè¿›MLLMçš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´ç°å®çš„åœºæ™¯ä¸­æ¥è¿‘äººç±»æ°´å¹³çš„è§†é¢‘ç†è§£å’Œäº¤äº’èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03628v1">PDF</a> </p>
<p><strong>Summary</strong><br>     éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå…¶èƒ½åŠ›å·²ä»å›¾åƒç†è§£æ‰©å±•åˆ°è§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°MLLMsä¸»è¦å…³æ³¨ç¦»çº¿è§†é¢‘ç†è§£ï¼Œåœ¨æå‡ºä»»ä½•æŸ¥è¯¢ä¹‹å‰éœ€è¦å¯¹æ‰€æœ‰è§†é¢‘å¸§è¿›è¡Œå¤§é‡å¤„ç†ï¼Œè¿™ä¸äººèƒ½å¤Ÿå®æ—¶è§‚çœ‹ã€è†å¬ã€æ€è€ƒå’Œå›åº”æµåª’ä½“è¾“å…¥çš„èƒ½åŠ›ç›¸æ¯”å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æœ¬æ–‡ä»‹ç»äº†StreamingBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°MLLMsæµåª’ä½“è§†é¢‘ç†è§£èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚StreamingBenchè¯„ä¼°æµåª’ä½“è§†é¢‘ç†è§£çš„æ ¸å¿ƒä¸‰ä¸ªæ–¹é¢ï¼šå®æ—¶è§†è§‰ç†è§£ã€å…¨æºç†è§£å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«18ä¸ªä»»åŠ¡ï¼Œæ¶‰åŠ900ä¸ªè§†é¢‘å’Œ4500ä¸ªäººå·¥åˆ¶ä½œçš„é—®ç­”å¯¹ã€‚æ¯ä¸ªè§†é¢‘åœ¨ä¸åŒæ—¶é—´ç‚¹å‘ˆç°äº”ä¸ªé—®é¢˜ï¼Œä»¥æ¨¡æ‹Ÿè¿ç»­æµåª’ä½“åœºæ™¯ã€‚åœ¨StreamingBenchä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„ä¸“æœ‰MLLMsï¼Œå¦‚Gemini 1.5 Proå’ŒGPT-4oï¼Œåœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„è¡¨ç°ä¹Ÿè¿œè¿œä½äºäººç±»æ°´å¹³ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™é¡¹å·¥ä½œä¿ƒè¿›MLLMsçš„è¿›ä¸€æ­¥å‘å±•ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ›´ç°å®çš„åœºæ™¯ä¸­å®ç°æ¥è¿‘äººç±»æ°´å¹³çš„è§†é¢‘ç†è§£å’Œäº¤äº’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²èƒ½ä»å›¾åƒç†è§£æ‰©å±•åˆ°è§†é¢‘ç†è§£ã€‚</li>
<li>å½“å‰çš„MLLMsä¸»è¦å…³æ³¨ç¦»çº¿è§†é¢‘ç†è§£ï¼Œå¹¶éœ€é¢„å…ˆå¤„ç†æ‰€æœ‰è§†é¢‘å¸§ï¼Œè¿™ä¸äººç±»å®æ—¶å¤„ç†æµåª’ä½“çš„èƒ½åŠ›å­˜åœ¨å·®è·ã€‚</li>
<li>StreamingBenchæ˜¯é¦–ä¸ªè¯„ä¼°MLLMsåœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„èƒ½åŠ›çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚</li>
<li>StreamingBenchè¯„ä¼°äº†æµåª’ä½“è§†é¢‘ç†è§£çš„ä¸‰ä¸ªæ ¸å¿ƒæ–¹é¢ï¼šå®æ—¶è§†è§‰ç†è§£ã€å…¨æºç†è§£å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„MLLMsï¼Œåœ¨æµåª’ä½“è§†é¢‘ç†è§£æ–¹é¢çš„è¡¨ç°ä¹Ÿè¿œä½äºäººç±»æ°´å¹³ã€‚</li>
<li>StreamingBenchåŒ…å«18ä¸ªä»»åŠ¡ï¼Œ900ä¸ªè§†é¢‘å’Œ4500ä¸ªäººå·¥åˆ¶ä½œçš„é—®ç­”å¯¹ï¼Œä»¥æ¨¡æ‹Ÿè¿ç»­æµåª’ä½“åœºæ™¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-16506a4ce647ff934940d33abe92f356.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d028989d68c85414c19b01e255b4282f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b91de720cd002a4878e4fef0c59ac01d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7aab84f487bc869456696c59efb20c5.jpg" align="middle">
</details>




<h2 id="Personalized-Video-Summarization-by-Multimodal-Video-Understanding"><a href="#Personalized-Video-Summarization-by-Multimodal-Video-Understanding" class="headerlink" title="Personalized Video Summarization by Multimodal Video Understanding"></a>Personalized Video Summarization by Multimodal Video Understanding</h2><p><strong>Authors:Brian Chen, Xiangyuan Zhao, Yingnan Zhu</strong></p>
<p>Video summarization techniques have been proven to improve the overall user experience when it comes to accessing and comprehending video content. If the userâ€™s preference is known, video summarization can identify significant information or relevant content from an input video, aiding them in obtaining the necessary information or determining their interest in watching the original video. Adapting video summarization to various types of video and user preferences requires significant training data and expensive human labeling. To facilitate such research, we proposed a new benchmark for video summarization that captures various user preferences. Also, we present a pipeline called Video Summarization with Language (VSL) for user-preferred video summarization that is based on pre-trained visual language models (VLMs) to avoid the need to train a video summarization system on a large training dataset. The pipeline takes both video and closed captioning as input and performs semantic analysis at the scene level by converting video frames into text. Subsequently, the userâ€™s genre preference was used as the basis for selecting the pertinent textual scenes. The experimental results demonstrate that our proposed pipeline outperforms current state-of-the-art unsupervised video summarization models. We show that our method is more adaptable across different datasets compared to supervised query-based video summarization models. In the end, the runtime analysis demonstrates that our pipeline is more suitable for practical use when scaling up the number of user preferences and videos. </p>
<blockquote>
<p>è§†é¢‘æ‘˜è¦æŠ€æœ¯å·²è¢«è¯æ˜åœ¨è®¿é—®å’Œç†è§£è§†é¢‘å†…å®¹æ—¶ï¼Œèƒ½å¤Ÿæå‡æ•´ä½“ç”¨æˆ·ä½“éªŒã€‚å¦‚æœäº†è§£ç”¨æˆ·çš„åå¥½ï¼Œè§†é¢‘æ‘˜è¦å¯ä»¥ä»è¾“å…¥çš„è§†é¢‘ä¸­è¯†åˆ«å‡ºé‡è¦ä¿¡æ¯æˆ–ç›¸å…³å†…å®¹ï¼Œå¸®åŠ©ç”¨æˆ·è·å¾—å¿…è¦çš„ä¿¡æ¯æˆ–ç¡®å®šä»–ä»¬å¯¹åŸå§‹è§†é¢‘çš„å…´è¶£ã€‚é€‚åº”å„ç§ç±»å‹è§†é¢‘å’Œç”¨æˆ·åå¥½çš„è§†é¢‘æ‘˜è¦éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œæ˜‚è´µçš„äººå·¥æ ‡æ³¨ã€‚ä¸ºäº†æ¨åŠ¨ç›¸å…³ç ”ç©¶ï¼Œæˆ‘ä»¬ä¸ºè§†é¢‘æ‘˜è¦æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•èƒ½å¤Ÿæ•æ‰å„ç§ç”¨æˆ·åå¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„ç”¨æˆ·åå¥½è§†é¢‘æ‘˜è¦ç®¡é“ï¼ˆVideo Summarization with Language (VSL)ï¼‰ã€‚è¯¥ç®¡é“é¿å…äº†éœ€è¦åœ¨å¤§é‡è®­ç»ƒæ•°æ®é›†ä¸Šè®­ç»ƒè§†é¢‘æ‘˜è¦ç³»ç»Ÿçš„éœ€æ±‚ï¼Œä»¥è§†é¢‘å’Œå­—å¹•ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡å°†è§†é¢‘å¸§è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œåœ¨åœºæ™¯çº§åˆ«è¿›è¡Œè¯­ä¹‰åˆ†æã€‚ç„¶åï¼Œä½¿ç”¨ç”¨æˆ·çš„ç±»å‹åå¥½ä½œä¸ºé€‰æ‹©ç›¸å…³æ–‡æœ¬åœºæ™¯çš„åŸºç¡€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç®¡é“ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ— ç›‘ç£è§†é¢‘æ‘˜è¦æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šæ¯”ç›‘ç£æŸ¥è¯¢åŸºäºçš„è§†é¢‘æ‘˜è¦æ¨¡å‹æ›´å…·é€‚åº”æ€§ã€‚æœ€åï¼Œè¿è¡Œæ—¶é—´åˆ†æè¡¨æ˜ï¼Œå½“æ‰©å¤§ç”¨æˆ·åå¥½å’Œè§†é¢‘æ•°é‡æ—¶ï¼Œæˆ‘ä»¬çš„ç®¡é“æ›´é€‚åˆå®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03531v1">PDF</a> In Proceedings of CIKM 2024 Applied Research Track</p>
<p><strong>Summary</strong><br>è§†é¢‘æ‘˜è¦æŠ€æœ¯èƒ½æå‡ç”¨æˆ·è®¿é—®å’Œç†è§£è§†é¢‘å†…å®¹çš„ä½“éªŒã€‚é€šè¿‡è¯†åˆ«ç”¨æˆ·åå¥½ï¼Œè§†é¢‘æ‘˜è¦å¯ä»è¾“å…¥è§†é¢‘ä¸­è¯†åˆ«é‡è¦ä¿¡æ¯æˆ–ç›¸å…³å†…å®¹ï¼Œå¸®åŠ©ç”¨æˆ·è·å–å¿…è¦ä¿¡æ¯æˆ–å†³å®šæ˜¯å¦éœ€è¦è§‚çœ‹åŸè§†é¢‘ã€‚ä¸ºåº”å¯¹ä¸åŒè§†é¢‘ç±»å‹å’Œç”¨æˆ·éœ€æ±‚ï¼Œéœ€å¤§é‡è®­ç»ƒæ•°æ®å’Œæ˜‚è´µçš„äººåŠ›æ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†æ–°çš„è§†é¢‘æ‘˜è¦åŸºå‡†æµ‹è¯•ï¼Œåæ˜ å„ç§ç”¨æˆ·åå¥½ï¼Œå¹¶æå‡ºåŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹çš„VSLç®¡é“ï¼Œæ— éœ€åœ¨å¤§è®­ç»ƒæ•°æ®é›†ä¸Šè®­ç»ƒè§†é¢‘æ‘˜è¦ç³»ç»Ÿã€‚ç®¡é“åŒæ—¶å¤„ç†è§†é¢‘å’Œå­—å¹•ï¼Œé€šè¿‡åœºæ™¯çº§åˆ«çš„è¯­ä¹‰åˆ†æå°†è§†é¢‘å¸§è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå¹¶æ ¹æ®ç”¨æˆ·å–œå¥½é€‰æ‹©ç›¸å…³æ–‡æœ¬åœºæ™¯ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„ç®¡é“æ€§èƒ½ä¼˜äºå½“å‰å…ˆè¿›çš„æ— ç›‘ç£è§†é¢‘æ‘˜è¦æ¨¡å‹ï¼Œä¸”æ¯”ç›‘ç£æŸ¥è¯¢å‹è§†é¢‘æ‘˜è¦æ¨¡å‹æ›´é€‚ç”¨äºä¸åŒæ•°æ®é›†ã€‚è¿è¡Œåˆ†æè¡¨æ˜ï¼Œåœ¨ç”¨æˆ·åå¥½å’Œè§†é¢‘æ•°é‡å¢åŠ æ—¶ï¼Œæˆ‘ä»¬çš„ç®¡é“æ›´é€‚åˆå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‘˜è¦æŠ€æœ¯å¯ä»¥å¢å¼ºç”¨æˆ·ç†è§£å’Œä½“éªŒè§†é¢‘å†…å®¹çš„æ•ˆæœã€‚</li>
<li>é€šè¿‡è¯†åˆ«ç”¨æˆ·åå¥½ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°ä»è§†é¢‘ä¸­æå–å…³é”®ä¿¡æ¯ã€‚</li>
<li>é€‚åº”ä¸åŒç±»å‹çš„è§†é¢‘å’Œç”¨æˆ·éœ€æ±‚çš„è§†é¢‘æ‘˜è¦éœ€è¦å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®å’Œæ ‡æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘æ‘˜è¦åŸºå‡†æµ‹è¯•ï¼Œåæ˜ ä¸åŒçš„ç”¨æˆ·åå¥½ã€‚</li>
<li>å¼•å…¥VSLç®¡é“ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œç”¨æˆ·åå¥½çš„è§†é¢‘æ‘˜è¦ã€‚</li>
<li>VSLç®¡é“é€šè¿‡åœºæ™¯çº§åˆ«çš„è¯­ä¹‰åˆ†æå¤„ç†è§†é¢‘å’Œå­—å¹•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cdc729ba164c57eb03d047f197497b30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66400efff66bfc36bef1d430ddb9c869.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25dbdd2bea0668f25714b07fef0264fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e56c6675197bc1032701c26533d585b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5eda7f993de600069b719702871b489.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-894450163221f19de27c19f8fd92d8c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b35b951069bae025d8d9336ec99562b3.jpg" align="middle">
</details>




<h2 id="PPLLaVA-Varied-Video-Sequence-Understanding-With-Prompt-Guidance"><a href="#PPLLaVA-Varied-Video-Sequence-Understanding-With-Prompt-Guidance" class="headerlink" title="PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance"></a>PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance</h2><p><strong>Authors:Ruyang Liu, Haoran Tang, Haibo Liu, Yixiao Ge, Ying Shan, Chen Li, Jiankun Yang</strong></p>
<p>The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the userâ€™s instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at <a target="_blank" rel="noopener" href="https://github.com/farewellthree/PPLLaVA">https://github.com/farewellthree/PPLLaVA</a>. </p>
<blockquote>
<p>è¿‡å»ä¸€å¹´ï¼ŒåŸºäºè§†é¢‘çš„çš„å¤§å‹è¯­è¨€æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¼€å‘ä¸€ç§åŒæ—¶é€‚ç”¨äºçŸ­è§†é¢‘å’Œé•¿è§†é¢‘ç†è§£çš„ç»Ÿä¸€æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„è§†é¢‘LLMæ— æ³•å¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼Œè€Œé’ˆå¯¹é•¿è§†é¢‘çš„å®šåˆ¶æ–¹æ³•å¯¹äºçŸ­è§†é¢‘å’Œå›¾åƒåˆ™å¾€å¾€æ•ˆæœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å…³é”®é—®é¢˜ç¡®å®šä¸ºè§†é¢‘ä¸­çš„å†—ä½™å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ± åŒ–ç­–ç•¥ï¼Œå¯ä»¥åŒæ—¶å®ç°ä»¤ç‰Œå‹ç¼©å’ŒæŒ‡ä»¤æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾èšåˆã€‚æˆ‘ä»¬çš„æ¨¡å‹è¢«ç§°ä¸ºPromptå¼•å¯¼æ± åŒ–LLaVAï¼Œç®€ç§°PPLLaVAã€‚å…·ä½“æ¥è¯´ï¼ŒPPLLaVAåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºCLIPçš„è§†è§‰æç¤ºå¯¹é½ï¼Œç”¨äºæå–ä¸ç”¨æˆ·æŒ‡ä»¤ç›¸å…³çš„è§†è§‰ä¿¡æ¯ï¼›æç¤ºå¼•å¯¼æ± åŒ–ï¼Œä½¿ç”¨å·ç§¯å¼æ± åŒ–å°†è§†è§‰åºåˆ—å‹ç¼©åˆ°ä»»æ„è§„æ¨¡ï¼›ä»¥åŠé’ˆå¯¹è§†è§‰å¯¹è¯ä¸­å¸¸è§å†—é•¿æç¤ºè®¾è®¡çš„CLIPä¸Šä¸‹æ–‡æ‰©å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä»£ç åº“è¿˜é›†æˆäº†æœ€å…ˆè¿›çš„è§†é¢‘ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œè§†è§‰äº¤æ›¿è®­ç»ƒã€‚å¤§é‡å®éªŒéªŒè¯äº†æ¨¡å‹æ€§èƒ½ã€‚PPLLaVAå…·æœ‰å‡ºè‰²çš„ååé‡å’Œä»…ä½¿ç”¨1024ä¸ªè§†è§‰ä¸Šä¸‹æ–‡ï¼Œä½œä¸ºè§†é¢‘LLMåœ¨å›¾åƒåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ“…é•¿ä»ç”Ÿæˆå­—å¹•åˆ°å¤šé¡¹é€‰æ‹©é¢˜çš„å„ç§ä»»åŠ¡ï¼Œå¹¶èƒ½å¤„ç†ä»å‡ ç§’åˆ°å‡ å°æ—¶çš„è§†é¢‘é•¿åº¦ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/farewellthree/PPLLaVA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/farewellthree/PPLLaVAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02327v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹è§†é¢‘ç†è§£çš„æ–°å‹ç»Ÿä¸€æ¨¡å‹PPLLaVAï¼Œé€šè¿‡è¯†åˆ«è§†é¢‘å†—ä½™å†…å®¹çš„é—®é¢˜å¹¶å¼•å…¥æ–°çš„æ± åŒ–ç­–ç•¥ï¼Œå®ç°äº†å¯¹é•¿çŸ­è§†é¢‘çš„å…¨é¢ç†è§£ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºCLIPçš„è§†è§‰æç¤ºå¯¹é½ã€å¼•å¯¼å¼æ± åŒ–å’Œç”¨äºè§†é¢‘å¯¹è¯çš„å‰ªè¾‘ä¸Šä¸‹æ–‡æ‰©å±•ã€‚æ­¤å¤–ï¼Œå®ƒç»“åˆäº†æœ€æ–°çš„è§†é¢‘ç›´æ¥åå¥½ä¼˜åŒ–å’Œè§†è§‰äº¤æ›¿è®­ç»ƒæŠ€æœ¯ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜è¶Šï¼Œä»…ä½¿ç”¨å°‘é‡è§†è§‰ä¸Šä¸‹æ–‡å°±èƒ½åœ¨å¤„ç†è§†é¢‘é•¿çŸ­ä¸åŒä»»åŠ¡ä¸Šè¾¾åˆ°é¡¶å°–è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç†è§£çš„ç°çŠ¶å’ŒæŒ‘æˆ˜ï¼šå½“å‰è§†é¢‘å¤§è¯­è¨€æ¨¡å‹åœ¨é•¿çŸ­è§†é¢‘ç†è§£ä¸Šå­˜åœ¨ç»Ÿä¸€æ€§é—®é¢˜ã€‚å¤§å¤šæ•°æ¨¡å‹æ— æ³•å¤„ç†é•¿æ—¶é—´è§†é¢‘ï¼Œè€Œé’ˆå¯¹é•¿è§†é¢‘çš„ç‰¹å®šæ–¹æ³•å¯¹äºçŸ­è§†é¢‘å’Œå›¾åƒåˆ™æ•ˆæœä¸ä½³ã€‚</li>
<li>æ ¸å¿ƒé—®é¢˜è¯†åˆ«ï¼šè§†é¢‘å†—ä½™å†…å®¹æ˜¯å¯¼è‡´æ¨¡å‹è¡¨ç°ä¸ä½³çš„å…³é”®é—®é¢˜ã€‚</li>
<li>PPLLaVAæ¨¡å‹åŠå…¶ç»„ä»¶ï¼šä»‹ç»æ–°å‹æ¨¡å‹PPLLaVAåŠå…¶ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶â€”â€”åŸºäºCLIPçš„è§†è§‰æç¤ºå¯¹é½ã€å¼•å¯¼å¼æ± åŒ–å’Œç”¨äºè§†é¢‘å¯¹è¯çš„å‰ªè¾‘ä¸Šä¸‹æ–‡æ‰©å±•ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼šç»“åˆæœ€æ–°çš„è§†é¢‘ç›´æ¥åå¥½ä¼˜åŒ–å’Œè§†è§‰äº¤æ›¿è®­ç»ƒæŠ€æœ¯æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœï¼šPPLLaVAæ¨¡å‹æ€§èƒ½ä¼˜è¶Šï¼Œèƒ½åœ¨ä»…ä½¿ç”¨å°‘é‡è§†è§‰ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œåœ¨å¤„ç†é•¿çŸ­ä¸ä¸€çš„è§†é¢‘ä»»åŠ¡ä¸Šè¾¾åˆ°é¡¶å°–è¡¨ç°ã€‚</li>
<li>æ¨¡å‹å¯ç”¨æ€§å’Œä»£ç å…±äº«ï¼šPPLLaVAæ¨¡å‹çš„ä»£ç å·²å…¬å¼€åˆ†äº«åœ¨GitHubä¸Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8c67eeba32950d5b35f86b9c95a602ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adfd7b1728325290e63efaf7113160e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a301c2f5f1474b56beec3c476d038b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7d7531ec0abbeecd679931082a5b676.jpg" align="middle">
</details>




<h2 id="From-Seconds-to-Hours-Reviewing-MultiModal-Large-Language-Models-on-Comprehensive-Long-Video-Understanding"><a href="#From-Seconds-to-Hours-Reviewing-MultiModal-Large-Language-Models-on-Comprehensive-Long-Video-Understanding" class="headerlink" title="From Seconds to Hours: Reviewing MultiModal Large Language Models on   Comprehensive Long Video Understanding"></a>From Seconds to Hours: Reviewing MultiModal Large Language Models on   Comprehensive Long Video Understanding</h2><p><strong>Authors:Heqing Zou, Tianze Luo, Guiyang Xie,  Victor,  Zhang, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang</strong></p>
<p>The integration of Large Language Models (LLMs) with visual encoders has recently shown promising performance in visual understanding tasks, leveraging their inherent capability to comprehend and generate human-like text for visual reasoning. Given the diverse nature of visual data, MultiModal Large Language Models (MM-LLMs) exhibit variations in model designing and training for understanding images, short videos, and long videos. Our paper focuses on the substantial differences and unique challenges posed by long video understanding compared to static image and short video understanding. Unlike static images, short videos encompass sequential frames with both spatial and within-event temporal information, while long videos consist of multiple events with between-event and long-term temporal information. In this survey, we aim to trace and summarize the advancements of MM-LLMs from image understanding to long video understanding. We review the differences among various visual understanding tasks and highlight the challenges in long video understanding, including more fine-grained spatiotemporal details, dynamic events, and long-term dependencies. We then provide a detailed summary of the advancements in MM-LLMs in terms of model design and training methodologies for understanding long videos. Finally, we compare the performance of existing MM-LLMs on video understanding benchmarks of various lengths and discuss potential future directions for MM-LLMs in long video understanding. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è§†è§‰ç¼–ç å™¨çš„ç»“åˆåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œåˆ©ç”¨å®ƒä»¬ç†è§£å’Œç”Ÿæˆç”¨äºè§†è§‰æ¨ç†çš„äººç±»æ–‡æœ¬çš„èƒ½åŠ›ã€‚è€ƒè™‘åˆ°è§†è§‰æ•°æ®çš„å¤šæ ·æ€§ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMM-LLMï¼‰åœ¨ç†è§£å›¾åƒã€çŸ­è§†é¢‘å’Œé•¿è§†é¢‘æ—¶ï¼Œåœ¨æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ–¹é¢å±•ç°å‡ºå·®å¼‚ã€‚æˆ‘ä»¬çš„è®ºæ–‡é‡ç‚¹å…³æ³¨é•¿è§†é¢‘ç†è§£ä¸é™æ€å›¾åƒå’ŒçŸ­è§†é¢‘ç†è§£ç›¸æ¯”å­˜åœ¨çš„å·¨å¤§å·®å¼‚å’Œç‹¬ç‰¹æŒ‘æˆ˜ã€‚ä¸åŒäºé™æ€å›¾åƒï¼ŒçŸ­è§†é¢‘åŒ…å«å…·æœ‰ç©ºé—´å’Œæ—¶é—´ä¿¡æ¯çš„è¿ç»­å¸§ï¼Œè€Œé•¿è§†é¢‘åˆ™ç”±å¤šä¸ªäº‹ä»¶ç»„æˆï¼Œå…·æœ‰äº‹ä»¶é—´å’Œé•¿æœŸçš„æ—¶é—´ä¿¡æ¯ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨è¿½è¸ªå¹¶æ€»ç»“MM-LLMä»å›¾åƒç†è§£åˆ°é•¿è§†é¢‘ç†è§£çš„è¿›å±•ã€‚æˆ‘ä»¬å›é¡¾äº†ä¸åŒè§†è§‰ç†è§£ä»»åŠ¡ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å¼ºè°ƒäº†é•¿è§†é¢‘ç†è§£ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ›´ç²¾ç»†çš„æ—¶ç©ºç»†èŠ‚ã€åŠ¨æ€äº‹ä»¶å’Œé•¿æœŸä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹MM-LLMåœ¨ç†è§£é•¿è§†é¢‘æ–¹é¢çš„æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•çš„è¿›å±•è¿›è¡Œäº†è¯¦ç»†æ€»ç»“ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ç°æœ‰MM-LLMåœ¨å„ç§é•¿åº¦çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ï¼Œå¹¶è®¨è®ºäº†MM-LLMåœ¨æœªæ¥é•¿è§†é¢‘ç†è§£æ–¹é¢çš„æ½œåœ¨å‘å±•æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18938v2">PDF</a> 11 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰ç¼–ç å™¨çš„ç»“åˆåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºè‰¯å¥½æ€§èƒ½ï¼Œå°¤å…¶æ“…é•¿åˆ©ç”¨æ¨¡å‹å›ºæœ‰çš„æ–‡æœ¬ç†è§£å’Œç”Ÿæˆèƒ½åŠ›è¿›è¡Œè§†è§‰æ¨ç†ã€‚é’ˆå¯¹é•¿è§†é¢‘ç†è§£ï¼Œæœ¬æ–‡é‡ç‚¹æ¢è®¨äº†ä¸é™æ€å›¾åƒå’ŒçŸ­è§†é¢‘ç†è§£ç›¸æ¯”ï¼Œé•¿è§†é¢‘ç†è§£ä¸­çš„æ˜¾è‘—å·®å¼‚å’Œç‹¬ç‰¹æŒ‘æˆ˜ã€‚é•¿è§†é¢‘åŒ…å«å¤šä¸ªäº‹ä»¶ï¼Œæ¶‰åŠè·¨äº‹ä»¶å’Œé•¿æœŸæ—¶é—´ä¿¡æ¯ã€‚æœ¬æ–‡æ—¨åœ¨å›é¡¾å¹¶æ€»ç»“å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä»å›¾åƒç†è§£åˆ°é•¿è§†é¢‘ç†è§£çš„è¿›å±•ï¼ŒåŒæ—¶ä»‹ç»äº†è®¾è®¡æ¨¡å‹å’Œè®­ç»ƒæ–¹æ³•çš„æœ€æ–°è¿›å±•ã€‚æœ€åå¯¹ç°æœ‰æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶æ¢è®¨äº†æœªæ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£é¢†åŸŸçš„å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰ç¼–ç å™¨çš„ç»“åˆåœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>é•¿è§†é¢‘ç†è§£ç›¸è¾ƒäºé™æ€å›¾åƒå’ŒçŸ­è§†é¢‘ç†è§£å­˜åœ¨æ˜¾è‘—å·®å¼‚å’Œç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>é•¿è§†é¢‘åŒ…å«è·¨äº‹ä»¶å’Œé•¿æœŸæ—¶é—´ä¿¡æ¯ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—è¿›å±•ï¼ŒåŒ…æ‹¬æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒæ–¹æ³•çš„è¿›æ­¥ã€‚</li>
<li>é•¿è§†é¢‘ç†è§£éœ€è¦æ›´ç²¾ç»†çš„æ—¶ç©ºç»†èŠ‚ã€åŠ¨æ€äº‹ä»¶å’Œé•¿æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢çš„æ€§èƒ½æœ‰æ‰€å·®å¼‚ï¼Œæœªæ¥éœ€è¦è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„é²æ£’æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d8291c5b2da70ca9638633e67ca5a4f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09bbec944c4a6dc73e026e45613a6263.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13bea4d57f8b15e887afb2141b27df2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91e6d1634c014968480674fb7703b5ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28c4616200de29b6b025edabcc5b3a0c.jpg" align="middle">
</details>




<h2 id="OmAgent-A-Multi-modal-Agent-Framework-for-Complex-Video-Understanding-with-Task-Divide-and-Conquer"><a href="#OmAgent-A-Multi-modal-Agent-Framework-for-Complex-Video-Understanding-with-Task-Divide-and-Conquer" class="headerlink" title="OmAgent: A Multi-modal Agent Framework for Complex Video Understanding   with Task Divide-and-Conquer"></a>OmAgent: A Multi-modal Agent Framework for Complex Video Understanding   with Task Divide-and-Conquer</h2><p><strong>Authors:Lu Zhang, Tiancheng Zhao, Heting Ying, Yibo Ma, Kyusong Lee</strong></p>
<p>Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding. However, processing extensive videos such as 24-hour CCTV footage or full-length films presents significant challenges due to the vast data and processing demands. Traditional methods, like extracting key frames or converting frames to text, often result in substantial information loss. To address these shortcomings, we develop OmAgent, efficiently stores and retrieves relevant video frames for specific queries, preserving the detailed content of videos. Additionally, it features an Divide-and-Conquer Loop capable of autonomous reasoning, dynamically invoking APIs and tools to enhance query processing and accuracy. This approach ensures robust video understanding, significantly reducing information loss. Experimental results affirm OmAgentâ€™s efficacy in handling various types of videos and complex tasks. Moreover, we have endowed it with greater autonomy and a robust tool-calling system, enabling it to accomplish even more intricate tasks. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥å·²ç»å°†å…¶èƒ½åŠ›æ‰©å±•åˆ°äº†å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬å…¨é¢çš„è§†é¢‘ç†è§£ã€‚ç„¶è€Œï¼Œå¤„ç†é•¿è¾¾24å°æ—¶çš„ç›‘æ§å½•åƒæˆ–å…¨é•¿ç”µå½±ç­‰å¤§é‡è§†é¢‘ç”±äºå·¨å¤§çš„æ•°æ®å’Œå¤„ç†éœ€æ±‚è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¦‚æå–å…³é”®å¸§æˆ–å°†å¸§è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œé€šå¸¸ä¼šå¯¼è‡´å¤§é‡ä¿¡æ¯ä¸¢å¤±ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å¼€å‘äº†OmAgentï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°å­˜å‚¨å’Œæ£€ç´¢ä¸ç‰¹å®šæŸ¥è¯¢ç›¸å…³çš„è§†é¢‘å¸§ï¼ŒåŒæ—¶ä¿ç•™è§†é¢‘çš„è¯¦ç»†å†…å®¹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰ä¸€ç§åˆ†è€Œæ²»ä¹‹å¾ªç¯èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œè‡ªä¸»æ¨ç†ï¼ŒåŠ¨æ€è°ƒç”¨APIå’Œå·¥å…·ä»¥æé«˜æŸ¥è¯¢å¤„ç†å’Œå‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç¨³å¥çš„è§†é¢‘ç†è§£ï¼Œæ˜¾è‘—å‡å°‘äº†ä¿¡æ¯ä¸¢å¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmAgentåœ¨å¤„ç†å„ç§è§†é¢‘å’Œå¤æ‚ä»»åŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜èµ‹äºˆäº†å®ƒæ›´å¤§çš„è‡ªä¸»æ€§å’Œå¼ºå¤§çš„å·¥å…·è°ƒç”¨ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿå®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16620v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ï¼Œå…¶åœ¨å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¸­çš„èƒ½åŠ›å¾—åˆ°äº†æ‰©å±•ï¼ŒåŒ…æ‹¬å¯¹è§†é¢‘çš„ç»¼åˆç†è§£ã€‚ç„¶è€Œï¼Œå¤„ç†é•¿ç¯‡è§†é¢‘å¦‚24å°æ—¶ç›‘æ§å½•åƒæˆ–å…¨é•¿ç”µå½±æ—¶ï¼Œç”±äºå¤§é‡æ•°æ®å’Œå¤„ç†éœ€æ±‚ï¼Œå­˜åœ¨å·¨å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸å¯¼è‡´ä¿¡æ¯å¤§é‡ä¸¢å¤±ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†OmAgentï¼Œå®ƒèƒ½é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢ç›¸å…³è§†é¢‘å¸§ä»¥åº”å¯¹ç‰¹å®šæŸ¥è¯¢ï¼ŒåŒæ—¶ä¿ç•™è§†é¢‘çš„è¯¦ç»†å†…å®¹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰ä¸€ç§åˆ†è€Œæ²»ä¹‹å¾ªç¯èƒ½åŠ›ï¼Œå¯è‡ªä¸»æ¨ç†ã€åŠ¨æ€è°ƒç”¨APIå’Œå·¥å…·ä»¥æé«˜æŸ¥è¯¢å¤„ç†å’Œå‡†ç¡®æ€§ã€‚è¿™ç§æ–¹æ³•ç¡®ä¿äº†ç¨³å¥çš„è§†é¢‘ç†è§£ï¼Œæ˜¾è‘—å‡å°‘äº†ä¿¡æ¯ä¸¢å¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmAgentåœ¨å¤„ç†å„ç§è§†é¢‘å’Œå¤æ‚ä»»åŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜èµ‹äºˆå®ƒæ›´å¤§çš„è‡ªä¸»æ€§å’Œå¼ºå¤§çš„å·¥å…·è°ƒç”¨ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿå®Œæˆæ›´å¤æ‚çš„ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°åœ¨èƒ½å¤Ÿåº”ç”¨äºè§†é¢‘ç†è§£ã€‚</li>
<li>å¤„ç†é•¿ç¯‡è§†é¢‘æ—¶å­˜åœ¨å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>OmAgentèƒ½å¤Ÿé«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢ç›¸å…³è§†é¢‘å¸§ä»¥åº”å¯¹ç‰¹å®šæŸ¥è¯¢ã€‚</li>
<li>OmAgentå…·æœ‰åˆ†è€Œæ²»ä¹‹å¾ªç¯èƒ½åŠ›ï¼Œå¯ä»¥è‡ªä¸»æ¨ç†å¹¶åŠ¨æ€è°ƒç”¨APIå’Œå·¥å…·ã€‚</li>
<li>è¿™ç§æ–°çš„æ–¹æ³•ç¡®ä¿äº†ç¨³å¥çš„è§†é¢‘ç†è§£ï¼Œæ˜¾è‘—å‡å°‘äº†ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜OmAgentåœ¨å¤„ç†å„ç§è§†é¢‘å’Œå¤æ‚ä»»åŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a80595e26ae096ef538b43215efd9dee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a024f4bf208839552c837ff489132655.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac154fcc574709b8affd7ef18f23690d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcd72f9efb60c797dc61cc690b5df5b7.jpg" align="middle">
</details>




<h2 id="DrVideo-Document-Retrieval-Based-Long-Video-Understanding"><a href="#DrVideo-Document-Retrieval-Based-Long-Video-Understanding" class="headerlink" title="DrVideo: Document Retrieval Based Long Video Understanding"></a>DrVideo: Document Retrieval Based Long Video Understanding</h2><p><strong>Authors:Ziyu Ma, Chenhui Gou, Hengcan Shi, Bin Sun, Shutao Li, Hamid Rezatofighi, Jianfei Cai</strong></p>
<p>Most of the existing methods for video understanding primarily focus on videos only lasting tens of seconds, with limited exploration of techniques for handling long videos. The increased number of frames in long videos poses two main challenges: difficulty in locating key information and performing long-range reasoning. Thus, we propose DrVideo, a document-retrieval-based system designed for long video understanding. Our key idea is to convert the long-video understanding problem into a long-document understanding task so as to effectively leverage the power of large language models. Specifically, DrVideo first transforms a long video into a coarse text-based long document to initially retrieve key frames and then updates the documents with the augmented key frame information. It then employs an agent-based iterative loop to continuously search for missing information and augment the document until sufficient question-related information is gathered for making the final predictions in a chain-of-thought manner. Extensive experiments on long video benchmarks confirm the effectiveness of our method. DrVideo significantly outperforms existing LLM-based state-of-the-art methods on EgoSchema benchmark (3 minutes), MovieChat-1K benchmark (10 minutes), and the long split of Video-MME benchmark (average of 44 minutes). </p>
<blockquote>
<p>ç°æœ‰çš„è§†é¢‘ç†è§£æ–¹æ³•å¤§å¤šä¸»è¦å…³æ³¨æŒç»­æ—¶é—´åªæœ‰å‡ åç§’çš„è§†é¢‘ï¼Œå¯¹äºå¤„ç†é•¿è§†é¢‘çš„æŠ€æœ¯æ¢ç´¢æœ‰é™ã€‚é•¿è§†é¢‘ä¸­å¸§æ•°çš„å¢åŠ å¸¦æ¥äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šéš¾ä»¥å®šä½å…³é”®ä¿¡æ¯è¿›è¡Œé•¿ç¨‹æ¨ç†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DrVideoï¼Œä¸€ä¸ªåŸºäºæ–‡æ¡£æ£€ç´¢çš„ç³»ç»Ÿï¼Œä¸“ä¸ºé•¿è§†é¢‘ç†è§£è€Œè®¾è®¡ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†é•¿è§†é¢‘ç†è§£é—®é¢˜è½¬åŒ–ä¸ºé•¿æ–‡æ¡£ç†è§£ä»»åŠ¡ï¼Œä»¥ä¾¿æœ‰æ•ˆåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¨åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒDrVideoé¦–å…ˆå°†é•¿è§†é¢‘è½¬æ¢ä¸ºåŸºäºæ–‡æœ¬çš„ç²—ç•¥é•¿æ–‡æ¡£ï¼Œä»¥åˆæ­¥æ£€ç´¢å…³é”®å¸§ï¼Œç„¶åä½¿ç”¨å¢å¼ºåçš„å…³é”®å¸§ä¿¡æ¯æ›´æ–°æ–‡æ¡£ã€‚æ¥ç€ï¼Œå®ƒé‡‡ç”¨åŸºäºä»£ç†çš„è¿­ä»£å¾ªç¯æ¥ä¸æ–­æœç´¢ç¼ºå¤±ä¿¡æ¯å¹¶æ›´æ–°æ–‡æ¡£ï¼Œç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œä»¥è¿›è¡Œæœ€ç»ˆçš„é¢„æµ‹ã€‚åœ¨å¤§å‹è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚DrVideoåœ¨EgoSchemaåŸºå‡†æµ‹è¯•ï¼ˆ3åˆ†é’Ÿï¼‰ã€MovieChat-1KåŸºå‡†æµ‹è¯•ï¼ˆ10åˆ†é’Ÿï¼‰å’Œè§†é¢‘MMEåŸºå‡†æµ‹è¯•çš„é•¿ç‰‡æ®µï¼ˆå¹³å‡44åˆ†é’Ÿï¼‰ä¸Šçš„è¡¨ç°éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12846v2">PDF</a> 17 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDrVideoçš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿï¼Œç”¨äºå¤„ç†é•¿æ—¶é—´è§†é¢‘çš„ç†è§£é—®é¢˜ã€‚è¯¥ç³»ç»Ÿå°†é•¿æ—¶é—´è§†é¢‘è½¬åŒ–ä¸ºåŸºäºæ–‡æœ¬çš„æ–‡æ¡£ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¿›è¡Œå¤„ç†ã€‚DrVideoé€šè¿‡è¿­ä»£æœç´¢å’Œæ›´æ–°æ–‡æ¡£çš„æ–¹å¼æ”¶é›†ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œåœ¨æ€æƒ³é“¾ä¸­å½¢æˆæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚å®ƒåœ¨å¤šä¸ªé•¿æ—¶é—´è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­æ•ˆæœæ˜¾è‘—ï¼Œå¦‚EgoSchemaã€MovieChat-1Kå’Œè§†é¢‘MMEçš„é•¿åˆ†å‰²ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è§†é¢‘ç†è§£æ–¹æ³•ä¸»è¦å…³æ³¨æ—¶é•¿å‡ åç§’çš„çŸ­è§†é¢‘ï¼Œå¯¹äºå¤„ç†é•¿è§†é¢‘çš„æŠ€æœ¯æ¢ç´¢æœ‰é™ã€‚</li>
<li>é•¿è§†é¢‘ç”±äºå¸§æ•°å¢å¤šï¼Œå­˜åœ¨å®šä½å…³é”®ä¿¡æ¯å’Œè¿›è¡Œé•¿è·ç¦»æ¨ç†ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>DrVideoç³»ç»Ÿé€šè¿‡å°†é•¿è§†é¢‘è½¬åŒ–ä¸ºåŸºäºæ–‡æœ¬çš„æ–‡æ¡£æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»è€Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åŠ›é‡ã€‚</li>
<li>DrVideoé¦–å…ˆä¼šå°†é•¿è§†é¢‘è½¬åŒ–ä¸ºç²—ç•¥çš„æ–‡æœ¬æ–‡æ¡£ä»¥æ£€ç´¢å…³é”®å¸§ã€‚</li>
<li>ä¹‹åï¼Œç³»ç»Ÿä¼šåˆ©ç”¨å¢å¼ºåçš„å…³é”®å¸§ä¿¡æ¯æ›´æ–°æ–‡æ¡£ã€‚</li>
<li>DrVideoé‡‡ç”¨åŸºäºä»£ç†çš„è¿­ä»£å¾ªç¯æ¥æŒç»­æœç´¢ç¼ºå¤±ä¿¡æ¯å¹¶æ›´æ–°æ–‡æ¡£ï¼Œç›´è‡³æ”¶é›†åˆ°è¶³å¤Ÿä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ä»¥åšå‡ºæœ€ç»ˆé¢„æµ‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-89daf1c4819d5cb88f5b5e6828d90099.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecde543951047edd301ae115316bb87d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-275d8b02285ffe7ad4e236c713f3fe19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-def42c2e0edf6c89386fb9f0738abf52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbac2066af0dea7cda2215c0051a67c1.jpg" align="middle">
</details>




<h2 id="MAMBA4D-Efficient-Long-Sequence-Point-Cloud-Video-Understanding-with-Disentangled-Spatial-Temporal-State-Space-Models"><a href="#MAMBA4D-Efficient-Long-Sequence-Point-Cloud-Video-Understanding-with-Disentangled-Spatial-Temporal-State-Space-Models" class="headerlink" title="MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with   Disentangled Spatial-Temporal State Space Models"></a>MAMBA4D: Efficient Long-Sequence Point Cloud Video Understanding with   Disentangled Spatial-Temporal State Space Models</h2><p><strong>Authors:Jiuming Liu, Jinru Han, Lihao Liu, Angelica I. Aviles-Rivero, Chaokang Jiang, Zhe Liu, Hesheng Wang</strong></p>
<p>Point cloud videos can faithfully capture real-world spatial geometries and temporal dynamics, which are essential for enabling intelligent agents to understand the dynamically changing world. However, designing an effective 4D backbone remains challenging, mainly due to the irregular and unordered distribution of points and temporal inconsistencies across frames. Also, recent transformer-based 4D backbones commonly suffer from large computational costs due to their quadratic complexity, particularly for long video sequences.To address these challenges, we propose a novel point cloud video understanding backbone purely based on the State Space Models (SSMs). Specifically, we first disentangle space and time in 4D video sequences and then establish the spatio-temporal correlation with our designed Mamba blocks. The Intra-frame Spatial Mamba module is developed to encode locally similar geometric structures within a certain temporal stride. Subsequently, locally correlated tokens are delivered to the Inter-frame Temporal Mamba module, which integrates long-term point features across the entire video with linear complexity. Our proposed Mamba4d achieves competitive performance on the MSR-Action3D action recognition (+10.4% accuracy), HOI4D action segmentation (+0.7 F1 Score), and Synthia4D semantic segmentation (+0.19 mIoU) datasets. Especially, for long video sequences, our method has a significant efficiency improvement with 87.5% GPU memory reduction and 5.36 times speed-up. </p>
<blockquote>
<p>ç‚¹äº‘è§†é¢‘èƒ½å¤Ÿå¿ å®æ•æ‰ç°å®ä¸–ç•Œä¸­çš„ç©ºé—´å‡ ä½•å’ŒåŠ¨æ€æ—¶åºä¿¡æ¯ï¼Œè¿™å¯¹äºæ™ºèƒ½ä¸»ä½“ç†è§£åŠ¨æ€å˜åŒ–çš„ä¸–ç•Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè®¾è®¡æœ‰æ•ˆçš„å››ç»´ä¸»å¹²ç½‘ç»œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºç‚¹çš„åˆ†å¸ƒä¸è§„åˆ™ä¸”æ— åºï¼Œä»¥åŠå¸§ä¹‹é—´çš„æ—¶é—´ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºæœ€æ–°å˜æ¢å™¨çš„å››ç»´ä¸»å¹²ç½‘ç»œé€šå¸¸å› ä¸ºå…¶äºŒæ¬¡å¤æ‚æ€§è€Œé¢ä¸´å·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿è§†é¢‘åºåˆ—ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„ç‚¹äº‘è§†é¢‘ç†è§£ä¸»å¹²ç½‘ç»œï¼Œè¯¥ç½‘ç»œå®Œå…¨åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè§£å¼€å››ç»´è§†é¢‘åºåˆ—ä¸­çš„ç©ºé—´å’Œæ—¶é—´å…ƒç´ ï¼Œç„¶åé€šè¿‡è®¾è®¡çš„Mambaå—å»ºç«‹æ—¶ç©ºå…³è”ã€‚æˆ‘ä»¬å¼€å‘äº†å¸§å†…ç©ºé—´Mambaæ¨¡å—ï¼Œä»¥ç¼–ç æŸä¸€æ—¶é—´æ­¥é•¿å†…å±€éƒ¨ç›¸ä¼¼çš„å‡ ä½•ç»“æ„ã€‚éšåï¼Œå±€éƒ¨ç›¸å…³ä»¤ç‰Œè¢«ä¼ é€’ç»™å¸§é—´æ—¶é—´Mambaæ¨¡å—ï¼Œè¯¥æ¨¡å—ä»¥çº¿æ€§å¤æ‚åº¦æ•´åˆæ•´ä¸ªè§†é¢‘çš„ç‚¹ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºçš„Mamba4dåœ¨MSR-Action3DåŠ¨ä½œè¯†åˆ«ï¼ˆæé«˜10.4%çš„å‡†ç¡®æ€§ï¼‰ã€HOI4DåŠ¨ä½œåˆ†å‰²ï¼ˆæé«˜0.7çš„F1åˆ†æ•°ï¼‰å’ŒSynthia4Dè¯­ä¹‰åˆ†å‰²ï¼ˆæé«˜0.19çš„mIoUï¼‰ç­‰æ•°æ®é›†ä¸Šå®ç°äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯å¯¹äºé•¿è§†é¢‘åºåˆ—ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨GPUå†…å­˜æ–¹é¢å‡å°‘äº†87.5%ï¼Œå¹¶å®ç°äº†5.36å€çš„é€Ÿåº¦æå‡ï¼Œå…·æœ‰æ˜¾è‘—çš„æ•ˆç‡æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14338v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç‚¹äº‘è§†é¢‘èƒ½å¤Ÿæ•æ‰çœŸå®ä¸–ç•Œçš„ç©ºé—´å‡ ä½•å’Œæ—¶æ€åŠ¨æ€ï¼Œè¿™å¯¹äºæ™ºèƒ½ä»£ç†ç†è§£åŠ¨æ€å˜åŒ–çš„ä¸–ç•Œè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè®¾è®¡æœ‰æ•ˆçš„4Dä¸»å¹²ç½‘ç»œé¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æºäºç‚¹çš„åˆ†å¸ƒä¸è§„åˆ™ã€æ— åºä»¥åŠè·¨å¸§çš„æ—¶ç©ºä¸ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„ç‚¹äº‘è§†é¢‘ç†è§£ä¸»å¹²ç½‘ç»œã€‚é€šè¿‡è®¾è®¡Mambaå—å»ºç«‹æ—¶ç©ºå…³è”ï¼Œå®ç°å¸§å†…ç©ºé—´Mambaæ¨¡å—å’Œå¸§é—´æ—¶é—´Mambaæ¨¡å—çš„ååŒå·¥ä½œï¼Œæœ‰æ•ˆæå‡äº†ç‚¹äº‘è§†é¢‘å¤„ç†çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‚¹äº‘è§†é¢‘èƒ½å¤Ÿæ•æ‰çœŸå®ä¸–ç•Œçš„ç©ºé—´å‡ ä½•å’Œæ—¶æ€åŠ¨æ€ï¼Œå¯¹æ™ºèƒ½ä»£ç†ç†è§£åŠ¨æ€ä¸–ç•Œè‡³å…³é‡è¦ã€‚</li>
<li>è®¾è®¡æœ‰æ•ˆçš„4Dä¸»å¹²ç½‘ç»œé¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬ç‚¹çš„åˆ†å¸ƒä¸è§„åˆ™ã€æ— åºä»¥åŠè·¨å¸§çš„æ—¶ç©ºä¸ä¸€è‡´æ€§ã€‚</li>
<li>åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰çš„ç‚¹äº‘è§†é¢‘ç†è§£ä¸»å¹²ç½‘ç»œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æå‡ºçš„Mambaå—é€šè¿‡å»ºç«‹æ—¶ç©ºå…³è”ï¼Œå®ç°å¸§å†…ç©ºé—´Mambaæ¨¡å—å’Œå¸§é—´æ—¶é—´Mambaæ¨¡å—çš„ååŒå·¥ä½œã€‚</li>
<li>Mamba4dåœ¨MSR-Action3DåŠ¨ä½œè¯†åˆ«ã€HOI4DåŠ¨ä½œåˆ†å‰²å’ŒSynthia4Dè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
<li>å¯¹äºé•¿è§†é¢‘åºåˆ—ï¼Œæ‰€ææ–¹æ³•å…·æœ‰æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œå®ç°äº†87.5%çš„GPUå†…å­˜å‡å°‘å’Œ5.36å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fce7541d5bab248c60d84773653b25db.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd913674462f90ebd4ef052bba94a5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18055ebda74aa4c36eb2e54ab29e8865.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8b13940a251fda92a79f19525744957.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a36744120d276b1535a359c03a3ceadc.jpg" align="middle">
</details>




<h2 id="Understanding-Long-Videos-with-Multimodal-Language-Models"><a href="#Understanding-Long-Videos-with-Multimodal-Language-Models" class="headerlink" title="Understanding Long Videos with Multimodal Language Models"></a>Understanding Long Videos with Multimodal Language Models</h2><p><strong>Authors:Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo</strong></p>
<p>Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. Building on this, we exploring injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establish its strong generality. Our code will be released publicly. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿å¾—æœ€è¿‘çš„åŸºäºLLMçš„æ–¹æ³•åœ¨é•¿æ—¶é—´è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬ç ”ç©¶åŸºç¡€LLMçš„å¹¿æ³›ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„æ¨ç†æŠ€èƒ½å¦‚ä½•å½±å“è¿™ä¸€å‡ºè‰²æ€§èƒ½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°åŸºäºLLMçš„æ–¹æ³•åœ¨æœ‰é™çš„è§†é¢‘ä¿¡æ¯ä¸Šï¼Œç”šè‡³åœ¨æ²¡æœ‰ä»»ä½•ç‰¹å®šè§†é¢‘ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œéƒ½èƒ½åœ¨é•¿æ—¶é—´è§†é¢‘ä»»åŠ¡ä¸Šäº§ç”Ÿä»¤äººæƒŠè®¶çš„è‰¯å¥½å‡†ç¡®æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æ¢ç´¢å°†ç‰¹å®šè§†é¢‘ä¿¡æ¯æ³¨å…¥åˆ°åŸºäºLLMçš„æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬ä½¿ç”¨ç°æˆçš„è§†è§‰å·¥å…·ä»è§†é¢‘ä¸­æå–ä¸‰ç§ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ä¿¡æ¯æ¨¡å¼ï¼Œç„¶ååˆ©ç”¨è‡ªç„¶è¯­è¨€ä½œä¸ºèåˆè¿™äº›ä¿¡æ¯çš„åª’ä»‹ã€‚æˆ‘ä»¬ç”±æ­¤äº§ç”Ÿçš„å¤šæ¨¡æ€è§†é¢‘ç†è§£ï¼ˆMVUï¼‰æ¡†æ¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚åœ¨æœºå™¨äººé¢†åŸŸä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°ä¹Ÿè¯æ˜äº†å…¶å¼ºå¤§çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.16998v2">PDF</a> Code available at <a target="_blank" rel="noopener" href="https://github.com/kahnchana/mvu">https://github.com/kahnchana/mvu</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒLLMçš„ä¸°å¯ŒçŸ¥è¯†å’Œå¼ºå¤§æ¨ç†èƒ½åŠ›æ˜¯å…³é”®ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä¾¿è§†é¢‘ä¿¡æ¯æœ‰é™ç”šè‡³æ¯«æ— è§†é¢‘ç‰¹å®šä¿¡æ¯ï¼ŒLLMæ–¹æ³•ä¹Ÿèƒ½å®ç°è‰¯å¥½çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ³¨å…¥è§†é¢‘ç‰¹å®šä¿¡æ¯åˆ°LLMæ¡†æ¶ä¸­ï¼Œå¹¶ç»“åˆç°æˆçš„è§†è§‰å·¥å…·æå–è§†é¢‘çš„ä¸‰å¯¹è±¡ä¸­å¿ƒä¿¡æ¯æ¨¡å¼ï¼Œå†åˆ©ç”¨è‡ªç„¶è¯­è¨€èåˆè¿™äº›ä¿¡æ¯ï¼Œç ”å‘å‡ºçš„å¤šæ¨¡å¼è§†é¢‘ç†è§£ï¼ˆMVUï¼‰æ¡†æ¶åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä¸”åœ¨æœºå™¨äººé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°åŒæ ·å‡ºè‰²ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>LLMçš„ä¸°å¯ŒçŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>LLMæ–¹æ³•èƒ½åœ¨è§†é¢‘ä¿¡æ¯æœ‰é™ç”šè‡³æ¯«æ— ç‰¹å®šä¿¡æ¯çš„æƒ…å†µä¸‹å®ç°è‰¯å¥½å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æ³¨å…¥è§†é¢‘ç‰¹å®šä¿¡æ¯åˆ°LLMæ¡†æ¶ä¸­ï¼Œèƒ½æé«˜æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨ç°æˆçš„è§†è§‰å·¥å…·æå–è§†é¢‘çš„ä¸‰å¯¹è±¡ä¸­å¿ƒä¿¡æ¯æ¨¡å¼ã€‚</li>
<li>è‡ªç„¶è¯­è¨€è¢«ç”¨äºèåˆè¿™äº›è§†é¢‘ä¿¡æ¯æ¨¡å¼ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd47edfc4efaad02fbe1c8ae81588f3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca7c74b8575d300a26437c183627af86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb40d270a96c16c00be9379cd1a24869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-734af567c33112b4b62ddb82e5c213a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee46af8aceeef707410d849e34698a6f.jpg" align="middle">
</details>




<h2 id="Towards-Neuro-Symbolic-Video-Understanding"><a href="#Towards-Neuro-Symbolic-Video-Understanding" class="headerlink" title="Towards Neuro-Symbolic Video Understanding"></a>Towards Neuro-Symbolic Video Understanding</h2><p><strong>Authors:Minkyu Choi, Harsh Goel, Mohammad Omama, Yunhao Yang, Sahil Shah, Sandeep Chinchali</strong></p>
<p>The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reasoning improves the F1 score of complex event identification by 9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art self-driving datasets such as Waymo and NuScenes. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè§†é¢‘æ•°æ®ç”Ÿäº§çš„ç©ºå‰å¢é•¿éœ€è¦å¤§é‡æœ‰æ•ˆçš„å·¥å…·ä»è§†é¢‘ä¸­æå–æœ‰æ„ä¹‰çš„å¸§ä»¥ä¾›ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€‚é•¿æœŸæ—¶é—´æ¨ç†æ˜¯å¸§æ£€ç´¢ç³»ç»Ÿçš„å…³é”®éœ€æ±‚ã€‚è™½ç„¶æœ€å…ˆè¿›çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚VideoLLaMAå’ŒViCLIPï¼Œæ“…é•¿çŸ­æœŸè¯­ä¹‰ç†è§£ï¼Œä½†å®ƒä»¬åœ¨é•¿æœŸè·¨å¸§æ¨ç†æ–¹é¢å´å‡ºäººæ„æ–™åœ°å¤±è´¥äº†ã€‚å¤±è´¥çš„ä¸€ä¸ªå…³é”®åŸå› æ˜¯å®ƒä»¬å°†é€å¸§æ„ŸçŸ¥å’Œæ—¶é—´æ¨ç†äº¤ç»‡åœ¨ä¸€ä¸ªæ·±åº¦ç½‘ç»œä¸­ã€‚å› æ­¤ï¼Œè§£è€¦ä½†ååŒè®¾è®¡è¯­ä¹‰ç†è§£å’Œæ—¶é—´æ¨ç†å¯¹äºæœ‰æ•ˆåœºæ™¯è¯†åˆ«è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å•ä¸ªå¸§è¿›è¡Œè¯­ä¹‰ç†è§£ï¼Œå¹¶åˆ©ç”¨çŠ¶æ€æœºå’Œæ—¶é—´é€»è¾‘ï¼ˆTLï¼‰å…¬å¼æœ‰æ•ˆåœ°æ¨ç†é•¿æœŸäº‹ä»¶çš„æ¼”å˜ï¼Œè¿™äº›å…¬å¼å¯ä»¥å›ºæœ‰åœ°æ•è·è®°å¿†ã€‚ä¸åœ¨Waymoå’ŒNuScenesç­‰å…ˆè¿›è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šä½¿ç”¨GPT4è¿›è¡Œæ¨ç†çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŸºäºTLçš„æ¨ç†å°†å¤æ‚äº‹ä»¶è¯†åˆ«çš„F1åˆ†æ•°æé«˜äº†9-15%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11021v3">PDF</a> Accepted by The European Conference on Computer Vision (ECCV) 2024</p>
<p><strong>Summary</strong><br>     è§†é¢‘æ•°æ®é‡çš„æ¿€å¢è¦æ±‚å¼€å‘æœ‰æ•ˆçš„å·¥å…·ä»è§†é¢‘ä¸­æå–æœ‰æ„ä¹‰çš„å¸§ä»¥ä¾›ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€‚é•¿æœŸæ—¶é—´æ¨ç†æ˜¯å¸§æ£€ç´¢ç³»ç»Ÿçš„å…³é”®è¦ç´ ã€‚å°½ç®¡æœ€å‰æ²¿çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚VideoLLaMAå’ŒViCLIPï¼Œæ“…é•¿çŸ­æœŸè¯­ä¹‰ç†è§£ï¼Œä½†åœ¨é•¿æœŸè·¨å¸§æ¨ç†æ–¹é¢å´è¡¨ç°ä¸ä½³ã€‚å…¶åŸå› åœ¨äºå®ƒä»¬å°†å•å¸§æ„ŸçŸ¥å’Œæ—¶é—´æ¨ç†äº¤ç»‡åœ¨ä¸€ä¸ªæ·±åº¦ç½‘ç»œä¸­ã€‚å› æ­¤ï¼Œå¯¹è¯­ä¹‰ç†è§£å’Œæ—¶é—´æ¨ç†è¿›è¡Œè§£è€¦å’ŒååŒè®¾è®¡å¯¹äºæœ‰æ•ˆåœºæ™¯è¯†åˆ«è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å•ä¸ªå¸§è¿›è¡Œè¯­ä¹‰ç†è§£ï¼ŒåŒæ—¶ä½¿ç”¨çŠ¶æ€æœºå’Œæ—¶é—´é€»è¾‘ï¼ˆTLï¼‰å…¬å¼æœ‰æ•ˆæ¨ç†é•¿æœŸäº‹ä»¶çš„å‘å±•ï¼Œè¿™äº›å…¬å¼èƒ½å›ºæœ‰åœ°æ•æ‰è®°å¿†ã€‚ä¸åœ¨å‰æ²¿è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ï¼ˆå¦‚Waymoå’ŒNuScenesï¼‰ä¸Šä½¿ç”¨GPT4è¿›è¡Œæ¨ç†çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„TLåŸºäºæ¨ç†å°†å¤æ‚äº‹ä»¶è¯†åˆ«çš„F1åˆ†æ•°æé«˜äº†9-15%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ•°æ®é‡çš„æ¿€å¢éœ€è¦æœ‰æ•ˆçš„å·¥å…·ä»è§†é¢‘ä¸­æå–æœ‰æ„ä¹‰çš„å¸§ã€‚</li>
<li>é•¿æœŸæ—¶é—´æ¨ç†æ˜¯å¸§æ£€ç´¢ç³»ç»Ÿçš„å…³é”®è¦ç´ ã€‚</li>
<li>æœ€æ–°çš„åŸºç¡€æ¨¡å‹ï¼ˆå¦‚VideoLLaMAå’ŒViCLIPï¼‰åœ¨çŸ­æœŸè¯­ä¹‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨é•¿æœŸè·¨å¸§æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å½“å‰æ¨¡å‹å°†å•å¸§æ„ŸçŸ¥å’Œæ—¶é—´æ¨ç†äº¤ç»‡åœ¨ä¸€èµ·ï¼Œéœ€è¦è§£è€¦å’ŒååŒè®¾è®¡ã€‚</li>
<li>æå‡ºçš„ç³»ç»Ÿåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç†è§£ï¼Œå¹¶ç»“åˆçŠ¶æ€æœºå’Œæ—¶é—´é€»è¾‘å…¬å¼è¿›è¡Œé•¿æœŸäº‹ä»¶æ¨ç†ã€‚</li>
<li>æ—¶é—´é€»è¾‘å…¬å¼èƒ½å›ºæœ‰åœ°æ•æ‰è®°å¿†ï¼Œè¿™æ˜¯æé«˜å¤æ‚äº‹ä»¶è¯†åˆ«æ€§èƒ½çš„å…³é”®ã€‚</li>
<li>ä¸ä½¿ç”¨GPT4çš„åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œç³»ç»Ÿåœ¨å¤æ‚äº‹ä»¶è¯†åˆ«æ–¹é¢çš„F1åˆ†æ•°æé«˜äº†9-15%ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f71f6243939a08b1348f5bddffd2adcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ca06a29250c3666bb48302dfea92c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-127ce9d376d4c6995824c1a4a95203fe.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9747fcfed9389ef7a615593f1bc27fa6.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Visual Lexicon Rich Image Features in Language Space
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-102376073a70ff046fee9027769501d3.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-10  JAPAGEN Efficient Few/Zero-shot Learning via Japanese Training Dataset   Generation with LLM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
