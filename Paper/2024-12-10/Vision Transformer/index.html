<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Visual Lexicon Rich Image Features in Language Space">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9747fcfed9389ef7a615593f1bc27fa6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2024-12-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    31k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    126 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2024-12-11-æ›´æ–°"><a href="#2024-12-11-æ›´æ–°" class="headerlink" title="2024-12-11 æ›´æ–°"></a>2024-12-11 æ›´æ–°</h1><h2 id="Visual-Lexicon-Rich-Image-Features-in-Language-Space"><a href="#Visual-Lexicon-Rich-Image-Features-in-Language-Space" class="headerlink" title="Visual Lexicon: Rich Image Features in Language Space"></a>Visual Lexicon: Rich Image Features in Language Space</h2><p><strong>Authors:XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell, Cordelia Schmid</strong></p>
<p>We present Visual Lexicon, a novel visual language that encodes rich image information into the text space of vocabulary tokens while retaining intricate visual details that are often challenging to convey in natural language. Unlike traditional methods that prioritize either high-level semantics (e.g., CLIP) or pixel-level reconstruction (e.g., VAE), ViLex simultaneously captures rich semantic content and fine visual details, enabling high-quality image generation and comprehensive visual scene understanding. Through a self-supervised learning pipeline, ViLex generates tokens optimized for reconstructing input images using a frozen text-to-image (T2I) diffusion model, preserving the detailed information necessary for high-fidelity semantic-level reconstruction. As an image embedding in the language space, ViLex tokens leverage the compositionality of natural languages, allowing them to be used independently as â€œtext tokensâ€ or combined with natural language tokens to prompt pretrained T2I models with both visual and textual inputs, mirroring how we interact with vision-language models (VLMs). Experiments demonstrate that ViLex achieves higher fidelity in image reconstruction compared to text embeddingsâ€“even with a single ViLex token. Moreover, ViLex successfully performs various DreamBooth tasks in a zero-shot, unsupervised manner without fine-tuning T2I models. Additionally, ViLex serves as a powerful vision encoder, consistently improving vision-language model performance across 15 benchmarks relative to a strong SigLIP baseline. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ç§åä¸ºè§†è§‰è¯å…¸ï¼ˆVisual Lexiconï¼‰çš„æ–°å‹è§†è§‰è¯­è¨€ã€‚è¿™ç§è¯­è¨€å°†ä¸°å¯Œçš„å›¾åƒä¿¡æ¯ç¼–ç æˆè¯æ±‡ç¬¦å·çš„æ–‡æœ¬ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤æ‚çš„è§†è§‰ç»†èŠ‚ï¼Œè¿™äº›è§†è§‰ç»†èŠ‚åœ¨è‡ªç„¶ç•Œè¯­è¨€ä¸­å¾€å¾€éš¾ä»¥ä¼ è¾¾ã€‚ä¸ä¼ ç»Ÿçš„ä¼˜å…ˆå…³æ³¨é«˜çº§è¯­ä¹‰ï¼ˆä¾‹å¦‚CLIPï¼‰æˆ–åƒç´ çº§é‡å»ºï¼ˆä¾‹å¦‚VAEï¼‰çš„æ–¹æ³•ä¸åŒï¼ŒViLexèƒ½å¤ŸåŒæ—¶æ•è·ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ï¼Œä»è€Œå®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œå…¨é¢çš„è§†è§‰åœºæ™¯ç†è§£ã€‚é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ ç®¡é“ï¼ŒViLexç”Ÿæˆäº†é’ˆå¯¹ä½¿ç”¨å†»ç»“çš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹é‡å»ºè¾“å…¥å›¾åƒçš„ä»¤ç‰Œï¼Œä¿ç•™äº†ç”¨äºé«˜ä¿çœŸè¯­ä¹‰çº§é‡å»ºçš„è¯¦ç»†ä¿¡æ¯ã€‚ä½œä¸ºè¯­è¨€ç©ºé—´ä¸­çš„å›¾åƒåµŒå…¥ï¼ŒViLexä»¤ç‰Œåˆ©ç”¨è‡ªç„¶è¯­è¨€çš„ç»„åˆæ€§ï¼Œå¯ä»¥ç‹¬ç«‹ç”¨ä½œâ€œæ–‡æœ¬ä»¤ç‰Œâ€ï¼Œä¹Ÿå¯ä»¥ä¸è‡ªç„¶è¯­è¨€ä»¤ç‰Œç»“åˆï¼Œæç¤ºé¢„è®­ç»ƒçš„T2Iæ¨¡å‹åŒæ—¶ä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„äº¤äº’æ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å•ä¸ªViLexä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒViLexåœ¨å›¾åƒé‡å»ºæ–¹é¢çš„ä¿çœŸåº¦ä¹Ÿé«˜äºæ–‡æœ¬åµŒå…¥ã€‚æ­¤å¤–ï¼ŒViLexèƒ½å¤Ÿä»¥é›¶æ ·æœ¬ã€æ— ç›‘ç£çš„æ–¹å¼æˆåŠŸæ‰§è¡Œå„ç§DreamBoothä»»åŠ¡ï¼Œæ— éœ€å¾®è°ƒT2Iæ¨¡å‹ã€‚å¦å¤–ï¼ŒViLexä½œä¸ºä¸€ç§å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨ï¼Œåœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸­ç›¸å¯¹äºå¼ºå¤§çš„SigLIPåŸºçº¿æŒç»­æé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06774v1">PDF</a> Tech report. 16 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Visual Lexiconè¿™ä¸€æ–°é¢–è§†è§‰è¯­è¨€ï¼Œå®ƒèƒ½å¤Ÿå°†ä¸°å¯Œçš„å›¾åƒä¿¡æ¯ç¼–ç æˆè¯æ±‡ä»¤çš„æ–‡æœ¬ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å¤æ‚çš„è§†è§‰ç»†èŠ‚ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒVisual Lexiconèƒ½å¤ŸåŒæ—¶æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ï¼Œå®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆå’Œå…¨é¢çš„è§†è§‰åœºæ™¯ç†è§£ã€‚é€šè¿‡è‡ªç›‘ç£å­¦ä¹ æµç¨‹ï¼Œå®ƒç”Ÿæˆäº†ä¼˜åŒ–çš„ä»¤ç‰Œï¼Œç”¨äºé‡å»ºè¾“å…¥å›¾åƒï¼ŒåŒæ—¶ä¿ç•™é«˜ä¿çœŸè¯­ä¹‰çº§é‡å»ºæ‰€éœ€çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒVisual Lexiconä»¤ç‰Œåˆ©ç”¨è‡ªç„¶è¯­è¨€çš„ç»„åˆæ€§ï¼Œå¯ç‹¬ç«‹ä½œä¸ºâ€œæ–‡æœ¬ä»¤ç‰Œâ€ä½¿ç”¨ï¼Œä¹Ÿå¯ä¸è‡ªç„¶è¯­è¨€ä»¤ç‰Œç»“åˆï¼Œæç¤ºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åŒæ—¶ä½¿ç”¨è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œæ¨¡æ‹Ÿæˆ‘ä»¬ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„äº¤äº’æ–¹å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒVisual Lexiconåœ¨å›¾åƒé‡å»ºæ–¹é¢å®ç°äº†é«˜ä¿çœŸåº¦ï¼Œå¹¶èƒ½æˆåŠŸæ‰§è¡Œå„ç§DreamBoothä»»åŠ¡ï¼Œä¸”æ— éœ€å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒVisual Lexiconä½œä¸ºå¼ºå¤§çš„è§†è§‰ç¼–ç å™¨ï¼Œç›¸å¯¹äºå¼ºå¤§çš„SigLIPåŸºçº¿ï¼Œåœ¨15ä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Visual Lexiconæ˜¯ä¸€ç§æ–°é¢–çš„è§†è§‰è¯­è¨€ï¼Œèƒ½å°†å›¾åƒä¿¡æ¯ç¼–ç æˆæ–‡æœ¬ç©ºé—´çš„è¯æ±‡ä»¤ç‰Œã€‚</li>
<li>å®ƒèƒ½åŒæ—¶æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰å†…å®¹å’Œç²¾ç»†çš„è§†è§‰ç»†èŠ‚ã€‚</li>
<li>é€šè¿‡è‡ªç›‘ç£å­¦ä¹ æµç¨‹ï¼ŒVisual Lexiconç”Ÿæˆäº†ç”¨äºé‡å»ºè¾“å…¥å›¾åƒçš„ä»¤ç‰Œã€‚</li>
<li>Visual Lexiconä»¤ç‰Œå¯ç‹¬ç«‹ä½¿ç”¨æˆ–ç»“åˆè‡ªç„¶è¯­è¨€ä»¤ç‰Œï¼Œä¸ºé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æä¾›è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºVisual Lexiconåœ¨å›¾åƒé‡å»ºæ–¹é¢å…·æœ‰é«˜ä¿çœŸåº¦ã€‚</li>
<li>Visual Lexiconèƒ½æˆåŠŸæ‰§è¡Œå„ç§DreamBoothä»»åŠ¡è€Œæ— éœ€å¾®è°ƒæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-72df5042ef65a43a6123414e25877c7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dae4020d6c37c4e42ce041d1175f272.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9b5d9c577f64f8f50b0e1edf057459d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1a0b5381ccc18c183bc192b5120ecdb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-902ce2c0af4b1bb6c3476a019175d349.jpg" align="middle">
</details>




<h2 id="Ranking-aware-adapter-for-text-driven-image-ordering-with-CLIP"><a href="#Ranking-aware-adapter-for-text-driven-image-ordering-with-CLIP" class="headerlink" title="Ranking-aware adapter for text-driven image ordering with CLIP"></a>Ranking-aware adapter for text-driven image ordering with CLIP</h2><p><strong>Authors:Wei-Hsiang Yu, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</strong></p>
<p>Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies typically focus on the reasoning based on a single image and heavily depend on text prompting, limiting their ability to learn comprehensive understanding from multiple images. To address this, we propose an effective yet efficient approach that reframes the CLIP model into a learning-to-rank task and introduces a lightweight adapter to augment CLIP for text-guided image ranking. Specifically, our approach incorporates learnable prompts to adapt to new instructions for ranking purposes and an auxiliary branch with ranking-aware attention, leveraging text-conditioned visual differences for additional supervision in image ranking. Our ranking-aware adapter consistently outperforms fine-tuned CLIPs on various tasks and achieves competitive results compared to state-of-the-art models designed for specific tasks like facial age estimation and image quality assessment. Overall, our approach primarily focuses on ranking images with a single instruction, which provides a natural and generalized way of learning from visual differences across images, bypassing the need for extensive text prompts tailored to individual tasks. Code is available: <a target="_blank" rel="noopener" href="https://github.com/uynaes/RankingAwareCLIP">https://github.com/uynaes/RankingAwareCLIP</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦å®šé‡æ¦‚å¿µï¼Œä¾‹å¦‚é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ã€‚è¿™ä½¿å¾—VLMsèƒ½å¤Ÿæ¢ç´¢å›¾åƒæ’åºå’Œæ£€ç´¢ç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶é€šå¸¸ä¾§é‡äºåŸºäºå•å¼ å›¾åƒçš„æ¨ç†ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºæ–‡æœ¬æç¤ºï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬ä»å¤šå¼ å›¾åƒä¸­å­¦ä¹ å…¨é¢ç†è§£çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†CLIPæ¨¡å‹é‡æ„ä¸ºå­¦ä¹ æ’åä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§é€‚é…å™¨ä»¥å¢å¼ºCLIPè¿›è¡Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒæ’åã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¯å­¦ä¹ çš„æç¤ºæ¥é€‚åº”æ–°çš„æ’åæŒ‡ä»¤ï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰æ’åæ„ŸçŸ¥æ³¨æ„åŠ›çš„è¾…åŠ©åˆ†æ”¯ï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶ä¸‹çš„è§†è§‰å·®å¼‚è¿›è¡Œå›¾åƒæ’åçš„é¢å¤–ç›‘ç£ã€‚æˆ‘ä»¬çš„æ’åæ„ŸçŸ¥é€‚é…å™¨åœ¨å„ç§ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºå¾®è°ƒè¿‡çš„CLIPï¼Œå¹¶åœ¨é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ç­‰ç‰¹å®šä»»åŠ¡è®¾è®¡çš„æœ€å…ˆè¿›æ¨¡å‹ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºä½¿ç”¨å•ä¸ªæŒ‡ä»¤å¯¹å›¾åƒè¿›è¡Œæ’åï¼Œè¿™æä¾›äº†ä¸€ç§ä»å„å›¾åƒä¹‹é—´çš„è§†è§‰å·®å¼‚è¿›è¡Œå­¦ä¹ çš„è‡ªç„¶ä¸”é€šç”¨çš„æ–¹å¼ï¼Œè€Œæ— éœ€é’ˆå¯¹å•ä¸ªä»»åŠ¡é‡èº«å®šåˆ¶å¤§é‡æ–‡æœ¬æç¤ºã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/uynaest">https://github.com/uynaest</a>RankingAwareCLIPæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06760v1">PDF</a> github link: <a target="_blank" rel="noopener" href="https://github.com/uynaes/RankingAwareCLIP">https://github.com/uynaes/RankingAwareCLIP</a></p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥ä¿ƒè¿›äº†ä¸‹æ¸¸ä»»åŠ¡å¦‚é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°çš„å‘å±•ï¼Œå¹¶æ¢ç´¢äº†å›¾åƒæ’åºå’Œæ£€ç´¢ç­‰åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶ä¾èµ–å•å›¾åƒæ¨ç†å’Œæ–‡æœ¬æç¤ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œå°†CLIPæ¨¡å‹é‡æ„ä¸ºå­¦ä¹ æ’åä»»åŠ¡ï¼Œå¹¶å¼•å…¥è½»é‡çº§é€‚é…å™¨è¿›è¡Œæ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒæ’åã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¯å­¦ä¹ çš„æç¤ºæ¥é€‚åº”æ’åæŒ‡ä»¤ï¼Œå¹¶å¼•å…¥å…·æœ‰æ’åæ„ŸçŸ¥æ³¨æ„åŠ›çš„è¾…åŠ©åˆ†æ”¯ï¼Œåˆ©ç”¨æ–‡æœ¬æ¡ä»¶ä¸‹çš„è§†è§‰å·®å¼‚è¿›è¡Œé¢å¤–çš„æ’åç›‘ç£ã€‚æˆ‘ä»¬çš„æ’åæ„ŸçŸ¥é€‚é…å™¨åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸€ç›´ä¼˜äºå¾®è°ƒè¿‡çš„CLIPï¼Œå¹¶åœ¨é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ç­‰ç‰¹å®šä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸æœ€æ–°æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸»è¦å…³æ³¨äºä½¿ç”¨å•ä¸€æŒ‡ä»¤å¯¹å›¾åƒè¿›è¡Œæ’åï¼Œæä¾›ä¸€ç§ä»å›¾åƒé—´è§†è§‰å·®å¼‚å­¦ä¹ çš„è‡ªç„¶ä¸”é€šç”¨æ–¹å¼ï¼Œæ— éœ€é’ˆå¯¹å„ä¸ªä»»åŠ¡å®šåˆ¶å¤§é‡æ–‡æœ¬æç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ç­‰ä¸‹æ¸¸ä»»åŠ¡å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦ä¾èµ–å•å›¾åƒæ¨ç†å’Œæ–‡æœ¬æç¤ºï¼Œç¼ºä¹ä»å¤šå›¾åƒå­¦ä¹ çš„ç»¼åˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸€ç§å°†CLIPæ¨¡å‹é‡æ„ä¸ºå­¦ä¹ æ’åä»»åŠ¡çš„æ–¹æ³•ï¼Œå¼•å…¥è½»é‡çº§é€‚é…å™¨è¿›è¡Œæ–‡æœ¬æŒ‡å¯¼çš„å›¾åƒæ’åã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬é€‚åº”æ’åæŒ‡ä»¤çš„å¯å­¦ä¹ æç¤ºï¼Œä»¥åŠå…·æœ‰æ’åæ„ŸçŸ¥æ³¨æ„åŠ›çš„è¾…åŠ©åˆ†æ”¯ã€‚</li>
<li>æ’åæ„ŸçŸ¥é€‚é…å™¨åœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå¾®è°ƒè¿‡çš„CLIPã€‚</li>
<li>åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œå¦‚é¢éƒ¨å¹´é¾„ä¼°è®¡å’Œå›¾åƒè´¨é‡è¯„ä¼°ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°ä¸æœ€æ–°æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9312efd23a2e4ae251220fbed81f1481.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f1bfcfce028942fc393a3cac11e96c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbc1c04be4e5682412e9073266fe5ed4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78f3c5ee7080d5b1f542d5b2dd9fdb7b.jpg" align="middle">
</details>




<h2 id="Impact-of-Privacy-Parameters-on-Deep-Learning-Models-for-Image-Classification"><a href="#Impact-of-Privacy-Parameters-on-Deep-Learning-Models-for-Image-Classification" class="headerlink" title="Impact of Privacy Parameters on Deep Learning Models for Image   Classification"></a>Impact of Privacy Parameters on Deep Learning Models for Image   Classification</h2><p><strong>Authors:Basanta Chaulagain</strong></p>
<p>The project aims to develop differentially private deep learning models for image classification on CIFAR-10 datasets \cite{cifar10} and analyze the impact of various privacy parameters on model accuracy. We have implemented five different deep learning models, namely ConvNet, ResNet18, EfficientNet, ViT, and DenseNet121 and three supervised classifiers namely K-Nearest Neighbors, Naive Bayes Classifier and Support Vector Machine. We evaluated the performance of these models under varying settings. Our best performing model to date is EfficientNet with test accuracy of $59.63%$ with the following parameters (Adam optimizer, batch size 256, epoch size 100, epsilon value 5.0, learning rate $1e-3$, clipping threshold 1.0, and noise multiplier 0.912). </p>
<blockquote>
<p>è¯¥é¡¹ç›®æ—¨åœ¨é’ˆå¯¹CIFAR-10æ•°æ®é›†å¼€å‘å…·æœ‰å·®åˆ†éšç§ä¿æŠ¤çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶åˆ†æå„ç§éšç§å‚æ•°å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“ã€‚æˆ‘ä»¬å®ç°äº†äº”ç§ä¸åŒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬ConvNetã€ResNet18ã€EfficientNetã€ViTå’ŒDenseNet121ï¼Œä»¥åŠä¸‰ç§æœ‰ç›‘ç£åˆ†ç±»å™¨ï¼Œå³Kè¿‘é‚»ç®—æ³•ã€æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨å’Œæ”¯æŒå‘é‡æœºã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„è®¾ç½®ä¸‹è¯„ä¼°äº†è¿™äº›æ¨¡å‹çš„æ€§èƒ½ã€‚è¿„ä»Šä¸ºæ­¢è¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯EfficientNetï¼Œæµ‹è¯•ç²¾åº¦ä¸º59.63%ï¼Œå…¶å‚æ•°å¦‚ä¸‹ï¼šä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæ‰¹å¤„ç†å¤§å°ä¸º256ï¼Œå‘¨æœŸå¤§å°ä¸º100ï¼Œepsilonå€¼ä¸º5.0ï¼Œå­¦ä¹ ç‡ä¸º$ 1e-3 $ï¼Œè£å‰ªé˜ˆå€¼ä¸º1.0ï¼Œå™ªå£°ä¹˜æ•°ä¸º0.912ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06689v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ç”¨äºCIFAR-10æ•°æ®é›†çš„å·®åˆ†éšç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¹¶åˆ†æä¸åŒéšç§å‚æ•°å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“ã€‚å®ç°äº†EfficientNetç­‰äº”ç§æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œä¸‰ç§æœ‰ç›‘ç£åˆ†ç±»å™¨ï¼Œå¹¶åœ¨ä¸åŒè®¾ç½®ä¸‹è¯„ä¼°äº†å…¶æ€§èƒ½ã€‚è¡¨ç°æœ€å¥½çš„æ¨¡å‹æ˜¯EfficientNetï¼Œæµ‹è¯•ç²¾åº¦ä¸º59.63%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¡¹ç›®ç›®æ ‡æ˜¯å¼€å‘ç”¨äºå›¾åƒåˆ†ç±»çš„å·®åˆ†éšç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨CIFAR-10æ•°æ®é›†ä¸Šã€‚</li>
<li>å®æ–½äº†äº”ç§æ·±åº¦å­¦ä¹ æ¨¡å‹å’Œä¸‰ç§æœ‰ç›‘ç£åˆ†ç±»å™¨ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¡¨ç°å—éšç§å‚æ•°å½±å“ã€‚</li>
<li>EfficientNetæ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œæµ‹è¯•ç²¾åº¦è¾¾åˆ°59.63%ã€‚</li>
<li>æœ€ä½³æ¨¡å‹ä½¿ç”¨çš„å‚æ•°åŒ…æ‹¬Adamä¼˜åŒ–å™¨ã€æ‰¹æ¬¡å¤§å°ä¸º256ã€å‘¨æœŸå¤§å°ä¸º100ç­‰ã€‚</li>
<li>å®ç°äº†å·®åˆ†éšç§ä¿æŠ¤ï¼Œé€šè¿‡è°ƒæ•´epsilonå€¼ã€å­¦ä¹ ç‡ã€è£å‰ªé˜ˆå€¼å’Œå™ªå£°ä¹˜æ•°ç­‰å‚æ•°æ¥ä¿æŠ¤æ•°æ®éšç§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4ee2b4deebfb7fcc58cede6b56a5d80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6cc1a13f097da87c070b9ce352f0979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f264aa1d1da7f5ad26be1823795e79a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a3caeb4f0e4fc4dcc424b6ebd81eb9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28b9f04394dce033e5e61f5f081e25c1.jpg" align="middle">
</details>




<h2 id="Vision-Transformer-based-Semantic-Communications-With-Importance-Aware-Quantization"><a href="#Vision-Transformer-based-Semantic-Communications-With-Importance-Aware-Quantization" class="headerlink" title="Vision Transformer-based Semantic Communications With Importance-Aware   Quantization"></a>Vision Transformer-based Semantic Communications With Importance-Aware   Quantization</h2><p><strong>Authors:Joohyuk Park, Yongjeong Oh, Yongjune Kim, Yo-Seb Jeon</strong></p>
<p>Semantic communications provide significant performance gains over traditional communications by transmitting task-relevant semantic features through wireless channels. However, most existing studies rely on end-to-end (E2E) training of neural-type encoders and decoders to ensure effective transmission of these semantic features. To enable semantic communications without relying on E2E training, this paper presents a vision transformer (ViT)-based semantic communication system with importance-aware quantization (IAQ) for wireless image transmission. The core idea of the presented system is to leverage the attention scores of a pretrained ViT model to quantify the importance levels of image patches. Based on this idea, our IAQ framework assigns different quantization bits to image patches based on their importance levels. This is achieved by formulating a weighted quantization error minimization problem, where the weight is set to be an increasing function of the attention score. Then, an optimal incremental allocation method and a low-complexity water-filling method are devised to solve the formulated problem. Our framework is further extended for realistic digital communication systems by modifying the bit allocation problem and the corresponding allocation methods based on an equivalent binary symmetric channel (BSC) model. Simulations on single-view and multi-view image classification tasks show that our IAQ framework outperforms conventional image compression methods in both error-free and realistic communication scenarios. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡é€šè¿‡æ— çº¿ä¿¡é“ä¼ è¾“ä¸ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ç‰¹å¾ï¼Œç›¸å¯¹äºä¼ ç»Ÿé€šä¿¡æ–¹å¼ï¼Œæä¾›äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¾èµ–äºç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰çš„ç¥ç»å‹ç¼–ç å™¨å’Œè§£ç å™¨çš„è®­ç»ƒï¼Œä»¥ç¡®ä¿è¿™äº›è¯­ä¹‰ç‰¹å¾çš„æœ‰æ•ˆä¼ è¾“ã€‚ä¸ºäº†åœ¨ä¸ä¾èµ–E2Eè®­ç»ƒçš„æƒ…å†µä¸‹å®ç°è¯­ä¹‰é€šä¿¡ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„é‡è¦æ€§æ„ŸçŸ¥é‡åŒ–ï¼ˆIAQï¼‰çš„è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œç”¨äºæ— çº¿å›¾åƒä¼ è¾“ã€‚æœ¬ç³»ç»Ÿçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„ViTæ¨¡å‹çš„æ³¨æ„åŠ›å¾—åˆ†æ¥é‡åŒ–å›¾åƒæ–‘å—çš„é‡è¦æ€§æ°´å¹³ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬çš„IAQæ¡†æ¶æ ¹æ®å›¾åƒæ–‘å—çš„é‡è¦æ€§æ°´å¹³åˆ†é…ä¸åŒçš„é‡åŒ–ä½æ•°ã€‚è¿™æ˜¯é€šè¿‡åˆ¶å®šåŠ æƒé‡åŒ–è¯¯å·®æœ€å°åŒ–é—®é¢˜æ¥å®ç°çš„ï¼Œå…¶ä¸­æƒé‡è®¾ç½®ä¸ºæ³¨æ„åŠ›å¾—åˆ†çš„é€’å¢å‡½æ•°ã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ç§æœ€ä¼˜å¢é‡åˆ†é…æ–¹æ³•å’Œä½å¤æ‚åº¦çš„æ³¨æ°´æ–¹æ³•æ¥è§£å†³æ‰€åˆ¶å®šçš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä¿®æ”¹ä½åˆ†é…é—®é¢˜å’Œç›¸åº”çš„åˆ†é…æ–¹æ³•ï¼ŒåŸºäºç­‰æ•ˆäºŒè¿›åˆ¶å¯¹ç§°ä¿¡é“ï¼ˆBSCï¼‰æ¨¡å‹ï¼Œå°†æˆ‘ä»¬çš„æ¡†æ¶è¿›ä¸€æ­¥æ‰©å±•åˆ°ç°å®çš„æ•°å­—é€šä¿¡ç³»ç»Ÿã€‚åœ¨å•è§†å›¾å’Œå¤šè§†å›¾å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ¨¡æ‹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬çš„IAQæ¡†æ¶åœ¨æ— è¯¯ç å’ŒçœŸå®é€šä¿¡åœºæ™¯ä¸­å‡ä¼˜äºä¼ ç»Ÿå›¾åƒå‹ç¼©æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06038v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„Vision Transformeræ¨¡å‹æ³¨æ„åŠ›å¾—åˆ†çš„é‡è¦æ€§æ„ŸçŸ¥é‡åŒ–ï¼ˆIAQï¼‰æ¡†æ¶ï¼Œå®ç°äº†æ— çº¿å›¾åƒä¼ è¾“çš„è¯­ä¹‰é€šä¿¡ã€‚é€šè¿‡åˆ©ç”¨ViTæ¨¡å‹çš„æ³¨æ„åŠ›å¾—åˆ†æ¥è¡¡é‡å›¾åƒå—çš„é‡è¦æ€§æ°´å¹³ï¼Œå¹¶ä¸ºä¸åŒçš„å›¾åƒå—åˆ†é…ä¸åŒçš„é‡åŒ–ä½æ•°ã€‚é‡‡ç”¨åŠ æƒé‡åŒ–è¯¯å·®æœ€å°åŒ–é—®é¢˜æ¥è§£å†³è¯¥é—®é¢˜ï¼Œå¹¶é’ˆå¯¹ç°å®æ•°å­—é€šä¿¡ç³»ç»Ÿè¿›è¡Œäº†æ¡†æ¶çš„è¿›ä¸€æ­¥æ‰©å±•ã€‚æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼Œè¯¥IAQæ¡†æ¶åœ¨æ— è¯¯ç å’ŒçœŸå®é€šä¿¡åœºæ™¯ä¸­å‡ä¼˜äºä¼ ç»Ÿå›¾åƒå‹ç¼©æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„Vision Transformeræ¨¡å‹çš„æ³¨æ„åŠ›å¾—åˆ†è¯„ä¼°å›¾åƒå—é‡è¦æ€§æ°´å¹³ã€‚</li>
<li>è¯­ä¹‰é€šä¿¡åœ¨ä¸ä¾èµ–ç«¯åˆ°ç«¯è®­ç»ƒçš„æƒ…å†µä¸‹å¯å®ç°é«˜æ€§èƒ½æ— çº¿å›¾åƒä¼ è¾“ã€‚</li>
<li>æå‡ºé‡è¦æ€§æ„ŸçŸ¥é‡åŒ–ï¼ˆIAQï¼‰æ¡†æ¶ï¼Œæ ¹æ®å›¾åƒå—çš„é‡è¦æ€§åˆ†é…ä¸åŒçš„é‡åŒ–ä½æ•°ã€‚</li>
<li>é€šè¿‡åŠ æƒé‡åŒ–è¯¯å·®æœ€å°åŒ–é—®é¢˜æ¥ä¼˜åŒ–IAQæ¡†æ¶çš„æ€§èƒ½ã€‚</li>
<li>æ‰©å±•æ¡†æ¶ä»¥é€‚ç”¨äºç°å®æ•°å­—é€šä¿¡ç³»ç»Ÿï¼ŒåŒ…æ‹¬åŸºäºç­‰æ•ˆäºŒè¿›åˆ¶å¯¹ç§°ä¿¡é“æ¨¡å‹çš„ä½åˆ†é…é—®é¢˜å’Œæ–¹æ³•ã€‚</li>
<li>æ¨¡æ‹Ÿå®éªŒè¡¨æ˜IAQæ¡†æ¶åœ¨æ— è¯¯ç å’ŒçœŸå®é€šä¿¡åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88703b37fa69fcdff72716882f9f1c85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50b46e39d646bbcf54dddb7749c4b7e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32e7b524da3bc1bc740df745b3b350fb.jpg" align="middle">
</details>




<h2 id="RefSAM3D-Adapting-SAM-with-Cross-modal-Reference-for-3D-Medical-Image-Segmentation"><a href="#RefSAM3D-Adapting-SAM-with-Cross-modal-Reference-for-3D-Medical-Image-Segmentation" class="headerlink" title="RefSAM3D: Adapting SAM with Cross-modal Reference for 3D Medical Image   Segmentation"></a>RefSAM3D: Adapting SAM with Cross-modal Reference for 3D Medical Image   Segmentation</h2><p><strong>Authors:Xiang Gao, Kai Lu</strong></p>
<p>The Segment Anything Model (SAM), originally built on a 2D Vision Transformer (ViT), excels at capturing global patterns in 2D natural images but struggles with 3D medical imaging modalities like CT and MRI. These modalities require capturing spatial information in volumetric space for tasks such as organ segmentation and tumor quantification. To address this challenge, we introduce RefSAM3D, which adapts SAM for 3D medical imaging by incorporating a 3D image adapter and cross-modal reference prompt generation. Our approach modifies the visual encoder to handle 3D inputs and enhances the mask decoder for direct 3D mask generation. We also integrate textual prompts to improve segmentation accuracy and consistency in complex anatomical scenarios. By employing a hierarchical attention mechanism, our model effectively captures and integrates information across different scales. Extensive evaluations on multiple medical imaging datasets demonstrate the superior performance of RefSAM3D over state-of-the-art methods. Our contributions advance the application of SAM in accurately segmenting complex anatomical structures in medical imaging. </p>
<blockquote>
<p>åˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰æœ€åˆæ˜¯å»ºç«‹åœ¨äºŒç»´è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¹‹ä¸Šï¼Œæ“…é•¿æ•æ‰äºŒç»´è‡ªç„¶å›¾åƒä¸­çš„å…¨å±€æ¨¡å¼ï¼Œä½†åœ¨å¤„ç†å¦‚CTå’ŒMRIç­‰ä¸‰ç»´åŒ»å­¦å½±åƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›æ¨¡æ€éœ€è¦åœ¨ä½“ç§¯ç©ºé—´ä¸­æ•è·ç©ºé—´ä¿¡æ¯ï¼Œä»¥å®Œæˆå¦‚å™¨å®˜åˆ†å‰²å’Œè‚¿ç˜¤é‡åŒ–ç­‰ä»»åŠ¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefSAM3Dï¼Œå®ƒé€šè¿‡èå…¥ä¸‰ç»´å›¾åƒé€‚é…å™¨å’Œè·¨æ¨¡æ€å‚è€ƒæç¤ºç”Ÿæˆï¼Œå°†SAMæ”¹ç¼–ä¸ºé€‚ç”¨äºä¸‰ç»´åŒ»å­¦å½±åƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿®æ”¹äº†è§†è§‰ç¼–ç å™¨ä»¥å¤„ç†ä¸‰ç»´è¾“å…¥ï¼Œå¹¶å¢å¼ºäº†æ©è†œè§£ç å™¨ä»¥è¿›è¡Œç›´æ¥çš„ä¸‰ç»´æ©è†œç”Ÿæˆã€‚æˆ‘ä»¬è¿˜æ•´åˆäº†æ–‡æœ¬æç¤ºï¼Œä»¥æé«˜å¤æ‚è§£å‰–åœºæ™¯ä¸­çš„åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚é€šè¿‡é‡‡ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒå°ºåº¦ä¸Šæœ‰æ•ˆæ•è·å’Œæ•´åˆä¿¡æ¯ã€‚åœ¨å¤šä¸ªåŒ»å­¦å½±åƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒRefSAM3Dçš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ¨åŠ¨äº†SAMåœ¨åŒ»å­¦å½±åƒä¸­å‡†ç¡®åˆ†å‰²å¤æ‚è§£å‰–ç»“æ„çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05605v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºSegment Anything Modelï¼ˆSAMï¼‰æ„å»ºçš„RefSAM3Dæ¨¡å‹ï¼Œé’ˆå¯¹ä¸‰ç»´åŒ»å­¦å½±åƒå¦‚CTå’ŒMRIçš„åˆ†å‰²é—®é¢˜è¿›è¡Œäº†ä¼˜åŒ–ã€‚å®ƒé€šè¿‡å¼•å…¥ä¸‰ç»´å›¾åƒé€‚é…å™¨å’Œè·¨æ¨¡æ€å‚è€ƒæç¤ºç”Ÿæˆï¼Œæ”¹è¿›äº†è§†è§‰ç¼–ç å™¨å’Œæ©è†œè§£ç å™¨ï¼Œå®ç°äº†ç›´æ¥ç”Ÿæˆä¸‰ç»´æ©è†œçš„åŠŸèƒ½ã€‚æ­¤å¤–ï¼Œè¿˜é€šè¿‡å¼•å…¥æ–‡æœ¬æç¤ºæé«˜äº†å¤æ‚è§£å‰–åœºæ™¯ä¸‹çš„åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚é‡‡ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆæ•æ‰å¹¶æ•´åˆäº†ä¸åŒå°ºåº¦çš„ä¿¡æ¯ã€‚åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒRefSAM3Dçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºSAMåœ¨åŒ»å­¦æˆåƒä¸­å‡†ç¡®åˆ†å‰²å¤æ‚è§£å‰–ç»“æ„çš„åº”ç”¨æä¾›äº†æ–°çš„çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RefSAM3Dæ˜¯åŸºäºSegment Anything Modelï¼ˆSAMï¼‰æ„å»ºçš„ï¼Œä¸“ä¸ºå¤„ç†ä¸‰ç»´åŒ»å­¦å½±åƒè®¾è®¡ã€‚</li>
<li>å®ƒé€šè¿‡å¼•å…¥ä¸‰ç»´å›¾åƒé€‚é…å™¨å’Œè·¨æ¨¡æ€å‚è€ƒæç¤ºç”Ÿæˆï¼Œè§£å†³äº†SAMåœ¨å¤„ç†CTå’ŒMRIç­‰åŒ»å­¦å½±åƒæ—¶çš„å±€é™æ€§ã€‚</li>
<li>RefSAM3Dæ”¹è¿›äº†è§†è§‰ç¼–ç å™¨ä»¥å¤„ç†ä¸‰ç»´è¾“å…¥ï¼Œå¹¶å¢å¼ºäº†æ©è†œè§£ç å™¨ä»¥ç›´æ¥ç”Ÿæˆä¸‰ç»´æ©è†œã€‚</li>
<li>é€šè¿‡å¼•å…¥æ–‡æœ¬æç¤ºï¼Œæé«˜äº†å¤æ‚è§£å‰–åœºæ™¯ä¸‹çš„åˆ†å‰²ç²¾åº¦å’Œä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨åˆ†å±‚æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½æœ‰æ•ˆæ•æ‰å¹¶æ•´åˆä¸åŒå°ºåº¦çš„ä¿¡æ¯ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»å­¦æˆåƒæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒRefSAM3Dçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6be6466209c0efbc9db2b74113c2dcb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4067c1698755c1039699a5ba4d58ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4d16b114ee6e065761f3d5666735f4b.jpg" align="middle">
</details>




<h2 id="Sparse-autoencoders-reveal-selective-remapping-of-visual-concepts-during-adaptation"><a href="#Sparse-autoencoders-reveal-selective-remapping-of-visual-concepts-during-adaptation" class="headerlink" title="Sparse autoencoders reveal selective remapping of visual concepts during   adaptation"></a>Sparse autoencoders reveal selective remapping of visual concepts during   adaptation</h2><p><strong>Authors:Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider</strong></p>
<p>Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g. shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms. </p>
<blockquote>
<p>é’ˆå¯¹ç‰¹å®šç›®çš„è°ƒæ•´åŸºç¡€æ¨¡å‹å·²ç»æˆä¸ºä¸ºä¸‹æ¸¸åº”ç”¨æ„å»ºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ ‡å‡†æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨è°ƒæ•´è¿‡ç¨‹ä¸­å“ªäº›æœºåˆ¶èµ·ä½œç”¨è¿˜æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ºCLIPè§†è§‰è½¬æ¢å™¨å¼€å‘äº†ä¸€ç§æ–°çš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œå‘½åä¸ºPatchSAEï¼Œä»¥åœ¨é¢—ç²’åº¦çº§åˆ«æå–å¯è§£é‡Šçš„æ¦‚å¿µï¼ˆä¾‹å¦‚å½¢çŠ¶ã€é¢œè‰²æˆ–å¯¹è±¡çš„è¯­ä¹‰ï¼‰åŠå…¶è¡¥ä¸å¼çš„ç©ºé—´å½’å±ã€‚æˆ‘ä»¬æ¢è®¨äº†è¿™äº›æ¦‚å¿µå¦‚ä½•å½±å“ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºï¼Œå¹¶ç ”ç©¶äº†æœ€æ–°çš„åŸºäºæç¤ºçš„é€‚åº”æŠ€æœ¯å¦‚ä½•æ”¹å˜æ¨¡å‹è¾“å…¥ä¸è¿™äº›æ¦‚å¿µçš„è”ç³»ã€‚è™½ç„¶é€‚åº”æ¨¡å‹å’Œéé€‚åº”æ¨¡å‹çš„æ¦‚å¿µæ¿€æ´»ç•¥æœ‰å˜åŒ–ï¼Œä½†æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¸¸è§çš„é€‚åº”ä»»åŠ¡ä¸­ï¼Œå¤§éƒ¨åˆ†æ”¶ç›Šéƒ½å¯ä»¥ç”¨éé€‚åº”åŸºç¡€æ¨¡å‹ä¸­å·²ç»å­˜åœ¨çš„æ¦‚å¿µæ¥è§£é‡Šã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œä½¿ç”¨è§†è§‰è½¬æ¢å™¨SAEçš„å…·ä½“æ¡†æ¶ï¼Œå¹¶æä¾›äº†è§£é‡Šé€‚åº”æœºåˆ¶çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05276v1">PDF</a> A demo is available at github.com&#x2F;dynamical-inference&#x2F;patchsae</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸ºç‰¹å®šç›®çš„é€‚åº”åŸºç¡€æ¨¡å‹çš„æ ‡å‡†æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†é€‚åº”è¿‡ç¨‹ä¸­çš„æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œå¼€å‘äº†ä¸€ç§åä¸ºPatchSAEçš„æ–°ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œç”¨äºæå–CLIPè§†è§‰è½¬æ¢å™¨ä¸­çš„å¯è§£é‡Šæ¦‚å¿µï¼Œå¦‚å½¢çŠ¶ã€é¢œè‰²å’Œå¯¹è±¡çš„è¯­ä¹‰ç­‰ï¼Œå¹¶æ¢è®¨è¿™äº›æ¦‚å¿µå¦‚ä½•å½±å“ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºã€‚ç ”ç©¶è¿˜è°ƒæŸ¥äº†æœ€è¿‘çš„æœ€æ–°æç¤ºé©±åŠ¨é€‚åº”æŠ€æœ¯å¦‚ä½•æ”¹å˜æ¨¡å‹è¾“å…¥ä¸è¿™äº›æ¦‚å¿µçš„è”ç³»ã€‚å°½ç®¡é€‚åº”å’Œéé€‚åº”æ¨¡å‹çš„æ¦‚å¿µæ¿€æ´»ç•¥æœ‰å˜åŒ–ï¼Œä½†ç ”ç©¶å‘ç°éé€‚åº”åŸºç¡€æ¨¡å‹å·²ç»å­˜åœ¨çš„æ¦‚å¿µèƒ½å¤Ÿè§£é‡Šå¤§å¤šæ•°å¸¸è§é€‚åº”ä»»åŠ¡ä¸Šçš„å¤§å¤šæ•°å¢ç›Šã€‚æœ¬ç ”ç©¶ä¸ºè®­ç»ƒå’Œä½¿ç”¨ç”¨äºè§†è§‰è½¬æ¢å™¨çš„SAEæä¾›äº†å…·ä½“æ¡†æ¶ï¼Œå¹¶æ·±å…¥è§£é‡Šäº†é€‚åº”æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‚åº”äº†åŸºç¡€æ¨¡å‹ä»¥é€‚åº”ç‰¹å®šç›®çš„å·²æˆä¸ºå»ºç«‹ä¸‹æ¸¸åº”ç”¨æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ ‡å‡†æ–¹æ³•ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åä¸ºPatchSAEçš„ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ç”¨äºCLIPè§†è§‰è½¬æ¢å™¨ï¼Œå¯ä»¥æå–å¯è§£é‡Šçš„æ¦‚å¿µï¼Œå¦‚å½¢çŠ¶ã€é¢œè‰²å’Œå¯¹è±¡è¯­ä¹‰ã€‚</li>
<li>è¿™äº›æ¦‚å¿µåœ¨ä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å½±å“è¢«æ¢ç´¢ã€‚</li>
<li>æœ€æ–°æç¤ºé©±åŠ¨é€‚åº”æŠ€æœ¯æ”¹å˜äº†æ¨¡å‹è¾“å…¥ä¸æ¦‚å¿µä¹‹é—´çš„è”ç³»ã€‚</li>
<li>é€‚åº”å’Œéé€‚åº”æ¨¡å‹ä¹‹é—´çš„æ¦‚å¿µæ¿€æ´»æœ‰è½»å¾®å˜åŒ–ã€‚</li>
<li>éé€‚åº”åŸºç¡€æ¨¡å‹ä¸­çš„ç°æœ‰æ¦‚å¿µå¯ä»¥è§£é‡Šå¤§å¤šæ•°å¸¸è§é€‚åº”ä»»åŠ¡ä¸Šçš„å¢ç›Šã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-488aa8e00555cec465309b563d95dd08.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4dda55f366f6502293efac3e23039301.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ba668415abea3a8d0da1fe158b0e35e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e59a33d72aa4a8c9b595ac0d487ac76d.jpg" align="middle">
</details>




<h2 id="Superpixel-Tokenization-for-Vision-Transformers-Preserving-Semantic-Integrity-in-Visual-Tokens"><a href="#Superpixel-Tokenization-for-Vision-Transformers-Preserving-Semantic-Integrity-in-Visual-Tokens" class="headerlink" title="Superpixel Tokenization for Vision Transformers: Preserving Semantic   Integrity in Visual Tokens"></a>Superpixel Tokenization for Vision Transformers: Preserving Semantic   Integrity in Visual Tokens</h2><p><strong>Authors:Jaihyun Lew, Soohyuk Jang, Jaehoon Lee, Seungryong Yoo, Eunji Kim, Saehyung Lee, Jisoo Mok, Siwon Kim, Sungroh Yoon</strong></p>
<p>Transformers, a groundbreaking architecture proposed for Natural Language Processing (NLP), have also achieved remarkable success in Computer Vision. A cornerstone of their success lies in the attention mechanism, which models relationships among tokens. While the tokenization process in NLP inherently ensures that a single token does not contain multiple semantics, the tokenization of Vision Transformer (ViT) utilizes tokens from uniformly partitioned square image patches, which may result in an arbitrary mixing of visual concepts in a token. In this work, we propose to substitute the grid-based tokenization in ViT with superpixel tokenization, which employs superpixels to generate a token that encapsulates a sole visual concept. Unfortunately, the diverse shapes, sizes, and locations of superpixels make integrating superpixels into ViT tokenization rather challenging. Our tokenization pipeline, comprised of pre-aggregate extraction and superpixel-aware aggregation, overcomes the challenges that arise in superpixel tokenization. Extensive experiments demonstrate that our approach, which exhibits strong compatibility with existing frameworks, enhances the accuracy and robustness of ViT on various downstream tasks. </p>
<blockquote>
<p>Transformeræ˜¯ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æå‡ºçš„çªç ´æ€§æ¶æ„ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å…¶æˆåŠŸçš„å…³é”®å› ç´ ä¹‹ä¸€æ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯¹æ ‡è®°ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚è™½ç„¶è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ ‡è®°åŒ–è¿‡ç¨‹ç¡®ä¿äº†å•ä¸ªæ ‡è®°ä¸åŒ…å«å¤šä¸ªè¯­ä¹‰ï¼Œä½†Vision Transformerï¼ˆViTï¼‰çš„æ ‡è®°åŒ–åˆ©ç”¨äº†å‡åŒ€åˆ†å‰²çš„å›¾åƒå—æ¥ç”Ÿæˆæ ‡è®°ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸€ä¸ªæ ‡è®°ä¸­åŒ…å«å¤šä¸ªè§†è§‰æ¦‚å¿µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨è¶…åƒç´ æ ‡è®°åŒ–æ›¿ä»£ViTä¸­çš„åŸºäºç½‘æ ¼çš„æ ‡è®°åŒ–æ–¹æ³•ã€‚è¶…åƒç´ æ ‡è®°åŒ–ä½¿ç”¨è¶…åƒç´ ç”Ÿæˆä¸€ä¸ªåŒ…å«å•ä¸€è§†è§‰æ¦‚å¿µçš„æ ‡è®°ã€‚ç„¶è€Œï¼Œç”±äºè¶…åƒç´ å…·æœ‰ä¸åŒçš„å½¢çŠ¶ã€å¤§å°å’Œä½ç½®ï¼Œå°†è¶…åƒç´ é›†æˆåˆ°ViTæ ‡è®°åŒ–ä¸­ç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„æ ‡è®°åŒ–ç®¡é“ç”±é¢„èšåˆæå–å’ŒåŸºäºè¶…åƒç´ çš„èšåˆç»„æˆï¼Œå…‹æœäº†è¶…åƒç´ æ ‡è®°åŒ–ä¸­å‡ºç°çš„æŒ‘æˆ˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æ¡†æ¶å…¼å®¹æ€§å¼ºï¼Œæé«˜äº†ViTåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04680v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„Vision Transformeræ¨¡å‹ï¼Œæ¢è®¨äº†å…¶åŸºäºè¶…åƒç´ çš„tokenåŒ–ç­–ç•¥ç›¸å¯¹äºä¼ ç»ŸåŸºäºç½‘æ ¼çš„tokenåŒ–ç­–ç•¥çš„ä¼˜è¶Šæ€§ã€‚è¯¥ç­–ç•¥æ—¨åœ¨ç¡®ä¿æ¯ä¸ªtokenåªåŒ…å«ä¸€ä¸ªè§†è§‰æ¦‚å¿µï¼Œå¹¶æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Vision Transformeræ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œå…¶æˆåŠŸçš„å…³é”®åœ¨äºæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥å»ºæ¨¡tokenä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>ä¼ ç»ŸåŸºäºç½‘æ ¼çš„tokenåŒ–ç­–ç•¥å¯èƒ½å¯¼è‡´å•ä¸ªtokenä¸­æ··åˆå¤šä¸ªè§†è§‰æ¦‚å¿µçš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºè¶…åƒç´ çš„tokenåŒ–ç­–ç•¥ï¼Œç¡®ä¿æ¯ä¸ªtokenåªåŒ…å«ä¸€ä¸ªè§†è§‰æ¦‚å¿µã€‚</li>
<li>å®ç°è¶…åƒç´ tokenåŒ–çš„è¿‡ç¨‹æ¶‰åŠé¢„èšåˆæå–å’Œè¶…çº§åƒç´ æ„ŸçŸ¥èšåˆç­‰æ­¥éª¤ã€‚</li>
<li>è¯¥ç­–ç•¥ä¸ç°æœ‰æ¡†æ¶å…¼å®¹æ€§å¼ºï¼Œå¯æé«˜Vision Transformeråœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59352f55aab62b9c2d31060a204dd3f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77c9aa05596185246575faf54c7a484d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e176a35f518028f0ece149667388aa64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85ac2c193c4fe3ca9da3d7ae3fe2400e.jpg" align="middle">
</details>




<h2 id="MetaFormer-High-fidelity-Metalens-Imaging-via-Aberration-Correcting-Transformers"><a href="#MetaFormer-High-fidelity-Metalens-Imaging-via-Aberration-Correcting-Transformers" class="headerlink" title="MetaFormer: High-fidelity Metalens Imaging via Aberration Correcting   Transformers"></a>MetaFormer: High-fidelity Metalens Imaging via Aberration Correcting   Transformers</h2><p><strong>Authors:Byeonghyeon Lee, Youbin Kim, Yongjae Jo, Hyunsu Kim, Hyemi Park, Yangkyu Kim, Debabrata Mandal, Praneeth Chakravarthula, Inki Kim, Eunbyung Park</strong></p>
<p>Metalens is an emerging optical system with an irreplaceable merit in that it can be manufactured in ultra-thin and compact sizes, which shows great promise of various applications such as medical imaging and augmented&#x2F;virtual reality (AR&#x2F;VR). Despite its advantage in miniaturization, its practicality is constrained by severe aberrations and distortions, which significantly degrade the image quality. Several previous arts have attempted to address different types of aberrations, yet most of them are mainly designed for the traditional bulky lens and not convincing enough to remedy harsh aberrations of the metalens. While there have existed aberration correction methods specifically for metalens, they still fall short of restoration quality. In this work, we propose MetaFormer, an aberration correction framework for metalens-captured images, harnessing Vision Transformers (ViT) that has shown remarkable restoration performance in diverse image restoration tasks. Specifically, we devise a Multiple Adaptive Filters Guidance (MAFG), where multiple Wiener filters enrich the degraded input images with various noise-detail balances, enhancing output restoration quality. In addition, we introduce a Spatial and Transposed self-Attention Fusion (STAF) module, which aggregates features from spatial self-attention and transposed self-attention modules to further ameliorate aberration correction. We conduct extensive experiments, including correcting aberrated images and videos, and clean 3D reconstruction from the degraded images. The proposed method outperforms the previous arts by a significant margin. We further fabricate a metalens and verify the practicality of MetaFormer by restoring the images captured with the manufactured metalens in the wild. Code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://benhenryl.github.io/MetaFormer">https://benhenryl.github.io/MetaFormer</a> </p>
<blockquote>
<p>é‡‘å±é€é•œæ˜¯ä¸€ç§æ–°å…´çš„å…‰å­¦ç³»ç»Ÿï¼Œå…·æœ‰ä¸å¯æ›¿ä»£çš„ä¼˜ç‚¹ï¼Œå³å¯ä»¥åˆ¶é€ ä¸ºè¶…è–„ä¸”ç´§å‡‘çš„å°ºå¯¸ï¼Œåœ¨åŒ»ç–—æˆåƒå’Œå¢å¼º&#x2F;è™šæ‹Ÿç°å®ï¼ˆAR&#x2F;VRï¼‰ç­‰å„ç§åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚å°½ç®¡å…¶åœ¨å°å‹åŒ–æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å…¶å®é™…åº”ç”¨å—åˆ°ä¸¥é‡åƒå·®å’Œå¤±çœŸçš„é™åˆ¶ï¼Œè¿™ä¼šä¸¥é‡é™ä½å›¾åƒè´¨é‡ã€‚ä¹‹å‰çš„å‡ é¡¹æŠ€æœ¯å°è¯•è§£å†³ä¸åŒç±»å‹çš„åƒå·®ï¼Œä½†å¤§å¤šæ•°ä¸»è¦æ˜¯ä¸ºä¼ ç»Ÿçš„ç¬¨é‡é•œå¤´è€Œè®¾è®¡çš„ï¼Œå¹¶ä¸è¶³ä»¥çŸ«æ­£é‡‘å±é€é•œçš„ä¸¥è‹›åƒå·®ã€‚å°½ç®¡å­˜åœ¨ä¸“é—¨é’ˆå¯¹é‡‘å±é€é•œçš„æ¶ˆåƒå·®æ ¡æ­£æ–¹æ³•ï¼Œä½†å®ƒä»¬ä»è¾¾ä¸åˆ°æ‰€éœ€çš„æ¢å¤è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MetaFormerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé‡‘å±é€é•œæ•è·å›¾åƒçš„åƒå·®æ ¡æ­£æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰åœ¨å¤šç§å›¾åƒæ¢å¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ¢å¤æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šé‡è‡ªé€‚åº”æ»¤æ³¢å™¨æŒ‡å¯¼ï¼ˆMAFGï¼‰ï¼Œå…¶ä¸­å¤šä¸ªWieneræ»¤æ³¢å™¨ä»¥ä¸åŒçš„å™ªå£°ç»†èŠ‚å¹³è¡¡ä¸°å¯Œé€€åŒ–è¾“å…¥å›¾åƒï¼Œæé«˜äº†è¾“å‡ºæ¢å¤è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç©ºé—´å’Œè½¬ç½®è‡ªæ³¨æ„åŠ›èåˆï¼ˆSTAFï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—èšåˆäº†æ¥è‡ªç©ºé—´è‡ªæ³¨æ„åŠ›å’Œè½¬ç½®è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æ”¹å–„äº†åƒå·®æ ¡æ­£ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬æ ¡æ­£å¤±çœŸå›¾åƒå’Œè§†é¢‘ï¼Œä»¥åŠä»é€€åŒ–å›¾åƒä¸­è¿›è¡Œæ¸…æ™°çš„3Dé‡å»ºã€‚æ‰€æå‡ºçš„æ–¹æ³•å¤§å¤§ä¼˜äºä»¥å‰çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ¶é€ äº†ä¸€ä¸ªé‡‘å±é€é•œï¼Œå¹¶é€šè¿‡åœ¨å®é™…ç¯å¢ƒä¸­æ¢å¤ç”¨åˆ¶é€ çš„é‡‘å±é€é•œæ•è·çš„å›¾åƒæ¥éªŒè¯MetaFormerçš„å®ç”¨æ€§ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://benhenryl.github.io/MetaFormer">https://benhenryl.github.io/MetaFormer</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04591v1">PDF</a> 19 pages, 18 figures</p>
<p><strong>Summary</strong><br>     é‡‘å±é€é•œæ˜¯ä¸€ç§æ–°å…´çš„å…‰å­¦ç³»ç»Ÿï¼Œå…·æœ‰åˆ¶é€ è¶…è–„ã€è¶…ç´§å‡‘çš„ä¼˜åŠ¿ï¼Œåœ¨åŒ»ç–—æˆåƒã€å¢å¼º&#x2F;è™šæ‹Ÿç°å®ç­‰é¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚ç„¶è€Œï¼Œé‡‘å±é€é•œå­˜åœ¨ä¸¥é‡çš„åƒå·®å’Œå¤±çœŸé—®é¢˜ï¼Œä¸¥é‡å½±å“å›¾åƒè´¨é‡ã€‚æœ¬æ–‡æå‡ºMetaFormeræ¡†æ¶ï¼Œåˆ©ç”¨Vision Transformersï¼ˆViTï¼‰å¯¹é‡‘å±é€é•œæ‹æ‘„çš„å›¾åƒè¿›è¡Œåƒå·®æ ¡æ­£ï¼Œé€šè¿‡Multiple Adaptive Filters Guidanceï¼ˆMAFGï¼‰å’ŒSpatial and Transposed self-Attention Fusionï¼ˆSTAFï¼‰æ¨¡å—æé«˜æ ¡æ­£è´¨é‡ï¼Œå®ç°äº†åƒå·®æ ¡æ­£å’Œå›¾åƒæ¢å¤çš„é«˜è´¨é‡è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘å±é€é•œå…·æœ‰è¶…è–„ã€è¶…ç´§å‡‘çš„ä¼˜åŠ¿ï¼Œåœ¨å¤šç§åº”ç”¨é¢†åŸŸä¸­å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚</li>
<li>é‡‘å±é€é•œå­˜åœ¨ä¸¥é‡çš„åƒå·®å’Œå¤±çœŸé—®é¢˜ï¼Œå½±å“å›¾åƒè´¨é‡ã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨ä¼ ç»Ÿé•œå¤´çš„åƒå·®æ ¡æ­£ï¼Œè€Œå¯¹é‡‘å±é€é•œçš„åƒå·®æ ¡æ­£æ•ˆæœä¸ä½³ã€‚</li>
<li>MetaFormeræ¡†æ¶åˆ©ç”¨Vision Transformersï¼ˆViTï¼‰è¿›è¡Œé‡‘å±é€é•œæ‹æ‘„çš„å›¾åƒåƒå·®æ ¡æ­£ã€‚</li>
<li>MAFGæ¨¡å—é€šè¿‡å¤šä¸ªWieneræ»¤æ³¢å™¨ä¸°å¯Œé€€åŒ–è¾“å…¥å›¾åƒçš„å™ªå£°ç»†èŠ‚å¹³è¡¡ï¼Œæé«˜è¾“å‡ºæ¢å¤è´¨é‡ã€‚</li>
<li>STAFæ¨¡å—ç»“åˆç©ºé—´è‡ªæ³¨æ„åŠ›å’Œè½¬ç½®è‡ªæ³¨æ„åŠ›æ¨¡å—çš„ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æ”¹å–„åƒå·®æ ¡æ­£ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8a0169bcdd84d7b38d15febab77427f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8cdd9ecc33ac8e85c23ddd772230bb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f9258d62db18e9af7d670f66f1e0a52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e3406e3a869959ede983fc3680cd077.jpg" align="middle">
</details>




<h2 id="ARTeFACT-Benchmarking-Segmentation-Models-on-Diverse-Analogue-Media-Damage"><a href="#ARTeFACT-Benchmarking-Segmentation-Models-on-Diverse-Analogue-Media-Damage" class="headerlink" title="ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media   Damage"></a>ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media   Damage</h2><p><strong>Authors:Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson</strong></p>
<p>Accurately detecting and classifying damage in analogue media such as paintings, photographs, textiles, mosaics, and frescoes is essential for cultural heritage preservation. While machine learning models excel in correcting degradation if the damage operator is known a priori, we show that they fail to robustly predict where the damage is even after supervised training; thus, reliable damage detection remains a challenge. Motivated by this, we introduce ARTeFACT, a dataset for damage detection in diverse types analogue media, with over 11,000 annotations covering 15 kinds of damage across various subjects, media, and historical provenance. Furthermore, we contribute human-verified text prompts describing the semantic contents of the images, and derive additional textual descriptions of the annotated damage. We evaluate CNN, Transformer, diffusion-based segmentation models, and foundation vision models in zero-shot, supervised, unsupervised and text-guided settings, revealing their limitations in generalising across media types. Our dataset is available at $\href{<a target="_blank" rel="noopener" href="https://daniela997.github.io/ARTeFACT/%7D%7Bhttps://daniela997.github.io/ARTeFACT/%7D$">https://daniela997.github.io/ARTeFACT/}{https://daniela997.github.io/ARTeFACT/}$</a> as the first-of-its-kind benchmark for analogue media damage detection and restoration. </p>
<blockquote>
<p>åœ¨æ¨¡æ‹Ÿåª’ä½“ï¼ˆå¦‚ç»˜ç”»ã€ç…§ç‰‡ã€çººç»‡å“ã€é©¬èµ›å…‹å’Œå£ç”»ï¼‰ä¸­å‡†ç¡®æ£€æµ‹å’Œåˆ†ç±»æŸä¼¤å¯¹äºæ–‡åŒ–é—äº§ä¿æŠ¤è‡³å…³é‡è¦ã€‚è™½ç„¶æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é¢„å…ˆçŸ¥é“æŸä¼¤æ“ä½œå‘˜çš„æƒ…å†µä¸‹æ“…é•¿çº æ­£é€€åŒ–ï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨ç»è¿‡ç›‘ç£è®­ç»ƒåä»ç„¶æ— æ³•ç¨³å¥åœ°é¢„æµ‹æŸä¼¤ä½ç½®ï¼›å› æ­¤ï¼Œå¯é çš„æŸä¼¤æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ARTeFACTæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç”¨äºæ£€æµ‹å¤šç§ç±»å‹çš„æ¨¡æ‹Ÿåª’ä½“ä¸­çš„æŸä¼¤ï¼ŒåŒ…å«è¶…è¿‡11,000ä¸ªæ³¨é‡Šï¼Œæ¶µç›–å„ç§ä¸»é¢˜ã€åª’ä½“å’Œå†å²æ¥æºçš„15ç§æŸä¼¤ç±»å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†ç»äººå·¥éªŒè¯çš„æ–‡æœ¬æç¤ºæ¥æè¿°å›¾åƒä¸­çš„è¯­ä¹‰å†…å®¹ï¼Œå¹¶æ¨å¯¼äº†æ³¨é‡ŠæŸä¼¤çš„é¢å¤–æ–‡æœ¬æè¿°ã€‚æˆ‘ä»¬è¯„ä¼°äº†CNNã€Transformerã€åŸºäºæ‰©æ•£çš„åˆ†å‰²æ¨¡å‹å’Œè§†è§‰åŸºç¡€æ¨¡å‹åœ¨é›¶æ ·æœ¬ã€ç›‘ç£ã€æ— ç›‘ç£å’Œæ–‡æœ¬å¼•å¯¼ä¸‹çš„è®¾ç½®ï¼Œæ­ç¤ºäº†å®ƒä»¬åœ¨è·¨åª’ä½“ç±»å‹æ¨å¹¿æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨[<a target="_blank" rel="noopener" href="https://daniela997.github.io/ARTeFACT/]%E4%BD%9C%E4%B8%BA%E6%A8%A1%E6%8B%9F%E5%AA%92%E4%BD%93%E6%8D%9F%E4%BC%A4%E6%A3%80%E6%B5%8B%E5%92%8C%E6%81%A2%E5%A4%8D%E7%9A%84%E5%90%8C%E7%B1%BB%E9%A6%96%E5%88%9B%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8%E3%80%82">https://daniela997.github.io/ARTeFACT/]ä½œä¸ºæ¨¡æ‹Ÿåª’ä½“æŸä¼¤æ£€æµ‹å’Œæ¢å¤çš„åŒç±»é¦–åˆ›åŸºå‡†æ•°æ®é›†ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04580v1">PDF</a> Accepted for publication at WACV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ARTeFACTæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸“æ³¨äºæ¨¡æ‹Ÿåª’ä½“ï¼ˆå¦‚ç»˜ç”»ã€ç…§ç‰‡ã€çººç»‡å“ã€é©¬èµ›å…‹å’Œå£ç”»ï¼‰çš„æŸåæ£€æµ‹ã€‚æ•°æ®é›†åŒ…å«è¶…è¿‡11,000ä¸ªæ³¨é‡Šï¼Œæ¶µç›–15ç§ä¸åŒç±»å‹çš„æŸåï¼Œæ¶‰åŠå„ç§ä¸»é¢˜ã€åª’ä½“å’Œå†å²æ¥æºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯„ä¼°äº†å‡ ç§ä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆåŒ…æ‹¬CNNã€Transformerã€åŸºäºæ‰©æ•£çš„åˆ†å‰²æ¨¡å‹å’ŒåŸºç¡€è§†è§‰æ¨¡å‹ï¼‰åœ¨ä¸åŒè®¾ç½®ä¸‹çš„æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºäº†å®ƒä»¬åœ¨æ¨å¹¿æ–¹é¢çš„å±€é™æ€§ã€‚æ•°æ®é›†ä½œä¸ºæ¨¡æ‹Ÿåª’ä½“æŸåæ£€æµ‹å’Œæ¢å¤çš„åŸºå‡†æµ‹è¯•é›†ï¼Œç°å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARTeFACTæ•°æ®é›†ç”¨äºæ£€æµ‹æ¨¡æ‹Ÿåª’ä½“ï¼ˆå¦‚ç»˜ç”»ã€ç…§ç‰‡ç­‰ï¼‰çš„æŸåã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡11,000ä¸ªæ³¨é‡Šï¼Œæ¶µç›–å¤šç§æŸåç±»å‹ã€‚</li>
<li>ä¸åŒç±»å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ¨¡æ‹Ÿåª’ä½“æŸåæ£€æµ‹æ–¹é¢çš„å±€é™æ€§è¢«æ­ç¤ºã€‚</li>
<li>æ•°æ®é›†æä¾›å¯¹å›¾åƒè¯­ä¹‰å†…å®¹çš„æ–‡æœ¬æè¿°å’Œå¯¹æŸåçš„é¢å¤–æ–‡æœ¬æè¿°ã€‚</li>
<li>æ•°æ®é›†æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æ¨¡æ‹Ÿåª’ä½“æŸåæ£€æµ‹å’Œæ¢å¤çš„åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>åœ¨ä¸åŒè®¾ç½®ä¸‹ï¼ˆå¦‚é›¶æ ·æœ¬ã€ç›‘ç£ã€æ— ç›‘ç£å’Œæ–‡æœ¬å¼•å¯¼ï¼‰ï¼Œæ¨¡å‹çš„è¡¨ç°å¾—åˆ°äº†è¯„ä¼°ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-45bc2bceba51848e0ba223a933947898.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64bc00862427cbe54298013103996392.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e0a20652877c8354fd716fb6aba16d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a841d1a21236f184e70444337c75622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2d5dfc335b479d754399bafa5c0d5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de483f4ea6185b4e3c5561564f126ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-760c7632127c8d5527c0997039687a1f.jpg" align="middle">
</details>




<h2 id="Florence-VL-Enhancing-Vision-Language-Models-with-Generative-Vision-Encoder-and-Depth-Breadth-Fusion"><a href="#Florence-VL-Enhancing-Vision-Language-Models-with-Generative-Vision-Encoder-and-Depth-Breadth-Fusion" class="headerlink" title="Florence-VL: Enhancing Vision-Language Models with Generative Vision   Encoder and Depth-Breadth Fusion"></a>Florence-VL: Enhancing Vision-Language Models with Generative Vision   Encoder and Depth-Breadth Fusion</h2><p><strong>Authors:Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao, Tianyi Zhou, Bin Xiao</strong></p>
<p>We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2â€™s visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose â€œdepth-breath fusion (DBFusion)â€ to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VLâ€™s visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. <a target="_blank" rel="noopener" href="https://github.com/JiuhaiChen/Florence-VL">https://github.com/JiuhaiChen/Florence-VL</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Florence-VLï¼Œè¿™æ˜¯ç”±Florence-2ç”Ÿæˆå¼è§†è§‰åŸºç¡€æ¨¡å‹äº§ç”Ÿä¸°å¯Œè§†è§‰è¡¨å¾çš„æ–°ä¸€ä»£å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚ä¸åŒäºå¹¿æ³›ä½¿ç”¨çš„é€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„CLIPé£æ ¼è§†è§‰è½¬æ¢å™¨ï¼ŒFlorence-2èƒ½å¤Ÿæ•æ‰ä¸åŒçº§åˆ«å’Œæ–¹é¢çš„è§†è§‰ç‰¹å¾ï¼Œæ›´æ˜“äºé€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾èåˆæ¶æ„å’Œåˆ›æ–°è®­ç»ƒæ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæœ‰æ•ˆåœ°å°†Florence-2çš„è§†è§‰ç‰¹å¾é›†æˆåˆ°é¢„è®­ç»ƒçš„LLMä¸­ï¼Œä¾‹å¦‚Phi 3.5å’ŒLLama 3ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†â€œæ·±åº¦å¹¿åº¦èåˆï¼ˆDBFusionï¼‰â€æ–¹æ³•ï¼Œä»¥èåˆä»ä¸åŒæ·±åº¦å’Œå¤šä¸ªæç¤ºæå–çš„è§†è§‰ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ¨¡å‹è®­ç»ƒåŒ…æ‹¬æ•´ä¸ªæ¨¡å‹çš„ç«¯åˆ°ç«¯é¢„è®­ç»ƒï¼Œç„¶åæ˜¯æŠ•å½±å±‚å’ŒLLMçš„å¾®è°ƒï¼Œåœ¨ç²¾å¿ƒè®¾è®¡çš„åŒ…æ‹¬é«˜è´¨é‡å›¾åƒå­—å¹•å’ŒæŒ‡ä»¤è°ƒæ•´å¯¹çš„å„ç§å¼€æºæ•°æ®é›†ä¸Šã€‚æˆ‘ä»¬å¯¹Florence-VLçš„è§†è§‰ç‰¹å¾è¿›è¡Œäº†å®šé‡åˆ†æå’Œå¯è§†åŒ–ï¼Œè¯æ˜äº†å…¶åœ¨è§†è§‰è¯­è¨€å¯¹é½æ–¹é¢ç›¸è¾ƒäºæµè¡Œè§†è§‰ç¼–ç å™¨çš„ä¼˜åŠ¿ï¼Œå…¶ä¸­ä¸°å¯Œçš„æ·±åº¦å’Œå¹¿åº¦æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚Florence-VLåœ¨å„ç§å¤šæ¨¡æ€å’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¯¹ç°æœ‰æœ€å…ˆè¿›çš„MLLMçš„æ˜¾è‘—æ”¹è¿›ï¼Œæ¶µç›–é€šç”¨VQAã€æ„ŸçŸ¥ã€å¹»è§‰ã€OCRã€å›¾è¡¨ã€çŸ¥è¯†å¯†é›†å‹ç†è§£ç­‰ã€‚ä¸ºäº†ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å’Œå®Œæ•´çš„è®­ç»ƒæ–¹æ¡ˆå‡å·²å¼€æºã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ï¼š<a target="_blank" rel="noopener" href="https://github.com/JiuhaiChen/Florence-VL">https://github.com/JiuhaiChen/Florence-VL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04424v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä½›ç½—ä¼¦è¨-VLç³»åˆ—æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œå®ƒåˆ©ç”¨ä½›ç½—ä¼¦è¨-2è¿™ä¸€ç”Ÿæˆå¼è§†è§‰åŸºç¡€æ¨¡å‹äº§ç”Ÿä¸°å¯Œçš„è§†è§‰è¡¨å¾ã€‚ä¸åŒäºå¹¿æ³›ä½¿ç”¨çš„åŸºäºå¯¹æ¯”å­¦ä¹ çš„CLIPé£æ ¼è§†è§‰è½¬æ¢å™¨ï¼Œä½›ç½—ä¼¦è¨-2èƒ½å¤Ÿæ•æ‰ä¸åŒå±‚çº§å’Œæ–¹é¢çš„è§†è§‰ç‰¹å¾ï¼Œæ›´çµæ´»åœ°é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾èåˆæ¶æ„å’Œåˆ›æ–°è®­ç»ƒç­–ç•¥ï¼Œæœ‰æ•ˆåœ°å°†ä½›ç½—ä¼¦è¨-2çš„è§†è§‰ç‰¹å¾èå…¥é¢„è®­ç»ƒçš„LLMsï¼Œå¦‚Phi 3.5å’ŒLLama 3ã€‚æˆ‘ä»¬ç‰¹åˆ«æå‡ºäº†â€œæ·±åº¦å¹¿åº¦èåˆï¼ˆDBFusionï¼‰â€æŠ€æœ¯ï¼Œèåˆä»ä¸åŒæ·±åº¦å’Œå¤šé‡æç¤ºæå–çš„è§†è§‰ç‰¹å¾ã€‚æ¨¡å‹è®­ç»ƒåŒ…æ‹¬æ•´ä¸ªæ¨¡å‹çš„ç«¯åˆ°ç«¯é¢„è®­ç»ƒï¼Œä»¥åŠæŠ•å½±å±‚å’ŒLLMsçš„å¾®è°ƒï¼Œè®­ç»ƒæ•°æ®æ˜¯ç²¾å¿ƒè®¾è®¡çš„å¤šæ ·åŒ–å¼€æºæ•°æ®é›†ï¼ŒåŒ…æ‹¬é«˜è´¨é‡çš„å›¾ç‰‡æ ‡é¢˜å’ŒæŒ‡ä»¤è°ƒæ•´å¯¹ã€‚å®šé‡åˆ†æä»¥åŠä½›ç½—ä¼¦è¨-VLçš„è§†è§‰ç‰¹å¾å¯è§†åŒ–æ˜¾ç¤ºï¼Œåœ¨è§†è§‰è¯­è¨€å¯¹é½æ–¹é¢ï¼Œç›¸è¾ƒäºæµè¡Œçš„è§†è§‰ç¼–ç å™¨ï¼Œå…¶æ·±åº¦ä¸å¹¿åº¦æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ä½›ç½—ä¼¦è¨-VLåœ¨å„ç§å¤šæ¨¡æ€å’Œè§†è§‰ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œæ¶µç›–äº†ä¸€èˆ¬æ€§é—®ç­”ã€æ„ŸçŸ¥ã€å¹»è§‰ã€OCRã€å›¾è¡¨ã€çŸ¥è¯†å¯†é›†å‹ç†è§£ç­‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œå®Œæ•´çš„è®­ç»ƒç­–ç•¥å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Florence-VLä»‹ç»äº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œç»“åˆäº†è§†è§‰å’Œè¯­è¨€ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨Florence-2è¿™ä¸€ç”Ÿæˆå¼è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†ä¸°å¯Œçš„è§†è§‰è¡¨å¾ã€‚</li>
<li>ä¸åŒäºä¼ ç»Ÿçš„CLIPé£æ ¼è§†è§‰è½¬æ¢å™¨ï¼ŒFlorence-2èƒ½æ•æ‰ä¸åŒå±‚çº§å’Œæ–¹é¢çš„è§†è§‰ç‰¹å¾ï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç‰¹å¾èåˆæ¶æ„åŠè®­ç»ƒç­–ç•¥ï¼Œå°†è§†è§‰ç‰¹å¾èå…¥é¢„è®­ç»ƒçš„LLMsã€‚</li>
<li>æ·±åº¦å¹¿åº¦èåˆï¼ˆDBFusionï¼‰æŠ€æœ¯ç”¨äºèåˆä¸åŒæ·±åº¦å’Œå¤šé‡æç¤ºä¸‹çš„è§†è§‰ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒåŒ…æ‹¬ç«¯åˆ°ç«¯é¢„è®­ç»ƒåŠå¾®è°ƒé˜¶æ®µï¼Œä½¿ç”¨äº†å¤šæ ·åŒ–çš„å¼€æºæ•°æ®é›†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3feb67717273b74f31e6f12e78462691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96ff1592db551cd02af08164fd8b8567.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b5291d1a4531c44597ca6cc291e8f5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc67b4238738ac2f3b4dcf348b12019e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f019fed18b9d718fb48c6b5015aee381.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa9fc75dd1ec8a58cfad69a2c3c5f6dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-50cabe6ac97ef235a9c0d392c2db611c.jpg" align="middle">
</details>




<h2 id="Discriminative-Fine-tuning-of-LVLMs"><a href="#Discriminative-Fine-tuning-of-LVLMs" class="headerlink" title="Discriminative Fine-tuning of LVLMs"></a>Discriminative Fine-tuning of LVLMs</h2><p><strong>Authors:Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Brais Martinez, Georgios Tzimiropoulos</strong></p>
<p>Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a â€œbag of wordsâ€ behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine â€œthe best of both worldsâ€: a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training&#x2F;optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our frameworkâ€™s components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality. </p>
<blockquote>
<p>å¯¹æ¯”è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å·²æˆä¸ºåˆ¤åˆ«å¼è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„é»˜è®¤æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›æœ‰é™ï¼Œé€šå¸¸è¡¨ç°å‡ºâ€œè¯è¢‹â€è¡Œä¸ºã€‚åŒæ—¶ï¼Œç»“åˆäº†è§†è§‰ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²æ˜¾ç¤ºå‡ºè¯¦ç»†çš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„è‡ªå›å½’æ€§è´¨ä½¿å…¶ä¸å¤ªé€‚åˆåˆ¤åˆ«ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç»“åˆäº†â€œä¸¤è€…çš„ä¼˜ç‚¹â€ï¼šä¸€ç§ç”¨äºLVLMsåˆ¤åˆ«ç²¾ç»†è°ƒæ•´çš„æ–°è®­ç»ƒæ–¹æ³•ï¼Œå…·æœ‰å¼ºå¤§çš„åˆ¤åˆ«å’Œç»„åˆèƒ½åŠ›ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†ç”Ÿæˆå¼LVLMè½¬æ¢ä¸ºåˆ¤åˆ«å¼ï¼Œè§£é”å…¶å¼ºå¤§çš„å›¾åƒæ–‡æœ¬åˆ¤åˆ«èƒ½åŠ›ï¼Œå¹¶å¢å¼ºäº†è¯­è¨€ç†è§£ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒ&#x2F;ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨å¯¹æ¯”å’Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŸå¤±æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ä¼´éšç€æ¶ˆèç ”ç©¶ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶ç»„ä»¶çš„å¿…è¦æ€§ã€‚ï¼ˆ2ï¼‰ä½¿ç”¨è½¯æç¤ºå’ŒLoRAé€‚é…å™¨ç»„åˆçš„å‚æ•°é«˜æ•ˆé€‚åº”æ–¹æ³•ã€‚ï¼ˆ3ï¼‰åœ¨å…·æœ‰ç±»ä¼¼è§„æ¨¡çš„æœ€å…ˆè¿›çš„CLIPç±»æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬æ ‡å‡†å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•å’Œç»„åˆæ€§çš„æ˜¾è‘—æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04378v2">PDF</a> Preprint. The first two authors contributed equally</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†å¯¹æ¯”è®­ç»ƒä¸‹çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆCLIPç­‰ï¼‰åœ¨è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„åº”ç”¨åŠå…¶å±€é™æ€§ï¼ŒåŒ…æ‹¬åœ¨è¯­è¨€è¡¨è¾¾å’ŒåŒºåˆ†ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºä¸€ç§æ–°å‹çš„ç”¨äºLVLMsçš„åˆ¤åˆ«å¾®è°ƒè®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨ç»“åˆç”Ÿæˆå’Œåˆ¤åˆ«æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæé«˜æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡å›¾åƒæ–‡æœ¬å¯¹è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆå¯¹æ¯”å’Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŸå¤±è¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨è½¯æç¤ºå’ŒLoRAé€‚é…å™¨ç­‰æ–¹æ³•å®ç°å‚æ•°é«˜æ•ˆé€‚åº”ã€‚è¯¥ç­–ç•¥åœ¨æ ‡å‡†å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¯¹CLIPç­‰æ¨¡å‹çš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¯¹æ¯”è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆCLIPç­‰ï¼‰æ˜¯åˆ¤åˆ«å¼è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„ä¸»æµæ–¹æ³•ï¼Œä½†å­˜åœ¨è¯­è¨€ç†è§£å±€é™ï¼Œè¡¨ç°å‡ºâ€œè¯è¢‹â€è¡Œä¸ºã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ç»“åˆäº†è§†è§‰ç¼–ç å™¨å’ŒLLMsï¼Œèƒ½å¤Ÿè¿›è¡Œè¯¦ç»†çš„è§†è§‰è¯­è¨€æ¨ç†ï¼Œä½†å…¶è‡ªå›å½’æ€§è´¨ä½¿å…¶ä¸é€‚åˆè¿›è¡Œåˆ¤åˆ«ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„LVLMsåˆ¤åˆ«å¾®è°ƒè®­ç»ƒç­–ç•¥ï¼Œç»“åˆäº†ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä½¿æ¨¡å‹å…·æœ‰å¼ºå¤§çš„å›¾åƒæ–‡æœ¬é‰´åˆ«èƒ½åŠ›å’Œå¢å¼ºçš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒæ¡†æ¶åˆ©ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒæ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œç»“åˆå¯¹æ¯”å’Œä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹æŸå¤±ã€‚</li>
<li>é‡‡ç”¨è½¯æç¤ºå’ŒLoRAé€‚é…å™¨ç­‰æ–¹æ³•å®ç°å‚æ•°é«˜æ•ˆé€‚åº”ã€‚</li>
<li>è¯¥ç­–ç•¥åœ¨æ ‡å‡†å›¾åƒæ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒ…æ‹¬å¯¹CLIPç­‰æ¨¡å‹çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f3acabb3086291af1ae3fefeaa4d8d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-33526abf96d9fe32710bb3d943b3b421.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-730f03d7b98e99a8a7743b15faef3e7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44a345fe8a02728acab967cf687ef360.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f3894c3cec45fdb82b434c25080dad0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ef8a090eeac34d3a243329dd6b08363.jpg" align="middle">
</details>




<h2 id="PANGAEA-A-Global-and-Inclusive-Benchmark-for-Geospatial-Foundation-Models"><a href="#PANGAEA-A-Global-and-Inclusive-Benchmark-for-Geospatial-Foundation-Models" class="headerlink" title="PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation   Models"></a>PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation   Models</h2><p><strong>Authors:Valerio Marsocci, Yuru Jia, Georges Le Bellier, David Kerekes, Liang Zeng, Sebastian Hafner, Sebastian Gerard, Eric Brune, Ritu Yadav, Ali Shibli, Heng Fang, Yifang Ban, Maarten Vergauwen, Nicolas Audebert, Andrea Nascetti</strong></p>
<p>Geospatial Foundation Models (GFMs) have emerged as powerful tools for extracting representations from Earth observation data, but their evaluation remains inconsistent and narrow. Existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of GFMs. Additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of GFM performance. In particular, most existing benchmarks are geographically biased towards North America and Europe, questioning the global applicability of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. It establishes a robust and widely applicable benchmark for GFMs. We evaluate the most popular GFMs openly available on this benchmark and analyze their performance across several domains. In particular, we compare these models to supervised baselines (e.g. UNet and vanilla ViT), and assess their effectiveness when faced with limited labeled data. Our findings highlight the limitations of GFMs, under different scenarios, showing that they do not consistently outperform supervised models. PANGAEA is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. By releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. The code is available at <a target="_blank" rel="noopener" href="https://github.com/VMarsocci/pangaea-bench">https://github.com/VMarsocci/pangaea-bench</a>. </p>
<blockquote>
<p>åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰ä½œä¸ºä»åœ°çƒè§‚æµ‹æ•°æ®ä¸­æå–è¡¨å¾çš„å¼ºå¤§å·¥å…·å·²ç»å´­éœ²å¤´è§’ï¼Œä½†å…¶è¯„ä¼°ä»ç„¶å­˜åœ¨ç€ä¸ä¸€è‡´å’Œç‹­çª„çš„é—®é¢˜ã€‚ç°æœ‰ä½œå“ç»å¸¸åœ¨æ¬¡ä¼˜çš„ä¸‹æ¸¸æ•°æ®é›†å’Œä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›ä»»åŠ¡é€šå¸¸è¿‡äºç®€å•æˆ–è¿‡äºç‹­çª„ï¼Œé™åˆ¶äº†è¯„ä¼°åœ¨è¯„ä¼°GFMsç°å®åº”ç”¨ä¸­çš„æœ‰ç”¨æ€§ã€‚æ­¤å¤–ï¼Œå½“å‰è¯„ä¼°åè®®ä¸­ç¼ºä¹å¤šæ ·æ€§ï¼Œæœªèƒ½è€ƒè™‘åˆ°å›¾åƒåˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨ç±»å‹å’Œæ—¶æ€çš„å¤šæ ·æ€§ï¼Œè¿™è¿›ä¸€æ­¥åŠ å‰§äº†å¯¹GFMæ€§èƒ½è¯„ä¼°çš„å¤æ‚æ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¤§å¤šæ•°ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨åœ°ç†ä¸Šåå‘äºåŒ—ç¾å’Œæ¬§æ´²ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹GFMså…¨çƒé€‚ç”¨æ€§çš„è´¨ç–‘ã€‚</p>
</blockquote>
<p>ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ³›å¤§é™†è¯„ä»·åè®®ï¼ˆPANGAEAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ï¼Œæ¶µç›–äº†ä¸€ç³»åˆ—å¤šæ ·åŒ–çš„æ•°æ®é›†ã€ä»»åŠ¡ã€åˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨æ¨¡æ€å’Œæ—¶æ€ã€‚å®ƒä¸ºGFMså»ºç«‹äº†ç¨³å¥ä¸”å¹¿æ³›é€‚ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šå¯¹æœ€å—æ¬¢è¿çš„GFMsè¿›è¡Œäº†å…¬å¼€è¯„ä¼°ï¼Œå¹¶åˆ†æäº†å®ƒä»¬åœ¨å¤šä¸ªé¢†åŸŸä¸­çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†è¿™äº›æ¨¡å‹ä¸æœ‰ç›‘ç£çš„åŸºçº¿ï¼ˆä¾‹å¦‚UNetå’ŒVanilla ViTï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨é¢ä¸´æœ‰é™æ ‡è®°æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†GFMsåœ¨ä¸åŒåœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œè¡¨æ˜å®ƒä»¬å¹¶ä¸å§‹ç»ˆä¼˜äºæœ‰ç›‘ç£æ¨¡å‹ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04204v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åœ°çƒè§‚æµ‹æ•°æ®è¡¨ç¤ºæå–çš„å¼ºå¤§å·¥å…·â€”â€”åœ°ç†ç©ºé—´åŸºç¡€æ¨¡å‹ï¼ˆGFMsï¼‰çš„è¯„ä»·ä»å­˜åœ¨ä¸ä¸€è‡´å’Œç‹­çª„çš„é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶åœ¨è¯„ä»·æ—¶å¸¸å¸¸ä½¿ç”¨æ¬¡ä¼˜çš„ä¸‹æ¸¸æ•°æ®é›†å’Œä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡è¿‡äºç®€å•æˆ–è¿‡äºç‹­çª„ï¼Œé™åˆ¶äº†è¯„ä»·åœ¨è¯„ä¼°GFMsç°å®åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œå½“å‰çš„è¯„ä»·åè®®ç¼ºä¹å¤šæ ·æ€§ï¼Œæœªèƒ½è€ƒè™‘åˆ°å›¾åƒåˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨ç±»å‹å’Œæ—¶æ€çš„å¤šå…ƒæ€§ï¼Œè¿›ä¸€æ­¥å¢åŠ äº†è¯„ä¼°GFMæ€§èƒ½çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå¼•å…¥PANGAEAæ ‡å‡†åŒ–è¯„ä»·åè®®ï¼Œæ¶µç›–å¤šæ ·åŒ–æ•°æ®é›†ã€ä»»åŠ¡ã€åˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨æ¨¡æ€å’Œæ—¶æ€ï¼Œå»ºç«‹ç¨³å¥ä¸”å¹¿æ³›é€‚ç”¨çš„GFMsåŸºå‡†ã€‚è¯„ä¼°äº†æœ€å—æ¬¢è¿çš„GFMsåœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå¹¶åˆ†æäº†å®ƒä»¬åœ¨å¤šä¸ªé¢†åŸŸä¸­çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°†è¿™äº›æ¨¡å‹ä¸ç›‘ç£åŸºçº¿ï¼ˆå¦‚UNetå’ŒVanilla ViTï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨é¢ä¸´æœ‰é™æ ‡è®°æ•°æ®æ—¶çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜GFMsçš„å±€é™æ€§åœ¨ä¸åŒåœºæ™¯ä¸‹è¡¨ç°ä¸ä¸€ï¼Œå¹¶ä¸å§‹ç»ˆä¼˜äºç›‘ç£æ¨¡å‹ã€‚PANGAEAè®¾è®¡å…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§ï¼Œå…è®¸æœªæ¥ç ”ç©¶ä¸­æ— ç¼é›†æˆæ–°æ•°æ®é›†ã€æ¨¡å‹å’Œä»»åŠ¡ã€‚å‘å¸ƒè¯„ä¼°ä»£ç å’ŒåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä½¿å…¶ä»–ç ”ç©¶äººå‘˜èƒ½å¤Ÿå¤åˆ¶æˆ‘ä»¬çš„å®éªŒå¹¶åŸºäºæˆ‘ä»¬çš„å·¥ä½œå»ºç«‹æ–°çš„ç ”ç©¶ï¼Œä¿ƒè¿›å¤§å‹é¢„è®­ç»ƒåœ°ç†ç©ºé—´æ¨¡å‹æ›´ä¸¥è°¨çš„è¯„ä»·åè®®çš„å½¢æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GFMsåœ¨åœ°çƒè§‚æµ‹æ•°æ®è¡¨ç¤ºæå–ä¸­æ˜¯å¼ºå¤§çš„å·¥å…·ï¼Œä½†è¯„ä»·å­˜åœ¨ä¸ä¸€è‡´å’Œç‹­çª„çš„é—®é¢˜ã€‚</li>
<li>å½“å‰è¯„ä»·åè®®ç¼ºä¹å¤šæ ·æ€§ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘å›¾åƒåˆ†è¾¨ç‡ã€ä¼ æ„Ÿå™¨ç±»å‹å’Œæ—¶æ€çš„å¤šå…ƒæ€§ã€‚</li>
<li>å¼•å…¥PANGAEAæ ‡å‡†åŒ–è¯„ä»·åè®®ï¼Œæ¶µç›–å¤šæ ·åŒ–æ•°æ®é›†ã€ä»»åŠ¡ç­‰ï¼Œå»ºç«‹ç¨³å¥ä¸”å¹¿æ³›é€‚ç”¨çš„GFMsåŸºå‡†ã€‚</li>
<li>è¯„ä¼°äº†GFMsåœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå¹¶ä¸ç›‘ç£åŸºçº¿è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>GFMsåœ¨ä¸åŒåœºæ™¯ä¸‹é¢ä¸´å±€é™æ€§ï¼Œå¹¶ä¸å§‹ç»ˆä¼˜äºç›‘ç£æ¨¡å‹ã€‚</li>
<li>PANGAEAè®¾è®¡å…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ— ç¼é›†æˆæ–°æ•°æ®é›†ã€æ¨¡å‹å’Œä»»åŠ¡çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50d3d8d22f6b90ff57a42abee72bfb74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a95737f9734c9c65b5f7ae579355e303.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-126f2b5b14fdde76326269ca7f7371bd.jpg" align="middle">
</details>




<h2 id="CLIP-PING-Boosting-Lightweight-Vision-Language-Models-with-Proximus-Intrinsic-Neighbors-Guidance"><a href="#CLIP-PING-Boosting-Lightweight-Vision-Language-Models-with-Proximus-Intrinsic-Neighbors-Guidance" class="headerlink" title="CLIP-PING: Boosting Lightweight Vision-Language Models with Proximus   Intrinsic Neighbors Guidance"></a>CLIP-PING: Boosting Lightweight Vision-Language Models with Proximus   Intrinsic Neighbors Guidance</h2><p><strong>Authors:Chu Myaet Thwal, Ye Lin Tun, Minh N. H. Nguyen, Eui-Nam Huh, Choong Seon Hong</strong></p>
<p>Beyond the success of Contrastive Language-Image Pre-training (CLIP), recent trends mark a shift toward exploring the applicability of lightweight vision-language models for resource-constrained scenarios. These models often deliver suboptimal performance when relying solely on a single image-text contrastive learning objective, spotlighting the need for more effective training mechanisms that guarantee robust cross-modal feature alignment. In this work, we propose CLIP-PING: Contrastive Language-Image Pre-training with Proximus Intrinsic Neighbors Guidance, a simple and efficient training paradigm designed to boost the performance of lightweight vision-language models with minimal computational overhead and lower data demands. CLIP-PING bootstraps unimodal features extracted from arbitrary pre-trained encoders to obtain intrinsic guidance of proximus neighbor samples, i.e., nearest-neighbor (NN) and cross nearest-neighbor (XNN). We find that extra contrastive supervision from these neighbors substantially boosts cross-modal alignment, enabling lightweight models to learn more generic features with rich semantic diversity. Extensive experiments reveal that CLIP-PING notably surpasses its peers in zero-shot generalization and cross-modal retrieval tasks. Specifically, a 5.5% gain on zero-shot ImageNet1K with 10.7% (I2T) and 5.7% (T2I) on Flickr30K, compared to the original CLIP when using ViT-XS image encoder trained on 3 million (image, text) pairs. Moreover, CLIP-PING showcases strong transferability under the linear evaluation protocol across several downstream tasks. </p>
<blockquote>
<p>æ‘˜è¦ï¼šåœ¨Contrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰å¤§è·æˆåŠŸä¹‹åï¼Œæœ€è¿‘çš„è¶‹åŠ¿å¼€å§‹è½¬å‘æ¢ç´¢è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™åœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚è¿™äº›æ¨¡å‹åœ¨ä»…ä¾èµ–äºå•å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›®æ ‡æ—¶ï¼Œå¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™çªæ˜¾äº†éœ€è¦æ›´æœ‰æ•ˆçš„è®­ç»ƒæœºåˆ¶æ¥ä¿è¯è·¨æ¨¡æ€ç‰¹å¾å¯¹é½çš„ç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CLIP-PINGï¼šå¸¦æœ‰Proximuså†…åœ¨é‚»å±…å¼•å¯¼ï¼ˆProximus Intralexic Inner Neighbors Guidanceï¼‰çš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒã€‚è¿™æ˜¯ä¸€ç§ç®€å•é«˜æ•ˆè®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„è®¡ç®—å¼€é”€å’Œæ›´ä½çš„æ•°æ®éœ€æ±‚æå‡è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚CLIP-PINGå¯åŠ¨ä»ä»»æ„é¢„è®­ç»ƒç¼–ç å™¨ä¸­æå–çš„å•æ¨¡æ€ç‰¹å¾æ¥è·å¾—é‚»è¿‘é‚»å±…æ ·æœ¬çš„å†…åœ¨å¼•å¯¼ï¼Œå³æœ€è¿‘é‚»å±…ï¼ˆNNï¼‰å’Œäº¤å‰æœ€è¿‘é‚»å±…ï¼ˆXNNï¼‰ã€‚æˆ‘ä»¬å‘ç°è¿™äº›é‚»å±…é¢å¤–çš„å¯¹æ¯”ç›‘ç£ä¿¡æ¯ä¼šæ˜¾è‘—ä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ï¼Œä½¿è½»é‡çº§æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´ä¸°å¯Œè¯­ä¹‰å¤šæ ·æ€§çš„æ›´é€šç”¨ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCLIP-PINGåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¿‡äº†åŒç±»æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ViT-XSå›¾åƒç¼–ç å™¨åœ¨3ç™¾ä¸‡ä¸ªï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹ä¸Šè®­ç»ƒçš„CLIPæ¨¡å‹ç›¸æ¯”ï¼Œåœ¨é›¶æ ·æœ¬ImageNet1Kä¸Šæé«˜äº†5.5%ï¼Œåœ¨Flickr30Kä¸Šçš„I2Tå’ŒT2Iåˆ†åˆ«æé«˜äº†10.7%å’Œ5.7%ã€‚æ­¤å¤–ï¼ŒCLIP-PINGåœ¨çº¿æ€§è¯„ä¼°åè®®ä¸‹å…·æœ‰è¾ƒå¼ºçš„è·¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡çš„è¿ç§»èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03871v1">PDF</a> 15 pages, 4 figures, 20 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºContrastive Language-Image Pre-trainingï¼ˆCLIPï¼‰çš„æˆåŠŸï¼Œå½“å‰è¶‹åŠ¿æ˜¯æ¢ç´¢è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™åœºæ™¯ä¸­çš„åº”ç”¨æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ä»…ä¾èµ–å•ä¸€å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›®æ ‡æ—¶é€šå¸¸è¡¨ç°ä¸ä½³ï¼Œå‡¸æ˜¾äº†éœ€è¦æ›´æœ‰æ•ˆçš„è®­ç»ƒæœºåˆ¶æ¥ä¿è¯è·¨æ¨¡æ€ç‰¹å¾å¯¹é½çš„ç¨³å¥æ€§ã€‚æœ¬æ¬¡ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºCLIP-PINGï¼šå¸¦æœ‰Proximuså†…åœ¨é‚»å±…æŒ‡å¼•çš„å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§ç®€å•é«˜æ•ˆçš„è®­ç»ƒèŒƒå¼ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„è®¡ç®—å¼€é”€å’Œæ›´ä½çš„æ•°æ®éœ€æ±‚æå‡è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚CLIP-PINGåˆ©ç”¨ä»ä»»æ„é¢„è®­ç»ƒç¼–ç å™¨æå–çš„å•æ¨¡æ€ç‰¹å¾æ¥å¼•å¯¼é‚»è¿‘é‚»å±…æ ·æœ¬çš„å†…åœ¨æŒ‡å¼•ï¼Œå³æœ€è¿‘é‚»ï¼ˆNNï¼‰å’Œäº¤å‰æœ€è¿‘é‚»ï¼ˆXNNï¼‰ã€‚æˆ‘ä»¬å‘ç°æ¥è‡ªè¿™äº›é‚»å±…çš„é¢å¤–å¯¹æ¯”ç›‘ç£æ˜¾è‘—æå‡äº†è·¨æ¨¡æ€å¯¹é½ï¼Œä½¿è½»é‡çº§æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ›´å…·é€šç”¨æ€§å’Œä¸°å¯Œè¯­ä¹‰å¤šæ ·æ€§çš„ç‰¹å¾ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ï¼ŒCLIP-PINGæ˜¾è‘—ä¼˜äºåŒç±»äº§å“ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ViT-XSå›¾åƒç¼–ç å™¨åœ¨3ç™¾ä¸‡ï¼ˆå›¾åƒã€æ–‡æœ¬ï¼‰å¯¹ä¸Šè®­ç»ƒçš„CLIP-PINGï¼Œåœ¨é›¶æ ·æœ¬ImageNet1Kä¸Šæé«˜äº†5.5%ï¼Œåœ¨Flickr30Kçš„å›¾åƒåˆ°æ–‡æœ¬å’Œæ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†10.7%å’Œ5.7%ã€‚æ­¤å¤–ï¼ŒCLIP-PINGåœ¨çº¿æ€§è¯„ä¼°åè®®ä¸‹åœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç¤ºå‡ºå¼ºå¤§çš„è¿ç§»æ€§ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>å½“å‰è¶‹åŠ¿æ˜¯æ¢ç´¢è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹åœ¨èµ„æºå—é™åœºæ™¯çš„åº”ç”¨ã€‚</li>
<li>ä»…ä¾èµ–å•ä¸€å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ç›®æ ‡çš„æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>CLIP-PINGè®­ç»ƒèŒƒå¼æ—¨åœ¨æå‡è½»é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå…·æœ‰ç®€å•é«˜æ•ˆã€è®¡ç®—å¼€é”€å°ã€æ•°æ®éœ€æ±‚ä½çš„ç‰¹ç‚¹ã€‚</li>
<li>CLIP-PINGåˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨æå–çš„å•æ¨¡æ€ç‰¹å¾è¿›è¡Œé‚»è¿‘é‚»å±…æ ·æœ¬çš„å†…åœ¨æŒ‡å¼•ã€‚</li>
<li>æ¥è‡ªé‚»å±…çš„é¢å¤–å¯¹æ¯”ç›‘ç£æ˜¾è‘—æå‡äº†è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>CLIP-PINGåœ¨é›¶æ ·æœ¬æ³›åŒ–å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåŸCLIPæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-563b639b616e099d7773347545f92fb8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4871e7cc5cf81c462229f56e1a06b1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-353c479b46afffef580789dc87c3cc92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7184ded88d165dec4107f2bed2f0bee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eb18229498cdb3129a61ce387bb902b.jpg" align="middle">
</details>




<h2 id="Beyond-cls-Exploring-the-true-potential-of-Masked-Image-Modeling-representations"><a href="#Beyond-cls-Exploring-the-true-potential-of-Masked-Image-Modeling-representations" class="headerlink" title="Beyond [cls]: Exploring the true potential of Masked Image Modeling   representations"></a>Beyond [cls]: Exploring the true potential of Masked Image Modeling   representations</h2><p><strong>Authors:Marcin PrzewiÄ™Åºlikowski, Randall Balestriero, Wojciech JasiÅ„ski, Marek Åšmieja, Bartosz ZieliÅ„ski</strong></p>
<p>Masked Image Modeling (MIM) has emerged as a popular method for Self-Supervised Learning (SSL) of visual representations. However, for high-level perception tasks, MIM-pretrained models offer lower out-of-the-box representation quality than the Joint-Embedding Architectures (JEA) - another prominent SSL paradigm. To understand this performance gap, we analyze the information flow in Vision Transformers (ViT) learned by both approaches. We reveal that whereas JEAs construct their representation on a selected set of relevant image fragments, MIM models aggregate nearly whole image content. Moreover, we demonstrate that MIM-trained ViTs retain valuable information within their patch tokens, which is not effectively captured by the global [cls] token representations. Therefore, selective aggregation of relevant patch tokens, without any fine-tuning, results in consistently higher-quality of MIM representations. To our knowledge, we are the first to highlight the lack of effective representation aggregation as an emergent issue of MIM and propose directions to address it, contributing to future advances in Self-Supervised Learning. </p>
<blockquote>
<p>åŸºäºå›¾åƒæ©è”½å»ºæ¨¡ï¼ˆMasked Image Modelingï¼ŒMIMï¼‰çš„è§†è§‰è¡¨ç¤ºè‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å·²ç»å˜å¾—éå¸¸æµè¡Œã€‚ç„¶è€Œï¼Œå¯¹äºé«˜çº§æ„ŸçŸ¥ä»»åŠ¡ï¼Œä½¿ç”¨MIMé¢„è®­ç»ƒçš„æ¨¡å‹ç›¸è¾ƒäºå¦ä¸€ç§é‡è¦çš„è‡ªç›‘ç£å­¦ä¹ èŒƒå¼â€”â€”è”åˆåµŒå…¥æ¶æ„ï¼ˆJoint-Embedding Architecturesï¼ŒJEAï¼‰æ‰€æä¾›çš„å³æ’å³ç”¨è¡¨ç¤ºè´¨é‡è¾ƒä½ã€‚ä¸ºäº†ç†è§£è¿™ç§æ€§èƒ½å·®è·ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸¤ç§æ–¹æ³•ä¸­è§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformersï¼ŒViTï¼‰çš„ä¿¡æ¯æµã€‚æˆ‘ä»¬å‘ç°ï¼ŒJEAåœ¨å…¶é€‰æ‹©çš„ä¸ä»»åŠ¡ç›¸å…³çš„å›¾åƒç‰‡æ®µä¸Šæ„å»ºè¡¨ç¤ºï¼Œè€ŒMIMæ¨¡å‹åˆ™æ±‡é›†äº†è¿‘ä¹æ•´ä¸ªå›¾åƒçš„å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†MIMè®­ç»ƒçš„ViTåœ¨å…¶è¡¥ä¸æ ‡è®°ä¸­ä¿ç•™äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯æ²¡æœ‰è¢«å…¨å±€[cls]æ ‡è®°è¡¨ç¤ºæœ‰æ•ˆåœ°æ•è·ã€‚å› æ­¤ï¼Œåœ¨ä¸è¿›è¡Œä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæœ‰é€‰æ‹©åœ°èšåˆç›¸å…³çš„è¡¥ä¸æ ‡è®°ï¼Œå¯ä»¥å¾—åˆ°è´¨é‡å§‹ç»ˆè¾ƒé«˜çš„MIMè¡¨ç¤ºã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡å¼ºè°ƒç¼ºä¹æœ‰æ•ˆçš„è¡¨ç¤ºèšåˆä½œä¸ºMIMçš„ä¸€ä¸ªæ–°å…´é—®é¢˜ï¼Œå¹¶æå‡ºäº†è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹å‘ï¼Œä¸ºè‡ªç›‘ç£å­¦ä¹ çš„æœªæ¥å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03215v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ©ç å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰ä½œä¸ºä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¯¹äºé«˜çº§æ„ŸçŸ¥ä»»åŠ¡ï¼ŒMIMé¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç°ä½äºè”åˆåµŒå…¥æ¶æ„ï¼ˆJEAï¼‰ã€‚é€šè¿‡åˆ†æä¸¤ç§æ–¹æ³•çš„è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸­çš„ä¿¡æ¯æµåŠ¨ï¼Œæˆ‘ä»¬å‘ç°JEAåœ¨é€‰å®šçš„ç›¸å…³å›¾åƒç‰‡æ®µä¸Šæ„å»ºè¡¨ç¤ºï¼Œè€ŒMIMæ¨¡å‹åˆ™å‡ ä¹æ•´åˆäº†æ•´ä¸ªå›¾åƒå†…å®¹ã€‚æ­¤å¤–ï¼ŒMIMè®­ç»ƒçš„ViTåœ¨è¡¥ä¸ä»¤ç‰Œä¸­ä¿ç•™äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä½†å…¨çƒ[cls]ä»¤ç‰Œè¡¨ç¤ºå¹¶æ²¡æœ‰æœ‰æ•ˆåœ°æ•è·è¿™äº›ä¿¡æ¯ã€‚å› æ­¤ï¼Œé€šè¿‡é€‰æ‹©æ€§èšåˆç›¸å…³è¡¥ä¸ä»¤ç‰Œï¼Œæ— éœ€å¾®è°ƒå³å¯æé«˜MIMè¡¨ç¤ºçš„è´¨é‡ã€‚æˆ‘ä»¬æ˜¯é¦–æ‰¹å¼ºè°ƒMIMç¼ºä¹æœ‰æ•ˆè¡¨ç¤ºèšåˆé—®é¢˜å¹¶æå‡ºè§£å†³æ–¹å‘çš„ç ”ç©¶äººå‘˜ä¹‹ä¸€ï¼Œä¸ºè‡ªç›‘ç£å­¦ä¹ çš„æœªæ¥å‘å±•åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIMä½œä¸ºä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ ä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>å¯¹äºé«˜çº§æ„ŸçŸ¥ä»»åŠ¡ï¼ŒMIMé¢„è®­ç»ƒæ¨¡å‹çš„è¡¨ç°ä½äºJEAã€‚</li>
<li>JEAåœ¨é€‰å®šçš„ç›¸å…³å›¾åƒç‰‡æ®µä¸Šæ„å»ºè¡¨ç¤ºï¼Œè€ŒMIMæ¨¡å‹æ•´åˆäº†å‡ ä¹æ•´ä¸ªå›¾åƒå†…å®¹ã€‚</li>
<li>MIMè®­ç»ƒçš„ViTåœ¨è¡¥ä¸ä»¤ç‰Œä¸­ä¿ç•™äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
<li>å…¨çƒ[cls]ä»¤ç‰Œè¡¨ç¤ºåœ¨MIMä¸­å¹¶æ²¡æœ‰æœ‰æ•ˆåœ°æ•è·æ‰€æœ‰æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡é€‰æ‹©æ€§èšåˆç›¸å…³è¡¥ä¸ä»¤ç‰Œï¼Œå¯ä»¥æé«˜MIMè¡¨ç¤ºçš„è´¨é‡ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59cddc933fd7c82994acfcf49feefb5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd3feebd0ab87d7cc587204e97bab4df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-050ba928c04d169956422a2288cd6204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b45fbb7c68f03b597258893a7bdc0a4.jpg" align="middle">
</details>




<h2 id="FCL-ViT-Task-Aware-Attention-Tuning-for-Continual-Learning"><a href="#FCL-ViT-Task-Aware-Attention-Tuning-for-Continual-Learning" class="headerlink" title="FCL-ViT: Task-Aware Attention Tuning for Continual Learning"></a>FCL-ViT: Task-Aware Attention Tuning for Continual Learning</h2><p><strong>Authors:Anestis Kaimakamidis, Ioannis Pitas</strong></p>
<p>Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN) knowledge to new tasks, without forgetting the old ones. However, modern CL techniques focus on provisioning memory capabilities to existing DNN models rather than designing new ones that are able to adapt according to the task at hand. This paper presents the novel Feedback Continual Learning Vision Transformer (FCL-ViT) that uses a feedback mechanism to generate real-time dynamic attention features tailored to the current task. The FCL-ViT operates in two Phases. In phase 1, the generic image features are produced and determine where the Transformer should attend on the current image. In phase 2, task-specific image features are generated that leverage dynamic attention. To this end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs) are introduced that operate in both phases and are responsible for tuning the TABs attention, respectively. The FCL-ViT surpasses state-of-the-art performance on Continual Learning compared to benchmark methods, while retaining a small number of trainable DNN parameters. </p>
<blockquote>
<p>æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ¶‰åŠå°†å…ˆå‰çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çŸ¥è¯†é€‚åº”äºæ–°ä»»åŠ¡ï¼ŒåŒæ—¶ä¸å¿˜æ—§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°ä»£CLæŠ€æœ¯æ›´æ³¨é‡ä¸ºç°æœ‰DNNæ¨¡å‹æä¾›è®°å¿†èƒ½åŠ›ï¼Œè€Œä¸æ˜¯è®¾è®¡èƒ½å¤Ÿæ ¹æ®å½“å‰ä»»åŠ¡è¿›è¡Œé€‚åº”çš„æ–°æ¨¡å‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åé¦ˆæŒç»­å­¦ä¹ è§†è§‰è½¬æ¢å™¨ï¼ˆFCL-ViTï¼‰ï¼Œå®ƒä½¿ç”¨åé¦ˆæœºåˆ¶ç”Ÿæˆå®æ—¶åŠ¨æ€æ³¨æ„åŠ›ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯é’ˆå¯¹å½“å‰ä»»åŠ¡è¿›è¡Œå®šåˆ¶ã€‚FCL-ViTåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚åœ¨é˜¶æ®µ1ä¸­ï¼Œç”Ÿæˆé€šç”¨å›¾åƒç‰¹å¾ï¼Œå¹¶ç¡®å®šè½¬æ¢å™¨åº”åœ¨å½“å‰å›¾åƒä¸Šå…³æ³¨çš„ä½ç½®ã€‚åœ¨é˜¶æ®µ2ä¸­ï¼Œç”Ÿæˆç‰¹å®šäºä»»åŠ¡çš„å›¾åƒç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åˆ©ç”¨åŠ¨æ€æ³¨æ„åŠ›ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†å¯è°ƒè‡ªæ³¨æ„åŠ›å—ï¼ˆTABsï¼‰å’Œä»»åŠ¡ç‰¹å®šå—ï¼ˆTSBsï¼‰ï¼Œå®ƒä»¬åœ¨ä¸¤ä¸ªé˜¶æ®µä¸­éƒ½èµ·ä½œç”¨ï¼Œå¹¶åˆ†åˆ«è´Ÿè´£è°ƒæ•´TABsçš„æ³¨æ„åŠ›ã€‚ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒFCL-ViTåœ¨æŒç»­å­¦ä¹ æ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒå°‘çš„å¯è®­ç»ƒDNNå‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02509v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†åŸºäºåé¦ˆæœºåˆ¶çš„åé¦ˆè¿ç»­å­¦ä¹ è§†è§‰å˜å‹å™¨ï¼ˆFCL-ViTï¼‰ï¼Œå®ƒé€šè¿‡ç”Ÿæˆé’ˆå¯¹å½“å‰ä»»åŠ¡çš„å®æ—¶åŠ¨æ€æ³¨æ„åŠ›ç‰¹å¾æ¥é€‚åº”å…ˆå‰çš„æ·±åº¦ç¥ç»ç½‘ç»œçŸ¥è¯†å¹¶å¤„ç†æ–°ä»»åŠ¡ã€‚ä¸¤ä¸ªé˜¶æ®µåˆ†åˆ«è´Ÿè´£ç”Ÿæˆé€šç”¨å›¾åƒç‰¹å¾å’Œä»»åŠ¡ç‰¹å®šå›¾åƒç‰¹å¾ï¼Œé€šè¿‡å¼•å…¥å¯è°ƒè‡ªæ³¨æ„åŠ›å—å’Œä»»åŠ¡ç‰¹å®šå—ï¼Œå®ç°æ³¨æ„åŠ›è°ƒæ•´ã€‚ä¸ä¼ ç»Ÿçš„è¿ç»­å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒFCL-ViTåœ¨ä¿æŒè¾ƒå°çš„ç¥ç»ç½‘ç»œå‚æ•°æ•°é‡çš„åŒæ—¶ï¼Œåœ¨è¿ç»­å­¦ä¹ ä¸Šå®ç°äº†ä¼˜äºå½“å‰æŠ€æœ¯æ°´å¹³çš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FCL-ViTæ˜¯ä¸€ç§åŸºäºåé¦ˆæœºåˆ¶çš„è¿ç»­å­¦ä¹ è§†è§‰å˜å‹å™¨æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡ç”Ÿæˆé’ˆå¯¹å½“å‰ä»»åŠ¡çš„å®æ—¶åŠ¨æ€æ³¨æ„åŠ›ç‰¹å¾æ¥é€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>FCL-ViTåŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”Ÿæˆé€šç”¨å›¾åƒç‰¹å¾ï¼Œç¬¬äºŒé˜¶æ®µç”Ÿæˆä»»åŠ¡ç‰¹å®šå›¾åƒç‰¹å¾ã€‚</li>
<li>å¯è°ƒè‡ªæ³¨æ„åŠ›å—å’Œä»»åŠ¡ç‰¹å®šå—è¢«å¼•å…¥ä»¥è°ƒæ•´æ³¨æ„åŠ›ã€‚</li>
<li>FCL-ViTåœ¨ä¿æŒè¾ƒå°çš„ç¥ç»ç½‘ç»œå‚æ•°æ•°é‡çš„åŒæ—¶è¶…è¶Šäº†å½“å‰çš„è¿ç»­å­¦ä¹ æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>FCL-ViTå…·æœ‰è‰¯å¥½çš„é€‚ç”¨æ€§ï¼Œå¯ä»¥åº”ç”¨äºå¤šç§éœ€è¦è¿ç»­å­¦ä¹ çš„åœºæ™¯å’Œä»»åŠ¡ä¸­ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7fb11ce3271378d7e2ce1575fd1eb01e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8092effbd128551ea00a4113ab0433f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9f407ddab338716c770024206dbdc33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6192050a97975a524756ddf9379b5a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09d5bb2510f0e07822bee59d6da2388a.jpg" align="middle">
</details>




<h2 id="Vision-Transformers-for-Weakly-Supervised-Microorganism-Enumeration"><a href="#Vision-Transformers-for-Weakly-Supervised-Microorganism-Enumeration" class="headerlink" title="Vision Transformers for Weakly-Supervised Microorganism Enumeration"></a>Vision Transformers for Weakly-Supervised Microorganism Enumeration</h2><p><strong>Authors:Javier UreÃ±a Santiago, Thomas StrÃ¶hle, Antonio RodrÃ­guez-SÃ¡nchez, Ruth Breu</strong></p>
<p>Microorganism enumeration is an essential task in many applications, such as assessing contamination levels or ensuring health standards when evaluating surface cleanliness. However, itâ€™s traditionally performed by human-supervised methods that often require manual counting, making it tedious and time-consuming. Previous research suggests automating this task using computer vision and machine learning methods, primarily through instance segmentation or density estimation techniques. This study conducts a comparative analysis of vision transformers (ViTs) for weakly-supervised counting in microorganism enumeration, contrasting them with traditional architectures such as ResNet and investigating ViT-based models such as TransCrowd. We trained different versions of ViTs as the architectural backbone for feature extraction using four microbiology datasets to determine potential new approaches for total microorganism enumeration in images. Results indicate that while ResNets perform better overall, ViTs performance demonstrates competent results across all datasets, opening up promising lines of research in microorganism enumeration. This comparative study contributes to the field of microbial image analysis by presenting innovative approaches to the recurring challenge of microorganism enumeration and by highlighting the capabilities of ViTs in the task of regression counting. </p>
<blockquote>
<p>å¾®ç”Ÿç‰©è®¡æ•°åœ¨è®¸å¤šåº”ç”¨ä¸­æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨è¯„ä¼°æ±¡æŸ“æ°´å¹³æˆ–è¯„ä¼°è¡¨é¢æ¸…æ´åº¦æ—¶ç¡®ä¿å«ç”Ÿæ ‡å‡†ã€‚ç„¶è€Œï¼Œå®ƒé€šå¸¸æ˜¯é‡‡ç”¨äººå·¥ç›‘ç£çš„æ–¹æ³•è¿›è¡Œï¼Œé€šå¸¸éœ€è¦æ‰‹åŠ¨è®¡æ•°ï¼Œè¿™ä½¿å¾—å®ƒå˜å¾—ä¹å‘³ä¸”è€—æ—¶ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ æ–¹æ³•è¿›è¡Œè‡ªåŠ¨åŒ–ï¼Œä¸»è¦é€šè¿‡å®ä¾‹åˆ†å‰²æˆ–å¯†åº¦ä¼°è®¡æŠ€æœ¯å®ç°ã€‚æœ¬ç ”ç©¶å¯¹ç”¨äºå¾®ç”Ÿç‰©è®¡æ•°ä¸­çš„å¼±ç›‘ç£è®¡æ•°çš„è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œå°†å®ƒä»¬ä¸ä¼ ç»Ÿçš„æ¶æ„ï¼ˆå¦‚ResNetï¼‰è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶ç ”ç©¶äº†åŸºäºViTçš„æ¨¡å‹ï¼ˆå¦‚TransCrowdï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨å››ä¸ªå¾®ç”Ÿç‰©å­¦æ•°æ®é›†è®­ç»ƒäº†ä¸åŒç‰ˆæœ¬çš„ViTä½œä¸ºç‰¹å¾æå–çš„æ¶æ„éª¨å¹²ï¼Œä»¥ç¡®å®šå›¾åƒä¸­å¾®ç”Ÿç‰©æ€»æ•°çš„æ½œåœ¨æ–°æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ResNetæ€»ä½“ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†ViTçš„æ€§èƒ½åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºäº†ç›¸å½“çš„ç»“æœï¼Œä¸ºå¾®ç”Ÿç‰©è®¡æ•°å¼€è¾Ÿäº†æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚æœ¬ç ”ç©¶é€šè¿‡æå‡ºåˆ›æ–°çš„è§£å†³å¾®ç”Ÿç‰©è®¡æ•°è¿™ä¸€æŒç»­æŒ‘æˆ˜çš„æ–¹æ³•ï¼Œå¹¶å¼ºè°ƒViTåœ¨å›å½’è®¡æ•°ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œä¸ºå¾®ç”Ÿç‰©å›¾åƒåˆ†æé¢†åŸŸåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02250v1">PDF</a> 8 pages, 3 figures, 3 tables, conference</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰å¯¹å¾®ç”Ÿç‰©è¿›è¡Œå¼±ç›‘ç£è®¡æ•°çš„æ–¹æ³•ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„ResNetæ¶æ„è¿›è¡Œäº†å¯¹æ¯”åˆ†æã€‚é€šè¿‡å››ä¸ªå¾®ç”Ÿç‰©å­¦æ•°æ®é›†è®­ç»ƒä¸åŒç‰ˆæœ¬çš„ViTä½œä¸ºç‰¹å¾æå–çš„æ¶æ„ä¸»å¹²ï¼Œå®éªŒç»“æœè¡¨æ˜ResNetæ€»ä½“ä¸Šè¡¨ç°æ›´å¥½ï¼Œä½†ViTçš„æ€§èƒ½åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šéƒ½è¡¨ç°ä¸ä¿—ã€‚æ­¤é¡¹ç ”ç©¶å¯¹å¾®ç”Ÿç‰©å›¾åƒåˆ†æé¢†åŸŸæå‡ºäº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†ViTåœ¨å›å½’è®¡æ•°ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¾®ç”Ÿç‰©è®¡æ•°åœ¨è®¸å¤šåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œå¦‚è¯„ä¼°æ±¡æŸ“æ°´å¹³æˆ–è¯„ä¼°è¡¨é¢æ¸…æ´åº¦çš„å¥åº·æ ‡å‡†ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡äººå·¥è®¡æ•°ï¼Œç¹çä¸”è€—æ—¶ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è‡ªåŠ¨åŒ–è¿›è¡Œå¾®ç”Ÿç‰©è®¡æ•°çš„å¼±ç›‘ç£æ–¹æ³•ã€‚</li>
<li>å¯¹æ¯”äº†ViTsä¸ä¼ ç»Ÿæ¶æ„ï¼ˆå¦‚ResNetï¼‰åœ¨å¾®ç”Ÿç‰©è®¡æ•°ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å››ä¸ªå¾®ç”Ÿç‰©å­¦æ•°æ®é›†ä¸Šè®­ç»ƒäº†ä¸åŒç‰ˆæœ¬çš„ViTï¼Œå®éªŒç»“æœè¡¨æ˜ViTæ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è™½ç„¶ResNetæ€»ä½“è¡¨ç°æ›´å¥½ï¼Œä½†ViTçš„æ½œåŠ›ä¸å®¹å¿½è§†ï¼Œä¸ºæœªæ¥ç ”ç©¶æ‰“å¼€äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6bc31ff36c835c8bc4f1e6e0d1951fb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5a9a136079a9aca43082148d7b39971.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8e929f32c76ed90d50a14b46ae6ced3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33dbc98a3b2963ad78c5a3fda6853ccd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98a1f020cf919368acf322e99725039c.jpg" align="middle">
</details>




<h2 id="Cascaded-Multi-Scale-Attention-for-Enhanced-Multi-Scale-Feature-Extraction-and-Interaction-with-Low-Resolution-Images"><a href="#Cascaded-Multi-Scale-Attention-for-Enhanced-Multi-Scale-Feature-Extraction-and-Interaction-with-Low-Resolution-Images" class="headerlink" title="Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature   Extraction and Interaction with Low-Resolution Images"></a>Cascaded Multi-Scale Attention for Enhanced Multi-Scale Feature   Extraction and Interaction with Low-Resolution Images</h2><p><strong>Authors:Xiangyong Lu, Masanori Suganuma, Takayuki Okatani</strong></p>
<p>In real-world applications of image recognition tasks, such as human pose estimation, cameras often capture objects, like human bodies, at low resolutions. This scenario poses a challenge in extracting and leveraging multi-scale features, which is often essential for precise inference. To address this challenge, we propose a new attention mechanism, named cascaded multi-scale attention (CMSA), tailored for use in CNN-ViT hybrid architectures, to handle low-resolution inputs effectively. The design of CMSA enables the extraction and seamless integration of features across various scales without necessitating the downsampling of the input image or feature maps. This is achieved through a novel combination of grouped multi-head self-attention mechanisms with window-based local attention and cascaded fusion of multi-scale features over different scales. This architecture allows for the effective handling of features across different scales, enhancing the modelâ€™s ability to perform tasks such as human pose estimation, head pose estimation, and more with low-resolution images. Our experimental results show that the proposed method outperforms existing state-of-the-art methods in these areas with fewer parameters, showcasing its potential for broad application in real-world scenarios where capturing high-resolution images is not feasible. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xyongLu/CMSA">https://github.com/xyongLu/CMSA</a>. </p>
<blockquote>
<p>åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡çš„ç°å®åº”ç”¨ï¼ˆå¦‚äººä½“å§¿æ€ä¼°è®¡ï¼‰ä¸­ï¼Œç›¸æœºé€šå¸¸ä¼šä»¥ä½åˆ†è¾¨ç‡æ•è·ç‰©ä½“ï¼ˆå¦‚äººä½“ï¼‰ã€‚è¿™ç§æƒ…å†µåœ¨æå–å’Œåˆ©ç”¨å¤šå°ºåº¦ç‰¹å¾æ–¹é¢æå‡ºäº†æŒ‘æˆ˜ï¼Œè€Œå¯¹äºç²¾ç¡®æ¨æ–­è€Œè¨€ï¼Œå¤šå°ºåº¦ç‰¹å¾å¾€å¾€æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåä¸ºçº§è”å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆCMSAï¼‰ï¼Œå®ƒé€‚ç”¨äºCNN-ViTæ··åˆæ¶æ„ï¼Œå¯æœ‰æ•ˆå¤„ç†ä½åˆ†è¾¨ç‡è¾“å…¥ã€‚CMSAçš„è®¾è®¡å®ç°äº†è·¨ä¸åŒå°ºåº¦çš„ç‰¹å¾æå–å’Œæ— ç¼é›†æˆï¼Œè€Œæ— éœ€å¯¹è¾“å…¥å›¾åƒæˆ–ç‰¹å¾å›¾è¿›è¡Œé™é‡‡æ ·ã€‚è¿™æ˜¯é€šè¿‡åˆ†ç»„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸åŸºäºçª—å£çš„å±€éƒ¨æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œä»¥åŠåœ¨ä¸åŒå°ºåº¦ä¸Šå®ç°å¤šå°ºåº¦ç‰¹å¾çš„çº§è”èåˆæ¥å®ç°çš„ã€‚è¯¥æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒå°ºåº¦çš„ç‰¹å¾ï¼Œæé«˜æ¨¡å‹åœ¨ä½åˆ†è¾¨ç‡å›¾åƒä¸‹æ‰§è¡Œè¯¸å¦‚äººä½“å§¿æ€ä¼°è®¡ã€å¤´éƒ¨å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œåœ¨è¿™äº›é¢†åŸŸè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®åœºæ™¯ä¸­å¹¿æ³›åº”ç”¨çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— æ³•æ•è·é«˜åˆ†è¾¨ç‡å›¾åƒçš„æƒ…å†µä¸‹ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xyongLu/CMSA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xyongLu/CMSAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02197v1">PDF</a> 9 pages, 4 figures, 5 tables. The paper is under consideration at   Computer Vision and Image Understanding</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹CNN-ViTæ··åˆæ¶æ„çš„çº§è”å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆCMSAï¼‰æœºåˆ¶ï¼Œæœ‰æ•ˆå¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒè¾“å…¥ã€‚CMSAè®¾è®¡èƒ½å¤Ÿæå–å¹¶æ— ç¼èåˆå¤šå°ºåº¦ç‰¹å¾ï¼Œæ— éœ€å¯¹è¾“å…¥å›¾åƒæˆ–ç‰¹å¾å›¾è¿›è¡Œé™é‡‡æ ·æ“ä½œã€‚é€šè¿‡åˆ†ç»„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€åŸºäºçª—å£çš„å±€éƒ¨æ³¨æ„åŠ›å’Œä¸åŒå°ºåº¦ä¸Šçš„å¤šå°ºåº¦ç‰¹å¾çº§è”èåˆï¼Œè¯¥æ¶æ„å®ç°äº†è·¨å°ºåº¦ç‰¹å¾å¤„ç†çš„æœ‰æ•ˆæ€§ï¼Œæé«˜äº†æ¨¡å‹åœ¨äººä½“å§¿æ€ä¼°è®¡ã€å¤´éƒ¨å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¾ƒå°‘çš„å‚æ•°ä¸‹è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒæ•è·ä¸å¯è¡Œçš„å®é™…åœºæ™¯ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åä¸ºçº§è”å¤šå°ºåº¦æ³¨æ„åŠ›ï¼ˆCMSAï¼‰çš„æ–°æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºå¤„ç†ä½åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>CMSAé€‚ç”¨äºCNN-ViTæ··åˆæ¶æ„ï¼Œèƒ½å¤Ÿæå–å¹¶èåˆå¤šå°ºåº¦ç‰¹å¾ã€‚</li>
<li>CMSAè®¾è®¡æ— éœ€å¯¹è¾“å…¥å›¾åƒæˆ–ç‰¹å¾å›¾è¿›è¡Œé™é‡‡æ ·ã€‚</li>
<li>é€šè¿‡ç»“åˆåˆ†ç»„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€çª—å£å±€éƒ¨æ³¨æ„åŠ›å’Œå¤šå°ºåº¦ç‰¹å¾çš„çº§è”èåˆï¼Œå®ç°äº†è·¨å°ºåº¦ç‰¹å¾å¤„ç†çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æœºåˆ¶æé«˜äº†æ¨¡å‹åœ¨äººä½“å§¿æ€ä¼°è®¡ã€å¤´éƒ¨å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒCMSAåœ¨è¾ƒå°‘çš„å‚æ•°ä¸‹è¶…è¶Šäº†ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a227191e8f244f36974e350a578807a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b990d57bd256332b488c47df32033ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8bb3d0115fee4d2a2ab4bd5d35743dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b142d6aff76871d54df6f75a4c62bf69.jpg" align="middle">
</details>




<h2 id="Attacks-on-multimodal-models"><a href="#Attacks-on-multimodal-models" class="headerlink" title="Attacks on multimodal models"></a>Attacks on multimodal models</h2><p><strong>Authors:Viacheslav Iablochnikov, Alexander Rogachev</strong></p>
<p>Today, models capable of working with various modalities simultaneously in a chat format are gaining increasing popularity. Despite this, there is an issue of potential attacks on these models, especially considering that many of them include open-source components. It is important to study whether the vulnerabilities of these components are inherited and how dangerous this can be when using such models in the industry. This work is dedicated to researching various types of attacks on such models and evaluating their generalization capabilities. Modern VLM models (LLaVA, BLIP, etc.) often use pre-trained parts from other models, so the main part of this research focuses on them, specifically on the CLIP architecture and its image encoder (CLIP-ViT) and various patch attack variations for it. </p>
<blockquote>
<p>å½“ä»Šï¼Œèƒ½å¤Ÿåœ¨èŠå¤©æ ¼å¼ä¸­åŒæ—¶å¤„ç†å¤šç§æ¨¡å¼çš„æ¨¡å‹æ­£æ—¥ç›Šå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›æ¨¡å‹å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œä½†å®ƒä»¬ä¹Ÿå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°è®¸å¤šæ¨¡å‹åŒ…å«å¼€æºç»„ä»¶ã€‚å› æ­¤ï¼Œç ”ç©¶è¿™äº›ç»„ä»¶çš„æ¼æ´æ˜¯å¦ä¼šç»§æ‰¿ä»¥åŠå…¶åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½å¸¦æ¥çš„é£é™©æ˜¯éå¸¸å…³é”®çš„ã€‚è¿™é¡¹å·¥ä½œä¸“æ³¨äºç ”ç©¶é’ˆå¯¹æ­¤ç±»æ¨¡å‹çš„å„ç§æ”»å‡»ï¼Œå¹¶è¯„ä¼°å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›ã€‚ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LLaVAã€BLIPç­‰ï¼‰ç»å¸¸ä½¿ç”¨å…¶ä»–æ¨¡å‹çš„é¢„è®­ç»ƒéƒ¨åˆ†ï¼Œå› æ­¤è¿™é¡¹ç ”ç©¶çš„ä¸»è¦éƒ¨åˆ†é›†ä¸­åœ¨è¿™ç±»æ¨¡å‹ä¸Šï¼Œç‰¹åˆ«æ˜¯CLIPæ¶æ„åŠå…¶å›¾åƒç¼–ç å™¨ï¼ˆCLIP-ViTï¼‰ä»¥åŠé’ˆå¯¹å®ƒçš„å„ç§è¡¥ä¸æ”»å‡»å˜ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01725v1">PDF</a> 19 pages, 13 figures, 3 tables</p>
<p><strong>Summary</strong><br>     å½“å‰ï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šç§æ¨¡æ€å¹¶ä»¥èŠå¤©å½¢å¼å·¥ä½œçš„æ¨¡å‹æ­£æ—¥ç›Šå—åˆ°æ¬¢è¿ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å­˜åœ¨æ½œåœ¨çš„å®‰å…¨æ”»å‡»é£é™©ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°è®¸å¤šæ¨¡å‹åŒ…å«å¼€æºç»„ä»¶ã€‚ç ”ç©¶é‡ç‚¹æ˜¯å¯¹è¿™äº›æ¨¡å‹çš„æ”»å‡»ç±»å‹ä»¥åŠè¯„ä¼°å…¶æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç°ä»£VLMæ¨¡å‹ï¼ˆå¦‚LLaVAã€BLIPç­‰ï¼‰åŠå…¶ä¸»è¦åŸºäºCLIPæ¶æ„å’Œå›¾åƒç¼–ç å™¨çš„éƒ¨åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€èŠå¤©æ¨¡å‹æ—¥ç›Šæ™®åŠï¼Œä½†å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚</li>
<li>æ¨¡å‹ä¸­çš„å¼€æºç»„ä»¶å¯èƒ½å¸¦æ¥ç»§æ‰¿çš„æ¼æ´ã€‚</li>
<li>å¯¹è¿™ç±»æ¨¡å‹çš„æ”»å‡»ç±»å‹ç ”ç©¶æ˜¯å…³é”®ã€‚</li>
<li>ç°ä»£VLMæ¨¡å‹ï¼ˆå¦‚LLaVAå’ŒBLIPï¼‰å¹¿æ³›ä½¿ç”¨é¢„è®­ç»ƒéƒ¨åˆ†ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹åœ¨äºCLIPæ¶æ„åŠå…¶å›¾åƒç¼–ç å™¨ï¼ˆCLIP-ViTï¼‰ã€‚</li>
<li>é’ˆå¯¹CLIPæ¶æ„çš„è¡¥ä¸æ”»å‡»å˜ç§æ˜¯ç ”ç©¶çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0368992488fc9dfd045df5f749613da3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1aa27f222feb87c307e9395eb22fbc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a76a69f4eb3b326e58f99a49b4387d49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-127da43047d8f4486e7bbc2977f26847.jpg" align="middle">
</details>




<h2 id="NLPrompt-Noise-Label-Prompt-Learning-for-Vision-Language-Models"><a href="#NLPrompt-Noise-Label-Prompt-Learning-for-Vision-Language-Models" class="headerlink" title="NLPrompt: Noise-Label Prompt Learning for Vision-Language Models"></a>NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</h2><p><strong>Authors:Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu, Jingya Wang, Jingyi Yu, Ye Shi</strong></p>
<p>The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text encoder representations in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representation and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„å‡ºç°ï¼Œå·²ç»å½»åº•æ”¹å˜äº†å›¾åƒæ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶é€šè¿‡æç¤ºå­¦ä¹ ï¼ˆprompt learningï¼‰å®ç°äº†å¹¿æ³›çš„åº”ç”¨ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†ç°å®ä¸–ç•Œçš„æ•°æ®é›†é€šå¸¸åŒ…å«å¸¦æœ‰å™ªå£°çš„æ ‡ç­¾ï¼Œè¿™ä¼šé™ä½æç¤ºå­¦ä¹ çš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨æç¤ºå­¦ä¹ ä¸­ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰æŸå¤±ï¼Œå³PromptMAEï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜å‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—æé«˜å¯¹å™ªå£°æ ‡ç­¾çš„é²æ£’æ€§ã€‚è™½ç„¶MAEç®€å•ç›´è§‚ä¸”ä»¥å…¶ç¨³å¥æ€§è‘—ç§°ï¼Œä½†ç”±äºå…¶æ”¶æ•›é€Ÿåº¦æ…¢ä»¥åŠåœ¨æç¤ºå­¦ä¹ åœºæ™¯å¤–çš„è¡¨ç°ä¸ä½³ï¼Œå®ƒåœ¨å™ªå£°æ ‡ç­¾å­¦ä¹ ä¸­å¾ˆå°‘è¢«ä½¿ç”¨ã€‚ä¸ºäº†é˜æ˜PromptMAEçš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨ç‰¹å¾å­¦ä¹ ç†è®ºè¡¨æ˜MAEèƒ½å¤ŸæŠ‘åˆ¶å™ªå£°æ ·æœ¬çš„å½±å“ï¼Œä»è€Œæé«˜ä¿¡å™ªæ¯”å¹¶å¢å¼ºæ•´ä½“ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæç¤ºçš„æœ€ä¼˜ä¼ è¾“æ•°æ®å‡€åŒ–æ–¹æ³•PromptOTï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºç¨³å¥æ€§ã€‚PromptOTåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨è¡¨ç¤ºä½œä¸ºåŸå‹æ¥æ„å»ºæœ€ä¼˜ä¼ è¾“çŸ©é˜µã€‚è¯¥çŸ©é˜µæœ‰æ•ˆåœ°å°†æ•°æ®é›†åˆ’åˆ†ä¸ºå¹²å‡€å’Œå™ªå£°å­é›†ï¼Œå…è®¸å¯¹å¹²å‡€å­é›†åº”ç”¨äº¤å‰ç†µæŸå¤±ï¼Œå¯¹å™ªå£°å­é›†åº”ç”¨MAEæŸå¤±ã€‚æˆ‘ä»¬çš„å™ªå£°æ ‡ç­¾æç¤ºå­¦ä¹ æ–¹æ³•NLPromptæä¾›äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾è¡¨ç¤ºå’Œç²¾ç¡®å¯¹é½èƒ½åŠ›æ¥è¿›è¡Œç¨³å¥çš„æç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸åŒå™ªå£°è®¾ç½®ä¸‹è¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†NLPromptçš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01256v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPç­‰è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„å‡ºç°ï¼Œå›¾åƒæ–‡æœ¬è¡¨ç¤ºæŠ€æœ¯å¾—ä»¥å¿«é€Ÿå‘å±•å¹¶åº”ç”¨äºå„ç§åœºæ™¯ä¸­çš„æç¤ºå­¦ä¹ ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ•°æ®é›†å¾€å¾€å«æœ‰å™ªå£°æ ‡ç­¾ï¼Œä¼šå½±å“æç¤ºå­¦ä¹ çš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰æŸå¤±è¿›è¡Œæç¤ºå­¦ä¹ ï¼Œå‘½åä¸ºPromptMAEï¼Œæ˜¾è‘—æé«˜å¯¹å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒé«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºæç¤ºçš„æœ€ä¼˜ä¼ è¾“æ•°æ®å‡€åŒ–æ–¹æ³•PromptOTï¼Œè¿›ä¸€æ­¥æé«˜ç¨³å¥æ€§ã€‚NLPromptæ–¹æ³•ç®€å•é«˜æ•ˆï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„è¡¨è¾¾è¡¨ç¤ºå’Œç²¾ç¡®å¯¹é½èƒ½åŠ›ï¼Œå®ç°ç¨³å¥çš„æç¤ºå­¦ä¹ ï¼Œå¹¶åœ¨å„ç§å™ªå£°è®¾ç½®ä¸‹é€šè¿‡å¤§é‡å®éªŒéªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPç­‰è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹æ¨åŠ¨äº†å›¾åƒæ–‡æœ¬è¡¨ç¤ºæŠ€æœ¯çš„é©å‘½ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºæç¤ºå­¦ä¹ ã€‚</li>
<li>ç°å®æ•°æ®é›†ä¸­å­˜åœ¨å™ªå£°æ ‡ç­¾é—®é¢˜ï¼Œå½±å“æç¤ºå­¦ä¹ æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰æŸå¤±è¿›è¡Œæç¤ºå­¦ä¹ ï¼Œæ˜¾è‘—æé«˜å¯¹å™ªå£°æ ‡ç­¾çš„ç¨³å¥æ€§ã€‚</li>
<li>MAEæŸå¤±èƒ½å¤ŸæŠ‘åˆ¶å™ªå£°æ ·æœ¬çš„å½±å“ï¼Œæé«˜ä¿¡å™ªæ¯”ã€‚</li>
<li>å¼•å…¥åŸºäºæç¤ºçš„æœ€ä¼˜ä¼ è¾“æ•°æ®å‡€åŒ–æ–¹æ³•PromptOTï¼Œè¿›ä¸€æ­¥å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>NLPromptæ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰¹æ€§å®ç°ç¨³å¥çš„æç¤ºå­¦ä¹ ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f294ab8dadc656ff111ea5aabe0adb97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-004c1a4389f0de62b2487def004db3e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03d372075a51300414ae8e7869fde6be.jpg" align="middle">
</details>




<h2 id="Inspiring-the-Next-Generation-of-Segment-Anything-Models-Comprehensively-Evaluate-SAM-and-SAM-2-with-Diverse-Prompts-Towards-Context-Dependent-Concepts-under-Different-Scenes"><a href="#Inspiring-the-Next-Generation-of-Segment-Anything-Models-Comprehensively-Evaluate-SAM-and-SAM-2-with-Diverse-Prompts-Towards-Context-Dependent-Concepts-under-Different-Scenes" class="headerlink" title="Inspiring the Next Generation of Segment Anything Models:   Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards   Context-Dependent Concepts under Different Scenes"></a>Inspiring the Next Generation of Segment Anything Models:   Comprehensively Evaluate SAM and SAM 2 with Diverse Prompts Towards   Context-Dependent Concepts under Different Scenes</h2><p><strong>Authors:Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang, Huchuan Lu, Jinsong Ouyang, Georges El Fakhri, Xiaofeng Liu</strong></p>
<p>As a foundational model, SAM has significantly influenced multiple fields within computer vision, and its upgraded version, SAM 2, enhances capabilities in video segmentation, poised to make a substantial impact once again. While SAMs (SAM and SAM 2) have demonstrated excellent performance in segmenting context-independent concepts like people, cars, and roads, they overlook more challenging context-dependent (CD) concepts, such as visual saliency, camouflage, product defects, and medical lesions. CD concepts rely heavily on global and local contextual information, making them susceptible to shifts in different contexts, which requires strong discriminative capabilities from the model. The lack of comprehensive evaluation of SAMs limits understanding of their performance boundaries, which may hinder the design of future models. In this paper, we conduct a thorough quantitative evaluation of SAMs on 11 CD concepts across 2D and 3D images and videos in various visual modalities within natural, medical, and industrial scenes. We develop a unified evaluation framework for SAM and SAM 2 that supports manual, automatic, and intermediate self-prompting, aided by our specific prompt generation and interaction strategies. We further explore the potential of SAM 2 for in-context learning and introduce prompt robustness testing to simulate real-world imperfect prompts. Finally, we analyze the benefits and limitations of SAMs in understanding CD concepts and discuss their future development in segmentation tasks. This work aims to provide valuable insights to guide future research in both context-independent and context-dependent concepts segmentation, potentially informing the development of the next version - SAM 3. </p>
<blockquote>
<p>ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼ŒSAMï¼ˆSelf-Attention Maskï¼‰åœ¨è®¡ç®—æœºè§†è§‰çš„å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œè€Œå…¶å‡çº§ç‰ˆSAM 2åœ¨è§†é¢‘åˆ†å‰²æ–¹é¢å¢å¼ºäº†èƒ½åŠ›ï¼Œæœ‰æœ›å†æ¬¡äº§ç”Ÿå·¨å¤§å½±å“ã€‚è™½ç„¶SAMï¼ˆåŒ…æ‹¬SAMå’ŒSAM 2ï¼‰åœ¨åˆ†å‰²ä¸Šä¸‹æ–‡ç‹¬ç«‹æ¦‚å¿µï¼ˆå¦‚äººã€æ±½è½¦å’Œé“è·¯ï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¿½è§†äº†æ›´å…·æŒ‘æˆ˜æ€§çš„ä¸Šä¸‹æ–‡ä¾èµ–ï¼ˆCDï¼‰æ¦‚å¿µï¼Œå¦‚è§†è§‰æ˜¾è‘—æ€§ã€ä¼ªè£…ã€äº§å“ç¼ºé™·å’ŒåŒ»å­¦ç—…å˜ã€‚CDæ¦‚å¿µä¸¥é‡ä¾èµ–äºå…¨å±€å’Œå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå› æ­¤åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­å®¹æ˜“å‘ç”Ÿå˜åŒ–ï¼Œè¿™è¦æ±‚æ¨¡å‹å…·å¤‡å¼ºå¤§çš„è¾¨åˆ«èƒ½åŠ›ã€‚ç¼ºä¹å¯¹SAMçš„å…¨é¢è¯„ä¼°é™åˆ¶äº†å¯¹å…¶æ€§èƒ½è¾¹ç•Œçš„ç†è§£ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢æœªæ¥æ¨¡å‹çš„è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹SAMåœ¨è·¨è¶Šè‡ªç„¶ã€åŒ»å­¦å’Œå·¥ä¸šåœºæ™¯çš„äºŒç»´å’Œä¸‰ç»´å›¾åƒå’Œè§†é¢‘ä¸­çš„11ä¸ªCDæ¦‚å¿µè¿›è¡Œäº†å…¨é¢çš„å®šé‡è¯„ä¼°ã€‚æˆ‘ä»¬ä¸ºSAMå’ŒSAM 2å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒæ‰‹åŠ¨ã€è‡ªåŠ¨å’Œä¸­é—´è‡ªæç¤ºï¼Œè¾…ä»¥æˆ‘ä»¬ç‰¹å®šçš„æç¤ºç”Ÿæˆå’Œäº¤äº’ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†SAM 2åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥æç¤ºç¨³å¥æ€§æµ‹è¯•æ¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­ä¸å®Œç¾çš„æç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†SAMåœ¨ç†è§£CDæ¦‚å¿µæ–¹é¢çš„ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†å…¶åœ¨åˆ†å‰²ä»»åŠ¡çš„æœªæ¥å‘å±•ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œä»¥æŒ‡å¯¼æœªæ¥åœ¨ä¸Šä¸‹æ–‡ç‹¬ç«‹å’Œä¸Šä¸‹æ–‡ä¾èµ–çš„æ¦‚å¿µåˆ†å‰²æ–¹é¢çš„ç ”ç©¶ï¼Œä¸ºä¸‹ä¸€ç‰ˆæœ¬SAM 3çš„å‘å±•æä¾›ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01240v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹ä½œä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åŸºç¡€æ¨¡å‹ï¼Œå·²ç»å¯¹å¤šä¸ªé¢†åŸŸäº§ç”Ÿäº†æ·±è¿œå½±å“ã€‚å…¶å‡çº§ç‰ˆSAM 2åœ¨è§†é¢‘åˆ†å‰²æ–¹é¢å¢å¼ºäº†èƒ½åŠ›ï¼Œæœ‰æœ›å†æ¬¡äº§ç”Ÿé‡å¤§å½±å“ã€‚è™½ç„¶SAMç³»åˆ—ï¼ˆSAMå’ŒSAM 2ï¼‰åœ¨åˆ†å‰²ç‹¬ç«‹ä¸Šä¸‹æ–‡æ¦‚å¿µï¼ˆå¦‚äººã€æ±½è½¦å’Œé“è·¯ï¼‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å®ƒä»¬å¿½è§†äº†æ›´å…·æŒ‘æˆ˜æ€§çš„ä¾èµ–ä¸Šä¸‹æ–‡ï¼ˆCDï¼‰æ¦‚å¿µï¼Œå¦‚è§†è§‰æ˜¾è‘—æ€§ã€ä¼ªè£…ã€äº§å“ç¼ºé™·å’ŒåŒ»å­¦ç—…ç¶ç­‰ã€‚æœ¬æ–‡è¿›è¡Œäº†å…¨é¢çš„å®šé‡è¯„ä¼°ï¼Œæ¢è®¨äº†SAMç³»åˆ—åœ¨æ¶‰åŠè‡ªç„¶ã€åŒ»å­¦å’Œå·¥ä¸šåœºæ™¯çš„äºŒç»´å’Œä¸‰ç»´å›¾åƒå’Œè§†é¢‘ä¸­çš„11ä¸ªCDæ¦‚å¿µä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒæ‰‹åŠ¨ã€è‡ªåŠ¨å’Œä¸­é—´è‡ªæç¤ºæ–¹å¼ï¼ŒåŒæ—¶æ¢è®¨äº†SAM 2åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å¼•å…¥äº†æç¤ºç¨³å¥æ€§æµ‹è¯•ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ä¸å®Œç¾æç¤ºã€‚åˆ†æSAMç³»åˆ—åœ¨ç†è§£CDæ¦‚å¿µæ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å·¥ä½œæä¾›äº†å®è´µè§è§£ï¼Œå¯èƒ½ä¸ºä¸‹ä¸€ä»£æ¨¡å‹SAM 3çš„å‘å±•æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMä½œä¸ºåŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸäº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œå…¶å‡çº§ç‰ˆSAM 2åœ¨è§†é¢‘åˆ†å‰²èƒ½åŠ›ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>SAMç³»åˆ—åœ¨åˆ†å‰²ç‹¬ç«‹ä¸Šä¸‹æ–‡æ¦‚å¿µä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨ä¾èµ–ä¸Šä¸‹æ–‡çš„å¤æ‚æ¦‚å¿µä¸Šè¡¨ç°æ¬ ä½³ã€‚</li>
<li>æœ¬æ–‡å¯¹SAMç³»åˆ—è¿›è¡Œäº†å…¨é¢çš„å®šé‡è¯„ä¼°ï¼Œæ¶µç›–äº†è‡ªç„¶ã€åŒ»å­¦å’Œå·¥ä¸šåœºæ™¯ä¸­çš„äºŒç»´å’Œä¸‰ç»´å›¾åƒä»¥åŠè§†é¢‘ã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¤šç§æç¤ºæ–¹å¼ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†SAM 2çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†æç¤ºç¨³å¥æ€§æµ‹è¯•ä»¥æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ä¸å®Œç¾æç¤ºã€‚</li>
<li>åˆ†ææ€»ç»“äº†SAMç³»åˆ—åœ¨ç†è§£ä¾èµ–ä¸Šä¸‹æ–‡æ¦‚å¿µæ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3594a8b6bc55e6c7fe63f35b0b0703f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9747fcfed9389ef7a615593f1bc27fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9dcfb838f762b4d34dc189fc6b5bffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670bedf36173cc540a6cdc7184d238af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c9bd318ce6cd271dbc8e8c20e0751bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f425c469cfcb3df7d2f2f6ee5932eebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da0d42cf9117b3e3c59f4f8e6a3f73f7.jpg" align="middle">
</details>




<h2 id="Token-Cropr-Faster-ViTs-for-Quite-a-Few-Tasks"><a href="#Token-Cropr-Faster-ViTs-for-Quite-a-Few-Tasks" class="headerlink" title="Token Cropr: Faster ViTs for Quite a Few Tasks"></a>Token Cropr: Faster ViTs for Quite a Few Tasks</h2><p><strong>Authors:Benjamin Bergner, Christoph Lippert, Aravindh Mahendran</strong></p>
<p>The adoption of Vision Transformers (ViTs) in resource-constrained applications necessitates improvements in inference throughput. To this end several token pruning and merging approaches have been proposed that improve efficiency by successively reducing the number of tokens. However, it remains an open problem to design a token reduction method that is fast, maintains high performance, and is applicable to various vision tasks. In this work, we present a token pruner that uses auxiliary prediction heads that learn to select tokens end-to-end based on task relevance. These auxiliary heads can be removed after training, leading to throughput close to that of a random pruner. We evaluate our method on image classification, semantic segmentation, object detection, and instance segmentation, and show speedups of 1.5 to 4x with small drops in performance. As a best case, on the ADE20k semantic segmentation benchmark, we observe a 2x speedup relative to the no-pruning baseline, with a negligible performance penalty of 0.1 median mIoU across 5 seeds. </p>
<blockquote>
<p>å°†Vision Transformersï¼ˆViTsï¼‰åº”ç”¨äºèµ„æºå—é™çš„åº”ç”¨åœºæ™¯éœ€è¦æ”¹è¿›æ¨ç†ååé‡ã€‚ä¸ºæ­¤ï¼Œå·²ç»æå‡ºäº†å¤šç§ä»¤ç‰Œä¿®å‰ªå’Œåˆå¹¶æ–¹æ³•ï¼Œé€šè¿‡é€æ­¥å‡å°‘ä»¤ç‰Œæ•°é‡æ¥æé«˜æ•ˆç‡ã€‚ç„¶è€Œï¼Œè®¾è®¡ä¸€ä¸ªå¿«é€Ÿã€é«˜æ€§èƒ½ä¸”é€‚ç”¨äºå„ç§è§†è§‰ä»»åŠ¡çš„ä»¤ç‰Œç¼©å‡æ–¹æ³•ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨è¾…åŠ©é¢„æµ‹å¤´çš„ä»¤ç‰Œä¿®å‰ªå™¨ï¼Œè¯¥é¢„æµ‹å¤´èƒ½å¤ŸåŸºäºä»»åŠ¡ç›¸å…³æ€§è¿›è¡Œç«¯åˆ°ç«¯çš„ä»¤ç‰Œé€‰æ‹©ã€‚è¿™äº›è¾…åŠ©å¤´åœ¨è®­ç»ƒåå¯ä»¥ç§»é™¤ï¼Œå¯¼è‡´ååé‡æ¥è¿‘éšæœºä¿®å‰ªå™¨ã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶æ˜¾ç¤ºå‡º1.5åˆ°4å€çš„é€Ÿåº¦æå‡ä»¥åŠæ€§èƒ½çš„å°å¹…ä¸‹é™ã€‚åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œåœ¨ADE20kè¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç›¸å¯¹äºæ— ä¿®å‰ªåŸºå‡†çš„é€Ÿåº¦æå‡äº†2å€ï¼Œåœ¨5ä¸ªç§å­ä¸­çš„ä¸­ä½æ•°mIoUæ€§èƒ½æŸå¤±å¾®ä¹å…¶å¾®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00965v1">PDF</a> 15 pages, 11 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä½¿ç”¨è¾…åŠ©é¢„æµ‹å¤´è¿›è¡Œä»¤ç‰Œé€‰æ‹©çš„æ–¹æ³•ï¼ŒåŸºäºä»»åŠ¡ç›¸å…³æ€§ç«¯å¯¹ç«¯åœ°é€‰æ‹©ä»¤ç‰Œï¼Œä»¥æé«˜Vision Transformeråœ¨èµ„æºå—é™åº”ç”¨ä¸­çš„æ¨ç†é€Ÿåº¦ã€‚è¾…åŠ©å¤´å¯ä»¥åœ¨è®­ç»ƒåç§»é™¤ï¼Œå®ç°æ¥è¿‘äºéšæœºå‰ªæå™¨çš„æ¨ç†é€Ÿåº¦ã€‚è¯„ä¼°è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šçš„é€Ÿåº¦æå‡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) åœ¨èµ„æºå—é™çš„åº”ç”¨ä¸­éœ€è¦æé«˜æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä½¿ç”¨è¾…åŠ©é¢„æµ‹å¤´è¿›è¡Œä»¤ç‰Œé€‰æ‹©å’Œå‡ç¼©çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºä»»åŠ¡ç›¸å…³æ€§ç«¯å¯¹ç«¯åœ°å­¦ä¹ ã€‚</li>
<li>è¾…åŠ©å¤´å¯ä»¥åœ¨è®­ç»ƒåç§»é™¤ï¼Œä½¿å¾—æ¨ç†é€Ÿåº¦æ¥è¿‘äºéšæœºå‰ªæå™¨ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>ä¸æ— å‰ªæåŸºå‡†ç›¸æ¯”ï¼Œåœ¨ADE20kè¯­ä¹‰åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šè§‚å¯Ÿåˆ°äº†2å€çš„é€Ÿåº¦æå‡ã€‚</li>
<li>åœ¨ä¸åŒçš„ç§å­è¿è¡Œå®éªŒä¸­ï¼Œæ€§èƒ½ä¸‹é™å¾®ä¹å…¶å¾®ï¼Œä»…ä¸º0.1çš„ä¸­å€¼mIoUã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b136bb9f6b0037c46ec2a5dfeb253adc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-763c75cc3e28ce47c74291c3ef51ce1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7512307b86f00c626112cdc9dc94e40f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-264d9a156be4d9db923ea605c839d913.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f8f0a49bcf7569b6e3d4935b0f8359.jpg" align="middle">
</details>




<h2 id="Categorical-Keypoint-Positional-Embedding-for-Robust-Animal-Re-Identification"><a href="#Categorical-Keypoint-Positional-Embedding-for-Robust-Animal-Re-Identification" class="headerlink" title="Categorical Keypoint Positional Embedding for Robust Animal   Re-Identification"></a>Categorical Keypoint Positional Embedding for Robust Animal   Re-Identification</h2><p><strong>Authors:Yuhao Lin, Lingqiao Liu, Javen Shi</strong></p>
<p>Animal re-identification (ReID) has become an indispensable tool in ecological research, playing a critical role in tracking population dynamics, analyzing behavioral patterns, and assessing ecological impacts, all of which are vital for informed conservation strategies. Unlike human ReID, animal ReID faces significant challenges due to the high variability in animal poses, diverse environmental conditions, and the inability to directly apply pre-trained models to animal data, making the identification process across species more complex. This work introduces an innovative keypoint propagation mechanism, which utilizes a single annotated image and a pre-trained diffusion model to propagate keypoints across an entire dataset, significantly reducing the cost of manual annotation. Additionally, we enhance the Vision Transformer (ViT) by implementing Keypoint Positional Encoding (KPE) and Categorical Keypoint Positional Embedding (CKPE), enabling the ViT to learn more robust and semantically-aware representations. This provides more comprehensive and detailed keypoint representations, leading to more accurate and efficient re-identification. Our extensive experimental evaluations demonstrate that this approach significantly outperforms existing state-of-the-art methods across four wildlife datasets. The code will be publicly released. </p>
<blockquote>
<p>åŠ¨ç‰©å†è¯†åˆ«ï¼ˆReIDï¼‰å·²æˆä¸ºç”Ÿæ€ç ”ç©¶ä¸­ä¸å¯æˆ–ç¼ºçš„å·¥å…·ï¼Œåœ¨è·Ÿè¸ªç§ç¾¤åŠ¨æ€ã€åˆ†æè¡Œä¸ºæ¨¡å¼ã€è¯„ä¼°ç”Ÿæ€å½±å“ç­‰æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯¹åˆ¶å®šæœ‰é’ˆå¯¹æ€§çš„ä¿æŠ¤ç­–ç•¥è‡³å…³é‡è¦ã€‚ä¸äººç±»ReIDä¸åŒï¼ŒåŠ¨ç‰©ReIDé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºåŠ¨ç‰©å§¿åŠ¿é«˜åº¦å¤šå˜ã€ç¯å¢ƒæ¡ä»¶å¤šæ ·ï¼Œä¸”æ— æ³•ç›´æ¥å°†é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºåŠ¨ç‰©æ•°æ®ï¼Œä½¿å¾—è·¨ç‰©ç§çš„è¯†åˆ«è¿‡ç¨‹æ›´åŠ å¤æ‚ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„å…³é”®ç‚¹ä¼ æ’­æœºåˆ¶ï¼Œå®ƒåˆ©ç”¨ä¸€å¼ å¸¦æ³¨é‡Šçš„å›¾åƒå’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­ä¼ æ’­å…³é”®ç‚¹ï¼Œå¤§å¤§é™ä½äº†æ‰‹åŠ¨æ³¨é‡Šçš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å®ç°å…³é”®ç‚¹ä½ç½®ç¼–ç ï¼ˆKPEï¼‰å’Œåˆ†ç±»å…³é”®ç‚¹ä½ç½®åµŒå…¥ï¼ˆCKPEï¼‰å¢å¼ºäº†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„åŠŸèƒ½ï¼Œä½¿ViTèƒ½å¤Ÿå­¦ä¹ æ›´ç¨³å¥ã€è¯­ä¹‰æ„ŸçŸ¥çš„è¡¨ç¤ºã€‚è¿™æä¾›äº†æ›´å…¨é¢ã€æ›´è¯¦ç»†çš„å…³é”®ç‚¹è¡¨ç¤ºï¼Œå¯¼è‡´å†è¯†åˆ«æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆã€‚æˆ‘ä»¬åœ¨å››ä¸ªé‡ç”ŸåŠ¨ç‰©æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00818v1">PDF</a> In review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŠ¨ç‰©å†è¯†åˆ«ï¼ˆReIDï¼‰åœ¨ç”Ÿæ€ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†åŠ¨ç‰©ReIDé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚åŠ¨ç‰©å§¿æ€çš„é«˜å¯å˜æ€§ã€ç¯å¢ƒæ¡ä»¶çš„å¤šæ ·æ€§ä»¥åŠæ— æ³•ç›´æ¥å°†é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºåŠ¨ç‰©æ•°æ®ç­‰ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„å…³é”®ç‚¹ä¼ æ’­æœºåˆ¶ï¼Œåˆ©ç”¨å•ä¸ªæ³¨é‡Šå›¾åƒå’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­ä¼ æ’­å…³é”®ç‚¹ï¼Œå¤§å¤§é™ä½äº†æ‰‹åŠ¨æ³¨é‡Šçš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œé€šè¿‡å®ç°å…³é”®ç‚¹ä½ç½®ç¼–ç ï¼ˆKPEï¼‰å’Œåˆ†ç±»å…³é”®ç‚¹ä½ç½®åµŒå…¥ï¼ˆCKPEï¼‰å¢å¼ºäº†è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œä½¿ViTèƒ½å¤Ÿå­¦ä¹ æ›´å¥å£®å’Œè¯­ä¹‰æ„ŸçŸ¥çš„è¡¨ç¤ºï¼Œæä¾›æ›´å…¨é¢ã€æ›´è¯¦ç»†çš„å…³é”®ç‚¹è¡¨ç¤ºï¼Œä»è€Œå®ç°æ›´å‡†ç¡®ã€æ›´é«˜æ•ˆçš„å†è¯†åˆ«ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ªé‡ç”ŸåŠ¨ç‰©æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨ç‰©å†è¯†åˆ«ï¼ˆReIDï¼‰åœ¨ç”Ÿæ€ç ”ç©¶ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå¯¹äºè·Ÿè¸ªç§ç¾¤åŠ¨æ€ã€åˆ†æè¡Œä¸ºæ¨¡å¼å’Œè¯„ä¼°ç”Ÿæ€å½±å“è‡³å…³é‡è¦ã€‚</li>
<li>åŠ¨ç‰©ReIDé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚åŠ¨ç‰©å§¿æ€å¤šå˜ã€ç¯å¢ƒæ¡ä»¶å¤šæ ·ä»¥åŠæ— æ³•ç›´æ¥å°†é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨äºåŠ¨ç‰©æ•°æ®ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„å…³é”®ç‚¹ä¼ æ’­æœºåˆ¶ï¼Œåˆ©ç”¨å•ä¸ªæ³¨é‡Šå›¾åƒå’Œé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­ä¼ æ’­å…³é”®ç‚¹ï¼Œé™ä½äº†æ‰‹åŠ¨æ³¨é‡Šçš„æˆæœ¬ã€‚</li>
<li>é€šè¿‡å®ç°å…³é”®ç‚¹ä½ç½®ç¼–ç ï¼ˆKPEï¼‰å’Œåˆ†ç±»å…³é”®ç‚¹ä½ç½®åµŒå…¥ï¼ˆCKPEï¼‰å¢å¼ºäº†è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ã€‚</li>
<li>ViTèƒ½å¤Ÿå­¦ä¹ æ›´å¥å£®å’Œè¯­ä¹‰æ„ŸçŸ¥çš„è¡¨ç¤ºï¼Œæä¾›æ›´å…¨é¢ã€æ›´è¯¦ç»†çš„å…³é”®ç‚¹è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å››ä¸ªé‡ç”ŸåŠ¨ç‰©æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15cba7399efc98c4f953f15867d0e102.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25e7980b1999628aef37d53d5da3f1a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35aece95212cf779f41c82d37a86f1dd.jpg" align="middle">
</details>




<h2 id="MambaNUT-Nighttime-UAV-Tracking-via-Mamba-and-Adaptive-Curriculum-Learning"><a href="#MambaNUT-Nighttime-UAV-Tracking-via-Mamba-and-Adaptive-Curriculum-Learning" class="headerlink" title="MambaNUT: Nighttime UAV Tracking via Mamba and Adaptive Curriculum   Learning"></a>MambaNUT: Nighttime UAV Tracking via Mamba and Adaptive Curriculum   Learning</h2><p><strong>Authors:You Wu, Xiangyang Yang, Xucheng Wang, Hengzhou Ye, Dan Zeng, Shuiwang Li</strong></p>
<p>Harnessing low-light enhancement and domain adaptation, nighttime UAV tracking has made substantial strides. However, over-reliance on image enhancement, scarcity of high-quality nighttime data, and neglecting the relationship between daytime and nighttime trackers, which hinders the development of an end-to-end trainable framework. Moreover, current CNN-based trackers have limited receptive fields, leading to suboptimal performance, while ViT-based trackers demand heavy computational resources due to their reliance on the self-attention mechanism. In this paper, we propose a novel pure Mamba-based tracking framework (\textbf{MambaNUT}) that employs a state space model with linear complexity as its backbone, incorporating a single-stream architecture that integrates feature learning and template-search coupling within Vision Mamba. We introduce an adaptive curriculum learning (ACL) approach that dynamically adjusts sampling strategies and loss weights, thereby improving the modelâ€™s ability of generalization. Our ACL is composed of two levels of curriculum schedulers: (1) sampling scheduler that transforms the data distribution from imbalanced to balanced, as well as from easier (daytime) to harder (nighttime) samples; (2) loss scheduler that dynamically assigns weights based on data frequency and the IOU. Exhaustive experiments on multiple nighttime UAV tracking benchmarks demonstrate that the proposed MambaNUT achieves state-of-the-art performance while requiring lower computational costs. The code will be available. </p>
<blockquote>
<p>åˆ©ç”¨ä½å…‰å¢å¼ºå’Œé¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯ï¼Œå¤œé—´æ— äººæœºè·Ÿè¸ªå·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå¯¹å›¾åƒå¢å¼ºçš„è¿‡åº¦ä¾èµ–ã€é«˜è´¨é‡å¤œé—´æ•°æ®çš„ç¨€ç¼ºä»¥åŠå¿½è§†ç™½å¤©å’Œå¤œé—´è·Ÿè¸ªå™¨ä¹‹é—´çš„å…³ç³»ï¼Œè¿™äº›é˜»ç¢äº†ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¡†æ¶çš„å‘å±•ã€‚æ­¤å¤–ï¼Œå½“å‰çš„CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰è·Ÿè¸ªå™¨å…·æœ‰æœ‰é™çš„æ„Ÿå—é‡ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ï¼Œè€ŒåŸºäºViTï¼ˆè§†è§‰è½¬æ¢å™¨ï¼‰çš„è·Ÿè¸ªå™¨ç”±äºä¾èµ–è‡ªæ³¨æ„åŠ›æœºåˆ¶è€Œéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçº¯Mambaçš„è·Ÿè¸ªæ¡†æ¶ï¼ˆMambaNUTï¼‰ï¼Œå®ƒé‡‡ç”¨çº¿æ€§å¤æ‚åº¦çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œå¹¶ç»“åˆå•æµæ¶æ„ï¼Œåœ¨Vision Mambaå†…é›†æˆç‰¹å¾å­¦ä¹ å’Œæ¨¡æ¿æœç´¢è€¦åˆã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼ˆACLï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥å’ŒæŸå¤±æƒé‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ACLç”±ä¸¤ä¸ªå±‚æ¬¡çš„è¯¾ç¨‹è°ƒåº¦å™¨ç»„æˆï¼šï¼ˆ1ï¼‰é‡‡æ ·è°ƒåº¦å™¨ï¼Œå®ƒå°†æ•°æ®åˆ†å¸ƒä»ä¸å¹³è¡¡è½¬å˜ä¸ºå¹³è¡¡ï¼ŒåŒæ—¶ä»æ›´å®¹æ˜“ï¼ˆç™½å¤©ï¼‰çš„æ ·æœ¬åˆ°æ›´å›°éš¾ï¼ˆå¤œé—´ï¼‰çš„æ ·æœ¬ï¼›ï¼ˆ2ï¼‰æŸå¤±è°ƒåº¦å™¨æ ¹æ®æ•°æ®é¢‘ç‡å’ŒIOUåŠ¨æ€åˆ†é…æƒé‡ã€‚åœ¨å¤šä¸ªå¤œé—´æ— äººæœºè·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯¦å°½å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„MambaNUTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬è¾ƒä½ã€‚ä»£ç å°†å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00626v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMambaçš„çº¯è·Ÿè¸ªæ¡†æ¶ï¼ˆMambaNUTï¼‰ï¼Œé‡‡ç”¨å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œé€šè¿‡å•æµæ¶æ„æ•´åˆç‰¹å¾å­¦ä¹ å’Œæ¨¡æ¿æœç´¢è€¦åˆåœ¨Vision Mambaä¸­ã€‚å¼•å…¥è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼ˆACLï¼‰æ–¹æ³•ï¼ŒåŠ¨æ€è°ƒæ•´é‡‡æ ·ç­–ç•¥å’ŒæŸå¤±æƒé‡ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªå¤œé—´æ— äººæœºè·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤œé—´æ— äººæœºè·Ÿè¸ªå·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç»“åˆä½å…‰å¢å¼ºå’Œé¢†åŸŸè‡ªé€‚åº”æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å›¾åƒå¢å¼ºã€é«˜è´¨é‡å¤œé—´æ•°æ®ä¸è¶³ä»¥åŠå¿½è§†æ˜¼å¤œè·Ÿè¸ªå…³ç³»ï¼Œé˜»ç¢ç«¯åˆ°ç«¯å¯è®­ç»ƒæ¡†æ¶çš„å‘å±•ã€‚</li>
<li>CNNåŸºè·Ÿè¸ªå™¨å…·æœ‰æœ‰é™çš„æ„Ÿå—é‡ï¼Œæ€§èƒ½ä¸ä½³ï¼Œè€ŒViTåŸºè·Ÿè¸ªå™¨å› ä¾èµ–è‡ªæ³¨æ„åŠ›æœºåˆ¶è€Œè®¡ç®—é‡å¤§ã€‚</li>
<li>å¼•å…¥åŸºäºMambaçš„è·Ÿè¸ªæ¡†æ¶ï¼ˆMambaNUTï¼‰ï¼Œé‡‡ç”¨å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚</li>
<li>MambaNUTé‡‡ç”¨å•æµæ¶æ„ï¼Œæ•´åˆç‰¹å¾å­¦ä¹ ä¸æ¨¡æ¿æœç´¢è€¦åˆã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”è¯¾ç¨‹å­¦ä¹ ï¼ˆACLï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®é‡‡æ ·å’ŒæŸå¤±æƒé‡åŠ¨æ€è°ƒæ•´ï¼Œæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dfaf5a1b0cd8d1787ea2cde36fe48b64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ce8dcff2738fb42fca9d817c7c4c743.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21085802ece6bd8f8be4d3b844c2a370.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23190dfca5f1ad980fb8582579c4c711.jpg" align="middle">
</details>




<h2 id="LQ-Adapter-ViT-Adapter-with-Learnable-Queries-for-Gallbladder-Cancer-Detection-from-Ultrasound-Image"><a href="#LQ-Adapter-ViT-Adapter-with-Learnable-Queries-for-Gallbladder-Cancer-Detection-from-Ultrasound-Image" class="headerlink" title="LQ-Adapter: ViT-Adapter with Learnable Queries for Gallbladder Cancer   Detection from Ultrasound Image"></a>LQ-Adapter: ViT-Adapter with Learnable Queries for Gallbladder Cancer   Detection from Ultrasound Image</h2><p><strong>Authors:Chetan Madan, Mayuna Gupta, Soumen Basu, Pankaj Gupta, Chetan Arora</strong></p>
<p>We focus on the problem of Gallbladder Cancer (GBC) detection from Ultrasound (US) images. The problem presents unique challenges to modern Deep Neural Network (DNN) techniques due to low image quality arising from noise, textures, and viewpoint variations. Tackling such challenges would necessitate precise localization performance by the DNN to identify the discerning features for the downstream malignancy prediction. While several techniques have been proposed in the recent years for the problem, all of these methods employ complex custom architectures. Inspired by the success of foundational models for natural image tasks, along with the use of adapters to fine-tune such models for the custom tasks, we investigate the merit of one such design, ViT-Adapter, for the GBC detection problem. We observe that ViT-Adapter relies predominantly on a primitive CNN-based spatial prior module to inject the localization information via cross-attention, which is inefficient for our problem due to the small pathology sizes, and variability in their appearances due to non-regular structure of the malignancy. In response, we propose, LQ-Adapter, a modified Adapter design for ViT, which improves localization information by leveraging learnable content queries over the basic spatial prior module. Our method surpasses existing approaches, enhancing the mean IoU (mIoU) scores by 5.4%, 5.8%, and 2.7% over ViT-Adapters, DINO, and FocalNet-DINO, respectively on the US image-based GBC detection dataset, and establishing a new state-of-the-art (SOTA). Additionally, we validate the applicability and effectiveness of LQ-Adapter on the Kvasir-Seg dataset for polyp detection from colonoscopy images. Superior performance of our design on this problem as well showcases its capability to handle diverse medical imaging tasks across different datasets. Code is released at <a target="_blank" rel="noopener" href="https://github.com/ChetanMadan/LQ-Adapter">https://github.com/ChetanMadan/LQ-Adapter</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä¸»è¦å…³æ³¨é€šè¿‡è¶…å£°ï¼ˆUSï¼‰å›¾åƒæ£€æµ‹èƒ†å›Šç™Œï¼ˆGBCï¼‰çš„é—®é¢˜ã€‚ç”±äºå™ªå£°ã€çº¹ç†å’Œè§†è§’å˜åŒ–å¯¼è‡´çš„å›¾åƒè´¨é‡ä½ä¸‹ï¼Œè¿™ä¸ªé—®é¢˜ç»™ç°ä»£æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æŠ€æœ¯å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è¦è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒDNNéœ€è¦å®ç°ç²¾ç¡®çš„å®šä½æ€§èƒ½ï¼Œä»¥è¯†åˆ«ç”¨äºä¸‹æ¸¸æ¶æ€§é¢„æµ‹çš„æœ‰åŒºåˆ«ç‰¹å¾ã€‚å°½ç®¡è¿‘å¹´æ¥å·²ç»æå‡ºäº†å‡ ç§è§£å†³è¯¥é—®é¢˜çš„æ–¹æ³•ï¼Œä½†æ‰€æœ‰è¿™äº›æ–¹æ³•éƒ½é‡‡ç”¨å¤æ‚çš„è‡ªå®šä¹‰æ¶æ„ã€‚å—è‡ªç„¶å›¾åƒä»»åŠ¡åŸºç¡€æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œä»¥åŠä½¿ç”¨é€‚é…å™¨å¯¹æ­¤ç±»æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥å®Œæˆè‡ªå®šä¹‰ä»»åŠ¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§åä¸ºViT-Adapterçš„è®¾è®¡ä¼˜åŠ¿ï¼Œç”¨äºèƒ†å›Šç™Œæ£€æµ‹é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ViT-Adapterä¸»è¦ä¾èµ–äºåŸºäºåŸå§‹CNNçš„ç©ºé—´å…ˆéªŒæ¨¡å—ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›æ³¨å…¥å®šä½ä¿¡æ¯ï¼Œè¿™å¯¹äºæˆ‘ä»¬çš„é—®é¢˜æ˜¯ä¸é«˜æ•ˆçš„ï¼Œå› ä¸ºæ¶æ€§ç—…å˜çš„å°ºå¯¸è¾ƒå°ï¼Œè€Œä¸”å…¶å¤–è§‚å› éè§„åˆ™ç»“æ„è€Œå…·æœ‰å¯å˜æ€§ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬æå‡ºäº†LQ-Adapterï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›çš„ViTé€‚é…å™¨è®¾è®¡ï¼Œå®ƒé€šè¿‡åˆ©ç”¨åŸºæœ¬ç©ºé—´å…ˆéªŒæ¨¡å—ä¸Šçš„å¯å­¦ä¹ å†…å®¹æŸ¥è¯¢æ¥æ”¹å–„å®šä½ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºäºè¶…å£°å›¾åƒçš„èƒ†å›Šç™Œæ£€æµ‹æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œç›¸è¾ƒäºViT-Adaptersã€DINOå’ŒFocalNet-DINOåˆ†åˆ«æé«˜äº†å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰å¾—åˆ†5.4%ã€5.8%å’Œ2.7%ï¼Œå¹¶å»ºç«‹äº†æ–°çš„æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç”¨äºç»“è‚ é•œå›¾åƒä¸­çš„æ¯è‚‰æ£€æµ‹çš„Kvasir-Segæ•°æ®é›†ä¸ŠéªŒè¯äº†LQ-Adapterçš„é€‚ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è®¾è®¡åœ¨è¿™ä¸ªé—®é¢˜ä¸Šçš„å“è¶Šæ€§èƒ½ä¹Ÿå±•ç¤ºäº†å®ƒåœ¨å¤„ç†ä¸åŒæ•°æ®é›†çš„å¤šæ ·åŒ»å­¦æˆåƒä»»åŠ¡çš„èƒ½åŠ›ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ChetanMadan/LQ-Adapter%E3%80%82">https://github.com/ChetanMadan/LQ-Adapterã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00374v1">PDF</a> Accepted at WACV 2025</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹èƒ†å›Šç™Œè¶…å£°å›¾åƒæ£€æµ‹é—®é¢˜ï¼Œç ”ç©¶é‡‡ç”¨ViT-Adapteræ”¹è¿›æ¨¡å‹LQ-Adapterã€‚LQ-Adapteré€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å†…å®¹æŸ¥è¯¢æ¨¡å—ï¼Œæ”¹è¿›äº†åŸå§‹åŸºäºCNNçš„ç©ºé—´å…ˆéªŒæ¨¡å—ï¼Œæé«˜äº†å®šä½ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…ç¶å°ä¸”éç»“æ„åŒ–çš„æ¶æ€§ç—…å˜æƒ…å†µä¸‹ã€‚åœ¨èƒ†å›Šç™Œè¶…å£°å›¾åƒæ£€æµ‹æ•°æ®é›†ä¸Šï¼ŒLQ-Adapterè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹³å‡äº¤å¹¶æ¯”(mIoU)å¾—åˆ†æé«˜äº†5.4%ã€5.8%å’Œ2.7%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜å¼‚æ€§èƒ½å’Œè·¨æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒLQ-Adapteråœ¨ç»“è‚ é•œæ£€æŸ¥å›¾åƒçš„å¤šå‘æ€§æ¯è‚‰æ£€æµ‹ä¸Šä¹Ÿæœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚æ­¤å¤–å°†å‘å¸ƒçš„ä»£ç ï¼ˆç½‘å€é“¾æ¥ï¼‰å‘å…¬ä¼—å¼€æ”¾å…±äº«ä»¥ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ã€‚å¯¹äºå°å‹ç—…ç†æ£€æµ‹å…·æœ‰é‡è¦çš„ä¸´åºŠä»·å€¼å’Œå¹¿æ³›çš„å¸‚åœºå‰æ™¯åº”ç”¨ä»·å€¼æœ‰ç€é‡è¦ä½œç”¨ã€‚éšç€å¯¹æ¶æ€§è‚¿ç˜¤ç–¾ç—…çš„è¯Šç–—å’Œç§‘ç ”å·¥ä½œçš„éœ€æ±‚ï¼Œè¯¥ç ”ç©¶åœ¨åŒ»ç–—å½±åƒåˆ†æé¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«è‚¿ç˜¤ç»„ç»‡ç‰¹å¾å¹¶ç²¾å‡†å®šä½ï¼Œæœ‰åŠ©äºæé«˜èƒ†å›Šç™Œçš„è¯Šæ–­ç²¾åº¦å’Œæ²»ç–—æˆåŠŸç‡ã€‚è¯¥é¡¹æŠ€æœ¯çš„æˆç†Ÿä¸åº”ç”¨æ— ç–‘ä¸ºæœªæ¥çš„èƒ†å›Šç™Œè¯Šæ–­å’Œæ²»ç–—å¸¦æ¥æ–°å˜é©ï¼Œä½¿å°å‹åŒ»ç–—æ£€æµ‹è®¾å¤‡é«˜æ•ˆæœåŠ¡äºæ™®é€šæ‚£è€…åŸºå±‚æ°‘ä¼—ä¸­å‘æŒ¥å‡ºé‡è¦è§’è‰²ï¼Œæ„ä¹‰æå…¶é‡å¤§ä¸”å……æ»¡å·¨å¤§å¸‚åœºæ½œåŠ›å‰æ™¯æ— é™å€¼å¾—æœŸå¾…å…¶æ­£å¼æŠ•äº§ä¸Šçº¿ä»¥ä¾¿é¢å‘ä¸´åºŠåº”ç”¨ä»¥åŠä¸ºå¹¿å¤§ç§‘ç ”äººå£«æä¾›æ›´å¤šçš„é€‰æ‹©å·¥å…·å’Œæ›´å¥½çš„ç”¨æˆ·ä½“éªŒç­‰æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰å’ŒæœŸå¾…ç©ºé—´ã€‚ã€‚ã€‚æ›´å…·ä½“çš„è¡¨ç°ä¼šå›´ç»•é€‚åº”æ€§å¹¿æ³›é€‚ç”¨é¢†åŸŸæ¶µç›–è¾ƒå¹¿é€šè¿‡è®­ç»ƒå’Œä¸æ–­ä¼˜åŒ–æŠ€æœ¯ä¼šæ»¡è¶³ä¸åŒæ•°æ®é›†è¦æ±‚ä¸­ä¸æ–­æå‡ç®—æ³•çš„é€‚åº”æ€§å’Œç²¾å‡†æ€§å¸¦æ¥è¯Šç–—æ–¹æ¡ˆä¸­çš„å®é™…åº”ç”¨å‰æ™¯ä¼šä¿ƒè¿›ç–¾ç—…çš„ç²¾å‡†è¯Šæ–­å’Œæ²»ç–—æ˜¯å½“å‰çš„ç§‘æŠ€å‘å±•ä¸­è¯¥é¢†åŸŸçš„æœ‰æ•ˆå‘å±•æ–¹å‘å…·ä½“è½å®æˆæ•ˆéœ€è¦ç»“åˆä¸´åºŠåº”ç”¨ä¸å®è·µæ¥è¯æ˜æœŸæœ›å¾—åˆ°å¹¿æ³›å…³æ³¨å’Œè¿›ä¸€æ­¥çš„æ¨å¹¿å®æ–½åº”ç”¨äºåŸºå±‚åŒ»ç–—æœºæ„é¢å‘æ°‘ä¼—å¼€æ”¾ä»¥å®ç°è¿œç¨‹æ£€æµ‹å’Œå¥åº·ç®¡ç†ç»™äººä»¬çš„æ—¥å¸¸ç”Ÿæ´»å¸¦æ¥ä¾¿æ·åŒ–æœåŠ¡çš„æŠ€æœ¯çªç ´æˆä¸ºå¯èƒ½ï¼æœªæ¥å‘å±•æ½œåŠ›å€¼å¾—æœŸå¾…å’Œæ”¹è¿›å’ŒæŠ€æœ¯çš„ä¸æ–­ä¼˜åŒ–å‘å±•ä»¥æœŸå‘æŒ¥æ›´å¤§çš„ä»·å€¼å’Œæ›´å¤šçš„å®é™…æ¡ˆä¾‹å’Œåº”ç”¨åœºæ™¯çš„è¦†ç›–å¸®åŠ©æ”¹å–„åŸºå±‚åŒ»ç–—æœºæ„å¯¹äºæ‚£è€…çš„ç–¾ç—…è¯Šæ–­èƒ½åŠ›ï¼</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>ç ”ç©¶é›†ä¸­äºèƒ†å›Šç™Œçš„è¶…å£°å›¾åƒæ£€æµ‹é—®é¢˜ï¼Œé¢ä¸´ä½å›¾åƒè´¨é‡çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºLQ-Adapteræ¨¡å‹ï¼Œæ”¹è¿›äº†ViT-Adapterï¼Œé€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„å†…å®¹æŸ¥è¯¢æ¨¡å—æé«˜å®šä½ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ã€‚</li>
<li>åœ¨èƒ†å›Šç™Œè¶…å£°å›¾åƒæ£€æµ‹æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹³å‡äº¤å¹¶æ¯”å¾—åˆ†æ˜¾è‘—æé«˜ã€‚</li>
<li>LQ-Adapteråœ¨ç»“è‚ é•œæ£€æŸ¥å›¾åƒçš„å¤šå‘æ€§æ¯è‚‰æ£€æµ‹ä»»åŠ¡ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>ä»£ç å·²å…¬å¼€å…±äº«ï¼Œæœ‰åŠ©äºå­¦ä¹ å’Œäº¤æµã€‚</li>
<li>è¯¥ç ”ç©¶åœ¨åŒ»ç–—å½±åƒåˆ†æé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œèƒ½å¤Ÿé«˜æ•ˆè¯†åˆ«è‚¿ç˜¤ç»„ç»‡ç‰¹å¾å¹¶ç²¾å‡†å®šä½ï¼Œæé«˜è¯Šæ–­ç²¾åº¦å’Œæ²»ç–—æˆåŠŸç‡ã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6635fe04cc390dcfbe2fa5ae736d742e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df0075a793c2eae629584081714751e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6e6c52f696daf4f7a021b95961bf9bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-320123ff25ba590bdaca2fc644518562.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e0b155becac9ab21448e9255c817662.jpg" align="middle">
</details>




<h2 id="EFTViT-Efficient-Federated-Training-of-Vision-Transformers-with-Masked-Images-on-Resource-Constrained-Edge-Devices"><a href="#EFTViT-Efficient-Federated-Training-of-Vision-Transformers-with-Masked-Images-on-Resource-Constrained-Edge-Devices" class="headerlink" title="EFTViT: Efficient Federated Training of Vision Transformers with Masked   Images on Resource-Constrained Edge Devices"></a>EFTViT: Efficient Federated Training of Vision Transformers with Masked   Images on Resource-Constrained Edge Devices</h2><p><strong>Authors:Meihan Wu, Tao Chang, Cui Miao, Jie Zhou, Chun Li, Xiangyu Xu, Ming Li, Xiaodong Wang</strong></p>
<p>Federated learning research has recently shifted from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) due to their superior capacity. ViTs training demands higher computational resources due to the lack of 2D inductive biases inherent in CNNs. However, efficient federated training of ViTs on resource-constrained edge devices remains unexplored in the community. In this paper, we propose EFTViT, a hierarchical federated framework that leverages masked images to enable efficient, full-parameter training on resource-constrained edge devices, offering substantial benefits for learning on heterogeneous data. In general, we patchify images and randomly mask a portion of the patches, observing that excluding them from training has minimal impact on performance while substantially reducing computation costs and enhancing data content privacy protection. Specifically, EFTViT comprises a series of lightweight local modules and a larger global module, updated independently on clients and the central server, respectively. The local modules are trained on masked image patches, while the global module is trained on intermediate patch features uploaded from the local client, balanced through a proposed median sampling strategy to erase client data distribution privacy. We analyze the computational complexity and privacy protection of EFTViT. Extensive experiments on popular benchmarks show that EFTViT achieves up to 28.17% accuracy improvement, reduces local training computational cost by up to 2.8$\times$, and cuts local training time by up to 4.4$\times$ compared to existing methods. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ç ”ç©¶æœ€è¿‘å·²ä»å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰è½¬å‘è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ï¼Œè¿™æ˜¯å› ä¸ºè§†è§‰è½¬æ¢å™¨å…·æœ‰å“è¶Šçš„èƒ½åŠ›ã€‚ç”±äºViTsç¼ºä¹CNNæ‰€å›ºæœ‰çš„2Då½’çº³åè§ï¼Œå› æ­¤å…¶è®­ç»ƒéœ€è¦æ›´é«˜çš„è®¡ç®—èµ„æºã€‚ç„¶è€Œï¼Œåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæœ‰æ•ˆåœ°è®­ç»ƒViTsçš„è”é‚¦å­¦ä¹ åœ¨ç¤¾åŒºä¸­å°šæœªå¾—åˆ°æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EFTViTï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚çš„è”é‚¦æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ©ç å›¾åƒå®ç°åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šçš„é«˜æ•ˆå…¨å‚æ•°è®­ç»ƒï¼Œä¸ºåœ¨å¼‚æ„æ•°æ®ä¸Šçš„å­¦ä¹ æä¾›äº†å®è´¨æ€§çš„å¥½å¤„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å°†å›¾åƒåˆ†æˆå°å—å¹¶éšæœºæ©ç›–ä¸€éƒ¨åˆ†å—ï¼Œè§‚å¯Ÿåˆ°åœ¨è®­ç»ƒæ—¶æ’é™¤å®ƒä»¬å¯¹æ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ï¼ŒåŒæ—¶èƒ½å¤§å¹…å‡å°‘è®¡ç®—æˆæœ¬å¹¶å¢å¼ºæ•°æ®å†…å®¹éšç§ä¿æŠ¤ã€‚å…·ä½“æ¥è¯´ï¼ŒEFTViTåŒ…æ‹¬ä¸€ç³»åˆ—è½»é‡çº§çš„æœ¬åœ°æ¨¡å—å’Œä¸€ä¸ªæ›´å¤§çš„å…¨å±€æ¨¡å—ï¼Œåˆ†åˆ«åœ¨å®¢æˆ·ç«¯å’Œä¸­å¤®æœåŠ¡å™¨ä¸Šç‹¬ç«‹æ›´æ–°ã€‚æœ¬åœ°æ¨¡å—åœ¨æ©ç å›¾åƒå—ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè€Œå…¨å±€æ¨¡å—åˆ™åœ¨ä»æœ¬åœ°å®¢æˆ·ç«¯ä¸Šä¼ çš„ä¸­é—´å—ç‰¹å¾ä¸Šè¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æå‡ºçš„ä¸­ä½æ•°é‡‡æ ·ç­–ç•¥æ¥å¹³è¡¡ï¼Œä»¥æ¶ˆé™¤å®¢æˆ·ç«¯æ•°æ®åˆ†å¸ƒéšç§ã€‚æˆ‘ä»¬åˆ†æäº†EFTViTçš„è®¡ç®—å¤æ‚æ€§å’Œéšç§ä¿æŠ¤ã€‚åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEFTViTå®ç°äº†é«˜è¾¾28.17%çš„å‡†ç¡®ç‡æå‡ï¼Œå°†æœ¬åœ°è®­ç»ƒçš„è®¡ç®—æˆæœ¬é™ä½äº†é«˜è¾¾2.8å€ï¼Œå¹¶å°†æœ¬åœ°è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†é«˜è¾¾4.4å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00334v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†EFTViTï¼Œä¸€ä¸ªç”¨äºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šè®­ç»ƒViTsçš„é«˜æ•ˆè”é‚¦æ¡†æ¶ã€‚é€šè¿‡ä½¿ç”¨å›¾åƒå—åˆ’åˆ†å’Œéƒ¨åˆ†æ©è”½çš„ç­–ç•¥ï¼Œæœ¬åœ°æ¨¡å—èƒ½å¤Ÿåœ¨è¢«æ©ç›–çš„å›¾åƒå—ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶å…¨çƒæ¨¡å—åˆ™é€šè¿‡ä¸€ç§å¹³è¡¡é‡‡æ ·ç­–ç•¥æ›´æ–°å¹¶å¤„ç†ä¸Šä¼ çš„ä¸­é—´ç‰¹å¾ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒEFTViTèƒ½å¤Ÿæ˜¾è‘—æé«˜è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¿æŠ¤æ•°æ®éšç§ã€‚ç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼ŒEFTViTåœ¨å‡†ç¡®æ€§ä¸Šæé«˜äº†é«˜è¾¾28.17%ï¼Œåœ¨æœ¬åœ°è®¡ç®—æˆæœ¬ä¸Šé™ä½äº†é«˜è¾¾2.8å€ï¼Œå¹¶å‡å°‘äº†é«˜è¾¾4.4å€çš„æœ¬åœ°è®­ç»ƒæ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶å°†è”é‚¦å­¦ä¹ ä»CNNè½¬å‘ViTä»¥é€‚åº”å…¶æ›´é«˜çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
<li>EFTViTåˆ©ç”¨å›¾åƒå—æ©è”½æŠ€æœ¯ä»¥é™ä½è®¡ç®—æˆæœ¬å¹¶æé«˜æ•°æ®éšç§ä¿æŠ¤ã€‚</li>
<li>EFTViTæ¡†æ¶åŒ…æ‹¬æœ¬åœ°æ¨¡å—å’Œå…¨çƒæ¨¡å—ï¼Œå¯ç‹¬ç«‹æ›´æ–°ä»¥é€‚åº”ä¸åŒè®­ç»ƒéœ€æ±‚ã€‚</li>
<li>å›¾åƒå—çš„æ©ç›–å¯¹äºè®­ç»ƒæ€§èƒ½çš„å½±å“å¾®ä¹å…¶å¾®ã€‚</li>
<li>ä½¿ç”¨æå‡ºçš„ä¸­é—´ç‰¹å¾ä¿¡æ¯é‡‡æ ·ç­–ç•¥å¯æé«˜å¹³è¡¡åº¦å’Œæ•°æ®å¤„ç†æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒEFTViTæ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€è®¡ç®—æ•ˆç‡å’Œæ—¶é—´æ•ˆç›Šã€‚</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a93a539021cd9f05d8a8f854693177ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cbeacf81910b71c5a1e9c8325b671c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2533f2d8c8af4a4a22ebfac28d8f5f6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-578aa3cd75ff16e1bed79adc925c59c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b0a235ad89c5e6ea2693a2cf4a9f065.jpg" align="middle">
</details>




<h2 id="Explaining-the-Impact-of-Training-on-Vision-Models-via-Activation-Clustering"><a href="#Explaining-the-Impact-of-Training-on-Vision-Models-via-Activation-Clustering" class="headerlink" title="Explaining the Impact of Training on Vision Models via Activation   Clustering"></a>Explaining the Impact of Training on Vision Models via Activation   Clustering</h2><p><strong>Authors:AhcÃ¨ne Boubekki, Samuel G. Fadel, Sebastian Mair</strong></p>
<p>Recent developments in the field of explainable artificial intelligence (XAI) for vision models investigate the information extracted by their feature encoder. We contribute to this effort and propose Neuro-Activated Vision Explanations (NAVE), which extracts the information captured by the encoder by clustering the feature activations of the frozen network to be explained. The method does not aim to explain the modelâ€™s prediction but to answer questions such as which parts of the image are processed similarly or which information is kept in deeper layers. Experimentally, we leverage NAVE to show that the training dataset and the level of supervision affect which concepts are captured. In addition, our method reveals the impact of registers on vision transformers (ViT) and the information saturation caused by the watermark Clever Hans effect in the training set. </p>
<blockquote>
<p>åœ¨è§£é‡Šæ€§äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰é¢†åŸŸï¼Œé’ˆå¯¹è§†è§‰æ¨¡å‹çš„æœ€æ–°å‘å±•æ­£åœ¨ç ”ç©¶ç‰¹å¾ç¼–ç å™¨æ‰€æå–çš„ä¿¡æ¯ã€‚æˆ‘ä»¬ä¸ºæ­¤åšå‡ºè´¡çŒ®å¹¶æå‡ºäº†ç¥ç»æ¿€æ´»è§†è§‰è§£é‡Šï¼ˆNAVEï¼‰ï¼Œå®ƒé€šè¿‡èšç±»è¦è§£é‡Šçš„å†»ç»“ç½‘ç»œçš„ç‰¹å¾æ¿€æ´»æ¥æå–ç¼–ç å™¨æ•è·çš„ä¿¡æ¯ã€‚è¯¥æ–¹æ³•å¹¶ä¸æ—¨åœ¨è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ï¼Œè€Œæ˜¯æ—¨åœ¨å›ç­”è¯¸å¦‚å›¾åƒçš„å“ªäº›éƒ¨åˆ†è¢«ç±»ä¼¼å¤„ç†æˆ–å“ªäº›ä¿¡æ¯è¢«ä¿ç•™åœ¨è¾ƒæ·±çš„å±‚æ¬¡ç­‰é—®é¢˜ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬åˆ©ç”¨NAVEæ˜¾ç¤ºè®­ç»ƒæ•°æ®é›†å’Œç›‘ç£æ°´å¹³ä¼šå½±å“æ•è·å“ªäº›æ¦‚å¿µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ­ç¤ºäº†å¯„å­˜å™¨å¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„å½±å“ä»¥åŠè®­ç»ƒé›†ä¸­æ°´å°Clever Hansæ•ˆåº”å¼•èµ·çš„ä¿¡æ¯é¥±å’Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19700v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬æ‘˜è¦æ¢è®¨äº†æœ€è¿‘å…³äºè§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰åœ¨è§†è§‰æ¨¡å‹é¢†åŸŸçš„è¿›å±•ï¼Œå¹¶æå‡ºäº†ç¥ç»æ¿€æ´»è§†è§‰è§£é‡Šï¼ˆNAVEï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡èšç±»è¢«è§£é‡Šå†»ç»“ç½‘ç»œçš„ç‰¹å¾æ¿€æ´»æ¥æå–ç¼–ç å™¨æ‰€æ•è·çš„ä¿¡æ¯ã€‚å®ƒæ—¨åœ¨è§£ç­”ç±»ä¼¼çš„é—®é¢˜ï¼Œå¦‚å›¾åƒçš„å“ªäº›éƒ¨åˆ†å¤„ç†ç›¸ä¼¼æˆ–æ·±å±‚ä¸­ä¿ç•™äº†å“ªäº›ä¿¡æ¯ï¼Œè€Œéè§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®é›†å’Œç›‘ç®¡æ°´å¹³ä¼šå½±å“æ‰€æ•è·çš„æ¦‚å¿µã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ­ç¤ºäº†æ³¨å†Œè¡¨å¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„å½±å“ä»¥åŠè®­ç»ƒé›†ä¸­æ°´å°å¯¼è‡´çš„Clever Hansæ•ˆåº”å¼•å‘çš„ä¿¡æ¯é¥±å’Œé—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œæ‘˜è¦æä¾›äº†æœ‰å…³ä½¿ç”¨è§†è§‰è§£é‡Šæ¨¡å‹è·å–å›¾åƒä¿¡æ¯çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯è¯¥æ–‡æœ¬çš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ul>
<li>NAVEæ–¹æ³•æ—¨åœ¨é€šè¿‡èšç±»ç‰¹å¾æ¿€æ´»æå–è§†è§‰æ¨¡å‹çš„ç¼–ç å™¨æ‰€æ•è·çš„ä¿¡æ¯ï¼Œè€Œä¸æ˜¯è§£é‡Šæ¨¡å‹çš„é¢„æµ‹ã€‚</li>
<li>NAVEå±•ç¤ºäº†ä¸åŒæ¦‚å¿µå¦‚ä½•é€šè¿‡è§†è§‰æ¨¡å‹è¿›è¡Œæ•æ‰å¹¶å¼ºè°ƒå®ƒä»¬ä¹‹é—´çš„å·®å¼‚å¦‚ä½•å—è®­ç»ƒæ•°æ®é›†å’Œç›‘ç®¡æ°´å¹³çš„å½±å“ã€‚</li>
<li>è®­ç»ƒæ•°æ®é›†å’Œç›‘ç®¡æ°´å¹³å¯¹è§†è§‰æ¨¡å‹æ•è·çš„æ¦‚å¿µæœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>è¯¥æ–¹æ³•æ­ç¤ºäº†æ³¨å†Œè¡¨ç°åœ¨å¯¹è§†è§‰å˜å‹å™¨æ€§èƒ½ä¸­çš„é‡è¦æ€§ä»¥åŠå¯¹æ³¨å†Œçš„å½±å“åœ¨å¤æ‚ç³»ç»Ÿä¸­æœªå……åˆ†äº†è§£çš„å¤æ‚æ€§çš„æ•æ„Ÿæ€§ã€‚å°½ç®¡æ­¤ç±»æ¨¡å‹çš„ä¼˜åŠ¿æ­£åœ¨è¢«å¹¿æ³›æ¢ç´¢ï¼ˆå°¤å…¶æ˜¯å¯ä»¥å–ä»£æŸäº›ä¼ ç»Ÿç®—æ³•ï¼‰ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œä¾‹å¦‚éœ€è¦æ›´æ·±å…¥çš„ä¼˜åŒ–å’Œæ›´å…¨é¢çš„æµ‹è¯•æ¥ç¡®ä¿å…¶åœ¨å„ç§æƒ…å†µä¸‹çš„æ€§èƒ½ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥ç¡®ä¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿå…¬å¹³åœ°å¤„ç†æ‰€æœ‰ç±»å‹çš„æ•°æ®å’Œåœºæ™¯ï¼Œé¿å…åè§å’Œæ­§è§†ã€‚ç”±äºæ•°æ®çš„æ•°é‡å’Œè´¨é‡æ—¥ç›Šæˆä¸ºé—®é¢˜è§£å†³çš„å…³é”®å› ç´ ä¹‹ä¸€ï¼Œç ”ç©¶å’Œç†è§£å„ç§æ•°æ®ç±»å‹ä»¥åŠè¿™äº›æ•°æ®ç±»å‹å¯¹è®­ç»ƒæ¨¡å‹çš„ç‹¬ç‰¹å½±å“ä¹Ÿè¶Šæ¥è¶Šé‡è¦ã€‚å¦ä¸€ä¸ªé‡è¦çš„é—®é¢˜åœ¨äºå¯¹è§†è§‰æ¨¡å‹çš„é€æ˜åº¦ç¼ºä¹æ·±å…¥äº†è§£ä»¥åŠå¦‚ä½•ç†è§£å¹¶æ”¹è¿›è¿™äº›æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶ã€‚é€šè¿‡æ›´å¥½åœ°äº†è§£æ¨¡å‹å¦‚ä½•å¤„ç†ä¿¡æ¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å®ƒä»¬å¹¶å¢å¼ºå®ƒä»¬çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦è¿›ä¸€æ­¥çš„ç ”ç©¶æ¥ç¡®ä¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿåº”å¯¹ç°å®ä¸–ç•Œä¸­çš„å¤æ‚æ€§å’Œä¸ç¡®å®šæ€§ï¼Œå¹¶èƒ½å¤Ÿåœ¨å„ç§æƒ…å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„é¢„æµ‹å’Œæ“ä½œå†³ç­–æ—¶ä¿æŒç¨³å®šå¯é çš„æ€§èƒ½æ°´å¹³ä»¥ç¡®ä¿å¯¹ç»ˆç«¯ç”¨æˆ·å…·æœ‰æŒç»­è€Œé•¿æœŸçš„ä»·å€¼äº§ç”Ÿæ›´å¤§çš„ä»·å€¼å¹¶æä¾›æ›´å‡†ç¡®çš„ä¿¡æ¯å’Œä¿¡æ¯æ›´æ–°çš„çŸ¥è¯†è¿­ä»£ä¸­æ”¹å–„äººä»¬å¯¹äºæ•´ä¸ªæ¨¡å‹çš„å¤„ç†æ–¹å¼ä¸è®¡ç®—åˆ†æã€‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ–¹æ³•å¯å‘äº†è¯¥é¢†åŸŸçš„è®¸å¤šæ–°æ¦‚å¿µåŒ…æ‹¬è¯­ä¹‰å±‚ç»“æ„åŒ–è¯­è¨€åº”ç”¨ç®—æ³•çš„ä¸Šä¸‹æ–‡å«ä¹‰å’Œå†…å®¹æ–¹é¢çš„å†³ç­–åå‘æ•´ä½“æ„å›¾ä¸Šçš„ä¸€äº›é€»è¾‘ç»“åˆå…³é”®ç‚¹ç­‰å¯¹äºæœªæ¥åœ¨è§†è§‰æ¨¡å‹é¢†åŸŸçš„å‘å±•å…·æœ‰æ·±è¿œå½±å“å¹¶å¯èƒ½æ¨åŠ¨è¯¥é¢†åŸŸå–å¾—æ›´å¤šçªç ´æ€§çš„è¿›å±•å’Œæˆæœã€‚ã€‚åŒæ—¶è¿™ä¹Ÿæé†’æˆ‘ä»¬å…³æ³¨æ•°æ®è´¨é‡å’Œæ•°æ®å¤šæ ·æ€§çš„é‡è¦æ€§åœ¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸä¸­æ•°æ®çš„é‡‡é›†æ•´ç†ä»¥åŠæ ‡å‡†åŒ–ç®¡ç†è¿‡ç¨‹æ˜¯ä¸€ä¸ªä¸å®¹å¿½è§†çš„ç¯èŠ‚ä¸ºäººå·¥æ™ºèƒ½æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ€§èƒ½æä¾›å…³é”®æ”¯æ’‘ä»¥è·å–æ›´ä¸ºç²¾ç¡®å…¨é¢çš„é¢„æµ‹ç»“æœå’Œåˆ†æç»“è®ºæœ€ç»ˆå¸®åŠ©å®ç°æŠ€æœ¯çš„ä»·å€¼å’Œå½±å“åŠ›ä»¥ä¾¿ä¿ƒè¿›äº§ä¸šçš„è¿›ä¸€æ­¥å‡çº§ä¸å‘å±•æ›´ä¼˜ç§€çš„æ¨¡å¼å¼€å‘åŠåº”ç”¨æä¾›æ›´å¤šæœ‰ç›Šçš„ä»·å€¼å¼•å¯¼æ€è·¯æ¨åŠ¨äººç±»ç¤¾ä¼šçš„å‘å±•ä¸å‰è¿›å…·æœ‰æ·±è¿œçš„ç°å®å’Œå†å²æ„ä¹‰</li>
</ul>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cbf67b205d949dbc508913841e2e08ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36c14db1840cb0e807a381d5c3489f07.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d83db7bbdedee5f907238fd92e01a28d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30bff7ebc9b5549ed1fa8696d585a162.jpg" align="middle">
</details>




<h2 id="Multiview-Equivariance-Improves-3D-Correspondence-Understanding-with-Minimal-Feature-Finetuning"><a href="#Multiview-Equivariance-Improves-3D-Correspondence-Understanding-with-Minimal-Feature-Finetuning" class="headerlink" title="Multiview Equivariance Improves 3D Correspondence Understanding with   Minimal Feature Finetuning"></a>Multiview Equivariance Improves 3D Correspondence Understanding with   Minimal Feature Finetuning</h2><p><strong>Authors:Yang You, Yixin Li, Congyue Deng, Yue Wang, Leonidas Guibas</strong></p>
<p>Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear. In this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose a simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D correspondence understanding of existing vision models. Remarkably, even finetuning on a single object for just one iteration results in substantial performance gains. All code and resources will be made publicly available to support further advancements in 3D-aware vision models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/qq456cvb/3DCorrEnhance">https://github.com/qq456cvb/3DCorrEnhance</a>. </p>
<blockquote>
<p>è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ViTç³»åˆ—æ¨¡å‹ï¼Œé€šè¿‡æä¾›ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œå·²ç»å®ç°äº†å¯¹å›¾åƒç†è§£çš„é©å‘½ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬åœ¨äºŒç»´ç†è§£æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨æ•æ‰ä¸‰ç»´ç©ºé—´å…³ç³»æ–¹é¢çš„èƒ½åŠ›ä»ç„¶ä¸æ˜ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°å¹¶å¢å¼ºäº†åŸºäºViTæ¨¡å‹çš„ä¸‰ç»´æ„è¯†ã€‚æˆ‘ä»¬é¦–å…ˆç³»ç»Ÿåœ°è¯„ä¼°äº†å®ƒä»¬å­¦ä¹ ä¸‰ç»´ç­‰ä»·ç‰¹å¾çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ£€æŸ¥äº†ä¸åŒè§†è§’ä¸‹çš„è¯­ä¹‰åµŒå…¥çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæé«˜ä¸‰ç»´ç­‰ä»·æ€§æœ‰åŠ©äºæé«˜å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å§¿æ€ä¼°è®¡ã€è·Ÿè¸ªå’Œè¯­ä¹‰è½¬ç§»ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºä¸‰ç»´å¯¹åº”å…³ç³»çš„å¾®è°ƒç­–ç•¥ï¼Œè¿™æå¤§åœ°æé«˜äº†ç°æœ‰è§†è§‰æ¨¡å‹çš„ä¸‰ç»´å¯¹åº”ç†è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ä»…åœ¨å•ä¸ªå¯¹è±¡ä¸Šå¾®è°ƒä¸€æ¬¡ä¹Ÿèƒ½äº§ç”Ÿæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ‰€æœ‰ä»£ç å’Œèµ„æºéƒ½å°†å…¬å¼€æä¾›ï¼Œä»¥æ”¯æŒä¸‰ç»´æ„ŸçŸ¥è§†è§‰æ¨¡å‹çš„è¿›ä¸€æ­¥å¼€å‘ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/qq456cvb/3DCorrEnhance">https://github.com/qq456cvb/3DCorrEnhance</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19458v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ViTå®¶æ—æ¨¡å‹åœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶åœ¨æŠŠæ¡3Dç©ºé—´å…³ç³»ä¸Šçš„èƒ½åŠ›å°šå¾…æ˜ç¡®ã€‚æœ¬æ–‡è¯„ä¼°å¹¶å¢å¼ºäº†ViTæ¨¡å‹çš„3Dæ„è¯†ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°å…¶åœ¨ä¸åŒè§†è§’ä¸‹çš„è¯­ä¹‰åµŒå…¥ä¸€è‡´æ€§ï¼Œå‘ç°æ”¹è¿›çš„3Dç­‰å˜æ€§æœ‰åŠ©äºæé«˜å§¿æ€ä¼°è®¡ã€è·Ÿè¸ªå’Œè¯­ä¹‰è¿ç§»ç­‰ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäº3Då¯¹åº”å…³ç³»çš„ç®€å•æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œå¯æ˜¾è‘—æé«˜ç°æœ‰è§†è§‰æ¨¡å‹çš„3Då¯¹åº”ç†è§£èƒ½åŠ›ã€‚å³ä½¿åªå¯¹å•ä¸ªå¯¹è±¡è¿›è¡Œå•æ¬¡è¿­ä»£å¾®è°ƒï¼Œä¹Ÿèƒ½å–å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViTæ¨¡å‹åœ¨å›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„è¯­ä¹‰ç‰¹å¾æå–èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†3Dç©ºé—´å…³ç³»æ—¶å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æ”¹è¿›æ¨¡å‹çš„3Dç­‰å˜æ€§æœ‰åŠ©äºæé«˜å…¶åœ¨å§¿æ€ä¼°è®¡ã€è·Ÿè¸ªå’Œè¯­ä¹‰è¿ç§»ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§†è§’ä¸‹çš„è¯­ä¹‰åµŒå…¥ä¸€è‡´æ€§ï¼Œå‘ç°äº†æ¨¡å‹åœ¨3Dæ„è¯†æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäº3Då¯¹åº”å…³ç³»çš„ç®€å•æœ‰æ•ˆçš„å¾®è°ƒç­–ç•¥ï¼Œç”¨äºå¢å¼ºç°æœ‰è§†è§‰æ¨¡å‹çš„3Dç†è§£ã€‚</li>
<li>å³ä½¿æ˜¯ç®€å•çš„å¾®è°ƒä¹Ÿèƒ½å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ‰€æœ‰ä»£ç å’Œèµ„æºå°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒ3Dæ„ŸçŸ¥è§†è§‰æ¨¡å‹çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09b2839d509045e265548ca89747a864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f90b4407c34b70f3b9435744fbf6453f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fd46627dbad89b94245b23e91a15cf8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be712b4c662f9f769117e53fcd3889cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-643d67e16e5313c3150f07943d6a2ee3.jpg" align="middle">
</details>




<h2 id="CLIP-meets-DINO-for-Tuning-Zero-Shot-Classifier-using-Unlabeled-Image-Collections"><a href="#CLIP-meets-DINO-for-Tuning-Zero-Shot-Classifier-using-Unlabeled-Image-Collections" class="headerlink" title="CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image   Collections"></a>CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image   Collections</h2><p><strong>Authors:Mohamed Fazli Imam, Rufael Fedaku Marew, Jameel Hassan, Mustansar Fiaz, Alham Fikri Aji, Hisham Cholakkal</strong></p>
<p>In the era of foundation models, CLIP has emerged as a powerful tool for aligning text and visual modalities into a common embedding space. However, the alignment objective used to train CLIP often results in subpar visual features for fine-grained tasks. In contrast, SSL-pretrained models like DINO excel at extracting rich visual features due to their specialized training paradigm. Yet, these SSL models require an additional supervised linear probing step, which relies on fully labeled data which is often expensive and difficult to obtain at scale. In this paper, we propose a label-free prompt-tuning method that leverages the rich visual features of self-supervised learning models (DINO) and the broad textual knowledge of large language models (LLMs) to largely enhance CLIP-based image classification performance using unlabeled images. Our approach unfolds in three key steps: (1) We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from LLMs, enabling more effective zero-shot classification compared to CLIPâ€™s default name-specific prompts. (2) These textual embeddings are then used to produce pseudo-labels to train an alignment module that integrates the complementary strengths of LLM description-based textual embeddings and DINOâ€™s visual features. (3) Finally, we prompt-tune CLIPâ€™s vision encoder through DINO-assisted supervision using the trained alignment module. This three-step process allows us to harness the best of visual and textual foundation models, resulting in a powerful and efficient approach that surpasses state-of-the-art label-free classification methods. Notably, our framework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6% over the state-of-the-art LaFter across 11 diverse image classification datasets. </p>
<blockquote>
<p>åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£ï¼ŒCLIPå·²ç»æˆä¸ºå¯¹é½æ–‡æœ¬å’Œè§†è§‰æ¨¡å¼åˆ°å…±åŒåµŒå…¥ç©ºé—´çš„æœ‰åŠ›å·¥å…·ã€‚ç„¶è€Œï¼Œç”¨äºè®­ç»ƒCLIPçš„å¯¹é½ç›®æ ‡å¸¸å¸¸å¯¼è‡´ç²¾ç»†ç²’åº¦ä»»åŠ¡çš„è§†è§‰ç‰¹å¾è¡¨ç°ä¸ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒDINOç­‰è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹ç”±äºå…¶ç‰¹æ®Šçš„è®­ç»ƒèŒƒå¼ï¼Œæ“…é•¿æå–ä¸°å¯Œçš„è§†è§‰ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™äº›SSLæ¨¡å‹éœ€è¦é¢å¤–çš„æœ‰ç›‘ç£çº¿æ€§æ¢æµ‹æ­¥éª¤ï¼Œè¿™ä¾èµ–äºå…¨é¢æ ‡æ³¨çš„æ•°æ®ï¼Œé€šå¸¸æˆæœ¬é«˜æ˜‚ä¸”éš¾ä»¥å¤§è§„æ¨¡è·å–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— æ ‡ç­¾æç¤ºå¾®è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ï¼ˆDINOï¼‰çš„ä¸°å¯Œè§†è§‰ç‰¹å¾å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹¿æ³›æ–‡æœ¬çŸ¥è¯†ï¼Œåˆ©ç”¨æ— æ ‡ç­¾å›¾åƒæå¤§åœ°æé«˜äº†åŸºäºCLIPçš„å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ç”Ÿæˆäº†é²æ£’æ€§æ–‡æœ¬ç‰¹å¾åµŒå…¥ï¼Œé€šè¿‡åˆ©ç”¨LLMçš„ç‰¹å®šç±»åˆ«æè¿°æ¥æ›´å‡†ç¡®ä»£è¡¨å¯¹è±¡ç±»åˆ«ï¼Œä»è€Œå®ç°ä¸CLIPçš„é»˜è®¤åç§°ç‰¹å®šæç¤ºç›¸æ¯”æ›´æœ‰æ•ˆçš„é›¶æ ·æœ¬åˆ†ç±»ã€‚å…¶æ¬¡ï¼Œè¿™äº›æ–‡æœ¬åµŒå…¥éšåç”¨äºç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»¥è®­ç»ƒä¸€ä¸ªæ•´åˆLLMæè¿°åŸºäºæ–‡æœ¬åµŒå…¥å’ŒDINOè§†è§‰ç‰¹å¾çš„äº’è¡¥ä¼˜ç‚¹çš„å¯¹é½æ¨¡å—ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è®­ç»ƒå¥½çš„å¯¹é½æ¨¡å—æ¥æç¤ºå¾®è°ƒCLIPçš„è§†è§‰ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨é€šè¿‡DINOè¾…åŠ©ç›‘ç£ã€‚è¿™ä¸‰æ­¥è¿‡ç¨‹ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬åŸºç¡€æ¨¡å‹çš„æœ€ä½³éƒ¨åˆ†ï¼Œä»è€Œå½¢æˆä¸€ç§å¼ºå¤§è€Œé«˜æ•ˆçš„æ–¹æ³•ï¼Œè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ— æ ‡ç­¾åˆ†ç±»æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶NoLAï¼ˆæ— æ ‡ç­¾é™„åŠ ï¼‰åœ¨11ä¸ªä¸åŒçš„å›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šå¹³å‡ç»å¯¹å¢ç›Šè¶…è¿‡æœ€æ–°æŠ€æœ¯LaFter 3.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19346v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨æ¨¡å‹æ—¶ä»£ï¼ŒCLIPå·²èƒ½å¤Ÿå°†æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€å¯¹é½åˆ°å…±åŒçš„åµŒå…¥ç©ºé—´ã€‚ä½†CLIPçš„å¯¹é½ç›®æ ‡è®­ç»ƒå¸¸å¸¸å¯¼è‡´ç²¾ç»†ä»»åŠ¡çš„è§†è§‰ç‰¹å¾è¡¨ç°ä¸ä½³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¦‚DINOçš„SSLé¢„è®­ç»ƒæ¨¡å‹æ“…é•¿æå–ä¸°å¯Œçš„è§†è§‰ç‰¹å¾ï¼Œä½†å…¶éœ€è¦é¢å¤–çš„ç›‘ç£çº¿æ€§æ¢æµ‹æ­¥éª¤ï¼Œä¾èµ–äºæ˜‚è´µçš„å…¨æ ‡è®°æ•°æ®ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ— æ ‡ç­¾æç¤ºè°ƒèŠ‚æ–¹æ³•ï¼Œåˆ©ç”¨SSLæ¨¡å‹çš„ä¸°å¯Œè§†è§‰ç‰¹å¾å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›æ–‡æœ¬çŸ¥è¯†ï¼Œé€šè¿‡æ— æ ‡ç­¾å›¾åƒå¤§å¹…æé«˜CLIPçš„å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚æ–¹æ³•åˆ†ä¸ºä¸‰æ­¥ï¼šç”Ÿæˆæ›´å‡†ç¡®çš„æ–‡æœ¬ç‰¹å¾åµŒå…¥ï¼Œåˆ©ç”¨LLMçš„ç‰¹å®šç±»åˆ«æè¿°å®ç°æ›´æœ‰æ•ˆçš„é›¶æ ·æœ¬åˆ†ç±»ï¼›ä½¿ç”¨è¿™äº›æ–‡æœ¬åµŒå…¥ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè®­ç»ƒæ•´åˆLLMæè¿°å’ŒDINOè§†è§‰ç‰¹å¾çš„å¯¹æ¯”æ¨¡å—ï¼›é€šè¿‡DINOè¾…åŠ©ç›‘ç£è°ƒæ•´CLIPçš„è§†è§‰ç¼–ç å™¨ã€‚æ­¤æµç¨‹ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè¶…è¶Šäº†ç°æœ‰æ— æ ‡ç­¾åˆ†ç±»æ–¹æ³•ã€‚å°¤å…¶åœ¨NoLAæ¡†æ¶ä¸‹ï¼Œå¹³å‡ç»å¯¹å¢ç›Šè¾¾3.6%ï¼Œè¶…è¿‡æœ€æ–°LaFteræ–¹æ³•ï¼Œåœ¨11ä¸ªä¸åŒå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨æ–‡æœ¬å’Œè§†è§‰æ¨¡æ€å¯¹é½æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç»†ä»»åŠ¡ä¸Šçš„è§†è§‰ç‰¹å¾è¡¨ç°ä¸ä½³ã€‚</li>
<li>SSLæ¨¡å‹å¦‚DINOèƒ½å¤Ÿæå–ä¸°å¯Œçš„è§†è§‰ç‰¹å¾ï¼Œä½†éœ€è¦é¢å¤–çš„å…¨æ ‡è®°æ•°æ®ã€‚</li>
<li>æå‡ºä¸€ç§æ— æ ‡ç­¾æç¤ºè°ƒèŠ‚æ–¹æ³•ï¼Œç»“åˆLLMçš„æ–‡æœ¬çŸ¥è¯†å’ŒDINOçš„è§†è§‰ç‰¹å¾ï¼Œæé«˜CLIPçš„å›¾åƒåˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬ç”Ÿæˆæ›´å‡†ç¡®çš„æ–‡æœ¬ç‰¹å¾åµŒå…¥ã€ä½¿ç”¨ä¼ªæ ‡ç­¾è®­ç»ƒå¯¹æ¯”æ¨¡å—ã€é€šè¿‡DINOè¾…åŠ©ç›‘ç£è°ƒæ•´CLIPè§†è§‰ç¼–ç å™¨ã€‚</li>
<li>NoLAæ¡†æ¶è¶…è¶Šç°æœ‰æ— æ ‡ç­¾åˆ†ç±»æ–¹æ³•ï¼Œå¹³å‡ç»å¯¹å¢ç›Šè¾¾3.6%ã€‚</li>
<li>NoLAæ¡†æ¶åœ¨å¤šä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59f661a51fd451454a94db4e6c4a1f29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5492887114059a9e317dc1321e923e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0f21059b7804dbd15c4ed9cd8863604.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07ec020771f2b067759156c3f06505a0.jpg" align="middle">
</details>




<h2 id="Enhancing-Parameter-Efficient-Fine-Tuning-of-Vision-Transformers-through-Frequency-Based-Adaptation"><a href="#Enhancing-Parameter-Efficient-Fine-Tuning-of-Vision-Transformers-through-Frequency-Based-Adaptation" class="headerlink" title="Enhancing Parameter-Efficient Fine-Tuning of Vision Transformers through   Frequency-Based Adaptation"></a>Enhancing Parameter-Efficient Fine-Tuning of Vision Transformers through   Frequency-Based Adaptation</h2><p><strong>Authors:Son Thai Ly, Hien V. Nguyen</strong></p>
<p>Adapting vision transformer foundation models through parameter-efficient fine-tuning (PEFT) methods has become increasingly popular. These methods optimize a limited subset of parameters, enabling efficient adaptation without the need to fine-tune the entire model while still achieving competitive performance. However, traditional PEFT methods may limit the modelâ€™s capacity to capture complex patterns, especially those associated with high-frequency spectra. This limitation becomes particularly problematic as existing research indicates that high-frequency features are crucial for distinguishing subtle image structures. To address this issue, we introduce FreqFit, a novel Frequency Fine-tuning module between ViT blocks to enhance model adaptability. FreqFit is simple yet surprisingly effective, and can be integrated with all existing PEFT methods to boost their performance. By manipulating features in the frequency domain, our approach allows models to capture subtle patterns more effectively. Extensive experiments on 24 datasets, using both supervised and self-supervised foundational models with various state-of-the-art PEFT methods, reveal that FreqFit consistently improves performance over the original PEFT methods with performance gains ranging from 1% to 16%. For instance, FreqFit-LoRA surpasses the performances of state-of-the-art baselines on CIFAR100 by more than 10% even without applying regularization or strong augmentation. For reproducibility purposes, the source code is available at <a target="_blank" rel="noopener" href="https://github.com/tsly123/FreqFiT">https://github.com/tsly123/FreqFiT</a>. </p>
<blockquote>
<p>é€‚åº”è§†è§‰å˜å‹å™¨åŸºç¡€æ¨¡å‹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚è¿™äº›æ–¹æ³•ä¼˜åŒ–äº†ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å®ç°é«˜æ•ˆçš„é€‚åº”ï¼ŒåŒæ—¶ä»ä¿æŒè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„PEFTæ–¹æ³•å¯èƒ½ä¼šé™åˆ¶æ¨¡å‹æ•æ‰å¤æ‚æ¨¡å¼çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯ä¸é«˜é¢‘å…‰è°±ç›¸å…³çš„æ¨¡å¼ã€‚è¿™ä¸ªé™åˆ¶ç‰¹åˆ«æˆé—®é¢˜ï¼Œå› ä¸ºç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œé«˜é¢‘ç‰¹å¾å¯¹äºåŒºåˆ†å¾®å¦™çš„å›¾åƒç»“æ„è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FreqFitï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é¢‘ç‡å¾®è°ƒæ¨¡å—ï¼Œä½äºViTå—ä¹‹é—´ï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚FreqFitç®€å•è€Œæœ‰æ•ˆï¼Œå¯ä»¥ä¸ç°æœ‰çš„æ‰€æœ‰PEFTæ–¹æ³•é›†æˆï¼Œä»¥æé«˜å®ƒä»¬çš„æ€§èƒ½ã€‚é€šè¿‡æ“ä½œé¢‘åŸŸä¸­çš„ç‰¹å¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å¾®å¦™çš„æ¨¡å¼ã€‚åœ¨24ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒï¼Œä½¿ç”¨å„ç§å…ˆè¿›çš„PEFTæ–¹æ³•çš„ç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£åŸºç¡€æ¨¡å‹ï¼Œè¡¨æ˜FreqFitåœ¨åŸå§‹PEFTæ–¹æ³•çš„åŸºç¡€ä¸ŠæŒç»­æé«˜äº†æ€§èƒ½ï¼Œæ€§èƒ½æå‡èŒƒå›´åœ¨1%åˆ°16%ä¹‹é—´ã€‚ä¾‹å¦‚ï¼ŒFreqFit-LoRAåœ¨CIFAR100ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†æœ€æ–°åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ˜¯åœ¨æ²¡æœ‰åº”ç”¨æ­£åˆ™åŒ–æˆ–å¼ºçƒˆå¢å¼ºçš„æƒ…å†µä¸‹å®ç°çš„ã€‚ä¸ºäº†å¯é‡å¤å®éªŒçš„ç›®çš„ï¼Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tsly123/FreqFiT">https://github.com/tsly123/FreqFiT</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19297v1">PDF</a> 24 pages</p>
<p><strong>æ‘˜è¦</strong><br>    å¼•å…¥äº†ä¸€ç§æ–°å‹çš„é¢‘ç‡å¾®è°ƒæ¨¡å—FreqFitï¼Œå®ƒèƒ½å¤Ÿåœ¨è§†è§‰è½¬æ¢å™¨åŸºç¡€æ¨¡å‹ä¹‹é—´å¢å¼ºæ¨¡å‹é€‚åº”æ€§ï¼Œæé«˜æ¨¡å‹å¯¹é«˜é¢‘ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚é€šè¿‡é¢‘ç‡åŸŸçš„ç‰¹å¾æ“ä½œï¼ŒFreqFitèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ•æ‰å¾®å¦™çš„æ¨¡å¼ï¼Œå¹¶ä¸ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç›¸ç»“åˆï¼Œä»¥æé«˜æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFreqFitåœ¨åŸå§‹å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„åŸºç¡€ä¸ŠæŒç»­æé«˜äº†æ€§èƒ½ï¼Œæ€§èƒ½æå‡èŒƒå›´åœ¨1%åˆ°16%ä¹‹é—´ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢‘ç‡å¾®è°ƒæ¨¡å—FreqFitè¢«å¼•å…¥ä»¥å¢å¼ºè§†è§‰è½¬æ¢å™¨æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>FreqFitèƒ½å¤Ÿåœ¨ViTå—ä¹‹é—´æé«˜æ¨¡å‹å¯¹é«˜é¢‘ç‰¹å¾çš„æ•æ‰èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ“ä½œé¢‘ç‡åŸŸçš„ç‰¹å¾ï¼ŒFreqFitèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰å¾®å¦™çš„æ¨¡å¼ã€‚</li>
<li>FreqFitå¯ä»¥ä¸ç°æœ‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ç›¸ç»“åˆï¼Œä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFreqFitåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸå§‹å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚</li>
<li>FreqFitçš„æ€§èƒ½æå‡èŒƒå›´åœ¨1%åˆ°16%ä¹‹é—´ã€‚</li>
<li>FreqFit-LoRAåœ¨CIFAR100ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰å…ˆè¿›åŸºçº¿ï¼Œå³ä½¿åœ¨æ²¡æœ‰åº”ç”¨æ­£åˆ™åŒ–æˆ–å¼ºå¢å¼ºçš„æƒ…å†µä¸‹ï¼Œæå‡å¹…åº¦ä¹Ÿè¶…è¿‡10%ã€‚æºä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7b30a2ba2a248aa084456ab767c923f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebdf22d6debcbf956456f7b59486a312.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a18446b3ab653abcc26e5bfa4968e07.jpg" align="middle">
</details>




<h2 id="MVFormer-Diversifying-Feature-Normalization-and-Token-Mixing-for-Efficient-Vision-Transformers"><a href="#MVFormer-Diversifying-Feature-Normalization-and-Token-Mixing-for-Efficient-Vision-Transformers" class="headerlink" title="MVFormer: Diversifying Feature Normalization and Token Mixing for   Efficient Vision Transformers"></a>MVFormer: Diversifying Feature Normalization and Token Mixing for   Efficient Vision Transformers</h2><p><strong>Authors:Jongseong Bae, Susang Kim, Minsu Cho, Ha Young Kim</strong></p>
<p>Active research is currently underway to enhance the efficiency of vision transformers (ViTs). Most studies have focused solely on effective token mixers, overlooking the potential relationship with normalization. To boost diverse feature learning, we propose two components: a normalization module called multi-view normalization (MVN) and a token mixer called multi-view token mixer (MVTM). The MVN integrates three differently normalized features via batch, layer, and instance normalization using a learnable weighted sum. Each normalization method outputs a different distribution, generating distinct features. Thus, the MVN is expected to offer diverse pattern information to the token mixer, resulting in beneficial synergy. The MVTM is a convolution-based multiscale token mixer with local, intermediate, and global filters, and it incorporates stage specificity by configuring various receptive fields for the token mixer at each stage, efficiently capturing ranges of visual patterns. We propose a novel ViT model, multi-vision transformer (MVFormer), adopting the MVN and MVTM in the MetaFormer block, the generalized ViT scheme. Our MVFormer outperforms state-of-the-art convolution-based ViTs on image classification, object detection, and instance and semantic segmentation with the same or lower parameters and MACs. Particularly, MVFormer variants, MVFormer-T, S, and B achieve 83.4%, 84.3%, and 84.6% top-1 accuracy, respectively, on ImageNet-1K benchmark. </p>
<blockquote>
<p>ç›®å‰ï¼Œæ­£åœ¨ç§¯æè¿›è¡Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æ•ˆç‡æå‡ç ”ç©¶ã€‚å¤§å¤šæ•°ç ”ç©¶ä»…ä¸“æ³¨äºæœ‰æ•ˆçš„ä»¤ç‰Œæ··åˆå™¨ï¼Œå¿½è§†äº†å…¶ä¸å½’ä¸€åŒ–çš„æ½œåœ¨å…³ç³»ã€‚ä¸ºäº†æå‡å¤šæ ·åŒ–çš„ç‰¹å¾å­¦ä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªç»„ä»¶ï¼šä¸€ä¸ªåä¸ºå¤šè§†å›¾å½’ä¸€åŒ–ï¼ˆMVNï¼‰çš„å½’ä¸€åŒ–æ¨¡å—å’Œä¸€ä¸ªåä¸ºå¤šè§†å›¾ä»¤ç‰Œæ··åˆå™¨ï¼ˆMVTMï¼‰çš„ä»¤ç‰Œæ··åˆå™¨ã€‚MVNé€šè¿‡æ‰¹æ¬¡ã€å±‚å’Œå®ä¾‹å½’ä¸€åŒ–ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„åŠ æƒå’Œï¼Œé›†æˆäº†ä¸‰ç§ä¸åŒçš„å½’ä¸€åŒ–ç‰¹å¾ã€‚æ¯ç§å½’ä¸€åŒ–æ–¹æ³•è¾“å‡ºä¸åŒçš„åˆ†å¸ƒï¼Œä»è€Œäº§ç”Ÿä¸åŒçš„ç‰¹å¾ã€‚å› æ­¤ï¼ŒMVNé¢„è®¡å°†ä¸ºä»¤ç‰Œæ··åˆå™¨æä¾›å¤šæ ·åŒ–çš„æ¨¡å¼ä¿¡æ¯ï¼Œä»è€Œäº§ç”Ÿæœ‰ç›Šçš„ååŒä½œç”¨ã€‚MVTMæ˜¯ä¸€ç§åŸºäºå·ç§¯çš„å¤šå°ºåº¦ä»¤ç‰Œæ··åˆå™¨ï¼Œå…·æœ‰å±€éƒ¨ã€ä¸­é—´å’Œå…¨å±€è¿‡æ»¤å™¨ï¼Œå®ƒé€šè¿‡é…ç½®ä»¤ç‰Œæ··åˆå™¨æ¯ä¸ªé˜¶æ®µçš„å„ç§æ„Ÿå—é‡æ¥å®ç°é˜¶æ®µç‰¹å¼‚æ€§ï¼Œæœ‰æ•ˆåœ°æ•è·äº†è§†è§‰æ¨¡å¼çš„èŒƒå›´ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ViTæ¨¡å‹ï¼Œå³å¤šè§†è§’è½¬æ¢å™¨ï¼ˆMVFormerï¼‰ï¼Œåœ¨MetaFormerå—ä¸­é‡‡ç”¨MVNå’ŒMVTMï¼Œä»¥åŠé€šç”¨çš„ViTæ–¹æ¡ˆã€‚æˆ‘ä»¬çš„MVFormeråœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢ä¼˜äºæœ€æ–°çš„åŸºäºå·ç§¯çš„ViTsï¼Œå…·æœ‰ç›¸åŒæˆ–æ›´ä½çš„å‚æ•°å’ŒMACsã€‚ç‰¹åˆ«æ˜¯ï¼ŒMVFormerçš„å˜ä½“MVFormer-Tã€Så’ŒBåœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«å®ç°äº†83.4%ã€84.3%å’Œ84.6%çš„top-1å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18995v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ´»è·ƒç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹MVFormerã€‚MVFormeré‡‡ç”¨äº†åä¸ºMVNçš„å½’ä¸€åŒ–æ¨¡å—å’Œåä¸ºMVTMçš„ä»¤ç‰Œæ··åˆå™¨ï¼ŒäºŒè€…ååŒä½œç”¨æå‡ç‰¹å¾å­¦ä¹ å¤šæ ·æ€§ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸æ¯”å½“å‰å…ˆè¿›çš„å·ç§¯åŸºViTså…·æœ‰ç›¸åŒæˆ–æ›´ä½çš„å‚æ•°å’ŒMACsã€‚åœ¨ImageNet-1KåŸºå‡†æµ‹è¯•ä¸­ï¼ŒMVFormerçš„ä¸åŒç‰ˆæœ¬MVFormer-Tã€Så’ŒBåˆ†åˆ«å®ç°äº†83.4%ã€84.3%å’Œ84.6%çš„top-1å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„æ•ˆç‡æå‡æ˜¯å½“å‰æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å½’ä¸€åŒ–æ¨¡å—MVNï¼Œé€šè¿‡é›†æˆä¸‰ç§ä¸åŒçš„å½’ä¸€åŒ–ç‰¹å¾ï¼ˆæ‰¹å¤„ç†ã€å±‚å’Œå®ä¾‹å½’ä¸€åŒ–ï¼‰æ¥å¢å¼ºç‰¹å¾å­¦ä¹ ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºMVTMçš„ä»¤ç‰Œæ··åˆå™¨ï¼Œä¸MVNæ¨¡å—ååŒå·¥ä½œï¼Œäº§ç”Ÿä¸åŒçš„ç‰¹å¾åˆ†å¸ƒï¼Œæä¾›å¤šæ ·åŒ–çš„æ¨¡å¼ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„ViTæ¨¡å‹MVFormerï¼Œé‡‡ç”¨å¹¿ä¹‰ViTæ–¹æ¡ˆMetaFormerå—ä¸­çš„MVNå’ŒMVTMã€‚</li>
<li>MVFormeråœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸å½“å‰å…ˆè¿›çš„å·ç§¯åŸºViTsç›¸æ¯”ï¼ŒMVFormerå…·æœ‰ç›¸åŒæˆ–æ›´ä½çš„å‚æ•°å’ŒMACsã€‚</li>
</ol>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b84ceaeaca252f3237ddd90d4e765d74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-821d53865b68e187a68214bc649b11c2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28339ba132be3ff329073a958f9176a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-524464286caf1fb9ed961ddb5d70fa9c.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-10/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de829c8eefa8229fee894461d46ba5e6.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  No Annotations for Object Detection in Art through Stable Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-10/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-21e5621df29b631d04b41f2f4008ecb3.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2024-12-11  Towards Long Video Understanding via Fine-detailed Video Story   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
