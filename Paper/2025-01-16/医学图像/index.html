<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Continual Deep Active Learning for Medical Imaging Replay-Base   Architecture for Context Adaptation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2e29c095df7cd113c3d52f99735feac3.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-16-æ›´æ–°"><a href="#2025-01-16-æ›´æ–°" class="headerlink" title="2025-01-16 æ›´æ–°"></a>2025-01-16 æ›´æ–°</h1><h2 id="Continual-Deep-Active-Learning-for-Medical-Imaging-Replay-Base-Architecture-for-Context-Adaptation"><a href="#Continual-Deep-Active-Learning-for-Medical-Imaging-Replay-Base-Architecture-for-Context-Adaptation" class="headerlink" title="Continual Deep Active Learning for Medical Imaging: Replay-Base   Architecture for Context Adaptation"></a>Continual Deep Active Learning for Medical Imaging: Replay-Base   Architecture for Context Adaptation</h2><p><strong>Authors:Rui Daniel, M. Rita Verdelho, Catarina Barata, Carlos Santiago</strong></p>
<p>Deep Learning for medical imaging faces challenges in adapting and generalizing to new contexts. Additionally, it often lacks sufficient labeled data for specific tasks requiring significant annotation effort. Continual Learning (CL) tackles adaptability and generalizability by enabling lifelong learning from a data stream while mitigating forgetting of previously learned knowledge. Active Learning (AL) reduces the number of required annotations for effective training. This work explores both approaches (CAL) to develop a novel framework for robust medical image analysis. Based on the automatic recognition of shifts in image characteristics, Replay-Base Architecture for Context Adaptation (RBACA) employs a CL rehearsal method to continually learn from diverse contexts, and an AL component to select the most informative instances for annotation. A novel approach to evaluate CAL methods is established using a defined metric denominated IL-Score, which allows for the simultaneous assessment of transfer learning, forgetting, and final model performance. We show that RBACA works in domain and class-incremental learning scenarios, by assessing its IL-Score on the segmentation and diagnosis of cardiac images. The results show that RBACA outperforms a baseline framework without CAL, and a state-of-the-art CAL method across various memory sizes and annotation budgets. Our code is available in <a target="_blank" rel="noopener" href="https://github.com/RuiDaniel/RBACA">https://github.com/RuiDaniel/RBACA</a> . </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ·±åº¦å­¦ä¹ åœ¨é€‚åº”å’Œæ³›åŒ–åˆ°æ–°ç¯å¢ƒæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå®ƒå¸¸å¸¸ç¼ºä¹é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å……è¶³æ ‡è®°æ•°æ®ï¼Œéœ€è¦å¤§é‡æ³¨é‡Šå·¥ä½œã€‚æŒç»­å­¦ä¹ ï¼ˆCLï¼‰é€šè¿‡å®ç°ä»æ•°æ®æµä¸­çš„ç»ˆèº«å­¦ä¹ å¹¶å‡è½»å¯¹å…ˆå‰å­¦ä¹ çŸ¥è¯†çš„é—å¿˜ï¼Œæ¥è§£å†³é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰å‡å°‘äº†è¿›è¡Œæœ‰æ•ˆè®­ç»ƒæ‰€éœ€çš„æ³¨é‡Šæ•°é‡ã€‚è¿™é¡¹å·¥ä½œæ¢è®¨äº†è¿™ä¸¤ç§æ–¹æ³•ï¼ˆCALï¼‰ï¼Œä»¥å¼€å‘ç”¨äºç¨³å¥åŒ»å­¦å›¾åƒåˆ†æçš„å…¨æ–°æ¡†æ¶ã€‚åŸºäºå›¾åƒç‰¹æ€§å˜åŒ–çš„è‡ªåŠ¨è¯†åˆ«ï¼Œç”¨äºä¸Šä¸‹æ–‡é€‚åº”çš„å›æ”¾åŸºç¡€æ¶æ„ï¼ˆRBACAï¼‰é‡‡ç”¨ä¸€ç§CLå¤ä¹ æ–¹æ³•ä»¥å®ç°ä»å¤šç§ä¸Šä¸‹æ–‡ä¸­çš„æŒç»­å­¦ä¹ ï¼Œå¹¶é‡‡ç”¨ALç»„ä»¶æ¥é€‰æ‹©æœ€å…·æœ‰ä¿¡æ¯æ€§çš„å®ä¾‹è¿›è¡Œæ³¨é‡Šã€‚ä½¿ç”¨å®šä¹‰çš„åº¦é‡æ ‡å‡†IL-Scoreå»ºç«‹äº†ä¸€ç§è¯„ä¼°CALæ–¹æ³•çš„æ–°æ–¹æ³•ï¼Œè¯¥æ ‡å‡†å¯ä»¥åŒæ—¶è¯„ä¼°è¿ç§»å­¦ä¹ ã€é—å¿˜å’Œæœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡è¯„ä¼°å¿ƒè„å›¾åƒåˆ†å‰²å’Œè¯Šæ–­çš„IL-Scoreï¼Œè¯æ˜äº†RBACAåœ¨é¢†åŸŸå’Œç±»å¢é‡å­¦ä¹ åœºæ™¯ä¸­çš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼ŒRBACAåœ¨å„ç§å†…å­˜å¤§å°å’Œæ³¨é‡Šé¢„ç®—æ–¹é¢ä¼˜äºæ— CALçš„åŸºçº¿æ¡†æ¶ä»¥åŠæœ€å…ˆè¿›çš„CALæ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RuiDaniel/RBACA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RuiDaniel/RBACAä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08245v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æ¢è®¨æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é€‚åº”æ–°è¯­å¢ƒå’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œä»¥åŠç¼ºä¹å……è¶³æ ‡æ³¨æ•°æ®çš„é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ç»“åˆæŒç»­å­¦ä¹ ï¼ˆCLï¼‰å’Œä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨å»ºç«‹ä¸€ä¸ªç¨³å¥çš„åŒ»å­¦å½±åƒåˆ†ææ¡†æ¶ã€‚é€šè¿‡é‡‡ç”¨åŸºäºå›¾åƒç‰¹æ€§å˜åŒ–çš„è‡ªåŠ¨è¯†åˆ«æŠ€æœ¯ï¼Œæ¡†æ¶å®ç°äº†æŒç»­å­¦ä¹ çš„é€‚åº”èƒ½åŠ›å’Œä¸»åŠ¨å­¦ä¹ æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨åŸŸå’Œç±»åˆ«å¢é‡å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºè¾ƒå¥½çš„æ•ˆæœã€‚éšç€å„ç§è®°å¿†å¤§å°å’Œæ ‡æ³¨é¢„ç®—çš„è¯„ä¼°ï¼Œè¯¥æ¡†æ¶ä¼˜äºåŸºçº¿æ¡†æ¶å’Œå½“å‰å…ˆè¿›çš„CALæ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/RuiDaniel/RBACA%E3%80%82">https://github.com/RuiDaniel/RBACAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒé¢†åŸŸé¢ä¸´é€‚åº”æ–°è¯­å¢ƒå’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œä»¥åŠæ ‡æ³¨æ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æŒç»­å­¦ä¹ ï¼ˆCLï¼‰å’Œä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰è¢«ç»“åˆæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå»ºç«‹ä¸€ä¸ªç¨³å¥çš„åŒ»å­¦å½±åƒåˆ†ææ¡†æ¶ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨åŸºäºå›¾åƒç‰¹æ€§å˜åŒ–çš„è‡ªåŠ¨è¯†åˆ«çš„æŒç»­å­¦ä¹ é€‚åº”èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ä¸»åŠ¨å­¦ä¹ æœºåˆ¶ï¼Œæ¡†æ¶èƒ½å¤Ÿé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„å®ä¾‹è¿›è¡Œæ ‡æ³¨ï¼Œå‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨åŸŸå’Œç±»åˆ«å¢é‡å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å„ç§è®°å¿†å¤§å°å’Œæ ‡æ³¨é¢„ç®—ä¸‹çš„æ€§èƒ½ä¼˜äºåŸºçº¿æ¡†æ¶å’Œå½“å‰å…ˆè¿›çš„CALæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df189efdb70273085876b8859714b8df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c857c0aacb642e39c2f974e25f445e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e20837f7b203298d0bde68976658054.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Efficient-Deep-Learning-based-Forward-Solvers-for-Brain-Tumor-Growth-Models"><a href="#Efficient-Deep-Learning-based-Forward-Solvers-for-Brain-Tumor-Growth-Models" class="headerlink" title="Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth   Models"></a>Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth   Models</h2><p><strong>Authors:Zeineb Haouari, Jonas Weidner, Ivan Ezhov, Aswathi Varma, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler</strong></p>
<p>Glioblastoma, a highly aggressive brain tumor, poses major challenges due to its poor prognosis and high morbidity rates. Partial differential equation-based models offer promising potential to enhance therapeutic outcomes by simulating patient-specific tumor behavior for improved radiotherapy planning. However, model calibration remains a bottleneck due to the high computational demands of optimization methods like Monte Carlo sampling and evolutionary algorithms. To address this, we recently introduced an approach leveraging a neural forward solver with gradient-based optimization to significantly reduce calibration time. This approach requires a highly accurate and fully differentiable forward model. We investigate multiple architectures, including (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a 3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best overall results, excelling in both tumor outline matching and voxel-level prediction of tumor cell concentration. It halved the MSE relative to the baseline model and achieved the highest Dice score across all tumor cell concentration thresholds. Our study demonstrates significant enhancement in forward solver performance and outlines important future research directions. </p>
<blockquote>
<p>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯ä¸€ç§é«˜åº¦ä¾µè¢­æ€§çš„è„‘è‚¿ç˜¤ï¼Œç”±äºå…¶é¢„åä¸è‰¯å’Œé«˜å‘ç—…ç‡ï¼Œå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åŸºäºåå¾®åˆ†æ–¹ç¨‹æ¨¡å‹çš„æ¨¡æ‹Ÿæ‚£è€…ç‰¹å¼‚æ€§è‚¿ç˜¤è¡Œä¸ºï¼Œåœ¨æé«˜æ”¾ç–—è®¡åˆ’æ–¹é¢ï¼Œæ˜¾ç¤ºå‡ºæé«˜æ²»ç–—æ•ˆæœçš„å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œæ¨¡å‹æ ¡å‡†ä»æ˜¯ç“¶é¢ˆï¼Œå› ä¸ºä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚è’™ç‰¹å¡æ´›é‡‡æ ·å’Œè¿›åŒ–ç®—æ³•ï¼‰çš„è®¡ç®—éœ€æ±‚å¾ˆé«˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æœ€è¿‘å¼•å…¥äº†ä¸€ç§åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ç¥ç»æ­£å‘æ±‚è§£å™¨çš„æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†æ ¡å‡†æ—¶é—´ã€‚è¿™ç§æ–¹æ³•éœ€è¦ä¸€ä¸ªé«˜åº¦ç²¾ç¡®å’Œå®Œå…¨å¯å¾®åˆ†çš„æ­£å‘æ¨¡å‹ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†å¤šç§æ¶æ„ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰å¢å¼ºçš„TumorSurrogateï¼Œï¼ˆiiï¼‰æ”¹è¿›çš„nnU-Netï¼Œï¼ˆiiiï¼‰3D Vision Transformerï¼ˆViTï¼‰ã€‚ä¼˜åŒ–åçš„TumorSurrogateå–å¾—äº†æœ€ä½³çš„æ•´ä½“ç»“æœï¼Œåœ¨è‚¿ç˜¤è½®å»“åŒ¹é…å’Œè‚¿ç˜¤ç»†èƒæµ“åº¦çš„ä½“ç´ çº§é¢„æµ‹æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ã€‚ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä½¿å‡æ–¹è¯¯å·®å‡åŠï¼Œå¹¶åœ¨æ‰€æœ‰è‚¿ç˜¤ç»†èƒæµ“åº¦é˜ˆå€¼ä¸Šå–å¾—äº†æœ€é«˜çš„Diceå¾—åˆ†ã€‚æˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†æ­£å‘æ±‚è§£å™¨æ€§èƒ½çš„æ˜¾è‘—æé«˜ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥é‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08226v1">PDF</a> </p>
<p><strong>Summary</strong><br>     èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯ä¸€ç§é«˜åº¦ä¾µè¢­æ€§çš„è„‘è‚¿ç˜¤ï¼Œé¢„åä¸è‰¯ä¸”å‘ç—…ç‡é«˜ï¼Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åŸºäºåå¾®åˆ†æ–¹ç¨‹æ¨¡å‹çš„ä¼˜åŒ–æ”¾ç–—è®¡åˆ’å¯¹æ‚£è€…ç‰¹å¼‚æ€§è‚¿ç˜¤è¡Œä¸ºè¿›è¡Œæ¨¡æ‹Ÿï¼Œå…·æœ‰æé«˜æ²»ç–—æ•ˆæœçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ¨¡å‹æ ¡å‡†å› è’™ç‰¹å¡ç½—é‡‡æ ·å’Œè¿›åŒ–ç®—æ³•ç­‰ä¼˜åŒ–æ–¹æ³•çš„é«˜è®¡ç®—éœ€æ±‚è€Œæˆä¸ºç“¶é¢ˆã€‚æœ€è¿‘ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¥ç»ç½‘ç»œå‰å‘æ±‚è§£å™¨ä¸åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†æ ¡å‡†æ—¶é—´ã€‚è¯¥æ–¹æ³•éœ€è¦é«˜åº¦ç²¾ç¡®å’Œå®Œå…¨å¯åŒºåˆ†çš„å‰å‘æ¨¡å‹ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†å¤šç§æ¶æ„ï¼ŒåŒ…æ‹¬å¢å¼ºçš„TumorSurrogateã€æ”¹è¿›çš„nnU-Netå’Œ3D Vision Transformerï¼ˆViTï¼‰ã€‚ä¼˜åŒ–åçš„TumorSurrogateå–å¾—æœ€ä½³ç»“æœï¼Œåœ¨è‚¿ç˜¤è½®å»“åŒ¹é…å’Œè‚¿ç˜¤ç»†èƒæµ“åº¦ä½“ç´ çº§é¢„æµ‹æ–¹é¢éƒ½è¡¨ç°å‡ºè‰²ï¼Œç›¸å¯¹åŸºå‡†æ¨¡å‹é™ä½äº†å‡æ–¹è¯¯å·®ï¼Œå¹¶åœ¨æ‰€æœ‰è‚¿ç˜¤ç»†èƒæµ“åº¦é˜ˆå€¼ä¸Šè·å¾—æœ€é«˜Diceå¾—åˆ†ã€‚æœ¬ç ”ç©¶æ˜¾è‘—æé«˜äº†å‰å‘æ±‚è§£å™¨çš„æ€§èƒ½ï¼Œå¹¶æŒ‡å‡ºäº†é‡è¦çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯ä¸€ç§å…·æœ‰é«˜åº¦ä¾µè¢­æ€§å’Œä¸è‰¯é¢„åçš„è„‘è‚¿ç˜¤ã€‚</li>
<li>åŸºäºåå¾®åˆ†æ–¹ç¨‹æ¨¡å‹çš„æ¨¡æ‹Ÿèƒ½å¤Ÿä¼˜åŒ–æ”¾ç–—è®¡åˆ’ä»¥æé«˜æ²»ç–—æ•ˆæœã€‚</li>
<li>æ¨¡å‹æ ¡å‡†æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¼˜åŒ–æ–¹æ³•çš„é«˜è®¡ç®—éœ€æ±‚è€Œæˆä¸ºç“¶é¢ˆã€‚</li>
<li>é‡‡ç”¨ç¥ç»ç½‘ç»œå‰å‘æ±‚è§£å™¨ä¸åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨¡å‹æ ¡å‡†æ—¶é—´ã€‚</li>
<li>ç ”ç©¶äº†å¤šç§æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬TumorSurrogateã€nnU-Netå’ŒViTã€‚</li>
<li>ä¼˜åŒ–åçš„TumorSurrogateåœ¨è‚¿ç˜¤è½®å»“åŒ¹é…å’Œè‚¿ç˜¤ç»†èƒæµ“åº¦é¢„æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08226">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e859715dbe86d090184c4b8ab3f1f5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0802f3354f864bd27354b9bdd1938342.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-955491489e021fbe51fc1533db483174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5aff4cb3369bea0c3f8c555bfc51fffd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51f0a9949274d0ac0b36d3861cd7ed01.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DM-Mamba-Dual-domain-Multi-scale-Mamba-for-MRI-reconstruction"><a href="#DM-Mamba-Dual-domain-Multi-scale-Mamba-for-MRI-reconstruction" class="headerlink" title="DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction"></a>DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction</h2><p><strong>Authors:Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi</strong></p>
<p>The accelerated MRI reconstruction poses a challenging ill-posed inverse problem due to the significant undersampling in k-space. Deep neural networks, such as CNNs and ViT, have shown substantial performance improvements for this task while encountering the dilemma between global receptive fields and efficient computation. To this end, this paper pioneers exploring Mamba, a new paradigm for long-range dependency modeling with linear complexity, for efficient and effective MRI reconstruction. However, directly applying Mamba to MRI reconstruction faces three significant issues: (1) Mambaâ€™s row-wise and column-wise scanning disrupts k-spaceâ€™s unique spectrum, leaving its potential in k-space learning unexplored. (2) Existing Mamba methods unfold feature maps with multiple lengthy scanning paths, leading to long-range forgetting and high computational burden. (3) Mamba struggles with spatially-varying contents, resulting in limited diversity of local representations. To address these, we propose a dual-domain multi-scale Mamba for MRI reconstruction from the following perspectives: (1) We pioneer vision Mamba in k-space learning. A circular scanning is customized for spectrum unfolding, benefiting the global modeling of k-space. (2) We propose a multi-scale Mamba with an efficient scanning strategy in both image and k-space domains. It mitigates long-range forgetting and achieves a better trade-off between efficiency and performance. (3) We develop a local diversity enhancement module to improve the spatially-varying representation of Mamba. Extensive experiments are conducted on three public datasets for MRI reconstruction under various undersampling patterns. Comprehensive results demonstrate that our method significantly outperforms state-of-the-art methods with lower computational cost. Implementation code will be available at <a target="_blank" rel="noopener" href="https://github.com/XiaoMengLiLiLi/DM-Mamba">https://github.com/XiaoMengLiLiLi/DM-Mamba</a>. </p>
<blockquote>
<p>åŠ é€ŸMRIé‡å»ºæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸é€‚å®šåé—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºkç©ºé—´ä¸­çš„æ˜¾è‘—æ¬ é‡‡æ ·é€ æˆçš„ã€‚æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’ŒVision Transformerï¼Œè™½ç„¶åœ¨æ­¤ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä½†åœ¨å…¨å±€æ„Ÿå—é‡å’Œé«˜æ•ˆè®¡ç®—ä¹‹é—´é‡åˆ°äº†å›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç‡å…ˆæ¢ç´¢Mambaè¿™ä¸€å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„é•¿è·ç¦»ä¾èµ–å»ºæ¨¡æ–°èŒƒå¼ï¼Œä»¥å®ç°é«˜æ•ˆä¸”æœ‰æ•ˆçš„MRIé‡å»ºã€‚ç„¶è€Œï¼Œç›´æ¥å°†Mambaåº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šï¼ˆ1ï¼‰Mambaçš„è¡Œå’Œåˆ—æ‰«æç ´åäº†kç©ºé—´çš„ç‹¬ç‰¹é¢‘è°±ï¼Œä½¿å¾—å…¶åœ¨kç©ºé—´å­¦ä¹ ä¸­çš„æ½œåŠ›å°šæœªè¢«æ¢ç´¢ã€‚ï¼ˆ2ï¼‰ç°æœ‰çš„Mambaæ–¹æ³•åœ¨å±•å¼€ç‰¹å¾æ˜ å°„æ—¶é‡‡ç”¨äº†å¤šæ¡å†—é•¿çš„æ‰«æè·¯å¾„ï¼Œå¯¼è‡´é•¿è·ç¦»é—å¿˜å’Œé«˜è®¡ç®—è´Ÿæ‹…ã€‚ï¼ˆ3ï¼‰Mambaåœ¨ç©ºé—´å˜åŒ–å†…å®¹ä¸Šé‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å±€éƒ¨è¡¨ç¤ºå¤šæ ·æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»ä»¥ä¸‹è§’åº¦æå‡ºäº†ç”¨äºMRIé‡å»ºçš„åŒåŸŸå¤šå°ºåº¦Mambaï¼šï¼ˆ1ï¼‰æˆ‘ä»¬ç‡å…ˆåœ¨kç©ºé—´å­¦ä¹ ä¸­å¼•å…¥è§†è§‰Mambaã€‚ä¸ºé¢‘è°±å±•å¼€å®šåˆ¶äº†åœ†å½¢æ‰«æï¼Œæœ‰åˆ©äºkç©ºé—´çš„å…¨å±€å»ºæ¨¡ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåœ¨å›¾åƒå’Œkç©ºé—´åŸŸä¸­å…·æœ‰é«˜æ•ˆæ‰«æç­–ç•¥çš„å¤šå°ºåº¦Mambaã€‚å®ƒå‡è½»äº†é•¿è·ç¦»é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—äº†æ›´å¥½çš„å¹³è¡¡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜Mambaçš„ç©ºé—´å˜åŒ–è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„MRIé‡å»ºå®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾ƒä½çš„è®¡ç®—æˆæœ¬ä¸‹æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ç›¸å…³å®ç°ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaoMengLiLiLi/DM-Mamba%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/XiaoMengLiLiLi/DM-Mambaä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08163v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ­¤æ–‡æœ¬ä»‹ç»äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é‡å»ºçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œä»¥åŠé’ˆå¯¹è¯¥é—®é¢˜é‡‡ç”¨çš„æ–°å‹æ¨¡å‹Mambaçš„ä¼˜ç¼ºç‚¹ã€‚ä¸ºæ”¹è¿›Mambaåœ¨MRIé‡å»ºä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†åŒåŸŸå¤šå°ºåº¦Mambaæ–¹æ³•ï¼Œåœ¨å›¾åƒå’Œkç©ºé—´åŸŸè¿›è¡Œé«˜æ•ˆæ‰«æï¼Œå¢å¼ºå±€éƒ¨å¤šæ ·æ€§ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰é‡å»ºé¢ä¸´å› kç©ºé—´ä¸­æ˜¾è‘—æ¬ é‡‡æ ·å¯¼è‡´çš„æŒ‘æˆ˜æ€§é€†é—®é¢˜ã€‚</li>
<li>Mambaæ¨¡å‹ä½œä¸ºé•¿è·ç¦»ä¾èµ–å»ºæ¨¡çš„æ–°èŒƒå¼ï¼Œå…·æœ‰é«˜æ•ˆå’Œæœ‰æ•ˆçš„MRIé‡å»ºæ½œåŠ›ã€‚</li>
<li>ç›´æ¥åº”ç”¨MambaäºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šå¯¹kç©ºé—´ç‹¬ç‰¹è°±çš„å¹²æ‰°ã€é•¿è·ç¦»é—å¿˜å’Œè®¡ç®—è´Ÿæ‹…é«˜ã€ä»¥åŠå¤„ç†ç©ºé—´å˜åŒ–å†…å®¹æ—¶çš„å±€é™æ€§ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†åŒåŸŸå¤šå°ºåº¦Mambaæ–¹æ³•ï¼ŒåŒ…æ‹¬åœ¨kç©ºé—´è¿›è¡Œè§†è§‰Mambaçš„å¼€åˆ›æ€§ç ”ç©¶ã€é‡‡ç”¨å¤šå°ºåº¦Mambaè¿›è¡Œå›¾åƒå’Œkç©ºé—´åŸŸçš„é«˜æ•ˆæ‰«æç­–ç•¥ä»¥åŠå¼€å‘å±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea974741d9a7e7365b087c10c9c2db9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c5c5129f8252500cb84034caa4b80b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f715628ebc166486384bddf33358cdd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad31e645e9120c6de378a1a2dcc693cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc981cd60c32908fe03dc12ff391c5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="An-Intra-and-Cross-frame-Topological-Consistency-Scheme-for-Semi-supervised-Atherosclerotic-Coronary-Plaque-Segmentation"><a href="#An-Intra-and-Cross-frame-Topological-Consistency-Scheme-for-Semi-supervised-Atherosclerotic-Coronary-Plaque-Segmentation" class="headerlink" title="An Intra- and Cross-frame Topological Consistency Scheme for   Semi-supervised Atherosclerotic Coronary Plaque Segmentation"></a>An Intra- and Cross-frame Topological Consistency Scheme for   Semi-supervised Atherosclerotic Coronary Plaque Segmentation</h2><p><strong>Authors:Ziheng Zhang, Zihan Li, Dandan Shan, Yuehui Qiu, Qingqi Hong, Qingqiang Wu</strong></p>
<p>Enhancing the precision of segmenting coronary atherosclerotic plaques from CT Angiography (CTA) images is pivotal for advanced Coronary Atherosclerosis Analysis (CAA), which distinctively relies on the analysis of vessel cross-section images reconstructed via Curved Planar Reformation. This task presents significant challenges due to the indistinct boundaries and structures of plaques and blood vessels, leading to the inadequate performance of current deep learning models, compounded by the inherent difficulty in annotating such complex data. To address these issues, we propose a novel dual-consistency semi-supervised framework that integrates Intra-frame Topological Consistency (ITC) and Cross-frame Topological Consistency (CTC) to leverage labeled and unlabeled data. ITC employs a dual-task network for simultaneous segmentation mask and Skeleton-aware Distance Transform (SDT) prediction, achieving similar prediction of topology structure through consistency constraint without additional annotations. Meanwhile, CTC utilizes an unsupervised estimator for analyzing pixel flow between skeletons and boundaries of adjacent frames, ensuring spatial continuity. Experiments on two CTA datasets show that our method surpasses existing semi-supervised methods and approaches the performance of supervised methods on CAA. In addition, our method also performs better than other methods on the ACDC dataset, demonstrating its generalization. </p>
<blockquote>
<p>æé«˜å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–æ–‘å—åœ¨CTè¡€ç®¡é€ å½±ï¼ˆCTAï¼‰å›¾åƒä¸­çš„åˆ†å‰²ç²¾åº¦å¯¹äºå…ˆè¿›çš„å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–åˆ†æï¼ˆCAAï¼‰è‡³å…³é‡è¦ã€‚CAAä¸»è¦ä¾èµ–äºé€šè¿‡æ›²é¢å¹³é¢é‡æ„æŠ€æœ¯é‡å»ºçš„è¡€ç®¡æ¨ªæˆªé¢å›¾åƒçš„åˆ†æã€‚è¿™ä¸€ä»»åŠ¡é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºæ–‘å—çš„è¾¹ç•Œå’Œç»“æ„æ¨¡ç³Šä¸æ¸…ï¼Œå¯¼è‡´å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ä¸è¶³ï¼ŒåŠ ä¸Šæ ‡æ³¨æ­¤ç±»å¤æ‚æ•°æ®çš„å›ºæœ‰å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒä¸€è‡´æ€§åŠç›‘ç£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¸§å†…æ‹“æ‰‘ä¸€è‡´æ€§ï¼ˆITCï¼‰å’Œè·¨å¸§æ‹“æ‰‘ä¸€è‡´æ€§ï¼ˆCTCï¼‰ï¼Œä»¥åˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„æ•°æ®ã€‚ITCé‡‡ç”¨åŒä»»åŠ¡ç½‘ç»œåŒæ—¶è¿›è¡Œåˆ†å‰²æ©è†œå’Œéª¨æ¶æ„ŸçŸ¥è·ç¦»å˜æ¢ï¼ˆSDTï¼‰é¢„æµ‹ï¼Œé€šè¿‡ä¸€è‡´æ€§çº¦æŸå®ç°ç±»ä¼¼çš„æ‹“æ‰‘ç»“æ„é¢„æµ‹ï¼Œæ— éœ€é¢å¤–çš„æ ‡æ³¨ã€‚åŒæ—¶ï¼ŒCTCåˆ©ç”¨æ— ç›‘ç£ä¼°è®¡å™¨åˆ†æç›¸é‚»å¸§ä¹‹é—´éª¨æ¶å’Œè¾¹ç•Œçš„åƒç´ æµï¼Œç¡®ä¿ç©ºé—´è¿ç»­æ€§ã€‚åœ¨ä¸¤ä¸ªCTAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†ç°æœ‰çš„åŠç›‘ç£æ–¹æ³•ï¼Œå¹¶æ¥è¿‘äº†CAAæ–¹é¢çš„ç›‘ç£æ–¹æ³•æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ACDCæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07850v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–åˆ†æï¼ˆCAAï¼‰ä¸­ä»CTè¡€ç®¡é€ å½±ï¼ˆCTAï¼‰å›¾åƒç²¾ç¡®åˆ†å‰²å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–æ–‘å—çš„é‡è¦æ€§ã€‚é’ˆå¯¹å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ ‡æ³¨å¤æ‚æ•°æ®æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŒä¸€è‡´æ€§åŠç›‘ç£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¸§å†…æ‹“æ‰‘ä¸€è‡´æ€§ï¼ˆITCï¼‰å’Œè·¨å¸§æ‹“æ‰‘ä¸€è‡´æ€§ï¼ˆCTCï¼‰ï¼Œä»¥åˆ©ç”¨æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾çš„æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CTAæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„åŠç›‘ç£æ–¹æ³•ï¼Œå¹¶æ¥è¿‘æœ‰ç›‘ç£æ–¹æ³•åœ¨CAAä¸Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ACDCæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç²¾ç¡®åˆ†å‰²å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–æ–‘å—å¯¹å† çŠ¶åŠ¨è„‰ç²¥æ ·ç¡¬åŒ–åˆ†æï¼ˆCAAï¼‰è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ ‡æ³¨å¤æ‚æ•°æ®æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŒä¸€è‡´æ€§åŠç›‘ç£æ¡†æ¶ï¼Œç»“åˆäº†ITCå’ŒCTCä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>ITCé€šè¿‡åŒä»»åŠ¡ç½‘ç»œè¿›è¡ŒåŒæ—¶åˆ†å‰²æ©è†œå’Œéª¨æ¶æ„ŸçŸ¥è·ç¦»å˜æ¢é¢„æµ‹ï¼Œå®ç°æ‹“æ‰‘ç»“æ„çš„ç›¸ä¼¼æ€§é¢„æµ‹ã€‚</li>
<li>CTCåˆ©ç”¨æ— ç›‘ç£ä¼°è®¡å™¨åˆ†æç›¸é‚»å¸§ä¹‹é—´åƒç´ æµä¸éª¨æ¶å’Œè¾¹ç•Œçš„å…³ç³»ï¼Œç¡®ä¿ç©ºé—´è¿ç»­æ€§ã€‚</li>
<li>åœ¨CTAæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰åŠç›‘ç£æ–¹æ³•ï¼Œå¹¶æ¥è¿‘æœ‰ç›‘ç£æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8139a2e70ce6e1d7014832ab4c97efc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cf199d9e5c5b9584bd1b44ef22f1110.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e062bb2b7832b22b2ab8c636d159fd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86106f0e5cb7fb6cc7b9e72bb3c219b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac34ee1beed41d7ab8bfda7ef61ca0b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ef286cec699864e0cfef26305a5cff4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50c23c5e72bde4b4448e2375044b1589.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd76645aa8ac078cecb860c5ae7bf68c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d05f91dd200d373cbc3f03892e00e4a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Low-cost-and-Ultra-lightweight-Binary-Neural-Network-for-Traffic-Signal-Recognition"><a href="#A-Low-cost-and-Ultra-lightweight-Binary-Neural-Network-for-Traffic-Signal-Recognition" class="headerlink" title="A Low-cost and Ultra-lightweight Binary Neural Network for Traffic   Signal Recognition"></a>A Low-cost and Ultra-lightweight Binary Neural Network for Traffic   Signal Recognition</h2><p><strong>Authors:Mingke Xiao, Yue Su, Liang Yu, Guanglong Qu, Yutong Jia, Yukuan Chang, Xu Zhang</strong></p>
<p>The deployment of neural networks in vehicle platforms and wearable Artificial Intelligence-of-Things (AIOT) scenarios has become a research area that has attracted much attention. With the continuous evolution of deep learning technology, many image classification models are committed to improving recognition accuracy, but this is often accompanied by problems such as large model resource usage, complex structure, and high power consumption, which makes it challenging to deploy on resource-constrained platforms. Herein, we propose an ultra-lightweight binary neural network (BNN) model designed for hardware deployment, and conduct image classification research based on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also verify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS) datasets. The proposed model shows excellent recognition performance with an accuracy of up to 97.64%, making it one of the best performing BNN models in the GTSRB dataset. Compared with the full-precision model, the accuracy loss is controlled within 1%, and the parameter storage overhead of the model is only 10% of that of the full-precision model. More importantly, our network model only relies on logical operations and low-bit width fixed-point addition and subtraction operations during the inference phase, which greatly simplifies the design complexity of the processing element (PE). Our research shows the great potential of BNN in the hardware deployment of computer vision models, especially in the field of computer vision tasks related to autonomous driving. </p>
<blockquote>
<p>ç¥ç»ç½‘ç»œåœ¨è½¦è¾†å¹³å°å’Œå¯ç©¿æˆ´äººå·¥æ™ºèƒ½ç‰©è”ç½‘ï¼ˆAIOTï¼‰åœºæ™¯çš„åº”ç”¨å·²æˆä¸ºä¸€ä¸ªå¤‡å—å…³æ³¨çš„ç ”ç©¶é¢†åŸŸã€‚éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œè®¸å¤šå›¾åƒåˆ†ç±»æ¨¡å‹è‡´åŠ›äºæé«˜è¯†åˆ«ç²¾åº¦ï¼Œä½†è¿™å¸¸å¸¸ä¼´éšç€æ¨¡å‹èµ„æºä½¿ç”¨é‡å¤§ã€ç»“æ„å¤æ‚ã€åŠŸè€—é«˜ç­‰é—®é¢˜ï¼Œä½¿å¾—åœ¨èµ„æºå—é™çš„å¹³å°ä¸Šçš„éƒ¨ç½²å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸“ä¸ºç¡¬ä»¶éƒ¨ç½²è®¾è®¡çš„è¶…è½»é‡çº§äºŒè¿›åˆ¶ç¥ç»ç½‘ç»œï¼ˆBNNï¼‰æ¨¡å‹ï¼Œå¹¶åŸºäºå¾·å›½äº¤é€šæ ‡å¿—è¯†åˆ«åŸºå‡†æµ‹è¯•ï¼ˆGTSRBï¼‰æ•°æ®é›†è¿›è¡Œå›¾åƒåˆ†ç±»ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ä¸­å›½äº¤é€šæ ‡å¿—ï¼ˆCTSï¼‰å’Œæ¯”åˆ©æ—¶äº¤é€šæ ‡å¿—ï¼ˆBTSï¼‰æ•°æ®é›†è¿›è¡Œäº†éªŒè¯ã€‚è¯¥æ¨¡å‹è¡¨ç°å‡ºä¼˜å¼‚çš„è¯†åˆ«æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜è¾¾97.64%ï¼Œæˆä¸ºGTSRBæ•°æ®é›†ä¸­æ€§èƒ½æœ€ä½³çš„BNNæ¨¡å‹ä¹‹ä¸€ã€‚ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸æ¯”ï¼Œç²¾åº¦æŸå¤±æ§åˆ¶åœ¨1%ä»¥å†…ï¼Œæ¨¡å‹å‚æ•°å­˜å‚¨å¼€é”€ä»…ä¸ºå…¨ç²¾åº¦æ¨¡å‹çš„10%ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç½‘ç»œæ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä»…ä¾èµ–é€»è¾‘æ“ä½œå’Œä½ä½å®½å›ºå®šç‚¹åŠ å‡è¿ç®—ï¼Œè¿™æå¤§åœ°ç®€åŒ–äº†å¤„ç†å…ƒä»¶ï¼ˆPEï¼‰çš„è®¾è®¡å¤æ‚åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºäº†BNNåœ¨è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„ç¡¬ä»¶éƒ¨ç½²ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸è‡ªåŠ¨é©¾é©¶ç›¸å…³çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07808v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è½¦è¾†å¹³å°å’Œå¯ç©¿æˆ´äººå·¥æ™ºèƒ½ç‰©è”ç½‘åœºæ™¯ï¼Œæå‡ºä¸€ç§è¶…è½»é‡çº§äºŒè¿›åˆ¶ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œå›¾åƒåˆ†ç±»ç ”ç©¶ï¼ŒåŸºäºå¾·å›½äº¤é€šæ ‡å¿—è¯†åˆ«åŸºå‡†æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œé«˜è¾¾97.64%çš„è¯†åˆ«å‡†ç¡®ç‡ã€‚ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°å­˜å‚¨å¼€é”€ä»…å å…¶ååˆ†ä¹‹ä¸€ï¼Œæ¨ç†é˜¶æ®µä»…ä¾èµ–é€»è¾‘æ“ä½œå’Œä½ä½å®šç‚¹åŠ å‡è¿ç®—ï¼Œå¤§å¹…ç®€åŒ–å¤„ç†å…ƒä»¶è®¾è®¡å¤æ‚åº¦ï¼Œå±•ç¤ºäºŒè¿›åˆ¶ç¥ç»ç½‘ç»œåœ¨è‡ªåŠ¨é©¾é©¶ç›¸å…³è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç¡¬ä»¶éƒ¨ç½²ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œåœ¨è½¦è¾†å¹³å°å’Œå¯ç©¿æˆ´AIoTåœºæ™¯ä¸­çš„åº”ç”¨å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨å›¾åƒåˆ†ç±»æ¨¡å‹ä¸­çš„ä¸æ–­è¿›æ­¥åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ¨¡å‹èµ„æºæ¶ˆè€—å¤§ã€ç»“æ„å¤æ‚ã€åŠŸè€—é«˜ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§è¶…è½»é‡çº§äºŒè¿›åˆ¶ç¥ç»ç½‘ç»œï¼ˆBNNï¼‰æ¨¡å‹ï¼Œé€‚ç”¨äºç¡¬ä»¶éƒ¨ç½²ã€‚</li>
<li>åœ¨å¾·å›½äº¤é€šæ ‡å¿—è¯†åˆ«åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯ï¼Œè¯†åˆ«å‡†ç¡®ç‡é«˜è¾¾97.64%ï¼Œæ€§èƒ½å“è¶Šã€‚</li>
<li>ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸æ¯”ï¼Œå‚æ•°å­˜å‚¨å¼€é”€å¤§å¹…é™ä½ï¼Œä»…ä¸ºå…¶ååˆ†ä¹‹ä¸€ã€‚</li>
<li>æ¨ç†é˜¶æ®µä»…ä¾èµ–é€»è¾‘æ“ä½œå’Œä½ä½å®šç‚¹åŠ å‡è¿ç®—ï¼Œç®€åŒ–å¤„ç†å…ƒä»¶è®¾è®¡å¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4de151c7d4e3c0ce9d48412fc99cd206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239dad9d946df14a420e06267d957af2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858d8dffd573fe3c15732d765082903b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-163983adb5be84bda63983958b21fe1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b65a0c2dfcd0ddad54c5808df5912969.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-716cfbc7e9d91e7299f2bb1cc15870c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6493cb1b2aaad7caeaa942fb77180f4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Confident-Pseudo-labeled-Diffusion-Augmentation-for-Canine-Cardiomegaly-Detection"><a href="#Confident-Pseudo-labeled-Diffusion-Augmentation-for-Canine-Cardiomegaly-Detection" class="headerlink" title="Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly   Detection"></a>Confident Pseudo-labeled Diffusion Augmentation for Canine Cardiomegaly   Detection</h2><p><strong>Authors:Shiman Zhang, Lakshmikar Reddy Polamreddy, Youshan Zhang</strong></p>
<p>Canine cardiomegaly, marked by an enlarged heart, poses serious health risks if undetected, requiring accurate diagnostic methods. Current detection models often rely on small, poorly annotated datasets and struggle to generalize across diverse imaging conditions, limiting their real-world applicability. To address these issues, we propose a Confident Pseudo-labeled Diffusion Augmentation (CDA) model for identifying canine cardiomegaly. Our approach addresses the challenge of limited high-quality training data by employing diffusion models to generate synthetic X-ray images and annotate Vertebral Heart Score key points, thereby expanding the dataset. We also employ a pseudo-labeling strategy with Monte Carlo Dropout to select high-confidence labels, refine the synthetic dataset, and improve accuracy. Iteratively incorporating these labels enhances the modelâ€™s performance, overcoming the limitations of existing approaches. Experimental results show that the CDA model outperforms traditional methods, achieving state-of-the-art accuracy in canine cardiomegaly detection. The code implementation is available at <a target="_blank" rel="noopener" href="https://github.com/Shira7z/CDA">https://github.com/Shira7z/CDA</a>. </p>
<blockquote>
<p>çŠ¬ç±»å¿ƒè„è‚¥å¤§è¡¨ç°ä¸ºå¿ƒè„å¢å¤§ï¼Œå¦‚æœæœªè¢«æ£€æµ‹å‡ºæ¥ï¼Œä¼šå¸¦æ¥ä¸¥é‡çš„å¥åº·é£é™©ï¼Œå› æ­¤éœ€è¦å‡†ç¡®çš„è¯Šæ–­æ–¹æ³•ã€‚å½“å‰çš„æ£€æµ‹æ¨¡å‹é€šå¸¸ä¾èµ–äºå°ä¸”æ ‡æ³¨ä¸ä½³çš„æ•°æ®é›†ï¼Œéš¾ä»¥åœ¨ä¸åŒæˆåƒæ¡ä»¶ä¸‹è¿›è¡Œæ¨å¹¿ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè¯†åˆ«çŠ¬ç±»å¿ƒè„è‚¥å¤§çš„è‡ªä¿¡ä¼ªæ ‡è®°æ‰©æ•£å¢å¼ºï¼ˆCDAï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡é‡‡ç”¨æ‰©æ•£æ¨¡å‹ç”ŸæˆåˆæˆXå°„çº¿å›¾åƒå¹¶æ ‡æ³¨æ¤ä½“å¿ƒè„è¯„åˆ†å…³é”®ç‚¹ï¼Œä»è€Œè§£å†³é«˜è´¨é‡è®­ç»ƒæ•°æ®æœ‰é™çš„é—®é¢˜ï¼Œä»¥æ­¤æ‰©å¤§æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§å¸¦æœ‰è’™ç‰¹å¡æ´›Dropoutçš„ä¼ªæ ‡è®°ç­–ç•¥ï¼Œé€‰æ‹©é«˜ç½®ä¿¡åº¦æ ‡ç­¾ï¼Œå¯¹åˆæˆæ•°æ®é›†è¿›è¡Œç²¾ç‚¼ï¼Œæé«˜äº†å‡†ç¡®æ€§ã€‚è¿­ä»£åœ°èå…¥è¿™äº›æ ‡ç­¾æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDAæ¨¡å‹ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåœ¨çŠ¬ç±»å¿ƒè„è‚¥å¤§æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ã€‚ä»£ç å®ç°å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Shira7z/CDA%E3%80%82">https://github.com/Shira7z/CDAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07533v1">PDF</a> WACV workshop</p>
<p><strong>Summary</strong></p>
<p>å½“å‰çŠ¬ç±»å¿ƒè„ç—…æ£€æµ‹æ¨¡å‹å—é™äºå°å‹ä¸”æ ‡æ³¨ä¸å…¨çš„æ•°æ®é›†ï¼Œéš¾ä»¥é€‚åº”ä¸åŒæˆåƒæ¡ä»¶ï¼Œå®é™…åº”ç”¨å—é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºConfident Pseudo-labeled Diffusion Augmentationï¼ˆCDAï¼‰çš„æ£€æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡æ‰©æ•£æ¨¡å‹ç”ŸæˆåˆæˆXå°„çº¿å›¾åƒå¹¶æ ‡æ³¨å¿ƒè„åˆ†æ•°å…³é”®ç‚¹ï¼Œä»¥æ‰©å……æ•°æ®é›†å¹¶æå‡å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCDAæ¨¡å‹è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¾¾åˆ°çŠ¬ç±»å¿ƒè„ç—…æ£€æµ‹çš„æœ€å…ˆè¿›æ°´å¹³ã€‚ä»£ç å®ç°å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Shira7z/CDA">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>çŠ¬å¿ƒè„ç—…è‹¥æœªæ£€æµ‹å¯èƒ½å¸¦æ¥å¥åº·é£é™©ã€‚</li>
<li>å½“å‰æ£€æµ‹æ¨¡å‹å—é™äºå°å‹æ•°æ®é›†å’Œéš¾ä»¥é€‚åº”ä¸åŒæˆåƒæ¡ä»¶çš„é—®é¢˜ã€‚</li>
<li>CDAæ¨¡å‹é€šè¿‡æ‰©æ•£æ¨¡å‹ç”ŸæˆåˆæˆXå°„çº¿å›¾åƒå¹¶æ ‡æ³¨å¿ƒè„åˆ†æ•°å…³é”®ç‚¹ä»¥æ‰©å……æ•°æ®é›†ã€‚</li>
<li>CDAæ¨¡å‹é‡‡ç”¨ä¼ªæ ‡ç­¾ç­–ç•¥ä¸Monte Carlo DropoutæŠ€æœ¯æå‡æ•°æ®è´¨é‡åŠæ¨¡å‹å‡†ç¡®æ€§ã€‚</li>
<li>CDAæ¨¡å‹æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œè¾¾åˆ°çŠ¬å¿ƒè„ç—…æ£€æµ‹çš„æœ€å…ˆè¿›æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4894fe2f2acffde46c53a2efb072b5b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c049ab3a72c9ad192f150729dbfd746d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fecbe5e32f583f99db206fb307d3576f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5472a7927b6f298ad255f0477f12b022.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RadAlign-Advancing-Radiology-Report-Generation-with-Vision-Language-Concept-Alignment"><a href="#RadAlign-Advancing-Radiology-Report-Generation-with-Vision-Language-Concept-Alignment" class="headerlink" title="RadAlign: Advancing Radiology Report Generation with Vision-Language   Concept Alignment"></a>RadAlign: Advancing Radiology Report Generation with Vision-Language   Concept Alignment</h2><p><strong>Authors:Difei Gu, Yunhe Gao, Yang Zhou, Mu Zhou, Dimitris Metaxas</strong></p>
<p>Automated chest radiographs interpretation requires both accurate disease classification and detailed radiology report generation, presenting a significant challenge in the clinical workflow. Current approaches either focus on classification accuracy at the expense of interpretability or generate detailed but potentially unreliable reports through image captioning techniques. In this study, we present RadAlign, a novel framework that combines the predictive accuracy of vision-language models (VLMs) with the reasoning capabilities of large language models (LLMs). Inspired by the radiologistâ€™s workflow, RadAlign first employs a specialized VLM to align visual features with key medical concepts, achieving superior disease classification with an average AUC of 0.885 across multiple diseases. These recognized medical conditions, represented as text-based concepts in the aligned visual-language space, are then used to prompt LLM-based report generation. Enhanced by a retrieval-augmented generation mechanism that grounds outputs in similar historical cases, RadAlign delivers superior report quality with a GREEN score of 0.678, outperforming state-of-the-art methodsâ€™ 0.634. Our framework maintains strong clinical interpretability while reducing hallucinations, advancing automated medical imaging and report analysis through integrated predictive and generative AI. Code is available at <a target="_blank" rel="noopener" href="https://github.com/difeigu/RadAlign">https://github.com/difeigu/RadAlign</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–èƒ¸éƒ¨æ”¾å°„å½±åƒè§£è¯»æ—¢éœ€è¦ç²¾ç¡®çš„ç–¾ç—…åˆ†ç±»ï¼Œä¹Ÿéœ€è¦ç”Ÿæˆè¯¦ç»†çš„æ”¾å°„å­¦æŠ¥å‘Šï¼Œè¿™åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•è¦ä¹ˆä¾§é‡äºåˆ†ç±»å‡†ç¡®æ€§è€Œç‰ºç‰²äº†å¯è§£é‡Šæ€§ï¼Œè¦ä¹ˆé€šè¿‡å›¾åƒæè¿°æŠ€æœ¯ç”Ÿæˆè¯¦ç»†ä½†å¯èƒ½ä¸å¯é çš„æŠ¥å‘Šã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RadAlignï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é¢„æµ‹å‡†ç¡®æ€§ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚RadAlignå—æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œæµç¨‹çš„å¯å‘ï¼Œé¦–å…ˆé‡‡ç”¨ä¸“ç”¨çš„VLMå¯¹é½è§†è§‰ç‰¹å¾ä¸å…³é”®åŒ»å­¦æ¦‚å¿µï¼Œå®ç°å¯¹å¤šç§ç–¾ç—…çš„å“è¶Šç–¾ç—…åˆ†ç±»ï¼Œå¹³å‡AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰ä¸º0.885ã€‚è¿™äº›è¢«è¯†åˆ«çš„åŒ»å­¦çŠ¶å†µï¼Œä»¥å¯¹é½çš„è§†è§‰è¯­è¨€ç©ºé—´ä¸­çš„åŸºäºæ–‡æœ¬çš„æ¦‚å¿µè¡¨ç¤ºï¼Œç„¶åç”¨äºæç¤ºåŸºäºLLMçš„æŠ¥å‘Šç”Ÿæˆã€‚é€šè¿‡å¢å¼ºä»¥ç›¸ä¼¼å†å²ç—…ä¾‹ä¸ºåŸºç¡€çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæœºåˆ¶ï¼ŒRadAlignåœ¨æŠ¥å‘Šè´¨é‡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒGREENå¾—åˆ†ä¸º0.678ï¼Œè¶…è¿‡äº†æœ€æ–°æ–¹æ³•ï¼ˆ0.634ï¼‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¿æŒå¼ºå¤§çš„ä¸´åºŠå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†è™šæ„æƒ…å†µï¼Œé€šè¿‡é›†æˆé¢„æµ‹å’Œç”Ÿæˆäººå·¥æ™ºèƒ½ï¼Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ»ç–—æˆåƒå’ŒæŠ¥å‘Šåˆ†æçš„å‘å±•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/difeigu/RadAlign">é“¾æ¥</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07525v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRadAlignçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é¢„æµ‹ç²¾åº¦å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œç”¨äºè‡ªåŠ¨è§£è¯»èƒ¸éƒ¨Xå…‰ç‰‡ã€‚RadAlignä¸ä»…èƒ½å‡†ç¡®åˆ†ç±»ç–¾ç—…ï¼Œè¿˜èƒ½ç”Ÿæˆè¯¦ç»†çš„æ”¾å°„å­¦æŠ¥å‘Šã€‚é€šè¿‡ä¸“é—¨çš„VLMå¯¹é½è§†è§‰ç‰¹å¾ä¸å…³é”®åŒ»å­¦æ¦‚å¿µï¼Œå®ç°å¤šç–¾ç—…å¹³å‡AUCå€¼ä¸º0.885çš„ä¼˜è¶Šç–¾ç—…åˆ†ç±»ã€‚ç„¶åï¼Œä½¿ç”¨LLMç”ŸæˆæŠ¥å‘Šï¼Œè¾“å‡ºåŸºäºç±»ä¼¼å†å²ç—…ä¾‹çš„è¾“å‡ºï¼Œæé«˜æŠ¥å‘Šè´¨é‡ã€‚RadAlignæ¡†æ¶æ—¢å…·æœ‰å¼ºå¤§çš„ä¸´åºŠå¯è§£é‡Šæ€§ï¼Œåˆå‡å°‘äº†è™šæ„ç°è±¡ï¼Œé€šè¿‡é¢„æµ‹å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é›†æˆï¼Œæ¨åŠ¨äº†è‡ªåŠ¨åŒ»å­¦æˆåƒå’ŒæŠ¥å‘Šåˆ†æçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RadAlignæ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¼˜åŠ¿ï¼Œç”¨äºè‡ªåŠ¨è§£è¯»èƒ¸éƒ¨Xå…‰ç‰‡ã€‚</li>
<li>RadAlignå®ç°é«˜å‡†ç¡®æ€§çš„ç–¾ç—…åˆ†ç±»ï¼Œå¹³å‡AUCå€¼ä¸º0.885ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆè¯¦ç»†çš„æ”¾å°„å­¦æŠ¥å‘Šï¼ŒæŠ¥å‘Šè´¨é‡é«˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>RadAligné€šè¿‡ä¸“é—¨è®¾è®¡çš„VLMå¯¹é½è§†è§‰ç‰¹å¾ä¸åŒ»å­¦æ¦‚å¿µï¼Œæé«˜äº†ç–¾ç—…åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>LLMç”¨äºç”ŸæˆæŠ¥å‘Šï¼Œå¯ä»¥è¾“å‡ºåŸºäºç±»ä¼¼å†å²ç—…ä¾‹çš„ä¿¡æ¯ï¼Œè¿›ä¸€æ­¥æé«˜æŠ¥å‘Šè´¨é‡ã€‚</li>
<li>RadAlignæ¡†æ¶å…·æœ‰å¼ºå¤§çš„ä¸´åºŠå¯è§£é‡Šæ€§ï¼Œå‡å°‘äº†è™šæ„ç°è±¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79cdb2e29f67be7f9053cccffad845bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88fb458a1f5422703428c671fab8fe78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8336ebe916a752e1f327a43c952e841e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Diff-Ensembler-Learning-to-Ensemble-2D-Diffusion-Models-for-Volume-to-Volume-Medical-Image-Translation"><a href="#Diff-Ensembler-Learning-to-Ensemble-2D-Diffusion-Models-for-Volume-to-Volume-Medical-Image-Translation" class="headerlink" title="Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for   Volume-to-Volume Medical Image Translation"></a>Diff-Ensembler: Learning to Ensemble 2D Diffusion Models for   Volume-to-Volume Medical Image Translation</h2><p><strong>Authors:Xiyue Zhu, Dou Hoon Kwark, Ruike Zhu, Kaiwen Hong, Yiqi Tao, Shirui Luo, Yudu Li, Zhi-Pei Liang, Volodymyr Kindratenko</strong></p>
<p>Despite success in volume-to-volume translations in medical images, most existing models struggle to effectively capture the inherent volumetric distribution using 3D representations. The current state-of-the-art approach combines multiple 2D-based networks through weighted averaging, thereby neglecting the 3D spatial structures. Directly training 3D models in medical imaging presents significant challenges due to high computational demands and the need for large-scale datasets. To address these challenges, we introduce Diff-Ensembler, a novel hybrid 2D-3D model for efficient and effective volumetric translations by ensembling perpendicularly trained 2D diffusion models with a 3D network in each diffusion step. Moreover, our model can naturally be used to ensemble diffusion models conditioned on different modalities, allowing flexible and accurate fusion of input conditions. Extensive experiments demonstrate that Diff-Ensembler attains superior accuracy and volumetric realism in 3D medical image super-resolution and modality translation. We further demonstrate the strength of our modelâ€™s volumetric realism using tumor segmentation as a downstream task. </p>
<blockquote>
<p>å°½ç®¡åœ¨åŒ»å­¦å›¾åƒçš„ä½“ç§¯åˆ°ä½“ç§¯è½¬æ¢ä¸­å–å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ¨¡å‹åœ¨åˆ©ç”¨3Dè¡¨ç¤ºæ•æ‰å†…åœ¨çš„ä½“ç§¯åˆ†å¸ƒæ—¶ä»é¢ä¸´å›°éš¾ã€‚å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ˜¯é€šè¿‡åŠ æƒå¹³å‡ç»„åˆå¤šä¸ªåŸºäº2Dçš„ç½‘ç»œï¼Œä»è€Œå¿½ç•¥äº†3Dç©ºé—´ç»“æ„ã€‚ç›´æ¥åœ¨åŒ»å­¦æˆåƒä¸­è®­ç»ƒ3Dæ¨¡å‹é¢ä¸´ç€å·¨å¤§çš„è®¡ç®—éœ€æ±‚å’Œå¤§è§„æ¨¡æ•°æ®é›†çš„éœ€æ±‚ï¼Œè¿™å¸¦æ¥äº†å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Diff-Ensemblerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ··åˆ2D-3Dæ¨¡å‹ï¼Œå®ƒé€šè¿‡åœ¨æ¯ä¸ªæ‰©æ•£æ­¥éª¤ä¸­å°†å‚ç›´è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ä¸3Dç½‘ç»œç›¸ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆä¸”æœ‰æ•ˆçš„ä½“ç§¯è½¬æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥è‡ªç„¶åœ°ç”¨äºç»„åˆä¸åŒæ¨¡æ€çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°è¾“å…¥æ¡ä»¶çš„çµæ´»å’Œå‡†ç¡®èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDiff-Ensembleråœ¨3DåŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡å’Œæ¨¡æ€è½¬æ¢æ–¹é¢è¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œä½“ç§¯çœŸå®æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡è‚¿ç˜¤åˆ†å‰²ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡æ¥å±•ç¤ºæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä½“ç§¯çœŸå®æ€§æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07430v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ··åˆ2D-3Dæ¨¡å‹â€”â€”Diff-Ensemblerï¼Œæ—¨åœ¨é€šè¿‡é›†æˆå‚ç›´è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ä¸3Dç½‘ç»œï¼Œå®ç°é«˜æ•ˆä¸”æœ‰æ•ˆçš„ä½“ç§¯è½¬æ¢ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿèåˆä¸åŒæ¨¡æ€çš„æ‰©æ•£æ¨¡å‹ï¼Œå…·æœ‰çµæ´»ä¸”ç²¾ç¡®çš„è¾“å…¥æ¡ä»¶èåˆèƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒDiff-Ensembleråœ¨3DåŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡å’Œæ¨¡æ€è½¬æ¢æ–¹é¢è¾¾åˆ°ä¼˜è¶Šçš„å‡†ç¡®æ€§åŠä½“ç§¯ç°å®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä½“ç§¯è½¬æ¢æ—¶éš¾ä»¥æœ‰æ•ˆæ•æ‰å†…åœ¨ä½“ç§¯åˆ†å¸ƒï¼Œç¼ºä¹3Dè¡¨ç¤ºã€‚</li>
<li>å½“å‰æœ€æ–°æ–¹æ³•æ˜¯é€šè¿‡åŠ æƒå¹³å‡å¤šä¸ªåŸºäº2Dçš„ç½‘ç»œï¼Œå¿½ç•¥äº†3Dç©ºé—´ç»“æ„ã€‚</li>
<li>ç›´æ¥è®­ç»ƒ3DåŒ»å­¦æˆåƒæ¨¡å‹é¢ä¸´é«˜è®¡ç®—éœ€æ±‚å’Œå¤§è§„æ¨¡æ•°æ®é›†æŒ‘æˆ˜ã€‚</li>
<li>Diff-Ensembleræ˜¯ä¸€ç§æ–°å‹æ··åˆ2D-3Dæ¨¡å‹ï¼Œé€šè¿‡é›†æˆå‚ç›´è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹å’Œ3Dç½‘ç»œï¼Œå®ç°é«˜æ•ˆå’Œæœ‰æ•ˆçš„ä½“ç§¯è½¬æ¢ã€‚</li>
<li>Diff-Ensemblerèƒ½è‡ªç„¶èåˆä¸åŒæ¨¡æ€çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°çµæ´»ä¸”ç²¾ç¡®çš„è¾“å…¥æ¡ä»¶èåˆã€‚</li>
<li>å®éªŒè¯æ˜Diff-Ensembleråœ¨3DåŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡å’Œæ¨¡æ€è½¬æ¢æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§èƒ½å’Œä½“ç§¯ç°å®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07430">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d8e883edb0eed8c205b07d1443916b5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-badd674dae4f1fac96bbcc27dde31393.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c6d1c3d0ec7abcaf2f0ea1c61f31551.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FedSemiDG-Domain-Generalized-Federated-Semi-supervised-Medical-Image-Segmentation"><a href="#FedSemiDG-Domain-Generalized-Federated-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image   Segmentation"></a>FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image   Segmentation</h2><p><strong>Authors:Zhipeng Deng, Zhe Xu, Tsuyoshi Isshiki, Yefeng Zheng</strong></p>
<p>Medical image segmentation is challenging due to the diversity of medical images and the lack of labeled data, which motivates recent developments in federated semi-supervised learning (FSSL) to leverage a large amount of unlabeled data from multiple centers for model training without sharing raw data. However, what remains under-explored in FSSL is the domain shift problem which may cause suboptimal model aggregation and low effectivity of the utilization of unlabeled data, eventually leading to unsatisfactory performance in unseen domains. In this paper, we explore this previously ignored scenario, namely domain generalized federated semi-supervised learning (FedSemiDG), which aims to learn a model in a distributed manner from multiple domains with limited labeled data and abundant unlabeled data such that the model can generalize well to unseen domains. We present a novel framework, Federated Generalization-Aware SemiSupervised Learning (FGASL), to address the challenges in FedSemiDG by effectively tackling critical issues at both global and local levels. Globally, we introduce Generalization-Aware Aggregation (GAA), assigning adaptive weights to local models based on their generalization performance. Locally, we use a Dual-Teacher Adaptive Pseudo Label Refinement (DR) strategy to combine global and domain-specific knowledge, generating more reliable pseudo labels. Additionally, Perturbation-Invariant Alignment (PIA) enforces feature consistency under perturbations, promoting domain-invariant learning. Extensive experiments on three medical segmentation tasks (cardiac MRI, spine MRI and bladder cancer MRI) demonstrate that our method significantly outperforms state-of-the-art FSSL and domain generalization approaches, achieving robust generalization on unseen domains. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºåŒ»å­¦å›¾åƒçš„å¤šæ ·æ€§å’Œç¼ºä¹æ ‡è®°æ•°æ®ã€‚è¿™æ¨åŠ¨äº†è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFSSLï¼‰çš„è¿‘æœŸå‘å±•ï¼Œä»¥åˆ©ç”¨å¤šä¸ªä¸­å¿ƒçš„å¤§é‡æœªæ ‡è®°æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œè€Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨FSSLä¸­ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢çš„æ˜¯åŸŸåç§»é—®é¢˜ï¼Œå®ƒå¯èƒ½å¯¼è‡´æ¨¡å‹èšåˆä¸ä½³ä»¥åŠåˆ©ç”¨æœªæ ‡è®°æ•°æ®çš„æ•ˆæœä½ä¸‹ï¼Œæœ€ç»ˆå¯¼è‡´åœ¨æœªè§è¿‡çš„é¢†åŸŸä¸­çš„æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡æ¢ç´¢äº†ä»¥å‰è¢«å¿½ç•¥çš„åœºæ™¯ï¼Œå³é¢†åŸŸæ³›åŒ–è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFedSemiDGï¼‰ï¼Œå…¶ç›®æ ‡æ˜¯ä»å¤šä¸ªé¢†åŸŸä»¥åˆ†å¸ƒå¼æ–¹å¼å­¦ä¹ æ¨¡å‹ï¼Œåˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å’Œä¸°å¯Œçš„æœªæ ‡è®°æ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„é¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è”é‚¦æ³›åŒ–æ„ŸçŸ¥åŠç›‘ç£å­¦ä¹ ï¼ˆFGASLï¼‰ï¼Œä»¥åº”å¯¹FedSemiDGä¸­çš„æŒ‘æˆ˜ï¼Œåœ¨å…¨å±€å’Œå±€éƒ¨å±‚é¢æœ‰æ•ˆè§£å†³å…³é”®é—®é¢˜ã€‚åœ¨å…¨å±€å±‚é¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ³›åŒ–æ„ŸçŸ¥èšåˆï¼ˆGAAï¼‰ï¼Œæ ¹æ®å±€éƒ¨æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½åˆ†é…è‡ªé€‚åº”æƒé‡ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒæ•™å¸ˆè‡ªé€‚åº”ä¼ªæ ‡ç­¾ç»†åŒ–ï¼ˆDRï¼‰ç­–ç•¥æ¥ç»“åˆå…¨å±€å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œç”Ÿæˆæ›´å¯é çš„ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæ‰°åŠ¨ä¸å˜å¯¹é½ï¼ˆPIAï¼‰å¼ºåˆ¶æ‰§è¡Œæ‰°åŠ¨ä¸‹çš„ç‰¹å¾ä¸€è‡´æ€§ï¼Œä¿ƒè¿›é¢†åŸŸä¸å˜å­¦ä¹ ã€‚åœ¨å¿ƒè„MRIã€è„ŠæŸ±MRIå’Œè†€èƒ±ç™ŒMRIç­‰ä¸‰ä¸ªåŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€æ–°çš„FSSLå’Œé¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œåœ¨æœªè§è¿‡çš„é¢†åŸŸä¸Šå®ç°äº†ç¨³å¥çš„æ³›åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07378v1">PDF</a> 17 pages</p>
<p><strong>æ‘˜è¦</strong><br>    åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´å¤šæ ·æ€§å›¾åƒä¸ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æŒ‘æˆ˜ã€‚è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFSSLï¼‰çš„å‘å±•æ—¨åœ¨åˆ©ç”¨å¤šä¸ªä¸­å¿ƒçš„å¤§é‡æœªæ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä½†ä¸åˆ†äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼ŒFSSLä¸­å°šæœªæ¢ç´¢çš„é—®é¢˜æ˜¯é¢†åŸŸåç§»ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹èšåˆä¸ä½³ã€åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æ•ˆç‡ä½ï¼Œä»¥åŠåœ¨æœªè§é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¹‹å‰è¢«å¿½ç•¥çš„åœºæ™¯â€”â€”é¢†åŸŸå¹¿ä¹‰è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFedSemiDGï¼‰ï¼Œæ—¨åœ¨ä»å¤šä¸ªé¢†åŸŸä»¥åˆ†å¸ƒå¼æ–¹å¼å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨æœ‰é™çš„æ ‡æ³¨æ•°æ®å’Œä¸°å¯Œçš„æœªæ ‡æ³¨æ•°æ®ï¼Œä½¿æ¨¡å‹èƒ½åœ¨æœªè§é¢†åŸŸè¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”è”é‚¦æ³›åŒ–æ„ŸçŸ¥åŠç›‘ç£å­¦ä¹ ï¼ˆFGASLï¼‰ï¼Œé€šè¿‡å…¨çƒå’Œæœ¬åœ°å±‚é¢æœ‰æ•ˆåº”å¯¹FedSemiDGä¸­çš„æŒ‘æˆ˜ã€‚å…¨çƒå±‚é¢ï¼Œæˆ‘ä»¬å¼•å…¥æ³›åŒ–æ„ŸçŸ¥èšåˆï¼ˆGAAï¼‰ï¼Œæ ¹æ®æ³›åŒ–æ€§èƒ½ä¸ºæœ¬åœ°æ¨¡å‹åˆ†é…è‡ªé€‚åº”æƒé‡ã€‚æœ¬åœ°å±‚é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨åŒæ•™å¸ˆè‡ªé€‚åº”ä¼ªæ ‡ç­¾ç»†åŒ–ï¼ˆDRï¼‰ç­–ç•¥ï¼Œç»“åˆå…¨å±€å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œç”Ÿæˆæ›´å¯é çš„ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæ‰°åŠ¨ä¸å˜å¯¹é½ï¼ˆPIAï¼‰åœ¨æ‰°åŠ¨ä¸‹å¼ºåˆ¶ç‰¹å¾ä¸€è‡´æ€§ï¼Œä¿ƒè¿›é¢†åŸŸä¸å˜å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦åˆ†å‰²ä»»åŠ¡ï¼ˆå¿ƒè„MRIã€è„ŠæŸ±MRIå’Œè†€èƒ±ç™ŒMRIï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€æ–°çš„FSSLå’Œé¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œåœ¨æœªè§é¢†åŸŸå®ç°ç¨³å¥çš„æ³›åŒ–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´å¤šæ ·æ€§å›¾åƒå’Œç¼ºä¹æ ‡æ³¨æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFSSLï¼‰åœ¨åˆ©ç”¨å¤šä¸­å¿ƒå¤§é‡æœªæ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é¢†åŸŸåç§»é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†é¢†åŸŸå¹¿ä¹‰è”é‚¦åŠç›‘ç£å­¦ä¹ ï¼ˆFedSemiDGï¼‰çš„æ¦‚å¿µï¼Œæ—¨åœ¨å­¦ä¹ èƒ½é€‚åº”æœªè§é¢†åŸŸçš„æ¨¡å‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”è”é‚¦æ³›åŒ–æ„ŸçŸ¥åŠç›‘ç£å­¦ä¹ ï¼ˆFGASLï¼‰ï¼Œç»“åˆå…¨çƒå’Œæœ¬åœ°ç­–ç•¥æ¥åº”å¯¹FedSemiDGä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>åœ¨å…¨çƒå±‚é¢ï¼Œé€šè¿‡æ³›åŒ–æ„ŸçŸ¥èšåˆï¼ˆGAAï¼‰æ ¹æ®æ³›åŒ–æ€§èƒ½åˆ†é…è‡ªé€‚åº”æƒé‡ç»™æœ¬åœ°æ¨¡å‹ã€‚</li>
<li>åœ¨æœ¬åœ°å±‚é¢ï¼Œé‡‡ç”¨åŒæ•™å¸ˆè‡ªé€‚åº”ä¼ªæ ‡ç­¾ç»†åŒ–ï¼ˆDRï¼‰ç­–ç•¥ï¼Œç»“åˆå…¨å±€å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œæé«˜ä¼ªæ ‡ç­¾çš„å¯é æ€§ã€‚</li>
<li>æ‰°åŠ¨ä¸å˜å¯¹é½ï¼ˆPIAï¼‰æŠ€æœ¯å¼ºåˆ¶ç‰¹å¾åœ¨æ‰°åŠ¨ä¸‹ä¿æŒä¸€è‡´æ€§ï¼Œä¿ƒè¿›é¢†åŸŸä¸å˜å­¦ä¹ ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f03d64ace97590f6bda98eca686f651.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8701c36fc010e0ea1b5b45010732ac9.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Bigger-Isnâ€™t-Always-Better-Towards-a-General-Prior-for-Medical-Image-Reconstruction"><a href="#Bigger-Isnâ€™t-Always-Better-Towards-a-General-Prior-for-Medical-Image-Reconstruction" class="headerlink" title="Bigger Isnâ€™t Always Better: Towards a General Prior for Medical Image   Reconstruction"></a>Bigger Isnâ€™t Always Better: Towards a General Prior for Medical Image   Reconstruction</h2><p><strong>Authors:Lukas Glaszner, Martin Zach</strong></p>
<p>Diffusion model have been successfully applied to many inverse problems, including MRI and CT reconstruction. Researchers typically re-purpose models originally designed for unconditional sampling without modifications. Using two different posterior sampling algorithms, we show empirically that such large networks are not necessary. Our smallest model, effectively a ResNet, performs almost as good as an attention U-Net on in-distribution reconstruction, while being significantly more robust towards distribution shifts. Furthermore, we introduce models trained on natural images and demonstrate that they can be used in both MRI and CT reconstruction, out-performing model trained on medical images in out-of-distribution cases. As a result of our findings, we strongly caution against simply re-using very large networks and encourage researchers to adapt the model complexity to the respective task. Moreover, we argue that a key step towards a general diffusion-based prior is training on natural images. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºè®¸å¤šåé—®é¢˜ï¼ŒåŒ…æ‹¬MRIå’ŒCTé‡å»ºã€‚ç ”ç©¶è€…é€šå¸¸å°†åŸæœ¬ä¸ºæ— æ¡ä»¶é‡‡æ ·è®¾è®¡çš„æ¨¡å‹ç¨åŠ æ”¹åŠ¨åé‡æ–°ç”¨äºæ–°çš„åº”ç”¨åœºæ™¯ã€‚é€šè¿‡ä½¿ç”¨ä¸¤ç§ä¸åŒçš„åé‡‡æ ·ç®—æ³•ï¼Œæˆ‘ä»¬ä»å®è¯è§’åº¦è¯æ˜ï¼Œå…¶å®å¹¶ä¸éœ€è¦å¦‚æ­¤åºå¤§çš„ç½‘ç»œç»“æ„ã€‚æˆ‘ä»¬æœ€å°çš„æ¨¡å‹ï¼ˆå®è´¨ä¸Šæ˜¯ä¸€ä¸ªResNetï¼‰åœ¨åˆ†å¸ƒå†…çš„é‡å»ºä¸Šè¡¨ç°ä¸æ³¨æ„åŠ›U-Netå‡ ä¹ä¸€æ ·å¥½ï¼ŒåŒæ—¶å¯¹äºåˆ†å¸ƒè½¬ç§»è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè‡ªç„¶å›¾åƒè®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶è¯æ˜å…¶å¯ç”¨äºMRIå’ŒCTé‡å»ºï¼Œåœ¨åˆ†å¸ƒå¤–çš„æ¡ˆä¾‹ä¸­è¡¨ç°ä¼˜äºåŸºäºåŒ»å­¦å›¾åƒè®­ç»ƒçš„æ¨¡å‹ã€‚åŸºäºæˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬å¼ºçƒˆåå¯¹ç®€å•åœ°é‡å¤ä½¿ç”¨å¤§å‹ç½‘ç»œç»“æ„ï¼Œå¹¶é¼“åŠ±ç ”ç©¶è€…æ ¹æ®å„è‡ªä»»åŠ¡è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤ä¸ºé¢å‘é€šç”¨æ‰©æ•£å…ˆéªŒçš„å…³é”®æ­¥éª¤æ˜¯åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07376v1">PDF</a> To appear in the German Conference on Pattern Recognition   proceedings. Code available at   <a target="_blank" rel="noopener" href="https://github.com/VLOGroup/bigger-isnt-always-better">https://github.com/VLOGroup/bigger-isnt-always-better</a></p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºMRIå’ŒCTé‡å»ºç­‰å¤šä¸ªåé—®é¢˜ã€‚ç ”ç©¶é€šå¸¸ç›´æ¥ä½¿ç”¨åŸæœ¬è®¾è®¡ç”¨äºæ— æ¡ä»¶é‡‡æ ·çš„æ¨¡å‹ï¼Œè€Œæ— éœ€ä¿®æ”¹ã€‚é€šè¿‡ä¸¤ç§ä¸åŒçš„åé‡‡æ ·ç®—æ³•ï¼Œæˆ‘ä»¬å‘ç°å¤§å‹ç½‘ç»œå¹¶éå¿…éœ€ã€‚æˆ‘ä»¬çš„æœ€å°æ¨¡å‹ï¼ˆå³ResNetï¼‰åœ¨å†…éƒ¨åˆ†å¸ƒé‡å»ºæ–¹é¢çš„è¡¨ç°å‡ ä¹ä¸æ³¨æ„åŠ›U-Netä¸€æ ·å¥½ï¼ŒåŒæ—¶åœ¨åˆ†å¸ƒè½¬ç§»æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç»è‡ªç„¶å›¾åƒè®­ç»ƒæ¨¡å‹åœ¨MRIå’ŒCTé‡å»ºä¸­çš„åº”ç”¨ï¼Œå¹¶åœ¨å¤–éƒ¨åˆ†å¸ƒæƒ…å†µä¸‹ä¼˜äºåŒ»å­¦å›¾åƒè®­ç»ƒæ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä¸è¦ç®€å•åœ°é‡å¤ä½¿ç”¨å¤§å‹ç½‘ç»œï¼Œå¹¶é¼“åŠ±ç ”ç©¶äººå‘˜æ ¹æ®å„è‡ªä»»åŠ¡è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¤ä¸ºæœç€åŸºäºæ‰©æ•£çš„å…ˆéªŒçš„å…³é”®ä¸€æ­¥æ˜¯åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºMRIå’ŒCTé‡å»ºç­‰åé—®é¢˜ã€‚</li>
<li>ç ”ç©¶ä¸­é€šå¸¸ç›´æ¥ä½¿ç”¨æ— æ¡ä»¶é‡‡æ ·çš„æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ã€‚</li>
<li>é€šè¿‡å®éªŒæ¯”è¾ƒï¼Œå‘ç°å¤§å‹ç½‘ç»œåœ¨é‡å»ºä»»åŠ¡ä¸­å¹¶éå¿…éœ€ã€‚æœ€å°æ¨¡å‹è¡¨ç°ä¸æ›´å¤æ‚çš„æ³¨æ„åŠ›U-Netç›¸ä¼¼æˆ–æ›´ä½³ã€‚</li>
<li>å¯¹äºå†…éƒ¨åˆ†å¸ƒé‡å»ºä»»åŠ¡ï¼Œæœ€å°æ¨¡å‹åœ¨æ€§èƒ½ä¸Šæ›´ä¸ºç¨³å¥ã€‚</li>
<li>é€šè¿‡è‡ªç„¶å›¾åƒè®­ç»ƒçš„æ¨¡å‹åœ¨MRIå’ŒCTé‡å»ºä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§ï¼Œå°¤å…¶åœ¨å¤–éƒ¨åˆ†å¸ƒæƒ…å†µä¸‹æ€§èƒ½æ›´ä½³ã€‚</li>
<li>å¯¹ä½¿ç”¨å¤§å‹ç½‘ç»œçš„å»ºè®®æŒè°¨æ…æ€åº¦ï¼Œåº”é€‚åº”æ€§åœ°è°ƒæ•´æ¨¡å‹å¤æ‚åº¦ä»¥é€‚åº”ä»»åŠ¡éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4def5f8c927208b72d65c3be6031b11c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-766b51a854b1057039ecb140f16af600.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e6f3071b6540b64aa99504f90b011f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81256807f2e9db62e2b29cc5aa0b4b3d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="BIOMEDICA-An-Open-Biomedical-Image-Caption-Archive-Dataset-and-Vision-Language-Models-Derived-from-Scientific-Literature"><a href="#BIOMEDICA-An-Open-Biomedical-Image-Caption-Archive-Dataset-and-Vision-Language-Models-Derived-from-Scientific-Literature" class="headerlink" title="BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and   Vision-Language Models Derived from Scientific Literature"></a>BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and   Vision-Language Models Derived from Scientific Literature</h2><p><strong>Authors:Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey J Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, Anita Rau, Xiaohan Wang, Yuhui Zhang, Alfred Seunghoon Song, Robert Tibshirani, Serena Yeung-Levy</strong></p>
<p>The development of vision-language models (VLMs) is driven by large-scale and diverse multimodal datasets. However, progress toward generalist biomedical VLMs is limited by the lack of annotated, publicly accessible datasets across biology and medicine. Existing efforts are restricted to narrow domains, missing the full diversity of biomedical knowledge encoded in scientific literature. To address this gap, we introduce BIOMEDICA, a scalable, open-source framework to extract, annotate, and serialize the entirety of the PubMed Central Open Access subset into an easy-to-use, publicly accessible dataset. Our framework produces a comprehensive archive with over 24 million unique image-text pairs from over 6 million articles. Metadata and expert-guided annotations are also provided. We demonstrate the utility and accessibility of our resource by releasing BMCA-CLIP, a suite of CLIP-style models continuously pre-trained on the BIOMEDICA dataset via streaming, eliminating the need to download 27 TB of data locally. On average, our models achieve state-of-the-art performance across 40 tasks - spanning pathology, radiology, ophthalmology, dermatology, surgery, molecular biology, parasitology, and cell biology - excelling in zero-shot classification with a 6.56% average improvement (as high as 29.8% and 17.5% in dermatology and ophthalmology, respectively), and stronger image-text retrieval, all while using 10x less compute. To foster reproducibility and collaboration, we release our codebase and dataset for the broader research community. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•æ˜¯ç”±å¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®é›†é©±åŠ¨çš„ã€‚ç„¶è€Œï¼Œé€šç”¨ç”Ÿç‰©åŒ»å­¦VLMsçš„è¿›å±•å—é™äºç”Ÿç‰©å­¦å’ŒåŒ»å­¦é¢†åŸŸç¼ºä¹å…¬å¼€è®¿é—®çš„æ³¨é‡Šæ•°æ®é›†ã€‚ç°æœ‰çš„åŠªåŠ›ä»…é™äºç‹­çª„çš„é¢†åŸŸï¼Œé”™è¿‡äº†ç§‘å­¦æ–‡çŒ®ä¸­ç¼–ç çš„å®Œæ•´ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å¤šæ ·æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BIOMEDICAï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æºæ¡†æ¶ï¼Œç”¨äºæå–ã€æ³¨é‡Šå’Œåºåˆ—åŒ–PubMed Central Open Accesså­é›†çš„å®Œæ•´å†…å®¹ï¼Œä½¿å…¶æˆä¸ºæ˜“äºä½¿ç”¨ã€å¯å…¬å¼€è®¿é—®çš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ¡†æ¶äº§ç”Ÿäº†åŒ…å«è¶…è¿‡2.4äº¿ä¸ªå”¯ä¸€å›¾åƒæ–‡æœ¬å¯¹çš„ç»¼åˆæ¡£æ¡ˆï¼Œæ¶µç›–è¶…è¿‡6ç™¾ä¸‡ç¯‡æ–‡ç« ã€‚æˆ‘ä»¬è¿˜æä¾›äº†å…ƒæ•°æ®å’Œä¸“ä¸šæŒ‡å¯¼çš„æ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡å‘å¸ƒBMCA-CLIPæ¥è¯æ˜æˆ‘ä»¬èµ„æºçš„å®ç”¨æ€§å’Œå¯è®¿é—®æ€§ï¼ŒBMCA-CLIPæ˜¯ä¸€å¥—CLIPé£æ ¼çš„æ¨¡å‹ï¼Œåœ¨BIOMEDICAæ•°æ®é›†ä¸Šé€šè¿‡æµå¼ä¼ è¾“è¿›è¡Œè¿ç»­é¢„è®­ç»ƒï¼Œæ— éœ€ä¸‹è½½æœ¬åœ°å­˜å‚¨çš„27TBæ•°æ®ã€‚å¹³å‡è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ¶µç›–ç—…ç†å­¦ã€æ”¾å°„å­¦ã€çœ¼ç§‘ã€çš®è‚¤ç§‘ã€å¤–ç§‘ã€åˆ†å­ç”Ÿç‰©å­¦ã€å¯„ç”Ÿè™«å­¦å’Œç»†èƒç”Ÿç‰©å­¦ç­‰é¢†åŸŸçš„å››åå¤šé¡¹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬åˆ†ç±»æ–¹é¢å–å¾—äº†å¹³å‡æ”¹è¿›6.56%ï¼ˆçš®è‚¤ç—…å­¦å’Œçœ¼ç§‘åˆ†åˆ«é«˜è¾¾29.8%å’Œ17.5%ï¼‰ï¼Œå¹¶ä¸”å›¾åƒæ–‡æœ¬æ£€ç´¢èƒ½åŠ›æ›´å¼ºï¼ŒåŒæ—¶ä½¿ç”¨çš„è®¡ç®—èµ„æºå‡å°‘äº†åå€ã€‚ä¸ºäº†ä¿ƒè¿›å¯å¤åˆ¶æ€§å’Œåˆä½œï¼Œæˆ‘ä»¬å‘æ›´å¹¿æ³›çš„ç ”ç©¶ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07171v2">PDF</a> </p>
<p><strong>Summary</strong><br>    BIOMEDICAæ¡†æ¶è§£å†³äº†ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å‘å±•çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ã€‚å®ƒé€šè¿‡æå–ã€æ ‡æ³¨å’Œåºåˆ—åŒ–PubMed Central Open Accesså­é›†çš„å…¨éƒ¨å†…å®¹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡2.4äº¿ä¸ªå”¯ä¸€å›¾åƒæ–‡æœ¬å¯¹çš„ç»¼åˆæ¡£æ¡ˆã€‚æ¡†æ¶è¿˜æä¾›å…ƒæ•°æ®å’Œä¸“ä¸šæŒ‡å¯¼çš„æ³¨é‡Šã€‚é€šè¿‡å‘å¸ƒåœ¨BIOMEDICAæ•°æ®é›†ä¸ŠæŒç»­é¢„è®­ç»ƒçš„BMCA-CLIPæ¨¡å‹å¥—ä»¶ï¼Œå±•ç¤ºäº†èµ„æºçš„å®ç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚è¿™äº›æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ç—…ç†å­¦ã€æ”¾å°„å­¦ã€çœ¼ç§‘ã€çš®è‚¤ç§‘ã€æ‰‹æœ¯ã€åˆ†å­ç”Ÿç‰©å­¦ã€å¯„ç”Ÿè™«å­¦å’Œç»†èƒç”Ÿç‰©å­¦ç­‰é¢†åŸŸï¼Œå¹¶ä¸”åœ¨é›¶æ ·æœ¬åˆ†ç±»å’Œå›¾åƒæ–‡æœ¬æ£€ç´¢æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä¸ºä¿ƒè¿›å¯é‡å¤æ€§å’Œåˆä½œï¼Œå‘å¹¿å¤§ç ”ç©¶ç¤¾åŒºå‘å¸ƒä»£ç åº“å’Œæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BIOMEDICAæ¡†æ¶æ—¨åœ¨è§£å†³ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹å‘å±•çš„æ•°æ®é™åˆ¶é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æå–ã€æ ‡æ³¨å’Œåºåˆ—åŒ–PubMed Central Open Accesså­é›†çš„å†…å®¹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ä¸”å¤šæ ·çš„æ•°æ®é›†ã€‚</li>
<li>BIOMEDICAæ•°æ®é›†åŒ…å«è¶…è¿‡2.4äº¿ä¸ªç‹¬ç‰¹çš„å›¾åƒæ–‡æœ¬å¯¹ï¼Œæ¶µç›–å¹¿æ³›çš„ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†ã€‚</li>
<li>BMCA-CLIPæ¨¡å‹å¥—ä»¶åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬åˆ†ç±»æ–¹é¢æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
<li>BMCA-CLIPæ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€ä¸‹è½½å¤§é‡æ•°æ®ï¼Œè®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>BIOMEDICAæ¡†æ¶å’Œæ•°æ®é›†è¢«å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œåˆä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07171">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3daeb36818b524569b9169d49934f803.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e29c095df7cd113c3d52f99735feac3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0ab3e77fbe8858fcea6838c1c777d1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a45171928f6763d255ab78005c5c892.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24763da77081904b11d5894f615a57f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c2957209517e4fd8fe9779c56c02ebc.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Adaptive-Noise-Tolerant-Network-for-Image-Segmentation"><a href="#Adaptive-Noise-Tolerant-Network-for-Image-Segmentation" class="headerlink" title="Adaptive Noise-Tolerant Network for Image Segmentation"></a>Adaptive Noise-Tolerant Network for Image Segmentation</h2><p><strong>Authors:Weizhi Li</strong></p>
<p>Unlike image classification and annotation, for which deep network models have achieved dominating superior performances compared to traditional computer vision algorithms, deep learning for automatic image segmentation still faces critical challenges. One of such hurdles is to obtain ground-truth segmentations as the training labels for deep network training. Especially when we study biomedical images, such as histopathological images (histo-images), it is unrealistic to ask for manual segmentation labels as the ground truth for training due to the fine image resolution as well as the large image size and complexity. In this paper, instead of relying on clean segmentation labels, we study whether and how integrating imperfect or noisy segmentation results from off-the-shelf segmentation algorithms may help achieve better segmentation results through a new Adaptive Noise-Tolerant Network (ANTN) model. We extend the noisy label deep learning to image segmentation with two novel aspects: (1) multiple noisy labels can be integrated into one deep learning model; (2) noisy segmentation modeling, including probabilistic parameters, is adaptive, depending on the given testing image appearance. Implementation of the new ANTN model on both the synthetic data and real-world histo-images demonstrates its effectiveness and superiority over off-the-shelf and other existing deep-learning-based image segmentation algorithms. </p>
<blockquote>
<p>ä¸å›¾åƒåˆ†ç±»å’Œæ ‡æ³¨ä¸åŒï¼Œæ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨å›¾åƒåˆ†å‰²æ–¹é¢ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°½ç®¡æ·±åº¦ç½‘ç»œæ¨¡å‹åœ¨ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰ç®—æ³•ä¸Šå–å¾—äº†ä¸»å¯¼ä¼˜åŠ¿ã€‚å…¶ä¸­ä¸€å¤§æŒ‘æˆ˜æ˜¯è·å–çœŸå®åˆ†å‰²ä½œä¸ºæ·±åº¦ç½‘ç»œè®­ç»ƒçš„è®­ç»ƒæ ‡ç­¾ã€‚ç‰¹åˆ«æ˜¯åœ¨ç ”ç©¶ç”Ÿç‰©åŒ»å­¦å›¾åƒï¼ˆå¦‚ç»„ç»‡ç—…ç†å­¦å›¾åƒï¼‰æ—¶ï¼Œç”±äºå›¾åƒåˆ†è¾¨ç‡é«˜ã€å°ºå¯¸å¤§ä¸”å¤æ‚ï¼Œè¦æ±‚æ‰‹åŠ¨åˆ†å‰²æ ‡ç­¾ä½œä¸ºè®­ç»ƒçš„çœŸå®ä¾æ®æ˜¯ä¸ç°å®çš„ã€‚</p>
</blockquote>
<p>æœ¬æ–‡ä¸ä¾èµ–äºå¹²å‡€çš„åˆ†å‰²æ ‡ç­¾ï¼Œè€Œæ˜¯ç ”ç©¶å°†ç°æˆçš„åˆ†å‰²ç®—æ³•äº§ç”Ÿçš„æœ‰ç¼ºé™·æˆ–å˜ˆæ‚çš„åˆ†å‰²ç»“æœé›†æˆèµ·æ¥ï¼Œæ˜¯å¦ä»¥åŠå¦‚ä½•èƒ½å¤Ÿé€šè¿‡ä¸€ç§æ–°çš„è‡ªé€‚åº”å™ªå£°å®¹å¿ç½‘ç»œï¼ˆANTNï¼‰æ¨¡å‹ï¼Œå®ç°æ›´å¥½çš„åˆ†å‰²ç»“æœã€‚æˆ‘ä»¬å°†å™ªå£°æ ‡ç­¾æ·±åº¦å­¦ä¹ æ‰©å±•åˆ°å›¾åƒåˆ†å‰²ï¼Œå¹¶å¸¦æ¥äº†ä¸¤ä¸ªæ–°çš„è§‚ç‚¹ï¼šï¼ˆ1ï¼‰å¤šä¸ªå™ªå£°æ ‡ç­¾å¯ä»¥é›†æˆåˆ°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼›ï¼ˆ2ï¼‰å™ªå£°åˆ†å‰²å»ºæ¨¡ï¼ŒåŒ…æ‹¬æ¦‚ç‡å‚æ•°ï¼Œæ˜¯è‡ªé€‚åº”çš„ï¼Œå–å†³äºç»™å®šçš„æµ‹è¯•å›¾åƒå¤–è§‚ã€‚ANTNæ¨¡å‹åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œç»„ç»‡ç—…ç†å­¦å›¾åƒä¸Šçš„å®ç°ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä¼˜äºç°æˆçš„ä»¥åŠå…¶ä»–ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†å‰²ç®—æ³•ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07163v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨å›¾åƒåˆ†å‰²æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè·å–ç”¨äºæ·±åº¦ç½‘ç»œè®­ç»ƒçš„ç²¾ç¡®åˆ†å‰²æ ‡ç­¾éå¸¸å›°éš¾ã€‚é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç­‰åœºæ™¯ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å™ªå£°å®¹å¿ç½‘ç»œï¼ˆANTNï¼‰æ¨¡å‹ï¼Œé€šè¿‡é›†æˆç°æˆçš„åˆ†å‰²ç®—æ³•å¾—åˆ°çš„ä¸å®Œç¾æˆ–å™ªå£°åˆ†å‰²ç»“æœï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚è¯¥æ¨¡å‹å°†å™ªå£°æ ‡ç­¾æ·±åº¦å­¦ä¹ æ‰©å±•åˆ°å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œå…·æœ‰ä¸¤ä¸ªåˆ›æ–°ç‚¹ï¼šä¸€æ˜¯èƒ½å°†å¤šä¸ªå™ªå£°æ ‡ç­¾é›†æˆåˆ°ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼›äºŒæ˜¯æ ¹æ®ç»™å®šçš„æµ‹è¯•å›¾åƒå¤–è§‚è¿›è¡Œè‡ªé€‚åº”çš„æ¦‚ç‡å‚æ•°å»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œå†å²å›¾åƒä¸Šçš„è¡¨ç°ä¼˜äºç°æˆçš„å’Œå…¶ä»–åŸºäºæ·±åº¦å­¦ä¹ çš„å›¾åƒåˆ†å‰²ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨å›¾åƒåˆ†å‰²ä¸Šä»é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è·å–ç”¨äºæ·±åº¦ç½‘ç»œè®­ç»ƒçš„ç²¾ç¡®åˆ†å‰²æ ‡ç­¾çš„éš¾åº¦å¤§ã€‚</li>
<li>æ‰‹åŠ¨ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒï¼ˆå¦‚ç»„ç»‡ç—…ç†å­¦å›¾åƒï¼‰åˆ¶ä½œç²¾ç¡®çš„åˆ†å‰²æ ‡ç­¾ä¸åˆ‡å®é™…ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”å™ªå£°å®¹å¿ç½‘ç»œï¼ˆANTNï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿé›†æˆä¸å®Œç¾æˆ–å™ªå£°åˆ†å‰²ç»“æœï¼Œæé«˜å›¾åƒåˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ANTNæ¨¡å‹å°†å¤šä¸ªå™ªå£°æ ‡ç­¾é›†æˆåˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œå¹¶å®ç°è‡ªé€‚åº”çš„æ¦‚ç‡å‚æ•°å»ºæ¨¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfc75eef329290a83b1b46ccaff1ce24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c2af8947e628bfe475b0b2486ef7864.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Superpixel-Segmentation-via-Structural-Information-Theory"><a href="#Hierarchical-Superpixel-Segmentation-via-Structural-Information-Theory" class="headerlink" title="Hierarchical Superpixel Segmentation via Structural Information Theory"></a>Hierarchical Superpixel Segmentation via Structural Information Theory</h2><p><strong>Authors:Minhui Xie, Hao Peng, Pu Li, Guangjie Zeng, Shuhai Wang, Jia Wu, Peng Li, Philip S. Yu</strong></p>
<p>Superpixel segmentation is a foundation for many higher-level computer vision tasks, such as image segmentation, object recognition, and scene understanding. Existing graph-based superpixel segmentation methods typically concentrate on the relationships between a given pixel and its directly adjacent pixels while overlooking the influence of non-adjacent pixels. These approaches do not fully leverage the global information in the graph, leading to suboptimal segmentation quality. To address this limitation, we present SIT-HSS, a hierarchical superpixel segmentation method based on structural information theory. Specifically, we first design a novel graph construction strategy that incrementally explores the pixel neighborhood to add edges based on 1-dimensional structural entropy (1D SE). This strategy maximizes the retention of graph information while avoiding an overly complex graph structure. Then, we design a new 2D SE-guided hierarchical graph partitioning method, which iteratively merges pixel clusters layer by layer to reduce the graphâ€™s 2D SE until a predefined segmentation scale is achieved. Experimental results on three benchmark datasets demonstrate that the SIT-HSS performs better than state-of-the-art unsupervised superpixel segmentation algorithms. The source code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/SELGroup/SIT-HSS%7D">https://github.com/SELGroup/SIT-HSS}</a>. </p>
<blockquote>
<p>è¶…åƒç´ åˆ†å‰²æ˜¯è®¸å¤šé«˜çº§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„åŸºç¡€ï¼Œå¦‚å›¾åƒåˆ†å‰²ã€å¯¹è±¡è¯†åˆ«å’Œåœºæ™¯ç†è§£ã€‚ç°æœ‰çš„åŸºäºå›¾çš„è¶…åƒç´ åˆ†å‰²æ–¹æ³•é€šå¸¸ä¾§é‡äºç»™å®šåƒç´ åŠå…¶ç›´æ¥ç›¸é‚»åƒç´ ä¹‹é—´çš„å…³ç³»ï¼Œè€Œå¿½ç•¥äº†éç›¸é‚»åƒç´ çš„å½±å“ã€‚è¿™äº›æ–¹æ³•æ²¡æœ‰å……åˆ†åˆ©ç”¨å›¾ä¸­çš„å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´åˆ†å‰²è´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç»“æ„ä¿¡æ¯ç†è®ºçš„åˆ†å±‚è¶…åƒç´ åˆ†å‰²æ–¹æ³•ï¼ˆç®€ç§°SIT-HSSï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ–°çš„å›¾æ„å»ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºä¸€ç»´ç»“æ„ç†µï¼ˆ1D SEï¼‰å¢é‡åœ°æ¢ç´¢åƒç´ é‚»åŸŸä»¥æ·»åŠ è¾¹ã€‚è¿™ç§ç­–ç•¥æœ€å¤§é™åº¦åœ°ä¿ç•™äº†å›¾å½¢ä¿¡æ¯ï¼ŒåŒæ—¶é¿å…äº†è¿‡äºå¤æ‚çš„å›¾å½¢ç»“æ„ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„ç”±äºŒç»´SEå¼•å¯¼çš„åˆ†å±‚æ¬¡å›¾åˆ’åˆ†æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€å±‚è¿­ä»£åˆå¹¶åƒç´ ç°‡ï¼Œä»¥å‡å°‘å›¾çš„äºŒç»´SEï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šçš„åˆ†å‰²å°ºåº¦ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„æ— ç›‘ç£è¶…åƒç´ åˆ†å‰²ç®—æ³•ç›¸æ¯”ï¼ŒSIT-HSSå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SELGroup/SIT-HSS%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SELGroup/SIT-HSSè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07069v1">PDF</a> Accepted by SDM 2025</p>
<p><strong>Summary</strong><br>     åŸºäºç»“æ„ä¿¡æ¯ç†è®ºçš„åˆ†å±‚è¶…åƒç´ åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡æ–°çš„å›¾æ„å»ºç­–ç•¥ï¼ŒåŸºäºä¸€ç»´ç»“æ„ç†µï¼ˆ1D SEï¼‰å¢é‡æ¢ç´¢åƒç´ é‚»åŸŸæ¥æ·»åŠ è¾¹ç¼˜ï¼Œå¹¶è®¾è®¡æ–°çš„äºŒç»´SEå¼•å¯¼åˆ†å±‚å›¾åˆ’åˆ†æ–¹æ³•ï¼Œå®ç°å›¾åƒè¶…åƒç´ åˆ†å‰²ï¼Œä¼˜äºç°æœ‰å…ˆè¿›æ— ç›‘ç£è¶…åƒç´ åˆ†å‰²ç®—æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…åƒç´ åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„åŸºç¡€ï¼Œå¦‚å›¾åƒåˆ†å‰²ã€ç›®æ ‡è¯†åˆ«å’Œåœºæ™¯ç†è§£ã€‚</li>
<li>ç°æœ‰åŸºäºå›¾çš„è¶…åƒç´ åˆ†å‰²æ–¹æ³•ä¸»è¦å…³æ³¨ç»™å®šåƒç´ åŠå…¶ç›´æ¥ç›¸é‚»åƒç´ ä¹‹é—´çš„å…³ç³»ï¼Œå¿½ç•¥äº†éç›¸é‚»åƒç´ çš„å½±å“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ²¡æœ‰å……åˆ†åˆ©ç”¨å›¾ä¸­çš„å…¨å±€ä¿¡æ¯ï¼Œå¯¼è‡´åˆ†å‰²è´¨é‡ä¸ä½³ã€‚</li>
<li>SIT-HSSæ˜¯ä¸€ç§åŸºäºç»“æ„ä¿¡æ¯ç†è®ºçš„åˆ†å±‚è¶…åƒç´ åˆ†å‰²æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>SIT-HSSè®¾è®¡äº†ä¸€ç§æ–°çš„å›¾æ„å»ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºä¸€ç»´ç»“æ„ç†µï¼ˆ1D SEï¼‰å¢é‡æ¢ç´¢åƒç´ é‚»åŸŸæ¥æ·»åŠ è¾¹ç¼˜ã€‚</li>
<li>SIT-HSSè¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„äºŒç»´SEå¼•å¯¼åˆ†å±‚å›¾åˆ’åˆ†æ–¹æ³•ï¼Œé€šè¿‡é€å±‚åˆå¹¶åƒç´ ç°‡æ¥å‡å°‘å›¾çš„äºŒç»´SEï¼Œç›´è‡³è¾¾åˆ°é¢„è®¾çš„åˆ†å‰²å°ºåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b40929047172e292c85037b488b93f6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4fffd18fb5fdc98fd2389db8fe991c78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1c8e87fe2f3bfb55a4df8f853765da.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM"><a href="#UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM" class="headerlink" title="UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM"></a>UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM</h2><p><strong>Authors:Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi</strong></p>
<p>3D medical image segmentation has progressed considerably due to Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these methods struggle to balance long-range dependency acquisition with computational efficiency. To address this challenge, we propose UNETVL (U-Net Vision-LSTM), a novel architecture that leverages recent advancements in temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for improved scalability and memory functions, alongside an efficient Chebyshev Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency patterns more effectively. We validated our method on the ACDC and AMOS2022 (post challenge Task 2) benchmark datasets, showing a significant improvement in mean Dice score compared to recent state-of-the-art approaches, especially over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS, respectively. Extensive ablation studies were conducted to demonstrate the impact of each component in UNETVL, providing a comprehensive understanding of its architecture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL">https://github.com/tgrex6/UNETVL</a>, facilitating further research and applications in this domain. </p>
<blockquote>
<p>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸå·²ç»ç”±äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„å‘å±•å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¹³è¡¡é•¿è·ç¦»ä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UNETVLï¼ˆU-Net Vision-LSTMï¼‰è¿™ä¸€æ–°å‹æ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨æœ€æ–°çš„æ—¶é—´ä¿¡æ¯å¤„ç†è¿›å±•ã€‚UNETVLç»“åˆäº†Vision-LSTMï¼ˆViLï¼‰ä»¥å¢å¼ºå…¶å¯æ‰©å±•æ€§å’Œè®°å¿†åŠŸèƒ½ï¼Œå¹¶é‡‡ç”¨äº†é«˜æ•ˆçš„åˆ‡æ¯”é›ªå¤«Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰ä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚ä¸”é•¿è·ç¦»çš„ä¾èµ–æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ACDCå’ŒAMOS2022ï¼ˆæŒ‘æˆ˜èµ›ä»»åŠ¡2ä¹‹åï¼‰çš„åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç›¸è¾ƒäºæœ€æ–°çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å‰èº«UNETRä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ACDCå’ŒAMOSä¸Šçš„å¹³å‡Diceå¾—åˆ†å‡æ˜¾è‘—æé«˜ï¼Œåˆ†åˆ«æé«˜äº†7.3%å’Œ15.6%ã€‚è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºUNETVLä¸­æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œä¸ºå…¶æ¶æ„æä¾›äº†å…¨é¢çš„ç†è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL%E8%8E%B7%E5%8F%96%EF%BC%8C%E4%B8%BA%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BA%94%E7%94%A8%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BE%BF%E5%88%A9%E3%80%82">https://github.com/tgrex6/UNETVLè·å–ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07017v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†æ–°çš„æ¶æ„UNETVLï¼Œè¯¥æ¶æ„ç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œæ—¶ç©ºä¿¡æ¯å¤„ç†çš„æœ€æ–°è¿›å±•æ¥è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥Vision-LSTMï¼ˆViLï¼‰æé«˜äº†å¯æ‰©å±•æ€§å’Œå†…å­˜åŠŸèƒ½ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„åˆ‡æ¯”é›ªå¤«æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œé•¿æœŸçš„ä¾èµ–æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¶æ„åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºæœ€æ–°çš„å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯ä¸å‰èº«çš„UNETç›¸æ¯”ï¼ŒDiceç³»æ•°çš„å¹³å‡å¾—åˆ†åˆ†åˆ«æé«˜äº†7.3%å’Œå¢åŠ äº†ä¸¤å€ã€‚æ–‡ç« æä¾›äº†å®Œæ•´çš„UNETVLä»£ç ä¾›å…¬ä¼—å‚è€ƒï¼Œä»¥ä¾¿åœ¨æ­¤é¢†åŸŸè¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ã€‚è¿™ä¸€è®¾è®¡å¯ä¸ºå¤æ‚åŒ»ç–—å›¾åƒå¤„ç†ä»»åŠ¡æä¾›æ›´é«˜æ•ˆå’Œå‡†ç¡®çš„æŠ€æœ¯æ‰‹æ®µã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºä¸Šè¿°æ–‡æœ¬çš„é‡è¦è§è§£åˆ—è¡¨ï¼š</p>
<ul>
<li>UNETVLæ˜¯ä¸€ä¸ªç»“åˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œæ—¶ç©ºä¿¡æ¯å¤„ç†çš„æœ€æ–°è¿›å±•çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ã€‚å®ƒæ—¨åœ¨è§£å†³é•¿æœŸä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚</li>
<li>UNETVLå¼•å…¥äº†Vision-LSTMï¼ˆViLï¼‰ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„å†…å­˜åŠŸèƒ½å’Œå¯æ‰©å±•æ€§ã€‚å®ƒé€šè¿‡æ›´é«˜æ•ˆå¤„ç†é•¿æœŸä¾èµ–æ¨¡å¼æ”¹è¿›äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ‡æ¯”é›ªå¤«æŸ¯å°”è«å“¥æ´›å¤«-é˜¿è¯ºå°”å¾·ç½‘ç»œï¼ˆKANï¼‰è¢«ç”¨äºUNETVLä¸­ï¼Œä»¥æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚çš„é•¿æœŸä¾èµ–å…³ç³»æ¨¡å¼ã€‚è¿™ä¸ºæ¨¡å‹æä¾›äº†æ›´å¥½çš„æ€§èƒ½æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9a0c173c68fcf2fc3f02c8a72e9f0df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ac52a42d87ef19502b538c400175a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3895c5b8d10051a070407ba7635abe83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89d5f5cf8fff3dec48d4df4779011549.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Multi-Modal-Deep-Learning-Framework-for-Pan-Cancer-Prognosis"><a href="#A-Multi-Modal-Deep-Learning-Framework-for-Pan-Cancer-Prognosis" class="headerlink" title="A Multi-Modal Deep Learning Framework for Pan-Cancer Prognosis"></a>A Multi-Modal Deep Learning Framework for Pan-Cancer Prognosis</h2><p><strong>Authors:Binyu Zhang, Shichao Li, Junpeng Jian, Zhu Meng, Limei Guo, Zhicheng Zhao</strong></p>
<p>Prognostic task is of great importance as it closely related to the survival analysis of patients, the optimization of treatment plans and the allocation of resources. The existing prognostic models have shown promising results on specific datasets, but there are limitations in two aspects. On the one hand, they merely explore certain types of modal data, such as patient histopathology WSI and gene expression analysis. On the other hand, they adopt the per-cancer-per-model paradigm, which means the trained models can only predict the prognostic effect of a single type of cancer, resulting in weak generalization ability. In this paper, a deep-learning based model, named UMPSNet, is proposed. Specifically, to comprehensively understand the condition of patients, in addition to constructing encoders for histopathology images and genomic expression profiles respectively, UMPSNet further integrates four types of important meta data (demographic information, cancer type information, treatment protocols, and diagnosis results) into text templates, and then introduces a text encoder to extract textual features. In addition, the optimal transport OT-based attention mechanism is utilized to align and fuse features of different modalities. Furthermore, a guided soft mixture of experts (GMoE) mechanism is introduced to effectively address the issue of distribution differences among multiple cancer datasets. By incorporating the multi-modality of patient data and joint training, UMPSNet outperforms all SOTA approaches, and moreover, it demonstrates the effectiveness and generalization ability of the proposed learning paradigm of a single model for multiple cancer types. The code of UMPSNet is available at <a target="_blank" rel="noopener" href="https://github.com/binging512/UMPSNet">https://github.com/binging512/UMPSNet</a>. </p>
<blockquote>
<p>é¢„åä»»åŠ¡éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒä¸æ‚£è€…çš„ç”Ÿå­˜åˆ†æã€æ²»ç–—è®¡åˆ’çš„ä¼˜åŒ–å’Œèµ„æºçš„åˆ†é…å¯†åˆ‡ç›¸å…³ã€‚ç°æœ‰çš„é¢„åæ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†æœ‰ä¸¤ä¸ªæ–¹é¢çš„å±€é™æ€§ã€‚ä¸€æ–¹é¢ï¼Œå®ƒä»¬ä»…ä»…æ¢ç´¢äº†æŸäº›ç±»å‹çš„æ¨¡æ€æ•°æ®ï¼Œå¦‚æ‚£è€…çš„ç—…ç†ç»„ç»‡å­¦WSIå’ŒåŸºå› è¡¨è¾¾åˆ†æã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒä»¬é‡‡ç”¨äº†ä¸€ç§é’ˆå¯¹æ¯ç§ç™Œç—‡çš„æ¨¡å‹æ¨¡å¼ï¼Œè¿™æ„å‘³ç€è®­ç»ƒè¿‡çš„æ¨¡å‹åªèƒ½é¢„æµ‹å•ä¸€ç±»å‹ç™Œç—‡çš„é¢„åæ•ˆæœï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ï¼Œåä¸ºUMPSNetã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å…¨é¢ç†è§£æ‚£è€…çš„ç—…æƒ…ï¼Œé™¤äº†åˆ†åˆ«ä¸ºç—…ç†ç»„ç»‡å­¦å›¾åƒå’ŒåŸºå› è¡¨è¾¾è°±æ„å»ºç¼–ç å™¨å¤–ï¼ŒUMPSNetè¿˜è¿›ä¸€æ­¥å°†å››ç§é‡è¦çš„å…ƒæ•°æ®ï¼ˆäººå£ç»Ÿè®¡ä¿¡æ¯ã€ç™Œç—‡ç±»å‹ä¿¡æ¯ã€æ²»ç–—åè®®å’Œè¯Šæ–­ç»“æœï¼‰æ•´åˆåˆ°æ–‡æœ¬æ¨¡æ¿ä¸­ï¼Œç„¶åå¼•å…¥ä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨æ¥æå–æ–‡æœ¬ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨åŸºäºæœ€ä¼˜ä¼ è¾“çš„æ³¨æ„åŠ›æœºåˆ¶æ¥å¯¹é½å’Œèåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¯¼å‘è½¯æ··åˆä¸“å®¶ï¼ˆGMoEï¼‰æœºåˆ¶ï¼Œæœ‰æ•ˆè§£å†³å¤šä¸ªç™Œç—‡æ•°æ®é›†ä¹‹é—´åˆ†å¸ƒå·®å¼‚çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆæ‚£è€…æ•°æ®çš„å¤šæ¨¡æ€è”åˆè®­ç»ƒï¼ŒUMPSNetä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè€Œä¸”éªŒè¯äº†å•ä¸€æ¨¡å‹ç”¨äºå¤šç§ç™Œç—‡ç±»å‹çš„å­¦ä¹ æ¨¡å¼çš„çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚UMPSNetçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/binging512/UMPSNet">https://github.com/binging512/UMPSNet</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07016v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€èåˆæ¨¡å‹UMPSNetï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°äº†è§£ç—…äººæƒ…å†µå¹¶é¢„æµ‹å¤šç±»å‹ç™Œç—‡çš„é¢„åæ•ˆæœã€‚æ¨¡å‹èåˆäº†ç—…äººçš„å››ç§é‡è¦å…ƒæ•°æ®ï¼Œé‡‡ç”¨æ–‡æœ¬ç¼–ç å™¨æå–æ–‡æœ¬ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æœ€ä¼˜ä¼ è¾“çš„æ³¨æ„åŠ›æœºåˆ¶å¯¹é½å’Œèåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥å¼•å¯¼è½¯æ··åˆä¸“å®¶æœºåˆ¶è§£å†³å¤šä¸ªç™Œç—‡æ•°æ®é›†é—´çš„åˆ†å¸ƒå·®å¼‚é—®é¢˜ã€‚è¯¥æ¨¡å‹å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMPSNetæ¨¡å‹æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€èåˆæ¨¡å‹ï¼Œç”¨äºç—…äººçš„å…¨é¢æƒ…å†µç†è§£å’Œå¤šç±»å‹ç™Œç—‡çš„é¢„åæ•ˆæœé¢„æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹èåˆäº†ç—…äººçš„å››ç§é‡è¦å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬äººå£å­¦ä¿¡æ¯ã€ç™Œç—‡ç±»å‹ä¿¡æ¯ã€æ²»ç–—åè®®å’Œè¯Šæ–­ç»“æœï¼Œå¹¶è½¬åŒ–ä¸ºæ–‡æœ¬æ¨¡æ¿è¿›è¡Œå¤„ç†ã€‚</li>
<li>UMPSNeté‡‡ç”¨æ–‡æœ¬ç¼–ç å™¨æå–æ–‡æœ¬ç‰¹å¾ï¼Œå¹¶åº”ç”¨æœ€ä¼˜ä¼ è¾“çš„æ³¨æ„åŠ›æœºåˆ¶æ¥å¯¹é½å’Œèåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ã€‚</li>
<li>å¼•å¯¼è½¯æ··åˆä¸“å®¶æœºåˆ¶è¢«å¼•å…¥ä»¥è§£å†³å¤šä¸ªç™Œç—‡æ•°æ®é›†é—´çš„åˆ†å¸ƒå·®å¼‚é—®é¢˜ã€‚</li>
<li>UMPSNetæ¨¡å‹å±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¼˜äºå½“å‰æ‰€æœ‰å…ˆè¿›æ–¹æ³•ï¼Œå¹¶éªŒè¯äº†å•ä¸€æ¨¡å‹å¯¹å¤šç§ç™Œç—‡ç±»å‹å­¦ä¹ çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>UMPSNetæ¨¡å‹çš„ä»£ç å·²å…¬å¼€å¯è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc8c644a708425bacdd1a479cfd4fcf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db4849f995eafab83b8ff09887773f0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Comparison-of-Autoencoders-for-tokenization-of-ASL-datasets"><a href="#Comparison-of-Autoencoders-for-tokenization-of-ASL-datasets" class="headerlink" title="Comparison of Autoencoders for tokenization of ASL datasets"></a>Comparison of Autoencoders for tokenization of ASL datasets</h2><p><strong>Authors:Vouk Praun-Petrovic, Aadhvika Koundinya, Lavanya Prahallad</strong></p>
<p>Generative AI, powered by large language models (LLMs), has revolutionized applications across text, audio, images, and video. This study focuses on developing and evaluating encoder-decoder architectures for the American Sign Language (ASL) image dataset, consisting of 87,000 images across 29 hand sign classes. Three approaches were compared: Feedforward Autoencoders, Convolutional Autoencoders, and Diffusion Autoencoders. The Diffusion Autoencoder outperformed the others, achieving the lowest mean squared error (MSE) and highest Mean Opinion Score (MOS) due to its probabilistic noise modeling and iterative denoising capabilities. The Convolutional Autoencoder demonstrated effective spatial feature extraction but lacked the robustness of the diffusion process, while the Feedforward Autoencoder served as a baseline with limitations in handling complex image data. Objective and subjective evaluations confirmed the superiority of the Diffusion Autoencoder for high-fidelity image reconstruction, emphasizing its potential in multimodal AI applications such as sign language recognition and generation. This work provides critical insights into designing robust encoder-decoder systems to advance multimodal AI capabilities. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenerative AIï¼‰å‡­å€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŠ›é‡ï¼Œåœ¨æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ç­‰é¢†åŸŸçš„åº”ç”¨ä¸­å¼•å‘äº†é©å‘½æ€§çš„å˜é©ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºä¸ºç¾å›½æ‰‹è¯­ï¼ˆASLï¼‰å›¾åƒæ•°æ®é›†å¼€å‘å¹¶è¯„ä¼°ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚è¯¥æ•°æ®é›†åŒ…å«è·¨29ä¸ªæ‰‹è¯­ç±»åˆ«çš„87,000å¼ å›¾åƒã€‚ç ”ç©¶æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šå‰é¦ˆè‡ªç¼–ç å™¨ï¼ˆFeedforward Autoencodersï¼‰ã€å·ç§¯è‡ªç¼–ç å™¨ï¼ˆConvolutional Autoencodersï¼‰å’Œæ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆDiffusion Autoencodersï¼‰ã€‚ç”±äºå…·æœ‰æ¦‚ç‡å™ªå£°å»ºæ¨¡å’Œè¿­ä»£å»å™ªèƒ½åŠ›ï¼Œæ‰©æ•£è‡ªç¼–ç å™¨åœ¨æ€§èƒ½ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå–å¾—äº†æœ€ä½çš„å¹³å‡å¹³æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œæœ€é«˜çš„å¹³å‡æ„è§åˆ†æ•°ï¼ˆMOSï¼‰ã€‚å·ç§¯è‡ªç¼–ç å™¨æ˜¾ç¤ºå‡ºæœ‰æ•ˆçš„ç©ºé—´ç‰¹å¾æå–èƒ½åŠ›ï¼Œä½†åœ¨æ‰©æ•£è¿‡ç¨‹çš„ç¨³å¥æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè€Œå‰é¦ˆè‡ªç¼–ç å™¨åˆ™ä½œä¸ºå¤„ç†å¤æ‚å›¾åƒæ•°æ®å­˜åœ¨å±€é™æ€§çš„åŸºçº¿æ¨¡å‹ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¯å®äº†æ‰©æ•£è‡ªç¼–ç å™¨åœ¨é«˜ä¿çœŸå›¾åƒé‡å»ºæ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œå¼ºè°ƒå…¶åœ¨æ‰‹è¯­è¯†åˆ«å’Œç”Ÿæˆç­‰å¤šæ¨¡æ€äººå·¥æ™ºèƒ½åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸ºè®¾è®¡ç¨³å¥çš„ç¼–ç å™¨-è§£ç å™¨ç³»ç»Ÿæä¾›äº†å…³é”®è§è§£ï¼Œæœ‰åŠ©äºæ¨åŠ¨å¤šæ¨¡æ€äººå·¥æ™ºèƒ½èƒ½åŠ›çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06942v1">PDF</a> 9 pages, 2 tables, 4 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå¼AIå·²ç»å½»åº•æ”¹å˜äº†æ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘çš„åº”ç”¨æ–¹å¼ã€‚æœ¬ç ”ç©¶ä¸“æ³¨äºé’ˆå¯¹ç¾å›½æ‰‹è¯­ï¼ˆASLï¼‰å›¾åƒæ•°æ®é›†å¼€å‘å’Œè¯„ä¼°ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ŒåŒ…å«87,000å¼ è·¨è¶Š29ä¸ªæ‰‹åŠ¿ç±»åˆ«çš„å›¾åƒã€‚å¯¹æ¯”äº†ä¸‰ç§æ–¹æ³•ï¼šå‰é¦ˆè‡ªç¼–ç å™¨ã€å·ç§¯è‡ªç¼–ç å™¨å’Œæ‰©æ•£è‡ªç¼–ç å™¨ã€‚æ‰©æ•£è‡ªç¼–ç å™¨å› æ¦‚ç‡å™ªå£°å»ºæ¨¡å’Œè¿­ä»£å»å™ªèƒ½åŠ›è¡¨ç°æœ€ä½³ï¼Œå®ç°äº†æœ€ä½å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œæœ€é«˜å¹³å‡æ„è§å¾—åˆ†ï¼ˆMOSï¼‰ã€‚å·ç§¯è‡ªç¼–ç å™¨åœ¨æå–ç©ºé—´ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ‰©æ•£è¿‡ç¨‹çš„ç¨³å¥æ€§ï¼›è€Œå‰é¦ˆè‡ªç¼–ç å™¨ä½œä¸ºåŸºå‡†ï¼Œåœ¨å¤„ç†å¤æ‚å›¾åƒæ•°æ®æ—¶å­˜åœ¨å±€é™æ€§ã€‚å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¯å®äº†æ‰©æ•£è‡ªç¼–ç å™¨åœ¨é«˜ä¿çœŸå›¾åƒé‡å»ºä¸­çš„ä¼˜è¶Šæ€§ï¼Œå¼ºè°ƒå…¶åœ¨æ‰‹åŠ¿è¯­è¨€è¯†åˆ«å’Œç”Ÿæˆç­‰å¤šæ¨¡å¼AIåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸ºè®¾è®¡ç¨³å¥çš„ç¼–ç å™¨-è§£ç å™¨ç³»ç»Ÿä»¥æ¨åŠ¨å¤šæ¨¡å¼AIèƒ½åŠ›æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼AIå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºæ–‡æœ¬ã€éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘é¢†åŸŸã€‚</li>
<li>æœ¬ç ”ç©¶èšç„¦äºé’ˆå¯¹ç¾å›½æ‰‹è¯­ï¼ˆASLï¼‰å›¾åƒæ•°æ®é›†å¼€å‘å’Œè¯„ä¼°ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
<li>å¯¹æ¯”äº†ä¸‰ç§å›¾åƒå¤„ç†æ–¹æ³•ï¼šå‰é¦ˆè‡ªç¼–ç å™¨ã€å·ç§¯è‡ªç¼–ç å™¨å’Œæ‰©æ•£è‡ªç¼–ç å™¨ã€‚</li>
<li>æ‰©æ•£è‡ªç¼–ç å™¨å› æ¦‚ç‡å™ªå£°å»ºæ¨¡å’Œè¿­ä»£å»å™ªèƒ½åŠ›è¡¨ç°æœ€ä½³ã€‚</li>
<li>å·ç§¯è‡ªç¼–ç å™¨åœ¨æå–ç©ºé—´ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹æ‰©æ•£ç¨³å¥æ€§ã€‚</li>
<li>é«˜ä¿çœŸå›¾åƒé‡å»ºä¸­æ‰©æ•£è‡ªç¼–ç å™¨çš„ä¼˜è¶Šæ€§é€šè¿‡å®¢è§‚å’Œä¸»è§‚è¯„ä¼°å¾—åˆ°è¯å®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1c9926ae24a34990bbdd26f036ac2f9a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a034943c54f2b8059afd13c75ac96eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a1df6a20c2761f46830eb1fcef6b08b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ddb6c47fd1b1b76584adbcee2a2919e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbfbcfe873f9ec1aeff168ca9f9af80e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-Foundational-Generative-Model-for-Breast-Ultrasound-Image-Analysis"><a href="#A-Foundational-Generative-Model-for-Breast-Ultrasound-Image-Analysis" class="headerlink" title="A Foundational Generative Model for Breast Ultrasound Image Analysis"></a>A Foundational Generative Model for Breast Ultrasound Image Analysis</h2><p><strong>Authors:Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Haotian Ye, Siyu He, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, James Zou, Qingli Zhu, Yong Wang, Liwei Wang</strong></p>
<p>Foundational models have emerged as powerful tools for addressing various tasks in clinical settings. However, their potential development to breast ultrasound analysis remains untapped. In this paper, we present BUSGen, the first foundational generative model specifically designed for breast ultrasound image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen has acquired extensive knowledge of breast structures, pathological features, and clinical variations. With few-shot adaptation, BUSGen can generate repositories of realistic and informative task-specific data, facilitating the development of models for a wide range of downstream tasks. Extensive experiments highlight BUSGenâ€™s exceptional adaptability, significantly exceeding real-data-trained foundational models in breast cancer screening, diagnosis, and prognosis. In breast cancer early diagnosis, our approach outperformed all board-certified radiologists (n&#x3D;9), achieving an average sensitivity improvement of 16.5% (P-value&lt;0.0001). Additionally, we characterized the scaling effect of using generated data which was as effective as the collected real-world data for training diagnostic models. Moreover, extensive experiments demonstrated that our approach improved the generalization ability of downstream models. Importantly, BUSGen protected patient privacy by enabling fully de-identified data sharing, making progress forward in secure medical data utilization. An online demo of BUSGen is available at <a target="_blank" rel="noopener" href="https://aibus.bio/">https://aibus.bio</a>. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹å·²ä½œä¸ºä¸´åºŠç¯å¢ƒä¸­å¤„ç†å„ç§ä»»åŠ¡çš„å¼ºå¤§å·¥å…·è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¹³è…ºè¶…å£°åˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªè¢«å¼€å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†BUSGenï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºä¹³è…ºè¶…å£°å›¾åƒåˆ†æè®¾è®¡çš„é¦–ä¸ªåŸºç¡€ç”Ÿæˆæ¨¡å‹ã€‚ç»è¿‡åœ¨è¶…è¿‡350ä¸‡å¼ ä¹³è…ºè¶…å£°å›¾åƒä¸Šçš„é¢„è®­ç»ƒï¼ŒBUSGenè·å¾—äº†å…³äºä¹³æˆ¿ç»“æ„ã€ç—…ç†ç‰¹å¾å’Œä¸´åºŠå˜åŒ–çš„å¹¿æ³›çŸ¥è¯†ã€‚é€šè¿‡å°‘é‡çš„é€‚åº”ï¼ŒBUSGenå¯ä»¥ç”Ÿæˆå®é™…ä¸”ä¿¡æ¯é‡ä¸°å¯Œçš„ç‰¹å®šä»»åŠ¡æ•°æ®ä»“åº“ï¼Œä¿ƒè¿›é’ˆå¯¹å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹å‘å±•ã€‚å¤§é‡å®éªŒçªå‡ºäº†BUSGençš„å‡ºè‰²é€‚åº”æ€§ï¼Œåœ¨ä¹³è…ºç™Œç­›æŸ¥ã€è¯Šæ–­å’Œé¢„åæ–¹é¢æ˜¾è‘—è¶…è¿‡äº†ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚åœ¨ä¹³è…ºç™Œæ—©æœŸè¯Šæ–­ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæ‰€æœ‰æ‰§ä¸šåŒ»å¸ˆè®¤è¯çš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼ˆn&#x3D;9ï¼‰ï¼Œå¹³å‡æ•æ„Ÿæ€§æé«˜äº†16.5%ï¼ˆPå€¼&lt;0.0001ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†æäº†ä½¿ç”¨ç”Ÿæˆæ•°æ®ä¸æ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®è®­ç»ƒè¯Šæ–­æ¨¡å‹çš„è§„æ¨¡æ•ˆåº”ã€‚è€Œä¸”ï¼Œå¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é‡è¦çš„æ˜¯ï¼ŒBUSGené€šè¿‡å®ç°å®Œå…¨åŒ¿ååŒ–çš„æ•°æ®å…±äº«ï¼Œä¿æŠ¤äº†æ‚£è€…éšç§ï¼Œåœ¨å®‰å…¨åˆ©ç”¨åŒ»ç–—æ•°æ®æ–¹é¢å–å¾—äº†è¿›å±•ã€‚BUSGençš„åœ¨çº¿æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://aibus.bioä¸Šæ‰¾åˆ°./">https://aibus.bioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06869v1">PDF</a> Peking University; Stanford University; Peking University Cancer   Hospital &amp; Institute; Peking Union Medical College Hospital; Cancer Hospital,   Chinese Academy of Medical Sciences</p>
<p><strong>Summary</strong></p>
<p>BUSGenæ˜¯ç¬¬ä¸€æ¬¾ä¸“ä¸ºä¹³è…ºè¶…å£°å›¾åƒåˆ†æè®¾è®¡çš„é€šç”¨ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹é¢„è®­ç»ƒåœ¨è¶…è¿‡3.5ä¸‡ä¾‹ä¹³è…ºè¶…å£°å›¾åƒä¸Šï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚å®ƒèƒ½å¤Ÿåœ¨å°‘é‡æ ·æœ¬é€‚åº”ä¸‹ç”Ÿæˆç°å®ä¸”å…·æœ‰ä¿¡æ¯æ€§çš„ä»»åŠ¡ç‰¹å®šæ•°æ®ï¼Œä¿ƒè¿›äº†å„ç§ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹çš„å‘å±•ã€‚å¯¹äºä¹³è…ºç™Œç­›æŸ¥ã€è¯Šæ–­åŠé¢„åç­‰æ–¹é¢ï¼ŒBUSGençš„è¡¨ç°æä¸ºå‡ºè‰²ï¼Œå…¶é€‚åº”æ€§æ˜¾è‘—è¶…è¿‡äº†ç”±çœŸå®æ•°æ®è®­ç»ƒå‡ºçš„é€šç”¨æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜èƒ½æœ‰æ•ˆä¿æŠ¤æ‚£è€…éšç§ï¼Œæ¨åŠ¨å®‰å…¨åŒ»ç–—æ•°æ®çš„åˆ©ç”¨ã€‚ç›®å‰å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://aibus.bioåœ¨çº¿ä½“éªŒ./">https://aibus.bioåœ¨çº¿ä½“éªŒã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BUSGenæ˜¯ä¸“ä¸ºä¹³è…ºè¶…å£°å›¾åƒåˆ†æè®¾è®¡çš„é¦–æ¬¾é€šç”¨ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>é¢„è®­ç»ƒåœ¨å¤§é‡ä¹³è…ºè¶…å£°å›¾åƒä¸Šï¼Œæ¶µç›–å¹¿æ³›çš„ä¹³è…ºç»“æ„ã€ç—…ç†ç‰¹å¾å’Œä¸´åºŠå˜åŒ–çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡å°‘é‡æ ·æœ¬é€‚åº”ï¼Œèƒ½ç”Ÿæˆå…·æœ‰ç°å®æ€§å’Œä¿¡æ¯æ€§çš„ä»»åŠ¡ç‰¹å®šæ•°æ®ã€‚</li>
<li>åœ¨ä¹³è…ºç™Œç­›æŸ¥ã€è¯Šæ–­å’Œé¢„åæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šçœŸå®æ•°æ®è®­ç»ƒçš„é€šç”¨æ¨¡å‹ã€‚</li>
<li>æœ‰æ•ˆæé«˜ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¿æŠ¤æ‚£è€…éšç§ï¼Œæ¨åŠ¨å®‰å…¨åŒ»ç–—æ•°æ®çš„åˆ©ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7322cf4afa33c26ffecfb09a1d8995be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c2c0bdda179f08501d8ef8cfa3d4b5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd0fc013160fe061c51062e2d26588ab.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="UR2P-Dehaze-Learning-a-Simple-Image-Dehaze-Enhancer-via-Unpaired-Rich-Physical-Prior"><a href="#UR2P-Dehaze-Learning-a-Simple-Image-Dehaze-Enhancer-via-Unpaired-Rich-Physical-Prior" class="headerlink" title="UR2P-Dehaze: Learning a Simple Image Dehaze Enhancer via Unpaired Rich   Physical Prior"></a>UR2P-Dehaze: Learning a Simple Image Dehaze Enhancer via Unpaired Rich   Physical Prior</h2><p><strong>Authors:Minglong Xue, Shuaibin Fan, Shivakumara Palaiahnakote, Mingliang Zhou</strong></p>
<p>Image dehazing techniques aim to enhance contrast and restore details, which are essential for preserving visual information and improving image processing accuracy. Existing methods rely on a single manual prior, which cannot effectively reveal image details. To overcome this limitation, we propose an unpaired image dehazing network, called the Simple Image Dehaze Enhancer via Unpaired Rich Physical Prior (UR2P-Dehaze). First, to accurately estimate the illumination, reflectance, and color information of the hazy image, we design a shared prior estimator (SPE) that is iteratively trained to ensure the consistency of illumination and reflectance, generating clear, high-quality images. Additionally, a self-monitoring mechanism is introduced to eliminate undesirable features, providing reliable priors for image reconstruction. Next, we propose Dynamic Wavelet Separable Convolution (DWSC), which effectively integrates key features across both low and high frequencies, significantly enhancing the preservation of image details and ensuring global consistency. Finally, to effectively restore the color information of the image, we propose an Adaptive Color Corrector that addresses the problem of unclear colors. The PSNR, SSIM, LPIPS, FID and CIEDE2000 metrics on the benchmark dataset show that our method achieves state-of-the-art performance. It also contributes to the performance improvement of downstream tasks. The project code will be available at <a target="_blank" rel="noopener" href="https://github.com/Fan-pixel/UR2P-Dehaze">https://github.com/Fan-pixel/UR2P-Dehaze</a>. \end{abstract} </p>
<blockquote>
<p>å›¾åƒå»é›¾æŠ€æœ¯æ—¨åœ¨æé«˜å¯¹æ¯”åº¦å’Œæ¢å¤ç»†èŠ‚ï¼Œè¿™å¯¹äºä¿ç•™è§†è§‰ä¿¡æ¯ã€æé«˜å›¾åƒå¤„ç†ç²¾åº¦è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå•ä¸€çš„äººå·¥å…ˆéªŒï¼Œæ— æ³•æœ‰æ•ˆæ­ç¤ºå›¾åƒç»†èŠ‚ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éé…å¯¹å›¾åƒå»é›¾ç½‘ç»œï¼Œç§°ä¸ºé€šè¿‡éé…å¯¹ä¸°å¯Œç‰©ç†å…ˆéªŒçš„ç®€å•å›¾åƒå»é›¾å¢å¼ºå™¨ï¼ˆUR2P-Dehazeï¼‰ã€‚é¦–å…ˆï¼Œä¸ºäº†å‡†ç¡®ä¼°è®¡é›¾å›¾åƒçš„ç…§æ˜ã€åå°„å’Œé¢œè‰²ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå…±äº«å…ˆéªŒä¼°è®¡å™¨ï¼ˆSPEï¼‰ï¼Œé€šè¿‡è¿­ä»£è®­ç»ƒç¡®ä¿ç…§æ˜å’Œåå°„çš„ä¸€è‡´æ€§ï¼Œç”Ÿæˆæ¸…æ™°çš„é«˜è´¨é‡å›¾åƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘ç›‘æ§æœºåˆ¶ï¼Œä»¥æ¶ˆé™¤ä¸è‰¯ç‰¹å¾ï¼Œä¸ºå›¾åƒé‡å»ºæä¾›å¯é çš„å…ˆéªŒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å°æ³¢å¯åˆ†ç¦»å·ç§¯ï¼ˆDWSCï¼‰ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆä½é¢‘å’Œé«˜é¢‘çš„å…³é”®ç‰¹å¾ï¼Œæ˜¾è‘—å¢å¼ºå›¾åƒç»†èŠ‚çš„ä¿ç•™ï¼Œå¹¶ç¡®ä¿å…¨å±€ä¸€è‡´æ€§ã€‚æœ€åï¼Œä¸ºäº†æœ‰æ•ˆæ¢å¤å›¾åƒçš„é¢œè‰²ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”é¢œè‰²æ ¡æ­£å™¨ï¼Œè§£å†³äº†é¢œè‰²ä¸æ¸…æ™°çš„é—®é¢˜ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„PSNRã€SSIMã€LPIPSã€FIDå’ŒCIEDE2000æŒ‡æ ‡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä¸ºä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½æ”¹è¿›åšå‡ºäº†è´¡çŒ®ã€‚é¡¹ç›®ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Fan-pixel/UR2P-Dehaze%E4%B8%8A%E6%8F%AD%E6%9C%AC%E5%AF%BC%E5%85%A5%E3%80%82">https://github.com/Fan-pixel/UR2P-Dehazeä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06818v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸ºUR2P-Dehazeçš„å»é›¾ç½‘ç»œã€‚è¯¥ç½‘ç»œæ—¨åœ¨æé«˜å›¾åƒçš„å¯¹æ¯”åº¦å’Œç»†èŠ‚æ¢å¤èƒ½åŠ›ï¼Œé€šè¿‡è®¾è®¡å…±äº«å…ˆéªŒä¼°è®¡å™¨ï¼ˆSPEï¼‰å’ŒåŠ¨æ€å°æ³¢å¯åˆ†ç¦»å·ç§¯ï¼ˆDWSCï¼‰ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œæé«˜äº†å»é›¾æ•ˆæœã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”é¢œè‰²æ ¡æ­£å™¨æ¥è§£å†³é¢œè‰²ä¸æ¸…æ™°çš„é—®é¢˜ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„PSNRã€SSIMã€LPIPSã€FIDå’ŒCIEDE2000ç­‰æŒ‡æ ‡è¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å»é›¾æŠ€æœ¯æ—¨åœ¨æé«˜å›¾åƒçš„å¯¹æ¯”åº¦å’Œç»†èŠ‚æ¢å¤èƒ½åŠ›ï¼Œå¯¹äºä¿æŠ¤è§†è§‰ä¿¡æ¯å’Œæé«˜å›¾åƒå¤„ç†ç²¾åº¦è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºå•ä¸€çš„æ‰‹åŠ¨å…ˆéªŒï¼Œæ— æ³•æœ‰æ•ˆæ­ç¤ºå›¾åƒç»†èŠ‚ã€‚</li>
<li>UR2P-Dehazeç½‘ç»œé€šè¿‡è®¾è®¡å…±äº«å…ˆéªŒä¼°è®¡å™¨ï¼ˆSPEï¼‰æ¥å‡†ç¡®ä¼°è®¡é›¾éœ¾å›¾åƒçš„ç…§æ˜ã€åå°„å’Œé¢œè‰²ä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è‡ªæˆ‘ç›‘æ§æœºåˆ¶ï¼Œä»¥æ¶ˆé™¤ä¸è‰¯ç‰¹å¾ï¼Œä¸ºå›¾åƒé‡å»ºæä¾›å¯é çš„å…ˆéªŒã€‚</li>
<li>åŠ¨æ€å°æ³¢å¯åˆ†ç¦»å·ç§¯ï¼ˆDWSCï¼‰èƒ½æœ‰æ•ˆæ•´åˆé«˜ä½é¢‘çš„å…³é”®ç‰¹å¾ï¼Œå¢å¼ºå›¾åƒç»†èŠ‚çš„ä¿æŠ¤å’Œå…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>UR2P-Dehazeé€šè¿‡è‡ªé€‚åº”é¢œè‰²æ ¡æ­£å™¨è§£å†³äº†é¢œè‰²ä¸æ¸…æ™°çš„é—®é¢˜ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤šé¡¹æŒ‡æ ‡è¡¨æ˜ï¼ŒUR2P-Dehazeæ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1bf002b7743d57a5f6e2fcaad86b0e04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d72effebb4ce3c7198a6dbb93220dd1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e5dce7ea3d586273335c442e316c794.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Improved-joint-modelling-of-breast-cancer-radiomics-features-and-hazard-by-image-registration-aided-longitudinal-CT-data"><a href="#Improved-joint-modelling-of-breast-cancer-radiomics-features-and-hazard-by-image-registration-aided-longitudinal-CT-data" class="headerlink" title="Improved joint modelling of breast cancer radiomics features and hazard   by image registration aided longitudinal CT data"></a>Improved joint modelling of breast cancer radiomics features and hazard   by image registration aided longitudinal CT data</h2><p><strong>Authors:Subrata Mukherjee</strong></p>
<p>Patients with metastatic breast cancer (mBC) undergo continuous medical imaging during treatment, making accurate lesion detection and monitoring over time critical for clinical decisions. Predicting drug response from post-treatment data is essential for personalized care and pharmacological research. In collaboration with the U.S. Food and Drug Administration and Novartis Pharmaceuticals, we analyzed serial chest CT scans from two large-scale Phase III trials, MONALEESA 3 and MONALEESA 7. This paper has two objectives (a) Data Structuring developing a Registration Aided Automated Correspondence (RAMAC) algorithm for precise lesion tracking in longitudinal CT data, and (b) Survival Analysis creating imaging features and models from RAMAC structured data to predict patient outcomes. The RAMAC algorithm uses a two phase pipeline: three dimensional rigid registration aligns CT images, and a distance metric-based Hungarian algorithm tracks lesion correspondence. Using structured data, we developed interpretable models to assess progression-free survival (PFS) in mBC patients by combining baseline radiomics, post-treatment changes (Weeks 8, 16, 24), and demographic features. Radiomics effects were studied across time points separately and through a non-correlated additive framework. Radiomics features were reduced using (a) a regularized (L1-penalized) additive Cox proportional hazards model, and (b) variable selection via best subset selection. Performance, measured using the concordance index (C-index), improved with additional time points. Joint modeling, considering correlations among radiomics effects over time, provided insights into relationships between longitudinal radiomics and survival outcomes. </p>
<blockquote>
<p>æ‚£æœ‰è½¬ç§»æ€§ä¹³è…ºç™Œï¼ˆmBCï¼‰çš„æ‚£è€…åœ¨æ²»ç–—è¿‡ç¨‹ä¸­ä¼šç»å†æŒç»­æ€§çš„åŒ»å­¦å½±åƒæ£€æŸ¥ï¼Œå› æ­¤å¯¹ç—…ç¶è¿›è¡Œå‡†ç¡®çš„æ£€æµ‹å’Œé•¿æ—¶é—´çš„ç›‘æµ‹å¯¹ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚ä»æ²»ç–—åçš„æ•°æ®ä¸­é¢„æµ‹è¯ç‰©ååº”å¯¹äºä¸ªæ€§åŒ–æŠ¤ç†å’Œè¯ç‰©ç ”ç©¶è‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä¸ç¾å›½é£Ÿå“è¯å“ç›‘ç£ç®¡ç†å±€ï¼ˆFDAï¼‰å’Œè¯ºååˆ¶è¯å…¬å¸åˆä½œï¼Œåˆ†æäº†æ¥è‡ªä¸¤é¡¹å¤§è§„æ¨¡IIIæœŸè¯•éªŒï¼ˆMONALEESA 3å’ŒMONALEESA 7ï¼‰çš„è¿ç»­èƒ¸éƒ¨CTæ‰«ææ•°æ®ã€‚æœ¬æ–‡æœ‰ä¸¤ä¸ªç›®æ ‡ï¼šï¼ˆaï¼‰å¼€å‘ä¸€ç§åä¸ºæ³¨å†Œè¾…åŠ©è‡ªåŠ¨å¯¹åº”ï¼ˆRAMACï¼‰çš„ç®—æ³•ï¼Œç”¨äºç²¾ç¡®è¿½è¸ªçºµå‘CTæ•°æ®ä¸­çš„ç—…ç¶ï¼›ï¼ˆbï¼‰åˆ©ç”¨RAMACç»“æ„åŒ–æ•°æ®è¿›è¡Œç”Ÿå­˜åˆ†æï¼Œä»¥é¢„æµ‹æ‚£è€…çš„æ²»ç–—ç»“æœã€‚RAMACç®—æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæµç¨‹ï¼šä¸‰ç»´åˆšæ€§æ³¨å†Œå¯¹é½CTå›¾åƒï¼ŒåŸºäºè·ç¦»åº¦é‡çš„åŒˆç‰™åˆ©ç®—æ³•è·Ÿè¸ªç—…ç¶å¯¹åº”å…³ç³»ã€‚æˆ‘ä»¬ä½¿ç”¨ç»“æ„åŒ–æ•°æ®ï¼Œç»“åˆåŸºçº¿æ”¾å°„ç»„å­¦ã€æ²»ç–—åå˜åŒ–ï¼ˆç¬¬8å‘¨ã€ç¬¬16å‘¨ã€ç¬¬24å‘¨ï¼‰å’Œäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œå¼€å‘äº†å¯è§£é‡Šæ¨¡å‹æ¥è¯„ä¼°mBCæ‚£è€…çš„æ— è¿›å±•ç”Ÿå­˜æœŸï¼ˆPFSï¼‰ã€‚æ”¾å°„ç»„å­¦çš„å½±å“æ˜¯åˆ†åˆ«åœ¨ä¸åŒæ—¶é—´ç‚¹ä»¥åŠé€šè¿‡éç›¸å…³é™„åŠ æ¡†æ¶è¿›è¡Œç ”ç©¶çš„ã€‚ä½¿ç”¨ï¼ˆaï¼‰æ­£åˆ™åŒ–ï¼ˆL1æƒ©ç½šï¼‰é™„åŠ Coxæ¯”ä¾‹é£é™©æ¨¡å‹å’Œï¼ˆbï¼‰é€šè¿‡æœ€ä½³å­é›†é€‰æ‹©è¿›è¡Œå˜é‡é€‰æ‹©æ¥å‡å°‘æ”¾å°„å­¦ç‰¹å¾ã€‚é€šè¿‡å¢åŠ æ—¶é—´ç‚¹ï¼Œä»¥ä¸€è‡´æ€§æŒ‡æ•°ï¼ˆCæŒ‡æ•°ï¼‰è¡¡é‡çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚è”åˆå»ºæ¨¡ï¼Œè€ƒè™‘åˆ°æ”¾å°„å­¦æ•ˆåº”åœ¨æ—¶é—´ä¸Šçš„ç›¸å…³æ€§ï¼Œæä¾›äº†çºµå‘æ”¾å°„ç»„å­¦å’Œç”Ÿå­˜ç»“æœä¹‹é—´å…³ç³»çš„æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06814v1">PDF</a> 20 pages, 13 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹è½¬ç§»æ€§ä¹³è…ºç™Œæ‚£è€…çš„æ²»ç–—è¿‡ç¨‹ä¸­çš„åŒ»å­¦å½±åƒåˆ†æï¼Œæå‡ºäº†ä¸¤å¤§ç›®æ ‡ï¼šä¸€æ˜¯å¼€å‘æ³¨å†Œè¾…åŠ©è‡ªåŠ¨å¯¹åº”ç®—æ³•ï¼ˆRAMACï¼‰ï¼Œç”¨äºç²¾ç¡®è¿½è¸ªçºµå‘CTæ•°æ®ä¸­çš„ç—…ç¶ï¼›äºŒæ˜¯åˆ©ç”¨RAMACç»“æ„åŒ–æ•°æ®è¿›è¡Œç”Ÿå­˜åˆ†æï¼Œé¢„æµ‹æ‚£è€…é¢„åã€‚ç ”ç©¶é€šè¿‡ç»“åˆåŸºçº¿æ”¾å°„å­¦ç‰¹å¾ã€æ²»ç–—åçš„å˜åŒ–ä»¥åŠäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œè¯„ä¼°äº†æ— è¿›å±•ç”Ÿå­˜æœŸã€‚é€šè¿‡å»ºæ¨¡åˆ†æï¼Œè€ƒè™‘æ”¾å°„å­¦æ•ˆåº”åœ¨æ—¶é—´ä¸Šçš„ç›¸å…³æ€§ï¼Œæ­ç¤ºäº†çºµå‘æ”¾å°„å­¦ä¸ç”Ÿå­˜æœŸç»“æœä¹‹é—´çš„å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹è½¬ç§»æ€§ä¹³è…ºç™Œï¼ˆmBCï¼‰æ‚£è€…çš„æ²»ç–—è¿‡ç¨‹è¿›è¡ŒåŒ»å­¦å½±åƒåˆ†æï¼Œæ¶‰åŠä¸¤å¤§ç›®æ ‡ï¼šæ•°æ®ç»“æ„åŒ–å’Œç”Ÿå­˜åˆ†æã€‚</li>
<li>å¼€å‘æ³¨å†Œè¾…åŠ©è‡ªåŠ¨å¯¹åº”ç®—æ³•ï¼ˆRAMACï¼‰è¿›è¡Œç²¾ç¡®ç—…ç¶è¿½è¸ªã€‚</li>
<li>RAMACç®—æ³•é‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“å¤„ç†ï¼ŒåŒ…æ‹¬ä¸‰ç»´åˆšæ€§æ³¨å†Œå¯¹é½CTå›¾åƒå’Œä½¿ç”¨åŸºäºè·ç¦»åº¦é‡çš„åŒˆç‰™åˆ©ç®—æ³•è¿½è¸ªç—…ç¶å¯¹åº”å…³ç³»ã€‚</li>
<li>ç»“åˆåŸºçº¿æ”¾å°„å­¦ç‰¹å¾ã€æ²»ç–—åçš„å˜åŒ–å’Œäººå£ç»Ÿè®¡å­¦ç‰¹å¾ï¼Œè¯„ä¼°æ— è¿›å±•ç”Ÿå­˜æœŸï¼ˆPFSï¼‰ã€‚</li>
<li>æ”¾å°„å­¦ç‰¹å¾çš„å½±å“åœ¨ä¸åŒæ—¶é—´ç‚¹è¢«å•ç‹¬ä»¥åŠé€šè¿‡ä¸€ä¸ªéç›¸å…³æ·»åŠ æ¡†æ¶è¿›è¡Œç ”ç©¶ã€‚</li>
<li>é€šè¿‡æ­£åˆ™åŒ–ï¼ˆL1æƒ©ç½šï¼‰çš„é™„åŠ Coxæ¯”ä¾‹é£é™©æ¨¡å‹å’Œæœ€ä½³å­é›†é€‰æ‹©è¿›è¡Œå˜é‡é€‰æ‹©ï¼Œä»¥é™ä½æ”¾å°„å­¦ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cae2222d1aaba07e56bb645959c45e59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-865234cf79296ef35fb6e45844a8f864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42d406a2e7f1e2dab36b1083ba075dd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f29f7a85f75f0e5033f011eaee2f2f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-160818025f45b0819741e9deac811788.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-483748fc5193b8bd62aefc120e463b65.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RSRefSeg-Referring-Remote-Sensing-Image-Segmentation-with-Foundation-Models"><a href="#RSRefSeg-Referring-Remote-Sensing-Image-Segmentation-with-Foundation-Models" class="headerlink" title="RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation   Models"></a>RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation   Models</h2><p><strong>Authors:Keyan Chen, Jiafan Zhang, Chenyang Liu, Zhengxia Zou, Zhenwei Shi</strong></p>
<p>Referring remote sensing image segmentation is crucial for achieving fine-grained visual understanding through free-format textual input, enabling enhanced scene and object extraction in remote sensing applications. Current research primarily utilizes pre-trained language models to encode textual descriptions and align them with visual modalities, thereby facilitating the expression of relevant visual features. However, these approaches often struggle to establish robust alignments between fine-grained semantic concepts, leading to inconsistent representations across textual and visual information. To address these limitations, we introduce a referring remote sensing image segmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual and textual encoding, employing both global and local textual semantics as filters to generate referring-related visual activation features in the latent space. These activated features then serve as input prompts for SAM, which refines the segmentation masks through its robust visual generalization capabilities. Experimental results on the RRSIS-D dataset demonstrate that RSRefSeg outperforms existing methods, underscoring the effectiveness of foundational models in enhancing multimodal task comprehension. The code is available at \url{<a target="_blank" rel="noopener" href="https://github.com/KyanChen/RSRefSeg%7D">https://github.com/KyanChen/RSRefSeg}</a>. </p>
<blockquote>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºé€šè¿‡è‡ªç”±æ ¼å¼çš„æ–‡æœ¬è¾“å…¥å®ç°ç²¾ç»†çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿåœ¨é¥æ„Ÿåº”ç”¨ä¸­å®ç°å¢å¼ºçš„åœºæ™¯å’Œå¯¹è±¡æå–ã€‚ç›®å‰çš„ç ”ç©¶ä¸»è¦åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬æè¿°è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å…¶ä¸è§†è§‰æ¨¡å¼å¯¹é½ï¼Œä»è€Œä¿ƒè¿›ç›¸å…³è§†è§‰ç‰¹å¾çš„è¡¨è¾¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å»ºç«‹ç²¾ç»†è¯­ä¹‰æ¦‚å¿µä¹‹é—´çš„ç¨³å¥å¯¹é½æ—¶å¾€å¾€é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ä¹‹é—´çš„è¡¨ç¤ºä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹RSRefSegã€‚RSRefSegåˆ©ç”¨CLIPè¿›è¡Œè§†è§‰å’Œæ–‡æœ¬ç¼–ç ï¼Œé‡‡ç”¨å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬è¯­ä¹‰ä½œä¸ºè¿‡æ»¤å™¨æ¥ç”Ÿæˆç›¸å…³çš„è§†è§‰æ¿€æ´»ç‰¹å¾ã€‚è¿™äº›æ¿€æ´»ç‰¹å¾éšåä½œä¸ºSAMçš„è¾“å…¥æç¤ºï¼Œé€šè¿‡å…¶å¼ºå¤§çš„è§†è§‰æ³›åŒ–èƒ½åŠ›æ¥å®Œå–„åˆ†å‰²æ©è†œã€‚åœ¨RRSIS-Dæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRSRefSegä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†åŸºç¡€æ¨¡å‹åœ¨æé«˜å¤šæ¨¡æ€ä»»åŠ¡ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KyanChen/RSRefSeg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/KyanChen/RSRefSegä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06809v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºé€šè¿‡è‡ªç”±æ ¼å¼çš„æ–‡æœ¬è¾“å…¥å®ç°ç²¾ç»†çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºå¢å¼ºåœºæ™¯å’Œå¯¹è±¡çš„æå–ã€‚å½“å‰ç ”ç©¶ä¸»è¦åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬æè¿°è¿›è¡Œç¼–ç ï¼Œå¹¶å°†å…¶ä¸è§†è§‰æ¨¡å¼å¯¹é½ï¼Œä»è€Œè¡¨è¾¾ç›¸å…³çš„è§†è§‰ç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å»ºç«‹ç²¾ç»†è¯­ä¹‰æ¦‚å¿µä¹‹é—´çš„ç¨³å¥å¯¹é½æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´è·¨æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„è¡¨ç¤ºä¸ä¸€è‡´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¥æ„Ÿå›¾åƒåˆ†å‰²åŸºç¡€æ¨¡å‹RSRefSegã€‚RSRefSegåˆ©ç”¨CLIPè¿›è¡Œè§†è§‰å’Œæ–‡æœ¬ç¼–ç ï¼Œé‡‡ç”¨å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬è¯­ä¹‰ä½œä¸ºè¿‡æ»¤å™¨ï¼Œç”Ÿæˆä¸å¼•ç”¨ç›¸å…³çš„è§†è§‰æ¿€æ´»ç‰¹å¾ã€‚è¿™äº›æ¿€æ´»ç‰¹å¾ä½œä¸ºSAMçš„è¾“å…¥æç¤ºï¼Œé€šè¿‡å…¶å¼ºå¤§çš„è§†è§‰æ³›åŒ–èƒ½åŠ›ç»†åŒ–åˆ†å‰²æ©è†œã€‚åœ¨RRSIS-Dæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRSRefSegä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†åŸºç¡€æ¨¡å‹åœ¨å¢å¼ºå¤šæ¨¡å¼ä»»åŠ¡ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²å¯¹äºç²¾ç»†çš„è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºåœºæ™¯å’Œå¯¹è±¡çš„å¢å¼ºæå–ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦åˆ©ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬å’Œè§†è§‰çš„ç¼–ç ä¸å¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å»ºç«‹ç²¾ç»†è¯­ä¹‰æ¦‚å¿µä¹‹é—´çš„ç¨³å¥å¯¹é½ï¼Œå¯¼è‡´è·¨æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯çš„è¡¨ç¤ºä¸ä¸€è‡´ã€‚</li>
<li>å¼•å…¥çš„RSRefSegæ¨¡å‹åˆ©ç”¨CLIPè¿›è¡Œè§†è§‰å’Œæ–‡æœ¬ç¼–ç ã€‚</li>
<li>RSRefSegé‡‡ç”¨å…¨å±€å’Œå±€éƒ¨æ–‡æœ¬è¯­ä¹‰ä½œä¸ºè¿‡æ»¤å™¨ï¼Œç”Ÿæˆä¸å¼•ç”¨ç›¸å…³çš„è§†è§‰æ¿€æ´»ç‰¹å¾ã€‚</li>
<li>è¿™äº›æ¿€æ´»ç‰¹å¾è¢«ç”¨ä½œSAMçš„è¾“å…¥æç¤ºï¼Œä»¥ç»†åŒ–åˆ†å‰²æ©è†œã€‚</li>
<li>åœ¨RRSIS-Dæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒRSRefSegæ¨¡å‹ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†åŸºç¡€æ¨¡å‹åœ¨å¢å¼ºå¤šæ¨¡å¼ä»»åŠ¡ç†è§£æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f2bb65f3b83d96cecbd7a8379685be0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c86a5b183f37a64a8a4f37c1a7346906.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57d9de03c71e94b21823131c039f3841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e9b46cd159e2be5ebdd23dfceb27e34.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-30bc66b2e02dd3b2a6acfb028c14f431.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  MathReader  Text-to-Speech for Mathematical Documents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f0630155f9c742358970c17709fdf691.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  MangaNinja Line Art Colorization with Precise Reference Following
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
