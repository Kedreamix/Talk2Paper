<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-01-16  Threshold Attention Network for Semantic Segmentation of Remote Sensing   Images">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-eda112a70646e26e626a9e100d78d313.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-16-更新"><a href="#2025-01-16-更新" class="headerlink" title="2025-01-16 更新"></a>2025-01-16 更新</h1><h2 id="Threshold-Attention-Network-for-Semantic-Segmentation-of-Remote-Sensing-Images"><a href="#Threshold-Attention-Network-for-Semantic-Segmentation-of-Remote-Sensing-Images" class="headerlink" title="Threshold Attention Network for Semantic Segmentation of Remote Sensing   Images"></a>Threshold Attention Network for Semantic Segmentation of Remote Sensing   Images</h2><p><strong>Authors:Wei Long, Yongjun Zhang, Zhongwei Cui, Yujie Xu, Xuexue Zhang</strong></p>
<p>Semantic segmentation of remote sensing images is essential for various applications, including vegetation monitoring, disaster management, and urban planning. Previous studies have demonstrated that the self-attention mechanism (SA) is an effective approach for designing segmentation networks that can capture long-range pixel dependencies. SA enables the network to model the global dependencies between the input features, resulting in improved segmentation outcomes. However, the high density of attentional feature maps used in this mechanism causes exponential increases in computational complexity. Additionally, it introduces redundant information that negatively impacts the feature representation. Inspired by traditional threshold segmentation algorithms, we propose a novel threshold attention mechanism (TAM). This mechanism significantly reduces computational effort while also better modeling the correlation between different regions of the feature map. Based on TAM, we present a threshold attention network (TANet) for semantic segmentation. TANet consists of an attentional feature enhancement module (AFEM) for global feature enhancement of shallow features and a threshold attention pyramid pooling module (TAPP) for acquiring feature information at different scales for deep features. We have conducted extensive experiments on the ISPRS Vaihingen and Potsdam datasets. The results demonstrate the validity and superiority of our proposed TANet compared to the most state-of-the-art models. </p>
<blockquote>
<p>遥感图像的语义分割对于各种应用至关重要，包括植被监测、灾害管理和城市规划。先前的研究表明，自注意力机制（SA）是设计分割网络的有效方法，能够捕获远程像素之间的依赖关系。自注意力机制使网络能够对输入特征之间的全局依赖关系进行建模，从而提高分割效果。然而，该机制中使用的注意力特征图的高密度导致计算复杂度呈指数增长。此外，它还引入了冗余信息，对特征表示产生负面影响。受传统阈值分割算法的启发，我们提出了一种新型阈值注意力机制（TAM）。该机制在降低计算成本的同时，更好地对特征图中不同区域的关联性进行建模。基于TAM，我们提出了用于语义分割的阈值注意力网络（TANet）。TANet包括一个注意力特征增强模块（AFEM），用于增强浅层特征的全局特征，以及一个阈值注意力金字塔池模块（TAPP），用于获取不同尺度的特征信息以供深度特征使用。我们在ISPRS Vaihingen和Potsdam数据集上进行了大量实验。结果表明，与我们提出的最新模型相比，我们所提出的TANet是有效和优越的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07984v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇文本介绍了遥感图像语义分割的重要性及其在各种应用中的应用，包括植被监测、灾害管理和城市规划。文章指出，自注意力机制（SA）在设计分割网络时能有效捕捉长距离像素依赖关系，提高分割效果。然而，SA的高密度注意力特征映射导致了计算复杂性的指数增长，并引入了影响特征表示的冗余信息。为此，本文提出了一种新型阈值注意力机制（TAM），显著降低了计算成本，更好地建模了特征映射不同区域之间的相关性。基于TAM，本文提出了用于语义分割的阈值注意力网络（TANet）。TANet包括注意力特征增强模块（AFEM）用于增强浅层特征的全局特征，以及阈值注意力金字塔池模块（TAPP）用于获取不同尺度的深层特征信息。在ISPRS Vaihingen和Potsdam数据集上的实验结果表明，本文提出的TANet模型与最先进的模型相比具有有效性和优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>遥感图像语义分割在植被监测、灾害管理和城市规划等多个领域有广泛应用。</li>
<li>自注意力机制（SA）能捕捉长距离像素依赖关系，提高分割效果，但计算复杂度高，存在冗余信息。</li>
<li>提出的新型阈值注意力机制（TAM）能显著降低计算成本，更有效地建模特征映射不同区域间的相关性。</li>
<li>基于TAM的阈值注意力网络（TANet）由注意力特征增强模块（AFEM）和阈值注意力金字塔池模块（TAPP）组成。</li>
<li>AFEM用于增强浅层特征的全局特征。</li>
<li>TAPP用于获取不同尺度的深层特征信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07984">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-271ab322defd97b608eb466f37198bca.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_检测_分割_跟踪/2501.07984v1/page_1_0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb2f27ddf02bb3e178ba0d468f0503eb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b84b5941b8edaee80c59c23dfcf8951.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-541ec1ac1c9dbc770ed919671be2ab61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-293a800f8a629d46db49754c446b79a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LarvSeg-Exploring-Image-Classification-Data-For-Large-Vocabulary-Semantic-Segmentation-via-Category-wise-Attentive-Classifier"><a href="#LarvSeg-Exploring-Image-Classification-Data-For-Large-Vocabulary-Semantic-Segmentation-via-Category-wise-Attentive-Classifier" class="headerlink" title="LarvSeg: Exploring Image Classification Data For Large Vocabulary   Semantic Segmentation via Category-wise Attentive Classifier"></a>LarvSeg: Exploring Image Classification Data For Large Vocabulary   Semantic Segmentation via Category-wise Attentive Classifier</h2><p><strong>Authors:Haojun Yu, Di Dai, Ziwei Zhao, Di He, Han Hu, Liwei Wang</strong></p>
<p>Scaling up the vocabulary of semantic segmentation models is extremely challenging because annotating large-scale mask labels is labour-intensive and time-consuming. Recently, language-guided segmentation models have been proposed to address this challenge. However, their performance drops significantly when applied to out-of-distribution categories. In this paper, we propose a new large vocabulary semantic segmentation framework, called LarvSeg. Different from previous works, LarvSeg leverages image classification data to scale the vocabulary of semantic segmentation models as large-vocabulary classification datasets usually contain balanced categories and are much easier to obtain. However, for classification tasks, the category is image-level, while for segmentation we need to predict the label at pixel level. To address this issue, we first propose a general baseline framework to incorporate image-level supervision into the training process of a pixel-level segmentation model, making the trained network perform semantic segmentation on newly introduced categories in the classification data. We then observe that a model trained on segmentation data can group pixel features of categories beyond the training vocabulary. Inspired by this finding, we design a category-wise attentive classifier to apply supervision to the precise regions of corresponding categories to improve the model performance. Extensive experiments demonstrate that LarvSeg significantly improves the large vocabulary semantic segmentation performance, especially in the categories without mask labels. For the first time, we provide a 21K-category semantic segmentation model with the help of ImageNet21K. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HaojunYu1998/large_voc_seg">https://github.com/HaojunYu1998/large_voc_seg</a>. </p>
<blockquote>
<p>扩充语义分割模型的词汇量极具挑战性，因为大规模掩膜标签的标注工作繁重且耗时。最近，语言引导分割模型被提出来应对这一挑战。然而，当应用到分布外的类别时，它们的性能会显著下降。在本文中，我们提出了一种新的大词汇量语义分割框架，称为LarvSeg。不同于以前的工作，LarvSeg利用图像分类数据来扩展语义分割模型的词汇量，因为大词汇量分类数据集通常包含平衡类别且更容易获取。然而，对于分类任务，类别是图像级别的，而对于分割，我们需要在像素级别预测标签。为了解决这一问题，我们首先提出一个通用的基线框架，将图像级别的监督纳入像素级别的分割模型的训练过程中，使训练好的网络在新引入的类别上进行语义分割。然后我们发现一个训练在分割数据上的模型可以汇集超出训练词汇量的类别像素特征。受此启发，我们设计了一个类别特定的注意力分类器，对相应类别的精确区域施加监督，以提高模型性能。大量实验表明，LarvSeg显著提高了大词汇量语义分割的性能，特别是在没有掩膜标签的类别中。我们首次借助ImageNet21K提供了一个包含21K类别的语义分割模型。代码可在<a target="_blank" rel="noopener" href="https://github.com/HaojunYu1998/large_voc_seg">https://github.com/HaojunYu1998/large_voc_seg</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06862v1">PDF</a> PRCV 2024</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的大规模词汇语义分割框架——LarvSeg。它通过结合图像分类数据来扩展语义分割模型的词汇量，解决了标注大规模掩膜标签的劳动密集和时间消耗问题。该框架通过使用图像级别的监督信息训练像素级别的分割模型，使得模型能够在分类数据中对新引入的类别进行语义分割。此外，还设计了一个类别感知的分类器，以提高模型性能。实验表明，LarvSeg在大量词汇的语义分割任务上显著提高性能，特别是在没有掩膜标签的类别上。该模型首次提供了使用ImageNet21K数据的21K类别语义分割模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LarvSeg利用图像分类数据来扩展语义分割模型的词汇量，解决大规模掩膜标签标注的挑战。</li>
<li>提出了一种结合图像级别监督信息训练像素级别分割模型的通用基线框架。</li>
<li>通过实验发现，在分割数据上训练的模型能够分组超出训练词汇量的类别像素特征。</li>
<li>受到了这一发现的启发，设计了一个类别感知的分类器，对相应类别的精确区域进行监管，以提高模型性能。</li>
<li>LarvSeg在大量词汇的语义分割任务上表现优异，特别是在没有掩膜标签的类别上。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7d4b96eefb38b8c96823fed42bb4c00f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4194e2582f4a377291f66e565b33d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e47c49573cad0e8a12bdacebe2ce1e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffbe9bba3b8c4b28d0a986d5d5aef2b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c88cf11aad2168cc9efe3394401eeb2a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CoreNet-Conflict-Resolution-Network-for-Point-Pixel-Misalignment-and-Sub-Task-Suppression-of-3D-LiDAR-Camera-Object-Detection"><a href="#CoreNet-Conflict-Resolution-Network-for-Point-Pixel-Misalignment-and-Sub-Task-Suppression-of-3D-LiDAR-Camera-Object-Detection" class="headerlink" title="CoreNet: Conflict Resolution Network for Point-Pixel Misalignment and   Sub-Task Suppression of 3D LiDAR-Camera Object Detection"></a>CoreNet: Conflict Resolution Network for Point-Pixel Misalignment and   Sub-Task Suppression of 3D LiDAR-Camera Object Detection</h2><p><strong>Authors:Yiheng Li, Yang Yang, Zhen Lei</strong></p>
<p>Fusing multi-modality inputs from different sensors is an effective way to improve the performance of 3D object detection. However, current methods overlook two important conflicts: point-pixel misalignment and sub-task suppression. The former means a pixel feature from the opaque object is projected to multiple point features of the same ray in the world space, and the latter means the classification prediction and bounding box regression may cause mutual suppression. In this paper, we propose a novel method named Conflict Resolution Network (CoreNet) to address the aforementioned issues. Specifically, we first propose a dual-stream transformation module to tackle point-pixel misalignment. It consists of ray-based and point-based 2D-to-BEV transformations. Both of them achieve approximately unique mapping from the image space to the world space. Moreover, we introduce a task-specific predictor to tackle sub-task suppression. It uses the dual-branch structure which adopts class-specific query and Bbox-specific query to corresponding sub-tasks. Each task-specific query is constructed of task-specific feature and general feature, which allows the heads to adaptively select information of interest based on different sub-tasks. Experiments on the large-scale nuScenes dataset demonstrate the superiority of our proposed CoreNet, by achieving 75.6% NDS and 73.3% mAP on the nuScenes test set without test-time augmentation and model ensemble techniques. The ample ablation study also demonstrates the effectiveness of each component. The code is released on <a target="_blank" rel="noopener" href="https://github.com/liyih/CoreNet">https://github.com/liyih/CoreNet</a>. </p>
<blockquote>
<p>融合不同传感器的多模态输入是提高3D目标检测性能的有效途径。然而，当前的方法忽略了两个重要的冲突：点像素不对齐和子任务抑制。前者意味着来自不透明对象的像素特征被投射到世界空间中的同一射线的多个点特征上，后者则意味着分类预测和边界框回归可能会相互抑制。在本文中，我们提出了一种名为Conflict Resolution Network（CoreNet）的新方法来解决上述问题。具体来说，我们首先提出了一个双流转换模块来解决点像素不对齐问题。它包括基于射线和基于点的2D-to-BEV转换。它们都实现了从图像空间到世界空间的近似唯一映射。此外，我们引入了一个针对子任务抑制的特定任务预测器。它采用双分支结构，采用类特定查询和Bbox特定查询来对应子任务。每个特定任务查询由特定任务特征和一般特征构成，这使得头部能够根据不同的子任务自适应地选择感兴趣的信息。在大型nuScenes数据集上的实验表明，我们提出的CoreNet具有优越性，在nuScenes测试集上实现了75.6%的NDS和73.3%的mAP，无需测试时间增强和模型集成技术。丰富的消融研究也证明了每个组件的有效性。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/liyih/CoreNet%E3%80%82">https://github.com/liyih/CoreNet。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06550v1">PDF</a> Accepted by Information Fusion 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为Conflict Resolution Network (CoreNet)的新方法，解决了多模态输入融合在3D目标检测中的两大冲突问题：点像素错位和子任务抑制。通过引入双流转换模块和针对任务的预测器，实现了对这两个问题的有效处理，提高了3D目标检测的性能。在nuScenes数据集上的实验证明了CoreNet的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态输入融合是提高3D目标检测性能的有效方式。</li>
<li>当前方法忽略了点像素错位和子任务抑制两大冲突。</li>
<li>CoreNet通过引入双流转换模块解决点像素错位问题。</li>
<li>CoreNet采用任务特定预测器解决子任务抑制问题。</li>
<li>双流转换模块包括基于射线和点基的2D-to-BEV转换，实现了从图像空间到世界空间的近似唯一映射。</li>
<li>CoreNet在nuScenes数据集上的实验表现出优越性，达到了75.6%的NDS和73.3%的mAP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06550">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-456a7ac80290e3cd11eee045c74e804b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8a8bb5fed2e6a32b2d2998fce27bef9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Feedback-driven-object-detection-and-iterative-model-improvement"><a href="#Feedback-driven-object-detection-and-iterative-model-improvement" class="headerlink" title="Feedback-driven object detection and iterative model improvement"></a>Feedback-driven object detection and iterative model improvement</h2><p><strong>Authors:Sönke Tenckhoff, Mario Koddenbrock, Erik Rodner</strong></p>
<p>Automated object detection has become increasingly valuable across diverse applications, yet efficient, high-quality annotation remains a persistent challenge. In this paper, we present the development and evaluation of a platform designed to interactively improve object detection models. The platform allows uploading and annotating images as well as fine-tuning object detection models. Users can then manually review and refine annotations, further creating improved snapshots that are used for automatic object detection on subsequent image uploads - a process we refer to as semi-automatic annotation resulting in a significant gain in annotation efficiency.   Whereas iterative refinement of model results to speed up annotation has become common practice, we are the first to quantitatively evaluate its benefits with respect to time, effort, and interaction savings. Our experimental results show clear evidence for a significant time reduction of up to 53% for semi-automatic compared to manual annotation. Importantly, these efficiency gains did not compromise annotation quality, while matching or occasionally even exceeding the accuracy of manual annotations. These findings demonstrate the potential of our lightweight annotation platform for creating high-quality object detection datasets and provide best practices to guide future development of annotation platforms.   The platform is open-source, with the frontend and backend repositories available on GitHub (<a target="_blank" rel="noopener" href="https://github.com/ml-lab-htw/iterative-annotate">https://github.com/ml-lab-htw/iterative-annotate</a>). To support the understanding of our labeling process, we have created an explanatory video demonstrating the methodology using microscopy images of E. coli bacteria as an example. The video is available on YouTube (<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=CM9uhE8NN5E">https://www.youtube.com/watch?v=CM9uhE8NN5E</a>). </p>
<blockquote>
<p>自动目标检测在不同应用中的价值日益凸显，然而高效、高质量的标注仍然是一个持续的挑战。在本文中，我们介绍了一个交互式改进目标检测模型的平台的开发评估。该平台允许上传和标注图像，以及微调目标检测模型。用户随后可以手动审查和修改标注，进一步创建改进的快照，用于后续图像上传的自动目标检测——我们称之为半自动标注，这大大提高了标注效率。虽然通过迭代优化模型结果来加速标注已成为一种常见做法，但我们是首次就时间、精力和交互节省等方面定量评估其效益。我们的实验结果表明，与手动标注相比，半自动标注的时间减少了高达53%。重要的是，这些效率的提升并没有损害标注质量，甚至在某些情况下还超过了手动标注的准确度。这些发现证明了我们轻量级标注平台的潜力，可用于创建高质量的目标检测数据集，并为未来标注平台的发展提供了最佳实践指导。平台是开源的，前端和后端仓库可在GitHub上找到（<a target="_blank" rel="noopener" href="https://github.com/ml-lab-htw/iterative-annotate%EF%BC%89%E3%80%82%E4%B8%BA%E4%BA%86%E6%94%AF%E6%8C%81%E5%AF%B9%E6%A0%87%E6%B3%A8%E8%BF%87%E7%A8%8B%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%8C%E6%88%91%E4%BB%AC%E5%88%B6%E4%BD%9C%E4%BA%86%E4%B8%80%E4%B8%AA%E8%A7%A3%E9%87%8A%E6%80%A7%E8%A7%86%E9%A2%91%EF%BC%8C%E4%BD%BF%E7%94%A8%E5%A4%A7%E8%82%A0%E6%9D%86%E8%8F%8C%E6%98%BE%E5%BE%AE%E9%95%9C%E5%9B%BE%E5%83%8F%E4%BD%9C%E4%B8%BA%E7%A4%BA%E4%BE%8B%E6%9D%A5%E5%B1%95%E7%A4%BA%E6%96%B9%E6%B3%95%E3%80%82%E8%A7%86%E9%A2%91%E5%8F%AF%E5%9C%A8YouTube%E4%B8%8A%E8%A7%82%E7%9C%8B%EF%BC%88https://www.youtube.com/watch?v=CM9uhE8NN5E%EF%BC%89%E3%80%82">https://github.com/ml-lab-htw/iterative-annotate）。为了支持对标注过程的理解，我们制作了一个解释性视频，使用大肠杆菌显微镜图像作为示例来展示方法。视频可在YouTube上观看（https://www.youtube.com/watch?v=CM9uhE8NN5E）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19835v2">PDF</a> AI4EA24</p>
<p><strong>Summary</strong>：本研究开发并评估了一个交互式平台，用于提高物体检测模型的性能。该平台支持图像上传、标注和模型微调。用户可手动审查和修正标注，创建改进后的快照用于后续自动物体检测。通过半自动标注，显著提高了标注效率，减少了时间和交互成本，同时保证了标注质量。平台开源，并提供相关视频教程支持理解标注过程。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>该平台旨在提高物体检测模型的性能，支持图像上传、标注和模型微调。</li>
<li>用户可以手动审查和修正标注，进一步改进自动物体检测效果。</li>
<li>平台通过半自动标注显著提高了标注效率，减少了时间和交互成本。</li>
<li>实验结果表明，半自动标注与传统手动标注相比，时间可减少高达53%。</li>
<li>平台在保证标注质量的同时实现了效率的提升，甚至在某些情况下超过了手动标注的准确度。</li>
<li>平台开源，并提供GitHub和YouTube上的相关资源以供学习和使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19835">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9c3898778d74101f9ed5a2eba7705e43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06d40879c9a9ceb8bf640acf76ca66c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e99e056e486b130ed5c012ae102fbf7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Rethinking-Decoders-for-Transformer-based-Semantic-Segmentation-A-Compression-Perspective"><a href="#Rethinking-Decoders-for-Transformer-based-Semantic-Segmentation-A-Compression-Perspective" class="headerlink" title="Rethinking Decoders for Transformer-based Semantic Segmentation: A   Compression Perspective"></a>Rethinking Decoders for Transformer-based Semantic Segmentation: A   Compression Perspective</h2><p><strong>Authors:Qishuai Wen, Chun-Guang Li</strong></p>
<p>State-of-the-art methods for Transformer-based semantic segmentation typically adopt Transformer decoders that are used to extract additional embeddings from image embeddings via cross-attention, refine either or both types of embeddings via self-attention, and project image embeddings onto the additional embeddings via dot-product. Despite their remarkable success, these empirical designs still lack theoretical justifications or interpretations, thus hindering potentially principled improvements. In this paper, we argue that there are fundamental connections between semantic segmentation and compression, especially between the Transformer decoders and Principal Component Analysis (PCA). From such a perspective, we derive a white-box, fully attentional DEcoder for PrIncipled semantiC segemenTation (DEPICT), with the interpretations as follows: 1) the self-attention operator refines image embeddings to construct an ideal principal subspace that aligns with the supervision and retains most information; 2) the cross-attention operator seeks to find a low-rank approximation of the refined image embeddings, which is expected to be a set of orthonormal bases of the principal subspace and corresponds to the predefined classes; 3) the dot-product operation yields compact representation for image embeddings as segmentation masks. Experiments conducted on dataset ADE20K find that DEPICT consistently outperforms its black-box counterpart, Segmenter, and it is light weight and more robust. </p>
<blockquote>
<p>基于Transformer的语义分割的先进方法通常采用Transformer解码器，通过交叉注意力从图像嵌入中提取额外的嵌入，通过自注意力对任一或两种嵌入进行精炼，并通过点积将图像嵌入投影到额外的嵌入上。尽管这些方法取得了显著的成功，但这些经验设计仍然缺乏理论证明或解释，从而阻碍了可能的原则性改进。在本文中，我们认为语义分割与压缩之间存在根本联系，特别是Transformer解码器与主成分分析（PCA）之间的联系。从这一角度出发，我们推导出了一个白盒、全注意力的解码器DEPICT（用于原则性语义分割），其解释如下：1）自注意力运算符通过完善图像嵌入来构建一个理想的主子空间，该空间与监督对齐并保留最多信息；2）交叉注意力运算符试图找到精炼图像嵌入的低阶逼近，这期望是一组正交基的子集，并与预定义的类别相对应；3）点积操作为图像嵌入生成紧凑的表示形式作为分割掩膜。在ADE20K数据集上进行的实验发现，DEPICT持续优于其黑盒竞品Segmenter，并且它更加轻便和稳健。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03033v3">PDF</a> NeurIPS2024. Code:<a target="_blank" rel="noopener" href="https://github.com/QishuaiWen/DEPICT/">https://github.com/QishuaiWen/DEPICT/</a></p>
<p><strong>Summary</strong>：本文提出了一种基于主成分分析（PCA）的语义分割模型的全新理论框架DEPICT，它通过解释性的角度解析了Transformer解码器的内部机制。DEPICT通过自我关注操作精炼图像嵌入，构建一个理想的主成分子空间，并通过交叉关注操作寻找精炼图像嵌入的低秩近似，最后通过点积操作得到图像嵌入的紧凑表示作为分割掩码。实验证明，DEPICT在ADE20K数据集上表现优异，相较于非解释性的模型Segmenter具有更高的性能、更轻量级和更强的鲁棒性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>该论文提出了一种基于主成分分析（PCA）和注意力机制的语义分割模型DEPICT，通过解释Transformer解码器的内部机制来构建模型。</li>
<li>DEPICT通过自我关注操作精炼图像嵌入，构建一个理想的主分子空间以对齐监督信息并保留大部分信息。</li>
<li>DEPICT利用交叉关注操作寻找精炼图像嵌入的低秩近似，该低秩近似对应于预定义类别的一组正交基。</li>
<li>点积操作用于产生图像嵌入的紧凑表示形式作为分割掩码。</li>
<li>实验结果表明，DEPICT在ADE20K数据集上的表现优于现有的黑箱模型Segmenter。</li>
<li>DEPICT模型具有轻量级和高度鲁棒性的特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03033">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6647a0e3cd3fb0bdc6f8a11d4751f2c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87a9eea614d20a6cec7db6f61c60688e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f66a85e0ca9a3bd7ab98fb8b81dbb560.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8e7e1bf4a199ecbcd8590a7069f5d11.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Semantic-Prompt-Learning-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Semantic-Prompt-Learning-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation"></a>Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation</h2><p><strong>Authors:Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen</strong></p>
<p>Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each object category. In this way, SemPLeS can perform better semantic alignment between object regions and class labels, resulting in desired pseudo masks for training segmentation models. The proposed SemPLeS framework achieves competitive performance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO 2014, and shows compatibility with other WSSS methods. Code: <a target="_blank" rel="noopener" href="https://github.com/NVlabs/SemPLeS">https://github.com/NVlabs/SemPLeS</a>. </p>
<blockquote>
<p>弱监督语义分割（WSSS）旨在使用仅带有图像级监督的图像数据来训练分割模型。由于无法获得精确的像素级注释，现有方法通常通过优化CAM（类激活映射）类热图来生成用于训练分割模型的伪掩码。然而，生成的热图可能只捕获对象类别的辨别性图像区域或相关的共发生背景。为了解决这些问题，我们提出了弱监督语义分割的语义提示学习（SemPLeS）框架，该框架学习有效地提示CLIP潜在空间，以增强分割区域与目标对象类别之间的语义对齐。更具体地说，我们提出了对比提示学习和提示引导语义细化，以学习足够描述并抑制与每个对象类别相关的共发生背景的提示。通过这种方式，SemPLeS可以在对象区域和类别标签之间实现更好的语义对齐，从而生成用于训练分割模型的理想伪掩码。所提出的SemPLeS框架在标准WSSS基准测试（PASCAL VOC 2012和MS COCO 2014）上取得了具有竞争力的性能，并显示出与其他WSSS方法的兼容性。代码地址：<a target="_blank" rel="noopener" href="https://github.com/NVlabs/SemPLeS">https://github.com/NVlabs/SemPLeS</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.11791v4">PDF</a> WACV 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/NVlabs/SemPLeS">https://github.com/NVlabs/SemPLeS</a>. Project page:   <a target="_blank" rel="noopener" href="https://projectdisr.github.io/semples/">https://projectdisr.github.io/semples/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了弱监督语义分割（WSSS）的挑战，并提出了一个新的框架SemPLeS来解决这个问题。SemPLeS利用CLIP潜在空间中的提示来学习有效地描述目标对象的语义信息，并抑制与之相关的背景。通过对比提示学习和提示引导语义细化，SemPLeS能够生成更准确的伪掩码来训练分割模型。该框架在PASCAL VOC 2012和MS COCO 2014等标准WSSS基准测试中取得了有竞争力的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>弱监督语义分割（WSSS）的目标是使用仅带有图像级别监督的图像数据来训练分割模型。</li>
<li>由于缺乏精确的像素级别注释，现有方法通常通过优化CAM热图来生成伪掩码进行训练。</li>
<li>提出的SemPLeS框架旨在解决现有方法可能只捕获对象类别的鉴别性图像区域或相关共发生背景的问题。</li>
<li>SemPLeS利用CLIP潜在空间中的提示来学习有效地描述目标对象的语义信息，并抑制背景。</li>
<li>通过对比提示学习和提示引导语义细化，SemPLeS能更准确地描述和抑制与每个对象类别相关的共发生背景。</li>
<li>SemPLeS框架在标准WSSS基准测试中表现出竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.11791">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-60769a1ed37392d1b4a0be9cd5d8f375.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dc962294b118695a975dd024c8582b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcbbd2228136050f70aece6ab295e87e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eda112a70646e26e626a9e100d78d313.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="On-the-Robustness-of-Object-Detection-Models-on-Aerial-Images"><a href="#On-the-Robustness-of-Object-Detection-Models-on-Aerial-Images" class="headerlink" title="On the Robustness of Object Detection Models on Aerial Images"></a>On the Robustness of Object Detection Models on Aerial Images</h2><p><strong>Authors:Haodong He, Jian Ding, Bowen Xu, Gui-Song Xia</strong></p>
<p>The robustness of object detection models is a major concern when applied to real-world scenarios. The performance of most models tends to degrade when confronted with images affected by corruptions, since they are usually trained and evaluated on clean datasets. While numerous studies have explored the robustness of object detection models on natural images, there is a paucity of research focused on models applied to aerial images, which feature complex backgrounds, substantial variations in scales, and orientations of objects. This paper addresses the challenge of assessing the robustness of object detection models on aerial images, with a specific emphasis on scenarios where images are affected by clouds. In this study, we introduce two novel benchmarks based on DOTA-v1.0. The first benchmark encompasses 19 prevalent corruptions, while the second focuses on the cloud-corrupted condition-a phenomenon uncommon in natural images yet frequent in aerial photography. We systematically evaluate the robustness of mainstream object detection models and perform necessary ablation experiments. Through our investigations, we find that rotation-invariant modeling and enhanced backbone architectures can improve the robustness of models. Furthermore, increasing the capacity of Transformer-based backbones can strengthen their robustness. The benchmarks we propose and our comprehensive experimental analyses can facilitate research on robust object detection on aerial images. The codes and datasets are available at: <a target="_blank" rel="noopener" href="https://github.com/hehaodong530/DOTA-C">https://github.com/hehaodong530/DOTA-C</a>. </p>
<blockquote>
<p>对象检测模型在现实世界场景中的应用的鲁棒性是一个重要的关注点。当面对受污染影响的图像时，大多数模型的性能往往会下降，因为它们通常是在干净的数据集上进行训练和评估的。虽然许多研究已经探索了对象检测模型在自然图像上的鲁棒性，但关于应用于具有复杂背景、尺度变化大以及物体方向变化显著的高空图像模型的深入研究很少。本文解决了评估对象检测模型在高空图像上鲁棒性的挑战，特别是在图像受云层影响的情况下的场景。在这项研究中，我们基于DOTA-v1.0引入了两个新的基准测试。第一个基准测试包括19种常见的腐败形式，而第二个则专注于云层腐蚀的条件——这是一种在自然图像中很少见，但在航空摄影中经常发生的现象。我们系统地评估了主流对象检测模型的鲁棒性，并进行了必要的消融实验。通过我们的调查，我们发现旋转不变建模和增强的主干架构可以提高模型的鲁棒性。此外，增加基于Transformer的主干的容量可以加强其鲁棒性。我们提出的基准测试和全面的实验分析可以推动高空图像上鲁棒对象检测的研究。相关代码和数据集可在以下网址找到：[<a target="_blank" rel="noopener" href="https://github.com/hehaodong530/DOTA-C]%E3%80%82">https://github.com/hehaodong530/DOTA-C]。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15378v2">PDF</a> accepted by IEEE TGRS</p>
<p><strong>Summary</strong>：针对现实场景中的对象检测模型鲁棒性问题，该文对航拍图像的对象检测模型鲁棒性进行了评估。针对航拍图像复杂背景、尺度及方向变化等特点，引入两个基于DOTA-v1.0的新基准测试集，一个包含19种常见腐蚀情况，另一个专注于云腐蚀情况。通过系统评估主流对象检测模型的鲁棒性和进行必要的消融实验，发现旋转不变建模和增强主干架构能提高模型鲁棒性，增加基于Transformer的主干容量也能增强鲁棒性。提出的基准测试和实验分析有助于研究航拍图像上的稳健对象检测。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>对象检测模型在现实世界场景中的鲁棒性是一个重要问题，特别是在面对受腐蚀影响的图像时。</li>
<li>目前对航拍图像中的对象检测模型鲁棒性的研究较少，这些图像具有复杂背景、尺度及方向变化等特点。</li>
<li>引入两个新的基准测试集，一个包含多种常见腐蚀情况，另一个专注于云腐蚀情况。</li>
<li>系统评估主流对象检测模型的鲁棒性，发现旋转不变建模和增强主干架构能提高模型性能。</li>
<li>增加基于Transformer的主干容量也能增强模型的鲁棒性。</li>
<li>提出的基准测试和实验分析有助于推进航拍图像上的稳健对象检测研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.15378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-11740f03d5fb416599682b062718438a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f8fa8f49b7e17efa514065b58dcc9de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f017ad9a0d49a0b9c9f3724a76b93e59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06975e82940abbbde679696a62484f8b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-09812d8a27c8a10046b4b6ab6396f036.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-01-16  Radial Distortion in Face Images Detection and Impact
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f262d0f78c82bdcda7a592ee0b830dd.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-01-16  DM-Mamba Dual-domain Multi-scale Mamba for MRI reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23667.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
