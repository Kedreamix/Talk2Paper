<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Text-Diffusion Red-Teaming of Large Language Models Unveiling Harmful   Behaviors with Proximity Constraints">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-16-æ›´æ–°"><a href="#2025-01-16-æ›´æ–°" class="headerlink" title="2025-01-16 æ›´æ–°"></a>2025-01-16 æ›´æ–°</h1><h2 id="Text-Diffusion-Red-Teaming-of-Large-Language-Models-Unveiling-Harmful-Behaviors-with-Proximity-Constraints"><a href="#Text-Diffusion-Red-Teaming-of-Large-Language-Models-Unveiling-Harmful-Behaviors-with-Proximity-Constraints" class="headerlink" title="Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints"></a>Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints</h2><p><strong>Authors:Jonathan NÃ¶ther, Adish Singla, Goran RadanoviÄ‡</strong></p>
<p>Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red-teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this paper, we study red-teaming strategies that enable a targeted security assessment. We propose an optimization framework for red-teaming with proximity constraints, where the discovered prompts must be similar to reference prompts from a given dataset. This dataset serves as a template for the discovered prompts, anchoring the search for test-cases to specific topics, writing styles, or types of harmful behavior. We show that established auto-regressive model architectures do not perform well in this setting. We therefore introduce a black-box red-teaming method inspired by text-diffusion models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the reference prompt by perturbing it in the embedding space, directly controlling the amount of change introduced. We systematically evaluate our method by comparing its effectiveness with established methods based on model fine-tuning and zero- and few-shot prompting. Our results show that DART is significantly more effective at discovering harmful inputs in close proximity to the reference prompt. </p>
<blockquote>
<p>æœ€è¿‘çš„å·¥ä½œæå‡ºäº†é’ˆå¯¹ç»™å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¼æ´æµ‹è¯•çš„è‡ªåŠ¨åŒ–çº¢é˜Ÿæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨çº¢é˜ŸLLMæ¥å‘ç°ä¼šåœ¨ç›®æ ‡LLMä¸­å¼•å‘æœ‰å®³è¡Œä¸ºçš„è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†èƒ½å¤Ÿè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å®‰å…¨è¯„ä¼°çš„çº¢é˜Ÿç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†å¸¦æœ‰æ¥è¿‘åº¦çº¦æŸçš„çº¢é˜Ÿä¼˜åŒ–æ¡†æ¶ï¼Œå…¶ä¸­å‘ç°çš„æç¤ºå¿…é¡»ä¸ç»™å®šæ•°æ®é›†ä¸­çš„å‚è€ƒæç¤ºç›¸ä¼¼ã€‚è¯¥æ•°æ®é›†ä½œä¸ºå‘ç°çš„æç¤ºçš„æ¨¡æ¿ï¼Œå°†æµ‹è¯•ç”¨ä¾‹æœç´¢é”šå®šåˆ°ç‰¹å®šä¸»é¢˜ã€å†™ä½œé£æ ¼æˆ–æœ‰å®³è¡Œä¸ºç±»å‹ä¸Šã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œç°æœ‰çš„è‡ªåŠ¨å›å½’æ¨¡å‹æ¶æ„åœ¨æ­¤è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—æ–‡æœ¬æ‰©æ•£æ¨¡å‹å¯å‘çš„é»‘ç›’çº¢é˜Ÿæ–¹æ³•ï¼šç”¨äºå®¡è®¡å’Œçº¢é˜Ÿçš„æ‰©æ•£ï¼ˆDARTï¼‰ã€‚DARTé€šè¿‡åµŒå…¥ç©ºé—´ä¸­å¯¹å…¶è¿›è¡Œæ‰°åŠ¨æ¥ä¿®æ”¹å‚è€ƒæç¤ºï¼Œç›´æ¥æ§åˆ¶å¼•å…¥çš„æ›´æ”¹é‡ã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶ä¸åŸºäºæ¨¡å‹å¾®è°ƒä»¥åŠé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºçš„ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒDARTåœ¨å‘ç°ä¸å‚è€ƒæç¤ºæ¥è¿‘çš„æœ‰å®³è¾“å…¥æ–¹é¢æ˜¾è‘—æ›´æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08246v1">PDF</a> This is an extended version of a paper published at AAAI 25</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨çº¢é˜Ÿæµ‹è¯•æ–¹æ³•ï¼Œæ—¨åœ¨å‘ç°é’ˆå¯¹ç›®æ ‡LLMäº§ç”Ÿæœ‰å®³è¡Œä¸ºçš„è¾“å…¥ã€‚æœ¬æ–‡ç ”ç©¶äº†ä½¿ç›®æ ‡å®‰å…¨è¯„ä¼°æˆä¸ºå¯èƒ½çš„çº¢é˜Ÿç­–ç•¥ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¸¦æœ‰è¿‘ä¼¼çº¦æŸçš„çº¢é˜Ÿä¼˜åŒ–æ¡†æ¶ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å‘ç°ç°æœ‰çš„è‡ªåŠ¨å›å½’æ¨¡å‹æ¶æ„åœ¨æ­¤è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ï¼Œå› æ­¤å¼•å…¥äº†ä¸€ç§åŸºäºæ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„é»‘è‰²ç›’å­çº¢é˜Ÿæ–¹æ³•ï¼šå®¡è®¡å’Œçº¢é˜Ÿæ‰©æ•£ï¼ˆDARTï¼‰ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ï¼Œç»“æœè¡¨æ˜DARTåœ¨å‘ç°ä¸å‚è€ƒæç¤ºç›¸è¿‘çš„æœ‰å®³è¾“å…¥æ–¹é¢æ˜¾è‘—æ›´æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨åŒ–çº¢é˜Ÿæ–¹æ³•ç”¨äºæµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¼æ´ã€‚</li>
<li>çº¢é˜Ÿç­–ç•¥æ—¨åœ¨å‘ç°å¯¼è‡´ç›®æ ‡LLMäº§ç”Ÿæœ‰å®³è¡Œä¸ºçš„è¾“å…¥ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¸¦æœ‰è¿‘ä¼¼çº¦æŸçš„çº¢é˜Ÿä¼˜åŒ–æ¡†æ¶ï¼Œå‚è€ƒæç¤ºä½œä¸ºæ¨¡æ¿ï¼Œä½¿æœç´¢æµ‹è¯•ç”¨ä¾‹é›†ä¸­äºç‰¹å®šä¸»é¢˜ã€å†™ä½œé£æ ¼æˆ–æœ‰å®³è¡Œä¸ºç±»å‹ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨å›å½’æ¨¡å‹æ¶æ„åœ¨æ­¤è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºæ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„é»‘è‰²ç›’å­çº¢é˜Ÿæ–¹æ³•ï¼šDARTï¼ˆå®¡è®¡å’Œçº¢é˜Ÿæ‰©æ•£ï¼‰ã€‚</li>
<li>DARTé€šè¿‡æ‰°åŠ¨å‚è€ƒæç¤ºçš„åµŒå…¥ç©ºé—´è¿›è¡Œä¿®æ”¹ï¼Œç›´æ¥æ§åˆ¶å¼•å…¥çš„å˜åŒ–é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-22dd814af019e9de46d4ed048460b266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f498a75494c0c0cc7ff491491e944e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f4c9c18f39bb34f8c354c9df82ea589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f81556c8fa01d54c709542b9f8bb37bc.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.08246v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LeapVAD-A-Leap-in-Autonomous-Driving-via-Cognitive-Perception-and-Dual-Process-Thinking"><a href="#LeapVAD-A-Leap-in-Autonomous-Driving-via-Cognitive-Perception-and-Dual-Process-Thinking" class="headerlink" title="LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and   Dual-Process Thinking"></a>LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and   Dual-Process Thinking</h2><p><strong>Authors:Yukai Ma, Tiantian Wei, Naiting Zhong, Jianbiao Mei, Tao Hu, Licheng Wen, Xuemeng Yang, Botian Shi, Yong Liu</strong></p>
<p>While autonomous driving technology has made remarkable strides, data-driven approaches still struggle with complex scenarios due to their limited reasoning capabilities. Meanwhile, knowledge-driven autonomous driving systems have evolved considerably with the popularization of visual language models. In this paper, we propose LeapVAD, a novel method based on cognitive perception and dual-process thinking. Our approach implements a human-attentional mechanism to identify and focus on critical traffic elements that influence driving decisions. By characterizing these objects through comprehensive attributes - including appearance, motion patterns, and associated risks - LeapVAD achieves more effective environmental representation and streamlines the decision-making process. Furthermore, LeapVAD incorporates an innovative dual-process decision-making module miming the human-driving learning process. The system consists of an Analytic Process (System-II) that accumulates driving experience through logical reasoning and a Heuristic Process (System-I) that refines this knowledge via fine-tuning and few-shot learning. LeapVAD also includes reflective mechanisms and a growing memory bank, enabling it to learn from past mistakes and continuously improve its performance in a closed-loop environment. To enhance efficiency, we develop a scene encoder network that generates compact scene representations for rapid retrieval of relevant driving experiences. Extensive evaluations conducted on two leading autonomous driving simulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior performance compared to camera-only approaches despite limited training data. Comprehensive ablation studies further emphasize its effectiveness in continuous learning and domain adaptation. Project page: <a target="_blank" rel="noopener" href="https://pjlab-adg.github.io/LeapVAD/">https://pjlab-adg.github.io/LeapVAD/</a>. </p>
<blockquote>
<p>è™½ç„¶è‡ªåŠ¨é©¾é©¶æŠ€æœ¯åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†æ•°æ®é©±åŠ¨çš„æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹ä»ç„¶é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶æ¨ç†èƒ½åŠ›æœ‰é™ã€‚ä¸æ­¤åŒæ—¶ï¼Œéšç€è§†è§‰è¯­è¨€æ¨¡å‹çš„æ™®åŠï¼ŒçŸ¥è¯†é©±åŠ¨çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæœ‰äº†æ˜¾è‘—çš„å‘å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè®¤çŸ¥æ„ŸçŸ¥å’ŒåŒè¿‡ç¨‹æ€è€ƒçš„æ–°å‹æ–¹æ³•LeapVADã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†äººç±»æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºè¯†åˆ«å’Œå…³æ³¨å½±å“é©¾é©¶å†³ç­–çš„å…³é”®äº¤é€šè¦ç´ ã€‚é€šè¿‡å¯¹è¿™äº›ç‰©ä½“è¿›è¡Œå…¨é¢çš„ç‰¹å¾æè¿°ï¼ŒåŒ…æ‹¬å¤–è§‚ã€è¿åŠ¨æ¨¡å¼å’Œç›¸å…³é£é™©ï¼ŒLeapVADå®ç°äº†æ›´æœ‰æ•ˆçš„ç¯å¢ƒè¡¨å¾å’Œç®€åŒ–çš„å†³ç­–è¿‡ç¨‹ã€‚æ­¤å¤–ï¼ŒLeapVADèå…¥äº†ä¸€ä¸ªåˆ›æ–°çš„åŒè¿‡ç¨‹å†³ç­–æ¨¡å—ï¼Œæ¨¡ä»¿äººç±»é©¾é©¶å­¦ä¹ è¿‡ç¨‹ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªåˆ†æè¿‡ç¨‹ï¼ˆç³»ç»ŸIIï¼‰ï¼Œé€šè¿‡é€»è¾‘æ¨ç†ç§¯ç´¯é©¾é©¶ç»éªŒï¼Œä»¥åŠä¸€ä¸ªå¯å‘å¼è¿‡ç¨‹ï¼ˆç³»ç»ŸIï¼‰ï¼Œé€šè¿‡å¾®è°ƒå’Œå°æ ·æœ¬å­¦ä¹ æ¥å®Œå–„çŸ¥è¯†ã€‚LeapVADè¿˜åŒ…æ‹¬åå°„æœºåˆ¶å’Œä¸æ–­å¢é•¿çš„è®°å¿†åº“ï¼Œä½¿å…¶èƒ½å¤Ÿä»è¿‡å»çš„é”™è¯¯ä¸­å­¦ä¹ å¹¶åœ¨é—­ç¯ç¯å¢ƒä¸­æŒç»­æé«˜å…¶æ€§èƒ½ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåœºæ™¯ç¼–ç å™¨ç½‘ç»œï¼Œç”¨äºç”Ÿæˆç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œä»¥ä¾¿å¿«é€Ÿæ£€ç´¢ç›¸å…³çš„é©¾é©¶ç»éªŒã€‚åœ¨CARLAå’ŒDriveArenaä¸¤ä¸ªé¢†å…ˆçš„è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡è®­ç»ƒæ•°æ®æœ‰é™ï¼ŒLeapVADä»ä¼˜äºä»…ä½¿ç”¨æ‘„åƒå¤´çš„æ–¹æ¡ˆå¹¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†å…¶åœ¨æŒç»­å­¦ä¹ å’ŒåŸŸé€‚åº”æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://pjlab-adg.github.io/LeapVAD/%E3%80%82">https://pjlab-adg.github.io/LeapVAD/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08168v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†LeapVADè¿™ä¸€æ–°å‹çš„åŸºäºè®¤çŸ¥æ„ŸçŸ¥å’ŒåŒè¿‡ç¨‹æ€ç»´çš„è‡ªåŠ¨é©¾é©¶æŠ€æœ¯ã€‚å®ƒé€šè¿‡å¼•å…¥äººç±»æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯¹å½±å“é©¾é©¶å†³ç­–çš„å…³é”®äº¤é€šå…ƒç´ è¿›è¡Œè¯†åˆ«å’Œèšç„¦ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„ç¯å¢ƒè¡¨å¾å’Œå†³ç­–è¿‡ç¨‹ã€‚LeapVADç»“åˆäº†åˆ›æ–°æ€§çš„åŒè¿‡ç¨‹å†³ç­–æ¨¡å—ï¼Œæ¨¡ä»¿äººç±»é©¾é©¶å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒ…æ‹¬åˆ†æè¿‡ç¨‹ï¼ˆç³»ç»Ÿ-IIï¼‰å’Œç»éªŒç§¯ç´¯ä»¥åŠé€šè¿‡å¾®è°ƒå’Œå°æ ·æœ¬å­¦ä¹ çš„å¯å‘å¼è¿‡ç¨‹ï¼ˆç³»ç»Ÿ-Iï¼‰ã€‚æ­¤å¤–ï¼ŒLeapVADè¿˜å…·æœ‰åæ€æœºåˆ¶å’Œæˆé•¿è®°å¿†åº“ï¼Œå¯ä»è¿‡å»çš„é”™è¯¯ä¸­å­¦ä¹ å¹¶æŒç»­æ”¹è¿›æ€§èƒ½ã€‚åœ¨ä¸¤å¤§ä¸»æµè‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿå™¨CARLAå’ŒDriveArenaä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLeapVADåœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹å®ç°äº†è¶…è¶Šä»…ä½¿ç”¨æ‘„åƒå¤´çš„ä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LeapVADç»“åˆè®¤çŸ¥æ„ŸçŸ¥å’ŒåŒè¿‡ç¨‹æ€ç»´ï¼Œé’ˆå¯¹è‡ªåŠ¨é©¾é©¶æŠ€æœ¯æå‡ºæ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¼•å…¥äººç±»æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°å¯¹å…³é”®äº¤é€šå…ƒç´ çš„è¯†åˆ«å’Œèšç„¦ã€‚</li>
<li>LeapVADæ¨¡ä»¿äººç±»é©¾é©¶å­¦ä¹ è¿‡ç¨‹ï¼ŒåŒ…å«åˆ†æè¿‡ç¨‹å’Œå¯å‘å¼è¿‡ç¨‹ã€‚</li>
<li>ç³»ç»Ÿå…·æœ‰åæ€æœºåˆ¶å’Œæˆé•¿è®°å¿†åº“ï¼Œå¯ä»é”™è¯¯ä¸­å­¦ä¹ å¹¶æŒç»­æ”¹è¿›ã€‚</li>
<li>åœ¨ä¸¤å¤§æ¨¡æ‹Ÿå™¨ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLeapVADåœ¨æœ‰é™æ•°æ®ä¸‹è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>LeapVADé€šè¿‡åœºæ™¯ç¼–ç å™¨ç½‘ç»œç”Ÿæˆç´§å‡‘çš„åœºæ™¯è¡¨ç¤ºï¼Œæé«˜æ•ˆç‡ã€‚</li>
<li>ç»¼åˆæ¶ˆèç ”ç©¶å¼ºè°ƒäº†å…¶åœ¨æŒç»­å­¦ä¹ å’ŒåŸŸé€‚åº”æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aaaeb47d09aaf29b22b20847700e3dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f647010db2741e36967e15ba5e27fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fe98f369efde3b3917d94ff310f8224.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57fa463d68962f0bced3c3d7ef653d40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e999ca558a6df3a249f23c8ea3cc4dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d92fc3db46657e34b6aa7a3060900b7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="I-Can-Find-You-in-Seconds-Leveraging-Large-Language-Models-for-Code-Authorship-Attribution"><a href="#I-Can-Find-You-in-Seconds-Leveraging-Large-Language-Models-for-Code-Authorship-Attribution" class="headerlink" title="I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution"></a>I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution</h2><p><strong>Authors:Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung</strong></p>
<p>Source code authorship attribution is important in software forensics, plagiarism detection, and protecting software patch integrity. Existing techniques often rely on supervised machine learning, which struggles with generalization across different programming languages and coding styles due to the need for large labeled datasets. Inspired by recent advances in natural language authorship analysis using large language models (LLMs), which have shown exceptional performance without task-specific tuning, this paper explores the use of LLMs for source code authorship attribution.   We present a comprehensive study demonstrating that state-of-the-art LLMs can successfully attribute source code authorship across different languages. LLMs can determine whether two code snippets are written by the same author with zero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of 0.78, and can attribute code authorship from a small set of reference code snippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show some adversarial robustness against misattribution attacks.   Despite these capabilities, we found that naive prompting of LLMs does not scale well with a large number of authors due to input token limitations. To address this, we propose a tournament-style approach for large-scale attribution. Evaluating this approach on datasets of C++ (500 authors, 26,355 samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve classification accuracy of up to 65% for C++ and 68.7% for Java using only one reference per author. These results open new possibilities for applying LLMs to code authorship attribution in cybersecurity and software engineering. </p>
<blockquote>
<p>æºä»£ç ä½œè€…å½’å±åœ¨è½¯ä»¶å–è¯ã€æŠ„è¢­æ£€æµ‹ä»¥åŠè½¯ä»¶è¡¥ä¸å®Œæ•´æ€§ä¿æŠ¤æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰æŠ€æœ¯é€šå¸¸ä¾èµ–äºæœ‰ç›‘ç£æœºå™¨å­¦ä¹ ï¼Œç”±äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œå®ƒåœ¨ä¸åŒç¼–ç¨‹è¯­è¨€å’Œç¼–ç é£æ ¼ä¹‹é—´çš„æ³›åŒ–æ–¹é¢è¡¨ç°å›°éš¾ã€‚å—è‡ªç„¶è¯­è¨€ä½œè€…åˆ†æä¸­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œè¯¥è®ºæ–‡æ¢ç´¢äº†ä½¿ç”¨LLMsè¿›è¡Œæºä»£ç ä½œè€…å½’å±ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œè¡¨æ˜æœ€å…ˆè¿›çš„LLMså¯ä»¥æˆåŠŸåœ°åœ¨ä¸åŒè¯­è¨€ä¸­è¿›è¡Œæºä»£ç ä½œè€…å½’å±ã€‚LLMså¯ä»¥é€šè¿‡é›¶æ ·æœ¬æç¤ºç¡®å®šä¸¤ä¸ªä»£ç ç‰‡æ®µæ˜¯å¦ç”±åŒä¸€ä½œè€…ç¼–å†™ï¼Œè¾¾åˆ°é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰0.78ï¼Œå¹¶ä¸”å¯ä»¥ä»å°‘é‡å‚è€ƒä»£ç ç‰‡æ®µä¸­é€šè¿‡å°æ ·æœ¬å­¦ä¹ è¿›è¡Œä»£ç ä½œè€…å½’å±ï¼Œè¾¾åˆ°MCC 0.77ã€‚æ­¤å¤–ï¼ŒLLMså¯¹äºä¸€äº›è¯¯å½’å±æ”»å‡»è¿˜è¡¨ç°å‡ºä¸€å®šçš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚å°½ç®¡å…·å¤‡äº†è¿™äº›åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°å¯¹LLMsçš„ç›´ç™½æç¤ºå¹¶ä¸é€‚ç”¨äºå¤§é‡ä½œè€…çš„æƒ…å†µï¼Œå› ä¸ºå­˜åœ¨è¾“å…¥æ ‡è®°çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡å½’å±çš„é”¦æ ‡èµ›å¼æ–¹æ³•ã€‚åœ¨GitHubä¸Šçš„C++ï¼ˆ500ä½ä½œè€…ï¼Œ26,355ä¸ªæ ·æœ¬ï¼‰å’ŒJavaï¼ˆ686ä½ä½œè€…ï¼Œ55,267ä¸ªæ ·æœ¬ï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°è¯¥æ–¹æ³•ï¼Œä»…ä½¿ç”¨æ¯ä½ä½œè€…çš„ä¸€ä¸ªå‚è€ƒæ ·æœ¬ï¼Œå³å¯è¾¾åˆ°C++çš„65%å’ŒJavaçš„68.7%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœä¸ºåœ¨ç½‘ç»œå®‰å…¨å’Œè½¯ä»¶å·¥ç¨‹ä¸­åº”ç”¨LLMsè¿›è¡Œä»£ç ä½œè€…å½’å±æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08165v1">PDF</a> 12 pages, 5 figures,</p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªç„¶è¯­è¨€ä½œè€…åˆ†ææœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæœ¬æ–‡æ¢ç´¢äº†ç”¨äºæºä»£ç ä½œè€…å½’å±æ€§çš„LLMsåº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå…ˆè¿›LLMså¯æˆåŠŸå½’å±ä¸åŒè¯­è¨€çš„æºä»£ç ä½œè€…ï¼Œé€šè¿‡é›¶æ ·æœ¬æç¤ºç¡®å®šä¸¤ä¸ªä»£ç ç‰‡æ®µæ˜¯å¦ç”±åŒä¸€ä½œè€…æ’°å†™ï¼Œè¾¾åˆ°é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰ä¸º0.78ï¼Œå¹¶é€šè¿‡å°‘é‡å­¦ä¹ ä»å‚è€ƒä»£ç ç‰‡æ®µä¸­å½’å±ä½œè€…ï¼Œå®ç°MCCä¸º0.77ã€‚å°½ç®¡å…·æœ‰è¿™äº›èƒ½åŠ›ï¼Œä½†å‘ç°LLMsçš„ç›´è§‚æç¤ºå¹¶ä¸é€‚ç”¨äºå¤§é‡ä½œè€…çš„åœºæ™¯ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é”¦æ ‡èµ›é£æ ¼çš„å¤§è§„æ¨¡å½’å±æ–¹æ³•ï¼Œåœ¨GitHubçš„C++å’ŒJavaä»£ç æ•°æ®é›†ä¸Šå®ç°é«˜è¾¾65%å’Œ68.7%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™ä¸ºåœ¨ç½‘ç»œå®‰å…¨å’Œè½¯ä»¶å·¥ç¨‹ä¸­åº”ç”¨LLMsè¿›è¡Œä»£ç ä½œè€…å½’å±æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æºä»£ç ä½œè€…å½’å±åœ¨è½¯ä»¶å–è¯ã€æŠ„è¢­æ£€æµ‹å’Œè½¯ä»¶è¡¥ä¸å®Œæ•´æ€§ä¿æŠ¤ä¸­å…·æœ‰é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯é€šå¸¸ä¾èµ–äºç›‘ç£æœºå™¨å­¦ä¹ ï¼Œéš¾ä»¥åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€å’Œç¼–ç é£æ ¼ä¹‹é—´è¿›è¡Œæ³›åŒ–ï¼Œéœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>LLMsåœ¨æºä»£ç ä½œè€…å½’å±æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯æˆåŠŸå½’å› äºä¸åŒè¯­è¨€çš„ä½œè€…ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬æç¤ºå’Œå°‘é‡å­¦ä¹ ï¼ŒLLMså¯ä»¥å‡†ç¡®åœ°ç¡®å®šä»£ç ç‰‡æ®µçš„ä½œè€…ã€‚</li>
<li>LLMså…·æœ‰ä¸€å®šçš„å¯¹æŠ—è¯¯å½’å±æ”»å‡»çš„èƒ½åŠ›ã€‚</li>
<li>ç›´è§‚æç¤ºLLMså¹¶ä¸é€‚ç”¨äºå¤§é‡ä½œè€…çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3ba44bf30dae6d243d65ca517465ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c32f47ed7fd1b61a38178840cda9932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832769875fc44830ccb1bfce922bd5cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237ec6bfecd9b962568537e68dc31921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38671386cf8e269db94af2e29c299467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb2ea110deca59a9425d4b300a61ae0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Compute-More-Faithful-A-Research-Note"><a href="#Inference-Time-Compute-More-Faithful-A-Research-Note" class="headerlink" title="Inference-Time-Compute: More Faithful? A Research Note"></a>Inference-Time-Compute: More Faithful? A Research Note</h2><p><strong>Authors:James Chua, Owain Evans</strong></p>
<p>Models trained specifically to generate long Chains of Thought (CoTs) have recently achieved impressive results. We refer to these models as Inference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful compared to traditional non-ITC models? We evaluate two ITC models (based on Qwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure faithfulness, we test if models articulate cues in their prompt that influence their answers to MMLU questions. For example, when the cue â€œA Stanford Professor thinks the answer is Dâ€™â€ is added to the prompt, models sometimes switch their answer to D. In such cases, the Gemini ITC model articulates the cue 54% of the time, compared to 14% for the non-ITC Gemini.   We evaluate 7 types of cue, such as misleading few-shot examples and anchoring on past responses. ITC models articulate cues that influence them much more reliably than all the 6 non-ITC models tested, such as Claude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.   However, our study has important limitations. We evaluate only two ITC models â€“ we cannot evaluate OpenAIâ€™s SOTA o1 model. We also lack details about the training of these ITC models, making it hard to attribute our findings to specific processes.   We think faithfulness of CoT is an important property for AI Safety. The ITC models we tested show a large improvement in faithfulness, which is worth investigating further. To speed up this investigation, we release these early results as a research note. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç»è¿‡ä¸“é—¨è®­ç»ƒä»¥ç”Ÿæˆé•¿é“¾æ¡æ€ç»´ï¼ˆChain of Thoughts, CoTï¼‰çš„æ¨¡å‹å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚æˆ‘ä»¬å°†è¿™äº›æ¨¡å‹ç§°ä¸ºæ¨ç†æ—¶é—´è®¡ç®—ï¼ˆInference-Time-Compute, ITCï¼‰æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„éITCæ¨¡å‹ç›¸æ¯”ï¼ŒITCæ¨¡å‹çš„æ€ç»´é“¾æ¡æ˜¯å¦æ›´åŠ å¿ å®ï¼Ÿæˆ‘ä»¬å¯¹ä¸¤æ¬¾åŸºäºQwen-2.5å’ŒGemini-2çš„ITCæ¨¡å‹è¿›è¡Œäº†å¿ è¯šæ€§çš„ç°æœ‰æµ‹è¯•ã€‚ä¸ºäº†è¡¡é‡æ€ç»´é“¾æ¡çš„å¿ å®æ€§ï¼Œæˆ‘ä»¬æµ‹è¯•äº†æ¨¡å‹æ˜¯å¦èƒ½æ˜ç¡®å…¶æç¤ºä¸­çš„çº¿ç´¢æ¥å½±å“å…¶ç­”æ¡ˆåˆ°MMLUé—®é¢˜çš„æ–¹å‘ã€‚ä¾‹å¦‚ï¼Œå½“æç¤ºä¸­åŠ å…¥â€œæ–¯å¦ç¦æ•™æˆè®¤ä¸ºç­”æ¡ˆæ˜¯Dâ€è¿™ä¸€çº¿ç´¢æ—¶ï¼Œæ¨¡å‹æœ‰æ—¶ä¼šæ”¹å˜ç­”æ¡ˆé€‰æ‹©Dã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒGemini ITCæ¨¡å‹èƒ½å¤Ÿæ˜ç¡®è¿™ä¸€çº¿ç´¢çš„å æ¯”è¾¾åˆ°54%ï¼Œè€ŒéITCçš„Geminiä»…å 14%ã€‚æˆ‘ä»¬å¯¹åŒ…æ‹¬è¯¯å¯¼æ€§çš„å°‘é‡ç¤ºä¾‹å’ŒåŸºäºè¿‡å»å›åº”çš„é”šå®šåœ¨å†…çš„7ç§çº¿ç´¢è¿›è¡Œäº†è¯„ä¼°ã€‚ITCæ¨¡å‹æ›´åŠ å¯é åœ°æè¿°äº†å½±å“ä»–ä»¬çš„çº¿ç´¢ï¼Œç›¸æ¯”äºæˆ‘ä»¬æµ‹è¯•çš„å…­ä¸ªéITCæ¨¡å‹ï¼Œå¦‚Claude-3.5-Sonnetå’ŒGPT-4oç­‰ï¼Œå®ƒä»¬é€šå¸¸æ˜ç¡®è¡¨è¿°çš„æ¯”ä¾‹æ¥è¿‘ä¸º0%ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç ”ç©¶å­˜åœ¨é‡è¦å±€é™æ€§ã€‚æˆ‘ä»¬åªè¯„ä¼°äº†ä¸¤ä¸ªITCæ¨¡å‹ï¼Œæ— æ³•è¯„ä¼°OpenAIçš„æœ€æ–°æ¨¡å‹o1ã€‚æ­¤å¤–ï¼Œç”±äºç¼ºä¹å…³äºè¿™äº›ITCæ¨¡å‹çš„è®­ç»ƒç»†èŠ‚ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„å‘ç°éš¾ä»¥å½’å› äºç‰¹å®šçš„è¿‡ç¨‹ã€‚æˆ‘ä»¬è®¤ä¸ºæ€ç»´é“¾æ¡çš„å¿ å®æ€§æ˜¯AIå®‰å…¨æ€§çš„ä¸€ä¸ªé‡è¦å±æ€§ã€‚æˆ‘ä»¬æµ‹è¯•çš„ITCæ¨¡å‹åœ¨å¿ å®æ€§æ–¹é¢å–å¾—äº†å¾ˆå¤§æ”¹è¿›ï¼Œå€¼å¾—è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚ä¸ºäº†åŠ é€Ÿè¿™ä¸€è°ƒæŸ¥è¿›ç¨‹ï¼Œæˆ‘ä»¬å‘å¸ƒè¿™äº›æ—©æœŸç»“æœä½œä¸ºç ”ç©¶ç¬”è®°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08156v1">PDF</a> 7 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ç”Ÿæˆé•¿é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Inference-Time-Computeï¼ˆITCï¼‰æ¨¡å‹çš„ä¿¡ä»°åº¦æµ‹è¯•ã€‚é€šè¿‡å¯¹ä¸¤æ¬¾ITCæ¨¡å‹ï¼ˆåŸºäºQwen-2.5å’ŒGemini-2ï¼‰çš„è¯„ä¼°ï¼Œå‘ç°å®ƒä»¬ç›¸æ¯”ä¼ ç»ŸéITCæ¨¡å‹æ›´å¯é åœ°è¡¨è¾¾å½±å“ç­”æ¡ˆçš„çº¿ç´¢ã€‚ç„¶è€Œï¼Œç ”ç©¶å­˜åœ¨å±€é™æ€§ï¼Œä»…è¯„ä¼°äº†ä¸¤ä¸ªITCæ¨¡å‹ï¼Œä¸”ç¼ºä¹å…³äºè¿™äº›ITCæ¨¡å‹è®­ç»ƒçš„å…·ä½“ç»†èŠ‚ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒITCæ¨¡å‹åœ¨ä¿¡ä»°åº¦æ–¹é¢æ˜¾ç¤ºå‡ºé‡å¤§æ”¹è¿›ï¼Œå€¼å¾—è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ITCæ¨¡å‹è¢«è®­ç»ƒç”¨äºç”Ÿæˆé•¿é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰ï¼Œåœ¨ä¿¡ä»°åº¦æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ä¼ ç»ŸéITCæ¨¡å‹ç›¸æ¯”ï¼ŒITCæ¨¡å‹æ›´å¯é åœ°è¡¨è¾¾å½±å“ç­”æ¡ˆçš„çº¿ç´¢ã€‚</li>
<li>åœ¨è¯„ä¼°çš„7ç§çº¿ç´¢ä¸­ï¼ŒITCæ¨¡å‹æ¯”6æ¬¾éITCæ¨¡å‹æ›´å¯é åœ°è¡¨è¾¾è¿™äº›çº¿ç´¢ã€‚</li>
<li>ç ”ç©¶ä»…è¯„ä¼°äº†ä¸¤ä¸ªITCæ¨¡å‹ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç¼ºä¹å…³äºITCæ¨¡å‹è®­ç»ƒçš„å…·ä½“ç»†èŠ‚ï¼Œä½¿å¾—éš¾ä»¥å°†å‘ç°å½’å› äºç‰¹å®šè¿‡ç¨‹ã€‚</li>
<li>ä¿¡ä»°åº¦æ˜¯AIå®‰å…¨çš„é‡è¦å±æ€§ï¼ŒITCæ¨¡å‹åœ¨ä¿¡ä»°åº¦æ–¹é¢çš„æ”¹è¿›å€¼å¾—è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd35e36514e28e2ec6297d6036793862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f563a5121cd120c5dbbe59154871de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e754382dc00f6340b1575ed00836a421.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52319b3ae7ddabf7d4172d761563ef2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9af38c19d4d6eead5f82a2da98fbc927.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Instruct-Few-Shot-Jailbreaking-Decompose-the-Attack-into-Pattern-and-Behavior-Learning"><a href="#Self-Instruct-Few-Shot-Jailbreaking-Decompose-the-Attack-into-Pattern-and-Behavior-Learning" class="headerlink" title="Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning"></a>Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning</h2><p><strong>Authors:Jiaqi Hua, Wanxu Wei</strong></p>
<p>Recently, several works have been conducted on jailbreaking Large Language Models (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024) focuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search. Nevertheless, this method lacks generality since it specifies the instruction-response structure. Moreover, the reason why inserting special tokens takes effect in inducing harmful behaviors is only empirically discussed. In this paper, we take a deeper insight into the mechanism of special token injection and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the modelâ€™s vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/iphosi/Self-Instruct-FSJ">https://github.com/iphosi/Self-Instruct-FSJ</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæœ‰ä¸€äº›å…³äºåˆ©ç”¨å°‘é‡æ¶æ„æ¼”ç¤ºç ´è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç ”ç©¶ã€‚ç‰¹åˆ«æ˜¯ï¼ŒZhengç­‰äººï¼ˆ2024å¹´ï¼‰ä¸“æ³¨äºé€šè¿‡å‘æ¼”ç¤ºä¸­æ³¨å…¥ç‰¹æ®Šä»¤ç‰Œå¹¶é‡‡ç”¨æ¼”ç¤ºçº§éšæœºæœç´¢æ¥æé«˜å°‘æ ·æœ¬ç ´è§£ï¼ˆFSJï¼‰çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ç¼ºä¹é€šç”¨æ€§ï¼Œå› ä¸ºå®ƒè§„å®šäº†æŒ‡ä»¤-å“åº”ç»“æ„ã€‚æ­¤å¤–ï¼Œæ’å…¥ç‰¹æ®Šä»¤ç‰Œä¸ºä½•èƒ½æœ‰æ•ˆè¯±å¯¼æœ‰å®³è¡Œä¸ºåªæ˜¯è¿›è¡Œäº†ç»éªŒæ€§è®¨è®ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ç‰¹æ®Šä»¤ç‰Œæ³¨å…¥çš„æœºåˆ¶è¿›è¡Œäº†æ›´æ·±å…¥çš„ç ”ç©¶ï¼Œå¹¶æå‡ºäº†å€ŸåŠ©æ¼”ç¤ºçº§è´ªå©ªæœç´¢çš„è¾…åŠ©è¿›è¡Œè‡ªæˆ‘æŒ‡å¯¼å°‘æ ·æœ¬ç ´è§£ï¼ˆSelf-Instruct FSJï¼‰ã€‚è¯¥æ¡†æ¶å°†FSJæ”»å‡»åˆ†è§£ä¸ºæ¨¡å¼å­¦ä¹ å’Œè¡Œä¸ºå­¦ä¹ ï¼Œä»¥æ›´é€šç”¨å’Œé«˜æ•ˆçš„æ–¹å¼åˆ©ç”¨æ¨¡å‹çš„æ¼æ´ã€‚æˆ‘ä»¬åœ¨å¸¸è§çš„å¼€æºæ¨¡å‹ä¸Šè¿›è¡Œäº†ç²¾å¿ƒè®¾è®¡çš„å®éªŒæ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å°†å…¶ä¸åŸºçº¿ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/iphosi/Self-Instruct-FSJ%E3%80%82">https://github.com/iphosi/Self-Instruct-FSJã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬è¶Šç‹±æ”»å‡»ç ”ç©¶è¿‘æœŸå¤‡å—å…³æ³¨ã€‚æœ¬æ–‡æ·±å…¥æ¢è®¨äº†ç‰¹æ®Šä»¤ç‰Œæ³¨å…¥çš„æœºåˆ¶ï¼Œå¹¶æå‡ºäº†åŸºäºæ¼”ç¤ºçº§è´ªå©ªæœç´¢çš„Self-Instructå°‘æ ·æœ¬è¶Šç‹±æ”»å‡»æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†è¶Šç‹±æ”»å‡»åˆ†è§£ä¸ºæ¨¡å¼å’Œè¡Œä¸ºå­¦ä¹ ï¼Œä»¥æ›´é€šç”¨å’Œé«˜æ•ˆçš„æ–¹å¼åˆ©ç”¨æ¨¡å‹çš„æ¼æ´ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€æºæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿ç®—æ³•ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬è¶Šç‹±æ”»å‡»ç ”ç©¶æ´»è·ƒã€‚</li>
<li>Zhengç­‰äººçš„ç ”ç©¶é€šè¿‡æ’å…¥ç‰¹æ®Šä»¤ç‰Œæé«˜äº†è¶Šç‹±æ•ˆç‡ï¼Œä½†ç¼ºä¹é€šç”¨æ€§ã€‚</li>
<li>æœ¬æ–‡æ·±å…¥æ¢è®¨äº†ç‰¹æ®Šä»¤ç‰Œæ³¨å…¥çš„æœºåˆ¶ï¼Œå¹¶æå‡ºäº†æ–°çš„Self-Instructå°‘æ ·æœ¬è¶Šç‹±æ”»å‡»æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆ†è§£è¶Šç‹±æ”»å‡»ä¸ºæ¨¡å¼å’Œè¡Œä¸ºå­¦ä¹ ï¼Œä»¥æ›´é€šç”¨å’Œé«˜æ•ˆçš„æ–¹å¼åˆ©ç”¨æ¨¡å‹æ¼æ´ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ–°æ¡†æ¶åœ¨å¼€æºæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼Œä¾¿äºä»–äººæŸ¥é˜…å’Œä½¿ç”¨ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§å’Œç¨³å®šæ€§å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜å’Œç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-737f5ee493e0e08d4ef8dd285262ef1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa16b1bc90e1efd77494df5bdd1ada95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dad7a2a35d15fa9e1b0d65986e7124c0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Interface-for-Home-Energy-Management-Systems"><a href="#Large-Language-Model-Interface-for-Home-Energy-Management-Systems" class="headerlink" title="Large Language Model Interface for Home Energy Management Systems"></a>Large Language Model Interface for Home Energy Management Systems</h2><p><strong>Authors:FranÃ§ois Michelon, Yihong Zhou, Thomas Morstyn</strong></p>
<p>Home Energy Management Systems (HEMSs) help households tailor their electricity usage based on power system signals such as energy prices. This technology helps to reduce energy bills and offers greater demand-side flexibility that supports the power system stability. However, residents who lack a technical background may find it difficult to use HEMSs effectively, because HEMSs require well-formatted parameterization that reflects the characteristics of the energy resources, houses, and usersâ€™ needs. Recently, Large-Language Models (LLMs) have demonstrated an outstanding ability in language understanding. Motivated by this, we propose an LLM-based interface that interacts with users to understand and parameterize their &#96;&#96;badly-formatted answersâ€™â€™, and then outputs well-formatted parameters to implement an HEMS. We further use Reason and Act method (ReAct) and few-shot prompting to enhance the LLM performance. Evaluating the interface performance requires multiple userâ€“LLM interactions. To avoid the efforts in finding volunteer users and reduce the evaluation time, we additionally propose a method that uses another LLM to simulate users with varying expertise, ranging from knowledgeable to non-technical. By comprehensive evaluation, the proposed LLM-based HEMS interface achieves an average parameter retrieval accuracy of 88%, outperforming benchmark models without ReAct and&#x2F;or few-shot prompting. </p>
<blockquote>
<p>å®¶åº­èƒ½æºç®¡ç†ç³»ç»Ÿï¼ˆHEMSï¼‰èƒ½å¤Ÿæ ¹æ®ç”µåŠ›ç³»ç»Ÿä¿¡å·ï¼ˆå¦‚èƒ½æºä»·æ ¼ï¼‰å¸®åŠ©å®¶åº­è°ƒæ•´å…¶ç”¨ç”µè¡Œä¸ºã€‚è¿™é¡¹æŠ€æœ¯æœ‰åŠ©äºé™ä½èƒ½æºè´¦å•ï¼Œå¹¶æä¾›æ›´å¤§çš„éœ€æ±‚ä¾§çµæ´»æ€§ï¼Œæ”¯æŒç”µåŠ›ç³»ç»Ÿç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œç¼ºä¹æŠ€æœ¯èƒŒæ™¯çš„å±…æ°‘å¯èƒ½ä¼šå‘ç°æœ‰æ•ˆä½¿ç”¨HEMSå¾ˆå›°éš¾ï¼Œå› ä¸ºHEMSéœ€è¦åæ˜ èƒ½æºèµ„æºã€æˆ¿å±‹å’Œç”¨æˆ·éœ€æ±‚çš„ç‰¹æ€§çš„æ ¼å¼åŒ–å‚æ•°ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†å‡ºè‰²çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºLLMçš„æ¥å£ï¼Œè¯¥æ¥å£ä¸ç”¨æˆ·äº’åŠ¨ï¼Œç†è§£å’Œå‚æ•°åŒ–ä»–ä»¬çš„â€œæ ¼å¼é”™è¯¯çš„ç­”æ¡ˆâ€ï¼Œç„¶åè¾“å‡ºæ ¼å¼è‰¯å¥½çš„å‚æ•°æ¥å®ç°HEMSã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨Reason and Actæ–¹æ³•ï¼ˆReActï¼‰å’Œå°‘é‡æç¤ºæ¥å¢å¼ºLLMçš„æ€§èƒ½ã€‚è¯„ä¼°æ¥å£æ€§èƒ½éœ€è¦è¿›è¡Œå¤šæ¬¡ç”¨æˆ·ä¸LLMçš„äº’åŠ¨ã€‚ä¸ºäº†é¿å…å¯»æ‰¾å¿—æ„¿è€…ç”¨æˆ·å’Œå‡å°‘è¯„ä¼°æ—¶é—´ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå³ä½¿ç”¨å¦ä¸€ä¸ªLLMæ¥æ¨¡æ‹Ÿå…·æœ‰ä¸åŒä¸“ä¸šçŸ¥è¯†æ°´å¹³çš„ç”¨æˆ·ï¼Œä»çŸ¥è¯†æ¸Šåšåˆ°éæŠ€æœ¯ã€‚é€šè¿‡å…¨é¢è¯„ä¼°ï¼Œæ‰€æå‡ºçš„åŸºäºLLMçš„HEMSæ¥å£å¹³å‡å‚æ•°æ£€ç´¢å‡†ç¡®ç‡ä¸º88%ï¼Œä¼˜äºæ²¡æœ‰ReActå’Œ&#x2F;æˆ–å°‘é‡æç¤ºçš„åŸºå‡†æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07919v1">PDF</a> 13 pages conference paper</p>
<p><strong>Summary</strong>ï¼š<br>å®¶åº­èƒ½æºç®¡ç†ç³»ç»Ÿï¼ˆHEMSï¼‰å¯æ ¹æ®ç”µåŠ›ç³»ç»Ÿä¿¡å·å¦‚èƒ½æºä»·æ ¼å®šåˆ¶å®¶åº­ç”¨ç”µã€‚è¯¥æŠ€æœ¯æœ‰åŠ©äºé™ä½èƒ½æºè´¦å•ï¼Œå¹¶ä¸ºç”µåŠ›ç³»ç»Ÿç¨³å®šæ€§æä¾›æ›´å¤§çš„éœ€æ±‚ä¾§çµæ´»æ€§ã€‚ç„¶è€Œï¼Œç¼ºä¹æŠ€æœ¯èƒŒæ™¯çš„å±…æ°‘å¯èƒ½éš¾ä»¥æœ‰æ•ˆä½¿ç”¨HEMSã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç¤ºäº†å‡ºè‰²çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œæå‡ºä¸€ç§åŸºäºLLMçš„æ¥å£ï¼Œä¸ç”¨æˆ·äº¤äº’ä»¥ç†è§£å’Œå‚æ•°åŒ–ä»–ä»¬çš„â€œæ ¼å¼é”™è¯¯çš„ç­”æ¡ˆâ€ï¼Œå¹¶è¾“å‡ºæ ¼å¼è‰¯å¥½çš„å‚æ•°æ¥å®ç°HEMSã€‚é€šè¿‡é‡‡ç”¨Reason and Actæ–¹æ³•ï¼ˆReActï¼‰å’Œå°‘é‡æç¤ºå¢å¼ºLLMæ€§èƒ½ã€‚è¯„ä¼°æ¥å£æ€§èƒ½éœ€è¦å¤šæ¬¡ç”¨æˆ·ä¸LLMçš„äº’åŠ¨ã€‚ä¸ºäº†é¿å…å¯»æ‰¾å¿—æ„¿è€…ç”¨æˆ·å’Œå‡å°‘è¯„ä¼°æ—¶é—´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨å¦ä¸€ä¸ªLLMæ¨¡æ‹Ÿä¸åŒä¸“ä¸šç¨‹åº¦çš„ç”¨æˆ·çš„æ–¹æ³•ï¼Œä»çŸ¥è¯†æ¸Šåšåˆ°éæŠ€æœ¯ç”¨æˆ·ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„åŸºäºLLMçš„HEMSæ¥å£å¹³å‡å‚æ•°æ£€ç´¢å‡†ç¡®ç‡ä¸º88%ï¼Œä¼˜äºæœªä½¿ç”¨ReActå’Œ&#x2F;æˆ–å°‘é‡æç¤ºçš„åŸºå‡†æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å®¶åº­èƒ½æºç®¡ç†ç³»ç»Ÿ(HEMS)èƒ½æ ¹æ®ç”µåŠ›ä¿¡å·è°ƒæ•´ç”µåŠ›ä½¿ç”¨ï¼Œæœ‰åŠ©äºå‡å°‘èƒ½æºè´¦å•å’Œæ”¯æŒç”µåŠ›ç³»ç»Ÿç¨³å®šæ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œå‚æ•°åŒ–å±…æ°‘å¯¹HEMSçš„ä½¿ç”¨æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>æå‡ºäº†åŸºäºLLMçš„æ¥å£ï¼Œé€šè¿‡ç”¨æˆ·äº¤äº’ç†è§£å¹¶å‚æ•°åŒ–â€œæ ¼å¼é”™è¯¯çš„ç­”æ¡ˆâ€ã€‚</li>
<li>é‡‡ç”¨Reason and Actæ–¹æ³•ï¼ˆReActï¼‰å’Œå°‘é‡æç¤ºå¢å¼ºLLMæ€§èƒ½ï¼Œæé«˜HEMSæ¥å£çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨å¦ä¸€ä¸ªLLMæ¨¡æ‹Ÿä¸åŒä¸“ä¸šç¨‹åº¦çš„ç”¨æˆ·ï¼Œä»çŸ¥è¯†æ¸Šåšåˆ°éæŠ€æœ¯ç”¨æˆ·ï¼Œä¾¿äºæ¥å£æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒåŸºäºLLMçš„HEMSæ¥å£å¹³å‡å‚æ•°æ£€ç´¢å‡†ç¡®ç‡ä¸º88%ã€‚</li>
<li>è¯¥æ¥å£æ€§èƒ½ä¼˜äºæœªä½¿ç”¨ReActå’Œ&#x2F;æˆ–å°‘é‡æç¤ºçš„åŸºå‡†æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4d7f39903b68214c5ceca075aeda6304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa86491bda8f8ee9fb97fd85212ea1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d705c8894121b7370f821a3b4e14413.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a3c9c86ab66123c25a71594c6b1bfd5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leveraging-Metamemory-Mechanisms-for-Enhanced-Data-Free-Code-Generation-in-LLMs"><a href="#Leveraging-Metamemory-Mechanisms-for-Enhanced-Data-Free-Code-Generation-in-LLMs" class="headerlink" title="Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation   in LLMs"></a>Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation   in LLMs</h2><p><strong>Authors:Shuai Wang, Liang Ding, Yibing Zhan, Yong Luo, Zheng He, Dapeng Tao</strong></p>
<p>Automated code generation using large language models (LLMs) has gained attention due to its efficiency and adaptability. However, real-world coding tasks or benchmarks like HumanEval and StudentEval often lack dedicated training datasets, challenging existing few-shot prompting approaches that rely on reference examples. Inspired by human metamemory-a cognitive process involving recall and evaluation-we present a novel framework (namely M^2WF) for improving LLMsâ€™ one-time code generation. This approach enables LLMs to autonomously generate, evaluate, and utilize synthetic examples to enhance reliability and performance. Unlike prior methods, it minimizes dependency on curated data and adapts flexibly to various coding scenarios. Our experiments demonstrate significant improvements in coding benchmarks, offering a scalable and robust solution for data-free environments. The code and framework will be publicly available on GitHub and HuggingFace. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨ä»£ç ç”Ÿæˆå·²ç»å› å…¶æ•ˆç‡å’Œé€‚åº”æ€§è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œä¸­çš„ç¼–ç ä»»åŠ¡æˆ–HumanEvalå’Œå­¦ç”Ÿè¯„ä¼°ï¼ˆStudentEvalï¼‰ç­‰åŸºå‡†æµ‹è¯•é€šå¸¸ç¼ºä¹ä¸“ç”¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™ç»™ä¾èµ–å‚è€ƒæ ·æœ¬çš„ç°æœ‰å°‘æ•°æç¤ºæ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å—äººç±»å…ƒè®°å¿†ï¼ˆä¸€ç§æ¶‰åŠå›å¿†å’Œè¯„ä»·çš„è®¤çŸ¥è¿‡ç¨‹ï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›LLMä¸€æ¬¡æ€§ä»£ç ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼ˆå³M^2WFï¼‰ã€‚è¿™ç§æ–¹æ³•ä½¿LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å’Œåˆ©ç”¨åˆæˆç¤ºä¾‹ï¼Œä»¥æé«˜å¯é æ€§å’Œæ€§èƒ½ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼Œå®ƒæœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹æ•°æ®é›†æ•´ç†çš„ä¾èµ–ï¼Œå¹¶çµæ´»åœ°é€‚åº”å„ç§ç¼–ç åœºæ™¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç¼–ç åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸ºæ— æ•°æ®ç¯å¢ƒæä¾›äº†å¯ä¼¸ç¼©å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ¡†æ¶å°†åœ¨GitHubå’ŒHuggingFaceä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07892v1">PDF</a> 11 pages,6 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå› å…¶æ•ˆç‡å’Œé€‚åº”æ€§è€Œå—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„ç¼–ç¨‹ä»»åŠ¡æˆ–åŸºå‡†æµ‹è¯•ï¼ˆå¦‚HumanEvalå’ŒStudentEvalï¼‰å¾€å¾€ç¼ºä¹ä¸“ç”¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿™ç»™ä¾èµ–å‚è€ƒæ ·ä¾‹çš„ç°æœ‰å°‘æ ·æœ¬æç¤ºæ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å—äººç±»å…ƒè®°å¿†ï¼ˆä¸€ç§æ¶‰åŠå›å¿†å’Œè¯„ä¼°çš„è®¤çŸ¥è¿‡ç¨‹ï¼‰çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºM^2WFçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›LLMçš„ä¸€æ¬¡æ€§ä»£ç ç”Ÿæˆã€‚è¯¥æ–¹æ³•ä½¿LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å’Œåˆ©ç”¨åˆæˆç¤ºä¾‹ï¼Œä»¥æé«˜å¯é æ€§å’Œæ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œå®ƒæœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹ç²¾é€‰æ•°æ®çš„ä¾èµ–ï¼Œå¹¶çµæ´»é€‚åº”å„ç§ç¼–ç åœºæ™¯ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç¼–ç åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¡†æ¶å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸ºæ— æ•°æ®ç¯å¢ƒæä¾›äº†å¯ä¼¸ç¼©å’Œç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>ç°å®ä¸–ç•Œçš„ç¼–ç¨‹ä»»åŠ¡æˆ–åŸºå‡†æµ‹è¯•ç¼ºä¹ä¸“ç”¨è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>ç°æœ‰å°‘æ ·æœ¬æç¤ºæ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å—äººç±»å…ƒè®°å¿†å¯å‘ï¼Œæå‡ºæ–°å‹æ¡†æ¶M^2WFæ”¹è¿›LLMçš„ä¸€æ¬¡æ€§ä»£ç ç”Ÿæˆã€‚</li>
<li>M^2WFä½¿LLMèƒ½å¤Ÿè‡ªä¸»ç”Ÿæˆã€è¯„ä¼°å’Œåˆ©ç”¨åˆæˆç¤ºä¾‹ã€‚</li>
<li>M^2WFæé«˜LLMçš„å¯é æ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd13f89a04ebff3e1c41c94467af83ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bba97ebadbda9321a4aeabbd33f7d1cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fefe0ff6388e86ba781d07cc8c689cb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0cf7aea9193ee91f2688bc80ccc227e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications â€“ such as author-publication history, author affiliation, research topics, and citation counts â€“ we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>ç§‘ç ”å›¢é˜Ÿçš„åŠ¨æ€åœ¨å†³å®šç ”ç©¶æˆæœçš„æ€§è´¨å’Œå½±å“æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å¯¹è´¡çŒ®çš„å…¨é¢ä¸Šä¸‹æ–‡åˆ†æã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ç§‘ç ”å›¢é˜Ÿä¸­çš„ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»çš„å˜é©æ€§æ–¹æ³•ï¼Œä¸ä¼ ç»Ÿèšç±»æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´ä¸ºç²¾ç»†çš„åˆ†æã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ©ç”¨å¼€æºå’Œä¸“æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚GPT-4ã€Llama3 70Bã€Llama2 70Bå’ŒMistral 7x8Bæ¥è¿›è¡Œè§’è‰²åˆ†ç±»ï¼Œä»¥è¡¥å……å’Œå¢å¼ºè¿™äº›ä¼ ç»Ÿæ–¹æ³•ã€‚é€šè¿‡å°‘æ ·æœ¬æç¤ºï¼Œæˆ‘ä»¬å¯¹ä½œè€…è§’è‰²è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶è¯æ˜GPT-4åœ¨å¤šç±»åˆ«ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„XGBoostå’ŒBERTç­‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬ä½¿ç”¨10ä¸ªç‰¹å¾æ„å»ºé¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚é€šè¿‡åœ¨OpenAlexæ•°æ®åº“è¡ç”Ÿçš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ï¼ˆè¯¥æ•°æ®åº“æä¾›æœ‰å…³å­¦æœ¯å‡ºç‰ˆçš„è¯¦ç»†å…ƒæ•°æ®ï¼Œå¦‚ä½œè€…å‡ºç‰ˆå†å²ã€ä½œè€…éš¶å±å…³ç³»ã€ç ”ç©¶ä¸»é¢˜å’Œå¼•ç”¨è®¡æ•°ï¼‰ï¼Œæˆ‘ä»¬è·å¾—äº†0.76çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†ä½œè€…è§’è‰²åˆ†ç±»çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v1">PDF</a> 14 pages, 4 figures, 3 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç§‘ç ”å›¢é˜ŸåŠ¨æ€å¯¹äºå†³å®šç ”ç©¶äº§å‡ºçš„æ€§è´¨å’Œå½±å“åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å…¨é¢åˆ†æè´¡çŒ®çš„è¯­å¢ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å˜é©æ€§çš„æ–¹æ³•ï¼Œåˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ç§‘ç ”å›¢é˜Ÿä¸­çš„ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„èšç±»æ–¹æ³•ï¼ŒLLMsæä¾›äº†æ›´ä¸ºç²¾ç»†çš„åˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆå¹¶æ”¹è¿›ä¼ ç»Ÿæ–¹æ³•ï¼Œä½¿ç”¨å¼€æºå’Œä¸“æœ‰LLMsï¼ˆå¦‚GPT-4ã€Llama3 70Bã€Llama2 70Bå’ŒMistral 7x8Bï¼‰è¿›è¡Œè§’è‰²åˆ†ç±»ã€‚é€šè¿‡å°‘é‡æç¤ºï¼Œæˆ‘ä»¬å¯¹ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯æ˜GPT-4åœ¨å¤šç±»åˆ«ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¶…è¶Šäº†å¦‚XGBoostå’ŒBERTç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬ä½¿ç”¨OpenAlexæ•°æ®åº“æ„å»ºçš„é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ•°æ®åº“æä¾›æœ‰å…³å­¦æœ¯å‡ºç‰ˆçš„è¯¦ç»†å…ƒæ•°æ®ï¼Œå¦‚ä½œè€…å‡ºç‰ˆå†å²ã€ä½œè€…éš¶å±å…³ç³»ã€ç ”ç©¶ä¸»é¢˜å’Œå¼•ç”¨è®¡æ•°ç­‰ã€‚é€šè¿‡è¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†F1åˆ†æ•°ä¸º0.76ï¼Œè¯æ˜äº†ä½œè€…è§’è‰²åˆ†ç±»çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç§‘ç ”å›¢é˜ŸåŠ¨æ€å¯¹ç ”ç©¶äº§å‡ºçš„æ€§è´¨å’Œå½±å“åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å…¨é¢åˆ†æè´¡çŒ®çš„è¯­å¢ƒã€‚</li>
<li>åˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä½œè€…è§’è‰²åˆ†ç±»æä¾›æ›´ç²¾ç»†çš„åˆ†æã€‚</li>
<li>GPT-4åœ¨ä½œè€…è§’è‰²åˆ†ç±»ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>GPT-4åœ¨å¤šä¸ªç±»åˆ«ä¸­è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚XGBoostå’ŒBERTã€‚</li>
<li>ä½¿ç”¨OpenAlexæ•°æ®åº“çš„é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°ç¨³å¥çš„ä½œè€…è§’è‰²åˆ†ç±»ï¼ŒF1åˆ†æ•°ä¸º0.76ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-12b34ca472332862f19792303db83430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-451d4ddb239873b9c80879e371f6b5a1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07267v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Use-of-Contrastive-Language-Image-Pre-Training-for-Human-Posture-Classification-Insights-from-Yoga-Pose-Analysis"><a href="#Exploring-the-Use-of-Contrastive-Language-Image-Pre-Training-for-Human-Posture-Classification-Insights-from-Yoga-Pose-Analysis" class="headerlink" title="Exploring the Use of Contrastive Language-Image Pre-Training for Human   Posture Classification: Insights from Yoga Pose Analysis"></a>Exploring the Use of Contrastive Language-Image Pre-Training for Human   Posture Classification: Insights from Yoga Pose Analysis</h2><p><strong>Authors:Andrzej D. Dobrzycki, Ana M. Bernardos, Luca Bergesio, Andrzej Pomirski, Daniel SÃ¡ez-Trigueros</strong></p>
<p>Accurate human posture classification in images and videos is crucial for automated applications across various fields, including work safety, physical rehabilitation, sports training, or daily assisted living. Recently, multimodal learning methods, such as Contrastive Language-Image Pretraining (CLIP), have advanced significantly in jointly understanding images and text. This study aims to assess the effectiveness of CLIP in classifying human postures, focusing on its application in yoga. Despite the initial limitations of the zero-shot approach, applying transfer learning on 15,301 images (real and synthetic) with 82 classes has shown promising results. The article describes the full procedure for fine-tuning, including the choice for image description syntax, models and hyperparameters adjustment. The fine-tuned CLIP model, tested on 3826 images, achieves an accuracy of over 85%, surpassing the current state-of-the-art of previous works on the same dataset by approximately 6%, its training time being 3.5 times lower than what is needed to fine-tune a YOLOv8-based model. For more application-oriented scenarios, with smaller datasets of six postures each, containing 1301 and 401 training images, the fine-tuned models attain an accuracy of 98.8% and 99.1%, respectively. Furthermore, our experiments indicate that training with as few as 20 images per pose can yield around 90% accuracy in a six-class dataset. This study demonstrates that this multimodal technique can be effectively used for yoga pose classification, and possibly for human posture classification, in general. Additionally, CLIP inference time (around 7 ms) supports that the model can be integrated into automated systems for posture evaluation, e.g., for developing a real-time personal yoga assistant for performance assessment. </p>
<blockquote>
<p>å‡†ç¡®åœ°å¯¹å›¾åƒå’Œè§†é¢‘ä¸­çš„äººä½“å§¿æ€è¿›è¡Œåˆ†ç±»ï¼Œå¯¹äºåŒ…æ‹¬å·¥ä½œå®‰å…¨ã€èº«ä½“åº·å¤ã€è¿åŠ¨è®­ç»ƒå’Œæ—¥å¸¸ç”Ÿæ´»è¾…åŠ©åœ¨å†…çš„å„ä¸ªé¢†åŸŸä¸­çš„è‡ªåŠ¨åŒ–åº”ç”¨è‡³å…³é‡è¦ã€‚æœ€è¿‘ï¼Œå¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰ç­‰å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•åœ¨è”åˆç†è§£å›¾åƒå’Œæ–‡å­—æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°CLIPåœ¨äººä½“å§¿æ€åˆ†ç±»æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œé‡ç‚¹å…³æ³¨å…¶åœ¨ç‘œä¼½ä¸­çš„åº”ç”¨ã€‚å°½ç®¡é›¶æ ·æœ¬æ–¹æ³•å­˜åœ¨ä¸€äº›åˆæ­¥é™åˆ¶ï¼Œä½†åœ¨15301å¼ ï¼ˆçœŸå®å’Œåˆæˆï¼‰åŒ…å«82ç±»çš„å›¾åƒä¸Šåº”ç”¨è¿ç§»å­¦ä¹ å·²æ˜¾ç¤ºå‡ºä»¤äººé¼“èˆçš„ç»“æœã€‚æ–‡ç« æè¿°äº†å¾®è°ƒçš„å…¨è¿‡ç¨‹ï¼ŒåŒ…æ‹¬å›¾åƒæè¿°è¯­æ³•ã€æ¨¡å‹å’Œè¶…å‚æ•°è°ƒæ•´çš„é€‰æ‹©ã€‚ç»è¿‡ç²¾ç»†è°ƒæ•´çš„CLIPæ¨¡å‹åœ¨3826å¼ å›¾åƒä¸Šæµ‹è¯•æ—¶ï¼Œå‡†ç¡®ç‡è¶…è¿‡85%ï¼Œæ¯”åŒä¸€æ•°æ®é›†ä¸Šçš„å…ˆå‰æœ€æ–°æŠ€æœ¯é«˜å‡ºçº¦6%ï¼Œè€Œä¸”å…¶è®­ç»ƒæ—¶é—´æ˜¯å¾®è°ƒYOLOv8æ¨¡å‹æ‰€éœ€æ—¶é—´çš„3.5å€ä½ã€‚å¯¹äºæ›´é¢å‘åº”ç”¨çš„ç¯å¢ƒï¼ŒåŒ…å«æ¯ä¸ªå§¿æ€ä»…æœ‰1301å¼ å’Œ401å¼ è®­ç»ƒå›¾åƒçš„å°æ•°æ®é›†ï¼Œç²¾ç»†è°ƒæ•´åçš„æ¨¡å‹åˆ†åˆ«è¾¾åˆ°äº†98.8%å’Œ99.1%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ¯å§¿æ€ä»…ä½¿ç”¨20å¼ å›¾åƒè¿›è¡Œè®­ç»ƒå³å¯åœ¨åŒ…å«å…­ä¸ªç±»åˆ«çš„æ•°æ®é›†ä¸­è¾¾åˆ°çº¦90%çš„å‡†ç¡®ç‡ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§å¤šæ¨¡æ€æŠ€æœ¯å¯æœ‰æ•ˆç”¨äºç‘œä¼½å§¿åŠ¿åˆ†ç±»ï¼Œä¹Ÿå¯èƒ½å¹¿æ³›ç”¨äºä¸€èˆ¬çš„äººä½“å§¿æ€åˆ†ç±»ã€‚æ­¤å¤–ï¼ŒCLIPæ¨ç†æ—¶é—´çº¦ä¸º7æ¯«ç§’ï¼Œæ”¯æŒå°†æ¨¡å‹é›†æˆåˆ°å§¿åŠ¿è¯„ä¼°çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­ï¼Œä¾‹å¦‚å¼€å‘ç”¨äºæ€§èƒ½è¯„ä¼°çš„å®æ—¶ä¸ªäººç‘œä¼½åŠ©ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07221v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•ï¼Œå¦‚Contrastive Language-Image Pretraining (CLIP)ï¼Œè¿›è¡Œäººç±»å§¿åŠ¿åˆ†ç±»çš„åº”ç”¨ã€‚ç‰¹åˆ«æ˜¯åœ¨ç‘œä¼½å§¿åŠ¿åˆ†ç±»æ–¹é¢ï¼Œé€šè¿‡è°ƒæ•´æ¨¡å‹ã€å‚æ•°å’Œä¼˜åŒ–å›¾åƒæè¿°è¯­æ³•ç­‰æ­¥éª¤è¿›è¡Œå¾®è°ƒï¼Œå®éªŒç»“æœæ˜¾ç¤ºfine-tunedçš„CLIPæ¨¡å‹åœ¨ç‘œä¼½å§¿åŠ¿åˆ†ç±»ä¸Šå–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡ç°æœ‰æŠ€æœ¯æ°´å¹³çº¦6%ï¼Œä¸”è®­ç»ƒæ—¶é—´å‡å°‘äº†3.5å€ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œä½¿ç”¨å°‘é‡å›¾åƒï¼ˆå¦‚æ¯å§¿åŠ¿ä»…20å¼ å›¾åƒï¼‰è¿›è¡Œè®­ç»ƒä¹Ÿèƒ½è¾¾åˆ°çº¦90%çš„å‡†ç¡®ç‡ã€‚å› æ­¤ï¼Œè¯¥å¤šæ¨¡æ€æŠ€æœ¯å¯æœ‰æ•ˆåº”ç”¨äºç‘œä¼½å§¿åŠ¿åˆ†ç±»ï¼Œç”šè‡³å¯èƒ½é€‚ç”¨äºä¸€èˆ¬çš„äººç±»å§¿åŠ¿åˆ†ç±»ã€‚æ­¤å¤–ï¼ŒCLIPæ¨¡å‹çš„æ¨ç†æ—¶é—´çº¦ä¸º7æ¯«ç§’ï¼Œæ”¯æŒé›†æˆåˆ°è‡ªåŠ¨å§¿åŠ¿è¯„ä¼°ç³»ç»Ÿä¸­ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ æ–¹æ³•å¦‚CLIPåœ¨å§¿åŠ¿åˆ†ç±»ä¸­æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>CLIPæ¨¡å‹åœ¨ç‘œä¼½å§¿åŠ¿åˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡è¶…è¿‡ç°æœ‰æŠ€æœ¯çº¦6%ã€‚</li>
<li>é€šè¿‡å¾®è°ƒï¼ŒCLIPæ¨¡å‹å¯ä»¥åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šå®ç°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>ä½¿ç”¨å°‘é‡å›¾åƒï¼ˆæ¯å§¿åŠ¿ä»…20å¼ ï¼‰è¿›è¡Œè®­ç»ƒä¹Ÿå¯è¾¾åˆ°è‰¯å¥½çš„å‡†ç¡®ç‡ã€‚</li>
<li>CLIPæ¨¡å‹çš„æ¨ç†æ—¶é—´çŸ­ï¼Œé€‚åˆé›†æˆåˆ°å®æ—¶ç³»ç»Ÿä¸­è¿›è¡Œå§¿åŠ¿è¯„ä¼°ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºç‘œä¼½å’Œå…¶ä»–é¢†åŸŸè‡ªåŠ¨å§¿åŠ¿è¯„ä¼°ç³»ç»Ÿçš„å¼€å‘æä¾›äº†æ–°æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07221v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07221v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Matching-Free-Depth-Recovery-from-Structured-Light"><a href="#Matching-Free-Depth-Recovery-from-Structured-Light" class="headerlink" title="Matching Free Depth Recovery from Structured Light"></a>Matching Free Depth Recovery from Structured Light</h2><p><strong>Authors:Zhuohang Yu, Kai Wang, Juyong Zhang</strong></p>
<p>We present a novel approach for depth estimation from images captured by structured light systems. Unlike many previous methods that rely on image matching process, our approach uses a density voxel grid to represent scene geometry, which is trained via self-supervised differentiable volume rendering. Our method leverages color fields derived from projected patterns in structured light systems during the rendering process, enabling the isolated optimization of the geometry field. This contributes to faster convergence and high-quality output. Additionally, we incorporate normalized device coordinates (NDC), a distortion loss, and a novel surface-based color loss to enhance geometric fidelity. Experimental results demonstrate that our method outperforms existing matching-based techniques in geometric performance for few-shot scenarios, achieving approximately a 60% reduction in average estimated depth errors on synthetic scenes and about 30% on real-world captured scenes. Furthermore, our approach delivers fast training, with a speed roughly three times faster than previous matching-free methods that employ implicit representations. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç»“æ„å…‰ç³»ç»Ÿé‡‡é›†çš„å›¾åƒè¿›è¡Œæ·±åº¦ä¼°è®¡çš„æ–°æ–¹æ³•ã€‚ä¸åŒäºè®¸å¤šä¾èµ–äºå›¾åƒåŒ¹é…è¿‡ç¨‹çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¯†åº¦ä½“ç´ ç½‘æ ¼æ¥è¡¨ç¤ºåœºæ™¯å‡ ä½•ç»“æ„ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘ç›‘ç£çš„å¯å¾®ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¸²æŸ“è¿‡ç¨‹ä¸­åˆ©ç”¨ç»“æ„å…‰ç³»ç»Ÿä¸­æŠ•å½±å›¾æ¡ˆäº§ç”Ÿçš„é¢œè‰²åœºï¼Œå®ç°å¯¹å‡ ä½•åœºçš„ç‹¬ç«‹ä¼˜åŒ–ã€‚è¿™æœ‰åŠ©äºæ›´å¿«çš„æ”¶æ•›å’Œæ›´é«˜è´¨é‡çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†å½’ä¸€åŒ–è®¾å¤‡åæ ‡ï¼ˆNDCï¼‰ã€ç•¸å˜æŸå¤±å’ŒåŸºäºè¡¨é¢çš„é¢œè‰²æŸå¤±ï¼Œä»¥æé«˜å‡ ä½•ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘é‡åœºæ™¯ä¸‹çš„å‡ ä½•æ€§èƒ½è¡¨ç°ä¼˜äºç°æœ‰çš„åŸºäºåŒ¹é…çš„æŠ€æœ¯ã€‚åœ¨åˆæˆåœºæ™¯ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡ä¼°è®¡æ·±åº¦è¯¯å·®ä¸Šå¤§çº¦å‡å°‘äº†60%ï¼Œåœ¨çœŸå®æ•è·çš„åœºæ™¯ä¸Šå¤§çº¦å‡å°‘äº†30%ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå¤§çº¦æ˜¯ä¹‹å‰ä½¿ç”¨éšå¼è¡¨ç¤ºçš„æ— åŒ¹é…æ–¹æ³•çš„ä¸‰å€é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07113v1">PDF</a> 10 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç»“æ„åŒ–å…‰ç³»ç»Ÿå›¾åƒçš„æ–°æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚ä¸å…¶ä»–ä¾èµ–å›¾åƒåŒ¹é…è¿‡ç¨‹çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¯†åº¦ä½“ç´ ç½‘æ ¼è¡¨ç¤ºåœºæ™¯å‡ ä½•ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£çš„å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨ç»“æ„åŒ–å…‰ç³»ç»Ÿä¸­æ¸²æŸ“è¿‡ç¨‹ä¸­çš„æŠ•å½±æ¨¡å¼äº§ç”Ÿçš„é¢œè‰²åœºï¼Œå®ç°å‡ ä½•åœºçš„ç‹¬ç«‹ä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†æ”¶æ•›é€Ÿåº¦å’Œè¾“å‡ºè´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜ç»“åˆäº†å½’ä¸€åŒ–è®¾å¤‡åæ ‡ï¼ˆNDCï¼‰ã€ç•¸å˜æŸå¤±å’ŒåŸºäºè¡¨é¢çš„é¢œè‰²æŸå¤±ï¼Œä»¥æé«˜å‡ ä½•ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å‡ ä½•æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„åŸºäºåŒ¹é…çš„æŠ€æœ¯ï¼Œåˆæˆåœºæ™¯çš„å¹³å‡ä¼°è®¡æ·±åº¦è¯¯å·®é™ä½äº†çº¦60%ï¼ŒçœŸå®ä¸–ç•Œæ•è·çš„åœºæ™¯é™ä½äº†çº¦30%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è®­ç»ƒé€Ÿåº¦å¿«ï¼Œå¤§çº¦æ˜¯ä¹‹å‰ä½¿ç”¨éšå¼è¡¨ç¤ºçš„æ— åŒ¹é…æ–¹æ³•çš„ä¸‰å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºç»“æ„åŒ–å…‰ç³»ç»Ÿçš„æ–°å‹æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨å¯†åº¦ä½“ç´ ç½‘æ ¼è¡¨ç¤ºåœºæ™¯å‡ ä½•ï¼Œå¹¶é€šè¿‡è‡ªç›‘ç£å¯å¾®åˆ†ä½“ç§¯æ¸²æŸ“è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åˆ©ç”¨é¢œè‰²åœºå®ç°å‡ ä½•åœºçš„ç‹¬ç«‹ä¼˜åŒ–ï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦å¹¶æé«˜è¾“å‡ºè´¨é‡ã€‚</li>
<li>ç»“åˆå½’ä¸€åŒ–è®¾å¤‡åæ ‡ï¼ˆNDCï¼‰ã€ç•¸å˜æŸå¤±å’ŒåŸºäºè¡¨é¢çš„é¢œè‰²æŸå¤±å¢å¼ºå‡ ä½•ä¿çœŸåº¦ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®åœºæ™¯ä¸‹çš„æ·±åº¦ä¼°è®¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•æ˜¾è‘—å‡å°‘äº†ä¼°è®¡æ·±åº¦è¯¯å·®ã€‚</li>
<li>è¯¥æ–¹æ³•çš„è®­ç»ƒé€Ÿåº¦è¾ƒå¿«ï¼Œç›¸è¾ƒäºå…¶ä»–æ— åŒ¹é…æ–¹æ³•æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚</li>
<li>ä¸ºæ·±åº¦ä¼°è®¡é¢†åŸŸæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œæœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Motion-Tracks-A-Unified-Representation-for-Human-Robot-Transfer-in-Few-Shot-Imitation-Learning"><a href="#Motion-Tracks-A-Unified-Representation-for-Human-Robot-Transfer-in-Few-Shot-Imitation-Learning" class="headerlink" title="Motion Tracks: A Unified Representation for Human-Robot Transfer in   Few-Shot Imitation Learning"></a>Motion Tracks: A Unified Representation for Human-Robot Transfer in   Few-Shot Imitation Learning</h2><p><strong>Authors:Juntao Ren, Priya Sundaresan, Dorsa Sadigh, Sanjiban Choudhury, Jeannette Bohg</strong></p>
<p>Teaching robots to autonomously complete everyday tasks remains a challenge. Imitation Learning (IL) is a powerful approach that imbues robots with skills via demonstrations, but is limited by the labor-intensive process of collecting teleoperated robot data. Human videos offer a scalable alternative, but it remains difficult to directly train IL policies from them due to the lack of robot action labels. To address this, we propose to represent actions as short-horizon 2D trajectories on an image. These actions, or motion tracks, capture the predicted direction of motion for either human hands or robot end-effectors. We instantiate an IL policy called Motion Track Policy (MT-pi) which receives image observations and outputs motion tracks as actions. By leveraging this unified, cross-embodiment action space, MT-pi completes tasks with high success given just minutes of human video and limited additional robot demonstrations. At test time, we predict motion tracks from two camera views, recovering 6DoF trajectories via multi-view synthesis. MT-pi achieves an average success rate of 86.5% across 4 real-world tasks, outperforming state-of-the-art IL baselines which do not leverage human data or our action space by 40%, and generalizes to scenarios seen only in human videos. Code and videos are available on our website <a target="_blank" rel="noopener" href="https://portal-cornell.github.io/motion_track_policy/">https://portal-cornell.github.io/motion_track_policy/</a>. </p>
<blockquote>
<p>æ•™æˆæœºå™¨äººè‡ªä¸»å®Œæˆæ—¥å¸¸ä»»åŠ¡ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ˜¯ä¸€ç§é€šè¿‡æ¼”ç¤ºèµ‹äºˆæœºå™¨äººæŠ€èƒ½çš„å¼ºå¤§æ–¹æ³•ï¼Œä½†å—é™äºæ”¶é›†é¥æ§æœºå™¨äººæ•°æ®çš„è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦é«˜ã€‚äººç±»è§†é¢‘æä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ç”±äºç¼ºä¹æœºå™¨äººåŠ¨ä½œæ ‡ç­¾ï¼Œä»äººç±»è§†é¢‘ä¸­ç›´æ¥è®­ç»ƒILç­–ç•¥ä»ç„¶å¾ˆå›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå°†åŠ¨ä½œè¡¨ç¤ºä¸ºå›¾åƒä¸Šçš„çŸ­å‘¨æœŸ2Dè½¨è¿¹ã€‚è¿™äº›åŠ¨ä½œæˆ–è¿åŠ¨è½¨è¿¹æ•æ‰äº†äººç±»æ‰‹éƒ¨æˆ–æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨é¢„æµ‹çš„è¿åŠ¨æ–¹å‘ã€‚æˆ‘ä»¬å®ä¾‹åŒ–äº†ä¸€ç§åä¸ºè¿åŠ¨è½¨è¿¹ç­–ç•¥ï¼ˆMT-piï¼‰çš„ILç­–ç•¥ï¼Œå®ƒæ¥æ”¶å›¾åƒè§‚å¯Ÿç»“æœå¹¶è¾“å‡ºè¿åŠ¨è½¨è¿¹ä½œä¸ºåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨è¿™ç§ç»Ÿä¸€çš„ã€è·¨å®ä½“çš„åŠ¨ä½œç©ºé—´ï¼ŒMT-piä»…ä½¿ç”¨å‡ åˆ†é’Ÿçš„äººç±»è§†é¢‘å’Œæœ‰é™çš„é¢å¤–æœºå™¨äººæ¼”ç¤ºå³å¯æˆåŠŸå®Œæˆä»»åŠ¡ã€‚åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªæ‘„åƒæœºè§†è§’é¢„æµ‹è¿åŠ¨è½¨è¿¹ï¼Œé€šè¿‡å¤šè§†è§’åˆæˆæ¢å¤6DoFè½¨è¿¹ã€‚MT-piåœ¨4ä¸ªçœŸå®ä»»åŠ¡ä¸­çš„å¹³å‡æˆåŠŸç‡è¾¾åˆ°86.5%ï¼Œæ¯”ä¸åˆ©ç”¨äººç±»æ•°æ®æˆ–æˆ‘ä»¬åŠ¨ä½œç©ºé—´çš„æœ€æ–°ILåŸºçº¿é«˜å‡º40%ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¨å¹¿åˆ°ä»…åœ¨äººç±»è§†é¢‘ä¸­å‡ºç°çš„åœºæ™¯ã€‚æˆ‘ä»¬çš„ç½‘ç«™<a target="_blank" rel="noopener" href="https://portal-cornell.github.io/motion_track_policy/%E4%B8%8A%E6%9C%89%E4%BB%A3%E7%A0%81%E5%92%8C%E8%A7%86%E9%A2%91%E5%8F%AF%E4%BE%9B%E6%9F%A5%E7%9C%8B%E3%80%82">https://portal-cornell.github.io/motion_track_policy/ä¸Šæœ‰ä»£ç å’Œè§†é¢‘å¯ä¾›æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æœºå™¨äººè‡ªä¸»å®Œæˆæ—¥å¸¸ä»»åŠ¡çš„æ•™å­¦æŒ‘æˆ˜ã€‚æå‡ºäº†é€šè¿‡å°†åŠ¨ä½œè¡¨ç¤ºä¸ºå›¾åƒä¸Šçš„çŸ­å‘¨æœŸäºŒç»´è½¨è¿¹æ¥è§£å†³æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰é¢ä¸´çš„æŒ‘æˆ˜ã€‚æå‡ºäº†ä¸€é¡¹åä¸ºè¿åŠ¨è½¨è¿¹ç­–ç•¥ï¼ˆMT-piï¼‰çš„æ¨¡ä»¿å­¦ä¹ ç­–ç•¥ï¼Œå¯ä»äººç±»è§†é¢‘æ¥æ”¶å›¾åƒè§‚å¯Ÿå¹¶ä»¥è¿åŠ¨è½¨è¿¹çš„å½¢å¼è¾“å‡ºåŠ¨ä½œã€‚é€šè¿‡åˆ©ç”¨è¿™ä¸€ç»Ÿä¸€çš„è·¨ä½“ç°åŠ¨ä½œç©ºé—´ï¼ŒMT-piä»…ä½¿ç”¨å‡ åˆ†é’Ÿçš„äººç±»è§†é¢‘å’Œæœ‰é™çš„é¢å¤–æœºå™¨äººæ¼”ç¤ºå³èƒ½æˆåŠŸå®Œæˆä»»åŠ¡ã€‚æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬ä»ä¸¤ä¸ªæ‘„åƒæœºè§†è§’é¢„æµ‹è¿åŠ¨è½¨è¿¹ï¼Œé€šè¿‡å¤šè§†è§’åˆæˆæ¢å¤å…­è‡ªç”±åº¦è½¨è¿¹ã€‚MT-piåœ¨å››ä¸ªçœŸå®ä»»åŠ¡ä¸­çš„å¹³å‡æˆåŠŸç‡è¾¾åˆ°86.5%ï¼Œä¼˜äºä¸åˆ©ç”¨äººç±»æ•°æ®æˆ–æˆ‘ä»¬åŠ¨ä½œç©ºé—´çš„æœ€æ–°ILåŸºçº¿ï¼Œå¹¶æ¨å¹¿åˆ°ä»…å­˜åœ¨äºäººç±»è§†é¢‘çš„åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººè‡ªä¸»å®Œæˆæ—¥å¸¸ä»»åŠ¡çš„æ•™å­¦å­˜åœ¨æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ•°æ®æ”¶é›†æ–¹é¢ã€‚</li>
<li>æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„æœºå™¨äººæŠ€èƒ½ä¼ æˆæ–¹æ³•ï¼Œä½†éœ€å¤§é‡çš„æœºå™¨äººæ•°æ®ï¼Œäººç±»è§†é¢‘æä¾›äº†ä¸€ç§å¯æ›¿ä»£çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡ä»¿å­¦ä¹ ç­–ç•¥â€”â€”è¿åŠ¨è½¨è¿¹ç­–ç•¥ï¼ˆMT-piï¼‰ï¼Œèƒ½ä»äººç±»è§†é¢‘ä¸­å­¦ä¹ å¹¶æˆåŠŸå®Œæˆä»»åŠ¡ã€‚</li>
<li>MT-pié€šè¿‡å°†åŠ¨ä½œè¡¨ç¤ºä¸ºå›¾åƒä¸Šçš„çŸ­å‘¨æœŸäºŒç»´è½¨è¿¹æ¥è§£å†³ç›´æ¥ä»äººç±»è§†é¢‘è®­ç»ƒILç­–ç•¥çš„æŒ‘æˆ˜ã€‚</li>
<li>MT-piåˆ©ç”¨ç»Ÿä¸€çš„è·¨ä½“ç°åŠ¨ä½œç©ºé—´ï¼Œä»…ä½¿ç”¨å‡ åˆ†é’Ÿçš„äººç±»è§†é¢‘å’Œæœ‰é™çš„é¢å¤–æœºå™¨äººæ¼”ç¤ºå°±èƒ½æˆåŠŸå®Œæˆä»»åŠ¡ã€‚</li>
<li>åœ¨å››ä¸ªçœŸå®ä»»åŠ¡çš„æµ‹è¯•ä¸­ï¼ŒMT-piçš„å¹³å‡æˆåŠŸç‡è¾¾åˆ°86.5%ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–ILç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle two major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to state-of-the-art monocular methods that require thousands of real training images, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SynShotï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å¯é©¾é©¶å¤´éƒ¨åŒ–èº«å°‘æ ·æœ¬åè½¬çš„æ–°å‹æ–¹æ³•ã€‚æˆ‘ä»¬è§£å†³äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œè®­ç»ƒå¯æ§çš„3Dç”Ÿæˆç½‘ç»œéœ€è¦å¤§é‡çš„ä¸åŒåºåˆ—ï¼Œè€Œå›¾åƒå’Œé«˜è´¨é‡è·Ÿè¸ªç½‘æ ¼çš„é…å¯¹å¹¶ä¸æ€»æ˜¯å¯ç”¨çš„ã€‚å…¶æ¬¡ï¼Œæœ€å…ˆè¿›çš„å•ç›®åŒ–èº«æ¨¡å‹å¾ˆéš¾æ¨å¹¿åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…ï¼Œç¼ºä¹å¼ºå¤§çš„å…ˆéªŒçŸ¥è¯†ï¼Œç»å¸¸è¿‡åº¦é€‚åº”ç‰¹å®šçš„è§†è§’åˆ†å¸ƒã€‚å—åˆ°ä»…ç”±åˆæˆæ•°æ®è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»å¤§é‡åˆæˆå¤´éƒ¨æ•°æ®ä¸­å­¦ä¹ å…ˆéªŒæ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™äº›åˆæˆæ•°æ®å…·æœ‰ä¸åŒçš„èº«ä»½ã€è¡¨æƒ…å’Œè§†è§’ã€‚å‡­å€Ÿå°‘é‡çš„è¾“å…¥å›¾åƒï¼ŒSynShotå¯¹é¢„è®­ç»ƒçš„åˆæˆå…ˆéªŒè¿›è¡Œäº†å¾®è°ƒï¼Œä»¥å¼¥åˆé¢†åŸŸä¹‹é—´çš„å·®è·ï¼Œä»è€Œæ¨¡æ‹Ÿä¸€ä¸ªé€šç”¨åˆ°å„ç§æ–°è¡¨æƒ…å’Œè§†è§’çš„çœŸå®å¤´éƒ¨åŒ–èº«ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç»´é«˜æ–¯å–·ç»˜å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨æ¥è¾“å‡ºUVçº¹ç†ç©ºé—´çš„é«˜æ–¯å‚æ•°æ¥æ¨¡æ‹Ÿå¤´éƒ¨åŒ–èº«ã€‚è€ƒè™‘åˆ°å¤´éƒ¨å„éƒ¨åˆ†å»ºæ¨¡å¤æ‚æ€§çš„å·®å¼‚ï¼ˆä¾‹å¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡å…ˆéªŒåµŒå…¥æ¥æ˜ç¡®æ§åˆ¶æ¯ä¸ªéƒ¨åˆ†åŸå§‹æ•°æ®çš„ä¸Šé‡‡æ ·æ•°é‡ã€‚ä¸éœ€è¦æ•°åƒå¼ çœŸå®è®­ç»ƒå›¾åƒçš„æœ€æ–°å•ç›®æ–¹æ³•ç›¸æ¯”ï¼ŒSynShotæå¤§åœ°æ”¹è¿›äº†æ–°é¢–è§†è§’å’Œè¡¨æƒ…çš„åˆæˆæ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v1">PDF</a> Website <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SynShotæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°å¤´éƒ¨å¯é©±åŠ¨è§’è‰²çš„æ–°æ–¹æ³•ã€‚è§£å†³äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹å¤§è§„æ¨¡å¤šæ ·åºåˆ—çš„è®­ç»ƒæ•°æ®å’Œå¯¹æ–°è§†è§’å’Œè¡¨æƒ…çš„æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚SynShotå­¦ä¹ äº†ä¸€ä¸ªå…ˆéªŒæ¨¡å‹ï¼Œä»å¤§é‡åˆæˆå¤´éƒ¨æ•°æ®ä¸­è®­ç»ƒå¾—æ¥ï¼Œå¹¶åˆ©ç”¨å°‘é‡è¾“å…¥å›¾åƒå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥ç¼©å°é¢†åŸŸå·®è·ï¼Œç”Ÿæˆé€¼çœŸçš„å¤´éƒ¨è§’è‰²æ¨¡å‹ï¼Œèƒ½å¤Ÿæ³›åŒ–åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…ã€‚é€šè¿‡3Dé«˜æ–¯æ¶‚ç‰‡å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨å»ºæ¨¡å¤´éƒ¨è§’è‰²ï¼Œè¾“å‡ºUVçº¹ç†ç©ºé—´çš„é«˜æ–¯å‚æ•°ã€‚é’ˆå¯¹å¤´éƒ¨ä¸åŒéƒ¨åˆ†çš„å»ºæ¨¡å¤æ‚æ€§ï¼ˆå¦‚çš®è‚¤å’Œå¤´å‘ï¼‰ï¼ŒåµŒå…¥å…ˆéªŒçŸ¥è¯†ä»¥æ§åˆ¶æ¯ä¸ªéƒ¨åˆ†çš„åŸå§‹æ•°é‡ã€‚ç›¸è¾ƒäºéœ€è¦æ•°åƒå¼ çœŸå®è®­ç»ƒå›¾åƒçš„æœ€å…ˆè¿›å•çœ¼æ–¹æ³•ï¼ŒSynShotæ˜¾è‘—æ”¹å–„äº†å¯¹æ–°è§†è§’å’Œè¡¨æƒ…çš„åˆæˆæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynShotæ˜¯ä¸€ç§åŸºäºåˆæˆå…ˆéªŒçš„å°‘æ•°å¤´éƒ¨å¯é©±åŠ¨è§’è‰²çš„æ–°æ–¹æ³•ã€‚</li>
<li>è§£å†³ç¼ºä¹å¤§è§„æ¨¡å¤šæ ·åºåˆ—è®­ç»ƒæ•°æ®å’Œæ³›åŒ–èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡å­¦ä¹ å…ˆéªŒæ¨¡å‹ï¼Œåˆ©ç”¨å°‘é‡è¾“å…¥å›¾åƒå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œç”Ÿæˆé€¼çœŸçš„å¤´éƒ¨è§’è‰²æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°æ–°çš„è§†è§’å’Œè¡¨æƒ…ã€‚</li>
<li>é‡‡ç”¨3Dé«˜æ–¯æ¶‚ç‰‡å’Œå·ç§¯ç¼–ç å™¨è§£ç å™¨å»ºæ¨¡å¤´éƒ¨è§’è‰²ã€‚</li>
<li>è¾“å‡ºUVçº¹ç†ç©ºé—´çš„å‚æ•°ä»¥ç²¾ç»†æ§åˆ¶æ¨¡å‹çš„å¤–è§‚å’Œçº¹ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Foundational-Generative-Model-for-Breast-Ultrasound-Image-Analysis"><a href="#A-Foundational-Generative-Model-for-Breast-Ultrasound-Image-Analysis" class="headerlink" title="A Foundational Generative Model for Breast Ultrasound Image Analysis"></a>A Foundational Generative Model for Breast Ultrasound Image Analysis</h2><p><strong>Authors:Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Haotian Ye, Siyu He, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, James Zou, Qingli Zhu, Yong Wang, Liwei Wang</strong></p>
<p>Foundational models have emerged as powerful tools for addressing various tasks in clinical settings. However, their potential development to breast ultrasound analysis remains untapped. In this paper, we present BUSGen, the first foundational generative model specifically designed for breast ultrasound image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen has acquired extensive knowledge of breast structures, pathological features, and clinical variations. With few-shot adaptation, BUSGen can generate repositories of realistic and informative task-specific data, facilitating the development of models for a wide range of downstream tasks. Extensive experiments highlight BUSGenâ€™s exceptional adaptability, significantly exceeding real-data-trained foundational models in breast cancer screening, diagnosis, and prognosis. In breast cancer early diagnosis, our approach outperformed all board-certified radiologists (n&#x3D;9), achieving an average sensitivity improvement of 16.5% (P-value&lt;0.0001). Additionally, we characterized the scaling effect of using generated data which was as effective as the collected real-world data for training diagnostic models. Moreover, extensive experiments demonstrated that our approach improved the generalization ability of downstream models. Importantly, BUSGen protected patient privacy by enabling fully de-identified data sharing, making progress forward in secure medical data utilization. An online demo of BUSGen is available at <a target="_blank" rel="noopener" href="https://aibus.bio/">https://aibus.bio</a>. </p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹å·²ä½œä¸ºä¸´åºŠç¯å¢ƒä¸­è§£å†³å„ç§ä»»åŠ¡çš„å¼ºå¤§å·¥å…·è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¹³è…ºè¶…å£°åˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªè¢«å¼€å‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†BUSGenï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºä¹³è…ºè¶…å£°å›¾åƒåˆ†æè®¾è®¡çš„åŸºç¡€ç”Ÿæˆæ¨¡å‹ã€‚åœ¨è¶…è¿‡350ä¸‡å¼ ä¹³è…ºè¶…å£°å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼ŒBUSGenè·å¾—äº†å…³äºä¹³æˆ¿ç»“æ„ã€ç—…ç†ç‰¹å¾å’Œä¸´åºŠå˜åŒ–çš„å¹¿æ³›çŸ¥è¯†ã€‚é€šè¿‡å°‘é‡æ ·æœ¬é€‚åº”ï¼ŒBUSGenå¯ä»¥ç”Ÿæˆç°å®ä¸”ä¿¡æ¯ä¸°å¯Œçš„ç‰¹å®šä»»åŠ¡æ•°æ®ä»“åº“ï¼Œä¿ƒè¿›é’ˆå¯¹å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹å‘å±•ã€‚å¤§é‡å®éªŒçªå‡ºäº†BUSGençš„å“è¶Šé€‚åº”æ€§ï¼Œåœ¨ä¹³è…ºç™Œç­›æŸ¥ã€è¯Šæ–­å’Œé¢„åæ–¹é¢æ˜¾è‘—è¶…è¿‡äº†ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚åœ¨ä¹³è…ºç™Œæ—©æœŸè¯Šæ–­æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æ‰€æœ‰æ‰§ä¸šåŒ»å¸ˆè®¤è¯çš„æ”¾å°„ç§‘åŒ»ç”Ÿï¼ˆn&#x3D;9ï¼‰ï¼Œå¹³å‡æ•æ„Ÿæ€§æé«˜äº†16.5%ï¼ˆPå€¼&lt;0.0001ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æè¿°äº†ä½¿ç”¨ç”Ÿæˆæ•°æ®ä¸æ”¶é›†çš„çœŸå®ä¸–ç•Œæ•°æ®ä¸€æ ·æœ‰æ•ˆçš„æ‰©å±•æ•ˆåº”ï¼Œç”¨äºè®­ç»ƒè¯Šæ–­æ¨¡å‹ã€‚è€Œä¸”ï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é‡è¦çš„æ˜¯ï¼ŒBUSGené€šè¿‡å®ç°å®Œå…¨åŒ¿åæ•°æ®å…±äº«ä¿æŠ¤äº†æ‚£è€…éšç§ï¼Œåœ¨å®‰å…¨åŒ»ç–—æ•°æ®åˆ©ç”¨æ–¹é¢å–å¾—äº†è¿›å±•ã€‚BUSGençš„åœ¨çº¿æ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://aibus.bioæŸ¥çœ‹./">https://aibus.bioæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06869v1">PDF</a> Peking University; Stanford University; Peking University Cancer   Hospital &amp; Institute; Peking Union Medical College Hospital; Cancer Hospital,   Chinese Academy of Medical Sciences</p>
<p><strong>Summary</strong></p>
<p>BUSGenæ˜¯ä¸“é—¨ä¸ºä¹³è…ºè¶…å£°å›¾åƒåˆ†æè®¾è®¡çš„åŸºç¡€ç”Ÿæˆæ¨¡å‹ã€‚å®ƒåœ¨è¶…è¿‡350ä¸‡å¼ ä¹³è…ºè¶…å£°å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å¯é€šè¿‡å°‘é‡æ ·æœ¬é€‚åº”ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„æ•°æ®ã€‚è¯¥æ¨¡å‹åœ¨ä¹³è…ºç™Œç­›æŸ¥ã€è¯Šæ–­å’Œé¢„åç­‰æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚å®ƒæé«˜äº†ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å®ç°äº†æ‚£è€…æ•°æ®çš„å®Œå…¨åŒ¿åå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BUSGenæ˜¯é¦–ä¸ªé’ˆå¯¹ä¹³è…ºè¶…å£°åˆ†æçš„åŸºç¡€ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤§é‡ä¹³è…ºè¶…å£°å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–ä¹³æˆ¿ç»“æ„ã€ç—…ç†ç‰¹å¾å’Œä¸´åºŠå˜åŒ–ã€‚</li>
<li>é€šè¿‡å°‘é‡æ ·æœ¬é€‚åº”ï¼ŒBUSGenå¯ä»¥ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„æ•°æ®ã€‚</li>
<li>BUSGenåœ¨ä¹³è…ºç™Œç­›æŸ¥ã€è¯Šæ–­å’Œé¢„åæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹æé«˜äº†ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>BUSGenå®ç°äº†æ‚£è€…æ•°æ®çš„å®Œå…¨åŒ¿åå…±äº«ï¼Œä¿ƒè¿›äº†åŒ»ç–—æ•°æ®çš„åˆ©ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06869v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06869v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06869v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="An-efficient-approach-to-represent-enterprise-web-application-structure-using-Large-Language-Model-in-the-service-of-Intelligent-Quality-Engineering"><a href="#An-efficient-approach-to-represent-enterprise-web-application-structure-using-Large-Language-Model-in-the-service-of-Intelligent-Quality-Engineering" class="headerlink" title="An efficient approach to represent enterprise web application structure   using Large Language Model in the service of Intelligent Quality Engineering"></a>An efficient approach to represent enterprise web application structure   using Large Language Model in the service of Intelligent Quality Engineering</h2><p><strong>Authors:Zaber Al Hassan Ayon, Gulam Husain, Roshankumar Bisoi, Waliur Rahman, Dr Tom Osborn</strong></p>
<p>This paper presents a novel approach to represent enterprise web application structures using Large Language Models (LLMs) to enable intelligent quality engineering at scale. We introduce a hierarchical representation methodology that optimizes the few-shot learning capabilities of LLMs while preserving the complex relationships and interactions within web applications. The approach encompasses five key phases: comprehensive DOM analysis, multi-page synthesis, test suite generation, execution, and result analysis. Our methodology addresses existing challenges around usage of Generative AI techniques in automated software testing by developing a structured format that enables LLMs to understand web application architecture through in-context learning. We evaluated our approach using two distinct web applications: an e-commerce platform (Swag Labs) and a healthcare application (MediBox) which is deployed within Atalgo engineering environment. The results demonstrate success rates of 90% and 70%, respectively, in achieving automated testing, with high relevance scores for test cases across multiple evaluation criteria. The findings suggest that our representation approach significantly enhances LLMsâ€™ ability to generate contextually relevant test cases and provide better quality assurance overall, while reducing the time and effort required for testing. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç¤ºä¼ä¸šWebåº”ç”¨ç¨‹åºç»“æ„çš„æ–°æ–¹æ³•ï¼Œä»¥å®ç°å¤§è§„æ¨¡æ™ºèƒ½è´¨é‡å·¥ç¨‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚è¡¨ç¤ºæ³•ï¼Œè¯¥æ–¹æ³•åœ¨ä¼˜åŒ–LLMçš„å°‘é‡å­¦ä¹ åŠŸèƒ½çš„åŒæ—¶ï¼Œä¿æŒWebåº”ç”¨ç¨‹åºå†…éƒ¨çš„å¤æ‚å…³ç³»å’Œäº¤äº’ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬äº”ä¸ªå…³é”®é˜¶æ®µï¼šå…¨é¢çš„DOMåˆ†æã€å¤šé¡µé¢åˆæˆã€æµ‹è¯•å¥—ä»¶ç”Ÿæˆã€æ‰§è¡Œå’Œç»“æœåˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼€å‘ä¸€ç§ç»“æ„åŒ–æ ¼å¼ï¼Œä½¿LLMèƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç†è§£Webåº”ç”¨ç¨‹åºæ¶æ„ï¼Œè§£å†³äº†åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•ä¸­ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„Webåº”ç”¨ç¨‹åºè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šç”µå­å•†åŠ¡å¹³å°ï¼ˆSwag Labsï¼‰å’Œéƒ¨ç½²åœ¨Atalgoå·¥ç¨‹ç¯å¢ƒä¸­çš„åŒ»ç–—åº”ç”¨ç¨‹åºï¼ˆMediBoxï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨è‡ªåŠ¨åŒ–æµ‹è¯•æ–¹é¢ï¼Œç”µå­å•†åŠ¡å¹³å°çš„æˆåŠŸç‡è¾¾åˆ°90%ï¼ŒåŒ»ç–—åº”ç”¨ç¨‹åºçš„æˆåŠŸç‡è¾¾åˆ°70%ï¼Œæµ‹è¯•ç”¨ä¾‹åœ¨å¤šä¸ªè¯„ä¼°æ ‡å‡†ä¸Šå…·æœ‰å¾ˆé«˜çš„ç›¸å…³æ€§å¾—åˆ†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¡¨ç¤ºæ³•æ˜¾è‘—æé«˜äº†LLMç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³æµ‹è¯•ç”¨ä¾‹çš„èƒ½åŠ›ï¼Œå¹¶æ€»ä½“ä¸Šæä¾›äº†æ›´å¥½çš„è´¨é‡ä¿è¯ï¼ŒåŒæ—¶å‡å°‘äº†æµ‹è¯•æ‰€éœ€çš„æ—¶é—´å’Œç²¾åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06837v1">PDF</a> 16 pages, 1 figure and 4 tables, relevant for Gen AI and enterprise   AI use cases</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ™ºèƒ½è´¨é‡å·¥ç¨‹çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä»£è¡¨ä¼ä¸šWebåº”ç”¨ç¨‹åºç»“æ„ã€‚å¼•å…¥äº†ä¸€ç§å±‚æ¬¡åŒ–çš„è¡¨ç¤ºæ–¹æ³•ï¼Œä¼˜åŒ–äº†LLMsçš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†Webåº”ç”¨ç¨‹åºå†…çš„å¤æ‚å…³ç³»å’Œäº¤äº’ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬äº”ä¸ªå…³é”®é˜¶æ®µï¼šå…¨é¢çš„DOMåˆ†æã€å¤šé¡µé¢åˆæˆã€æµ‹è¯•å¥—ä»¶ç”Ÿæˆã€æ‰§è¡Œå’Œç»“æœåˆ†æã€‚é€šè¿‡å¼€å‘ä¸€ç§ç»“æ„åŒ–æ ¼å¼ï¼Œä½¿LLMsèƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç†è§£Webåº”ç”¨ç¨‹åºæ¶æ„ï¼Œè§£å†³äº†åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•ä¸­ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ç°æœ‰æŒ‘æˆ˜ã€‚ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„Webåº”ç”¨ç¨‹åºï¼ˆç”µå­å•†åŠ¡å¹³å°çš„Swag Labså’ŒAtalgoå·¥ç¨‹ç¯å¢ƒä¸­çš„åŒ»ç–—ä¿å¥åº”ç”¨ç¨‹åºMediBoxï¼‰è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè‡ªåŠ¨åŒ–æµ‹è¯•çš„æˆåŠŸç‡åˆ†åˆ«ä¸º90%å’Œ70%ï¼Œæµ‹è¯•ç”¨ä¾‹çš„ç›¸å…³æ€§å¾—åˆ†è¾ƒé«˜ã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„è¡¨ç¤ºæ–¹æ³•æ˜¾è‘—æé«˜äº†LLMsç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³æµ‹è¯•ç”¨ä¾‹çš„èƒ½åŠ›ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„è´¨é‡ä¿è¯ï¼ŒåŒæ—¶å‡å°‘äº†æµ‹è¯•æ‰€éœ€çš„æ—¶é—´å’Œç²¾åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œä¼ä¸šWebåº”ç”¨ç¨‹åºçš„æ™ºèƒ½è´¨é‡å·¥ç¨‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å±‚æ¬¡åŒ–è¡¨ç¤ºæ–¹æ³•ï¼Œä¼˜åŒ–äº†LLMsçš„å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬å…¨é¢çš„DOMåˆ†æã€å¤šé¡µé¢åˆæˆã€æµ‹è¯•å¥—ä»¶ç”Ÿæˆã€æ‰§è¡Œå’Œç»“æœåˆ†æã€‚</li>
<li>é€šè¿‡å¼€å‘ç»“æ„åŒ–æ ¼å¼ï¼Œè§£å†³äº†åœ¨è‡ªåŠ¨åŒ–è½¯ä»¶æµ‹è¯•ä¸­ä½¿ç”¨ç”Ÿæˆå¼AIæŠ€æœ¯çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªWebåº”ç”¨ç¨‹åºçš„è¯„ä¼°ï¼Œæ˜¾ç¤ºäº†è‡ªåŠ¨åŒ–æµ‹è¯•çš„é«˜æˆåŠŸç‡ã€‚</li>
<li>è¡¨ç¤ºæ–¹æ³•æé«˜äº†LLMsç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³æµ‹è¯•ç”¨ä¾‹çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06837">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06837v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Comparing-Few-Shot-Prompting-of-GPT-4-LLMs-with-BERT-Classifiers-for-Open-Response-Assessment-in-Tutor-Equity-Training"><a href="#Comparing-Few-Shot-Prompting-of-GPT-4-LLMs-with-BERT-Classifiers-for-Open-Response-Assessment-in-Tutor-Equity-Training" class="headerlink" title="Comparing Few-Shot Prompting of GPT-4 LLMs with BERT Classifiers for   Open-Response Assessment in Tutor Equity Training"></a>Comparing Few-Shot Prompting of GPT-4 LLMs with BERT Classifiers for   Open-Response Assessment in Tutor Equity Training</h2><p><strong>Authors:Sanjit Kakarla, Conrad Borchers, Danielle Thomas, Shambhavi Bhushan, Kenneth R. Koedinger</strong></p>
<p>Assessing learners in ill-defined domains, such as scenario-based human tutoring training, is an area of limited research. Equity training requires a nuanced understanding of context, but do contemporary large language models (LLMs) have a knowledge base that can navigate these nuances? Legacy transformer models like BERT, in contrast, have less real-world knowledge but can be more easily fine-tuned than commercial LLMs. Here, we study whether fine-tuning BERT on human annotations outperforms state-of-the-art LLMs (GPT-4o and GPT-4-Turbo) with few-shot prompting and instruction. We evaluate performance on four prediction tasks involving generating and explaining open-ended responses in advocacy-focused training lessons in a higher education student population learning to become middle school tutors. Leveraging a dataset of 243 human-annotated open responses from tutor training lessons, we find that BERT demonstrates superior performance using an offline fine-tuning approach, which is more resource-efficient than commercial GPT models. We conclude that contemporary GPT models may not adequately capture nuanced response patterns, especially in complex tasks requiring explanation. This work advances the understanding of AI-driven learner evaluation under the lens of fine-tuning versus few-shot prompting on the nuanced task of equity training, contributing to more effective training solutions and assisting practitioners in choosing adequate assessment methods. </p>
<blockquote>
<p>åœ¨åŸºäºæƒ…æ™¯çš„äººç±»è¾…å¯¼è®­ç»ƒç­‰ä¸æ˜ç¡®é¢†åŸŸä¸­è¯„ä¼°å­¦ä¹ è€…æ˜¯ä¸€ä¸ªç ”ç©¶è¾ƒå°‘çš„é¢†åŸŸã€‚å…¬å¹³è®­ç»ƒéœ€è¦å¯¹èƒŒæ™¯æœ‰å¾®å¦™çš„ç†è§£ï¼Œä½†å½“ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦æ‹¥æœ‰èƒ½å¤Ÿåº”å¯¹è¿™äº›å¾®å¦™ä¹‹å¤„çš„çŸ¥è¯†åº“å‘¢ï¼Ÿç›¸æ¯”ä¹‹ä¸‹ï¼Œé—ç•™çš„è½¬æ¢å™¨æ¨¡å‹ï¼ˆå¦‚BERTï¼‰è™½ç„¶å¯¹çœŸå®ä¸–ç•Œçš„äº†è§£è¾ƒå°‘ï¼Œä½†æ¯”å•†ä¸šLLMæ›´å®¹æ˜“å¾®è°ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¾®è°ƒBERTåœ¨äººç±»æ³¨é‡Šä¸Šçš„è¡¨ç°æ˜¯å¦ä¼˜äºæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹GPT-4oå’ŒGPT-4 Turboï¼Œé‡‡ç”¨å°‘æ ·æœ¬æç¤ºå’ŒæŒ‡ä»¤ã€‚æˆ‘ä»¬åœ¨å››ä¸ªé¢„æµ‹ä»»åŠ¡ä¸­è¯„ä¼°äº†æ€§èƒ½ï¼Œæ¶‰åŠåœ¨é«˜ç­‰æ•™è‚²å­¦ç”Ÿç¾¤ä½“ä¸­ï¼Œä»¥å€¡å¯¼ä¸ºæ ¸å¿ƒçš„å­¦ä¹ æˆä¸ºä¸­å­¦è¾…å¯¼è€å¸ˆçš„åŸ¹è®­è¯¾ç¨‹ä¸­ï¼Œç”Ÿæˆå’Œè§£é‡Šå¼€æ”¾å¼ç­”æ¡ˆçš„å“åº”ã€‚æˆ‘ä»¬åˆ©ç”¨ç”±äººç±»æ³¨é‡Šçš„è¾…å¯¼è®­ç»ƒè¯¾ç¨‹çš„å¼€æ”¾å¼ç­”æ¡ˆæ•°æ®é›†ï¼ˆå…±243ä»½æ•°æ®ï¼‰ï¼Œå‘ç°ä½¿ç”¨ç¦»çº¿å¾®è°ƒæ–¹æ³•çš„BERTè¡¨ç°å‡ºäº†ä¼˜è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œè¯¥æ–¹æ³•æ¯”å•†ä¸šGPTæ¨¡å‹æ›´åŠ èµ„æºé«˜æ•ˆã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå½“ä»£GPTæ¨¡å‹å¯èƒ½æ— æ³•å……åˆ†æ•æ‰å¤æ‚çš„å“åº”æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦è§£é‡Šçš„å¤æ‚ä»»åŠ¡ä¸­ã€‚è¿™é¡¹å·¥ä½œåœ¨å¾®è°ƒä¸å°‘æ ·æœ¬æç¤ºä¸‹ï¼Œè¿›ä¸€æ­¥äº†è§£AIé©±åŠ¨çš„å­¦ä¹ è€…è¯„ä¼°åœ¨å…¬å¹³è®­ç»ƒä»»åŠ¡ä¸­çš„å¾®å¦™ä¹‹å¤„ï¼Œä¸ºæ›´æœ‰æ•ˆçš„è®­ç»ƒè§£å†³æ–¹æ¡ˆåšå‡ºè´¡çŒ®ï¼Œå¹¶å¸®åŠ©ä»ä¸šè€…é€‰æ‹©é€‚å½“çš„è¯„ä¼°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06658v1">PDF</a> 8 Page Workshop Paper, AAAI2025 Workshop on Innovation and   Responsibility in AI-Supported Education (iRAISE) - Open-response Grading,   Feedback, Equity Training, LLMs, BERT, GPT-4</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†è¯„ä¼°åŸºäºæƒ…æ™¯çš„äººç±»è¾…å¯¼è®­ç»ƒä¸­çš„å­¦ä¹ è€…çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä½†é€šè¿‡å¾®è°ƒBERTæ¨¡å‹åœ¨å››é¡¹é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºGPTæ¨¡å‹ã€‚åˆ©ç”¨äººç±»æ³¨é‡Šæ•°æ®é›†è¿›è¡Œç ”ç©¶çš„è¯„ä¼°å‘ç°ï¼Œé€šè¿‡çº¿ä¸‹å¾®è°ƒçš„BERTæ¨¡å‹æ›´å…·ä¼˜åŠ¿ã€‚è¿™æ˜¾ç¤ºå‡ºé’ˆå¯¹å…·ä½“æƒ…å¢ƒè®­ç»ƒéœ€æ±‚çš„ç»†è‡´è€ƒè™‘ï¼Œå¹¶ä¸ºæ›´æœ‰æ•ˆçš„è®­ç»ƒè§£å†³æ–¹æ¡ˆå’Œè¯„ä¼°æ–¹æ³•æä¾›äº†å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é¢†åŸŸï¼šè¯¥ç ”ç©¶å…³æ³¨äºåŸºäºæƒ…æ™¯çš„äººç±»è¾…å¯¼è®­ç»ƒä¸­çš„å­¦ä¹ è€…è¯„ä¼°é—®é¢˜ï¼Œæ¢è®¨GPTæ¨¡å‹å’ŒBERTæ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>å®éªŒè®¾è®¡ï¼šé€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨å››é¡¹é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°BERTæ¨¡å‹æ›´å…·ä¼˜åŠ¿ã€‚é‡‡ç”¨äººç±»æ³¨é‡Šæ•°æ®é›†è¿›è¡Œç ”ç©¶è¯„ä¼°ã€‚</li>
<li>æ€§èƒ½è¯„ä¼°ï¼šç ”ç©¶å‘ç°ï¼Œé€šè¿‡å¾®è°ƒçš„BERTæ¨¡å‹åœ¨é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºGPTæ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚ä»»åŠ¡ä¸­éœ€è¦è§£é‡Šçš„éƒ¨åˆ†ã€‚</li>
<li>èµ„æºæ•ˆç‡ï¼šä½¿ç”¨çº¿ä¸‹å¾®è°ƒçš„æ–¹æ³•ä½¿å¾—BERTæ¨¡å‹æ›´å…·èµ„æºæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06658">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06658v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06658v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BayesAdapter-enhanced-uncertainty-estimation-in-CLIP-few-shot-adaptation"><a href="#BayesAdapter-enhanced-uncertainty-estimation-in-CLIP-few-shot-adaptation" class="headerlink" title="BayesAdapter: enhanced uncertainty estimation in CLIP few-shot   adaptation"></a>BayesAdapter: enhanced uncertainty estimation in CLIP few-shot   adaptation</h2><p><strong>Authors:Pablo Morales-Ãlvarez, Stergios Christodoulidis, Maria Vakalopoulou, Pablo Piantanida, Jose Dolz</strong></p>
<p>The emergence of large pre-trained vision-language models (VLMs) represents a paradigm shift in machine learning, with unprecedented results in a broad span of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited remarkable zero-shot and transfer learning capabilities in classification. To transfer CLIP to downstream tasks, adapters constitute a parameter-efficient approach that avoids backpropagation through the large model (unlike related prompt learning methods). However, CLIP adapters have been developed to target discriminative performance, and the quality of their uncertainty estimates has been overlooked. In this work we show that the discriminative performance of state-of-the-art CLIP adapters does not always correlate with their uncertainty estimation capabilities, which are essential for a safe deployment in real-world scenarios. We also demonstrate that one of such adapters is obtained through MAP inference from a more general probabilistic framework. Based on this observation we introduce BayesAdapter, which leverages Bayesian inference to estimate a full probability distribution instead of a single point, better capturing the variability inherent in the parameter space. In a comprehensive empirical evaluation we show that our approach obtains high quality uncertainty estimates in the predictions, standing out in calibration and selective classification. Our code will be publicly available upon acceptance of the paper. </p>
<blockquote>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°ä»£è¡¨äº†æœºå™¨å­¦ä¹ èŒƒå¼çš„ä¸€ç§è½¬å˜ï¼Œå¹¶åœ¨å¹¿æ³›çš„è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„ç»“æœã€‚CLIPä½œä¸ºæœ€å—æ¬¢è¿çš„VLMä¹‹ä¸€ï¼Œåœ¨åˆ†ç±»æ–¹é¢å±•ç°å‡ºäº†æƒŠäººçš„é›¶æ ·æœ¬å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚ä¸ºäº†å°†CLIPè¿ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ï¼Œé€‚é…å™¨ï¼ˆadaptersï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„ç­–ç•¥ï¼Œé¿å…äº†åœ¨å¤§æ¨¡å‹ä¸­è¿›è¡Œåå‘ä¼ æ’­ï¼ˆè¿™ä¸ç›¸å…³çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸åŒï¼‰ã€‚ç„¶è€Œï¼ŒCLIPé€‚é…å™¨æ—¨åœ¨æé«˜åˆ¤åˆ«æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†å…¶ä¸ç¡®å®šæ€§ä¼°è®¡çš„è´¨é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æœ€å…ˆè¿›çš„CLIPé€‚é…å™¨çš„åˆ¤åˆ«æ€§èƒ½å¹¶ä¸æ€»æ˜¯ä¸ä»–ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡èƒ½åŠ›ç›¸å…³ï¼Œè¿™å¯¹äºåœ¨ç°å®åœºæ™¯ä¸­çš„å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œå…¶ä¸­ä¸€äº›é€‚é…å™¨æ˜¯é€šè¿‡æ›´é€šç”¨çš„æ¦‚ç‡æ¡†æ¶é€šè¿‡æœ€å¤§åéªŒæ¦‚ç‡æ¨æ–­å¾—åˆ°çš„ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†BayesAdapterï¼Œå®ƒåˆ©ç”¨è´å¶æ–¯æ¨æ–­æ¥ä¼°è®¡ä¸€ä¸ªå®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„ç‚¹ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å‚æ•°ç©ºé—´ä¸­å›ºæœ‰çš„å˜å¼‚æ€§ã€‚åœ¨ä¸€é¡¹å…¨é¢çš„ç»éªŒè¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢„æµ‹ä¸­è·å¾—é«˜è´¨é‡çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œåœ¨æ ¡å‡†å’Œé€‰æ‹©æ€§åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09718v2">PDF</a> 30 pages, 5 figures, 23 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°ä»£è¡¨äº†æœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªèŒƒå¼è½¬å˜ï¼Œå·²ç»åœ¨å¹¿æ³›çš„è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†å‰æ‰€æœªæœ‰çš„ç»“æœã€‚CLIPä½œä¸ºä¸€ç§æµè¡Œçš„VLMï¼Œå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚ä¸ºäº†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿ç§»CLIPï¼Œé€‚é…å™¨æˆä¸ºä¸€ç§å‚æ•°é«˜æ•ˆçš„æ–¹æ³•ï¼Œé¿å…äº†é€šè¿‡å¤§å‹æ¨¡å‹çš„åå‘ä¼ æ’­ï¼ˆä¸ç›¸å…³çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸åŒï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„CLIPé€‚é…å™¨ä¸»è¦å…³æ³¨åˆ¤åˆ«æ€§èƒ½ï¼Œå¿½ç•¥äº†ä¸ç¡®å®šæ€§ä¼°è®¡çš„è´¨é‡ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„CLIPé€‚é…å™¨çš„åˆ¤åˆ«æ€§èƒ½å¹¶ä¸æ€»æ˜¯ä¸ä»–ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡èƒ½åŠ›ç›¸å…³ï¼Œè¿™å¯¹äºåœ¨çœŸå®åœºæ™¯ä¸­çš„å®‰å…¨éƒ¨ç½²è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡æ›´é€šç”¨çš„æ¦‚ç‡æ¡†æ¶è¿›è¡ŒMAPæ¨ç†å¯ä»¥è·å¾—ä¸€ç§é€‚é…å™¨ã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬å¼•å…¥äº†BayesAdapterï¼Œå®ƒåˆ©ç”¨è´å¶æ–¯æ¨ç†æ¥ä¼°è®¡ä¸€ä¸ªå®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç‚¹ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰å‚æ•°ç©ºé—´ä¸­çš„å›ºæœ‰å˜åŒ–ã€‚ç»è¿‡ç»¼åˆçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¢„æµ‹ä¸­è·å¾—é«˜è´¨é‡çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œåœ¨æ ¡å‡†å’Œé€‰æ‹©æ€§åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—æˆæœï¼Œæ ‡å¿—ç€æœºå™¨å­¦ä¹ é¢†åŸŸçš„èŒƒå¼è½¬å˜ã€‚</li>
<li>CLIPä½œä¸ºä¸€ç§æµè¡Œçš„VLMï¼Œå…·æœ‰é›¶æ ·æœ¬å’Œè¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>é€‚é…å™¨æ˜¯å‚æ•°é«˜æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿ç§»CLIPï¼Œé¿å…å¤§å‹æ¨¡å‹åå‘ä¼ æ’­ã€‚</li>
<li>ç°æœ‰CLIPé€‚é…å™¨ä¸»è¦å…³æ³¨åˆ¤åˆ«æ€§èƒ½ï¼Œä½†ä¸ç¡®å®šæ€§ä¼°è®¡è´¨é‡åŒæ ·é‡è¦ï¼Œå°¤å…¶åœ¨çœŸå®åœºæ™¯éƒ¨ç½²ä¸­ã€‚</li>
<li>é€šè¿‡æ›´é€šç”¨çš„æ¦‚ç‡æ¡†æ¶è¿›è¡ŒMAPæ¨ç†å¯å¾—åˆ°ä¸€ç§é€‚é…å™¨ã€‚</li>
<li>å¼•å…¥BayesAdapterï¼Œåˆ©ç”¨è´å¶æ–¯æ¨ç†ä¼°è®¡å®Œæ•´çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæé«˜ä¸ç¡®å®šæ€§ä¼°è®¡è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Sparse-Attention-Vectors-Generative-Multimodal-Model-Features-Are-Discriminative-Vision-Language-Classifiers"><a href="#Sparse-Attention-Vectors-Generative-Multimodal-Model-Features-Are-Discriminative-Vision-Language-Classifiers" class="headerlink" title="Sparse Attention Vectors: Generative Multimodal Model Features Are   Discriminative Vision-Language Classifiers"></a>Sparse Attention Vectors: Generative Multimodal Model Features Are   Discriminative Vision-Language Classifiers</h2><p><strong>Authors:Chancharik Mitra, Brandon Huang, Tianning Chai, Zhiqiu Lin, Assaf Arbelle, Rogerio Feris, Leonid Karlinsky, Trevor Darrell, Deva Ramanan, Roei Herzig</strong></p>
<p>Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for foundational discriminative vision-language tasks (i.e., tasks requiring discrete label predictions) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for discriminative tasks is the extraction of useful features from generative models. To overcome this issue, we propose an approach for finding features in the modelâ€™s latent space to more effectively leverage LMMs for discriminative tasks. Toward this end, we present Sparse Attention Vectors (SAVs) â€“ a finetuning-free method that leverages sparse attention head activations (fewer than 1% of the heads) in LMMs as strong features for VL tasks. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of discriminative tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations. </p>
<blockquote>
<p>ç”Ÿæˆå¼å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œå¦‚LLaVAå’ŒQwen-VLï¼Œåœ¨å„ç§è§†è§‰è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ï¼ˆå¦‚å›¾åƒæè¿°æˆ–è§†è§‰é—®ç­”ï¼‰ä¸­éƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡è¡¨ç°å¼ºåŠ²ï¼Œä½†LMMså¹¶ä¸ç›´æ¥é€‚ç”¨äºåŸºç¡€åˆ¤åˆ«å¼è§†è§‰è¯­è¨€ä»»åŠ¡ï¼ˆå³éœ€è¦ç¦»æ•£æ ‡ç­¾é¢„æµ‹çš„ä»»åŠ¡ï¼‰ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©VQAã€‚åˆ©ç”¨LMMsè¿›è¡Œåˆ¤åˆ«ä»»åŠ¡çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯ä»ç”Ÿæˆæ¨¡å‹ä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨æ¨¡å‹æ½œåœ¨ç©ºé—´ä¸­æ‰¾åˆ°ç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨LMMsè¿›è¡Œåˆ¤åˆ«ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–æ³¨æ„åŠ›å‘é‡ï¼ˆSAVsï¼‰â€”â€”è¿™æ˜¯ä¸€ç§æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨LMMä¸­ä¸åˆ°1%çš„ç¨€ç–æ³¨æ„åŠ›å¤´æ¿€æ´»ä½œä¸ºå¼ºå¤§çš„VLä»»åŠ¡ç‰¹å¾ã€‚å‡­å€Ÿå°‘é‡æ ·æœ¬ï¼ŒSAVsåœ¨å¤šä¸ªåˆ¤åˆ«ä»»åŠ¡ä¸Šç›¸å¯¹äºå„ç§å°æ ·æ–¹æ³•å’Œå¾®è°ƒåŸºå‡†çº¿å±•ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜æš—ç¤ºï¼Œéšç€é¢å¤–æ ·æœ¬çš„å¢åŠ ï¼ŒSAVsçš„æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œå¹¶èƒ½å¤Ÿæ¨å¹¿è‡³ç±»ä¼¼ä»»åŠ¡ï¼Œè¿™è¡¨æ˜SAVsæ˜¯æœ‰æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00142v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å¦‚LLaVAå’ŒQwen-VLçš„ä¼˜å¼‚æ€§èƒ½ï¼Œå®ƒä»¬æ“…é•¿å¤šç§è§†è§‰è¯­è¨€ï¼ˆVLï¼‰ä»»åŠ¡ï¼Œå¦‚å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ã€‚ç„¶è€Œï¼Œå¯¹äºéœ€è¦ç¦»æ•£æ ‡ç­¾é¢„æµ‹çš„åŸºç¡€é‰´åˆ«å‹è§†è§‰è¯­è¨€ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œå¤šé¡¹é€‰æ‹©é—®ç­”ç­‰ï¼ŒLMMså¹¶ä¸ç›´æ¥é€‚ç”¨ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨æ¨¡å‹æ½œåœ¨ç©ºé—´å¯»æ‰¾ç‰¹å¾çš„æ–¹æ³•ï¼Œä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨LMMsè¿›è¡Œé‰´åˆ«ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–æ³¨æ„åŠ›å‘é‡ï¼ˆSAVsï¼‰â€”â€”ä¸€ç§æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨LMMsä¸­ä¸åˆ°1%çš„ç¨€ç–æ³¨æ„åŠ›å¤´æ¿€æ´»ä½œä¸ºè§†è§‰è¯­è¨€ä»»åŠ¡çš„å¼ºç‰¹å¾ã€‚ä»…é€šè¿‡å°‘é‡æ ·æœ¬ï¼ŒSAVsåœ¨å¤šä¸ªé‰´åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸å¤šç§å°‘æ ·æœ¬å’Œå¾®è°ƒåŸºçº¿ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œéšç€æ ·æœ¬æ•°é‡çš„å¢åŠ ï¼ŒSAVsçš„æ€§èƒ½å¯è¿›ä¸€æ­¥æå‡ï¼Œå¹¶èƒ½æ¨å¹¿åˆ°ç±»ä¼¼ä»»åŠ¡ï¼Œè¯æ˜äº†å…¶ä½œä¸ºæœ‰æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMså¦‚LLaVAå’ŒQwen-VLæ“…é•¿å›¾åƒæè¿°å’Œè§†è§‰é—®ç­”ç­‰VLä»»åŠ¡ã€‚</li>
<li>LMMsä¸ç›´æ¥é€‚ç”¨äºéœ€è¦ç¦»æ•£æ ‡ç­¾é¢„æµ‹çš„é‰´åˆ«å‹è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨æ¨¡å‹æ½œåœ¨ç©ºé—´å¯»æ‰¾ç‰¹å¾çš„æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨LMMsè¿›è¡Œé‰´åˆ«ä»»åŠ¡ã€‚</li>
<li>SAVsæ˜¯ä¸€ç§æ— éœ€å¾®è°ƒçš„æ–¹æ³•ï¼Œåˆ©ç”¨ç¨€ç–æ³¨æ„åŠ›å¤´æ¿€æ´»ä½œä¸ºå¼ºç‰¹å¾ã€‚</li>
<li>SAVsåœ¨å°‘é‡æ ·æœ¬ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªé‰´åˆ«ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>SAVsæ€§èƒ½å¯éšæ ·æœ¬æ•°é‡å¢åŠ è€Œæå‡ï¼Œå¹¶å…·å¤‡æ¨å¹¿åˆ°ç±»ä¼¼ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>SAVsä½œä¸ºæœ‰æ•ˆä¸”ç¨³å¥çš„å¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºå…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Task-Learning-through-Inverse-Generative-Modeling"><a href="#Few-Shot-Task-Learning-through-Inverse-Generative-Modeling" class="headerlink" title="Few-Shot Task Learning through Inverse Generative Modeling"></a>Few-Shot Task Learning through Inverse Generative Modeling</h2><p><strong>Authors:Aviv Netanyahu, Yilun Du, Antonia Bronars, Jyothish Pari, Joshua Tenenbaum, Tianmin Shu, Pulkit Agrawal</strong></p>
<p>Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), our method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. We evaluate our method in five domains â€“ object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. Our experimental results demonstrate that via the pretrained generative model, we successfully learn novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts. </p>
<blockquote>
<p>ä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ ä»£ç†çš„æ„å›¾ï¼ˆç”±å…¶ç›®æ ‡æˆ–åŠ¨ä½œé£æ ¼å®šä¹‰ï¼‰é€šå¸¸æå…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å°†æ­¤é—®é¢˜ç§°ä¸ºä»»åŠ¡æ¦‚å¿µå­¦ä¹ ï¼Œå¹¶æå‡ºæˆ‘ä»¬çš„æ–¹æ³•â€”â€”é€šè¿‡é€†å‘ç”Ÿæˆå»ºæ¨¡è¿›è¡Œå°‘é‡ä»»åŠ¡å­¦ä¹ ï¼ˆFTL-IGMï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯é€†ç¥ç»ç”Ÿæˆæ¨¡å‹æ¥å­¦ä¹ æ–°ä»»åŠ¡æ¦‚å¿µã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ä¸€ç»„åŸºæœ¬æ¦‚å¿µåŠå…¶æ¼”ç¤ºä¸Šé¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ã€‚ç„¶åï¼Œå¯¹äºæ–°æ¦‚å¿µçš„ä¸€äº›æ¼”ç¤ºï¼ˆä¾‹å¦‚æ–°ç›®æ ‡æˆ–æ–°åŠ¨ä½œï¼‰ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ æ½œåœ¨æ¦‚å¿µï¼Œå¹¶ä¸”ç”±äºç”Ÿæˆæ¨¡å‹çš„å¯é€†æ€§ï¼Œæ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªé¢†åŸŸè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•â€”â€”ç‰©ä½“é‡ç»„ã€ç›®æ ‡å¯¼å‘å¯¼èˆªã€äººç±»åŠ¨ä½œçš„è¿åŠ¨å­—å¹•ã€è‡ªåŠ¨é©¾é©¶å’Œç°å®ä¸–ç•Œæ¡Œé¢æ“ä½œã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬æˆåŠŸåœ°å­¦ä¹ äº†æ–°æ¦‚å¿µå’Œç”Ÿæˆä¸è¿™äº›æ¦‚å¿µç›¸å¯¹åº”çš„ä»£ç†è®¡åˆ’æˆ–è¿åŠ¨ï¼šï¼ˆ1ï¼‰åœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­ï¼›ï¼ˆ2ï¼‰ä¸è®­ç»ƒæ¦‚å¿µç»„åˆåœ¨ä¸€èµ·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04987v2">PDF</a> Added acknowledgment</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ‰é™ç¤ºä¾‹å­¦ä¹ ä»£ç†çš„æ„å›¾ï¼Œå³å…¶ç›®æ ‡æˆ–åŠ¨ä½œé£æ ¼ï¼Œæ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç§°æ­¤é—®é¢˜ä¸ºä»»åŠ¡æ¦‚å¿µå­¦ä¹ ï¼Œå¹¶æå‡ºæˆ‘ä»¬çš„æ–¹æ³•â€”â€”é€šè¿‡é€†å‘ç”Ÿæˆæ¨¡å‹è¿›è¡Œå°‘é‡ä»»åŠ¡å­¦ä¹ ï¼ˆFTL-IGMï¼‰ï¼Œåˆ©ç”¨å¯é€†ç¥ç»ç”Ÿæˆæ¨¡å‹å­¦ä¹ æ–°ä»»åŠ¡æ¦‚å¿µã€‚æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨åŸºæœ¬æ¦‚å¿µåŠå…¶æ¼”ç¤ºä¸Šé¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œç„¶ååªéœ€å°‘é‡æ–°æ¦‚å¿µçš„æ¼”ç¤ºï¼ˆå¦‚æ–°ç›®æ ‡æˆ–æ–°åŠ¨ä½œï¼‰ï¼Œå³å¯é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ æ½œåœ¨æ¦‚å¿µï¼Œå¾—ç›Šäºç”Ÿæˆæ¨¡å‹çš„å¯é€†æ€§ï¼Œæ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªé¢†åŸŸè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•â€”â€”ç‰©ä½“é‡æ–°æ’åˆ—ã€ç›®æ ‡å¯¼å‘å¯¼èˆªã€äººç±»åŠ¨ä½œè¿åŠ¨å­—å¹•ã€è‡ªåŠ¨é©¾é©¶å’Œç°å®ä¸–ç•Œæ¡Œé¢æ“ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬æˆåŠŸå­¦ä¹ äº†æ–°æ¦‚å¿µå’Œç”Ÿæˆä¸è¿™äº›æ¦‚å¿µç›¸å¯¹åº”çš„ä»£ç†è®¡åˆ’æˆ–è¿åŠ¨ï¼Œåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­ä»¥åŠç»„åˆè®­ç»ƒæ¦‚å¿µçš„æƒ…å†µä¸‹å‡å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) æ˜¯ä¸€ç§è§£å†³ä»»åŠ¡æ¦‚å¿µå­¦ä¹ é—®é¢˜çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¯é€†ç¥ç»ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ åŸºæœ¬æ¦‚å¿µåŠå…¶æ¼”ç¤ºã€‚</li>
<li>ä»…éœ€å°‘é‡æ–°æ¦‚å¿µçš„æ¼”ç¤ºï¼Œå³å¯é€šè¿‡åå‘ä¼ æ’­å­¦ä¹ æ½œåœ¨æ¦‚å¿µï¼Œæ— éœ€æ›´æ–°æ¨¡å‹æƒé‡ã€‚</li>
<li>FTL-IGM åœ¨äº”ä¸ªä¸åŒé¢†åŸŸè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬ç‰©ä½“é‡æ–°æ’åˆ—ã€ç›®æ ‡å¯¼å‘å¯¼èˆªã€è¿åŠ¨å­—å¹•ã€è‡ªåŠ¨é©¾é©¶å’Œæ¡Œé¢æ“ä½œã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸæˆåŠŸå­¦ä¹ æ–°æ¦‚å¿µå’Œç”Ÿæˆä¸è¿™äº›æ–°æ¦‚å¿µç›¸å¯¹åº”çš„ä»£ç†è®¡åˆ’æˆ–è¿åŠ¨ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„ç¯å¢ƒä¸­ä»¥åŠç»„åˆè®­ç»ƒæ¦‚å¿µçš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FoMo-A-Foundation-Model-for-Mobile-Traffic-Forecasting-with-Diffusion-Model"><a href="#FoMo-A-Foundation-Model-for-Mobile-Traffic-Forecasting-with-Diffusion-Model" class="headerlink" title="FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model"></a>FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model</h2><p><strong>Authors:Haoye Chai, Xiaoqian Qi, Shiyuan Zhang, Yong Li</strong></p>
<p>Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero&#x2F;few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short&#x2F;long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero&#x2F;few-shot learning, showcasing a strong universality. </p>
<blockquote>
<p>ç§»åŠ¨æµé‡é¢„æµ‹ä½¿è¿è¥å•†èƒ½å¤Ÿæå‰é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ï¼Œä¸ºæé«˜æœåŠ¡è´¨é‡å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒæä¾›äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯é¢å‘ä»»åŠ¡çš„ï¼Œå¹¶ä¸”ä½¿ç”¨å®šåˆ¶æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨åŸºç«™éƒ¨ç½²ã€èµ„æºé…ç½®ã€èƒ½æºä¼˜åŒ–ç­‰å¤šæ ·åŒ–ç§»åŠ¨ç½‘ç»œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é˜»ç¢äº†å®ƒä»¬åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç”±äºå…¶åœ¨å¤šä»»åŠ¡é€‚åº”å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ–¹é¢çš„çªå‡ºè¡¨ç°ï¼ŒåŸºç¡€æ¨¡å‹å·²åœ¨NLPå’ŒCVçš„å„ä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†çŸ­æœŸ&#x2F;é•¿æœŸé¢„æµ‹å’Œè·¨å¤šä¸ªåŸå¸‚çš„åˆ†å¸ƒç”Ÿæˆçš„å¤šæ ·åŒ–é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ï¼Œå…¶ä¸­æå‡ºäº†å„ç§æ—¶ç©ºæ©ç ï¼Œä»¥ä½¿å…¶èƒ½å¤Ÿå­¦ä¹ ä¸åŒä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡ä¸åŸå¸‚èƒŒæ™¯ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œæé«˜å…¶è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨9ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·åŒ–çš„é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ ä¸Šä¼˜äºå½“å‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15322v2">PDF</a> 11 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†å¤šæ ·çš„é¢„æµ‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»¥åŠè·¨å¤šä¸ªåŸå¸‚çš„åˆ†å¸ƒç”Ÿæˆï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ï¼Œé€šè¿‡æå‡ºå„ç§æ—¶ç©ºæ©ç æ¥å­¦ä¹ ä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶å‘å±•äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ç¯å¢ƒä¹‹é—´çš„å…³è”ï¼Œæé«˜äº†å…¶è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨9ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ æ–¹é¢ä¼˜äºå½“å‰æ¨¡å‹ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æµé‡é¢„æµ‹å…è®¸è¿è¥å•†é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ï¼Œå¯¹æé«˜æœåŠ¡è´¨é‡å’Œç”¨æˆ·ä½“éªŒæœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é€šå¸¸é¢å‘ç‰¹å®šä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡ï¼ˆå¦‚åŸºç«™éƒ¨ç½²ã€èµ„æºåˆ†é…ã€èƒ½æºä¼˜åŒ–ç­‰ï¼‰ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶é˜»ç¢äº†åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–ã€‚</li>
<li>æå‡ºçš„FoMoæ¨¡å‹æ—¨åœ¨å¤„ç†å¤šæ ·åŒ–çš„é¢„æµ‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»¥åŠè·¨å¤šä¸ªåŸå¸‚çš„åˆ†å¸ƒç”Ÿæˆï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚</li>
<li>FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ã€‚</li>
<li>FoMoé€šè¿‡æå‡ºå„ç§æ—¶ç©ºæ©ç æ¥å­¦ä¹ ä»»åŠ¡çš„å†…è•´ç‰¹å¾ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç­–ç•¥è¢«å¼€å‘æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ç¯å¢ƒä¹‹é—´çš„å…³è”ï¼Œæé«˜äº†æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Retrieval-Reasoning-Large-Language-Model-based-Synthetic-Clinical-Trial-Generation"><a href="#Retrieval-Reasoning-Large-Language-Model-based-Synthetic-Clinical-Trial-Generation" class="headerlink" title="Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial   Generation"></a>Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial   Generation</h2><p><strong>Authors:Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao</strong></p>
<p>Machine learning (ML) exhibits promise in the clinical domain. However, it is constrained by data scarcity and ethical considerations, as the generation of clinical trials presents significant challenges due to stringent privacy regulations, high costs, and the extended duration required for conducting studies with human participants. Despite the advancements of large language models (LLMs) in general generation tasks, their potential in facilitating the generation of synthetic clinical trials is under-explored. To address this gap, we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs to generate artificial yet realistic and diverse clinical trials with binary success&#x2F;failure labels. Experiments conducted on real clinical trials from the \url{ClinicalTrials.gov} database demonstrate that our synthetic data can effectively augment real datasets. Furthermore, by fine-tuning a pre-trained model as a binary classifier on synthetic clinical trial datasets, we demonstrate that this augmentation enhances model training for downstream tasks such as trial outcome prediction. Our findings suggest that LLMs for synthetic clinical trial generation hold promise for accelerating clinical research and upholding ethical standards for patient privacy. The code is publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4">https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4</a>. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åœ¨ä¸´åºŠé¢†åŸŸå…·æœ‰å¹¿é˜”å‰æ™¯ã€‚ç„¶è€Œï¼Œç”±äºä¸¥æ ¼çš„éšç§æ³•è§„ã€é«˜æ˜‚æˆæœ¬å’Œå¼€å±•ç ”ç©¶æ‰€éœ€çš„é•¿å‘¨æœŸï¼Œä¸´åºŠè¯•éªŒçš„ç”Ÿæˆé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´æ•°æ®ç¨€ç¼ºå’Œä¼¦ç†è€ƒé‡é™åˆ¶äº†å…¶å‘å±•ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€šç”¨ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å…¶åœ¨ä¿ƒè¿›åˆæˆä¸´åºŠè¯•éªŒç”Ÿæˆæ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ£€ç´¢æ¨ç†å°æ ·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMç”Ÿæˆäººå·¥ä½†ç°å®ä¸”å¤šæ ·çš„ä¸´åºŠè¯•éªŒæ•°æ®ï¼Œå¹¶å¸¦æœ‰äºŒå…ƒæˆåŠŸ&#x2F;å¤±è´¥æ ‡ç­¾ã€‚åœ¨ClinicalTrials.govæ•°æ®åº“çš„å®é™…ä¸´åºŠè¯•éªŒä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®å¯ä»¥æœ‰æ•ˆåœ°æ‰©å……çœŸå®æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶ä½œä¸ºåˆæˆä¸´åºŠè¯•éªŒæ•°æ®é›†ä¸Šçš„äºŒå…ƒåˆ†ç±»å™¨ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§æ‰©å……å¯ä»¥å¢å¼ºä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹è®­ç»ƒï¼Œå¦‚è¯•éªŒç»“å±€é¢„æµ‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç”¨äºåˆæˆä¸´åºŠè¯•éªŒç”Ÿæˆçš„LLMåœ¨åŠ é€Ÿä¸´åºŠç ”ç©¶å’Œç»´æŒæ‚£è€…éšç§çš„ä¼¦ç†æ ‡å‡†æ–¹é¢å…·å‘å±•æ½œåŠ›ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r%2Fr%2FRetrieval_Reasoning_Clinical_Trial_Generation-3EC4%E3%80%82">https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12476v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨å­¦ä¹ åœ¨ä¸´åºŠé¢†åŸŸå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ï¼Œä½†å—é™äºæ•°æ®ç¨€ç¼ºå’Œä¼¦ç†è€ƒé‡ã€‚ç”±äºä¸¥æ ¼çš„éšç§è§„å®šã€é«˜æ˜‚æˆæœ¬å’Œé•¿æœŸçš„ç ”ç©¶å‘¨æœŸï¼Œä¸´åºŠè¯•éªŒçš„ç”Ÿæˆé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šç”¨ç”Ÿæˆä»»åŠ¡ä¸Šæœ‰æ‰€çªç ´ï¼Œä½†å…¶ç”¨äºç”Ÿæˆåˆæˆä¸´åºŠè¯•éªŒçš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ£€ç´¢æ¨ç†å°‘æ ·æœ¬æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå…·æœ‰äºŒè¿›åˆ¶æˆåŠŸ&#x2F;å¤±è´¥æ ‡ç­¾çš„åˆæˆä¸´åºŠè¯•éªŒæ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œåˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ‰©å……çœŸå®æ•°æ®é›†ï¼Œä¸”é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºäºŒå…ƒåˆ†ç±»å™¨è¿›è¡Œè®­ç»ƒï¼Œèƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡å¦‚è¯•éªŒç»“å±€é¢„æµ‹çš„æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆä¸´åºŠè¯•éªŒæ•°æ®å¯¹äºåŠ é€Ÿä¸´åºŠç ”ç©¶å¹¶ç»´æŒæ‚£è€…éšç§çš„ä¼¦ç†æ ‡å‡†å…·æœ‰æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨ä¸´åºŠåº”ç”¨é¢ä¸´æ•°æ®ç¨€ç¼ºå’Œä¼¦ç†æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠè¯•éªŒç”Ÿæˆæ–¹é¢çš„æ½œåŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ£€ç´¢æ¨ç†å°‘æ ·æœ¬æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆåˆæˆä¸´åºŠè¯•éªŒæ•°æ®ã€‚</li>
<li>åˆæˆæ•°æ®èƒ½æœ‰æ•ˆæ‰©å……çœŸå®æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆæˆæ•°æ®èƒ½æé«˜æ¨¡å‹åœ¨ä¸´åºŠè¯•éªŒç»“å±€é¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåˆæˆä¸´åºŠè¯•éªŒæ•°æ®æœ‰åŠ©äºåŠ é€Ÿä¸´åºŠç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4c6d1c3d0ec7abcaf2f0ea1c61f31551.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  SAR Strikes Back A New Hope for RSVQA
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2464381031b26a2c2b8f311f42c0e32.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
