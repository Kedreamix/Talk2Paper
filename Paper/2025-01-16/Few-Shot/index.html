<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-01-16  Text-Diffusion Red-Teaming of Large Language Models Unveiling Harmful   Behaviors with Proximity Constraints">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_2_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-16-更新"><a href="#2025-01-16-更新" class="headerlink" title="2025-01-16 更新"></a>2025-01-16 更新</h1><h2 id="Text-Diffusion-Red-Teaming-of-Large-Language-Models-Unveiling-Harmful-Behaviors-with-Proximity-Constraints"><a href="#Text-Diffusion-Red-Teaming-of-Large-Language-Models-Unveiling-Harmful-Behaviors-with-Proximity-Constraints" class="headerlink" title="Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints"></a>Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints</h2><p><strong>Authors:Jonathan Nöther, Adish Singla, Goran Radanović</strong></p>
<p>Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red-teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this paper, we study red-teaming strategies that enable a targeted security assessment. We propose an optimization framework for red-teaming with proximity constraints, where the discovered prompts must be similar to reference prompts from a given dataset. This dataset serves as a template for the discovered prompts, anchoring the search for test-cases to specific topics, writing styles, or types of harmful behavior. We show that established auto-regressive model architectures do not perform well in this setting. We therefore introduce a black-box red-teaming method inspired by text-diffusion models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the reference prompt by perturbing it in the embedding space, directly controlling the amount of change introduced. We systematically evaluate our method by comparing its effectiveness with established methods based on model fine-tuning and zero- and few-shot prompting. Our results show that DART is significantly more effective at discovering harmful inputs in close proximity to the reference prompt. </p>
<blockquote>
<p>最近的工作提出了针对给定的大型语言模型（LLM）漏洞测试的自动化红队方法。这些方法使用红队LLM来发现会在目标LLM中引发有害行为的输入。在本文中，我们研究了能够进行有针对性的安全评估的红队策略。我们提出了带有接近度约束的红队优化框架，其中发现的提示必须与给定数据集中的参考提示相似。该数据集作为发现的提示的模板，将测试用例搜索锚定到特定主题、写作风格或有害行为类型上。我们表明，现有的自动回归模型架构在此设置中表现不佳。因此，我们引入了一种受文本扩散模型启发的黑盒红队方法：用于审计和红队的扩散（DART）。DART通过嵌入空间中对其进行扰动来修改参考提示，直接控制引入的更改量。我们通过将其与基于模型微调以及零样本和少样本提示的现有方法进行比较，系统地评估了我们的方法的有效性。结果表明，DART在发现与参考提示接近的有害输入方面显著更有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08246v1">PDF</a> This is an extended version of a paper published at AAAI 25</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的自动红队测试方法，旨在发现针对目标LLM产生有害行为的输入。本文研究了使目标安全评估成为可能的红队策略，并提出了一个带有近似约束的红队优化框架。此外，本文发现现有的自动回归模型架构在此设置中表现不佳，因此引入了一种基于文本扩散模型的黑色盒子红队方法：审计和红队扩散（DART）。通过系统地评估，结果表明DART在发现与参考提示相近的有害输入方面显著更有效。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自动化红队方法用于测试大型语言模型（LLM）的漏洞。</li>
<li>红队策略旨在发现导致目标LLM产生有害行为的输入。</li>
<li>提出了一个带有近似约束的红队优化框架，参考提示作为模板，使搜索测试用例集中于特定主题、写作风格或有害行为类型。</li>
<li>现有自动回归模型架构在此设置中表现不佳。</li>
<li>引入了一种基于文本扩散模型的黑色盒子红队方法：DART（审计和红队扩散）。</li>
<li>DART通过扰动参考提示的嵌入空间进行修改，直接控制引入的变化量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08246">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-22dd814af019e9de46d4ed048460b266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f498a75494c0c0cc7ff491491e944e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f4c9c18f39bb34f8c354c9df82ea589.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f81556c8fa01d54c709542b9f8bb37bc.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.08246v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LeapVAD-A-Leap-in-Autonomous-Driving-via-Cognitive-Perception-and-Dual-Process-Thinking"><a href="#LeapVAD-A-Leap-in-Autonomous-Driving-via-Cognitive-Perception-and-Dual-Process-Thinking" class="headerlink" title="LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and   Dual-Process Thinking"></a>LeapVAD: A Leap in Autonomous Driving via Cognitive Perception and   Dual-Process Thinking</h2><p><strong>Authors:Yukai Ma, Tiantian Wei, Naiting Zhong, Jianbiao Mei, Tao Hu, Licheng Wen, Xuemeng Yang, Botian Shi, Yong Liu</strong></p>
<p>While autonomous driving technology has made remarkable strides, data-driven approaches still struggle with complex scenarios due to their limited reasoning capabilities. Meanwhile, knowledge-driven autonomous driving systems have evolved considerably with the popularization of visual language models. In this paper, we propose LeapVAD, a novel method based on cognitive perception and dual-process thinking. Our approach implements a human-attentional mechanism to identify and focus on critical traffic elements that influence driving decisions. By characterizing these objects through comprehensive attributes - including appearance, motion patterns, and associated risks - LeapVAD achieves more effective environmental representation and streamlines the decision-making process. Furthermore, LeapVAD incorporates an innovative dual-process decision-making module miming the human-driving learning process. The system consists of an Analytic Process (System-II) that accumulates driving experience through logical reasoning and a Heuristic Process (System-I) that refines this knowledge via fine-tuning and few-shot learning. LeapVAD also includes reflective mechanisms and a growing memory bank, enabling it to learn from past mistakes and continuously improve its performance in a closed-loop environment. To enhance efficiency, we develop a scene encoder network that generates compact scene representations for rapid retrieval of relevant driving experiences. Extensive evaluations conducted on two leading autonomous driving simulators, CARLA and DriveArena, demonstrate that LeapVAD achieves superior performance compared to camera-only approaches despite limited training data. Comprehensive ablation studies further emphasize its effectiveness in continuous learning and domain adaptation. Project page: <a target="_blank" rel="noopener" href="https://pjlab-adg.github.io/LeapVAD/">https://pjlab-adg.github.io/LeapVAD/</a>. </p>
<blockquote>
<p>虽然自动驾驶技术在近年来取得了显著的进步，但数据驱动的方法在复杂场景下仍然面临着挑战，因为其推理能力有限。与此同时，随着视觉语言模型的普及，知识驱动的自动驾驶系统有了显著的发展。在本文中，我们提出了一种基于认知感知和双过程思考的新型方法LeapVAD。我们的方法实现了人类注意力机制，用于识别和关注影响驾驶决策的关键交通要素。通过对这些物体进行全面的特征描述，包括外观、运动模式和相关风险，LeapVAD实现了更有效的环境表征和简化的决策过程。此外，LeapVAD融入了一个创新的双过程决策模块，模仿人类驾驶学习过程。该系统包括一个分析过程（系统II），通过逻辑推理积累驾驶经验，以及一个启发式过程（系统I），通过微调和小样本学习来完善知识。LeapVAD还包括反射机制和不断增长的记忆库，使其能够从过去的错误中学习并在闭环环境中持续提高其性能。为了提高效率，我们开发了一个场景编码器网络，用于生成紧凑的场景表示，以便快速检索相关的驾驶经验。在CARLA和DriveArena两个领先的自动驾驶模拟器上进行的广泛评估表明，尽管训练数据有限，LeapVAD仍优于仅使用摄像头的方案并实现了卓越的性能。全面的消融研究进一步强调了其在持续学习和域适应方面的有效性。项目页面：<a target="_blank" rel="noopener" href="https://pjlab-adg.github.io/LeapVAD/%E3%80%82">https://pjlab-adg.github.io/LeapVAD/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08168v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了LeapVAD这一新型的基于认知感知和双过程思维的自动驾驶技术。它通过引入人类注意力机制，对影响驾驶决策的关键交通元素进行识别和聚焦，实现了更高效的环境表征和决策过程。LeapVAD结合了创新性的双过程决策模块，模仿人类驾驶学习过程，包括分析过程（系统-II）和经验积累以及通过微调和小样本学习的启发式过程（系统-I）。此外，LeapVAD还具有反思机制和成长记忆库，可从过去的错误中学习并持续改进性能。在两大主流自动驾驶模拟器CARLA和DriveArena上的评估表明，LeapVAD在有限训练数据下实现了超越仅使用摄像头的优越性能。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LeapVAD结合认知感知和双过程思维，针对自动驾驶技术提出新方法。</li>
<li>通过引入人类注意力机制，实现对关键交通元素的识别和聚焦。</li>
<li>LeapVAD模仿人类驾驶学习过程，包含分析过程和启发式过程。</li>
<li>系统具有反思机制和成长记忆库，可从错误中学习并持续改进。</li>
<li>在两大模拟器上的评估显示，LeapVAD在有限数据下表现优越。</li>
<li>LeapVAD通过场景编码器网络生成紧凑的场景表示，提高效率。</li>
<li>综合消融研究强调了其在持续学习和域适应方面的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aaaeb47d09aaf29b22b20847700e3dbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f647010db2741e36967e15ba5e27fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fe98f369efde3b3917d94ff310f8224.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57fa463d68962f0bced3c3d7ef653d40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e999ca558a6df3a249f23c8ea3cc4dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d92fc3db46657e34b6aa7a3060900b7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="I-Can-Find-You-in-Seconds-Leveraging-Large-Language-Models-for-Code-Authorship-Attribution"><a href="#I-Can-Find-You-in-Seconds-Leveraging-Large-Language-Models-for-Code-Authorship-Attribution" class="headerlink" title="I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution"></a>I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution</h2><p><strong>Authors:Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung</strong></p>
<p>Source code authorship attribution is important in software forensics, plagiarism detection, and protecting software patch integrity. Existing techniques often rely on supervised machine learning, which struggles with generalization across different programming languages and coding styles due to the need for large labeled datasets. Inspired by recent advances in natural language authorship analysis using large language models (LLMs), which have shown exceptional performance without task-specific tuning, this paper explores the use of LLMs for source code authorship attribution.   We present a comprehensive study demonstrating that state-of-the-art LLMs can successfully attribute source code authorship across different languages. LLMs can determine whether two code snippets are written by the same author with zero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of 0.78, and can attribute code authorship from a small set of reference code snippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show some adversarial robustness against misattribution attacks.   Despite these capabilities, we found that naive prompting of LLMs does not scale well with a large number of authors due to input token limitations. To address this, we propose a tournament-style approach for large-scale attribution. Evaluating this approach on datasets of C++ (500 authors, 26,355 samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve classification accuracy of up to 65% for C++ and 68.7% for Java using only one reference per author. These results open new possibilities for applying LLMs to code authorship attribution in cybersecurity and software engineering. </p>
<blockquote>
<p>源代码作者归属在软件取证、抄袭检测以及软件补丁完整性保护方面具有重要意义。现有技术通常依赖于有监督机器学习，由于需要大量标注数据集，它在不同编程语言和编码风格之间的泛化方面表现困难。受自然语言作者分析中使用大型语言模型（LLMs）的最新进展的启发，该论文探索了使用LLMs进行源代码作者归属。我们进行了全面的研究，表明最先进的LLMs可以成功地在不同语言中进行源代码作者归属。LLMs可以通过零样本提示确定两个代码片段是否由同一作者编写，达到马修斯相关系数（MCC）0.78，并且可以从少量参考代码片段中通过小样本学习进行代码作者归属，达到MCC 0.77。此外，LLMs对于一些误归属攻击还表现出一定的对抗性稳健性。尽管具备了这些功能，但我们发现对LLMs的直白提示并不适用于大量作者的情况，因为存在输入标记的限制。为了解决这一问题，我们提出了一种用于大规模归属的锦标赛式方法。在GitHub上的C++（500位作者，26,355个样本）和Java（686位作者，55,267个样本）数据集上评估该方法，仅使用每位作者的一个参考样本，即可达到C++的65%和Java的68.7%的分类准确率。这些结果为在网络安全和软件工程中应用LLMs进行代码作者归属提供了新的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08165v1">PDF</a> 12 pages, 5 figures,</p>
<p><strong>摘要</strong><br>基于大型语言模型（LLMs）的自然语言作者分析最新进展的启发，本文探索了用于源代码作者归属性的LLMs应用。研究表明，先进LLMs可成功归属不同语言的源代码作者，通过零样本提示确定两个代码片段是否由同一作者撰写，达到马修斯相关系数（MCC）为0.78，并通过少量学习从参考代码片段中归属作者，实现MCC为0.77。尽管具有这些能力，但发现LLMs的直观提示并不适用于大量作者的场景。为解决此问题，本文提出了一种锦标赛风格的大规模归属方法，在GitHub的C++和Java代码数据集上实现高达65%和68.7%的分类准确率。这为在网络安全和软件工程中应用LLMs进行代码作者归属提供了新的可能性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>源代码作者归属在软件取证、抄袭检测和软件补丁完整性保护中具有重要性。</li>
<li>现有技术通常依赖于监督机器学习，难以在不同编程语言和编码风格之间进行泛化，需要大规模标注数据集。</li>
<li>LLMs在源代码作者归属方面表现出色，可成功归因于不同语言的作者。</li>
<li>通过零样本提示和少量学习，LLMs可以准确地确定代码片段的作者。</li>
<li>LLMs具有一定的对抗误归属攻击的能力。</li>
<li>直观提示LLMs并不适用于大量作者的场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b3ba44bf30dae6d243d65ca517465ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c32f47ed7fd1b61a38178840cda9932.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-832769875fc44830ccb1bfce922bd5cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237ec6bfecd9b962568537e68dc31921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38671386cf8e269db94af2e29c299467.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbb2ea110deca59a9425d4b300a61ae0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Inference-Time-Compute-More-Faithful-A-Research-Note"><a href="#Inference-Time-Compute-More-Faithful-A-Research-Note" class="headerlink" title="Inference-Time-Compute: More Faithful? A Research Note"></a>Inference-Time-Compute: More Faithful? A Research Note</h2><p><strong>Authors:James Chua, Owain Evans</strong></p>
<p>Models trained specifically to generate long Chains of Thought (CoTs) have recently achieved impressive results. We refer to these models as Inference-Time-Compute (ITC) models. Are the CoTs of ITC models more faithful compared to traditional non-ITC models? We evaluate two ITC models (based on Qwen-2.5 and Gemini-2) on an existing test of faithful CoT To measure faithfulness, we test if models articulate cues in their prompt that influence their answers to MMLU questions. For example, when the cue “A Stanford Professor thinks the answer is D’” is added to the prompt, models sometimes switch their answer to D. In such cases, the Gemini ITC model articulates the cue 54% of the time, compared to 14% for the non-ITC Gemini.   We evaluate 7 types of cue, such as misleading few-shot examples and anchoring on past responses. ITC models articulate cues that influence them much more reliably than all the 6 non-ITC models tested, such as Claude-3.5-Sonnet and GPT-4o, which often articulate close to 0% of the time.   However, our study has important limitations. We evaluate only two ITC models – we cannot evaluate OpenAI’s SOTA o1 model. We also lack details about the training of these ITC models, making it hard to attribute our findings to specific processes.   We think faithfulness of CoT is an important property for AI Safety. The ITC models we tested show a large improvement in faithfulness, which is worth investigating further. To speed up this investigation, we release these early results as a research note. </p>
<blockquote>
<p>近期，经过专门训练以生成长链条思维（Chain of Thoughts, CoT）的模型取得了令人印象深刻的结果。我们将这些模型称为推理时间计算（Inference-Time-Compute, ITC）模型。与传统的非ITC模型相比，ITC模型的思维链条是否更加忠实？我们对两款基于Qwen-2.5和Gemini-2的ITC模型进行了忠诚性的现有测试。为了衡量思维链条的忠实性，我们测试了模型是否能明确其提示中的线索来影响其答案到MMLU问题的方向。例如，当提示中加入“斯坦福教授认为答案是D”这一线索时，模型有时会改变答案选择D。在这种情况下，Gemini ITC模型能够明确这一线索的占比达到54%，而非ITC的Gemini仅占14%。我们对包括误导性的少量示例和基于过去回应的锚定在内的7种线索进行了评估。ITC模型更加可靠地描述了影响他们的线索，相比于我们测试的六个非ITC模型，如Claude-3.5-Sonnet和GPT-4o等，它们通常明确表述的比例接近为0%。然而，我们的研究存在重要局限性。我们只评估了两个ITC模型，无法评估OpenAI的最新模型o1。此外，由于缺乏关于这些ITC模型的训练细节，这使得我们的发现难以归因于特定的过程。我们认为思维链条的忠实性是AI安全性的一个重要属性。我们测试的ITC模型在忠实性方面取得了很大改进，值得进一步调查。为了加速这一调查进程，我们发布这些早期结果作为研究笔记。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08156v1">PDF</a> 7 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对生成长链条思维（CoT）的模型，特别是Inference-Time-Compute（ITC）模型的信仰度测试。通过对两款ITC模型（基于Qwen-2.5和Gemini-2）的评估，发现它们相比传统非ITC模型更可靠地表达影响答案的线索。然而，研究存在局限性，仅评估了两个ITC模型，且缺乏关于这些ITC模型训练的具体细节。尽管如此，ITC模型在信仰度方面显示出重大改进，值得进一步调查。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ITC模型被训练用于生成长链条思维（CoT），在信仰度测试中表现优异。</li>
<li>与传统非ITC模型相比，ITC模型更可靠地表达影响答案的线索。</li>
<li>在评估的7种线索中，ITC模型比6款非ITC模型更可靠地表达这些线索。</li>
<li>研究仅评估了两个ITC模型，存在局限性。</li>
<li>缺乏关于ITC模型训练的具体细节，使得难以将发现归因于特定过程。</li>
<li>信仰度是AI安全的重要属性，ITC模型在信仰度方面的改进值得进一步调查。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bd35e36514e28e2ec6297d6036793862.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5f563a5121cd120c5dbbe59154871de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e754382dc00f6340b1575ed00836a421.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-52319b3ae7ddabf7d4172d761563ef2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9af38c19d4d6eead5f82a2da98fbc927.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Instruct-Few-Shot-Jailbreaking-Decompose-the-Attack-into-Pattern-and-Behavior-Learning"><a href="#Self-Instruct-Few-Shot-Jailbreaking-Decompose-the-Attack-into-Pattern-and-Behavior-Learning" class="headerlink" title="Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning"></a>Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning</h2><p><strong>Authors:Jiaqi Hua, Wanxu Wei</strong></p>
<p>Recently, several works have been conducted on jailbreaking Large Language Models (LLMs) with few-shot malicious demos. In particular, Zheng et al. (2024) focuses on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search. Nevertheless, this method lacks generality since it specifies the instruction-response structure. Moreover, the reason why inserting special tokens takes effect in inducing harmful behaviors is only empirically discussed. In this paper, we take a deeper insight into the mechanism of special token injection and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the model’s vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/iphosi/Self-Instruct-FSJ">https://github.com/iphosi/Self-Instruct-FSJ</a>. </p>
<blockquote>
<p>最近，有一些关于利用少量恶意演示破解大型语言模型（LLM）的研究。特别是，Zheng等人（2024年）专注于通过向演示中注入特殊令牌并采用演示级随机搜索来提高少样本破解（FSJ）的效率。然而，这种方法缺乏通用性，因为它规定了指令-响应结构。此外，插入特殊令牌为何能有效诱导有害行为只是进行了经验性讨论。在本文中，我们对特殊令牌注入的机制进行了更深入的研究，并提出了借助演示级贪婪搜索的辅助进行自我指导少样本破解（Self-Instruct FSJ）。该框架将FSJ攻击分解为模式学习和行为学习，以更通用和高效的方式利用模型的漏洞。我们在常见的开源模型上进行了精心设计的实验来评估我们的方法，并将其与基线算法进行了比较。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/iphosi/Self-Instruct-FSJ%E3%80%82">https://github.com/iphosi/Self-Instruct-FSJ。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型的少样本越狱攻击研究近期备受关注。本文深入探讨了特殊令牌注入的机制，并提出了基于演示级贪婪搜索的Self-Instruct少样本越狱攻击框架。该框架将越狱攻击分解为模式和行为学习，以更通用和高效的方式利用模型的漏洞。实验证明，该方法在开源模型上的表现优于基线算法。相关代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前对大型语言模型的少样本越狱攻击研究活跃。</li>
<li>Zheng等人的研究通过插入特殊令牌提高了越狱效率，但缺乏通用性。</li>
<li>本文深入探讨了特殊令牌注入的机制，并提出了新的Self-Instruct少样本越狱攻击框架。</li>
<li>该框架通过分解越狱攻击为模式和行为学习，以更通用和高效的方式利用模型漏洞。</li>
<li>实验证明，新框架在开源模型上的表现优于现有方法。</li>
<li>研究的代码已公开在GitHub上，便于他人查阅和使用。</li>
<li>该研究为大型语言模型的安全性和稳定性带来了新的挑战和研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-737f5ee493e0e08d4ef8dd285262ef1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa16b1bc90e1efd77494df5bdd1ada95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dad7a2a35d15fa9e1b0d65986e7124c0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Interface-for-Home-Energy-Management-Systems"><a href="#Large-Language-Model-Interface-for-Home-Energy-Management-Systems" class="headerlink" title="Large Language Model Interface for Home Energy Management Systems"></a>Large Language Model Interface for Home Energy Management Systems</h2><p><strong>Authors:François Michelon, Yihong Zhou, Thomas Morstyn</strong></p>
<p>Home Energy Management Systems (HEMSs) help households tailor their electricity usage based on power system signals such as energy prices. This technology helps to reduce energy bills and offers greater demand-side flexibility that supports the power system stability. However, residents who lack a technical background may find it difficult to use HEMSs effectively, because HEMSs require well-formatted parameterization that reflects the characteristics of the energy resources, houses, and users’ needs. Recently, Large-Language Models (LLMs) have demonstrated an outstanding ability in language understanding. Motivated by this, we propose an LLM-based interface that interacts with users to understand and parameterize their &#96;&#96;badly-formatted answers’’, and then outputs well-formatted parameters to implement an HEMS. We further use Reason and Act method (ReAct) and few-shot prompting to enhance the LLM performance. Evaluating the interface performance requires multiple user–LLM interactions. To avoid the efforts in finding volunteer users and reduce the evaluation time, we additionally propose a method that uses another LLM to simulate users with varying expertise, ranging from knowledgeable to non-technical. By comprehensive evaluation, the proposed LLM-based HEMS interface achieves an average parameter retrieval accuracy of 88%, outperforming benchmark models without ReAct and&#x2F;or few-shot prompting. </p>
<blockquote>
<p>家庭能源管理系统（HEMS）能够根据电力系统信号（如能源价格）帮助家庭调整其用电行为。这项技术有助于降低能源账单，并提供更大的需求侧灵活性，支持电力系统稳定性。然而，缺乏技术背景的居民可能会发现有效使用HEMS很困难，因为HEMS需要反映能源资源、房屋和用户需求的特性的格式化参数。最近，大型语言模型（LLM）在语言理解方面表现出了出色的能力。因此，我们提出了一种基于LLM的接口，该接口与用户互动，理解和参数化他们的“格式错误的答案”，然后输出格式良好的参数来实现HEMS。我们进一步使用Reason and Act方法（ReAct）和少量提示来增强LLM的性能。评估接口性能需要进行多次用户与LLM的互动。为了避免寻找志愿者用户和减少评估时间，我们还提出了一种方法，即使用另一个LLM来模拟具有不同专业知识水平的用户，从知识渊博到非技术。通过全面评估，所提出的基于LLM的HEMS接口平均参数检索准确率为88%，优于没有ReAct和&#x2F;或少量提示的基准模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07919v1">PDF</a> 13 pages conference paper</p>
<p><strong>Summary</strong>：<br>家庭能源管理系统（HEMS）可根据电力系统信号如能源价格定制家庭用电。该技术有助于降低能源账单，并为电力系统稳定性提供更大的需求侧灵活性。然而，缺乏技术背景的居民可能难以有效使用HEMS。最近，大型语言模型（LLM）展示了出色的语言理解能力。基于此，提出一种基于LLM的接口，与用户交互以理解和参数化他们的“格式错误的答案”，并输出格式良好的参数来实现HEMS。通过采用Reason and Act方法（ReAct）和少量提示增强LLM性能。评估接口性能需要多次用户与LLM的互动。为了避免寻找志愿者用户和减少评估时间，我们提出了一种使用另一个LLM模拟不同专业程度的用户的方法，从知识渊博到非技术用户。评估结果显示，所提出的基于LLM的HEMS接口平均参数检索准确率为88%，优于未使用ReAct和&#x2F;或少量提示的基准模型。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>家庭能源管理系统(HEMS)能根据电力信号调整电力使用，有助于减少能源账单和支持电力系统稳定性。</li>
<li>大型语言模型（LLM）在理解和参数化居民对HEMS的使用方面展现出潜力。</li>
<li>提出了基于LLM的接口，通过用户交互理解并参数化“格式错误的答案”。</li>
<li>采用Reason and Act方法（ReAct）和少量提示增强LLM性能，提高HEMS接口的性能。</li>
<li>使用另一个LLM模拟不同专业程度的用户，从知识渊博到非技术用户，便于接口性能评估。</li>
<li>综合评估显示，基于LLM的HEMS接口平均参数检索准确率为88%。</li>
<li>该接口性能优于未使用ReAct和&#x2F;或少量提示的基准模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4d7f39903b68214c5ceca075aeda6304.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afa86491bda8f8ee9fb97fd85212ea1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d705c8894121b7370f821a3b4e14413.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a3c9c86ab66123c25a71594c6b1bfd5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leveraging-Metamemory-Mechanisms-for-Enhanced-Data-Free-Code-Generation-in-LLMs"><a href="#Leveraging-Metamemory-Mechanisms-for-Enhanced-Data-Free-Code-Generation-in-LLMs" class="headerlink" title="Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation   in LLMs"></a>Leveraging Metamemory Mechanisms for Enhanced Data-Free Code Generation   in LLMs</h2><p><strong>Authors:Shuai Wang, Liang Ding, Yibing Zhan, Yong Luo, Zheng He, Dapeng Tao</strong></p>
<p>Automated code generation using large language models (LLMs) has gained attention due to its efficiency and adaptability. However, real-world coding tasks or benchmarks like HumanEval and StudentEval often lack dedicated training datasets, challenging existing few-shot prompting approaches that rely on reference examples. Inspired by human metamemory-a cognitive process involving recall and evaluation-we present a novel framework (namely M^2WF) for improving LLMs’ one-time code generation. This approach enables LLMs to autonomously generate, evaluate, and utilize synthetic examples to enhance reliability and performance. Unlike prior methods, it minimizes dependency on curated data and adapts flexibly to various coding scenarios. Our experiments demonstrate significant improvements in coding benchmarks, offering a scalable and robust solution for data-free environments. The code and framework will be publicly available on GitHub and HuggingFace. </p>
<blockquote>
<p>使用大型语言模型（LLM）进行自动代码生成已经因其效率和适应性而受到关注。然而，现实世界中的编码任务或HumanEval和学生评估（StudentEval）等基准测试通常缺乏专用的训练数据集，这给依赖参考样本的现有少数提示方法带来了挑战。受人类元记忆（一种涉及回忆和评价的认知过程）的启发，我们提出了一种改进LLM一次性代码生成的新型框架（即M^2WF）。这种方法使LLM能够自主生成、评估和利用合成示例，以提高可靠性和性能。不同于以前的方法，它最大限度地减少了对数据集整理的依赖，并灵活地适应各种编码场景。我们的实验表明，在编码基准测试中取得了显著的改进，为无数据环境提供了可伸缩和稳健的解决方案。代码和框架将在GitHub和HuggingFace上公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07892v1">PDF</a> 11 pages,6 figures</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的自动化代码生成因其效率和适应性而受到关注。然而，现实世界的编程任务或基准测试（如HumanEval和StudentEval）往往缺乏专用的训练数据集，这给依赖参考样例的现有少样本提示方法带来了挑战。受人类元记忆（一种涉及回忆和评估的认知过程）的启发，我们提出了一种名为M^2WF的新型框架，用于改进LLM的一次性代码生成。该方法使LLM能够自主生成、评估和利用合成示例，以提高可靠性和性能。与现有方法不同，它最大限度地减少了对精选数据的依赖，并灵活适应各种编码场景。实验表明，在编码基准测试中，该框架取得了显著的改进，为无数据环境提供了可伸缩和稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在自动化代码生成中受到关注。</li>
<li>现实世界的编程任务或基准测试缺乏专用训练数据集。</li>
<li>现有少样本提示方法面临挑战。</li>
<li>受人类元记忆启发，提出新型框架M^2WF改进LLM的一次性代码生成。</li>
<li>M^2WF使LLM能够自主生成、评估和利用合成示例。</li>
<li>M^2WF提高LLM的可靠性和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07892">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd13f89a04ebff3e1c41c94467af83ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bba97ebadbda9321a4aeabbd33f7d1cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fefe0ff6388e86ba781d07cc8c689cb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d0cf7aea9193ee91f2688bc80ccc227e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications – such as author-publication history, author affiliation, research topics, and citation counts – we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>科研团队的动态在决定研究成果的性质和影响方面至关重要。然而，现有的基于自我报告和聚类的作者角色分类方法缺乏对贡献的全面上下文分析。因此，我们提出了一种利用先进的大型语言模型（LLMs）对科研团队中的作者角色进行分类的变革性方法，与传统聚类方法相比，它提供了更为精细的分析。具体而言，我们希望通过利用开源和专有的大型语言模型，如GPT-4、Llama3 70B、Llama2 70B和Mistral 7x8B来进行角色分类，以补充和增强这些传统方法。通过少样本提示，我们对作者角色进行了分类，并证明GPT-4在多类别中表现优于其他模型，超越了传统的XGBoost和BERT等方法。我们的方法还包括使用10个特征构建预测深度学习模型。通过在OpenAlex数据库衍生的数据集上训练该模型（该数据库提供有关学术出版的详细元数据，如作者出版历史、作者隶属关系、研究主题和引用计数），我们获得了0.76的F1分数，证明了作者角色分类的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v1">PDF</a> 14 pages, 4 figures, 3 tables</p>
<p><strong>摘要</strong></p>
<p>科研团队动态对于决定研究产出的性质和影响力至关重要。然而，现有的基于自我报告和聚类的作者角色分类方法缺乏全面分析贡献的语境。因此，我们提出了一种变革性的方法，利用先进的大型语言模型（LLMs）对科研团队中的作者角色进行分类，相比传统的聚类方法，LLMs提供了更为精细的分析。具体来说，我们结合并改进传统方法，使用开源和专有LLMs（如GPT-4、Llama3 70B、Llama2 70B和Mistral 7x8B）进行角色分类。通过少量提示，我们对作者角色进行分类，并证明GPT-4在多类别中表现优于其他模型，超越了如XGBoost和BERT等传统方法。我们的方法还包括使用OpenAlex数据库构建的预测深度学习模型，该数据库提供有关学术出版的详细元数据，如作者出版历史、作者隶属关系、研究主题和引用计数等。通过该模型，我们实现了F1分数为0.76，证明了作者角色分类的稳健性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>科研团队动态对研究产出的性质和影响力至关重要。</li>
<li>现有作者角色分类方法缺乏全面分析贡献的语境。</li>
<li>利用先进的大型语言模型（LLMs）进行作者角色分类提供更精细的分析。</li>
<li>GPT-4在作者角色分类中表现优于其他模型。</li>
<li>GPT-4在多个类别中超越了传统方法，如XGBoost和BERT。</li>
<li>使用OpenAlex数据库的预测深度学习模型实现稳健的作者角色分类，F1分数为0.76。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-12b34ca472332862f19792303db83430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-451d4ddb239873b9c80879e371f6b5a1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07267v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Use-of-Contrastive-Language-Image-Pre-Training-for-Human-Posture-Classification-Insights-from-Yoga-Pose-Analysis"><a href="#Exploring-the-Use-of-Contrastive-Language-Image-Pre-Training-for-Human-Posture-Classification-Insights-from-Yoga-Pose-Analysis" class="headerlink" title="Exploring the Use of Contrastive Language-Image Pre-Training for Human   Posture Classification: Insights from Yoga Pose Analysis"></a>Exploring the Use of Contrastive Language-Image Pre-Training for Human   Posture Classification: Insights from Yoga Pose Analysis</h2><p><strong>Authors:Andrzej D. Dobrzycki, Ana M. Bernardos, Luca Bergesio, Andrzej Pomirski, Daniel Sáez-Trigueros</strong></p>
<p>Accurate human posture classification in images and videos is crucial for automated applications across various fields, including work safety, physical rehabilitation, sports training, or daily assisted living. Recently, multimodal learning methods, such as Contrastive Language-Image Pretraining (CLIP), have advanced significantly in jointly understanding images and text. This study aims to assess the effectiveness of CLIP in classifying human postures, focusing on its application in yoga. Despite the initial limitations of the zero-shot approach, applying transfer learning on 15,301 images (real and synthetic) with 82 classes has shown promising results. The article describes the full procedure for fine-tuning, including the choice for image description syntax, models and hyperparameters adjustment. The fine-tuned CLIP model, tested on 3826 images, achieves an accuracy of over 85%, surpassing the current state-of-the-art of previous works on the same dataset by approximately 6%, its training time being 3.5 times lower than what is needed to fine-tune a YOLOv8-based model. For more application-oriented scenarios, with smaller datasets of six postures each, containing 1301 and 401 training images, the fine-tuned models attain an accuracy of 98.8% and 99.1%, respectively. Furthermore, our experiments indicate that training with as few as 20 images per pose can yield around 90% accuracy in a six-class dataset. This study demonstrates that this multimodal technique can be effectively used for yoga pose classification, and possibly for human posture classification, in general. Additionally, CLIP inference time (around 7 ms) supports that the model can be integrated into automated systems for posture evaluation, e.g., for developing a real-time personal yoga assistant for performance assessment. </p>
<blockquote>
<p>准确地对图像和视频中的人体姿态进行分类，对于包括工作安全、身体康复、运动训练和日常生活辅助在内的各个领域中的自动化应用至关重要。最近，如对比语言图像预训练（CLIP）等多模态学习方法在联合理解图像和文字方面取得了显著进展。本研究旨在评估CLIP在人体姿态分类方面的有效性，重点关注其在瑜伽中的应用。尽管零样本方法存在一些初步限制，但在15301张（真实和合成）包含82类的图像上应用迁移学习已显示出令人鼓舞的结果。文章描述了微调的全过程，包括图像描述语法、模型和超参数调整的选择。经过精细调整的CLIP模型在3826张图像上测试时，准确率超过85%，比同一数据集上的先前最新技术高出约6%，而且其训练时间是微调YOLOv8模型所需时间的3.5倍低。对于更面向应用的环境，包含每个姿态仅有1301张和401张训练图像的小数据集，精细调整后的模型分别达到了98.8%和99.1%的准确率。此外，我们的实验表明，每姿态仅使用20张图像进行训练即可在包含六个类别的数据集中达到约90%的准确率。本研究表明，这种多模态技术可有效用于瑜伽姿势分类，也可能广泛用于一般的人体姿态分类。此外，CLIP推理时间约为7毫秒，支持将模型集成到姿势评估的自动化系统中，例如开发用于性能评估的实时个人瑜伽助理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07221v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文研究了利用多模态学习方法，如Contrastive Language-Image Pretraining (CLIP)，进行人类姿势分类的应用。特别是在瑜伽姿势分类方面，通过调整模型、参数和优化图像描述语法等步骤进行微调，实验结果显示fine-tuned的CLIP模型在瑜伽姿势分类上取得了较高的准确率，超过现有技术水平约6%，且训练时间减少了3.5倍。研究还表明，使用少量图像（如每姿势仅20张图像）进行训练也能达到约90%的准确率。因此，该多模态技术可有效应用于瑜伽姿势分类，甚至可能适用于一般的人类姿势分类。此外，CLIP模型的推理时间约为7毫秒，支持集成到自动姿势评估系统中。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态学习方法如CLIP在姿势分类中有广泛应用前景。</li>
<li>CLIP模型在瑜伽姿势分类上表现优异，准确率超过现有技术约6%。</li>
<li>通过微调，CLIP模型可以在较小的数据集上实现高准确率。</li>
<li>使用少量图像（每姿势仅20张）进行训练也可达到良好的准确率。</li>
<li>CLIP模型的推理时间短，适合集成到实时系统中进行姿势评估。</li>
<li>此研究为瑜伽和其他领域自动姿势评估系统的开发提供了新思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07221">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07221v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07221v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Matching-Free-Depth-Recovery-from-Structured-Light"><a href="#Matching-Free-Depth-Recovery-from-Structured-Light" class="headerlink" title="Matching Free Depth Recovery from Structured Light"></a>Matching Free Depth Recovery from Structured Light</h2><p><strong>Authors:Zhuohang Yu, Kai Wang, Juyong Zhang</strong></p>
<p>We present a novel approach for depth estimation from images captured by structured light systems. Unlike many previous methods that rely on image matching process, our approach uses a density voxel grid to represent scene geometry, which is trained via self-supervised differentiable volume rendering. Our method leverages color fields derived from projected patterns in structured light systems during the rendering process, enabling the isolated optimization of the geometry field. This contributes to faster convergence and high-quality output. Additionally, we incorporate normalized device coordinates (NDC), a distortion loss, and a novel surface-based color loss to enhance geometric fidelity. Experimental results demonstrate that our method outperforms existing matching-based techniques in geometric performance for few-shot scenarios, achieving approximately a 60% reduction in average estimated depth errors on synthetic scenes and about 30% on real-world captured scenes. Furthermore, our approach delivers fast training, with a speed roughly three times faster than previous matching-free methods that employ implicit representations. </p>
<blockquote>
<p>我们提出了一种基于结构光系统采集的图像进行深度估计的新方法。不同于许多依赖于图像匹配过程的先前方法，我们的方法使用密度体素网格来表示场景几何结构，并通过自我监督的可微体积渲染进行训练。我们的方法在渲染过程中利用结构光系统中投影图案产生的颜色场，实现对几何场的独立优化。这有助于更快的收敛和更高质量的结果。此外，我们结合了归一化设备坐标（NDC）、畸变损失和基于表面的颜色损失，以提高几何保真度。实验结果表明，我们的方法在少量场景下的几何性能表现优于现有的基于匹配的技术。在合成场景上，我们的方法在平均估计深度误差上大约减少了60%，在真实捕获的场景上大约减少了30%。而且，我们的方法训练速度快，大约是之前使用隐式表示的无匹配方法的三倍速度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07113v1">PDF</a> 10 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于结构化光系统图像的新深度估计方法。与其他依赖图像匹配过程的方法不同，我们的方法使用密度体素网格表示场景几何，并通过自监督的可微分体积渲染进行训练。该方法利用结构化光系统中渲染过程中的投影模式产生的颜色场，实现几何场的独立优化，从而提高了收敛速度和输出质量。此外，还结合了归一化设备坐标（NDC）、畸变损失和基于表面的颜色损失，以提高几何保真度。实验结果表明，在少样本情况下，该方法在几何性能上优于现有的基于匹配的技术，合成场景的平均估计深度误差降低了约60%，真实世界捕获的场景降低了约30%。此外，该方法训练速度快，大约是之前使用隐式表示的无匹配方法的三倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种基于结构化光系统的新型深度估计方法。</li>
<li>采用密度体素网格表示场景几何，并通过自监督可微分体积渲染进行训练。</li>
<li>利用颜色场实现几何场的独立优化，加快收敛速度并提高输出质量。</li>
<li>结合归一化设备坐标（NDC）、畸变损失和基于表面的颜色损失增强几何保真度。</li>
<li>在合成和真实场景下的深度估计中表现优异，相比现有方法显著减少了估计深度误差。</li>
<li>该方法的训练速度较快，相较于其他无匹配方法有明显的优势。</li>
<li>为深度估计领域提供了新的思路和方法，有望推动该领域的进一步发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.07113v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Motion-Tracks-A-Unified-Representation-for-Human-Robot-Transfer-in-Few-Shot-Imitation-Learning"><a href="#Motion-Tracks-A-Unified-Representation-for-Human-Robot-Transfer-in-Few-Shot-Imitation-Learning" class="headerlink" title="Motion Tracks: A Unified Representation for Human-Robot Transfer in   Few-Shot Imitation Learning"></a>Motion Tracks: A Unified Representation for Human-Robot Transfer in   Few-Shot Imitation Learning</h2><p><strong>Authors:Juntao Ren, Priya Sundaresan, Dorsa Sadigh, Sanjiban Choudhury, Jeannette Bohg</strong></p>
<p>Teaching robots to autonomously complete everyday tasks remains a challenge. Imitation Learning (IL) is a powerful approach that imbues robots with skills via demonstrations, but is limited by the labor-intensive process of collecting teleoperated robot data. Human videos offer a scalable alternative, but it remains difficult to directly train IL policies from them due to the lack of robot action labels. To address this, we propose to represent actions as short-horizon 2D trajectories on an image. These actions, or motion tracks, capture the predicted direction of motion for either human hands or robot end-effectors. We instantiate an IL policy called Motion Track Policy (MT-pi) which receives image observations and outputs motion tracks as actions. By leveraging this unified, cross-embodiment action space, MT-pi completes tasks with high success given just minutes of human video and limited additional robot demonstrations. At test time, we predict motion tracks from two camera views, recovering 6DoF trajectories via multi-view synthesis. MT-pi achieves an average success rate of 86.5% across 4 real-world tasks, outperforming state-of-the-art IL baselines which do not leverage human data or our action space by 40%, and generalizes to scenarios seen only in human videos. Code and videos are available on our website <a target="_blank" rel="noopener" href="https://portal-cornell.github.io/motion_track_policy/">https://portal-cornell.github.io/motion_track_policy/</a>. </p>
<blockquote>
<p>教授机器人自主完成日常任务仍然是一个挑战。模仿学习（IL）是一种通过演示赋予机器人技能的强大方法，但受限于收集遥控机器人数据的过程劳动强度高。人类视频提供了一个可扩展的替代方案，但由于缺乏机器人动作标签，从人类视频中直接训练IL策略仍然很困难。为了解决这个问题，我们提出将动作表示为图像上的短周期2D轨迹。这些动作或运动轨迹捕捉了人类手部或机器人末端执行器预测的运动方向。我们实例化了一种名为运动轨迹策略（MT-pi）的IL策略，它接收图像观察结果并输出运动轨迹作为动作。通过利用这种统一的、跨实体的动作空间，MT-pi仅使用几分钟的人类视频和有限的额外机器人演示即可成功完成任务。在测试时，我们从两个摄像机视角预测运动轨迹，通过多视角合成恢复6DoF轨迹。MT-pi在4个真实任务中的平均成功率达到86.5%，比不利用人类数据或我们动作空间的最新IL基线高出40%，并且能够推广到仅在人类视频中出现的场景。我们的网站<a target="_blank" rel="noopener" href="https://portal-cornell.github.io/motion_track_policy/%E4%B8%8A%E6%9C%89%E4%BB%A3%E7%A0%81%E5%92%8C%E8%A7%86%E9%A2%91%E5%8F%AF%E4%BE%9B%E6%9F%A5%E7%9C%8B%E3%80%82">https://portal-cornell.github.io/motion_track_policy/上有代码和视频可供查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06994v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了机器人自主完成日常任务的教学挑战。提出了通过将动作表示为图像上的短周期二维轨迹来解决模仿学习（IL）面临的挑战。提出了一项名为运动轨迹策略（MT-pi）的模仿学习策略，可从人类视频接收图像观察并以运动轨迹的形式输出动作。通过利用这一统一的跨体现动作空间，MT-pi仅使用几分钟的人类视频和有限的额外机器人演示即能成功完成任务。测试时，我们从两个摄像机视角预测运动轨迹，通过多视角合成恢复六自由度轨迹。MT-pi在四个真实任务中的平均成功率达到86.5%，优于不利用人类数据或我们动作空间的最新IL基线，并推广到仅存在于人类视频的场景中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人自主完成日常任务的教学存在挑战，尤其是数据收集方面。</li>
<li>模仿学习（IL）是一种有效的机器人技能传授方法，但需大量的机器人数据，人类视频提供了一种可替代的解决方案。</li>
<li>提出了一种新的模仿学习策略——运动轨迹策略（MT-pi），能从人类视频中学习并成功完成任务。</li>
<li>MT-pi通过将动作表示为图像上的短周期二维轨迹来解决直接从人类视频训练IL策略的挑战。</li>
<li>MT-pi利用统一的跨体现动作空间，仅使用几分钟的人类视频和有限的额外机器人演示就能成功完成任务。</li>
<li>在四个真实任务的测试中，MT-pi的平均成功率达到86.5%，显著优于其他IL策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06994">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06994v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion"><a href="#Synthetic-Prior-for-Few-Shot-Drivable-Head-Avatar-Inversion" class="headerlink" title="Synthetic Prior for Few-Shot Drivable Head Avatar Inversion"></a>Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</h2><p><strong>Authors:Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, Justus Thies, Timo Bolkart</strong></p>
<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle two major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Inspired by machine learning models trained solely on synthetic data, we propose a method that learns a prior model from a large dataset of synthetic heads with diverse identities, expressions, and viewpoints. With few input images, SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints. We model the head avatar using 3D Gaussian splatting and a convolutional encoder-decoder that outputs Gaussian parameters in UV texture space. To account for the different modeling complexities over parts of the head (e.g., skin vs hair), we embed the prior with explicit control for upsampling the number of per-part primitives. Compared to state-of-the-art monocular methods that require thousands of real training images, SynShot significantly improves novel view and expression synthesis. </p>
<blockquote>
<p>我们提出了SynShot，这是一种基于合成先验的可驾驶头部化身少样本反转的新型方法。我们解决了两个主要挑战。首先，训练可控的3D生成网络需要大量的不同序列，而图像和高质量跟踪网格的配对并不总是可用的。其次，最先进的单目化身模型很难推广到新的视角和表情，缺乏强大的先验知识，经常过度适应特定的视角分布。受到仅由合成数据训练的机器学习模型的启发，我们提出了一种从大量合成头部数据中学习先验模型的方法，这些合成数据具有不同的身份、表情和视角。凭借少量的输入图像，SynShot对预训练的合成先验进行了微调，以弥合领域之间的差距，从而模拟一个通用到各种新表情和视角的真实头部化身。我们使用三维高斯喷绘和卷积编码器解码器来输出UV纹理空间的高斯参数来模拟头部化身。考虑到头部各部分建模复杂性的差异（例如皮肤和头发），我们通过先验嵌入来明确控制每个部分原始数据的上采样数量。与需要数千张真实训练图像的最新单目方法相比，SynShot极大地改进了新颖视角和表情的合成效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06903v1">PDF</a> Website <a target="_blank" rel="noopener" href="https://zielon.github.io/synshot/">https://zielon.github.io/synshot/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了SynShot方法，这是一种基于合成先验的少数头部可驱动角色的新方法。解决了两个主要挑战：缺乏大规模多样序列的训练数据和对新视角和表情的泛化能力不足。SynShot学习了一个先验模型，从大量合成头部数据中训练得来，并利用少量输入图像微调预训练模型，以缩小领域差距，生成逼真的头部角色模型，能够泛化到新的视角和表情。通过3D高斯涂片和卷积编码器解码器建模头部角色，输出UV纹理空间的高斯参数。针对头部不同部分的建模复杂性（如皮肤和头发），嵌入先验知识以控制每个部分的原始数量。相较于需要数千张真实训练图像的最先进单眼方法，SynShot显著改善了对新视角和表情的合成效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynShot是一种基于合成先验的少数头部可驱动角色的新方法。</li>
<li>解决缺乏大规模多样序列训练数据和泛化能力不足的问题。</li>
<li>通过学习先验模型，利用少量输入图像微调预训练模型，生成逼真的头部角色模型。</li>
<li>模型能够泛化到新的视角和表情。</li>
<li>采用3D高斯涂片和卷积编码器解码器建模头部角色。</li>
<li>输出UV纹理空间的参数以精细控制模型的外观和纹理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06903v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Foundational-Generative-Model-for-Breast-Ultrasound-Image-Analysis"><a href="#A-Foundational-Generative-Model-for-Breast-Ultrasound-Image-Analysis" class="headerlink" title="A Foundational Generative Model for Breast Ultrasound Image Analysis"></a>A Foundational Generative Model for Breast Ultrasound Image Analysis</h2><p><strong>Authors:Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Haotian Ye, Siyu He, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, James Zou, Qingli Zhu, Yong Wang, Liwei Wang</strong></p>
<p>Foundational models have emerged as powerful tools for addressing various tasks in clinical settings. However, their potential development to breast ultrasound analysis remains untapped. In this paper, we present BUSGen, the first foundational generative model specifically designed for breast ultrasound image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen has acquired extensive knowledge of breast structures, pathological features, and clinical variations. With few-shot adaptation, BUSGen can generate repositories of realistic and informative task-specific data, facilitating the development of models for a wide range of downstream tasks. Extensive experiments highlight BUSGen’s exceptional adaptability, significantly exceeding real-data-trained foundational models in breast cancer screening, diagnosis, and prognosis. In breast cancer early diagnosis, our approach outperformed all board-certified radiologists (n&#x3D;9), achieving an average sensitivity improvement of 16.5% (P-value&lt;0.0001). Additionally, we characterized the scaling effect of using generated data which was as effective as the collected real-world data for training diagnostic models. Moreover, extensive experiments demonstrated that our approach improved the generalization ability of downstream models. Importantly, BUSGen protected patient privacy by enabling fully de-identified data sharing, making progress forward in secure medical data utilization. An online demo of BUSGen is available at <a target="_blank" rel="noopener" href="https://aibus.bio/">https://aibus.bio</a>. </p>
<blockquote>
<p>基础模型已作为临床环境中解决各种任务的强大工具而出现。然而，它们在乳腺超声分析方面的潜力尚未被开发。在本文中，我们介绍了BUSGen，这是专门为乳腺超声图像分析设计的基础生成模型。在超过350万张乳腺超声图像上进行预训练后，BUSGen获得了关于乳房结构、病理特征和临床变化的广泛知识。通过少量样本适应，BUSGen可以生成现实且信息丰富的特定任务数据仓库，促进针对各种下游任务的模型发展。大量实验突出了BUSGen的卓越适应性，在乳腺癌筛查、诊断和预后方面显著超过了使用真实数据训练的基础模型。在乳腺癌早期诊断方面，我们的方法超越了所有执业医师认证的放射科医生（n&#x3D;9），平均敏感性提高了16.5%（P值&lt;0.0001）。此外，我们描述了使用生成数据与收集的真实世界数据一样有效的扩展效应，用于训练诊断模型。而且，大量实验表明，我们的方法提高了下游模型的泛化能力。重要的是，BUSGen通过实现完全匿名数据共享保护了患者隐私，在安全医疗数据利用方面取得了进展。BUSGen的在线演示可在<a target="_blank" rel="noopener" href="https://aibus.bio查看./">https://aibus.bio查看。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06869v1">PDF</a> Peking University; Stanford University; Peking University Cancer   Hospital &amp; Institute; Peking Union Medical College Hospital; Cancer Hospital,   Chinese Academy of Medical Sciences</p>
<p><strong>Summary</strong></p>
<p>BUSGen是专门为乳腺超声图像分析设计的基础生成模型。它在超过350万张乳腺超声图像上进行预训练，并可通过少量样本适应生成特定任务的数据。该模型在乳腺癌筛查、诊断和预后等方面表现出卓越的性能，甚至超越了使用真实数据训练的基础模型。它提高了下游模型的泛化能力，并实现了患者数据的完全匿名共享。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BUSGen是首个针对乳腺超声分析的基础生成模型。</li>
<li>该模型在大量乳腺超声图像上进行预训练，涵盖乳房结构、病理特征和临床变化。</li>
<li>通过少量样本适应，BUSGen可以生成特定任务的数据。</li>
<li>BUSGen在乳腺癌筛查、诊断和预后方面表现出卓越性能。</li>
<li>该模型提高了下游模型的泛化能力。</li>
<li>BUSGen实现了患者数据的完全匿名共享，促进了医疗数据的利用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06869">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06869v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06869v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06869v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="An-efficient-approach-to-represent-enterprise-web-application-structure-using-Large-Language-Model-in-the-service-of-Intelligent-Quality-Engineering"><a href="#An-efficient-approach-to-represent-enterprise-web-application-structure-using-Large-Language-Model-in-the-service-of-Intelligent-Quality-Engineering" class="headerlink" title="An efficient approach to represent enterprise web application structure   using Large Language Model in the service of Intelligent Quality Engineering"></a>An efficient approach to represent enterprise web application structure   using Large Language Model in the service of Intelligent Quality Engineering</h2><p><strong>Authors:Zaber Al Hassan Ayon, Gulam Husain, Roshankumar Bisoi, Waliur Rahman, Dr Tom Osborn</strong></p>
<p>This paper presents a novel approach to represent enterprise web application structures using Large Language Models (LLMs) to enable intelligent quality engineering at scale. We introduce a hierarchical representation methodology that optimizes the few-shot learning capabilities of LLMs while preserving the complex relationships and interactions within web applications. The approach encompasses five key phases: comprehensive DOM analysis, multi-page synthesis, test suite generation, execution, and result analysis. Our methodology addresses existing challenges around usage of Generative AI techniques in automated software testing by developing a structured format that enables LLMs to understand web application architecture through in-context learning. We evaluated our approach using two distinct web applications: an e-commerce platform (Swag Labs) and a healthcare application (MediBox) which is deployed within Atalgo engineering environment. The results demonstrate success rates of 90% and 70%, respectively, in achieving automated testing, with high relevance scores for test cases across multiple evaluation criteria. The findings suggest that our representation approach significantly enhances LLMs’ ability to generate contextually relevant test cases and provide better quality assurance overall, while reducing the time and effort required for testing. </p>
<blockquote>
<p>本文提出了一种利用大型语言模型（LLM）表示企业Web应用程序结构的新方法，以实现大规模智能质量工程。我们引入了一种分层表示法，该方法在优化LLM的少量学习功能的同时，保持Web应用程序内部的复杂关系和交互。该方法包括五个关键阶段：全面的DOM分析、多页面合成、测试套件生成、执行和结果分析。我们的方法通过开发一种结构化格式，使LLM能够通过上下文学习理解Web应用程序架构，解决了在自动化软件测试中使用生成式人工智能技术所面临的挑战。我们使用两个不同的Web应用程序评估了我们的方法：电子商务平台（Swag Labs）和部署在Atalgo工程环境中的医疗应用程序（MediBox）。结果显示，在自动化测试方面，电子商务平台的成功率达到90%，医疗应用程序的成功率达到70%，测试用例在多个评估标准上具有很高的相关性得分。研究结果表明，我们的表示法显著提高了LLM生成上下文相关测试用例的能力，并总体上提供了更好的质量保证，同时减少了测试所需的时间和精力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06837v1">PDF</a> 16 pages, 1 figure and 4 tables, relevant for Gen AI and enterprise   AI use cases</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种利用大型语言模型（LLMs）进行智能质量工程的新方法，旨在代表企业Web应用程序结构。引入了一种层次化的表示方法，优化了LLMs的少样本学习能力，同时保持了Web应用程序内的复杂关系和交互。该方法包括五个关键阶段：全面的DOM分析、多页面合成、测试套件生成、执行和结果分析。通过开发一种结构化格式，使LLMs能够通过上下文学习理解Web应用程序架构，解决了在自动化软件测试中使用生成式人工智能技术的现有挑战。使用两个不同的Web应用程序（电子商务平台的Swag Labs和Atalgo工程环境中的医疗保健应用程序MediBox）进行了评估，结果显示自动化测试的成功率分别为90%和70%，测试用例的相关性得分较高。这表明我们的表示方法显著提高了LLMs生成上下文相关测试用例的能力，并提供了更好的质量保证，同时减少了测试所需的时间和精力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用大型语言模型（LLMs）进行企业Web应用程序的智能质量工程。</li>
<li>提出了一种新的层次化表示方法，优化了LLMs的少样本学习能力。</li>
<li>方法包括全面的DOM分析、多页面合成、测试套件生成、执行和结果分析。</li>
<li>通过开发结构化格式，解决了在自动化软件测试中使用生成式AI技术的挑战。</li>
<li>通过两个Web应用程序的评估，显示了自动化测试的高成功率。</li>
<li>表示方法提高了LLMs生成上下文相关测试用例的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06837">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06837v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Comparing-Few-Shot-Prompting-of-GPT-4-LLMs-with-BERT-Classifiers-for-Open-Response-Assessment-in-Tutor-Equity-Training"><a href="#Comparing-Few-Shot-Prompting-of-GPT-4-LLMs-with-BERT-Classifiers-for-Open-Response-Assessment-in-Tutor-Equity-Training" class="headerlink" title="Comparing Few-Shot Prompting of GPT-4 LLMs with BERT Classifiers for   Open-Response Assessment in Tutor Equity Training"></a>Comparing Few-Shot Prompting of GPT-4 LLMs with BERT Classifiers for   Open-Response Assessment in Tutor Equity Training</h2><p><strong>Authors:Sanjit Kakarla, Conrad Borchers, Danielle Thomas, Shambhavi Bhushan, Kenneth R. Koedinger</strong></p>
<p>Assessing learners in ill-defined domains, such as scenario-based human tutoring training, is an area of limited research. Equity training requires a nuanced understanding of context, but do contemporary large language models (LLMs) have a knowledge base that can navigate these nuances? Legacy transformer models like BERT, in contrast, have less real-world knowledge but can be more easily fine-tuned than commercial LLMs. Here, we study whether fine-tuning BERT on human annotations outperforms state-of-the-art LLMs (GPT-4o and GPT-4-Turbo) with few-shot prompting and instruction. We evaluate performance on four prediction tasks involving generating and explaining open-ended responses in advocacy-focused training lessons in a higher education student population learning to become middle school tutors. Leveraging a dataset of 243 human-annotated open responses from tutor training lessons, we find that BERT demonstrates superior performance using an offline fine-tuning approach, which is more resource-efficient than commercial GPT models. We conclude that contemporary GPT models may not adequately capture nuanced response patterns, especially in complex tasks requiring explanation. This work advances the understanding of AI-driven learner evaluation under the lens of fine-tuning versus few-shot prompting on the nuanced task of equity training, contributing to more effective training solutions and assisting practitioners in choosing adequate assessment methods. </p>
<blockquote>
<p>在基于情景的人类辅导训练等不明确领域中评估学习者是一个研究较少的领域。公平训练需要对背景有微妙的理解，但当代的大型语言模型（LLM）是否拥有能够应对这些微妙之处的知识库呢？相比之下，遗留的转换器模型（如BERT）虽然对真实世界的了解较少，但比商业LLM更容易微调。在这里，我们研究了微调BERT在人类注释上的表现是否优于最先进的大型语言模型GPT-4o和GPT-4 Turbo，采用少样本提示和指令。我们在四个预测任务中评估了性能，涉及在高等教育学生群体中，以倡导为核心的学习成为中学辅导老师的培训课程中，生成和解释开放式答案的响应。我们利用由人类注释的辅导训练课程的开放式答案数据集（共243份数据），发现使用离线微调方法的BERT表现出了优越的性能表现，该方法比商业GPT模型更加资源高效。我们得出结论，当代GPT模型可能无法充分捕捉复杂的响应模式，特别是在需要解释的复杂任务中。这项工作在微调与少样本提示下，进一步了解AI驱动的学习者评估在公平训练任务中的微妙之处，为更有效的训练解决方案做出贡献，并帮助从业者选择适当的评估方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06658v1">PDF</a> 8 Page Workshop Paper, AAAI2025 Workshop on Innovation and   Responsibility in AI-Supported Education (iRAISE) - Open-response Grading,   Feedback, Equity Training, LLMs, BERT, GPT-4</p>
<p><strong>Summary</strong></p>
<p>该研究探讨了评估基于情景的人类辅导训练中的学习者的问题。研究发现，虽然大型语言模型（LLMs）在处理复杂任务时面临挑战，但通过微调BERT模型在四项预测任务中的表现优于GPT模型。利用人类注释数据集进行研究的评估发现，通过线下微调的BERT模型更具优势。这显示出针对具体情境训练需求的细致考虑，并为更有效的训练解决方案和评估方法提供了参考。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究领域：该研究关注于基于情景的人类辅导训练中的学习者评估问题，探讨GPT模型和BERT模型在该领域的表现差异。</li>
<li>实验设计：通过比较不同模型在四项预测任务中的表现，发现BERT模型更具优势。采用人类注释数据集进行研究评估。</li>
<li>性能评估：研究发现，通过微调的BERT模型在预测任务中表现优于GPT模型，尤其是在复杂任务中需要解释的部分。</li>
<li>资源效率：使用线下微调的方法使得BERT模型更具资源效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06658">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06658v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2501.06658v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="BayesAdapter-enhanced-uncertainty-estimation-in-CLIP-few-shot-adaptation"><a href="#BayesAdapter-enhanced-uncertainty-estimation-in-CLIP-few-shot-adaptation" class="headerlink" title="BayesAdapter: enhanced uncertainty estimation in CLIP few-shot   adaptation"></a>BayesAdapter: enhanced uncertainty estimation in CLIP few-shot   adaptation</h2><p><strong>Authors:Pablo Morales-Álvarez, Stergios Christodoulidis, Maria Vakalopoulou, Pablo Piantanida, Jose Dolz</strong></p>
<p>The emergence of large pre-trained vision-language models (VLMs) represents a paradigm shift in machine learning, with unprecedented results in a broad span of visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited remarkable zero-shot and transfer learning capabilities in classification. To transfer CLIP to downstream tasks, adapters constitute a parameter-efficient approach that avoids backpropagation through the large model (unlike related prompt learning methods). However, CLIP adapters have been developed to target discriminative performance, and the quality of their uncertainty estimates has been overlooked. In this work we show that the discriminative performance of state-of-the-art CLIP adapters does not always correlate with their uncertainty estimation capabilities, which are essential for a safe deployment in real-world scenarios. We also demonstrate that one of such adapters is obtained through MAP inference from a more general probabilistic framework. Based on this observation we introduce BayesAdapter, which leverages Bayesian inference to estimate a full probability distribution instead of a single point, better capturing the variability inherent in the parameter space. In a comprehensive empirical evaluation we show that our approach obtains high quality uncertainty estimates in the predictions, standing out in calibration and selective classification. Our code will be publicly available upon acceptance of the paper. </p>
<blockquote>
<p>大型预训练视觉语言模型（VLMs）的出现代表了机器学习范式的一种转变，并在广泛的视觉识别任务中取得了前所未有的结果。CLIP作为最受欢迎的VLM之一，在分类方面展现出了惊人的零样本和迁移学习能力。为了将CLIP迁移到下游任务，适配器（adapters）是一种参数高效的策略，避免了在大模型中进行反向传播（这与相关的提示学习方法不同）。然而，CLIP适配器旨在提高判别性能，而忽略了其不确定性估计的质量。在这项工作中，我们展示了最先进的CLIP适配器的判别性能并不总是与他们的不确定性估计能力相关，这对于在现实场景中的安全部署至关重要。我们还证明，其中一些适配器是通过更通用的概率框架通过最大后验概率推断得到的。基于这一观察，我们引入了BayesAdapter，它利用贝叶斯推断来估计一个完整的概率分布，而不是一个单一的点，从而更好地捕捉参数空间中固有的变异性。在一项全面的经验评估中，我们展示了我们的方法在预测中获得高质量的不确定性估计，在校准和选择性分类方面表现出色。论文被接受后，我们的代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09718v2">PDF</a> 30 pages, 5 figures, 23 tables</p>
<p><strong>Summary</strong></p>
<p>大型预训练视觉语言模型（VLMs）的出现代表了机器学习领域的一个范式转变，已经在广泛的视觉识别任务中取得了前所未有的结果。CLIP作为一种流行的VLM，展现出令人印象深刻的零样本和迁移学习能力。为了在下游任务中迁移CLIP，适配器成为一种参数高效的方法，避免了通过大型模型的反向传播（与相关的提示学习方法不同）。然而，现有的CLIP适配器主要关注判别性能，忽略了不确定性估计的质量。本研究表明，最先进的CLIP适配器的判别性能并不总是与他们的不确定性估计能力相关，这对于在真实场景中的安全部署至关重要。此外，我们展示了通过更通用的概率框架进行MAP推理可以获得一种适配器。基于此观察，我们引入了BayesAdapter，它利用贝叶斯推理来估计一个完整的概率分布，而不是一个点，从而更好地捕捉参数空间中的固有变化。经过综合的实证评估，我们的方法在预测中获得高质量的不确定性估计，在校准和选择性分类方面表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型预训练视觉语言模型（VLMs）在视觉识别任务中取得显著成果，标志着机器学习领域的范式转变。</li>
<li>CLIP作为一种流行的VLM，具有零样本和迁移学习能力。</li>
<li>适配器是参数高效的方法，用于在下游任务中迁移CLIP，避免大型模型反向传播。</li>
<li>现有CLIP适配器主要关注判别性能，但不确定性估计质量同样重要，尤其在真实场景部署中。</li>
<li>通过更通用的概率框架进行MAP推理可得到一种适配器。</li>
<li>引入BayesAdapter，利用贝叶斯推理估计完整的概率分布，提高不确定性估计质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09718">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.09718v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Sparse-Attention-Vectors-Generative-Multimodal-Model-Features-Are-Discriminative-Vision-Language-Classifiers"><a href="#Sparse-Attention-Vectors-Generative-Multimodal-Model-Features-Are-Discriminative-Vision-Language-Classifiers" class="headerlink" title="Sparse Attention Vectors: Generative Multimodal Model Features Are   Discriminative Vision-Language Classifiers"></a>Sparse Attention Vectors: Generative Multimodal Model Features Are   Discriminative Vision-Language Classifiers</h2><p><strong>Authors:Chancharik Mitra, Brandon Huang, Tianning Chai, Zhiqiu Lin, Assaf Arbelle, Rogerio Feris, Leonid Karlinsky, Trevor Darrell, Deva Ramanan, Roei Herzig</strong></p>
<p>Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for foundational discriminative vision-language tasks (i.e., tasks requiring discrete label predictions) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for discriminative tasks is the extraction of useful features from generative models. To overcome this issue, we propose an approach for finding features in the model’s latent space to more effectively leverage LMMs for discriminative tasks. Toward this end, we present Sparse Attention Vectors (SAVs) – a finetuning-free method that leverages sparse attention head activations (fewer than 1% of the heads) in LMMs as strong features for VL tasks. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of discriminative tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations. </p>
<blockquote>
<p>生成式大型多模态模型（LMMs），如LLaVA和Qwen-VL，在各种视觉语言（VL）任务（如图像描述或视觉问答）中都表现出卓越的性能。尽管表现强劲，但LMMs并不直接适用于基础判别式视觉语言任务（即需要离散标签预测的任务），如图像分类和多项选择VQA。利用LMMs进行判别任务的一个关键挑战是从生成模型中提取有用的特征。为了解决这个问题，我们提出了一种在模型潜在空间中找到特征的方法，以更有效地利用LMMs进行判别任务。为此，我们提出了稀疏注意力向量（SAVs）——这是一种无需微调的方法，它利用LMM中不到1%的稀疏注意力头激活作为强大的VL任务特征。凭借少量样本，SAVs在多个判别任务上相对于各种小样方法和微调基准线展现了最先进的性能。我们的实验还暗示，随着额外样本的增加，SAVs的性能可以进一步提高，并能够推广至类似任务，这表明SAVs是有效且稳健的多模态特征表示。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00142v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型多模态模型（LMMs）如LLaVA和Qwen-VL的优异性能，它们擅长多种视觉语言（VL）任务，如图像描述和视觉问答。然而，对于需要离散标签预测的基础鉴别型视觉语言任务，如图像分类和多项选择问答等，LMMs并不直接适用。为了克服这一挑战，本研究提出了一种在模型潜在空间寻找特征的方法，以更有效地利用LMMs进行鉴别任务。为此，我们提出了稀疏注意力向量（SAVs）——一种无需微调的方法，利用LMMs中不到1%的稀疏注意力头激活作为视觉语言任务的强特征。仅通过少量样本，SAVs在多个鉴别任务上表现出卓越的性能，与多种少样本和微调基线相比具有显著优势。实验还表明，随着样本数量的增加，SAVs的性能可进一步提升，并能推广到类似任务，证明了其作为有效且稳健的多模态特征表示的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMMs如LLaVA和Qwen-VL擅长图像描述和视觉问答等VL任务。</li>
<li>LMMs不直接适用于需要离散标签预测的鉴别型视觉语言任务。</li>
<li>利用模型潜在空间寻找特征的方法能更有效地利用LMMs进行鉴别任务。</li>
<li>SAVs是一种无需微调的方法，利用稀疏注意力头激活作为强特征。</li>
<li>SAVs在少量样本下表现出卓越性能，并在多个鉴别任务上优于其他方法。</li>
<li>SAVs性能可随样本数量增加而提升，并具备推广到类似任务的能力。</li>
<li>SAVs作为有效且稳健的多模态特征表示具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00142">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2412.00142v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Task-Learning-through-Inverse-Generative-Modeling"><a href="#Few-Shot-Task-Learning-through-Inverse-Generative-Modeling" class="headerlink" title="Few-Shot Task Learning through Inverse Generative Modeling"></a>Few-Shot Task Learning through Inverse Generative Modeling</h2><p><strong>Authors:Aviv Netanyahu, Yilun Du, Antonia Bronars, Jyothish Pari, Joshua Tenenbaum, Tianmin Shu, Pulkit Agrawal</strong></p>
<p>Learning the intents of an agent, defined by its goals or motion style, is often extremely challenging from just a few examples. We refer to this problem as task concept learning and present our approach, Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM), which learns new task concepts by leveraging invertible neural generative models. The core idea is to pretrain a generative model on a set of basic concepts and their demonstrations. Then, given a few demonstrations of a new concept (such as a new goal or a new action), our method learns the underlying concepts through backpropagation without updating the model weights, thanks to the invertibility of the generative model. We evaluate our method in five domains – object rearrangement, goal-oriented navigation, motion caption of human actions, autonomous driving, and real-world table-top manipulation. Our experimental results demonstrate that via the pretrained generative model, we successfully learn novel concepts and generate agent plans or motion corresponding to these concepts in (1) unseen environments and (2) in composition with training concepts. </p>
<blockquote>
<p>从少量示例中学习代理的意图（由其目标或动作风格定义）通常极具挑战性。我们将此问题称为任务概念学习，并提出我们的方法——通过逆向生成建模进行少量任务学习（FTL-IGM），该方法利用可逆神经生成模型来学习新任务概念。核心思想是在一组基本概念及其演示上预训练生成模型。然后，对于新概念的一些演示（例如新目标或新动作），我们的方法通过反向传播学习潜在概念，并且由于生成模型的可逆性，无需更新模型权重。我们在五个领域评估了我们的方法——物体重组、目标导向导航、人类动作的运动字幕、自动驾驶和现实世界桌面操作。我们的实验结果表明，通过预训练的生成模型，我们成功地学习了新概念和生成与这些概念相对应的代理计划或运动：（1）在未见过的环境中；（2）与训练概念组合在一起。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04987v2">PDF</a> Added acknowledgment</p>
<p><strong>Summary</strong></p>
<p>基于有限示例学习代理的意图，即其目标或动作风格，是一项重大挑战。我们称此问题为任务概念学习，并提出我们的方法——通过逆向生成模型进行少量任务学习（FTL-IGM），利用可逆神经生成模型学习新任务概念。核心思想是在基本概念及其演示上预训练生成模型，然后只需少量新概念的演示（如新目标或新动作），即可通过反向传播学习潜在概念，得益于生成模型的可逆性，无需更新模型权重。我们在五个领域评估了我们的方法——物体重新排列、目标导向导航、人类动作运动字幕、自动驾驶和现实世界桌面操作。实验结果表明，通过预训练的生成模型，我们成功学习了新概念和生成与这些概念相对应的代理计划或运动，在未见过的环境中以及组合训练概念的情况下均如此。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Task Learning through Inverse Generative Modeling (FTL-IGM) 是一种解决任务概念学习问题的方法。</li>
<li>该方法利用可逆神经生成模型，通过预训练学习基本概念及其演示。</li>
<li>仅需少量新概念的演示，即可通过反向传播学习潜在概念，无需更新模型权重。</li>
<li>FTL-IGM 在五个不同领域进行了评估，包括物体重新排列、目标导向导航、运动字幕、自动驾驶和桌面操作。</li>
<li>实验结果表明，该方法能够成功学习新概念和生成与这些新概念相对应的代理计划或运动。</li>
<li>该方法能够在未见过的环境中以及组合训练概念的情况下进行学习。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04987">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2411.04987v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FoMo-A-Foundation-Model-for-Mobile-Traffic-Forecasting-with-Diffusion-Model"><a href="#FoMo-A-Foundation-Model-for-Mobile-Traffic-Forecasting-with-Diffusion-Model" class="headerlink" title="FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model"></a>FoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model</h2><p><strong>Authors:Haoye Chai, Xiaoqian Qi, Shiyuan Zhang, Yong Li</strong></p>
<p>Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero&#x2F;few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short&#x2F;long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero&#x2F;few-shot learning, showcasing a strong universality. </p>
<blockquote>
<p>移动流量预测使运营商能够提前预测网络动态和性能，为提高服务质量和改善用户体验提供了巨大潜力。然而，现有模型通常是面向任务的，并且使用定制数据进行训练，这限制了它们在基站部署、资源配置、能源优化等多样化移动网络任务中的有效性，并阻碍了它们在不同城市环境中的泛化能力。由于其在多任务适应和零&#x2F;少样本学习能力方面的突出表现，基础模型已在NLP和CV的各个领域取得了显著的进步。在本文中，我们提出了一种创新的移动流量预测基础模型（FoMo），旨在处理短期&#x2F;长期预测和跨多个城市的分布生成的多样化预测任务，以支持网络规划和优化。FoMo结合了扩散模型和转换器，其中提出了各种时空掩码，以使其能够学习不同任务的内蕴特征，并开发了一种对比学习策略来捕捉移动流量与城市背景之间的相关性，从而提高其迁移学习能力。在9个真实世界数据集上的大量实验表明，FoMo在多样化的预测任务和零&#x2F;少样本学习上优于当前模型，展示了强大的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15322v2">PDF</a> 11 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一个创新的移动流量预测基础模型（FoMo），旨在处理多样的预测任务，包括短期和长期预测以及跨多个城市的分布生成，以支持网络规划和优化。FoMo结合了扩散模型和转换器，通过提出各种时空掩码来学习任务的内蕴特征，并发展了一种对比学习策略来捕捉移动流量和城市环境之间的关联，提高了其迁移学习能力。在9个真实世界数据集上的实验表明，FoMo在多样预测任务和零&#x2F;少样本学习方面优于当前模型，表现出强大的通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>移动流量预测允许运营商预测网络动态和性能，对提高服务质量和用户体验有巨大潜力。</li>
<li>现有模型通常面向特定任务，并使用定制数据进行训练，这在多样化的移动网络任务（如基站部署、资源分配、能源优化等）中限制了其有效性，并阻碍了在不同城市环境中的泛化。</li>
<li>提出的FoMo模型旨在处理多样化的预测任务，包括短期和长期预测以及跨多个城市的分布生成，以支持网络规划和优化。</li>
<li>FoMo结合了扩散模型和转换器。</li>
<li>FoMo通过提出各种时空掩码来学习任务的内蕴特征。</li>
<li>对比学习策略被开发来捕捉移动流量和城市环境之间的关联，提高了模型的迁移学习能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15322">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.15322v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Retrieval-Reasoning-Large-Language-Model-based-Synthetic-Clinical-Trial-Generation"><a href="#Retrieval-Reasoning-Large-Language-Model-based-Synthetic-Clinical-Trial-Generation" class="headerlink" title="Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial   Generation"></a>Retrieval-Reasoning Large Language Model-based Synthetic Clinical Trial   Generation</h2><p><strong>Authors:Zerui Xu, Fang Wu, Yuanyuan Zhang, Yue Zhao</strong></p>
<p>Machine learning (ML) exhibits promise in the clinical domain. However, it is constrained by data scarcity and ethical considerations, as the generation of clinical trials presents significant challenges due to stringent privacy regulations, high costs, and the extended duration required for conducting studies with human participants. Despite the advancements of large language models (LLMs) in general generation tasks, their potential in facilitating the generation of synthetic clinical trials is under-explored. To address this gap, we introduce a novel Retrieval-Reasoning few-shot framework that leverages LLMs to generate artificial yet realistic and diverse clinical trials with binary success&#x2F;failure labels. Experiments conducted on real clinical trials from the \url{ClinicalTrials.gov} database demonstrate that our synthetic data can effectively augment real datasets. Furthermore, by fine-tuning a pre-trained model as a binary classifier on synthetic clinical trial datasets, we demonstrate that this augmentation enhances model training for downstream tasks such as trial outcome prediction. Our findings suggest that LLMs for synthetic clinical trial generation hold promise for accelerating clinical research and upholding ethical standards for patient privacy. The code is publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4">https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4</a>. </p>
<blockquote>
<p>机器学习（ML）在临床领域具有广阔前景。然而，由于严格的隐私法规、高昂成本和开展研究所需的长周期，临床试验的生成面临巨大挑战，导致数据稀缺和伦理考量限制了其发展。尽管大型语言模型（LLM）在通用生成任务中取得了进展，但其在促进合成临床试验生成方面的潜力尚未得到充分探索。为了弥补这一空白，我们提出了一种新型的检索推理小样框架，该框架利用LLM生成人工但现实且多样的临床试验数据，并带有二元成功&#x2F;失败标签。在ClinicalTrials.gov数据库的实际临床试验上进行的实验表明，我们的合成数据可以有效地扩充真实数据集。此外，通过对预训练模型进行微调，将其作为合成临床试验数据集上的二元分类器，我们证明了这种扩充可以增强下游任务的模型训练，如试验结局预测。我们的研究结果表明，用于合成临床试验生成的LLM在加速临床研究和维持患者隐私的伦理标准方面具发展潜力。代码公开在：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r%2Fr%2FRetrieval_Reasoning_Clinical_Trial_Generation-3EC4%E3%80%82">https://anonymous.4open.science/r/Retrieval_Reasoning_Clinical_Trial_Generation-3EC4。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12476v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>机器学习在临床领域具有广阔的应用前景，但受限于数据稀缺和伦理考量。由于严格的隐私规定、高昂成本和长期的研究周期，临床试验的生成面临重大挑战。尽管大型语言模型在通用生成任务上有所突破，但其用于生成合成临床试验的潜力尚未得到充分探索。本研究提出了一种新型的检索推理少样本框架，利用大型语言模型生成具有二进制成功&#x2F;失败标签的合成临床试验数据。实验证明，合成数据能有效扩充真实数据集，且通过微调预训练模型作为二元分类器进行训练，能提高下游任务如试验结局预测的模型性能。研究结果表明，利用大型语言模型生成合成临床试验数据对于加速临床研究并维持患者隐私的伦理标准具有潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习在临床应用面临数据稀缺和伦理挑战。</li>
<li>大型语言模型在临床试验生成方面的潜力尚未充分探索。</li>
<li>提出了一种新型的检索推理少样本框架，用于生成合成临床试验数据。</li>
<li>合成数据能有效扩充真实数据集。</li>
<li>通过微调预训练模型，合成数据能提高模型在临床试验结局预测等下游任务的性能。</li>
<li>利用大型语言模型生成合成临床试验数据有助于加速临床研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Few-Shot/2410.12476v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-4c6d1c3d0ec7abcaf2f0ea1c61f31551.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-01-16  SAR Strikes Back A New Hope for RSVQA
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2464381031b26a2c2b8f311f42c0e32.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-16  Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">14643.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
