<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Loudspeaker Beamforming to Enhance Speech Recognition Performance of   Voice Driven Applications">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.10376v2/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-16-æ›´æ–°"><a href="#2025-01-16-æ›´æ–°" class="headerlink" title="2025-01-16 æ›´æ–°"></a>2025-01-16 æ›´æ–°</h1><h2 id="Loudspeaker-Beamforming-to-Enhance-Speech-Recognition-Performance-of-Voice-Driven-Applications"><a href="#Loudspeaker-Beamforming-to-Enhance-Speech-Recognition-Performance-of-Voice-Driven-Applications" class="headerlink" title="Loudspeaker Beamforming to Enhance Speech Recognition Performance of   Voice Driven Applications"></a>Loudspeaker Beamforming to Enhance Speech Recognition Performance of   Voice Driven Applications</h2><p><strong>Authors:Dimme de Groot, Baturalp Karslioglu, Odette Scharenborg, Jorge Martinez</strong></p>
<p>In this paper we propose a robust loudspeaker beamforming algorithm which is used to enhance the performance of voice driven applications in scenarios where the loudspeakers introduce the majority of the noise, e.g. when music is playing loudly. The loudspeaker beamformer modifies the loudspeaker playback signals to create a low-acoustic-energy region around the device that implements automatic speech recognition for a voice driven application (VDA). The algorithm utilises a distortion measure based on human auditory perception to limit the distortion perceived by human listeners. Simulations and real-world experiments show that the proposed loudspeaker beamformer improves the speech recognition performance in all tested scenarios. Moreover, the algorithm allows to further reduce the acoustic energy around the VDA device at the expense of reduced objective audio quality at the listenerâ€™s location. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ‰¬å£°å™¨æ³¢æŸå½¢æˆç®—æ³•ï¼Œç”¨äºæé«˜åœ¨æ‰¬å£°å™¨äº§ç”Ÿå¤§éƒ¨åˆ†å™ªéŸ³çš„åœºæ™¯ä¸­è¯­éŸ³é©±åŠ¨åº”ç”¨çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨æ’­æ”¾å¤§å£°éŸ³ä¹æ—¶ã€‚æ‰¬å£°å™¨æ³¢æŸå½¢æˆå™¨ä¿®æ”¹æ‰¬å£°å™¨æ’­æ”¾ä¿¡å·ï¼Œä»¥åœ¨è®¾å¤‡å‘¨å›´åˆ›å»ºä¸€ä¸ªä½å£°å­¦èƒ½é‡åŒºåŸŸï¼Œä»è€Œå®ç°è¯­éŸ³é©±åŠ¨åº”ç”¨ï¼ˆVDAï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚è¯¥ç®—æ³•åˆ©ç”¨åŸºäºäººç±»å¬è§‰æ„ŸçŸ¥çš„å¤±çœŸåº¦é‡æ¥é™åˆ¶äººç±»å¬ä¼—æ„ŸçŸ¥åˆ°çš„å¤±çœŸã€‚æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ‰¬å£°å™¨æ³¢æŸå½¢æˆå™¨åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­æé«˜äº†è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•å…è®¸åœ¨ç‰ºç‰²å¬ä¼—ä½ç½®çš„å®¢è§‚éŸ³é¢‘è´¨é‡çš„æƒ…å†µä¸‹ï¼Œè¿›ä¸€æ­¥å‡å°‘VDAè®¾å¤‡å‘¨å›´çš„å£°å­¦èƒ½é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08104v1">PDF</a> To appear at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¨³å¥çš„æ‰¬å£°å™¨æ³¢æŸæˆå½¢ç®—æ³•ï¼Œç”¨äºæå‡åœ¨ç‰¹å®šåœºæ™¯ï¼ˆå¦‚éŸ³ä¹æ’­æ”¾ç­‰ç¯å¢ƒå™ªå£°ä¸»è¦æ¥æºäºæ‰¬å£°å™¨ï¼‰ä¸‹è¯­éŸ³é©±åŠ¨åº”ç”¨çš„æ€§èƒ½ã€‚è¯¥æ³¢æŸæˆå½¢å™¨é€šè¿‡ä¿®æ”¹æ‰¬å£°å™¨æ’­æ”¾ä¿¡å·ï¼Œåœ¨è®¾å¤‡å‘¨å›´åˆ›å»ºä¸€ä¸ªä½å£°èƒ½åŒºåŸŸï¼Œç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚ç®—æ³•åŸºäºäººç±»å¬è§‰æ„ŸçŸ¥çš„å¤±çœŸåº¦é‡æ¥é™åˆ¶äººç±»å¬ä¼—æ„ŸçŸ¥åˆ°çš„å¤±çœŸã€‚æ¨¡æ‹Ÿå’ŒçœŸå®å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ‰¬å£°å™¨æ³¢æŸæˆå½¢å™¨åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­æé«˜äº†è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•å…è®¸è¿›ä¸€æ­¥é™ä½VDAè®¾å¤‡å‘¨å›´çš„å£°èƒ½ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²å¬ä¼—ä½ç½®çš„å®¢è§‚éŸ³é¢‘è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ‰¬å£°å™¨çš„æ³¢æŸæˆå½¢ç®—æ³•ï¼Œæ—¨åœ¨æé«˜è¯­éŸ³é©±åŠ¨åº”ç”¨åœ¨æœ‰å™ªå£°åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>è¯¥ç®—æ³•é€šè¿‡ä¿®æ”¹æ‰¬å£°å™¨æ’­æ”¾ä¿¡å·ï¼Œåœ¨è®¾å¤‡å‘¨å›´å½¢æˆä½å£°èƒ½åŒºåŸŸï¼Œæœ‰åŠ©äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>ç®—æ³•è€ƒè™‘äº†äººç±»å¬è§‰æ„ŸçŸ¥çš„å¤±çœŸåº¦é‡ï¼Œä»¥é™åˆ¶å¬ä¼—æ„ŸçŸ¥åˆ°çš„å¤±çœŸã€‚</li>
<li>æ¨¡æ‹Ÿå’ŒçœŸå®å®éªŒè¡¨æ˜ï¼Œæ­¤ç®—æ³•æé«˜äº†æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>è¯¥ç®—æ³•å…è®¸è¿›ä¸€æ­¥é™ä½è®¾å¤‡å‘¨å›´çš„å£°èƒ½ï¼Œä½†å¯èƒ½å½±å“å¬ä¼—ä½ç½®çš„éŸ³é¢‘è´¨é‡ã€‚</li>
<li>æ­¤ç®—æ³•ç‰¹åˆ«é€‚ç”¨äºéŸ³ä¹æ’­æ”¾ç­‰åœºæ™¯ï¼Œå…¶ä¸­æ‰¬å£°å™¨æ˜¯ä¸»è¦å™ªå£°æ¥æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29c62c46ecc3f5731369fc8652937416.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3e640b1b4577c434f286d203911f11e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04f1d124cccb281a7ad18989b84175b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-41a8a7a1ce886ccc9dee9685a6464d56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f8f9829e1b16091db0fa7b569c76b5d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Efficient-Event-based-Delay-Learning-in-Spiking-Neural-Networks"><a href="#Efficient-Event-based-Delay-Learning-in-Spiking-Neural-Networks" class="headerlink" title="Efficient Event-based Delay Learning in Spiking Neural Networks"></a>Efficient Event-based Delay Learning in Spiking Neural Networks</h2><p><strong>Authors:BalÃ¡zs MÃ©szÃ¡ros, James C. Knight, Thomas Nowotny</strong></p>
<p>Spiking Neural Networks (SNNs) are attracting increased attention as a more energy-efficient alternative to traditional Artificial Neural Networks. Spiking neurons are stateful and intrinsically recurrent, making them well-suited for spatio-temporal tasks. However, this intrinsic memory is limited by synaptic and membrane time constants. A powerful additional mechanism are delays. In this paper, we propose a novel event-based training method for SNNs with delays, grounded in the EventProp formalism and enabling the calculation of exact gradients with respect to weights and delays. Our method supports multiple spikes per neuron and, to our best knowledge, is the first delay learning method applicable to recurrent connections. We evaluate our method on a simple sequence detection task, and the Yin-Yang, Spiking Heidelberg Digits and Spiking Speech Commands datasets, demonstrating that our algorithm can optimize delays from suboptimal initial conditions and enhance classification accuracy compared to architectures without delays. Finally, we show that our approach uses less than half the memory of the current state-of-the-art delay-learning method and is up to 26x faster. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpiking Neural Networksï¼Œç®€ç§°SNNsï¼‰ä½œä¸ºä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œçš„ä¸€ç§èƒ½æ•ˆæ›´é«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ­£å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚è„‰å†²ç¥ç»å…ƒå…·æœ‰çŠ¶æ€å’Œå†…åœ¨å¤ç°æ€§ï¼Œä½¿å…¶éå¸¸é€‚åˆå¤„ç†æ—¶ç©ºä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§å†…åœ¨è®°å¿†å—åˆ°çªè§¦å’Œè†œæ—¶é—´å¸¸æ•°çš„é™åˆ¶ã€‚ä¸€ç§å¼ºå¤§çš„é™„åŠ æœºåˆ¶æ˜¯å»¶è¿Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶çš„æ–°å‹è„‰å†²ç¥ç»ç½‘ç»œå»¶è¿Ÿè®­ç»ƒæ³•ï¼Œè¯¥æ–¹æ³•ä»¥EventPropå½¢å¼ä¸ºåŸºç¡€ï¼Œèƒ½å¤Ÿè®¡ç®—å…³äºæƒé‡å’Œå»¶è¿Ÿçš„ç¡®åˆ‡æ¢¯åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒæ¯ä¸ªç¥ç»å…ƒçš„å¤šæ¬¡è„‰å†²ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªé€‚ç”¨äºå¤ç°è¿æ¥çš„å»¶è¿Ÿå­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ç®€å•çš„åºåˆ—æ£€æµ‹ä»»åŠ¡ä»¥åŠé˜´é˜³ã€è„‰å†²æµ·å¾·å ¡æ•°å­—å’Œè„‰å†²è¯­éŸ³å‘½ä»¤æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¯ä»¥ä»æ¬¡ä¼˜åˆå§‹æ¡ä»¶ä¼˜åŒ–å»¶è¿Ÿï¼Œå¹¶åœ¨åˆ†ç±»å‡†ç¡®æ€§æ–¹é¢è¶…è¿‡äº†æ²¡æœ‰å»¶è¿Ÿçš„æ¶æ„ã€‚æœ€åï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨çš„å†…å­˜ä¸åˆ°å½“å‰æœ€å…ˆè¿›çš„å»¶è¿Ÿå­¦ä¹ æ–¹æ³•çš„ä¸€åŠï¼Œå¹¶ä¸”é€Ÿåº¦æœ€å¿«å¯è¾¾å…¶26å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07331v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>ç¥ç»ç½‘ç»œé‡‡ç”¨è„‰å†²æ´»åŠ¨èƒ½èŠ‚çœèƒ½è€—ã€‚è„‰å†²ç¥ç»å…ƒå…·å¤‡çŠ¶æ€å’Œæ—¶ç©ºé€’å½’å±æ€§ï¼Œä½†å…¶æœ‰é™æ—¶é—´å¸¸æ•°ä½¿å¾—å­˜å‚¨èƒ½åŠ›ä¸è¶³ã€‚å¯¹æ­¤ä¸è¶³çš„é—®é¢˜å¼•å…¥äº†å»¶è¿Ÿæœºåˆ¶ä½œä¸ºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„ä¸€ç§å¼ºå¤§è¡¥å……ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºäº‹ä»¶è®­ç»ƒè„‰å†²ç¥ç»ç½‘ç»œçš„æ–°æ–¹æ³•ï¼Œä»¥EventPropä¸ºç†è®ºåŸºç¡€ï¼Œèƒ½å¤Ÿç²¾ç¡®è®¡ç®—æƒé‡å’Œå»¶è¿Ÿçš„æ¢¯åº¦ã€‚è¯¥æ–¹æ³•æ”¯æŒç¥ç»å…ƒå¤šé‡è„‰å†²ï¼Œä¸”æ˜¯é¦–ä¸ªé€‚ç”¨äºé€’å½’è¿æ¥çš„å»¶è¿Ÿå­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡ç®€å•åºåˆ—æ£€æµ‹ä»»åŠ¡ä»¥åŠYin-Yangã€Spiking Heidelberg Digitså’ŒSpiking Speech Commandsæ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥ä»ä¸ç†æƒ³çš„åˆå§‹æ¡ä»¶ä¼˜åŒ–å»¶è¿Ÿå¹¶æé«˜åˆ†ç±»ç²¾åº¦ï¼ŒåŒæ—¶é™ä½äº†å½“å‰æœ€ä½³æ–¹æ³•çš„å†…å­˜æ¶ˆè€—å¹¶æé«˜äº†è¿è¡Œé€Ÿåº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰ä½œä¸ºä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œèŠ‚èƒ½çš„æ›¿ä»£æ–¹æ¡ˆæ­£å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>è„‰å†²ç¥ç»å…ƒå…·å¤‡çŠ¶æ€å’Œæ—¶ç©ºé€’å½’å±æ€§ï¼Œä½†å­˜åœ¨å­˜å‚¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³å­˜å‚¨èƒ½åŠ›ä¸è¶³çš„é—®é¢˜å¼•å…¥äº†å»¶è¿Ÿæœºåˆ¶ä½œä¸ºè¡¥å……ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºäº‹ä»¶è®­ç»ƒè„‰å†²ç¥ç»ç½‘ç»œçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç²¾ç¡®è®¡ç®—æƒé‡å’Œå»¶è¿Ÿçš„æ¢¯åº¦å¹¶æ”¯æŒç¥ç»å…ƒå¤šé‡è„‰å†²ã€‚</li>
<li>æ­¤æ–¹æ³•æ˜¯é¦–ä¸ªé€‚ç”¨äºé€’å½’è¿æ¥çš„å»¶è¿Ÿå­¦ä¹ æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.07331v1/page_0_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-985974598a650db595f2e82aee117459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb9b02ac2bbea6ac68b241cdf29c6b21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9dd79f1fc8a708d9639de21b95b99de.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Joint-Automatic-Speech-Recognition-And-Structure-Learning-For-Better-Speech-Understanding"><a href="#Joint-Automatic-Speech-Recognition-And-Structure-Learning-For-Better-Speech-Understanding" class="headerlink" title="Joint Automatic Speech Recognition And Structure Learning For Better   Speech Understanding"></a>Joint Automatic Speech Recognition And Structure Learning For Better   Speech Understanding</h2><p><strong>Authors:Jiliang Hu, Zuchao Li, Mengjia Shen, Haojun Ai, Sheng Li, Jun Zhang</strong></p>
<p>Spoken language understanding (SLU) is a structure prediction task in the field of speech. Recently, many works on SLU that treat it as a sequence-to-sequence task have achieved great success. However, This method is not suitable for simultaneous speech recognition and understanding. In this paper, we propose a joint speech recognition and structure learning framework (JSRSL), an end-to-end SLU model based on span, which can accurately transcribe speech and extract structured content simultaneously. We conduct experiments on name entity recognition and intent classification using the Chinese dataset AISHELL-NER and the English dataset SLURP. The results show that our proposed method not only outperforms the traditional sequence-to-sequence method in both transcription and extraction capabilities but also achieves state-of-the-art performance on the two datasets. </p>
<blockquote>
<p>è¯­éŸ³è¯­è¨€ç†è§£ï¼ˆSLUï¼‰æ˜¯è¯­éŸ³é¢†åŸŸçš„ä¸€ç§ç»“æ„é¢„æµ‹ä»»åŠ¡ã€‚æœ€è¿‘ï¼Œå°†SLUè§†ä¸ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„è®¸å¤šå·¥ä½œéƒ½å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¸é€‚ç”¨äºåŒæ­¥è¯­éŸ³è¯†åˆ«å’Œç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè”åˆè¯­éŸ³è¯†åˆ«å’Œç»“æ„å­¦ä¹ æ¡†æ¶ï¼ˆJSRSLï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºèŒƒå›´çš„ç«¯åˆ°ç«¯SLUæ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶å‡†ç¡®è½¬å½•è¯­éŸ³å¹¶æå–ç»“æ„å†…å®¹ã€‚æˆ‘ä»¬ä½¿ç”¨AISHELL-NERä¸­æ–‡æ•°æ®é›†å’ŒSLURPè‹±æ–‡æ•°æ®é›†è¿›è¡Œå‘½åå®ä½“è¯†åˆ«å’Œæ„å›¾åˆ†ç±»å®éªŒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸ä»…åœ¨è½¬å½•å’Œæå–èƒ½åŠ›ä¸Šä¼˜äºä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—æ–¹æ³•ï¼Œè€Œä¸”åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07329v1">PDF</a> 5 pages, 2 figures, accepted by ICASSP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§è”åˆè¯­éŸ³è¯†åˆ«ä¸ç»“æ„å­¦ä¹ æ¡†æ¶ï¼ˆJSRSLï¼‰ï¼Œè¯¥æ¡†æ¶æ˜¯ä¸€ç§åŸºäºè·¨åº¦çš„ç«¯åˆ°ç«¯SLUæ¨¡å‹ï¼Œèƒ½åŒæ—¶å‡†ç¡®è½¬å½•è¯­éŸ³å¹¶æå–ç»“æ„åŒ–å†…å®¹ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿåºåˆ—åˆ°åºåˆ—æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨è½¬å½•å’Œæå–èƒ½åŠ›æ–¹é¢è¡¨ç°æ›´ä¼˜ç§€ï¼ŒåŒæ—¶åœ¨ä¸­æ–‡æ•°æ®é›†AISHELL-NERå’Œè‹±æ–‡æ•°æ®é›†SLURPä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SLUè¢«è§†ä¸ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡å¹¶å·²æœ‰è®¸å¤šæˆåŠŸå®è·µã€‚</li>
<li>ä¼ ç»Ÿåºåˆ—åˆ°åºåˆ—æ–¹æ³•ä¸é€‚ç”¨äºåŒæ—¶è¯­éŸ³è¯†åˆ«çš„ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è”åˆè¯­éŸ³è¯†åˆ«ä¸ç»“æ„å­¦ä¹ æ¡†æ¶ï¼ˆJSRSLï¼‰ã€‚</li>
<li>JSRSLæ˜¯ä¸€ä¸ªåŸºäºè·¨åº¦çš„ç«¯åˆ°ç«¯SLUæ¨¡å‹ï¼Œå¯åŒæ—¶å‡†ç¡®è½¬å½•è¯­éŸ³å’Œæå–ç»“æ„åŒ–å†…å®¹ã€‚</li>
<li>åœ¨ä¸­æ–‡æ•°æ®é›†AISHELL-NERå’Œè‹±æ–‡æ•°æ®é›†SLURPä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>JSRSLåœ¨è½¬å½•å’Œæå–èƒ½åŠ›ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿåºåˆ—åˆ°åºåˆ—æ–¹æ³•ã€‚</li>
<li>JSRSLå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2845d2122719d4770c3a7ecbd71eed9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc6bf83485a8dacb7fed11cd74d77555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53739feb33157342530b35785b30d07f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e44519ad3d7028791d4b1d2b6f2ba791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e21be9aec2b62f0ce2c904ab1173901.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AdaCS-Adaptive-Normalization-for-Enhanced-Code-Switching-ASR"><a href="#AdaCS-Adaptive-Normalization-for-Enhanced-Code-Switching-ASR" class="headerlink" title="AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR"></a>AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR</h2><p><strong>Authors:The Chuong Chu, Vu Tuan Dat Pham, Kien Dao, Hoang Nguyen, Quoc Hung Truong</strong></p>
<p>Intra-sentential code-switching (CS) refers to the alternation between languages that happens within a single utterance and is a significant challenge for Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese speaker uses foreign proper names or specialized terms within their speech. ASR systems often struggle to accurately transcribe intra-sentential CS due to their training on monolingual data and the unpredictable nature of CS. This issue is even more pronounced for low-resource languages, where limited data availability hinders the development of robust models. In this study, we propose AdaCS, a normalization model integrates an adaptive bias attention module (BAM) into encoder-decoder network. This novel approach provides a robust solution to CS ASR in unseen domains, thereby significantly enhancing our contribution to the field. By utilizing BAM to both identify and normalize CS phrases, AdaCS enhances its adaptive capabilities with a biased list of words provided during inference. Our method demonstrates impressive performance and the ability to handle unseen CS phrases across various domains. Experiments show that AdaCS outperforms previous state-of-the-art method on Vietnamese CS ASR normalization by considerable WER reduction of 56.2% and 36.8% on the two proposed test sets. </p>
<blockquote>
<p>å¥å­å†…è¯­è¨€åˆ‡æ¢ï¼ˆCSï¼‰æ˜¯æŒ‡åœ¨å•ä¸ªå‘è¨€ä¸­å‘ç”Ÿçš„ä¸¤ç§è¯­è¨€ä¹‹é—´çš„äº¤æ›¿åˆ‡æ¢ï¼Œå¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œå½“è¶Šå—è¯­è¯´è¯è€…åœ¨ä»–ä»¬çš„æ¼”è®²ä¸­ä½¿ç”¨å¤–è¯­ä¸“æœ‰åè¯æˆ–ä¸“ä¸šæœ¯è¯­æ—¶ã€‚ç”±äºASRç³»ç»Ÿçš„è®­ç»ƒæ˜¯åŸºäºå•è¯­è¨€æ•°æ®ä»¥åŠCSçš„ä¸å¯é¢„æµ‹æ€§ï¼Œå› æ­¤å®ƒä»¬ç»å¸¸éš¾ä»¥å‡†ç¡®è½¬å½•å¥å­å†…çš„CSã€‚å¯¹äºèµ„æºè´«ä¹çš„è¯­è¨€ï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºçªå‡ºï¼Œæœ‰é™çš„å¯ç”¨æ•°æ®é˜»ç¢äº†ç¨³å¥æ¨¡å‹çš„å‘å±•ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AdaCSï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆè‡ªé€‚åº”åè§æ³¨æ„åŠ›æ¨¡å—ï¼ˆBAMï¼‰çš„è§„èŒƒåŒ–æ¨¡å‹ï¼Œå¹¶å°†å…¶é›†æˆåˆ°ç¼–ç å™¨-è§£ç å™¨ç½‘ç»œä¸­ã€‚è¿™ç§æ–°é¢–çš„æ–¹æ³•ä¸ºæœªçŸ¥åŸŸçš„CS ASRæä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œä»è€Œæå¤§åœ°å¢å¼ºäº†æˆ‘ä»¬å¯¹è¯¥é¢†åŸŸçš„è´¡çŒ®ã€‚é€šè¿‡åˆ©ç”¨BAMæ¥è¯†åˆ«å’Œè§„èŒƒåŒ–CSçŸ­è¯­ï¼ŒAdaCSåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€šè¿‡æä¾›çš„å•è¯åˆ—è¡¨å¢å¼ºäº†å…¶è‡ªé€‚åº”èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œå¹¶å…·å¤‡å¤„ç†å„ç§é¢†åŸŸä¸­çš„æœªè§CSçŸ­è¯­çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒAdaCSåœ¨è¶Šå—è¯­CS ASRè§„èŒƒåŒ–æ–¹é¢ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ä¸¤ä¸ªå»ºè®®çš„æµ‹è¯•é›†ä¸Šçš„WERåˆ†åˆ«é™ä½äº†56.2%å’Œ36.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07102v1">PDF</a> Accepted at ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­è®¨è®ºäº†è·¨è¯­æ–™çš„è¯­å†…æ··åˆè½¬æ¢ï¼ˆCSï¼‰é—®é¢˜å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¶Šå—è¯­ä½¿ç”¨è€…ä½¿ç”¨å¤–è¯­ä¸“æœ‰åè¯æˆ–ä¸“ä¸šæœ¯è¯­çš„æƒ…å†µä¸‹ã€‚ç°æœ‰çš„ASRç³»ç»Ÿç”±äºåªè®­ç»ƒåœ¨å•è¯­è¨€æ•°æ®ä¸Šï¼Œæ— æ³•å‡†ç¡®è½¬å½•è¿™ç§ä¸å¯é¢„æµ‹çš„è¯­å†…æ··åˆè½¬æ¢ã€‚å¯¹äºèµ„æºåŒ®ä¹çš„è¯­è¨€ï¼Œè¿™ä¸ªé—®é¢˜æ›´ä¸ºçªå‡ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé›†æˆè‡ªé€‚åº”åå·®æ³¨æ„æ¨¡å—ï¼ˆBAMï¼‰çš„æ ‡å‡†åŒ–æ¨¡å‹AdaCSæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨BAMæ¥è¯†åˆ«å’Œæ ‡å‡†åŒ–CSçŸ­è¯­ï¼ŒAdaCSåœ¨æœªè§è¿‡çš„é¢†åŸŸæä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜¾è‘—å¢å¼ºäº†å¤„ç†ä¸åŒé¢†åŸŸæœªè§CSçŸ­è¯­çš„èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒAdaCSåœ¨è¶Šå—è¯­CS ASRæ ‡å‡†åŒ–æ–¹é¢ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨æå‡ºçš„ä¸¤ä¸ªæµ‹è¯•é›†ä¸Šåˆ†åˆ«å®ç°äº†å¯è§‚çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½56.2%å’Œ36.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­å†…ä»£ç è½¬æ¢ï¼ˆCSï¼‰æŒ‡çš„æ˜¯åœ¨åŒä¸€å¥è¯å†…ä½¿ç”¨å¤šç§è¯­è¨€çš„æƒ…å†µï¼Œè¿™å¯¹è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿæ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>ASRç³»ç»Ÿåœ¨å¤„ç†è¶Šå—è¯­ä¸­å¤¹æ‚å¤–è¯­ä¸“æœ‰åè¯æˆ–ä¸“ä¸šæœ¯è¯­æ—¶å°¤å…¶éš¾ä»¥å‡†ç¡®è½¬å½•ã€‚</li>
<li>ç°æœ‰çš„ASRç³»ç»Ÿä¸»è¦åŸºäºå•è¯­è¨€æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— æ³•æœ‰æ•ˆå¤„ç†è¿™ç§ä¸å¯é¢„æµ‹çš„è¯­å†…ä»£ç è½¬æ¢ã€‚</li>
<li>å¯¹äºèµ„æºæœ‰é™çš„è¯­è¨€ï¼ˆå¦‚è¶Šå—è¯­ï¼‰ï¼Œå¼€å‘ç¨³å¥çš„ASRæ¨¡å‹æ›´åŠ å›°éš¾ã€‚</li>
<li>AdaCSæ˜¯ä¸€ä¸ªæ–°çš„æ ‡å‡†åŒ–æ¨¡å‹ï¼Œå®ƒé€šè¿‡é›†æˆè‡ªé€‚åº”åå·®æ³¨æ„æ¨¡å—ï¼ˆBAMï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>BAMæ¨¡å—èƒ½å¤Ÿè¯†åˆ«å’Œæ ‡å‡†åŒ–CSçŸ­è¯­ï¼Œä½¿å¾—AdaCSåœ¨æœªè§è¿‡çš„é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07102">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a359d59d019b73cfba0098fc06841d3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c176aa1586edf3e3bf70c5b62798102c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbbcd15b7c524fdd110883464a299290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19f4e74880121bec16b96d3fe643bac3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33ac2869b3682c1a46320e6eb1ecfa70.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improving-Cross-Lingual-Phonetic-Representation-of-Low-Resource-Languages-Through-Language-Similarity-Analysis"><a href="#Improving-Cross-Lingual-Phonetic-Representation-of-Low-Resource-Languages-Through-Language-Similarity-Analysis" class="headerlink" title="Improving Cross-Lingual Phonetic Representation of Low-Resource   Languages Through Language Similarity Analysis"></a>Improving Cross-Lingual Phonetic Representation of Low-Resource   Languages Through Language Similarity Analysis</h2><p><strong>Authors:Minu Kim, Kangwook Jang, Hoirin Kim</strong></p>
<p>This paper examines how linguistic similarity affects cross-lingual phonetic representation in speech processing for low-resource languages, emphasizing effective source language selection. Previous cross-lingual research has used various source languages to enhance performance for the target low-resource language without thorough consideration of selection. Our study stands out by providing an in-depth analysis of language selection, supported by a practical approach to assess phonetic proximity among multiple language families. We investigate how within-family similarity impacts performance in multilingual training, which aids in understanding language dynamics. We also evaluate the effect of using phonologically similar languages, regardless of family. For the phoneme recognition task, utilizing phonologically similar languages consistently achieves a relative improvement of 55.6% over monolingual training, even surpassing the performance of a large-scale self-supervised learning model. Multilingual training within the same language family demonstrates that higher phonological similarity enhances performance, while lower similarity results in degraded performance compared to monolingual training. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­è¨€ç›¸ä¼¼æ€§å¦‚ä½•å½±å“ä½èµ„æºè¯­è¨€çš„è·¨è¯­è¨€è¯­éŸ³è¡¨ç¤ºä¸­çš„è¯­éŸ³è¯†åˆ«å¤„ç†ã€‚æ–‡ç« å¼ºè°ƒäº†æœ‰æ•ˆçš„æºè¯­è¨€é€‰æ‹©ã€‚ä¹‹å‰çš„è·¨è¯­è¨€ç ”ç©¶ä½¿ç”¨äº†å„ç§æºè¯­è¨€æ¥æé«˜ç›®æ ‡ä½èµ„æºè¯­è¨€çš„æ€§èƒ½ï¼Œä½†æ²¡æœ‰æ·±å…¥è€ƒè™‘è¯­è¨€çš„é€‰æ‹©é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶é€šè¿‡æä¾›å¯¹è¯­è¨€é€‰æ‹©çš„æ·±å…¥åˆ†æè€Œè„±é¢–è€Œå‡ºï¼Œè¾…ä»¥è¯„ä¼°å¤šä¸ªè¯­è¨€å®¶æ—ä¹‹é—´è¯­éŸ³ç›¸ä¼¼åº¦çš„å®ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬ç ”ç©¶äº†å®¶æ—å†…ç›¸ä¼¼æ€§å¯¹å¤šè¯­è¨€è®­ç»ƒæ€§èƒ½çš„å½±å“ï¼Œè¿™æœ‰åŠ©äºç†è§£è¯­è¨€åŠ¨æ€ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†ä½¿ç”¨è¯­éŸ³ç›¸ä¼¼åº¦è¾ƒé«˜çš„è¯­è¨€çš„å½±å“ï¼Œä¸è®ºå…¶å±äºå“ªä¸ªå®¶æ—ã€‚å¯¹äºéŸ³ä½è¯†åˆ«ä»»åŠ¡ï¼Œä½¿ç”¨è¯­éŸ³ç›¸ä¼¼åº¦é«˜çš„è¯­è¨€è¿›è¡Œè®­ç»ƒï¼Œå…¶æ€§èƒ½ç›¸è¾ƒäºå•è¯­è®­ç»ƒæé«˜äº†55.6%ï¼Œç”šè‡³è¶…è¿‡äº†å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨åŒä¸€è¯­è¨€å®¶æ—ä¸­è¿›è¡Œçš„å¤šè¯­è¨€è®­ç»ƒè¡¨æ˜ï¼Œæ›´é«˜çš„è¯­éŸ³ç›¸ä¼¼æ€§å¯ä»¥æé«˜æ€§èƒ½ï¼Œè€Œè¾ƒä½çš„ç›¸ä¼¼æ€§åˆ™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œç›¸è¾ƒäºå•è¯­è®­ç»ƒè¡¨ç°è¾ƒå·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06810v1">PDF</a> 10 pages, 5 figures, accepted to ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è¯­è¨€ç›¸ä¼¼æ€§å¯¹ä½èµ„æºè¯­è¨€è·¨è¯­è¨€è¯­éŸ³ç‰¹å¾è¡¨ç¤ºçš„å½±å“ï¼Œå¹¶ç€é‡äºæœ‰æ•ˆçš„æºè¯­è¨€é€‰æ‹©ã€‚ä»¥å¾€çš„è·¨è¯­è¨€ç ”ç©¶ä½¿ç”¨å„ç§æºè¯­è¨€æ¥æé«˜ç›®æ ‡ä½èµ„æºè¯­è¨€çš„æ€§èƒ½ï¼Œä½†æ²¡æœ‰å……åˆ†è€ƒè™‘è¯­è¨€é€‰æ‹©é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡æ·±å…¥åˆ†æè¯­è¨€é€‰æ‹©ï¼Œæ”¯æŒè¯„ä¼°å¤šç§è¯­è¨€å®¶æ—ä¹‹é—´è¯­éŸ³ç›¸ä¼¼æ€§çš„å®ç”¨æ–¹æ³•ï¼Œçªå‡ºäº†ç‹¬ç‰¹æ€§ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†å®¶æ—å†…ç›¸ä¼¼æ€§å¯¹å¤šè¯­è¨€è®­ç»ƒæ€§èƒ½çš„å½±å“ï¼Œè¿™æœ‰åŠ©äºç†è§£è¯­è¨€åŠ¨æ€ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†ä½¿ç”¨è¯­éŸ³ç›¸ä¼¼çš„è¯­è¨€ï¼ˆæ— è®ºå®¶æ—å¦‚ä½•ï¼‰çš„æ•ˆæœã€‚åœ¨è¯­éŸ³ç‰¹å¾è¯†åˆ«ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨è¯­éŸ³ç›¸ä¼¼çš„è¯­è¨€ç›¸è¾ƒäºå•è¯­è®­ç»ƒå¯ä»¥å®ç°ç›¸å¯¹æé«˜55.6%çš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¿‡å¤§è§„æ¨¡è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨åŒä¸€è¯­è¨€å®¶æ—å†…è¿›è¡Œçš„å¤šè¯­è¨€è®­ç»ƒè¡¨æ˜ï¼Œè¾ƒé«˜çš„è¯­éŸ³ç›¸ä¼¼æ€§å¯ä»¥æé«˜æ€§èƒ½ï¼Œè€Œè¾ƒä½çš„ç›¸ä¼¼æ€§åˆ™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸å•è¯­è®­ç»ƒç›¸æ¯”ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è·¨è¯­è¨€ç ”ç©¶ä¸­ï¼Œæºè¯­è¨€çš„é€‰æ‹©å¯¹äºæé«˜ä½èµ„æºè¯­è¨€çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>æ·±å…¥ç ”ç©¶è¯­è¨€é€‰æ‹©é—®é¢˜æœ‰å…¶ç‹¬ç‰¹æ€§ï¼Œæœ‰åŠ©äºç†è§£ä¸åŒè¯­è¨€é—´çš„åŠ¨æ€å…³ç³»ã€‚</li>
<li>åœ¨å¤šè¯­è¨€è®­ç»ƒä¸­ï¼ŒåŒä¸€è¯­è¨€å®¶æ—å†…çš„è¯­éŸ³ç›¸ä¼¼æ€§å¯¹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>ä½¿ç”¨è¯­éŸ³ç›¸ä¼¼çš„è¯­è¨€è¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜è¯­éŸ³ç‰¹å¾è¯†åˆ«çš„æ€§èƒ½ã€‚</li>
<li>ä¸å•è¯­è®­ç»ƒç›¸æ¯”ï¼Œä½¿ç”¨è¯­éŸ³ç›¸ä¼¼çš„è¯­è¨€å¯ä»¥æé«˜æ€§èƒ½é«˜è¾¾55.6%ã€‚</li>
<li>åœ¨è¯„ä¼°ä¸åŒè¯­è¨€çš„è¯­éŸ³ç›¸ä¼¼æ€§æ—¶ï¼Œå®ç”¨è¯„ä¼°æ–¹æ³•å…·æœ‰é‡è¦çš„ç ”ç©¶ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4675346de93fbe3774707fd89a2d9fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fdb81e10ebb1f9b769689f2c98b24d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43a8e75d2928a5e0bd6de3109086233e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92ffa18f3c4317183eb5b36707358a9c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Integrating-Pause-Information-with-Word-Embeddings-in-Language-Models-for-Alzheimerâ€™s-Disease-Detection-from-Spontaneous-Speech"><a href="#Integrating-Pause-Information-with-Word-Embeddings-in-Language-Models-for-Alzheimerâ€™s-Disease-Detection-from-Spontaneous-Speech" class="headerlink" title="Integrating Pause Information with Word Embeddings in Language Models   for Alzheimerâ€™s Disease Detection from Spontaneous Speech"></a>Integrating Pause Information with Word Embeddings in Language Models   for Alzheimerâ€™s Disease Detection from Spontaneous Speech</h2><p><strong>Authors:Yu Pu, Wei-Qiang Zhang</strong></p>
<p>Alzheimerâ€™s disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline and memory loss. Early detection of AD is crucial for effective intervention and treatment. In this paper, we propose a novel approach to AD detection from spontaneous speech, which incorporates pause information into language models. Our method involves encoding pause information into embeddings and integrating them into the typical transformer-based language model, enabling it to capture both semantic and temporal features of speech data. We conduct experiments on the Alzheimerâ€™s Dementia Recognition through Spontaneous Speech (ADReSS) dataset and its extension, the ADReSSo dataset, comparing our method with existing approaches. Our method achieves an accuracy of 83.1% in the ADReSSo test set. The results demonstrate the effectiveness of our approach in discriminating between AD patients and healthy individuals, highlighting the potential of pauses as a valuable indicator for AD detection. By leveraging speech analysis as a non-invasive and cost-effective tool for AD detection, our research contributes to early diagnosis and improved management of this debilitating disease. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œä»¥è®¤çŸ¥è¡°é€€å’Œè®°å¿†ä¸§å¤±ä¸ºç‰¹å¾ã€‚æ—©æœŸå‘ç°ADå¯¹äºæœ‰æ•ˆå¹²é¢„å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è‡ªå‘è¯­è¨€è¿›è¡ŒADæ£€æµ‹çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†åœé¡¿ä¿¡æ¯èå…¥è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å°†åœé¡¿ä¿¡æ¯ç¼–ç ä¸ºåµŒå…¥å½¢å¼ï¼Œå¹¶å°†å…¶æ•´åˆåˆ°å…¸å‹çš„åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰è¯­éŸ³æ•°æ®çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨é˜¿å°”èŒ¨æµ·é»˜æ°ç—‡ç—´å‘†ç—‡é€šè¿‡è‡ªå‘è¯­è¨€è¯†åˆ«ï¼ˆADReSSï¼‰æ•°æ®é›†åŠå…¶æ‰©å±•æ•°æ®é›†ADReSSoä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ADReSSoæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†83.1%çš„å‡†ç¡®ç‡ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒºåˆ†ADæ‚£è€…å’Œå¥åº·äººç¾¤æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¼ºè°ƒåœé¡¿ä½œä¸ºADæ£€æµ‹çš„é‡è¦æŒ‡æ ‡çš„æ½œåŠ›ã€‚é€šè¿‡åˆ©ç”¨è¯­éŸ³åˆ†æä½œä¸ºéä¾µå…¥æ€§å’Œæˆæœ¬æ•ˆç›Šé«˜çš„ADæ£€æµ‹å·¥å…·ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºè¿™ç§ç–¾ç—…çš„æ—©æœŸè¯Šæ–­å’Œæ²»ç–—ç®¡ç†åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06727v1">PDF</a> accepted by ICASSP2025. Copyright 2025 IEEE. Personal use of this   material is permitted. Permission from IEEE must be obtained for all other   uses, in any current or future media, including reprinting&#x2F;republishing this   material for advertising or promotional purposes, creating new collective   works, for resale or redistribution to servers or lists, or reuse of any   copyrighted component</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆåœé¡¿ä¿¡æ¯æ£€æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†åœé¡¿ä¿¡æ¯ç¼–ç ä¸ºåµŒå…¥ï¼Œå¹¶é›†æˆåˆ°åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰è¯­éŸ³æ•°æ®çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—‡ç—´å‘†é€šè¿‡è‡ªå‘æ€§è¨€è¯­è¯†åˆ«ï¼ˆADReSSï¼‰æ•°æ®é›†åŠå…¶æ‰©å±•æ•°æ®é›†ADReSSoä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ADæ£€æµ‹æ–¹é¢å–å¾—äº†83.1%çš„å‡†ç¡®ç‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœé¡¿å¯èƒ½æ˜¯æ£€æµ‹ADçš„é‡è¦æŒ‡æ ‡ä¹‹ä¸€ï¼Œè€Œåˆ©ç”¨è¯­éŸ³åˆ†æä½œä¸ºéä¾µå…¥æ€§å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·è¿›è¡ŒADæ£€æµ‹æœ‰åŠ©äºæ—©æœŸè¯Šæ–­å’Œæ²»ç–—ç®¡ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰æ˜¯ä¸€ç§è¿›è¡Œæ€§ç¥ç»é€€è¡Œæ€§ç–¾ç—…ï¼Œæ—©æœŸæ£€æµ‹å¯¹æœ‰æ•ˆå¹²é¢„å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆåœé¡¿ä¿¡æ¯çš„ADæ£€æµ‹æ–¹æ³•ï¼Œå°†å…¶é›†æˆåˆ°åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæ•æ‰è¯­éŸ³æ•°æ®çš„è¯­ä¹‰å’Œæ—¶é—´ç‰¹å¾ã€‚</li>
<li>åœ¨ADReSSå’ŒADReSSoæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹ADæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå‡†ç¡®ç‡ä¸º83.1%ã€‚</li>
<li>åœé¡¿ä¿¡æ¯ä½œä¸ºæ£€æµ‹ADçš„é‡è¦æŒ‡æ ‡ä¹‹ä¸€ã€‚</li>
<li>è¯­éŸ³åˆ†æä½œä¸ºéä¾µå…¥æ€§å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å·¥å…·ï¼Œæœ‰åŠ©äºADçš„æ—©æœŸè¯Šæ–­å’Œæ²»ç–—ç®¡ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-308725e1b9b06ad1106e766c290cc24f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb2c2249c93cb1b3fd3f2d5c839691b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3098c0571ec870e406842c8411e60103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-347961b619d3ce89ce0346b8f5bf1a8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-689538c34924c6431e8c3547658282c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bae574aeae43fac13816f0d2df39cc53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717d8e5ae5a65d10cab61c16e1fed327.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Discrete-Speech-Unit-Extraction-via-Independent-Component-Analysis"><a href="#Discrete-Speech-Unit-Extraction-via-Independent-Component-Analysis" class="headerlink" title="Discrete Speech Unit Extraction via Independent Component Analysis"></a>Discrete Speech Unit Extraction via Independent Component Analysis</h2><p><strong>Authors:Tomohiko Nakamura, Kwanghee Choi, Keigo Hojo, Yoshiaki Bando, Satoru Fukayama, Shinji Watanabe</strong></p>
<p>Self-supervised speech models (S3Ms) have become a common tool for the speech processing community, leveraging representations for downstream tasks. Clustering S3M representations yields discrete speech units (DSUs), which serve as compact representations for speech signals. DSUs are typically obtained by k-means clustering. Using DSUs often leads to strong performance in various tasks, including automatic speech recognition (ASR). However, even with the high dimensionality and redundancy of S3M representations, preprocessing S3M representations for better clustering remains unexplored, even though it can affect the quality of DSUs. In this paper, we investigate the potential of linear preprocessing methods for extracting DSUs. We evaluate standardization, principal component analysis, whitening, and independent component analysis (ICA) on DSU-based ASR benchmarks and demonstrate their effectiveness as preprocessing for k-means. We also conduct extensive analyses of their behavior, such as orthogonality or interpretability of individual components of ICA. </p>
<blockquote>
<p>è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹ï¼ˆS3Mï¼‰å·²æˆä¸ºè¯­éŸ³å¤„ç†é¢†åŸŸçš„å¸¸ç”¨å·¥å…·ï¼Œç”¨äºä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›è¡¨å¾ã€‚é€šè¿‡å¯¹S3Mè¡¨å¾è¿›è¡Œèšç±»å¯ä»¥å¾—åˆ°ç¦»æ•£è¯­éŸ³å•å…ƒï¼ˆDSUï¼‰ï¼Œå®ƒä»¬å¯ä»¥ä½œä¸ºè¯­éŸ³ä¿¡å·çš„ç´§å‡‘è¡¨å¾ã€‚DSUé€šå¸¸æ˜¯é€šè¿‡K-meansèšç±»è·å¾—çš„ã€‚ä½¿ç”¨DSUå¾€å¾€å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸­å–å¾—å‡ºè‰²çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ç„¶è€Œï¼Œå°½ç®¡S3Mè¡¨å¾å…·æœ‰é«˜åº¦çš„ç»´åº¦å’Œå†—ä½™æ€§ï¼Œä½†å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ä»¥æ›´å¥½åœ°èšç±»ä»å°šæœªå¾—åˆ°æ¢ç´¢ï¼Œè€Œè¿™å¯èƒ½ä¼šå½±å“DSUçš„è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†çº¿æ€§é¢„å¤„ç†æå–DSUçš„æ½œåŠ›ã€‚æˆ‘ä»¬å¯¹æ ‡å‡†åŒ–ã€ä¸»æˆåˆ†åˆ†æã€ç™½åŒ–ä»¥åŠç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰åœ¨åŸºäºDSUçš„ASRåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å®ƒä»¬ä½œä¸ºK-meansé¢„å¤„ç†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è¿˜å¯¹å…¶è¡Œä¸ºè¿›è¡Œäº†å¹¿æ³›çš„åˆ†æï¼Œä¾‹å¦‚ICAå„ä¸ªç»„ä»¶çš„æ­£äº¤æ€§æˆ–å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06562v1">PDF</a> Accepted to ICASSP 2025 SALMA Workshop. Code available at   <a target="_blank" rel="noopener" href="https://github.com/TomohikoNakamura/ica_dsu_espnet">https://github.com/TomohikoNakamura/ica_dsu_espnet</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªæˆ‘ç›‘ç£è¯­éŸ³æ¨¡å‹ï¼ˆS3Mï¼‰è¡¨ç¤ºä¸­çš„ç¦»æ•£è¯­éŸ³å•å…ƒï¼ˆDSUï¼‰çš„ç”Ÿæˆè¿‡ç¨‹ã€‚é€šè¿‡å¯¹S3Mè¡¨ç¤ºè¿›è¡Œèšç±»å¾—åˆ°DSUï¼Œå…¶åœ¨è¯­éŸ³è¯†åˆ«ç­‰ä»»åŠ¡ä¸­æœ‰è‰¯å¥½è¡¨ç°ã€‚ä½†å½“å‰å¯¹S3Mè¡¨ç¤ºè¿›è¡Œé¢„å¤„ç†ä»¥æ”¹è¿›èšç±»è´¨é‡çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚æœ¬æ–‡å°è¯•ä½¿ç”¨çº¿æ€§é¢„å¤„ç†æ–¹æ³•æ¥æå–DSUï¼Œå¹¶å¯¹æ ‡å‡†åŒ–ã€ä¸»æˆåˆ†åˆ†æã€ç™½åŒ–ä»¥åŠç‹¬ç«‹æˆåˆ†åˆ†æç­‰æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å®ƒä»¬åœ¨k-meansé¢„å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å¯¹è¿™äº›æ–¹æ³•çš„è¡Œä¸ºè¿›è¡Œäº†æ·±å…¥çš„åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªæˆ‘ç›‘ç£è¯­éŸ³æ¨¡å‹ï¼ˆS3Mï¼‰å·²æˆä¸ºè¯­éŸ³å¤„ç†ç¤¾åŒºä¸­å¸¸è§çš„å·¥å…·ï¼Œç”¨äºä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡èšç±»S3Mè¡¨ç¤ºå¾—åˆ°ç¦»æ•£è¯­éŸ³å•å…ƒï¼ˆDSUï¼‰ï¼Œä½œä¸ºè¯­éŸ³ä¿¡å·çš„ç´§å‡‘è¡¨ç¤ºã€‚</li>
<li>DSUåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚</li>
<li>å°½ç®¡S3Mè¡¨ç¤ºå…·æœ‰é«˜ç»´åº¦å’Œå†—ä½™æ€§ï¼Œä½†å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ä»¥æ”¹è¿›èšç±»çš„ç ”ç©¶å°šæœªå¾—åˆ°æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡ç ”ç©¶äº†çº¿æ€§é¢„å¤„ç†æ–¹æ³•çš„æ½œåŠ›ï¼Œç”¨äºæå–DSUï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–ã€ä¸»æˆåˆ†åˆ†æã€ç™½åŒ–å’Œç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰ã€‚</li>
<li>è¯„ä¼°è¿™äº›æ–¹æ³•åœ¨DSUåŸºç¡€ä¸Šçš„ASRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºé¢„å¤„ç†çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bdab83a4738cf42e9a20b6693acfc10b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e500307f05c3fe9cb53ad2c9f657c42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30650289c2b9f7120d5d1ee6d48fb1e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f14aa89fe35645c3193935ea1cdaf77.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-modal-Speech-Enhancement-with-Limited-Electromyography-Channels"><a href="#Multi-modal-Speech-Enhancement-with-Limited-Electromyography-Channels" class="headerlink" title="Multi-modal Speech Enhancement with Limited Electromyography Channels"></a>Multi-modal Speech Enhancement with Limited Electromyography Channels</h2><p><strong>Authors:Fuyuan Feng, Longting Xu, Rohan Kumar Das</strong></p>
<p>Speech enhancement (SE) aims to improve the clarity, intelligibility, and quality of speech signals for various speech enabled applications. However, air-conducted (AC) speech is highly susceptible to ambient noise, particularly in low signal-to-noise ratio (SNR) and non-stationary noise environments. Incorporating multi-modal information has shown promise in enhancing speech in such challenging scenarios. Electromyography (EMG) signals, which capture muscle activity during speech production, offer noise-resistant properties beneficial for SE in adverse conditions. Most previous EMG-based SE methods required 35 EMG channels, limiting their practicality. To address this, we propose a novel method that considers only 8-channel EMG signals with acoustic signals using a modified SEMamba network with added cross-modality modules. Our experiments demonstrate substantial improvements in speech quality and intelligibility over traditional approaches, especially in extremely low SNR settings. Notably, compared to the SE (AC) approach, our method achieves a significant PESQ gain of 0.235 under matched low SNR conditions and 0.527 under mismatched conditions, highlighting its robustness. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ—¨åœ¨æé«˜å„ç§è¯­éŸ³åº”ç”¨ç¨‹åºçš„è¯­éŸ³ä¿¡å·çš„æ¸…æ™°åº¦ã€å¯æ‡‚åº¦å’Œè´¨é‡ã€‚ç„¶è€Œï¼Œæ°”å¯¼è¯­éŸ³éå¸¸å®¹æ˜“å—åˆ°ç¯å¢ƒå™ªå£°çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰å’Œéå¹³ç¨³å™ªå£°ç¯å¢ƒä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé‡‡ç”¨å¤šæ¨¡æ€ä¿¡æ¯å¢å¼ºè¯­éŸ³è¡¨ç°å‡ºäº†è‰¯å¥½çš„å‰æ™¯ã€‚è‚Œç”µå›¾ï¼ˆEMGï¼‰ä¿¡å·èƒ½å¤Ÿæ•æ‰åˆ°è¯­éŸ³äº§ç”Ÿè¿‡ç¨‹ä¸­çš„è‚Œè‚‰æ´»åŠ¨ï¼Œå¯¹äºæ¶åŠ£æ¡ä»¶ä¸‹çš„è¯­éŸ³å¢å¼ºå…·æœ‰æŠ—å™ªå£°ç‰¹æ€§ã€‚ä»¥å‰çš„å¤§å¤šæ•°åŸºäºEMGçš„SEæ–¹æ³•éœ€è¦35ä¸ªEMGé€šé“ï¼Œè¿™é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»…è€ƒè™‘ä½¿ç”¨ç»è¿‡ä¿®æ”¹çš„SEMambaç½‘ç»œå¹¶åˆ©ç”¨å£°å­¦ä¿¡å·çš„8é€šé“EMGä¿¡å·çš„æ–°å‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¢åŠ äº†è·¨æ¨¡æ€æ¨¡å—ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„è¯­éŸ³å¢å¼ºæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯­éŸ³è´¨é‡å’Œå¯æ‡‚åº¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æä½SNRæ¡ä»¶ä¸‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ä¼ ç»Ÿçš„SEï¼ˆACï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŒ¹é…çš„ä½SNRæ¡ä»¶ä¸‹å®ç°äº†æ˜¾è‘—çš„PESQå¢ç›Šä¸º0.235ï¼Œåœ¨å¤±é…æ¡ä»¶ä¸‹ä¸º0.527ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06530v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨è¯­éŸ³å¢å¼ºæŠ€æœ¯åœ¨ä½ä¿¡å™ªæ¯”å’Œéç¨³æ€å™ªå£°ç¯å¢ƒä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§ä»…ä½¿ç”¨8é€šé“è‚Œç”µå›¾ä¿¡å·å’Œå£°éŸ³ä¿¡å·çš„æ”¹è¿›SEMambaç½‘ç»œæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è¯­éŸ³è´¨é‡å’Œæ¸…æ™°åº¦ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æä½ä¿¡å™ªæ¯”ç¯å¢ƒä¸‹ï¼Œè¯¥æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºæ—¨åœ¨æé«˜è¯­éŸ³ä¿¡å·çš„æ¸…æ™°åº¦ã€å¯æ‡‚åº¦å’Œè´¨é‡ã€‚</li>
<li>ç©ºæ°”ä¼ å¯¼è¯­éŸ³åœ¨æ¶åŠ£ç¯å¢ƒä¸‹å®¹æ˜“å—åˆ°èƒŒæ™¯å™ªå£°å¹²æ‰°ã€‚</li>
<li>å¤šæ¨¡æ€ä¿¡æ¯èåˆåœ¨æŒ‘æˆ˜åœºæ™¯ä¸‹å¢å¼ºè¯­éŸ³è¡¨ç°è‰¯å¥½ã€‚</li>
<li>è‚Œç”µå›¾ä¿¡å·æ•æ‰è¯´è¯æ—¶è‚Œè‚‰æ´»åŠ¨ï¼Œæœ‰åŠ©äºå¢å¼ºæ¶åŠ£æ¡ä»¶ä¸‹çš„è¯­éŸ³è´¨é‡ã€‚</li>
<li>å¤§å¤šæ•°å…ˆå‰çš„åŸºäºè‚Œç”µå›¾çš„è¯­éŸ³å¢å¼ºæ–¹æ³•éœ€è¦35ä¸ªè‚Œç”µå›¾é€šé“ï¼Œé™åˆ¶äº†å®ç”¨æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°æ–¹æ³•ä»…ä½¿ç”¨8é€šé“è‚Œç”µå›¾ä¿¡å·å’Œå£°éŸ³ä¿¡å·ï¼Œé€šè¿‡æ”¹è¿›SEMambaç½‘ç»œå¹¶ç»“åˆè·¨æ¨¡æ€æ¨¡å—å®ç°è¯­éŸ³å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0762bcdf8201c1ff846b79e2a6f1b0d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec7eeedf972ad639fc95a71531b9e97c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3ae569b83e7c4eb6813f48a604a75d5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c05b288d82c13689ae46f508614363b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Speech-Recognition-for-Automatically-Assessing-Afrikaans-and-isiXhosa-Preschool-Oral-Narratives"><a href="#Speech-Recognition-for-Automatically-Assessing-Afrikaans-and-isiXhosa-Preschool-Oral-Narratives" class="headerlink" title="Speech Recognition for Automatically Assessing Afrikaans and isiXhosa   Preschool Oral Narratives"></a>Speech Recognition for Automatically Assessing Afrikaans and isiXhosa   Preschool Oral Narratives</h2><p><strong>Authors:Christiaan Jacobs, Annelien Smith, Daleen Klop, OndÅ™ej Klejch, Febe de Wet, Herman Kamper</strong></p>
<p>We develop automatic speech recognition (ASR) systems for stories told by Afrikaans and isiXhosa preschool children. Oral narratives provide a way to assess childrenâ€™s language development before they learn to read. We consider a range of prior child-speech ASR strategies to determine which is best suited to this unique setting. Using Whisper and only 5 minutes of transcribed in-domain child speech, we find that additional in-domain adult data (adult speech matching the story domain) provides the biggest improvement, especially when coupled with voice conversion. Semi-supervised learning also helps for both languages, while parameter-efficient fine-tuning helps on Afrikaans but not on isiXhosa (which is under-represented in the Whisper model). Few child-speech studies look at non-English data, and even fewer at the preschool ages of 4 and 5. Our work therefore represents a unique validation of a wide range of previous child-speech ASR strategies in an under-explored setting. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºè®²å—éè¯­å’Œç§‘è¨è¯­çš„å­¦é¾„å‰å„¿ç«¥çš„æ•…äº‹å¼€å‘è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚å£å¤´å™äº‹ä¸ºè¯„ä¼°å„¿ç«¥çš„è¯­è¨€å‘å±•æä¾›äº†ä¸€ç§æ–¹å¼ï¼Œåœ¨ä»–ä»¬å­¦ä¼šé˜…è¯»ä¹‹å‰ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸€ç³»åˆ—å…ˆå‰çš„å„¿ç«¥è¯­éŸ³ASRç­–ç•¥ï¼Œä»¥ç¡®å®šå“ªç§ç­–ç•¥æœ€é€‚åˆè¿™ç§ç‹¬ç‰¹çš„åœºæ™¯ã€‚ä½¿ç”¨Whisperè¯­éŸ³æŠ€æœ¯ä»…è½¬å½•äº”åˆ†é’Ÿç‰¹å®šé¢†åŸŸçš„å„¿ç«¥è¯­éŸ³ï¼Œæˆ‘ä»¬å‘ç°é¢å¤–çš„ç‰¹å®šé¢†åŸŸæˆäººæ•°æ®ï¼ˆä¸æ•…äº‹é¢†åŸŸåŒ¹é…çš„æˆäººè¯­éŸ³ï¼‰æä¾›äº†æœ€å¤§çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯ä¸è¯­éŸ³è½¬æ¢ç›¸ç»“åˆæ—¶ã€‚åŠç›‘ç£å­¦ä¹ ä¹Ÿæœ‰åŠ©äºè¿™ä¸¤ç§è¯­è¨€ï¼Œè€Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒåœ¨å—éè¯­ä¸Šæœ‰æ‰€å¸®åŠ©ï¼Œä½†åœ¨ç§‘è¨è¯­ä¸Šæ²¡æœ‰å¸®åŠ©ï¼ˆç§‘è¨è¯­åœ¨Whisperæ¨¡å‹ä¸­ä»£è¡¨æ€§ä¸è¶³ï¼‰ã€‚å¾ˆå°‘æœ‰å„¿ç«¥è¯­éŸ³ç ”ç©¶å…³æ³¨éè‹±è¯­æ•°æ®ï¼Œç”šè‡³åœ¨å­¦é¾„å‰å¹´é¾„å…³æ³¨å¾—æ›´å°‘ï¼ˆåˆ†åˆ«ä¸ºå››åˆ°äº”å²ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œä»£è¡¨äº†åœ¨ä¸€ä¸ªæœªè¢«å……åˆ†ç ”ç©¶çš„åœºæ™¯ä¸­ï¼Œå¯¹å„ç§å…ˆå‰å„¿ç«¥è¯­éŸ³ASRç­–ç•¥çš„å¹¿æ³›éªŒè¯çš„ç‹¬ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06478v1">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹å—éè¯­å’Œç§‘è¨è¯­å­¦é¾„å‰å„¿ç«¥è®²è¿°æ•…äº‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚æ–‡ç« æ¢è®¨äº†å„¿ç«¥å£è¯­å™äº‹åœ¨è¯„ä¼°å„¿ç«¥è¯­è¨€å‘å±•ä¸­çš„ä½œç”¨ï¼Œå¹¶åœ¨æ­¤ç‰¹æ®Šç¯å¢ƒä¸‹è¯„ä¼°äº†å¤šç§å„¿ç«¥è¯­éŸ³ASRç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨WhisperæŠ€æœ¯å¹¶ç»“åˆä»…äº”åˆ†é’Ÿé¢†åŸŸå†…çš„å„¿ç«¥è¯­éŸ³è½¬å½•æ•°æ®ï¼Œå¼•å…¥åŒ¹é…æ•…äº‹é¢†åŸŸçš„æˆäººè¯­éŸ³æ•°æ®èƒ½å¤Ÿæä¾›æœ€å¤§æ”¹å–„ï¼Œå°¤å…¶åœ¨ç»“åˆè¯­éŸ³è½¬æ¢æŠ€æœ¯æ—¶æ›´æ˜¯å¦‚æ­¤ã€‚åŠç›‘ç£å­¦ä¹ å¯¹ä¸¤ç§è¯­è¨€éƒ½æœ‰å¸®åŠ©ï¼Œè€Œå‚æ•°é«˜æ•ˆçš„å¾®è°ƒåœ¨å—éè¯­ä¸Šæœ‰æ‰€å¸®åŠ©ï¼Œä½†åœ¨ç§‘è¨è¯­ä¸Šæ²¡æœ‰æ˜æ˜¾æ•ˆæœï¼ˆç§‘è¨è¯­åœ¨Whisperæ¨¡å‹ä¸­è¡¨ç°ä¸è¶³ï¼‰ã€‚æœ¬ç ”ç©¶æ¶‰åŠéè‹±è¯­æ•°æ®å’Œ4-5å²å­¦é¾„å‰å„¿ç«¥çš„ç ”ç©¶è¾ƒå°‘ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬çš„å·¥ä½œä»£è¡¨äº†åœ¨æ­¤æœªå……åˆ†æ¢ç´¢çš„ç¯å¢ƒä¸­ï¼Œå¯¹å„ç§å…ˆå‰å„¿ç«¥è¯­éŸ³ASRç­–ç•¥çš„ç‹¬ç‰¹éªŒè¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹å—éè¯­å’Œç§‘è¨è¯­å­¦é¾„å‰å„¿ç«¥è®²è¿°æ•…äº‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿã€‚</li>
<li>å„¿ç«¥å£è¯­å™äº‹æ˜¯è¯„ä¼°å„¿ç«¥è¯­è¨€å‘å±•çš„é‡è¦æ–¹å¼ã€‚</li>
<li>ä½¿ç”¨WhisperæŠ€æœ¯å¹¶ç»“åˆé¢†åŸŸå†…çš„å„¿ç«¥è¯­éŸ³æ•°æ®ï¼Œå‘ç°å¼•å…¥åŒ¹é…æ•…äº‹é¢†åŸŸçš„æˆäººè¯­éŸ³æ•°æ®èƒ½å¤Ÿæ˜¾è‘—æé«˜ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>è¯­éŸ³è½¬æ¢æŠ€æœ¯ä¸æˆäººè¯­éŸ³æ•°æ®ç»“åˆèƒ½å¸¦æ¥æ›´å¥½çš„æ•ˆæœã€‚</li>
<li>åŠç›‘ç£å­¦ä¹ å¯¹ä¸¤ç§è¯­è¨€éƒ½æœ‰å¸®åŠ©ã€‚</li>
<li>å‚æ•°é«˜æ•ˆçš„å¾®è°ƒåœ¨å—éè¯­ä¸Šæœ‰æ•ˆï¼Œä½†åœ¨ç§‘è¨è¯­ä¸Šæ•ˆæœä¸æ˜æ˜¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06478v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06478v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06478v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06478v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="On-Creating-A-Brain-To-Text-Decoder"><a href="#On-Creating-A-Brain-To-Text-Decoder" class="headerlink" title="On Creating A Brain-To-Text Decoder"></a>On Creating A Brain-To-Text Decoder</h2><p><strong>Authors:Zenon Lamprou, Yashar Moshfeghi</strong></p>
<p>Brain decoding has emerged as a rapidly advancing and extensively utilized technique within neuroscience. This paper centers on the application of raw electroencephalogram (EEG) signals for decoding human brain activity, offering a more expedited and efficient methodology for enhancing our understanding of the human brain. The investigation specifically scrutinizes the efficacy of brain-computer interfaces (BCI) in deciphering neural signals associated with speech production, with particular emphasis on the impact of vocabulary size, electrode density, and training data on the frameworkâ€™s performance. The study reveals the competitive word error rates (WERs) achievable on the Librispeech benchmark through pre-training on unlabelled data for speech processing. Furthermore, the study evaluates the efficacy of voice recognition under configurations with limited labeled data, surpassing previous state-of-the-art techniques while utilizing significantly fewer labels. Additionally, the research provides a comprehensive analysis of error patterns in voice recognition and the influence of model size and unlabelled training data. It underscores the significance of factors such as vocabulary size and electrode density in enhancing BCI performance, advocating for an increase in microelectrodes and refinement of language models. </p>
<blockquote>
<p>è„‘è§£ç å·²ç»ä½œä¸ºç¥ç»ç§‘å­¦ä¸­ä¸€ä¸ªå¿«é€Ÿè¿›æ­¥å’Œå¹¿æ³›åº”ç”¨çš„ç¥ç»è§£ç æŠ€æœ¯ã€‚æœ¬æ–‡ä¸»è¦ç ”ç©¶åŸå§‹è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·åœ¨è§£ç äººè„‘æ´»åŠ¨ä¸­çš„åº”ç”¨ï¼Œä¸ºæˆ‘ä»¬æ›´å¿«é€Ÿåœ°ç†è§£å’Œæå‡äººè„‘åŠŸèƒ½æä¾›äº†æ›´æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬ç ”ç©¶çš„è°ƒæŸ¥é‡ç‚¹åœ¨äºé’ˆå¯¹ä¸è¯­è¨€ç”Ÿæˆç›¸å…³çš„ç¥ç»ä¿¡å·ï¼Œç‰¹åˆ«ç€é‡ç ”ç©¶è„‘æœºæ¥å£ï¼ˆBCIï¼‰çš„æ•ˆç”¨ï¼Œä»¥åŠè¯æ±‡å¤§å°ã€ç”µæå¯†åº¦å’Œè®­ç»ƒæ•°æ®å¯¹æ¡†æ¶æ€§èƒ½çš„å½±å“ã€‚è¯¥ç ”ç©¶é€šè¿‡åœ¨æœªæ ‡è®°æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ­ç¤ºäº†è¾¾åˆ°LibrispeechåŸºå‡†ç«äº‰çš„å•è¯é”™è¯¯ç‡ï¼ˆWERsï¼‰ï¼Œè¿™å¯¹è¯­éŸ³å¤„ç†è€Œè¨€å…·æœ‰å®ç°æ„ä¹‰ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¯„ä¼°äº†åœ¨æœ‰é™æ ‡è®°æ•°æ®é…ç½®ä¸‹è¯­éŸ³è¯†åˆ«çš„æœ‰æ•ˆæ€§ï¼Œåœ¨åˆ©ç”¨æ˜¾è‘—æ›´å°‘çš„æ ‡ç­¾çš„åŒæ—¶è¶…è¶Šäº†ä»¥å‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯¹è¯­éŸ³è¯†åˆ«ä¸­çš„é”™è¯¯æ¨¡å¼ä»¥åŠæ¨¡å‹å¤§å°å’Œæœªæ ‡è®°è®­ç»ƒæ•°æ®çš„å½±å“è¿›è¡Œäº†å…¨é¢çš„åˆ†æã€‚å®ƒå¼ºè°ƒäº†æ‰©å¤§è¯æ±‡é‡ä»¥åŠæ”¹è¿›ç”µæå¯†åº¦åœ¨æé«˜BCIæ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸»å¼ å¢åŠ å¾®ç”µæçš„æ•°é‡ä»¥åŠå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œç²¾ç‚¼ä¼˜åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06326v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è„‘è§£ç æŠ€æœ¯å·²é€æ¸æˆä¸ºç¥ç»ç§‘å­¦é¢†åŸŸå¿«é€Ÿå‘å±•çš„æŠ€æœ¯ä¹‹ä¸€ã€‚æœ¬æ–‡èšç„¦äºåˆ©ç”¨åŸå§‹è„‘ç”µå›¾ä¿¡å·è§£ç äººç±»å¤§è„‘æ´»åŠ¨ï¼Œæä¾›ä¸€ç§æ›´å¿«é€Ÿã€æ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥æé«˜æˆ‘ä»¬å¯¹äººç±»å¤§è„‘çš„ç†è§£ã€‚ç ”ç©¶é‡ç‚¹æ¢è®¨äº†è„‘æœºæ¥å£åœ¨è§£æä¸è¨€è¯­äº§ç”Ÿç›¸å…³çš„ç¥ç»ä¿¡å·æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ç‰¹åˆ«å…³æ³¨è¯æ±‡é‡ã€ç”µæå¯†åº¦å’Œè®­ç»ƒæ•°æ®å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¯¹åˆ©ç”¨éæ ‡è®°æ•°æ®å¯¹LibrispeechåŸºå‡†æµ‹è¯•è¿›è¡Œè¯­éŸ³å¤„ç†çš„ç«äº‰è¯é”™è¯¯ç‡è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†æ¨¡å‹å¤§å°å’Œæ— æ ‡ç­¾è®­ç»ƒæ•°æ®å¯¹è¯­éŸ³è¯†åˆ«çš„é”™è¯¯æ¨¡å¼çš„å½±å“ï¼Œå¹¶å¼ºè°ƒå¢åŠ å¾®ç”µææ•°é‡å’Œæ”¹è¿›è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è§‚ç‚¹çš„æ¦‚è¦ï¼š</p>
<ol>
<li>ç ”ç©¶çš„é‡ç‚¹åœ¨äºåº”ç”¨è„‘ç”µå›¾ä¿¡å·è§£ç äººç±»å¤§è„‘æ´»åŠ¨ï¼Œä¿ƒè¿›äººç±»å¯¹å¤§è„‘ç†è§£çš„æé«˜ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†è„‘æœºæ¥å£åœ¨è§£æä¸è¨€è¯­äº§ç”Ÿç›¸å…³çš„ç¥ç»ä¿¡å·æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¢è®¨äº†è¯æ±‡é‡ã€ç”µæå¯†åº¦å’Œè®­ç»ƒæ•°æ®å¯¹è„‘æœºæ¥å£æ€§èƒ½çš„å½±å“ã€‚è¯æ±‡é‡ä¸°å¯Œå’Œç”µæå¯†åº¦å¢åŠ èƒ½æé«˜è„‘æœºæ¥å£æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†åœ¨LibrispeechåŸºå‡†æµ‹è¯•ä¸‹ï¼Œåˆ©ç”¨æœªæ ‡è®°æ•°æ®è¿›è¡Œé¢„è®­ç»ƒå¯¹è¯­éŸ³å¤„ç†çš„æ€§èƒ½å½±å“ï¼Œå…¶è¡¨ç°è¶…è¿‡äº†å…ˆå‰çš„æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ç ”ç©¶å‘ç°æ˜¾è‘—å‡å°‘æ ‡ç­¾æ•°é‡ä¹Ÿèƒ½è¾¾åˆ°è¶…è¶Šå…ˆå‰æŠ€æœ¯çš„æ•ˆæœã€‚</li>
<li>ç ”ç©¶å¯¹è¯­éŸ³è¯†åˆ«ä¸­çš„é”™è¯¯æ¨¡å¼è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œæ¢è®¨äº†æ¨¡å‹å¤§å°å’Œæ— æ ‡ç­¾è®­ç»ƒæ•°æ®å¯¹æ€§èƒ½çš„å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06326v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06326v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TTS-Transducer-End-to-End-Speech-Synthesis-with-Neural-Transducer"><a href="#TTS-Transducer-End-to-End-Speech-Synthesis-with-Neural-Transducer" class="headerlink" title="TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer"></a>TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer</h2><p><strong>Authors:Vladimir Bataev, Subhankar Ghosh, Vitaly Lavrukhin, Jason Li</strong></p>
<p>This work introduces TTS-Transducer - a novel architecture for text-to-speech, leveraging the strengths of audio codec models and neural transducers. Transducers, renowned for their superior quality and robustness in speech recognition, are employed to learn monotonic alignments and allow for avoiding using explicit duration predictors. Neural audio codecs efficiently compress audio into discrete codes, revealing the possibility of applying text modeling approaches to speech generation. However, the complexity of predicting multiple tokens per frame from several codebooks, as necessitated by audio codec models with residual quantizers, poses a significant challenge. The proposed system first uses a transducer architecture to learn monotonic alignments between tokenized text and speech codec tokens for the first codebook. Next, a non-autoregressive Transformer predicts the remaining codes using the alignment extracted from transducer loss. The proposed system is trained end-to-end. We show that TTS-Transducer is a competitive and robust alternative to contemporary TTS systems. </p>
<blockquote>
<p>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†TTS-Transducerâ€”â€”ä¸€ç§æ–°å‹æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ¶æ„ï¼Œå®ƒç»“åˆäº†éŸ³é¢‘ç¼–è§£ç æ¨¡å‹å’Œç¥ç»è½¬æ¢å™¨çš„ä¼˜åŠ¿ã€‚è½¬æ¢å™¨ä»¥å…¶å‡ºè‰²çš„è¯­éŸ³è¯†åˆ«è´¨é‡å’Œç¨³å¥æ€§è€Œé—»åï¼Œç”¨äºå­¦ä¹ å•è°ƒå¯¹é½ï¼Œå¹¶é¿å…ä½¿ç”¨æ˜ç¡®çš„æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨æœ‰æ•ˆåœ°å°†éŸ³é¢‘å‹ç¼©æˆç¦»æ•£ä»£ç ï¼Œæ­ç¤ºäº†å°†æ–‡æœ¬å»ºæ¨¡æ–¹æ³•åº”ç”¨äºè¯­éŸ³ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç”±äºéŸ³é¢‘ç¼–è§£ç æ¨¡å‹éœ€è¦å¸¦æœ‰æ®‹å·®é‡åŒ–å™¨çš„ä»£ç æœ¬ï¼Œé¢„æµ‹æ¯å¸§å¤šä¸ªä»¤ç‰Œï¼ˆtokensï¼‰çš„å¤æ‚æ€§æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚æ‰€æè®®çš„ç³»ç»Ÿé¦–å…ˆä½¿ç”¨è½¬æ¢å™¨æ¶æ„æ¥å­¦ä¹ ä»¤ç‰ŒåŒ–æ–‡æœ¬å’Œè¯­éŸ³ç¼–è§£ç ä»¤ç‰Œä¹‹é—´çš„å•è°ƒå¯¹é½ï¼ˆé’ˆå¯¹ç¬¬ä¸€ä¸ªä»£ç æœ¬ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œéè‡ªå›å½’Transformerä½¿ç”¨ä»è½¬æ¢å™¨æŸå¤±ä¸­æå–çš„å¯¹é½æ¥é¢„æµ‹å‰©ä½™çš„ä»£ç ã€‚æ‰€æè®®çš„ç³»ç»Ÿæ˜¯ç«¯åˆ°ç«¯è¿›è¡Œè®­ç»ƒçš„ã€‚æˆ‘ä»¬è¯æ˜äº†TTS-Transduceræ˜¯å½“ä»£æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ç³»ç»Ÿçš„ä¸€ä¸ªæœ‰ç«äº‰åŠ›å’Œç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06320v1">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†TTS-Transducerï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨éŸ³é¢‘ç¼–è§£ç å™¨æ¨¡å‹å’Œç¥ç»è½¬æ¢å™¨ä¼˜åŠ¿çš„æ–°å‹æ–‡æœ¬è½¬è¯­éŸ³æ¶æ„ã€‚è½¬æ¢å™¨ä»¥å…¶å‡ºè‰²çš„è´¨é‡å’Œç¨³å¥æ€§åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸå¤‡å—èµèª‰ï¼Œè¢«ç”¨æ¥å­¦ä¹ å•è°ƒå¯¹é½ï¼Œå¹¶é¿å…ä½¿ç”¨æ˜ç¡®çš„æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†éŸ³é¢‘å‹ç¼©æˆç¦»æ•£ä»£ç ï¼Œæ­ç¤ºäº†å°†æ–‡æœ¬å»ºæ¨¡æ–¹æ³•åº”ç”¨äºè¯­éŸ³ç”Ÿæˆçš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œç”±äºéŸ³é¢‘ç¼–è§£ç å™¨æ¨¡å‹éœ€è¦é¢„æµ‹æ¯å¸§çš„å¤šä¸ªä»¤ç‰Œå¹¶ä½¿ç”¨æ®‹ç•™é‡åŒ–å™¨æ¥å¤„ç†æ¥è‡ªå¤šä¸ªä»£ç åº“çš„å¤šä¸ªä»¤ç‰Œçš„é—®é¢˜å¸¦æ¥å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚æ‰€æå‡ºçš„æ–¹æ³•é¦–æ¬¡ä½¿ç”¨è½¬æ¢å™¨æ¶æ„æ¥å­¦ä¹ æ ‡è®°åŒ–æ–‡æœ¬å’Œè¯­éŸ³ç¼–è§£ç å™¨ä»¤ç‰Œä¹‹é—´çš„å•è°ƒå¯¹é½å…³ç³»ï¼Œç”¨äºç¬¬ä¸€ä¸ªä»£ç åº“ã€‚ç„¶åï¼Œéè‡ªå›å½’Transformerä½¿ç”¨ä»è½¬æ¢å™¨æŸå¤±ä¸­æå–çš„å¯¹é½æ¥é¢„æµ‹å‰©ä½™çš„ç¼–ç ã€‚æ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚æˆ‘ä»¬è¯æ˜äº†TTS-Transduceræ˜¯å½“ä»£TTSç³»ç»Ÿçš„ä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„ç¨³å¥æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>TTS-Transduceræ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬è½¬è¯­éŸ³æ¶æ„ï¼Œç»“åˆäº†éŸ³é¢‘ç¼–è§£ç å™¨æ¨¡å‹å’Œç¥ç»è½¬æ¢å™¨çš„ä¼˜ç‚¹ã€‚</li>
<li>è½¬æ¢å™¨è¢«ç”¨äºå­¦ä¹ å•è°ƒå¯¹é½ï¼Œé¿å…äº†ä½¿ç”¨æ˜ç¡®çš„æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚è¿™ç§å¯¹é½ä½¿å¾—ç³»ç»Ÿå¯ä»¥æ›´å¥½åœ°åŒ¹é…æ–‡æœ¬å’Œè¯­éŸ³ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨çš„ä½¿ç”¨æœ‰æ•ˆåœ°å°†éŸ³é¢‘å‹ç¼©æˆç¦»æ•£ä»£ç ï¼Œä¸ºè¯­éŸ³ç”Ÿæˆæä¾›äº†æ–°æ€è·¯ã€‚ç„¶è€Œï¼Œä»å¤šä¸ªä»£ç åº“ä¸­é¢„æµ‹å¤šä¸ªä»¤ç‰Œå¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•ä½¿ç”¨è½¬æ¢å™¨æ¶æ„æ¥å­¦ä¹ æ–‡æœ¬å’Œè¯­éŸ³ç¼–è§£ç å™¨ä¹‹é—´çš„å•è°ƒå¯¹é½å…³ç³»ï¼Œå¹¶è®­ç»ƒæ¨¡å‹ä»¥é¢„æµ‹å‰©ä½™çš„ç¼–ç ã€‚</li>
<li>è¯¥ç³»ç»Ÿé‡‡ç”¨éè‡ªå›å½’Transformerè¿›è¡Œé¢„æµ‹ï¼Œè¿™ç§ç»“æ„ä½¿å¾—ç³»ç»Ÿçš„è®­ç»ƒå’Œé¢„æµ‹æ•ˆç‡æ›´é«˜ã€‚</li>
<li>TTS-Transducerå…·æœ‰ç«äº‰åŠ›ä¸”ç¨³å¥ï¼Œå¯ä»¥ä½œä¸ºå½“ä»£TTSç³»ç»Ÿçš„æ›¿ä»£æ–¹æ¡ˆã€‚å®ƒå¯èƒ½ä¼šåœ¨æœªæ¥æˆä¸ºæ–‡æœ¬è½¬è¯­éŸ³é¢†åŸŸçš„ä¸€ç§é‡è¦æŠ€æœ¯è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06320v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06320v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06320v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MinMo-A-Multimodal-Large-Language-Model-for-Seamless-Voice-Interaction"><a href="#MinMo-A-Multimodal-Large-Language-Model-for-Seamless-Voice-Interaction" class="headerlink" title="MinMo: A Multimodal Large Language Model for Seamless Voice Interaction"></a>MinMo: A Multimodal Large Language Model for Seamless Voice Interaction</h2><p><strong>Authors:Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, Yabin Li, Xiang Lv, Jiaqing Liu, Haoneng Luo, Bin Ma, Chongjia Ni, Xian Shi, Jialong Tang, Hui Wang, Hao Wang, Wen Wang, Yuxuan Wang, Yunlan Xu, Fan Yu, Zhijie Yan, Yexin Yang, Baosong Yang, Xian Yang, Guanrou Yang, Tianyu Zhao, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Pei Zhang, Chong Zhang, Jinren Zhou</strong></p>
<p>Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training. Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is <a target="_blank" rel="noopener" href="https://funaudiollm.github.io/minmo">https://funaudiollm.github.io/minmo</a>, and the code and models will be released soon. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€è¯­éŸ³æ–‡æœ¬æ¨¡å‹çš„è¿›æ­¥ä¸ºæ— ç¼è¯­éŸ³äº¤äº’å¥ å®šäº†åŸºç¡€ï¼Œå®ç°äº†å®æ—¶ã€è‡ªç„¶ã€äººæ€§åŒ–çš„å¯¹è¯ã€‚ä¹‹å‰çš„è¯­éŸ³äº¤äº’æ¨¡å‹å¯åˆ†ä¸ºåŸç”Ÿå’Œå¯¹é½ä¸¤ç±»ã€‚åŸç”Ÿæ¨¡å‹å°†è¯­éŸ³å’Œæ–‡æœ¬å¤„ç†é›†æˆåœ¨ä¸€ä¸ªæ¡†æ¶ä¸­ï¼Œä½†é¢ä¸´ç€åºåˆ—é•¿åº¦ä¸åŒå’Œé¢„è®­ç»ƒä¸è¶³ç­‰é—®é¢˜ã€‚å¯¹é½æ¨¡å‹ä¿ç•™äº†æ–‡æœ¬LLMçš„åŠŸèƒ½ï¼Œä½†å¾€å¾€å—é™äºå°æ•°æ®é›†å’Œç‹­çª„çš„è¯­éŸ³ä»»åŠ¡å…³æ³¨èŒƒå›´ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MinMoï¼Œä¸€ä¸ªç”¨äºæ— ç¼è¯­éŸ³äº¤äº’çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ‹¥æœ‰å¤§çº¦8äº¿å‚æ•°ã€‚æˆ‘ä»¬è§£å†³äº†å…ˆå‰å¯¹é½çš„å¤šæ¨¡æ€æ¨¡å‹çš„ä¸»è¦å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¤šä¸ªé˜¶æ®µçš„è¯­éŸ³åˆ°æ–‡æœ¬çš„å¯¹é½ã€æ–‡æœ¬åˆ°è¯­éŸ³çš„å¯¹é½ã€è¯­éŸ³åˆ°è¯­éŸ³çš„å¯¹é½ä»¥åŠåŒå‘äº¤äº’å¯¹é½ï¼Œåœ¨140ä¸‡å°æ—¶çš„å¤šæ ·è¯­éŸ³æ•°æ®å’Œå¹¿æ³›çš„è¯­éŸ³ä»»åŠ¡ä¸Šè®­ç»ƒMinMoã€‚ç»è¿‡å¤šé˜¶æ®µè®­ç»ƒåï¼ŒMinMoåœ¨è¯­éŸ³ç†è§£å’Œç”Ÿæˆçš„å„ç§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†æ–‡æœ¬LLMçš„åŠŸèƒ½ï¼Œè¿˜æ”¯æŒå…¨åŒå·¥å¯¹è¯ï¼Œå³ç”¨æˆ·ä¸ç³»ç»Ÿä¹‹é—´çš„åŒæ—¶åŒå‘é€šä¿¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–è€Œç®€å•çš„è¯­éŸ³è§£ç å™¨ï¼Œåœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢ä¼˜äºä»¥å‰çš„æ¨¡å‹ã€‚MinMoå¢å¼ºçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ”¯æŒæ ¹æ®ç”¨æˆ·æŒ‡ä»¤æ§åˆ¶è¯­éŸ³ç”Ÿæˆï¼ŒåŒ…æ‹¬å„ç§ç»†å¾®å·®åˆ«ï¼Œå¦‚æƒ…ç»ªã€æ–¹è¨€å’Œè¯­é€Ÿï¼Œä»¥åŠæ¨¡ä»¿ç‰¹å®šå£°éŸ³ã€‚å¯¹äºMinMoæ¥è¯´ï¼Œè¯­éŸ³åˆ°æ–‡æœ¬çš„å»¶è¿Ÿå¤§çº¦ä¸º100æ¯«ç§’ï¼Œå…¨åŒå·¥å»¶è¿Ÿç†è®ºä¸Šçº¦ä¸º600æ¯«ç§’ï¼Œå®é™…ä¸Šçº¦ä¸º800æ¯«ç§’ã€‚MinMoé¡¹ç›®ç½‘é¡µæ˜¯<a target="_blank" rel="noopener" href="https://funaudiollm.github.io/minmo%EF%BC%8C%E4%BB%A3%E7%A0%81%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%B0%86%E5%BE%88%E5%BF%AB%E5%8F%91%E5%B8%83%E3%80%82">https://funaudiollm.github.io/minmoï¼Œä»£ç å’Œæ¨¡å‹å°†å¾ˆå¿«å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06282v1">PDF</a> Work in progress. Authors are listed in alphabetical order by family   name</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€è¯­éŸ³æ–‡æœ¬æ¨¡å‹è¿›å±•ï¼Œä¸ºæ— ç¼è¯­éŸ³äº¤äº’å¥ å®šäº†åŸºç¡€ï¼Œå®ç°äº†å®æ—¶ã€è‡ªç„¶å’Œäººæ€§åŒ–çš„å¯¹è¯ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åä¸ºMinMoçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒï¼Œè§£å†³äº†å…ˆå‰å¯¹é½å¤šæ¨¡æ€æ¨¡å‹çš„ä¸»è¦å±€é™æ€§ã€‚MinMoåœ¨è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ï¼Œå¹¶ä¿æŒäº†æ–‡æœ¬LLMçš„èƒ½åŠ›ï¼Œè¿˜æ”¯æŒå…¨åŒå·¥å¯¹è¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•è¯­éŸ³è§£ç å™¨ï¼Œåœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢ä¼˜äºå…ˆå‰çš„æ¨¡å‹ã€‚MinMoå¢å¼ºäº†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œæ”¯æŒæ ¹æ®ç”¨æˆ·æŒ‡ä»¤æ§åˆ¶è¯­éŸ³ç”Ÿæˆï¼ŒåŒ…æ‹¬æƒ…æ„Ÿã€æ–¹è¨€å’Œè¯­é€Ÿç­‰ç»†å¾®å·®åˆ«ï¼Œå¹¶å¯æ¨¡ä»¿ç‰¹å®šå£°éŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€è¯­éŸ³æ–‡æœ¬æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºæ— ç¼è¯­éŸ³äº¤äº’å¥ å®šäº†åŸºç¡€ã€‚</li>
<li>MinMoæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè§£å†³äº†å…ˆå‰æ¨¡å‹çš„å±€é™æ€§ï¼Œå®ç°äº†æ— ç¼è¯­éŸ³äº¤äº’ã€‚</li>
<li>MinMoé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒï¼Œåœ¨è¯­éŸ³ç†è§£å’Œç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚</li>
<li>MinMoä¿æŒäº†æ–‡æœ¬LLMçš„èƒ½åŠ›ï¼Œæ”¯æŒå…¨åŒå·¥å¯¹è¯ã€‚</li>
<li>MinMoé‡‡ç”¨äº†ä¸€ç§æ–°çš„ç®€å•è¯­éŸ³è§£ç å™¨ï¼Œåœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢ä¼˜äºå…ˆå‰çš„æ¨¡å‹ã€‚</li>
<li>MinMoå¢å¼ºäº†æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œå¯æ ¹æ®ç”¨æˆ·æŒ‡ä»¤æ§åˆ¶è¯­éŸ³ç”Ÿæˆçš„å„ç§ç»†å¾®å·®åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06282v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06282v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06282v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.06282v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Effective-and-Efficient-Mixed-Precision-Quantization-of-Speech-Foundation-Models"><a href="#Effective-and-Efficient-Mixed-Precision-Quantization-of-Speech-Foundation-Models" class="headerlink" title="Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models"></a>Effective and Efficient Mixed Precision Quantization of Speech   Foundation Models</h2><p><strong>Authors:Haoning Xu, Zhaoqing Li, Zengrui Jin, Huimeng Wang, Youjun Chen, Guinan Li, Mengzhe Geng, Shujie Hu, Jiajun Deng, Xunying Liu</strong></p>
<p>This paper presents a novel mixed-precision quantization approach for speech foundation models that tightly integrates mixed-precision learning and quantized model parameter estimation into one single model compression stage. Experiments conducted on LibriSpeech dataset with fine-tuned wav2vec2.0-base and HuBERT-large models suggest the resulting mixed-precision quantized models increased the lossless compression ratio by factors up to 1.7x and 1.9x over the respective uniform-precision and two-stage mixed-precision quantized baselines that perform precision learning and model parameters quantization in separate and disjointed stages, while incurring no statistically word error rate (WER) increase over the 32-bit full-precision models. The system compression time of wav2vec2.0-base and HuBERT-large models is reduced by up to 1.9 and 1.5 times over the two-stage mixed-precision baselines, while both produce lower WERs. The best-performing 3.5-bit mixed-precision quantized HuBERT-large model produces a lossless compression ratio of 8.6x over the 32-bit full-precision system. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è¯­éŸ³åŸºç¡€æ¨¡å‹çš„æ–°å‹æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ··åˆç²¾åº¦å­¦ä¹ å’Œé‡åŒ–æ¨¡å‹å‚æ•°ä¼°è®¡ç´§å¯†é›†æˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ¨¡å‹å‹ç¼©é˜¶æ®µä¸­ã€‚åœ¨LibriSpeechæ•°æ®é›†ä¸Šå¯¹fine-tunedçš„wav2vec2.0-baseå’ŒHuBERT-largeæ¨¡å‹è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæ‰€å¾—çš„æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹ç›¸å¯¹äºå„è‡ªçš„ç»Ÿä¸€ç²¾åº¦å’Œä¸¤é˜¶æ®µæ··åˆç²¾åº¦é‡åŒ–åŸºçº¿ï¼Œæ— æŸå‹ç¼©æ¯”æé«˜äº†é«˜è¾¾1.7å€å’Œ1.9å€ã€‚è¿™äº›åŸºçº¿åœ¨ä¸åŒçš„ã€äº’ä¸ç›¸å…³çš„é˜¶æ®µåˆ†åˆ«è¿›è¡Œç²¾åº¦å­¦ä¹ å’Œæ¨¡å‹å‚æ•°é‡åŒ–ã€‚åŒæ—¶ï¼Œç›¸å¯¹äº32ä½å…¨ç²¾åº¦æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ç»Ÿè®¡ä¸Šæ²¡æœ‰å¢åŠ è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚ç›¸è¾ƒäºä¸¤é˜¶æ®µæ··åˆç²¾åº¦åŸºçº¿ï¼Œwav2vec2.0-baseå’ŒHuBERT-largeæ¨¡å‹çš„å‹ç¼©æ—¶é—´å‡å°‘äº†æœ€å¤š1.9å€å’Œ1.5å€ï¼ŒåŒæ—¶ä¸¤è€…éƒ½äº§ç”Ÿäº†æ›´ä½çš„WERã€‚è¡¨ç°æœ€ä½³çš„3.5ä½æ··åˆç²¾åº¦é‡åŒ–HuBERT-largeæ¨¡å‹ç›¸å¯¹äº32ä½å…¨ç²¾åº¦ç³»ç»Ÿå®ç°äº†8.6å€çš„æ— æŸå‹ç¼©æ¯”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03643v2">PDF</a> To appear at IEEE ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œç”¨äºè¯­éŸ³åŸºç¡€æ¨¡å‹ã€‚è¯¥æ–¹æ³•å°†æ··åˆç²¾åº¦å­¦ä¹ å’Œé‡åŒ–æ¨¡å‹å‚æ•°ä¼°è®¡ç´§å¯†é›†æˆåˆ°ä¸€ä¸ªå•ä¸€çš„æ¨¡å‹å‹ç¼©é˜¶æ®µã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„å‹ç¼©æ¯”ä¾‹æœ€é«˜å¯è¾¾åŸæœ‰ç³»ç»Ÿçš„ä¸¤å€ï¼ŒåŒæ—¶ä¸ä¼šå¢åŠ è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æ˜¾è‘—å‡å°‘äº†æ¨¡å‹å‹ç¼©æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ç”¨äºè¯­éŸ³åŸºç¡€æ¨¡å‹ã€‚</li>
<li>é›†æˆæ··åˆç²¾åº¦å­¦ä¹ ä¸é‡åŒ–æ¨¡å‹å‚æ•°ä¼°è®¡åˆ°ä¸€ä¸ªå•ä¸€å‹ç¼©é˜¶æ®µã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜æ··åˆç²¾åº¦é‡åŒ–æ¨¡å‹çš„å‹ç¼©ç‡æ¯”åŸºå‡†é«˜å‡ºçº¦ä¸¤å€ï¼Œæ²¡æœ‰å¢åŠ è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>ç³»ç»Ÿå‹ç¼©æ—¶é—´å‡å°‘äº†æœ€é«˜è¾¾åˆ°ä¸€åŠçš„åŸºçº¿å€¼ã€‚åŒæ—¶å®éªŒç»“æœå±•ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚ </li>
<li>æ··åˆç²¾åº¦é‡åŒ–çš„æœ€ä½³è¡¨ç°æ˜¯åœ¨ä½¿ç”¨ HuBERT-large æ¨¡å‹ä¸Šï¼Œç›¸è¾ƒäºåŸæœ‰çš„ 32 ä½å…¨ç²¾åº¦ç³»ç»Ÿå®ç°äº†é«˜è¾¾ 8.6 å€çš„æ— æŸå‹ç¼©ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.03643v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.03643v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.03643v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Advancing-Singlish-Understanding-Bridging-the-Gap-with-Datasets-and-Multimodal-Models"><a href="#Advancing-Singlish-Understanding-Bridging-the-Gap-with-Datasets-and-Multimodal-Models" class="headerlink" title="Advancing Singlish Understanding: Bridging the Gap with Datasets and   Multimodal Models"></a>Advancing Singlish Understanding: Bridging the Gap with Datasets and   Multimodal Models</h2><p><strong>Authors:Bin Wang, Xunlong Zou, Shuo Sun, Wenyu Zhang, Yingxu He, Zhuohan Liu, Chengwei Wei, Nancy F. Chen, AiTi Aw</strong></p>
<p>Singlish, a Creole language rooted in English, is a key focus in linguistic research within multilingual and multicultural contexts. However, its spoken form remains underexplored, limiting insights into its linguistic structure and applications. To address this gap, we standardize and annotate the largest spoken Singlish corpus, introducing the Multitask National Speech Corpus (MNSC). These datasets support diverse tasks, including Automatic Speech Recognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue Summarization (SDS), and Paralinguistic Question Answering (PQA). We release standardized splits and a human-verified test set to facilitate further research. Additionally, we propose SingAudioLLM, a multi-task multimodal model leveraging multimodal large language models to handle these tasks concurrently. Experiments reveal our models adaptability to Singlish context, achieving state-of-the-art performance and outperforming prior models by 10-30% in comparison with other AudioLLMs and cascaded solutions. </p>
<blockquote>
<p>æ–°åŠ å¡å¼è‹±è¯­æ˜¯ä¸€ç§æ ¹æ¤äºè‹±è¯­çš„å…‹é‡Œå¥¥å°”è¯­è¨€ï¼Œåœ¨å¤šè¯­è¨€å’Œå¤šå…ƒæ–‡åŒ–èƒŒæ™¯ä¸‹ï¼Œå®ƒæˆä¸ºäº†è¯­è¨€å­¦ç ”ç©¶çš„å…³é”®ç„¦ç‚¹ã€‚ç„¶è€Œï¼Œå®ƒçš„å£è¯­å½¢å¼ä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ï¼Œå¯¹äºå…¶è¯­è¨€ç»“æ„å’Œåº”ç”¨æ–¹é¢çš„è§è§£æœ‰æ‰€å±€é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹æœ€å¤§çš„æ–°åŠ å¡å¼è‹±è¯­å£è¯­è¯­æ–™åº“è¿›è¡Œäº†æ ‡å‡†åŒ–å’Œæ³¨é‡Šï¼Œå¹¶å¼•å…¥äº†å¤šä»»åŠ¡å›½å®¶è¯­éŸ³è¯­æ–™åº“ï¼ˆMNSCï¼‰ã€‚è¿™äº›æ•°æ®é›†æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­é—®é¢˜å›ç­”ï¼ˆSQAï¼‰ã€å£è¯­å¯¹è¯æ‘˜è¦ï¼ˆSDSï¼‰å’Œå‰¯è¯­è¨€é—®é¢˜å›ç­”ï¼ˆPQAï¼‰ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ ‡å‡†åŒ–çš„åˆ†å‰²æ•°æ®é›†å’Œç»è¿‡äººå·¥éªŒè¯çš„æµ‹è¯•é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†SingAudioLLMï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»»åŠ¡å¤šæ¨¡å¼æ¨¡å‹ï¼Œåˆ©ç”¨å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹åŒæ—¶å¤„ç†è¿™äº›ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé€‚åº”æ–°åŠ å¡å¼è‹±è¯­çš„è¯­å¢ƒï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œä¸å…¶ä»–éŸ³é¢‘LLMå’Œçº§è”è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œæ€§èƒ½æé«˜äº†10-30%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01034v2">PDF</a> Open-Source: <a target="_blank" rel="noopener" href="https://github.com/AudioLLMs/Singlish">https://github.com/AudioLLMs/Singlish</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†Singlishè¿™ä¸€æºè‡ªè‹±è¯­çš„æœ‰ç‰¹è‰²çš„å…‹é‡Œå¥¥å°”è¯­è¨€åœ¨å¤šè¯­ç§ã€å¤šå…ƒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è¯­è¨€ç ”ç©¶é‡è¦æ€§ã€‚é’ˆå¯¹å…¶å£è¯­å½¢å¼æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†å»ºç«‹æœ€å¤§çš„å£è¯­Singlishè¯­æ–™åº“Multitask National Speech Corpusï¼ˆMNSCï¼‰ï¼Œå¹¶æ ‡æ³¨æ•°æ®é›†ä»¥æ”¯æŒè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å£è¯­é—®ç­”ï¼ˆSQAï¼‰ã€å£è¯­å¯¹è¯æ‘˜è¦ï¼ˆSDSï¼‰å’Œå‰¯è¯­è¨€é—®ç­”ï¼ˆPQAï¼‰ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†é’ˆå¯¹è¿™äº›ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹SingAudioLLMçš„åº”ç”¨ï¼Œå®éªŒè¯æ˜è¯¥æ¨¡å‹é€‚åº”Singlishè¯­å¢ƒï¼Œæ€§èƒ½ä¼˜äºå…¶ä»–AudioLLMså’Œçº§è”è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Singlishæ˜¯ä¸€ç§é‡è¦çš„å…‹é‡Œå¥¥å°”è¯­è¨€ï¼Œå…·æœ‰ç‹¬ç‰¹çš„ç ”ç©¶ä»·å€¼ã€‚</li>
<li>ç°æœ‰çš„å…³äºSinglishçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å…¶ä¹¦é¢å½¢å¼ä¸Šï¼Œå¯¹å…¶å£è¯­å½¢å¼çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>Multitask National Speech Corpusï¼ˆMNSCï¼‰æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å£è¯­Singlishè¯­æ–™åº“ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚ASRã€SQAç­‰ã€‚</li>
<li>SingAudioLLMæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬ASRã€SQAç­‰ã€‚</li>
<li>SingAudioLLMé€šè¿‡å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®æ¥æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„AudioLLMså’Œçº§è”è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒSingAudioLLMæ€§èƒ½æ›´å¥½ï¼Œå°¤å…¶æ˜¯é€‚åº”äº†Singlishè¯­å¢ƒçš„ç‰¹æ®Šæ€§ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜ï¼Œå¯¹äºæ¶‰åŠSinglishçš„è¯­è¨€ä»»åŠ¡ï¼ŒSingAudioLLMæ˜¾è‘—æå‡äº†è¡¨ç°æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.01034v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.01034v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.01034v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2501.01034v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Attacking-Voice-Anonymization-Systems-with-Augmented-Feature-and-Speaker-Identity-Difference"><a href="#Attacking-Voice-Anonymization-Systems-with-Augmented-Feature-and-Speaker-Identity-Difference" class="headerlink" title="Attacking Voice Anonymization Systems with Augmented Feature and Speaker   Identity Difference"></a>Attacking Voice Anonymization Systems with Augmented Feature and Speaker   Identity Difference</h2><p><strong>Authors:Yanzhe Zhang, Zhonghao Bi, Feiyang Xiao, Xuefeng Yang, Qiaoxi Zhu, Jian Guan</strong></p>
<p>This study focuses on the First VoicePrivacy Attacker Challenge within the ICASSP 2025 Signal Processing Grand Challenge, which aims to develop speaker verification systems capable of determining whether two anonymized speech signals are from the same speaker. However, differences between feature distributions of original and anonymized speech complicate this task. To address this challenge, we propose an attacker system that combines Data Augmentation enhanced feature representation and Speaker Identity Difference enhanced classifier to improve verification performance, termed DA-SID. Specifically, data augmentation strategies (i.e., data fusion and SpecAugment) are utilized to mitigate feature distribution gaps, while probabilistic linear discriminant analysis (PLDA) is employed to further enhance speaker identity difference. Our system significantly outperforms the baseline, demonstrating exceptional effectiveness and robustness against various voice anonymization systems, ultimately securing a top-5 ranking in the challenge. </p>
<blockquote>
<p>æœ¬ç ”ç©¶èšç„¦äºICASSP 2025ä¿¡å·å¤„ç†æŒ‘æˆ˜èµ›ä¸­çš„â€œç¬¬ä¸€è¯­éŸ³éšç§æ”»å‡»è€…æŒ‘æˆ˜â€ï¼Œæ—¨åœ¨å¼€å‘èƒ½å¤Ÿåˆ¤æ–­ä¸¤ä¸ªåŒ¿åè¯­éŸ³ä¿¡å·æ˜¯å¦æ¥è‡ªåŒä¸€è¯´è¯äººçš„è¯´è¯äººéªŒè¯ç³»ç»Ÿã€‚ç„¶è€Œï¼ŒåŸå§‹è¯­éŸ³å’ŒåŒ¿åè¯­éŸ³ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ä½¿è¿™ä¸€ä»»åŠ¡å¤æ‚åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”»å‡»è€…ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†æ•°æ®å¢å¼ºå¢å¼ºç‰¹å¾è¡¨ç¤ºå’Œè¯´è¯äººèº«ä»½å·®å¼‚å¢å¼ºåˆ†ç±»å™¨æ¥æé«˜éªŒè¯æ€§èƒ½ï¼Œè¢«ç§°ä¸ºDA-SIDã€‚å…·ä½“æ¥è¯´ï¼Œæ•°æ®å¢å¼ºç­–ç•¥ï¼ˆå¦‚æ•°æ®èåˆå’ŒSpecAugmentï¼‰è¢«ç”¨æ¥ç¼©å°ç‰¹å¾åˆ†å¸ƒå·®è·ï¼Œæ¦‚ç‡çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆPLDAï¼‰è¢«ç”¨æ¥è¿›ä¸€æ­¥å¢å¼ºè¯´è¯äººèº«ä»½å·®å¼‚ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿ç³»ç»Ÿï¼Œè¡¨ç°å‡ºå¯¹å„ç§è¯­éŸ³åŒ¿åç³»ç»Ÿçš„å‡ºè‰²æ•ˆæœå’Œç¨³å¥æ€§ï¼Œæœ€ç»ˆåœ¨æŒ‘æˆ˜ä¸­è·å¾—äº†å‰äº”åçš„æ’åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.19068v2">PDF</a> 2 pages, submitted to ICASSP 2025 GC-7: The First VoicePrivacy   Attacker Challenge (by invitation), fixed a numerical typo: In Table II, the   EER% for DA-SID w&#x2F;o DA under T8-5 is corrected to 26.96</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ICASSP 2025ä¿¡å·å¤„ç†æŒ‘æˆ˜èµ›ä¸­çš„First VoicePrivacy Attacker Challengeã€‚è¯¥æŒ‘æˆ˜æ—¨åœ¨å¼€å‘èƒ½å¤Ÿç¡®å®šä¸¤ä¸ªåŒ¿åè¯­éŸ³ä¿¡å·æ˜¯å¦æ¥è‡ªåŒä¸€å‘è¨€äººçš„è¯´è¯äººéªŒè¯ç³»ç»Ÿã€‚ä¸ºè§£å†³åŸå§‹è¯­éŸ³ä¸åŒ¿ååŒ–è¯­éŸ³ç‰¹å¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆæ•°æ®å¢å¼ºç‰¹å¾è¡¨ç¤ºå’Œè¯´è¯äººèº«ä»½å·®å¼‚åˆ†ç±»å™¨çš„æ”»å‡»è€…ç³»ç»Ÿï¼Œç§°ä¸ºDA-SIDã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨æ•°æ®èåˆå’ŒSpecAugmentç­‰æ•°æ®å¢å¼ºç­–ç•¥æ¥ç¼©å°ç‰¹å¾åˆ†å¸ƒå·®è·ï¼Œå¹¶ä½¿ç”¨æ¦‚ç‡çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆPLDAï¼‰è¿›ä¸€æ­¥å¢å¼ºè¯´è¯äººèº«ä»½å·®å¼‚è¯†åˆ«ã€‚è¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¯¹å„ç§è¯­éŸ³åŒ¿ååŒ–ç³»ç»Ÿè¡¨ç°å‡ºæå¼ºçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ï¼Œæœ€ç»ˆåœ¨æŒ‘æˆ˜ä¸­è·»èº«å‰äº”åã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¬¬ä¸€é¡¹æŒ‘æˆ˜æ˜¯å¼€å‘èƒ½å¤Ÿç¡®å®šä¸¤ä¸ªåŒ¿åè¯­éŸ³ä¿¡å·æ˜¯å¦æ¥è‡ªåŒä¸€å‘è¨€äººçš„è¯´è¯äººéªŒè¯ç³»ç»Ÿã€‚</li>
<li>åŸå§‹è¯­éŸ³ä¸åŒ¿ååŒ–è¯­éŸ³çš„ç‰¹å¾åˆ†å¸ƒå­˜åœ¨å·®å¼‚ï¼Œä½¿å¾—è¿™ä¸€ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDA-SIDçš„ç³»ç»Ÿï¼Œç»“åˆæ•°æ®å¢å¼ºç‰¹å¾è¡¨ç¤ºå’Œè¯´è¯äººèº«ä»½å·®å¼‚åˆ†ç±»å™¨æ¥æ”¹å–„éªŒè¯æ€§èƒ½ã€‚</li>
<li>æ•°æ®èåˆå’ŒSpecAugmentç­‰æ•°æ®å¢å¼ºç­–ç•¥ç”¨äºç¼©å°ç‰¹å¾åˆ†å¸ƒå·®è·ã€‚</li>
<li>æ¦‚ç‡çº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆPLDAï¼‰ç”¨äºå¢å¼ºè¯´è¯äººèº«ä»½å·®å¼‚çš„è¯†åˆ«ã€‚</li>
<li>è¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¯¹å„ç§è¯­éŸ³åŒ¿ååŒ–ç³»ç»Ÿå…·æœ‰å¾ˆå¼ºçš„æœ‰æ•ˆæ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.19068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2412.19068v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2412.19068v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Blind-Estimation-of-Sub-band-Acoustic-Parameters-from-Ambisonics-Recordings-using-Spectro-Spatial-Covariance-Features"><a href="#Blind-Estimation-of-Sub-band-Acoustic-Parameters-from-Ambisonics-Recordings-using-Spectro-Spatial-Covariance-Features" class="headerlink" title="Blind Estimation of Sub-band Acoustic Parameters from Ambisonics   Recordings using Spectro-Spatial Covariance Features"></a>Blind Estimation of Sub-band Acoustic Parameters from Ambisonics   Recordings using Spectro-Spatial Covariance Features</h2><p><strong>Authors:Hanyu Meng, Jeroen Breebaart, Jeremy Stoddard, Vidhyasaharan Sethu, Eliathamby Ambikairajah</strong></p>
<p>Estimating frequency-varying acoustic parameters is essential for enhancing immersive perception in realistic spatial audio creation. In this paper, we propose a unified framework that blindly estimates reverberation time (T60), direct-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands using first-order Ambisonics (FOA) speech recordings as inputs. The proposed framework utilizes a novel feature named Spectro-Spatial Covariance Vector (SSCV), efficiently representing temporal, spectral as well as spatial information of the FOA signal. Our models significantly outperform existing single-channel methods with only spectral information, reducing estimation errors by more than half for all three acoustic parameters. Additionally, we introduce FOA-Conv3D, a novel back-end network for effectively utilising the SSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the convolutional neural network (CNN) and recurrent convolutional neural network (CRNN) backends, achieving lower estimation errors and accounting for a higher proportion of variance (PoV) for all 3 acoustic parameters. </p>
<blockquote>
<p>ä¼°è®¡é¢‘ç‡å˜åŒ–çš„å£°å­¦å‚æ•°å¯¹äºæé«˜çœŸå®ç©ºé—´éŸ³é¢‘åˆ›å»ºçš„æ²‰æµ¸å¼æ„ŸçŸ¥è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç›²ä¼°è®¡æ··å“æ—¶é—´ï¼ˆT60ï¼‰ã€ç›´è¾¾å£°ä¸æ··å“å£°ä¹‹æ¯”ï¼ˆDRRï¼‰å’Œæ¸…æ™°åº¦ï¼ˆC50ï¼‰åœ¨10ä¸ªé¢‘ç‡èŒƒå›´å†…ï¼Œä»¥é‡‡ç”¨ä¸€é˜¶ç¯ç»•å£°ï¼ˆFOAï¼‰å½•éŸ³ä½œä¸ºè¾“å…¥ã€‚æ‰€æå‡ºçš„æ¡†æ¶åˆ©ç”¨äº†ä¸€ç§åä¸ºè°±æ—¶ç©ºåæ–¹å·®å‘é‡ï¼ˆSSCVï¼‰çš„æ–°ç‰¹æ€§ï¼Œæœ‰æ•ˆåœ°è¡¨ç¤ºäº†FOAä¿¡å·çš„æ—¶ç©ºä»¥åŠé¢‘è°±ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»…ä½¿ç”¨é¢‘è°±ä¿¡æ¯ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å•é€šé“æ–¹æ³•ï¼Œå°†ä¸‰ç§å£°å­¦å‚æ•°çš„ä¼°è®¡è¯¯å·®å‡å°‘äº†ä¸€åŠä»¥ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†FOA-Conv3Dï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆä½¿ç”¨SSCVç‰¹æ€§çš„æ–°å‹åç«¯ç½‘ç»œï¼Œé‡‡ç”¨3Då·ç§¯ç¼–ç å™¨ã€‚FOA-Conv3Dçš„è¡¨ç°ä¼˜äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰çš„åç«¯ï¼Œå®ç°äº†æ›´ä½çš„ä¼°è®¡è¯¯å·®ï¼Œå¹¶ä¸”ä¸‰ç§å£°å­¦å‚æ•°çš„æ–¹å·®å æ¯”ï¼ˆPoVï¼‰æ›´é«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.03172v2">PDF</a> Accepted by ICASSP2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºåŸºäºç¬¬ä¸€é˜¶Ambisonicsï¼ˆFOAï¼‰å½•éŸ³è¾“å…¥ï¼Œç›²ç›®ä¼°è®¡åä¸ªé¢‘ç‡å¸¦çš„æ··å“æ—¶é—´ï¼ˆT60ï¼‰ã€ç›´è¾¾ä¸æ··å“æ¯”ï¼ˆDRRï¼‰å’Œæ¸…æ™°åº¦ï¼ˆC50ï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åä¸ºSpectro-Spatialåæ–¹å·®å‘é‡ï¼ˆSSCVï¼‰çš„æ–°ç‰¹æ€§ï¼Œæœ‰æ•ˆåœ°è¡¨ç¤ºFOAä¿¡å·çš„æ—¶ç©ºè°±å’Œç©ºé—´ä¿¡æ¯ã€‚ä¸ä»…ä½¿ç”¨è°±ä¿¡æ¯çš„å•é€šé“æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ä¼°è®¡è¿™ä¸‰ä¸ªå£°å­¦å‚æ•°æ—¶è¯¯å·®é™ä½äº†è¶…è¿‡ä¸€åŠã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†FOA-Conv3Dè¿™ä¸€æ–°å‹åç«¯ç½‘ç»œï¼Œåˆ©ç”¨SSCVç‰¹æ€§ä¸ä¸‰ç»´å·ç§¯ç¼–ç å™¨è¿›è¡Œæœ‰æ•ˆå¤„ç†ã€‚ç›¸è¾ƒäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCRNNï¼‰åç«¯ï¼ŒFOA-Conv3Dåœ¨ä¼°è®¡è¯¯å·®å’Œè§£é‡Šæ–¹å·®ï¼ˆPoVï¼‰æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ç”¨äºä¼°è®¡é¢‘ç‡å˜åŒ–çš„å£°å­¦å‚æ•°ï¼ŒåŒ…æ‹¬æ··å“æ—¶é—´ã€ç›´è¾¾ä¸æ··å“æ¯”å’Œæ¸…æ™°åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶åŸºäºç¬¬ä¸€é˜¶Ambisonicså½•éŸ³ä½œä¸ºè¾“å…¥ï¼Œèƒ½ç›²ç›®ä¼°è®¡åä¸ªé¢‘ç‡å¸¦çš„å£°å­¦å‚æ•°ã€‚</li>
<li>æå‡ºäº†Spectro-Spatialåæ–¹å·®å‘é‡è¿™ä¸€æ–°ç‰¹æ€§ï¼Œæœ‰æ•ˆè¡¨ç¤ºFOAä¿¡å·çš„æ—¶ç©ºè°±å’Œç©ºé—´ä¿¡æ¯ã€‚</li>
<li>ä¸å•é€šé“æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨ä¼°è®¡å£°å­¦å‚æ•°æ–¹é¢çš„æ€§èƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>å¼•å…¥äº†FOA-Conv3Dè¿™ä¸€æ–°å‹åç«¯ç½‘ç»œï¼Œæ›´æœ‰æ•ˆåœ°å¤„ç†FOAä¿¡å·ç‰¹å¾ã€‚</li>
<li>FOA-Conv3Dåœ¨ä¼°è®¡è¯¯å·®å’Œè§£é‡Šæ–¹å·®æ–¹é¢ä¼˜äºå·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯å·ç§¯ç¥ç»ç½‘ç»œåç«¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.03172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2411.03172v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2411.03172v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2411.03172v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2411.03172v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Biodenoising-Animal-Vocalization-Denoising-without-Access-to-Clean-Data"><a href="#Biodenoising-Animal-Vocalization-Denoising-without-Access-to-Clean-Data" class="headerlink" title="Biodenoising: Animal Vocalization Denoising without Access to Clean Data"></a>Biodenoising: Animal Vocalization Denoising without Access to Clean Data</h2><p><strong>Authors:Marius Miron, Sara Keen, Jen-Yu Liu, Benjamin Hoffman, Masato Hagiwara, Olivier Pietquin, Felix Effenberger, Maddie Cusimano</strong></p>
<p>Animal vocalization denoising is a task similar to human speech enhancement, which is relatively well-studied. In contrast to the latter, it comprises a higher diversity of sound production mechanisms and recording environments, and this higher diversity is a challenge for existing models. Adding to the challenge and in contrast to speech, we lack large and diverse datasets comprising clean vocalizations. As a solution we use as training data pseudo-clean targets, i.e. pre-denoised vocalizations, and segments of background noise without a vocalization. We propose a train set derived from bioacoustics datasets and repositories representing diverse species, acoustic environments, geographic regions. Additionally, we introduce a non-overlapping benchmark set comprising clean vocalizations from different taxa and noise samples. We show that that denoising models (demucs, CleanUNet) trained on pseudo-clean targets obtained with speech enhancement models achieve competitive results on the benchmarking set. We publish data, code, libraries, and demos at <a target="_blank" rel="noopener" href="https://mariusmiron.com/research/biodenoising">https://mariusmiron.com/research/biodenoising</a>. </p>
<blockquote>
<p>åŠ¨ç‰©å‘å£°å»å™ªä¸äººç±»è¯­éŸ³å¢å¼ºä»»åŠ¡ç±»ä¼¼ï¼Œåè€…ç ”ç©¶ç›¸å¯¹æˆç†Ÿã€‚ç„¶è€Œï¼Œç›¸è¾ƒäºè¯­éŸ³å¢å¼ºï¼ŒåŠ¨ç‰©å‘å£°å»å™ªæ¶µç›–äº†æ›´å¤šç§ç±»çš„å£°éŸ³äº§ç”Ÿæœºåˆ¶å’Œå½•éŸ³ç¯å¢ƒï¼Œè¿™ç§å¤šæ ·æ€§å¯¹ç°æœ‰æ¨¡å‹æ„æˆæŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œä¸è¯­éŸ³ä¸åŒï¼Œæˆ‘ä»¬ç¼ºä¹åŒ…å«æ¸…æ™°å‘å£°çš„å¤§å‹å¤šæ ·åŒ–æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¼ªæ¸…æ´ç›®æ ‡ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå³é¢„å»å™ªå‘å£°å’Œæ— å‘å£°çš„èƒŒæ™¯å™ªå£°ç‰‡æ®µã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªåŸºäºç”Ÿç‰©å£°å­¦æ•°æ®é›†å’Œå­˜å‚¨åº“çš„è®­ç»ƒé›†ï¼Œæ¶µç›–å¤šç§ç‰©ç§ã€å£°å­¦ç¯å¢ƒå’Œåœ°ç†åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªéé‡å çš„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«æ¥è‡ªä¸åŒåˆ†ç±»ç¾¤çš„å¹²å‡€å‘å£°å’Œå™ªå£°æ ·æœ¬ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹è·å¾—çš„ä¼ªæ¸…æ´ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„å»å™ªæ¨¡å‹ï¼ˆå¦‚demucså’ŒCleanUNetï¼‰åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://mariusmiron.com/research/biodenoising">https://mariusmiron.com/research/biodenoising</a>ä¸Šå…¬å¼€äº†æ•°æ®ã€ä»£ç ã€åº“å’Œæ¼”ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03427v2">PDF</a> 5 pages, 2 tables</p>
<p><strong>Summary</strong></p>
<p>åŠ¨ç‰©å‘å£°å»å™ªä¸äººç±»è¯­éŸ³å¢å¼ºä»»åŠ¡ç±»ä¼¼ï¼Œä½†æ¶‰åŠæ›´å¹¿æ³›çš„å£°æºå’Œå½•éŸ³ç¯å¢ƒï¼Œå¢åŠ äº†å¤„ç†éš¾åº¦ã€‚ç”±äºç¼ºä¹åŒ…å«æ¸…æ™°å‘å£°çš„å¤§å‹å¤šå…ƒæ•°æ®é›†ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¼ªæ¸…æ´ç›®æ ‡ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œå³é¢„å»å™ªå‘å£°å’Œæ— å‘å£°çš„èƒŒæ™¯å™ªå£°ç‰‡æ®µã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªåŸºäºç”Ÿç‰©å£°å­¦æ•°æ®é›†å’Œä»£è¡¨å„ç§ç‰©ç§ã€å£°å­¦ç¯å¢ƒåŠåœ°ç†åŒºåŸŸçš„èµ„æ–™åº“çš„è®­ç»ƒé›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªä¸åŒ…å«é‡å å‘å£°çš„åŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«æ¥è‡ªä¸åŒåˆ†ç±»ç¾¤å’Œå™ªå£°æ ·æœ¬çš„æ¸…æ´å‘å£°ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œåˆ©ç”¨è¯­éŸ³å¢å¼ºæ¨¡å‹è·å¾—çš„ä¼ªæ¸…æ´ç›®æ ‡è®­ç»ƒçš„é™å™ªæ¨¡å‹ï¼ˆå¦‚demucså’ŒCleanUNetï¼‰åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚ç›¸å…³æ•°æ®ã€ä»£ç ã€åº“å’Œæ¼”ç¤ºå†…å®¹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://mariusmiron.com/research/biodenoising%E3%80%82">https://mariusmiron.com/research/biodenoisingã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠ¨ç‰©å‘å£°å»å™ªä»»åŠ¡ç±»ä¼¼äºäººç±»è¯­éŸ³å¢å¼ºï¼Œä½†é¢ä¸´æ›´é«˜çš„å£°éŸ³å¤šæ ·æ€§å’Œå½•éŸ³ç¯å¢ƒæŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹å¤§å‹å¤šå…ƒæ•°æ®é›†ï¼Œå› æ­¤ä½¿ç”¨ä¼ªæ¸…æ´ç›®æ ‡ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚</li>
<li>å¼•å…¥åŸºäºç”Ÿç‰©å£°å­¦æ•°æ®é›†å’Œå¹¿æ³›èµ„æ–™åº“çš„è®­ç»ƒé›†ã€‚</li>
<li>æå‡ºä¸åŒ…å«é‡å å‘å£°çš„åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>é™å™ªæ¨¡å‹ï¼ˆå¦‚demucså’ŒCleanUNetï¼‰åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—ç«äº‰åŠ›ç»“æœã€‚</li>
<li>ä¼ªæ¸…æ´ç›®æ ‡æ˜¯é€šè¿‡è¯­éŸ³å¢å¼ºæ¨¡å‹è·å¾—çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2410.03427v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2410.03427v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2410.03427v2/page_3_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Leveraging-Joint-Spectral-and-Spatial-Learning-with-MAMBA-for-Multichannel-Speech-Enhancement"><a href="#Leveraging-Joint-Spectral-and-Spatial-Learning-with-MAMBA-for-Multichannel-Speech-Enhancement" class="headerlink" title="Leveraging Joint Spectral and Spatial Learning with MAMBA for   Multichannel Speech Enhancement"></a>Leveraging Joint Spectral and Spatial Learning with MAMBA for   Multichannel Speech Enhancement</h2><p><strong>Authors:Wenze Ren, Haibin Wu, Yi-Cheng Lin, Xuanjun Chen, Rong Chao, Kuo-Hsuan Hung, You-Jin Li, Wen-Yuan Ting, Hsin-Min Wang, Yu Tsao</strong></p>
<p>In multichannel speech enhancement, effectively capturing spatial and spectral information across different microphones is crucial for noise reduction. Traditional methods, such as CNN or LSTM, attempt to model the temporal dynamics of full-band and sub-band spectral and spatial features. However, these approaches face limitations in fully modeling complex temporal dependencies, especially in dynamic acoustic environments. To overcome these challenges, we modify the current advanced model McNet by introducing an improved version of Mamba, a state-space model, and further propose MCMamba. MCMamba has been completely reengineered to integrate full-band and narrow-band spatial information with sub-band and full-band spectral features, providing a more comprehensive approach to modeling spatial and spectral information. Our experimental results demonstrate that MCMamba significantly improves the modeling of spatial and spectral features in multichannel speech enhancement, outperforming McNet and achieving state-of-the-art performance on the CHiME-3 dataset. Additionally, we find that Mamba performs exceptionally well in modeling spectral information. </p>
<blockquote>
<p>åœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­ï¼Œæœ‰æ•ˆæ•æ‰ä¸åŒéº¦å…‹é£é—´çš„ç©ºé—´å’Œæ—¶é—´è°±ä¿¡æ¯å¯¹äºé™å™ªè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ï¼Œè¯•å›¾å¯¹å…¨é¢‘å¸¦å’Œå­é¢‘å¸¦çš„æ—¶é—´åŠ¨æ€è¿›è¡Œå»ºæ¨¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å®Œå…¨å»ºæ¨¡å¤æ‚çš„æ—¶é—´ä¾èµ–æ€§æ–¹é¢é¢ä¸´å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€å£°å­¦ç¯å¢ƒä¸­ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ”¹è¿›çš„MambaçŠ¶æ€ç©ºé—´æ¨¡å‹æ¥ä¿®æ”¹å½“å‰å…ˆè¿›çš„McNetæ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†MCMambaã€‚MCMambaè¿›è¡Œäº†å…¨é¢æ”¹é€ ï¼Œèåˆäº†å…¨é¢‘å¸¦å’Œçª„é¢‘å¸¦ç©ºé—´ä¿¡æ¯ä¸å­é¢‘å¸¦å’Œå…¨é¢‘å¸¦é¢‘è°±ç‰¹å¾ï¼Œä¸ºå»ºæ¨¡ç©ºé—´å’Œé¢‘è°±ä¿¡æ¯æä¾›äº†æ›´å…¨é¢çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMCMambaåœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºçš„ç©ºé—´å’Œé¢‘è°±ç‰¹å¾å»ºæ¨¡ä¸Šæœ‰äº†æ˜¾è‘—æ”¹è¿›ï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†McNetï¼Œå¹¶åœ¨CHiME-3æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°Mambaåœ¨å»ºæ¨¡é¢‘è°±ä¿¡æ¯æ–¹é¢çš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10376v2">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­ç©ºé—´ä¸å…‰è°±ä¿¡æ¯æ•æ‰çš„é‡è¦æ€§ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•å¦‚CNNå’ŒLSTMåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹çš„å±€é™æ€§ï¼Œæ”¹è¿›äº†ç°æœ‰å…ˆè¿›æ¨¡å‹McNetï¼Œå¼•å…¥äº†Mambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ”¹è¿›ç‰ˆï¼Œæå‡ºMCMambaæ¨¡å‹ã€‚MCMambaæ¨¡å‹é‡æ–°æ•´åˆäº†å…¨é¢‘å¸¦å’Œçª„é¢‘å¸¦ç©ºé—´ä¿¡æ¯ä¸å­é¢‘å¸¦å’Œå…¨é¢‘å¸¦å…‰è°±ç‰¹å¾ï¼Œä¸ºç©ºé—´ä¸å…‰è°±ä¿¡æ¯çš„å»ºæ¨¡æä¾›äº†æ›´å…¨é¢çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMCMambaåœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­æ˜¾è‘—æé«˜äº†ç©ºé—´ä¸å…‰è°±ç‰¹å¾çš„å»ºæ¨¡æ•ˆæœï¼Œåœ¨CHiME-3æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼ŒMambaåœ¨å…‰è°±ä¿¡æ¯å»ºæ¨¡ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­ï¼Œæ•æ‰ç©ºé—´ä¸å…‰è°±ä¿¡æ¯æ˜¯å…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸‹ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚CNNå’ŒLSTMåœ¨å»ºæ¨¡å¤æ‚çš„æ—¶é—´ä¾èµ–æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ”¹è¿›äº†ç°æœ‰å…ˆè¿›æ¨¡å‹McNetï¼Œå¼•å…¥Mambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ”¹è¿›ç‰ˆï¼Œå½¢æˆMCMambaæ¨¡å‹ã€‚</li>
<li>MCMambaæ¨¡å‹æ•´åˆäº†å…¨é¢‘å¸¦å’Œçª„é¢‘å¸¦ç©ºé—´ä¿¡æ¯ä»¥åŠå­é¢‘å¸¦å’Œå…¨é¢‘å¸¦å…‰è°±ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºMCMambaåœ¨å¤šé€šé“è¯­éŸ³å¢å¼ºä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨CHiME-3æ•°æ®é›†ä¸Šã€‚</li>
<li>MCMambaæ˜¾è‘—æé«˜äº†ç©ºé—´ä¸å…‰è°±ç‰¹å¾çš„å»ºæ¨¡æ•ˆæœï¼Œå¯¹æ¯”McNetæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.10376v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.10376v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.10376v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.10376v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Effective-Integration-of-KAN-for-Keyword-Spotting"><a href="#Effective-Integration-of-KAN-for-Keyword-Spotting" class="headerlink" title="Effective Integration of KAN for Keyword Spotting"></a>Effective Integration of KAN for Keyword Spotting</h2><p><strong>Authors:Anfeng Xu, Biqiao Zhang, Shuyu Kong, Yiteng Huang, Zhaojun Yang, Sangeeta Srivastava, Ming Sun</strong></p>
<p>Keyword spotting (KWS) is an important speech processing component for smart devices with voice assistance capability. In this paper, we investigate if Kolmogorov-Arnold Networks (KAN) can be used to enhance the performance of KWS. We explore various approaches to integrate KAN for a model architecture based on 1D Convolutional Neural Networks (CNN). We find that KAN is effective at modeling high-level features in lower-dimensional spaces, resulting in improved KWS performance when integrated appropriately. The findings shed light on understanding KAN for speech processing tasks and on other modalities for future researchers. </p>
<blockquote>
<p>å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰æ˜¯æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹è®¾å¤‡çš„é‡è¦è¯­éŸ³å¤„ç†ç»„ä»¶ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ˜¯å¦å¯ä»¥æé«˜KWSçš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¢ç´¢äº†å„ç§æ–¹æ³•ï¼Œä»¥å°†KANé›†æˆåˆ°åŸºäºä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹æ¶æ„ä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä½ç»´ç©ºé—´ä¸­å»ºæ¨¡é«˜çº§ç‰¹å¾æ—¶ï¼ŒKANå…·æœ‰å¾ˆå¼ºçš„æ•ˆæœï¼Œå¹¶ä¸”åœ¨é€‚å½“é›†æˆåï¼Œå¯ä»¥æ”¹å–„KWSçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæœªæ¥ç ”ç©¶äººå‘˜äº†è§£KANåœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå¹¶ä¸ºå…¶ä»–æ¨¡å¼æä¾›å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08605v3">PDF</a> Accepted to ICASSP 2025</p>
<p><strong>æ‘˜è¦</strong><br>è®ºæ–‡ç ”ç©¶äº†Kolmogorov-Arnold Networksï¼ˆKANï¼‰åœ¨å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ä¸­çš„ä½œç”¨ã€‚è¯¥ç ”ç©¶å‘ç°ï¼Œé€šè¿‡åˆç†æ•´åˆï¼ŒKANèƒ½æå‡CNNæ¨¡å‹åœ¨KWSä¸Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°åœ¨è¾ƒä½ç»´åº¦çš„ç©ºé—´ä¸­æ¨¡æ‹Ÿé«˜çº§ç‰¹å¾ã€‚è¯¥ç ”ç©¶çš„æˆæœå¯¹äºç†è§£KANåœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ä»¥åŠå¯¹æœªæ¥ç ”ç©¶è€…çš„å…¶ä»–æ¨¡æ€ç ”ç©¶éƒ½æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¢è®¨äº†Kolmogorov-Arnold Networksï¼ˆKANï¼‰åœ¨è¯­éŸ³å¤„ç†é¢†åŸŸçš„æ½œåœ¨åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ä¸å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ç»“åˆæ—¶çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶åˆ©ç”¨äº†å…·æœ‰è¯­éŸ³åŠ©æ‰‹åŠŸèƒ½çš„æ™ºèƒ½è®¾å¤‡çš„è¯­éŸ³å¤„ç†ç»„ä»¶ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œé€‚å½“æ•´åˆåï¼ŒKANèƒ½æœ‰æ•ˆæé«˜åŸºäºä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹æ¶æ„åœ¨å…³é”®è¯è¯†åˆ«æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>KANèƒ½å¤Ÿåœ¨è¾ƒä½ç»´åº¦çš„ç©ºé—´ä¸­æ¨¡æ‹Ÿé«˜çº§ç‰¹å¾ï¼Œè¿™å¯¹äºè¯­éŸ³å¤„ç†ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹äºç†è§£KANåœ¨è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶è€…æä¾›äº†å¯¹å…¶ä»–æ¨¡æ€ç ”ç©¶çš„å¯ç¤ºå’Œå¯èƒ½çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_1_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2409.08605v3/page_3_2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Improving-Zero-Shot-Chinese-English-Code-Switching-ASR-with-kNN-CTC-and-Gated-Monolingual-Datastores"><a href="#Improving-Zero-Shot-Chinese-English-Code-Switching-ASR-with-kNN-CTC-and-Gated-Monolingual-Datastores" class="headerlink" title="Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and   Gated Monolingual Datastores"></a>Improving Zero-Shot Chinese-English Code-Switching ASR with kNN-CTC and   Gated Monolingual Datastores</h2><p><strong>Authors:Jiaming Zhou, Shiwan Zhao, Hui Wang, Tian-Hao Zhang, Haoqin Sun, Xuechen Wang, Yong Qin</strong></p>
<p>The kNN-CTC model has proven to be effective for monolingual automatic speech recognition (ASR). However, its direct application to multilingual scenarios like code-switching, presents challenges. Although there is potential for performance improvement, a kNN-CTC model utilizing a single bilingual datastore can inadvertently introduce undesirable noise from the alternative language. To address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR) framework that employs dual monolingual datastores and a gated datastore selection mechanism to reduce noise interference. Our method selects the appropriate datastore for decoding each frame, ensuring the injection of language-specific information into the ASR process. We apply this framework to cutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive experiments demonstrate the remarkable effectiveness of our gated datastore mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR. </p>
<blockquote>
<p>kNN-CTCæ¨¡å‹åœ¨å•è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ã€‚ç„¶è€Œï¼Œå°†å…¶ç›´æ¥åº”ç”¨äºå¤šè¯­è¨€åœºæ™¯ï¼Œå¦‚ä»£ç åˆ‡æ¢ï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚å°½ç®¡æœ‰æé«˜æ€§èƒ½çš„æ½œåŠ›ï¼Œä½†ä½¿ç”¨å•ä¸€åŒè¯­æ•°æ®å­˜å‚¨çš„kNN-CTCæ¨¡å‹å¯èƒ½ä¼šæ— æ„ä¸­å¼•å…¥æ¥è‡ªå¦ä¸€ç§è¯­è¨€çš„ä¸å¸Œæœ›æœ‰çš„å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºkNN-CTCçš„æ–°å‹ä»£ç åˆ‡æ¢ASRï¼ˆCS-ASRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨åŒå•è¯­æ•°æ®å­˜å‚¨å’Œå—æ§æ•°æ®å­˜å‚¨é€‰æ‹©æœºåˆ¶æ¥å‡å°‘å™ªå£°å¹²æ‰°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‰æ‹©é€‚å½“çš„å­˜å‚¨åº“æ¥è§£ç æ¯ä¸€å¸§ï¼Œç¡®ä¿è¯­è¨€ç‰¹å®šä¿¡æ¯çš„æ³¨å…¥åˆ°ASRè¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬å°†æ­¤æ¡†æ¶åº”ç”¨äºæœ€æ–°çš„CTCæ¨¡å‹ï¼Œå¼€å‘äº†ä¸€ä¸ªå…ˆè¿›çš„CS-ASRç³»ç»Ÿã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„å—æ§æ•°æ®å­˜å‚¨æœºåˆ¶åœ¨é›¶ä¸­æ–‡-è‹±æ–‡åˆ‡æ¢çš„CS-ASRæ€§èƒ½æå‡ä¸­æ•ˆæœæ˜¾è‘—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03814v4">PDF</a> Accepted by ICASSP 2025</p>
<p><strong>æ€»ç»“</strong></p>
<p>KNN-CTCæ¨¡å‹åœ¨å•è¯­ç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤šè¯­ç§åœºæ™¯å¦‚ä»£ç åˆ‡æ¢ä¸­ç›´æ¥åº”ç”¨æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ä»å•ä¸€åŒè¯­æ•°æ®åº“ä¸­å¯èƒ½å¼•å…¥çš„å¹²æ‰°å™ªå£°é—®é¢˜ï¼Œæå‡ºäº†åŸºäºKNN-CTCçš„ä»£ç åˆ‡æ¢ASRï¼ˆCS-ASRï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒå•è¯­æ•°æ®åº“å’Œé—¨æ§æ•°æ®åº“é€‰æ‹©æœºåˆ¶ï¼Œå‡å°‘å™ªå£°å¹²æ‰°ï¼Œç¡®ä¿ä¸ºæ¯ä¸€å¸§è§£ç æ³¨å…¥ç‰¹å®šè¯­è¨€ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼Œé—¨æ§æ•°æ®åº“æœºåˆ¶å¯¹æå‡é›¶æ ·æœ¬ä¸­æ–‡-è‹±æ–‡CS-ASRçš„æ€§èƒ½æœ‰æ˜¾è‘—æ•ˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>KNN-CTCæ¨¡å‹åœ¨å•è¯­ç§ASRä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šè¯­ç§åœºæ™¯ï¼ˆå¦‚ä»£ç åˆ‡æ¢ï¼‰ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>å•ä¸€åŒè¯­æ•°æ®åº“å¯èƒ½å¼•å…¥å¹²æ‰°å™ªå£°çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„CS-ASRæ¡†æ¶é‡‡ç”¨åŒå•è¯­æ•°æ®åº“å’Œé—¨æ§é€‰æ‹©æœºåˆ¶ã€‚</li>
<li>è¯¥æœºåˆ¶èƒ½å‡å°‘å™ªå£°å¹²æ‰°ï¼Œç¡®ä¿ä¸ºæ¯ä¸€å¸§è§£ç æ³¨å…¥ç‰¹å®šè¯­è¨€ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜é—¨æ§æ•°æ®åº“æœºåˆ¶å¯¹æå‡æ€§èƒ½æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_2_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Speech/2406.03814v4/page_3_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-60988a4b5e7dbfb48d12443a876bf29d.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Generating and Detecting Various Types of Fake Image and Audio Content   A Review of Modern Deep Learning Technologies and Tools
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-dd0fc013160fe061c51062e2d26588ab.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  A Foundational Generative Model for Breast Ultrasound Image Analysis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17012.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
