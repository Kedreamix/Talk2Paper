<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  PokerBench Training Large Language Models to become Professional Poker   Players">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2bc79a8f10e94343da4935821411091d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-16-æ›´æ–°"><a href="#2025-01-16-æ›´æ–°" class="headerlink" title="2025-01-16 æ›´æ–°"></a>2025-01-16 æ›´æ–°</h1><h2 id="PokerBench-Training-Large-Language-Models-to-become-Professional-Poker-Players"><a href="#PokerBench-Training-Large-Language-Models-to-become-Professional-Poker-Players" class="headerlink" title="PokerBench: Training Large Language Models to become Professional Poker   Players"></a>PokerBench: Training Large Language Models to become Professional Poker   Players</h2><p><strong>Authors:Richard Zhuang, Akshat Gupta, Richard Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli</strong></p>
<p>We introduce PokerBench - a benchmark for evaluating the poker-playing abilities of large language models (LLMs). As LLMs excel in traditional NLP tasks, their application to complex, strategic games like poker poses a new challenge. Poker, an incomplete information game, demands a multitude of skills such as mathematics, reasoning, planning, strategy, and a deep understanding of game theory and human psychology. This makes Poker the ideal next frontier for large language models. PokerBench consists of a comprehensive compilation of 11,000 most important scenarios, split between pre-flop and post-flop play, developed in collaboration with trained poker players. We evaluate prominent models including GPT-4, ChatGPT 3.5, and various Llama and Gemma series models, finding that all state-of-the-art LLMs underperform in playing optimal poker. However, after fine-tuning, these models show marked improvements. We validate PokerBench by having models with different scores compete with each other, demonstrating that higher scores on PokerBench lead to higher win rates in actual poker games. Through gameplay between our fine-tuned model and GPT-4, we also identify limitations of simple supervised fine-tuning for learning optimal playing strategy, suggesting the need for more advanced methodologies for effectively training language models to excel in games. PokerBench thus presents a unique benchmark for a quick and reliable evaluation of the poker-playing ability of LLMs as well as a comprehensive benchmark to study the progress of LLMs in complex game-playing scenarios. The dataset and code will be made available at: \url{<a target="_blank" rel="noopener" href="https://github.com/pokerllm/pokerbench%7D">https://github.com/pokerllm/pokerbench}</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†PokerBenchâ€”â€”ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰‘å…‹ç‰ŒæŠ€èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼ ç»Ÿè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚æˆ˜ç•¥æ¸¸æˆå¦‚æ‰‘å…‹ä¸­çš„åº”ç”¨æ„æˆäº†ä¸€é¡¹æ–°æŒ‘æˆ˜ã€‚æ‰‘å…‹æ˜¯ä¸€ç§ä¿¡æ¯ä¸å®Œå…¨çš„æ¸¸æˆï¼Œéœ€è¦å¤šç§æŠ€èƒ½ï¼Œå¦‚æ•°å­¦ã€æ¨ç†ã€è§„åˆ’ã€æˆ˜ç•¥ï¼Œä»¥åŠå¯¹åšå¼ˆè®ºå’Œäººç±»å¿ƒç†å­¦çš„æ·±åˆ»ç†è§£ã€‚è¿™ä½¿å¾—æ‰‘å…‹æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç†æƒ³ä¸‹ä¸€ä¸ªå‰æ²¿é¢†åŸŸã€‚PokerBenchç”±æœ€é‡è¦11000ä¸ªåœºæ™¯çš„å…¨é¢æ±‡ç¼–ç»„æˆï¼Œè¿™äº›åœºæ™¯åˆ†ä¸ºå¼€æ‹å’Œé—­æ‹ä¸¤ç§ï¼Œæ˜¯ä¸è®­ç»ƒè¿‡çš„æ‰‘å…‹ç©å®¶å…±åŒå¼€å‘çš„ã€‚æˆ‘ä»¬è¯„ä¼°äº†åŒ…æ‹¬GPT-4ã€ChatGPT 3.5ä»¥åŠå„ç§Llamaå’ŒGemmaç³»åˆ—æ¨¡å‹åœ¨å†…çš„çªå‡ºæ¨¡å‹ï¼Œå‘ç°æ‰€æœ‰æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°éƒ½ä¸ä½³ã€‚ç„¶è€Œï¼Œç»è¿‡å¾®è°ƒåï¼Œè¿™äº›æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—æ”¹å–„ã€‚æˆ‘ä»¬é€šè¿‡è®©ä¸åŒå¾—åˆ†çš„æ¨¡å‹ç›¸äº’ç«äº‰æ¥éªŒè¯PokerBenchï¼Œç»“æœè¡¨æ˜åœ¨PokerBenchä¸Šå¾—åˆ†è¾ƒé«˜çš„æ¨¡å‹åœ¨å®é™…çš„æ‰‘å…‹æ¸¸æˆä¸­çš„èƒœç‡ä¹Ÿè¾ƒé«˜ã€‚é€šè¿‡æˆ‘ä»¬å¾®è°ƒåçš„æ¨¡å‹ä¸GPT-4ä¹‹é—´çš„æ¸¸æˆï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ç®€å•ç›‘ç£å¾®è°ƒåœ¨å­¦ä¹ æœ€ä¼˜ç­–ç•¥æ–¹é¢çš„å±€é™æ€§ï¼Œè¿™æç¤ºæˆ‘ä»¬éœ€è¦æ›´å…ˆè¿›çš„æ–¹æ³•æœ‰æ•ˆåœ°è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶åœ¨æ¸¸æˆä¸­è¡¨ç°å‡ºè‰²ã€‚å› æ­¤ï¼ŒPokerBenchæä¾›äº†ä¸€ä¸ªç‹¬ç‰¹ã€å¿«é€Ÿå¯é çš„åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰‘å…‹ç‰Œèƒ½åŠ›ï¼Œä»¥åŠä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ¥ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¸¸æˆåœºæ™¯ä¸­çš„è¿›å±•ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨ä»¥ä¸‹ç½‘å€æä¾›ï¼š<a target="_blank" rel="noopener" href="https://github.com/pokerllm/pokerbench">https://github.com/pokerllm/pokerbench</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08328v1">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PokerBenchâ€”â€”ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰‘å…‹ç‰ŒæŠ€èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚æ–‡ç« æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰‘å…‹ç‰Œè¿™ç±»å¤æ‚æˆ˜ç•¥æ¸¸æˆä¸­çš„è¡¨ç°é¢ä¸´æ–°çš„æŒ‘æˆ˜ã€‚PokerBenchåŒ…å«äº†ä¸è®­ç»ƒæœ‰ç´ çš„æ‰‘å…‹ç©å®¶åˆä½œå¼€å‘çš„æœ€ä¸ºå…³é”®çš„1.1ä¸‡ä¸ªåœºæ™¯ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿åœ¨fine-tuningä¹‹åï¼Œå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°ä»ç„¶ä¸ç†æƒ³ã€‚é€šè¿‡ä¸åŒå¾—åˆ†æ¨¡å‹çš„ç«äº‰éªŒè¯ï¼Œä»¥åŠç²¾ç»†è°ƒæ•´æ¨¡å‹ä¸GPT-4çš„å¯¹æˆ˜ï¼Œæ­ç¤ºäº†ç®€å•ç›‘ç£fine-tuningåœ¨å­¦ä¹ æœ€ä½³ç­–ç•¥æ–¹é¢çš„å±€é™æ€§ã€‚å› æ­¤ï¼ŒPokerBenchæˆä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ‰‘å…‹ç‰Œèƒ½åŠ›çš„ç‹¬ç‰¹åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸ºå¤æ‚æ¸¸æˆåœºæ™¯ä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•è¿›æ­¥æä¾›äº†ç»¼åˆè¯„ä¼°æ‰‹æ®µã€‚æ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒåœ¨æŒ‡å®šé“¾æ¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†PokerBenchä½œä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ‰‘å…‹ç‰ŒæŠ€èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰‘å…‹ç‰Œè¿™ç±»å¤æ‚æˆ˜ç•¥æ¸¸æˆä¸­çš„è¡¨ç°é¢ä¸´æ–°çš„æŒ‘æˆ˜ã€‚</li>
<li>PokerBenchåŒ…å«ä¸è®­ç»ƒæœ‰ç´ çš„æ‰‘å…‹ç©å®¶åˆä½œçš„1.1ä¸‡ä¸ªå…³é”®åœºæ™¯ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰‘å…‹æ¸¸æˆä¸­çš„è¡¨ç°ä¸ç†æƒ³ï¼Œä½†fine-tuningåæœ‰æ‰€æ”¹å–„ã€‚</li>
<li>é€šè¿‡ä¸åŒå¾—åˆ†æ¨¡å‹çš„ç«äº‰éªŒè¯ï¼Œè¡¨æ˜é«˜åˆ†çš„PokerBenchæ¨¡å‹åœ¨çœŸå®æ‰‘å…‹æ¸¸æˆä¸­çš„èƒœç‡æ›´é«˜ã€‚</li>
<li>ä¸GPT-4çš„å¯¹æˆ˜æ­ç¤ºäº†ç®€å•ç›‘ç£fine-tuningåœ¨å­¦ä¹ æœ€ä½³ç­–ç•¥æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19a8f70d179c72cc444b497a51d7f06f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dd840fce7fc6b1b5ae5729aea836942.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67e8f24a1224d45257aabf6638829b23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9979c4fba8eca08aa01a5023e6108381.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64c899e3703a9ddcdec5cb3ef887a9a4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Omni-RGPT-Unifying-Image-and-Video-Region-level-Understanding-via-Token-Marks"><a href="#Omni-RGPT-Unifying-Image-and-Video-Region-level-Understanding-via-Token-Marks" class="headerlink" title="Omni-RGPT: Unifying Image and Video Region-level Understanding via Token   Marks"></a>Omni-RGPT: Unifying Image and Video Region-level Understanding via Token   Marks</h2><p><strong>Authors:Miran Heo, Min-Hung Chen, De-An Huang, Sifei Liu, Subhashree Radhakrishnan, Seon Joo Kim, Yu-Chiang Frank Wang, Ryo Hachiuma</strong></p>
<p>We present Omni-RGPT, a multimodal large language model designed to facilitate region-level comprehension for both images and videos. To achieve consistent region representation across spatio-temporal dimensions, we introduce Token Mark, a set of tokens highlighting the target regions within the visual feature space. These tokens are directly embedded into spatial regions using region prompts (e.g., boxes or masks) and simultaneously incorporated into the text prompt to specify the target, establishing a direct connection between visual and text tokens. To further support robust video understanding without requiring tracklets, we introduce an auxiliary task that guides Token Mark by leveraging the consistency of the tokens, enabling stable region interpretation across the video. Additionally, we introduce a large-scale region-level video instruction dataset (RegVID-300k). Omni-RGPT achieves state-of-the-art results on image and video-based commonsense reasoning benchmarks while showing strong performance in captioning and referring expression comprehension tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Omni-RGPTï¼Œè¿™æ˜¯ä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¿ƒè¿›å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸçº§ç†è§£ã€‚ä¸ºäº†å®ç°æ—¶ç©ºç»´åº¦ä¸Šçš„ä¸€è‡´åŒºåŸŸè¡¨ç¤ºï¼Œæˆ‘ä»¬å¼•å…¥äº†Token Markï¼Œè¿™æ˜¯ä¸€ç»„çªå‡ºæ˜¾ç¤ºè§†è§‰ç‰¹å¾ç©ºé—´å†…ç›®æ ‡åŒºåŸŸçš„ä»¤ç‰Œã€‚è¿™äº›ä»¤ç‰Œç›´æ¥ä½¿ç”¨åŒºåŸŸæç¤ºï¼ˆä¾‹å¦‚ï¼Œç›’å­æˆ–è’™ç‰ˆï¼‰åµŒå…¥åˆ°ç©ºé—´åŒºåŸŸä¸­ï¼Œå¹¶åŒæ—¶çº³å…¥æ–‡æœ¬æç¤ºä»¥æŒ‡å®šç›®æ ‡ï¼Œä»è€Œåœ¨è§†è§‰ä»¤ç‰Œå’Œæ–‡æœ¬ä»¤ç‰Œä¹‹é—´å»ºç«‹ç›´æ¥è”ç³»ã€‚ä¸ºäº†è¿›ä¸€æ­¥æ”¯æŒä¸éœ€è¦è½¨è¿¹çš„è§†é¢‘ç†è§£ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªè¾…åŠ©ä»»åŠ¡æ¥å¼•å¯¼Token Markï¼Œåˆ©ç”¨ä»¤ç‰Œçš„è¿ç»­æ€§ï¼Œå®ç°åœ¨è§†é¢‘ä¸­çš„ç¨³å®šåŒºåŸŸè§£é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤§è§„æ¨¡çš„åŒºåŸŸçº§è§†é¢‘æŒ‡ä»¤æ•°æ®é›†ï¼ˆRegVID-300kï¼‰ã€‚Omni-RGPTåœ¨å›¾åƒå’ŒåŸºäºè§†é¢‘çš„å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶åœ¨æè¿°å’ŒæŒ‡ä»£è¡¨è¾¾ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08326v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://miranheo.github.io/omni-rgpt/">https://miranheo.github.io/omni-rgpt/</a></p>
<p><strong>Summary</strong></p>
<p>Omni-RGPTæ˜¯ä¸€æ¬¾å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå®ç°å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸçº§ç†è§£ã€‚é€šè¿‡å¼•å…¥Token Markï¼Œå®ç°åœ¨æ—¶ç©ºç»´åº¦ä¸Šçš„åŒºåŸŸä¸€è‡´æ€§è¡¨ç¤ºã€‚Token Marké€šè¿‡åŒºåŸŸæç¤ºï¼ˆå¦‚ç›’å­æˆ–æ©è†œï¼‰ç›´æ¥åœ¨ç©ºé—´åŒºåŸŸå†…åµŒå…¥æ ‡è®°ï¼Œå¹¶é€šè¿‡æ–‡æœ¬æç¤ºæŒ‡å®šç›®æ ‡ï¼Œå»ºç«‹è§†è§‰å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚ä¸ºæ”¯æŒæ— éœ€è½¨è¿¹çš„è§†é¢‘ç†è§£ï¼Œé€šè¿‡åˆ©ç”¨æ ‡è®°çš„ä¸€è‡´æ€§ï¼Œå¼•å…¥è¾…åŠ©ä»»åŠ¡å¼•å¯¼Token Markï¼Œå®ç°è§†é¢‘ä¸­çš„åŒºåŸŸç¨³å®šè§£è¯»ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¤§è§„æ¨¡çš„åŒºåŸŸçº§è§†é¢‘æŒ‡ä»¤æ•°æ®é›†RegVID-300kã€‚Omni-RGPTåœ¨å›¾åƒå’Œè§†é¢‘å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼ŒåŒæ—¶åœ¨æè¿°å’ŒæŒ‡ä»£è¡¨è¾¾ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Omni-RGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°å›¾åƒå’Œè§†é¢‘çš„åŒºåŸŸçº§ç†è§£ã€‚</li>
<li>Token Markæ˜¯å¼•å…¥çš„ä¸€ç»„æ ‡è®°ï¼Œç”¨äºåœ¨è§†è§‰ç‰¹å¾ç©ºé—´ä¸­çªå‡ºç›®æ ‡åŒºåŸŸã€‚</li>
<li>é€šè¿‡åŒºåŸŸæç¤ºå’Œæ–‡æœ¬æç¤ºï¼ŒToken Markå»ºç«‹äº†è§†è§‰å’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„ç›´æ¥è”ç³»ã€‚</li>
<li>å¼•å…¥è¾…åŠ©ä»»åŠ¡ï¼Œåˆ©ç”¨æ ‡è®°çš„ä¸€è‡´æ€§ï¼Œæ”¯æŒç¨³å®šçš„åŒºåŸŸè§£è¯»ï¼Œæ— éœ€è½¨è¿¹å³å¯ç†è§£è§†é¢‘ã€‚</li>
<li>å¼•å…¥äº†å¤§è§„æ¨¡åŒºåŸŸçº§è§†é¢‘æŒ‡ä»¤æ•°æ®é›†RegVID-300kã€‚</li>
<li>Omni-RGPTåœ¨å›¾åƒå’Œè§†é¢‘å¸¸è¯†æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…ˆè¿›ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f810ae5cb59f6daae9905d97f7d40254.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f705d6a3fae393dbd183fc4b63f42e95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bc79a8f10e94343da4935821411091d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad065e7e1e95a10fdf75b28a4cb781f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b234e9dff1b51b8b4624d22fbf651bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5833537aa44ce898235bce02f83c37bb.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Automated-Interpretability-with-Output-Centric-Feature-Descriptions"><a href="#Enhancing-Automated-Interpretability-with-Output-Centric-Feature-Descriptions" class="headerlink" title="Enhancing Automated Interpretability with Output-Centric Feature   Descriptions"></a>Enhancing Automated Interpretability with Output-Centric Feature   Descriptions</h2><p><strong>Authors:Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva</strong></p>
<p>Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the modelâ€™s representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary â€œunembeddingâ€ head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be â€œdeadâ€. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ç‰¹å¾ï¼ˆå¦‚æ¤ç‰©æˆ–å¥å­çš„ç¬¬ä¸€ä¸ªè¯ï¼‰ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚è¿™äº›æè¿°æ˜¯é€šè¿‡æ¿€æ´»ç‰¹å¾çš„è¾“å…¥å¾—å‡ºçš„ï¼Œè¿™å¯èƒ½æ˜¯æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­çš„ä¸€ä¸ªç»´åº¦æˆ–æ–¹å‘ã€‚ç„¶è€Œï¼Œç¡®å®šæ¿€æ´»è¾“å…¥æˆæœ¬å¾ˆé«˜ï¼Œè€Œç‰¹å¾åœ¨æ¨¡å‹è¡Œä¸ºä¸­çš„æœºæ¢°ä½œç”¨å–å†³äºè¾“å…¥å¦‚ä½•å¯¼è‡´ç‰¹å¾è¢«æ¿€æ´»ä»¥åŠç‰¹å¾æ¿€æ´»å¦‚ä½•å½±å“è¾“å‡ºã€‚é€šè¿‡è½¬å‘è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å½“å‰çš„ç®¡é“æä¾›çš„æè¿°æœªèƒ½æ•æ‰åˆ°ç‰¹å¾å¯¹è¾“å‡ºçš„å› æœæ•ˆåº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆã€ä»¥è¾“å‡ºä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆç‰¹å¾æè¿°ã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨ç‰¹å¾åˆºæ¿€åæƒé‡æ›´é«˜çš„ä»¤ç‰Œï¼Œæˆ–ç›´æ¥åº”ç”¨è¯æ±‡â€œunembeddingâ€å¤´åˆ°ç‰¹å¾ä»¥è·å¾—æœ€é«˜æƒé‡çš„ä»¤ç‰Œã€‚æˆ‘ä»¬çš„ä»¥è¾“å‡ºä¸ºä¸­å¿ƒçš„æè¿°æ¯”ä»¥è¾“å…¥ä¸ºä¸­å¿ƒçš„æè¿°æ›´èƒ½æ•æ‰ç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„å› æœæ•ˆåº”ï¼Œä½†ä¸¤è€…ç›¸ç»“åˆåœ¨è¾“å…¥å’Œè¾“å‡ºè¯„ä¼°æ–¹é¢çš„è¡¨ç°éƒ½æœ€ä½³ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†ä»¥è¾“å‡ºä¸ºä¸­å¿ƒçš„æè¿°å¯ä»¥ç”¨äºæ‰¾åˆ°ä»¥å‰è¢«è®¤ä¸ºæ˜¯â€œæ­»â€çš„ç‰¹å¾çš„æ¿€æ´»è¾“å…¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08319v1">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨è§£é‡Šæ€§ç®¡é“ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ¦‚å¿µç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ï¼Œè¿™äº›æè¿°æ­ç¤ºäº†æ¨¡å‹çš„è¿ä½œæ–¹å¼ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£é‡Šæ€§ç®¡é“éš¾ä»¥æ•æ‰ç‰¹å¾å¯¹è¾“å‡ºçš„å› æœæ•ˆåº”ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºè¾“å‡ºçš„ç‰¹å¾æè¿°æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ­ç¤ºç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚ç»“åˆè¾“å…¥å’Œè¾“å‡ºçš„æè¿°èƒ½å–å¾—æœ€ä½³æ•ˆæœï¼Œå¹¶å¯ç”¨æ¥å¯»æ‰¾æ¿€æ´»é‚£äº›è¢«è®¤ä¸ºä¸æ´»è·ƒçš„æ¨¡å‹ç‰¹å¾ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ¬æ–‡å¼ºè°ƒäº†è‡ªåŠ¨åŒ–è§£é‡Šæ€§ç®¡é“å¯¹ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨è§£é‡Šæ€§ç®¡é“ä¸ºå¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ¦‚å¿µç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚</li>
<li>æè¿°æ˜¯åŸºäºè¾“å…¥æ¿€æ´»çš„ç‰¹å¾ç”Ÿæˆçš„ï¼Œä½†è¿™ç§æ–¹æ³•æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æè¿°éœ€è¦æ•æ‰ç‰¹å¾å¯¹è¾“å‡ºçš„å› æœæ•ˆåº”ï¼Œç°æœ‰æ–¹æ³•éš¾ä»¥åšåˆ°ã€‚</li>
<li>è¾“å‡ºä¸ºä¸­å¿ƒçš„ç‰¹å¾æè¿°æ–¹æ³•èƒ½æ›´å¥½åœ°æ­ç¤ºç‰¹å¾å¯¹æ¨¡å‹è¾“å‡ºçš„å½±å“ã€‚</li>
<li>ç»“åˆè¾“å…¥å’Œè¾“å‡ºçš„æè¿°å¯ä»¥æé«˜è§£é‡Šçš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14fe6919654e22a5e0f1e7377bcbb94d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28e8f6ead290d716a0ba45cde6591eeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b12c14eed4ede6e0e8e8799d3ca83059.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="HALoGEN-Fantastic-LLM-Hallucinations-and-Where-to-Find-Them"><a href="#HALoGEN-Fantastic-LLM-Hallucinations-and-Where-to-Find-Them" class="headerlink" title="HALoGEN: Fantastic LLM Hallucinations and Where to Find Them"></a>HALoGEN: Fantastic LLM Hallucinations and Where to Find Them</h2><p><strong>Authors:Abhilasha Ravichander, Shrusti Ghela, David Wadden, Yejin Choi</strong></p>
<p>Despite their impressive ability to generate high-quality and fluent text, generative large language models (LLMs) also produce hallucinations: statements that are misaligned with established world knowledge or provided input context. However, measuring hallucination can be challenging, as having humans verify model generations on-the-fly is both expensive and time-consuming. In this work, we release HALoGEN, a comprehensive hallucination benchmark consisting of: (1) 10,923 prompts for generative models spanning nine domains including programming, scientific attribution, and summarization, and (2) automatic high-precision verifiers for each use case that decompose LLM generations into atomic units, and verify each unit against a high-quality knowledge source. We use this framework to evaluate ~150,000 generations from 14 language models, finding that even the best-performing models are riddled with hallucinations (sometimes up to 86% of generated atomic facts depending on the domain). We further define a novel error classification for LLM hallucinations based on whether they likely stem from incorrect recollection of training data (Type A errors), or incorrect knowledge in training data (Type B errors), or are fabrication (Type C errors). We hope our framework provides a foundation to enable the principled study of why generative models hallucinate, and advances the development of trustworthy large language models. </p>
<blockquote>
<p>å°½ç®¡ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œæµç•…çš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šäº§ç”Ÿå¹»è§‰ï¼šå³ä¸æ—¢å®šä¸–ç•ŒçŸ¥è¯†æˆ–æä¾›çš„è¾“å…¥ä¸Šä¸‹æ–‡ä¸åŒ¹é…çš„é™ˆè¿°ã€‚ç„¶è€Œï¼Œè¡¡é‡å¹»è§‰æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºè®©äººå·¥å®æ—¶éªŒè¯æ¨¡å‹çš„ç”Ÿæˆæ—¢æ˜‚è´µåˆè€—æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘å¸ƒäº†HALoGENï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¹»è§‰è¯„ä¼°åŸºå‡†ï¼ŒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰æ¶µç›–ç¼–ç¨‹ã€ç§‘å­¦å½’å±å’Œæ‘˜è¦ç­‰ä¹ä¸ªé¢†åŸŸçš„10923ä¸ªç”Ÿæˆæ¨¡å‹æç¤ºï¼›ï¼ˆ2ï¼‰é’ˆå¯¹æ¯ä¸ªç”¨ä¾‹çš„è‡ªåŠ¨é«˜ç²¾åº¦éªŒè¯å™¨ï¼Œå®ƒå°†LLMç”Ÿæˆç‰©åˆ†è§£æˆåŸå­å•ä½ï¼Œå¹¶é’ˆå¯¹é«˜è´¨é‡çš„çŸ¥è¯†æ¥æºå¯¹æ¯ä¸ªå•ä½è¿›è¡ŒéªŒè¯ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ¡†æ¶è¯„ä¼°äº†å¤§çº¦15ä¸‡ä¸ªæ¥è‡ª14ä¸ªè¯­è¨€æ¨¡å‹çš„ç”Ÿæˆç‰©ï¼Œå‘ç°å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¹Ÿå……æ–¥ç€å¹»è§‰ï¼ˆåœ¨æŸäº›é¢†åŸŸï¼Œç”Ÿæˆçš„åŸå­äº‹å®ä¸­æœ‰æ—¶é«˜è¾¾86%æ˜¯å¹»è§‰ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®šä¹‰äº†LLMå¹»è§‰çš„æ–°å‹é”™è¯¯åˆ†ç±»ï¼ŒåŸºäºå®ƒä»¬æ˜¯å¦å¯èƒ½æºäºè®­ç»ƒæ•°æ®çš„ä¸æ­£ç¡®å›å¿†ï¼ˆAç±»é”™è¯¯ï¼‰ï¼Œæˆ–è®­ç»ƒæ•°æ®ä¸­çš„é”™è¯¯çŸ¥è¯†ï¼ˆBç±»é”™è¯¯ï¼‰ï¼Œæˆ–æ˜¯è™šæ„ï¼ˆCç±»é”™è¯¯ï¼‰ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿä¸ºç ”ç©¶ç”Ÿæˆæ¨¡å‹ä¸ºä½•ä¼šäº§ç”Ÿå¹»è§‰æä¾›åŸºç¡€ï¼Œå¹¶æ¨åŠ¨å¼€å‘å¯ä¿¡èµ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08292v1">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¹»è§‰é—®é¢˜ã€‚è™½ç„¶LLMèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œæµç•…çš„æ–‡æœ¬ï¼Œä½†å®ƒä»¬ä¹Ÿä¼šäº§ç”Ÿä¸æ—¢å®šä¸–ç•ŒçŸ¥è¯†æˆ–æä¾›è¾“å…¥ä¸Šä¸‹æ–‡ä¸åŒ¹é…çš„å¹»è§‰ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºHALoGENçš„ç»¼åˆå¹»è§‰è¯„ä¼°å·¥å…·ï¼ŒåŒ…æ‹¬ç”¨äºç”Ÿæˆæ¨¡å‹çš„10923ä¸ªæç¤ºå’Œé’ˆå¯¹æ¯ä¸ªç”¨ä¾‹è‡ªåŠ¨è¿›è¡Œç²¾ç¡®éªŒè¯çš„éªŒè¯å™¨ã€‚é€šè¿‡å¯¹å¤§çº¦æ¥è‡ªä½¿ç”¨æ­¤æ¡†æ¶è¿›è¡Œè¯„ä¼°çš„è¯­è¨€æ¨¡å‹çš„ç”Ÿå­˜è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æœ€å¥½çš„æ¨¡å‹ä¸­ï¼Œä¹Ÿå­˜åœ¨å¤§é‡çš„å¹»è§‰ç°è±¡ï¼ˆåœ¨æŸäº›é¢†åŸŸé«˜è¾¾ç”Ÿæˆçš„åŸå­äº‹å®çš„ç™¾åˆ†ä¹‹å…«åå…­ï¼‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å®šä¹‰äº†ä¸€ç§æ–°å‹é”™è¯¯åˆ†ç±»ï¼Œæ ¹æ®æ˜¯å¦å¯èƒ½æºäºè®­ç»ƒæ•°æ®çš„é”™è¯¯å›å¿†ï¼ˆç±»å‹Aé”™è¯¯ï¼‰ã€è®­ç»ƒæ•°æ®ä¸­çš„é”™è¯¯çŸ¥è¯†ï¼ˆç±»å‹Bé”™è¯¯ï¼‰æˆ–ä¼ªé€ ï¼ˆç±»å‹Cé”™è¯¯ï¼‰ï¼Œæ¥å¯¹LLMå¹»è§‰è¿›è¡Œåˆ†ç±»ã€‚ä½œè€…å¸Œæœ›é€šè¿‡å…¶æ¡†æ¶æ·±å…¥ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹å‡ºç°å¹»è§‰çš„åŸå› ï¼Œå¹¶æ¨åŠ¨å¼€å‘å¯ä¿¡èµ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥è®ºæ–‡å¯¹äºäº†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹å’Œå±€é™æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡è¯„ä¼°å›°éš¾ï¼Œä½†ä»æå‡ºäº†è§£å†³æ–¹æ¡ˆå¹¶åˆ†äº«äº†ç›¸å…³æ•°æ®å’Œåˆ†æã€‚å°½ç®¡è¯¥é¢†åŸŸä»ç„¶éœ€è¦æ·±å…¥ç ”ç©¶å’Œåˆ†ææ‰èƒ½è§£å†³é—®é¢˜ã€‚ä½†ä»ç„¶æ¨åŠ¨äº†åŸºäºé«˜çº§åˆ«å¯ä¿¡æ€§ç†è§£çš„å‘å±•ç­–ç•¥çš„æ–¹å‘ç ”ç©¶å‘å±•å…·æœ‰å‰ç»æ€§ä»·å€¼çš„ç ”ç©¶ï¼Œå¸®åŠ©ç†è§£è¿™äº›ç°è±¡å‘ç”Ÿçš„åŸå› åŠå…¶é•¿æœŸå½±å“ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶ä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„åŸºç¡€ã€‚</p>
<p><strong>å…³é”®å‘ç°åˆ—è¡¨</strong>ï¼š</p>
<p>ä»¥ä¸‹æ˜¯è¿™ç¯‡è®ºæ–‡çš„å‡ ä¸ªä¸»è¦å‘ç°å’Œé‡ç‚¹è§‚ç‚¹ï¼š</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fedeb61f262b3972b0d97611516ee550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-152d2c4d721161f59c42618f5ca3e088.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8a58b1f916c592a8803de12116c1609.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-efd0ca0e7fcb5a8b059861a8174f87fa.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLaVA-ST-A-Multimodal-Large-Language-Model-for-Fine-Grained-Spatial-Temporal-Understanding"><a href="#LLaVA-ST-A-Multimodal-Large-Language-Model-for-Fine-Grained-Spatial-Temporal-Understanding" class="headerlink" title="LLaVA-ST: A Multimodal Large Language Model for Fine-Grained   Spatial-Temporal Understanding"></a>LLaVA-ST: A Multimodal Large Language Model for Fine-Grained   Spatial-Temporal Understanding</h2><p><strong>Authors:Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, Si Liu</strong></p>
<p>Recent advancements in multimodal large language models (MLLMs) have shown promising results, yet existing approaches struggle to effectively handle both temporal and spatial localization simultaneously. This challenge stems from two key issues: first, incorporating spatial-temporal localization introduces a vast number of coordinate combinations, complicating the alignment of linguistic and visual coordinate representations; second, encoding fine-grained temporal and spatial information during video feature compression is inherently difficult. To address these issues, we propose LLaVA-ST, a MLLM for fine-grained spatial-temporal multimodal understanding. In LLaVA-ST, we propose Language-Aligned Positional Embedding, which embeds the textual coordinate special token into the visual space, simplifying the alignment of fine-grained spatial-temporal correspondences. Additionally, we design the Spatial-Temporal Packer, which decouples the feature compression of temporal and spatial resolutions into two distinct point-to-region attention processing streams. Furthermore, we propose ST-Align dataset with 4.3M training samples for fine-grained spatial-temporal multimodal understanding. With ST-align, we present a progressive training pipeline that aligns the visual and textual feature through sequential coarse-to-fine stages.Additionally, we introduce an ST-Align benchmark to evaluate spatial-temporal interleaved fine-grained understanding tasks, which include Spatial-Temporal Video Grounding (STVG) , Event Localization and Captioning (ELC) and Spatial Video Grounding (SVG). LLaVA-ST achieves outstanding performance on 11 benchmarks requiring fine-grained temporal, spatial, or spatial-temporal interleaving multimodal understanding. Our code, data and benchmark will be released at Our code, data and benchmark will be released at <a target="_blank" rel="noopener" href="https://github.com/appletea233/LLaVA-ST">https://github.com/appletea233/LLaVA-ST</a> . </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥å·²ç»å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†ç°æœ‰æ–¹æ³•å¾ˆéš¾åŒæ—¶æœ‰æ•ˆåœ°å¤„ç†æ—¶é—´å’Œç©ºé—´çš„å®šä½ã€‚è¿™ä¸€æŒ‘æˆ˜æºäºä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé¦–å…ˆï¼Œèå…¥æ—¶ç©ºå®šä½ä¼šå¼•å…¥å¤§é‡çš„åæ ‡ç»„åˆï¼Œå¯¼è‡´è¯­è¨€å’Œè§†è§‰åæ ‡è¡¨ç¤ºçš„å¯¹é½å˜å¾—å¤æ‚ï¼›å…¶æ¬¡ï¼Œåœ¨è§†é¢‘ç‰¹å¾å‹ç¼©è¿‡ç¨‹ä¸­ç¼–ç ç²¾ç»†çš„æ—¶é—´ä¿¡æ¯å’Œç©ºé—´ä¿¡æ¯å…·æœ‰å›ºæœ‰çš„éš¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-STï¼Œä¸€ä¸ªç”¨äºç²¾ç»†æ—¶ç©ºå¤šæ¨¡æ€ç†è§£çš„MLLMã€‚åœ¨LLaVA-STä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­è¨€å¯¹é½ä½ç½®åµŒå…¥ï¼Œå®ƒå°†æ–‡æœ¬åæ ‡ç‰¹æ®Šä»¤ç‰ŒåµŒå…¥åˆ°è§†è§‰ç©ºé—´ä¸­ï¼Œç®€åŒ–äº†ç²¾ç»†æ—¶ç©ºå¯¹åº”å…³ç³»çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶ç©ºæ‰“åŒ…å™¨ï¼Œå®ƒå°†æ—¶é—´åˆ†è¾¨ç‡å’Œç©ºé—´åˆ†è¾¨ç‡çš„ç‰¹å¾å‹ç¼©è§£è€¦ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ç‚¹-åŒºåŸŸæ³¨æ„åŠ›å¤„ç†æµã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†ST-Alignæ•°æ®é›†ï¼ŒåŒ…å«430ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œç”¨äºç²¾ç»†æ—¶ç©ºå¤šæ¨¡æ€ç†è§£ã€‚é€šè¿‡ST-alignï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„è®­ç»ƒç®¡é“ï¼Œé€šè¿‡ä»ç²—åˆ°ç»†çš„åºåˆ—é˜¶æ®µå¯¹é½è§†è§‰å’Œæ–‡æœ¬ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ST-AlignåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°æ—¶ç©ºäº¤é”™ç²¾ç»†ç†è§£ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶ç©ºè§†é¢‘å®šä½ï¼ˆSTVGï¼‰ã€äº‹ä»¶å®šä½å’Œæè¿°ï¼ˆELCï¼‰ä»¥åŠç©ºé—´è§†é¢‘å®šä½ï¼ˆSVGï¼‰ã€‚LLaVA-STåœ¨éœ€è¦ç²¾ç»†æ—¶é—´ã€ç©ºé—´æˆ–æ—¶ç©ºäº¤é”™å¤šæ¨¡æ€ç†è§£çš„11ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’ŒåŸºå‡†æµ‹è¯•å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/appletea233/LLaVA-ST%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/appletea233/LLaVA-STä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MLLMåœ¨å¤„ç†æ—¶ç©ºå®šä½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLLaVA-STçš„ç»†ç²’åº¦æ—¶ç©ºå¤šæ¨¡æ€ç†è§£æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è¯­è¨€å¯¹é½ä½ç½®åµŒå…¥å’Œç©ºé—´æ—¶é—´æ‰“åŒ…å™¨è§£å†³è¿™äº›é—®é¢˜ã€‚è¿˜æ¨å‡ºäº†ST-Alignæ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°ç»†ç²’åº¦æ—¶ç©ºå¤šæ¨¡æ€ç†è§£ä»»åŠ¡ã€‚LLaVA-STåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨å¤„ç†æ—¶ç©ºå®šä½æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºè¯­è¨€å’Œè§†è§‰åæ ‡è¡¨ç¤ºçš„å¯¹é½å¤æ‚æ€§ä»¥åŠè§†é¢‘ç‰¹å¾å‹ç¼©ä¸­ç²¾ç»†æ—¶ç©ºä¿¡æ¯çš„ç¼–ç éš¾åº¦ã€‚</li>
<li>LLaVA-STæ¨¡å‹é€šè¿‡è¯­è¨€å¯¹é½ä½ç½®åµŒå…¥å’Œç©ºé—´æ—¶é—´æ‰“åŒ…å™¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>LLaVA-STå°†æ–‡æœ¬åæ ‡ç‰¹æ®Šä»¤ç‰ŒåµŒå…¥è§†è§‰ç©ºé—´ï¼Œç®€åŒ–äº†ç²¾ç»†æ—¶ç©ºå¯¹åº”å…³ç³»çš„å¯¹é½ã€‚</li>
<li>ç©ºé—´æ—¶é—´æ‰“åŒ…å™¨å°†æ—¶ç©ºåˆ†è¾¨ç‡çš„ç‰¹å¾å‹ç¼©è§£è€¦ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„ç‚¹åŒºåŸŸæ³¨æ„åŠ›å¤„ç†æµã€‚</li>
<li>æ¨å‡ºST-Alignæ•°æ®é›†ï¼ŒåŒ…å«430ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œç”¨äºç»†ç²’åº¦æ—¶ç©ºå¤šæ¨¡æ€ç†è§£ã€‚</li>
<li>å¼•å…¥ST-AlignåŸºå‡†æµ‹è¯•ï¼Œè¯„ä¼°æ—¶ç©ºäº¤ç»‡çš„ç»†ç²’åº¦ç†è§£ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶ç©ºè§†é¢‘å®šä½ã€äº‹ä»¶å®šä½å’Œæè¿°ä»¥åŠç©ºé—´è§†é¢‘å®šä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c49883ec73b27b763a16ff8af3c47b90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1338e18a63856b5e5df85560b6412eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81472ecdbdeaf7a0468cfe608a9d5df2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-124bdb92dcb9d84d88029ac7f13e7be7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6673a7b85beb94913278ba2a9046715b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Efficient-Adapter-Based-Fine-Tuning-of-State-of-the-Art-Transformer-Models"><a href="#Comparative-Analysis-of-Efficient-Adapter-Based-Fine-Tuning-of-State-of-the-Art-Transformer-Models" class="headerlink" title="Comparative Analysis of Efficient Adapter-Based Fine-Tuning of   State-of-the-Art Transformer Models"></a>Comparative Analysis of Efficient Adapter-Based Fine-Tuning of   State-of-the-Art Transformer Models</h2><p><strong>Authors:Saad Mashkoor Siddiqui, Mohammad Ali Sheikh, Muhammad Aleem, Kajol R Singh</strong></p>
<p>In this work, we investigate the efficacy of various adapter architectures on supervised binary classification tasks from the SuperGLUE benchmark as well as a supervised multi-class news category classification task from Kaggle. Specifically, we compare classification performance and time complexity of three transformer models, namely DistilBERT, ELECTRA, and BART, using conventional fine-tuning as well as nine state-of-the-art (SoTA) adapter architectures. Our analysis reveals performance differences across adapter architectures, highlighting their ability to achieve comparable or better performance relative to fine-tuning at a fraction of the training time. Similar results are observed on the new classification task, further supporting our findings and demonstrating adapters as efficient and flexible alternatives to fine-tuning. This study provides valuable insights and guidelines for selecting and implementing adapters in diverse natural language processing (NLP) applications. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„é€‚é…å™¨æ¶æ„åœ¨SuperGLUEåŸºå‡†æµ‹è¯•ä¸­çš„ç›‘ç£äºŒåˆ†ç±»ä»»åŠ¡ä»¥åŠKaggleä¸­çš„ç›‘ç£å¤šç±»æ–°é—»ç±»åˆ«åˆ†ç±»ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§è½¬æ¢å™¨æ¨¡å‹ï¼ˆå³DistilBERTã€ELECTRAå’ŒBARTï¼‰åœ¨å¸¸è§„å¾®è°ƒä»¥åŠä¹ç§æœ€æ–°é€‚é…å™¨æ¶æ„ä¸‹çš„åˆ†ç±»æ€§èƒ½å’Œæ—¶é—´å¤æ‚åº¦ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒé€‚é…å™¨æ¶æ„ä¹‹é—´çš„æ€§èƒ½å·®å¼‚ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨æçŸ­çš„è®­ç»ƒæ—¶é—´å†…è¾¾åˆ°ä¸å¾®è°ƒç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½çš„èƒ½åŠ›ã€‚åœ¨æ–°çš„åˆ†ç±»ä»»åŠ¡ä¸Šä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„ç»“æœï¼Œè¿™è¿›ä¸€æ­¥æ”¯æŒäº†æˆ‘ä»¬çš„å‘ç°ï¼Œå¹¶è¯æ˜äº†é€‚é…å™¨æ˜¯å¾®è°ƒçš„æœ‰æ•ˆå’Œçµæ´»æ›¿ä»£æ–¹æ¡ˆã€‚è¿™é¡¹ç ”ç©¶ä¸ºåœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº”ç”¨ä¸­é€‰æ‹©å’Œå®æ–½é€‚é…å™¨æä¾›äº†å®è´µçš„è§è§£å’ŒæŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08271v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸åŒé€‚é…å™¨æ¶æ„åœ¨SuperGLUEåŸºå‡†æµ‹è¯•ä¸­çš„ç›‘ç£äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä»¥åŠæ¥è‡ªKaggleçš„ç›‘ç£å¤šç±»æ–°é—»åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ•ˆæœã€‚æ–‡ç« å¯¹æ¯”äº†DistilBERTã€ELECTRAå’ŒBARTä¸‰ç§è½¬æ¢å™¨æ¨¡å‹çš„åˆ†ç±»æ€§èƒ½å’Œæ—¶é—´æˆæœ¬ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿå¾®è°ƒä»¥åŠä¹ç§å…ˆè¿›çš„é€‚é…å™¨æ¶æ„ã€‚ç ”ç©¶å‘ç°ï¼Œé€‚é…å™¨æ¶æ„é—´å­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œå®ƒä»¬èƒ½å¤Ÿåœ¨å¾ˆçŸ­çš„æ—¶é—´å†…è¾¾åˆ°ä¸å¾®è°ƒç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚åœ¨æ–°åˆ†ç±»ä»»åŠ¡ä¸Šä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„ç»“æœï¼Œè¿›ä¸€æ­¥éªŒè¯äº†é€‚é…å™¨åœ¨NLPåº”ç”¨ä¸­çš„é«˜æ•ˆæ€§å’Œçµæ´»æ€§ã€‚æœ¬æ–‡æä¾›äº†å®è´µçš„è§è§£å’ŒæŒ‡å¯¼æ–¹é’ˆï¼Œä¸ºåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­é€‰æ‹©å’Œå®æ–½é€‚é…å™¨æä¾›äº†ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¯¹å¤šç§é€‚é…å™¨æ¶æ„åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚</li>
<li>å¯¹æ¯”äº†DistilBERTã€ELECTRAå’ŒBARTä¸‰ç§è½¬æ¢å™¨æ¨¡å‹åœ¨ç›‘ç£åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œæ—¶é—´æˆæœ¬ã€‚</li>
<li>ç ”ç©¶å‘ç°é€‚é…å™¨æ¶æ„åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¸å¾®è°ƒç›¸å½“æˆ–æ›´å¥½ï¼Œä¸”è®­ç»ƒæ—¶é—´æ›´çŸ­ã€‚</li>
<li>æ–°åˆ†ç±»ä»»åŠ¡çš„ç»“æœè¿›ä¸€æ­¥æ”¯æŒäº†é€‚é…å™¨æ¶æ„çš„æœ‰æ•ˆæ€§å’Œçµæ´»æ€§ã€‚</li>
<li>è®ºæ–‡æä¾›äº†å…³äºé€‰æ‹©å’Œå®æ–½é€‚é…å™¨çš„å®è´µè§è§£å’ŒæŒ‡å¯¼æ–¹é’ˆã€‚</li>
<li>ç ”ç©¶æˆæœæœ‰åŠ©äºç†è§£å’Œä¼˜åŒ–è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„æ¨¡å‹è®­ç»ƒæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83070431facd3e3bbe9938bcf3ba2a4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e64b5d58b8daee645b7c0bf84660bd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53fc5cfa03ac1011b544bcbba1291e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67d6b1527e573e4aff71f4e0675a4e1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8a40c683a695712d5e65596ea857ce5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a390765693c42ebd6e13876aff08c597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4f5bfe5589f2d328adcff3377101c69.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Text-Diffusion-Red-Teaming-of-Large-Language-Models-Unveiling-Harmful-Behaviors-with-Proximity-Constraints"><a href="#Text-Diffusion-Red-Teaming-of-Large-Language-Models-Unveiling-Harmful-Behaviors-with-Proximity-Constraints" class="headerlink" title="Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints"></a>Text-Diffusion Red-Teaming of Large Language Models: Unveiling Harmful   Behaviors with Proximity Constraints</h2><p><strong>Authors:Jonathan NÃ¶ther, Adish Singla, Goran RadanoviÄ‡</strong></p>
<p>Recent work has proposed automated red-teaming methods for testing the vulnerabilities of a given target large language model (LLM). These methods use red-teaming LLMs to uncover inputs that induce harmful behavior in a target LLM. In this paper, we study red-teaming strategies that enable a targeted security assessment. We propose an optimization framework for red-teaming with proximity constraints, where the discovered prompts must be similar to reference prompts from a given dataset. This dataset serves as a template for the discovered prompts, anchoring the search for test-cases to specific topics, writing styles, or types of harmful behavior. We show that established auto-regressive model architectures do not perform well in this setting. We therefore introduce a black-box red-teaming method inspired by text-diffusion models: Diffusion for Auditing and Red-Teaming (DART). DART modifies the reference prompt by perturbing it in the embedding space, directly controlling the amount of change introduced. We systematically evaluate our method by comparing its effectiveness with established methods based on model fine-tuning and zero- and few-shot prompting. Our results show that DART is significantly more effective at discovering harmful inputs in close proximity to the reference prompt. </p>
<blockquote>
<p>æœ€è¿‘çš„å·¥ä½œæå‡ºäº†é’ˆå¯¹ç»™å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–çº¢é˜Ÿæµ‹è¯•çš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨çº¢é˜ŸLLMæ¥å‘ç°è¯±å¯¼ç›®æ ‡LLMå‡ºç°æœ‰å®³è¡Œä¸ºçš„è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†èƒ½å¤Ÿè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å®‰å…¨è¯„ä¼°çš„çº¢é˜Ÿç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¸¦æœ‰æ¥è¿‘åº¦çº¦æŸçš„çº¢é˜Ÿä¼˜åŒ–æ¡†æ¶ï¼Œå…¶ä¸­å‘ç°çš„æç¤ºå¿…é¡»ä¸ç»™å®šæ•°æ®é›†ä¸­çš„å‚è€ƒæç¤ºç›¸ä¼¼ã€‚è¯¥æ•°æ®é›†ä½œä¸ºå‘ç°çš„æç¤ºçš„æ¨¡æ¿ï¼Œå°†æµ‹è¯•ç”¨ä¾‹çš„æœç´¢é”šå®šåˆ°ç‰¹å®šä¸»é¢˜ã€å†™ä½œé£æ ¼æˆ–æœ‰å®³è¡Œä¸ºç±»å‹ä¸Šã€‚æˆ‘ä»¬å‘ç°ç°æœ‰çš„è‡ªåŠ¨å›å½’æ¨¡å‹æ¶æ„åœ¨æ­¤è®¾ç½®ä¸­è¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—æ–‡æœ¬æ‰©æ•£æ¨¡å‹å¯å‘çš„é»‘ç›’çº¢é˜Ÿæ–¹æ³•ï¼šç”¨äºå®¡è®¡å’Œçº¢é˜Ÿçš„æ‰©æ•£ï¼ˆDARTï¼‰ã€‚DARTé€šè¿‡æ‰°åŠ¨å‚è€ƒæç¤ºçš„åµŒå…¥ç©ºé—´è¿›è¡Œä¿®æ”¹ï¼Œç›´æ¥æ§åˆ¶å¼•å…¥çš„å˜åŒ–é‡ã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶ä¸åŸºäºæ¨¡å‹å¾®è°ƒä»¥åŠé›¶æ¬¡å’Œå°‘æ¬¡æç¤ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§è¿›è¡Œæ¯”è¾ƒï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼ŒDARTåœ¨å‘ç°ä¸å‚è€ƒæç¤ºæ¥è¿‘çš„æœ‰å®³è¾“å…¥æ–¹é¢æ˜¾è‘—æ›´æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08246v1">PDF</a> This is an extended version of a paper published at AAAI 25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨è¯„ä¼°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯çº¢é˜Ÿæ”»å‡»ç­–ç•¥ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å¸¦æœ‰è¿‘ä¼¼çº¦æŸçš„çº¢é˜Ÿä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è¦æ±‚å‘ç°çš„æç¤ºå¿…é¡»ä¸ç»™å®šæ•°æ®é›†ä¸­çš„å‚è€ƒæç¤ºç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œæ–‡ç« ä»‹ç»äº†åŸºäºæ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„é»‘è‰²ç›’å­çº¢é˜Ÿæ–¹æ³•â€”â€”å®¡è®¡ä¸çº¢é˜Ÿæ‰©æ•£ï¼ˆDARTï¼‰ã€‚DARTé€šè¿‡åµŒå…¥ç©ºé—´ä¸­æ‰°åŠ¨å‚è€ƒæç¤ºæ¥ä¿®æ”¹å®ƒï¼Œå¹¶ç›´æ¥æ§åˆ¶å¼•å…¥çš„å˜åŒ–é‡ã€‚é€šè¿‡ç³»ç»Ÿåœ°è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºDARTåœ¨å‘ç°ä¸å‚è€ƒæç¤ºç›¸è¿‘çš„æœ‰å®³è¾“å…¥æ–¹é¢ï¼Œæ¯”åŸºäºæ¨¡å‹å¾®è°ƒã€é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºçš„æ–¹æ³•æ›´ä¸ºæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†å¸¦æœ‰è¿‘ä¼¼çº¦æŸçš„çº¢é˜Ÿä¼˜åŒ–æ¡†æ¶ï¼Œé’ˆå¯¹LLMè¿›è¡Œå®‰å…¨è¯„ä¼°ã€‚</li>
<li>å‘ç°ç°æœ‰è‡ªåŠ¨å›å½’æ¨¡å‹æ¶æ„åœ¨æ­¤è®¾ç½®ä¸­çš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>ä»‹ç»äº†åŸºäºæ–‡æœ¬æ‰©æ•£æ¨¡å‹çš„é»‘è‰²ç›’å­çº¢é˜Ÿæ–¹æ³•â€”â€”DARTã€‚</li>
<li>DARTé€šè¿‡åµŒå…¥ç©ºé—´ä¸­çš„æ‰°åŠ¨æ¥ä¿®æ”¹å‚è€ƒæç¤ºã€‚</li>
<li>DARTèƒ½æ›´æœ‰æ•ˆåœ°å‘ç°ä¸å‚è€ƒæç¤ºç›¸è¿‘çš„æœ‰å®³è¾“å…¥ã€‚</li>
<li>DARTåœ¨å‘ç°æµ‹è¯•æ¡ˆä¾‹æ—¶ï¼Œèƒ½å¤Ÿé”šå®šåˆ°ç‰¹å®šä¸»é¢˜ã€å†™ä½œé£æ ¼æˆ–æœ‰å®³è¡Œä¸ºç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-22dd814af019e9de46d4ed048460b266.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f498a75494c0c0cc7ff491491e944e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f4c9c18f39bb34f8c354c9df82ea589.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f81556c8fa01d54c709542b9f8bb37bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d4af3f0335c4572b2a7884ad40148f9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Engineering-LLM-Powered-Multi-agent-Framework-for-Autonomous-CloudOps"><a href="#Engineering-LLM-Powered-Multi-agent-Framework-for-Autonomous-CloudOps" class="headerlink" title="Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps"></a>Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps</h2><p><strong>Authors:Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan</strong></p>
<p>Cloud Operations (CloudOps) is a rapidly growing field focused on the automated management and optimization of cloud infrastructure which is essential for organizations navigating increasingly complex cloud environments. MontyCloud Inc. is one of the major companies in the CloudOps domain that leverages autonomous bots to manage cloud compliance, security, and continuous operations. To make the platform more accessible and effective to the customers, we leveraged the use of GenAI.   Developing a GenAI-based solution for autonomous CloudOps for the existing MontyCloud system presented us with various challenges such as i) diverse data sources; ii) orchestration of multiple processes; and iii) handling complex workflows to automate routine tasks. To this end, we developed MOYA, a multi-agent framework that leverages GenAI and balances autonomy with the necessary human control. This framework integrates various internal and external systems and is optimized for factors like task orchestration, security, and error mitigation while producing accurate, reliable, and relevant insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our multi-agent system with the help of practitioners as well as using automated checks demonstrate enhanced accuracy, responsiveness, and effectiveness over non-agentic approaches across complex workflows. </p>
<blockquote>
<p>äº‘æ“ä½œï¼ˆCloudOpsï¼‰æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œä¸“æ³¨äºäº‘åŸºç¡€è®¾æ–½çš„è‡ªåŠ¨åŒ–ç®¡ç†å’Œä¼˜åŒ–ï¼Œå¯¹äºåœ¨æ—¥ç›Šå¤æ‚çš„äº‘ç¯å¢ƒä¸­å¯¼èˆªçš„ç»„ç»‡è€Œè¨€ï¼Œè¿™æ˜¯å¿…ä¸å¯å°‘çš„ã€‚MontyCloud Inc.æ˜¯äº‘æ“ä½œé¢†åŸŸçš„ä¸»è¦å…¬å¸ä¹‹ä¸€ï¼Œåˆ©ç”¨è‡ªä¸»æœºå™¨äººç®¡ç†äº‘åˆè§„æ€§ã€å®‰å…¨æ€§å’ŒæŒç»­è¿è¥ã€‚ä¸ºäº†ä½¿å¹³å°å¯¹å®¢æˆ·æ›´åŠ å¯ç”¨å’Œæœ‰æ•ˆï¼Œæˆ‘ä»¬åˆ©ç”¨äº†GenAIã€‚ä¸ºç°æœ‰çš„MontyCloudç³»ç»Ÿå¼€å‘åŸºäºGenAIçš„è‡ªä¸»äº‘æ“ä½œç³»ç»Ÿè§£å†³æ–¹æ¡ˆï¼Œç»™æˆ‘ä»¬å¸¦æ¥äº†å„ç§æŒ‘æˆ˜ï¼Œä¾‹å¦‚ï¼ˆiï¼‰å¤šæ ·åŒ–çš„æ•°æ®æºï¼›ï¼ˆiiï¼‰å¤šä¸ªæµç¨‹çš„ç¼–æ’ï¼›ï¼ˆiiiï¼‰å¤„ç†å¤æ‚çš„å·¥ä½œæµä»¥è‡ªåŠ¨åŒ–å¸¸è§„ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†MOYAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šä»£ç†æ¡†æ¶ï¼Œåˆ©ç”¨GenAIï¼Œåœ¨è‡ªä¸»æ€§å’Œå¿…è¦çš„äººä¸ºæ§åˆ¶ä¹‹é—´å–å¾—å¹³è¡¡ã€‚è¯¥æ¡†æ¶æ•´åˆäº†å†…éƒ¨å’Œå¤–éƒ¨ç³»ç»Ÿï¼Œé’ˆå¯¹ä»»åŠ¡ç¼–æ’ã€å®‰å…¨å’Œé”™è¯¯ç¼“è§£ç­‰å› ç´ è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŒæ—¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æä¾›å‡†ç¡®ã€å¯é å’Œç›¸å…³çš„è§è§£ã€‚é€šè¿‡å®è·µè€…çš„å¸®åŠ©ä»¥åŠè‡ªåŠ¨æ£€æŸ¥å¯¹æˆ‘ä»¬çš„å¤šä»£ç†ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼Œè¡¨æ˜åœ¨å¤æ‚çš„å·¥ä½œæµä¸­ï¼Œä¸æ— ä»£ç†æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€å“åº”æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08243v1">PDF</a> The paper has been accepted as full paper to CAIN 2025   (<a target="_blank" rel="noopener" href="https://conf.researchr.org/home/cain-2025">https://conf.researchr.org/home/cain-2025</a>), co-located with ICSE 2025   (<a target="_blank" rel="noopener" href="https://conf.researchr.org/home/icse-2025">https://conf.researchr.org/home/icse-2025</a>). The paper was submitted to CAIN   for review on 9 November 2024</p>
<p><strong>Summary</strong><br>è’™è’‚äº‘å…¬å¸æ­£åœ¨å¼€å‘ä¸€ç§åŸºäºGenAIçš„è‡ªä¸»äº‘æ“ä½œï¼ˆCloudOpsï¼‰è§£å†³æ–¹æ¡ˆã€‚ä»–ä»¬é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬å¤šæ ·çš„æ•°æ®æ¥æºã€å¤šä¸ªæµç¨‹çš„ååŒä»¥åŠå¤æ‚å·¥ä½œæµçš„è‡ªåŠ¨åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä»–ä»¬å¼€å‘äº†MOYAå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¼˜åŒ–äº†ä»»åŠ¡ååŒã€å®‰å…¨æ€§å’Œé”™è¯¯ç¼“è§£ç­‰å› ç´ ï¼Œå¹¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æä¾›å‡†ç¡®ã€å¯é å’Œç›¸å…³çš„è§è§£ã€‚è¯„ä¼°è¡¨æ˜ï¼Œä¸æ²¡æœ‰æ™ºèƒ½ä½“çš„æ–¹æ³•ç›¸æ¯”ï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨å¤æ‚å·¥ä½œæµç¨‹ä¸­çš„å‡†ç¡®æ€§ã€å“åº”æ€§å’Œæœ‰æ•ˆæ€§éƒ½å¾—åˆ°äº†æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cloud Operations (CloudOps) æ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œä¸“æ³¨äºäº‘åŸºç¡€è®¾æ–½çš„è‡ªåŠ¨åŒ–ç®¡ç†å’Œä¼˜åŒ–ã€‚</li>
<li>è’™è’‚äº‘å…¬å¸æ˜¯äº‘æ“ä½œé¢†åŸŸçš„ä¸»è¦å…¬å¸ä¹‹ä¸€ï¼Œåˆ©ç”¨è‡ªä¸»æœºå™¨äººç®¡ç†äº‘åˆè§„æ€§ã€å®‰å…¨å’ŒæŒç»­è¿è¥ã€‚</li>
<li>è’™è’‚äº‘å…¬å¸ä¸ºäº†ä½¿å…¶å¹³å°æ›´æ˜“äºå®¢æˆ·ä½¿ç”¨å¹¶æé«˜å…¶æ•ˆæœï¼Œä½¿ç”¨äº†GenAIæŠ€æœ¯ã€‚</li>
<li>å¼€å‘åŸºäºGenAIçš„è‡ªä¸»äº‘æ“ä½œè§£å†³æ–¹æ¡ˆé¢ä¸´äº†å¤šæ ·çš„æ•°æ®æ¥æºã€å¤šä¸ªæµç¨‹çš„ååŒä»¥åŠå¤æ‚å·¥ä½œæµçš„è‡ªåŠ¨åŒ–ç­‰æŒ‘æˆ˜ã€‚</li>
<li>MOYAå¤šæ™ºèƒ½ä½“æ¡†æ¶æ˜¯ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜è€Œå¼€å‘çš„ï¼Œå®ƒæ•´åˆäº†å†…å¤–éƒ¨ç³»ç»Ÿï¼Œå¹¶ä¼˜åŒ–äº†ä»»åŠ¡ååŒã€å®‰å…¨æ€§å’Œé”™è¯¯ç¼“è§£ç­‰å› ç´ ã€‚</li>
<li>MOYAæ¡†æ¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯æä¾›å‡†ç¡®ã€å¯é å’Œç›¸å…³çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d6eab35dc5ec81bf9eb48a6846e591b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2464381031b26a2c2b8f311f42c0e32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27069b1c7132bbc3fc997b0e89de2a60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb0b67320dacac79b95e3c62747a1bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a8180b7e70abc703e1895753b743985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a5d98cfe39579685fa6d3c7c5eeca92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-065c510958ed88f430f63e8b13962800.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Investigating-Energy-Efficiency-and-Performance-Trade-offs-in-LLM-Inference-Across-Tasks-and-DVFS-Settings"><a href="#Investigating-Energy-Efficiency-and-Performance-Trade-offs-in-LLM-Inference-Across-Tasks-and-DVFS-Settings" class="headerlink" title="Investigating Energy Efficiency and Performance Trade-offs in LLM   Inference Across Tasks and DVFS Settings"></a>Investigating Energy Efficiency and Performance Trade-offs in LLM   Inference Across Tasks and DVFS Settings</h2><p><strong>Authors:Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic</strong></p>
<p>Large language models (LLMs) have shown significant improvements in many natural language processing (NLP) tasks, accelerating their rapid adoption across many industries. These models are resource-intensive, requiring extensive computational resources both during training and inference, leading to increased energy consumption and negative environmental impact. As their adoption accelerates, the sustainability of LLMs has become a critical issue, necessitating strategies to optimize their runtime efficiency without compromising performance. Hence, it is imperative to identify the parameters that significantly influence the performance and energy efficiency of LLMs. To that end, in this work, we investigate the effect of important parameters on the performance and energy efficiency of LLMs during inference and examine their trade-offs.   First, we analyze how different types of models with varying numbers of parameters and architectures perform on tasks like text generation, question answering, and summarization by benchmarking LLMs such as Falcon-7B, Mistral-7B-v0.1, T5-3B, GPT-2, GPT-J-6B, and GPT-Neo-2.7B. Second, we study input and output sequence characteristics such as sequence length concerning energy consumption, performance, and throughput. Finally, we explore the impact of hardware-based power-saving techniques, i.e., Dynamic Voltage Frequency Scaling (DVFS), on the modelsâ€™ latency and energy efficiency. Our extensive benchmarking and statistical analysis reveal many interesting findings, uncovering how specific optimizations can reduce energy consumption while maintaining throughput and accuracy. This study provides actionable insights for researchers and practitioners to design energy-efficient LLM inference systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŠ é€Ÿäº†å…¶åœ¨å¤šä¸ªè¡Œä¸šçš„å¿«é€Ÿé‡‡ç”¨ã€‚è¿™äº›æ¨¡å‹èµ„æºå¯†é›†ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­éƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œå¯¼è‡´èƒ½æºæ¶ˆè€—å¢åŠ å¹¶å¯¹ç¯å¢ƒäº§ç”Ÿè´Ÿé¢å½±å“ã€‚éšç€å…¶é‡‡ç”¨çš„åŠ é€Ÿï¼ŒLLMçš„å¯æŒç»­æ€§å·²æˆä¸ºä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œéœ€è¦åˆ¶å®šç­–ç•¥ä»¥ä¼˜åŒ–å…¶è¿è¡Œæ•ˆç‡ï¼ŒåŒæ—¶ä¸å½±å“æ€§èƒ½ã€‚å› æ­¤ï¼Œç¡®å®šå¯¹LLMçš„æ€§èƒ½å’Œèƒ½æ•ˆäº§ç”Ÿé‡å¤§å½±å“çš„å‚æ•°è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é¡¹å·¥ä½œä¸­è°ƒæŸ¥äº†é‡è¦å‚æ•°å¯¹LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ€§èƒ½å’Œèƒ½æ•ˆçš„å½±å“ï¼Œå¹¶ç ”ç©¶äº†å®ƒä»¬ä¹‹é—´çš„æƒè¡¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åŸºå‡†æµ‹è¯•åˆ†æå…·æœ‰ä¸åŒå‚æ•°å’Œæ¶æ„çš„å„ç§æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆã€é—®ç­”å’Œæ‘˜è¦ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬Falcon-7Bã€Mistral-7B-v0.1ã€T5-3Bã€GPT-2ã€GPT-J-6Bå’ŒGPT-Neo-2.7Bç­‰LLMã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¾“å…¥å’Œè¾“å‡ºåºåˆ—ç‰¹å¾ï¼Œå¦‚åºåˆ—é•¿åº¦ä¸èƒ½æºæ¶ˆè´¹ã€æ€§èƒ½å’Œååé‡çš„å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºç¡¬ä»¶çš„èŠ‚èƒ½æŠ€æœ¯ï¼Œå³åŠ¨æ€ç”µå‹é¢‘ç‡ç¼©æ”¾ï¼ˆDVFSï¼‰å¯¹æ¨¡å‹çš„å»¶è¿Ÿå’Œèƒ½æ•ˆçš„å½±å“ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•å’Œç»Ÿè®¡åˆ†æï¼Œå‘ç°äº†è®¸å¤šæœ‰è¶£çš„ç»“æœï¼Œæ­ç¤ºäº†å¦‚ä½•é€šè¿‡ç‰¹å®šä¼˜åŒ–å‡å°‘èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒååé‡å’Œå‡†ç¡®æ€§ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†è®¾è®¡èƒ½æºé«˜æ•ˆLLMæ¨ç†ç³»ç»Ÿçš„å¯è¡Œè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08219v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶èµ„æºå¯†é›†å‹çš„ç‰¹æ€§å¯¼è‡´äº†èƒ½æºæ¶ˆè€—å’Œç¯å¢ƒè´Ÿæ‹…åŠ é‡ã€‚ä¸ºäº†æå‡LLMçš„å¯æŒç»­æ€§ï¼Œå¯¹å…¶è¿è¡Œæ•ˆç‡çš„ä¼˜åŒ–è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é€šè¿‡åŸºå‡†æµ‹è¯•ï¼Œæ¢è®¨äº†æ¨¡å‹å‚æ•°å¯¹LLMæ€§èƒ½å’Œèƒ½æºæ•ˆç‡çš„å½±å“ï¼Œå¹¶åˆ†æäº†ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚åŒæ—¶ï¼Œè¿˜ç ”ç©¶äº†è¾“å…¥è¾“å‡ºåºåˆ—ç‰¹æ€§ä¸èƒ½æºæ¶ˆè€—ã€æ€§èƒ½å’Œååé‡çš„å…³ç³»ï¼Œå¹¶æ¢ç´¢äº†åŸºäºç¡¬ä»¶çš„èŠ‚èƒ½æŠ€æœ¯å¦‚åŠ¨æ€ç”µå‹é¢‘ç‡ç¼©æ”¾ï¼ˆDVFSï¼‰å¯¹æ¨¡å‹å»¶è¿Ÿå’Œèƒ½æºæ•ˆç‡çš„å½±å“ã€‚æœ¬ç ”ç©¶ä¸ºè®¾è®¡èƒ½æºé«˜æ•ˆçš„LLMæ¨ç†ç³»ç»Ÿæä¾›äº†å®è·µæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†èµ„æºå¯†é›†å‹ç‰¹æ€§å¼•å‘èƒ½æºå’Œç¯å¢ƒé—®é¢˜ã€‚</li>
<li>å¯¹LLMæ€§èƒ½å’Œèƒ½æºæ•ˆç‡çš„ä¼˜åŒ–è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ é€Ÿå…¶æ¨ç†è¿‡ç¨‹æ—¶ã€‚</li>
<li>ä¸åŒç±»å‹å’Œè§„æ¨¡çš„LLMåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€é—®ç­”å’Œæ‘˜è¦ç­‰ã€‚</li>
<li>è¾“å…¥è¾“å‡ºåºåˆ—é•¿åº¦ä¸èƒ½æºæ¶ˆè€—ã€æ€§èƒ½å’Œååé‡å¯†åˆ‡ç›¸å…³ã€‚</li>
<li>ç¡¬ä»¶èŠ‚èƒ½æŠ€æœ¯å¦‚DVFSå¯¹LLMçš„å»¶è¿Ÿå’Œèƒ½æºæ•ˆç‡æœ‰é‡è¦å½±å“ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡åŸºå‡†æµ‹è¯•å’Œç»Ÿè®¡åˆ†æï¼Œæ­ç¤ºäº†ä¼˜åŒ–LLMä»¥å‡å°‘èƒ½æºæ¶ˆè€—çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08219">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8821957eee1f00e26c3c089538681900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2cb74453716deec5a71019f8459e3bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9be232dd89608953cdb0d617d20fdeed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea344d4bec613eff09803ec44cb89385.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7093581b9b636671af065f25e024adc1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-860e26fd29ed4634f55222193cef353c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d654e5f4944665fb00dcdf811fba2eb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="ASTRID-â€“-An-Automated-and-Scalable-TRIaD-for-the-Evaluation-of-RAG-based-Clinical-Question-Answering-Systems"><a href="#ASTRID-â€“-An-Automated-and-Scalable-TRIaD-for-the-Evaluation-of-RAG-based-Clinical-Question-Answering-Systems" class="headerlink" title="ASTRID â€“ An Automated and Scalable TRIaD for the Evaluation of   RAG-based Clinical Question Answering Systems"></a>ASTRID â€“ An Automated and Scalable TRIaD for the Evaluation of   RAG-based Clinical Question Answering Systems</h2><p><strong>Authors:Mohita Chowdhury, Yajie Vera He, Aisling Higham, Ernest Lim</strong></p>
<p>Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a modelâ€™s response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸´åºŠé—®é¢˜å›ç­”ï¼ˆQAï¼‰æ–¹é¢æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ½œåŠ›ï¼Œå…¶ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä½œä¸ºç¡®ä¿æ¨¡å‹åº”ç­”äº‹å®å‡†ç¡®æ€§çš„é¢†å…ˆæ–¹æ³•è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå½“å‰çš„è‡ªåŠ¨åŒ–RAGæŒ‡æ ‡åœ¨ä¸´åºŠå’Œå¯¹è¯ç”¨ä¾‹ä¸­çš„è¡¨ç°ä¸ä½³ã€‚ä½¿ç”¨ä¸´åºŠäººå‘˜å¯¹åº”ç­”çš„è¯„ä»·æˆæœ¬é«˜æ˜‚ï¼Œä¸å¯æ‰©å±•ï¼Œä¸”ä¸åˆ©äºRAGç³»ç»Ÿçš„æŒç»­è¿­ä»£å¼€å‘ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ASTRIDâ€”â€”ä¸€ç§åˆ©ç”¨RAGè¯„ä¼°ä¸´åºŠQAç³»ç»Ÿçš„è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„TRIaDï¼Œç”±ä¸‰ä¸ªæŒ‡æ ‡ç»„æˆï¼šä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼ˆCRï¼‰ã€æ‹’ç»å‡†ç¡®ç‡ï¼ˆRAï¼‰å’Œå¯¹è¯å¿ å®æ€§ï¼ˆCFï¼‰ã€‚æˆ‘ä»¬æ–°é¢–çš„è¯„ä»·æŒ‡æ ‡CFæ—¨åœ¨æ›´å¥½åœ°æ•æ‰æ¨¡å‹å“åº”çŸ¥è¯†åº“çš„çœŸå®æ€§ï¼ŒåŒæ—¶ä¸æƒ©ç½šå¯¹è¯å…ƒç´ ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„ä¸‰äººç»„ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬200å¤šä¸ªç°å®ä¸–ç•Œçš„æ‚£è€…é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ˜¯åœ¨åŸºäºLLMçš„QAä»£ç†è¿›è¡Œç™½å†…éšœæ‰‹æœ¯éšè®¿æ—¶æå‡ºçš„ï¼Œè¿˜æœ‰ä¸´åºŠåŒ»ç”Ÿé€‰æ‹©çš„å…³äºç´§æ€¥ã€ä¸´åºŠå’Œéä¸´åºŠåŸŸå¤–åœºæ™¯çš„é¢˜ç›®ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒCFèƒ½å¤Ÿæ¯”ç°æœ‰å®šä¹‰æ›´å¥½åœ°é¢„æµ‹äººç±»å¯¹è¯ç”¨ä¾‹ä¸­çš„å¿ å®æ€§è¯„ä»·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†ä½¿ç”¨ç”±CFã€RAå’ŒCRç»„æˆçš„ä¸‰äººç»„è¯„ä¼°çš„å›åº”ä¸ä¸´åºŠåŒ»ç”Ÿå¯¹ä¸å½“ã€æœ‰å®³æˆ–æ— åŠ©çš„å›åº”çš„è¯„ä¼°ç›¸ä¸€è‡´ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¹ä¸ªä¸åŒçš„LLMè¿›è¡Œå®éªŒï¼Œè¯æ˜è¿™ä¸‰ä¸ªæŒ‡æ ‡å¯ä»¥ç´§å¯†åœ°ä¸äººç±»è¯„ä¼°è¾¾æˆä¸€è‡´ï¼Œçªå‡ºäº†è¿™äº›æŒ‡æ ‡åœ¨LLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜å…¬å¸ƒäº†è¿™äº›å®éªŒçš„æç¤ºå’Œæ•°æ®é›†ï¼Œä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08208v1">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨ä¸´åºŠé—®ç­”ç³»ç»Ÿä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•èƒ½ç¡®ä¿æ¨¡å‹å›ç­”çš„äº‹å®å‡†ç¡®æ€§ã€‚ä½†ç°æœ‰è‡ªåŠ¨åŒ–RAGæŒ‡æ ‡åœ¨ä¸´åºŠå’Œå¯¹è¯åœºæ™¯ä¸­çš„è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°å‹è¯„ä¼°æ–¹æ³•ASTRIDï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæŒ‡æ ‡ï¼šä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼ˆCRï¼‰ã€æ‹’ç»å‡†ç¡®æ€§ï¼ˆRAï¼‰å’Œå¯¹è¯å¿ å®åº¦ï¼ˆCFï¼‰ã€‚CFèƒ½æ›´å‡†ç¡®åœ°æ•æ‰æ¨¡å‹å›åº”çš„å¿ å®åº¦ï¼ŒåŒæ—¶é¿å…å¯¹è¯å…ƒç´ çš„æƒ©ç½šã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼ŒCFèƒ½æ›´å¥½é¢„æµ‹äººç±»å¯¹è¯åœºæ™¯çš„å¿ å®åº¦è¯„ä»·ï¼›åŒæ—¶åŒ…å«CFã€RAå’ŒCRçš„è¯„ä¼°æ–¹æ³•èƒ½ç´§å¯†è´´åˆä¸´åºŠåŒ»ç”Ÿå¯¹ä¸å½“ã€æœ‰å®³æˆ–æ— å¸®åŠ©å›åº”çš„è¯„ä¼°ã€‚å¯¹ä¹ç§ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®éªŒè¿›ä¸€æ­¥éªŒè¯äº†è¿™ä¸‰ä¸ªæŒ‡æ ‡ä¸äººç±»è¯„ä»·çš„å¥‘åˆåº¦ï¼Œå±•ç¤ºäº†è¿™äº›æŒ‡æ ‡åœ¨LLMè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨ä¸´åºŠé—®ç­”ç³»ç»Ÿä¸­å±•ç°æ½œåŠ›ï¼ŒRAGæ–¹æ³•ç¡®ä¿å›ç­”äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–RAGæŒ‡æ ‡åœ¨ä¸´åºŠå’Œå¯¹è¯åœºæ™¯ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¼•å…¥æ–°å‹è¯„ä¼°æ–¹æ³•ASTRIDï¼ŒåŒ…æ‹¬CRã€RAå’ŒCFä¸‰ä¸ªæŒ‡æ ‡ã€‚</li>
<li>CFèƒ½æ›´å‡†ç¡®åœ°é¢„æµ‹äººç±»å¯¹è¯åœºæ™¯çš„å¿ å®åº¦è¯„ä»·ã€‚</li>
<li>åŒ…å«CFã€RAå’ŒCRçš„è¯„ä¼°æ–¹æ³•ä¸ä¸´åºŠåŒ»ç”Ÿè¯„ä¼°ç´§å¯†è´´åˆï¼Œèƒ½è¯†åˆ«ä¸å½“ã€æœ‰å®³æˆ–æ— å¸®åŠ©çš„å›åº”ã€‚</li>
<li>ä¸‰ä¸ªæŒ‡æ ‡ä¸äººç±»è¯„ä»·çš„å¥‘åˆåº¦é«˜ï¼Œæœ‰æ½œåŠ›ç”¨äºLLMè‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d45711c5d088bbe1bcf3929e5cf8e900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5ff65f37063955980c304c0a60f9de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b854a7f0297699e058a90eea48bf62ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec4c5c7425f146cae2ffd7bb26b258ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e1abd03d69cfad0e19d81e048d7ef56.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CWEval-Outcome-driven-Evaluation-on-Functionality-and-Security-of-LLM-Code-Generation"><a href="#CWEval-Outcome-driven-Evaluation-on-Functionality-and-Security-of-LLM-Code-Generation" class="headerlink" title="CWEval: Outcome-driven Evaluation on Functionality and Security of LLM   Code Generation"></a>CWEval: Outcome-driven Evaluation on Functionality and Security of LLM   Code Generation</h2><p><strong>Authors:Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray</strong></p>
<p>Large Language Models (LLMs) have significantly aided developers by generating or assisting in code writing, enhancing productivity across various tasks. While identifying incorrect code is often straightforward, detecting vulnerabilities in functionally correct code is more challenging, especially for developers with limited security knowledge, which poses considerable security risks of using LLM-generated code and underscores the need for robust evaluation benchmarks that assess both functional correctness and security. Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but are hindered by unclear and impractical specifications, failing to assess both functionality and security accurately. To tackle these deficiencies, we introduce CWEval, a novel outcome-driven evaluation framework designed to enhance the evaluation of secure code generation by LLMs. This framework not only assesses code functionality but also its security simultaneously with high-quality task specifications and outcome-driven test oracles which provides high accuracy. Coupled with CWEval-bench, a multilingual, security-critical coding benchmark, CWEval provides a rigorous empirical security evaluation on LLM-generated code, overcoming previous benchmarksâ€™ shortcomings. Through our evaluations, CWEval reveals a notable portion of functional but insecure code produced by LLMs, and shows a serious inaccuracy of previous evaluations, ultimately contributing significantly to the field of secure code generation. We open-source our artifact at: <a target="_blank" rel="noopener" href="https://github.com/Co1lin/CWEval">https://github.com/Co1lin/CWEval</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæˆ–è¾…åŠ©ç¼–å†™æ–¹é¢ä¸ºå¼€å‘è€…æä¾›äº†å·¨å¤§å¸®åŠ©ï¼Œæé«˜äº†åœ¨å„ç§ä»»åŠ¡ä¸­çš„ç”Ÿäº§æ•ˆç‡ã€‚è™½ç„¶è¯†åˆ«é”™è¯¯çš„ä»£ç é€šå¸¸å¾ˆç›´æ¥ï¼Œä½†åœ¨åŠŸèƒ½æ­£ç¡®çš„ä»£ç ä¸­æ£€æµ‹æ¼æ´æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯å¯¹äºå®‰å…¨çŸ¥è¯†æœ‰é™çš„å¼€å‘è€…æ¥è¯´ã€‚è¿™å¸¦æ¥äº†ä½¿ç”¨LLMç”Ÿæˆä»£ç çš„å®‰å…¨é£é™©ï¼Œå¹¶å¼ºè°ƒéœ€è¦å¯¹åŠŸèƒ½æ­£ç¡®æ€§å’Œå®‰å…¨æ€§è¿›è¡Œè¯„ä¼°çš„ç¨³å¥è¯„ä¼°åŸºå‡†çš„éœ€æ±‚ã€‚å½“å‰çš„åŸºå‡†æµ‹è¯•ï¼Œå¦‚CyberSecEvalå’Œå®‰å…¨è¯„ä¼°ï¼ˆSecurityEvalï¼‰ï¼Œè¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†ç”±äºè§„æ ¼ä¸æ˜ç¡®ä¸”ä¸åˆ‡å®é™…è€Œå—åˆ°é˜»ç¢ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°åŠŸèƒ½å’Œå®‰å…¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†CWEvalï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ç»“æœé©±åŠ¨è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMç”Ÿæˆä»£ç çš„å®‰å…¨è¯„ä¼°èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ä¸ä»…è¯„ä¼°ä»£ç çš„åŠŸèƒ½æ€§ï¼Œè€Œä¸”è¿˜åŒæ—¶è¯„ä¼°å…¶å®‰å…¨æ€§ï¼Œå…·æœ‰é«˜æ ‡å‡†çš„ä»»åŠ¡è§„æ ¼å’Œç»“æœé©±åŠ¨çš„æµ‹è¯•å·¥å…·ï¼Œæä¾›é«˜ç²¾åº¦ã€‚ä¸å¤šè¯­è¨€å®‰å…¨å…³é”®ç¼–ç åŸºå‡†CWEval-benchç›¸ç»“åˆï¼ŒCWEvalå¯¹LLMç”Ÿæˆçš„ä»£ç è¿›è¡Œäº†ä¸¥æ ¼çš„å®è¯å®‰å…¨è¯„ä¼°ï¼Œå…‹æœäº†ä»¥å‰åŸºå‡†æµ‹è¯•çš„ç¼ºç‚¹ã€‚é€šè¿‡æˆ‘ä»¬çš„è¯„ä¼°ï¼ŒCWEvalæ­ç¤ºäº†LLMäº§ç”Ÿçš„åŠŸèƒ½å¼ºå¤§ä½†å­˜åœ¨å®‰å…¨éšæ‚£çš„ä»£ç æ˜¾è‘—éƒ¨åˆ†ï¼Œæ˜¾ç¤ºå‡ºä¹‹å‰è¯„ä¼°çš„ä¸¥é‡ä¸å‡†ç¡®æ€§ï¼Œæœ€ç»ˆä¸ºå®‰å…¨ä»£ç ç”Ÿæˆé¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æˆ‘ä»¬çš„å¼€æºå·¥ä»¶å¯ä»¥åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Co1lin/CWEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Co1lin/CWEvalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08200v1">PDF</a> to be published in LLM4Code 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢ä¸ºå¼€å‘è€…æä¾›äº†æå¤§çš„å¸®åŠ©ï¼Œæé«˜äº†ç”Ÿäº§åŠ›ã€‚ç„¶è€Œï¼Œæ£€æµ‹åŠŸèƒ½æ­£ç¡®ä»£ç ä¸­çš„æ¼æ´æ›´å…·æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå®‰å…¨çŸ¥è¯†æœ‰é™çš„å¼€å‘è€…ï¼Œè¿™å¢åŠ äº†ä½¿ç”¨LLMç”Ÿæˆä»£ç çš„å®‰å…¨é£é™©ã€‚å½“å‰çš„å®‰å…¨è¯„ä¼°åŸºå‡†å¦‚CyberSecEvalå’ŒSecurityEvalå­˜åœ¨æ¨¡ç³Šå’Œä¸åˆ‡å®é™…çš„è§„æ ¼é—®é¢˜ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°åŠŸèƒ½å’Œå®‰å…¨æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CWEvalï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„ç»“æœé©±åŠ¨è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMç”Ÿæˆä»£ç çš„å®‰å…¨æ€§è¯„ä¼°æ°´å¹³ã€‚å®ƒä¸ä»…è¯„ä¼°ä»£ç çš„åŠŸèƒ½æ€§ï¼Œè¿˜åŒæ—¶è¯„ä¼°å…¶å®‰å…¨æ€§ï¼Œå…·æœ‰é«˜è´¨é‡çš„ä»»åŠ¡è§„æ ¼å’Œç»“æœé©±åŠ¨çš„æµ‹è¯•éªŒè¯ï¼Œæä¾›é«˜ç²¾ç¡®åº¦ã€‚æ­é…CWEval-benchâ€”â€”ä¸€ä¸ªå¤šè¯­è¨€ã€å®‰å…¨å…³é”®çš„ç¼–ç åŸºå‡†ï¼ŒCWEvalå¯¹LLMç”Ÿæˆçš„ä»£ç è¿›è¡Œäº†ä¸¥æ ¼çš„å®è¯å®‰å…¨è¯„ä¼°ï¼Œå…‹æœäº†å…ˆå‰åŸºå‡†çš„ç¼ºç‚¹ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCWEvalå‘ç°äº†å¤§é‡ç”±LLMç”Ÿæˆçš„åŠŸèƒ½æ€§ä½†å­˜åœ¨å®‰å…¨éšæ‚£çš„ä»£ç ï¼Œå¹¶æŒ‡å‡ºäº†å…ˆå‰è¯„ä¼°çš„ä¸¥é‡ä¸å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä»£ç ç”Ÿæˆæ–¹é¢å¯¹å¼€å‘è€…æœ‰é‡å¤§å¸®åŠ©ï¼Œä½†å­˜åœ¨æ£€æµ‹åŠŸèƒ½æ­£ç¡®ä»£ç ä¸­çš„æ¼æ´çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰çš„å®‰å…¨è¯„ä¼°åŸºå‡†å¦‚CyberSecEvalå’ŒSecurityEvalå­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°LLMç”Ÿæˆä»£ç çš„åŠŸèƒ½å’Œå®‰å…¨æ€§ã€‚</li>
<li>CWEvalæ˜¯ä¸€ä¸ªæ–°å‹çš„ç»“æœé©±åŠ¨è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMç”Ÿæˆä»£ç çš„å®‰å…¨æ€§è¯„ä¼°æ°´å¹³ã€‚</li>
<li>CWEvalåŒæ—¶è¯„ä¼°ä»£ç çš„åŠŸèƒ½æ€§å’Œå®‰å…¨æ€§ï¼Œå…·æœ‰é«˜è´¨é‡çš„ä»»åŠ¡è§„æ ¼å’Œç»“æœé©±åŠ¨çš„æµ‹è¯•éªŒè¯ã€‚</li>
<li>CWEvalæ­é…å¤šè¯­è¨€ã€å®‰å…¨å…³é”®çš„ç¼–ç åŸºå‡†CWEval-benchï¼Œå¯¹LLMç”Ÿæˆçš„ä»£ç è¿›è¡Œä¸¥æ ¼çš„å®è¯å®‰å…¨è¯„ä¼°ã€‚</li>
<li>CWEvalå‘ç°äº†å¤§é‡ç”±LLMç”Ÿæˆçš„åŠŸèƒ½æ€§ä½†å­˜åœ¨å®‰å…¨éšæ‚£çš„ä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a9074a72e12a2b6c7b2cbaf0d24f4739.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5dd9fcd7c15517c2811037ea58ac50d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3aa2c87dc0ce67ab4302555f5b78cf8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="PRESERVE-Prefetching-Model-Weights-and-KV-Cache-in-Distributed-LLM-Serving"><a href="#PRESERVE-Prefetching-Model-Weights-and-KV-Cache-in-Distributed-LLM-Serving" class="headerlink" title="PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving"></a>PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving</h2><p><strong>Authors:Ahmet Caner YÃ¼zÃ¼gÃ¼ler, Jiawei Zhuang, Lukas Cavigelli</strong></p>
<p>Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åº”ç”¨ä¸­å¾—åˆ°å¹¿æ³›è¿ç”¨ï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å¸¦å®½å†…å­˜ï¼ˆHBMï¼‰ç“¶é¢ˆå’Œè®¾å¤‡é—´é€šä¿¡å¼€é”€æ–¹é¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PRESERVEï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é¢„å–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é‡å æ¨¡å‹æƒé‡å’ŒKVç¼“å­˜çš„å†…å­˜è¯»å–ä¸é›†ä½“é€šä¿¡æ“ä½œæ¥ä¼˜åŒ–LLMæ¨ç†ã€‚æˆ‘ä»¬åœ¨å•†ä¸šäººå·¥æ™ºèƒ½åŠ é€Ÿå™¨ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜åœ¨æœ€æ–°å¼€æºLLMä¸Šæœ€å¤šå¯å®ç°1.6å€ç«¯åˆ°ç«¯åŠ é€Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è®¾è®¡ç©ºé—´æ¢ç´¢ï¼Œç¡®å®šäº†æ‰€ææ–¹æ³•çš„æœ€ä½³ç¡¬ä»¶é…ç½®ï¼Œé€šè¿‡é€‰æ‹©æœ€ä½³çš„L2ç¼“å­˜å¤§å°ï¼Œåœ¨æ€§èƒ½æˆæœ¬æ–¹é¢å®ç°äº†è¿›ä¸€æ­¥çš„1.25å€æå‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒPRESERVEå…·æœ‰ç¼“è§£å†…å­˜ç“¶é¢ˆå’Œé€šä¿¡å¼€é”€çš„æ½œåŠ›ï¼Œä¸ºè§£å†³æé«˜LLMæ¨ç†ç³»ç»Ÿæ€§èƒ½å’Œå¯æ‰©å±•æ€§çš„é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08192v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸåº”ç”¨å¹¿æ³›ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚æŒ‘æˆ˜é‡é‡ï¼Œç‰¹åˆ«æ˜¯é¢ä¸´é«˜é€Ÿç¼“å†²å­˜å‚¨å™¨å¸¦å®½ç“¶é¢ˆå’Œè®¾å¤‡é—´é€šä¿¡å¼€é”€é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºPRESERVEçš„æ–°å‹é¢„å–æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é‡å æ¨¡å‹æƒé‡å’ŒKVç¼“å­˜çš„å†…å­˜è¯»å–ä¸é›†ä½“é€šä¿¡æ“ä½œæ¥ä¼˜åŒ–LLMæ¨ç†ã€‚åœ¨å•†ç”¨äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯¹å…ˆè¿›å¼€æºLLMçš„ç«¯åˆ°ç«¯é€Ÿåº¦æé«˜äº†1.6å€ã€‚å¦å¤–ï¼Œé€šè¿‡è®¾è®¡ç©ºé—´æ¢ç´¢ï¼Œç¡®å®šäº†è¯¥æ–¹æ³•çš„æœ€ä¼˜ç¡¬ä»¶é…ç½®ï¼Œåœ¨é€‰ç”¨æœ€ä½³L2ç¼“å­˜å¤§å°æ—¶ï¼Œæ€§èƒ½æˆæœ¬æ¯”è¿›ä¸€æ­¥æé«˜1.25å€ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒPRESERVEå…·æœ‰ç¼“è§£å†…å­˜ç“¶é¢ˆå’Œé€šä¿¡å¼€é”€çš„æ½œåŠ›ï¼Œä¸ºè§£å†³æé«˜LLMæ¨ç†ç³»ç»Ÿæ€§èƒ½å’Œå¯æ‰©å±•æ€§é—®é¢˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRESERVEæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢„å–æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–LLMæ¨ç†æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡é‡å å†…å­˜è¯»å–å’Œé›†ä½“é€šä¿¡æ“ä½œæ¥å‡å°‘è®¡ç®—å»¶è¿Ÿã€‚</li>
<li>åœ¨å•†ç”¨äººå·¥æ™ºèƒ½åŠ é€Ÿå™¨ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒPRESERVEå¯ä»¥æé«˜ç«¯åˆ°ç«¯é€Ÿåº¦è¾¾1.6å€ã€‚</li>
<li>PRESERVEçš„è®¾è®¡ç©ºé—´æ¢ç´¢ç¡®å®šäº†æœ€ä¼˜ç¡¬ä»¶é…ç½®ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½æˆæœ¬æ¯”ã€‚</li>
<li>é€šè¿‡é€‰ç”¨æœ€ä½³çš„L2ç¼“å­˜å¤§å°ï¼Œå¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½æå‡ã€‚</li>
<li>PRESERVEå…·æœ‰ç¼“è§£å†…å­˜ç“¶é¢ˆå’Œé€šä¿¡å¼€é”€çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d03bbf95055d247964d4af556e6972b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1956eb181ea4f486e432dfe80620de5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9216a948cbda24d51ef4cd2d22c818b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5af25b50f59f3856626740d7aba5b0b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e90d911c35d12dfd9246c6355692c3c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="I-Can-Find-You-in-Seconds-Leveraging-Large-Language-Models-for-Code-Authorship-Attribution"><a href="#I-Can-Find-You-in-Seconds-Leveraging-Large-Language-Models-for-Code-Authorship-Attribution" class="headerlink" title="I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution"></a>I Can Find You in Seconds! Leveraging Large Language Models for Code   Authorship Attribution</h2><p><strong>Authors:Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung</strong></p>
<p>Source code authorship attribution is important in software forensics, plagiarism detection, and protecting software patch integrity. Existing techniques often rely on supervised machine learning, which struggles with generalization across different programming languages and coding styles due to the need for large labeled datasets. Inspired by recent advances in natural language authorship analysis using large language models (LLMs), which have shown exceptional performance without task-specific tuning, this paper explores the use of LLMs for source code authorship attribution.   We present a comprehensive study demonstrating that state-of-the-art LLMs can successfully attribute source code authorship across different languages. LLMs can determine whether two code snippets are written by the same author with zero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of 0.78, and can attribute code authorship from a small set of reference code snippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show some adversarial robustness against misattribution attacks.   Despite these capabilities, we found that naive prompting of LLMs does not scale well with a large number of authors due to input token limitations. To address this, we propose a tournament-style approach for large-scale attribution. Evaluating this approach on datasets of C++ (500 authors, 26,355 samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve classification accuracy of up to 65% for C++ and 68.7% for Java using only one reference per author. These results open new possibilities for applying LLMs to code authorship attribution in cybersecurity and software engineering. </p>
<blockquote>
<p>æºä»£ç ä½œè€…å½’å±åœ¨è½¯ä»¶å–è¯ã€æŠ„è¢­æ£€æµ‹ä»¥åŠè½¯ä»¶è¡¥ä¸å®Œæ•´æ€§ä¿æŠ¤ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰æŠ€æœ¯é€šå¸¸ä¾èµ–äºæœ‰ç›‘ç£æœºå™¨å­¦ä¹ ï¼Œç”±äºéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®é›†ï¼Œå…¶åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€å’Œç¼–ç é£æ ¼ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›å—åˆ°é™åˆ¶ã€‚å—è‡ªç„¶è¯­è¨€ä½œè€…åˆ†ææ–¹é¢è¿‘æœŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›æ­¥çš„å¯å‘ï¼Œè¿™äº›è¿›æ­¥åœ¨æ²¡æœ‰ç‰¹å®šä»»åŠ¡çš„è°ƒæ•´ä¸‹è¡¨ç°å‡ºäº†ä»¤äººæƒŠå¹çš„æ€§èƒ½ï¼Œæœ¬æ–‡æ¢ç´¢äº†ä½¿ç”¨LLMè¿›è¡Œæºä»£ç ä½œè€…å½’å±ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ï¼Œè¡¨æ˜æœ€å…ˆè¿›çš„LLMå¯ä»¥æˆåŠŸåœ°åœ¨ä¸åŒè¯­è¨€ä¹‹é—´è¿›è¡Œæºä»£ç ä½œè€…å½’å±ã€‚LLMå¯ä»¥é€šè¿‡é›¶æ ·æœ¬æç¤ºç¡®å®šä¸¤ä¸ªä»£ç ç‰‡æ®µæ˜¯å¦ç”±åŒä¸€ä½œè€…ç¼–å†™ï¼Œå®ç°é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰ä¸º0.78ï¼Œå¹¶ä¸”å¯ä»¥ä»å°è§„æ¨¡çš„å‚è€ƒä»£ç ç‰‡æ®µé›†ä¸­é€šè¿‡å°‘é‡å­¦ä¹ è¿›è¡Œä»£ç ä½œè€…å½’å±ï¼Œå®ç°MCCä¸º0.77ã€‚æ­¤å¤–ï¼ŒLLMå¯¹ä¸€äº›è¯¯åˆ¤æ”»å‡»è¿˜æ˜¾ç¤ºå‡ºä¸€å®šçš„å¯¹æŠ—æ€§ç¨³å¥æ€§ã€‚å°½ç®¡å…·å¤‡äº†è¿™äº›åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬å‘ç°å¯¹LLMçš„ç®€å•æç¤ºå¹¶ä¸é€‚ç”¨äºå¤§é‡ä½œè€…çš„æƒ…å†µï¼Œå› ä¸ºå­˜åœ¨è¾“å…¥æ ‡è®°çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¤§è§„æ¨¡å½’å±çš„é”¦æ ‡èµ›å¼æ–¹æ³•ã€‚åœ¨GitHubä¸Šçš„C++ï¼ˆ500ä½ä½œè€…ï¼Œ26355ä¸ªæ ·æœ¬ï¼‰å’ŒJavaï¼ˆ686ä½ä½œè€…ï¼Œ55267ä¸ªæ ·æœ¬ï¼‰æ•°æ®é›†ä¸Šè¯„ä¼°æ­¤æ–¹æ³•ï¼Œå¯¹äºæ¯ä½ä½œè€…ä»…ä½¿ç”¨ä¸€ä¸ªå‚è€ƒæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾65%çš„C++åˆ†ç±»å‡†ç¡®ç‡å’Œ68.7%çš„Javaåˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœå¼€å¯äº†å°†LLMåº”ç”¨äºç½‘ç»œå®‰å…¨å’Œè½¯ä»¶å·¥ç¨‹ä¸­çš„ä»£ç ä½œè€…å½’å±çš„æ–°å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08165v1">PDF</a> 12 pages, 5 figures,</p>
<p><strong>æ‘˜è¦</strong><br>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯åœ¨è½¯ä»¶ä»£ç çš„ä½œè€…èº«ä»½é‰´åˆ«ä¸Šè¿›è¡Œäº†ä¸€é¡¹åˆ›æ–°ç ”ç©¶ã€‚è¯¥ç ”ç©¶æ¢è®¨äº†é›¶å°„æç¤ºå’Œå°‘å°„å­¦ä¹ æŠ€æœ¯åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸­çš„ä»£ç ä½œè€…èº«ä»½é‰´åˆ«åº”ç”¨ï¼Œå¹¶å‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ä½œè€…èº«ä»½é‰´åˆ«ä¸Šå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚å°½ç®¡å­˜åœ¨è¾“å…¥ä»¤ç‰Œé™åˆ¶ï¼Œä½†é€šè¿‡é”¦æ ‡èµ›é£æ ¼çš„ç­–ç•¥å¯ä»¥åœ¨å¤§è§„æ¨¡å±æ€§é‰´åˆ«ä¸­å®ç°åˆ†ç±»å‡†ç¡®ç‡é«˜è¾¾65%ï¼ˆé’ˆå¯¹C++ï¼‰å’Œ68.7%ï¼ˆé’ˆå¯¹Javaï¼‰ã€‚è¿™ä¸ºåœ¨ç½‘ç»œå®‰å…¨å’Œè½¯ä»¶å·¥ç¨‹ä¸­åº”ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä»£ç ä½œè€…èº«ä»½é‰´åˆ«æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>æºç ä½œè€…å½’å±åœ¨è½¯ä»¶å–è¯ã€æŠ„è¢­æ£€æµ‹å’Œè½¯ä»¶è¡¥ä¸ä¿æŠ¤ä¸­å æ®é‡è¦åœ°ä½ã€‚ä¼ ç»ŸæŠ€æœ¯å¾€å¾€ä¾èµ–äºç›‘ç£æœºå™¨å­¦ä¹ ï¼Œè¿™åœ¨è·¨ä¸åŒç¼–ç¨‹è¯­è¨€å’Œç¼–ç é£æ ¼æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯èƒ½æˆåŠŸåº”ç”¨åœ¨ä»£ç ä½œè€…èº«ä»½é‰´åˆ«ä¸Šï¼Œä¸”è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡çš„è°ƒæ•´å’Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡é›¶å°„æç¤ºæŠ€æœ¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆ¤æ–­ä¸¤ä¸ªä»£ç ç‰‡æ®µæ˜¯å¦ç”±åŒä¸€ä½œè€…ç¼–å†™ï¼Œä¸”å…·æœ‰è¾ƒé«˜çš„é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ï¼ˆMCCï¼‰è¾¾åˆ°0.78ã€‚æ­¤å¤–ï¼Œä½¿ç”¨å°‘é‡çš„å‚è€ƒä»£ç ç‰‡æ®µè¿›è¡Œå°‘é‡å­¦ä¹ ä¹Ÿèƒ½è¾¾åˆ°è¾ƒé«˜çš„é©¬ä¿®æ–¯ç›¸å…³ç³»æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3ba44bf30dae6d243d65ca517465ed2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c32f47ed7fd1b61a38178840cda9932.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832769875fc44830ccb1bfce922bd5cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237ec6bfecd9b962568537e68dc31921.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38671386cf8e269db94af2e29c299467.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dbb2ea110deca59a9425d4b300a61ae0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Change-Captioning-in-Remote-Sensing-Evolution-to-SAT-Cap-â€“-A-Single-Stage-Transformer-Approach"><a href="#Change-Captioning-in-Remote-Sensing-Evolution-to-SAT-Cap-â€“-A-Single-Stage-Transformer-Approach" class="headerlink" title="Change Captioning in Remote Sensing: Evolution to SAT-Cap â€“ A   Single-Stage Transformer Approach"></a>Change Captioning in Remote Sensing: Evolution to SAT-Cap â€“ A   Single-Stage Transformer Approach</h2><p><strong>Authors:Yuduo Wang, Weikang Yu, Pedram Ghamisi</strong></p>
<p>Change captioning has become essential for accurately describing changes in multi-temporal remote sensing data, providing an intuitive way to monitor Earthâ€™s dynamics through natural language. However, existing change captioning methods face two key challenges: high computational demands due to multistage fusion strategy, and insufficient detail in object descriptions due to limited semantic extraction from individual images. To solve these challenges, we propose SAT-Cap based on the transformers model with a single-stage feature fusion for remote sensing change captioning. In particular, SAT-Cap integrates a Spatial-Channel Attention Encoder, a Difference-Guided Fusion module, and a Caption Decoder. Compared to typical models that require multi-stage fusion in transformer encoder and fusion module, SAT-Cap uses only a simple cosine similarity-based fusion module for information integration, reducing the complexity of the model architecture. By jointly modeling spatial and channel information in Spatial-Channel Attention Encoder, our approach significantly enhances the modelâ€™s ability to extract semantic information from objects in multi-temporal remote sensing images. Extensive experiments validate the effectiveness of SAT-Cap, achieving CIDEr scores of 140.23% on the LEVIR-CC dataset and 97.74% on the DUBAI-CC dataset, surpassing current state-of-the-art methods. The code and pre-trained models will be available online. </p>
<blockquote>
<p>å˜åŒ–æ ‡æ³¨å·²æˆä¸ºå‡†ç¡®æè¿°å¤šæ—¶ç›¸é¥æ„Ÿæ•°æ®å˜åŒ–çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æä¾›äº†ä¸€ç§ç›´è§‚ç›‘æµ‹åœ°çƒåŠ¨æ€çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å˜åŒ–æ ‡æ³¨æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯ç”±äºå¤šé˜¶æ®µèåˆç­–ç•¥å¯¼è‡´çš„é«˜è®¡ç®—éœ€æ±‚ï¼ŒäºŒæ˜¯ç”±äºä»å•ä¸ªå›¾åƒä¸­æå–æœ‰é™è¯­ä¹‰è€Œå¯¼è‡´çš„ç‰©ä½“æè¿°ç»†èŠ‚ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè½¬æ¢å™¨æ¨¡å‹çš„SAT-Capé¥æ„Ÿå˜åŒ–æ ‡æ³¨æ–¹æ³•ï¼Œé‡‡ç”¨å•é˜¶æ®µç‰¹å¾èåˆã€‚ç‰¹åˆ«æ˜¯ï¼ŒSAT-Capé›†æˆäº†ç©ºé—´é€šé“æ³¨æ„ç¼–ç å™¨ã€å·®å¼‚å¯¼å‘èåˆæ¨¡å—å’Œå­—å¹•è§£ç å™¨ã€‚ä¸å…¸å‹éœ€è¦è½¬æ¢å™¨ç¼–ç å™¨å’Œèåˆæ¨¡å—å¤šé˜¶æ®µèåˆçš„æ¨¡å‹ç›¸æ¯”ï¼ŒSAT-Capä»…ä½¿ç”¨ä¸€ä¸ªç®€å•çš„åŸºäºä½™å¼¦ç›¸ä¼¼æ€§çš„èåˆæ¨¡å—è¿›è¡Œä¿¡æ¯æ•´åˆï¼Œé™ä½äº†æ¨¡å‹æ¶æ„çš„å¤æ‚æ€§ã€‚é€šè¿‡ç©ºé—´é€šé“æ³¨æ„ç¼–ç å™¨è”åˆå»ºæ¨¡ç©ºé—´å’Œé€šé“ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹ä»å¤šæ—¶ç›¸é¥æ„Ÿå›¾åƒä¸­çš„ç‰©ä½“æå–è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚å¤§é‡å®éªŒéªŒè¯äº†SAT-Capçš„æœ‰æ•ˆæ€§ï¼Œåœ¨LEVIR-CCæ•°æ®é›†ä¸Šè¾¾åˆ°äº†CIDEråˆ†æ•°140.23%ï¼Œåœ¨DUBAI-CCæ•°æ®é›†ä¸Šè¾¾åˆ°äº†97.74%ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†åœ¨çº¿æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºTransformeræ¨¡å‹çš„å•é˜¶æ®µç‰¹å¾èåˆé¥æ„Ÿå˜åŒ–æ ‡æ³¨æ–¹æ³•ï¼ˆSAT-Capï¼‰èƒ½æœ‰æ•ˆè§£å†³ç°æœ‰å˜åŒ–æ ‡æ³¨é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¡ç®—éœ€æ±‚é«˜å’Œå¯¹è±¡æè¿°ç»†èŠ‚ä¸è¶³çš„é—®é¢˜ã€‚SAT-Capé€šè¿‡æ•´åˆç©ºé—´é€šé“æ³¨æ„åŠ›ç¼–ç å™¨ã€å·®å¼‚å¼•å¯¼èåˆæ¨¡å—å’Œå­—å¹•è§£ç å™¨ï¼Œé‡‡ç”¨åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„ç®€å•èåˆæ¨¡å—è¿›è¡Œä¿¡æ¯æ•´åˆï¼Œé™ä½æ¨¡å‹ç»“æ„å¤æ‚æ€§ã€‚åœ¨LEVIR-CCå’ŒDUBAI-CCæ•°æ®é›†ä¸Šå–å¾—ä¼˜å¼‚è¡¨ç°ï¼ŒCIDErå¾—åˆ†åˆ†åˆ«é«˜è¾¾140.23%å’Œ97.74%ï¼Œè¶…è¶Šç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¥æ„Ÿå˜åŒ–æ ‡æ³¨çš„é‡è¦æ€§åœ¨äºèƒ½ç›´è§‚åœ°é€šè¿‡è‡ªç„¶è¯­è¨€ç›‘æµ‹åœ°çƒåŠ¨æ€å˜åŒ–ã€‚</li>
<li>ç›®å‰å˜åŒ–æ ‡æ³¨é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè®¡ç®—éœ€æ±‚é«˜å’Œå¯¹è±¡æè¿°ç»†èŠ‚ä¸è¶³ã€‚</li>
<li>SAT-CapåŸºäºTransformeræ¨¡å‹ï¼Œé‡‡ç”¨å•é˜¶æ®µç‰¹å¾èåˆè¿›è¡Œé¥æ„Ÿå˜åŒ–æ ‡æ³¨ã€‚</li>
<li>SAT-Capé€šè¿‡æ•´åˆç©ºé—´é€šé“æ³¨æ„åŠ›ç¼–ç å™¨å’Œå·®å¼‚å¼•å¯¼èåˆæ¨¡å—æå‡æ¨¡å‹ä»å¤šæ—¶ç›¸é¥æ„Ÿå›¾åƒä¸­æå–è¯­ä¹‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>SAT-Capä½¿ç”¨ç®€å•çš„åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„èåˆæ¨¡å—è¿›è¡Œä¿¡æ¯æ•´åˆï¼Œé™ä½æ¨¡å‹å¤æ‚æ€§ã€‚</li>
<li>SAT-Capåœ¨LEVIR-CCå’ŒDUBAI-CCæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒCIDErå¾—åˆ†é«˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ab2491693b46f0bd3117af66202b252.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bbfbc24ef160a7ca1bece00e292eb37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f328b873709926657651842559e711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd4e2dd37efd1640db7c9f2f3fe211bc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transforming-Indoor-Localization-Advanced-Transformer-Architecture-for-NLOS-Dominated-Wireless-Environments-with-Distributed-Sensors"><a href="#Transforming-Indoor-Localization-Advanced-Transformer-Architecture-for-NLOS-Dominated-Wireless-Environments-with-Distributed-Sensors" class="headerlink" title="Transforming Indoor Localization: Advanced Transformer Architecture for   NLOS Dominated Wireless Environments with Distributed Sensors"></a>Transforming Indoor Localization: Advanced Transformer Architecture for   NLOS Dominated Wireless Environments with Distributed Sensors</h2><p><strong>Authors:Saad Masrur,  Jung-Fu,  Cheng, Atieh R. Khamesi, Ismail Guvenc</strong></p>
<p>Indoor localization in challenging non-line-of-sight (NLOS) environments often leads to mediocre accuracy with traditional approaches. Deep learning (DL) has been applied to tackle these challenges; however, many DL approaches overlook computational complexity, especially for floating-point operations (FLOPs), making them unsuitable for resource-limited devices. Transformer-based models have achieved remarkable success in natural language processing (NLP) and computer vision (CV) tasks, motivating their use in wireless applications. However, their use in indoor localization remains nascent, and directly applying Transformers for indoor localization can be both computationally intensive and exhibit limitations in accuracy. To address these challenges, in this work, we introduce a novel tokenization approach, referred to as Sensor Snapshot Tokenization (SST), which preserves variable-specific representations of power delay profile (PDP) and enhances attention mechanisms by effectively capturing multi-variate correlation. Complementing this, we propose a lightweight Swish-Gated Linear Unit-based Transformer (L-SwiGLU Transformer) model, designed to reduce computational complexity without compromising localization accuracy. Together, these contributions mitigate the computational burden and dependency on large datasets, making Transformer models more efficient and suitable for resource-constrained scenarios. The proposed tokenization method enables the Vanilla Transformer to achieve a 90th percentile positioning error of 0.388 m in a highly NLOS indoor factory, surpassing conventional tokenization methods. The L-SwiGLU ViT further reduces the error to 0.355 m, achieving an 8.51% improvement. Additionally, the proposed model outperforms a 14.1 times larger model with a 46.13% improvement, underscoring its computational efficiency. </p>
<blockquote>
<p>åœ¨å®¤å†…å®šä½é¢†åŸŸä¸­ï¼ŒæŒ‘æˆ˜éç›´è§†ï¼ˆNLOSï¼‰ç¯å¢ƒä¸‹é‡‡ç”¨ä¼ ç»Ÿæ–¹æ³•çš„å‡†ç¡®æ€§å¾€å¾€è¡¨ç°å¹³åº¸ã€‚æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å·²åº”ç”¨äºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼›ç„¶è€Œï¼Œè®¸å¤šæ·±åº¦å­¦ä¹ æ–¹æ³•å¿½è§†äº†è®¡ç®—å¤æ‚æ€§ï¼Œå°¤å…¶æ˜¯æµ®ç‚¹è¿ç®—ï¼ˆFLOPsï¼‰ï¼Œä½¿å¾—å®ƒä»¬ä¸é€‚åˆèµ„æºå—é™çš„è®¾å¤‡ã€‚åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œæ¿€å‘äº†å®ƒä»¬åœ¨æ— çº¿åº”ç”¨ä¸­çš„ä½¿ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®¤å†…å®šä½ä¸­çš„åº”ç”¨ä»å¤„äºåˆçº§é˜¶æ®µï¼Œç›´æ¥åº”ç”¨Transformerè¿›è¡Œå®¤å†…å®šä½å¯èƒ½ä¼šè®¡ç®—é‡å¤§ä¸”ç²¾åº¦æœ‰é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä»¤ç‰ŒåŒ–æ–¹æ³•ï¼Œç§°ä¸ºSensor Snapshot Tokenizationï¼ˆSSTï¼‰ï¼Œå®ƒä¿ç•™äº†åŠŸç‡å»¶è¿Ÿé…ç½®æ–‡ä»¶ï¼ˆPDPï¼‰çš„å˜é‡ç‰¹å®šè¡¨ç¤ºï¼Œå¹¶é€šè¿‡æœ‰æ•ˆåœ°æ•è·å¤šå˜é‡ç›¸å…³æ€§å¢å¼ºäº†æ³¨æ„åŠ›æœºåˆ¶ã€‚ä½œä¸ºè¡¥å……ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè½»é‡çº§Swishé—¨æ§çº¿æ€§å•å…ƒï¼ˆL-SwiGLUï¼‰çš„Transformeræ¨¡å‹ï¼Œæ—¨åœ¨é™ä½è®¡ç®—å¤æ‚åº¦è€Œä¸å¦¥åå®šä½ç²¾åº¦ã€‚è¿™äº›è´¡çŒ®å…±åŒå‡è½»äº†è®¡ç®—è´Ÿæ‹…å’Œå¯¹å¤§æ•°æ®é›†çš„ä¾èµ–ï¼Œä½¿Transformeræ¨¡å‹åœ¨èµ„æºå—é™çš„åœºæ™¯ä¸­æ›´åŠ é«˜æ•ˆé€‚ç”¨ã€‚æ‰€æå‡ºçš„ä»¤ç‰ŒåŒ–æ–¹æ³•ä½¿å¾—æ™®é€šTransformerèƒ½å¤Ÿåœ¨é«˜åº¦éç›´è§†çš„å®¤å†…å·¥å‚ä¸­å®ç°90ç™¾åˆ†ä½å®šä½è¯¯å·®ä¸º0.388ç±³ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„ä»¤ç‰ŒåŒ–æ–¹æ³•ã€‚L-SwiGLU ViTè¿›ä¸€æ­¥å°†è¯¯å·®é™ä½åˆ°0.355ç±³ï¼Œå®ç°äº†8.51%çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹è¿˜ä¼˜äºä¸€ä¸ªè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ï¼ˆè§„æ¨¡æ›´å¤§14.1å€ï¼‰ï¼Œå…·æœ‰46.13%çš„æ”¹è¿›ï¼Œçªæ˜¾äº†å…¶è®¡ç®—æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07774v1">PDF</a> The paper has been submitted to IEEE Transactions on Machine Learning   in Communications and Networking</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„éè§†è·ï¼ˆNLOSï¼‰å®¤å†…å®šä½ç¯å¢ƒä¸­ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ³•ã€‚ç ”ç©¶è€…å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ ‡è®°åŒ–æ–¹æ³•â€”â€”ä¼ æ„Ÿå™¨å¿«ç…§æ ‡è®°åŒ–ï¼ˆSSTï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä¿ç•™ç”µåŠ›å»¶è¿Ÿç‰¹å¾çš„æ•°æ®ç»“æ„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†åŸºäºSwishé—¨æ§çº¿æ€§å•å…ƒè®¾è®¡çš„è½»é‡åŒ–Transformeræ¨¡å‹ï¼Œä»¥é™ä½è®¡ç®—å¤æ‚æ€§å¹¶ä¿ç•™å®šä½å‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›åˆ›æ–°æŠ€æœ¯ï¼Œå¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æ¡ä»¶ä¸‹å®ç°é«˜æ•ˆå‡†ç¡®çš„å®¤å†…å®šä½ã€‚ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼Œæ–°çš„æŠ€æœ¯æ¨¡å‹æ˜¾è‘—æé«˜äº†å®šä½ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®¤å†…å®šä½åœ¨æŒ‘æˆ˜æ€§çš„éè§†è·ç¯å¢ƒä¸­å‡†ç¡®åº¦é™ä½çš„é—®é¢˜å­˜åœ¨ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ åœ¨è§£å†³è¿™äº›é—®é¢˜ä¸Šå‘æŒ¥äº†ä½œç”¨ï¼Œä½†åœ¨å¤„ç†æµ®ç‚¹æ•°æ“ä½œä¸Šçš„è®¡ç®—å¤æ‚æ€§å¯¹èµ„æºæœ‰é™çš„è®¾å¤‡æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¯¥é—®é¢˜ï¼Œä¸€ç§æ–°çš„Sensor Snapshot Tokenizationï¼ˆSSTï¼‰æŠ€æœ¯è¢«å¼•å…¥ç”¨äºå¤„ç†æ•°æ®ç‰¹ç‚¹ä¸­çš„æŸäº›å¤æ‚éƒ¨åˆ†ï¼Œå¹¶ä¿ç•™é‡è¦çš„æ•°æ®ç»“æ„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07774">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96b2facbb2705ef1fadd1a38b5e6c32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-346fb2f022de7d150ac480fdc05eadb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bed254b56811c9ed339836c819f7ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48255a8e6083fc3131481bc5d4a7d1c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-955b75e7d5ceb856eb0946b5442d2eaa.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics"><a href="#Transforming-Role-Classification-in-Scientific-Teams-Using-LLMs-and-Advanced-Predictive-Analytics" class="headerlink" title="Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics"></a>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</h2><p><strong>Authors:Wonduk Seo, Yi Bu</strong></p>
<p>Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications â€“ such as author-publication history, author affiliation, research topics, and citation counts â€“ we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </p>
<blockquote>
<p>ç§‘ç ”å›¢é˜Ÿçš„åŠ¨æ€åœ¨å†³å®šç ”ç©¶æˆæœçš„æ€§è´¨å’Œå½±å“æ–¹é¢è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å¯¹è´¡çŒ®çš„å…¨é¢æƒ…å¢ƒåˆ†æã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç§‘ç ”å›¢é˜Ÿä¸­çš„ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»çš„å˜é©æ€§æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„èšç±»æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•æä¾›äº†æ›´ä¸ºç²¾ç»†çš„åˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ©ç”¨å¼€æºå’Œä¸“æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4ã€Llama 3 70Bã€Llama 2 70Bå’ŒMistral 7x8Bï¼‰æ¥è¡¥å……å’Œæ”¹è¿›è¿™äº›æ–¹æ³•ï¼Œä»¥è¿›è¡Œè§’è‰²åˆ†ç±»ã€‚é€šè¿‡å°‘æ ·æœ¬æç¤ºï¼Œæˆ‘ä»¬å¯¹ä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯æ˜GPT-4åœ¨å¤šç±»åˆ«ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„XGBoostå’ŒBERTç­‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜åŒ…æ‹¬å»ºç«‹ä¸€ä¸ªä½¿ç”¨10ä¸ªç‰¹å¾çš„é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚é€šè¿‡åœ¨OpenAlexæ•°æ®åº“è¡ç”Ÿçš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ï¼Œè¯¥æ•°æ®åº“æä¾›äº†å…³äºå­¦æœ¯å‡ºç‰ˆç‰©çš„è¯¦ç»†å…ƒæ•°æ®ï¼ˆå¦‚ä½œè€…å‡ºç‰ˆå†å²ã€ä½œè€…å…³è”ã€ç ”ç©¶ä¸»é¢˜å’Œå¼•ç”¨è®¡æ•°ï¼‰ï¼Œæˆ‘ä»¬è·å¾—äº†0.76çš„F1åˆ†æ•°ï¼Œè¯æ˜äº†ä½œè€…è§’è‰²ç¨³å¥çš„åˆ†ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07267v1">PDF</a> 14 pages, 4 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>ç§‘ç ”å›¢é˜Ÿä¸­çš„åŠ¨æ€å¯¹ç ”ç©¶ç»“æœæœ‰ç€é‡è¦å½±å“ã€‚ç°æœ‰çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ä¸»è¦åŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»ï¼Œç¼ºä¹å¯¹è´¡çŒ®çš„å…¨æ–¹ä½ä¸Šä¸‹æ–‡åˆ†æã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç§‘ç ”å›¢é˜Ÿä½œè€…è§’è‰²è¿›è¡Œåˆ†ç±»çš„é©æ–°æ–¹æ³•ï¼Œç›¸æ¯”ä¼ ç»Ÿèšç±»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æä¾›äº†æ›´ä¸ºç²¾ç»†çš„åˆ†æã€‚æœ¬ç ”ç©¶ä½¿ç”¨å¼€æºå’Œä¸“æœ‰LLMï¼Œå¦‚GPT-4ã€Llama3 70Bã€Llama2 70Bå’ŒMistral 7x8Bç­‰å·¥å…·è¿›è¡Œè§’è‰²åˆ†ç±»ã€‚é€šè¿‡å°‘æ ·æœ¬æç¤ºè¿›è¡Œè§’è‰²åˆ†ç±»ï¼Œå¹¶å‘ç°GPT-4åœ¨å¤šç±»åˆ«ä¸­è¡¨ç°æœ€ä½³ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„XGBoostå’ŒBERTç­‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜ä½¿ç”¨OpenAlexæ•°æ®åº“çš„è¯¦ç»†å…ƒä¿¡æ¯å»ºç«‹äº†æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å–å¾—äº†F1åˆ†æ•°ä¸º0.76çš„è‰¯å¥½æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½œè€…è§’è‰²åˆ†ç±»åœ¨ç§‘ç ”å›¢é˜Ÿä¸­è‡³å…³é‡è¦ï¼Œå½±å“ç ”ç©¶è´¨é‡å’Œç»“æœã€‚</li>
<li>ä¼ ç»ŸåŸºäºè‡ªæˆ‘æŠ¥å‘Šå’Œèšç±»çš„ä½œè€…è§’è‰²åˆ†ç±»æ–¹æ³•ç¼ºä¹å…¨é¢çš„ä¸Šä¸‹æ–‡åˆ†æã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä½œè€…è§’è‰²åˆ†ç±»æ˜¯ä¸€ç§åˆ›æ–°ä¸”ç²¾ç»†çš„æ–¹æ³•ã€‚</li>
<li>GPT-4åœ¨å¤šç§åˆ†ç±»ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œæˆä¸ºæœ€ä½³çš„è¯­è¨€æ¨¡å‹å·¥å…·ã€‚</li>
<li>ç»“åˆå°‘æ ·æœ¬æç¤ºè¿›è¡Œè§’è‰²åˆ†ç±»æ˜¯æœ‰æ•ˆçš„ã€‚</li>
<li>ä½¿ç”¨OpenAlexæ•°æ®åº“çš„è¯¦ç»†å…ƒä¿¡æ¯å»ºç«‹çš„æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹å–å¾—äº†è¾ƒé«˜çš„F1åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12b34ca472332862f19792303db83430.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-451d4ddb239873b9c80879e371f6b5a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a17099fd3b9b031368e854102a95bda.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Breaking-Memory-Limits-Gradient-Wavelet-Transform-Enhances-LLMs-Training"><a href="#Breaking-Memory-Limits-Gradient-Wavelet-Transform-Enhances-LLMs-Training" class="headerlink" title="Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs   Training"></a>Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs   Training</h2><p><strong>Authors:Ziqing Wen, Ping Luo, Jiahuan Wang, Xiaoge Deng, Jinping Zou, Kun Yuan, Tao Sun, Dongsheng Li</strong></p>
<p>Large language models (LLMs) have shown impressive performance across a range of natural language processing tasks. However, their vast number of parameters introduces significant memory challenges during training, particularly when using memory-intensive optimizers like Adam. Existing memory-efficient algorithms often rely on techniques such as singular value decomposition projection or weight freezing. While these approaches help alleviate memory constraints, they generally produce suboptimal results compared to full-rank updates. In this paper, we investigate the memory-efficient method beyond low-rank training, proposing a novel solution called Gradient Wavelet Transform (GWT), which applies wavelet transforms to gradients in order to significantly reduce the memory requirements for maintaining optimizer states. We demonstrate that GWT can be seamlessly integrated with memory-intensive optimizers, enabling efficient training without sacrificing performance. Through extensive experiments on both pre-training and fine-tuning tasks, we show that GWT achieves state-of-the-art performance compared with advanced memory-efficient optimizers and full-rank approaches in terms of both memory usage and training performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶åºå¤§çš„å‚æ•°æ•°é‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥äº†é‡å¤§çš„å†…å­˜æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“ä½¿ç”¨å¦‚Adamè¿™æ ·çš„å†…å­˜å¯†é›†å‹ä¼˜åŒ–å™¨æ—¶ã€‚ç°æœ‰çš„å†…å­˜é«˜æ•ˆç®—æ³•é€šå¸¸ä¾èµ–äºå¥‡å¼‚å€¼åˆ†è§£æŠ•å½±æˆ–æƒé‡å†»ç»“ç­‰æŠ€æœ¯ã€‚è™½ç„¶è¿™äº›æ–¹æ³•æœ‰åŠ©äºç¼“è§£å†…å­˜çº¦æŸï¼Œä½†å®ƒä»¬é€šå¸¸äº§ç”Ÿçš„ç»“æœä¸å¦‚å…¨ç§©æ›´æ–°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½ç§©è®­ç»ƒä¹‹å¤–çš„å†…å­˜é«˜æ•ˆæ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§ç§°ä¸ºæ¢¯åº¦å°æ³¢å˜æ¢ï¼ˆGWTï¼‰çš„æ–°è§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå°†å°æ³¢å˜æ¢åº”ç”¨äºæ¢¯åº¦ï¼Œä»¥æ˜¾è‘—é™ä½ç»´æŠ¤ä¼˜åŒ–å™¨çŠ¶æ€æ‰€éœ€çš„å†…å­˜è¦æ±‚ã€‚æˆ‘ä»¬è¯æ˜GWTå¯ä»¥æ— ç¼é›†æˆåˆ°å†…å­˜å¯†é›†å‹ä¼˜åŒ–å™¨ä¸­ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒè€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒå’Œå¾®è°ƒä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨å†…å­˜ä½¿ç”¨é‡å’Œè®­ç»ƒæ€§èƒ½æ–¹é¢ï¼ŒGWTä¸å…ˆè¿›çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨å’Œå…¨ç§©æ–¹æ³•ç›¸æ¯”å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07237v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„å‚æ•°æ•°é‡åœ¨è®­ç»ƒæ—¶å¸¦æ¥å†…å­˜æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å¦‚Adamç­‰å†…å­˜å¯†é›†å‹ä¼˜åŒ–å™¨æ—¶ã€‚ç°æœ‰å†…å­˜é«˜æ•ˆç®—æ³•å¸¸ä¾èµ–ä½é˜¶æŠ€æœ¯æˆ–æƒé‡å†»ç»“ç­‰æŠ€æœ¯ï¼Œè™½ç„¶æœ‰åŠ©äºç¼“è§£å†…å­˜å‹åŠ›ï¼Œä½†é€šå¸¸äº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºæ¢¯åº¦å°æ³¢å˜æ¢ï¼ˆGWTï¼‰çš„æ–°å‹å†…å­˜é«˜æ•ˆæ–¹æ³•ï¼Œé€šè¿‡åº”ç”¨å°æ³¢å˜æ¢å¯¹æ¢¯åº¦è¿›è¡Œå¤„ç†ï¼Œæ˜¾è‘—é™ä½ç»´æŠ¤ä¼˜åŒ–å™¨çŠ¶æ€æ‰€éœ€çš„å†…å­˜ã€‚å®éªŒè¯æ˜ï¼ŒGWTå¯æ— ç¼é›†æˆäºå†…å­˜å¯†é›†å‹ä¼˜åŒ–å™¨ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒä¸”ä¸å½±å“æ€§èƒ½ã€‚ä¸å…ˆè¿›å†…å­˜ä¼˜åŒ–å™¨å’Œå…¨é˜¶æ–¹æ³•ç›¸æ¯”ï¼ŒGWTåœ¨å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ€§èƒ½ä¸Šå‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œä½†è®­ç»ƒæ—¶é¢ä¸´å†…å­˜æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å†…å­˜é«˜æ•ˆç®—æ³•å¯èƒ½äº§ç”Ÿæ¬¡ä¼˜ç»“æœã€‚</li>
<li>æ¢¯åº¦å°æ³¢å˜æ¢ï¼ˆGWTï¼‰æ˜¯ä¸€ç§æ–°å‹å†…å­˜é«˜æ•ˆæ–¹æ³•ï¼Œé€šè¿‡åº”ç”¨å°æ³¢å˜æ¢é™ä½å†…å­˜éœ€æ±‚ã€‚</li>
<li>GWTå¯æ— ç¼é›†æˆäºå†…å­˜å¯†é›†å‹ä¼˜åŒ–å™¨ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>GWTåœ¨å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ€§èƒ½ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›å†…å­˜ä¼˜åŒ–å™¨å’Œå…¨é˜¶æ–¹æ³•ã€‚</li>
<li>GWTæœ‰åŠ©äºè§£å†³LLMsè®­ç»ƒä¸­çš„å†…å­˜æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨ä½¿ç”¨å†…å­˜å¯†é›†å‹ä¼˜åŒ–å™¨æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f7640f9ab8aeb80dbdde556224c4745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5f81bef6cd8380de46e953a708517b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-054c167406c5653f39573bdfa40b32d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e01ab63f14c8b8220bedd1ff3c01cb1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="How-GPT-learns-layer-by-layer"><a href="#How-GPT-learns-layer-by-layer" class="headerlink" title="How GPT learns layer by layer"></a>How GPT learns layer by layer</h2><p><strong>Authors:Jason Du, Kelly Hong, Alishba Imran, Erfan Jahanparast, Mehdi Khfifi, Kaichun Qiao</strong></p>
<p>Large Language Models (LLMs) excel at tasks like language processing, strategy games, and reasoning but struggle to build generalizable internal representations essential for adaptive decision-making in agents. For agents to effectively navigate complex environments, they must construct reliable world models. While LLMs perform well on specific benchmarks, they often fail to generalize, leading to brittle representations that limit their real-world effectiveness. Understanding how LLMs build internal world models is key to developing agents capable of consistent, adaptive behavior across tasks. We analyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a controlled testbed for studying representation learning. Despite being trained solely on next-token prediction with random valid moves, OthelloGPT shows meaningful layer-wise progression in understanding board state and gameplay. Early layers capture static attributes like board edges, while deeper layers reflect dynamic tile changes. To interpret these representations, we compare Sparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more robust, disentangled insights into compositional features, whereas linear probes mainly detect features useful for classification. We use SAEs to decode features related to tile color and tile stability, a previously unexamined feature that reflects complex gameplay concepts like board control and long-term planning. We study the progression of linear probe accuracy and tile color using both SAEâ€™s and linear probes to compare their effectiveness at capturing what the model is learning. Although we begin with a smaller language model, OthelloGPT, this study establishes a framework for understanding the internal representations learned by GPT models, transformers, and LLMs more broadly. Our code is publicly available: <a target="_blank" rel="noopener" href="https://github.com/ALT-JS/OthelloSAE">https://github.com/ALT-JS/OthelloSAE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿è¯­è¨€å¤„ç†ã€ç­–ç•¥æ¸¸æˆå’Œæ¨ç†ç­‰ä»»åŠ¡ï¼Œä½†åœ¨æ„å»ºå¯¹äºæ™ºèƒ½ä½“è‡ªé€‚åº”å†³ç­–è‡³å…³é‡è¦çš„å¯æ¦‚æ‹¬å†…éƒ¨è¡¨ç¤ºæ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è®©æ™ºèƒ½ä½“åœ¨å¤æ‚ç¯å¢ƒä¸­æœ‰æ•ˆå¯¼èˆªï¼Œå®ƒä»¬å¿…é¡»æ„å»ºå¯é çš„ä¸–ç•Œæ¨¡å‹ã€‚è™½ç„¶LLMåœ¨ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥æ¦‚æ‹¬ï¼Œå¯¼è‡´è„†å¼±çš„è¡¨ç¤ºå½¢å¼ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„æœ‰æ•ˆæ€§ã€‚äº†è§£LLMå¦‚ä½•æ„å»ºå†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼Œå¯¹äºå¼€å‘èƒ½å¤Ÿåœ¨ä»»åŠ¡ä¹‹é—´ä¿æŒä¸€è‡´æ€§ã€è‡ªé€‚åº”è¡Œä¸ºçš„æ™ºèƒ½ä½“è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åˆ†æäº†OthelloGPTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºGPTçš„æ¨¡å‹ï¼Œåœ¨Othelloæ¸¸æˆä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œä½œä¸ºç ”ç©¶è¡¨ç¤ºå­¦ä¹ çš„å—æ§æµ‹è¯•å¹³å°ã€‚å°½ç®¡ä»…é€šè¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œéšæœºæœ‰æ•ˆåŠ¨ä½œè¿›è¡Œè®­ç»ƒï¼ŒOthelloGPTåœ¨ç†è§£æ£‹ç›˜çŠ¶æ€å’Œæ¸¸æˆæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰æ„ä¹‰çš„é€å±‚è¿›å±•ã€‚è¾ƒæ—©çš„å±‚æ¬¡æ•æ‰é™æ€å±æ€§ï¼Œå¦‚æ£‹ç›˜è¾¹ç¼˜ï¼Œè€Œè¾ƒæ·±çš„å±‚æ¬¡åæ˜ åŠ¨æ€æ£‹å­å˜åŒ–ã€‚ä¸ºäº†è§£é‡Šè¿™äº›è¡¨ç¤ºå½¢å¼ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å’Œçº¿æ€§æ¢å¤´ï¼Œå‘ç°SAEæä¾›äº†å¯¹ç»„åˆç‰¹å¾æ›´ç¨³å¥ã€æ›´åˆ†ç¦»çš„è§è§£ï¼Œè€Œçº¿æ€§æ¢å¤´ä¸»è¦æ£€æµ‹ç”¨äºåˆ†ç±»çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨SAEè§£ç ä¸æ£‹å­é¢œè‰²å’Œæ£‹å­ç¨³å®šæ€§ç›¸å…³çš„ç‰¹å¾ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å‰æœªæ£€æŸ¥çš„ç‰¹å¾ï¼Œåæ˜ äº†å¤æ‚çš„æ¸¸æˆæ¦‚å¿µï¼Œå¦‚æ£‹ç›˜æ§åˆ¶å’Œé•¿æœŸè§„åˆ’ã€‚æˆ‘ä»¬ç ”ç©¶äº†çº¿æ€§æ¢å¤´å‡†ç¡®åº¦å’Œç“·ç –é¢œè‰²çš„è¿›å±•ï¼Œä½¿ç”¨SAEå’Œçº¿æ€§æ¢å¤´æ¥æ¯”è¾ƒå®ƒä»¬åœ¨æ•æ‰æ¨¡å‹å­¦ä¹ æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶æˆ‘ä»¬ä»è¾ƒå°çš„è¯­è¨€æ¨¡å‹OthelloGPTå¼€å§‹ï¼Œä½†è¿™é¡¹ç ”ç©¶ä¸ºç†è§£GPTæ¨¡å‹ã€è½¬æ¢å™¨å’ŒLLMæ›´å¹¿æ³›å­¦ä¹ çš„å†…éƒ¨è¡¨ç¤ºå»ºç«‹äº†æ¡†æ¶ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ALT-JS/OthelloSAE%E3%80%82">https://github.com/ALT-JS/OthelloSAEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07108v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†è¯­è¨€ã€ç­–ç•¥æ¸¸æˆå’Œæ¨ç†ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ„å»ºå¯¹äºæ™ºèƒ½ä½“è‡ªé€‚åº”å†³ç­–è‡³å…³é‡è¦çš„é€šç”¨å†…éƒ¨è¡¨ç¤ºæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†æœ‰æ•ˆåœ°é€‚åº”å¤æ‚ç¯å¢ƒï¼Œæ™ºèƒ½ä½“å¿…é¡»æ„å»ºå¯é çš„ä¸–ç•Œæ¨¡å‹ã€‚å°½ç®¡LLMåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æ³›åŒ–ï¼Œå¯¼è‡´è¡¨ç¤ºå½¢å¼è„†å¼±ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„æœ‰æ•ˆæ€§ã€‚ç†è§£LLMå¦‚ä½•æ„å»ºå†…éƒ¨ä¸–ç•Œæ¨¡å‹æ˜¯å¼€å‘èƒ½å¤Ÿåœ¨ä»»åŠ¡ä¹‹é—´è¡¨ç°å‡ºä¸€è‡´æ€§å’Œè‡ªé€‚åº”è¡Œä¸ºçš„æ™ºèƒ½ä½“çš„å…³é”®ã€‚æœ¬ç ”ç©¶ä»¥OthelloGPTä¸ºä¾‹è¿›è¡Œåˆ†æï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºGPTçš„æ¨¡å‹ï¼Œç»è¿‡Othelloæ¸¸æˆè®­ç»ƒï¼Œä½œä¸ºç ”ç©¶è¡¨ç¤ºå­¦ä¹ çš„å—æ§æµ‹è¯•å¹³å°ã€‚å°½ç®¡å®ƒä»…ç»è¿‡ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹å’Œéšæœºæœ‰æ•ˆåŠ¨ä½œçš„è®­ç»ƒï¼Œä½†OthelloGPTåœ¨ç†è§£æ£‹ç›˜çŠ¶æ€å’Œæ¸¸æˆè§„åˆ™æ–¹é¢æ˜¾ç¤ºå‡ºæœ‰æ„ä¹‰çš„é€å±‚è¿›å±•ã€‚æ—©æœŸçš„å±‚æ¬¡æ•æ‰é™æ€å±æ€§ï¼Œå¦‚æ£‹ç›˜è¾¹ç¼˜ï¼Œè€Œè¾ƒæ·±çš„å±‚æ¬¡åˆ™åæ˜ åŠ¨æ€ç“·ç –å˜åŒ–ã€‚æœ¬ç ”ç©¶é€šè¿‡ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ä¸çº¿æ€§æ¢é’ˆçš„æ¯”è¾ƒï¼Œå‘ç°SAEä¸ºç»„åˆç‰¹å¾æä¾›äº†æ›´ç¨³å¥ã€æ›´æ¾æ•£çš„è§è§£ï¼Œè€Œçº¿æ€§æ¢é’ˆåˆ™ä¸»è¦æ£€æµ‹ç”¨äºåˆ†ç±»çš„ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨SAEè§£ç ä¸ç“·ç –é¢œè‰²å’Œç“·ç –ç¨³å®šæ€§ç›¸å…³çš„ç‰¹å¾ï¼Œè¿™æ˜¯ä¸€ä¸ªä¹‹å‰æœªè¢«æ£€æŸ¥è¿‡çš„ç‰¹å¾ï¼Œåæ˜ äº†è¯¸å¦‚æ£‹ç›˜æ§åˆ¶å’Œé•¿æœŸè§„åˆ’ä¹‹ç±»çš„å¤æ‚æ¸¸æˆè§„åˆ™æ¦‚å¿µã€‚æœ¬ç ”ç©¶ä»¥è¾ƒå°çš„è¯­è¨€æ¨¡å‹OthelloGPTä¸ºå‡ºå‘ç‚¹ï¼Œå»ºç«‹äº†ç†è§£GPTæ¨¡å‹ã€å˜å‹å™¨å’ŒLLMæ›´å¹¿æ³›é¢†åŸŸå†…å­¦ä¹ çš„å†…éƒ¨è¡¨ç¤ºæœºåˆ¶ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/ALT-JS/OthelloSAE">é“¾æ¥</a>ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ„å»ºé€šç”¨å†…éƒ¨è¡¨ç¤ºå’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸ºäº†æœ‰æ•ˆé€‚åº”å¤æ‚ç¯å¢ƒï¼Œæ™ºèƒ½ä½“éœ€è¦æ„å»ºå¯é çš„ä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>OthelloGPTä½œä¸ºä¸€ä¸ªç ”ç©¶æ¡ˆä¾‹ï¼Œå±•ç¤ºäº†åœ¨ç†è§£æ£‹ç›˜çŠ¶æ€å’Œæ¸¸æˆè§„åˆ™æ–¹é¢çš„é€å±‚è¿›å±•ã€‚</li>
<li>æ—©æœŸå±‚æ¬¡æ•æ‰é™æ€å±æ€§ï¼Œå¦‚æ£‹ç›˜è¾¹ç¼˜ï¼›è¾ƒæ·±çš„å±‚æ¬¡åæ˜ åŠ¨æ€å˜åŒ–ã€‚</li>
<li>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æä¾›äº†å¯¹æ¨¡å‹å­¦ä¹ ç‰¹å¾çš„æ›´ç¨³å¥å’Œæ¾æ•£çš„è§è§£ï¼Œè€Œçº¿æ€§æ¢é’ˆä¸»è¦ç”¨äºåˆ†ç±»ç‰¹å¾æ£€æµ‹ã€‚</li>
<li>é€šè¿‡SAEè§£ç äº†ä¸ç“·ç –é¢œè‰²å’Œç¨³å®šæ€§ç›¸å…³çš„ç‰¹å¾ï¼Œåæ˜ äº†å¤æ‚æ¸¸æˆè§„åˆ™æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07108">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab43c1bc848735b4be894955f1ea29a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51969e9a8263c52af7bb68a262f2d057.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5968dda269ab12f1291c3cefdddf56a5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Punctuationâ€™s-Semantic-Role-between-Brain-and-Transformers-Models"><a href="#Punctuationâ€™s-Semantic-Role-between-Brain-and-Transformers-Models" class="headerlink" title="Punctuationâ€™s Semantic Role between Brain and Transformers Models"></a>Punctuationâ€™s Semantic Role between Brain and Transformers Models</h2><p><strong>Authors:Zenon Lamprou, Frank Polick, Yashar Moshfeghi</strong></p>
<p>Contemporary neural networks intended for natural language processing (NLP) are not designed with specific linguistic rules. It suggests that they may acquire a general understanding of language. This attribute has led to extensive research in deciphering their internal representations. A pioneering method involves an experimental setup using human brain data to explore if a translation between brain and neural network representations can be established. Since this technique emerged, more sophisticated NLP models have been developed. In our study, we apply this method to evaluate four new NLP models aiming to identify the one most compatible with brain activity. Additionally, to explore how the brain comprehends text semantically, we alter the text by removing punctuation in four different ways to understand its impact on semantic processing by the human brain. Our findings indicate that the RoBERTa model aligns best with brain activity, outperforming BERT in accuracy according to our metrics. Furthermore, for BERT, higher accuracy was noted when punctuation was excluded, and increased context length did not significantly diminish accuracy compared to the original results with punctuation. </p>
<blockquote>
<p>é’ˆå¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç°ä»£ç¥ç»ç½‘ç»œå¹¶æ²¡æœ‰æ ¹æ®ç‰¹å®šçš„è¯­è¨€è§„åˆ™è¿›è¡Œè®¾è®¡ï¼Œè¿™è¡¨æ˜å®ƒä»¬å¯èƒ½è·å¾—äº†å¯¹è¯­è¨€çš„æ™®éç†è§£ã€‚è¿™ä¸€å±æ€§å¯¼è‡´äº†å¯¹å…¶å†…éƒ¨è¡¨å¾è§£å¯†çš„å¤§è§„æ¨¡ç ”ç©¶ã€‚ä¸€ç§é¦–åˆ›çš„æ–¹æ³•æ¶‰åŠä½¿ç”¨äººç±»è„‘æ•°æ®æ¥æ¢ç´¢æ˜¯å¦å¯ä»¥åœ¨å¤§è„‘å’Œç¥ç»ç½‘ç»œè¡¨å¾ä¹‹é—´å»ºç«‹ç¿»è¯‘çš„å®éªŒè®¾ç½®ã€‚è‡ªè¿™é¡¹æŠ€æœ¯å‡ºç°ä»¥æ¥ï¼Œæ›´å…ˆè¿›çš„NLPæ¨¡å‹å·²ç»è¢«å¼€å‘å‡ºæ¥ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨è¿™ç§æ–¹æ³•æ¥è¯„ä¼°å››ç§æ–°çš„NLPæ¨¡å‹ï¼Œæ—¨åœ¨æ‰¾å‡ºä¸è„‘æ´»åŠ¨æœ€ç›¸å®¹çš„ä¸€ç§ã€‚æ­¤å¤–ï¼Œä¸ºäº†æ¢ç´¢å¤§è„‘å¦‚ä½•ç†è§£æ–‡æœ¬è¯­ä¹‰ï¼Œæˆ‘ä»¬é€šè¿‡å››ç§ä¸åŒçš„æ–¹å¼å»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä»¥äº†è§£å…¶å¯¹äººç±»å¤§è„‘è¯­ä¹‰å¤„ç†çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒRoBERTaæ¨¡å‹ä¸è„‘æ´»åŠ¨æœ€ä¸ºå»åˆï¼Œæ ¹æ®æˆ‘ä»¬çš„æŒ‡æ ‡ï¼Œå…¶åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¿‡äº†BERTã€‚æ­¤å¤–ï¼Œå¯¹äºBERTè€Œè¨€ï¼Œåœ¨æ’é™¤æ ‡ç‚¹ç¬¦å·åï¼Œå…¶å‡†ç¡®æ€§æœ‰æ‰€æé«˜ï¼Œè€Œä¸”ä¸åŸå§‹å¸¦æœ‰æ ‡ç‚¹ç¬¦å·çš„ç»“æœç›¸æ¯”ï¼Œå¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦å¹¶æ²¡æœ‰æ˜¾è‘—é™ä½å…¶å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å½“ä»£ç¥ç»ç½‘ç»œåœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ—¶çš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬å…¶ä¸ä¾èµ–ç‰¹å®šè¯­è¨€å­¦è§„åˆ™çš„æ™®éè¯­è¨€ç†è§£èƒ½åŠ›ã€‚ä¸ºç ”ç©¶ç¥ç»ç½‘ç»œçš„å†…éƒ¨è¡¨å¾ï¼Œä¸€ç§ä½¿ç”¨äººç±»è„‘æ•°æ®çš„ç ”ç©¶æ–¹æ³•åº”è¿è€Œç”Ÿã€‚æœ¬æ–‡åº”ç”¨æ­¤æ–¹æ³•è¯„ä¼°äº†å››ç§æ–°çš„NLPæ¨¡å‹ï¼Œå¹¶å‘ç°RoBERTaæ¨¡å‹ä¸è„‘æ´»åŠ¨æœ€ä¸ºå¥‘åˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜ï¼Œå»é™¤æ–‡æœ¬ä¸­çš„æ ‡ç‚¹ç¬¦å·ä¼šå½±å“è¯­ä¹‰å¤„ç†ï¼Œä½†åœ¨BERTæ¨¡å‹ä¸­ï¼Œé«˜å‡†ç¡®æ€§å¯åœ¨æ— æ ‡ç‚¹æƒ…å†µä¸‹ç»´æŒï¼Œä¸”ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ å¯¹å‡†ç¡®æ€§å½±å“ä¸æ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“ä»£ç¥ç»ç½‘ç»œåœ¨å¤„ç†è‡ªç„¶è¯­è¨€æ—¶ä¸ä¾èµ–ç‰¹å®šè¯­è¨€å­¦è§„åˆ™ï¼Œå±•ç°å‡ºå¯¹è¯­è¨€çš„æ™®éç†è§£èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨äººç±»è„‘æ•°æ®çš„ç ”ç©¶æ–¹æ³•æ­£åœ¨æ­ç¤ºç¥ç»ç½‘ç»œçš„å†…éƒ¨è¡¨å¾ã€‚</li>
<li>åœ¨è¯„ä¼°å››ç§æ–°çš„NLPæ¨¡å‹åï¼Œå‘ç°RoBERTaæ¨¡å‹ä¸è„‘æ´»åŠ¨æœ€ä¸ºå¥‘åˆã€‚</li>
<li>å»é™¤æ–‡æœ¬ä¸­çš„æ ‡ç‚¹ç¬¦å·ä¼šå½±å“è¯­ä¹‰å¤„ç†ã€‚</li>
<li>åœ¨BERTæ¨¡å‹ä¸­ï¼Œå»é™¤æ ‡ç‚¹åä»å¯ç»´æŒé«˜å‡†ç¡®æ€§ã€‚</li>
<li>ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ å¯¹NLPæ¨¡å‹çš„å‡†ç¡®æ€§å½±å“ä¸æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7035fe5e5c11b913130247fd212aab1d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="KaLM-Embedding-Superior-Training-Data-Brings-A-Stronger-Embedding-Model"><a href="#KaLM-Embedding-Superior-Training-Data-Brings-A-Stronger-Embedding-Model" class="headerlink" title="KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"></a>KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model</h2><p><strong>Authors:Xinshuo Hu, Zifei Shan, Xinping Zhao, Zetian Sun, Zhenyu Liu, Dongfang Li, Shaolin Ye, Xinyuan Wei, Qian Chen, Baotian Hu, Min Zhang</strong></p>
<p>As retrieval-augmented generation prevails in large language models, embedding models are becoming increasingly crucial. Despite the growing number of general embedding models, prior work often overlooks the critical role of training data quality. In this work, we introduce KaLM-Embedding, a general multilingual embedding model that leverages a large quantity of cleaner, more diverse, and domain-specific training data. Our model has been trained with key techniques proven to enhance performance: (1) persona-based synthetic data to create diversified examples distilled from LLMs, (2) ranking consistency filtering to remove less informative samples, and (3) semi-homogeneous task batch sampling to improve training efficacy. Departing from traditional BERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model, facilitating the adaptation of auto-regressive language models for general embedding tasks. Extensive evaluations of the MTEB benchmark across multiple languages show that our model outperforms others of comparable size, setting a new standard for multilingual embedding models with &lt;1B parameters. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„æ™®åŠï¼ŒåµŒå…¥æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å°½ç®¡é€šç”¨åµŒå…¥æ¨¡å‹çš„æ•°é‡ä¸æ–­å¢åŠ ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å¾€å¾€å¿½è§†äº†è®­ç»ƒæ•°æ®è´¨é‡çš„é‡è¦ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†KaLM-Embeddingï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¤šè¯­è¨€åµŒå…¥æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å¤§é‡æ›´æ¸…æ´ã€æ›´å¤šæ ·åŒ–ä¸”ç‰¹å®šäºåŸŸçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨äº†ä¸€äº›ç»è¿‡éªŒè¯çš„å…³é”®æŠ€æœ¯æ¥æé«˜æ€§èƒ½ï¼šï¼ˆ1ï¼‰åŸºäºè§’è‰²çš„åˆæˆæ•°æ®ï¼Œä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æç‚¼å‡ºå¤šæ ·åŒ–çš„ç¤ºä¾‹ï¼›ï¼ˆ2ï¼‰æ’åä¸€è‡´æ€§è¿‡æ»¤ä»¥æ¶ˆé™¤ä¿¡æ¯é‡è¾ƒå°‘çš„æ ·æœ¬ï¼›ï¼ˆ3ï¼‰åŠå‡åŒ€ä»»åŠ¡æ‰¹é‡é‡‡æ ·ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚æˆ‘ä»¬æ‘’å¼ƒäº†ä¼ ç»Ÿçš„BERTç±»ä¼¼æ¶æ„ï¼Œé‡‡ç”¨Qwen2-0.5Bä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾¿äºè‡ªé€‚åº”é€šç”¨åµŒå…¥ä»»åŠ¡çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚åœ¨å¤šç§è¯­è¨€çš„MTEBåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œä¸ºå‚æ•°å°äº1Bçš„å¤šè¯­è¨€åµŒå…¥æ¨¡å‹è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01028v3">PDF</a> Technical Report. 23 pages, 6 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ä¸‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆè¶‹åŠ¿ï¼ŒåµŒå…¥æ¨¡å‹çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚é’ˆå¯¹ç°æœ‰åµŒå…¥æ¨¡å‹å¿½è§†è®­ç»ƒæ•°æ®è´¨é‡çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†KaLM-Embeddingè¿™ä¸€é€šç”¨å¤šè¯­è¨€åµŒå…¥æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ•°é‡åºå¤§çš„æ›´æ¸…æ´ã€æ›´å¤šæ ·åŒ–å’Œç‰¹å®šé¢†åŸŸçš„è®­ç»ƒæ•°æ®ï¼Œå¹¶é‡‡ç”¨ç»è¿‡éªŒè¯èƒ½æé«˜æ€§èƒ½çš„å…³é”®æŠ€æœ¯ï¼Œå¦‚åŸºäºäººæ ¼çš„åˆæˆæ•°æ®åˆ›å»ºã€æ’åä¸€è‡´æ€§è¿‡æ»¤ä»¥åŠåŠåŒæ„ä»»åŠ¡æ‰¹é‡é‡‡æ ·ç­‰ã€‚ä¸ä¼ ç»Ÿçš„BERTç±»ä¼¼æ¶æ„ä¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨Qwen2-0.5Bä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾¿äºè‡ªé€‚åº”é€šç”¨åµŒå…¥ä»»åŠ¡çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸Šçš„å¤šé¡¹è¯­è¨€è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸ºå‚æ•°å°äº1Bçš„å¤šè¯­è¨€åµŒå…¥æ¨¡å‹æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆè¶‹åŠ¿ä½¿å¾—åµŒå…¥æ¨¡å‹æ„ˆå‘é‡è¦ã€‚</li>
<li>KaLM-Embeddingæ˜¯ä¸€ä¸ªé€šç”¨å¤šè¯­è¨€åµŒå…¥æ¨¡å‹ï¼Œæ³¨é‡è®­ç»ƒæ•°æ®è´¨é‡ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨å¤šç§æŠ€æœ¯æé«˜æ€§èƒ½ï¼ŒåŒ…æ‹¬åŸºäºäººæ ¼çš„åˆæˆæ•°æ®ã€æ’åä¸€è‡´æ€§è¿‡æ»¤å’ŒåŠåŒæ„ä»»åŠ¡æ‰¹é‡é‡‡æ ·ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨Qwen2-0.5Bä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œé€‚ç”¨äºè‡ªå›å½’è¯­è¨€æ¨¡å‹è¿›è¡Œé€šç”¨åµŒå…¥ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡åŒç±»æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸ºå‚æ•°å°äº1Bçš„å¤šè¯­è¨€åµŒå…¥æ¨¡å‹æ ‘ç«‹äº†æ–°æ ‡å‡†ã€‚</li>
<li>æ¨¡å‹çš„æˆåŠŸéªŒè¯äº†è®­ç»ƒæ•°æ®è´¨é‡åœ¨åµŒå…¥æ¨¡å‹ä¸­çš„å…³é”®ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d65e646e9c629acd30eee0d07035a0d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8571020606cbd5249ca7c471fee53a2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5601bbdbf9f116a49c434704e360f22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-992df85551a7b6e4911f14eb40d76dd4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09bfdf410db0099e408cf9e757132c6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c26883c1df6d03014066431ba8314d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-758a5d7f4bd1b1da848402526dcf3f86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f6514e3ec2fc3698ad865f6bc166f4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a2464381031b26a2c2b8f311f42c0e32.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-164c95061e1a5ba4751769867bb72733.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-14  Motion-X++ A Large-Scale Multimodal 3D Whole-body Human Motion Dataset
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28292.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
