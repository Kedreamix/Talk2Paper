<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  DM-Mamba Dual-domain Multi-scale Mamba for MRI reconstruction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7f262d0f78c82bdcda7a592ee0b830dd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    36 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-16-æ›´æ–°"><a href="#2025-01-16-æ›´æ–°" class="headerlink" title="2025-01-16 æ›´æ–°"></a>2025-01-16 æ›´æ–°</h1><h2 id="DM-Mamba-Dual-domain-Multi-scale-Mamba-for-MRI-reconstruction"><a href="#DM-Mamba-Dual-domain-Multi-scale-Mamba-for-MRI-reconstruction" class="headerlink" title="DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction"></a>DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction</h2><p><strong>Authors:Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi</strong></p>
<p>The accelerated MRI reconstruction poses a challenging ill-posed inverse problem due to the significant undersampling in k-space. Deep neural networks, such as CNNs and ViT, have shown substantial performance improvements for this task while encountering the dilemma between global receptive fields and efficient computation. To this end, this paper pioneers exploring Mamba, a new paradigm for long-range dependency modeling with linear complexity, for efficient and effective MRI reconstruction. However, directly applying Mamba to MRI reconstruction faces three significant issues: (1) Mambaâ€™s row-wise and column-wise scanning disrupts k-spaceâ€™s unique spectrum, leaving its potential in k-space learning unexplored. (2) Existing Mamba methods unfold feature maps with multiple lengthy scanning paths, leading to long-range forgetting and high computational burden. (3) Mamba struggles with spatially-varying contents, resulting in limited diversity of local representations. To address these, we propose a dual-domain multi-scale Mamba for MRI reconstruction from the following perspectives: (1) We pioneer vision Mamba in k-space learning. A circular scanning is customized for spectrum unfolding, benefiting the global modeling of k-space. (2) We propose a multi-scale Mamba with an efficient scanning strategy in both image and k-space domains. It mitigates long-range forgetting and achieves a better trade-off between efficiency and performance. (3) We develop a local diversity enhancement module to improve the spatially-varying representation of Mamba. Extensive experiments are conducted on three public datasets for MRI reconstruction under various undersampling patterns. Comprehensive results demonstrate that our method significantly outperforms state-of-the-art methods with lower computational cost. Implementation code will be available at <a target="_blank" rel="noopener" href="https://github.com/XiaoMengLiLiLi/DM-Mamba">https://github.com/XiaoMengLiLiLi/DM-Mamba</a>. </p>
<blockquote>
<p>åŠ é€ŸMRIé‡å»ºæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä¸é€‚å®šåé—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºkç©ºé—´ä¸­çš„æ˜¾è‘—æ¬ é‡‡æ ·é€ æˆçš„ã€‚æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå’ŒVision Transformerï¼Œè™½ç„¶åœ¨æ­¤ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œä½†åœ¨å…¨å±€æ„Ÿå—é‡å’Œé«˜æ•ˆè®¡ç®—ä¹‹é—´é‡åˆ°äº†å›°å¢ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç‡å…ˆæ¢ç´¢Mambaï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰çº¿æ€§å¤æ‚åº¦è¿›è¡Œé•¿è·ç¦»ä¾èµ–å»ºæ¨¡çš„æ–°èŒƒå¼ï¼Œç”¨äºé«˜æ•ˆä¸”æœ‰æ•ˆçš„MRIé‡å»ºã€‚ç„¶è€Œï¼Œç›´æ¥å°†Mambaåº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šï¼ˆ1ï¼‰Mambaçš„è¡Œçº§å’Œåˆ—çº§æ‰«æç ´åäº†kç©ºé—´çš„ç‹¬ç‰¹é¢‘è°±ï¼Œä½¿å¾—å…¶åœ¨kç©ºé—´å­¦ä¹ æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°æ¢ç´¢ã€‚ï¼ˆ2ï¼‰ç°æœ‰çš„Mambaæ–¹æ³•å±•å¼€ç‰¹å¾æ˜ å°„å…·æœ‰å¤šæ¡å†—é•¿çš„æ‰«æè·¯å¾„ï¼Œå¯¼è‡´é•¿è·ç¦»é—å¿˜å’Œè¾ƒé«˜çš„è®¡ç®—è´Ÿæ‹…ã€‚ï¼ˆ3ï¼‰Mambaåœ¨å¤„ç†ç©ºé—´å˜åŒ–å†…å®¹æ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´å±€éƒ¨è¡¨ç¤ºç¼ºä¹å¤šæ ·æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»ä»¥ä¸‹è§’åº¦æå‡ºç”¨äºMRIé‡å»ºçš„åŒåŸŸå¤šå°ºåº¦Mambaï¼šï¼ˆ1ï¼‰æˆ‘ä»¬ç‡å…ˆåœ¨kç©ºé—´å­¦ä¹ ä¸­æ¢ç´¢è§†è§‰Mambaã€‚é’ˆå¯¹é¢‘è°±å±•å¼€å®šåˆ¶å¾ªç¯æ‰«æï¼Œæœ‰åˆ©äºkç©ºé—´çš„å…¨å±€å»ºæ¨¡ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå°ºåº¦Mambaï¼Œåœ¨å›¾åƒå’Œkç©ºé—´åŸŸéƒ½é‡‡ç”¨äº†é«˜æ•ˆçš„æ‰«æç­–ç•¥ã€‚å®ƒå‡è½»äº†é•¿è·ç¦»é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å®ç°äº†æ›´å¥½çš„æƒè¡¡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜Mambaçš„ç©ºé—´å˜åŒ–è¡¨ç¤ºèƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡MRIé‡å»ºå®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§æ¬ é‡‡æ ·æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚ç›¸å…³å®ç°ä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaoMengLiLiLi/DM-Mamba%E3%80%82">https://github.com/XiaoMengLiLiLi/DM-Mambaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08163v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„MRIé‡å»ºæ–¹æ³•â€”â€”åŒåŸŸå¤šå°ºåº¦MambaæŠ€æœ¯ã€‚é’ˆå¯¹ç›´æ¥åº”ç”¨MambaæŠ€æœ¯æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¯¥æ–¹æ³•ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œä¸­çš„è·¨ç©ºé—´å’Œè°±é¢†åŸŸæå‡ºåˆ›æ–°æ€§ç­–ç•¥ï¼Œæ—¨åœ¨å®ç°æ›´é«˜æ•ˆä¸”æœ‰æ•ˆçš„MRIé‡å»ºã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å°†Mambaå¼•å…¥kç©ºé—´å­¦ä¹ ï¼Œæå‡ºå¤šå°ºåº¦Mambaä»¥å‡è½»è®¡ç®—è´Ÿæ‹…å’Œé•¿èŒƒå›´é—å¿˜é—®é¢˜ï¼Œå¹¶å¢å¼ºå±€éƒ¨å¤šæ ·æ€§ä»¥å¢å¼ºç©ºé—´å˜åŒ–çš„è¡¨ç¤ºã€‚å®éªŒç»“æœè¯æ˜äº†å…¶åœ¨å¤šç§é‡‡æ ·æ¨¡å¼ä¸‹çš„MRIé‡å»ºä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MambaæŠ€æœ¯è¢«å¼•å…¥MRIé‡å»ºé¢†åŸŸï¼Œæ—¨åœ¨è§£å†³è®¡ç®—æ•ˆç‡å’Œå…¨å±€å»ºæ¨¡é—®é¢˜ã€‚</li>
<li>ç›´æ¥åº”ç”¨MambaäºMRIé‡å»ºé¢ä¸´ç‹¬ç‰¹é¢‘è°±çš„ç ´åã€é•¿èŒƒå›´é—å¿˜å’Œé«˜è®¡ç®—è´Ÿæ‹…ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºåŒåŸŸå¤šå°ºåº¦MambaæŠ€æœ¯ï¼Œç»“åˆå›¾åƒå’Œkç©ºé—´åŸŸçš„å¤šå°ºåº¦åˆ†æï¼Œå®ç°æ•ˆç‡å’Œæ€§èƒ½çš„å¹³è¡¡ã€‚</li>
<li>å¼•å…¥å±€éƒ¨å¤šæ ·æ€§å¢å¼ºæ¨¡å—ï¼Œæé«˜ç©ºé—´å˜åŒ–å†…å®¹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea974741d9a7e7365b087c10c9c2db9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c5c5129f8252500cb84034caa4b80b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f715628ebc166486384bddf33358cdd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad31e645e9120c6de378a1a2dcc693cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc981cd60c32908fe03dc12ff391c5b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Parameter-Inverted-Image-Pyramid-Networks-for-Visual-Perception-and-Multimodal-Understanding"><a href="#Parameter-Inverted-Image-Pyramid-Networks-for-Visual-Perception-and-Multimodal-Understanding" class="headerlink" title="Parameter-Inverted Image Pyramid Networks for Visual Perception and   Multimodal Understanding"></a>Parameter-Inverted Image Pyramid Networks for Visual Perception and   Multimodal Understanding</h2><p><strong>Authors:Zhaokai Wang, Xizhou Zhu, Xue Yang, Gen Luo, Hao Li, Changyao Tian, Wenhan Dou, Junqi Ge, Lewei Lu, Yu Qiao, Jifeng Dai</strong></p>
<p>Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-scale images, where images of higher resolutions are processed by smaller network branches to balance computational cost and performance. To integrate information from different spatial scales, we further propose a novel cross-branch feature interaction mechanism. To validate PIIP, we apply it to various perception models and a representative multimodal large language model called LLaVA, and conduct extensive experiments on various tasks such as object detection, segmentation, image classification and multimodal understanding. PIIP achieves superior performance compared to single-branch and existing multi-resolution approaches with lower computational cost. When applied to InternViT-6B, a large-scale vision foundation model, PIIP can improve its performance by 1%-2% on detection and segmentation with only 40%-60% of the original computation, finally achieving 60.0 box AP on MS COCO and 59.7 mIoU on ADE20K. For multimodal understanding, our PIIP-LLaVA achieves 73.0% accuracy on TextVQA and 74.5% on MMBench with only 2.8M training data. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/PIIP">https://github.com/OpenGVLab/PIIP</a>. </p>
<blockquote>
<p>å›¾åƒé‡‘å­—å¡”å·²åœ¨é«˜æ€§èƒ½æ–¹æ³•ä¸­å¹¿æ³›é‡‡ç”¨ï¼Œä»¥è·å¾—ç”¨äºç²¾ç¡®è§†è§‰æ„ŸçŸ¥å’Œç†è§£çš„å¤šå°ºåº¦ç‰¹å¾ã€‚ç„¶è€Œï¼Œå½“å‰å›¾åƒé‡‘å­—å¡”ä½¿ç”¨ç›¸åŒçš„å¤§è§„æ¨¡æ¨¡å‹æ¥å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬æ˜¾è‘—ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºå‚æ•°å€’ç½®å›¾åƒé‡‘å­—å¡”ç½‘ç»œï¼ˆPIIPï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒPIIPä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆViTsæˆ–CNNï¼‰ä½œä¸ºåˆ†æ”¯æ¥å¤„ç†å¤šå°ºåº¦å›¾åƒï¼Œå…¶ä¸­é«˜åˆ†è¾¨ç‡å›¾åƒç”±è¾ƒå°çš„ç½‘ç»œåˆ†æ”¯è¿›è¡Œå¤„ç†ï¼Œä»¥å¹³è¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ã€‚ä¸ºäº†æ•´åˆä¸åŒç©ºé—´å°ºåº¦çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°å‹è·¨åˆ†æ”¯ç‰¹å¾äº¤äº’æœºåˆ¶ã€‚ä¸ºäº†éªŒè¯PIIPçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºå„ç§æ„ŸçŸ¥æ¨¡å‹å’Œåä¸ºLLaVAçš„ä»£è¡¨æ€§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ã€å›¾åƒåˆ†ç±»å’Œå¤šæ¨¡æ€ç†è§£ï¼‰ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒã€‚PIIPç›¸è¾ƒäºå•åˆ†æ”¯å’Œç°æœ‰å¤šåˆ†è¾¨ç‡æ–¹æ³•å–å¾—äº†æ›´ä¼˜è¶Šçš„æ€§èƒ½ï¼Œä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚å½“åº”ç”¨äºå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹InternViT-6Bæ—¶ï¼ŒPIIPåœ¨æ£€æµ‹ä¸åˆ†å‰²æ–¹é¢çš„æ€§èƒ½æé«˜äº†1%-2%ï¼Œä»…ä½¿ç”¨åŸå§‹è®¡ç®—çš„40%-60%ï¼Œæœ€ç»ˆåœ¨MS COCOä¸Šå®ç°äº†60.0çš„box APï¼Œåœ¨ADE20Kä¸Šå®ç°äº†59.7çš„mIoUã€‚å¯¹äºå¤šæ¨¡æ€ç†è§£ï¼Œæˆ‘ä»¬çš„PIIP-LLaVAåœ¨TextVQAä¸Šè¾¾åˆ°äº†73.0%çš„å‡†ç¡®ç‡ï¼Œåœ¨MMBenchä¸Šè¾¾åˆ°äº†74.5%çš„å‡†ç¡®ç‡ï¼Œä¸”ä»…ä½¿ç”¨äº†2.8Mçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/PIIP%E3%80%82">https://github.com/OpenGVLab/PIIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07783v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å›¾åƒé‡‘å­—å¡”åœ¨å¤„ç†å¤šå°ºåº¦å›¾åƒæ—¶è®¡ç®—æˆæœ¬è¾ƒé«˜çš„é—®é¢˜ï¼Œæå‡ºäº†å‚æ•°å€’ç½®å›¾åƒé‡‘å­—å¡”ç½‘ç»œï¼ˆPIIPï¼‰ã€‚PIIPåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ViTsæˆ–CNNsï¼‰ä½œä¸ºåˆ†æ”¯å¤„ç†å¤šå°ºåº¦å›¾åƒï¼Œé€šè¿‡ä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒå¤„ç†åˆ†æ”¯æ¥å¹³è¡¡è®¡ç®—æˆæœ¬ä¸æ€§èƒ½ã€‚é€šè¿‡è·¨åˆ†æ”¯ç‰¹å¾äº¤äº’æœºåˆ¶æ•´åˆä¸åŒç©ºé—´å°ºåº¦çš„ä¿¡æ¯ï¼Œå¹¶åœ¨å„ç§æ„ŸçŸ¥æ¨¡å‹å’Œåä¸ºLLaVAçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚PIIPç›¸è¾ƒäºå•åˆ†æ”¯å’Œç°æœ‰å¤šåˆ†è¾¨ç‡æ–¹æ³•ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚åº”ç”¨äºå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹InternViT-6Bæ—¶ï¼ŒPIIPåœ¨æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢çš„æ€§èƒ½æå‡1%-2%ï¼Œåœ¨MS COCOä¸Šå®ç°60.0 box APï¼Œåœ¨ADE20Kä¸Šå®ç°59.7 mIoUã€‚å¯¹äºå¤šæ¨¡æ€ç†è§£ï¼ŒPIIP-LLaVAåœ¨TextVQAä¸Šè¾¾åˆ°73.0%çš„å‡†ç¡®ç‡ï¼Œåœ¨MMBenchä¸Šè¾¾åˆ°74.5%çš„å‡†ç¡®ç‡ï¼Œä»…ä½¿ç”¨2.8Mè®­ç»ƒæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å›¾åƒé‡‘å­—å¡”ä½¿ç”¨åŒä¸€å¤§è§„æ¨¡æ¨¡å‹å¤„ç†å¤šåˆ†è¾¨ç‡å›¾åƒå­˜åœ¨è®¡ç®—æˆæœ¬è¾ƒé«˜çš„é—®é¢˜ã€‚</li>
<li>PIIPç½‘ç»œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºåˆ†æ”¯å¤„ç†å¤šå°ºåº¦å›¾åƒï¼Œä»¥å¹³è¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ã€‚</li>
<li>PIIPé€šè¿‡è·¨åˆ†æ”¯ç‰¹å¾äº¤äº’æœºåˆ¶æ•´åˆä¸åŒç©ºé—´å°ºåº¦çš„ä¿¡æ¯ã€‚</li>
<li>PIIPåœ¨å¤šç§è§†è§‰ä»»åŠ¡ï¼ˆå¦‚ç›®æ ‡æ£€æµ‹ã€åˆ†å‰²ã€å›¾åƒåˆ†ç±»ï¼‰ä¸Šå®ç°äº†ä¼˜äºå•åˆ†æ”¯å’Œç°æœ‰å¤šåˆ†è¾¨ç‡æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>PIIPåœ¨å¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹InternViT-6Bä¸Šçš„æ€§èƒ½æå‡æ˜¾è‘—ï¼Œåœ¨MS COCOå’ŒADE20Kä¸Šçš„ç»“æœè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å¯¹äºå¤šæ¨¡æ€ç†è§£ï¼ŒPIIP-LLaVAåœ¨TextVQAå’ŒMMBenchä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½æˆç»©ï¼Œä¸”ä»…ä½¿ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>PIIPçš„ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ee3e1521d799c02b6af142657bb271d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-150074e182711ec3a5ad733347b58c6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6d85d2fd451db0f73abfff889c94a82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-573ba64ed1f21ae1649b2637dbf75111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74b747d6c603e65c540dcbba0fe3bb72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fecd713cd28ac9f22ba67df16310cf3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f262d0f78c82bdcda7a592ee0b830dd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SST-EM-Advanced-Metrics-for-Evaluating-Semantic-Spatial-and-Temporal-Aspects-in-Video-Editing"><a href="#SST-EM-Advanced-Metrics-for-Evaluating-Semantic-Spatial-and-Temporal-Aspects-in-Video-Editing" class="headerlink" title="SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal   Aspects in Video Editing"></a>SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal   Aspects in Video Editing</h2><p><strong>Authors:Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang</strong></p>
<p>Video editing models have advanced significantly, but evaluating their performance remains challenging. Traditional metrics, such as CLIP text and image scores, often fall short: text scores are limited by inadequate training data and hierarchical dependencies, while image scores fail to assess temporal consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation Metric), a novel evaluation framework that leverages modern Vision-Language Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM comprises four components: (1) semantic extraction from frames using a VLM, (2) primary object tracking with Object Detection, (3) focused object refinement via an LLM agent, and (4) temporal consistency assessment using a Vision Transformer (ViT). These components are integrated into a unified metric with weights derived from human evaluations and regression analysis. The name SST-EM reflects its focus on Semantic, Spatial, and Temporal aspects of video evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and temporal smoothness in video editing. The source code is available in the \textbf{\href{<a target="_blank" rel="noopener" href="https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git%7D%7BGitHub">https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub</a> Repository}}. </p>
<blockquote>
<p>è§†é¢‘ç¼–è¾‘æ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†æ˜¯è¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚CLIPçš„æ–‡æœ¬å’Œå›¾åƒåˆ†æ•°ï¼Œé€šå¸¸å­˜åœ¨ä¸è¶³ï¼šæ–‡æœ¬åˆ†æ•°å—é™äºä¸è¶³çš„è®­ç»ƒæ•°æ®å’Œå±‚æ¬¡ä¾èµ–æ€§ï¼Œè€Œå›¾åƒåˆ†æ•°åˆ™æ— æ³•è¯„ä¼°æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†SST-EMï¼ˆè¯­ä¹‰ã€ç©ºé—´å’Œæ—¶é—´è¯„ä¼°æŒ‡æ ‡ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç°ä»£çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€ç›®æ ‡æ£€æµ‹å’Œä¸€è‡´æ€§æ£€æŸ¥ã€‚SST-EMåŒ…å«å››ä¸ªç»„æˆéƒ¨åˆ†ï¼šï¼ˆ1ï¼‰ä½¿ç”¨VLMä»å¸§ä¸­æå–è¯­ä¹‰ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨ç›®æ ‡æ£€æµ‹è¿›è¡Œä¸»è¦å¯¹è±¡è·Ÿè¸ªï¼Œï¼ˆ3ï¼‰é€šè¿‡LLMä»£ç†è¿›è¡Œå¯¹è±¡ç²¾ç‚¼ï¼Œï¼ˆ4ï¼‰ä½¿ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œæ—¶é—´ä¸€è‡´æ€§è¯„ä¼°ã€‚è¿™äº›ç»„ä»¶è¢«é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä»·æŒ‡æ ‡ä¸­ï¼Œæƒé‡æ¥æºäºäººç±»è¯„ä¼°å’Œå›å½’åˆ†æã€‚SST-EMçš„åå­—åæ˜ äº†å…¶åœ¨è§†é¢‘è¯„ä¼°çš„è¯­ä¹‰ã€ç©ºé—´å’Œæ—¶é—´æ–¹é¢çš„é‡ç‚¹ã€‚SST-EMä¸ºè§†é¢‘ç¼–è¾‘çš„è¯­ä¹‰ä¿çœŸåº¦å’Œæ—¶é—´å¹³æ»‘æ€§æä¾›äº†å…¨é¢çš„è¯„ä¼°ã€‚æºä»£ç å¯åœ¨\href{<a target="_blank" rel="noopener" href="https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git%7D%7BGitHub">https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub</a> Repository}ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07554v1">PDF</a> WACV workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘ç¼–è¾‘æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶SST-EMã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ã€ç›®æ ‡æ£€æµ‹å’Œæ—¶ç©ºä¸€è‡´æ€§æ£€æŸ¥æŠ€æœ¯ï¼ŒåŒ…æ‹¬è¯­ä¹‰æå–ã€ä¸»è¦å¯¹è±¡è·Ÿè¸ªã€ç‰¹å®šå¯¹è±¡ä¼˜åŒ–å’Œæ—¶ç©ºä¸€è‡´æ€§è¯„ä¼°å››ä¸ªç»„ä»¶ã€‚SST-EMä¸ºè§†é¢‘ç¼–è¾‘çš„è¯­ä¹‰ä¿çœŸåº¦å’Œæ—¶é—´å¹³æ»‘åº¦æä¾›äº†å…¨é¢çš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç¼–è¾‘æ¨¡å‹æ€§èƒ½è¯„ä¼°å­˜åœ¨æŒ‘æˆ˜ï¼Œä¼ ç»Ÿè¯„ä¼°æŒ‡æ ‡å¦‚CLIPæ–‡æœ¬å’Œå›¾åƒåˆ†æ•°å¸¸å¸¸ä¸è¶³ã€‚</li>
<li>SST-EMæ˜¯ä¸€ä¸ªæ–°å‹è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«å››ä¸ªç»„ä»¶ï¼šè¯­ä¹‰æå–ã€ä¸»è¦å¯¹è±¡è·Ÿè¸ªã€ç‰¹å®šå¯¹è±¡ä¼˜åŒ–å’Œæ—¶ç©ºä¸€è‡´æ€§è¯„ä¼°ã€‚</li>
<li>SST-EMåˆ©ç”¨ç°ä»£è§†è§‰è¯­è¨€æ¨¡å‹ã€ç›®æ ‡æ£€æµ‹æŠ€æœ¯å’Œæ—¶ç©ºä¸€è‡´æ€§æ£€æŸ¥æŠ€æœ¯ã€‚</li>
<li>SST-EMé‡è§†è§†é¢‘çš„è¯­ä¹‰ã€ç©ºé—´å’Œæ—¶é—´çš„è¯„ä»·æ–¹é¢ã€‚</li>
<li>SST-EMæä¾›äº†å¯¹è§†é¢‘ç¼–è¾‘çš„è¯­ä¹‰ä¿çœŸåº¦å’Œæ—¶é—´å¹³æ»‘åº¦çš„å…¨é¢è¯„ä¼°ã€‚</li>
<li>SST-EMçš„ç»„ä»¶é€šè¿‡äººç±»è¯„ä¼°å’Œå›å½’åˆ†ææ¥ç¡®å®šæƒé‡ï¼Œé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„åº¦é‡æ ‡å‡†ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c3dc8f3508bbdcb9ea781e9ee4c7535.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8581ca8049cc3a2c2894e44280bc31b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-083fe77dc488d3769f09957c86a91710.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72f584a7e1368680c38c724d02d9812c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee7b594197ac71ce3ba0b89e52a860b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d646cf40334812e776a68ee8c459567.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ad048bab501d185d83eb47e88fb025f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Duplex-Dual-Prototype-Learning-for-Compositional-Zero-Shot-Learning"><a href="#Duplex-Dual-Prototype-Learning-for-Compositional-Zero-Shot-Learning" class="headerlink" title="Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning"></a>Duplex: Dual Prototype Learning for Compositional Zero-Shot Learning</h2><p><strong>Authors:Zhong Peng, Yishi Xu, Gerong Wang, Wenchao Chen, Bo Chen, Jing Zhang</strong></p>
<p>Compositional Zero-Shot Learning (CZSL) aims to enable models to recognize novel compositions of visual states and objects that were absent during training. Existing methods predominantly focus on learning semantic representations of seen compositions but often fail to disentangle the independent features of states and objects in images, thereby limiting their ability to generalize to unseen compositions. To address this challenge, we propose Duplex, a novel dual-prototype learning method that integrates semantic and visual prototypes through a carefully designed dual-branch architecture, enabling effective representation learning for compositional tasks. Duplex utilizes a Graph Neural Network (GNN) to adaptively update visual prototypes, capturing complex interactions between states and objects. Additionally, it leverages the strong visual-semantic alignment of pre-trained Vision-Language Models (VLMs) and employs a multi-path architecture combined with prompt engineering to align image and text representations, ensuring robust generalization. Extensive experiments on three benchmark datasets demonstrate that Duplex outperforms state-of-the-art methods in both closed-world and open-world settings. </p>
<blockquote>
<p>ç»„åˆé›¶å°„å­¦ä¹ ï¼ˆCZSLï¼‰æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«è®­ç»ƒæœŸé—´æœªå‡ºç°çš„è§†è§‰çŠ¶æ€å’Œå¯¹è±¡çš„æ–°ç»„åˆã€‚ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å­¦ä¹ å·²è§ç»„åˆçš„è¯­ä¹‰è¡¨ç¤ºä¸Šï¼Œä½†å¾€å¾€æ— æ³•è§£å¼€å›¾åƒä¸­çŠ¶æ€å’Œå¯¹è±¡çš„ç‹¬ç«‹ç‰¹å¾ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬å¯¹æœªè§ç»„åˆçš„æ¨å¹¿èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Duplexï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒåŸå‹å­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„åŒåˆ†æ”¯æ¶æ„èåˆäº†è¯­ä¹‰å’Œè§†è§‰åŸå‹ï¼Œä¸ºç»„åˆä»»åŠ¡å®ç°äº†æœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ ã€‚Duplexåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è‡ªé€‚åº”åœ°æ›´æ–°è§†è§‰åŸå‹ï¼Œæ•æ‰çŠ¶æ€å’Œå¯¹è±¡ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚æ­¤å¤–ï¼Œå®ƒåˆ©ç”¨äº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹çš„å¼ºå¤§è§†è§‰è¯­ä¹‰å¯¹é½åŠŸèƒ½ï¼Œå¹¶é‡‡ç”¨å¤šè·¯å¾„æ¶æ„ä¸æç¤ºå·¥ç¨‹ç›¸ç»“åˆï¼Œå¯¹é½å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œç¡®ä¿ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDuplexåœ¨å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07114v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆDuplexæ¥è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ï¼ˆCZSLï¼‰æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚Duplexé€šè¿‡æ•´åˆè¯­ä¹‰å’Œè§†è§‰åŸå‹ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡åŒåˆ†æ”¯æ¶æ„è¿›è¡Œæœ‰æ•ˆè¡¨å¾å­¦ä¹ ï¼Œå®ç°å›¾åƒä¸­çŠ¶æ€å’Œå¯¹è±¡çš„ç‹¬ç«‹ç‰¹å¾åˆ†ç¦»ï¼Œä»è€Œåº”å¯¹æœªè§ç»„åˆçš„æ–°è¯†åˆ«ä»»åŠ¡ã€‚è¯¥è§£å†³æ–¹æ¡ˆé‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è‡ªé€‚åº”æ›´æ–°è§†è§‰åŸå‹ï¼Œæ•æ‰çŠ¶æ€å’Œå¯¹è±¡é—´çš„å¤æ‚äº¤äº’ï¼Œå¹¶ç»“åˆé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¼ºå¤§è§†è§‰è¯­ä¹‰å¯¹é½èƒ½åŠ›ï¼Œé€šè¿‡å¤šè·¯å¾„æ¶æ„å’Œæç¤ºå·¥ç¨‹ç¡®ä¿å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºçš„ç¨³å¥æ³›åŒ–ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDuplexåœ¨å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Compositional Zero-Shot Learning (CZSL)æ—¨åœ¨ä½¿æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«è®­ç»ƒæœŸé—´æœªå‡ºç°çš„è§†è§‰çŠ¶æ€å’Œå¯¹è±¡çš„æ–°ç»„åˆã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨å·²è§ç»„åˆè¯­ä¹‰è¡¨ç¤ºçš„å­¦ä¹ ï¼Œä½†å¾€å¾€æ— æ³•åˆ†ç¦»å›¾åƒä¸­çŠ¶æ€å’Œå¯¹è±¡çš„ç‹¬ç«‹ç‰¹å¾ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–åˆ°æœªè§ç»„åˆçš„èƒ½åŠ›ã€‚</li>
<li>Duplexæ˜¯ä¸€ç§æ–°å‹åŒåŸå‹å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ•´åˆè¯­ä¹‰å’Œè§†è§‰åŸå‹ï¼Œåº”å¯¹æœªè§ç»„åˆçš„æ–°è¯†åˆ«ä»»åŠ¡ã€‚</li>
<li>Duplexé‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è‡ªé€‚åº”æ›´æ–°è§†è§‰åŸå‹ï¼Œä»¥æ•æ‰çŠ¶æ€å’Œå¯¹è±¡é—´çš„å¤æ‚äº¤äº’ã€‚</li>
<li>è¯¥è§£å†³æ–¹æ¡ˆåˆ©ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è§†è§‰è¯­ä¹‰å¯¹é½èƒ½åŠ›ï¼Œç¡®ä¿å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºçš„ç¨³å¥æ³›åŒ–ã€‚</li>
<li>é€šè¿‡å¤šè·¯å¾„æ¶æ„å’Œæç¤ºå·¥ç¨‹ï¼ŒDuplexèƒ½å¤Ÿå¼ºåŒ–å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-db1f28f36b6cb284e316fa864fd074d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5279e22bf1c5e6d89d6e362023feae47.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-01-16\./crop_Vision Transformer/2501.07114v1/page_4_0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b95d170a559df0dd5fbb0cc95648c364.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4d48fcc028811dcd5537d7d0c5cce1c9.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM"><a href="#UNetVL-Enhancing-3D-Medical-Image-Segmentation-with-Chebyshev-KAN-Powered-Vision-LSTM" class="headerlink" title="UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM"></a>UNetVL: Enhancing 3D Medical Image Segmentation with Chebyshev KAN   Powered Vision-LSTM</h2><p><strong>Authors:Xuhui Guo, Tanmoy Dam, Rohan Dhamdhere, Gourav Modanwal, Anant Madabhushi</strong></p>
<p>3D medical image segmentation has progressed considerably due to Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), yet these methods struggle to balance long-range dependency acquisition with computational efficiency. To address this challenge, we propose UNETVL (U-Net Vision-LSTM), a novel architecture that leverages recent advancements in temporal information processing. UNETVL incorporates Vision-LSTM (ViL) for improved scalability and memory functions, alongside an efficient Chebyshev Kolmogorov-Arnold Networks (KAN) to handle complex and long-range dependency patterns more effectively. We validated our method on the ACDC and AMOS2022 (post challenge Task 2) benchmark datasets, showing a significant improvement in mean Dice score compared to recent state-of-the-art approaches, especially over its predecessor, UNETR, with increases of 7.3% on ACDC and 15.6% on AMOS, respectively. Extensive ablation studies were conducted to demonstrate the impact of each component in UNETVL, providing a comprehensive understanding of its architecture. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL">https://github.com/tgrex6/UNETVL</a>, facilitating further research and applications in this domain. </p>
<blockquote>
<p>3DåŒ»å­¦å›¾åƒåˆ†å‰²ç”±äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„è¿›å±•è€Œå–å¾—äº†ç›¸å½“å¤§çš„è¿›å±•ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¹³è¡¡é•¿ç¨‹ä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†UNETVLï¼ˆU-Net Vision-LSTMï¼‰è¿™ä¸€æ–°å‹æ¶æ„ï¼Œå®ƒåˆ©ç”¨äº†å¯¹æ—¶åºä¿¡æ¯å¤„ç†æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚UNETVLç»“åˆäº†Vision-LSTMï¼ˆViLï¼‰ä»¥æé«˜å…¶å¯æ‰©å±•æ€§å’Œå†…å­˜åŠŸèƒ½ï¼Œå¹¶é‡‡ç”¨äº†é«˜æ•ˆçš„å¥‘æ¯”é›ªå¤«Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ¥æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œé•¿ç¨‹ä¾èµ–æ¨¡å¼ã€‚æˆ‘ä»¬åœ¨ACDCå’ŒAMOS2022ï¼ˆæŒ‘æˆ˜èµ›åçš„ä»»åŠ¡2ï¼‰åŸºå‡†æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç›¸è¾ƒäºæœ€æ–°çš„å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å‰èº«UNETRä¸Šï¼Œæ˜¾ç¤ºå‡ºå¹³å‡Diceåˆ†æ•°çš„æ˜¾è‘—æé«˜ï¼ŒACDCä¸Šæé«˜äº†7.3%ï¼ŒAMOSä¸Šæé«˜äº†15.6%ã€‚è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œä»¥å±•ç¤ºUNETVLä¸­æ¯ä¸ªç»„ä»¶çš„å½±å“ï¼Œæä¾›äº†å¯¹å…¶æ¶æ„çš„å…¨é¢ç†è§£ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/tgrex6/UNETVL%EF%BC%8C%E4%B8%BA%E8%BF%99%E4%B8%80%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BA%94%E7%94%A8%E6%8F%90%E4%BE%9B%E4%BA%86%E4%BE%BF%E5%88%A9%E3%80%82">https://github.com/tgrex6/UNETVLï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¾¿åˆ©ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.07017v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>UNETVLæ˜¯ä¸€ç§ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰Transformerï¼ˆViTï¼‰ä¼˜ç‚¹çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é•¿è·ç¦»ä¾èµ–è·å–ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥è§†è§‰LSTMï¼ˆViLï¼‰æé«˜å¯æ‰©å±•æ€§å’Œè®°å¿†åŠŸèƒ½ï¼Œå¹¶é‡‡ç”¨é«˜æ•ˆçš„Chebyshev Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰æ›´æœ‰æ•ˆåœ°å¤„ç†å¤æ‚å’Œé•¿è·ç¦»ä¾èµ–æ¨¡å¼ã€‚åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼Œä¸æœ€æ–°å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œå°¤å…¶æ˜¯åœ¨å…¶å‰èº«UNETRä¸Šï¼Œå¹³å‡Diceå¾—åˆ†æ˜¾è‘—æé«˜ã€‚è¯¥æ¶æ„çš„ç»¼åˆç ”ç©¶è¯æ˜äº†å…¶æ¯ä¸ªç»„ä»¶çš„å½±å“åŠ›ã€‚ä»£ç å·²å…¬å¼€ä¾›è¿›ä¸€æ­¥ç ”ç©¶åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNETVLæ˜¯ä¸€ç§ç»“åˆäº†CNNå’ŒViTçš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é•¿è·ç¦»ä¾èµ–é—®é¢˜ã€‚</li>
<li>UNETVLå¼•å…¥è§†è§‰LSTMï¼ˆViLï¼‰ä»¥æé«˜å¯æ‰©å±•æ€§å’Œè®°å¿†åŠŸèƒ½ã€‚</li>
<li>UNETVLé‡‡ç”¨é«˜æ•ˆçš„Chebyshev Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å¤„ç†å¤æ‚å’Œé•¿è·ç¦»ä¾èµ–æ¨¡å¼ã€‚</li>
<li>åœ¨ACDCå’ŒAMOS2022åŸºå‡†æ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼ŒUNETVLç›¸å¯¹äºå…¶ä»–æœ€æ–°æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ï¼Œç‰¹åˆ«æ˜¯ä¸å…¶å‰èº«UNETRç›¸æ¯”ï¼Œå…¶Diceå¾—åˆ†æœ‰æ‰€æé«˜ã€‚</li>
<li>UNETVLçš„æ¶æ„åŒ…æ‹¬å…¨é¢çš„ç ”ç©¶åˆ†æï¼Œå±•ç¤ºäº†æ¯ä¸ªç»„ä»¶çš„å½±å“åŠ›ã€‚</li>
<li>UNETVLçš„ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.07017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9a0c173c68fcf2fc3f02c8a72e9f0df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ac52a42d87ef19502b538c400175a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3895c5b8d10051a070407ba7635abe83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89d5f5cf8fff3dec48d4df4779011549.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rice-Leaf-Disease-Detection-A-Comparative-Study-Between-CNN-Transformer-and-Non-neural-Network-Architectures"><a href="#Rice-Leaf-Disease-Detection-A-Comparative-Study-Between-CNN-Transformer-and-Non-neural-Network-Architectures" class="headerlink" title="Rice Leaf Disease Detection: A Comparative Study Between CNN,   Transformer and Non-neural Network Architectures"></a>Rice Leaf Disease Detection: A Comparative Study Between CNN,   Transformer and Non-neural Network Architectures</h2><p><strong>Authors:Samia Mehnaz, Md. Touhidul Islam</strong></p>
<p>In nations such as Bangladesh, agriculture plays a vital role in providing livelihoods for a significant portion of the population. Identifying and classifying plant diseases early is critical to prevent their spread and minimize their impact on crop yield and quality. Various computer vision techniques can be used for such detection and classification. While CNNs have been dominant on such image classification tasks, vision transformers has become equally good in recent time also. In this paper we study the various computer vision techniques for Bangladeshi rice leaf disease detection. We use the Dhan-Shomadhan â€“ a Bangladeshi rice leaf disease dataset, to experiment with various CNN and ViT models. We also compared the performance of such deep neural network architecture with traditional machine learning architecture like Support Vector Machine(SVM). We leveraged transfer learning for better generalization with lower amount of training data. Among the models tested, ResNet50 exhibited the best performance over other CNN and transformer-based models making it the optimal choice for this task. </p>
<blockquote>
<p>åœ¨å­ŸåŠ æ‹‰å›½ç­‰å›½å®¶ï¼Œå†œä¸šåœ¨ä¸ºæ•°ä¼—å¤šçš„äººå£æä¾›ç”Ÿè®¡æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æ—©æœŸè¯†åˆ«å’Œåˆ†ç±»æ¤ç‰©ç—…å®³å¯¹é˜²æ­¢å…¶æ‰©æ•£å’Œå°½é‡å‡å°‘å…¶å¯¹ä½œç‰©äº§é‡å’Œè´¨é‡çš„å½±å“è‡³å…³é‡è¦ã€‚å¯ä»¥ä½¿ç”¨å„ç§è®¡ç®—æœºè§†è§‰æŠ€æœ¯è¿›è¡Œæ­¤ç±»æ£€æµ‹å’Œåˆ†ç±»ã€‚è™½ç„¶å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨æ­¤ç±»å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†è§†è§‰è½¬æ¢å™¨ï¼ˆvision transformersï¼‰åœ¨æœ€è¿‘çš„æ—¶é—´ä¸­ä¹ŸåŒæ ·è¡¨ç°å‡ºè‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”¨äºå­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç‰‡ç—…å®³æ£€æµ‹çš„å„ç§è®¡ç®—æœºè§†è§‰æŠ€æœ¯ã€‚æˆ‘ä»¬ä½¿ç”¨å­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç‰‡æ•°æ®é›†Dhan-Shomadhanï¼Œå¯¹å„ç§CNNå’ŒViTæ¨¡å‹è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†è¿™ç§æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ä¸è¯¸å¦‚æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¶æ„çš„æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨è¿ç§»å­¦ä¹ ï¼Œä»¥åœ¨è¾ƒå°‘è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å®ç°æ›´å¥½çš„æ³›åŒ–ã€‚åœ¨æµ‹è¯•çš„æ¨¡å‹ä¸­ï¼ŒResNet50åœ¨å…¶ä»–CNNå’ŒåŸºäºtransformerçš„æ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œæˆä¸ºæ­¤ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06740v1">PDF</a> 6 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨å­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç‰‡ç—…å®³æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚é€šè¿‡ä½¿ç”¨Dhan-Shomadhanæ•°æ®é›†ï¼Œå®éªŒäº†CNNå’ŒViTæ¨¡å‹ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¶æ„SVMè¿›è¡Œäº†æ€§èƒ½æ¯”è¾ƒã€‚åˆ©ç”¨è¿ç§»å­¦ä¹ æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸‹å–å¾—è‰¯å¥½æ•ˆæœã€‚åœ¨æµ‹è¯•çš„æ¨¡å‹ä¸­ï¼ŒResNet50ç›¸è¾ƒäºå…¶ä»–CNNå’ŒåŸºäºtransformerçš„æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œæˆä¸ºè¯¥ä»»åŠ¡çš„æœ€ä¼˜é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†œä¸šåœ¨å­ŸåŠ æ‹‰å›½å…·æœ‰é‡è¦åœ°ä½ï¼Œæ—©æœŸè¯†åˆ«å’Œåˆ†ç±»æ¤ç‰©ç—…å®³å¯¹é˜²æ­¢ç—…å®³æ‰©æ•£å’Œå‡å°‘ä½œç‰©äº§é‡åŠå“è´¨çš„å½±å“è‡³å…³é‡è¦ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰æŠ€æœ¯å¯ç”¨äºæ¤ç‰©ç—…å®³çš„æ£€æµ‹å’Œåˆ†ç±»ã€‚</li>
<li>CNNå’ŒViTæ¨¡å‹åœ¨å­ŸåŠ æ‹‰å›½æ°´ç¨»å¶ç‰‡ç—…å®³æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¶æ„ç›¸æ¯”ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„å¦‚CNNå’ŒViTæ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>è¿ç§»å­¦ä¹ å¯ç”¨äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸‹å–å¾—è‰¯å¥½æ•ˆæœã€‚</li>
<li>åœ¨å®éªŒä¸­ï¼ŒResNet50æ¨¡å‹åœ¨ç—…å®³æ£€æµ‹ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œæˆä¸ºè¯¥ä»»åŠ¡çš„æœ€ä¼˜é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4b9a2875192e25383aa012b855034c25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0a763f1f45fa37c62ce320a2551c3cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a58bae6f03d7f23d21aa0a74dace69e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-633ed0700f61ea6af0528ef501f0365a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f99c56014ac4acb844d90b9d953c32c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da49fd9ee7daffe3ba0901aa559789b9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FocusDD-Real-World-Scene-Infusion-for-Robust-Dataset-Distillation"><a href="#FocusDD-Real-World-Scene-Infusion-for-Robust-Dataset-Distillation" class="headerlink" title="FocusDD: Real-World Scene Infusion for Robust Dataset Distillation"></a>FocusDD: Real-World Scene Infusion for Robust Dataset Distillation</h2><p><strong>Authors:Youbing Hu, Yun Cheng, Olga Saukh, Firat Ozdemir, Anqi Lu, Zhiqiang Cao, Zhijun Li</strong></p>
<p>Dataset distillation has emerged as a strategy to compress real-world datasets for efficient training. However, it struggles with large-scale and high-resolution datasets, limiting its practicality. This paper introduces a novel resolution-independent dataset distillation method Focus ed Dataset Distillation (FocusDD), which achieves diversity and realism in distilled data by identifying key information patches, thereby ensuring the generalization capability of the distilled dataset across different network architectures. Specifically, FocusDD leverages a pre-trained Vision Transformer (ViT) to extract key image patches, which are then synthesized into a single distilled image. These distilled images, which capture multiple targets, are suitable not only for classification tasks but also for dense tasks such as object detection. To further improve the generalization of the distilled dataset, each synthesized image is augmented with a downsampled view of the original image. Experimental results on the ImageNet-1K dataset demonstrate that, with 100 images per class (IPC), ResNet50 and MobileNet-v2 achieve validation accuracies of 71.0% and 62.6%, respectively, outperforming state-of-the-art methods by 2.8% and 4.7%. Notably, FocusDD is the first method to use distilled datasets for object detection tasks. On the COCO2017 dataset, with an IPC of 50, YOLOv11n and YOLOv11s achieve 24.4% and 32.1% mAP, respectively, further validating the effectiveness of our approach. </p>
<blockquote>
<p>æ•°æ®é›†è’¸é¦ä½œä¸ºä¸€ç§å‹ç¼©ç°å®ä¸–ç•Œæ•°æ®é›†ä»¥è¿›è¡Œé«˜æ•ˆè®­ç»ƒçš„ç­–ç•¥å·²ç»å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å¤„ç†å¤§è§„æ¨¡å’Œé«˜åˆ†è¾¨ç‡æ•°æ®é›†æ—¶é‡åˆ°äº†å›°éš¾ï¼Œä»è€Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åˆ†è¾¨ç‡ç‹¬ç«‹æ•°æ®é›†è’¸é¦æ–¹æ³•â€”â€”Focus ed Dataset Distillationï¼ˆFocusDDï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡è¯†åˆ«å…³é”®ä¿¡æ¯æ–‘å—ï¼Œå®ç°åœ¨è’¸é¦æ•°æ®ä¸­çš„å¤šæ ·æ€§å’Œç°å®æ€§ï¼Œä»è€Œç¡®ä¿è’¸é¦æ•°æ®é›†åœ¨ä¸åŒç½‘ç»œæ¶æ„ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒFocusDDåˆ©ç”¨é¢„å…ˆè®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æå–å…³é”®å›¾åƒæ–‘å—ï¼Œç„¶åå°†å…¶åˆæˆä¸ºå•ä¸ªè’¸é¦å›¾åƒã€‚è¿™äº›æ•è·äº†å¤šä¸ªç›®æ ‡çš„è’¸é¦å›¾åƒä¸ä»…é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œè€Œä¸”é€‚ç”¨äºå¯¹è±¡æ£€æµ‹ç­‰å¯†é›†ä»»åŠ¡ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è’¸é¦æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹åˆæˆçš„æ¯ä¸ªå›¾åƒéƒ½å¢åŠ äº†åŸå§‹å›¾åƒçš„ä¸‹é‡‡æ ·è§†å›¾ã€‚åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä»¥æ¯ç±»100å¼ å›¾åƒï¼ˆIPCï¼‰è®¡ç®—ï¼ŒResNet50å’ŒMobileNet-v2çš„éªŒè¯å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°71.0%å’Œ62.6%ï¼Œæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•åˆ†åˆ«é«˜å‡º2.8%å’Œ4.7%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒFocusDDæ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨è’¸é¦æ•°æ®é›†è¿›è¡Œå¯¹è±¡æ£€æµ‹ä»»åŠ¡çš„æ–¹æ³•ã€‚åœ¨COCO2017æ•°æ®é›†ä¸Šï¼Œä»¥æ¯ç±»50å¼ å›¾åƒï¼ˆIPCï¼‰è®¡ç®—ï¼ŒYOLOv11nå’ŒYOLOv11sçš„mAPåˆ†åˆ«è¾¾åˆ°24.4%å’Œ32.1%ï¼Œè¿™è¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06405v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹åˆ†è¾¨ç‡æ— å…³çš„æ•°æ®é›†è’¸é¦æ–¹æ³•â€”â€”FocusDDã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„Vision Transformerè¯†åˆ«å…³é”®ä¿¡æ¯å—ï¼Œä»å¤§è§„æ¨¡é«˜åˆ†è¾¨æ•°æ®é›†ä¸­æå–å…³é”®å›¾åƒå—å¹¶åˆæˆè’¸é¦æ•°æ®å›¾åƒï¼Œè¿›è€Œå®ç°æ•°æ®é›†çš„å‹ç¼©å’Œé«˜æ•ˆè®­ç»ƒã€‚FocusDDèƒ½åº”ç”¨äºåˆ†ç±»ä»»åŠ¡å’Œå¯†é›†ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹ï¼Œå¹¶é€šè¿‡å°†åˆæˆå›¾åƒä¸åŸå§‹ä½åˆ†è¾¨ç‡å›¾åƒç»“åˆï¼Œæé«˜è’¸é¦æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨FocusDDçš„ResNet50å’ŒMobileNet-v2æ¨¡å‹åœ¨100 IPCä¸‹è¾¾åˆ°71.0%å’Œ62.6%çš„éªŒè¯ç²¾åº¦ï¼Œä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒFocusDDæ˜¯é¦–ä¸ªç”¨äºç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ï¼Œåœ¨COCO2017æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FocusDDæ˜¯ä¸€ç§åˆ†è¾¨ç‡æ— å…³çš„æ•°æ®é›†è’¸é¦æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡é«˜åˆ†è¾¨æ•°æ®é›†ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„Vision Transformeræå–å…³é”®å›¾åƒå—ï¼Œåˆæˆè’¸é¦æ•°æ®å›¾åƒã€‚</li>
<li>FocusDDåŒæ—¶æ”¯æŒåˆ†ç±»ä»»åŠ¡å’Œç›®æ ‡æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡ç»“åˆåˆæˆå›¾åƒå’ŒåŸå§‹ä½åˆ†è¾¨ç‡å›¾åƒï¼Œæé«˜è’¸é¦æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨ImageNet-1Kæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒFocusDDæé«˜äº†æ¨¡å‹çš„éªŒè¯ç²¾åº¦ï¼Œä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FocusDDçš„æ¨¡å‹åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</li>
<li>FocusDDä¸ºæ•°æ®é›†è’¸é¦åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ff9d87a3e0bc2979e55435fcf792795a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b52238bb91195296b04513af7cda82d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eddbe3aad027a7ba64037e185e90be52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-246cc6cb3a2c342502c36d62a5ddc962.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9762e40c24bd16000d30b1776ac03bec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7f9d1fd5b4e9916fbba27d320af580e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PViT-Prior-augmented-Vision-Transformer-for-Out-of-distribution-Detection"><a href="#PViT-Prior-augmented-Vision-Transformer-for-Out-of-distribution-Detection" class="headerlink" title="PViT: Prior-augmented Vision Transformer for Out-of-distribution   Detection"></a>PViT: Prior-augmented Vision Transformer for Out-of-distribution   Detection</h2><p><strong>Authors:Tianhao Zhang, Zhixiang Chen, Lyudmila S. Mihaylova</strong></p>
<p>Vision Transformers (ViTs) have achieved remarkable success over various vision tasks, yet their robustness against data distribution shifts and inherent inductive biases remain underexplored. To enhance the robustness of ViT models for image Out-of-Distribution (OOD) detection, we introduce a novel and generic framework named Prior-augmented Vision Transformer (PViT). Taking as input the prior class logits from a pretrained model, we train PViT to predict the class logits. During inference, PViT identifies OOD samples by quantifying the divergence between the predicted class logits and the prior logits obtained from pre-trained models. Unlike existing state-of-the-art(SOTA) OOD detection methods, PViT shapes the decision boundary between ID and OOD by utilizing the proposed prior guided confidence, without requiring additional data modeling, generation methods, or structural modifications. Extensive experiments on the large-scale ImageNet benchmark, evaluated against over seven OOD datasets, demonstrate that PViT significantly outperforms existing SOTA OOD detection methods in terms of FPR95 and AUROC. The codebase is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RanchoGoose/PViT">https://github.com/RanchoGoose/PViT</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç„¶è€Œå®ƒä»¬å¯¹æ•°æ®åˆ†å¸ƒå˜åŒ–å’Œå›ºæœ‰å½’çº³åå¥½çš„é²æ£’æ€§ä»ç„¶æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¢å¼ºViTæ¨¡å‹å¯¹å›¾åƒåˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºå…ˆéªŒå¢å¼ºè§†è§‰Transformerï¼ˆPViTï¼‰çš„æ–°å‹é€šç”¨æ¡†æ¶ã€‚æˆ‘ä»¬ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒç±»åˆ«é€»è¾‘å€¼ä½œä¸ºè¾“å…¥ï¼Œè®­ç»ƒPViTæ¥é¢„æµ‹ç±»åˆ«é€»è¾‘å€¼ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒPViTé€šè¿‡é‡åŒ–é¢„æµ‹ç±»åˆ«é€»è¾‘å€¼ä¸ä»é¢„è®­ç»ƒæ¨¡å‹è·å¾—çš„å…ˆéªŒé€»è¾‘å€¼ä¹‹é—´çš„å·®å¼‚æ¥è¯†åˆ«OODæ ·æœ¬ã€‚ä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•ä¸åŒï¼ŒPViTé€šè¿‡åˆ©ç”¨æ‰€æå‡ºçš„å…ˆéªŒå¼•å¯¼ç½®ä¿¡åº¦æ¥å¡‘é€ IDå’ŒOODä¹‹é—´çš„å†³ç­–è¾¹ç•Œï¼Œæ— éœ€é¢å¤–çš„æ•°æ®å»ºæ¨¡ã€ç”Ÿæˆæ–¹æ³•æˆ–ç»“æ„ä¿®æ”¹ã€‚åœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œä¸ä¸ƒä¸ªOODæ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜PViTåœ¨FPR95å’ŒAUROCæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•ã€‚è¯¥ä»£ç åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RanchoGoose/PViT%E5%85%AC%E5%BC%BA%E9%A1%B5%E9%9D%A2%E5%AE%9E%E9%AA%8C%E4%BF%AE%E6%94%B9%E7%89%B9%E6%9D%BF/">https://github.com/RanchoGoose/PViTå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20631v2">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒç±»åˆ«logitsï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºPrior-augmented Vision Transformerï¼ˆPViTï¼‰çš„æ–°å‹é€šç”¨æ¡†æ¶ï¼Œç”¨äºå¢å¼ºå›¾åƒOut-of-Distributionï¼ˆOODï¼‰æ£€æµ‹çš„é²æ£’æ€§ã€‚PViTé€šè¿‡è¡¡é‡é¢„æµ‹ç±»åˆ«logitsä¸å…ˆéªŒlogitsä¹‹é—´çš„åå·®æ¥è¯†åˆ«OODæ ·æœ¬ã€‚åœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸ä¸ƒä¸ªOODæ•°æ®é›†è¿›è¡Œå¯¹æ¯”å®éªŒï¼ŒPViTåœ¨FPR95å’ŒAUROCæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PViTæ¡†æ¶è¢«å¼•å…¥ä»¥å¢å¼ºVision Transformerï¼ˆViTï¼‰æ¨¡å‹å¯¹å›¾åƒOut-of-Distributionï¼ˆOODï¼‰æ£€æµ‹çš„é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡è¡¡é‡é¢„æµ‹ç±»åˆ«logitsä¸æ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒlogitsä¹‹é—´çš„åå·®ï¼ŒPViTèƒ½å¤Ÿè¯†åˆ«OODæ ·æœ¬ã€‚</li>
<li>PViTé€šè¿‡åˆ©ç”¨æå‡ºçš„å…ˆéªŒå¼•å¯¼ç½®ä¿¡åº¦æ¥å¡‘é€ IDä¸OODä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚</li>
<li>PViTä¸éœ€è¦é¢å¤–çš„æ•°æ®å»ºæ¨¡ã€ç”Ÿæˆæ–¹æ³•æˆ–ç»“æ„ä¿®æ”¹ã€‚</li>
<li>åœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†PViTçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ä¸ƒä¸ªOODæ•°æ®é›†å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒPViTåœ¨FPR95å’ŒAUROCæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f76ad8b72a5470ccc1e5fdc5bfb4740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6df4c20b756f4515db16f90da7a726af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96a7492705ab8d89452572c94c549542.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9123d4ac61a7e8cc32ae038f91d32613.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc46d0979ecafbf005891239a6a6cfaa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bdc495bec58f6b9499c36e2b723b1bd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Scaling-White-Box-Transformers-for-Vision"><a href="#Scaling-White-Box-Transformers-for-Vision" class="headerlink" title="Scaling White-Box Transformers for Vision"></a>Scaling White-Box Transformers for Vision</h2><p><strong>Authors:Jinrui Yang, Xianhang Li, Druv Pai, Yuyin Zhou, Yi Ma, Yaodong Yu, Cihang Xie</strong></p>
<p>CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address. Specifically, we propose CRATE-$\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE. Through extensive experiments, we demonstrate that CRATE-$\alpha$ can effectively scale with larger model sizes and datasets. For example, our CRATE-$\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our CRATE-$\alpha$-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images. The project page is <a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/">https://rayjryang.github.io/CRATE-alpha/</a>. </p>
<blockquote>
<p>CRATEæ˜¯ä¸€ç§ç™½ç›’å˜å‹å™¨æ¶æ„ï¼Œæ—¨åœ¨å­¦ä¹ å‹ç¼©å’Œç¨€ç–è¡¨ç¤ºï¼Œç”±äºå…¶å›ºæœ‰çš„æ•°å­¦å¯è§£é‡Šæ€§ï¼Œå®ƒä¸ºæ ‡å‡†è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰æä¾›äº†æœ‰è¶£çš„æ›¿ä»£æ–¹æ¡ˆã€‚å°½ç®¡å¯¹è¯­è¨€å’Œè§†è§‰å˜å‹å™¨çš„è§„æ¨¡è¡Œä¸ºè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†CRATEçš„å¯æ‰©å±•æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†CRATE-$\alpha$ï¼Œå®ƒå¯¹CRATEæ¶æ„è®¾è®¡ä¸­çš„ç¨€ç–ç¼–ç å—è¿›è¡Œäº†æˆ˜ç•¥æ€§çš„æœ€å°ä¿®æ”¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„è®­ç»ƒé…æ–¹ï¼Œæ—¨åœ¨æé«˜CRATEçš„å¯æ‰©å±•æ€§ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CRATE-$\alpha$å¯ä»¥æœ‰æ•ˆåœ°æ‰©å±•åˆ°æ›´å¤§çš„æ¨¡å‹å’Œæ›´å¤§çš„æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Båœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡äº†ä¹‹å‰æœ€ä½³çš„CRATE-Bæ¨¡å‹å‡†ç¡®ç‡3.7%ï¼Œè¾¾åˆ°äº†83.2%ã€‚åŒæ—¶ï¼Œå½“æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å¤§è§„æ¨¡æ—¶ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Låœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†85.1%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹æ€§èƒ½çš„æå‡æ˜¯åœ¨ä¿ç•™ç”šè‡³æé«˜å­¦åˆ°çš„CRATEæ¨¡å‹çš„è§£é‡Šæ€§çš„å‰æä¸‹å®ç°çš„ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹è®­ç»ƒåçš„è¶Šæ¥è¶Šå¤§CRATE-$\alpha$æ¨¡å‹çš„æ ‡è®°è¡¨ç¤ºè¿›è¡Œå±•ç¤ºï¼Œè¯æ˜äº†å…¶è¿›è¡Œå›¾åƒçš„æ— ç›‘ç£å¯¹è±¡åˆ†å‰²çš„è´¨é‡è¶Šæ¥è¶Šé«˜ã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/%E3%80%82">https://rayjryang.github.io/CRATE-alpha/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.20299v4">PDF</a> project page: <a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/">https://rayjryang.github.io/CRATE-alpha/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>CRATEæ˜¯ä¸€ç§æ—¨åœ¨å­¦ä¹ å‹ç¼©å’Œç¨€ç–è¡¨ç¤ºçš„ç™½ç›’å˜å‹å™¨æ¶æ„ï¼Œä¸ºæ ‡å‡†çš„è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰æä¾›äº†æœ‰è¶£çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”±äºå…¶å›ºæœ‰çš„æ•°å­¦å¯è§£é‡Šæ€§ã€‚å°½ç®¡å·²ç»å¯¹è¯­è¨€å’Œè§†è§‰å˜å‹å™¨çš„è§„æ¨¡è¡Œä¸ºè¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†CRATEçš„å¯æ‰©å±•æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†CRATE-$\alpha$ï¼Œå¯¹CRATEæ¶æ„è®¾è®¡ä¸­çš„ç¨€ç–ç¼–ç å—è¿›è¡Œäº†æˆ˜ç•¥æ€§çš„æœ€å°ä¿®æ”¹ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è½»é‡çº§çš„è®­ç»ƒé…æ–¹ï¼Œä»¥æé«˜CRATEçš„å¯æ‰©å±•æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†CRATE-$\alpha$å¯ä»¥æœ‰æ•ˆåœ°æ‰©å±•æ›´å¤§çš„æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Båœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡æ¯”ä¹‹å‰çš„æœ€ä½³CRATE-Bæ¨¡å‹é«˜å‡º3.7%ï¼Œè¾¾åˆ°83.2%ã€‚åŒæ—¶ï¼Œå½“æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±•æ—¶ï¼Œæˆ‘ä»¬çš„CRATE-$\alpha$-Låœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†85.1%ã€‚æ›´å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹æ€§èƒ½çš„æå‡æ˜¯åœ¨ä¿æŒå’Œå¯èƒ½æé«˜CRATEæ¨¡å‹çš„è§£é‡Šæ€§ä¸‹å®ç°çš„ï¼Œæˆ‘ä»¬é€šè¿‡å±•ç¤ºè¶Šæ¥è¶Šå¤§ã€ç»è¿‡è®­ç»ƒçš„CRATE-$\alpha$æ¨¡å‹çš„æ ‡è®°è¡¨ç¤ºï¼Œå¯ä»¥è·å¾—è¶Šæ¥è¶Šé«˜è´¨é‡çš„æ— ç›‘ç£å›¾åƒå¯¹è±¡åˆ†å‰²æ¥è¯å®è¿™ä¸€ç‚¹ã€‚é¡¹ç›®é¡µé¢ä¸º<a target="_blank" rel="noopener" href="https://rayjryang.github.io/CRATE-alpha/%E3%80%82">https://rayjryang.github.io/CRATE-alpha/ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CRATEä½œä¸ºä¸€ç§ç™½ç›’å˜å‹å™¨æ¶æ„ï¼Œå…·æœ‰å­¦ä¹ å‹ç¼©å’Œç¨€ç–è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä¸ºè§†è§‰å˜å‹å™¨æä¾›äº†å¯è§£é‡Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>è®ºæ–‡æ—¨åœ¨è§£å†³CRATEå¯æ‰©å±•æ€§çš„é—®é¢˜ï¼Œæå‡ºCRATE-$\alpha$æ¨¡å‹ï¼Œé€šè¿‡æˆ˜ç•¥æ€§çš„æœ€å°ä¿®æ”¹å’Œè½»é‡çº§è®­ç»ƒé…æ–¹æé«˜å…¶å¯æ‰©å±•æ€§ã€‚</li>
<li>CRATE-$\alpha$åœ¨ImageNetåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¾ƒå¤§æ¨¡å‹CRATE-$\alpha$-Bçš„å‡†ç¡®ç‡ä¸º83.2%ï¼Œè¿›ä¸€æ­¥æ‰©å±•çš„CRATE-$\alpha$-Lçš„å‡†ç¡®ç‡è¾¾åˆ°äº†85.1%ã€‚</li>
<li>CRATE-$\alpha$åœ¨ä¿æŒç”šè‡³æé«˜æ¨¡å‹çš„è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå®ç°äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</li>
<li>è¶Šå¤§ã€ç»è¿‡è®­ç»ƒçš„CRATE-$\alpha$æ¨¡å‹çš„æ ‡è®°è¡¨ç¤ºï¼Œå¯ä»¥è·å¾—è¶Šæ¥è¶Šä¼˜è´¨çš„æ— ç›‘ç£å›¾åƒå¯¹è±¡åˆ†å‰²ï¼Œè¿™è¯å®äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†å…³äºCRATE-$\alpha$çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„å…·ä½“å®ç°å’Œå®éªŒç»“æœç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.20299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a24ba35722ac2eee920763a420af1edd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21125489eedd37271b85d39e3352cf47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4091fdfceb0a4347ae574f228e226555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0325f158223eb224c38cf96d4a215cb6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-16/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eda112a70646e26e626a9e100d78d313.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Threshold Attention Network for Semantic Segmentation of Remote Sensing   Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-16/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5833537aa44ce898235bce02f83c37bb.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-16  Omni-RGPT Unifying Image and Video Region-level Understanding via Token   Marks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13632.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
