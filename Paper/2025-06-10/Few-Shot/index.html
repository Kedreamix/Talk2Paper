<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Masked Language Models are Good Heterogeneous Graph Generalizers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6851b9d6494f5f8d348e0aaab1ae3941.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    48 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-10-æ›´æ–°"><a href="#2025-06-10-æ›´æ–°" class="headerlink" title="2025-06-10 æ›´æ–°"></a>2025-06-10 æ›´æ–°</h1><h2 id="Masked-Language-Models-are-Good-Heterogeneous-Graph-Generalizers"><a href="#Masked-Language-Models-are-Good-Heterogeneous-Graph-Generalizers" class="headerlink" title="Masked Language Models are Good Heterogeneous Graph Generalizers"></a>Masked Language Models are Good Heterogeneous Graph Generalizers</h2><p><strong>Authors:Jinyu Yang, Cheng Yang, Shanyuan Cui, Zeyuan Guo, Liangwei Yang, Muhan Zhang, Chuan Shi</strong></p>
<p>Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. Recently, some researchers have turned to integrating HGNNs with large language models (LLMs) for more generalizable heterogeneous graph learning. However, these approaches typically extract structural information via HGNNs as HG tokens, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLMâ€™s comprehension of HGs. Moreover, as these HG tokens are often derived from node-level tasks, the modelâ€™s ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style â€œmaskâ€ token prediction paradigm. Specifically, MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/MLM4HG">https://github.com/BUPT-GAMMA/MLM4HG</a>. </p>
<blockquote>
<p>å¼‚è´¨å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰æ“…é•¿æ•æ‰å¼‚è´¨å›¾ï¼ˆHGï¼‰ä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä½†åœ¨è·¨åŸŸå’Œä»»åŠ¡é—´çš„æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ€è¿‘ï¼Œä¸€äº›ç ”ç©¶äººå‘˜è¯•å›¾å°†HGNNsä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸ç»“åˆï¼Œä»¥å®ç°æ›´å…·é€šç”¨æ€§çš„å¼‚è´¨å›¾å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡HGNNsæå–ç»“æ„ä¿¡æ¯ä½œä¸ºHGä»¤ç‰Œï¼ŒHGNNså’ŒLLMsä¹‹é—´çš„åµŒå…¥ç©ºé—´å·®å¼‚å·²è¢«è¯æ˜ä¼šåå‘LLMå¯¹HGçš„ç†è§£ã€‚æ­¤å¤–ï¼Œç”±äºè¿™äº›HGä»¤ç‰Œé€šå¸¸æ¥æºäºèŠ‚ç‚¹çº§ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºMasked Language Modelingçš„æ–¹æ³•ï¼Œç§°ä¸ºMLM4HGã€‚MLM4HGå¼•å…¥åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—ï¼Œè€Œä¸æ˜¯HGä»¤ç‰Œï¼Œä»¥æå–HGä¸­å›ºæœ‰çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶è®¾è®¡å®šåˆ¶çš„æ–‡æœ¬æ¨¡æ¿ï¼Œå°†ä¸åŒçš„å›¾ä»»åŠ¡ç»Ÿä¸€ä¸ºè¿è´¯çš„å¡«ç©ºå¼â€œæ©ç â€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼ŒMLM4HGé¦–å…ˆæ ¹æ®å…ƒè·¯å¾„å°†æ¥è‡ªä¸åŒé¢†åŸŸçš„HGè½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åå°†å…¶ä¸ç»Ÿä¸€çš„ä»»åŠ¡æ–‡æœ¬ç»“åˆï¼Œå½¢æˆåŸºäºHGçš„è¯­æ–™åº“ã€‚æ­¤å¤–ï¼Œå°†è¯¥è¯­æ–™åº“è¾“å…¥é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨å—é™çš„ç›®æ ‡è¯æ±‡è¡¨ï¼Œä½¿å¾®è°ƒåçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„ç›®æ ‡HGã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è·¨åŸŸå’Œå¤šä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­ï¼ŒMLM4HGçš„æ³›åŒ–æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/MLM4HG%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BUPT-GAMMA/MLM4HGä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMasked Language Modelingçš„æ–¹æ³•ï¼Œåä¸ºMLM4HGï¼Œç”¨äºæé«˜å¼‚è´¨å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰åœ¨ä¸åŒé¢†åŸŸå’Œä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—æ›¿ä»£HG tokensæ¥æå–å¼‚è´¨å›¾ä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„æ–‡æœ¬æ¨¡æ¿å°†ä¸åŒçš„å›¾ä»»åŠ¡è½¬åŒ–ä¸ºä¸€ç§è¿è´¯çš„å¡«å……å¼â€œæ©ç â€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ã€‚å®éªŒè¯æ˜ï¼ŒMLM4HGåœ¨è·¨åŸŸå¤šä»»åŠ¡ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLM4HGç»“åˆäº†å¼‚è´¨å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œæ—¨åœ¨æé«˜å¼‚è´¨å›¾å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿçš„HGNNsä¸LLMsç»“åˆæ–¹æ³•å­˜åœ¨åµŒå…¥ç©ºé—´å·®å¼‚é—®é¢˜ï¼Œå½±å“LLMå¯¹å¼‚è´¨å›¾çš„ç†è§£ã€‚</li>
<li>MLM4HGä½¿ç”¨åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—æ›¿ä»£HG tokensï¼Œä»¥æå–å¼‚è´¨å›¾ä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>MLM4HGé‡‡ç”¨ç»Ÿä¸€çš„æ–‡æœ¬æ¨¡æ¿ï¼Œå°†ä¸åŒçš„å›¾ä»»åŠ¡è½¬åŒ–ä¸ºä¸€ç§è¿è´¯çš„å¡«å……å¼â€œæ©ç â€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ã€‚</li>
<li>MLM4HGé€šè¿‡è½¬æ¢ä¸åŒé¢†åŸŸçš„å¼‚è´¨å›¾ä¸ºæ–‡æœ¬ï¼Œç»“åˆç»Ÿä¸€ä»»åŠ¡æ–‡æœ¬ï¼Œå½¢æˆåŸºäºå¼‚è´¨å›¾çš„è¯­æ–™åº“ã€‚</li>
<li>ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¯¹è¯­æ–™åº“è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨çº¦æŸçš„ç›®æ ‡è¯æ±‡è¡¨ï¼Œä½¿å¾®è°ƒåçš„è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„å¼‚è´¨å›¾ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒMLM4HGåœ¨è·¨åŸŸå’Œè·¨ä»»åŠ¡çš„æ³›åŒ–æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸”åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-31fa2dabe71dd56c8302f460befba4f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-921bc2b43ffa46547f1ed62d745eac82.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Letâ€™s-CONFER-A-Dataset-for-Evaluating-Natural-Language-Inference-Models-on-CONditional-InFERence-and-Presupposition"><a href="#Letâ€™s-CONFER-A-Dataset-for-Evaluating-Natural-Language-Inference-Models-on-CONditional-InFERence-and-Presupposition" class="headerlink" title="Letâ€™s CONFER: A Dataset for Evaluating Natural Language Inference Models   on CONditional InFERence and Presupposition"></a>Letâ€™s CONFER: A Dataset for Evaluating Natural Language Inference Models   on CONditional InFERence and Presupposition</h2><p><strong>Authors:Tara Azin, Daniel Dumitrescu, Diana Inkpen, Raj Singh</strong></p>
<p>Natural Language Inference (NLI) is the task of determining whether a sentence pair represents entailment, contradiction, or a neutral relationship. While NLI models perform well on many inference tasks, their ability to handle fine-grained pragmatic inferences, particularly presupposition in conditionals, remains underexplored. In this study, we introduce CONFER, a novel dataset designed to evaluate how NLI models process inference in conditional sentences. We assess the performance of four NLI models, including two pre-trained models, to examine their generalization to conditional reasoning. Additionally, we evaluate Large Language Models (LLMs), including GPT-4o, LLaMA, Gemma, and DeepSeek-R1, in zero-shot and few-shot prompting settings to analyze their ability to infer presuppositions with and without prior context. Our findings indicate that NLI models struggle with presuppositional reasoning in conditionals, and fine-tuning on existing NLI datasets does not necessarily improve their performance. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰çš„ä»»åŠ¡æ˜¯åˆ¤æ–­å¥å­å¯¹ä¹‹é—´æ˜¯å¦å­˜åœ¨è•´å«ã€çŸ›ç›¾æˆ–ä¸­æ€§å…³ç³»ã€‚å°½ç®¡NLIæ¨¡å‹åœ¨è®¸å¤šæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬å¤„ç†ç»†å¾®è¯­ç”¨æ¨ç†çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯æ¡ä»¶å¥ä¸­çš„é¢„è®¾ï¼Œä»ç„¶æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CONFERï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°NLIæ¨¡å‹å¤„ç†æ¡ä»¶å¥ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¯„ä¼°äº†å››ç§NLIæ¨¡å‹ï¼ˆåŒ…æ‹¬ä¸¤ç§é¢„è®­ç»ƒæ¨¡å‹ï¼‰åœ¨æ¡ä»¶æ¨ç†æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4oã€LLaMAã€Gemmaå’ŒDeepSeek-R1ï¼Œåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºè®¾ç½®ä¸‹ï¼Œåˆ†æå®ƒä»¬æœ‰æ— å…ˆéªŒä¸Šä¸‹æ–‡æ—¶æ¨æ–­é¢„è®¾çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒNLIæ¨¡å‹åœ¨æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œä¸”åœ¨ç°æœ‰NLIæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¹¶ä¸ä¸€å®šèƒ½æé«˜å®ƒä»¬çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06133v1">PDF</a> This paper is published in the Proceedings of the 38th Canadian   Conference on Artificial Intelligence (CAIAC 2025). Please cite the   conference version at <a target="_blank" rel="noopener" href="https://caiac.pubpub.org/pub/keh8ij01">https://caiac.pubpub.org/pub/keh8ij01</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰åœ¨å¤„ç†æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ä¸ºäº†è¯„ä¼°NLIæ¨¡å‹åœ¨å¤„ç†æ¡ä»¶å¥ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œç ”ç©¶å¼•å…¥äº†CONFERæ•°æ®é›†ã€‚ç ”ç©¶è¯„ä¼°äº†å››ç§NLIæ¨¡å‹ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚å‘ç°NLIæ¨¡å‹åœ¨æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸”ç°æœ‰NLIæ•°æ®é›†çš„å¾®è°ƒå¹¶ä¸ä¸€å®šèƒ½å¤Ÿæ”¹å–„å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ˜¯ç¡®å®šå¥å­å¯¹ä¹‹é—´å…³ç³»çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬è•´æ¶µã€çŸ›ç›¾å’Œä¸­ç«‹å…³ç³»ã€‚</li>
<li>NLIæ¨¡å‹åœ¨å¤„ç†ç²¾ç»†çš„è¯­ç”¨æ¨ç†ï¼Œå°¤å…¶æ˜¯æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ—¶ï¼Œå…¶èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>CONFERæ•°æ®é›†è¢«å¼•å…¥ä»¥è¯„ä¼°NLIæ¨¡å‹å¤„ç†æ¡ä»¶å¥ä¸­çš„æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å››ç§NLIæ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚</li>
<li>NLIæ¨¡å‹åœ¨æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç°æœ‰NLIæ•°æ®é›†çš„å¾®è°ƒå¹¶ä¸ä¸€å®šèƒ½æ”¹å–„NLIæ¨¡å‹åœ¨æ¡ä»¶å¥ä¸­çš„é¢„è®¾æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6857f506767e6fe0e336f80655a5f110.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4699b4c06c99beaf24fd5ad868e9cedc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fec518e15847be9156901210bdc3d09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c586e222bb4dc46b17a3c87272ae4b5.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-are-Demonstration-Pre-Selectors-for-Themselves"><a href="#Large-Language-Models-are-Demonstration-Pre-Selectors-for-Themselves" class="headerlink" title="Large Language Models are Demonstration Pre-Selectors for Themselves"></a>Large Language Models are Demonstration Pre-Selectors for Themselves</h2><p><strong>Authors:Jiarui Jin, Yuwei Wu, Haoxuan Li, Xiaoting He, Weinan Zhang, Yiming Yang, Yong Yu, Jun Wang, Mengyue Yang</strong></p>
<p>In-context learning (ICL) with large language models (LLMs) delivers strong few-shot performance by choosing few-shot demonstrations from the entire training data. However, existing ICL methods, which rely on similarity or diversity scores to choose demonstrations, incur high computational costs due to repeatedly retrieval from large-scale datasets for each query. To this end, we propose FEEDER (FEw yet Essential Demonstration prE-selectoR), a novel pre-selection framework that identifies a representative subset of demonstrations containing the most representative examples in the training data, tailored to specific LLMs. To construct this subset, we introduce the â€œsufficiencyâ€ and â€œnecessityâ€ metrics in the pre-selection stage and design a tree-based algorithm to identify representative examples efficiently. Once pre-selected, this representative subset can effectively replace the full training data, improving efficiency while maintaining comparable performance in ICL. Additionally, our pre-selected subset also benefits fine-tuning LLMs, where we introduce a bi-level optimization method that enhances training efficiency without sacrificing performance. Experiments with LLMs ranging from 300M to 8B parameters show that FEEDER can reduce training data size by over 20% while maintaining performance and seamlessly integrating with various downstream demonstration selection strategies in ICL. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰é€šè¿‡ä»æ•´ä¸ªè®­ç»ƒæ•°æ®ä¸­é€‰å–å°‘é‡ç¤ºèŒƒæ¥å®ç°å¼ºå¤§çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ICLæ–¹æ³•ä¾èµ–äºç›¸ä¼¼æ€§æˆ–å¤šæ ·æ€§åˆ†æ•°æ¥é€‰æ‹©ç¤ºèŒƒï¼Œç”±äºæ¯æ¬¡æŸ¥è¯¢éƒ½éœ€è¦ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­è¿›è¡Œé‡å¤æ£€ç´¢ï¼Œå› æ­¤è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†FEEDERï¼ˆFEw yet Essential Demonstration pre-selectoRï¼‰è¿™ä¸€æ–°å‹é¢„é€‰æ‹©æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šçš„LLMï¼Œè¯†åˆ«å‡ºåŒ…å«è®­ç»ƒæ•°æ®ä¸­æœ€å…·ä»£è¡¨æ€§å®ä¾‹çš„ä»£è¡¨æ€§ç¤ºèŒƒå­é›†ã€‚ä¸ºäº†æ„å»ºè¿™ä¸ªå­é›†ï¼Œæˆ‘ä»¬åœ¨é¢„é€‰æ‹©é˜¶æ®µå¼•å…¥äº†â€œå……åˆ†æ€§â€å’Œâ€œå¿…è¦æ€§â€è¿™ä¸¤ä¸ªæŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºæ ‘çš„ç®—æ³•æ¥é«˜æ•ˆåœ°è¯†åˆ«ä»£è¡¨æ€§å®ä¾‹ã€‚ä¸€æ—¦é¢„é€‰å‡ºè¿™äº›å®ä¾‹ï¼Œå®ƒä»¬å°±å¯ä»¥æœ‰æ•ˆåœ°æ›¿æ¢å…¨é‡è®­ç»ƒæ•°æ®ï¼Œåœ¨æé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒICLçš„ç›¸å½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é¢„é€‰æ‹©å­é›†ä¹Ÿå¯¹å¾®è°ƒLLMæœ‰ç›Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤çº§ä¼˜åŒ–æ–¹æ³•ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚å®éªŒæ˜¾ç¤ºï¼Œä½¿ç”¨å‚æ•°èŒƒå›´ä»3äº¿åˆ°8äº¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒFEEDERå¯ä»¥åœ¨ç»´æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå‡å°‘è¶…è¿‡2 ç»“æµ‹è¯•æ•°æ®é›†å¤§å°çš„ç™¾åˆ†2çš„è®­ç»ƒæ•°æ®é‡å…¶ä¸ICLä¸­çš„å„ç§ä¸‹æ¸¸ç¤ºèŒƒé€‰æ‹©ç­–ç•¥æ— ç¼é›†æˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06033v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰é€šè¿‡é€‰æ‹©æ•´ä¸ªè®­ç»ƒæ•°æ®ä¸­çš„å°‘é‡æ ·æœ¬æ¼”ç¤ºå®ç°äº†å¼ºå¤§çš„å°‘æ ·æœ¬æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ICLæ–¹æ³•ä¾èµ–äºç›¸ä¼¼æ€§æˆ–å¤šæ ·æ€§è¯„åˆ†æ¥é€‰æ‹©æ¼”ç¤ºæ ·æœ¬ï¼Œç”±äºæ¯æ¬¡æŸ¥è¯¢éƒ½éœ€è¦ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­åå¤æ£€ç´¢ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FEEDERï¼ˆå…³é”®ç¤ºèŒƒé¢„é€‰å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é¢„é€‰æ‹©æ¡†æ¶ï¼Œå®ƒèƒ½åœ¨é’ˆå¯¹ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹å®šåˆ¶è®­ç»ƒæ—¶ï¼Œä»è®­ç»ƒæ•°æ®ä¸­è¯†åˆ«å‡ºæœ€å…·ä»£è¡¨æ€§çš„æ¼”ç¤ºæ ·æœ¬å­é›†ã€‚åœ¨æ„å»ºå­é›†æ—¶ï¼Œæˆ‘ä»¬åœ¨é¢„é€‰æ‹©é˜¶æ®µå¼•å…¥äº†â€œå……åˆ†æ€§â€å’Œâ€œå¿…è¦æ€§â€æŒ‡æ ‡ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºæ ‘çš„ç®—æ³•æ¥é«˜æ•ˆè¯†åˆ«ä»£è¡¨æ€§æ ·æœ¬ã€‚ä¸€æ—¦å®Œæˆé¢„é€‰æ‹©ï¼Œè¿™ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å­é›†å¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£å®Œæ•´çš„è®­ç»ƒæ•°æ®ï¼Œæé«˜æ•ˆç‡çš„åŒæ—¶ä¿æŒç›¸å½“çš„æ€§èƒ½è¡¨ç°äºICLä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é¢„é€‰å­é›†ä¹Ÿå¯¹å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æœ‰ç›Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤çº§ä¼˜åŒ–æ–¹æ³•ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡è€Œä¸æŸå¤±æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹å‚æ•°èŒƒå›´ä»3äº¿åˆ°8äº¿ä¸ç­‰çš„æƒ…å†µä¸‹ï¼ŒFEEDERèƒ½åœ¨ç»´æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘è¶…è¿‡20%çš„è®­ç»ƒæ•°æ®å¤§å°ï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°å„ç§ä¸‹æ¸¸æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FEEDERæ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„é€‰æ‹©æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘è®¡ç®—æˆæœ¬å¹¶ç»´æŒå°‘æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¼•å…¥â€œå……åˆ†æ€§â€å’Œâ€œå¿…è¦æ€§â€æŒ‡æ ‡ï¼ŒFEEDERèƒ½è¯†åˆ«æœ€å…·ä»£è¡¨æ€§çš„æ¼”ç¤ºæ ·æœ¬ã€‚</li>
<li>FEEDERåˆ©ç”¨åŸºäºæ ‘çš„ç®—æ³•é«˜æ•ˆåœ°ä»è®­ç»ƒæ•°æ®ä¸­é€‰å‡ºä»£è¡¨æ€§å­é›†ã€‚</li>
<li>é¢„é€‰å­é›†ä¸ä»…èƒ½æé«˜ä¸Šä¸‹æ–‡å­¦ä¹ çš„æ•ˆç‡ï¼Œä¹Ÿèƒ½åº”ç”¨äºå¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>FEEDERèƒ½åœ¨å‡å°‘è®­ç»ƒæ•°æ®å¤§å°çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>FEEDERèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å„ç§ä¸‹æ¸¸æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06033">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2de53c94b981b2ec6d1ed820643d4fa2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30ce95bc0aee1694de0f88bc1d489598.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad64996976634500ba8f6a0c2ab8e834.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9c6415a7687e3e0a5b551a1e7902782d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Domain-RAG-Retrieval-Guided-Compositional-Image-Generation-for-Cross-Domain-Few-Shot-Object-Detection"><a href="#Domain-RAG-Retrieval-Guided-Compositional-Image-Generation-for-Cross-Domain-Few-Shot-Object-Detection" class="headerlink" title="Domain-RAG: Retrieval-Guided Compositional Image Generation for   Cross-Domain Few-Shot Object Detection"></a>Domain-RAG: Retrieval-Guided Compositional Image Generation for   Cross-Domain Few-Shot Object Detection</h2><p><strong>Authors:Yu Li, Xingyu Qiu, Yuqian Fu, Jie Chen, Tianwen Qian, Xu Zheng, Danda Pani Paudel, Yanwei Fu, Xuanjing Huang, Luc Van Gool, Yu-Gang Jiang</strong></p>
<p>Cross-Domain Few-Shot Object Detection (CD-FSOD) aims to detect novel objects with only a handful of labeled samples from previously unseen domains. While data augmentation and generative methods have shown promise in few-shot learning, their effectiveness for CD-FSOD remains unclear due to the need for both visual realism and domain alignment. Existing strategies, such as copy-paste augmentation and text-to-image generation, often fail to preserve the correct object category or produce backgrounds coherent with the target domain, making them non-trivial to apply directly to CD-FSOD. To address these challenges, we propose Domain-RAG, a training-free, retrieval-guided compositional image generation framework tailored for CD-FSOD. Domain-RAG consists of three stages: domain-aware background retrieval, domain-guided background generation, and foreground-background composition. Specifically, the input image is first decomposed into foreground and background regions. We then retrieve semantically and stylistically similar images to guide a generative model in synthesizing a new background, conditioned on both the original and retrieved contexts. Finally, the preserved foreground is composed with the newly generated domain-aligned background to form the generated image. Without requiring any additional supervision or training, Domain-RAG produces high-quality, domain-consistent samples across diverse tasks, including CD-FSOD, remote sensing FSOD, and camouflaged FSOD. Extensive experiments show consistent improvements over strong baselines and establish new state-of-the-art results. Codes will be released upon acceptance. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆCD-FSODï¼‰æ—¨åœ¨ä»å…ˆå‰æœªè§è¿‡çš„é¢†åŸŸä¸­ï¼Œä»…ä½¿ç”¨å°‘é‡æ ‡è®°æ ·æœ¬å¯¹æ–°å‹ç›®æ ‡è¿›è¡Œæ£€æµ‹ã€‚è™½ç„¶æ•°æ®å¢å¼ºå’Œç”Ÿæˆæ–¹æ³•åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬å¯¹äºCD-FSODçš„æœ‰æ•ˆæ€§ä»ç„¶ä¸æ˜ç¡®ï¼Œå› ä¸ºè¿™éœ€è¦è§†è§‰ç°å®æ€§å’Œé¢†åŸŸå¯¹é½ã€‚ç°æœ‰ç­–ç•¥ï¼Œå¦‚å¤åˆ¶ç²˜è´´å¢å¼ºå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œå¾€å¾€æ— æ³•ä¿ç•™æ­£ç¡®çš„å¯¹è±¡ç±»åˆ«æˆ–äº§ç”Ÿä¸ç›®æ ‡é¢†åŸŸä¸€è‡´çš„èƒŒæ™¯ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥ç›´æ¥åº”ç”¨äºCD-FSODã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Domain-RAGï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€ç”±æ£€ç´¢å¼•å¯¼çš„ç»„æˆå›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œä¸“ä¸ºCD-FSODå®šåˆ¶ã€‚Domain-RAGç”±ä¸‰ä¸ªé˜¶æ®µç»„æˆï¼šé¢†åŸŸæ„ŸçŸ¥èƒŒæ™¯æ£€ç´¢ã€é¢†åŸŸå¼•å¯¼èƒŒæ™¯ç”Ÿæˆå’Œå‰æ™¯èƒŒæ™¯ç»„åˆã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆï¼Œå°†è¾“å…¥å›¾åƒåˆ†è§£ä¸ºå‰æ™¯å’ŒèƒŒæ™¯åŒºåŸŸã€‚ç„¶åï¼Œæˆ‘ä»¬æ£€ç´¢è¯­ä¹‰å’Œé£æ ¼ç›¸ä¼¼çš„å›¾åƒï¼Œä»¥æŒ‡å¯¼ç”Ÿæˆæ¨¡å‹æ ¹æ®åŸå§‹å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆæˆæ–°çš„èƒŒæ™¯ã€‚æœ€åï¼Œå°†ä¿ç•™çš„å‰æ™¯ä¸æ–°ç”Ÿæˆçš„é¢†åŸŸå¯¹é½çš„èƒŒæ™¯ç»„åˆèµ·æ¥å½¢æˆç”Ÿæˆçš„å›¾åƒã€‚æ— éœ€ä»»ä½•é¢å¤–çš„ç›‘ç£æˆ–è®­ç»ƒï¼ŒDomain-RAGåœ¨å¤šç§ä»»åŠ¡ä¸Šç”Ÿæˆé«˜è´¨é‡ã€é¢†åŸŸä¸€è‡´çš„æ ·æœ¬ï¼ŒåŒ…æ‹¬CD-FSODã€é¥æ„ŸFSODå’Œéšè”½FSODã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å¼ºå¤§çš„åŸºå‡†çº¿ç›¸æ¯”ï¼Œå®ƒå®ç°äº†æŒç»­ä¸€è‡´çš„æ”¹è¿›ï¼Œå¹¶åˆ›é€ äº†æ–°çš„æœ€ä½³ç»“æœã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05872v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é’ˆå¯¹è·¨åŸŸå°‘æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆCD-FSODï¼‰çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ã€ä»¥æ£€ç´¢ä¸ºæŒ‡å¯¼çš„ç»„åˆå›¾åƒç”Ÿæˆæ¡†æ¶â€”â€”Domain-RAGã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè§£å†³èƒŒæ™¯ä¸å‰æ™¯çš„ç»„åˆé—®é¢˜ï¼ŒåŒ…æ‹¬é¢†åŸŸæ„ŸçŸ¥èƒŒæ™¯æ£€ç´¢ã€é¢†åŸŸæŒ‡å¯¼èƒŒæ™¯ç”Ÿæˆå’Œå‰æ™¯-èƒŒæ™¯ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¹¶è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSODçš„ç›®æ ‡æ˜¯æ£€æµ‹å…ˆå‰æœªè§åŸŸä¸­åªæœ‰å°‘é‡æ ‡è®°æ ·æœ¬çš„æ–°å¯¹è±¡ã€‚</li>
<li>æ•°æ®å¢å¼ºå’Œç”Ÿæˆæ–¹æ³•åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ½œåŠ›åœ¨CD-FSODä¸­å°šä¸æ¸…æ¥šã€‚</li>
<li>ç°æœ‰ç­–ç•¥å¦‚å¤åˆ¶ç²˜è´´å¢å¼ºå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œå¸¸å¸¸æ— æ³•ä¿æŒæ­£ç¡®çš„å¯¹è±¡ç±»åˆ«æˆ–äº§ç”Ÿä¸ç›®æ ‡åŸŸèƒŒæ™¯ä¸ä¸€è‡´çš„å›¾åƒã€‚</li>
<li>Domain-RAGæ˜¯ä¸€ä¸ªé’ˆå¯¹CD-FSODçš„æ— éœ€è®­ç»ƒã€ä»¥æ£€ç´¢ä¸ºæŒ‡å¯¼çš„ç»„åˆå›¾åƒç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>Domain-RAGåŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šé¢†åŸŸæ„ŸçŸ¥èƒŒæ™¯æ£€ç´¢ã€é¢†åŸŸæŒ‡å¯¼èƒŒæ™¯ç”Ÿæˆå’Œå‰æ™¯-èƒŒæ™¯ç»„åˆã€‚</li>
<li>Domain-RAGåœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬CD-FSODã€é¥æ„ŸFSODå’Œéšè”½FSODã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05872">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71d91a23c6215fa8e28456be1d573622.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88f08f32c1d56de6ce1d73700367dc8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e31b5a795ef14e95ff40fffb7a6f716.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bd88954bd0b564c2d668413d271bff2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Efficient-Online-RFT-with-Plug-and-Play-LLM-Judges-Unlocking-State-of-the-Art-Performance"><a href="#Efficient-Online-RFT-with-Plug-and-Play-LLM-Judges-Unlocking-State-of-the-Art-Performance" class="headerlink" title="Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking   State-of-the-Art Performance"></a>Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking   State-of-the-Art Performance</h2><p><strong>Authors:Rudransh Agnihotri, Ananya Pandey</strong></p>
<p>Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the modelâ€™s parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately &#x3D; 9&#x2F;10 in similarity to human explanations, while zero-shot judges score around &#x3D;5&#x2F;10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹è®­ç»ƒæ˜¯ç°ä»£å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ç®¡é“ä¸­çš„æˆæœ¬ç“¶é¢ˆï¼Œé€šå¸¸éœ€è¦æ•°åäº¿å‚æ•°å’Œç¦»çº¿åå¥½è°ƒæ•´é˜¶æ®µã€‚åœ¨æå‡ºçš„æ–¹æ³•ä¸­ï¼Œä¸€ä¸ªå†»ç»“çš„ã€ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„7Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»…é€šè¿‡ä¸€è¡ŒJSONå‡†åˆ™å’Œä¸€ä¸ªæ’åç¬¬16çš„LoRAé€‚é…å™¨ï¼ˆä»…å½±å“æ¨¡å‹0.8%çš„å‚æ•°ï¼‰è¿›è¡Œå¢å¼ºï¼Œèƒ½å¤Ÿä½œä¸ºä»¥å‰ä½¿ç”¨çš„é‡å‹è¯„ä¼°æ¨¡å‹çš„ç»¼åˆæ›¿ä»£å“ã€‚å³æ’å³ç”¨åˆ¤å®˜åœ¨RewardBenchä¸Šè¾¾åˆ°äº†96.2%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ä»27Båˆ°70Bå‚æ•°çš„ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚æ­¤å¤–ï¼Œå®ƒå…è®¸ä¸€ä¸ª7Bæ¼”å‘˜åœ¨GSM-8Kä¸Šå®ç°92%çš„ç²¾ç¡®åŒ¹é…ç‡ï¼Œè¶…è¶Šäº†å¾—åˆ†61.8%çš„70B DPOåŸºçº¿ã€‚å½»åº•çš„å‰¥ç¦»å®éªŒè¡¨æ˜ï¼šï¼ˆiï¼‰å…­ä¸ªä¸Šä¸‹æ–‡æ¼”ç¤ºå®ç°äº†å¤§éƒ¨åˆ†é›¶åˆ°å°‘æ ·æœ¬çš„æ”¹è¿›ï¼ˆ+2ppï¼‰ï¼›ï¼ˆiiï¼‰LoRAæœ‰æ•ˆåœ°è§£å†³äº†å‰©ä½™çš„å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å’Œå¯¹æŠ—æ€§çš„Chat-Hardç‰‡æ®µä¸­ã€‚æ‰€æå‡ºçš„æ¨¡å‹å¼•å…¥äº†HH-Rationalesï¼Œè¿™æ˜¯ä»Anthropic HH-RLHFä¸­çš„1ä¸‡å¯¹å­é›†ï¼Œç”¨äºæ£€æŸ¥è§£é‡Šæ€§ï¼Œå¹¶æœ‰äººç±»ç”Ÿæˆçš„æ­£å½“ç†ç”±ã€‚GPT-4è¯„åˆ†æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„LoRAåˆ¤å®˜åœ¨ç›¸ä¼¼æ€§æ–¹é¢è¾¾åˆ°äº†çº¦9&#x2F;10ï¼Œè€Œé›¶å°„å‡»åˆ¤å®˜å¾—åˆ†çº¦ä¸º5&#x2F;10ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæç¤ºå·¥ç¨‹å’Œå¾®å°çš„LoRAç›¸ç»“åˆäº§ç”Ÿäº†æˆæœ¬æ•ˆç›Šé«˜ã€é€æ˜ã€æ˜“äºè°ƒæ•´çš„å¥–åŠ±å‡½æ•°ï¼Œå»é™¤äº†ç¦»çº¿é˜¶æ®µï¼ŒåŒæ—¶ä¸ºå®ç°é™æ€è¯„ä¼°å’Œåœ¨çº¿RLHFçš„æœ€æ–°æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05748v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä»…ä½¿ç”¨ä¸€è¡ŒJSONå‡†åˆ™å’Œä¸€ä¸ªå½±å“æ¨¡å‹å‚æ•°ä»…0.8%çš„rank-16 LoRAé€‚é…å™¨ï¼Œæ¥æ›¿ä»£ä¼ ç»Ÿçš„é‡é‡çº§è¯„ä¼°æ¨¡å‹ã€‚è¯¥æ–¹æ³•åœ¨RewardBenchä¸Šè¾¾åˆ°96.2%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å‚æ•°èŒƒå›´ä»27Båˆ°70Bçš„ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½ä½¿7Bæ¼”å‘˜åœ¨GSM-8Kä¸Šè¾¾åˆ°92%çš„ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†é¡¶çº§70B DPOåŸºå‡†æµ‹è¯•61.8%çš„å¾—åˆ†ã€‚æ­¤æ–¹æ³•è¿˜å¼•å…¥äº†HH-Rationalesï¼Œæ£€æŸ¥äº†å¯è§£é‡Šæ€§ï¼Œå¹¶ç”±äººç±»ç”Ÿæˆäº†ç†ç”±ã€‚GPT-4è¯„åˆ†æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„LoRAè£åˆ¤åœ¨ç±»ä¼¼äººç±»è§£é‡Šæ–¹é¢è¾¾åˆ°çº¦9&#x2F;10ï¼Œè€Œé›¶å°„å‡»è£åˆ¤çº¦ä¸º5&#x2F;10ã€‚æ­¤æ–¹æ³•æä¾›äº†ä¸€ç§ä½æˆæœ¬ã€é€æ˜ã€å¯è°ƒæ•´çš„å›é¦ˆåŠŸèƒ½ï¼Œç§»é™¤äº†ç¦»çº¿é˜¶æ®µï¼ŒåŒæ—¶ä¸ºé™æ€è¯„ä¼°å’Œåœ¨çº¿RLHFè¾¾åˆ°äº†æœ€æ–°çš„ä¸€æµç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºä¸€ç§æ–°å‹å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæ–¹æ³•ï¼Œä½¿ç”¨JSONå‡†åˆ™å’ŒLoRAé€‚é…å™¨æ›¿ä»£é‡é‡çº§è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨RewardBenchä¸Šå®ç°é«˜å‡†ç¡®ç‡ï¼Œæ˜¾è‘—è¶…è¶Šå…¶ä»–ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚</li>
<li>LoRAé€‚é…å™¨ä½¿å¾—å°æ¨¡å‹èƒ½åœ¨æ€§èƒ½ä¸Šè¶…è¶Šå¤§å‹åŸºå‡†æ¨¡å‹ï¼Œå¦‚åœ¨GSM-8Kä¸Šçš„è¡¨ç°ã€‚</li>
<li>å¼•å…¥HH-Rationalesä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå¹¶å¾—åˆ°äººç±»ç”Ÿæˆçš„ç†ç”±æ”¯æŒã€‚</li>
<li>LoRAè£åˆ¤åœ¨æ¨¡æ‹Ÿäººç±»è§£é‡Šæ–¹é¢çš„è¡¨ç°å¾—åˆ°äº†GPT-4çš„é«˜è¯„åˆ†ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰æ•ˆç»“åˆäº†æç¤ºå·¥ç¨‹å’Œå¾®å°çš„LoRAï¼Œæä¾›äº†æˆæœ¬æ•ˆç›Šé«˜ã€é€æ˜ã€å¯è°ƒæ•´çš„å›é¦ˆåŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23d60b20577b59ddb38a656bb9f575ff.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Hallucinate-Ground-Repeat-A-Framework-for-Generalized-Visual-Relationship-Detection"><a href="#Hallucinate-Ground-Repeat-A-Framework-for-Generalized-Visual-Relationship-Detection" class="headerlink" title="Hallucinate, Ground, Repeat: A Framework for Generalized Visual   Relationship Detection"></a>Hallucinate, Ground, Repeat: A Framework for Generalized Visual   Relationship Detection</h2><p><strong>Authors:Shanmukha Vellamcheti, Sanjoy Kundu, Sathyanarayanan N. Aakur</strong></p>
<p>Understanding relationships between objects is central to visual intelligence, with applications in embodied AI, assistive systems, and scene understanding. Yet, most visual relationship detection (VRD) models rely on a fixed predicate set, limiting their generalization to novel interactions. A key challenge is the inability to visually ground semantically plausible, but unannotated, relationships hypothesized from external knowledge. This work introduces an iterative visual grounding framework that leverages large language models (LLMs) as structured relational priors. Inspired by expectation-maximization (EM), our method alternates between generating candidate scene graphs from detected objects using an LLM (expectation) and training a visual model to align these hypotheses with perceptual evidence (maximization). This process bootstraps relational understanding beyond annotated data and enables generalization to unseen predicates. Additionally, we introduce a new benchmark for open-world VRD on Visual Genome with 21 held-out predicates and evaluate under three settings: seen, unseen, and mixed. Our model outperforms LLM-only, few-shot, and debiased baselines, achieving mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on these three sets. These results highlight the promise of grounded LLM priors for scalable open-world visual understanding. </p>
<blockquote>
<p>ç†è§£ç‰©ä½“ä¹‹é—´çš„å…³ç³»æ˜¯è§†è§‰æ™ºèƒ½çš„æ ¸å¿ƒï¼Œå…¶åº”ç”¨åœºæ™¯åŒ…æ‹¬å®ä½“äººå·¥æ™ºèƒ½ã€è¾…åŠ©ç³»ç»Ÿå’Œåœºæ™¯ç†è§£ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ¨¡å‹ä¾èµ–äºå›ºå®šçš„è°“è¯é›†ï¼Œé™åˆ¶äº†å…¶åœ¨æ–°å‹äº¤äº’åœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯æ— æ³•ä»å¤–éƒ¨çŸ¥è¯†ä¸­æ¨æ–­å‡ºè§†è§‰åŸºç¡€è¯­ä¹‰ä¸Šåˆç†ä½†æœªè¿›è¡Œæ ‡æ³¨çš„å…³ç³»ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§è¿­ä»£è§†è§‰åŸºç¡€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºç»“æ„åŒ–å…³ç³»å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•çš„å¯å‘ï¼Œåœ¨åˆ©ç”¨LLMç”ŸæˆåŸºäºæ£€æµ‹å¯¹è±¡çš„å€™é€‰åœºæ™¯å›¾ï¼ˆæœŸæœ›ï¼‰å’Œè®­ç»ƒè§†è§‰æ¨¡å‹å°†è¿™äº›å‡è®¾ä¸æ„ŸçŸ¥è¯æ®å¯¹é½ï¼ˆæœ€å¤§åŒ–ï¼‰ä¹‹é—´äº¤æ›¿è¿›è¡Œã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡æ ‡æ³¨æ•°æ®æå‡å…³ç³»ç†è§£ï¼Œå¹¶å®ç°å¯¹æœªè§è°“è¯åœºæ™¯çš„æ³›åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è§†è§‰åŸºå› ç»„çš„å¼€æ”¾ä¸–ç•ŒVRDä»»åŠ¡ä¸Šå¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬21ä¸ªæœªéœ²é¢è°“è¯ï¼Œå¹¶åœ¨ä¸‰ç§è®¾ç½®ä¸‹è¿›è¡Œè¯„ä¼°ï¼šå¯è§ã€ä¸å¯è§å’Œæ··åˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è°“è¯åˆ†ç±»æ–¹é¢çš„mR@50æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ä»…ä½¿ç”¨LLMã€å°‘æ ·æœ¬å’Œå»ååŸºçº¿ï¼Œåœ¨è¿™ä¸‰ä¸ªé›†åˆä¸Šçš„å¾—åˆ†åˆ†åˆ«ä¸º15.9ã€13.1å’Œ11.7ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†åŸºäºLLMå…ˆéªŒçŸ¥è¯†çš„å¯æ‰©å±•å¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05651v1">PDF</a> 22 pages, 9 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†è§†è§‰æ™ºèƒ½çš„æ ¸å¿ƒâ€”â€”ç‰©ä½“é—´å…³ç³»çš„ç†è§£ï¼Œå¹¶ä»‹ç»äº†å…¶åœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½ã€è¾…åŠ©ç³»ç»Ÿå’Œåœºæ™¯ç†è§£ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚é’ˆå¯¹å½“å‰è§†è§‰å…³ç³»æ£€æµ‹æ¨¡å‹ä¾èµ–äºå›ºå®šè°“è¯é›†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç»“æ„åŒ–å…³ç³»å…ˆéªŒçš„è¿­ä»£è§†è§‰å®šä½æ¡†æ¶ã€‚è¯¥æ¡†æ¶å—åˆ°æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•çš„å¯å‘ï¼Œé€šè¿‡äº¤æ›¿ç”Ÿæˆå€™é€‰åœºæ™¯å›¾å¹¶è®­ç»ƒè§†è§‰æ¨¡å‹ï¼Œä»¥å®ç°å¯¹æœªæ ‡æ³¨å…³ç³»çš„ç†è§£ï¼Œå¹¶æ¨å¹¿åˆ°æœªè§è¿‡çš„è°“è¯ä¸Šã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†åœ¨è§†è§‰åŸºå› ç»„ä¸Šé’ˆå¯¹å¼€æ”¾ä¸–ç•Œè§†è§‰å…³ç³»æ£€æµ‹çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨ä¸‰ç§è®¾ç½®ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨è°“è¯åˆ†ç±»ä¸Šçš„å¹³å‡å¬å›ç‡ï¼ˆmR@50ï¼‰åˆ†åˆ«ä¸º15.9ã€13.1å’Œ11.7ï¼Œçªæ˜¾äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†åœ¨å¯æ‰©å±•çš„å¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ™ºèƒ½çš„æ ¸å¿ƒåœ¨äºç†è§£å’Œåˆ†æç‰©ä½“é—´çš„å…³ç³»ï¼Œå¹¿æ³›åº”ç”¨äºåµŒå…¥å¼AIã€è¾…åŠ©ç³»ç»Ÿå’Œåœºæ™¯ç†è§£ã€‚</li>
<li>å½“å‰è§†è§‰å…³ç³»æ£€æµ‹æ¨¡å‹ä¸»è¦ä¾èµ–äºå›ºå®šçš„è°“è¯é›†ï¼Œé™åˆ¶äº†å…¶åœ¨æ–°äº¤äº’åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§è¿­ä»£è§†è§‰å®šä½æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç»“æ„åŒ–å…³ç³»å…ˆéªŒï¼Œä»¥çªç ´å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œå®ç°æœªè§è°“è¯çš„æ¨å¹¿ã€‚</li>
<li>è¯¥æ¡†æ¶å—åˆ°æœŸæœ›æœ€å¤§åŒ–ç®—æ³•çš„å¯å‘ï¼Œäº¤æ›¿ç”Ÿæˆå€™é€‰åœºæ™¯å›¾å’Œè®­ç»ƒè§†è§‰æ¨¡å‹ã€‚</li>
<li>ä»‹ç»äº†åœ¨è§†è§‰åŸºå› ç»„ä¸Šçš„æ–°åŸºå‡†æµ‹è¯•ï¼Œé’ˆå¯¹å¼€æ”¾ä¸–ç•Œçš„è§†è§‰å…³ç³»æ£€æµ‹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>æ¨¡å‹åœ¨ä¸‰ç§ä¸åŒè®¾ç½®ä¸‹çš„è°“è¯åˆ†ç±»è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œçªæ˜¾äº†å¤§å‹è¯­è¨€æ¨¡å‹å…ˆéªŒçŸ¥è¯†å¯¹äºå¼€æ”¾ä¸–ç•Œè§†è§‰ç†è§£çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¯æ‰©å±•çš„è§†è§‰æ™ºèƒ½å’Œå¼€æ”¾ä¸–ç•Œåœºæ™¯ç†è§£æä¾›äº†æ–°çš„ç ”ç©¶æ–¹å‘å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe96dc848313d1af4b898a61e905531f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95a03242219f28b28131ac1d134c8a57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-037b344ad8b9c577d7bf741627510c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb1c2eee732ee083a4c848d012abd976.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PCEvolve-Private-Contrastive-Evolution-for-Synthetic-Dataset-Generation-via-Few-Shot-Private-Data-and-Generative-APIs"><a href="#PCEvolve-Private-Contrastive-Evolution-for-Synthetic-Dataset-Generation-via-Few-Shot-Private-Data-and-Generative-APIs" class="headerlink" title="PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation   via Few-Shot Private Data and Generative APIs"></a>PCEvolve: Private Contrastive Evolution for Synthetic Dataset Generation   via Few-Shot Private Data and Generative APIs</h2><p><strong>Authors:Jianqing Zhang, Yang Liu, Jie Fu, Yang Hua, Tianyuan Zou, Jian Cao, Qiang Yang</strong></p>
<p>The rise of generative APIs has fueled interest in privacy-preserving synthetic data generation. While the Private Evolution (PE) algorithm generates Differential Privacy (DP) synthetic images using diffusion model APIs, it struggles with few-shot private data due to the limitations of its DP-protected similarity voting approach. In practice, the few-shot private data challenge is particularly prevalent in specialized domains like healthcare and industry. To address this challenge, we propose a novel API-assisted algorithm, Private Contrastive Evolution (PCEvolve), which iteratively mines inherent inter-class contrastive relationships in few-shot private data beyond individual data points and seamlessly integrates them into an adapted Exponential Mechanism (EM) to optimize DPâ€™s utility in an evolution loop. We conduct extensive experiments on four specialized datasets, demonstrating that PCEvolve outperforms PE and other API-assisted baselines. These results highlight the potential of leveraging API access with private data for quality evaluation, enabling the generation of high-quality DP synthetic images and paving the way for more accessible and effective privacy-preserving generative API applications. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TsingZ0/PCEvolve">https://github.com/TsingZ0/PCEvolve</a>. </p>
<blockquote>
<p>ç”Ÿæˆå¼APIçš„å…´èµ·æ¿€å‘äº†äººä»¬å¯¹éšç§ä¿æŠ¤åˆæˆæ•°æ®ç”Ÿæˆçš„å…´è¶£ã€‚è™½ç„¶Private Evolutionï¼ˆPEï¼‰ç®—æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹APIç”Ÿæˆå·®åˆ†éšç§ï¼ˆDPï¼‰åˆæˆå›¾åƒï¼Œä½†ç”±äºå…¶DPä¿æŠ¤ç›¸ä¼¼æ€§æŠ•ç¥¨æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ƒåœ¨å°æ ·æœ¬ç§æœ‰æ•°æ®æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚åœ¨å®è·µä¸­ï¼Œå°æ ·æœ¬ç§æœ‰æ•°æ®æŒ‘æˆ˜åœ¨åŒ»ç–—å’Œå·¥ä¸šç­‰ç‰¹å®šé¢†åŸŸå°¤ä¸ºæ™®éã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„APIè¾…åŠ©ç®—æ³•â€”â€”Private Contrastive Evolutionï¼ˆPCEvolveï¼‰ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿè¿­ä»£æŒ–æ˜å°æ ·æœ¬ç§æœ‰æ•°æ®ä¸­å›ºæœ‰çš„ç±»é—´å¯¹æ¯”å…³ç³»ï¼Œè¶…è¶Šå•ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶æ— ç¼é›†æˆåˆ°æ”¹è¿›çš„æŒ‡æ•°æœºåˆ¶ï¼ˆEMï¼‰ä¸­ï¼Œåœ¨è¿›åŒ–å¾ªç¯ä¸­ä¼˜åŒ–DPçš„å®ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä¸“ç”¨æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œç»“æœè¡¨æ˜PCEvolveä¼˜äºPEå’Œå…¶ä»–APIè¾…åŠ©åŸºçº¿ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åˆ©ç”¨APIè®¿é—®ç§æœ‰æ•°æ®è¿›è¡Œè´¨é‡è¯„ä¼°çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„DPåˆæˆå›¾åƒç”Ÿæˆï¼Œå¹¶ä¸ºæ›´å¯è®¿é—®å’Œæœ‰æ•ˆçš„éšç§ä¿æŠ¤ç”Ÿæˆå¼APIåº”ç”¨ç¨‹åºé“ºå¹³é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TsingZ0/PCEvolve%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/TsingZ0/PCEvolveè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05407v1">PDF</a> Accepted as ICML Spotlight (top 2.6%)</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£ç”Ÿæˆå¼APIæ­£åœ¨æ¿€å‘éšç§ä¿æŠ¤åˆæˆæ•°æ®ç”Ÿæˆçš„å…³æ³¨åº¦æå‡ã€‚æå‡ºçš„Private Contrastive Evolutionï¼ˆPCEvolveï¼‰ç®—æ³•è§£å†³äº†ä½¿ç”¨æ‰©æ•£æ¨¡å‹APIç”Ÿæˆå·®åˆ†éšç§ï¼ˆDPï¼‰åˆæˆå›¾åƒæ—¶é¢ä¸´çš„å°‘æ•°éšç§æ•°æ®æŒ‘æˆ˜ã€‚é€šè¿‡æŒ–æ˜å°‘æ•°éšç§æ•°æ®ä¸­çš„ç±»é—´å¯¹æ¯”å…³ç³»ï¼ŒPCEvolveå°†å…¶æ— ç¼é›†æˆåˆ°æ”¹è¿›çš„æŒ‡æ•°æœºåˆ¶ä¸­ï¼Œä¼˜åŒ–äº†è¿›åŒ–å¾ªç¯ä¸­çš„å·®åˆ†éšç§æ•ˆç”¨ã€‚åœ¨å››ä¸ªä¸“ä¸šæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPCEvolveä¼˜äºPrivate Evolutionï¼ˆPEï¼‰å’Œå…¶ä»–APIè¾…åŠ©åŸºçº¿æ–¹æ³•ã€‚è¿™å‡¸æ˜¾äº†åˆ©ç”¨APIè®¿é—®è¿›è¡Œè´¨é‡è¯„ä¼°çš„æ½œåŠ›ï¼Œå¯å®ç°é«˜è´¨é‡çš„å·®åˆ†éšç§åˆæˆå›¾åƒç”Ÿæˆï¼Œå¹¶ä¸ºæ›´ä¾¿æ·æœ‰æ•ˆçš„éšç§ä¿æŠ¤ç”Ÿæˆå¼APIåº”ç”¨å¼€è¾Ÿäº†é“è·¯ã€‚ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/TsingZ0/PCEvolve">https://github.com/TsingZ0/PCEvolve</a> è¿›è¡ŒæŸ¥é˜…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼APIæ­£åœ¨ä¿ƒè¿›éšç§ä¿æŠ¤åˆæˆæ•°æ®ç”Ÿæˆçš„å…³æ³¨åº¦æå‡ã€‚</li>
<li>Private Evolutionï¼ˆPEï¼‰ç®—æ³•åœ¨ç”Ÿæˆå·®åˆ†éšç§åˆæˆå›¾åƒæ—¶é¢ä¸´å°‘æ•°éšç§æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºPrivate Contrastive Evolutionï¼ˆPCEvolveï¼‰çš„æ–°å‹ç®—æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PCEvolveç®—æ³•èƒ½å¤ŸæŒ–æ˜å°‘æ•°éšç§æ•°æ®ä¸­çš„ç±»é—´å¯¹æ¯”å…³ç³»å¹¶ä¼˜åŒ–å·®åˆ†éšç§çš„æ•ˆç”¨ã€‚</li>
<li>PCEvolveåœ¨å››ä¸ªä¸“ä¸šæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœä¼˜äºPEå’Œå…¶ä»–APIè¾…åŠ©åŸºçº¿æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨APIè®¿é—®èƒ½å¤Ÿæé«˜è´¨é‡è¯„ä¼°èƒ½åŠ›å¹¶å®ç°é«˜è´¨é‡çš„å·®åˆ†éšç§åˆæˆå›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05407">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bee180ef11fbf4e6c64a25c2bd95545.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7220e506ee0c4d5221b783985abc3114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0911b0b538bd684f5542f193301a0f7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AdaReasoner-Adaptive-Reasoning-Enables-More-Flexible-Thinking"><a href="#AdaReasoner-Adaptive-Reasoning-Enables-More-Flexible-Thinking" class="headerlink" title="AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking"></a>AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking</h2><p><strong>Authors:Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang</strong></p>
<p>LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work â€˜well enoughâ€™ across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œå¦‚æ¸©åº¦å’Œæ¨ç†æ­¥éª¤ï¼Œæ¥å¤„ç†ä»ç¬‘è¯ç”Ÿæˆåˆ°æ•°å­¦æ¨ç†ç­‰éœ€è¦å¤æ‚æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æç¤ºæ–¹æ³•é€šå¸¸é‡‡ç”¨é€šç”¨ã€å›ºå®šçš„é…ç½®ï¼Œè¿™äº›é…ç½®åœ¨å„é¡¹ä»»åŠ¡ä¸­â€œè¶³å¤Ÿå¥½â€åœ°å·¥ä½œï¼Œä½†å¾ˆå°‘å®ç°é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AdaReasonerï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹çš„é€šç”¨æ’ä»¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–é€‚åº”éœ€è¦ä¸åŒç±»å‹æ€è€ƒçš„ä»»åŠ¡çš„æ¨ç†é…ç½®ã€‚AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œç»“åˆåˆ†è§£çš„åŠ¨ä½œç©ºé—´å’Œæœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥ï¼Œä»¥åŠé¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–ä»…åœ¨å°‘æ•°å¼•å¯¼ä¸‹çš„æ¨ç†é…ç½®çš„ç­–ç•¥æ¨¡å‹ã€‚AdaReasoneræœ‰ç†è®ºä¿è¯ï¼Œå®éªŒè¯æ˜å…¶æ”¶æ•›é€Ÿåº¦å¿«ã€ç­–ç•¥å·®è·å‘ˆäºšçº¿æ€§ã€‚åœ¨å…­ç§ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šç§æ¨ç†ä»»åŠ¡ä¸Šï¼Œå®ƒå§‹ç»ˆä¼˜äºæ ‡å‡†åŸºå‡†æµ‹è¯•ï¼Œä¿æŒäº†ç¦»ç¾¤åˆ†å¸ƒçš„ç¨³å¥æ€§ï¼Œå¹¶é€šè¿‡å®šåˆ¶æç¤ºåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šäº§ç”Ÿäº†æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17312v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMéœ€è¦æœ‰æ•ˆçš„é…ç½®ï¼Œå¦‚æ¸©åº¦å’Œæ¨ç†æ­¥éª¤ï¼Œä»¥å¤„ç†ä»ç¬‘è¯ç”Ÿæˆåˆ°æ•°å­¦æ¨ç†ç­‰éœ€è¦é«˜çº§æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›çš„ä»»åŠ¡ã€‚ç°æœ‰æç¤ºæ–¹æ³•é€šå¸¸é‡‡ç”¨é€šç”¨ã€å›ºå®šçš„é…ç½®ï¼Œè¿™äº›é…ç½®åœ¨è·¨ä»»åŠ¡æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†å¾ˆå°‘å®ç°é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†AdaReasonerï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä»»ä½•LLMè®¾è®¡çš„LLMæ„ŸçŸ¥æ’ä»¶ï¼Œå¯ä»¥è‡ªåŠ¨é€‚åº”éœ€è¦ä¸åŒç±»å‹æ€è€ƒçš„æ¨ç†ä»»åŠ¡çš„é…ç½®ã€‚AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†å› å­åŒ–çš„åŠ¨ä½œç©ºé—´ã€æœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥ä»¥åŠé¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œä»¥åœ¨æœ‰é™çš„æŒ‡å¯¼ä¸‹ä¼˜åŒ–æ¨ç†é…ç½®çš„ç­–ç•¥æ¨¡å‹ã€‚åœ¨å…­ä¸ªä¸åŒçš„LLMå’Œå¤šç§æ¨ç†ä»»åŠ¡ä¸Šï¼ŒAdaReasonerè¡¨ç°ä¼˜è¶Šäºæ ‡å‡†åŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¿è¯äº†è¾“å‡ºåˆ†å¸ƒçš„é²æ£’æ€§ï¼ŒåŒæ—¶å¯é€šè¿‡é’ˆå¯¹æ€§çš„æç¤ºæé«˜çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å¢ç›Šã€‚å®ƒçš„è¿è¡Œè¿˜åŸºäºç†è®ºä¿éšœçš„å¿«é€Ÿæ”¶æ•›æ€§å’Œç­–ç•¥ä¸€è‡´æ€§ç‰¹ç‚¹ã€‚å…¶ä¸ä»…èƒ½ä¸ç°æœ‰ä»»åŠ¡å¾ˆå¥½ç»“åˆè¿˜å…·æœ‰ä»»åŠ¡åŒ¹é…æœ€ä¼˜åŒ–è¡¨ç°ï¼Œä¸ºæé«˜è¯­è¨€æ¨¡å‹å®é™…åº”ç”¨ä¸­æ•ˆèƒ½é—®é¢˜æä¾›äº†ä¸€ç§æ–°é¢–çµæ´»çš„æ–¹æ¡ˆæ€è·¯ã€‚æ€»ä½“æ¥è¯´æ­¤æ¨¡å‹èµ‹äºˆäº†æ›´å¤šå®é™…åº”ç”¨é¢†åŸŸæ“ä½œçš„ç²¾å‡†åº¦æå‡èƒ½åŠ›å’Œè¾ƒå¼ºçš„ä¸ªæ€§åŒ–æ½œèƒ½ä»¥åŠå•†ä¸šæ½œèƒ½çš„å·¨å¤§å¯æœŸå¾…è¶‹åŠ¿çš„å‘å±•æ•ˆæœåé¦ˆå‡ºè‰²æœ‰æ˜¾è‘—æ”¹å–„å¯èƒ½æ€§é¢„è®¡æå¤§é¢ è¦†ä¼˜åŒ–æ”¹å˜ä¿ƒè¿›ç°ä»£åŒ–é€‚åº”æ€§å¤šè¡Œä¸šç”¨é€”çš„æ€§èƒ½å’ŒåŠŸèƒ½é›†æˆä»¥åŠå®Œå–„å…¶ç»†èŠ‚å®šåˆ¶æ€§çªå‡ºé€‚ç”¨åº¦éå¸¸å¹¿æ³›å¸‚åœºå¹¿é˜”ååº”å¸‚åœºéœ€æ±‚å…·å¤‡é‡å¤§åº”ç”¨ä»·å€¼å’Œåˆ›æ–°çªç ´å…·æœ‰æ·±è¿œçš„ç§¯æå½±å“å’Œåº”ç”¨æ½œåŠ›ä¸å¯ä¼°é‡å€¼å¾—æ·±å…¥ç ”ç©¶æ¨å¹¿ä½¿ç”¨å¹¶åº”ç”¨è‡³å„ä¸ªç›¸å…³é¢†åŸŸä»¥æå‡æ™ºèƒ½åŒ–åº”ç”¨çš„æ•ˆèƒ½ã€‚é€šè¿‡å®é™…åº”ç”¨è¡¨æ˜å…¶èƒ½æ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæœ‰ç€å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œå¸‚åœºæ½œåŠ›ã€‚ç®€è¨€ä¹‹å°±æ˜¯å®ƒèƒ½å¤Ÿæœ‰æ•ˆèµ‹èƒ½AIæ›´èªæ˜åœ°è¿›è¡Œè‡ªä¸»å†³ç­–åŠé«˜æ•ˆè§£å†³é—®é¢˜èµ‹èƒ½ç°ä»£åŒ–è¡Œä¸šèµ‹èƒ½åº”ç”¨æ½œèƒ½ä¸å¯ä¼°é‡å…·å¤‡é•¿è¿œçš„ç¤¾ä¼šä»·å€¼å’Œå¸‚åœºä»·å€¼æ½œåŠ›ã€‚å—åˆ°å¹¿æ³›å…³æ³¨ä¸”å€¼å¾—æ·±å…¥ç ”ç©¶æ¨å¹¿ä½¿ç”¨åŠéƒ¨ç½²ã€‚<br><strong>Key Takeaways</strong></p>
<ol>
<li>LLMséœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é…ç½®ä¼˜åŒ–æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>AdaReasoneræ˜¯ä¸€ä¸ªLLMæ„ŸçŸ¥æ’ä»¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–é€‚åº”ä¸åŒç±»å‹çš„æ¨ç†ä»»åŠ¡é…ç½®ã€‚</li>
<li>AdaReasonerä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è®­ç»ƒï¼Œç»“åˆäº†å› å­åŒ–çš„åŠ¨ä½œç©ºé—´ã€æœ‰é’ˆå¯¹æ€§çš„æ¢ç´¢ç­–ç•¥å’Œé¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>AdaReasoneråœ¨å¤šä¸ªLLMså’Œæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰å¿«é€Ÿæ”¶æ•›æ€§å’Œç­–ç•¥ä¸€è‡´æ€§ç‰¹ç‚¹ã€‚</li>
<li>AdaReasoneræé«˜äº†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>AdaReasonerå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå¸‚åœºæ½œåŠ›ï¼Œèƒ½å¤Ÿæé«˜æ™ºèƒ½åŒ–åº”ç”¨çš„æ•ˆèƒ½å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-632bdb76d6adb5e7baae1a8ae856fb1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8115749b7475c6209d4b4e20de63900.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25f1c2a9bd20e546943ad9e5a3952244.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Detect-Explain-Escalate-Low-Carbon-Dialogue-Breakdown-Management-for-LLM-Powered-Agents"><a href="#Detect-Explain-Escalate-Low-Carbon-Dialogue-Breakdown-Management-for-LLM-Powered-Agents" class="headerlink" title="Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for   LLM-Powered Agents"></a>Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for   LLM-Powered Agents</h2><p><strong>Authors:Abdellah Ghassel, Xianzhi Li, Xiaodan Zhu</strong></p>
<p>While Large Language Models (LLMs) are transforming numerous applications, their susceptibility to conversational breakdowns remains a critical challenge undermining user trust. This paper introduces a â€œDetect, Explain, Escalateâ€ framework to manage dialogue breakdowns in LLM-powered agents, emphasizing low-carbon operation. Our approach integrates two key strategies: (1) We fine-tune a compact 8B-parameter model, augmented with teacher-generated reasoning traces, which serves as an efficient real-time breakdown â€˜detectorâ€™ and â€˜explainerâ€™. This model demonstrates robust classification and calibration on English and Japanese dialogues, and generalizes well to the BETOLD dataset, improving accuracy by 7% over its baseline. (2) We systematically evaluate frontier LLMs using advanced prompting (few-shot, chain-of-thought, analogical reasoning) for high-fidelity breakdown assessment. These are integrated into an â€˜escalationâ€™ architecture where our efficient detector defers to larger models only when necessary, substantially reducing operational costs and energy consumption. Our fine-tuned model and prompting strategies establish new state-of-the-art results on dialogue breakdown detection benchmarks, outperforming specialized classifiers and significantly narrowing the performance gap to larger proprietary models. The proposed monitor-escalate pipeline reduces inference costs by 54%, offering a scalable, efficient, and more interpretable solution for robust conversational AI in high-impact domains. Code and models will be publicly released. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£åœ¨æ”¹å˜è®¸å¤šåº”ç”¨ç¨‹åºï¼Œä½†å®ƒä»¬å®¹æ˜“å—åˆ°å¯¹è¯ä¸­æ–­çš„å½±å“ï¼Œè¿™ä»ç„¶æ˜¯ç ´åç”¨æˆ·ä¿¡ä»»çš„å…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªâ€œæ£€æµ‹ã€è§£é‡Šã€å‡çº§â€æ¡†æ¶ï¼Œç”¨äºç®¡ç†LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†ä¸­çš„å¯¹è¯ä¸­æ–­é—®é¢˜ï¼Œå¼ºè°ƒä½ç¢³æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸¤ç§å…³é”®ç­–ç•¥ï¼š(1)æˆ‘ä»¬å¾®è°ƒäº†ä¸€ä¸ªç´§å‡‘çš„8Bå‚æ•°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é…å¤‡äº†æ•™å¸ˆç”Ÿæˆçš„æ¨ç†è½¨è¿¹ï¼Œå¯ä½œä¸ºé«˜æ•ˆçš„å®æ—¶ä¸­æ–­â€œæ£€æµ‹å™¨â€å’Œâ€œè§£é‡Šå™¨â€ã€‚è¯¥æ¨¡å‹åœ¨è‹±è¯­å’Œæ—¥è¯­å¯¹è¯ä¸­æ˜¾ç¤ºå‡ºç¨³å¥çš„åˆ†ç±»å’Œæ ¡å‡†èƒ½åŠ›ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°BETOLDæ•°æ®é›†ï¼Œæ¯”å…¶åŸºçº¿æé«˜äº†7%çš„å‡†ç¡®åº¦ã€‚ (2) æˆ‘ä»¬ç³»ç»Ÿåœ°ä½¿ç”¨é«˜çº§æç¤ºï¼ˆå°‘é‡ã€æ€è€ƒé“¾ã€ç±»æ¯”æ¨ç†ï¼‰æ¥è¯„ä¼°å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿›è¡Œé«˜ç²¾åº¦çš„ä¸­æ–­è¯„ä¼°ã€‚è¿™äº›æç¤ºè¢«æ•´åˆåˆ°ä¸€ä¸ªâ€œå‡çº§â€æ¶æ„ä¸­ï¼Œæˆ‘ä»¬çš„é«˜æ•ˆæ£€æµ‹å™¨åªåœ¨å¿…è¦æ—¶è°ƒç”¨æ›´å¤§çš„æ¨¡å‹ï¼Œå¤§å¤§é™ä½äº†æ“ä½œæˆæœ¬å’Œèƒ½æºæ¶ˆè€—ã€‚æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹å’Œæç¤ºç­–ç•¥åœ¨å¯¹è¯ä¸­æ–­æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œè¶…è¶Šäº†ä¸“ä¸šåˆ†ç±»å™¨ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†ä¸æ›´å¤§ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚æ‰€æå‡ºçš„ç›‘æ§å‡çº§ç®¡é“é™ä½äº†54%çš„æ¨ç†æˆæœ¬ï¼Œä¸ºé«˜é£é™©é¢†åŸŸæä¾›äº†å¯æ‰©å±•ã€é«˜æ•ˆä¸”æ›´å…·å¯è§£é‡Šæ€§çš„ç¨³å¥å¯¹è¯äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18839v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºâ€œæ£€æµ‹ã€è§£é‡Šã€å‡çº§â€æ¡†æ¶çš„LLMå¯¹è¯å´©æºƒç®¡ç†æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜ç”¨æˆ·ä¿¡ä»»å¹¶é™ä½è¿è¥æˆæœ¬ã€‚é€šè¿‡å¾®è°ƒç´§å‡‘çš„8Bå‚æ•°æ¨¡å‹å¹¶é›†æˆæ•™å¸ˆç”Ÿæˆçš„æ¨ç†è½¨è¿¹ï¼Œå®ç°äº†é«˜æ•ˆçš„å®æ—¶å¯¹è¯å´©æºƒæ£€æµ‹å’Œè§£é‡Šã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç³»ç»Ÿåœ°è¯„ä¼°äº†å‰æ²¿LLMçš„æ€§èƒ½ï¼Œé‡‡ç”¨å…ˆè¿›çš„æç¤ºæ–¹æ³•ï¼ˆå°‘æ ·æœ¬ã€é“¾å¼æ€ç»´ã€ç±»æ¯”æ¨ç†ï¼‰è¿›è¡Œé«˜ä¿çœŸåº¦å´©æºƒè¯„ä¼°ï¼Œå¹¶å°†å®ƒä»¬é›†æˆåˆ°å‡çº§æ¶æ„ä¸­ï¼Œä»…åœ¨å¿…è¦æ—¶è°ƒç”¨å¤§å‹æ¨¡å‹ï¼Œä»è€Œå¤§å¹…é™ä½è¿è¥æˆæœ¬å’Œèƒ½æºæ¶ˆè€—ã€‚è¯¥æ–¹æ³•å’Œç­–ç•¥åœ¨å¯¹è¯å´©æºƒæ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œè¶…è¶Šäº†ä¸“ä¸šåˆ†ç±»å™¨ï¼Œå¹¶æ˜¾è‘—ç¼©å°äº†ä¸å¤§å‹ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹LLMå¯¹è¯å´©æºƒçš„â€œæ£€æµ‹ã€è§£é‡Šã€å‡çº§â€ç®¡ç†æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºç”¨æˆ·ä¿¡ä»»å¹¶å®ç°ä½ç¢³è¿è¥ã€‚</li>
<li>é€šè¿‡å¾®è°ƒç´§å‡‘çš„8Bå‚æ•°æ¨¡å‹ï¼Œå®ç°äº†é«˜æ•ˆçš„å®æ—¶å¯¹è¯å´©æºƒæ£€æµ‹ä¸è§£é‡Šã€‚</li>
<li>é‡‡ç”¨äº†å…ˆè¿›çš„æç¤ºæ–¹æ³•å¯¹å‰æ²¿LLMè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬ã€é“¾å¼æ€ç»´å’Œç±»æ¯”æ¨ç†ã€‚</li>
<li>ä»…åœ¨å¿…è¦æ—¶è°ƒç”¨å¤§å‹æ¨¡å‹ï¼Œé™ä½äº†è¿è¥æˆæœ¬å¹¶å‡å°‘äº†èƒ½æºæ¶ˆè€—ã€‚</li>
<li>åœ¨å¯¹è¯å´©æºƒæ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œè¶…è¶Šäº†ä¸“ä¸šåˆ†ç±»å™¨ã€‚</li>
<li>ç¼©å°äº†ä¸å¤§å‹ä¸“æœ‰æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3561317f6fe009d8d032cec77cbb006f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee3c92f537db55057375f00b9e912fc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb4c70fca478ef32233bd606afc1864a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-Masked-Autoencoders-Also-Listen-to-Birds"><a href="#Can-Masked-Autoencoders-Also-Listen-to-Birds" class="headerlink" title="Can Masked Autoencoders Also Listen to Birds?"></a>Can Masked Autoencoders Also Listen to Birds?</h2><p><strong>Authors:Lukas Rauch, RenÃ© Heinrich, Ilyass Moummad, Alexis Joly, Bernhard Sick, Christoph Scholz</strong></p>
<p>Masked Autoencoders (MAEs) have shown competitive results in audio classification by learning rich semantic representations through an efficient self-supervised reconstruction task. However, general-purpose models fail to generalize well when applied directly to fine-grained audio domains. Specifically, bird-sound classification requires distinguishing subtle inter-species differences and managing high intra-species acoustic variability, thereby revealing the performance limitations of general-domain Audio-MAE models. This work demonstrates that bridging this domain gap requires more than domain-specific pretraining data; adapting the entire training pipeline is crucial. We systematically revisit and adapt the pretraining recipe, fine-tuning methods, and frozen feature utilization to bird sounds using BirdSet, a large-scale bioacoustic dataset comparable to AudioSet. Our resulting Bird-MAE achieves new state-of-the-art results in BirdSetâ€™s multi-label classification benchmark. Additionally, we introduce the parameter-efficient prototypical probing, enhancing the utility of frozen MAE representations and closely approaching fine-tuning performance in low-resource settings. Bird-MAEâ€™s prototypical probes outperform linear probing by up to 37%$_\text{p}$ in MAP and narrow the gap to fine-tuning to approximately 3.3%$_\text{p}$ on average across BirdSet downstream tasks. Bird-MAE also demonstrates robust few-shot capabilities with prototypical probing in our newly established few-shot benchmark on BirdSet, highlighting the potential of tailored self-supervised learning pipelines for fine-grained audio domains. </p>
<blockquote>
<p>åŸºäºMasked Autoencodersï¼ˆMAEsï¼‰åœ¨éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ï¼Œé€šè¿‡é«˜æ•ˆçš„è‡ªç›‘ç£é‡å»ºä»»åŠ¡å­¦ä¹ ä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå½“é€šç”¨æ¨¡å‹ç›´æ¥åº”ç”¨äºç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸæ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›å¾€å¾€ä¸ä½³ã€‚ç‰¹åˆ«æ˜¯é¸Ÿç±»å£°éŸ³åˆ†ç±»éœ€è¦åŒºåˆ†ç‰©ç§é—´çš„ç»†å¾®å·®å¼‚å¹¶åº”å¯¹é«˜ç‰©ç§å†…éƒ¨çš„å£°å­¦å˜åŒ–ï¼Œä»è€Œæ­ç¤ºäº†é€šç”¨é¢†åŸŸAudio-MAEæ¨¡å‹çš„æ€§èƒ½å±€é™æ€§ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç¼©å°è¿™ä¸€é¢†åŸŸå·®è·ä¸ä»…éœ€è¦ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®ï¼›æ•´ä¸ªè®­ç»ƒç®¡é“çš„è°ƒæ•´ä¹Ÿæ˜¯è‡³å…³é‡è¦çš„ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾å¹¶é€‚åº”äº†é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•ä»¥åŠä½¿ç”¨BirdSetï¼ˆä¸€ä¸ªä¸AudioSetç›¸å½“çš„å¤§è§„æ¨¡ç”Ÿç‰©å£°å­¦æ•°æ®é›†ï¼‰çš„é¸Ÿç±»å£°éŸ³çš„å†»ç»“ç‰¹å¾åˆ©ç”¨æ–¹æ³•ã€‚æˆ‘ä»¬å¾—åˆ°çš„Bird-MAEåœ¨BirdSetçš„å¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–° state-of-the-art ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹ï¼Œå¢å¼ºäº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸­æ¥è¿‘å¾®è°ƒæ€§èƒ½ã€‚Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨MAPä¸Šçš„è¡¨ç°ä¼˜äºçº¿æ€§æ¢é’ˆé«˜è¾¾37%ï¼Œå¹¶å°†ä¸å¾®è°ƒä¹‹é—´çš„å·®è·ç¼©å°åˆ°BirdSetä¸‹æ¸¸ä»»åŠ¡å¹³å‡çº¦3.3%ã€‚Bird-MAEè¿˜å±•ç¤ºäº†æˆ‘ä»¬æ–°å»ºç«‹çš„BirdSetå°‘æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­åŸºäºåŸå‹æ¢é’ˆçš„å¼ºå¤§å°‘æ ·æœ¬èƒ½åŠ›ï¼Œçªæ˜¾äº†é’ˆå¯¹ç»†ç²’åº¦éŸ³é¢‘é¢†åŸŸé‡èº«å®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ ç®¡é“çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12880v3">PDF</a> under review @TMLR</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºMasked Autoencodersï¼ˆMAEsï¼‰çš„éŸ³é¢‘åˆ†ç±»å±•ç°å‡ºå¼ºå¤§çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ï¼Œå°¤å…¶åœ¨è‡ªç›‘ç£é‡å»ºä»»åŠ¡ä¸­æ•ˆæœæ˜¾è‘—ã€‚ç„¶è€Œï¼Œå¯¹äºç»†ç²’åº¦éŸ³é¢‘åŸŸï¼Œé€šç”¨æ¨¡å‹çš„è¡¨ç°ä¸å°½å¦‚äººæ„ã€‚é¸Ÿå£°åˆ†ç±»éœ€åŒºåˆ†ä¸åŒç‰©ç§é—´çš„å¾®å¦™å·®å¼‚å¹¶å¤„ç†é«˜ç§å†…å£°å­¦å˜å¼‚ï¼Œæš´éœ²å‡ºé€šç”¨é¢†åŸŸAudio-MAEæ¨¡å‹çš„æ€§èƒ½å±€é™ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç¼©å°è¿™ä¸€é¢†åŸŸå·®è·ä¸ä»…éœ€è¦ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®ï¼Œè¿˜éœ€è¦é€‚åº”æ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚é€šè¿‡ç³»ç»Ÿæ€§åœ°é‡æ–°è€ƒè™‘å’Œé€‚åº”é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•ä»¥åŠå†»ç»“ç‰¹å¾çš„åˆ©ç”¨ï¼Œå¹¶ç»“åˆå¤§è§„æ¨¡ç”Ÿç‰©å£°å­¦æ•°æ®é›†BirdSetï¼ˆä¸AudioSetç›¸å½“ï¼‰ï¼Œæˆ‘ä»¬æ¨å‡ºçš„Bird-MAEåœ¨BirdSetçš„å¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œå¢å¼ºäº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸­æ¥è¿‘å¾®è°ƒæ€§èƒ½ã€‚Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨MAPä¸Šçš„è¡¨ç°æ¯”çº¿æ€§æ¢é’ˆé«˜å‡ºé«˜è¾¾37%ï¼Œå¹¶å°†ä¸å¾®è°ƒä¹‹é—´çš„å·®è·ç¼©å°åˆ°BirdSetä¸‹æ¸¸ä»»åŠ¡å¹³å‡çº¦3.3%ã€‚Bird-MAEåœ¨æ–°çš„å°‘é‡æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸­å±•ç°äº†å¼ºå¤§çš„å°‘æ ·æœ¬èƒ½åŠ›ï¼Œçªæ˜¾äº†é’ˆå¯¹ç»†ç²’åº¦éŸ³é¢‘åŸŸé‡èº«å®šåˆ¶çš„è‡ªç›‘ç£å­¦ä¹ æµç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Masked Autoencoders (MAEs) åœ¨éŸ³é¢‘åˆ†ç±»ä¸­å±•ç°å‡ºå¼ºå¤§çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>é€šç”¨æ¨¡å‹åœ¨ç»†ç²’åº¦éŸ³é¢‘åŸŸï¼ˆå¦‚é¸Ÿå£°åˆ†ç±»ï¼‰ä¸­è¡¨ç°ä¸ä½³ï¼Œéœ€è¦åŒºåˆ†ç‰©ç§é—´çš„å¾®å¦™å·®å¼‚å’Œå¤„ç†é«˜ç§å†…å£°å­¦å˜å¼‚ã€‚</li>
<li>ç¼©å°é¢†åŸŸå·®è·éœ€è¦ä¸ä»…ä»…æ˜¯ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®ï¼Œè¿˜éœ€è¦é€‚åº”æ•´ä¸ªè®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒé…æ–¹ã€å¾®è°ƒæ–¹æ³•å’Œå†»ç»“ç‰¹å¾çš„åˆ©ç”¨ã€‚</li>
<li>Bird-MAEåœ¨BirdSetçš„å¤šæ ‡ç­¾åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>å¼•å…¥å‚æ•°é«˜æ•ˆçš„åŸå‹æ¢æµ‹æŠ€æœ¯ï¼Œå¢å¼ºäº†å†»ç»“MAEè¡¨ç¤ºçš„å®ç”¨æ€§ï¼Œå¹¶æé«˜äº†ä½èµ„æºç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚</li>
<li>Bird-MAEçš„åŸå‹æ¢é’ˆåœ¨MAPä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºçº¿æ€§æ¢é’ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15da3b4cdcac5b10204082bb507e73a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3241ad7918215e514014c801ff5ce4f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83fd6ee9d0f1462de6c3ed1077eefd77.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Feedforward-Few-shot-Species-Range-Estimation"><a href="#Feedforward-Few-shot-Species-Range-Estimation" class="headerlink" title="Feedforward Few-shot Species Range Estimation"></a>Feedforward Few-shot Species Range Estimation</h2><p><strong>Authors:Christian Lange, Max Hamilton, Elijah Cole, Alexander Shepard, Samuel Heinrich, Angela Zhu, Subhransu Maji, Grant Van Horn, Oisin Mac Aodha</strong></p>
<p>Knowing where a particular species can or cannot be found on Earth is crucial for ecological research and conservation efforts. By mapping the spatial ranges of all species, we would obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. However, accurate range estimates are only available for a relatively small proportion of all known species. For the majority of the remaining species, we typically only have a small number of records denoting the spatial locations where they have previously been observed. We outline a new approach for few-shot species range estimation to address the challenge of accurately estimating the range of a species from limited data. During inference, our model takes a set of spatial locations as input, along with optional metadata such as text or an image, and outputs a species encoding that can be used to predict the range of a previously unseen species in a feedforward manner. We evaluate our approach on two challenging benchmarks, where we obtain state-of-the-art range estimation performance, in a fraction of the compute time, compared to recent alternative approaches. </p>
<blockquote>
<p>äº†è§£ç‰¹å®šç‰©ç§åœ¨åœ°çƒä¸Šå¯ä»¥æˆ–ä¸èƒ½è¢«å‘ç°çš„ä½ç½®å¯¹ç”Ÿæ€ç ”ç©¶å’Œä¿æŠ¤å·¥ä½œè‡³å…³é‡è¦ã€‚é€šè¿‡ç»˜åˆ¶æ‰€æœ‰ç‰©ç§çš„ç©ºé—´åˆ†å¸ƒèŒƒå›´å›¾ï¼Œæˆ‘ä»¬å°†æ·±å…¥äº†è§£å…¨çƒç”Ÿç‰©å¤šæ ·æ€§å¦‚ä½•å—åˆ°æ°”å€™å˜åŒ–å’Œæ –æ¯åœ°ä¸§å¤±çš„å½±å“ã€‚ç„¶è€Œï¼Œåªæœ‰ç›¸å¯¹è¾ƒå°‘çš„å·²çŸ¥ç‰©ç§æœ‰å‡†ç¡®çš„èŒƒå›´ä¼°è®¡ã€‚å¯¹äºå¤§å¤šæ•°å‰©ä½™ç‰©ç§ï¼Œæˆ‘ä»¬é€šå¸¸åªæœ‰å°‘é‡è®°å½•è¡¨æ˜å®ƒä»¬ä¹‹å‰è¢«è§‚å¯Ÿåˆ°çš„ç©ºé—´ä½ç½®ã€‚æˆ‘ä»¬æ¦‚è¿°äº†ä¸€ç§æ–°çš„ç”¨äºå°‘é‡ç‰©ç§èŒƒå›´ä¼°è®¡çš„æ–¹æ³•ï¼Œä»¥è§£å†³ä»æœ‰é™æ•°æ®ä¸­å‡†ç¡®ä¼°è®¡ç‰©ç§èŒƒå›´æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»¥ä¸€ç»„ç©ºé—´ä½ç½®ä¸ºè¾“å…¥ï¼Œè¿˜å¯ä»¥åŠ å…¥æ–‡æœ¬æˆ–å›¾åƒç­‰å¯é€‰å…ƒæ•°æ®ï¼Œè¾“å‡ºä¸€ç§ç‰©ç§ç¼–ç ï¼Œè¯¥ç¼–ç å¯ä»¥ç”¨äºä»¥å‰æœªè§è¿‡çš„ç‰©ç§çš„èŒƒå›´é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä¸æœ€è¿‘çš„æ›¿ä»£æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—æ—¶é—´çš„ä¸€å°éƒ¨åˆ†å†…è·å¾—äº†æœ€å…ˆè¿›çš„èŒƒå›´ä¼°è®¡æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.14977v2">PDF</a> Published in the Proceedings of the 42nd International Conference on   Machine Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå°‘é‡æ•°æ®çš„æ–°ç‰©ç§åˆ†å¸ƒèŒƒå›´ä¼°ç®—æ–¹æ³•ã€‚é€šè¿‡è¾“å…¥ç‰©ç§çš„ç©ºé—´ä½ç½®å’Œå¯é€‰çš„å…ƒæ•°æ®ï¼ˆå¦‚æ–‡æœ¬æˆ–å›¾åƒï¼‰ï¼Œæ¨¡å‹å¯ä»¥é¢„æµ‹æœªçŸ¥ç‰©ç§çš„åˆ†å¸ƒèŒƒå›´ã€‚è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”è®¡ç®—æ—¶é—´è¾ƒå…¶ä»–æ–¹æ³•å¤§å¤§ç¼©çŸ­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç§çš„åˆ†å¸ƒèŒƒå›´å¯¹äºç”Ÿæ€ç ”ç©¶å’Œä¿æŠ¤è‡³å…³é‡è¦ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°ç‰©ç§çš„åˆ†å¸ƒèŒƒå›´ä¼°è®¡ä»åŸºäºæœ‰é™çš„æ•°æ®ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå°‘é‡æ•°æ®çš„ç‰©ç§åˆ†å¸ƒèŒƒå›´ä¼°ç®—æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ç‰©ç§çš„ç©ºé—´ä½ç½®å’Œå¯é€‰çš„å…ƒæ•°æ®è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>åœ¨åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•çš„è®¡ç®—æ—¶é—´å¤§å¤§å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.14977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f8ddea3153f65f0ff72c82e25f9b1e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee427df3d05456f892f208eb1d84196e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6851b9d6494f5f8d348e0aaab1ae3941.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-10b0430b6b1ddf4630cd617e2fa3fcc5.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal   Pathology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f32a620e1dcc6c1169b3c854b451e2bd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  PersonaAgent When Large Language Model Agents Meet Personalization at   Test Time
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
