<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-06-10  BecomingLit Relightable Gaussian Avatars with Hybrid Neural Shading">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f0e5b04be193e4ef02149a1e153b0f31.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-10-更新"><a href="#2025-06-10-更新" class="headerlink" title="2025-06-10 更新"></a>2025-06-10 更新</h1><h2 id="BecomingLit-Relightable-Gaussian-Avatars-with-Hybrid-Neural-Shading"><a href="#BecomingLit-Relightable-Gaussian-Avatars-with-Hybrid-Neural-Shading" class="headerlink" title="BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading"></a>BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading</h2><p><strong>Authors:Jonathan Schmidt, Simon Giebenhain, Matthias Niessner</strong></p>
<p>We introduce BecomingLit, a novel method for reconstructing relightable, high-resolution head avatars that can be rendered from novel viewpoints at interactive rates. Therefore, we propose a new low-cost light stage capture setup, tailored specifically towards capturing faces. Using this setup, we collect a novel dataset consisting of diverse multi-view sequences of numerous subjects under varying illumination conditions and facial expressions. By leveraging our new dataset, we introduce a new relightable avatar representation based on 3D Gaussian primitives that we animate with a parametric head model and an expression-dependent dynamics module. We propose a new hybrid neural shading approach, combining a neural diffuse BRDF with an analytical specular term. Our method reconstructs disentangled materials from our dynamic light stage recordings and enables all-frequency relighting of our avatars with both point lights and environment maps. In addition, our avatars can easily be animated and controlled from monocular videos. We validate our approach in extensive experiments on our dataset, where we consistently outperform existing state-of-the-art methods in relighting and reenactment by a significant margin. </p>
<blockquote>
<p>我们介绍了BecomingLit这一新型方法，用于重建可重新照明的、高分辨率的头部化身，可从新颖的观点以交互速率进行渲染。因此，我们提出了一种新的低成本灯光舞台捕获设置，专门用于捕捉面部。使用该设置，我们收集了一个新的数据集，该数据集包含在不同照明条件和面部表情下多个主体的各种多角度序列。通过利用我们的新数据集，我们引入了基于3D高斯原始数据的新的可重新照明的化身表示，我们使用参数化头部模型和表情依赖的动态模块使其动画化。我们提出了一种新的混合神经着色方法，将神经漫反射BRDF与分析镜面术语相结合。我们的方法从动态灯光舞台记录中重建了纠缠材料，并使我们能够使用点光源和环境贴图对所有频率的化身进行重新照明。此外，我们的化身可以轻松地从单目视频进行动画和操控。我们在自己的数据集上进行了广泛的实验验证，我们的方法在重新照明和重新演绎方面始终显著优于现有最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06271v1">PDF</a> Project Page: see <a target="_blank" rel="noopener" href="https://jonathsch.github.io/becominglit/">https://jonathsch.github.io/becominglit/</a> ; YouTube   Video: see <a target="_blank" rel="noopener" href="https://youtu.be/xPyeIqKdszA">https://youtu.be/xPyeIqKdszA</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了BecomingLit这一新型方法，用于重建可重新照明的、高分辨率的头部化身，可从新的视角以互动速率进行渲染。提出一种专门针对面部捕捉的低成本光线捕捉设置，收集包含多样多角度序列的新数据集，涉及不同照明条件和面部表情。基于3D高斯原始数据和参数化头部模型和表情依赖动态模块，引入可重新照明的化身表示。提出一种结合神经漫反射BRDF与分析镜面术语的新型混合神经着色方法。该方法从动态光照舞台记录中重建分离材料，使化身的全方位频率重新照明成为可能，既有点光源也有环境地图。此外，其化身可轻松从单目视频进行动画和控制。在数据集上的广泛实验验证了该方法的有效性，其在重新照明和重演方面显著优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BecomingLit是一种重建高分辨率头部化身的新方法，支持从新视角以互动速率进行渲染。</li>
<li>提出一种低成本的面部捕捉光照设置，专门用于捕捉面部表情和光照变化。</li>
<li>收集了一个新数据集，包含多角度、多表情、多变照明条件下的面部序列。</li>
<li>基于3D高斯原始数据引入可重新照明的化身表示，结合参数化头部模型和表情依赖动态模块。</li>
<li>提出了混合神经着色方法，结合了神经漫反射BRDF与分析镜面术语，实现全方位频率的重新照明。</li>
<li>方法能从动态光照舞台记录中重建分离材料，支持点光源和环境地图的重新照明。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06271">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b47579da8d1ec3be9ca1d5570b6c4fdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2be50d89297b8a7c1dc1aae47833f462.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc88620958dcda4ec13a8987838d0223.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dy3DGS-SLAM-Monocular-3D-Gaussian-Splatting-SLAM-for-Dynamic-Environments"><a href="#Dy3DGS-SLAM-Monocular-3D-Gaussian-Splatting-SLAM-for-Dynamic-Environments" class="headerlink" title="Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic   Environments"></a>Dy3DGS-SLAM: Monocular 3D Gaussian Splatting SLAM for Dynamic   Environments</h2><p><strong>Authors:Mingrui Li, Yiming Zhou, Hongxing Zhou, Xinggang Hu, Florian Roemer, Hongyu Wang, Ahmad Osman</strong></p>
<p>Current Simultaneous Localization and Mapping (SLAM) methods based on Neural Radiance Fields (NeRF) or 3D Gaussian Splatting excel in reconstructing static 3D scenes but struggle with tracking and reconstruction in dynamic environments, such as real-world scenes with moving elements. Existing NeRF-based SLAM approaches addressing dynamic challenges typically rely on RGB-D inputs, with few methods accommodating pure RGB input. To overcome these limitations, we propose Dy3DGS-SLAM, the first 3D Gaussian Splatting (3DGS) SLAM method for dynamic scenes using monocular RGB input. To address dynamic interference, we fuse optical flow masks and depth masks through a probabilistic model to obtain a fused dynamic mask. With only a single network iteration, this can constrain tracking scales and refine rendered geometry. Based on the fused dynamic mask, we designed a novel motion loss to constrain the pose estimation network for tracking. In mapping, we use the rendering loss of dynamic pixels, color, and depth to eliminate transient interference and occlusion caused by dynamic objects. Experimental results demonstrate that Dy3DGS-SLAM achieves state-of-the-art tracking and rendering in dynamic environments, outperforming or matching existing RGB-D methods. </p>
<blockquote>
<p>当前基于神经辐射场（NeRF）或3D高斯喷绘的同步定位与地图构建（SLAM）方法在重建静态3D场景方面表现优异，但在动态环境（例如具有移动元素的真实场景）中的跟踪和重建方面存在困难。现有的解决动态挑战的NeRF基SLAM方法通常依赖于RGB-D输入，只有少数方法接受纯RGB输入。为了克服这些限制，我们提出了Dy3DGS-SLAM，这是使用单目RGB输入对动态场景进行3D高斯喷绘（3DGS）SLAM方法的首创。为了解决动态干扰问题，我们通过概率模型融合光学流动掩膜和深度掩膜，以获得融合动态掩膜。仅通过一次网络迭代，这可以约束跟踪尺度并优化渲染几何。基于融合动态掩膜，我们设计了一种新型运动损失，以约束姿态估计网络进行跟踪。在地图构建方面，我们使用动态像素的渲染损失、颜色和深度，以消除由动态物体引起的短暂干扰和遮挡。实验结果表明，Dy3DGS-SLAM在动态环境中实现了最先进的跟踪和渲染，优于或匹配现有的RGB-D方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05965v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于神经辐射场（NeRF）或三维高斯喷绘的当前SLAM（Simultaneous Localization and Mapping，即时定位与地图构建）方法在重建静态三维场景方面表现出色，但在处理动态环境如具有移动元素的真实场景中则存在困难。为了克服这些局限性，我们提出了Dy3DGS-SLAM，这是首个使用单目RGB输入的针对动态场景的3D高斯喷绘SLAM方法。通过融合光学流掩膜和深度掩膜生成融合动态掩膜，只需一次网络迭代即可约束跟踪尺度并优化渲染几何。基于融合动态掩膜，我们设计了一种新的运动损失来约束姿态估计网络的跟踪。在映射方面，我们使用动态像素的渲染损失、颜色和深度来消除动态物体引起的瞬态干扰和遮挡。实验结果表明，Dy3DGS-SLAM在动态环境下实现了先进的跟踪和渲染效果，优于或与现有RGB-D方法相匹配。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前基于NeRF或3DGS的SLAM方法在静态场景重建方面表现优秀，但在动态环境中跟踪和重建遇到困难。</li>
<li>Dy3DGS-SLAM是首个针对动态场景的3DGS SLAM方法，使用单目RGB输入。</li>
<li>通过融合光学流掩膜和深度掩膜生成融合动态掩膜，用于约束跟踪尺度和优化渲染几何。</li>
<li>引入新的运动损失，基于融合动态掩膜来约束姿态估计网络的跟踪。</li>
<li>在映射过程中使用动态像素的渲染损失、颜色和深度，以消除动态物体引起的干扰和遮挡。</li>
<li>Dy3DGS-SLAM在动态环境下实现了先进的跟踪和渲染效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a07884fabd415f410417e2809f166e36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df7a6dd734785e62bc439d6d889be607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccaccd830834fb587b42806a9c78396e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39e0020237ecd537a6b8ce45f6b2a6c3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SurGSplat-Progressive-Geometry-Constrained-Gaussian-Splatting-for-Surgical-Scene-Reconstruction"><a href="#SurGSplat-Progressive-Geometry-Constrained-Gaussian-Splatting-for-Surgical-Scene-Reconstruction" class="headerlink" title="SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for   Surgical Scene Reconstruction"></a>SurGSplat: Progressive Geometry-Constrained Gaussian Splatting for   Surgical Scene Reconstruction</h2><p><strong>Authors:Yuchao Zheng, Jianing Zhang, Guochen Ning, Hongen Liao</strong></p>
<p>Intraoperative navigation relies heavily on precise 3D reconstruction to ensure accuracy and safety during surgical procedures. However, endoscopic scenarios present unique challenges, including sparse features and inconsistent lighting, which render many existing Structure-from-Motion (SfM)-based methods inadequate and prone to reconstruction failure. To mitigate these constraints, we propose SurGSplat, a novel paradigm designed to progressively refine 3D Gaussian Splatting (3DGS) through the integration of geometric constraints. By enabling the detailed reconstruction of vascular structures and other critical features, SurGSplat provides surgeons with enhanced visual clarity, facilitating precise intraoperative decision-making. Experimental evaluations demonstrate that SurGSplat achieves superior performance in both novel view synthesis (NVS) and pose estimation accuracy, establishing it as a high-fidelity and efficient solution for surgical scene reconstruction. More information and results can be found on the page <a target="_blank" rel="noopener" href="https://surgsplat.github.io/">https://surgsplat.github.io/</a>. </p>
<blockquote>
<p>术中导航严重依赖于精确的3D重建，以确保手术过程中的准确性和安全性。然而，内镜场景存在独特的挑战，包括特征稀疏和照明不一致，这使得许多现有的基于运动结构（SfM）的方法不足以应对，并容易出现重建失败。为了缓解这些限制，我们提出了SurGSplat，这是一种通过结合几何约束来逐步优化3D高斯平铺（3DGS）的新型范式。通过实现血管结构和其他关键特征的详细重建，SurGSplat为外科医生提供了增强的视觉清晰度，促进了精确的术中决策。实验评估表明，SurGSplat在新型视图合成（NVS）和姿态估计精度方面实现了卓越的性能，确立了其在手术场景重建中的高保真度和高效解决方案地位。更多信息和结果可在<a target="_blank" rel="noopener" href="https://surgsplat.github.io/%E9%A1%B5%E9%9D%A2%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://surgsplat.github.io/页面上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05935v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了手术过程中的导航依赖于精确的3D重建以确保手术准确性和安全性。然而，内窥镜场景存在特征稀疏和光照不一致等挑战，使得现有的基于SfM的方法难以满足需求并容易出现重建失败。为解决这些问题，提出了SurGSplat，一种通过整合几何约束逐步优化3D高斯描绘（3DGS）的新型范式。它能详细重建血管结构和其他关键特征，为外科医生提供增强的视觉清晰度，促进手术过程中的精确决策。实验评估表明，SurGSplat在新型视图合成和姿态估计准确性方面表现优越，成为手术场景重建的高保真度和高效率解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>手术导航依赖精确的3D重建确保手术准确性和安全性。</li>
<li>内窥镜场景存在特征稀疏和光照不一致的挑战。</li>
<li>现有基于SfM的方法难以满足需求，容易出现重建失败。</li>
<li>SurGSplat是一种新型范式，通过整合几何约束逐步优化3DGS。</li>
<li>SurGSplat能详细重建血管结构和其他关键特征，提供增强的视觉清晰度。</li>
<li>SurGSplat在新型视图合成和姿态估计准确性方面表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05935">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-36da06bc83cd629a1619c6dfc245eeac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43986d27ad64335b09140f6a3bfb1919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7953ee6ce31241eb82cb9d6d072da1d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0394238a047e7601da000caec7997d70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b028264c3dfcc29335de08285f5f899f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VoxelSplat-Dynamic-Gaussian-Splatting-as-an-Effective-Loss-for-Occupancy-and-Flow-Prediction"><a href="#VoxelSplat-Dynamic-Gaussian-Splatting-as-an-Effective-Loss-for-Occupancy-and-Flow-Prediction" class="headerlink" title="VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for   Occupancy and Flow Prediction"></a>VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for   Occupancy and Flow Prediction</h2><p><strong>Authors:Ziyue Zhu, Shenlong Wang, Jin Xie, Jiang-jiang Liu, Jingdong Wang, Jian Yang</strong></p>
<p>Recent advancements in camera-based occupancy prediction have focused on the simultaneous prediction of 3D semantics and scene flow, a task that presents significant challenges due to specific difficulties, e.g., occlusions and unbalanced dynamic environments. In this paper, we analyze these challenges and their underlying causes. To address them, we propose a novel regularization framework called VoxelSplat. This framework leverages recent developments in 3D Gaussian Splatting to enhance model performance in two key ways: (i) Enhanced Semantics Supervision through 2D Projection: During training, our method decodes sparse semantic 3D Gaussians from 3D representations and projects them onto the 2D camera view. This provides additional supervision signals in the camera-visible space, allowing 2D labels to improve the learning of 3D semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene flow to model the motion of Gaussians, and is thus able to learn the scene flow of moving objects in a self-supervised manner using the labels of adjacent frames. Our method can be seamlessly integrated into various existing occupancy models, enhancing performance without increasing inference time. Extensive experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat in improving the accuracy of both semantic occupancy and scene flow estimation. The project page and codes are available at <a target="_blank" rel="noopener" href="https://zzy816.github.io/VoxelSplat-Demo/">https://zzy816.github.io/VoxelSplat-Demo/</a>. </p>
<blockquote>
<p>近期基于相机的占用预测进展主要集中在3D语义和场景流的同时预测上，这一任务由于特定的困难（例如遮挡和不平衡的动态环境）而具有重大挑战。在本文中，我们分析了这些挑战及其根本原因。为了应对这些挑战，我们提出了一种名为VoxelSplat的新型正则化框架。该框架利用最新的3D高斯模糊技术，通过以下两种方式提高模型性能：（i）通过2D投影增强语义监督：在训练过程中，我们的方法从3D表示中解码稀疏语义3D高斯并将其投影到2D相机视图上。这为相机可见空间提供了额外的监督信号，允许2D标签改进3D语义的学习。（ii）场景流学习：我们的框架使用预测的场运动对高斯值进行建模，因此能够使用相邻帧的标签以自监督的方式学习移动对象的场景流。我们的方法可以无缝集成到各种现有的占用模型中，提高性能而不会增加推理时间。在基准数据集上的大量实验证明了VoxelSplat在提高语义占用和场景流估计的准确性方面的有效性。项目页面和代码可在<a target="_blank" rel="noopener" href="https://zzy816.github.io/VoxelSplat-Demo/%E6%89%BE%E5%88%B0%E3%80%82">https://zzy816.github.io/VoxelSplat-Demo/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05563v1">PDF</a> Accepted by CVPR 2025 Project Page:   <a target="_blank" rel="noopener" href="https://zzy816.github.io/VoxelSplat-Demo/">https://zzy816.github.io/VoxelSplat-Demo/</a></p>
<p><strong>Summary</strong></p>
<p>本文分析了基于摄像头的占用预测面临的挑战，如遮挡和不平衡的动态环境等。为解决这些问题，提出了一种名为VoxelSplat的新型正则化框架。该框架利用最新的三维高斯延展技术，通过以下两种方式提高模型性能：一是通过二维投影增强语义监督；二是场景流学习。VoxelSplat方法可无缝集成到各种现有的占用模型中，提高性能，且不会增加推理时间。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>论文分析了基于摄像头的占用预测中的挑战，如遮挡和不平衡的动态环境。</li>
<li>提出了一种新型正则化框架VoxelSplat，用于解决这些挑战。</li>
<li>VoxelSplat利用最新的三维高斯延展技术，通过二维投影增强语义监督，提供额外的监督信号，改善三维语义学习。</li>
<li>VoxelSplat使用预测的场景流来模拟高斯运动，从而以自监督的方式学习移动物体的场景流。</li>
<li>VoxelSplat可集成到各种现有的占用模型中，提高语义占用和场景流估计的准确性，且不影响推理时间。</li>
<li>论文在基准数据集上进行了大量实验，证明了VoxelSplat的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05563">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8eb9724793360aec978a4423e9fb029a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4752a29098602c1abf80d953f8963610.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fa0ed46eec65d5b9cb96d636353a252.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e989288ebfc478212b6299e392582125.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ODE-GS-Latent-ODEs-for-Dynamic-Scene-Extrapolation-with-3D-Gaussian-Splatting"><a href="#ODE-GS-Latent-ODEs-for-Dynamic-Scene-Extrapolation-with-3D-Gaussian-Splatting" class="headerlink" title="ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian   Splatting"></a>ODE-GS: Latent ODEs for Dynamic Scene Extrapolation with 3D Gaussian   Splatting</h2><p><strong>Authors:Daniel Wang, Patrick Rim, Tian Tian, Alex Wong, Ganesh Sundaramoorthi</strong></p>
<p>We present ODE-GS, a novel method that unifies 3D Gaussian Splatting with latent neural ordinary differential equations (ODEs) to forecast dynamic 3D scenes far beyond the time span seen during training. Existing neural rendering systems - whether NeRF- or 3DGS-based - embed time directly in a deformation network and therefore excel at interpolation but collapse when asked to predict the future, where timestamps are strictly out-of-distribution. ODE-GS eliminates this dependency: after learning a high-fidelity, time-conditioned deformation model for the training window, we freeze it and train a Transformer encoder that summarizes past Gaussian trajectories into a latent state whose continuous evolution is governed by a neural ODE. Numerical integration of this latent flow yields smooth, physically plausible Gaussian trajectories that can be queried at any future instant and rendered in real time. Coupled with a variational objective and a lightweight second-derivative regularizer, ODE-GS attains state-of-the-art extrapolation on D-NeRF and NVFI benchmarks, improving PSNR by up to 10 dB and halving perceptual error (LPIPS) relative to the strongest baselines. Our results demonstrate that continuous-time latent dynamics are a powerful, practical route to photorealistic prediction of complex 3D scenes. </p>
<blockquote>
<p>我们提出了ODE-GS这一新方法，它将三维高斯喷溅技术与潜在神经常微分方程（ODEs）相结合，以预测训练期间时间跨度之外的动态三维场景。现有的神经渲染系统——无论是基于NeRF还是3DGS——都将时间直接嵌入变形网络中，因此在插值方面表现出色，但在要求预测未来时却会崩溃，未来的时间戳完全是超出范围分布的。ODE-GS消除了这种依赖：在针对训练窗口学习高保真、时间条件变形模型后，我们将其冻结，并训练一个Transformer编码器，该编码器对过去的高斯轨迹进行总结，形成一个潜在状态，其连续演化由神经ODE控制。该潜在流的数值积分会产生平滑且符合物理规律的高斯轨迹，可以在任何未来时刻进行查询并实时呈现。结合变分目标和轻量级二阶导数正则化器，ODE-GS在D-NeRF和NVFI基准测试上实现了最先进的预测能力，相较于最强的基线模型，PSNR提高了高达10分贝，感知误差（LPIPS）减半。我们的结果证明了连续时间潜在动力学是预测复杂三维场景的光照现实性的强大实用途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05480v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ODE-GS是一种结合3D高斯喷溅和潜在神经常微分方程（ODEs）的新方法，用于预测训练时间范围之外的动态3D场景。该方法消除了对时间戳的依赖，通过学习高保真、时间条件变形模型，然后冻结模型并训练概括过去高斯轨迹的Transformer编码器，该编码器的连续演化由神经ODE控制。数值积分此潜在流可产生流畅、物理上可行的轨迹，可实时查询并渲染任何未来时刻的场景。该方法在D-NeRF和NVFI基准测试中实现了最佳的外推性能，与最强基线相比，峰值信噪比（PSNR）提高了高达10分贝，感知误差（LPIPS）减半。结果表明，连续时间潜在动力学是预测复杂3D场景的光学真实性的强大实用途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ODE-GS结合了3D高斯喷溅和潜在神经常微分方程（ODEs），用于预测未来动态3D场景。</li>
<li>该方法通过训练一个时间条件变形模型并冻结它，消除了对时间戳的依赖。</li>
<li>ODE-GS使用Transformer编码器来概括过去的高斯轨迹，并利用神经ODE控制其连续演化。</li>
<li>通过数值积分潜在流生成未来的高斯轨迹，可实时查询和渲染。</li>
<li>在D-NeRF和NVFI基准测试中，ODE-GS实现了最佳的外推性能。</li>
<li>与其他方法相比，ODE-GS在峰值信噪比（PSNR）上提高了高达10分贝，在感知误差（LPIPS）上减半。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05480">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ea8dcde250a0fefd863fbc5457ebbd85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7654b98ad08a218f0a605384e6623eb9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Gen4D-Synthesizing-Humans-and-Scenes-in-the-Wild"><a href="#Gen4D-Synthesizing-Humans-and-Scenes-in-the-Wild" class="headerlink" title="Gen4D: Synthesizing Humans and Scenes in the Wild"></a>Gen4D: Synthesizing Humans and Scenes in the Wild</h2><p><strong>Authors:Jerrin Bright, Zhibo Wang, Yuhao Chen, Sirisha Rambhatla, John Zelek, David Clausi</strong></p>
<p>Lack of input data for in-the-wild activities often results in low performance across various computer vision tasks. This challenge is particularly pronounced in uncommon human-centric domains like sports, where real-world data collection is complex and impractical. While synthetic datasets offer a promising alternative, existing approaches typically suffer from limited diversity in human appearance, motion, and scene composition due to their reliance on rigid asset libraries and hand-crafted rendering pipelines. To address this, we introduce Gen4D, a fully automated pipeline for generating diverse and photorealistic 4D human animations. Gen4D integrates expert-driven motion encoding, prompt-guided avatar generation using diffusion-based Gaussian splatting, and human-aware background synthesis to produce highly varied and lifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale synthetic dataset spanning three sports: baseball, icehockey, and soccer. Together, Gen4D and SportPAL provide a scalable foundation for constructing synthetic datasets tailored to in-the-wild human-centric vision tasks, with no need for manual 3D modeling or scene design. </p>
<blockquote>
<p>缺乏野外活动的输入数据通常会导致各种计算机视觉任务性能下降。这一挑战在以人为中心的不常见领域（如体育）中尤为突出，因为现实世界的数据收集复杂且不切实际。虽然合成数据集提供了一种有前景的替代方案，但现有方法通常由于依赖于僵化的资产库和手工制作的渲染管道，而遭受人类外观、动作和场景组合多样性有限的困扰。为了解决这一问题，我们引入了Gen4D，这是一个全自动的生成多样化且逼真的4D人类动画管道。Gen4D集成了专家驱动的运动编码、基于扩散的高斯拼贴引导的人物形象生成以及人类感知的背景合成，以产生高度多样化和逼真的人类序列。基于Gen4D，我们推出了SportPAL，这是一个跨越三项运动的大规模合成数据集：棒球、冰球和足球。Gen4D和SportPAL一起，为针对野外以人为中心的计算机视觉任务构建合成数据集提供了可扩展的基础，无需进行手动3D建模或场景设计。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05397v1">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR) Workshops</p>
<p><strong>Summary</strong></p>
<p>该文本针对计算机视觉任务在现实世界数据收集上的挑战，特别是在运动等以人为中心的不常见领域的数据收集困难问题，提出了一种全自动化的生成多样且逼真的四维人类动画管道Gen4D。该管道结合了专家驱动的运动编码、基于扩散的高斯喷溅法引导的角色生成以及人类感知背景合成技术，以生成高度多样化和逼真的人类序列。基于Gen4D，推出大型合成数据集SportPAL，涵盖棒球、冰球和足球三项运动。这两项成果为解决以人类为中心的现实世界视觉任务提供了可扩充的合成数据集基础，无需手动进行三维建模或场景设计。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算机视觉任务在收集现实世界中以人为中心的数据时面临挑战，特别是在不常见的领域如体育中。</li>
<li>合成数据集提供了一个有前景的替代方案，但现有方法通常受限于人类外观、动作和场景组成的多样性。</li>
<li>Gen4D是一个全自动化的管道，用于生成多样且逼真的四维人类动画，解决了现有方法的局限性。</li>
<li>Gen4D结合了专家驱动的运动编码、基于扩散的高斯喷溅法引导的角色生成及人类感知背景合成技术。</li>
<li>基于Gen4D推出的SportPAL是一个大型合成数据集，涵盖棒球、冰球和足球等多项运动。</li>
<li>Gen4D和SportPAL为以人类为中心的现实世界视觉任务提供了合成数据集的基础，无需手动进行三维建模或场景设计。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05397">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1949ab6a5c4c86c4c656aeddea03d855.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e202aef44ded977d300ada582ebd6d63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfecab83a55dc256cea16becf8ff44a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0e5b04be193e4ef02149a1e153b0f31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-346d1176349fb655c2886b00f1e25317.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FreeTimeGS-Free-Gaussian-Primitives-at-Anytime-and-Anywhere-for-Dynamic-Scene-Reconstruction"><a href="#FreeTimeGS-Free-Gaussian-Primitives-at-Anytime-and-Anywhere-for-Dynamic-Scene-Reconstruction" class="headerlink" title="FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic   Scene Reconstruction"></a>FreeTimeGS: Free Gaussian Primitives at Anytime and Anywhere for Dynamic   Scene Reconstruction</h2><p><strong>Authors:Yifan Wang, Peishan Yang, Zhen Xu, Jiaming Sun, Zhanhua Zhang, Yong Chen, Hujun Bao, Sida Peng, Xiaowei Zhou</strong></p>
<p>This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin. Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/freetimegs/">https://zju3dv.github.io/freetimegs/</a> . </p>
<blockquote>
<p>本文旨在解决重建具有复杂运动的动态三维场景的挑战。最近的一些作品在规范空间中定义了三维高斯基本元素，并使用变形场将规范基本元素映射到观测空间，实现了实时动态视图合成。然而，由于优化变形场的困难，这些方法在处理具有复杂运动的场景时往往表现不佳。为了解决这个问题，我们提出了FreeTimeGS，这是一种新型的四维表示方法，允许高斯基本元素出现在任意的时间和位置。与规范高斯基本元素相比，我们的表示方法具有更强的灵活性，从而提高了对动态三维场景的建模能力。此外，我们为每个高斯基本元素赋予一个运动函数，使其能够在时间推移下移动到邻近区域，从而减少了时间冗余。在几个数据集上的实验结果表明，我们的方法的渲染质量大大优于最近的方法。项目页面：<a target="_blank" rel="noopener" href="https://zju3dv.github.io/freetimegs/%E3%80%82">https://zju3dv.github.io/freetimegs/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05348v2">PDF</a> CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://zju3dv.github.io/freetimegs/">https://zju3dv.github.io/freetimegs/</a></p>
<p><strong>Summary</strong><br>本文提出一种基于四维表示的动态三维场景重建方法，称为FreeTimeGS。该方法通过引入灵活的Gaussian原始表示，解决了复杂动态场景重建中的难题。新方法不仅具有强大的建模能力，还能通过赋予每个Gaussian原始运动功能，减少时间冗余，从而提高渲染质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>本文介绍了动态三维场景重建的挑战，尤其是处理复杂运动场景的问题。</li>
<li>现有方法通过使用变形场映射规范空间中的基本体素到观测空间来实现实时动态视图合成，但难以处理复杂运动场景的优化问题。</li>
<li>提出了一种新的四维表示方法FreeTimeGS，允许Gaussian原始在任意时间和位置出现，具有强大的灵活性，能够更有效地建模动态三维场景。</li>
<li>新方法通过赋予每个Gaussian原始运动功能，使其能够在时间范围内移动到邻近区域，从而减少时间冗余，提高渲染质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05348">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf355607a7e1a559de854a06cd5cd6f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4308e60064a837fe9a9dcd684e951842.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2dd957d4ee3c83943eddcabebff26031.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa77851a2ff61965a7c6c498bc3e9b47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7edd1b0fe3f8e5161baa284c05d7bddb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TT-Occ-Test-Time-Compute-for-Self-Supervised-Occupancy-via-Spatio-Temporal-Gaussian-Splatting"><a href="#TT-Occ-Test-Time-Compute-for-Self-Supervised-Occupancy-via-Spatio-Temporal-Gaussian-Splatting" class="headerlink" title="TT-Occ: Test-Time Compute for Self-Supervised Occupancy via   Spatio-Temporal Gaussian Splatting"></a>TT-Occ: Test-Time Compute for Self-Supervised Occupancy via   Spatio-Temporal Gaussian Splatting</h2><p><strong>Authors:Fengyi Zhang, Huitong Yang, Zheng Zhang, Zi Huang, Yadan Luo</strong></p>
<p>Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense occupancy decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and once trained, such models struggle to adapt to varying voxel resolutions or novel object categories without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-Occ. Our method incrementally constructs, optimizes and voxelizes time-aware 3D Gaussians from raw sensor streams by integrating vision foundation models (VLMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at arbitrary user-specified resolutions, while the generalization ability of VLMs enables accurate perception and open-vocabulary recognition, without any network training or fine-tuning. Specifically, TT-Occ operates in a lift-track-voxelize symphony: We first lift the geometry and semantics of surrounding-view extracted from VLMs to instantiate Gaussians at 3D space; Next, we track dynamic Gaussians while accumulating static ones to complete the scene and enforce temporal consistency; Finally, we voxelize the optimized Gaussians to generate occupancy prediction. Optionally, inherent noise in VLM predictions and tracking is mitigated by periodically smoothing neighboring Gaussians during optimization. To validate the generality and effectiveness of our framework, we offer two variants: one LiDAR-based and one vision-centric, and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying voxel resolutions. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/Xian-Bei/TT-Occ">https://github.com/Xian-Bei/TT-Occ</a>. </p>
<blockquote>
<p>自监督的3D占用预测为理解复杂的驾驶场景提供了一个很有前景的解决方案，而无需昂贵的3D注释。然而，训练密集的占用解码器来捕捉精细的几何和语义可能需要数百个GPU小时，而且一旦训练完成，这些模型在适应不同的体素分辨率或新的对象类别时，如果没有大量的重新训练，会面临困难。为了克服这些限制，我们提出了一个实用且灵活的测试时间占用预测框架，称为TT-Occ。我们的方法通过整合视觉基础模型（VLMs）从原始传感器流中增量构建、优化和体素化时间感知的3D高斯，从而实现灵活的预测。3D高斯图的灵活性允许以任意用户指定的分辨率进行体素化，而VLMs的泛化能力则可实现无需任何网络训练或微调即可进行准确的感知和开放词汇识别。具体来说，TT-Occ在操作中可以看作是一场升降-跟踪-体素化的协奏曲：我们首先将从VLMs中提取的周围视图的几何和语义提升到三维空间以实例化高斯；接下来，我们跟踪动态高斯的同时积累静态高斯来完成场景并强制执行时间一致性；最后，我们对优化后的高斯进行体素化以生成占用预测。可选地，我们可以通过在优化过程中定期平滑相邻的高斯来缓解VLM预测和跟踪中的固有噪声。为了验证我们框架的通用性和有效性，我们提供了两个版本：一个基于激光雷达，一个以视觉为中心，并在具有不同体素分辨率的Occ3D和nuCraft基准上进行了大量实验。代码将在<a target="_blank" rel="noopener" href="https://github.com/Xian-Bei/TT-Occ">https://github.com/Xian-Bei/TT-Occ</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08485v2">PDF</a> </p>
<p><strong>Summary</strong><br>     基于无标注数据的自监督3D占用预测为理解复杂的驾驶场景提供了有前景的解决方案。然而，训练密集的占用解码器以捕捉精细的几何和语义信息可能需要数百小时的GPU时间，并且一旦训练完成，这些模型在适应不同的体素分辨率或新的对象类别时，需要重新训练。为了克服这些限制，我们提出了一个实用且灵活的测试时间占用预测框架TT-Occ。该方法通过整合视觉基础模型（VLMs），从原始传感器流中增量构建、优化和体素化时间感知的3D高斯分布。TT-Occ操作在提升、追踪、体素化的协同中：首先，我们从VLMs中提取的环绕视图几何和语义信息，在三维空间中实例化高斯分布；然后，在累积静态高斯的同时跟踪动态高斯以完成场景并强制执行时间一致性；最后，对优化后的高斯进行体素化以生成占用预测。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督3D占用预测是理解复杂驾驶场景的一种有前途的解决方案，无需昂贵的3D注释。</li>
<li>训练密集的占用解码器捕捉精细几何和语义信息成本高昂且难以适应不同体素分辨率或新对象类别。</li>
<li>提出了一种新的测试时间占用预测框架TT-Occ，结合视觉基础模型（VLMs），增量构建和优化时间感知的3D高斯分布。</li>
<li>TT-Occ允许任意用户指定的体素分辨率，并具有对未知对象的准确感知能力。</li>
<li>该方法包括将几何和语义信息从环绕视图提升到三维空间的高斯分布实例化，跟踪动态和静态高斯，以及对优化后的高斯进行体素化以生成占用预测。</li>
<li>通过平滑相邻高斯来减轻VLM预测和跟踪中的固有噪声。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08485">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6085882fe73b9cb4fcfb21764eb3d24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6685751e842adfb6886061845476dc56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a5c205841e602931ebf1baa252e7f12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b8836d2c08121a0dd2296d5845f8123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-115cb016515d9916d1a6fe26af110597.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3882ab7f62c5a0c00d382b681b2933a6.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-06-10  Dy3DGS-SLAM Monocular 3D Gaussian Splatting SLAM for Dynamic   Environments
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fc88620958dcda4ec13a8987838d0223.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-06-10  BecomingLit Relightable Gaussian Avatars with Hybrid Neural Shading
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
