<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Integrating Complexity and Biological Realism High-Performance Spiking   Neural Networks for Breast Cancer Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-16f5c5f351a9a28f3ab2c432f175292a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    57 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-10-æ›´æ–°"><a href="#2025-06-10-æ›´æ–°" class="headerlink" title="2025-06-10 æ›´æ–°"></a>2025-06-10 æ›´æ–°</h1><h2 id="Integrating-Complexity-and-Biological-Realism-High-Performance-Spiking-Neural-Networks-for-Breast-Cancer-Detection"><a href="#Integrating-Complexity-and-Biological-Realism-High-Performance-Spiking-Neural-Networks-for-Breast-Cancer-Detection" class="headerlink" title="Integrating Complexity and Biological Realism: High-Performance Spiking   Neural Networks for Breast Cancer Detection"></a>Integrating Complexity and Biological Realism: High-Performance Spiking   Neural Networks for Breast Cancer Detection</h2><p><strong>Authors:Zofia Rudnicka, Januszcz Szczepanski, Agnieszka Pregowska</strong></p>
<p>Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding of spatial and temporal features, making them suitable for dynamic time-dependent data processing. Despite their biological relevance, SNNs have seen limited application in medical image recognition due to difficulties in matching the performance of conventional deep learning models. To address this, we propose a novel breast cancer classification approach that combines SNNs with Lempel-Ziv Complexity (LZC) a computationally efficient measure of sequence complexity. LZC enhances the interpretability and accuracy of spike-based models by capturing structural patterns in neural activity. Our study explores both biophysical Leaky Integrate-and-Fire (LIF) and probabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised, and hybrid learning regimes. Experiments were conducted on the Breast Cancer Wisconsin dataset using numerical features derived from medical imaging. LB-based models consistently exceeded 90.00% accuracy, while LIF-based models reached over 85.00%. The highest accuracy of 98.25% was achieved using an ANN-to-SNN conversion method applied to both neuron models comparable to traditional deep learning with back-propagation, but at up to 100 times lower computational cost. This hybrid approach merges deep learning performance with the efficiency and plausibility of SNNs, yielding top results at lower computational cost. We hypothesize that the synergy between temporal-coding, spike-sparsity, and LZC-driven complexity analysis enables more-efficient feature extraction. Our findings demonstrate that SNNs combined with LZC offer promising, biologically plausible alternative to conventional neural networks in medical diagnostics, particularly for resource-constrained or real-time systems. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„äº‹ä»¶é©±åŠ¨ç‰¹æ€§èƒ½å¤Ÿå®ç°ç©ºé—´å’Œæ—¶é—´ç‰¹å¾çš„æœ‰æ•ˆç¼–ç ï¼Œä½¿å…¶æˆä¸ºåŠ¨æ€æ—¶é—´ä¾èµ–æ•°æ®å¤„ç†çš„ç†æƒ³é€‰æ‹©ã€‚å°½ç®¡å®ƒä»¬åœ¨ç”Ÿç‰©å­¦ä¸Šå…·æœ‰ç›¸å…³æ€§ï¼Œä½†ç”±äºéš¾ä»¥åŒ¹é…ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼ŒSNNåœ¨åŒ»å­¦å›¾åƒè¯†åˆ«æ–¹é¢çš„åº”ç”¨ä»ç„¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆSNNå’ŒLempel-Zivå¤æ‚åº¦ï¼ˆLZCï¼‰çš„ä¹³è…ºç™Œåˆ†ç±»æ–°æ–¹æ³•ã€‚LZCæ˜¯ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„åºåˆ—å¤æ‚åº¦åº¦é‡æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ•æ‰ç¥ç»æ´»åŠ¨ä¸­çš„ç»“æ„æ¨¡å¼æ¥æé«˜åŸºäºè„‰å†²æ¨¡å‹çš„è§£é‡Šèƒ½åŠ›å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¢ç´¢äº†åœ¨ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œæ··åˆå­¦ä¹ ä½“åˆ¶ä¸‹ï¼Œç”Ÿç‰©ç‰©ç†æ³„æ¼ç§¯åˆ†ä¸å‘å°„ï¼ˆLIFï¼‰å’Œæ¦‚ç‡æ€§Levy-Baxterï¼ˆLBï¼‰ç¥ç»å…ƒæ¨¡å‹çš„åº”ç”¨ã€‚å®éªŒé‡‡ç”¨ä¹³è…ºç™Œå¨æ–¯åº·æ˜Ÿæ•°æ®é›†è¿›è¡Œï¼Œæ•°æ®ç‰¹å¾æ¥æºäºåŒ»å­¦æˆåƒã€‚åŸºäºLBçš„æ¨¡å‹å‡†ç¡®ç‡æŒç»­è¶…è¿‡90.00%ï¼Œè€ŒåŸºäºLIFçš„æ¨¡å‹å‡†ç¡®ç‡è¶…è¿‡85.00%ã€‚é€šè¿‡ä½¿ç”¨åº”ç”¨äºä¸¤ç§ç¥ç»å…ƒæ¨¡å‹çš„ANNåˆ°SNNè½¬æ¢æ–¹æ³•ï¼Œè¾¾åˆ°æœ€é«˜çš„98.25%å‡†ç¡®ç‡ï¼Œä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ ä¸­çš„åå‘ä¼ æ’­ç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬é™ä½äº†é«˜è¾¾100å€ã€‚è¿™ç§æ··åˆæ–¹æ³•èåˆäº†æ·±åº¦å­¦ä¹ çš„æ€§èƒ½ä¸SNNçš„æ•ˆç‡å’Œåˆç†æ€§ï¼Œä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°äº†æœ€ä½³ç»“æœã€‚æˆ‘ä»¬å‡è®¾è„‰å†²ç¼–ç ã€ç¨€ç–æ€§å’ŒLZCé©±åŠ¨çš„å¤æ‚æ€§åˆ†æä¹‹é—´çš„ååŒä½œç”¨èƒ½å¤Ÿå®ç°æ›´æœ‰æ•ˆçš„ç‰¹å¾æå–ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“åˆLZCçš„SNNåœ¨åŒ»å­¦è¯Šæ–­ä¸­æä¾›äº†æœ‰å‰æ™¯çš„ç”Ÿç‰©å­¦åˆç†çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯å¯¹äºèµ„æºå—é™æˆ–å®æ—¶ç³»ç»Ÿè€Œè¨€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06265v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆSpiking Neural Networksï¼ˆSNNsï¼‰å’ŒLempel-Ziv Complexityï¼ˆLZCï¼‰çš„ä¹³è…ºç™Œåˆ†ç±»æ–°æ–¹æ³•ã€‚LZCæé«˜äº†åŸºäºè„‰å†²çš„æ¨¡å‹çš„è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œé€šè¿‡æ•æ‰ç¥ç»æ´»åŠ¨ä¸­çš„ç»“æ„æ¨¡å¼æ¥å®ç°ã€‚ç ”ç©¶æ¢è®¨äº†ç”Ÿç‰©ç‰©ç†çš„Leaky Integrate-and-Fireï¼ˆLIFï¼‰å’Œæ¦‚ç‡æ€§çš„Levy-Baxterï¼ˆLBï¼‰ç¥ç»å…ƒæ¨¡å‹åœ¨ç›‘ç£ã€æ— ç›‘ç£å’Œæ··åˆå­¦ä¹ æ¨¡å¼ä¸‹çš„è¡¨ç°ã€‚åœ¨ä¹³è…ºç™Œå¨æ–¯åº·è¾›æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒLBæ¨¡å‹å‡†ç¡®ç‡è¶…è¿‡90%ï¼ŒLIFæ¨¡å‹å‡†ç¡®ç‡è¶…è¿‡85%ã€‚æœ€é«˜å‡†ç¡®ç‡ä¸º98.25%ï¼Œæ˜¯é€šè¿‡å°†äººå·¥ç¥ç»ç½‘ç»œè½¬æ¢ä¸ºSNNçš„æ–¹æ³•å®ç°çš„ï¼Œä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ ç›¸å½“ï¼Œä½†è®¡ç®—æˆæœ¬é™ä½äº†é«˜è¾¾100å€ã€‚è¿™ç§æ··åˆæ–¹æ³•ç»“åˆäº†æ·±åº¦å­¦ä¹ çš„æ€§èƒ½ä¸SNNsçš„æ•ˆç‡å’Œåˆç†æ€§ï¼Œåœ¨è¾ƒä½çš„è®¡ç®—æˆæœ¬ä¸‹å–å¾—äº†æœ€ä½³ç»“æœã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ—¶é—´ç¼–ç ã€è„‰å†²ç¨€ç–æ€§å’ŒLZCé©±åŠ¨çš„å¤æ‚æ€§åˆ†æä¹‹é—´çš„ååŒä½œç”¨ä½¿ç‰¹å¾æå–æ›´åŠ é«˜æ•ˆï¼ŒSNNsç»“åˆLZCåœ¨åŒ»å­¦è¯Šæ–­ä¸­æä¾›äº†æœ‰å‰æ™¯çš„ç”Ÿç‰©å¯è¡Œæ€§æ›¿ä»£æ–¹æ¡ˆï¼Œå°¤å…¶é€‚ç”¨äºèµ„æºå—é™æˆ–å®æ—¶ç³»ç»Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Spiking Neural Networksï¼ˆSNNsï¼‰å…·æœ‰äº‹ä»¶é©±åŠ¨çš„æ€§è´¨ï¼Œèƒ½å¤Ÿé«˜æ•ˆç¼–ç æ—¶ç©ºç‰¹å¾ï¼Œé€‚åˆåŠ¨æ€æ—¶é—´ä¾èµ–æ•°æ®å¤„ç†ã€‚</li>
<li>Lempel-Ziv Complexityï¼ˆLZCï¼‰æé«˜äº†åŸºäºè„‰å†²çš„æ¨¡å‹çš„è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ï¼Œé€šè¿‡æ•æ‰ç¥ç»æ´»åŠ¨ä¸­çš„ç»“æ„æ¨¡å¼å®ç°ã€‚</li>
<li>LBç¥ç»å…ƒæ¨¡å‹åœ¨ä¹³è…ºç™Œåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†90%ã€‚</li>
<li>LIFç¥ç»å…ƒæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡è¶…è¿‡85%ã€‚</li>
<li>é€šè¿‡å°†äººå·¥ç¥ç»ç½‘ç»œè½¬æ¢ä¸ºSNNçš„æ–¹æ³•ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ï¼ˆ98.25%ï¼‰ï¼Œä¸”è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
<li>æ··åˆæ–¹æ³•ç»“åˆäº†æ·±åº¦å­¦ä¹ çš„æ€§èƒ½ä¸SNNsçš„æ•ˆç‡å’Œåˆç†æ€§ï¼Œå±•ç°å‡ºåœ¨ä½è®¡ç®—æˆæœ¬ä¸‹çš„æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-217edfeea78884f4d4be60baf25df8a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b9d93cbf8b13b7a58c883e2c3fbd51e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LinGuinE-Longitudinal-Guidance-Estimation-for-Volumetric-Lung-Tumour-Segmentation"><a href="#LinGuinE-Longitudinal-Guidance-Estimation-for-Volumetric-Lung-Tumour-Segmentation" class="headerlink" title="LinGuinE: Longitudinal Guidance Estimation for Volumetric Lung Tumour   Segmentation"></a>LinGuinE: Longitudinal Guidance Estimation for Volumetric Lung Tumour   Segmentation</h2><p><strong>Authors:Nadine Garibli, Mayank Patwari, Bence Csiba, Yi Wei, Kostas Sidiropoulos</strong></p>
<p>Segmentation of lung gross tumour volumes is an important first step in radiotherapy and surgical intervention, and is starting to play a role in assessing chemotherapy response. Response to a drug is measured by tracking the tumour volumes over a series of CT scans over a time period i.e. a longitudinal study. However, there currently exist few solutions for automated or semi-automated longitudinal tumour segmentation. This paper introduces LinGuinE, an automated method to segment a longitudinal series of lung tumours. A radiologist must provide an initial input, indicating the location of the tumour in a CT scan at an arbitrary time point. LinGuinE samples points inside this tumour and propagates them to another time point using rigid registration. A click validity classifier selects points which still fall within the tumour; these are used to automatically create a segmentation in the new time point. We test LinGuinE on a dataset acquired from a phase 3 clinical trial for lung tumours and the publicly available 4-D lung CBCT dataset. We find that LinGuinE improves the Dice on both test sets by over 20% (p&lt; 0.05) across 63 longitudinal studies. We show that any time point can be used as a starting point, conduct ablation experiments, and find that our LinGuinE setup yields the best results on both test datasets. </p>
<blockquote>
<p>è‚ºéƒ¨è‚¿ç˜¤å¤§ä½“ç§¯åˆ†å‰²æ˜¯æ”¾ç–—å’Œæ‰‹æœ¯å¹²é¢„çš„é‡è¦ç¬¬ä¸€æ­¥ï¼Œå¹¶ä¸”åœ¨è¯„ä¼°åŒ–ç–—ååº”æ–¹é¢å¼€å§‹å‘æŒ¥ä½œç”¨ã€‚è¯ç‰©çš„ååº”æ˜¯é€šè¿‡è¿½è¸ªä¸€æ®µæ—¶é—´å†…çš„ä¸€ç³»åˆ—CTæ‰«æçš„è‚¿ç˜¤ä½“ç§¯æ¥è¡¡é‡çš„ï¼Œå³ä¸€é¡¹çºµå‘ç ”ç©¶ã€‚ç„¶è€Œï¼Œç›®å‰å‡ ä¹æ²¡æœ‰é’ˆå¯¹çºµå‘è‚¿ç˜¤åˆ†å‰²çš„è‡ªåŠ¨æˆ–åŠè‡ªåŠ¨è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLinGuinEçš„è‡ªåŠ¨æ–¹æ³•ï¼Œç”¨äºåˆ†å‰²ä¸€ç³»åˆ—çºµå‘è‚ºéƒ¨è‚¿ç˜¤ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿå¿…é¡»æä¾›ä¸€ä¸ªåˆå§‹è¾“å…¥ï¼ŒæŒ‡ç¤ºCTæ‰«æä¸­è‚¿ç˜¤çš„ä½ç½®ï¼ˆä»»æ„æ—¶é—´ç‚¹ï¼‰ã€‚LinGuinEä»è‚¿ç˜¤å†…éƒ¨å–æ ·ç‚¹ï¼Œå¹¶ä½¿ç”¨åˆšæ€§æ³¨å†Œå°†å®ƒä»¬ä¼ æ’­åˆ°å¦ä¸€ä¸ªæ—¶é—´ç‚¹ã€‚ç‚¹å‡»æœ‰æ•ˆæ€§åˆ†ç±»å™¨ä¼šé€‰æ‹©ä»åœ¨è‚¿ç˜¤å†…çš„ç‚¹ï¼›è¿™äº›ç‚¹è¢«ç”¨æ¥è‡ªåŠ¨åˆ›å»ºæ–°æ—¶é—´ç‚¹çš„åˆ†å‰²ã€‚æˆ‘ä»¬åœ¨ä»è‚ºéƒ¨è‚¿ç˜¤çš„ç¬¬ä¸‰é˜¶æ®µä¸´åºŠè¯•éªŒå’Œå…¬å¼€å¯ç”¨çš„4Dè‚ºCBCTæ•°æ®é›†è·å–çš„æ•°æ®é›†ä¸Šæµ‹è¯•äº†LinGuinEã€‚æˆ‘ä»¬å‘ç°LinGuinEåœ¨ä¸¤ç»„æµ‹è¯•é›†ä¸Šçš„Diceç³»æ•°æé«˜äº†è¶…è¿‡20%ï¼ˆp&lt;0.05ï¼‰ï¼Œæ¶‰åŠ63é¡¹çºµå‘ç ”ç©¶ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¯ä»¥ä½¿ç”¨ä»»ä½•æ—¶é—´ç‚¹ä½œä¸ºèµ·ç‚¹ï¼Œè¿›è¡Œæ¶ˆèå®éªŒï¼Œå¹¶å‘ç°æˆ‘ä»¬çš„LinGuinEè®¾ç½®åœ¨ä¸¤ç»„æµ‹è¯•æ•°æ®é›†ä¸Šéƒ½äº§ç”Ÿäº†æœ€ä½³ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06092v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLinGuinEçš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œç”¨äºå¯¹è‚ºéƒ¨è‚¿ç˜¤çš„é•¿æœŸåºåˆ—è¿›è¡Œåˆ†å‰²ã€‚è¯¥æ–¹æ³•éœ€è¦åŒ»ç”Ÿåœ¨ä»»æ„æ—¶é—´ç‚¹çš„CTæ‰«æä¸ŠæŒ‡ç¤ºè‚¿ç˜¤ä½ç½®ä½œä¸ºåˆå§‹è¾“å…¥ï¼Œç„¶åé€šè¿‡åˆšä½“æ³¨å†ŒæŠ€æœ¯å°†ç‚¹ä¼ æ’­åˆ°å¦ä¸€æ—¶é—´ç‚¹ã€‚è¯¥æ–¹æ³•è‡ªåŠ¨åˆ›å»ºæ–°æ—¶é—´ç‚¹çš„åˆ†å‰²ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLinGuinEåœ¨å¤šä¸ªé•¿æœŸç ”ç©¶æµ‹è¯•ä¸­ï¼Œæé«˜äº†æµ‹è¯•é›†çš„Diceç³»æ•°è¶…è¿‡20%ï¼ˆp&lt;0.05ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LinGuinEæ˜¯ä¸€ç§ç”¨äºåˆ†å‰²è‚ºéƒ¨è‚¿ç˜¤é•¿æœŸåºåˆ—çš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚</li>
<li>åŒ»ç”Ÿéœ€è¦åœ¨ä»»æ„æ—¶é—´ç‚¹çš„CTæ‰«æä¸ŠæŒ‡ç¤ºè‚¿ç˜¤ä½ç½®ä½œä¸ºåˆå§‹è¾“å…¥ã€‚</li>
<li>LinGuinEé€šè¿‡åˆšä½“æ³¨å†ŒæŠ€æœ¯å°†ç‚¹ä»åˆå§‹æ—¶é—´ç‚¹ä¼ æ’­åˆ°å¦ä¸€æ—¶é—´ç‚¹ã€‚</li>
<li>LinGuinEè‡ªåŠ¨åœ¨æ–°æ—¶é—´ç‚¹åˆ›å»ºåˆ†å‰²ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLinGuinEåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†Diceç³»æ•°è¶…è¿‡20%ï¼ˆp&lt;0.05ï¼‰ã€‚</li>
<li>LinGuinEå¯ä»¥ä»ä»»ä½•æ—¶é—´ç‚¹ä½œä¸ºèµ·ç‚¹ä½¿ç”¨ï¼Œå…·æœ‰çµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ae7200a43fb2cafc6fe34e85ea068dd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-016cafd7655f0c785d7e5de407f8f028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd60a7e9f0c449253301d3ea139f4e60.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Implicit-Neural-Representation-Based-MRI-Reconstruction-Method-with-Sensitivity-Map-Constraints"><a href="#Implicit-Neural-Representation-Based-MRI-Reconstruction-Method-with-Sensitivity-Map-Constraints" class="headerlink" title="Implicit Neural Representation-Based MRI Reconstruction Method with   Sensitivity Map Constraints"></a>Implicit Neural Representation-Based MRI Reconstruction Method with   Sensitivity Map Constraints</h2><p><strong>Authors:Lixuan Rao, Xinlin Zhang, Yiman Huang, Tao Tan, Tong Tong</strong></p>
<p>Magnetic Resonance Imaging (MRI) is a widely utilized diagnostic tool in clinical settings, but its application is limited by the relatively long acquisition time. As a result, fast MRI reconstruction has become a significant area of research. In recent years, Implicit Neural Representation (INR), as a scan-specific method, has demonstrated outstanding performance in fast MRI reconstruction without fully-sampled images for training. High acceleration reconstruction poses a challenging problem, and a key component in achieving high-quality reconstruction with much few data is the accurate estimation of coil sensitivity maps. However, most INR-based methods apply regularization constraints solely to the generated images, while overlooking the characteristics of the coil sensitivity maps. To handle this, this work proposes a joint coil sensitivity map and image estimation network, termed INR-CRISTAL. The proposed INR-CRISTAL introduces an extra sensitivity map regularization in the INR networks to make use of the smooth characteristics of the sensitivity maps. Experimental results show that INR-CRISTAL provides more accurate coil sensitivity estimates with fewer artifacts, and delivers superior reconstruction performance in terms of artifact removal and structure preservation. Moreover, INR-CRISTAL demonstrates stronger robustness to automatic calibration signals and the acceleration rate compared to existing methods. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸´åºŠç¯å¢ƒä¸­å¹¿æ³›ä½¿ç”¨çš„è¯Šæ–­å·¥å…·ï¼Œä½†å…¶åº”ç”¨å—åˆ°é‡‡é›†æ—¶é—´ç›¸å¯¹è¾ƒé•¿çš„é™åˆ¶ã€‚å› æ­¤ï¼Œå¿«é€ŸMRIé‡å»ºå·²æˆä¸ºä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸã€‚è¿‘å¹´æ¥ï¼Œä½œä¸ºä¸€ç§æ‰«æç‰¹å®šæ–¹æ³•ï¼Œéšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰åœ¨æ— éœ€å®Œå…¨é‡‡æ ·å›¾åƒè¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå·²åœ¨å¿«é€ŸMRIé‡å»ºä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é«˜é€Ÿé‡å»ºæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œåœ¨å®ç°é«˜è´¨é‡é‡å»ºè¿‡ç¨‹ä¸­ä½¿ç”¨æ›´å°‘æ•°æ®çš„å…³é”®ç»„ä»¶æ˜¯å‡†ç¡®ä¼°ç®—çº¿åœˆçµæ•åº¦å›¾ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åŸºäºINRçš„æ–¹æ³•ä»…å°†æ­£åˆ™åŒ–çº¦æŸåº”ç”¨äºç”Ÿæˆçš„å›¾åƒï¼Œè€Œå¿½ç•¥äº†çº¿åœˆçµæ•åº¦å›¾çš„ç‰¹æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬å·¥ä½œæå‡ºäº†ä¸€ä¸ªè”åˆçº¿åœˆçµæ•åº¦å›¾å’Œå›¾åƒä¼°è®¡ç½‘ç»œï¼Œç§°ä¸ºINR-CRISTALã€‚æå‡ºçš„INR-CRISTALåœ¨INRç½‘ç»œä¸­å¼•å…¥äº†é¢å¤–çš„çµæ•åº¦å›¾æ­£åˆ™åŒ–ï¼Œä»¥åˆ©ç”¨çµæ•åº¦å›¾çš„å¹³æ»‘ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINR-CRISTALæä¾›äº†æ›´å‡†ç¡®çš„çº¿åœˆçµæ•åº¦ä¼°è®¡ï¼Œå‡å°‘äº†ä¼ªå½±ï¼Œå¹¶åœ¨ä¼ªå½±å»é™¤å’Œç»“æ„ä¿ç•™æ–¹é¢æä¾›äº†å‡ºè‰²çš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒINR-CRISTALå¯¹è‡ªåŠ¨æ ¡å‡†ä¿¡å·å’ŒåŠ é€Ÿç‡è¡¨ç°å‡ºæ›´å¼ºçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06043v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œå¦‚é‡‡é›†æ—¶é—´é•¿ã€‚è¿‘å¹´æ¥ï¼Œéšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä½œä¸ºä¸€ç§æ‰«æç‰¹å®šæ–¹æ³•åœ¨å¿«é€ŸMRIé‡å»ºä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ— éœ€å…¨é‡‡æ ·å›¾åƒè¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ã€‚æ–‡ç« æŒ‡å‡ºé«˜é€Ÿé‡å»ºé¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å‡†ç¡®ä¼°ç®—çº¿åœˆçµæ•åº¦å›¾ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºäº†è”åˆçº¿åœˆçµæ•åº¦å›¾å’Œå›¾åƒä¼°è®¡ç½‘ç»œâ€”â€”INR-CRISTALã€‚è¯¥ç½‘ç»œåœ¨INRç½‘ç»œä¸­å¼•å…¥äº†é¢å¤–çš„çµæ•åº¦å›¾æ­£åˆ™åŒ–ï¼Œåˆ©ç”¨çµæ•åº¦å›¾çš„å¹³æ»‘ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒINR-CRISTALåœ¨è¾ƒå°‘ä¼ªå½±çš„æƒ…å†µä¸‹æä¾›äº†æ›´å‡†ç¡®çš„çº¿åœˆçµæ•åº¦ä¼°è®¡ï¼Œå¹¶åœ¨ä¼ªå½±å»é™¤å’Œç»“æ„ä¿ç•™æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„é‡å»ºæ€§èƒ½ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰æ›´å¼ºçš„è‡ªåŠ¨æ ¡å‡†ä¿¡å·å’ŒåŠ é€Ÿç‡çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å±€é™æ€§åœ¨äºé‡‡é›†æ—¶é—´è¾ƒé•¿ï¼Œå¿«é€ŸMRIé‡å»ºæˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ–¹æ³•åœ¨å¿«é€ŸMRIé‡å»ºä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨ä¸ä¾èµ–å…¨é‡‡æ ·å›¾åƒè¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ã€‚</li>
<li>é«˜åŠ é€Ÿé‡å»ºé¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å‡†ç¡®ä¼°ç®—çº¿åœˆçµæ•åº¦å›¾ã€‚</li>
<li>INR-CRISTALç½‘ç»œé€šè¿‡å¼•å…¥é¢å¤–çš„çµæ•åº¦å›¾æ­£åˆ™åŒ–æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œåˆ©ç”¨çµæ•åº¦å›¾çš„å¹³æ»‘ç‰¹æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºINR-CRISTALæä¾›æ›´å‡†ç¡®çš„çº¿åœˆçµæ•åº¦ä¼°è®¡ï¼Œå‡å°‘ä¼ªå½±ã€‚</li>
<li>INR-CRISTALåœ¨ä¼ªå½±å»é™¤å’Œç»“æ„ä¿ç•™æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„é‡å»ºæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3d161be0a82c9b2dbc2c10f67b9f47fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-969f390354c82e89f4510b619f146340.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-023775acaab0b73d68e5f25e729d0799.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01d41a963c01ed8fbe12bb5c3a011972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69762ac746be8efff00d47e9cf479432.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FuseUNet-A-Multi-Scale-Feature-Fusion-Method-for-U-like-Networks"><a href="#FuseUNet-A-Multi-Scale-Feature-Fusion-Method-for-U-like-Networks" class="headerlink" title="FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks"></a>FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks</h2><p><strong>Authors:Quansong He, Xiangde Min, Kaishen Wang, Tao He</strong></p>
<p>Medical image segmentation is a critical task in computer vision, with UNet serving as a milestone architecture. The typical component of UNet family is the skip connection, however, their skip connections face two significant limitations: (1) they lack effective interaction between features at different scales, and (2) they rely on simple concatenation or addition operations, which constrain efficient information integration. While recent improvements to UNet have focused on enhancing encoder and decoder capabilities, these limitations remain overlooked. To overcome these challenges, we propose a novel multi-scale feature fusion method that reimagines the UNet decoding process as solving an initial value problem (IVP), treating skip connections as discrete nodes. By leveraging principles from the linear multistep method, we propose an adaptive ordinary differential equation method to enable effective multi-scale feature fusion. Our approach is independent of the encoder and decoder architectures, making it adaptable to various U-Net-like networks. Experiments on ACDC, KiTS2023, MSD brain tumor, and ISIC2017&#x2F;2018 skin lesion segmentation datasets demonstrate improved feature utilization, reduced network parameters, and maintained high performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/nayutayuki/FuseUNet">https://github.com/nayutayuki/FuseUNet</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œå…¶ä¸­UNetä½œä¸ºä¸€ç§é‡Œç¨‹ç¢‘å¼çš„æ¶æ„å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚UNetå®¶æ—çš„å…¸å‹ç»„ä»¶æ˜¯è·³è·ƒè¿æ¥ï¼Œç„¶è€Œï¼Œå®ƒä»¬çš„è·³è·ƒè¿æ¥é¢ä¸´ä¸¤ä¸ªé‡å¤§å±€é™æ€§ï¼š(1)å®ƒä»¬åœ¨ä¸åŒå°ºåº¦ç‰¹å¾ä¹‹é—´çš„æœ‰æ•ˆäº¤äº’ä¸è¶³ï¼›(2)å®ƒä»¬ä¾èµ–äºç®€å•çš„æ‹¼æ¥æˆ–åŠ æ³•æ“ä½œï¼Œè¿™é™åˆ¶äº†ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚å°½ç®¡æœ€è¿‘çš„UNetæ”¹è¿›ä¸»è¦é›†ä¸­åœ¨å¢å¼ºç¼–ç å™¨å’Œè§£ç å™¨çš„åŠŸèƒ½ï¼Œä½†è¿™äº›é™åˆ¶ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦ç‰¹å¾èåˆæ–¹æ³•ï¼Œå®ƒå°†UNetçš„è§£ç è¿‡ç¨‹é‡æ–°æ„æƒ³ä¸ºæ±‚è§£åˆå€¼é—®é¢˜ï¼ˆIVPï¼‰ï¼Œå°†è·³è·ƒè¿æ¥è§†ä¸ºç¦»æ•£èŠ‚ç‚¹ã€‚æˆ‘ä»¬å€ŸåŠ©çº¿æ€§å¤šæ­¥æ³•çš„åŸç†ï¼Œæå‡ºäº†ä¸€ç§è‡ªé€‚åº”å¸¸å¾®åˆ†æ–¹ç¨‹æ–¹æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šå°ºåº¦ç‰¹å¾èåˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç‹¬ç«‹äºç¼–ç å™¨å’Œè§£ç å™¨æ¶æ„ï¼Œä½¿å…¶é€‚åº”äºå„ç§U-Netç±»ç½‘ç»œã€‚åœ¨ACDCã€KiTS2023ã€MSDè„‘è‚¿ç˜¤å’ŒISIC2017&#x2F;2018çš®è‚¤ç—…å˜åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æé«˜çš„ç‰¹å¾åˆ©ç”¨ç‡ã€å‡å°‘çš„ç½‘ç»œå‚æ•°ä»¥åŠç»´æŒçš„é«˜æ€§èƒ½ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nayutayuki/FuseUNet">https://github.com/nayutayuki/FuseUNet</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05821v1">PDF</a> ICML2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹UNetåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦ç‰¹å¾èåˆæ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†UNetçš„è§£ç è¿‡ç¨‹è§†ä¸ºæ±‚è§£åˆå€¼é—®é¢˜ï¼ˆIVPï¼‰ï¼Œåˆ©ç”¨çº¿æ€§å¤šæ­¥æ³•çš„åŸç†ï¼Œé€šè¿‡è‡ªé€‚åº”å¸¸å¾®åˆ†æ–¹ç¨‹æ–¹æ³•å®ç°æœ‰æ•ˆçš„å¤šå°ºåº¦ç‰¹å¾èåˆã€‚æ­¤æ–¹æ³•ç‹¬ç«‹äºç¼–ç å™¨è§£ç å™¨æ¶æ„ï¼Œå¯é€‚åº”å„ç§U-Netç±»ç½‘ç»œï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UNetæ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é‡è¦æ¶æ„ï¼Œä½†å…¶skipè¿æ¥å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ä¸åŒå°ºåº¦ç‰¹å¾çš„æœ‰æ•ˆäº¤äº’ï¼Œä»¥åŠä¿¡æ¯æ•´åˆçš„çº¦æŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå°ºåº¦ç‰¹å¾èåˆæ–¹æ³•ï¼Œä»¥å…‹æœè¿™äº›å±€é™æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å°†UNetçš„è§£ç è¿‡ç¨‹è§†ä¸ºæ±‚è§£åˆå€¼é—®é¢˜ï¼ˆIVPï¼‰ï¼Œåˆ©ç”¨çº¿æ€§å¤šæ­¥æ³•çš„åŸç†ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”å¸¸å¾®åˆ†æ–¹ç¨‹æ–¹æ³•å®ç°æœ‰æ•ˆçš„å¤šå°ºåº¦ç‰¹å¾èåˆã€‚</li>
<li>æ­¤æ–¹æ³•ç‹¬ç«‹äºç¼–ç å™¨è§£ç å™¨æ¶æ„ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚åº”æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æ”¹è¿›çš„ç‰¹å¾åˆ©ç”¨ã€å‡å°‘çš„ç½‘ç»œå‚æ•°ä»¥åŠç»´æŒçš„é«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6221ce6b388eae517167c248f61c934c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c32d42676eeb3a28f2f33c309a2e662c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-837492893a8fee87dbb13e9490ace53c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2442e9dac497f452f960c26b33429f4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db3912ebbe6b3106258096d3a551b847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6fc05fc707f87d2cd1ff10848a0914e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DeformCL-Learning-Deformable-Centerline-Representation-for-Vessel-Extraction-in-3D-Medical-Image"><a href="#DeformCL-Learning-Deformable-Centerline-Representation-for-Vessel-Extraction-in-3D-Medical-Image" class="headerlink" title="DeformCL: Learning Deformable Centerline Representation for Vessel   Extraction in 3D Medical Image"></a>DeformCL: Learning Deformable Centerline Representation for Vessel   Extraction in 3D Medical Image</h2><p><strong>Authors:Ziwei Zhao, Zhixing Zhang, Yuhang Liu, Zhao Zhang, Haojun Yu, Dong Wang, Liwei Wang</strong></p>
<p>In the field of 3D medical imaging, accurately extracting and representing the blood vessels with curvilinear structures holds paramount importance for clinical diagnosis. Previous methods have commonly relied on discrete representation like mask, often resulting in local fractures or scattered fragments due to the inherent limitations of the per-pixel classification paradigm. In this work, we introduce DeformCL, a new continuous representation based on Deformable Centerlines, where centerline points act as nodes connected by edges that capture spatial relationships. Compared with previous representations, DeformCL offers three key advantages: natural connectivity, noise robustness, and interaction facility. We present a comprehensive training pipeline structured in a cascaded manner to fully exploit these favorable properties of DeformCL. Extensive experiments on four 3D vessel segmentation datasets demonstrate the effectiveness and superiority of our method. Furthermore, the visualization of curved planar reformation images validates the clinical significance of the proposed framework. We release the code in <a target="_blank" rel="noopener" href="https://github.com/barry664/DeformCL">https://github.com/barry664/DeformCL</a> </p>
<blockquote>
<p>åœ¨ä¸‰ç»´åŒ»å­¦å½±åƒé¢†åŸŸï¼Œå‡†ç¡®æå–å’Œè¡¨ç¤ºå…·æœ‰æ›²çº¿ç»“æ„çš„è¡€ç®¡å¯¹äºä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ã€‚ä»¥å¾€çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç¦»æ•£è¡¨ç¤ºï¼Œå¦‚æ©è†œï¼Œç”±äºåƒç´ çº§åˆ†ç±»æ¨¡å¼çš„å›ºæœ‰å±€é™æ€§ï¼Œå¸¸å¸¸å¯¼è‡´å±€éƒ¨æ–­è£‚æˆ–åˆ†æ•£çš„ç‰‡æ®µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå¯å˜å½¢ä¸­å¿ƒçº¿çš„è¿ç»­è¡¨ç¤ºæ–¹æ³•DeformCLã€‚ä¸­å¿ƒçº¿ç‚¹ä½œä¸ºèŠ‚ç‚¹é€šè¿‡è¾¹ç¼˜è¿æ¥ï¼Œä»è€Œæ•è·ç©ºé—´å…³ç³»ã€‚ä¸ä¹‹å‰çš„è¡¨ç¤ºæ–¹æ³•ç›¸æ¯”ï¼ŒDeformCLå…·æœ‰ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼šè‡ªç„¶è¿æ¥ã€å™ªå£°é²æ£’æ€§å’Œäº¤äº’ä¾¿åˆ©æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨çº§è”çš„ç»¼åˆè®­ç»ƒç®¡é“ï¼Œä»¥å……åˆ†åˆ©ç”¨DeformCLçš„è¿™äº›æœ‰åˆ©å±æ€§ã€‚åœ¨å››ä¸ªä¸‰ç»´è¡€ç®¡åˆ†å‰²æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæ›²é¢å¹³é¢æ”¹é©å›¾åƒçš„å¯è§†åŒ–éªŒè¯äº†æ‰€æå‡ºæ¡†æ¶çš„ä¸´åºŠæ„ä¹‰ã€‚æˆ‘ä»¬å·²å°†ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/barry664/DeformCL%E3%80%82">https://github.com/barry664/DeformCLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05820v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨3DåŒ»å­¦å½±åƒé¢†åŸŸä¸­ï¼Œé‡‡ç”¨åŸºäºå¯å˜å½¢ä¸­å¿ƒçº¿çš„è¿ç»­è¡¨ç¤ºæ–¹æ³•ï¼ˆDeformCLï¼‰å¯¹è¡€ç®¡è¿›è¡Œå‡†ç¡®æå–å’Œè¡¨ç¤ºçš„é‡è¦æ€§ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„ç¦»æ•£è¡¨ç¤ºæ–¹æ³•ï¼ŒDeformCLèƒ½å¤Ÿæ›´è‡ªç„¶åœ°æè¿°è¡€ç®¡çš„è¿ç»­æ€§ç»“æ„ï¼Œå…·æœ‰æ›´å¥½çš„å™ªå£°é²æ£’æ€§å’Œäº¤äº’æ€§ã€‚æ–‡ç« æå‡ºäº†ä¸€å¥—çº§è”çš„è®­ç»ƒæµç¨‹ï¼Œå……åˆ†å‘æ˜äº†DeformCLçš„ä¼˜åŠ¿ç‰¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ä¸ª3Dè¡€ç®¡åˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¸´åºŠæ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DåŒ»å­¦å½±åƒä¸­è¡€ç®¡çš„å‡†ç¡®æå–å’Œè¡¨ç¤ºå¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿçš„ç¦»æ•£è¡¨ç¤ºæ–¹æ³•å¦‚æ©è†œï¼ˆmaskï¼‰å­˜åœ¨å±€é™æ€§ï¼Œæ˜“å¯¼è‡´è¡€ç®¡ç»“æ„æ–­è£‚æˆ–æ•£ä¹±ã€‚</li>
<li>DeformCLæ˜¯ä¸€ç§åŸºäºå¯å˜å½¢ä¸­å¿ƒçº¿çš„è¿ç»­è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½æ›´è‡ªç„¶åœ°æè¿°è¡€ç®¡çš„è¿ç»­æ€§ç»“æ„ã€‚</li>
<li>DeformCLç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•å…·æœ‰å¤©ç„¶è¿é€šæ€§ã€å™ªå£°é²æ£’æ€§å’Œè‰¯å¥½çš„äº¤äº’æ€§ä¸‰ä¸ªä¸»è¦ä¼˜åŠ¿ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸€å¥—çº§è”çš„è®­ç»ƒæµç¨‹æ¥å……åˆ†åˆ©ç”¨DeformCLçš„ç‰¹æ€§ã€‚</li>
<li>åœ¨å››ä¸ª3Dè¡€ç®¡åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05820">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e5bd1b12eed6ea4d04c244d97438f33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81e32ebb6a792503ddd29bf4507f4c55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a46667056ab2018d33a15026223d73f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4ae1b99388b45f263dfc29a2cce83d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125a57af2bdc5e4a7a5bc0de9ed5a791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48ff3d2c66aa62020328689dd1d886cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16f5c5f351a9a28f3ab2c432f175292a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TissUnet-Improved-Extracranial-Tissue-and-Cranium-Segmentation-for-Children-through-Adulthood"><a href="#TissUnet-Improved-Extracranial-Tissue-and-Cranium-Segmentation-for-Children-through-Adulthood" class="headerlink" title="TissUnet: Improved Extracranial Tissue and Cranium Segmentation for   Children through Adulthood"></a>TissUnet: Improved Extracranial Tissue and Cranium Segmentation for   Children through Adulthood</h2><p><strong>Authors:Markian Mandzak, Elvira Yang, Anna Zapaishchykova, Yu-Hui Chen, Lucas Heilbroner, John Zielke, Divyanshu Tak, Reza Mojahed-Yazdi, Francesca Romana Mussa, Zezhong Ye, Sridhar Vajapeyam, Viviana Benitez, Ralph Salloum, Susan N. Chi, Houman Sotoudeh, Jakob Seidlitz, Sabine Mueller, Hugo J. W. L. Aerts, Tina Y. Poussaint, Benjamin H. Kann</strong></p>
<p>Extracranial tissues visible on brain magnetic resonance imaging (MRI) may hold significant value for characterizing health conditions and clinical decision-making, yet they are rarely quantified. Current tools have not been widely validated, particularly in settings of developing brains or underlying pathology. We present TissUnet, a deep learning model that segments skull bone, subcutaneous fat, and muscle from routine three-dimensional T1-weighted MRI, with or without contrast enhancement. The model was trained on 155 paired MRI-computed tomography (CT) scans and validated across nine datasets covering a wide age range and including individuals with brain tumors. In comparison to AI-CT-derived labels from 37 MRI-CT pairs, TissUnet achieved a median Dice coefficient of 0.79 [IQR: 0.77-0.81] in a healthy adult cohort. In a second validation using expert manual annotations, median Dice was 0.83 [IQR: 0.83-0.84] in healthy individuals and 0.81 [IQR: 0.78-0.83] in tumor cases, outperforming previous state-of-the-art method. Acceptability testing resulted in an 89% acceptance rate after adjudication by a tie-breaker(N&#x3D;108 MRIs), and TissUnet demonstrated excellent performance in the blinded comparative review (N&#x3D;45 MRIs), including both healthy and tumor cases in pediatric populations. TissUnet enables fast, accurate, and reproducible segmentation of extracranial tissues, supporting large-scale studies on craniofacial morphology, treatment effects, and cardiometabolic risk using standard brain T1w MRI. </p>
<blockquote>
<p>å¤§è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­å¯è§çš„å¤´é¢…å¤–ç»„ç»‡å¯¹äºè¡¨å¾å¥åº·çŠ¶å†µå’Œä¸´åºŠå†³ç­–å…·æœ‰é‡è¦çš„ä»·å€¼ï¼Œä½†å¾ˆå°‘è¢«é‡åŒ–ã€‚å½“å‰å·¥å…·å°šæœªå¾—åˆ°å¹¿æ³›éªŒè¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘è‚²ä¸­çš„å¤§è„‘æˆ–åŸºç¡€ç—…ç†å­¦çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†TissUnetï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ä»å¸¸è§„çš„ä¸‰ç»´T1åŠ æƒMRIä¸­åˆ†å‰²é¢…éª¨ã€çš®ä¸‹è„‚è‚ªå’Œè‚Œè‚‰ï¼Œæ— è®ºæ˜¯å¦è¿›è¡Œå¢å¼ºå¯¹æ¯”ã€‚è¯¥æ¨¡å‹åœ¨155å¯¹MRI-è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¶µç›–å¹¿æ³›å¹´é¾„èŒƒå›´ä¸”åŒ…æ‹¬è„‘è‚¿ç˜¤æ‚£è€…åœ¨å†…çš„ä¹ä¸ªæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ã€‚ä¸æ¥è‡ª37å¯¹MRI-CTçš„äººå·¥æ™ºèƒ½CTæ ‡ç­¾ç›¸æ¯”ï¼ŒTissUnetåœ¨å¥åº·æˆäººé˜Ÿåˆ—ä¸­çš„ä¸­ä½Diceç³»æ•°ä¸º0.79 [IQRï¼š0.77-0.81]ã€‚åœ¨å¦ä¸€é¡¹ä½¿ç”¨ä¸“å®¶æ‰‹åŠ¨æ³¨é‡Šçš„éªŒè¯ä¸­ï¼Œå¥åº·ä¸ªä½“çš„ä¸­ä½Diceç³»æ•°ä¸º0.83 [IQRï¼š0.83-0.84]ï¼Œè‚¿ç˜¤ç—…ä¾‹çš„ä¸­ä½Diceç³»æ•°ä¸º0.81 [IQRï¼š0.78-0.83]ï¼Œè¶…è¿‡äº†ä¹‹å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å¯æ¥å—æ€§æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»²è£è€…ï¼ˆN&#x3D;108 MRIï¼‰è£å†³åï¼Œæ¥å—ç‡ä¸º89%ï¼ŒTissUnetåœ¨ç›²æ¯”è¾ƒå®¡æŸ¥ï¼ˆN&#x3D;45 MRIï¼‰ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å„¿ç§‘äººç¾¤ä¸­çš„å¥åº·ç—…ä¾‹å’Œè‚¿ç˜¤ç—…ä¾‹ã€‚TissUnetèƒ½å¤Ÿå®ç°å¿«é€Ÿã€å‡†ç¡®å’Œå¯é‡å¤çš„é¢…å¤–ç»„ç»‡åˆ†å‰²ï¼Œæ”¯æŒåˆ©ç”¨æ ‡å‡†å¤§è„‘T1w MRIè¿›è¡Œå¤§è§„æ¨¡é¢…é¢å½¢æ€å­¦ã€æ²»ç–—æ•ˆåº”å’Œä»£è°¢é£é™©ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05660v1">PDF</a> 44 pages, 4 tables, 6 figures, supplementary material</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­ï¼Œé¢…å¤–ç»„ç»‡çš„å¯è§æ€§å¯¹äºè¡¨å¾å¥åº·çŠ¶å†µå’Œä¸´åºŠå†³ç­–å¯èƒ½å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†ç›®å‰å°šç¼ºä¹å¯¹è¿™äº›ç»„ç»‡çš„é‡åŒ–è¯„ä¼°å·¥å…·ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹â€”â€”TissUnetï¼Œå®ƒå¯ä»¥å¯¹å¸¸è§„ä¸‰ç»´T1åŠ æƒMRIè¿›è¡Œé¢…éª¨ã€çš®ä¸‹è„‚è‚ªå’Œè‚Œè‚‰çš„åˆ†å‰²ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨å¯¹æ¯”å¢å¼ºã€‚è¯¥æ¨¡å‹åœ¨155å¯¹MRI-è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨æ¶µç›–å¹¿æ³›å¹´é¾„èŒƒå›´ä¸”åŒ…æ‹¬è„‘è‚¿ç˜¤æ‚£è€…çš„äººç¾¤ä¸­è¿›è¡Œäº†éªŒè¯ã€‚ä¸æ¥è‡ª37å¯¹MRI-CTçš„AI-CTæ ‡ç­¾ç›¸æ¯”ï¼Œåœ¨å¥åº·æˆäººé˜Ÿåˆ—ä¸­ï¼ŒTissUnetçš„Diceç³»æ•°ä¸­ä½æ•°ä¸º0.79[IQRï¼š0.77-0.81]ã€‚ä½¿ç”¨ä¸“å®¶æ‰‹åŠ¨æ³¨é‡Šè¿›è¡Œçš„ç¬¬äºŒæ¬¡éªŒè¯æ˜¾ç¤ºï¼Œåœ¨å¥åº·ä¸ªä½“å’Œè‚¿ç˜¤ç—…ä¾‹ä¸­ï¼ŒDiceç³»æ•°ä¸­ä½æ•°åˆ†åˆ«ä¸º0.83[IQRï¼š0.83-0.84]å’Œ0.81[IQRï¼š0.78-0.83]ï¼Œä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚å¯æ¥å—æ€§æµ‹è¯•ç»“æœä¸ºæ¥å—ç‡ä¸º89%ï¼ˆç»è¿‡ä»²è£è€…è§£å†³å¹³å±€åï¼‰ã€‚æ­¤å¤–ï¼Œåœ¨ç›²æ¯”è¾ƒå®¡æŸ¥ä¸­ï¼ˆåŒ…æ‹¬å¥åº·å’Œè‚¿ç˜¤ç—…ä¾‹çš„å„¿ç§‘äººç¾¤ï¼‰ï¼ŒTissUnetè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚TissUnetå¯ä»¥å¿«é€Ÿã€å‡†ç¡®ã€å¯é‡å¤åœ°åˆ†å‰²é¢…å¤–ç»„ç»‡ï¼Œæ”¯æŒåˆ©ç”¨æ ‡å‡†è„‘T1w MRIè¿›è¡Œå¤§è§„æ¨¡é¢…é¢å½¢æ€å­¦ã€æ²»ç–—å½±å“å’Œä»£è°¢é£é™©ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>TissUnetæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿåˆ†å‰²é¢…å¤–ç»„ç»‡å¦‚é¢…éª¨ã€çš®ä¸‹è„‚è‚ªå’Œè‚Œè‚‰ã€‚</li>
<li>æ¨¡å‹åŸºäºMRIå›¾åƒè¿›è¡Œè®­ç»ƒä¸åˆ†å‰²ï¼Œé€‚ç”¨äºå¸¸è§„ä¸‰ç»´T1åŠ æƒMRIï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨å¯¹æ¯”å¢å¼ºã€‚</li>
<li>TissUnetåœ¨å¹¿æ³›çš„å¹´é¾„èŒƒå›´å’ŒåŒ…æ‹¬è„‘è‚¿ç˜¤æ‚£è€…çš„äººç¾¤ä¸­è¿›è¡Œäº†éªŒè¯ï¼Œè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>ä¸AI-CTå’Œä¸“å®¶æ‰‹åŠ¨æ³¨é‡Šç›¸æ¯”ï¼ŒTissUnetçš„åˆ†å‰²ç»“æœå…·æœ‰è¾ƒé«˜çš„Diceç³»æ•°ï¼Œæ˜¾ç¤ºå‡ºå…¶å‡†ç¡®æ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>TissUnetåœ¨æ¥å—æ€§æµ‹è¯•ä¸­è·å¾—äº†è¾ƒé«˜çš„æ¥å—ç‡ã€‚</li>
<li>TissUnetåœ¨ç›²æ¯”è¾ƒå®¡æŸ¥ä¸­è¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬å¤„ç†å„¿ç§‘äººç¾¤ä¸­çš„å¥åº·å’Œè‚¿ç˜¤ç—…ä¾‹ã€‚</li>
<li>TissUnetçš„åº”ç”¨æœ‰åŠ©äºå¤§è§„æ¨¡ç ”ç©¶é¢…é¢å½¢æ€å­¦ã€æ²»ç–—å½±å“å’Œä»£è°¢é£é™©ç­‰é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05660">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0ab01fe826aa8dd97c251a40302965ea.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Challenging-Spontaneous-Quantum-Collapse-with-XENONnT"><a href="#Challenging-Spontaneous-Quantum-Collapse-with-XENONnT" class="headerlink" title="Challenging Spontaneous Quantum Collapse with XENONnT"></a>Challenging Spontaneous Quantum Collapse with XENONnT</h2><p><strong>Authors:E. Aprile, J. Aalbers, K. Abe, S. Ahmed Maouloud, L. Althueser, B. Andrieu, E. Angelino, D. AntÃ³n Martin, S. R. Armbruster, F. Arneodo, L. Baudis, M. Bazyk, L. Bellagamba, R. Biondi, A. Bismark, K. Boese, A. Brown, G. Bruno, R. Budnik, C. Cai, C. Capelli, J. M. R. Cardoso, A. P. Cimental ChÃ¡vez, A. P. Colijn, J. Conrad, J. J. Cuenca-GarcÃ­a, C. Curceanu, V. Dâ€™Andrea, L. C. Daniel Garcia, M. P. Decowski, A. Deisting, C. Di Donato, P. Di Gangi, S. Diglio, K. Eitel, S. el Morabit, A. Elykov, A. D. Ferella, C. Ferrari, H. Fischer, T. Flehmke, M. Flierman, W. Fulgione, C. Fuselli, P. Gaemers, R. Gaior, F. Gao, S. Ghosh, R. Giacomobono, F. Girard, R. Glade-Beucke, L. Grandi, J. Grigat, H. Guan, M. Guida, P. Gyorgy, R. Hammann, A. Higuera, C. Hils, L. Hoetzsch, N. F. Hood, M. Iacovacci, Y. Itow, J. Jakob, F. Joerg, Y. Kaminaga, M. Kara, P. Kavrigin, S. Kazama, P. Kharbanda, M. Kobayashi, D. Koke, A. Kopec, H. Landsman, R. F. Lang, L. Levinson, I. Li, S. Li, S. Liang, Z. Liang, Y. -T. Lin, S. Lindemann, K. Liu, M. Liu, J. Loizeau, F. Lombardi, J. Long, J. A. M. Lopes, G. M. Lucchetti, T. Luce, Y. Ma, C. Macolino, J. Mahlstedt, A. Mancuso, L. Manenti, S. Manti, F. Marignetti, T. MarrodÃ¡n Undagoitia, K. Martens, J. Masbou, S. Mastroianni, A. Melchiorre, J. Merz, M. Messina, A. Michael, K. Miuchi, A. Molinario, S. Moriyama, K. MorÃ¥, Y. Mosbacher, M. Murra, J. MÃ¼ller, K. Ni, U. Oberlack, B. Paetsch, Y. Pan, Q. Pellegrini, R. Peres, C. Peters, J. Pienaar, M. Pierre, K. Piscicchia, G. Plante, T. R. Pollmann, L. Principe, J. Qi, J. Qin, D. RamÃ­rez GarcÃ­a, M. Rajado, A. Ravindran, A. Razeto, L. Redard-Jacot, R. Singh, L. Sanchez, J. M. F. dos Santos, I. Sarnoff, G. Sartorelli, J. Schreiner, P. Schulte, H. Schulze EiÃŸing, M. Schumann, L. Scotto Lavina, M. Selvi, F. Semeria, P. Shagin, S. Shi, J. Shi, M. Silva, H. Simgen, A. Stevens, C. Szyszka, A. Takeda, Y. Takeuchi, P. -L. Tan, D. Thers, G. Trinchero, C. D. Tunnell, F. TÃ¶nnies, K. Valerius, S. Vecchi, S. Vetter, F. I. Villazon Solar, G. Volta, C. Weinheimer, M. Weiss, D. Wenz, C. Wittweg, V. H. S. Wu, Y. Xing, D. Xu, Z. Xu, M. Yamashita, L. Yang, J. Ye, L. Yuan, G. Zavattini, M. Zhong</strong></p>
<p>We report on the search for X-ray radiation as predicted from dynamical quantum collapse with low-energy electronic recoil data in the energy range of 1-140 keV from the first science run of the XENONnT dark matter detector. Spontaneous radiation is an unavoidable effect of dynamical collapse models, which were introduced as a possible solution to the long-standing measurement problem in quantum mechanics. The analysis utilizes a model that for the first time accounts for cancellation effects in the emitted spectrum, which arise in the X-ray range due to the opposing electron-proton charges in xenon atoms. New world-leading limits on the free parameters of the Markovian continuous spontaneous localization and Di&#39;osi-Penrose models are set, improving previous best constraints by two orders of magnitude and a factor of five, respectively. The original values proposed for the strength and the correlation length of the continuous spontaneous localization model are excluded experimentally for the first time. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†åœ¨XENONnTæš—ç‰©è´¨æ¢æµ‹å™¨é¦–æ¬¡ç§‘å­¦è¿è¡Œä¸­ï¼Œä»ä½èƒ½ç”µå­åå†²æ•°æ®ä¸­é¢„æµ‹çš„åŠ¨æ€é‡å­å¡Œç¼©æ‰€äº§ç”Ÿçš„Xå°„çº¿è¾å°„çš„æœå¯»æƒ…å†µã€‚è‡ªå‘è¾å°„æ˜¯åŠ¨æ€å¡Œç¼©æ¨¡å‹çš„ä¸å¯é¿å…çš„æ•ˆæœï¼Œè¯¥æ¨¡å‹è¢«å¼•å…¥ä½œä¸ºè§£å†³é‡å­åŠ›å­¦ä¸­é•¿æœŸæµ‹é‡é—®é¢˜çš„ä¸€ç§å¯èƒ½è§£å†³æ–¹æ¡ˆã€‚åˆ†æé‡‡ç”¨äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–æ¬¡è€ƒè™‘äº†å‘å°„å…‰è°±ä¸­çš„æŠµæ¶ˆæ•ˆåº”ï¼Œè¿™äº›æ•ˆåº”åœ¨Xå°„çº¿èŒƒå›´å†…ç”±äºæ°™åŸå­ä¸­ç”µå­å’Œè´¨å­ç”µè·çš„ç›¸äº’å¯¹ç«‹è€Œäº§ç”Ÿã€‚ä¸ºé©¬å°”å¯å¤«è¿ç»­è‡ªå‘å®šä½æ¨¡å‹å’ŒDiâ€™osi-Penroseæ¨¡å‹çš„è‡ªç”±å‚æ•°è®¾å®šäº†æ–°çš„ä¸–ç•Œé¢†å…ˆé™åˆ¶ï¼Œåˆ†åˆ«æ”¹å–„äº†ä¹‹å‰çš„æœ€ä½³çº¦æŸä¸¤ä¸ªæ•°é‡çº§å’Œäº”å€ã€‚è¿ç»­è‡ªå‘å®šä½æ¨¡å‹çš„å¼ºåº¦å’Œå…³è”é•¿åº¦çš„åŸå§‹æè®®å€¼è¢«å®éªŒé¦–æ¬¡æ’é™¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05507v1">PDF</a> 7 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŠ¥å‘Šäº†ä½¿ç”¨ä½èƒ½ç”µå­åå†²æ•°æ®ä»XENONnTæš—ç‰©è´¨æ¢æµ‹å™¨é¦–æ¬¡ç§‘å­¦è¿è¡Œä¸­é¢„æµ‹åˆ°çš„Xå°„çº¿è¾å°„çš„æœå¯»ç»“æœã€‚æ–‡ç« ä¸­è®¨è®ºäº†åŠ¨æ€å´©æºƒæ¨¡å‹äº§ç”Ÿçš„è‡ªå‘è¾å°„ï¼Œè¯¥æ¨¡å‹ä½œä¸ºè§£å†³é‡å­åŠ›å­¦é•¿æœŸæµ‹é‡é—®é¢˜çš„å¯èƒ½è§£å†³æ–¹æ¡ˆè¢«å¼•å…¥ã€‚åˆ†ææ—¶é‡‡ç”¨äº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é¦–æ¬¡è€ƒè™‘äº†å‘å°„å…‰è°±ä¸­çš„æŠµæ¶ˆæ•ˆåº”ï¼Œè¿™äº›æ•ˆåº”åœ¨Xå°„çº¿èŒƒå›´å†…äº§ç”Ÿäºæ°™åŸå­ä¸­çš„ç”µå­è´¨å­ç”µè·çš„å¯¹ç«‹ã€‚å¯¹æ–°è®¾å®šçš„Markovianè¿ç»­è‡ªå‘å®šä½æ¨¡å‹å’ŒDiâ€™osi-Penroseæ¨¡å‹è‡ªç”±å‚æ•°çš„ä¸–ç•Œé¢†å…ˆé™åˆ¶è¿›è¡Œäº†æ”¹å–„ï¼Œè¾ƒä¹‹å‰çš„æœ€ä½³çº¦æŸæé«˜äº†ä¸¤ä¸ªæ•°é‡çº§å’Œäº”å€ã€‚é¦–æ¬¡å®éªŒæ’é™¤äº†è¿ç»­è‡ªå‘å®šä½æ¨¡å‹çš„å¼ºåº¦å’Œå…³è”é•¿åº¦æå‡ºçš„åŸå§‹å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨XENONnTæš—ç‰©è´¨æ¢æµ‹å™¨çš„é¦–æ¬¡ç§‘å­¦è¿è¡Œæ•°æ®ï¼ŒæŠ¥å‘Šäº†å…³äºXå°„çº¿è¾å°„çš„é¢„æµ‹æœå¯»ç»“æœã€‚</li>
<li>åŠ¨æ€å´©æºƒæ¨¡å‹ä½œä¸ºè§£å†³é‡å­åŠ›å­¦æµ‹é‡é—®é¢˜çš„è§£å†³æ–¹æ¡ˆè¢«å¼•å…¥ï¼Œå¹¶è®¨è®ºäº†å…¶äº§ç”Ÿçš„è‡ªå‘è¾å°„ã€‚</li>
<li>é‡‡ç”¨äº†ä¸€ä¸ªè€ƒè™‘æ°™åŸå­ä¸­ç”µå­è´¨å­ç”µè·å¯¹ç«‹å¼•èµ·çš„å…‰è°±æŠµæ¶ˆæ•ˆåº”çš„æ¨¡å‹ã€‚</li>
<li>å¯¹Markovianè¿ç»­è‡ªå‘å®šä½æ¨¡å‹å’ŒDiâ€™osi-Penroseæ¨¡å‹çš„è‡ªç”±å‚æ•°è®¾å®šäº†æ–°çš„ä¸–ç•Œé¢†å…ˆé™åˆ¶ã€‚</li>
<li>è¾ƒä¹‹å‰çš„æœ€ä½³çº¦æŸï¼Œè¿™äº›æ–°é™åˆ¶åœ¨æ•°å€¼ä¸Šæœ‰äº†æ˜¾è‘—çš„æå‡ã€‚</li>
<li>é¦–æ¬¡å®éªŒæ’é™¤äº†è¿ç»­è‡ªå‘å®šä½æ¨¡å‹çš„æŸäº›åŸå§‹å‚æ•°å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05507">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88c8f70f18d6bd95ec33574c78381bc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3858d34e710476467c746d0eedd8407.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Statistical-microlocal-analysis-in-two-dimensional-X-ray-CT"><a href="#Statistical-microlocal-analysis-in-two-dimensional-X-ray-CT" class="headerlink" title="Statistical microlocal analysis in two-dimensional X-ray CT"></a>Statistical microlocal analysis in two-dimensional X-ray CT</h2><p><strong>Authors:Anuj Abhishek, Alexander Katsevich, James W. Webber</strong></p>
<p>In many imaging applications it is important to assess how well the edges of the original object, $f$, are resolved in an image, $f^\text{rec}$, reconstructed from the measured data, $g$. In this paper we consider the case of image reconstruction in 2D X-ray Computed Tomography (CT). Let $f$ be a function describing the object being scanned, and $g&#x3D;Rf + \eta$ be the Radon transform data in $\mathbb{R}^2$ corrupted by noise, $\eta$, and sampled with step size $\sim\epsilon$. Conventional microlocal analysis provides conditions for edge detectability based on the scanner geometry in the case of continuous, noiseless data (when $\eta &#x3D; 0$), but does not account for noise and finite sampling step size. We develop a novel technique called Statistical Microlocal Analysis (SMA), which uses a statistical hypothesis testing framework to determine if an image edge (singularity) of $f$ is detectable from $f^\text{rec}$, and we quantify edge detectability using the statistical power of the test. Our approach is based on the theory we developed in previous work, which provides a characterization of $f^\text{rec}$ in local $O(\epsilon)$-size neighborhoods when $\eta \neq 0$. We derive a statistical test for the presence and direction of an edge microlocally given the magnitude of $\eta$ and data sampling step size. Using the properties of the null distribution of the test, we quantify the uncertainty of the edge magnitude and direction. We validate our theory using simulations, which show strong agreement between our predictions and experimental observations. Our work is not only of practical value, but of theoretical value as well. SMA is a natural extension of classical microlocal analysis theory which accounts for practical measurement imperfections, such as noise and finite step size, at the highest possible resolution compatible with the data. </p>
<blockquote>
<p>åœ¨è®¸å¤šæˆåƒåº”ç”¨ä¸­ï¼Œè¯„ä¼°åŸå§‹å¯¹è±¡$f$çš„è¾¹ç¼˜åœ¨ç”±æµ‹é‡æ•°æ®$g$é‡å»ºçš„å›¾åƒ$f^\text{rec}$ä¸­æ¢å¤å¾—å¦‚ä½•éå¸¸é‡è¦ã€‚æœ¬æ–‡è€ƒè™‘äºŒç»´Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­çš„å›¾åƒé‡å»ºæƒ…å†µã€‚è®¾$f$ä¸ºæè¿°è¢«æ‰«æå¯¹è±¡çš„å‡½æ•°ï¼Œ$g&#x3D;Rf+\eta$ä¸ºå—å™ªå£°$\eta$å½±å“çš„Radonå˜æ¢æ•°æ®ï¼Œå¹¶ç”¨æ­¥é•¿$\sim\epsilon$è¿›è¡Œé‡‡æ ·ã€‚ä¼ ç»Ÿçš„å¾®å±€éƒ¨åˆ†æä¸ºè¿ç»­ã€æ— å™ªå£°æ•°æ®ï¼ˆå½“$\eta &#x3D; 0$æ—¶ï¼‰æä¾›äº†åŸºäºæ‰«æä»ªå‡ ä½•çš„è¾¹ç¼˜æ£€æµ‹æ¡ä»¶ï¼Œä½†ä¸è€ƒè™‘å™ªå£°å’Œæœ‰é™çš„é‡‡æ ·æ­¥é•¿ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºç»Ÿè®¡å¾®å±€éƒ¨åˆ†æï¼ˆSMAï¼‰çš„æ–°æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨ç»Ÿè®¡å‡è®¾æ£€éªŒæ¡†æ¶æ¥ç¡®å®š$f$çš„å›¾åƒè¾¹ç¼˜ï¼ˆå¥‡å¼‚æ€§ï¼‰æ˜¯å¦å¯ä»$f^\text{rec}$æ£€æµ‹å‡ºæ¥ï¼Œå¹¶ä½¿ç”¨æ£€éªŒçš„ç»Ÿè®¡æ•ˆåŠ›æ¥é‡åŒ–è¾¹ç¼˜æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œç†è®ºï¼Œå½“$\eta \neq 0$æ—¶ï¼Œå®ƒæä¾›äº†å¯¹å±€éƒ¨$O(\epsilon)$å¤§å°é‚»åŸŸä¸­çš„$f^\text{rec}$çš„ç‰¹å¾æè¿°ã€‚æˆ‘ä»¬æ ¹æ®$\eta$çš„å¹…åº¦å’Œæ•°æ®é‡‡æ ·æ­¥é•¿ï¼Œæ¨å¯¼å‡ºå±€éƒ¨è¾¹ç¼˜å­˜åœ¨çš„ç»Ÿè®¡æ£€éªŒåŠå…¶æ–¹å‘ã€‚åˆ©ç”¨æ£€éªŒçš„ç©ºåˆ†å¸ƒå±æ€§ï¼Œæˆ‘ä»¬é‡åŒ–è¾¹ç¼˜å¹…åº¦å’Œæ–¹å‘çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬é€šè¿‡æ¨¡æ‹ŸéªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºï¼Œæ¨¡æ‹Ÿç»“æœè¡¨æ˜æˆ‘ä»¬çš„é¢„æµ‹ä¸å®éªŒè§‚å¯Ÿç»“æœé«˜åº¦ä¸€è‡´ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ä»…å…·æœ‰å®ç”¨ä»·å€¼ï¼Œè€Œä¸”å…·æœ‰ç†è®ºä»·å€¼ã€‚SMAæ˜¯ç»å…¸å¾®å±€éƒ¨åˆ†æç†è®ºçš„è‡ªç„¶æ‰©å±•ï¼Œè¯¥ç†è®ºè€ƒè™‘äº†å®é™…æµ‹é‡ä¸­çš„ä¸å®Œç¾ä¹‹å¤„ï¼Œä¾‹å¦‚å™ªå£°å’Œæœ‰é™æ­¥é•¿ï¼Œå¹¶åœ¨ä¸æ•°æ®å…¼å®¹çš„æœ€é«˜å¯èƒ½åˆ†è¾¨ç‡ä¸‹è¿›è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05113v2">PDF</a> 27 pages, 13 figures</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºç»Ÿè®¡å¾®å±€éƒ¨åˆ†æï¼ˆSMAï¼‰çš„æ–°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨ç»Ÿè®¡å‡è®¾æ£€éªŒæ¡†æ¶æ¥ç¡®å®šä»é‡å»ºå›¾åƒä¸­æ˜¯å¦å¯æ£€æµ‹åˆ°åŸå§‹å¯¹è±¡çš„è¾¹ç¼˜ï¼ˆå¥‡ç‚¹ï¼‰ã€‚æ­¤æŠ€æœ¯è€ƒè™‘äº†å™ªå£°å’Œæœ‰é™é‡‡æ ·æ­¥é•¿ç­‰å®é™…æµ‹é‡è¯¯å·®çš„å½±å“ï¼Œä»è€Œæé«˜äº†è¾¹ç¼˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é€šè¿‡æ¨¡æ‹ŸéªŒè¯ï¼Œè¯¥æŠ€æœ¯é¢„æµ‹ä¸å®éªŒç»“æœé«˜åº¦ä¸€è‡´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…³æ³¨äºäºŒç»´Xå°„çº¿è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä¸­çš„å›¾åƒé‡å»ºé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”ç»Ÿè®¡å¾®å±€éƒ¨åˆ†æï¼ˆSMAï¼‰ï¼Œè¯¥æŠ€æœ¯ç”¨äºç¡®å®šä»é‡å»ºå›¾åƒä¸­æ˜¯å¦å¯æ£€æµ‹åˆ°åŸå§‹å¯¹è±¡çš„è¾¹ç¼˜ã€‚</li>
<li>SMAè€ƒè™‘äº†å™ªå£°å’Œæœ‰é™é‡‡æ ·æ­¥é•¿ç­‰å®é™…æµ‹é‡è¯¯å·®çš„å½±å“ï¼Œè¿™åœ¨æ­¤å‰çš„å¾®å±€éƒ¨åˆ†æä¸­å¹¶æœªæ¶‰åŠã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸéªŒè¯ï¼Œå±•ç¤ºäº†SMAé¢„æµ‹ä¸å®éªŒç»“æœçš„é«˜åº¦ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æŠ€æœ¯ä¸ä»…åœ¨å®è·µä¸­æœ‰ä»·å€¼ï¼Œè€Œä¸”åœ¨ç†è®ºä¸Šä¹Ÿæœ‰ä»·å€¼ï¼Œæ˜¯ç»å…¸å¾®å±€éƒ¨åˆ†æç†è®ºçš„è‡ªç„¶æ‰©å±•ã€‚</li>
<li>SMAæä¾›äº†ä¸€ç§é‡åŒ–è¾¹ç¼˜ä¸ç¡®å®šæ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è¾¹ç¼˜å¹…åº¦å’Œæ–¹å‘çš„ä¸ç¡®å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eaa43ac49c4dc0350ab79259f9a87eb3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visual-Text-Processing-A-Comprehensive-Review-and-Unified-Evaluation"><a href="#Visual-Text-Processing-A-Comprehensive-Review-and-Unified-Evaluation" class="headerlink" title="Visual Text Processing: A Comprehensive Review and Unified Evaluation"></a>Visual Text Processing: A Comprehensive Review and Unified Evaluation</h2><p><strong>Authors:Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, Xu-Cheng Yin, Nicu Sebe</strong></p>
<p>Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at <a target="_blank" rel="noopener" href="https://github.com/shuyansy/Visual-Text-Processing-survey">https://github.com/shuyansy/Visual-Text-Processing-survey</a>. </p>
<blockquote>
<p>è§†è§‰æ–‡æœ¬æ˜¯æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä¼ é€’äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶å¼•èµ·äº†è®¡ç®—æœºè§†è§‰ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚é™¤äº†æ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ç­‰ä¼ ç»Ÿä»»åŠ¡å¤–ï¼Œè§†è§‰æ–‡æœ¬å¤„ç†åœ¨åŸºç¡€æ¨¡å‹çš„æ¨åŠ¨ä¸‹è¿…é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ–‡æœ¬å›¾åƒæ“ä½œã€‚å°½ç®¡å–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†ç”±äºæ–‡æœ¬ä¸ä¸€èˆ¬ç‰©ä½“çš„ç‹¬ç‰¹å±æ€§ä¸åŒï¼Œä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ‰æ•ˆåœ°æ•è·å’Œåˆ©ç”¨è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾å¯¹äºå¼€å‘ç¨³å¥çš„è§†è§‰æ–‡æœ¬å¤„ç†æ¨¡å‹è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è§†è§‰æ–‡æœ¬å¤„ç†çš„æœ€æ–°è¿›å±•è¿›è¡Œäº†å…¨é¢ã€å¤šè§’åº¦çš„åˆ†æï¼Œé‡ç‚¹å›ç­”äº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰ä¸åŒçš„è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡æœ€é€‚åˆå“ªäº›æ–‡æœ¬ç‰¹å¾ï¼Ÿï¼ˆ2ï¼‰å¦‚ä½•æœ‰æ•ˆåœ°å°†è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾çº³å…¥å¤„ç†æ¡†æ¶ï¼Ÿæ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†VTPBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¹¿æ³›è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†çš„æ–°åŸºå‡†ã€‚æˆ‘ä»¬åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºäº†VTPScoreï¼Œä¸€ä¸ªæ—¨åœ¨ç¡®ä¿å…¬å¹³å¯é è¯„ä¼°çš„æ–°å‹è¯„ä»·æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯¹è¶…è¿‡20ç§ç‰¹å®šæ¨¡å‹çš„å®è¯ç ”ç©¶æ­ç¤ºäº†å½“å‰æŠ€æœ¯çš„å·¨å¤§æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†è¿™é¡¹å·¥ä½œå»ºç«‹ä¸ºè§†è§‰æ–‡æœ¬å¤„ç†è¿™ä¸€åŠ¨æ€é¢†åŸŸçš„åŸºæœ¬èµ„æºï¼Œä¿ƒè¿›æœªæ¥çš„æ¢ç´¢å’Œåˆ›æ–°ã€‚ç›¸å…³ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shuyansy/Visual-Text-Processing-survey%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/shuyansy/Visual-Text-Processing-surveyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21682v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰æ–‡æœ¬å¤„ç†çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬å…¶åœ¨æ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­çš„å…³é”®ä½œç”¨ï¼Œä»¥åŠåŸºç¡€æ¨¡å‹çš„å‡ºç°æ‰€å¸¦æ¥çš„æŠ€æœ¯è¿›æ­¥ã€‚æ–‡ç« å¯¹è¿‘æœŸè§†è§‰æ–‡æœ¬å¤„ç†çš„è¿›å±•è¿›è¡Œäº†å…¨é¢çš„å¤šè§†è§’åˆ†æï¼Œæå‡ºäº†ä¸¤ä¸ªå…³é”®é—®é¢˜å¹¶ä»‹ç»äº†VTPBenchæ–°åŸºå‡†å’ŒVTPScoreè¯„ä¼°æŒ‡æ ‡ã€‚æœ¬æ–‡æ—¨åœ¨æˆä¸ºä¿ƒè¿›è§†è§‰æ–‡æœ¬å¤„ç†é¢†åŸŸæœªæ¥æ¢ç´¢å’Œåˆ›æ–°çš„åŸºæœ¬èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ–‡æœ¬å¤„ç†æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é‡è¦åˆ†æ”¯ï¼Œæ¶‰åŠæ–‡æ¡£å’Œåœºæ™¯å›¾åƒä¸­çš„æ–‡æœ¬è¯†åˆ«å’Œè¯­ä¹‰ä¿¡æ¯æå–ã€‚</li>
<li>åŸºç¡€æ¨¡å‹çš„å‡ºç°æ¨åŠ¨äº†è§†è§‰æ–‡æœ¬å¤„ç†çš„å¿«é€Ÿå‘å±•ï¼ŒåŒ…æ‹¬æ–‡æœ¬å›¾åƒé‡å»ºå’Œæ–‡æœ¬å›¾åƒæ“ä½œç­‰ä»»åŠ¡ã€‚</li>
<li>æ–‡æœ¬ä¸é€šç”¨ç‰©ä½“ä¹‹é—´çš„å·®å¼‚ä½¿å¾—è§†è§‰æ–‡æœ¬å¤„ç†é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæœ‰æ•ˆæ•æ‰å’Œåˆ©ç”¨è¿™äº›ç‰¹å¾å¯¹äºå¼€å‘ç¨³å¥çš„æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>æ–‡ç« æå‡ºäº†ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šå“ªäº›æ–‡æœ¬ç‰¹å¾æœ€é€‚åˆä¸åŒçš„è§†è§‰æ–‡æœ¬å¤„ç†ä»»åŠ¡ï¼Œä»¥åŠå¦‚ä½•å°†è¿™äº›ç‹¬ç‰¹çš„æ–‡æœ¬ç‰¹å¾æœ‰æ•ˆåœ°èå…¥å¤„ç†æ¡†æ¶ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„åŸºå‡†VTPBenchï¼Œæ¶µç›–äº†å¹¿æ³›çš„è§†è§‰æ–‡æœ¬å¤„ç†æ•°æ®é›†ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›è§†è§‰è´¨é‡è¯„ä¼°èƒ½åŠ›ï¼Œæå‡ºäº†VTPScoreè¿™ä¸€æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ç¡®ä¿å…¬å¹³å¯é çš„è¯„ä»·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e6a1c4c5e087067fc792d6d8c35818c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8657c786a8dae4ecafb7370ee476921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f62cf1e080e5998dc17731b1eef14ea6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f83f4345ad4e0b8d493335a44549503.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce6b361c66b4d039193ab134a883dce.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒé‡å»ºé—®é¢˜ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œå¹¶å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ä¸€äº›ç ”ç©¶å°è¯•ä½¿ç”¨æ‰©æ•£æ¨¡å‹è§£å†³MRIé‡å»ºé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•ç›´æ¥åœ¨åƒç´ ç©ºé—´è¿›è¡Œæ“ä½œï¼Œå¯¼è‡´ä¼˜åŒ–å’Œæ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…·æœ‰ä¸°å¯Œçš„è§†è§‰å…ˆéªŒï¼Œæœ‰æœ›é€šè¿‡ä½ç»´æ½œåœ¨ç©ºé—´è§£å†³MRIé‡å»ºä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åŒ»å­¦ä¿çœŸåº¦çš„ç¼ºä¹æ˜ç¡®æ§åˆ¶æœºåˆ¶ï¼Œï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œï¼ˆ3ï¼‰æ½œåœ¨ç©ºé—´ä¸­çš„æ•°æ®ä¸€è‡´æ€§æœªå®šä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„LDPMæ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¸¦æœ‰ä¸¤æ­¥é‡å»ºç­–ç•¥çš„è‰å›¾å¼•å¯¼ç®¡é“ï¼Œå¹³è¡¡äº†æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ä¿çœŸåº¦ï¼›ï¼ˆ2ï¼‰ä¼˜åŒ–çš„MRIå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMR-VAEï¼‰ï¼Œåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šæ¯”SD-VAE \cite{sd}æé«˜äº†çº¦3.92 dBï¼›ï¼ˆ3ï¼‰åŒé˜¶æ®µé‡‡æ ·å™¨ï¼Œæ˜¯é—´éš”DDPMé‡‡æ ·å™¨çš„æ”¹è¿›ç‰ˆï¼Œå¼ºåˆ¶æ½œåœ¨ç©ºé—´çš„é«˜ä¿çœŸé‡å»ºã€‚åœ¨fastMRIæ•°æ®é›†\cite{fastmri}ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸­è¡¨ç°å‡ºç¨³å¥æ€§ã€‚é€šè¿‡æ¶ˆèå®éªŒä¹ŸéªŒè¯äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v3">PDF</a> accepted as oral presentation at EMBC 2025</p>
<p><strong>Summary</strong><br>     æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è§£å†³MRIé‡å»ºé—®é¢˜çš„å›¾åƒé‡å»ºåº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´ä¸‰ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–¹æ³•ï¼Œé€šè¿‡è‰å›¾å¼•å¯¼ç®¡é“ã€MRIä¼˜åŒ–çš„VAEå’ŒåŒé˜¶æ®µé‡‡æ ·å™¨ç­‰æŠ€æœ¯å®ç°å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé‡å»ºé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨MRIé‡å»ºä¸­ã€‚</li>
<li>ç›´æ¥åœ¨åƒç´ ç©ºé—´æ“ä½œä¼šå¯¼è‡´è¾ƒé«˜çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†æ•ˆç‡é™ä½ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹æœŸæœ›é€šè¿‡åœ¨ä½ç»´æ½œåœ¨ç©ºé—´æ“ä½œæ¥è§£å†³MRIé‡å»ºä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨MRIé‡å»ºä¸­é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåŒ»å­¦ä¿çœŸåº¦çš„ç¼ºä¹æ§åˆ¶æœºåˆ¶ã€è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠæ½œåœ¨ç©ºé—´ä¸­çš„æ•°æ®ä¸€è‡´æ€§æœªå®šä¹‰ã€‚</li>
<li>LDPMæ–¹æ³•é€šè¿‡è‰å›¾å¼•å¯¼ç®¡é“å’Œä¸¤æ­¥é‡å»ºç­–ç•¥å¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ä¿çœŸåº¦ã€‚</li>
<li>LDPMä½¿ç”¨MRIä¼˜åŒ–çš„VAEï¼Œåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šç›¸æ¯”SD-VAEæœ‰çº¦3.92dBçš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-59c1a174dbd1f401ef1c046c5604925b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SGD-Jittering-A-Training-Strategy-for-Robust-and-Accurate-Model-Based-Architectures"><a href="#SGD-Jittering-A-Training-Strategy-for-Robust-and-Accurate-Model-Based-Architectures" class="headerlink" title="SGD Jittering: A Training Strategy for Robust and Accurate Model-Based   Architectures"></a>SGD Jittering: A Training Strategy for Robust and Accurate Model-Based   Architectures</h2><p><strong>Authors:Peimeng Guan, Mark A. Davenport</strong></p>
<p>Inverse problems aim to reconstruct unseen data from corrupted or perturbed measurements. While most work focuses on improving reconstruction quality, generalization accuracy and robustness are equally important, especially for safety-critical applications. Model-based architectures (MBAs), such as loop unrolling methods, are considered more interpretable and achieve better reconstructions. Empirical evidence suggests that MBAs are more robust to perturbations than black-box solvers, but the accuracy-robustness tradeoff in MBAs remains underexplored. In this work, we propose a simple yet effective training scheme for MBAs, called SGD jittering, which injects noise iteration-wise during reconstruction. We theoretically demonstrate that SGD jittering not only generalizes better than the standard mean squared error training but is also more robust to average-case attacks. We validate SGD jittering using denoising toy examples, seismic deconvolution, and single-coil MRI reconstruction. Both SGD jittering and its SPGD extension yield cleaner reconstructions for out-of-distribution data and demonstrates enhanced robustness against adversarial attacks. </p>
<blockquote>
<p>é€†å‘é—®é¢˜æ—¨åœ¨ä»è¢«æŸåæˆ–å—åˆ°å¹²æ‰°çš„æµ‹é‡ç»“æœä¸­é‡å»ºæœªè§æ•°æ®ã€‚è™½ç„¶å¤§å¤šæ•°å·¥ä½œéƒ½é›†ä¸­åœ¨æé«˜é‡å»ºè´¨é‡ä¸Šï¼Œä½†æ³›åŒ–ç²¾åº¦å’Œç¨³å¥æ€§åŒæ ·é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹å®‰å…¨æ€§è¦æ±‚ä¸¥æ ¼çš„åº”ç”¨ä¸­ã€‚åŸºäºæ¨¡å‹çš„æ¶æ„ï¼ˆå¦‚å±•å¼€å¾ªç¯æ–¹æ³•ï¼‰è¢«è®¤ä¸ºæ›´å…·å¯è§£é‡Šæ€§ï¼Œå¹¶å¯å®ç°æ›´å¥½çš„é‡å»ºã€‚ç»éªŒè¯æ®è¡¨æ˜ï¼ŒåŸºäºæ¨¡å‹çš„æ¶æ„å¯¹å¹²æ‰°çš„é²æ£’æ€§ä¼˜äºé»‘ç®±æ±‚è§£å™¨ï¼Œä½†å¯¹åŸºäºæ¨¡å‹çš„æ¶æ„ä¸­çš„ç²¾åº¦ç¨³å¥æƒè¡¡ä»ç„¶ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŸºäºæ¨¡å‹çš„æ¶æ„çš„ç®€å•æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆï¼Œç§°ä¸ºSGDæŠ–åŠ¨ï¼Œè¯¥æ–¹æ¡ˆåœ¨é‡å»ºè¿‡ç¨‹ä¸­é€æ­¥æ³¨å…¥å™ªå£°ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†SGDæŠ–åŠ¨ä¸ä»…æ³›åŒ–æ€§èƒ½ä¼˜äºæ ‡å‡†å‡æ–¹è¯¯å·®è®­ç»ƒï¼Œè€Œä¸”å¯¹å¹³å‡æƒ…å†µä¸‹çš„æ”»å‡»æ›´å…·é²æ£’æ€§ã€‚æˆ‘ä»¬é€šè¿‡å»å™ªç©å…·ç¤ºä¾‹ã€åœ°éœ‡åå·ç§¯å’Œå•çº¿åœˆMRIé‡å»ºéªŒè¯äº†SGDæŠ–åŠ¨çš„æœ‰æ•ˆæ€§ã€‚SGDæŠ–åŠ¨åŠå…¶SPGDæ‰©å±•éƒ½èƒ½ä¸ºè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®æä¾›æ›´æ¸…æ™°çš„é‡å»ºç»“æœï¼Œå¹¶æ˜¾ç¤ºå‡ºå¢å¼ºçš„å¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14667v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é€†é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå³å¦‚ä½•ä»è¢«æ±¡æŸ“æˆ–å¹²æ‰°çš„æµ‹é‡å€¼ä¸­é‡å»ºæœªçŸ¥æ•°æ®ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å¤§å¤šæ•°å·¥ä½œé›†ä¸­åœ¨æé«˜é‡å»ºè´¨é‡ä¸Šï¼Œä½†é€šç”¨ç²¾åº¦å’Œç¨³å¥æ€§å¯¹äºå®‰å…¨å…³é”®åº”ç”¨åŒæ ·é‡è¦ã€‚åŸºäºæ¨¡å‹çš„æ¶æ„ï¼ˆMBAsï¼‰å¦‚å¾ªç¯å±•å¼€æ–¹æ³•è¢«è®¤ä¸ºæ›´å…·å¯è§£é‡Šæ€§ï¼Œå¹¶èƒ½å®ç°æ›´å¥½çš„é‡å»ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹MBAsçš„ç®€å•æœ‰æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆâ€”â€”SGDæŠ–åŠ¨ï¼Œè¯¥æ–¹æ¡ˆåœ¨é‡å»ºè¿‡ç¨‹ä¸­é€æ¬¡æ³¨å…¥å™ªå£°ã€‚ç†è®ºä¸Šï¼ŒSGDæŠ–åŠ¨ä¸ä»…æ¯”æ ‡å‡†çš„å‡æ–¹è¯¯å·®è®­ç»ƒå…·æœ‰æ›´å¥½çš„é€šç”¨æ€§ï¼Œè€Œä¸”å¯¹å¹³å‡æƒ…å†µä¸‹çš„æ”»å‡»ä¹Ÿå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚é€šè¿‡å»å™ªç©å…·ç¤ºä¾‹ã€åœ°éœ‡è§£å·ç§¯å’Œå•çº¿åœˆMRIé‡å»ºéªŒè¯äº†SGDæŠ–åŠ¨çš„æœ‰æ•ˆæ€§ã€‚SGDæŠ–åŠ¨åŠå…¶SPGDæ‰©å±•éƒ½èƒ½ä¸ºè¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„æ•°æ®æä¾›æ›´æ¸…æ™°çš„é‡å»ºç»“æœï¼Œå¹¶æ˜¾ç¤ºå‡ºå¯¹æŠ—æ”»å‡»çš„å¢å¼ºé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€†é—®é¢˜æ—¨åœ¨ä»å—å¹²æ‰°çš„æµ‹é‡å€¼ä¸­é‡å»ºæœªçŸ¥æ•°æ®ã€‚</li>
<li>æé«˜é‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œä¹Ÿéœ€è¦å…³æ³¨é€šç”¨ç²¾åº¦å’Œç¨³å¥æ€§ã€‚</li>
<li>åŸºäºæ¨¡å‹çš„æ¶æ„ï¼ˆMBAsï¼‰å¦‚å¾ªç¯å±•å¼€æ–¹æ³•å…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§å’Œé‡å»ºæ•ˆæœã€‚</li>
<li>SGDæŠ–åŠ¨æ˜¯ä¸€ç§é’ˆå¯¹MBAsçš„æœ‰æ•ˆè®­ç»ƒæ–¹æ¡ˆï¼Œèƒ½åœ¨é‡å»ºè¿‡ç¨‹ä¸­é€æ¬¡æ³¨å…¥å™ªå£°ã€‚</li>
<li>SGDæŠ–åŠ¨ç†è®ºä¸Šå…·æœ‰æ›´å¥½çš„é€šç”¨æ€§å’Œå¯¹å¹³å‡æ”»å‡»çš„é²æ£’æ€§ã€‚</li>
<li>é€šè¿‡å»å™ªç©å…·ç¤ºä¾‹ã€åœ°éœ‡è§£å·ç§¯å’Œå•çº¿åœˆMRIé‡å»ºéªŒè¯äº†SGDæŠ–åŠ¨çš„å®é™…æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-318039766b994fbb18d6b5e5316aa645.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd34cd6a1727d693edd8e22ad199b576.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="VisionTS-Visual-Masked-Autoencoders-Are-Free-Lunch-Zero-Shot-Time-Series-Forecasters"><a href="#VisionTS-Visual-Masked-Autoencoders-Are-Free-Lunch-Zero-Shot-Time-Series-Forecasters" class="headerlink" title="VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time   Series Forecasters"></a>VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time   Series Forecasters</h2><p><strong>Authors:Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, Chenghao Liu</strong></p>
<p>Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either repurpose large language models (LLMs) or build large-scale time series datasets to develop TSF foundation models for universal forecasting. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. This paper explores a new road to building a TSF foundation model from rich, high-quality natural images. Our key insight is that a visual masked autoencoder, pre-trained on the ImageNet dataset, can naturally be a numeric series forecaster. By reformulating TSF as an image reconstruction task, we bridge the gap between image pre-training and TSF downstream tasks. Surprisingly, without further adaptation in the time series domain, the proposed VisionTS could achieve better zero-shot forecast performance than existing TSF foundation models. With fine-tuning for one epoch, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases. Extensive experiments reveal intrinsic similarities between images and real-world time series, suggesting that visual models may offer a â€œfree lunchâ€ for TSF and highlight the potential for future cross-modality research. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Keytoyze/VisionTS">https://github.com/Keytoyze/VisionTS</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­ï¼ŒåŸºç¡€æ¨¡å‹ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ã€‚ç°æœ‰çš„æ–¹æ³•è¦ä¹ˆé‡æ–°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¦ä¹ˆæ„å»ºå¤§è§„æ¨¡æ—¶é—´åºåˆ—æ•°æ®é›†ï¼Œä»¥å¼€å‘ç”¨äºé€šç”¨é¢„æµ‹çš„æ—¶é—´åºåˆ—é¢„æµ‹åŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç€è·¨åŸŸå·®è·ä¸¥é‡æˆ–é¢†åŸŸå†…éƒ¨å¼‚è´¨æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€æ¡æ–°çš„é“è·¯ï¼Œä»ä¸°å¯Œã€é«˜è´¨é‡çš„è‡ªç„¶å›¾åƒä¸­æ„å»ºTSFåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œåœ¨ImageNetæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„å¯è§†åŒ–æ©ç è‡ªåŠ¨ç¼–ç å™¨å¯ä»¥è‡ªç„¶åœ°æˆä¸ºæ•°å€¼åºåˆ—é¢„æµ‹å™¨ã€‚é€šè¿‡å°†TSFé‡æ–°æ„å»ºä¸ºå›¾åƒé‡å»ºä»»åŠ¡ï¼Œæˆ‘ä»¬å¼¥åˆäº†å›¾åƒé¢„è®­ç»ƒå’ŒTSFä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„å·®è·ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ— éœ€åœ¨æ—¶é—´åºåˆ—é¢†åŸŸè¿›ä¸€æ­¥é€‚åº”ï¼Œæ‰€æå‡ºçš„VisionTSå¯ä»¥å®ç°åœ¨ç°æœ‰TSFåŸºç¡€æ¨¡å‹ä¸Šæ›´å¥½çš„é›¶æ ·æœ¬é¢„æµ‹æ€§èƒ½ã€‚é€šè¿‡å¾®è°ƒä¸€ä¸ªå‘¨æœŸï¼ŒVisionTSå¯ä»¥è¿›ä¸€æ­¥æé«˜é¢„æµ‹èƒ½åŠ›ï¼Œå¹¶åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚å¤§é‡å®éªŒæ­ç¤ºäº†å›¾åƒå’Œç°å®ä¸–ç•Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å†…åœ¨ç›¸ä¼¼æ€§ï¼Œè¿™è¡¨æ˜è§†è§‰æ¨¡å‹å¯èƒ½ä¸ºTSFæä¾›äº†â€œå…è´¹åˆé¤â€ï¼Œå¹¶çªå‡ºäº†æœªæ¥è·¨æ¨¡æ€ç ”ç©¶çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Keytoyze/VisionTS%E3%80%82">https://github.com/Keytoyze/VisionTSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.17253v4">PDF</a> v4: accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æ–°çš„æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰åŸºç¡€æ¨¡å‹æ„å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä¸°å¯Œã€é«˜è´¨é‡çš„è‡ªç„¶å›¾åƒè¿›è¡Œé¢„è®­ç»ƒã€‚ç ”ç©¶äººå‘˜å‘ç°ï¼Œé€šè¿‡åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸­é‡æ–°æ„å»ºæ—¶é—´åºåˆ—é¢„æµ‹ï¼Œå¯ä»¥ä½¿ç”¨åœ¨ImageNetæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„è§†è§‰æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œæ•°å€¼åºåˆ—é¢„æµ‹ã€‚è¯¥æ–¹æ³•æ— éœ€è¿›ä¸€æ­¥é€‚åº”æ—¶é—´åºåˆ—é¢†åŸŸï¼Œå³å¯å®ç°é›¶æ¬¡é¢„æµ‹æ€§èƒ½ä¼˜äºç°æœ‰TSFåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¯é€šè¿‡å¾®è°ƒè¿›ä¸€æ­¥æå‡é¢„æµ‹æ€§èƒ½ã€‚ç ”ç©¶æ­ç¤ºäº†å›¾åƒå’Œç°å®ä¸–ç•Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å†…åœ¨è”ç³»ï¼Œä¸ºæœªæ¥çš„è·¨æ¨¡æ€ç ”ç©¶æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰åŸºç¡€æ¨¡å‹æ„å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨è‡ªç„¶å›¾åƒè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
<li>ç ”ç©¶äººå‘˜å‘ç°ï¼Œè§†è§‰æ©ç è‡ªåŠ¨ç¼–ç å™¨å¯ä»¥è‡ªç„¶åœ°è¿›è¡Œæ•°å€¼åºåˆ—é¢„æµ‹ï¼Œè¿™æ˜¯é€šè¿‡å°†å…¶é‡æ–°æ„å»ºä¸ºå›¾åƒé‡å»ºä»»åŠ¡å®ç°çš„ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è¿›ä¸€æ­¥é€‚åº”æ—¶é—´åºåˆ—é¢†åŸŸï¼Œå³å¯å®ç°é›¶æ¬¡é¢„æµ‹æ€§èƒ½ä¼˜äºç°æœ‰TSFåŸºç¡€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒï¼Œè¯¥æ–¹æ³•çš„é¢„æµ‹æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œè¾¾åˆ°å¤§å¤šæ•°æƒ…å†µä¸‹çš„æœ€ä½³æ°´å¹³ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å›¾åƒå’Œç°å®ä¸–ç•Œæ—¶é—´åºåˆ—ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚</li>
<li>è§†è§‰æ¨¡å‹å¯èƒ½ä¸ºæ—¶é—´åºåˆ—é¢„æµ‹æä¾›â€œå…è´¹åˆé¤â€ï¼Œè¿™è¡¨æ˜è·¨æ¨¡æ€ç ”ç©¶çš„æ½œåŠ›å·¨å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.17253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a74b81d9bfa52fb6eead35cd76210e06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-349a9556e2cede7debc09dd2743444e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb5c0b435e6e5b98673d198a1f101ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72317b37a3e7f286cb188771fb63e0ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-633f62e585ba0aa0c9cdedaac39ad1e6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HilbertMamba-Local-Global-Reciprocal-Network-for-Uterine-Fibroid-Segmentation-in-Ultrasound-Videos"><a href="#HilbertMamba-Local-Global-Reciprocal-Network-for-Uterine-Fibroid-Segmentation-in-Ultrasound-Videos" class="headerlink" title="HilbertMamba: Local-Global Reciprocal Network for Uterine Fibroid   Segmentation in Ultrasound Videos"></a>HilbertMamba: Local-Global Reciprocal Network for Uterine Fibroid   Segmentation in Ultrasound Videos</h2><p><strong>Authors:Huihui Xu, Yijun Yang, Angelica I Aviles-Rivero, Guang Yang, Jing Qin, Lei Zhu</strong></p>
<p>Regular screening and early discovery of uterine fibroid are crucial for preventing potential malignant transformations and ensuring timely, life-saving interventions. To this end, we collect and annotate the first ultrasound video dataset with 100 videos for uterine fibroid segmentation (UFUV). We also present Local-Global Reciprocal Network (LGRNet) to efficiently and effectively propagate the long-term temporal context which is crucial to help distinguish between uninformative noisy surrounding tissues and target lesion regions. Specifically, the Cyclic Neighborhood Propagation (CNP) is introduced to propagate the inter-frame local temporal context in a cyclic manner. Moreover, to aggregate global temporal context, we first condense each frame into a set of frame bottleneck queries and devise Hilbert Selective Scan (HilbertSS) to both efficiently path connect each frame and preserve the locality bias. A distribute layer is then utilized to disseminate back the global context for reciprocal refinement. Extensive experiments on UFUV and three public Video Polyp Segmentation (VPS) datasets demonstrate consistent improvements compared to state-of-the-art segmentation methods, indicating the effectiveness and versatility of LGRNet. Code, checkpoints, and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/bio-mlhui/LGRNet">https://github.com/bio-mlhui/LGRNet</a> </p>
<blockquote>
<p>å®šæœŸç­›æŸ¥å’Œæ—©æœŸå‘ç°å­å®«çº¤ç»´ç˜¤å¯¹äºé¢„é˜²æ½œåœ¨çš„æ¶æ€§è½¬åŒ–å’Œç¡®ä¿åŠæ—¶æŒ½æ•‘ç”Ÿå‘½çš„å¹²é¢„è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ”¶é›†å’Œæ ‡æ³¨äº†ç¬¬ä¸€ä¸ªåŒ…å«100ä¸ªè§†é¢‘çš„å­å®«çº¤ç»´ç˜¤åˆ†å‰²è¶…å£°è§†é¢‘æ•°æ®é›†ï¼ˆUFUVï¼‰ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å±€éƒ¨å…¨å±€äº’æƒ ç½‘ç»œï¼ˆLGRNetï¼‰ï¼Œä»¥é«˜æ•ˆä¸”æœ‰æ•ˆåœ°ä¼ æ’­é•¿æœŸæ—¶é—´ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºåŒºåˆ†å‘¨å›´æ— ä¿¡æ¯å™ªå£°ç»„ç»‡å’Œç›®æ ‡ç—…å˜åŒºåŸŸè‡³å…³é‡è¦ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥äº†å¾ªç¯é‚»åŸŸä¼ æ’­ï¼ˆCNPï¼‰ï¼Œä»¥å¾ªç¯æ–¹å¼ä¼ æ’­å¸§é—´å±€éƒ¨æ—¶é—´ä¸Šä¸‹æ–‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†èšåˆå…¨å±€æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ¯ä¸€å¸§æµ“ç¼©ä¸ºä¸€ç»„å¸§ç“¶é¢ˆæŸ¥è¯¢ï¼Œå¹¶è®¾è®¡Hilberté€‰æ‹©æ€§æ‰«æï¼ˆHilbertSSï¼‰ä»¥æœ‰æ•ˆåœ°è¿æ¥æ¯ä¸€å¸§å¹¶ä¿æŒå±€éƒ¨åå‘æ€§ã€‚ç„¶ååˆ©ç”¨åˆ†å¸ƒå±‚å°†å…¨å±€ä¸Šä¸‹æ–‡å›ä¼ ç»™æ¯ä¸€å¸§è¿›è¡Œäº’æƒ ç»†åŒ–ã€‚åœ¨UFUVå’Œä¸‰ä¸ªå…¬å…±è§†é¢‘æ¯è‚‰åˆ†å‰²ï¼ˆVPSï¼‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ç›¸æ¯”ï¼ŒLGRNetå…·æœ‰ä¸€è‡´çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚ä»£ç ã€æ£€æŸ¥ç‚¹å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bio-mlhui/LGRNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bio-mlhui/LGRNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.05703v2">PDF</a> MICCAI2024 Early Accept</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒå®šæœŸç­›æŸ¥å’Œæ—©æœŸå‘ç°å­å®«çº¤ç»´ç˜¤çš„é‡è¦æ€§ï¼Œä»¥é˜²æ­¢æ½œåœ¨çš„æ¶æ€§è½¬åŒ–å¹¶ç¡®ä¿åŠæ—¶çš„ç”Ÿå‘½æ‹¯æ•‘å¹²é¢„ã€‚ä¸ºæ­¤ï¼Œä½œè€…æ”¶é›†å’Œæ ‡æ³¨äº†ç¬¬ä¸€ä¸ªå­å®«çº¤ç»´ç˜¤åˆ†å‰²è¶…å£°è§†é¢‘æ•°æ®é›†UFUVï¼ŒåŒ…å«100ä¸ªè§†é¢‘ã€‚æ­¤å¤–ï¼Œä½œè€…æå‡ºäº†Local-Global Reciprocal Networkï¼ˆLGRNetï¼‰ï¼Œè¯¥ç½‘ç»œèƒ½æœ‰æ•ˆä¼ æ’­é•¿æœŸæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ‰åŠ©äºåŒºåˆ†æ— ä¿¡æ¯å¹²æ‰°çš„å‘¨å›´ç»„ç»‡å’Œç›®æ ‡ç—…å˜åŒºåŸŸã€‚ç½‘ç»œåŒ…å«Cyclic Neighborhood Propagationï¼ˆCNPï¼‰å’ŒHilbert Selective Scanï¼ˆHilbertSSï¼‰ï¼Œåˆ†åˆ«ç”¨äºå¾ªç¯ä¼ æ’­å±€éƒ¨æ—¶é—´ä¸Šä¸‹æ–‡å’Œé«˜æ•ˆè·¯å¾„è¿æ¥æ¯ä¸€å¸§å¹¶ä¿æŒå±€éƒ¨åè§ã€‚æœ€ç»ˆï¼Œé€šè¿‡ä¸ç°æœ‰é¡¶çº§åˆ†å‰²æ–¹æ³•åœ¨å¤šæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒæ¯”è¾ƒï¼Œæ˜¾ç¤ºäº†LGRNetçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®šæœŸç­›æŸ¥å’Œæ—©æœŸå‘ç°å­å®«çº¤ç»´ç˜¤è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºé¢„é˜²æ¶æ€§è½¬åŒ–å¹¶ä¿è¯åŠæ—¶å¹²é¢„ã€‚</li>
<li>ä½œè€…åˆ›å»ºäº†ç¬¬ä¸€ä¸ªç”¨äºå­å®«çº¤ç»´ç˜¤åˆ†å‰²çš„è¶…å£°è§†é¢‘æ•°æ®é›†UFUVï¼ŒåŒ…å«100ä¸ªè§†é¢‘ã€‚</li>
<li>æå‡ºäº†Local-Global Reciprocal Networkï¼ˆLGRNetï¼‰ä»¥æœ‰æ•ˆä¼ æ’­é•¿æœŸæ—¶é—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>CNPå’ŒHilbertSSæ˜¯LGRNetä¸­çš„å…³é”®ç»„ä»¶ï¼Œåˆ†åˆ«è´Ÿè´£å±€éƒ¨æ—¶é—´ä¸Šä¸‹æ–‡çš„å¾ªç¯ä¼ æ’­å’Œé«˜æ•ˆå¸§é—´è·¯å¾„è¿æ¥ã€‚</li>
<li>ä½¿ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¼ æ’­ä»¥è¿›è¡Œé€’å½’ä¼˜åŒ–ã€‚</li>
<li>åœ¨UFUVå’Œä¸‰ä¸ªå…¬å…±è§†é¢‘æ¯è‚‰åˆ†å‰²æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLGRNetç›¸è¾ƒäºç°æœ‰é¡¶çº§åˆ†å‰²æ–¹æ³•æœ‰æ‰€æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.05703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0537065a9afeb93eeb3e451eb3c8509f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c476801f4fb68c5ee491a5b82ae04247.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d828524eca67d0ed9e8c925d181b0170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a310b1229860ddb48f9c8b3aa4259ba3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6b8836d2c08121a0dd2296d5845f8123.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Kinetics Rethinking Test-Time Scaling Laws
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fdef7621a4b0601a72c49b8fe875c298.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  STARFlow Scaling Latent Normalizing Flows for High-resolution Image   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
