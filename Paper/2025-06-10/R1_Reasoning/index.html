<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-10  PuzzleWorld A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    85 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-10-更新"><a href="#2025-06-10-更新" class="headerlink" title="2025-06-10 更新"></a>2025-06-10 更新</h1><h2 id="PuzzleWorld-A-Benchmark-for-Multimodal-Open-Ended-Reasoning-in-Puzzlehunts"><a href="#PuzzleWorld-A-Benchmark-for-Multimodal-Open-Ended-Reasoning-in-Puzzlehunts" class="headerlink" title="PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts"></a>PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts</h2><p><strong>Authors:Hengzhi Li, Brendon Jiang, Alexander Naehu, Regan Song, Justin Zhang, Megan Tjandrasuwita, Chanakya Ekbote, Steven-Shine Chen, Adithya Balachandran, Wei Dai, Rebecca Chang, Paul Pu Liang</strong></p>
<p>Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined problem definitions. In contrast to conventional reasoning benchmarks consisting of tasks with clear instructions, puzzlehunts require models to discover the underlying problem structure from multimodal evidence and iterative reasoning, mirroring real-world domains such as scientific discovery, exploratory data analysis, or investigative problem-solving. Despite recent progress in foundation models, their performance on such open-ended settings remains largely untested. In this paper, we introduce PuzzleWorld, a large-scale benchmark of 667 puzzlehunt-style problems designed to assess step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is annotated with the final solution, detailed reasoning traces, and cognitive skill labels, enabling holistic benchmarking and fine-grained diagnostic analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy, with the best model solving only 14% of puzzles and reaching 40% stepwise accuracy. To demonstrate the value of our reasoning annotations, we show that fine-tuning a small model on reasoning traces improves stepwise reasoning from 4% to 11%, while training on final answers alone degrades performance to near zero. Our error analysis reveals that current models exhibit myopic reasoning, are bottlenecked by the limitations of language-based inference, and lack sketching capabilities crucial for visual and spatial reasoning. We release PuzzleWorld at <a target="_blank" rel="noopener" href="https://github.com/MIT-MI/PuzzleWorld">https://github.com/MIT-MI/PuzzleWorld</a> to support future work on building more general, open-ended, and creative reasoning systems. </p>
<blockquote>
<p>谜题狩猎是一种缺乏明确问题定义的复杂多步骤谜题。与传统的包含明确指令的任务基准测试相比，谜题狩猎要求模型从多模式证据和迭代推理中发现潜在的问题结构，这反映了现实世界领域，如科学发现、探索性数据分析或调查解决问题。尽管基础模型最近有进展，但它们在这样开放环境设置中的表现仍主要未经验证。在本文中，我们介绍了 PuzzleWorld，这是一个包含 667 个谜题狩猎风格问题的大规模基准测试，旨在评估分步、开放性和创造性多模式推理。每个谜题都附有最终解决方案、详细的推理轨迹和认知技能标签，以实现整体基准测试和精细的诊断分析。最先进的模型最终答案准确率只有 1-2%，最佳模型解决的谜题只有 14%，分步准确率也只有 40%。为了证明我们的推理注释的价值，我们展示了在推理轨迹上微调的小型模型可以改进分步推理，从 4% 提高到 11%，而仅在最终答案上进行训练会损害性能，近乎为零。我们的错误分析表明，当前模型的推理存在视野狭隘的问题，受到基于语言的推理的限制，并且在视觉和空间推理方面缺乏草图绘制能力。我们在 <a target="_blank" rel="noopener" href="https://github.com/MIT-MI/PuzzleWorld">https://github.com/MIT-MI/PuzzleWorld</a> 发布了 PuzzleWorld，以支持未来关于构建更通用、开放性和创造性推理系统的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06211v1">PDF</a> </p>
<p><strong>Summary</strong><br>     谜题狩猎是一种缺乏明确问题定义的多步骤复杂谜题类型。与传统推理基准测试不同，谜题狩猎要求模型从多模式证据中发现潜在的问题结构，并进行迭代推理，这在现实世界的应用领域尤为普遍，如科学发现、探索性数据分析和问题解决。新发布的谜题世界（PuzzleWorld）大型基准测试包含设计用于评估开放式创造性多模式推理的667个谜题狩猎问题。然而，大多数先进模型在最后答案的准确性上只有百分之几的成绩，表明他们在解决开放式问题时面临巨大挑战。研究者提出使用推理痕迹训练模型可提高推理的准确性。当前模型显示出在开放和创意推理上的不足和瓶颈问题。<strong>Key Takeaways</strong></p>
<p> 谜题狩猎是一种涉及复杂多步骤谜题的活动，缺乏明确的问题定义。<br> 这些谜题要求模型从多模式证据中发现潜在的问题结构，进行迭代推理，符合现实世界情境的应用场景包括科学发现、探索性数据分析和问题解决等。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06211">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-86cb4afa9805196e0d4b14d712b8ae41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46aac697f2a476e51bdb999eada79514.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fcbc6c4659e6431c53dc282ad8ff0fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4870b6df18e5587202db31ba18fcb2be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-133776caf04c55676a0d19862209dbd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1d7a4b90fa073ee91047e754cf51c42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdbac4d1b7f51fd7faf03d3522e4d2d8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ICU-TSB-A-Benchmark-for-Temporal-Patient-Representation-Learning-for-Unsupervised-Stratification-into-Patient-Cohorts"><a href="#ICU-TSB-A-Benchmark-for-Temporal-Patient-Representation-Learning-for-Unsupervised-Stratification-into-Patient-Cohorts" class="headerlink" title="ICU-TSB: A Benchmark for Temporal Patient Representation Learning for   Unsupervised Stratification into Patient Cohorts"></a>ICU-TSB: A Benchmark for Temporal Patient Representation Learning for   Unsupervised Stratification into Patient Cohorts</h2><p><strong>Authors:Dimitrios Proios, Alban Bornet, Anthony Yazdani, Jose F Rodrigues Jr, Douglas Teodoro</strong></p>
<p>Patient stratification identifying clinically meaningful subgroups is essential for advancing personalized medicine through improved diagnostics and treatment strategies. Electronic health records (EHRs), particularly those from intensive care units (ICUs), contain rich temporal clinical data that can be leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal Stratification Benchmark), the first comprehensive benchmark for evaluating patient stratification based on temporal patient representation learning using three publicly available ICU EHR datasets. A key contribution of our benchmark is a novel hierarchical evaluation framework utilizing disease taxonomies to measure the alignment of discovered clusters with clinically validated disease groupings. In our experiments with ICU-TSB, we compared statistical methods and several recurrent neural networks, including LSTM and GRU, for their ability to generate effective patient representations for subsequent clustering of patient trajectories. Our results demonstrate that temporal representation learning can rediscover clinically meaningful patient cohorts; nevertheless, it remains a challenging task, with v-measuring varying from up to 0.46 at the top level of the taxonomy to up to 0.40 at the lowest level. To further enhance the practical utility of our findings, we also evaluate multiple strategies for assigning interpretable labels to the identified clusters. The experiments and benchmark are fully reproducible and available at <a target="_blank" rel="noopener" href="https://github.com/ds4dh/CBMS2025stratification">https://github.com/ds4dh/CBMS2025stratification</a>. </p>
<blockquote>
<p>患者分层识别具有临床意义的亚组是推动个性化医学发展的关键，通过改进的诊断和治疗策略来实现。电子健康记录（EHRs），特别是来自重症监护室（ICUs）的，包含丰富的时序临床数据，可以为此目的而加以利用。在这项工作中，我们介绍了ICU-TSB（时序分层基准），这是基于时序患者表示学习评估患者分层的第一个全面基准，使用了三个公开可用的ICU EHR数据集。我们基准的关键贡献在于采用疾病分类法的新型层次评估框架，以衡量发现集群与临床验证的疾病分组之间的对齐程度。在我们的ICU-TSB实验中，我们比较了统计方法和几种循环神经网络（包括LSTM和GRU）生成有效患者表示的能力，以便对随后的患者轨迹进行聚类。结果表明，时序表示学习能够重新发现具有临床意义的患者群体；然而，这仍然是一项具有挑战性的任务，在分类等级的最高层次上v值最高达到0.46，而在最低层次上最高达到0.40。为了进一步提高我们研究结果的实际效用，我们还评估了为已识别的集群分配可解释标签的多种策略。实验和基准都是完全可复制的，可在<a target="_blank" rel="noopener" href="https://github.com/ds4dh/CBMS2025stratification%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ds4dh/CBMS2025stratification找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06192v1">PDF</a> 6 pages 1 table 6 figures</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了ICU-TSB（基于时序分层基准）的重要性，它是利用重症监护病房电子健康记录（EHRs）数据进行患者分层的首个全面基准。该基准通过利用疾病分类法，采用层次评估框架来衡量发现的聚类与临床验证的疾病分组的对齐程度。实验结果显示，时序表示学习能够重新发现具有临床意义的患者群体，但仍面临挑战，在分类的最高和最低级别上得分不同。为提高结果的实用性，作者还评估了为识别出的聚类分配可解释标签的多种策略。该实验和基准测试完全可重现，并提供了一个在线链接以供进一步了解。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>患者分层对于推动个性化医学至关重要，有助于改进诊断和治疗策略。</li>
<li>ICU-TSB是首个利用重症监护病房（ICU）电子健康记录（EHRs）数据进行患者分层的全面基准。</li>
<li>ICU-TSB采用层次评估框架，结合疾病分类法来衡量发现的聚类与临床验证的疾病分组的匹配程度。</li>
<li>时序表示学习能够发现具有临床意义的患者群体，但分层仍具有挑战性。</li>
<li>在分类的最高和最低级别上，评估结果存在差异，需要进一步研究和改进。</li>
<li>为提高结果的实用性，作者探索了多种策略来为识别出的聚类分配可解释的标签。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06192">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f172cb307045ef8974bb53f57cedd36e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4310f8db48905463ddafe946c4c63c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1065586abe52320f3fdcff70eaf97799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a927a5b254c712e9ca2c9ac12d0d9a3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d96d61b24e60e96f1e9620b02b25017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d56e522b8ddeb99b7848508ef585d850.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Table-r1-Self-supervised-and-Reinforcement-Learning-for-Program-based-Table-Reasoning-in-Small-Language-Models"><a href="#Table-r1-Self-supervised-and-Reinforcement-Learning-for-Program-based-Table-Reasoning-in-Small-Language-Models" class="headerlink" title="Table-r1: Self-supervised and Reinforcement Learning for Program-based   Table Reasoning in Small Language Models"></a>Table-r1: Self-supervised and Reinforcement Learning for Program-based   Table Reasoning in Small Language Models</h2><p><strong>Authors:Rihui Jin, Zheyu Xin, Xing Xie, Zuoyi Li, Guilin Qi, Yongrui Chen, Xinbang Dai, Tongtong Wu, Gholamreza Haffari</strong></p>
<p>Table reasoning (TR) requires structured reasoning over semi-structured tabular data and remains challenging, particularly for small language models (SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs (LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR), which circumvents key limitations of text-based TR (T-TR), notably in numerical reasoning, by generating executable programs. However, applying P-TR to SLMs introduces two challenges: (i) vulnerability to heterogeneity in table layouts, and (ii) inconsistency in reasoning due to limited code generation capability. We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1 introduces an innovative self-supervised learning task, Layout Transformation Inference, to improve tabular layout generalization from a programmatic view. Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization, enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed. Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all SLM-based methods, achieving at least a 15% accuracy improvement over the base model (LLaMA-8B) across all datasets and reaching performance competitive with LLMs. </p>
<blockquote>
<p>表格推理（TR）要求对半结构化表格数据进行结构化推理，仍然是一项挑战，特别是对于小型语言模型（SLM，例如LLaMA-8B）而言，由于与大型语言模型（LLM，例如GPT-4o）相比，它们的容量有限。为了缩小这一差距，我们探索了基于程序的TR（P-TR），通过生成可执行程序，避免了基于文本的TR（T-TR）在数值推理等方面的关键局限性。然而，将P-TR应用于SLM引入了两个挑战：（i）对表格布局异质性的脆弱性，（ii）由于有限的代码生成能力导致的推理不一致性。我们提出了Table-r1，这是一个为SLM设计的两阶段P-TR方法。第一阶段引入了一种创新的自我监督学习任务，即“布局转换推理”，从程序视角提高表格布局概括能力。第二阶段采用混合范式的集团相对策略优化变体，提高P-TR的一致性，同时根据需要允许动态退回到T-TR。在四个TR基准测试上的实验表明，Table-r1优于所有SLM方法，在所有数据集上至少比基础模型（LLaMA-8B）提高了15%的准确率，且性能与LLM相竞争。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06137v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对表格推理（TR）的挑战，特别是小型语言模型（SLM）在数值推理方面的局限性，研究了一种基于程序化的表格推理（P-TR）方法。为应对SLMs在P-TR应用中的表格布局多样性和推理不一致问题，提出了名为Table-r1的两阶段方法。该方法通过自我监督学习任务提高表格布局泛化能力，并采用混合范式的优化策略增强推理一致性。实验证明，Table-r1在四个TR基准测试上优于所有SLM方法，至少在所有数据集上实现了15%的准确率提升，性能与大型语言模型（LLM）相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>表格推理（TR）对小型语言模型（SLM）而言具有挑战性，特别是在数值推理方面。</li>
<li>基于程序化的表格推理（P-TR）方法可以克服文本基于的表格推理（T-TR）的局限性。</li>
<li>SLM在P-TR应用中面临两个挑战：表格布局的多样性和推理的不一致性。</li>
<li>Table-r1是一个两阶段的P-TR方法，旨在解决上述问题。</li>
<li>第一阶段通过自我监督学习任务提高表格布局的泛化能力。</li>
<li>第二阶段采用混合范式的优化策略，增强P-TR的推理一致性，并能在需要时动态回退到T-TR。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06137">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7e5fac4133efab2fa678853c3d00d81c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee554d8c6ba8383c325c65597b68123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5bee06f4a0364904434d9e60ed79eb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3ff8b2e78c951e622dd8d649d9b8e5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VideoChat-A1-Thinking-with-Long-Videos-by-Chain-of-Shot-Reasoning"><a href="#VideoChat-A1-Thinking-with-Long-Videos-by-Chain-of-Shot-Reasoning" class="headerlink" title="VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning"></a>VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning</h2><p><strong>Authors:Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, Yali Wang</strong></p>
<p>The recent advance in video understanding has been driven by multimodal large language models (MLLMs). But these MLLMs are good at analyzing short videos, while suffering from difficulties in understanding videos with a longer context. To address this difficulty, several agent paradigms have recently been proposed, using MLLMs as agents for retrieving extra contextual knowledge in a long video. However, most existing agents ignore the key fact that a long video is composed with multiple shots, i.e., to answer the user question from a long video, it is critical to deeply understand its relevant shots like human. Without such insight, these agents often mistakenly find redundant even noisy temporal context, restricting their capacity for long video understanding. To fill this gap, we propose VideoChat-A1, a novel long video agent paradigm. Different from the previous works, our VideoChat-A1 can deeply think with long videos, via a distinct chain-of-shot reasoning paradigm. More specifically, it can progressively select the relevant shots of user question, and look into these shots in a coarse-to-fine partition. By multi-modal reasoning along the shot chain, VideoChat-A1 can effectively mimic step-by-step human thinking process, allowing to interactively discover preferable temporal context for thoughtful understanding in long videos. Extensive experiments show that, our VideoChat-A1 achieves the state-of-the-art performance on the mainstream long video QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema, outperforming its strong baselines (e.g., Intern2.5VL-8B and InternVideo2.5-8B), by up to 10.8% and 6.2%. Compared to leading close-source GPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with 7% input frames and 12% inference time on average. </p>
<blockquote>
<p>近期视频理解的进步得益于多模态大型语言模型（MLLMs）的推动。但这些MLLMs擅长分析短视频，在理解长视频时遇到困难。为了解决这一难题，最近提出了几种使用MLLMs作为代理检索长视频额外上下文知识的代理范式。然而，大多数现有代理忽略了长视频由多个镜头组成的关键事实，即要回答用户关于长视频的问题，需要像人类一样深刻地理解其相关镜头。没有这样的洞察力，这些代理往往会错误地找到冗余甚至嘈杂的时间上下文，限制了它们对长视频的理解能力。为了填补这一空白，我们提出了VideoChat-A1，这是一种新的长视频代理范式。与以前的工作不同，我们的VideoChat-A1可以通过独特的镜头链推理范式对长视频进行深度思考。更具体地说，它可以逐步选择与用户问题相关的镜头，并以从粗到细的分区查看这些镜头。通过沿着镜头链进行多模态推理，VideoChat-A1可以有效地模仿人类的逐步思考过程，允许交互式地发现用户问题所需的可选时间上下文，以在长视频中实现深思熟虑的理解。大量实验表明，我们的VideoChat-A1在主流的长视频问答基准测试中达到了最先进的性能，例如在VideoMME上达到了77.0，在EgoSchema上达到了70.1，超越了其强大的基准线（例如Intern2.5VL-8B和InternVideo2.5-8B），最多高达10.8%和6.2%。与领先的闭源GPT-4o和Gemini 1.5 Pro相比，VideoChat-A1具有竞争力的准确性，但平均使用了7%的输入帧和减少了12%的推理时间。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06097v1">PDF</a> </p>
<p><strong>Summary</strong><br>多媒体视频理解领域中存在一项挑战，即处理长视频的理解问题。当前的多模态大型语言模型在处理长视频时存在困难，因为它们无法有效地处理多个镜头中的信息。为了解决这个问题，提出了一种名为VideoChat-A1的新型长视频代理范式。它通过独特的镜头链推理模式，能够逐步选择与用户问题相关的镜头，并在这些镜头中进行粗到细的检索分析。该模型在主流的长视频问答基准测试中实现了卓越的性能。相较于其他强大基线，其准确性显著提高。同时，VideoChat-A1在输入帧和推理时间上也有优化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在处理长视频时存在挑战。</li>
<li>VideoChat-A1通过独特的镜头链推理模式来处理长视频。</li>
<li>VideoChat-A1能够逐步选择与用户问题相关的镜头，并进行粗到细的检索分析。</li>
<li>VideoChat-A1在主流的长视频问答基准测试中实现了卓越性能。</li>
<li>VideoChat-A1相较于其他强大基线，准确性显著提高。</li>
<li>VideoChat-A1在输入帧和推理时间方面进行了优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06097">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-14f23fec431be274ecd7d316796faa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62a4be4b0895d782dd279f7f48b4319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9df71f687a89026d3da51d3d8c4e9e68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b125df293444f14b08c3f47c3e8e1d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-173e8b8ab404f4deb5e766bf042fccf3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Route-and-Reason-Scaling-Large-Language-Model-Reasoning-with-Reinforced-Model-Router"><a href="#Route-and-Reason-Scaling-Large-Language-Model-Reasoning-with-Reinforced-Model-Router" class="headerlink" title="Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced   Model Router"></a>Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced   Model Router</h2><p><strong>Authors:Chenyang Shao, Xinyang Liu, Yutang Lin, Fengli Xu, Yong Li</strong></p>
<p>Multi-step reasoning has proven essential for enhancing the problem-solving capabilities of Large Language Models (LLMs) by decomposing complex tasks into intermediate steps, either explicitly or implicitly. Extending the reasoning chain at test time through deeper thought processes or broader exploration, can furthur improve performance, but often incurs substantial costs due to the explosion in token usage. Yet, many reasoning steps are relatively simple and can be handled by more efficient smaller-scale language models (SLMs). This motivates hybrid approaches that allocate subtasks across models of varying capacities. However, realizing such collaboration requires accurate task decomposition and difficulty-aware subtask allocation, which is challenging. To address this, we propose R2-Reasoner, a novel framework that enables collaborative reasoning across heterogeneous LLMs by dynamically routing sub-tasks based on estimated complexity. At the core of our framework is a Reinforced Model Router, composed of a task decomposer and a subtask allocator. The task decomposer segments complex input queries into logically ordered subtasks, while the subtask allocator assigns each subtask to the most appropriate model, ranging from lightweight SLMs to powerful LLMs, balancing accuracy and efficiency. To train this router, we introduce a staged pipeline that combines supervised fine-tuning on task-specific datasets with Group Relative Policy Optimization algorithm, enabling self-supervised refinement through iterative reinforcement learning. Extensive experiments across four challenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85% while maintaining or surpassing baseline accuracy. Our framework paves the way for more cost-effective and adaptive LLM reasoning. The code is open-source at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2_Reasoner">https://anonymous.4open.science/r/R2_Reasoner</a> . </p>
<blockquote>
<p>多步推理通过显式或隐式地将复杂任务分解为中间步骤，被证明对于增强大型语言模型（LLM）的问题解决能力是至关重要的。通过在测试时扩展推理链，进行更深入的思考过程或更广泛的探索，可以进一步提高性能，但这通常由于标记使用量的激增而产生巨大成本。然而，许多推理步骤相对简单，可以通过更高效的小规模语言模型（SLM）来处理。这推动了混合方法的使用，该方法在不同容量的模型之间分配子任务。然而，实现这种协作需要准确的任务分解和难度感知的子任务分配，这是具有挑战性的。为了解决这一问题，我们提出了R2推理机，这是一个新型框架，能够通过基于估计复杂度的动态路由在异构LLM之间进行协作推理。我们框架的核心是一个强化模型路由器，由任务分解器和子任务分配器组成。任务分解器将复杂的输入查询分割成逻辑上有序的子任务，而子任务分配器将每个子任务分配给最合适的模型，从轻量级的SLM到功能强大的LLM，平衡准确性和效率。为了训练这个路由器，我们引入了一个分阶段管道，结合针对特定任务的监督微调数据集和集团相对策略优化算法，通过迭代强化学习实现自我监督的细化。在四个具有挑战性的基准测试上的大量实验表明，R2推理机在保持或超过基线准确率的同时，降低了86.85%的API成本。我们的框架为更具成本效益和适应性的LLM推理铺平了道路。代码已开源在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2_Reasoner%E3%80%82">https://anonymous.4open.science/r/R2_Reasoner。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05901v1">PDF</a> </p>
<p><strong>Summary</strong><br>     多步推理对于提升大型语言模型（LLM）的问题解决能力至关重要，通过将复杂任务分解成中间步骤，无论是显式还是隐式。扩展推理链可以提高性能，但会增加令牌使用量，带来巨大成本。为此，我们提出了R2-Reasoner框架，该框架通过动态路由子任务实现跨不同容量模型的协作推理。其核心是强化模型路由器，由任务分解器和子任务分配器组成。任务分解器将复杂输入查询分割成逻辑有序的子任务，而子任务分配器则将每个子任务分配给最合适的模型。通过结合任务特定数据集的监督微调与群体相对策略优化算法的分阶段管道，实现了自我监督的精细化。实验表明，R2-Reasoner在降低86.85%的API成本的同时，维持或提高了基线准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多步推理对于提升大型语言模型的问题解决能力至关重要。</li>
<li>通过将复杂任务分解成中间步骤，可以提高大型语言模型的性能。</li>
<li>推理链的扩展会增加令牌使用量，带来成本问题。</li>
<li>R2-Reasoner框架通过动态路由子任务实现跨不同容量模型的协作推理。</li>
<li>R2-Reasoner框架包括任务分解器和子任务分配器两个核心组件。</li>
<li>R2-Reasoner结合了监督微调与群体相对策略优化算法，实现了自我监督的精细化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05901">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-152ed6dc60c37110f590d03f6d30c05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaaa9badb975940ba17081306fbd30b0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FinanceReasoning-Benchmarking-Financial-Numerical-Reasoning-More-Credible-Comprehensive-and-Challenging"><a href="#FinanceReasoning-Benchmarking-Financial-Numerical-Reasoning-More-Credible-Comprehensive-and-Challenging" class="headerlink" title="FinanceReasoning: Benchmarking Financial Numerical Reasoning More   Credible, Comprehensive and Challenging"></a>FinanceReasoning: Benchmarking Financial Numerical Reasoning More   Credible, Comprehensive and Challenging</h2><p><strong>Authors:Zichen Tang, Haihong E, Ziyan Ma, Haoyang He, Jiacheng Liu, Zhongjun Yang, Zihua Rong, Rongjin Li, Kun Ji, Qing Huang, Xinyang Hu, Yang Liu, Qianhe Zheng</strong></p>
<p>We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs’ financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs’ performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks. </p>
<blockquote>
<p>我们推出了FinanceReasoning，这是一个新的基准测试，旨在评估大型推理模型在财务数值推理问题中的推理能力。与现有基准测试相比，我们的工作提供了三个关键进展。(1) 可靠性：我们从四个公共数据集中更新了15.6%的问题，为908个新问题提供了详细的Python解决方案，并严格完善了评估标准。这能够准确评估大型推理模型的推理改进情况。(2) 全面性：FinanceReasoning涵盖了67.8%的财务概念和公式，显著超越了现有数据集。此外，我们构建了3133个Python格式的函数，通过精细的知识来提高大型推理模型的财务推理能力（例如，GPT-4o从83.2%提升到91.6%）。(3) 挑战性：模型需要应用多个财务公式对238个难题进行精确数值推理。表现最佳的模型（即OpenAI o1 with PoT）的准确率为89.1%，但大型推理模型在数值精度方面仍面临挑战。我们证明，结合Reasoner和Programmer模型可以有效地提高大型推理模型的性能（例如，DeepSeek-R1从83.2%提升到87.8%）。我们的工作为未来在特定领域复杂推理任务中评估和提高大型推理模型的研究铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05828v1">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>FinanceReasoning作为一种新型基准测试，旨在评估大型推理模型在金融数值推理问题中的推理能力。相较于现有基准测试，其具备可信度、全面性和挑战性三大优势。更新后的题目涵盖了金融概念和公式的绝大部分内容，对模型的金融推理能力提出了较高要求，并结合了Reasoner和Programmer模型以提升模型性能。此项研究为未来在特定领域复杂推理任务中评估和改进大型推理模型奠定了基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FinanceReasoning作为一种基准测试，旨在评估大型推理模型在金融数值推理方面的能力。</li>
<li>与现有基准测试相比，FinanceReasoning具备更高的可信度和更全面的内容覆盖，涵盖了金融概念和公式的绝大部分内容。</li>
<li>FinanceReasoning对模型的金融推理能力提出了挑战，要求模型应用多个金融公式进行精确数值推理。</li>
<li>更新后的题目包括详细的Python解决方案，并严格细化了评估标准，以准确评估模型的推理改进情况。</li>
<li>结合Reasoner和Programmer模型能有效提升大型推理模型的性能。</li>
<li>目前最佳模型在FinanceReasoning上的准确率为89.1%，但仍存在数值精度方面的挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9502d40b69102bf5dd0e6faecd464a01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b443ac2f8c4e3266d250d1d5705b76b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62c0a5796f457a2c146ba2364ec99f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-298051ed548a0fb03e4f6df4f38da31e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81c797722952189acfc7fba68d41c174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269c4e73bd089e8071b0f67b0e28500e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e49ae44b7172c613a2cb55d825ccdd9f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MAPLE-Multi-Agent-Adaptive-Planning-with-Long-Term-Memory-for-Table-Reasoning"><a href="#MAPLE-Multi-Agent-Adaptive-Planning-with-Long-Term-Memory-for-Table-Reasoning" class="headerlink" title="MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table   Reasoning"></a>MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table   Reasoning</h2><p><strong>Authors:Ye Bai, Minghan Wang, Thuy-Trang Vu</strong></p>
<p>Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones. </p>
<blockquote>
<p>基于表格的问题回答需要复杂的推理能力，而当前的大型语言模型（LLMs）在单遍推理中难以实现。现有的方法，如思维链推理和问题分解，缺乏错误检测机制并丢弃解决问题时的经验，这与人类解决此类问题的方式形成鲜明对比。在本文中，我们提出了MAPLE（具有长期记忆的多智能体自适应规划），这是一种通过专门设计的认知智能体在反馈驱动循环中工作来模拟人类解决问题的新型框架。MAPLE集成了四个关键组件：（1）使用ReAct范式进行推理的求解器，（2）用于答案验证的检查器，（3）用于错误诊断和策略修正的反射器，以及（4）管理经验重用和演化的长期记忆的存档器。在WikiTQ和TabFact上的实验表明，与现有方法相比，该方法实现了显著的改进，并在多个大型语言模型主干上实现了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05813v1">PDF</a> 26 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为MAPLE的新型框架，用于模拟人类解决表格问答问题。该框架包含四个关键组件：Solver用于推理，Checker用于答案验证，Reflector用于错误诊断和策略修正，Archiver用于长期记忆管理以实现经验复用和进化。实验结果表明，MAPLE在WikiTQ和TabFact数据集上显著改进了现有方法，实现了跨多个大型语言模型背书的卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>表格问答要求复杂的推理能力，当前的大型语言模型（LLMs）在单通道推理中难以实现。</li>
<li>现有方法如Chain-of-Thought推理和问题分解缺乏错误检测机制，并忽略了问题解决经验的积累。</li>
<li>MAPLE框架模拟人类问题解决过程，通过专业认知代理在反馈驱动循环中工作。</li>
<li>MAPLE包含四个关键组件：Solver用于推理，Checker用于答案验证，Reflector用于错误诊断和策略修正，Archiver用于长期记忆管理。</li>
<li>MAPLE实现了显著的性能改进，在WikiTQ和TabFact数据集上超过了现有方法。</li>
<li>MAPLE框架实现了跨多个大型语言模型背书的卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05813">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d25822054b04a7ecdec6fe0b75376e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fec7df02734945b3d89dd1fde0596119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a6625da34bfe0e1da9c95ad7ae551d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0d979c2b2391c78042ef0e8181c24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8477364e54d08c29619c38161e8b5c0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EASG-Bench-Video-Q-A-Benchmark-with-Egocentric-Action-Scene-Graphs"><a href="#EASG-Bench-Video-Q-A-Benchmark-with-Egocentric-Action-Scene-Graphs" class="headerlink" title="EASG-Bench: Video Q&amp;A Benchmark with Egocentric Action Scene Graphs"></a>EASG-Bench: Video Q&amp;A Benchmark with Egocentric Action Scene Graphs</h2><p><strong>Authors:Ivan Rodin, Tz-Ying Wu, Kyle Min, Sharath Nittur Sridhar, Antonino Furnari, Subarna Tripathi, Giovanni Maria Farinella</strong></p>
<p>We introduce EASG-Bench, a question-answering benchmark for egocentric videos where the question-answering pairs are created from spatio-temporally grounded dynamic scene graphs capturing intricate relationships among actors, actions, and objects. We propose a systematic evaluation framework and evaluate several language-only and video large language models (video-LLMs) on this benchmark. We observe a performance gap in language-only and video-LLMs, especially on questions focusing on temporal ordering, thus identifying a research gap in the area of long-context video understanding. To promote the reproducibility of our findings and facilitate further research, the benchmark and accompanying code are available at the following GitHub page: <a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/EASG-bench">https://github.com/fpv-iplab/EASG-bench</a>. </p>
<blockquote>
<p>我们介绍了EASG-Bench，这是一个针对第一人称视频的问答基准测试。该基准测试中的问答对是根据捕捉演员、动作和对象之间复杂关系的时空定位动态场景图生成的。我们提出了一个系统的评估框架，并在该基准测试上评估了多种仅使用语言和视频的大型语言模型（视频LLM）。我们观察到仅使用语言和视频LLM的性能差距，特别是在关注时间顺序的问题上，从而确定了长上下文视频理解领域的研究空白。为了促进我们研究的可重复性并推动进一步的研究，该基准测试和配套代码可在以下GitHub页面找到：<a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/EASG-bench%E3%80%82">https://github.com/fpv-iplab/EASG-bench。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05787v1">PDF</a> </p>
<p><strong>Summary</strong>：我们介绍了EASG-Bench，这是一个针对以自我为中心的视频的问答基准测试。该测试中的问答对是根据捕捉演员、动作和对象之间复杂关系的时空接地动态场景图创建的。我们提出了一个系统的评估框架，并在这个基准测试上评估了几种仅使用语言和视频的大型语言模型（video-LLMs）。我们发现语言和视频LLM在关注时间顺序的问题上表现存在差距，这表明在理解长视频领域存在研究空白。为了促进我们研究结果的再现性和进一步的研究，该基准测试和配套代码可以在以下GitHub页面找到：[<a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/EASG-bench%E3%80%82">https://github.com/fpv-iplab/EASG-bench。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>EASG-Bench是一个针对以自我为中心的视频问答的基准测试，利用时空接地的动态场景图创建问答对。</li>
<li>该评估框架揭示了语言和视频大型语言模型（video-LLMs）在理解长视频，特别是在关注时间顺序的问题上的性能差距。</li>
<li>这一发现表明在理解长视频领域存在研究空白。</li>
<li>该基准测试和配套代码可在GitHub页面找到，以促进研究结果的复现和进一步研究。</li>
<li>此基准测试对于评估模型在复杂视频场景中的理解和推理能力具有价值。</li>
<li>动态场景图在视频问答任务中起到了关键作用，能够捕捉视频中演员、动作和对象之间的复杂关系。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05787">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4f771ec210a1598eda06e161123b91d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e767ba5d7949786ffb75545aaf5f52d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0adbc77105ace2504f08d59ad9da9ed2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c457f4997ffc3b4c3f6fcf62579f89fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Topology-of-Reasoning-Understanding-Large-Reasoning-Models-through-Reasoning-Graph-Properties"><a href="#Topology-of-Reasoning-Understanding-Large-Reasoning-Models-through-Reasoning-Graph-Properties" class="headerlink" title="Topology of Reasoning: Understanding Large Reasoning Models through   Reasoning Graph Properties"></a>Topology of Reasoning: Understanding Large Reasoning Models through   Reasoning Graph Properties</h2><p><strong>Authors:Gouki Minegishi, Hiroki Furuta, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo</strong></p>
<p>Recent large-scale reasoning models have achieved state-of-the-art performance on challenging mathematical benchmarks, yet the internal mechanisms underlying their success remain poorly understood. In this work, we introduce the notion of a reasoning graph, extracted by clustering hidden-state representations at each reasoning step, and systematically analyze three key graph-theoretic properties: cyclicity, diameter, and small-world index, across multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly more recurrent cycles (about 5 per sample), substantially larger graph diameters, and pronounced small-world characteristics (about 6x) compared to their base counterparts. Notably, these structural advantages grow with task difficulty and model capacity, with cycle detection peaking at the 14B scale and exploration diameter maximized in the 32B variant, correlating positively with accuracy. Furthermore, we show that supervised fine-tuning on an improved dataset systematically expands reasoning graph diameters in tandem with performance gains, offering concrete guidelines for dataset design aimed at boosting reasoning capabilities. By bridging theoretical insights into reasoning graph structures with practical recommendations for data construction, our work advances both the interpretability and the efficacy of large reasoning models. </p>
<blockquote>
<p>近期的大规模推理模型已在具有挑战性的数学基准测试中达到了最先进的性能，然而，其成功背后的内在机制仍然知之甚少。在这项工作中，我们引入了推理图的概念，通过聚类每一步推理的隐藏状态表示来提取，并系统地分析了三个关键的图论属性：循环性、直径和小世界指数，跨越多个任务（GSM8K、MATH500、AIME 2024）。我们的研究发现，蒸馏推理模型（例如DeepSeek-R1-Distill-Qwen-32B）表现出更明显的循环（每个样本约5个循环）、更大的图直径和突出的小世界特征（约为原来的6倍），与基础模型相比。值得注意的是，这些结构优势随着任务难度和模型容量的增加而增强，循环检测在规模为14B时达到峰值，探索直径在规模为32B时最大化，与准确性呈正相关。此外，我们在改进的数据集上进行监督微调，系统地扩大了推理图的直径，同时提高了性能，为旨在提高推理能力的数据集设计提供了具体指导。我们的工作架起了推理图结构理论见解与数据构建实践建议之间的桥梁，提高了大规模推理模型的解释性和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05744v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该论文探讨了大规模推理模型中的推理图结构，分析了循环性、直径以及小世界指数等图论属性，揭示了这些模型在某些数学基准测试上的优秀表现背后的内部机制。研究表明，蒸馏推理模型相对于基础模型具有更多的循环周期、更大的图直径以及显著的小世界特性。随着任务难度和模型容量的增长，这些结构优势也呈现增长趋势，对精度具有积极影响。此外，该研究还提供了一些改进模型性能的指导建议，对于提升大型推理模型的解释性和有效性具有重要的指导意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入推理图概念，通过聚类每个推理步骤的隐藏状态表示来提取。</li>
<li>分析三个关键图论属性：循环性、直径和小世界指数。</li>
<li>蒸馏推理模型展现出更多的循环周期、更大的图直径和显著的小世界特性。</li>
<li>随着任务难度和模型容量的增长，这些结构优势呈现增长趋势。</li>
<li>模型的结构特点与精度正相关。循环检测在模型规模为14B时达到峰值，探索直径在规模为32B时最大化。</li>
<li>通过监督微调改进数据集可系统地扩大推理图的直径并提升性能增益。这提供了设计数据集以加强推理能力的具体指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05744">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-09c58c60e77b7fa849a50c8648fc2ed5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de13652a6bc2ff2475a1244ea386d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4089aeac831988279db97fb748b98475.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5795de29123531e7d14c9fb5e7e58ef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca5863545b3af5fcae1daedb3763737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-904dac42ad67a9229996a419f784fee1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Do-LLMs-Really-Forget-Evaluating-Unlearning-with-Knowledge-Correlation-and-Confidence-Awareness"><a href="#Do-LLMs-Really-Forget-Evaluating-Unlearning-with-Knowledge-Correlation-and-Confidence-Awareness" class="headerlink" title="Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation   and Confidence Awareness"></a>Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation   and Confidence Awareness</h2><p><strong>Authors:Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li</strong></p>
<p>Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Graph-COM/Knowledge_Unlearning.git">https://github.com/Graph-COM/Knowledge_Unlearning.git</a>. </p>
<blockquote>
<p>机器遗忘技术旨在减轻大型语言模型（LLM）中的意外记忆。然而，现有的方法主要集中在显式删除孤立的事实上，往往忽视了潜在推理依赖性和LLM中知识的非确定性。因此，假定被遗忘的事实可能会通过相关信息在隐式状态下持续存在。为了应对这些挑战，我们提出了一个知识遗忘评估框架，它通过构建相关事实上下文作为知识图谱并附带相应的置信度分数来更准确地捕捉现实知识的隐式结构。我们进一步开发了一种基于推理的评价协议，利用强大的LLM作为评判员；这些评判员对提取的知识子图进行推理，以确定遗忘的成功程度。我们的LLM评委使用精心设计提示词进行校准并相对于人类评估，以确保其可靠性和稳定性。在我们新构建的基准上进行的大量实验表明，我们的框架提供了对遗忘性能的更加现实和严格评估。此外，我们的研究结果表明，当前的评价策略往往高估了遗忘的有效性。我们的代码公开在<a target="_blank" rel="noopener" href="https://github.com/Graph-COM/Knowledge_Unlearning.git%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/Graph-COM/Knowledge_Unlearning.git上可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种知识遗忘评估框架，旨在更准确地捕捉大型语言模型中知识的隐含结构。该框架通过构建知识图谱和关联置信度评分来表示相关事实背景，并开发了一种基于推理的评价协议，利用强大的语言模型作为评估员来判断遗忘的效果。该框架提供了一个更现实和严格的评估遗忘性能的方法，并发现当前的评价策略往往高估了遗忘效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器遗忘技术旨在缓解大型语言模型中的无意记忆问题。</li>
<li>现有方法主要关注明确删除孤立事实，但忽略了潜在推理依赖性和语言模型内知识的非确定性。</li>
<li>提出的评估框架通过构建知识图谱和关联置信度评分来更准确地捕捉语言模型中知识的隐含结构。</li>
<li>开发了一种基于推理的评价协议，利用语言模型作为评估员来判断遗忘效果。</li>
<li>评估员通过精心设计的提示进行评估，并与人类评价进行校准，以确保其可靠性和稳定性。</li>
<li>在新构建的基准测试上进行的广泛实验表明，该框架提供了更现实和严格的遗忘性能评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05735">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-798234c05a2ed858510d5e7d47920ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c72f5f23ff1cf6620b384fffe3102250.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SafeGenBench-A-Benchmark-Framework-for-Security-Vulnerability-Detection-in-LLM-Generated-Code"><a href="#SafeGenBench-A-Benchmark-Framework-for-Security-Vulnerability-Detection-in-LLM-Generated-Code" class="headerlink" title="SafeGenBench: A Benchmark Framework for Security Vulnerability Detection   in LLM-Generated Code"></a>SafeGenBench: A Benchmark Framework for Security Vulnerability Detection   in LLM-Generated Code</h2><p><strong>Authors:Xinghang Li, Jingzhe Ding, Chao Peng, Bing Zhao, Xiang Gao, Hongwan Gao, Xinchen Gu</strong></p>
<p>The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce \benchmark, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on \benchmark, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon. </p>
<blockquote>
<p>大型语言模型（LLM）的代码生成能力已成为评估其整体性能的关键维度。然而，之前的研究在很大程度上忽视了生成代码中的安全风险。在这项工作中，我们引入了专门用于评估LLM生成代码安全性的基准测试集\benchmark。该数据集涵盖了多种常见的软件开发场景和漏洞类型。基于这个基准测试集，我们开发了一个自动评估框架，该框架利用静态应用程序安全测试（SAST）和基于LLM的判断来评估模型生成代码中存在的安全风险。通过对最新LLM在\benchmark上的实证评估，我们发现其在生成无漏洞代码方面存在明显不足。我们的研究结果突出了紧迫的挑战，并为未来提高LLM安全代码生成性能提供了切实可行的见解。数据集和代码很快会发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05692v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）的代码生成能力已成为评估其整体性能的关键维度，但先前的研究大多忽略了生成代码中的安全风险。本研究引入了\benchmark，这是一个专门用于评估LLM生成代码安全性的基准测试。该数据集涵盖了各种常见的软件开发场景和漏洞类型。在此基础上，我们开发了一个自动评估框架，该框架利用静态应用程序安全测试（SAST）和基于LLM的判据来评估模型生成代码中的安全漏洞。通过对最新LLMs在\benchmark上的实证评估，我们发现它们在生成无漏洞代码方面存在显著缺陷。我们的研究结果突出了未来的挑战，并为提高LLM的安全代码生成性能提供了可操作的见解。数据和代码将很快发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的代码生成能力已成为评估其性能的重要方面。<br>2.先前的研究主要忽视了LLM生成代码中的安全风险。</li>
<li>\benchmark是首个专门用于评估LLM生成代码安全性的基准测试。</li>
<li>该数据集涵盖了多种软件开发场景和漏洞类型。</li>
<li>我们开发了一个自动评估框架来检测模型生成代码中的安全漏洞。</li>
<li>实证评估发现，当前LLMs在生成无漏洞代码方面存在显著不足。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05692">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb16acc28cba349ce961455fa8bfe6b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d032adc2870e4bb936a697adb24471a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-379c4fea709546f50ecda95569e5845c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0df7b473dfd3e59cdf91bccaaac7d42b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b851e009ce21d7c0b8b2650f6b2fcef.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DriveAction-A-Benchmark-for-Exploring-Human-like-Driving-Decisions-in-VLA-Models"><a href="#DriveAction-A-Benchmark-for-Exploring-Human-like-Driving-Decisions-in-VLA-Models" class="headerlink" title="DriveAction: A Benchmark for Exploring Human-like Driving Decisions in   VLA Models"></a>DriveAction: A Benchmark for Exploring Human-like Driving Decisions in   VLA Models</h2><p><strong>Authors:Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong Qin, Mofan Zhou, Yifei Zhan, Peng Jia, Xianpeng Lang</strong></p>
<p>Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by users of production-level autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from users’ actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving. </p>
<blockquote>
<p>视觉语言动作（VLA）模型已经推动了自动驾驶技术的发展，但现有的基准测试仍然缺乏场景多样性、可靠的动作级别标注以及与人类偏好对齐的评估协议。为了解决这些局限性，我们引入了DriveAction，这是专门为VLA模型设计的第一个动作驱动基准测试，由从2,610个驾驶场景生成的16,185个问答对组成。DriveAction利用生产级自动驾驶车辆的用户主动收集的真实世界驾驶数据，以确保广泛和具有代表性的场景覆盖；提供从用户实际驾驶操作中直接收集的高级离散动作标签；实施以动作为核心的树状评估框架，该框架明确地将视觉、语言和动作任务联系起来，支持全面和特定任务的评估。我们的实验表明，最先进的视觉语言模型（VLMs）需要视觉和语言指导来进行准确的动作预测：平均而言，没有视觉输入时准确率下降3.3%，没有语言输入时下降4.1%，两者都没有时下降8.0%。我们的评估支持对模型瓶颈的精确识别，提供稳健且一致的结果，从而为提高自动驾驶中类似人类的决策提供了新的见解和严格的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05667v1">PDF</a> Benchmark:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LiAuto-DriveAction/drive-action">https://huggingface.co/datasets/LiAuto-DriveAction/drive-action</a></p>
<p><strong>Summary</strong>：</p>
<p>为解决现有自动驾驶视觉语言动作模型（VLA）的基准测试存在的缺乏场景多样性、可靠的行动级别标注以及符合人类偏好的评估协议的问题，本文推出了首个面向VLA模型的行动驱动基准测试DriveAction。它包含从生产级别的自动驾驶车辆用户主动收集的驾驶场景生成的16,185个问答对。DriveAction确保了广泛且具有代表性的场景覆盖，提供了用户实际驾驶操作收集的高级离散动作标签，并实施了一个以行动为基础的树形评估框架，明确地将视觉、语言和行动任务联系起来，既支持全面评估又支持特定任务评估。实验表明，最先进的视觉语言模型（VLMs）需要视觉和语言指导来进行准确的动作预测。在没有视觉输入的情况下，准确率平均下降3.3%；没有语言输入时下降4.1%；两者都没有时下降8.0%。本评估为识别模型瓶颈提供了精确且一致的结果，为提升自动驾驶中的人类决策提供了严谨的基础。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>DriveAction是首个面向Vision-Language-Action（VLA）模型的行动驱动基准测试。</li>
<li>DriveAction包含从生产级自动驾驶车辆用户主动收集的驾驶场景生成的丰富问答对。</li>
<li>它确保了广泛且具有代表性的场景覆盖，并提供高级离散动作标签。</li>
<li>DriveAction实施了一个以行动为基础的树形评估框架，支持全面和特定任务的评估。</li>
<li>实验表明，先进的视觉语言模型（VLMs）需要视觉和语言指导来完成动作预测任务。</li>
<li>在缺少视觉或语言输入的情况下，VLMs的准确率会显著下降。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-30d164badbda72f306d55df5ee9b2210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-497fa0781c48265a0fa9816eaf673072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94fba99a462ce36587c7c717b6250367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a134900736ce8531400f12440dddc8cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb6f7e96f9d428dcbbd4a7638a7d2232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ef0382fb3796a0b06318976c8fb1656.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-650c4c2b0aada31dfd3d4cc58b087f3d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ScaleRTL-Scaling-LLMs-with-Reasoning-Data-and-Test-Time-Compute-for-Accurate-RTL-Code-Generation"><a href="#ScaleRTL-Scaling-LLMs-with-Reasoning-Data-and-Test-Time-Compute-for-Accurate-RTL-Code-Generation" class="headerlink" title="ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for   Accurate RTL Code Generation"></a>ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for   Accurate RTL Code Generation</h2><p><strong>Authors:Chenhui Deng, Yun-Da Tsai, Guan-Ting Liu, Zhongzhi Yu, Haoxing Ren</strong></p>
<p>Recent advances in large language models (LLMs) have enabled near-human performance on software coding benchmarks, but their effectiveness in RTL code generation remains limited due to the scarcity of high-quality training data. While prior efforts have fine-tuned LLMs for RTL tasks, they do not fundamentally overcome the data bottleneck and lack support for test-time scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL, the first reasoning LLM for RTL coding that scales up both high-quality reasoning data and test-time compute. Specifically, we curate a diverse set of long chain-of-thought reasoning traces averaging 56K tokens each, resulting in a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a general-purpose reasoning model on this corpus yields ScaleRTL that is capable of deep RTL reasoning. Subsequently, we further enhance the performance of ScaleRTL through a novel test-time scaling strategy that extends the reasoning process via iteratively reflecting on and self-correcting previous reasoning steps. Experimental results show that ScaleRTL achieves state-of-the-art performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by up to 18.4% on VerilogEval and 12.7% on RTLLM. </p>
<blockquote>
<p>最近，大型语言模型（LLM）的进展已经在软件编码基准测试上实现了接近人类的性能。然而，由于其高质量训练数据的稀缺，它们在RTL代码生成中的有效性仍然有限。尽管先前的努力已经对LLM进行了RTL任务的微调，但它们并没有从根本上克服数据瓶颈，并且由于缺乏推理性质，它们不支持测试时的扩展。在这项工作中，我们引入了ScaleRTL，这是第一个用于RTL编码的推理LLM，它扩大了高质量推理数据和测试时的计算规模。具体来说，我们整理了一系列平均每个包含56K令牌的长思考推理轨迹，形成了一个包含35亿令牌的数据集，反映了丰富的RTL知识。在这个语料库上微调通用推理模型产生了能够进行深度RTL推理的ScaleRTL。随后，我们进一步通过一种新型测试时扩展策略增强ScaleRTL的性能，该策略通过反思和自我纠正之前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上达到了最先进的性能，在VerilogEval上超越了18个竞争基准，性能提高了高达18.4%，在RTLLM上性能提高了12.7%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05566v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>近期大型语言模型（LLM）在软件编码基准测试上展现出近乎人类的性能，但在RTL代码生成方面的有效性仍然受限，主要因为高质量训练数据的稀缺。先前的研究虽然对LLM进行了微调以应对RTL任务，但并没有从根本上突破数据瓶颈，且由于缺乏推理能力而不支持测试时的缩放。在此研究中，我们推出了ScaleRTL，这是首个用于RTL编码的推理LLM，可扩大高质量推理数据和测试时的计算规模。我们通过收集丰富的RTL知识的数据集，对通用推理模型进行微调，使其具备深度RTL推理能力。此外，我们还通过一种新的测试时缩放策略进一步提高了ScaleRTL的性能，该策略通过反思和纠正之前的推理步骤来扩展推理过程。实验结果表明，ScaleRTL在VerilogEval和RTLLM上达到了最先进的性能水平，相较于18个竞争基准测试，其在VerilogEval上的性能提高了18.4%，在RTLLM上提高了12.7%。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）在软件编码基准测试上取得了显著进展，但在RTL代码生成方面仍面临挑战，主要受制于高质量训练数据的稀缺。</li>
<li>现有的LLM在RTL任务上的研究没有从根本上解决数据瓶颈问题，并且缺乏推理能力，不支持测试时的缩放。</li>
<li>引入的ScaleRTL是首个针对RTL编码的推理LLM，能够扩大高质量推理数据和测试时的计算规模。</li>
<li>通过收集丰富的RTL知识的数据集，对通用推理模型进行微调，使ScaleRTL具备深度RTL推理能力。</li>
<li>ScaleRTL采用了一种新的测试时缩放策略，通过反思和纠正之前的推理步骤来扩展推理过程，进一步提高性能。</li>
<li>实验结果表明，ScaleRTL在VerilogEval和RTLLM等基准测试上达到了最先进的性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-983d5c46dae27fb2b07eed3eb87b54ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1ce7f9a2c0994f63a86b332171df0f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372a4501a539d769076c7ff08a44724f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6666b8a5b46657e9a0d475449ed052.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed3f99410bff62b10967f55d529072fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c043113b60c1931e978876e31eea487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c9e84214c7555e4e45e13ca2b7eae83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad34e3fdee8aa6105193d4171aa28e77.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Prefix-Grouper-Efficient-GRPO-Training-through-Shared-Prefix-Forward"><a href="#Prefix-Grouper-Efficient-GRPO-Training-through-Shared-Prefix-Forward" class="headerlink" title="Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward"></a>Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward</h2><p><strong>Authors:Zikang Liu, Tongtian Yue, Yepeng Tang, Longteng Guo, Junxian Cai, Qingbin Liu, Xi Chen, Jing Liu</strong></p>
<p>Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at <a target="_blank" rel="noopener" href="https://github.com/johncaged/PrefixGrouper">https://github.com/johncaged/PrefixGrouper</a> </p>
<blockquote>
<p>群体相对策略优化（GRPO）通过计算具有相同输入前缀的候选输出之间的相对比较梯度，增强了策略学习能力。尽管其效果显著，但在处理长的共享前缀时，GRPO需要为每个组成员重复编码，从而引入了大量的计算开销。这种低效性在长上下文学习场景中成为了可扩展性的主要瓶颈。我们提出了Prefix Grouper，这是一种高效的GRPO训练算法，它通过共享前缀前向策略消除了冗余的前缀计算。特别是，我们的方法通过将自注意力分为两部分进行重构，使共享前缀只需编码一次，同时保持完整的可微性和与端到端训练的兼容性。我们提供理论和实证证据表明，Prefix Grouper在训练上等同于标准的GRPO：它产生相同的前向输出和反向梯度，确保优化动态和最终的策略性能保持不变。从经验上看，我们的实验证实，Prefix Grouper在减少训练计算成本的同时实现了稳定的结果，特别是在长前缀场景中。所提出的方法是即插即用：它与现有的GRPO架构兼容，可以无缝地集成到当前的训练管道中作为即插即用替换，无需进行结构性修改，只需对输入构建和注意力计算进行最小的更改。Prefix Grouper能够在相同的计算预算下使用更大的群体规模，从而提高了GRPO在更复杂的任务和更大模型上的可扩展性。代码现在可在<a target="_blank" rel="noopener" href="https://github.com/johncaged/PrefixGrouper%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/johncaged/PrefixGrouper找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05433v1">PDF</a> 10 pages, technical report</p>
<p><strong>Summary</strong>：前缀分组器通过共享前缀前向策略消除了冗余的前缀计算，提高了相对策略优化（GRPO）的训练效率。该方法在保持优化动态和最终策略性能不变的情况下，实现了对标准GRPO的训练等效性，同时显著降低了训练的计算成本，特别是在长前缀场景下。前缀分组器可以与现有的GRPO架构无缝集成，不需要结构修改，只需对输入构建和注意力计算进行最小的更改。这将有助于实现更大的组大小和更复杂任务和更大模型的可扩展性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>GRPO通过计算候选输出之间的相对比较梯度来提高策略学习。</li>
<li>在处理长共享前缀时，GRPO存在计算开销大的问题。</li>
<li>Prefix Grouper通过共享前缀前向策略消除了冗余的前缀计算，提高了GRPO的训练效率。</li>
<li>Prefix Grouper实现了对标准GRPO的训练等效性，保持了优化动态和最终策略性能不变。</li>
<li>Prefix Grouper显著降低了训练的计算成本，特别是在长前缀场景下。</li>
<li>Prefix Grouper与现有的GRPO架构兼容，无需结构修改，可轻松集成到当前训练流程中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05433">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b45adf0fb04803c5cf9f9297c25dd77a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3107532b00da9b44a667b2ee0c1e3e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0282e91e35dbf5ad9ce5900c41ebc23d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd09224f5099f36d31469bccb6acbe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b3483bb02480dcf9d90f438753968de.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Q-Ponder-A-Unified-Training-Pipeline-for-Reasoning-based-Visual-Quality-Assessment"><a href="#Q-Ponder-A-Unified-Training-Pipeline-for-Reasoning-based-Visual-Quality-Assessment" class="headerlink" title="Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality   Assessment"></a>Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality   Assessment</h2><p><strong>Authors:Zhuoxuan Cai, Jian Zhang, Xinbin Yuan, Pengtao Jiang, Wenxiang Chen, Bowen Tang, Lujian Yao, Qiyuan Wang, Jinwen Chen, Bo Li</strong></p>
<p>Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks. </p>
<blockquote>
<p>最近的研究表明，多模态大型语言模型（MLLMs）能够通过可解释评估熟练地评估视觉质量。然而，现有方法通常将质量评分和推理描述视为具有不同优化目标的单独任务，这导致了一种权衡：擅长质量推理描述的模型在精确分数回归方面表现挣扎，而专注于分数的模型则缺乏可解释性。这一局限性阻碍了MLLMs在视觉质量评估中的全部潜力，其中准确性和可解释性应该相互增强。为解决这一问题，我们提出了一种包含冷启动阶段和基于强化学习的微调阶段的统一两阶段训练框架。具体来说，在第一阶段，我们通过专家设计的提示从教师模型中提炼高质量数据，并通过交叉熵损失监督初始化推理能力。在第二阶段，我们引入了一种新型奖励与集团相对策略优化（GRPO），以联合优化评分准确性和推理一致性。我们将这两个阶段衍生出的模型分别指定为Q-Ponder-CI和Q-Ponder。大量实验表明，Q-Ponder在质量分数回归基准测试上达到了最新技术水平（SOTA），在跨域数据集上的SRCC提高了高达6.5%。此外，Q-Ponder在描述准确性及合理性方面显著优于基于描述的SOTA模型，包括其教师模型Qwen-2.5-VL-72B，这证明了其在不同任务上的泛化潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05384v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究展示了多模态大型语言模型（MLLMs）在视觉质量评估方面的能力。现有方法通常将质量评分和推理描述视为单独的任务，存在优化目标脱节的问题，导致模型在精准度与可解释性之间权衡。为解决这个问题，研究提出了一个两阶段的统一训练框架，包括冷启动阶段和基于强化学习的微调阶段。在冷启动阶段，通过专家设计的提示从教师模型中提炼高质量数据，并通过交叉熵损失监督进行初步推理能力训练。在微调阶段，引入新的奖励机制——群体相对策略优化（GRPO），联合优化评分准确性和推理一致性。模型被称为Q-Ponder系列。实验显示，Q-Ponder在质量评分回归基准测试中达到最佳性能，在跨域数据集上的SRCC提高达6.5%。尤其在描述准确性和合理性方面，相较于教师模型Qwen-2.5-VL-72B有显著优势。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在视觉质量评估中展现潜力。</li>
<li>现有方法将质量评分和推理描述视为独立任务，存在优化目标脱节问题。</li>
<li>提出两阶段统一训练框架：冷启动阶段通过专家设计提示进行高质量数据提炼和初步推理能力训练；强化学习微调阶段联合优化评分准确性和推理一致性。</li>
<li>Q-Ponder系列模型表现最佳，在跨域数据集上SRCC提高达6.5%。</li>
<li>Q-Ponder相较于教师模型在描述准确性和合理性方面有明显优势。</li>
<li>Q-Ponder在质量评分回归基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05384">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa39aff3d7ca71396e7c355ca16f1c69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec65390d9262291373a27e0a4eb3e2a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9eb026e43b0e88695451e9c2a8a04736.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5881c2b119700d18b3a1fca934d22ff4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rethinking-Machine-Unlearning-in-Image-Generation-Models"><a href="#Rethinking-Machine-Unlearning-in-Image-Generation-Models" class="headerlink" title="Rethinking Machine Unlearning in Image Generation Models"></a>Rethinking Machine Unlearning in Image Generation Models</h2><p><strong>Authors:Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng</strong></p>
<p>With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/ryliu68/IGMU">https://github.com/ryliu68/IGMU</a>. </p>
<blockquote>
<p>随着图像生成模型的涌现和广泛应用，数据隐私和内容安全成为主要关注点，并引起了用户、服务提供商和政策制定者的极大关注。机器遗忘（MU）被认为是一种经济高效、前景广阔的解决这些挑战的手段。尽管取得了一些进展，但图像生成模型的遗忘（IGMU）在实践中仍然面临显著的差距，例如任务辨别和遗忘指南不明确、缺乏有效的评估框架和不可靠的评估指标。这些可能阻碍对遗忘机制的理解和实用遗忘算法的设计。我们对现有的最新遗忘算法和评估标准进行了全面的评估，并发现了图像生成任务（IGMU）中的几个关键缺陷和挑战。受这些局限性的驱动，我们做出了几项核心贡献，以促进对IGMU的全面理解、标准化分类和可靠评估。具体来说，（1）我们设计了CatIGMU，这是一种新型分层任务分类框架。它为IGMU提供了详细的实施指南，有助于设计遗忘算法和构建测试平台。（2）我们介绍了EvalIGMU，这是一个全面的评估框架。它包括五个关键方面的可靠定量指标。（3）我们构建了DataIGM，这是一个高质量的遗忘数据集，可用于对IGMU进行广泛评估、训练内容检测器进行判读和基准测试最新遗忘算法。借助EvalIGMU和DataIGM，我们发现大多数现有IGMU算法在不同的评估维度上无法很好地处理遗忘问题，特别是在保留性和稳健性方面。相关代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/ryliu68/IGMU%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ryliu68/IGMU找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02761v2">PDF</a> Accepted by ACM CCS 2025</p>
<p><strong>Summary</strong></p>
<p>文本探讨了图像生成模型普及带来的数据隐私和内容安全问题，机器遗忘（MU）被视为解决这些挑战的经济有效的有前途的手段。然而，图像生成模型遗忘（IGMU）在实践中仍存在显著差距，如任务辨别不清、遗忘准则不明确、缺乏有效的评估框架和不可靠的评估指标等。为了解决这个问题，研究者对现有的前沿遗忘算法和评估标准进行了全面评估，发现了IGMU任务中的几个关键缺陷和挑战。本文的主要贡献包括设计CatIGMU任务分类框架、引入EvalIGMU评估框架以及构建DataIGM数据集，以促进对IGMU的全面理解、标准化分类和可靠评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像生成模型的广泛应用引发了数据隐私和内容安全的主要关注。</li>
<li>机器遗忘（MU）被视为解决这些挑战的有效手段，但图像生成模型遗忘（IGMU）存在显著实践差距。</li>
<li>IGMU面临的任务辨别不清、遗忘准则不明确等问题阻碍了遗忘机制的理解和实用遗忘算法的设计。</li>
<li>研究者设计了CatIGMU任务分类框架，为IGMU的详细实施提供了指导。</li>
<li>EvalIGMU评估框架的引入，包括五个关键方面的可靠定量指标。</li>
<li>构建了DataIGM高质量遗忘数据集，用于广泛评估IGMU、训练内容检测器和评估最新遗忘算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-629542ff9b218e4a14f16463472fe13b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66c40af5bbac93e63719768733805865.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3811a96219ca0c187b93d8487937b3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073e8f2bc92d0cad485637fa18372d92.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="The-Coming-Crisis-of-Multi-Agent-Misalignment-AI-Alignment-Must-Be-a-Dynamic-and-Social-Process"><a href="#The-Coming-Crisis-of-Multi-Agent-Misalignment-AI-Alignment-Must-Be-a-Dynamic-and-Social-Process" class="headerlink" title="The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a   Dynamic and Social Process"></a>The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a   Dynamic and Social Process</h2><p><strong>Authors:Florian Carichon, Aditi Khandelwal, Marylou Fauchard, Golnoosh Farnadi</strong></p>
<p>This position paper states that AI Alignment in Multi-Agent Systems (MAS) should be considered a dynamic and interaction-dependent process that heavily depends on the social environment where agents are deployed, either collaborative, cooperative, or competitive. While AI alignment with human values and preferences remains a core challenge, the growing prevalence of MAS in real-world applications introduces a new dynamic that reshapes how agents pursue goals and interact to accomplish various tasks. As agents engage with one another, they must coordinate to accomplish both individual and collective goals. However, this complex social organization may unintentionally misalign some or all of these agents with human values or user preferences. Drawing on social sciences, we analyze how social structure can deter or shatter group and individual values. Based on these analyses, we call on the AI community to treat human, preferential, and objective alignment as an interdependent concept, rather than isolated problems. Finally, we emphasize the urgent need for simulation environments, benchmarks, and evaluation frameworks that allow researchers to assess alignment in these interactive multi-agent contexts before such dynamics grow too complex to control. </p>
<blockquote>
<p>本立场论文指出，多智能体系统（MAS）中的人工智能对齐应被视为一个动态且依赖于交互的过程，这很大程度上取决于智能体部署的社会环境，这些环境可能是协作、合作或竞争性的。虽然人工智能与人类价值观和偏好的对齐仍然是核心挑战，但在现实世界应用中日益普遍的多智能体系统引入了一种新的动态，这种动态重塑了智能体如何追求目标和完成各种任务时的交互方式。当智能体彼此交互时，它们必须协调以实现个人和集体目标。然而，这种复杂的组织结构可能会无意中使部分或所有智能体与人类价值观或用户偏好产生偏差。我们借鉴社会科学，分析社会结构如何阻碍或破坏群体和个人价值观。基于这些分析，我们呼吁人工智能界将人类、偏好和客观对齐视为相互依存的概念，而不是孤立的问题。最后，我们强调仿真环境、基准测试和评估框架的紧迫需求，这些框架允许研究人员在复杂的交互式多智能体环境中评估对齐情况，因为在这些动态变得过于复杂且难以控制之前对其进行评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01080v2">PDF</a> Preprint of NeurIPS 2025 Position Paper</p>
<p><strong>Summary</strong><br>     人工智能多智能体系统（MAS）中的对齐应被视为一个动态且依赖于交互的过程，这取决于智能体部署的社会环境，包括协作、合作或竞争。随着MAS在现实世界应用中的普及增长，人类价值观和偏好的对齐仍是核心挑战，但同时也出现了新的动态情况，重塑了智能体追求目标和完成任务时的交互方式。智能体间的复杂社会交互需要协调个体和集体目标，但这一过程可能无意中使部分或全部智能体与人类价值观或用户偏好不一致。通过分析社会结构如何破坏群体和个人价值观，我们呼吁人工智能界将人类、偏好和客观对齐视为相互依赖的概念，而非孤立的问题。最后，我们强调需要仿真环境、基准测试和评估框架，以便在动态增长过于复杂之前评估多智能体上下文中的对齐情况。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI Alignment in Multi-Agent Systems (MAS) 是一个动态且依赖于交互的过程。</li>
<li>智能体的社会环境影响其目标和任务的完成方式。</li>
<li>智能体间的复杂社会交互需要协调个体和集体目标。</li>
<li>智能体与人类价值观和偏好的对齐是核心挑战。</li>
<li>社会结构可能影响智能体的价值观和行为的对齐。</li>
<li>需要将人类、偏好和客观对齐视为相互依赖的概念。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cfec5a0398eb2321e7f0dcbe8b899e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54d266889e6e02e1f43157ab2a232862.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c447f1be192acb55d34f45b78cd72e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-813e7079af384cfcbbfbd3492a93379f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning"><a href="#AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning" class="headerlink" title="AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning"></a>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning</h2><p><strong>Authors:Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</strong></p>
<p>Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/">https://github.com/inclusionAI/AReaL/</a>. </p>
<blockquote>
<p>强化学习（RL）已成为训练大型语言模型（LLM）的主导范式，特别是在推理任务中。对于LLM的有效RL需要大规模并行化，并迫切需要高效的训练系统。大多数现有的用于LLM的大型RL系统都是同步的，批量生成和训练交替进行，每个训练批次中的滚动都是由同一模型生成的。这种方法稳定了RL训练，但系统级效率低下：生成必须等待批次中最长的输出完成才能进行模型更新，导致GPU利用率不足。我们提出了AReaL，一个完全异步的RL系统，它将生成和训练完全解耦。AReaL中的滚动工作人员可以持续生成新的输出而无需等待，而训练工作人员则会在每次收集到一批数据时更新模型。AReaL还包含一系列系统级优化，导致GPU利用率显著提高。为了稳定RL训练，AReaL平衡了滚动和训练工作人员的工作量，以控制数据的陈旧程度，并采用了增强陈旧性的PPO变体以更好地处理过时的训练样本。在数学和代码推理基准测试上的广泛实验表明，与使用相同数量GPU的同步系统相比，AReaL的训练速度提高了2.77倍，并且具有相匹配或更好的最终性能。AReaL的代码可在<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/inclusionAI/AReaL/获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24298v2">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习（RL）已成为训练大型语言模型（LLM）的主导范式，特别是在推理任务中。针对大型语言模型的强化学习需要大规模并行化，并对高效的训练系统提出了迫切需求。现有的大多数大规模RL系统为同步系统，批处理生成与训练交替进行，其中每个训练批次的生成由同一模型完成。这种方法虽然稳定了RL训练，但存在系统级别效率低下的问题：生成必须等待批次中最长的输出完成后才能进行模型更新，导致GPU利用率低下。我们提出了完全异步的RL系统AReaL，它将生成与训练完全解耦。AReaL中的生成工作线程持续生成新输出而无需等待，而训练工作线程在收集到一批数据时更新模型。AReaL还包含一系列系统级优化，大大提高了GPU利用率。为了稳定RL训练，AReaL平衡了生成和训练工作线程的工作量，以控制数据的陈旧程度，并采用了增强陈旧性的PPO变体以更好地处理过时的训练样本。在数理和代码推理基准测试的大量实验表明，与使用相同数量GPU的同步系统相比，AReaL的训练速度提高了最高2.77倍，同时实现了匹配或更好的最终性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习已成为训练大型语言模型的主导方法，特别是在处理推理任务时。</li>
<li>现有大规模RL系统多为同步系统，存在系统级别效率低下的问题。</li>
<li>AReaL是一个完全异步的RL系统，将生成与训练解耦，提高GPU利用率。</li>
<li>AReaL通过平衡生成和训练工作线程的工作量以及采用增强陈旧性的PPO变体来稳定RL训练。</li>
<li>AReaL实现了对同步系统的训练速度提升，最高可达2.77倍。</li>
<li>AReaL在数理和代码推理基准测试中表现出匹配或更好的最终性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-620a3e5ac443051d31ac1863a7c8c611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02ba18add4e6bc82e36cb8bc29cc0b26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58dbbc2ed3d951807c31482be9c871e8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration"><a href="#MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration" class="headerlink" title="MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration"></a>MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration</h2><p><strong>Authors:Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung</strong></p>
<p>In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance. </p>
<blockquote>
<p>近年来，多模态大型语言模型（MLLMs）取得了显著进展，但在多模态推理方面仍面临固有的挑战，这要求多层次（例如，感知、推理）和多粒度（例如，多步推理链）的高级推理。先前关于估计模型信心的工作往往集中在培训和校准的整体响应上，但未能评估每一步推理的信心，导致不理想的幻觉雪球效应。在这项工作中，我们提出了MMBoundary，这是一个新颖框架，通过推理步骤的信心校准提高MLLMs的知识边界意识。为实现这一目标，我们提议结合补充文本和跨模态自我奖励信号来估计MLLM推理过程中每一步的信心。除了使用自我奖励的信心估计信号集对MLLM进行有监督的微调以进行初始信心表达热身之外，我们还引入了具有多种奖励函数的强化学习阶段，以进一步对齐模型知识并校准每一步推理的信心，增强推理链的自我校正能力。经验结果表明，MMBoundary在跨不同领域数据集和指标上显著优于现有方法，平均减少7.5%的多模态信心校准误差，任务性能提高8.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23224v2">PDF</a> 18 pages, ACL 2025</p>
<p><strong>Summary</strong></p>
<p>近期，多模态大型语言模型（MLLMs）在多模态推理方面取得显著进展但仍面临挑战。该研究针对模型置信度评估的问题，提出了MMBoundary框架，通过推理步骤置信度校准提升模型的知识边界意识。该框架结合文本和跨模态自奖励信号，估计MLLM推理过程中每一步的置信度。通过监督微调MLLM和强化学习阶段，进一步对齐模型知识和校准每一步的置信度，增强推理链的自我校正能力。实证结果显示，MMBoundary在跨域数据集和指标上显著优于现有方法，平均减少7.5%的多模态置信度校准误差，任务性能提高8.3%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在多模态推理方面存在挑战，需要多级别和多粒度的推理能力。</li>
<li>现有模型置信度评估方法主要关注整体响应，未对每一步推理的置信度进行评估，导致不希望出现的幻觉累积。</li>
<li>MMBoundary框架通过推理步骤置信度校准提高MLLMs的知识边界意识。</li>
<li>MMBoundary结合文本和跨模态自奖励信号，对每一步的推理置信度进行估计。</li>
<li>MMBoundary采用监督微调与强化学习阶段，对齐模型知识并校准每一步的置信度，增强推理链的自我校正。</li>
<li>实证结果显示，MMBoundary在多种数据集和指标上显著优于现有方法，降低了多模态置信度校准误差并提高任务性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23224">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e84b70fafdec136fbf20f34fc34b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98fa6945fb647e0c77a6f1f4b4455683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724e70cad27d2693c6cc005ac8896228.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Infi-MMR-Curriculum-based-Unlocking-Multimodal-Reasoning-via-Phased-Reinforcement-Learning-in-Multimodal-Small-Language-Models"><a href="#Infi-MMR-Curriculum-based-Unlocking-Multimodal-Reasoning-via-Phased-Reinforcement-Learning-in-Multimodal-Small-Language-Models" class="headerlink" title="Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models"></a>Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models</h2><p><strong>Authors:Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang</strong></p>
<p>Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model’s logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). Resources are available at <a target="_blank" rel="noopener" href="https://huggingface.co/Reallm-Labs/Infi-MMR-3B">https://huggingface.co/Reallm-Labs/Infi-MMR-3B</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步在推理能力方面取得了显著成效，例如DeepSeek-R1，它利用基于规则的强化学习来显著增强逻辑推理能力。然而，将这些成就扩展到多模态大型语言模型（MLLM）却面临重大挑战，对于多模态小型语言模型（MSLM）而言，这些挑战通常更为突出，因为它们通常具有较弱的基础推理能力：一是高质量的多模态推理数据集的稀缺性；二是由于集成视觉处理而导致的推理能力下降；三是直接应用强化学习可能产生复杂而错误的推理过程的风险。为了解决这些挑战，我们设计了一种新型框架Infi-MMR，通过三个精心构建的阶段系统地解锁MSLM的推理潜力，并提出了我们的多模态推理模型Infi-MMR-3B。第一阶段，基础推理激活，利用高质量文本推理数据集来激活和加强模型的逻辑推理能力。第二阶段，跨模态推理适应，利用字幕增强多模态数据来促进推理技能向多模态环境的逐步转移。第三阶段，多模态推理增强，采用精选的无字幕多模态数据来缓解语言偏见，并促进稳健的跨模态推理。Infi-MMR-3B不仅达到了最先进的跨模态数学推理能力（在MathVerse测试集上达到43.68%，在MathVision测试集上达到27.04%，在OlympiadBench上达到21.33%），而且在通用推理能力方面也表现出色（在MathVista测试集上达到67.2%）。相关资源可通过链接<a target="_blank" rel="noopener" href="https://huggingface.co/Reallm-Labs/Infi-MMR-3B%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/Reallm-Labs/Infi-MMR-3B获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23091v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在推理能力方面取得了显著进展，如DeepSeek-R1。然而，将成果扩展到多模态大型语言模型（MLLM）面临挑战，特别是对于多模态小型语言模型（MSLM）而言。为了解决这些挑战，提出了一种新型框架Infi-MMR，通过三个精心设计的阶段解锁MSLM的推理潜力，并推出了多模态推理模型Infi-MMR-3B。该模型经历了基础推理激活、跨模态推理适应和跨模态推理增强三个阶段，实现了先进的跨模态数学推理能力和通用推理能力。相关信息可通过huggingface.co&#x2F;Reallm-Labs&#x2F;Infi-MMR-3B获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在推理能力上取得显著进步，如DeepSeek-R1。</li>
<li>将LLM的进展扩展到多模态大型语言模型（MLLM）面临挑战，特别是对于多模态小型语言模型（MSLM）。</li>
<li>提出新型框架Infi-MMR，通过三个阶段解锁MSLM的推理潜力。</li>
<li>Infi-MMR-3B模型经历基础推理激活、跨模态推理适应和跨模态推理增强三个阶段。</li>
<li>Infi-MMR-3B实现了先进的跨模态数学推理能力和通用推理能力。</li>
<li>该模型在MathVerse testmini、MathVision test、OlympiadBench和MathVista testmini上取得卓越表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-880be88612438bf8aa75db0b8244d20f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-631ffdb5149f238b9186a5dde5d53f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1b618682fd31369a000346471ea885d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f32a620e1dcc6c1169b3c854b451e2bd.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-10  Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79cc1066eda96c070681845e2ee1fb80.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-06-09  LLaDA-V Large Language Diffusion Models with Visual Instruction Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
