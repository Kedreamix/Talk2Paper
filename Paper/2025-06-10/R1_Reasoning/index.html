<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  PuzzleWorld A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-10-æ›´æ–°"><a href="#2025-06-10-æ›´æ–°" class="headerlink" title="2025-06-10 æ›´æ–°"></a>2025-06-10 æ›´æ–°</h1><h2 id="PuzzleWorld-A-Benchmark-for-Multimodal-Open-Ended-Reasoning-in-Puzzlehunts"><a href="#PuzzleWorld-A-Benchmark-for-Multimodal-Open-Ended-Reasoning-in-Puzzlehunts" class="headerlink" title="PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts"></a>PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts</h2><p><strong>Authors:Hengzhi Li, Brendon Jiang, Alexander Naehu, Regan Song, Justin Zhang, Megan Tjandrasuwita, Chanakya Ekbote, Steven-Shine Chen, Adithya Balachandran, Wei Dai, Rebecca Chang, Paul Pu Liang</strong></p>
<p>Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined problem definitions. In contrast to conventional reasoning benchmarks consisting of tasks with clear instructions, puzzlehunts require models to discover the underlying problem structure from multimodal evidence and iterative reasoning, mirroring real-world domains such as scientific discovery, exploratory data analysis, or investigative problem-solving. Despite recent progress in foundation models, their performance on such open-ended settings remains largely untested. In this paper, we introduce PuzzleWorld, a large-scale benchmark of 667 puzzlehunt-style problems designed to assess step-by-step, open-ended, and creative multimodal reasoning. Each puzzle is annotated with the final solution, detailed reasoning traces, and cognitive skill labels, enabling holistic benchmarking and fine-grained diagnostic analysis. Most state-of-the-art models achieve only 1-2% final answer accuracy, with the best model solving only 14% of puzzles and reaching 40% stepwise accuracy. To demonstrate the value of our reasoning annotations, we show that fine-tuning a small model on reasoning traces improves stepwise reasoning from 4% to 11%, while training on final answers alone degrades performance to near zero. Our error analysis reveals that current models exhibit myopic reasoning, are bottlenecked by the limitations of language-based inference, and lack sketching capabilities crucial for visual and spatial reasoning. We release PuzzleWorld at <a target="_blank" rel="noopener" href="https://github.com/MIT-MI/PuzzleWorld">https://github.com/MIT-MI/PuzzleWorld</a> to support future work on building more general, open-ended, and creative reasoning systems. </p>
<blockquote>
<p>è°œé¢˜ç‹©çŒæ˜¯ä¸€ç§ç¼ºä¹æ˜ç¡®é—®é¢˜å®šä¹‰çš„å¤æ‚å¤šæ­¥éª¤è°œé¢˜ã€‚ä¸ä¼ ç»Ÿçš„åŒ…å«æ˜ç¡®æŒ‡ä»¤çš„ä»»åŠ¡åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œè°œé¢˜ç‹©çŒè¦æ±‚æ¨¡å‹ä»å¤šæ¨¡å¼è¯æ®å’Œè¿­ä»£æ¨ç†ä¸­å‘ç°æ½œåœ¨çš„é—®é¢˜ç»“æ„ï¼Œè¿™åæ˜ äº†ç°å®ä¸–ç•Œé¢†åŸŸï¼Œå¦‚ç§‘å­¦å‘ç°ã€æ¢ç´¢æ€§æ•°æ®åˆ†ææˆ–è°ƒæŸ¥è§£å†³é—®é¢˜ã€‚å°½ç®¡åŸºç¡€æ¨¡å‹æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨è¿™æ ·å¼€æ”¾ç¯å¢ƒè®¾ç½®ä¸­çš„è¡¨ç°ä»ä¸»è¦æœªç»éªŒè¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† PuzzleWorldï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« 667 ä¸ªè°œé¢˜ç‹©çŒé£æ ¼é—®é¢˜çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°åˆ†æ­¥ã€å¼€æ”¾æ€§å’Œåˆ›é€ æ€§å¤šæ¨¡å¼æ¨ç†ã€‚æ¯ä¸ªè°œé¢˜éƒ½é™„æœ‰æœ€ç»ˆè§£å†³æ–¹æ¡ˆã€è¯¦ç»†çš„æ¨ç†è½¨è¿¹å’Œè®¤çŸ¥æŠ€èƒ½æ ‡ç­¾ï¼Œä»¥å®ç°æ•´ä½“åŸºå‡†æµ‹è¯•å’Œç²¾ç»†çš„è¯Šæ–­åˆ†æã€‚æœ€å…ˆè¿›çš„æ¨¡å‹æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡åªæœ‰ 1-2%ï¼Œæœ€ä½³æ¨¡å‹è§£å†³çš„è°œé¢˜åªæœ‰ 14%ï¼Œåˆ†æ­¥å‡†ç¡®ç‡ä¹Ÿåªæœ‰ 40%ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ¨ç†æ³¨é‡Šçš„ä»·å€¼ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æ¨ç†è½¨è¿¹ä¸Šå¾®è°ƒçš„å°å‹æ¨¡å‹å¯ä»¥æ”¹è¿›åˆ†æ­¥æ¨ç†ï¼Œä» 4% æé«˜åˆ° 11%ï¼Œè€Œä»…åœ¨æœ€ç»ˆç­”æ¡ˆä¸Šè¿›è¡Œè®­ç»ƒä¼šæŸå®³æ€§èƒ½ï¼Œè¿‘ä¹ä¸ºé›¶ã€‚æˆ‘ä»¬çš„é”™è¯¯åˆ†æè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹çš„æ¨ç†å­˜åœ¨è§†é‡ç‹­éš˜çš„é—®é¢˜ï¼Œå—åˆ°åŸºäºè¯­è¨€çš„æ¨ç†çš„é™åˆ¶ï¼Œå¹¶ä¸”åœ¨è§†è§‰å’Œç©ºé—´æ¨ç†æ–¹é¢ç¼ºä¹è‰å›¾ç»˜åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ <a target="_blank" rel="noopener" href="https://github.com/MIT-MI/PuzzleWorld">https://github.com/MIT-MI/PuzzleWorld</a> å‘å¸ƒäº† PuzzleWorldï¼Œä»¥æ”¯æŒæœªæ¥å…³äºæ„å»ºæ›´é€šç”¨ã€å¼€æ”¾æ€§å’Œåˆ›é€ æ€§æ¨ç†ç³»ç»Ÿçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06211v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è°œé¢˜ç‹©çŒæ˜¯ä¸€ç§ç¼ºä¹æ˜ç¡®é—®é¢˜å®šä¹‰çš„å¤šæ­¥éª¤å¤æ‚è°œé¢˜ç±»å‹ã€‚ä¸ä¼ ç»Ÿæ¨ç†åŸºå‡†æµ‹è¯•ä¸åŒï¼Œè°œé¢˜ç‹©çŒè¦æ±‚æ¨¡å‹ä»å¤šæ¨¡å¼è¯æ®ä¸­å‘ç°æ½œåœ¨çš„é—®é¢˜ç»“æ„ï¼Œå¹¶è¿›è¡Œè¿­ä»£æ¨ç†ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨é¢†åŸŸå°¤ä¸ºæ™®éï¼Œå¦‚ç§‘å­¦å‘ç°ã€æ¢ç´¢æ€§æ•°æ®åˆ†æå’Œé—®é¢˜è§£å†³ã€‚æ–°å‘å¸ƒçš„è°œé¢˜ä¸–ç•Œï¼ˆPuzzleWorldï¼‰å¤§å‹åŸºå‡†æµ‹è¯•åŒ…å«è®¾è®¡ç”¨äºè¯„ä¼°å¼€æ”¾å¼åˆ›é€ æ€§å¤šæ¨¡å¼æ¨ç†çš„667ä¸ªè°œé¢˜ç‹©çŒé—®é¢˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆè¿›æ¨¡å‹åœ¨æœ€åç­”æ¡ˆçš„å‡†ç¡®æ€§ä¸Šåªæœ‰ç™¾åˆ†ä¹‹å‡ çš„æˆç»©ï¼Œè¡¨æ˜ä»–ä»¬åœ¨è§£å†³å¼€æ”¾å¼é—®é¢˜æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ç ”ç©¶è€…æå‡ºä½¿ç”¨æ¨ç†ç—•è¿¹è®­ç»ƒæ¨¡å‹å¯æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚å½“å‰æ¨¡å‹æ˜¾ç¤ºå‡ºåœ¨å¼€æ”¾å’Œåˆ›æ„æ¨ç†ä¸Šçš„ä¸è¶³å’Œç“¶é¢ˆé—®é¢˜ã€‚<strong>Key Takeaways</strong></p>
<p> è°œé¢˜ç‹©çŒæ˜¯ä¸€ç§æ¶‰åŠå¤æ‚å¤šæ­¥éª¤è°œé¢˜çš„æ´»åŠ¨ï¼Œç¼ºä¹æ˜ç¡®çš„é—®é¢˜å®šä¹‰ã€‚<br> è¿™äº›è°œé¢˜è¦æ±‚æ¨¡å‹ä»å¤šæ¨¡å¼è¯æ®ä¸­å‘ç°æ½œåœ¨çš„é—®é¢˜ç»“æ„ï¼Œè¿›è¡Œè¿­ä»£æ¨ç†ï¼Œç¬¦åˆç°å®ä¸–ç•Œæƒ…å¢ƒçš„åº”ç”¨åœºæ™¯åŒ…æ‹¬ç§‘å­¦å‘ç°ã€æ¢ç´¢æ€§æ•°æ®åˆ†æå’Œé—®é¢˜è§£å†³ç­‰ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86cb4afa9805196e0d4b14d712b8ae41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46aac697f2a476e51bdb999eada79514.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4fcbc6c4659e6431c53dc282ad8ff0fd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4870b6df18e5587202db31ba18fcb2be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-133776caf04c55676a0d19862209dbd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1d7a4b90fa073ee91047e754cf51c42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdbac4d1b7f51fd7faf03d3522e4d2d8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ICU-TSB-A-Benchmark-for-Temporal-Patient-Representation-Learning-for-Unsupervised-Stratification-into-Patient-Cohorts"><a href="#ICU-TSB-A-Benchmark-for-Temporal-Patient-Representation-Learning-for-Unsupervised-Stratification-into-Patient-Cohorts" class="headerlink" title="ICU-TSB: A Benchmark for Temporal Patient Representation Learning for   Unsupervised Stratification into Patient Cohorts"></a>ICU-TSB: A Benchmark for Temporal Patient Representation Learning for   Unsupervised Stratification into Patient Cohorts</h2><p><strong>Authors:Dimitrios Proios, Alban Bornet, Anthony Yazdani, Jose F Rodrigues Jr, Douglas Teodoro</strong></p>
<p>Patient stratification identifying clinically meaningful subgroups is essential for advancing personalized medicine through improved diagnostics and treatment strategies. Electronic health records (EHRs), particularly those from intensive care units (ICUs), contain rich temporal clinical data that can be leveraged for this purpose. In this work, we introduce ICU-TSB (Temporal Stratification Benchmark), the first comprehensive benchmark for evaluating patient stratification based on temporal patient representation learning using three publicly available ICU EHR datasets. A key contribution of our benchmark is a novel hierarchical evaluation framework utilizing disease taxonomies to measure the alignment of discovered clusters with clinically validated disease groupings. In our experiments with ICU-TSB, we compared statistical methods and several recurrent neural networks, including LSTM and GRU, for their ability to generate effective patient representations for subsequent clustering of patient trajectories. Our results demonstrate that temporal representation learning can rediscover clinically meaningful patient cohorts; nevertheless, it remains a challenging task, with v-measuring varying from up to 0.46 at the top level of the taxonomy to up to 0.40 at the lowest level. To further enhance the practical utility of our findings, we also evaluate multiple strategies for assigning interpretable labels to the identified clusters. The experiments and benchmark are fully reproducible and available at <a target="_blank" rel="noopener" href="https://github.com/ds4dh/CBMS2025stratification">https://github.com/ds4dh/CBMS2025stratification</a>. </p>
<blockquote>
<p>æ‚£è€…åˆ†å±‚è¯†åˆ«å…·æœ‰ä¸´åºŠæ„ä¹‰çš„äºšç»„æ˜¯æ¨åŠ¨ä¸ªæ€§åŒ–åŒ»å­¦å‘å±•çš„å…³é”®ï¼Œé€šè¿‡æ”¹è¿›çš„è¯Šæ–­å’Œæ²»ç–—ç­–ç•¥æ¥å®ç°ã€‚ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ªé‡ç—‡ç›‘æŠ¤å®¤ï¼ˆICUsï¼‰çš„ï¼ŒåŒ…å«ä¸°å¯Œçš„æ—¶åºä¸´åºŠæ•°æ®ï¼Œå¯ä»¥ä¸ºæ­¤ç›®çš„è€ŒåŠ ä»¥åˆ©ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ICU-TSBï¼ˆæ—¶åºåˆ†å±‚åŸºå‡†ï¼‰ï¼Œè¿™æ˜¯åŸºäºæ—¶åºæ‚£è€…è¡¨ç¤ºå­¦ä¹ è¯„ä¼°æ‚£è€…åˆ†å±‚çš„ç¬¬ä¸€ä¸ªå…¨é¢åŸºå‡†ï¼Œä½¿ç”¨äº†ä¸‰ä¸ªå…¬å¼€å¯ç”¨çš„ICU EHRæ•°æ®é›†ã€‚æˆ‘ä»¬åŸºå‡†çš„å…³é”®è´¡çŒ®åœ¨äºé‡‡ç”¨ç–¾ç—…åˆ†ç±»æ³•çš„æ–°å‹å±‚æ¬¡è¯„ä¼°æ¡†æ¶ï¼Œä»¥è¡¡é‡å‘ç°é›†ç¾¤ä¸ä¸´åºŠéªŒè¯çš„ç–¾ç—…åˆ†ç»„ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨æˆ‘ä»¬çš„ICU-TSBå®éªŒä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ç»Ÿè®¡æ–¹æ³•å’Œå‡ ç§å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆåŒ…æ‹¬LSTMå’ŒGRUï¼‰ç”Ÿæˆæœ‰æ•ˆæ‚£è€…è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä»¥ä¾¿å¯¹éšåçš„æ‚£è€…è½¨è¿¹è¿›è¡Œèšç±»ã€‚ç»“æœè¡¨æ˜ï¼Œæ—¶åºè¡¨ç¤ºå­¦ä¹ èƒ½å¤Ÿé‡æ–°å‘ç°å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æ‚£è€…ç¾¤ä½“ï¼›ç„¶è€Œï¼Œè¿™ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œåœ¨åˆ†ç±»ç­‰çº§çš„æœ€é«˜å±‚æ¬¡ä¸Švå€¼æœ€é«˜è¾¾åˆ°0.46ï¼Œè€Œåœ¨æœ€ä½å±‚æ¬¡ä¸Šæœ€é«˜è¾¾åˆ°0.40ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æˆ‘ä»¬ç ”ç©¶ç»“æœçš„å®é™…æ•ˆç”¨ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ä¸ºå·²è¯†åˆ«çš„é›†ç¾¤åˆ†é…å¯è§£é‡Šæ ‡ç­¾çš„å¤šç§ç­–ç•¥ã€‚å®éªŒå’ŒåŸºå‡†éƒ½æ˜¯å®Œå…¨å¯å¤åˆ¶çš„ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ds4dh/CBMS2025stratification%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ds4dh/CBMS2025stratificationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06192v1">PDF</a> 6 pages 1 table 6 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ICU-TSBï¼ˆåŸºäºæ—¶åºåˆ†å±‚åŸºå‡†ï¼‰çš„é‡è¦æ€§ï¼Œå®ƒæ˜¯åˆ©ç”¨é‡ç—‡ç›‘æŠ¤ç—…æˆ¿ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰æ•°æ®è¿›è¡Œæ‚£è€…åˆ†å±‚çš„é¦–ä¸ªå…¨é¢åŸºå‡†ã€‚è¯¥åŸºå‡†é€šè¿‡åˆ©ç”¨ç–¾ç—…åˆ†ç±»æ³•ï¼Œé‡‡ç”¨å±‚æ¬¡è¯„ä¼°æ¡†æ¶æ¥è¡¡é‡å‘ç°çš„èšç±»ä¸ä¸´åºŠéªŒè¯çš„ç–¾ç—…åˆ†ç»„çš„å¯¹é½ç¨‹åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ—¶åºè¡¨ç¤ºå­¦ä¹ èƒ½å¤Ÿé‡æ–°å‘ç°å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æ‚£è€…ç¾¤ä½“ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ï¼Œåœ¨åˆ†ç±»çš„æœ€é«˜å’Œæœ€ä½çº§åˆ«ä¸Šå¾—åˆ†ä¸åŒã€‚ä¸ºæé«˜ç»“æœçš„å®ç”¨æ€§ï¼Œä½œè€…è¿˜è¯„ä¼°äº†ä¸ºè¯†åˆ«å‡ºçš„èšç±»åˆ†é…å¯è§£é‡Šæ ‡ç­¾çš„å¤šç§ç­–ç•¥ã€‚è¯¥å®éªŒå’ŒåŸºå‡†æµ‹è¯•å®Œå…¨å¯é‡ç°ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªåœ¨çº¿é“¾æ¥ä»¥ä¾›è¿›ä¸€æ­¥äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‚£è€…åˆ†å±‚å¯¹äºæ¨åŠ¨ä¸ªæ€§åŒ–åŒ»å­¦è‡³å…³é‡è¦ï¼Œæœ‰åŠ©äºæ”¹è¿›è¯Šæ–­å’Œæ²»ç–—ç­–ç•¥ã€‚</li>
<li>ICU-TSBæ˜¯é¦–ä¸ªåˆ©ç”¨é‡ç—‡ç›‘æŠ¤ç—…æˆ¿ï¼ˆICUï¼‰ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰æ•°æ®è¿›è¡Œæ‚£è€…åˆ†å±‚çš„å…¨é¢åŸºå‡†ã€‚</li>
<li>ICU-TSBé‡‡ç”¨å±‚æ¬¡è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆç–¾ç—…åˆ†ç±»æ³•æ¥è¡¡é‡å‘ç°çš„èšç±»ä¸ä¸´åºŠéªŒè¯çš„ç–¾ç—…åˆ†ç»„çš„åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>æ—¶åºè¡¨ç¤ºå­¦ä¹ èƒ½å¤Ÿå‘ç°å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æ‚£è€…ç¾¤ä½“ï¼Œä½†åˆ†å±‚ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>åœ¨åˆ†ç±»çš„æœ€é«˜å’Œæœ€ä½çº§åˆ«ä¸Šï¼Œè¯„ä¼°ç»“æœå­˜åœ¨å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
<li>ä¸ºæé«˜ç»“æœçš„å®ç”¨æ€§ï¼Œä½œè€…æ¢ç´¢äº†å¤šç§ç­–ç•¥æ¥ä¸ºè¯†åˆ«å‡ºçš„èšç±»åˆ†é…å¯è§£é‡Šçš„æ ‡ç­¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f172cb307045ef8974bb53f57cedd36e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4310f8db48905463ddafe946c4c63c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1065586abe52320f3fdcff70eaf97799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a927a5b254c712e9ca2c9ac12d0d9a3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d96d61b24e60e96f1e9620b02b25017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d56e522b8ddeb99b7848508ef585d850.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Table-r1-Self-supervised-and-Reinforcement-Learning-for-Program-based-Table-Reasoning-in-Small-Language-Models"><a href="#Table-r1-Self-supervised-and-Reinforcement-Learning-for-Program-based-Table-Reasoning-in-Small-Language-Models" class="headerlink" title="Table-r1: Self-supervised and Reinforcement Learning for Program-based   Table Reasoning in Small Language Models"></a>Table-r1: Self-supervised and Reinforcement Learning for Program-based   Table Reasoning in Small Language Models</h2><p><strong>Authors:Rihui Jin, Zheyu Xin, Xing Xie, Zuoyi Li, Guilin Qi, Yongrui Chen, Xinbang Dai, Tongtong Wu, Gholamreza Haffari</strong></p>
<p>Table reasoning (TR) requires structured reasoning over semi-structured tabular data and remains challenging, particularly for small language models (SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs (LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR), which circumvents key limitations of text-based TR (T-TR), notably in numerical reasoning, by generating executable programs. However, applying P-TR to SLMs introduces two challenges: (i) vulnerability to heterogeneity in table layouts, and (ii) inconsistency in reasoning due to limited code generation capability. We propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1 introduces an innovative self-supervised learning task, Layout Transformation Inference, to improve tabular layout generalization from a programmatic view. Stage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization, enhancing P-TR consistency while allowing dynamic fallback to T-TR when needed. Experiments on four TR benchmarks demonstrate that Table-r1 outperforms all SLM-based methods, achieving at least a 15% accuracy improvement over the base model (LLaMA-8B) across all datasets and reaching performance competitive with LLMs. </p>
<blockquote>
<p>è¡¨æ ¼æ¨ç†ï¼ˆTRï¼‰è¦æ±‚å¯¹åŠç»“æ„åŒ–è¡¨æ ¼æ•°æ®è¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼Œä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼Œä¾‹å¦‚LLaMA-8Bï¼‰è€Œè¨€ï¼Œç”±äºä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼Œä¾‹å¦‚GPT-4oï¼‰ç›¸æ¯”ï¼Œå®ƒä»¬çš„å®¹é‡æœ‰é™ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºç¨‹åºçš„TRï¼ˆP-TRï¼‰ï¼Œé€šè¿‡ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºï¼Œé¿å…äº†åŸºäºæ–‡æœ¬çš„TRï¼ˆT-TRï¼‰åœ¨æ•°å€¼æ¨ç†ç­‰æ–¹é¢çš„å…³é”®å±€é™æ€§ã€‚ç„¶è€Œï¼Œå°†P-TRåº”ç”¨äºSLMå¼•å…¥äº†ä¸¤ä¸ªæŒ‘æˆ˜ï¼šï¼ˆiï¼‰å¯¹è¡¨æ ¼å¸ƒå±€å¼‚è´¨æ€§çš„è„†å¼±æ€§ï¼Œï¼ˆiiï¼‰ç”±äºæœ‰é™çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å¯¼è‡´çš„æ¨ç†ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Table-r1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºSLMè®¾è®¡çš„ä¸¤é˜¶æ®µP-TRæ–¹æ³•ã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œå³â€œå¸ƒå±€è½¬æ¢æ¨ç†â€ï¼Œä»ç¨‹åºè§†è§’æé«˜è¡¨æ ¼å¸ƒå±€æ¦‚æ‹¬èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ··åˆèŒƒå¼çš„é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–å˜ä½“ï¼Œæé«˜P-TRçš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶æ ¹æ®éœ€è¦å…è®¸åŠ¨æ€é€€å›åˆ°T-TRã€‚åœ¨å››ä¸ªTRåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTable-r1ä¼˜äºæ‰€æœ‰SLMæ–¹æ³•ï¼Œåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè‡³å°‘æ¯”åŸºç¡€æ¨¡å‹ï¼ˆLLaMA-8Bï¼‰æé«˜äº†15%çš„å‡†ç¡®ç‡ï¼Œä¸”æ€§èƒ½ä¸LLMç›¸ç«äº‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06137v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è¡¨æ ¼æ¨ç†ï¼ˆTRï¼‰çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨æ•°å€¼æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œç ”ç©¶äº†ä¸€ç§åŸºäºç¨‹åºåŒ–çš„è¡¨æ ¼æ¨ç†ï¼ˆP-TRï¼‰æ–¹æ³•ã€‚ä¸ºåº”å¯¹SLMsåœ¨P-TRåº”ç”¨ä¸­çš„è¡¨æ ¼å¸ƒå±€å¤šæ ·æ€§å’Œæ¨ç†ä¸ä¸€è‡´é—®é¢˜ï¼Œæå‡ºäº†åä¸ºTable-r1çš„ä¸¤é˜¶æ®µæ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä»»åŠ¡æé«˜è¡¨æ ¼å¸ƒå±€æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨æ··åˆèŒƒå¼çš„ä¼˜åŒ–ç­–ç•¥å¢å¼ºæ¨ç†ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒTable-r1åœ¨å››ä¸ªTRåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæ‰€æœ‰SLMæ–¹æ³•ï¼Œè‡³å°‘åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå®ç°äº†15%çš„å‡†ç¡®ç‡æå‡ï¼Œæ€§èƒ½ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼æ¨ç†ï¼ˆTRï¼‰å¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰è€Œè¨€å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å€¼æ¨ç†æ–¹é¢ã€‚</li>
<li>åŸºäºç¨‹åºåŒ–çš„è¡¨æ ¼æ¨ç†ï¼ˆP-TRï¼‰æ–¹æ³•å¯ä»¥å…‹æœæ–‡æœ¬åŸºäºçš„è¡¨æ ¼æ¨ç†ï¼ˆT-TRï¼‰çš„å±€é™æ€§ã€‚</li>
<li>SLMåœ¨P-TRåº”ç”¨ä¸­é¢ä¸´ä¸¤ä¸ªæŒ‘æˆ˜ï¼šè¡¨æ ¼å¸ƒå±€çš„å¤šæ ·æ€§å’Œæ¨ç†çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>Table-r1æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„P-TRæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä»»åŠ¡æé«˜è¡¨æ ¼å¸ƒå±€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨æ··åˆèŒƒå¼çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¢å¼ºP-TRçš„æ¨ç†ä¸€è‡´æ€§ï¼Œå¹¶èƒ½åœ¨éœ€è¦æ—¶åŠ¨æ€å›é€€åˆ°T-TRã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7e5fac4133efab2fa678853c3d00d81c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ee554d8c6ba8383c325c65597b68123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5bee06f4a0364904434d9e60ed79eb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3ff8b2e78c951e622dd8d649d9b8e5b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VideoChat-A1-Thinking-with-Long-Videos-by-Chain-of-Shot-Reasoning"><a href="#VideoChat-A1-Thinking-with-Long-Videos-by-Chain-of-Shot-Reasoning" class="headerlink" title="VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning"></a>VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning</h2><p><strong>Authors:Zikang Wang, Boyu Chen, Zhengrong Yue, Yi Wang, Yu Qiao, Limin Wang, Yali Wang</strong></p>
<p>The recent advance in video understanding has been driven by multimodal large language models (MLLMs). But these MLLMs are good at analyzing short videos, while suffering from difficulties in understanding videos with a longer context. To address this difficulty, several agent paradigms have recently been proposed, using MLLMs as agents for retrieving extra contextual knowledge in a long video. However, most existing agents ignore the key fact that a long video is composed with multiple shots, i.e., to answer the user question from a long video, it is critical to deeply understand its relevant shots like human. Without such insight, these agents often mistakenly find redundant even noisy temporal context, restricting their capacity for long video understanding. To fill this gap, we propose VideoChat-A1, a novel long video agent paradigm. Different from the previous works, our VideoChat-A1 can deeply think with long videos, via a distinct chain-of-shot reasoning paradigm. More specifically, it can progressively select the relevant shots of user question, and look into these shots in a coarse-to-fine partition. By multi-modal reasoning along the shot chain, VideoChat-A1 can effectively mimic step-by-step human thinking process, allowing to interactively discover preferable temporal context for thoughtful understanding in long videos. Extensive experiments show that, our VideoChat-A1 achieves the state-of-the-art performance on the mainstream long video QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema, outperforming its strong baselines (e.g., Intern2.5VL-8B and InternVideo2.5-8B), by up to 10.8% and 6.2%. Compared to leading close-source GPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with 7% input frames and 12% inference time on average. </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘ç†è§£çš„è¿›æ­¥å¾—ç›Šäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨åŠ¨ã€‚ä½†è¿™äº›MLLMsæ“…é•¿åˆ†æçŸ­è§†é¢‘ï¼Œåœ¨ç†è§£é•¿è§†é¢‘æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€éš¾é¢˜ï¼Œæœ€è¿‘æå‡ºäº†å‡ ç§ä½¿ç”¨MLLMsä½œä¸ºä»£ç†æ£€ç´¢é•¿è§†é¢‘é¢å¤–ä¸Šä¸‹æ–‡çŸ¥è¯†çš„ä»£ç†èŒƒå¼ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ä»£ç†å¿½ç•¥äº†é•¿è§†é¢‘ç”±å¤šä¸ªé•œå¤´ç»„æˆçš„å…³é”®äº‹å®ï¼Œå³è¦å›ç­”ç”¨æˆ·å…³äºé•¿è§†é¢‘çš„é—®é¢˜ï¼Œéœ€è¦åƒäººç±»ä¸€æ ·æ·±åˆ»åœ°ç†è§£å…¶ç›¸å…³é•œå¤´ã€‚æ²¡æœ‰è¿™æ ·çš„æ´å¯ŸåŠ›ï¼Œè¿™äº›ä»£ç†å¾€å¾€ä¼šé”™è¯¯åœ°æ‰¾åˆ°å†—ä½™ç”šè‡³å˜ˆæ‚çš„æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œé™åˆ¶äº†å®ƒä»¬å¯¹é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†VideoChat-A1ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é•¿è§†é¢‘ä»£ç†èŒƒå¼ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„VideoChat-A1å¯ä»¥é€šè¿‡ç‹¬ç‰¹çš„é•œå¤´é“¾æ¨ç†èŒƒå¼å¯¹é•¿è§†é¢‘è¿›è¡Œæ·±åº¦æ€è€ƒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå®ƒå¯ä»¥é€æ­¥é€‰æ‹©ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„é•œå¤´ï¼Œå¹¶ä»¥ä»ç²—åˆ°ç»†çš„åˆ†åŒºæŸ¥çœ‹è¿™äº›é•œå¤´ã€‚é€šè¿‡æ²¿ç€é•œå¤´é“¾è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼ŒVideoChat-A1å¯ä»¥æœ‰æ•ˆåœ°æ¨¡ä»¿äººç±»çš„é€æ­¥æ€è€ƒè¿‡ç¨‹ï¼Œå…è®¸äº¤äº’å¼åœ°å‘ç°ç”¨æˆ·é—®é¢˜æ‰€éœ€çš„å¯é€‰æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œä»¥åœ¨é•¿è§†é¢‘ä¸­å®ç°æ·±æ€ç†Ÿè™‘çš„ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„VideoChat-A1åœ¨ä¸»æµçš„é•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¾‹å¦‚åœ¨VideoMMEä¸Šè¾¾åˆ°äº†77.0ï¼Œåœ¨EgoSchemaä¸Šè¾¾åˆ°äº†70.1ï¼Œè¶…è¶Šäº†å…¶å¼ºå¤§çš„åŸºå‡†çº¿ï¼ˆä¾‹å¦‚Intern2.5VL-8Bå’ŒInternVideo2.5-8Bï¼‰ï¼Œæœ€å¤šé«˜è¾¾10.8%å’Œ6.2%ã€‚ä¸é¢†å…ˆçš„é—­æºGPT-4oå’ŒGemini 1.5 Proç›¸æ¯”ï¼ŒVideoChat-A1å…·æœ‰ç«äº‰åŠ›çš„å‡†ç¡®æ€§ï¼Œä½†å¹³å‡ä½¿ç”¨äº†7%çš„è¾“å…¥å¸§å’Œå‡å°‘äº†12%çš„æ¨ç†æ—¶é—´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06097v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“è§†é¢‘ç†è§£é¢†åŸŸä¸­å­˜åœ¨ä¸€é¡¹æŒ‘æˆ˜ï¼Œå³å¤„ç†é•¿è§†é¢‘çš„ç†è§£é—®é¢˜ã€‚å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æœ‰æ•ˆåœ°å¤„ç†å¤šä¸ªé•œå¤´ä¸­çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºVideoChat-A1çš„æ–°å‹é•¿è§†é¢‘ä»£ç†èŒƒå¼ã€‚å®ƒé€šè¿‡ç‹¬ç‰¹çš„é•œå¤´é“¾æ¨ç†æ¨¡å¼ï¼Œèƒ½å¤Ÿé€æ­¥é€‰æ‹©ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„é•œå¤´ï¼Œå¹¶åœ¨è¿™äº›é•œå¤´ä¸­è¿›è¡Œç²—åˆ°ç»†çš„æ£€ç´¢åˆ†æã€‚è¯¥æ¨¡å‹åœ¨ä¸»æµçš„é•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ç›¸è¾ƒäºå…¶ä»–å¼ºå¤§åŸºçº¿ï¼Œå…¶å‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚åŒæ—¶ï¼ŒVideoChat-A1åœ¨è¾“å…¥å¸§å’Œæ¨ç†æ—¶é—´ä¸Šä¹Ÿæœ‰ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>VideoChat-A1é€šè¿‡ç‹¬ç‰¹çš„é•œå¤´é“¾æ¨ç†æ¨¡å¼æ¥å¤„ç†é•¿è§†é¢‘ã€‚</li>
<li>VideoChat-A1èƒ½å¤Ÿé€æ­¥é€‰æ‹©ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„é•œå¤´ï¼Œå¹¶è¿›è¡Œç²—åˆ°ç»†çš„æ£€ç´¢åˆ†æã€‚</li>
<li>VideoChat-A1åœ¨ä¸»æµçš„é•¿è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>VideoChat-A1ç›¸è¾ƒäºå…¶ä»–å¼ºå¤§åŸºçº¿ï¼Œå‡†ç¡®æ€§æ˜¾è‘—æé«˜ã€‚</li>
<li>VideoChat-A1åœ¨è¾“å…¥å¸§å’Œæ¨ç†æ—¶é—´æ–¹é¢è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06097">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14f23fec431be274ecd7d316796faa17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62a4be4b0895d782dd279f7f48b4319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9df71f687a89026d3da51d3d8c4e9e68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b125df293444f14b08c3f47c3e8e1d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-173e8b8ab404f4deb5e766bf042fccf3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Route-and-Reason-Scaling-Large-Language-Model-Reasoning-with-Reinforced-Model-Router"><a href="#Route-and-Reason-Scaling-Large-Language-Model-Reasoning-with-Reinforced-Model-Router" class="headerlink" title="Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced   Model Router"></a>Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced   Model Router</h2><p><strong>Authors:Chenyang Shao, Xinyang Liu, Yutang Lin, Fengli Xu, Yong Li</strong></p>
<p>Multi-step reasoning has proven essential for enhancing the problem-solving capabilities of Large Language Models (LLMs) by decomposing complex tasks into intermediate steps, either explicitly or implicitly. Extending the reasoning chain at test time through deeper thought processes or broader exploration, can furthur improve performance, but often incurs substantial costs due to the explosion in token usage. Yet, many reasoning steps are relatively simple and can be handled by more efficient smaller-scale language models (SLMs). This motivates hybrid approaches that allocate subtasks across models of varying capacities. However, realizing such collaboration requires accurate task decomposition and difficulty-aware subtask allocation, which is challenging. To address this, we propose R2-Reasoner, a novel framework that enables collaborative reasoning across heterogeneous LLMs by dynamically routing sub-tasks based on estimated complexity. At the core of our framework is a Reinforced Model Router, composed of a task decomposer and a subtask allocator. The task decomposer segments complex input queries into logically ordered subtasks, while the subtask allocator assigns each subtask to the most appropriate model, ranging from lightweight SLMs to powerful LLMs, balancing accuracy and efficiency. To train this router, we introduce a staged pipeline that combines supervised fine-tuning on task-specific datasets with Group Relative Policy Optimization algorithm, enabling self-supervised refinement through iterative reinforcement learning. Extensive experiments across four challenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85% while maintaining or surpassing baseline accuracy. Our framework paves the way for more cost-effective and adaptive LLM reasoning. The code is open-source at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2_Reasoner">https://anonymous.4open.science/r/R2_Reasoner</a> . </p>
<blockquote>
<p>å¤šæ­¥æ¨ç†é€šè¿‡æ˜¾å¼æˆ–éšå¼åœ°å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºä¸­é—´æ­¥éª¤ï¼Œè¢«è¯æ˜å¯¹äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›æ˜¯è‡³å…³é‡è¦çš„ã€‚é€šè¿‡åœ¨æµ‹è¯•æ—¶æ‰©å±•æ¨ç†é“¾ï¼Œè¿›è¡Œæ›´æ·±å…¥çš„æ€è€ƒè¿‡ç¨‹æˆ–æ›´å¹¿æ³›çš„æ¢ç´¢ï¼Œå¯ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œä½†è¿™é€šå¸¸ç”±äºæ ‡è®°ä½¿ç”¨é‡çš„æ¿€å¢è€Œäº§ç”Ÿå·¨å¤§æˆæœ¬ã€‚ç„¶è€Œï¼Œè®¸å¤šæ¨ç†æ­¥éª¤ç›¸å¯¹ç®€å•ï¼Œå¯ä»¥é€šè¿‡æ›´é«˜æ•ˆçš„å°è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰æ¥å¤„ç†ã€‚è¿™æ¨åŠ¨äº†æ··åˆæ–¹æ³•çš„ä½¿ç”¨ï¼Œè¯¥æ–¹æ³•åœ¨ä¸åŒå®¹é‡çš„æ¨¡å‹ä¹‹é—´åˆ†é…å­ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ç°è¿™ç§åä½œéœ€è¦å‡†ç¡®çš„ä»»åŠ¡åˆ†è§£å’Œéš¾åº¦æ„ŸçŸ¥çš„å­ä»»åŠ¡åˆ†é…ï¼Œè¿™æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†R2æ¨ç†æœºï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡åŸºäºä¼°è®¡å¤æ‚åº¦çš„åŠ¨æ€è·¯ç”±åœ¨å¼‚æ„LLMä¹‹é—´è¿›è¡Œåä½œæ¨ç†ã€‚æˆ‘ä»¬æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¼ºåŒ–æ¨¡å‹è·¯ç”±å™¨ï¼Œç”±ä»»åŠ¡åˆ†è§£å™¨å’Œå­ä»»åŠ¡åˆ†é…å™¨ç»„æˆã€‚ä»»åŠ¡åˆ†è§£å™¨å°†å¤æ‚çš„è¾“å…¥æŸ¥è¯¢åˆ†å‰²æˆé€»è¾‘ä¸Šæœ‰åºçš„å­ä»»åŠ¡ï¼Œè€Œå­ä»»åŠ¡åˆ†é…å™¨å°†æ¯ä¸ªå­ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ï¼Œä»è½»é‡çº§çš„SLMåˆ°åŠŸèƒ½å¼ºå¤§çš„LLMï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä¸ºäº†è®­ç»ƒè¿™ä¸ªè·¯ç”±å™¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ†é˜¶æ®µç®¡é“ï¼Œç»“åˆé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ç›‘ç£å¾®è°ƒæ•°æ®é›†å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£å¼ºåŒ–å­¦ä¹ å®ç°è‡ªæˆ‘ç›‘ç£çš„ç»†åŒ–ã€‚åœ¨å››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒR2æ¨ç†æœºåœ¨ä¿æŒæˆ–è¶…è¿‡åŸºçº¿å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œé™ä½äº†86.85%çš„APIæˆæœ¬ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºæ›´å…·æˆæœ¬æ•ˆç›Šå’Œé€‚åº”æ€§çš„LLMæ¨ç†é“ºå¹³äº†é“è·¯ã€‚ä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/R2_Reasoner%E3%80%82">https://anonymous.4open.science/r/R2_Reasonerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05901v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ­¥æ¨ç†å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é—®é¢˜è§£å†³èƒ½åŠ›è‡³å…³é‡è¦ï¼Œé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£æˆä¸­é—´æ­¥éª¤ï¼Œæ— è®ºæ˜¯æ˜¾å¼è¿˜æ˜¯éšå¼ã€‚æ‰©å±•æ¨ç†é“¾å¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†ä¼šå¢åŠ ä»¤ç‰Œä½¿ç”¨é‡ï¼Œå¸¦æ¥å·¨å¤§æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†R2-Reasoneræ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€è·¯ç”±å­ä»»åŠ¡å®ç°è·¨ä¸åŒå®¹é‡æ¨¡å‹çš„åä½œæ¨ç†ã€‚å…¶æ ¸å¿ƒæ˜¯å¼ºåŒ–æ¨¡å‹è·¯ç”±å™¨ï¼Œç”±ä»»åŠ¡åˆ†è§£å™¨å’Œå­ä»»åŠ¡åˆ†é…å™¨ç»„æˆã€‚ä»»åŠ¡åˆ†è§£å™¨å°†å¤æ‚è¾“å…¥æŸ¥è¯¢åˆ†å‰²æˆé€»è¾‘æœ‰åºçš„å­ä»»åŠ¡ï¼Œè€Œå­ä»»åŠ¡åˆ†é…å™¨åˆ™å°†æ¯ä¸ªå­ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„æ¨¡å‹ã€‚é€šè¿‡ç»“åˆä»»åŠ¡ç‰¹å®šæ•°æ®é›†çš„ç›‘ç£å¾®è°ƒä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•çš„åˆ†é˜¶æ®µç®¡é“ï¼Œå®ç°äº†è‡ªæˆ‘ç›‘ç£çš„ç²¾ç»†åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒR2-Reasoneråœ¨é™ä½86.85%çš„APIæˆæœ¬çš„åŒæ—¶ï¼Œç»´æŒæˆ–æé«˜äº†åŸºçº¿å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ­¥æ¨ç†å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜è§£å†³èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>é€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£æˆä¸­é—´æ­¥éª¤ï¼Œå¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨ç†é“¾çš„æ‰©å±•ä¼šå¢åŠ ä»¤ç‰Œä½¿ç”¨é‡ï¼Œå¸¦æ¥æˆæœ¬é—®é¢˜ã€‚</li>
<li>R2-Reasoneræ¡†æ¶é€šè¿‡åŠ¨æ€è·¯ç”±å­ä»»åŠ¡å®ç°è·¨ä¸åŒå®¹é‡æ¨¡å‹çš„åä½œæ¨ç†ã€‚</li>
<li>R2-Reasoneræ¡†æ¶åŒ…æ‹¬ä»»åŠ¡åˆ†è§£å™¨å’Œå­ä»»åŠ¡åˆ†é…å™¨ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚</li>
<li>R2-Reasonerç»“åˆäº†ç›‘ç£å¾®è°ƒä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œå®ç°äº†è‡ªæˆ‘ç›‘ç£çš„ç²¾ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05901">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-152ed6dc60c37110f590d03f6d30c05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaaa9badb975940ba17081306fbd30b0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FinanceReasoning-Benchmarking-Financial-Numerical-Reasoning-More-Credible-Comprehensive-and-Challenging"><a href="#FinanceReasoning-Benchmarking-Financial-Numerical-Reasoning-More-Credible-Comprehensive-and-Challenging" class="headerlink" title="FinanceReasoning: Benchmarking Financial Numerical Reasoning More   Credible, Comprehensive and Challenging"></a>FinanceReasoning: Benchmarking Financial Numerical Reasoning More   Credible, Comprehensive and Challenging</h2><p><strong>Authors:Zichen Tang, Haihong E, Ziyan Ma, Haoyang He, Jiacheng Liu, Zhongjun Yang, Zihua Rong, Rongjin Li, Kun Ji, Qing Huang, Xinyang Hu, Yang Liu, Qianhe Zheng</strong></p>
<p>We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMsâ€™ financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMsâ€™ performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†FinanceReasoningï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹åœ¨è´¢åŠ¡æ•°å€¼æ¨ç†é—®é¢˜ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„å·¥ä½œæä¾›äº†ä¸‰ä¸ªå…³é”®è¿›å±•ã€‚(1) å¯é æ€§ï¼šæˆ‘ä»¬ä»å››ä¸ªå…¬å…±æ•°æ®é›†ä¸­æ›´æ–°äº†15.6%çš„é—®é¢˜ï¼Œä¸º908ä¸ªæ–°é—®é¢˜æä¾›äº†è¯¦ç»†çš„Pythonè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸¥æ ¼å®Œå–„äº†è¯„ä¼°æ ‡å‡†ã€‚è¿™èƒ½å¤Ÿå‡†ç¡®è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ”¹è¿›æƒ…å†µã€‚(2) å…¨é¢æ€§ï¼šFinanceReasoningæ¶µç›–äº†67.8%çš„è´¢åŠ¡æ¦‚å¿µå’Œå…¬å¼ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†3133ä¸ªPythonæ ¼å¼çš„å‡½æ•°ï¼Œé€šè¿‡ç²¾ç»†çš„çŸ¥è¯†æ¥æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„è´¢åŠ¡æ¨ç†èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼ŒGPT-4oä»83.2%æå‡åˆ°91.6%ï¼‰ã€‚(3) æŒ‘æˆ˜æ€§ï¼šæ¨¡å‹éœ€è¦åº”ç”¨å¤šä¸ªè´¢åŠ¡å…¬å¼å¯¹238ä¸ªéš¾é¢˜è¿›è¡Œç²¾ç¡®æ•°å€¼æ¨ç†ã€‚è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆå³OpenAI o1 with PoTï¼‰çš„å‡†ç¡®ç‡ä¸º89.1%ï¼Œä½†å¤§å‹æ¨ç†æ¨¡å‹åœ¨æ•°å€¼ç²¾åº¦æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯æ˜ï¼Œç»“åˆReasonerå’ŒProgrammeræ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒDeepSeek-R1ä»83.2%æå‡åˆ°87.8%ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæœªæ¥åœ¨ç‰¹å®šé¢†åŸŸå¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¯„ä¼°å’Œæé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05828v1">PDF</a> Accepted by ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>FinanceReasoningä½œä¸ºä¸€ç§æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹åœ¨é‡‘èæ•°å€¼æ¨ç†é—®é¢˜ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œå…¶å…·å¤‡å¯ä¿¡åº¦ã€å…¨é¢æ€§å’ŒæŒ‘æˆ˜æ€§ä¸‰å¤§ä¼˜åŠ¿ã€‚æ›´æ–°åçš„é¢˜ç›®æ¶µç›–äº†é‡‘èæ¦‚å¿µå’Œå…¬å¼çš„ç»å¤§éƒ¨åˆ†å†…å®¹ï¼Œå¯¹æ¨¡å‹çš„é‡‘èæ¨ç†èƒ½åŠ›æå‡ºäº†è¾ƒé«˜è¦æ±‚ï¼Œå¹¶ç»“åˆäº†Reasonerå’ŒProgrammeræ¨¡å‹ä»¥æå‡æ¨¡å‹æ€§èƒ½ã€‚æ­¤é¡¹ç ”ç©¶ä¸ºæœªæ¥åœ¨ç‰¹å®šé¢†åŸŸå¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¯„ä¼°å’Œæ”¹è¿›å¤§å‹æ¨ç†æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FinanceReasoningä½œä¸ºä¸€ç§åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹åœ¨é‡‘èæ•°å€¼æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼ŒFinanceReasoningå…·å¤‡æ›´é«˜çš„å¯ä¿¡åº¦å’Œæ›´å…¨é¢çš„å†…å®¹è¦†ç›–ï¼Œæ¶µç›–äº†é‡‘èæ¦‚å¿µå’Œå…¬å¼çš„ç»å¤§éƒ¨åˆ†å†…å®¹ã€‚</li>
<li>FinanceReasoningå¯¹æ¨¡å‹çš„é‡‘èæ¨ç†èƒ½åŠ›æå‡ºäº†æŒ‘æˆ˜ï¼Œè¦æ±‚æ¨¡å‹åº”ç”¨å¤šä¸ªé‡‘èå…¬å¼è¿›è¡Œç²¾ç¡®æ•°å€¼æ¨ç†ã€‚</li>
<li>æ›´æ–°åçš„é¢˜ç›®åŒ…æ‹¬è¯¦ç»†çš„Pythonè§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸¥æ ¼ç»†åŒ–äº†è¯„ä¼°æ ‡å‡†ï¼Œä»¥å‡†ç¡®è¯„ä¼°æ¨¡å‹çš„æ¨ç†æ”¹è¿›æƒ…å†µã€‚</li>
<li>ç»“åˆReasonerå’ŒProgrammeræ¨¡å‹èƒ½æœ‰æ•ˆæå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ç›®å‰æœ€ä½³æ¨¡å‹åœ¨FinanceReasoningä¸Šçš„å‡†ç¡®ç‡ä¸º89.1%ï¼Œä½†ä»å­˜åœ¨æ•°å€¼ç²¾åº¦æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9502d40b69102bf5dd0e6faecd464a01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b443ac2f8c4e3266d250d1d5705b76b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62c0a5796f457a2c146ba2364ec99f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-298051ed548a0fb03e4f6df4f38da31e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81c797722952189acfc7fba68d41c174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269c4e73bd089e8071b0f67b0e28500e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e49ae44b7172c613a2cb55d825ccdd9f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MAPLE-Multi-Agent-Adaptive-Planning-with-Long-Term-Memory-for-Table-Reasoning"><a href="#MAPLE-Multi-Agent-Adaptive-Planning-with-Long-Term-Memory-for-Table-Reasoning" class="headerlink" title="MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table   Reasoning"></a>MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table   Reasoning</h2><p><strong>Authors:Ye Bai, Minghan Wang, Thuy-Trang Vu</strong></p>
<p>Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones. </p>
<blockquote>
<p>åŸºäºè¡¨æ ¼çš„é—®é¢˜å›ç­”éœ€è¦å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•éæ¨ç†ä¸­éš¾ä»¥å®ç°ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚æ€ç»´é“¾æ¨ç†å’Œé—®é¢˜åˆ†è§£ï¼Œç¼ºä¹é”™è¯¯æ£€æµ‹æœºåˆ¶å¹¶ä¸¢å¼ƒè§£å†³é—®é¢˜æ—¶çš„ç»éªŒï¼Œè¿™ä¸äººç±»è§£å†³æ­¤ç±»é—®é¢˜çš„æ–¹å¼å½¢æˆé²œæ˜å¯¹æ¯”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MAPLEï¼ˆå…·æœ‰é•¿æœŸè®°å¿†çš„å¤šæ™ºèƒ½ä½“è‡ªé€‚åº”è§„åˆ’ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ä¸“é—¨è®¾è®¡çš„è®¤çŸ¥æ™ºèƒ½ä½“åœ¨åé¦ˆé©±åŠ¨å¾ªç¯ä¸­å·¥ä½œæ¥æ¨¡æ‹Ÿäººç±»è§£å†³é—®é¢˜çš„æ–°å‹æ¡†æ¶ã€‚MAPLEé›†æˆäº†å››ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ReActèŒƒå¼è¿›è¡Œæ¨ç†çš„æ±‚è§£å™¨ï¼Œï¼ˆ2ï¼‰ç”¨äºç­”æ¡ˆéªŒè¯çš„æ£€æŸ¥å™¨ï¼Œï¼ˆ3ï¼‰ç”¨äºé”™è¯¯è¯Šæ–­å’Œç­–ç•¥ä¿®æ­£çš„åå°„å™¨ï¼Œä»¥åŠï¼ˆ4ï¼‰ç®¡ç†ç»éªŒé‡ç”¨å’Œæ¼”åŒ–çš„é•¿æœŸè®°å¿†çš„å­˜æ¡£å™¨ã€‚åœ¨WikiTQå’ŒTabFactä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå¹¶åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸»å¹²ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05813v1">PDF</a> 26 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMAPLEçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿäººç±»è§£å†³è¡¨æ ¼é—®ç­”é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼šSolverç”¨äºæ¨ç†ï¼ŒCheckerç”¨äºç­”æ¡ˆéªŒè¯ï¼ŒReflectorç”¨äºé”™è¯¯è¯Šæ–­å’Œç­–ç•¥ä¿®æ­£ï¼ŒArchiverç”¨äºé•¿æœŸè®°å¿†ç®¡ç†ä»¥å®ç°ç»éªŒå¤ç”¨å’Œè¿›åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMAPLEåœ¨WikiTQå’ŒTabFactæ•°æ®é›†ä¸Šæ˜¾è‘—æ”¹è¿›äº†ç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†è·¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹èƒŒä¹¦çš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼é—®ç­”è¦æ±‚å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•é€šé“æ¨ç†ä¸­éš¾ä»¥å®ç°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚Chain-of-Thoughtæ¨ç†å’Œé—®é¢˜åˆ†è§£ç¼ºä¹é”™è¯¯æ£€æµ‹æœºåˆ¶ï¼Œå¹¶å¿½ç•¥äº†é—®é¢˜è§£å†³ç»éªŒçš„ç§¯ç´¯ã€‚</li>
<li>MAPLEæ¡†æ¶æ¨¡æ‹Ÿäººç±»é—®é¢˜è§£å†³è¿‡ç¨‹ï¼Œé€šè¿‡ä¸“ä¸šè®¤çŸ¥ä»£ç†åœ¨åé¦ˆé©±åŠ¨å¾ªç¯ä¸­å·¥ä½œã€‚</li>
<li>MAPLEåŒ…å«å››ä¸ªå…³é”®ç»„ä»¶ï¼šSolverç”¨äºæ¨ç†ï¼ŒCheckerç”¨äºç­”æ¡ˆéªŒè¯ï¼ŒReflectorç”¨äºé”™è¯¯è¯Šæ–­å’Œç­–ç•¥ä¿®æ­£ï¼ŒArchiverç”¨äºé•¿æœŸè®°å¿†ç®¡ç†ã€‚</li>
<li>MAPLEå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œåœ¨WikiTQå’ŒTabFactæ•°æ®é›†ä¸Šè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>MAPLEæ¡†æ¶å®ç°äº†è·¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹èƒŒä¹¦çš„å“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05813">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d25822054b04a7ecdec6fe0b75376e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fec7df02734945b3d89dd1fde0596119.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a6625da34bfe0e1da9c95ad7ae551d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0d979c2b2391c78042ef0e8181c24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8477364e54d08c29619c38161e8b5c0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EASG-Bench-Video-Q-A-Benchmark-with-Egocentric-Action-Scene-Graphs"><a href="#EASG-Bench-Video-Q-A-Benchmark-with-Egocentric-Action-Scene-Graphs" class="headerlink" title="EASG-Bench: Video Q&amp;A Benchmark with Egocentric Action Scene Graphs"></a>EASG-Bench: Video Q&amp;A Benchmark with Egocentric Action Scene Graphs</h2><p><strong>Authors:Ivan Rodin, Tz-Ying Wu, Kyle Min, Sharath Nittur Sridhar, Antonino Furnari, Subarna Tripathi, Giovanni Maria Farinella</strong></p>
<p>We introduce EASG-Bench, a question-answering benchmark for egocentric videos where the question-answering pairs are created from spatio-temporally grounded dynamic scene graphs capturing intricate relationships among actors, actions, and objects. We propose a systematic evaluation framework and evaluate several language-only and video large language models (video-LLMs) on this benchmark. We observe a performance gap in language-only and video-LLMs, especially on questions focusing on temporal ordering, thus identifying a research gap in the area of long-context video understanding. To promote the reproducibility of our findings and facilitate further research, the benchmark and accompanying code are available at the following GitHub page: <a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/EASG-bench">https://github.com/fpv-iplab/EASG-bench</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†EASG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç¬¬ä¸€äººç§°è§†é¢‘çš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸­çš„é—®ç­”å¯¹æ˜¯æ ¹æ®æ•æ‰æ¼”å‘˜ã€åŠ¨ä½œå’Œå¯¹è±¡ä¹‹é—´å¤æ‚å…³ç³»çš„æ—¶ç©ºå®šä½åŠ¨æ€åœºæ™¯å›¾ç”Ÿæˆçš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å¤šç§ä»…ä½¿ç”¨è¯­è¨€å’Œè§†é¢‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆè§†é¢‘LLMï¼‰ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä»…ä½¿ç”¨è¯­è¨€å’Œè§†é¢‘LLMçš„æ€§èƒ½å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³æ³¨æ—¶é—´é¡ºåºçš„é—®é¢˜ä¸Šï¼Œä»è€Œç¡®å®šäº†é•¿ä¸Šä¸‹æ–‡è§†é¢‘ç†è§£é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†ä¿ƒè¿›æˆ‘ä»¬ç ”ç©¶çš„å¯é‡å¤æ€§å¹¶æ¨åŠ¨è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œè¯¥åŸºå‡†æµ‹è¯•å’Œé…å¥—ä»£ç å¯åœ¨ä»¥ä¸‹GitHubé¡µé¢æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/EASG-bench%E3%80%82">https://github.com/fpv-iplab/EASG-benchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05787v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬ä»‹ç»äº†EASG-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘çš„é—®ç­”åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•ä¸­çš„é—®ç­”å¯¹æ˜¯æ ¹æ®æ•æ‰æ¼”å‘˜ã€åŠ¨ä½œå’Œå¯¹è±¡ä¹‹é—´å¤æ‚å…³ç³»çš„æ—¶ç©ºæ¥åœ°åŠ¨æ€åœºæ™¯å›¾åˆ›å»ºçš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶åœ¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†å‡ ç§ä»…ä½¿ç”¨è¯­è¨€å’Œè§†é¢‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆvideo-LLMsï¼‰ã€‚æˆ‘ä»¬å‘ç°è¯­è¨€å’Œè§†é¢‘LLMåœ¨å…³æ³¨æ—¶é—´é¡ºåºçš„é—®é¢˜ä¸Šè¡¨ç°å­˜åœ¨å·®è·ï¼Œè¿™è¡¨æ˜åœ¨ç†è§£é•¿è§†é¢‘é¢†åŸŸå­˜åœ¨ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†ä¿ƒè¿›æˆ‘ä»¬ç ”ç©¶ç»“æœçš„å†ç°æ€§å’Œè¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œè¯¥åŸºå‡†æµ‹è¯•å’Œé…å¥—ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹GitHubé¡µé¢æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/EASG-bench%E3%80%82">https://github.com/fpv-iplab/EASG-benchã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>EASG-Benchæ˜¯ä¸€ä¸ªé’ˆå¯¹ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘é—®ç­”çš„åŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨æ—¶ç©ºæ¥åœ°çš„åŠ¨æ€åœºæ™¯å›¾åˆ›å»ºé—®ç­”å¯¹ã€‚</li>
<li>è¯¥è¯„ä¼°æ¡†æ¶æ­ç¤ºäº†è¯­è¨€å’Œè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆvideo-LLMsï¼‰åœ¨ç†è§£é•¿è§†é¢‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³æ³¨æ—¶é—´é¡ºåºçš„é—®é¢˜ä¸Šçš„æ€§èƒ½å·®è·ã€‚</li>
<li>è¿™ä¸€å‘ç°è¡¨æ˜åœ¨ç†è§£é•¿è§†é¢‘é¢†åŸŸå­˜åœ¨ç ”ç©¶ç©ºç™½ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•å’Œé…å¥—ä»£ç å¯åœ¨GitHubé¡µé¢æ‰¾åˆ°ï¼Œä»¥ä¿ƒè¿›ç ”ç©¶ç»“æœçš„å¤ç°å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æ­¤åŸºå‡†æµ‹è¯•å¯¹äºè¯„ä¼°æ¨¡å‹åœ¨å¤æ‚è§†é¢‘åœºæ™¯ä¸­çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›å…·æœ‰ä»·å€¼ã€‚</li>
<li>åŠ¨æ€åœºæ™¯å›¾åœ¨è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿæ•æ‰è§†é¢‘ä¸­æ¼”å‘˜ã€åŠ¨ä½œå’Œå¯¹è±¡ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f771ec210a1598eda06e161123b91d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e767ba5d7949786ffb75545aaf5f52d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0adbc77105ace2504f08d59ad9da9ed2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c457f4997ffc3b4c3f6fcf62579f89fd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Topology-of-Reasoning-Understanding-Large-Reasoning-Models-through-Reasoning-Graph-Properties"><a href="#Topology-of-Reasoning-Understanding-Large-Reasoning-Models-through-Reasoning-Graph-Properties" class="headerlink" title="Topology of Reasoning: Understanding Large Reasoning Models through   Reasoning Graph Properties"></a>Topology of Reasoning: Understanding Large Reasoning Models through   Reasoning Graph Properties</h2><p><strong>Authors:Gouki Minegishi, Hiroki Furuta, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo</strong></p>
<p>Recent large-scale reasoning models have achieved state-of-the-art performance on challenging mathematical benchmarks, yet the internal mechanisms underlying their success remain poorly understood. In this work, we introduce the notion of a reasoning graph, extracted by clustering hidden-state representations at each reasoning step, and systematically analyze three key graph-theoretic properties: cyclicity, diameter, and small-world index, across multiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled reasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly more recurrent cycles (about 5 per sample), substantially larger graph diameters, and pronounced small-world characteristics (about 6x) compared to their base counterparts. Notably, these structural advantages grow with task difficulty and model capacity, with cycle detection peaking at the 14B scale and exploration diameter maximized in the 32B variant, correlating positively with accuracy. Furthermore, we show that supervised fine-tuning on an improved dataset systematically expands reasoning graph diameters in tandem with performance gains, offering concrete guidelines for dataset design aimed at boosting reasoning capabilities. By bridging theoretical insights into reasoning graph structures with practical recommendations for data construction, our work advances both the interpretability and the efficacy of large reasoning models. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§è§„æ¨¡æ¨ç†æ¨¡å‹å·²åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç„¶è€Œï¼Œå…¶æˆåŠŸèƒŒåçš„å†…åœ¨æœºåˆ¶ä»ç„¶çŸ¥ä¹‹ç”šå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨ç†å›¾çš„æ¦‚å¿µï¼Œé€šè¿‡èšç±»æ¯ä¸€æ­¥æ¨ç†çš„éšè—çŠ¶æ€è¡¨ç¤ºæ¥æå–ï¼Œå¹¶ç³»ç»Ÿåœ°åˆ†æäº†ä¸‰ä¸ªå…³é”®çš„å›¾è®ºå±æ€§ï¼šå¾ªç¯æ€§ã€ç›´å¾„å’Œå°ä¸–ç•ŒæŒ‡æ•°ï¼Œè·¨è¶Šå¤šä¸ªä»»åŠ¡ï¼ˆGSM8Kã€MATH500ã€AIME 2024ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè’¸é¦æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚DeepSeek-R1-Distill-Qwen-32Bï¼‰è¡¨ç°å‡ºæ›´æ˜æ˜¾çš„å¾ªç¯ï¼ˆæ¯ä¸ªæ ·æœ¬çº¦5ä¸ªå¾ªç¯ï¼‰ã€æ›´å¤§çš„å›¾ç›´å¾„å’Œçªå‡ºçš„å°ä¸–ç•Œç‰¹å¾ï¼ˆçº¦ä¸ºåŸæ¥çš„6å€ï¼‰ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›ç»“æ„ä¼˜åŠ¿éšç€ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹å®¹é‡çš„å¢åŠ è€Œå¢å¼ºï¼Œå¾ªç¯æ£€æµ‹åœ¨è§„æ¨¡ä¸º14Bæ—¶è¾¾åˆ°å³°å€¼ï¼Œæ¢ç´¢ç›´å¾„åœ¨è§„æ¨¡ä¸º32Bæ—¶æœ€å¤§åŒ–ï¼Œä¸å‡†ç¡®æ€§å‘ˆæ­£ç›¸å…³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ”¹è¿›çš„æ•°æ®é›†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç³»ç»Ÿåœ°æ‰©å¤§äº†æ¨ç†å›¾çš„ç›´å¾„ï¼ŒåŒæ—¶æé«˜äº†æ€§èƒ½ï¼Œä¸ºæ—¨åœ¨æé«˜æ¨ç†èƒ½åŠ›çš„æ•°æ®é›†è®¾è®¡æä¾›äº†å…·ä½“æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„å·¥ä½œæ¶èµ·äº†æ¨ç†å›¾ç»“æ„ç†è®ºè§è§£ä¸æ•°æ®æ„å»ºå®è·µå»ºè®®ä¹‹é—´çš„æ¡¥æ¢ï¼Œæé«˜äº†å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05744v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æ¢è®¨äº†å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ä¸­çš„æ¨ç†å›¾ç»“æ„ï¼Œåˆ†æäº†å¾ªç¯æ€§ã€ç›´å¾„ä»¥åŠå°ä¸–ç•ŒæŒ‡æ•°ç­‰å›¾è®ºå±æ€§ï¼Œæ­ç¤ºäº†è¿™äº›æ¨¡å‹åœ¨æŸäº›æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„ä¼˜ç§€è¡¨ç°èƒŒåçš„å†…éƒ¨æœºåˆ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè’¸é¦æ¨ç†æ¨¡å‹ç›¸å¯¹äºåŸºç¡€æ¨¡å‹å…·æœ‰æ›´å¤šçš„å¾ªç¯å‘¨æœŸã€æ›´å¤§çš„å›¾ç›´å¾„ä»¥åŠæ˜¾è‘—çš„å°ä¸–ç•Œç‰¹æ€§ã€‚éšç€ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹å®¹é‡çš„å¢é•¿ï¼Œè¿™äº›ç»“æ„ä¼˜åŠ¿ä¹Ÿå‘ˆç°å¢é•¿è¶‹åŠ¿ï¼Œå¯¹ç²¾åº¦å…·æœ‰ç§¯æå½±å“ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æä¾›äº†ä¸€äº›æ”¹è¿›æ¨¡å‹æ€§èƒ½çš„æŒ‡å¯¼å»ºè®®ï¼Œå¯¹äºæå‡å¤§å‹æ¨ç†æ¨¡å‹çš„è§£é‡Šæ€§å’Œæœ‰æ•ˆæ€§å…·æœ‰é‡è¦çš„æŒ‡å¯¼æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ¨ç†å›¾æ¦‚å¿µï¼Œé€šè¿‡èšç±»æ¯ä¸ªæ¨ç†æ­¥éª¤çš„éšè—çŠ¶æ€è¡¨ç¤ºæ¥æå–ã€‚</li>
<li>åˆ†æä¸‰ä¸ªå…³é”®å›¾è®ºå±æ€§ï¼šå¾ªç¯æ€§ã€ç›´å¾„å’Œå°ä¸–ç•ŒæŒ‡æ•°ã€‚</li>
<li>è’¸é¦æ¨ç†æ¨¡å‹å±•ç°å‡ºæ›´å¤šçš„å¾ªç¯å‘¨æœŸã€æ›´å¤§çš„å›¾ç›´å¾„å’Œæ˜¾è‘—çš„å°ä¸–ç•Œç‰¹æ€§ã€‚</li>
<li>éšç€ä»»åŠ¡éš¾åº¦å’Œæ¨¡å‹å®¹é‡çš„å¢é•¿ï¼Œè¿™äº›ç»“æ„ä¼˜åŠ¿å‘ˆç°å¢é•¿è¶‹åŠ¿ã€‚</li>
<li>æ¨¡å‹çš„ç»“æ„ç‰¹ç‚¹ä¸ç²¾åº¦æ­£ç›¸å…³ã€‚å¾ªç¯æ£€æµ‹åœ¨æ¨¡å‹è§„æ¨¡ä¸º14Bæ—¶è¾¾åˆ°å³°å€¼ï¼Œæ¢ç´¢ç›´å¾„åœ¨è§„æ¨¡ä¸º32Bæ—¶æœ€å¤§åŒ–ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒæ”¹è¿›æ•°æ®é›†å¯ç³»ç»Ÿåœ°æ‰©å¤§æ¨ç†å›¾çš„ç›´å¾„å¹¶æå‡æ€§èƒ½å¢ç›Šã€‚è¿™æä¾›äº†è®¾è®¡æ•°æ®é›†ä»¥åŠ å¼ºæ¨ç†èƒ½åŠ›çš„å…·ä½“æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09c58c60e77b7fa849a50c8648fc2ed5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de13652a6bc2ff2475a1244ea386d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4089aeac831988279db97fb748b98475.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5795de29123531e7d14c9fb5e7e58ef7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca5863545b3af5fcae1daedb3763737.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-904dac42ad67a9229996a419f784fee1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Do-LLMs-Really-Forget-Evaluating-Unlearning-with-Knowledge-Correlation-and-Confidence-Awareness"><a href="#Do-LLMs-Really-Forget-Evaluating-Unlearning-with-Knowledge-Correlation-and-Confidence-Awareness" class="headerlink" title="Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation   and Confidence Awareness"></a>Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation   and Confidence Awareness</h2><p><strong>Authors:Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li</strong></p>
<p>Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Graph-COM/Knowledge_Unlearning.git">https://github.com/Graph-COM/Knowledge_Unlearning.git</a>. </p>
<blockquote>
<p>æœºå™¨é—å¿˜æŠ€æœ¯æ—¨åœ¨å‡è½»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ„å¤–è®°å¿†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ˜¾å¼åˆ é™¤å­¤ç«‹çš„äº‹å®ä¸Šï¼Œå¾€å¾€å¿½è§†äº†æ½œåœ¨æ¨ç†ä¾èµ–æ€§å’ŒLLMä¸­çŸ¥è¯†çš„éç¡®å®šæ€§ã€‚å› æ­¤ï¼Œå‡å®šè¢«é—å¿˜çš„äº‹å®å¯èƒ½ä¼šé€šè¿‡ç›¸å…³ä¿¡æ¯åœ¨éšå¼çŠ¶æ€ä¸‹æŒç»­å­˜åœ¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªçŸ¥è¯†é—å¿˜è¯„ä¼°æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ„å»ºç›¸å…³äº‹å®ä¸Šä¸‹æ–‡ä½œä¸ºçŸ¥è¯†å›¾è°±å¹¶é™„å¸¦ç›¸åº”çš„ç½®ä¿¡åº¦åˆ†æ•°æ¥æ›´å‡†ç¡®åœ°æ•æ‰ç°å®çŸ¥è¯†çš„éšå¼ç»“æ„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ç§åŸºäºæ¨ç†çš„è¯„ä»·åè®®ï¼Œåˆ©ç”¨å¼ºå¤§çš„LLMä½œä¸ºè¯„åˆ¤å‘˜ï¼›è¿™äº›è¯„åˆ¤å‘˜å¯¹æå–çš„çŸ¥è¯†å­å›¾è¿›è¡Œæ¨ç†ï¼Œä»¥ç¡®å®šé—å¿˜çš„æˆåŠŸç¨‹åº¦ã€‚æˆ‘ä»¬çš„LLMè¯„å§”ä½¿ç”¨ç²¾å¿ƒè®¾è®¡æç¤ºè¯è¿›è¡Œæ ¡å‡†å¹¶ç›¸å¯¹äºäººç±»è¯„ä¼°ï¼Œä»¥ç¡®ä¿å…¶å¯é æ€§å’Œç¨³å®šæ€§ã€‚åœ¨æˆ‘ä»¬æ–°æ„å»ºçš„åŸºå‡†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æä¾›äº†å¯¹é—å¿˜æ€§èƒ½çš„æ›´åŠ ç°å®å’Œä¸¥æ ¼è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„è¯„ä»·ç­–ç•¥å¾€å¾€é«˜ä¼°äº†é—å¿˜çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/Graph-COM/Knowledge_Unlearning.git%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/Graph-COM/Knowledge_Unlearning.gitä¸Šå¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§çŸ¥è¯†é—å¿˜è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æ›´å‡†ç¡®åœ°æ•æ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çŸ¥è¯†çš„éšå«ç»“æ„ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±å’Œå…³è”ç½®ä¿¡åº¦è¯„åˆ†æ¥è¡¨ç¤ºç›¸å…³äº‹å®èƒŒæ™¯ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºæ¨ç†çš„è¯„ä»·åè®®ï¼Œåˆ©ç”¨å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°å‘˜æ¥åˆ¤æ–­é—å¿˜çš„æ•ˆæœã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªæ›´ç°å®å’Œä¸¥æ ¼çš„è¯„ä¼°é—å¿˜æ€§èƒ½çš„æ–¹æ³•ï¼Œå¹¶å‘ç°å½“å‰çš„è¯„ä»·ç­–ç•¥å¾€å¾€é«˜ä¼°äº†é—å¿˜æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨é—å¿˜æŠ€æœ¯æ—¨åœ¨ç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ— æ„è®°å¿†é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ˜ç¡®åˆ é™¤å­¤ç«‹äº‹å®ï¼Œä½†å¿½ç•¥äº†æ½œåœ¨æ¨ç†ä¾èµ–æ€§å’Œè¯­è¨€æ¨¡å‹å†…çŸ¥è¯†çš„éç¡®å®šæ€§ã€‚</li>
<li>æå‡ºçš„è¯„ä¼°æ¡†æ¶é€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±å’Œå…³è”ç½®ä¿¡åº¦è¯„åˆ†æ¥æ›´å‡†ç¡®åœ°æ•æ‰è¯­è¨€æ¨¡å‹ä¸­çŸ¥è¯†çš„éšå«ç»“æ„ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åŸºäºæ¨ç†çš„è¯„ä»·åè®®ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºè¯„ä¼°å‘˜æ¥åˆ¤æ–­é—å¿˜æ•ˆæœã€‚</li>
<li>è¯„ä¼°å‘˜é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸äººç±»è¯„ä»·è¿›è¡Œæ ¡å‡†ï¼Œä»¥ç¡®ä¿å…¶å¯é æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>åœ¨æ–°æ„å»ºçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æä¾›äº†æ›´ç°å®å’Œä¸¥æ ¼çš„é—å¿˜æ€§èƒ½è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-798234c05a2ed858510d5e7d47920ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c72f5f23ff1cf6620b384fffe3102250.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SafeGenBench-A-Benchmark-Framework-for-Security-Vulnerability-Detection-in-LLM-Generated-Code"><a href="#SafeGenBench-A-Benchmark-Framework-for-Security-Vulnerability-Detection-in-LLM-Generated-Code" class="headerlink" title="SafeGenBench: A Benchmark Framework for Security Vulnerability Detection   in LLM-Generated Code"></a>SafeGenBench: A Benchmark Framework for Security Vulnerability Detection   in LLM-Generated Code</h2><p><strong>Authors:Xinghang Li, Jingzhe Ding, Chao Peng, Bing Zhao, Xiang Gao, Hongwan Gao, Xinchen Gu</strong></p>
<p>The code generation capabilities of large language models(LLMs) have emerged as a critical dimension in evaluating their overall performance. However, prior research has largely overlooked the security risks inherent in the generated code. In this work, we introduce \benchmark, a benchmark specifically designed to assess the security of LLM-generated code. The dataset encompasses a wide range of common software development scenarios and vulnerability types. Building upon this benchmark, we develop an automatic evaluation framework that leverages both static application security testing(SAST) and LLM-based judging to assess the presence of security vulnerabilities in model-generated code. Through the empirical evaluation of state-of-the-art LLMs on \benchmark, we reveal notable deficiencies in their ability to produce vulnerability-free code. Our findings highlight pressing challenges and offer actionable insights for future advancements in the secure code generation performance of LLMs. The data and code will be released soon. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ•´ä½“æ€§èƒ½çš„å…³é”®ç»´åº¦ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨é£é™©ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“é—¨ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•é›†\benchmarkã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šç§å¸¸è§çš„è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚åŸºäºè¿™ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é™æ€åº”ç”¨ç¨‹åºå®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å’ŒåŸºäºLLMçš„åˆ¤æ–­æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆä»£ç ä¸­å­˜åœ¨çš„å®‰å…¨é£é™©ã€‚é€šè¿‡å¯¹æœ€æ–°LLMåœ¨\benchmarkä¸Šçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å…¶åœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†ç´§è¿«çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥æé«˜LLMå®‰å…¨ä»£ç ç”Ÿæˆæ€§èƒ½æä¾›äº†åˆ‡å®å¯è¡Œçš„è§è§£ã€‚æ•°æ®é›†å’Œä»£ç å¾ˆå¿«ä¼šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05692v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ•´ä½“æ€§èƒ½çš„å…³é”®ç»´åº¦ï¼Œä½†å…ˆå‰çš„ç ”ç©¶å¤§å¤šå¿½ç•¥äº†ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨é£é™©ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†\benchmarkï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å„ç§å¸¸è§çš„è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é™æ€åº”ç”¨ç¨‹åºå®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å’ŒåŸºäºLLMçš„åˆ¤æ®æ¥è¯„ä¼°æ¨¡å‹ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨æ¼æ´ã€‚é€šè¿‡å¯¹æœ€æ–°LLMsåœ¨\benchmarkä¸Šçš„å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å®ƒä»¬åœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—ç¼ºé™·ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†æœªæ¥çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæé«˜LLMçš„å®‰å…¨ä»£ç ç”Ÿæˆæ€§èƒ½æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚æ•°æ®å’Œä»£ç å°†å¾ˆå¿«å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆèƒ½åŠ›å·²æˆä¸ºè¯„ä¼°å…¶æ€§èƒ½çš„é‡è¦æ–¹é¢ã€‚<br>2.å…ˆå‰çš„ç ”ç©¶ä¸»è¦å¿½è§†äº†LLMç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨é£é™©ã€‚</li>
<li>\benchmarkæ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMç”Ÿæˆä»£ç å®‰å…¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>è¯¥æ•°æ®é›†æ¶µç›–äº†å¤šç§è½¯ä»¶å¼€å‘åœºæ™¯å’Œæ¼æ´ç±»å‹ã€‚</li>
<li>æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨è¯„ä¼°æ¡†æ¶æ¥æ£€æµ‹æ¨¡å‹ç”Ÿæˆä»£ç ä¸­çš„å®‰å…¨æ¼æ´ã€‚</li>
<li>å®è¯è¯„ä¼°å‘ç°ï¼Œå½“å‰LLMsåœ¨ç”Ÿæˆæ— æ¼æ´ä»£ç æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05692">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eb16acc28cba349ce961455fa8bfe6b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d032adc2870e4bb936a697adb24471a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-379c4fea709546f50ecda95569e5845c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0df7b473dfd3e59cdf91bccaaac7d42b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b851e009ce21d7c0b8b2650f6b2fcef.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DriveAction-A-Benchmark-for-Exploring-Human-like-Driving-Decisions-in-VLA-Models"><a href="#DriveAction-A-Benchmark-for-Exploring-Human-like-Driving-Decisions-in-VLA-Models" class="headerlink" title="DriveAction: A Benchmark for Exploring Human-like Driving Decisions in   VLA Models"></a>DriveAction: A Benchmark for Exploring Human-like Driving Decisions in   VLA Models</h2><p><strong>Authors:Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong Qin, Mofan Zhou, Yifei Zhan, Peng Jia, Xianpeng Lang</strong></p>
<p>Vision-Language-Action (VLA) models have advanced autonomous driving, but existing benchmarks still lack scenario diversity, reliable action-level annotation, and evaluation protocols aligned with human preferences. To address these limitations, we introduce DriveAction, the first action-driven benchmark specifically designed for VLA models, comprising 16,185 QA pairs generated from 2,610 driving scenarios. DriveAction leverages real-world driving data proactively collected by users of production-level autonomous vehicles to ensure broad and representative scenario coverage, offers high-level discrete action labels collected directly from usersâ€™ actual driving operations, and implements an action-rooted tree-structured evaluation framework that explicitly links vision, language, and action tasks, supporting both comprehensive and task-specific assessment. Our experiments demonstrate that state-of-the-art vision-language models (VLMs) require both vision and language guidance for accurate action prediction: on average, accuracy drops by 3.3% without vision input, by 4.1% without language input, and by 8.0% without either. Our evaluation supports precise identification of model bottlenecks with robust and consistent results, thus providing new insights and a rigorous foundation for advancing human-like decisions in autonomous driving. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•ï¼Œä½†ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä»ç„¶ç¼ºä¹åœºæ™¯å¤šæ ·æ€§ã€å¯é çš„åŠ¨ä½œçº§åˆ«æ ‡æ³¨ä»¥åŠä¸äººç±»åå¥½å¯¹é½çš„è¯„ä¼°åè®®ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†DriveActionï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºVLAæ¨¡å‹è®¾è®¡çš„ç¬¬ä¸€ä¸ªåŠ¨ä½œé©±åŠ¨åŸºå‡†æµ‹è¯•ï¼Œç”±ä»2,610ä¸ªé©¾é©¶åœºæ™¯ç”Ÿæˆçš„16,185ä¸ªé—®ç­”å¯¹ç»„æˆã€‚DriveActionåˆ©ç”¨ç”Ÿäº§çº§è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç”¨æˆ·ä¸»åŠ¨æ”¶é›†çš„çœŸå®ä¸–ç•Œé©¾é©¶æ•°æ®ï¼Œä»¥ç¡®ä¿å¹¿æ³›å’Œå…·æœ‰ä»£è¡¨æ€§çš„åœºæ™¯è¦†ç›–ï¼›æä¾›ä»ç”¨æˆ·å®é™…é©¾é©¶æ“ä½œä¸­ç›´æ¥æ”¶é›†çš„é«˜çº§ç¦»æ•£åŠ¨ä½œæ ‡ç­¾ï¼›å®æ–½ä»¥åŠ¨ä½œä¸ºæ ¸å¿ƒçš„æ ‘çŠ¶è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜ç¡®åœ°å°†è§†è§‰ã€è¯­è¨€å’ŒåŠ¨ä½œä»»åŠ¡è”ç³»èµ·æ¥ï¼Œæ”¯æŒå…¨é¢å’Œç‰¹å®šä»»åŠ¡çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦è§†è§‰å’Œè¯­è¨€æŒ‡å¯¼æ¥è¿›è¡Œå‡†ç¡®çš„åŠ¨ä½œé¢„æµ‹ï¼šå¹³å‡è€Œè¨€ï¼Œæ²¡æœ‰è§†è§‰è¾“å…¥æ—¶å‡†ç¡®ç‡ä¸‹é™3.3%ï¼Œæ²¡æœ‰è¯­è¨€è¾“å…¥æ—¶ä¸‹é™4.1%ï¼Œä¸¤è€…éƒ½æ²¡æœ‰æ—¶ä¸‹é™8.0%ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ”¯æŒå¯¹æ¨¡å‹ç“¶é¢ˆçš„ç²¾ç¡®è¯†åˆ«ï¼Œæä¾›ç¨³å¥ä¸”ä¸€è‡´çš„ç»“æœï¼Œä»è€Œä¸ºæé«˜è‡ªåŠ¨é©¾é©¶ä¸­ç±»ä¼¼äººç±»çš„å†³ç­–æä¾›äº†æ–°çš„è§è§£å’Œä¸¥æ ¼çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05667v1">PDF</a> Benchmark:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LiAuto-DriveAction/drive-action">https://huggingface.co/datasets/LiAuto-DriveAction/drive-action</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¸ºè§£å†³ç°æœ‰è‡ªåŠ¨é©¾é©¶è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ï¼ˆVLAï¼‰çš„åŸºå‡†æµ‹è¯•å­˜åœ¨çš„ç¼ºä¹åœºæ™¯å¤šæ ·æ€§ã€å¯é çš„è¡ŒåŠ¨çº§åˆ«æ ‡æ³¨ä»¥åŠç¬¦åˆäººç±»åå¥½çš„è¯„ä¼°åè®®çš„é—®é¢˜ï¼Œæœ¬æ–‡æ¨å‡ºäº†é¦–ä¸ªé¢å‘VLAæ¨¡å‹çš„è¡ŒåŠ¨é©±åŠ¨åŸºå‡†æµ‹è¯•DriveActionã€‚å®ƒåŒ…å«ä»ç”Ÿäº§çº§åˆ«çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†ç”¨æˆ·ä¸»åŠ¨æ”¶é›†çš„é©¾é©¶åœºæ™¯ç”Ÿæˆçš„16,185ä¸ªé—®ç­”å¯¹ã€‚DriveActionç¡®ä¿äº†å¹¿æ³›ä¸”å…·æœ‰ä»£è¡¨æ€§çš„åœºæ™¯è¦†ç›–ï¼Œæä¾›äº†ç”¨æˆ·å®é™…é©¾é©¶æ“ä½œæ”¶é›†çš„é«˜çº§ç¦»æ•£åŠ¨ä½œæ ‡ç­¾ï¼Œå¹¶å®æ–½äº†ä¸€ä¸ªä»¥è¡ŒåŠ¨ä¸ºåŸºç¡€çš„æ ‘å½¢è¯„ä¼°æ¡†æ¶ï¼Œæ˜ç¡®åœ°å°†è§†è§‰ã€è¯­è¨€å’Œè¡ŒåŠ¨ä»»åŠ¡è”ç³»èµ·æ¥ï¼Œæ—¢æ”¯æŒå…¨é¢è¯„ä¼°åˆæ”¯æŒç‰¹å®šä»»åŠ¡è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦è§†è§‰å’Œè¯­è¨€æŒ‡å¯¼æ¥è¿›è¡Œå‡†ç¡®çš„åŠ¨ä½œé¢„æµ‹ã€‚åœ¨æ²¡æœ‰è§†è§‰è¾“å…¥çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®ç‡å¹³å‡ä¸‹é™3.3%ï¼›æ²¡æœ‰è¯­è¨€è¾“å…¥æ—¶ä¸‹é™4.1%ï¼›ä¸¤è€…éƒ½æ²¡æœ‰æ—¶ä¸‹é™8.0%ã€‚æœ¬è¯„ä¼°ä¸ºè¯†åˆ«æ¨¡å‹ç“¶é¢ˆæä¾›äº†ç²¾ç¡®ä¸”ä¸€è‡´çš„ç»“æœï¼Œä¸ºæå‡è‡ªåŠ¨é©¾é©¶ä¸­çš„äººç±»å†³ç­–æä¾›äº†ä¸¥è°¨çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>DriveActionæ˜¯é¦–ä¸ªé¢å‘Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹çš„è¡ŒåŠ¨é©±åŠ¨åŸºå‡†æµ‹è¯•ã€‚</li>
<li>DriveActionåŒ…å«ä»ç”Ÿäº§çº§è‡ªåŠ¨é©¾é©¶è½¦è¾†ç”¨æˆ·ä¸»åŠ¨æ”¶é›†çš„é©¾é©¶åœºæ™¯ç”Ÿæˆçš„ä¸°å¯Œé—®ç­”å¯¹ã€‚</li>
<li>å®ƒç¡®ä¿äº†å¹¿æ³›ä¸”å…·æœ‰ä»£è¡¨æ€§çš„åœºæ™¯è¦†ç›–ï¼Œå¹¶æä¾›é«˜çº§ç¦»æ•£åŠ¨ä½œæ ‡ç­¾ã€‚</li>
<li>DriveActionå®æ–½äº†ä¸€ä¸ªä»¥è¡ŒåŠ¨ä¸ºåŸºç¡€çš„æ ‘å½¢è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå…¨é¢å’Œç‰¹å®šä»»åŠ¡çš„è¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦è§†è§‰å’Œè¯­è¨€æŒ‡å¯¼æ¥å®ŒæˆåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>åœ¨ç¼ºå°‘è§†è§‰æˆ–è¯­è¨€è¾“å…¥çš„æƒ…å†µä¸‹ï¼ŒVLMsçš„å‡†ç¡®ç‡ä¼šæ˜¾è‘—ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30d164badbda72f306d55df5ee9b2210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-497fa0781c48265a0fa9816eaf673072.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94fba99a462ce36587c7c717b6250367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a134900736ce8531400f12440dddc8cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb6f7e96f9d428dcbbd4a7638a7d2232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ef0382fb3796a0b06318976c8fb1656.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-650c4c2b0aada31dfd3d4cc58b087f3d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ScaleRTL-Scaling-LLMs-with-Reasoning-Data-and-Test-Time-Compute-for-Accurate-RTL-Code-Generation"><a href="#ScaleRTL-Scaling-LLMs-with-Reasoning-Data-and-Test-Time-Compute-for-Accurate-RTL-Code-Generation" class="headerlink" title="ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for   Accurate RTL Code Generation"></a>ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for   Accurate RTL Code Generation</h2><p><strong>Authors:Chenhui Deng, Yun-Da Tsai, Guan-Ting Liu, Zhongzhi Yu, Haoxing Ren</strong></p>
<p>Recent advances in large language models (LLMs) have enabled near-human performance on software coding benchmarks, but their effectiveness in RTL code generation remains limited due to the scarcity of high-quality training data. While prior efforts have fine-tuned LLMs for RTL tasks, they do not fundamentally overcome the data bottleneck and lack support for test-time scaling due to their non-reasoning nature. In this work, we introduce ScaleRTL, the first reasoning LLM for RTL coding that scales up both high-quality reasoning data and test-time compute. Specifically, we curate a diverse set of long chain-of-thought reasoning traces averaging 56K tokens each, resulting in a dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a general-purpose reasoning model on this corpus yields ScaleRTL that is capable of deep RTL reasoning. Subsequently, we further enhance the performance of ScaleRTL through a novel test-time scaling strategy that extends the reasoning process via iteratively reflecting on and self-correcting previous reasoning steps. Experimental results show that ScaleRTL achieves state-of-the-art performance on VerilogEval and RTLLM, outperforming 18 competitive baselines by up to 18.4% on VerilogEval and 12.7% on RTLLM. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å·²ç»åœ¨è½¯ä»¶ç¼–ç åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ¥è¿‘äººç±»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºå…¶é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºï¼Œå®ƒä»¬åœ¨RTLä»£ç ç”Ÿæˆä¸­çš„æœ‰æ•ˆæ€§ä»ç„¶æœ‰é™ã€‚å°½ç®¡å…ˆå‰çš„åŠªåŠ›å·²ç»å¯¹LLMè¿›è¡Œäº†RTLä»»åŠ¡çš„å¾®è°ƒï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰ä»æ ¹æœ¬ä¸Šå…‹æœæ•°æ®ç“¶é¢ˆï¼Œå¹¶ä¸”ç”±äºç¼ºä¹æ¨ç†æ€§è´¨ï¼Œå®ƒä»¬ä¸æ”¯æŒæµ‹è¯•æ—¶çš„æ‰©å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ScaleRTLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºRTLç¼–ç çš„æ¨ç†LLMï¼Œå®ƒæ‰©å¤§äº†é«˜è´¨é‡æ¨ç†æ•°æ®å’Œæµ‹è¯•æ—¶çš„è®¡ç®—è§„æ¨¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ç³»åˆ—å¹³å‡æ¯ä¸ªåŒ…å«56Kä»¤ç‰Œçš„é•¿æ€è€ƒæ¨ç†è½¨è¿¹ï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«35äº¿ä»¤ç‰Œçš„æ•°æ®é›†ï¼Œåæ˜ äº†ä¸°å¯Œçš„RTLçŸ¥è¯†ã€‚åœ¨è¿™ä¸ªè¯­æ–™åº“ä¸Šå¾®è°ƒé€šç”¨æ¨ç†æ¨¡å‹äº§ç”Ÿäº†èƒ½å¤Ÿè¿›è¡Œæ·±åº¦RTLæ¨ç†çš„ScaleRTLã€‚éšåï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ä¸€ç§æ–°å‹æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥å¢å¼ºScaleRTLçš„æ€§èƒ½ï¼Œè¯¥ç­–ç•¥é€šè¿‡åæ€å’Œè‡ªæˆ‘çº æ­£ä¹‹å‰çš„æ¨ç†æ­¥éª¤æ¥æ‰©å±•æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒScaleRTLåœ¨VerilogEvalå’ŒRTLLMä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨VerilogEvalä¸Šè¶…è¶Šäº†18ä¸ªç«äº‰åŸºå‡†ï¼Œæ€§èƒ½æé«˜äº†é«˜è¾¾18.4%ï¼Œåœ¨RTLLMä¸Šæ€§èƒ½æé«˜äº†12.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05566v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶ç¼–ç åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºè¿‘ä¹äººç±»çš„æ€§èƒ½ï¼Œä½†åœ¨RTLä»£ç ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ä»ç„¶å—é™ï¼Œä¸»è¦å› ä¸ºé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºã€‚å…ˆå‰çš„ç ”ç©¶è™½ç„¶å¯¹LLMè¿›è¡Œäº†å¾®è°ƒä»¥åº”å¯¹RTLä»»åŠ¡ï¼Œä½†å¹¶æ²¡æœ‰ä»æ ¹æœ¬ä¸Šçªç ´æ•°æ®ç“¶é¢ˆï¼Œä¸”ç”±äºç¼ºä¹æ¨ç†èƒ½åŠ›è€Œä¸æ”¯æŒæµ‹è¯•æ—¶çš„ç¼©æ”¾ã€‚åœ¨æ­¤ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ScaleRTLï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºRTLç¼–ç çš„æ¨ç†LLMï¼Œå¯æ‰©å¤§é«˜è´¨é‡æ¨ç†æ•°æ®å’Œæµ‹è¯•æ—¶çš„è®¡ç®—è§„æ¨¡ã€‚æˆ‘ä»¬é€šè¿‡æ”¶é›†ä¸°å¯Œçš„RTLçŸ¥è¯†çš„æ•°æ®é›†ï¼Œå¯¹é€šç”¨æ¨ç†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶å…·å¤‡æ·±åº¦RTLæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡ä¸€ç§æ–°çš„æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥è¿›ä¸€æ­¥æé«˜äº†ScaleRTLçš„æ€§èƒ½ï¼Œè¯¥ç­–ç•¥é€šè¿‡åæ€å’Œçº æ­£ä¹‹å‰çš„æ¨ç†æ­¥éª¤æ¥æ‰©å±•æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒScaleRTLåœ¨VerilogEvalå’ŒRTLLMä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œç›¸è¾ƒäº18ä¸ªç«äº‰åŸºå‡†æµ‹è¯•ï¼Œå…¶åœ¨VerilogEvalä¸Šçš„æ€§èƒ½æé«˜äº†18.4%ï¼Œåœ¨RTLLMä¸Šæé«˜äº†12.7%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶ç¼–ç åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨RTLä»£ç ç”Ÿæˆæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦å—åˆ¶äºé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºã€‚</li>
<li>ç°æœ‰çš„LLMåœ¨RTLä»»åŠ¡ä¸Šçš„ç ”ç©¶æ²¡æœ‰ä»æ ¹æœ¬ä¸Šè§£å†³æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œå¹¶ä¸”ç¼ºä¹æ¨ç†èƒ½åŠ›ï¼Œä¸æ”¯æŒæµ‹è¯•æ—¶çš„ç¼©æ”¾ã€‚</li>
<li>å¼•å…¥çš„ScaleRTLæ˜¯é¦–ä¸ªé’ˆå¯¹RTLç¼–ç çš„æ¨ç†LLMï¼Œèƒ½å¤Ÿæ‰©å¤§é«˜è´¨é‡æ¨ç†æ•°æ®å’Œæµ‹è¯•æ—¶çš„è®¡ç®—è§„æ¨¡ã€‚</li>
<li>é€šè¿‡æ”¶é›†ä¸°å¯Œçš„RTLçŸ¥è¯†çš„æ•°æ®é›†ï¼Œå¯¹é€šç”¨æ¨ç†æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ScaleRTLå…·å¤‡æ·±åº¦RTLæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ScaleRTLé‡‡ç”¨äº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶ç¼©æ”¾ç­–ç•¥ï¼Œé€šè¿‡åæ€å’Œçº æ­£ä¹‹å‰çš„æ¨ç†æ­¥éª¤æ¥æ‰©å±•æ¨ç†è¿‡ç¨‹ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒScaleRTLåœ¨VerilogEvalå’ŒRTLLMç­‰åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-983d5c46dae27fb2b07eed3eb87b54ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c1ce7f9a2c0994f63a86b332171df0f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372a4501a539d769076c7ff08a44724f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6666b8a5b46657e9a0d475449ed052.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed3f99410bff62b10967f55d529072fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c043113b60c1931e978876e31eea487.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c9e84214c7555e4e45e13ca2b7eae83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad34e3fdee8aa6105193d4171aa28e77.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Prefix-Grouper-Efficient-GRPO-Training-through-Shared-Prefix-Forward"><a href="#Prefix-Grouper-Efficient-GRPO-Training-through-Shared-Prefix-Forward" class="headerlink" title="Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward"></a>Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward</h2><p><strong>Authors:Zikang Liu, Tongtian Yue, Yepeng Tang, Longteng Guo, Junxian Cai, Qingbin Liu, Xi Chen, Jing Liu</strong></p>
<p>Group Relative Policy Optimization (GRPO) enhances policy learning by computing gradients from relative comparisons among candidate outputs that share a common input prefix. Despite its effectiveness, GRPO introduces substantial computational overhead when processing long shared prefixes, which must be redundantly encoded for each group member. This inefficiency becomes a major scalability bottleneck in long-context learning scenarios. We propose Prefix Grouper, an efficient GRPO training algorithm that eliminates redundant prefix computation via a Shared-Prefix Forward strategy. In particular, by restructuring self-attention into two parts, our method enables the shared prefix to be encoded only once, while preserving full differentiability and compatibility with end-to-end training. We provide both theoretical and empirical evidence that Prefix Grouper is training-equivalent to standard GRPO: it yields identical forward outputs and backward gradients, ensuring that the optimization dynamics and final policy performance remain unchanged. Empirically, our experiments confirm that Prefix Grouper achieves consistent results while significantly reducing the computational cost of training, particularly in long-prefix scenarios. The proposed method is fully plug-and-play: it is compatible with existing GRPO-based architectures and can be seamlessly integrated into current training pipelines as a drop-in replacement, requiring no structural modifications and only minimal changes to input construction and attention computation. Prefix Grouper enables the use of larger group sizes under the same computational budget, thereby improving the scalability of GRPO to more complex tasks and larger models. Code is now available at <a target="_blank" rel="noopener" href="https://github.com/johncaged/PrefixGrouper">https://github.com/johncaged/PrefixGrouper</a> </p>
<blockquote>
<p>ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€šè¿‡è®¡ç®—å…·æœ‰ç›¸åŒè¾“å…¥å‰ç¼€çš„å€™é€‰è¾“å‡ºä¹‹é—´çš„ç›¸å¯¹æ¯”è¾ƒæ¢¯åº¦ï¼Œå¢å¼ºäº†ç­–ç•¥å­¦ä¹ èƒ½åŠ›ã€‚å°½ç®¡å…¶æ•ˆæœæ˜¾è‘—ï¼Œä½†åœ¨å¤„ç†é•¿çš„å…±äº«å‰ç¼€æ—¶ï¼ŒGRPOéœ€è¦ä¸ºæ¯ä¸ªç»„æˆå‘˜é‡å¤ç¼–ç ï¼Œä»è€Œå¼•å…¥äº†å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚è¿™ç§ä½æ•ˆæ€§åœ¨é•¿ä¸Šä¸‹æ–‡å­¦ä¹ åœºæ™¯ä¸­æˆä¸ºäº†å¯æ‰©å±•æ€§çš„ä¸»è¦ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†Prefix Grouperï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„GRPOè®­ç»ƒç®—æ³•ï¼Œå®ƒé€šè¿‡å…±äº«å‰ç¼€å‰å‘ç­–ç•¥æ¶ˆé™¤äº†å†—ä½™çš„å‰ç¼€è®¡ç®—ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†è‡ªæ³¨æ„åŠ›åˆ†ä¸ºä¸¤éƒ¨åˆ†è¿›è¡Œé‡æ„ï¼Œä½¿å…±äº«å‰ç¼€åªéœ€ç¼–ç ä¸€æ¬¡ï¼ŒåŒæ—¶ä¿æŒå®Œæ•´çš„å¯å¾®æ€§å’Œä¸ç«¯åˆ°ç«¯è®­ç»ƒçš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬æä¾›ç†è®ºå’Œå®è¯è¯æ®è¡¨æ˜ï¼ŒPrefix Grouperåœ¨è®­ç»ƒä¸Šç­‰åŒäºæ ‡å‡†çš„GRPOï¼šå®ƒäº§ç”Ÿç›¸åŒçš„å‰å‘è¾“å‡ºå’Œåå‘æ¢¯åº¦ï¼Œç¡®ä¿ä¼˜åŒ–åŠ¨æ€å’Œæœ€ç»ˆçš„ç­–ç•¥æ€§èƒ½ä¿æŒä¸å˜ã€‚ä»ç»éªŒä¸Šçœ‹ï¼Œæˆ‘ä»¬çš„å®éªŒè¯å®ï¼ŒPrefix Grouperåœ¨å‡å°‘è®­ç»ƒè®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°äº†ç¨³å®šçš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å‰ç¼€åœºæ™¯ä¸­ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¯å³æ’å³ç”¨ï¼šå®ƒä¸ç°æœ‰çš„GRPOæ¶æ„å…¼å®¹ï¼Œå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°å½“å‰çš„è®­ç»ƒç®¡é“ä¸­ä½œä¸ºå³æ’å³ç”¨æ›¿æ¢ï¼Œæ— éœ€è¿›è¡Œç»“æ„æ€§ä¿®æ”¹ï¼Œåªéœ€å¯¹è¾“å…¥æ„å»ºå’Œæ³¨æ„åŠ›è®¡ç®—è¿›è¡Œæœ€å°çš„æ›´æ”¹ã€‚Prefix Grouperèƒ½å¤Ÿåœ¨ç›¸åŒçš„è®¡ç®—é¢„ç®—ä¸‹ä½¿ç”¨æ›´å¤§çš„ç¾¤ä½“è§„æ¨¡ï¼Œä»è€Œæé«˜äº†GRPOåœ¨æ›´å¤æ‚çš„ä»»åŠ¡å’Œæ›´å¤§æ¨¡å‹ä¸Šçš„å¯æ‰©å±•æ€§ã€‚ä»£ç ç°åœ¨å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/johncaged/PrefixGrouper%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/johncaged/PrefixGrouperæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05433v1">PDF</a> 10 pages, technical report</p>
<p><strong>Summary</strong>ï¼šå‰ç¼€åˆ†ç»„å™¨é€šè¿‡å…±äº«å‰ç¼€å‰å‘ç­–ç•¥æ¶ˆé™¤äº†å†—ä½™çš„å‰ç¼€è®¡ç®—ï¼Œæé«˜äº†ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒä¼˜åŒ–åŠ¨æ€å’Œæœ€ç»ˆç­–ç•¥æ€§èƒ½ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†å¯¹æ ‡å‡†GRPOçš„è®­ç»ƒç­‰æ•ˆæ€§ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†è®­ç»ƒçš„è®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å‰ç¼€åœºæ™¯ä¸‹ã€‚å‰ç¼€åˆ†ç»„å™¨å¯ä»¥ä¸ç°æœ‰çš„GRPOæ¶æ„æ— ç¼é›†æˆï¼Œä¸éœ€è¦ç»“æ„ä¿®æ”¹ï¼Œåªéœ€å¯¹è¾“å…¥æ„å»ºå’Œæ³¨æ„åŠ›è®¡ç®—è¿›è¡Œæœ€å°çš„æ›´æ”¹ã€‚è¿™å°†æœ‰åŠ©äºå®ç°æ›´å¤§çš„ç»„å¤§å°å’Œæ›´å¤æ‚ä»»åŠ¡å’Œæ›´å¤§æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GRPOé€šè¿‡è®¡ç®—å€™é€‰è¾“å‡ºä¹‹é—´çš„ç›¸å¯¹æ¯”è¾ƒæ¢¯åº¦æ¥æé«˜ç­–ç•¥å­¦ä¹ ã€‚</li>
<li>åœ¨å¤„ç†é•¿å…±äº«å‰ç¼€æ—¶ï¼ŒGRPOå­˜åœ¨è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚</li>
<li>Prefix Grouperé€šè¿‡å…±äº«å‰ç¼€å‰å‘ç­–ç•¥æ¶ˆé™¤äº†å†—ä½™çš„å‰ç¼€è®¡ç®—ï¼Œæé«˜äº†GRPOçš„è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>Prefix Grouperå®ç°äº†å¯¹æ ‡å‡†GRPOçš„è®­ç»ƒç­‰æ•ˆæ€§ï¼Œä¿æŒäº†ä¼˜åŒ–åŠ¨æ€å’Œæœ€ç»ˆç­–ç•¥æ€§èƒ½ä¸å˜ã€‚</li>
<li>Prefix Grouperæ˜¾è‘—é™ä½äº†è®­ç»ƒçš„è®¡ç®—æˆæœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å‰ç¼€åœºæ™¯ä¸‹ã€‚</li>
<li>Prefix Grouperä¸ç°æœ‰çš„GRPOæ¶æ„å…¼å®¹ï¼Œæ— éœ€ç»“æ„ä¿®æ”¹ï¼Œå¯è½»æ¾é›†æˆåˆ°å½“å‰è®­ç»ƒæµç¨‹ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b45adf0fb04803c5cf9f9297c25dd77a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3107532b00da9b44a667b2ee0c1e3e79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0282e91e35dbf5ad9ce5900c41ebc23d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffd09224f5099f36d31469bccb6acbe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b3483bb02480dcf9d90f438753968de.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Q-Ponder-A-Unified-Training-Pipeline-for-Reasoning-based-Visual-Quality-Assessment"><a href="#Q-Ponder-A-Unified-Training-Pipeline-for-Reasoning-based-Visual-Quality-Assessment" class="headerlink" title="Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality   Assessment"></a>Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality   Assessment</h2><p><strong>Authors:Zhuoxuan Cai, Jian Zhang, Xinbin Yuan, Pengtao Jiang, Wenxiang Chen, Bowen Tang, Lujian Yao, Qiyuan Wang, Jinwen Chen, Bo Li</strong></p>
<p>Recent studies demonstrate that multimodal large language models (MLLMs) can proficiently evaluate visual quality through interpretable assessments. However, existing approaches typically treat quality scoring and reasoning descriptions as separate tasks with disjoint optimization objectives, leading to a trade-off: models adept at quality reasoning descriptions struggle with precise score regression, while score-focused models lack interpretability. This limitation hinders the full potential of MLLMs in visual quality assessment, where accuracy and interpretability should be mutually reinforcing. To address this, we propose a unified two-stage training framework comprising a cold-start stage and a reinforcement learning-based fine-tuning stage. Specifically, in the first stage, we distill high-quality data from a teacher model through expert-designed prompts, initializing reasoning capabilities via cross-entropy loss supervision. In the second stage, we introduce a novel reward with Group Relative Policy Optimization (GRPO) to jointly optimize scoring accuracy and reasoning consistency. We designate the models derived from these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show that Q-Ponder achieves state-of-the-art (SOTA) performance on quality score regression benchmarks, delivering up to 6.5% higher SRCC on cross-domain datasets. Furthermore, Q-Ponder significantly outperforms description-based SOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in description accuracy and reasonableness, demonstrating the generalization potential over diverse tasks. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿé€šè¿‡å¯è§£é‡Šè¯„ä¼°ç†Ÿç»ƒåœ°è¯„ä¼°è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†è´¨é‡è¯„åˆ†å’Œæ¨ç†æè¿°è§†ä¸ºå…·æœ‰ä¸åŒä¼˜åŒ–ç›®æ ‡çš„å•ç‹¬ä»»åŠ¡ï¼Œè¿™å¯¼è‡´äº†ä¸€ç§æƒè¡¡ï¼šæ“…é•¿è´¨é‡æ¨ç†æè¿°çš„æ¨¡å‹åœ¨ç²¾ç¡®åˆ†æ•°å›å½’æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œè€Œä¸“æ³¨äºåˆ†æ•°çš„æ¨¡å‹åˆ™ç¼ºä¹å¯è§£é‡Šæ€§ã€‚è¿™ä¸€å±€é™æ€§é˜»ç¢äº†MLLMsåœ¨è§†è§‰è´¨é‡è¯„ä¼°ä¸­çš„å…¨éƒ¨æ½œåŠ›ï¼Œå…¶ä¸­å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§åº”è¯¥ç›¸äº’å¢å¼ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ…å«å†·å¯åŠ¨é˜¶æ®µå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒé˜¶æ®µçš„ç»Ÿä¸€ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä¸“å®¶è®¾è®¡çš„æç¤ºä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é€šè¿‡äº¤å‰ç†µæŸå¤±ç›‘ç£åˆå§‹åŒ–æ¨ç†èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹å¥–åŠ±ä¸é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥è”åˆä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸¤ä¸ªé˜¶æ®µè¡ç”Ÿå‡ºçš„æ¨¡å‹åˆ†åˆ«æŒ‡å®šä¸ºQ-Ponder-CIå’ŒQ-Ponderã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒQ-Ponderåœ¨è´¨é‡åˆ†æ•°å›å½’åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ˆSOTAï¼‰ï¼Œåœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„SRCCæé«˜äº†é«˜è¾¾6.5%ã€‚æ­¤å¤–ï¼ŒQ-Ponderåœ¨æè¿°å‡†ç¡®æ€§åŠåˆç†æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºæè¿°çš„SOTAæ¨¡å‹ï¼ŒåŒ…æ‹¬å…¶æ•™å¸ˆæ¨¡å‹Qwen-2.5-VL-72Bï¼Œè¿™è¯æ˜äº†å…¶åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„æ³›åŒ–æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05384v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å±•ç¤ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è´¨é‡è¯„ä¼°æ–¹é¢çš„èƒ½åŠ›ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†è´¨é‡è¯„åˆ†å’Œæ¨ç†æè¿°è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡ï¼Œå­˜åœ¨ä¼˜åŒ–ç›®æ ‡è„±èŠ‚çš„é—®é¢˜ï¼Œå¯¼è‡´æ¨¡å‹åœ¨ç²¾å‡†åº¦ä¸å¯è§£é‡Šæ€§ä¹‹é—´æƒè¡¡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨é˜¶æ®µå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒé˜¶æ®µã€‚åœ¨å†·å¯åŠ¨é˜¶æ®µï¼Œé€šè¿‡ä¸“å®¶è®¾è®¡çš„æç¤ºä»æ•™å¸ˆæ¨¡å‹ä¸­æç‚¼é«˜è´¨é‡æ•°æ®ï¼Œå¹¶é€šè¿‡äº¤å‰ç†µæŸå¤±ç›‘ç£è¿›è¡Œåˆæ­¥æ¨ç†èƒ½åŠ›è®­ç»ƒã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œå¼•å…¥æ–°çš„å¥–åŠ±æœºåˆ¶â€”â€”ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œè”åˆä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚æ¨¡å‹è¢«ç§°ä¸ºQ-Ponderç³»åˆ—ã€‚å®éªŒæ˜¾ç¤ºï¼ŒQ-Ponderåœ¨è´¨é‡è¯„åˆ†å›å½’åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œåœ¨è·¨åŸŸæ•°æ®é›†ä¸Šçš„SRCCæé«˜è¾¾6.5%ã€‚å°¤å…¶åœ¨æè¿°å‡†ç¡®æ€§å’Œåˆç†æ€§æ–¹é¢ï¼Œç›¸è¾ƒäºæ•™å¸ˆæ¨¡å‹Qwen-2.5-VL-72Bæœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰è´¨é‡è¯„ä¼°ä¸­å±•ç°æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°†è´¨é‡è¯„åˆ†å’Œæ¨ç†æè¿°è§†ä¸ºç‹¬ç«‹ä»»åŠ¡ï¼Œå­˜åœ¨ä¼˜åŒ–ç›®æ ‡è„±èŠ‚é—®é¢˜ã€‚</li>
<li>æå‡ºä¸¤é˜¶æ®µç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼šå†·å¯åŠ¨é˜¶æ®µé€šè¿‡ä¸“å®¶è®¾è®¡æç¤ºè¿›è¡Œé«˜è´¨é‡æ•°æ®æç‚¼å’Œåˆæ­¥æ¨ç†èƒ½åŠ›è®­ç»ƒï¼›å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µè”åˆä¼˜åŒ–è¯„åˆ†å‡†ç¡®æ€§å’Œæ¨ç†ä¸€è‡´æ€§ã€‚</li>
<li>Q-Ponderç³»åˆ—æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œåœ¨è·¨åŸŸæ•°æ®é›†ä¸ŠSRCCæé«˜è¾¾6.5%ã€‚</li>
<li>Q-Ponderç›¸è¾ƒäºæ•™å¸ˆæ¨¡å‹åœ¨æè¿°å‡†ç¡®æ€§å’Œåˆç†æ€§æ–¹é¢æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>Q-Ponderåœ¨è´¨é‡è¯„åˆ†å›å½’åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05384">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa39aff3d7ca71396e7c355ca16f1c69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec65390d9262291373a27e0a4eb3e2a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9eb026e43b0e88695451e9c2a8a04736.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5881c2b119700d18b3a1fca934d22ff4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Rethinking-Machine-Unlearning-in-Image-Generation-Models"><a href="#Rethinking-Machine-Unlearning-in-Image-Generation-Models" class="headerlink" title="Rethinking Machine Unlearning in Image Generation Models"></a>Rethinking Machine Unlearning in Image Generation Models</h2><p><strong>Authors:Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng</strong></p>
<p>With the surge and widespread application of image generation models, data privacy and content safety have become major concerns and attracted great attention from users, service providers, and policymakers. Machine unlearning (MU) is recognized as a cost-effective and promising means to address these challenges. Despite some advancements, image generation model unlearning (IGMU) still faces remarkable gaps in practice, e.g., unclear task discrimination and unlearning guidelines, lack of an effective evaluation framework, and unreliable evaluation metrics. These can hinder the understanding of unlearning mechanisms and the design of practical unlearning algorithms. We perform exhaustive assessments over existing state-of-the-art unlearning algorithms and evaluation standards, and discover several critical flaws and challenges in IGMU tasks. Driven by these limitations, we make several core contributions, to facilitate the comprehensive understanding, standardized categorization, and reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel hierarchical task categorization framework. It provides detailed implementation guidance for IGMU, assisting in the design of unlearning algorithms and the construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation framework. It includes reliable quantitative metrics across five critical aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can be used for extensive evaluations of IGMU, training content detectors for judgment, and benchmarking the state-of-the-art unlearning algorithms. With EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot handle the unlearning well across different evaluation dimensions, especially for preservation and robustness. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/ryliu68/IGMU">https://github.com/ryliu68/IGMU</a>. </p>
<blockquote>
<p>éšç€å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¶Œç°å’Œå¹¿æ³›åº”ç”¨ï¼Œæ•°æ®éšç§å’Œå†…å®¹å®‰å…¨æˆä¸ºä¸»è¦å…³æ³¨ç‚¹ï¼Œå¹¶å¼•èµ·äº†ç”¨æˆ·ã€æœåŠ¡æä¾›å•†å’Œæ”¿ç­–åˆ¶å®šè€…çš„æå¤§å…³æ³¨ã€‚æœºå™¨é—å¿˜ï¼ˆMUï¼‰è¢«è®¤ä¸ºæ˜¯ä¸€ç§ç»æµé«˜æ•ˆã€å‰æ™¯å¹¿é˜”çš„è§£å†³è¿™äº›æŒ‘æˆ˜çš„æ‰‹æ®µã€‚å°½ç®¡å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†å›¾åƒç”Ÿæˆæ¨¡å‹çš„é—å¿˜ï¼ˆIGMUï¼‰åœ¨å®è·µä¸­ä»ç„¶é¢ä¸´æ˜¾è‘—çš„å·®è·ï¼Œä¾‹å¦‚ä»»åŠ¡è¾¨åˆ«å’Œé—å¿˜æŒ‡å—ä¸æ˜ç¡®ã€ç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶å’Œä¸å¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›å¯èƒ½é˜»ç¢å¯¹é—å¿˜æœºåˆ¶çš„ç†è§£å’Œå®ç”¨é—å¿˜ç®—æ³•çš„è®¾è®¡ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„æœ€æ–°é—å¿˜ç®—æ³•å’Œè¯„ä¼°æ ‡å‡†è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å‘ç°äº†å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆIGMUï¼‰ä¸­çš„å‡ ä¸ªå…³é”®ç¼ºé™·å’ŒæŒ‘æˆ˜ã€‚å—è¿™äº›å±€é™æ€§çš„é©±åŠ¨ï¼Œæˆ‘ä»¬åšå‡ºäº†å‡ é¡¹æ ¸å¿ƒè´¡çŒ®ï¼Œä»¥ä¿ƒè¿›å¯¹IGMUçš„å…¨é¢ç†è§£ã€æ ‡å‡†åŒ–åˆ†ç±»å’Œå¯é è¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬è®¾è®¡äº†CatIGMUï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹åˆ†å±‚ä»»åŠ¡åˆ†ç±»æ¡†æ¶ã€‚å®ƒä¸ºIGMUæä¾›äº†è¯¦ç»†çš„å®æ–½æŒ‡å—ï¼Œæœ‰åŠ©äºè®¾è®¡é—å¿˜ç®—æ³•å’Œæ„å»ºæµ‹è¯•å¹³å°ã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬ä»‹ç»äº†EvalIGMUï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ã€‚å®ƒåŒ…æ‹¬äº”ä¸ªå…³é”®æ–¹é¢çš„å¯é å®šé‡æŒ‡æ ‡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬æ„å»ºäº†DataIGMï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„é—å¿˜æ•°æ®é›†ï¼Œå¯ç”¨äºå¯¹IGMUè¿›è¡Œå¹¿æ³›è¯„ä¼°ã€è®­ç»ƒå†…å®¹æ£€æµ‹å™¨è¿›è¡Œåˆ¤è¯»å’ŒåŸºå‡†æµ‹è¯•æœ€æ–°é—å¿˜ç®—æ³•ã€‚å€ŸåŠ©EvalIGMUå’ŒDataIGMï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç°æœ‰IGMUç®—æ³•åœ¨ä¸åŒçš„è¯„ä¼°ç»´åº¦ä¸Šæ— æ³•å¾ˆå¥½åœ°å¤„ç†é—å¿˜é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™æ€§å’Œç¨³å¥æ€§æ–¹é¢ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ryliu68/IGMU%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ryliu68/IGMUæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02761v2">PDF</a> Accepted by ACM CCS 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ¢è®¨äº†å›¾åƒç”Ÿæˆæ¨¡å‹æ™®åŠå¸¦æ¥çš„æ•°æ®éšç§å’Œå†…å®¹å®‰å…¨é—®é¢˜ï¼Œæœºå™¨é—å¿˜ï¼ˆMUï¼‰è¢«è§†ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„ç»æµæœ‰æ•ˆçš„æœ‰å‰é€”çš„æ‰‹æ®µã€‚ç„¶è€Œï¼Œå›¾åƒç”Ÿæˆæ¨¡å‹é—å¿˜ï¼ˆIGMUï¼‰åœ¨å®è·µä¸­ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œå¦‚ä»»åŠ¡è¾¨åˆ«ä¸æ¸…ã€é—å¿˜å‡†åˆ™ä¸æ˜ç¡®ã€ç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶å’Œä¸å¯é çš„è¯„ä¼°æŒ‡æ ‡ç­‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…å¯¹ç°æœ‰çš„å‰æ²¿é—å¿˜ç®—æ³•å’Œè¯„ä¼°æ ‡å‡†è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°äº†IGMUä»»åŠ¡ä¸­çš„å‡ ä¸ªå…³é”®ç¼ºé™·å’ŒæŒ‘æˆ˜ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬è®¾è®¡CatIGMUä»»åŠ¡åˆ†ç±»æ¡†æ¶ã€å¼•å…¥EvalIGMUè¯„ä¼°æ¡†æ¶ä»¥åŠæ„å»ºDataIGMæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å¯¹IGMUçš„å…¨é¢ç†è§£ã€æ ‡å‡†åŒ–åˆ†ç±»å’Œå¯é è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†æ•°æ®éšç§å’Œå†…å®¹å®‰å…¨çš„ä¸»è¦å…³æ³¨ã€‚</li>
<li>æœºå™¨é—å¿˜ï¼ˆMUï¼‰è¢«è§†ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜çš„æœ‰æ•ˆæ‰‹æ®µï¼Œä½†å›¾åƒç”Ÿæˆæ¨¡å‹é—å¿˜ï¼ˆIGMUï¼‰å­˜åœ¨æ˜¾è‘—å®è·µå·®è·ã€‚</li>
<li>IGMUé¢ä¸´çš„ä»»åŠ¡è¾¨åˆ«ä¸æ¸…ã€é—å¿˜å‡†åˆ™ä¸æ˜ç¡®ç­‰é—®é¢˜é˜»ç¢äº†é—å¿˜æœºåˆ¶çš„ç†è§£å’Œå®ç”¨é—å¿˜ç®—æ³•çš„è®¾è®¡ã€‚</li>
<li>ç ”ç©¶è€…è®¾è®¡äº†CatIGMUä»»åŠ¡åˆ†ç±»æ¡†æ¶ï¼Œä¸ºIGMUçš„è¯¦ç»†å®æ–½æä¾›äº†æŒ‡å¯¼ã€‚</li>
<li>EvalIGMUè¯„ä¼°æ¡†æ¶çš„å¼•å…¥ï¼ŒåŒ…æ‹¬äº”ä¸ªå…³é”®æ–¹é¢çš„å¯é å®šé‡æŒ‡æ ‡ã€‚</li>
<li>æ„å»ºäº†DataIGMé«˜è´¨é‡é—å¿˜æ•°æ®é›†ï¼Œç”¨äºå¹¿æ³›è¯„ä¼°IGMUã€è®­ç»ƒå†…å®¹æ£€æµ‹å™¨å’Œè¯„ä¼°æœ€æ–°é—å¿˜ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-629542ff9b218e4a14f16463472fe13b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66c40af5bbac93e63719768733805865.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3811a96219ca0c187b93d8487937b3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-073e8f2bc92d0cad485637fa18372d92.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="The-Coming-Crisis-of-Multi-Agent-Misalignment-AI-Alignment-Must-Be-a-Dynamic-and-Social-Process"><a href="#The-Coming-Crisis-of-Multi-Agent-Misalignment-AI-Alignment-Must-Be-a-Dynamic-and-Social-Process" class="headerlink" title="The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a   Dynamic and Social Process"></a>The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a   Dynamic and Social Process</h2><p><strong>Authors:Florian Carichon, Aditi Khandelwal, Marylou Fauchard, Golnoosh Farnadi</strong></p>
<p>This position paper states that AI Alignment in Multi-Agent Systems (MAS) should be considered a dynamic and interaction-dependent process that heavily depends on the social environment where agents are deployed, either collaborative, cooperative, or competitive. While AI alignment with human values and preferences remains a core challenge, the growing prevalence of MAS in real-world applications introduces a new dynamic that reshapes how agents pursue goals and interact to accomplish various tasks. As agents engage with one another, they must coordinate to accomplish both individual and collective goals. However, this complex social organization may unintentionally misalign some or all of these agents with human values or user preferences. Drawing on social sciences, we analyze how social structure can deter or shatter group and individual values. Based on these analyses, we call on the AI community to treat human, preferential, and objective alignment as an interdependent concept, rather than isolated problems. Finally, we emphasize the urgent need for simulation environments, benchmarks, and evaluation frameworks that allow researchers to assess alignment in these interactive multi-agent contexts before such dynamics grow too complex to control. </p>
<blockquote>
<p>æœ¬ç«‹åœºè®ºæ–‡æŒ‡å‡ºï¼Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­çš„äººå·¥æ™ºèƒ½å¯¹é½åº”è¢«è§†ä¸ºä¸€ä¸ªåŠ¨æ€ä¸”ä¾èµ–äºäº¤äº’çš„è¿‡ç¨‹ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ™ºèƒ½ä½“éƒ¨ç½²çš„ç¤¾ä¼šç¯å¢ƒï¼Œè¿™äº›ç¯å¢ƒå¯èƒ½æ˜¯åä½œã€åˆä½œæˆ–ç«äº‰æ€§çš„ã€‚è™½ç„¶äººå·¥æ™ºèƒ½ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½çš„å¯¹é½ä»ç„¶æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä½†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­æ—¥ç›Šæ™®éçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¼•å…¥äº†ä¸€ç§æ–°çš„åŠ¨æ€ï¼Œè¿™ç§åŠ¨æ€é‡å¡‘äº†æ™ºèƒ½ä½“å¦‚ä½•è¿½æ±‚ç›®æ ‡å’Œå®Œæˆå„ç§ä»»åŠ¡æ—¶çš„äº¤äº’æ–¹å¼ã€‚å½“æ™ºèƒ½ä½“å½¼æ­¤äº¤äº’æ—¶ï¼Œå®ƒä»¬å¿…é¡»åè°ƒä»¥å®ç°ä¸ªäººå’Œé›†ä½“ç›®æ ‡ã€‚ç„¶è€Œï¼Œè¿™ç§å¤æ‚çš„ç»„ç»‡ç»“æ„å¯èƒ½ä¼šæ— æ„ä¸­ä½¿éƒ¨åˆ†æˆ–æ‰€æœ‰æ™ºèƒ½ä½“ä¸äººç±»ä»·å€¼è§‚æˆ–ç”¨æˆ·åå¥½äº§ç”Ÿåå·®ã€‚æˆ‘ä»¬å€Ÿé‰´ç¤¾ä¼šç§‘å­¦ï¼Œåˆ†æç¤¾ä¼šç»“æ„å¦‚ä½•é˜»ç¢æˆ–ç ´åç¾¤ä½“å’Œä¸ªäººä»·å€¼è§‚ã€‚åŸºäºè¿™äº›åˆ†æï¼Œæˆ‘ä»¬å‘¼åäººå·¥æ™ºèƒ½ç•Œå°†äººç±»ã€åå¥½å’Œå®¢è§‚å¯¹é½è§†ä¸ºç›¸äº’ä¾å­˜çš„æ¦‚å¿µï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒä»¿çœŸç¯å¢ƒã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶çš„ç´§è¿«éœ€æ±‚ï¼Œè¿™äº›æ¡†æ¶å…è®¸ç ”ç©¶äººå‘˜åœ¨å¤æ‚çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¯„ä¼°å¯¹é½æƒ…å†µï¼Œå› ä¸ºåœ¨è¿™äº›åŠ¨æ€å˜å¾—è¿‡äºå¤æ‚ä¸”éš¾ä»¥æ§åˆ¶ä¹‹å‰å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01080v2">PDF</a> Preprint of NeurIPS 2025 Position Paper</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ä¸­çš„å¯¹é½åº”è¢«è§†ä¸ºä¸€ä¸ªåŠ¨æ€ä¸”ä¾èµ–äºäº¤äº’çš„è¿‡ç¨‹ï¼Œè¿™å–å†³äºæ™ºèƒ½ä½“éƒ¨ç½²çš„ç¤¾ä¼šç¯å¢ƒï¼ŒåŒ…æ‹¬åä½œã€åˆä½œæˆ–ç«äº‰ã€‚éšç€MASåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ™®åŠå¢é•¿ï¼Œäººç±»ä»·å€¼è§‚å’Œåå¥½çš„å¯¹é½ä»æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ï¼Œä½†åŒæ—¶ä¹Ÿå‡ºç°äº†æ–°çš„åŠ¨æ€æƒ…å†µï¼Œé‡å¡‘äº†æ™ºèƒ½ä½“è¿½æ±‚ç›®æ ‡å’Œå®Œæˆä»»åŠ¡æ—¶çš„äº¤äº’æ–¹å¼ã€‚æ™ºèƒ½ä½“é—´çš„å¤æ‚ç¤¾ä¼šäº¤äº’éœ€è¦åè°ƒä¸ªä½“å’Œé›†ä½“ç›®æ ‡ï¼Œä½†è¿™ä¸€è¿‡ç¨‹å¯èƒ½æ— æ„ä¸­ä½¿éƒ¨åˆ†æˆ–å…¨éƒ¨æ™ºèƒ½ä½“ä¸äººç±»ä»·å€¼è§‚æˆ–ç”¨æˆ·åå¥½ä¸ä¸€è‡´ã€‚é€šè¿‡åˆ†æç¤¾ä¼šç»“æ„å¦‚ä½•ç ´åç¾¤ä½“å’Œä¸ªäººä»·å€¼è§‚ï¼Œæˆ‘ä»¬å‘¼åäººå·¥æ™ºèƒ½ç•Œå°†äººç±»ã€åå¥½å’Œå®¢è§‚å¯¹é½è§†ä¸ºç›¸äº’ä¾èµ–çš„æ¦‚å¿µï¼Œè€Œéå­¤ç«‹çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬å¼ºè°ƒéœ€è¦ä»¿çœŸç¯å¢ƒã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¾¿åœ¨åŠ¨æ€å¢é•¿è¿‡äºå¤æ‚ä¹‹å‰è¯„ä¼°å¤šæ™ºèƒ½ä½“ä¸Šä¸‹æ–‡ä¸­çš„å¯¹é½æƒ…å†µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI Alignment in Multi-Agent Systems (MAS) æ˜¯ä¸€ä¸ªåŠ¨æ€ä¸”ä¾èµ–äºäº¤äº’çš„è¿‡ç¨‹ã€‚</li>
<li>æ™ºèƒ½ä½“çš„ç¤¾ä¼šç¯å¢ƒå½±å“å…¶ç›®æ ‡å’Œä»»åŠ¡çš„å®Œæˆæ–¹å¼ã€‚</li>
<li>æ™ºèƒ½ä½“é—´çš„å¤æ‚ç¤¾ä¼šäº¤äº’éœ€è¦åè°ƒä¸ªä½“å’Œé›†ä½“ç›®æ ‡ã€‚</li>
<li>æ™ºèƒ½ä½“ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½çš„å¯¹é½æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>ç¤¾ä¼šç»“æ„å¯èƒ½å½±å“æ™ºèƒ½ä½“çš„ä»·å€¼è§‚å’Œè¡Œä¸ºçš„å¯¹é½ã€‚</li>
<li>éœ€è¦å°†äººç±»ã€åå¥½å’Œå®¢è§‚å¯¹é½è§†ä¸ºç›¸äº’ä¾èµ–çš„æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfec5a0398eb2321e7f0dcbe8b899e40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54d266889e6e02e1f43157ab2a232862.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c447f1be192acb55d34f45b78cd72e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-813e7079af384cfcbbfbd3492a93379f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning"><a href="#AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning" class="headerlink" title="AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning"></a>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning</h2><p><strong>Authors:Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</strong></p>
<p>Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/">https://github.com/inclusionAI/AReaL/</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»å¯¼èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ã€‚å¯¹äºLLMçš„æœ‰æ•ˆRLéœ€è¦å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ï¼Œå¹¶è¿«åˆ‡éœ€è¦é«˜æ•ˆçš„è®­ç»ƒç³»ç»Ÿã€‚å¤§å¤šæ•°ç°æœ‰çš„ç”¨äºLLMçš„å¤§å‹RLç³»ç»Ÿéƒ½æ˜¯åŒæ­¥çš„ï¼Œæ‰¹é‡ç”Ÿæˆå’Œè®­ç»ƒäº¤æ›¿è¿›è¡Œï¼Œæ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­çš„æ»šåŠ¨éƒ½æ˜¯ç”±åŒä¸€æ¨¡å‹ç”Ÿæˆçš„ã€‚è¿™ç§æ–¹æ³•ç¨³å®šäº†RLè®­ç»ƒï¼Œä½†ç³»ç»Ÿçº§æ•ˆç‡ä½ä¸‹ï¼šç”Ÿæˆå¿…é¡»ç­‰å¾…æ‰¹æ¬¡ä¸­æœ€é•¿çš„è¾“å‡ºå®Œæˆæ‰èƒ½è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†AReaLï¼Œä¸€ä¸ªå®Œå…¨å¼‚æ­¥çš„RLç³»ç»Ÿï¼Œå®ƒå°†ç”Ÿæˆå’Œè®­ç»ƒå®Œå…¨è§£è€¦ã€‚AReaLä¸­çš„æ»šåŠ¨å·¥ä½œäººå‘˜å¯ä»¥æŒç»­ç”Ÿæˆæ–°çš„è¾“å‡ºè€Œæ— éœ€ç­‰å¾…ï¼Œè€Œè®­ç»ƒå·¥ä½œäººå‘˜åˆ™ä¼šåœ¨æ¯æ¬¡æ”¶é›†åˆ°ä¸€æ‰¹æ•°æ®æ—¶æ›´æ–°æ¨¡å‹ã€‚AReaLè¿˜åŒ…å«ä¸€ç³»åˆ—ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡æ˜¾è‘—æé«˜ã€‚ä¸ºäº†ç¨³å®šRLè®­ç»ƒï¼ŒAReaLå¹³è¡¡äº†æ»šåŠ¨å’Œè®­ç»ƒå·¥ä½œäººå‘˜çš„å·¥ä½œé‡ï¼Œä»¥æ§åˆ¶æ•°æ®çš„é™ˆæ—§ç¨‹åº¦ï¼Œå¹¶é‡‡ç”¨äº†å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“ä»¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸ä½¿ç”¨ç›¸åŒæ•°é‡GPUçš„åŒæ­¥ç³»ç»Ÿç›¸æ¯”ï¼ŒAReaLçš„è®­ç»ƒé€Ÿåº¦æé«˜äº†2.77å€ï¼Œå¹¶ä¸”å…·æœ‰ç›¸åŒ¹é…æˆ–æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½ã€‚AReaLçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/inclusionAI/AReaL/è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24298v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»å¯¼èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ã€‚é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ éœ€è¦å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ï¼Œå¹¶å¯¹é«˜æ•ˆçš„è®­ç»ƒç³»ç»Ÿæå‡ºäº†è¿«åˆ‡éœ€æ±‚ã€‚ç°æœ‰çš„å¤§å¤šæ•°å¤§è§„æ¨¡RLç³»ç»Ÿä¸ºåŒæ­¥ç³»ç»Ÿï¼Œæ‰¹å¤„ç†ç”Ÿæˆä¸è®­ç»ƒäº¤æ›¿è¿›è¡Œï¼Œå…¶ä¸­æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„ç”Ÿæˆç”±åŒä¸€æ¨¡å‹å®Œæˆã€‚è¿™ç§æ–¹æ³•è™½ç„¶ç¨³å®šäº†RLè®­ç»ƒï¼Œä½†å­˜åœ¨ç³»ç»Ÿçº§åˆ«æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼šç”Ÿæˆå¿…é¡»ç­‰å¾…æ‰¹æ¬¡ä¸­æœ€é•¿çš„è¾“å‡ºå®Œæˆåæ‰èƒ½è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä½ä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†å®Œå…¨å¼‚æ­¥çš„RLç³»ç»ŸAReaLï¼Œå®ƒå°†ç”Ÿæˆä¸è®­ç»ƒå®Œå…¨è§£è€¦ã€‚AReaLä¸­çš„ç”Ÿæˆå·¥ä½œçº¿ç¨‹æŒç»­ç”Ÿæˆæ–°è¾“å‡ºè€Œæ— éœ€ç­‰å¾…ï¼Œè€Œè®­ç»ƒå·¥ä½œçº¿ç¨‹åœ¨æ”¶é›†åˆ°ä¸€æ‰¹æ•°æ®æ—¶æ›´æ–°æ¨¡å‹ã€‚AReaLè¿˜åŒ…å«ä¸€ç³»åˆ—ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œå¤§å¤§æé«˜äº†GPUåˆ©ç”¨ç‡ã€‚ä¸ºäº†ç¨³å®šRLè®­ç»ƒï¼ŒAReaLå¹³è¡¡äº†ç”Ÿæˆå’Œè®­ç»ƒå·¥ä½œçº¿ç¨‹çš„å·¥ä½œé‡ï¼Œä»¥æ§åˆ¶æ•°æ®çš„é™ˆæ—§ç¨‹åº¦ï¼Œå¹¶é‡‡ç”¨äº†å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“ä»¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨æ•°ç†å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä½¿ç”¨ç›¸åŒæ•°é‡GPUçš„åŒæ­¥ç³»ç»Ÿç›¸æ¯”ï¼ŒAReaLçš„è®­ç»ƒé€Ÿåº¦æé«˜äº†æœ€é«˜2.77å€ï¼ŒåŒæ—¶å®ç°äº†åŒ¹é…æˆ–æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»å¯¼æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¨ç†ä»»åŠ¡æ—¶ã€‚</li>
<li>ç°æœ‰å¤§è§„æ¨¡RLç³»ç»Ÿå¤šä¸ºåŒæ­¥ç³»ç»Ÿï¼Œå­˜åœ¨ç³»ç»Ÿçº§åˆ«æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>AReaLæ˜¯ä¸€ä¸ªå®Œå…¨å¼‚æ­¥çš„RLç³»ç»Ÿï¼Œå°†ç”Ÿæˆä¸è®­ç»ƒè§£è€¦ï¼Œæé«˜GPUåˆ©ç”¨ç‡ã€‚</li>
<li>AReaLé€šè¿‡å¹³è¡¡ç”Ÿæˆå’Œè®­ç»ƒå·¥ä½œçº¿ç¨‹çš„å·¥ä½œé‡ä»¥åŠé‡‡ç”¨å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“æ¥ç¨³å®šRLè®­ç»ƒã€‚</li>
<li>AReaLå®ç°äº†å¯¹åŒæ­¥ç³»ç»Ÿçš„è®­ç»ƒé€Ÿåº¦æå‡ï¼Œæœ€é«˜å¯è¾¾2.77å€ã€‚</li>
<li>AReaLåœ¨æ•°ç†å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºåŒ¹é…æˆ–æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-620a3e5ac443051d31ac1863a7c8c611.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02ba18add4e6bc82e36cb8bc29cc0b26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58dbbc2ed3d951807c31482be9c871e8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration"><a href="#MMBoundary-Advancing-MLLM-Knowledge-Boundary-Awareness-through-Reasoning-Step-Confidence-Calibration" class="headerlink" title="MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration"></a>MMBoundary: Advancing MLLM Knowledge Boundary Awareness through   Reasoning Step Confidence Calibration</h2><p><strong>Authors:Zhitao He, Sandeep Polisetty, Zhiyuan Fan, Yuchen Huang, Shujin Wu, Yi R. Fung</strong></p>
<p>In recent years, multimodal large language models (MLLMs) have made significant progress but continue to face inherent challenges in multimodal reasoning, which requires multi-level (e.g., perception, reasoning) and multi-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior work on estimating model confidence tends to focus on the overall response for training and calibration, but fails to assess confidence in each reasoning step, leading to undesirable hallucination snowballing. In this work, we present MMBoundary, a novel framework that advances the knowledge boundary awareness of MLLMs through reasoning step confidence calibration. To achieve this, we propose to incorporate complementary textual and cross-modal self-rewarding signals to estimate confidence at each step of the MLLM reasoning process. In addition to supervised fine-tuning MLLM on this set of self-rewarded confidence estimation signal for initial confidence expression warm-up, we introduce a reinforcement learning stage with multiple reward functions for further aligning model knowledge and calibrating confidence at each reasoning step, enhancing reasoning chain self-correction. Empirical results show that MMBoundary significantly outperforms existing methods across diverse domain datasets and metrics, achieving an average of 7.5% reduction in multimodal confidence calibration errors and up to 8.3% improvement in task performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢ä»é¢ä¸´å›ºæœ‰çš„æŒ‘æˆ˜ï¼Œè¿™è¦æ±‚å¤šå±‚æ¬¡ï¼ˆä¾‹å¦‚ï¼Œæ„ŸçŸ¥ã€æ¨ç†ï¼‰å’Œå¤šç²’åº¦ï¼ˆä¾‹å¦‚ï¼Œå¤šæ­¥æ¨ç†é“¾ï¼‰çš„é«˜çº§æ¨ç†ã€‚å…ˆå‰å…³äºä¼°è®¡æ¨¡å‹ä¿¡å¿ƒçš„å·¥ä½œå¾€å¾€é›†ä¸­åœ¨åŸ¹è®­å’Œæ ¡å‡†çš„æ•´ä½“å“åº”ä¸Šï¼Œä½†æœªèƒ½è¯„ä¼°æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒï¼Œå¯¼è‡´ä¸ç†æƒ³çš„å¹»è§‰é›ªçƒæ•ˆåº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MMBoundaryï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ­¥éª¤çš„ä¿¡å¿ƒæ ¡å‡†æé«˜MLLMsçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬æè®®ç»“åˆè¡¥å……æ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªæˆ‘å¥–åŠ±ä¿¡å·æ¥ä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ä¿¡å¿ƒã€‚é™¤äº†ä½¿ç”¨è‡ªæˆ‘å¥–åŠ±çš„ä¿¡å¿ƒä¼°è®¡ä¿¡å·é›†å¯¹MLLMè¿›è¡Œæœ‰ç›‘ç£çš„å¾®è°ƒä»¥è¿›è¡Œåˆå§‹ä¿¡å¿ƒè¡¨è¾¾çƒ­èº«ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å…·æœ‰å¤šç§å¥–åŠ±å‡½æ•°çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä»¥è¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å¹¶æ ¡å‡†æ¯ä¸€æ­¥æ¨ç†çš„ä¿¡å¿ƒï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒMMBoundaryåœ¨è·¨ä¸åŒé¢†åŸŸæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å‡å°‘7.5%çš„å¤šæ¨¡æ€ä¿¡å¿ƒæ ¡å‡†è¯¯å·®ï¼Œä»»åŠ¡æ€§èƒ½æé«˜8.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23224v2">PDF</a> 18 pages, ACL 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡å‹ç½®ä¿¡åº¦è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†MMBoundaryæ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ­¥éª¤ç½®ä¿¡åº¦æ ¡å‡†æå‡æ¨¡å‹çš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚è¯¥æ¡†æ¶ç»“åˆæ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªå¥–åŠ±ä¿¡å·ï¼Œä¼°è®¡MLLMæ¨ç†è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ç½®ä¿¡åº¦ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒMLLMå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œè¿›ä¸€æ­¥å¯¹é½æ¨¡å‹çŸ¥è¯†å’Œæ ¡å‡†æ¯ä¸€æ­¥çš„ç½®ä¿¡åº¦ï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼ŒMMBoundaryåœ¨è·¨åŸŸæ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡å‡å°‘7.5%çš„å¤šæ¨¡æ€ç½®ä¿¡åº¦æ ¡å‡†è¯¯å·®ï¼Œä»»åŠ¡æ€§èƒ½æé«˜8.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šæ¨¡æ€æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦å¤šçº§åˆ«å’Œå¤šç²’åº¦çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ç½®ä¿¡åº¦è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨æ•´ä½“å“åº”ï¼Œæœªå¯¹æ¯ä¸€æ­¥æ¨ç†çš„ç½®ä¿¡åº¦è¿›è¡Œè¯„ä¼°ï¼Œå¯¼è‡´ä¸å¸Œæœ›å‡ºç°çš„å¹»è§‰ç´¯ç§¯ã€‚</li>
<li>MMBoundaryæ¡†æ¶é€šè¿‡æ¨ç†æ­¥éª¤ç½®ä¿¡åº¦æ ¡å‡†æé«˜MLLMsçš„çŸ¥è¯†è¾¹ç•Œæ„è¯†ã€‚</li>
<li>MMBoundaryç»“åˆæ–‡æœ¬å’Œè·¨æ¨¡æ€è‡ªå¥–åŠ±ä¿¡å·ï¼Œå¯¹æ¯ä¸€æ­¥çš„æ¨ç†ç½®ä¿¡åº¦è¿›è¡Œä¼°è®¡ã€‚</li>
<li>MMBoundaryé‡‡ç”¨ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œå¯¹é½æ¨¡å‹çŸ¥è¯†å¹¶æ ¡å‡†æ¯ä¸€æ­¥çš„ç½®ä¿¡åº¦ï¼Œå¢å¼ºæ¨ç†é“¾çš„è‡ªæˆ‘æ ¡æ­£ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºï¼ŒMMBoundaryåœ¨å¤šç§æ•°æ®é›†å’ŒæŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œé™ä½äº†å¤šæ¨¡æ€ç½®ä¿¡åº¦æ ¡å‡†è¯¯å·®å¹¶æé«˜ä»»åŠ¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4e84b70fafdec136fbf20f34fc34b8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98fa6945fb647e0c77a6f1f4b4455683.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-724e70cad27d2693c6cc005ac8896228.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Infi-MMR-Curriculum-based-Unlocking-Multimodal-Reasoning-via-Phased-Reinforcement-Learning-in-Multimodal-Small-Language-Models"><a href="#Infi-MMR-Curriculum-based-Unlocking-Multimodal-Reasoning-via-Phased-Reinforcement-Learning-in-Multimodal-Small-Language-Models" class="headerlink" title="Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models"></a>Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models</h2><p><strong>Authors:Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang</strong></p>
<p>Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the modelâ€™s logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). Resources are available at <a target="_blank" rel="noopener" href="https://huggingface.co/Reallm-Labs/Infi-MMR-3B">https://huggingface.co/Reallm-Labs/Infi-MMR-3B</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä¾‹å¦‚DeepSeek-R1ï¼Œå®ƒåˆ©ç”¨åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ¥æ˜¾è‘—å¢å¼ºé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æˆå°±æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å´é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå¯¹äºå¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆMSLMï¼‰è€Œè¨€ï¼Œè¿™äº›æŒ‘æˆ˜é€šå¸¸æ›´ä¸ºçªå‡ºï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å…·æœ‰è¾ƒå¼±çš„åŸºç¡€æ¨ç†èƒ½åŠ›ï¼šä¸€æ˜¯é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®é›†çš„ç¨€ç¼ºæ€§ï¼›äºŒæ˜¯ç”±äºé›†æˆè§†è§‰å¤„ç†è€Œå¯¼è‡´çš„æ¨ç†èƒ½åŠ›ä¸‹é™ï¼›ä¸‰æ˜¯ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ å¯èƒ½äº§ç”Ÿå¤æ‚è€Œé”™è¯¯çš„æ¨ç†è¿‡ç¨‹çš„é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹æ¡†æ¶Infi-MMRï¼Œé€šè¿‡ä¸‰ä¸ªç²¾å¿ƒæ„å»ºçš„é˜¶æ®µç³»ç»Ÿåœ°è§£é”MSLMçš„æ¨ç†æ½œåŠ›ï¼Œå¹¶æå‡ºäº†æˆ‘ä»¬çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹Infi-MMR-3Bã€‚ç¬¬ä¸€é˜¶æ®µï¼ŒåŸºç¡€æ¨ç†æ¿€æ´»ï¼Œåˆ©ç”¨é«˜è´¨é‡æ–‡æœ¬æ¨ç†æ•°æ®é›†æ¥æ¿€æ´»å’ŒåŠ å¼ºæ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µï¼Œè·¨æ¨¡æ€æ¨ç†é€‚åº”ï¼Œåˆ©ç”¨å­—å¹•å¢å¼ºå¤šæ¨¡æ€æ•°æ®æ¥ä¿ƒè¿›æ¨ç†æŠ€èƒ½å‘å¤šæ¨¡æ€ç¯å¢ƒçš„é€æ­¥è½¬ç§»ã€‚ç¬¬ä¸‰é˜¶æ®µï¼Œå¤šæ¨¡æ€æ¨ç†å¢å¼ºï¼Œé‡‡ç”¨ç²¾é€‰çš„æ— å­—å¹•å¤šæ¨¡æ€æ•°æ®æ¥ç¼“è§£è¯­è¨€åè§ï¼Œå¹¶ä¿ƒè¿›ç¨³å¥çš„è·¨æ¨¡æ€æ¨ç†ã€‚Infi-MMR-3Bä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„è·¨æ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ˆåœ¨MathVerseæµ‹è¯•é›†ä¸Šè¾¾åˆ°43.68%ï¼Œåœ¨MathVisionæµ‹è¯•é›†ä¸Šè¾¾åˆ°27.04%ï¼Œåœ¨OlympiadBenchä¸Šè¾¾åˆ°21.33%ï¼‰ï¼Œè€Œä¸”åœ¨é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ï¼ˆåœ¨MathVistaæµ‹è¯•é›†ä¸Šè¾¾åˆ°67.2%ï¼‰ã€‚ç›¸å…³èµ„æºå¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://huggingface.co/Reallm-Labs/Infi-MMR-3B%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/Reallm-Labs/Infi-MMR-3Bè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23091v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¦‚DeepSeek-R1ã€‚ç„¶è€Œï¼Œå°†æˆæœæ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆMSLMï¼‰è€Œè¨€ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Infi-MMRï¼Œé€šè¿‡ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡çš„é˜¶æ®µè§£é”MSLMçš„æ¨ç†æ½œåŠ›ï¼Œå¹¶æ¨å‡ºäº†å¤šæ¨¡æ€æ¨ç†æ¨¡å‹Infi-MMR-3Bã€‚è¯¥æ¨¡å‹ç»å†äº†åŸºç¡€æ¨ç†æ¿€æ´»ã€è·¨æ¨¡æ€æ¨ç†é€‚åº”å’Œè·¨æ¨¡æ€æ¨ç†å¢å¼ºä¸‰ä¸ªé˜¶æ®µï¼Œå®ç°äº†å…ˆè¿›çš„è·¨æ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›å’Œé€šç”¨æ¨ç†èƒ½åŠ›ã€‚ç›¸å…³ä¿¡æ¯å¯é€šè¿‡huggingface.co&#x2F;Reallm-Labs&#x2F;Infi-MMR-3Bè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†èƒ½åŠ›ä¸Šå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå¦‚DeepSeek-R1ã€‚</li>
<li>å°†LLMçš„è¿›å±•æ‰©å±•åˆ°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆMSLMï¼‰ã€‚</li>
<li>æå‡ºæ–°å‹æ¡†æ¶Infi-MMRï¼Œé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè§£é”MSLMçš„æ¨ç†æ½œåŠ›ã€‚</li>
<li>Infi-MMR-3Bæ¨¡å‹ç»å†åŸºç¡€æ¨ç†æ¿€æ´»ã€è·¨æ¨¡æ€æ¨ç†é€‚åº”å’Œè·¨æ¨¡æ€æ¨ç†å¢å¼ºä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>Infi-MMR-3Bå®ç°äº†å…ˆè¿›çš„è·¨æ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›å’Œé€šç”¨æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨MathVerse testminiã€MathVision testã€OlympiadBenchå’ŒMathVista testminiä¸Šå–å¾—å“è¶Šè¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-880be88612438bf8aa75db0b8244d20f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-631ffdb5149f238b9186a5dde5d53f4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1b618682fd31369a000346471ea885d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f32a620e1dcc6c1169b3c854b451e2bd.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79cc1066eda96c070681845e2ee1fb80.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-09  LLaDA-V Large Language Diffusion Models with Visual Instruction Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
