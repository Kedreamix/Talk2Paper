<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-10  STARFlow Scaling Latent Normalizing Flows for High-resolution Image   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fdef7621a4b0601a72c49b8fe875c298.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-10-更新"><a href="#2025-06-10-更新" class="headerlink" title="2025-06-10 更新"></a>2025-06-10 更新</h1><h2 id="STARFlow-Scaling-Latent-Normalizing-Flows-for-High-resolution-Image-Synthesis"><a href="#STARFlow-Scaling-Latent-Normalizing-Flows-for-High-resolution-Image-Synthesis" class="headerlink" title="STARFlow: Scaling Latent Normalizing Flows for High-resolution Image   Synthesis"></a>STARFlow: Scaling Latent Normalizing Flows for High-resolution Image   Synthesis</h2><p><strong>Authors:Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, Shuangfei Zhai</strong></p>
<p>We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution. </p>
<blockquote>
<p>我们提出了STARFlow，这是一个基于规范化流的可扩展生成模型，在高分辨率图像合成中实现了强大的性能。STARFlow的核心是Transformer自回归流（TARFlow），它结合了规范化流的表达能力与自回归变压器的结构化建模能力。我们首先建立了TARFlow对连续分布进行建模的理论普遍性。在此基础上，我们引入了几项关键的架构和算法创新，以显著增强可扩展性：(1)深浅设计，其中深Transformer块捕获模型的大部分表示能力，辅以几个计算效率高但对模型大有裨益的浅层Transformer块；(2)在预训练自编码器的潜在空间中进行建模，这被证明比直接像素级建模更为有效；(3)一种新型指导算法，可大幅提高样本质量。最重要的是，我们的模型仍然是一个端到端的规范化流，能够在连续空间中进行精确的最大可能性训练，无需离散化。STARFlow在类条件图像生成和任务条件文本生成任务中实现了有竞争力的性能，接近最先进的扩散模型的样本质量。据我们所知，这项工作是首次成功展示了在这种规模和分辨率下有效运行的规范化流。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06276v1">PDF</a> TLDR: We show for the first time that normalizing flows can be scaled   for high-resolution and text-conditioned image synthesis</p>
<p><strong>Summary</strong></p>
<p>STARFlow是一个基于标准化流的可扩展生成模型，在高分辨率图像合成中表现出强大的性能。其核心是Transformer自回归流（TARFlow），结合了标准化流的表达力和自回归变压器的结构化建模能力。通过引入深度浅层设计、在预训练自编码器的潜在空间进行建模以及新型指导算法，显著增强了其可扩展性。模型在类条件图像生成和文本条件图像生成任务中表现出卓越性能，接近扩散模型的样本质量水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STARFlow是一个基于标准化流的生成模型，用于高分辨率图像合成。</li>
<li>核心是Transformer自回归流（TARFlow），结合了标准化流和自回归变压器的优点。</li>
<li>TARFlow的理论普遍性被用来对连续分布进行建模。</li>
<li>引入深度浅层设计以提高模型的可扩展性。</li>
<li>在预训练自编码器的潜在空间进行建模，证明其比直接像素级建模更有效。</li>
<li>新型指导算法显著提高样本质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06276">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ca70f10410bb81f97a96205eb98c0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-914729d7a64cff8e3f4dec00ea0ff105.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a13187e2812f4da476cdebe90930b555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e3efebf3239180d34d0e966527b5a3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Feedback-Guidance-of-Diffusion-Models"><a href="#Feedback-Guidance-of-Diffusion-Models" class="headerlink" title="Feedback Guidance of Diffusion Models"></a>Feedback Guidance of Diffusion Models</h2><p><strong>Authors:Koulischer Felix, Handke Florian, Deleu Johannes, Demeester Thomas, Ambrogioni Luca</strong></p>
<p>While Classifier-Free Guidance (CFG) has become standard for improving sample fidelity in conditional diffusion models, it can harm diversity and induce memorization by applying constant guidance regardless of whether a particular sample needs correction. We propose FeedBack Guidance (FBG), which uses a state-dependent coefficient to self-regulate guidance amounts based on need. Our approach is derived from first principles by assuming the learned conditional distribution is linearly corrupted by the unconditional distribution, contrasting with CFG’s implicit multiplicative assumption. Our scheme relies on feedback of its own predictions about the conditional signal informativeness to adapt guidance dynamically during inference, challenging the view of guidance as a fixed hyperparameter. The approach is benchmarked on ImageNet512x512, where it significantly outperforms Classifier-Free Guidance and is competitive to Limited Interval Guidance (LIG) while benefitting from a strong mathematical framework. On Text-To-Image generation, we demonstrate that, as anticipated, our approach automatically applies higher guidance scales for complex prompts than for simpler ones and that it can be easily combined with existing guidance schemes such as CFG or LIG. </p>
<blockquote>
<p>虽然无分类器引导（CFG）已成为提高条件扩散模型样本保真度的标准方法，但它可能会损害多样性并导致记忆，因为它会应用固定的指导方式，无论特定的样本是否需要修正。我们提出了反馈引导（FBG）方法，它使用一个状态依赖系数来根据需求自我调节引导量。我们的方法基于基本原理，假设学习的条件分布被无条件分布线性破坏，这与CFG的隐含乘法假设形成对比。我们的方案依赖于对其自身关于条件信号信息预测的反馈，在推理过程中动态适应指导，挑战了将指导视为固定超参数的观点。该方法在ImageNet512x512上进行了基准测试，显著优于无分类器引导且具备竞争力并与有限间隔引导（LIG）具有相当水平的表现同时得益于强大的数学框架支持。在文本到图像生成中，我们证明了我们的方法能够自动为复杂提示应用更高的指导比例而不是简单的提示，并且它可以很容易地与现有的引导方案如CFG或LIG结合使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06085v1">PDF</a> Preprint. Article currently under review. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/FelixKoulischer/FBG_using_edm2">https://github.com/FelixKoulischer/FBG_using_edm2</a></p>
<p><strong>Summary</strong></p>
<p>基于扩散模型的反馈指导（FBG）被提出以提高样本的保真度并自适应地调整指导量。相较于Classifier-Free Guidance（CFG）的恒定指导，FBG能够根据样本需求进行自我调节，避免损害多样性和产生记忆效应。FBG基于预测的条件信号信息量的反馈来动态调整指导，挑战了将指导视为固定超参数的观点。在ImageNet和文本转图像生成任务上，FBG显著优于CFG，并与Limited Interval Guidance（LIG）具有竞争力。此外，FBG能够轻松结合现有指导方案如CFG或LIG。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FBG提出一种基于反馈的扩散模型指导方法，能够自适应调整指导量以提高样本保真度。</li>
<li>FBG通过预测的条件信号信息量的反馈进行动态调整，克服了恒定指导可能带来的问题。</li>
<li>FBG与Classifier-Free Guidance（CFG）相比，能更好地适应不同样本的需求，避免损害多样性和产生记忆效应。</li>
<li>FBG在ImageNet上的性能显著优于CFG，与Limited Interval Guidance（LIG）相当并具有竞争力。</li>
<li>FBG适用于文本转图像生成任务，能自动为复杂提示应用更高的指导规模。</li>
<li>FBG能够轻松结合其他现有指导方案，如CFG或LIG。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e6e9a24a14ede56ecaff2ea6c80dc313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d5f90e95dbf3cce057d275124aeeeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788dfc0d069434d76a2ecc9f2319bd8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c363b6a8eab76fe55fe6a52693ce03b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimization-Free-Universal-Watermark-Forgery-with-Regenerative-Diffusion-Models"><a href="#Optimization-Free-Universal-Watermark-Forgery-with-Regenerative-Diffusion-Models" class="headerlink" title="Optimization-Free Universal Watermark Forgery with Regenerative   Diffusion Models"></a>Optimization-Free Universal Watermark Forgery with Regenerative   Diffusion Models</h2><p><strong>Authors:Chaoyi Zhu, Zaitang Li, Renyi Yang, Robert Birke, Pin-Yu Chen, Tsung-Yi Ho, Lydia Y. Chen</strong></p>
<p>Watermarking becomes one of the pivotal solutions to trace and verify the origin of synthetic images generated by artificial intelligence models, but it is not free of risks. Recent studies demonstrate the capability to forge watermarks from a target image onto cover images via adversarial optimization without knowledge of the target generative model and watermark schemes. In this paper, we uncover a greater risk of an optimization-free and universal watermark forgery that harnesses existing regenerative diffusion models. Our proposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and integrates the target watermark via regenerating the image, without needing any additional optimization routine. It allows for universal watermark forgery that works independently of the target image’s origin or the watermarking model used. We explore the watermarked latent extracted from the target image and visual-textual context of cover images as priors to guide sampling of the regenerative process. Extensive evaluation on 24 scenarios of model-data-watermark combinations demonstrates that PnP can successfully forge the watermark (up to 100% detectability and user attribution), and maintain the best visual perception. By bypassing model retraining and enabling adaptability to any image, our approach significantly broadens the scope of forgery attacks, presenting a greater challenge to the security of current watermarking techniques for diffusion models and the authority of watermarking schemes in synthetic data generation and governance. </p>
<blockquote>
<p>水印已成为追踪和验证人工智能模型生成的合成图像来源的关键解决方案之一，但并非没有风险。最近的研究表明，可以通过对抗性优化将目标图像的水印伪造到覆盖图像上，而无需了解目标生成模型和水印方案。在本文中，我们揭示了利用现有再生扩散模型进行无优化通用水印伪造的最大风险。我们提出的伪造攻击方法PnP（即“即插即用”）通过再生图像无缝提取和集成目标水印，而无需任何额外的优化流程。它允许进行通用水印伪造，独立于目标图像来源或水印模型的使用情况。我们探索从目标图像中提取的水印潜力和覆盖图像的视觉文本上下文作为先验来指导再生过程的采样。对24种模型-数据-水印组合场景的广泛评估表明，PnP可以成功伪造水印（高达100%的可检测性和用户归属），并保持最佳的视觉感知。通过绕过模型重新训练并适应任何图像，我们的方法大大扩大了伪造攻击的范围，给当前水印技术在扩散模型中的安全性以及水印方案在合成数据生成和治理中的权威性带来了更大的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>水印已成为追踪和验证人工智能模型生成合成图像来源的关键解决方案之一，但并非没有风险。最近的研究表明，可以通过对抗性优化将目标水印伪造到覆盖图像上，而无需了解目标生成模型和水印方案。在本文中，我们揭示了利用现有再生扩散模型进行无优化和通用水印伪造的更大风险。我们提出的伪造攻击PnP（即插即用）方法，通过再生图像无缝提取和集成目标水印，而无需任何额外的优化流程。它允许进行独立于目标图像来源或水印模型使用的通用水印伪造。我们探索了从目标图像中提取的水印潜伏期和覆盖图像的视觉文本上下文作为先验，以指导再生过程的采样。在24种模型-数据-水印组合场景下的广泛评估表明，PnP可以成功伪造水印（高达100%的可检测性和用户归属），并保持良好的视觉感知。通过绕过模型重新训练并适应任何图像，我们的方法大大扩大了伪造攻击的范围，给扩散模型的水印技术安全性和合成数据生成、治理的水印方案权威性带来了更大的挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水印是追踪和验证AI生成图像来源的关键，但存在伪造风险。</li>
<li>最近研究展示了无需了解目标生成模型和水印方案的情况下，通过对抗性优化伪造水印。</li>
<li>提出了一种新的伪造攻击方法PnP，能够无缝提取并集成目标水印，通过再生图像进行。</li>
<li>PnP方法允许进行通用水印伪造，独立于目标图像来源和水印模型。</li>
<li>利用目标图像的水印潜伏期和覆盖图像的视觉文本上下文来提高伪造效果。</li>
<li>广泛评估表明，PnP方法成功伪造水印并保持良好视觉感知。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06018">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a9812ac9f09cbc397d546ca80edd2ff8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b06987c1583f5abf701aaf6e79f762cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f47c1b90ddfcee85e0ae6da4458d8de7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FADE-Frequency-Aware-Diffusion-Model-Factorization-for-Video-Editing"><a href="#FADE-Frequency-Aware-Diffusion-Model-Factorization-for-Video-Editing" class="headerlink" title="FADE: Frequency-Aware Diffusion Model Factorization for Video Editing"></a>FADE: Frequency-Aware Diffusion Model Factorization for Video Editing</h2><p><strong>Authors:Yixuan Zhu, Haolin Wang, Shilin Ma, Wenliang Zhao, Yansong Tang, Lei Chen, Jie Zhou</strong></p>
<p>Recent advancements in diffusion frameworks have significantly enhanced video editing, achieving high fidelity and strong alignment with textual prompts. However, conventional approaches using image diffusion models fall short in handling video dynamics, particularly for challenging temporal edits like motion adjustments. While current video diffusion models produce high-quality results, adapting them for efficient editing remains difficult due to the heavy computational demands that prevent the direct application of previous image editing techniques. To overcome these limitations, we introduce FADE, a training-free yet highly effective video editing approach that fully leverages the inherent priors from pre-trained video diffusion models via frequency-aware factorization. Rather than simply using these models, we first analyze the attention patterns within the video model to reveal how video priors are distributed across different components. Building on these insights, we propose a factorization strategy to optimize each component’s specialized role. Furthermore, we devise spectrum-guided modulation to refine the sampling trajectory with frequency domain cues, preventing information leakage and supporting efficient, versatile edits while preserving the basic spatial and temporal structure. Extensive experiments on real-world videos demonstrate that our method consistently delivers high-quality, realistic and temporally coherent editing results both qualitatively and quantitatively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/EternalEvan/FADE">https://github.com/EternalEvan/FADE</a> . </p>
<blockquote>
<p>近期扩散框架的进展在视频编辑方面取得了显著的提升，实现了高保真度以及与文本提示的强烈对齐。然而，使用图像扩散模型的传统方法在应对视频动态方面存在不足，特别是在动作调整等具有挑战性的时间编辑方面。虽然当前视频扩散模型能产生高质量的结果，但将其适应于高效编辑仍然很困难，因为它们需要大量的计算资源，这使得无法直接应用之前的图像编辑技术。为了克服这些限制，我们引入了FADE，这是一种无需训练但高效的视频编辑方法，它充分利用了预训练视频扩散模型的内在先验知识，通过频率感知分解。我们不是简单地使用这些模型，而是首先分析视频模型中的注意力模式，以揭示视频先验知识是如何分布在不同的组件中的。基于这些见解，我们提出了一种分解策略，以优化每个组件的专门作用。此外，我们设计了频谱引导调制，以利用频率域线索来完善采样轨迹，防止信息泄露，并支持高效、多功能编辑，同时保持基本的空间和时间结构。在真实世界视频上的广泛实验表明，我们的方法在定性和定量上都能提供高质量、现实感强且时间连贯的编辑结果。代码可在<a target="_blank" rel="noopener" href="https://github.com/EternalEvan/FADE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EternalEvan/FADE上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05934v1">PDF</a> Accepted by IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR) 2025</p>
<p><strong>Summary</strong></p>
<p>近期扩散框架的进展极大提升了视频编辑效果，实现了高保真和与文本提示的强对齐。然而，传统图像扩散模型在处理视频动态方面存在局限，尤其在运动调整等时间编辑方面更具挑战。当前视频扩散模型虽能生成高质量结果，但由于计算需求巨大，难以高效编辑。为此，我们推出FADE，一种无需训练的视频编辑方法，充分利用预训练视频扩散模型的内在先验知识，通过频率感知分解实现。我们分析视频模型中的注意力模式，揭示视频先验知识在不同组件中的分布。在此基础上，我们提出优化各组件专业角色的分解策略。同时，我们开发频谱引导调制，利用频率域线索优化采样轨迹，防止信息泄露，支持高效、多样化的编辑，同时保持基本的时间和空间结构。在真实视频上的广泛实验证明，我们的方法无论在定性还是定量上都能提供高质量、逼真的时间连贯编辑结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散框架的最新进展显著提高了视频编辑的质量和对文本提示的响应度。</li>
<li>传统图像扩散模型在处理视频动态方面存在不足，特别是在处理复杂的运动编辑时面临挑战。</li>
<li>当前视频扩散模型虽然能生成高质量结果，但难以高效编辑，计算需求大。</li>
<li>FADE方法是一种无需训练的视频编辑方法，充分利用预训练视频扩散模型的内在先验知识。</li>
<li>FADE通过分析视频模型中的注意力模式来揭示视频先验知识的分布。</li>
<li>FADE通过频率感知分解和频谱引导调制技术实现高质量的视频编辑。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05934">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ee46da539fd149708a51be012d2a70d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9daa2e66ca08afbffd29071c324b28f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce361e7fbb4cbe061fb9c9affc6af18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a33bd4a11613ef8daeda75d40465814.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Stealix-Model-Stealing-via-Prompt-Evolution"><a href="#Stealix-Model-Stealing-via-Prompt-Evolution" class="headerlink" title="Stealix: Model Stealing via Prompt Evolution"></a>Stealix: Model Stealing via Prompt Evolution</h2><p><strong>Authors:Zhixiong Zhuang, Hui-Po Wang, Maria-Irina Nicolae, Mario Fritz</strong></p>
<p>Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model’s data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized. </p>
<blockquote>
<p>模型窃取对机器学习的安全构成重大威胁。攻击者能够无需接触目标模型即可复现黑盒模型，导致知识产权受损且暴露敏感信息。近期使用预训练扩散模型进行数据合成的方法提高了效率和性能，但严重依赖人工设计的提示，限制了自动化和可扩展性，尤其不利于缺乏专业知识的攻击者。为了评估开源预训练模型带来的风险，我们提出了一个更现实的威胁模型，该模型消除了对提示设计技能或类名知识的需求。在此背景下，我们推出了Stealix，这是一种无需预设提示即可执行模型窃取的首个方法。Stealix使用两个开源预训练模型来推断目标模型的数据分布，并通过遗传算法迭代优化提示，逐步改进合成图像的精度和多样性。我们的实验结果表明，即使在相同的查询预算下，Stealix也显著优于其他方法，甚至优于那些能够访问类名或精细提示的方法。这些发现突显了我们方法的可扩展性，并表明预训练生成模型在模型窃取方面的风险可能比以前认识到的更大。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05867v1">PDF</a> Accepted at ICML 2025. The project page is at   <a target="_blank" rel="noopener" href="https://zhixiongzh.github.io/stealix/">https://zhixiongzh.github.io/stealix/</a></p>
<p><strong>Summary</strong></p>
<p>模型窃取对机器学习构成重大安全风险，攻击者可以在无需接触训练数据的情况下复制黑盒模型，从而危及知识产权和暴露敏感信息。尽管利用预训练扩散模型进行数据合成的方法提高了效率和性能，但它们严重依赖手动设计的提示，限制了自动化和可扩展性，尤其是对那些缺乏专业知识的攻击者。为了评估开源预训练模型的风险，我们提出了一个更现实的威胁模型，该模型无需提示设计技能或了解类名即可运行。在此背景下，我们推出了Stealix，这是一种无需预先设定提示即可执行模型窃取的首创方法。Stealix使用两个开源预训练模型来推断受害者模型的数据分布，并通过遗传算法迭代优化提示，逐步提高合成图像的精度和多样性。实验结果表明，Stealix显著优于其他方法，即使在相同的查询预算下，即使那些能够访问类名或精细提示的方法也被其超越。这强调了我们的方法的可扩展性，并表明预训练生成模型在模型窃取方面的风险可能比先前认识的要大。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>模型窃取对机器学习构成安全风险，攻击者可复制黑盒模型，危及知识产权和暴露敏感信息。</li>
<li>利用预训练扩散模型进行数据合成的方法虽提高效率和性能，但依赖手动提示，限制自动化和可扩展性。</li>
<li>提出了一个更现实的威胁模型，无需提示设计技能或了解类名即可执行模型窃取。</li>
<li>推出了Stealix方法，无需预设提示即可进行模型窃取。</li>
<li>Stealix使用两个开源预训练模型推断受害者模型的数据分布。</li>
<li>通过遗传算法迭代优化提示，提高合成图像的精度和多样性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05867">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-10ee92c6ccdd7f428068a093b2b15d7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e817da870e814597adda0d1e39b13a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-562e39916dd3996e67fb971b26775bbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-309a83cf82806e8c5a5ad57b91cb70d8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLIA-–-Enabling-Low-Latency-Interactive-Avatars-Real-Time-Audio-Driven-Portrait-Video-Generation-with-Diffusion-Models"><a href="#LLIA-–-Enabling-Low-Latency-Interactive-Avatars-Real-Time-Audio-Driven-Portrait-Video-Generation-with-Diffusion-Models" class="headerlink" title="LLIA – Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven   Portrait Video Generation with Diffusion Models"></a>LLIA – Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven   Portrait Video Generation with Diffusion Models</h2><p><strong>Authors:Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, Xunliang Cai</strong></p>
<p>Diffusion-based models have gained wide adoption in the virtual human generation due to their outstanding expressiveness. However, their substantial computational requirements have constrained their deployment in real-time interactive avatar applications, where stringent speed, latency, and duration requirements are paramount. We present a novel audio-driven portrait video generation framework based on the diffusion model to address these challenges. Firstly, we propose robust variable-length video generation to reduce the minimum time required to generate the initial video clip or state transitions, which significantly enhances the user experience. Secondly, we propose a consistency model training strategy for Audio-Image-to-Video to ensure real-time performance, enabling a fast few-step generation. Model quantization and pipeline parallelism are further employed to accelerate the inference speed. To mitigate the stability loss incurred by the diffusion process and model quantization, we introduce a new inference strategy tailored for long-duration video generation. These methods ensure real-time performance and low latency while maintaining high-fidelity output. Thirdly, we incorporate class labels as a conditional input to seamlessly switch between speaking, listening, and idle states. Lastly, we design a novel mechanism for fine-grained facial expression control to exploit our model’s inherent capacity. Extensive experiments demonstrate that our approach achieves low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX 4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45 FPS at a resolution of 512x512, with an initial video generation latency of 140 ms and 215 ms, respectively. </p>
<blockquote>
<p>基于扩散模型的方法在虚拟人生成领域因其出色的表现力而得到广泛应用。然而，其巨大的计算需求限制了其在实时交互式化身应用程序中的部署，这些应用程序对速度、延迟和持续时间有严格的要求。我们提出了一种基于扩散模型的新型音频驱动肖像视频生成框架，以解决这些挑战。首先，我们提出可变长度视频生成方法，以减少生成初始视频片段或状态转换所需的最短时间，从而显著增强用户体验。其次，我们为音频图像到视频的转换提出了一致的模型训练策略，以确保实时性能，实现快速多步生成。此外，还采用了模型量化和流水线并行化方法来加速推理速度。为了解决扩散过程和模型量化带来的稳定性损失，我们引入了一种适用于长持续时间视频生成的新型推理策略。这些方法确保了实时性能和低延迟，同时保持了高保真输出。第三，我们将类别标签作为条件输入融入其中，以无缝切换说话、倾听和空闲状态。最后，我们设计了一种用于精细面部表情控制的新机制，以利用我们模型的内在能力。大量实验表明，我们的方法实现了低延迟、流畅和逼真的双向通信。在NVIDIA RTX 4090D上，我们的模型在384x384分辨率下最高达到78帧每秒（FPS），在512x512分辨率下达到45 FPS，初始视频生成延迟分别为140毫秒和215毫秒。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05806v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于扩散模型的音频驱动肖像视频生成框架解决了现有挑战，包括实时互动化身应用的严格速度、延迟和持续时间要求。提出可变长度视频生成、一致性模型训练策略、模型量化和管道并行化等方法，以提高用户体验、保证实时性能和低延迟，同时维持高保真输出。此外，纳入类别标签作为条件输入，实现说话、聆听和空闲状态间的无缝切换，并设计精细面部表情控制机制。实验证明，该方法实现低延迟、流畅和真实的双向交流。在NVIDIA RTX 4090D上，模型最高达78帧&#x2F;秒（FPS）的帧率，分辨率分别为384x384和45 FPS在分辨率为512x512。初始视频生成延迟分别为140毫秒和215毫秒。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>扩散模型在虚拟人生成中的出色表达力。</li>
<li>提出的音频驱动肖像视频生成框架解决了实时互动化身应用的挑战。</li>
<li>变量长度视频生成提高了用户体验。</li>
<li>一致性模型训练策略确保了实时性能，实现了快速几步生成。</li>
<li>模型量化和管道并行化技术加速了推理速度。</li>
<li>新的推理策略提高了长时长视频生成的稳定性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05806">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d6076a44b790d08c6a4cdedfbbf3341.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d07c6a332c72395c8c36293ba950bd1f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FocusDiff-Advancing-Fine-Grained-Text-Image-Alignment-for-Autoregressive-Visual-Generation-through-RL"><a href="#FocusDiff-Advancing-Fine-Grained-Text-Image-Alignment-for-Autoregressive-Visual-Generation-through-RL" class="headerlink" title="FocusDiff: Advancing Fine-Grained Text-Image Alignment for   Autoregressive Visual Generation through RL"></a>FocusDiff: Advancing Fine-Grained Text-Image Alignment for   Autoregressive Visual Generation through RL</h2><p><strong>Authors:Kaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Recent studies extend the autoregression paradigm to text-to-image generation, achieving performance comparable to diffusion models. However, our new PairComp benchmark – featuring test cases of paired prompts with similar syntax but different fine-grained semantics – reveals that existing models struggle with fine-grained text-image alignment thus failing to realize precise control over visual tokens. To address this, we propose FocusDiff, which enhances fine-grained text-image semantic alignment by focusing on subtle differences between similar text-image pairs. We construct a new dataset of paired texts and images with similar overall expressions but distinct local semantics, further introducing a novel reinforcement learning algorithm to emphasize such fine-grained semantic differences for desired image generation. Our approach achieves state-of-the-art performance on existing text-to-image benchmarks and significantly outperforms prior methods on PairComp. </p>
<blockquote>
<p>最近的研究将自回归范式扩展到文本到图像生成，其性能与扩散模型相当。然而，我们新的PairComp基准测试——包含具有相似语法但具有不同细微语义的配对提示测试用例——表明，现有模型在细微的文本图像对齐方面存在困难，无法实现视觉标记的精确控制。为了解决这个问题，我们提出了FocusDiff，它通过关注相似文本图像对之间的细微差异，提高了文本图像的精细语义对齐。我们构建了一个新的配对文本和图像数据集，具有相似的整体表达但局部语义不同，并进一步引入了一种新的强化学习算法来强调这种细微的语义差异，以实现所需的图像生成。我们的方法在现有的文本到图像基准测试上达到了最先进的性能，并在PairComp上显著优于先前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05501v1">PDF</a> 15 pages, 8 figures. Project Page: <a target="_blank" rel="noopener" href="https://focusdiff.github.io/">https://focusdiff.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>近期研究将自回归范式扩展到文本到图像生成领域，性能与扩散模型相当。然而，新的PairComp基准测试显示，现有模型在精细文本图像对齐方面存在困难，无法实现视觉符号的精确控制。为解决这一问题，我们提出FocusDiff，通过关注相似文本图像对之间的细微差异，提高精细文本图像语义对齐。我们构建了新的配对文本和图像数据集，整体表达相似但局部语义不同，并引入新的强化学习算法来强调这种精细语义差异以实现所需的图像生成。FocusDiff在现有文本到图像基准测试中表现卓越，并在PairComp上显著优于先前方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期研究将自回归范式应用于文本到图像生成，性能与扩散模型相当。</li>
<li>现有模型在精细文本图像对齐方面存在困难。</li>
<li>PairComp基准测试揭示了这一挑战，强调对视觉符号的精确控制的重要性。</li>
<li>FocusDiff通过关注细微差异提高精细文本图像语义对齐。</li>
<li>FocusDiff构建了新的配对文本和图像数据集，整体表达相似但局部语义不同。</li>
<li>强化学习算法被用于FocusDiff，以强调精细语义差异实现所需图像生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-13a1040f0fa0913245e08a4dcfe8602c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72eb9f96c74f459adce38dd969cdb0b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea69ce3433f61c8c19286bf7b985839f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668c520498981fa1c3d5219b83100283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83c685c14eb0d04edbef449491094a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d898e9e177f88855dc061ea84a6bb3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations"><a href="#Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations" class="headerlink" title="Towards Reliable Identification of Diffusion-based Image Manipulations"></a>Towards Reliable Identification of Diffusion-based Image Manipulations</h2><p><strong>Authors:Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati</strong></p>
<p>Changing facial expressions, gestures, or background details may dramatically alter the meaning conveyed by an image. Notably, recent advances in diffusion models greatly improve the quality of image manipulation while also opening the door to misuse. Identifying changes made to authentic images, thus, becomes an important task, constantly challenged by new diffusion-based editing tools. To this end, we propose a novel approach for ReliAble iDentification of inpainted AReas (RADAR). RADAR builds on existing foundation models and combines features from different image modalities. It also incorporates an auxiliary contrastive loss that helps to isolate manipulated image patches. We demonstrate these techniques to significantly improve both the accuracy of our method and its generalisation to a large number of diffusion models. To support realistic evaluation, we further introduce BBC-PAIR, a new comprehensive benchmark, with images tampered by 28 diffusion models. Our experiments show that RADAR achieves excellent results, outperforming the state-of-the-art in detecting and localising image edits made by both seen and unseen diffusion models. Our code, data and models will be publicly available at alex-costanzino.github.io&#x2F;radar. </p>
<blockquote>
<p>改变面部表情、手势或背景细节可能会极大地改变图像所传达的含义。值得注意的是，扩散模型的最新进展在极大地提高了图像操作质量的同时，也打开了滥用的大门。因此，识别对真实图像所做的更改成为了一项重要任务，这项任务不断受到新的基于扩散的编辑工具的挑战。为此，我们提出了一种新型方法——可靠标识图像内填充区域（RADAR）。RADAR建立在现有的基础模型上，结合了不同图像模态的特征。它还融入了一种辅助对比损失，有助于隔离操纵过的图像区块。我们展示的这些技术极大地提高了我们方法的准确性及其对大量扩散模型的推广能力。为了支持现实评估，我们还引入了BBC-PAIR，这是一个新的综合基准测试，包含由28种扩散模型篡改过的图像。我们的实验表明，RADAR取得了优异的结果，在检测和定位已知和未知的扩散模型所做的图像编辑方面超过了最先进的技术。我们的代码、数据和模型将在alex-costanzino.github.io&#x2F;radar上公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05466v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用扩散模型进行图像操作的新挑战，并提出了一种名为RADAR的新方法，用于识别图像中的编辑区域。RADAR结合不同图像模态的特征，并引入辅助对比损失来隔离编辑过的图像区域。同时，为了支持真实评估，还引入了BBC-PAIR新基准测试，包含由28种扩散模型修改的图像。实验表明，RADAR在检测和定位由已知和未知扩散模型进行的图像编辑方面表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在改进图像质量的同时，也带来了新的图像操作滥用问题。</li>
<li>RADAR方法结合了不同图像模态的特征，以识别图像中的编辑区域。</li>
<li>RADAR通过引入辅助对比损失，提高了检测图像编辑区域的准确性和泛化能力。</li>
<li>BBC-PAIR是一个新的基准测试，用于支持对图像操作检测方法的真实评估。</li>
<li>RADAR在BBC-PAIR基准测试上表现出卓越性能，对已知和未知的扩散模型进行的图像编辑都有很好的检测效果。</li>
<li>RADAR的源代码、数据和模型将公开提供，便于其他研究者使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05466">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69663f8357c48500036575130339d759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af2166a234b6d1a78d8fac83caa42cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84359b69dca16787536bd8d3beb9726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6eaf4c668fffd453db25aedf5883b7f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-Diffusion-Transformer-Designs-via-Grafting"><a href="#Exploring-Diffusion-Transformer-Designs-via-Grafting" class="headerlink" title="Exploring Diffusion Transformer Designs via Grafting"></a>Exploring Diffusion Transformer Designs via Grafting</h2><p><strong>Authors:Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei</strong></p>
<p>Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present grafting, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL&#x2F;2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for DiT-XL&#x2F;2) using &lt;2% pretraining compute. We then graft a text-to-image model (PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL&#x2F;2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2x and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: <a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu</a> </p>
<blockquote>
<p>设计模型架构需要进行诸如选择运算符（例如注意力、卷积）和配置（例如深度、宽度）之类的决策。然而，评估这些决策对模型质量的影响需要大量的预训练成本，这限制了架构的研究。我们受启发于如何在现有代码上构建新软件，我们想知道：可以使用预训练模型来研究新的架构设计吗？为此，我们提出了嫁接，这是一种简单的编辑预训练扩散变压器（DiTs）的方法，以在较小的计算预算下实现新的架构。根据我们对激活行为和注意力局部性的分析，我们以DiT-XL&#x2F;2设计为基础构建了一个测试平台，以研究嫁接对模型质量的影响。使用这个测试平台，我们通过嫁接开发了一系列混合设计：用门控卷积、局部注意力和线性注意力替换softmax注意力，并用可变扩展率和卷积变体替换MLP。值得注意的是，许多混合设计在预训练计算量不到2%的情况下，使用FID（2.38-2.64与DiT-XL&#x2F;2的2.27）达到了良好的质量。然后我们将一个文本到图像模型（PixArt-Sigma）进行嫁接，实现了1.43倍的加速，同时GenEval分数下降不到2%。最后，我们进行了一个案例研究，通过嫁接重新构建DiT-XL&#x2F;2，将每一对连续的变压器块转换为并行块。这减少了模型深度的一半，并产生了比其他具有相似深度的模型更好的质量（FID：2.77）。总的来说，我们证明了可以通过嫁接预训练的DiTs来探索新的扩散模型设计，编辑范围从操作符替换到架构重组。代码和嫁接模型：<a target="_blank" rel="noopener" href="https://grafting.stanford.edu./">https://grafting.stanford.edu。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05340v2">PDF</a> 22 pages; Project website: <a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu</a></p>
<p><strong>Summary</strong><br>     本文通过借鉴软件开发中的复用思想，提出了扩散模型架构设计的新方法——嫁接预训练模型。通过对预训练的扩散转换器进行简单编辑，实现新架构在有限计算预算下的研究。通过在预训练模型上移植不同的设计，证明了这种方法的高效性，只需较少的计算就能得到良好质量的模型设计。该研究发现已在官方网站上公布，以推进研究。简单而言，作者提出了一种利用预训练模型进行扩散模型架构设计的新思路，为快速开发新模型提供了有效方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究者提出了通过嫁接预训练模型来设计新的扩散模型架构的方法。</li>
<li>通过编辑预训练的扩散转换器（DiTs），实现了在小计算预算下研究新架构的目的。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05340">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d046a83c26f21ff929ad26e2576f1a9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89076046b741e0b58281d672d44906d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d46bc3806e04a5d5868564af91a233b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3940bf603b77c759e8b1293fa88b839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b8d1b02559903edd1458b51dfa1a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb4c6c4074112e116ac2560fed9daca0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SeedEdit-3-0-Fast-and-High-Quality-Generative-Image-Editing"><a href="#SeedEdit-3-0-Fast-and-High-Quality-Generative-Image-Editing" class="headerlink" title="SeedEdit 3.0: Fast and High-Quality Generative Image Editing"></a>SeedEdit 3.0: Fast and High-Quality Generative Image Editing</h2><p><strong>Authors:Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang</strong></p>
<p>We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID&#x2F;IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real&#x2F;synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%). </p>
<blockquote>
<p>我们推出了SeedEdit 3.0版本，与我们的T2I模型Seedream 3.0一同介绍。相较于我们之前的SeedEdit版本，它在遵循编辑指令和保留图像内容（例如ID&#x2F;IP）方面对真实图像输入有了显著改进。除了使用T2I进行模型升级，本报告还展示了若干项重要改进。首先，我们开发了一个增强的数据整理管道，采用元信息范式和元信息嵌入策略，有助于混合来自多个数据源的图片。这使我们能够有效地扩展编辑数据，而元信息有助于更紧密地将VLM与扩散模型连接起来。其次，我们引入了一个联合学习管道，用于计算扩散损失和奖励损失。最后，我们在测试基准上对SeedEdit 3.0进行了真实&#x2F;合成图像编辑的评估，它在多方面达到了最佳平衡，使用性率高达56.1%，相比之下，SeedEdit 1.6为38.4%，GPT4o为37.1%，Gemini 2.0为30.3%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05083v2">PDF</a> Website: <a target="_blank" rel="noopener" href="https://seed.bytedance.com/tech/seededit">https://seed.bytedance.com/tech/seededit</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了新推出的SeedEdit 3.0与其配套的T2I模型Seedream 3.0，相较于之前的版本，它在遵循编辑指令和保留图像内容（如ID&#x2F;IP）方面有了显著提升。此外，本文还介绍了几个关键改进，包括采用元信息范式和元信息嵌入策略增强数据整理流程、引入联合学习管道计算扩散损失和奖励损失，并在测试基准上评估了SeedEdit 3.0在真实&#x2F;合成图像编辑方面的表现，实现了多方面的最佳平衡，使用率达到56.1%，相较于其他模型有显著提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SeedEdit 3.0与其配套的T2I模型Seedream 3.0推出，显著提升了编辑指令遵循和图像内容保留的能力。</li>
<li>采用了元信息范式和元信息嵌入策略的增强数据整理流程，能有效整合多源图像数据。</li>
<li>引入了联合学习管道，计算扩散损失和奖励损失，促进了模型性能的提升。</li>
<li>SeedEdit 3.0在真实&#x2F;合成图像编辑方面的评估表现优异，实现了多方面的最佳平衡。</li>
<li>SeedEdit 3.0的使用率达到56.1%，相较于之前版本和其他模型有显著提升。</li>
<li>模型的升级和改进有助于提升图像编辑任务的效率和效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05083">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1e3f44fbe8067708c38f62b117d6dd8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb4600dc35b860b01b8ad838e2f0ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2934c8258ac1cc2a447e3edeff776025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ef819710f5a2be0462d56c5b563e6cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1943abbace22e3888206647fb27b7fe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoders-Again"><a href="#Sparse-Autoencoders-Again" class="headerlink" title="Sparse Autoencoders, Again?"></a>Sparse Autoencoders, Again?</h2><p><strong>Authors:Yin Lu, Xuening Zhu, Tong He, David Wipf</strong></p>
<p>Is there really much more to say about sparse autoencoders (SAEs)? Autoencoders in general, and SAEs in particular, represent deep architectures that are capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite the wide-ranging applicability, there have been relatively few changes to SAEs beyond the original recipe from decades ago, namely, standard deep encoder&#x2F;decoder layers trained with a classical&#x2F;deterministic sparse regularizer applied within the latent space. One possible exception is the variational autoencoder (VAE), which adopts a stochastic encoder module capable of producing sparse representations when applied to manifold data. In this work we formalize underappreciated weaknesses with both canonical SAEs, as well as analogous VAEs applied to similar tasks, and propose a hybrid alternative model that circumvents these prior limitations. In terms of theoretical support, we prove that global minima of our proposed model recover certain forms of structured data spread across a union of manifolds. Meanwhile, empirical evaluations on synthetic and real-world datasets substantiate the efficacy of our approach in accurately estimating underlying manifold dimensions and producing sparser latent representations without compromising reconstruction error. In general, we are able to exceed the performance of equivalent-capacity SAEs and VAEs, as well as recent diffusion models where applicable, within domains such as images and language model activation patterns. </p>
<blockquote>
<p>关于稀疏自编码器（SAEs）还有什么更多可说的吗？总的来说，自编码器，尤其是SAEs，代表了能够建模数据中的低维潜在结构的深度架构。这种结构可能反映了大型语言模型激活中的相关性模式，或复杂的自然图像流形。尽管SAEs具有广泛的应用范围，但除了几十年前的原始配方之外，几乎没有太多变化。也就是说，用经典&#x2F;确定性稀疏正则化在潜在空间内训练的深度编码器&#x2F;解码器层。一个可能的例外是变分自编码器（VAE），它采用了一种随机编码器模块，在应用于流形数据时能够产生稀疏表示。在这项工作中，我们正式提出了被忽视的弱点，无论是典型的SAEs，还是类似任务的VAEs，并提出了一种混合的替代模型，该模型可以绕过这些先前的限制。在理论支持方面，我们证明了我们的模型全局最小值能够恢复跨多个流形的某些形式的结构化数据分布。同时，在合成和真实数据集上的实证评估证实了我们方法在准确估计潜在流形维度和产生稀疏潜在表示方面的有效性，而不会损害重建误差。总的来说，我们在图像和语言模型激活模式等领域超过了同等容量的SAEs和VAEs的性能表现，以及在适用的最新扩散模型的性能表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04859v2">PDF</a> Accepted to the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了稀疏自动编码器（SAE）的局限性，并提出了一种新型的混合模型来克服这些局限。文章指出，尽管SAE在许多领域都有广泛应用，但其结构和技术自几十年前的原始配方以来并没有太大变化。文章还介绍了变分自动编码器（VAE）的某些特性，并展示了新的混合模型在理论支持和实证研究上的优势，例如在估计底层流形维度和产生稀疏潜在表示方面的优势。总体而言，新模型能在图像和语言模型激活模式等领域超越等效容量的SAE和VAE以及最近的扩散模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稀疏自动编码器（SAE）是一种能够建模数据低维潜在结构的深度架构。</li>
<li>SAE在多种领域有广泛应用，但其结构和技术自提出以来变化较少。</li>
<li>变分自动编码器（VAE）采用随机编码器模块，能应用于流形数据产生稀疏表示。</li>
<li>本文指出了SAE和类似任务的VAE的不足之处，并提出了一种新型混合模型来克服这些局限。</li>
<li>新模型在理论上有证明，能够恢复跨多个流形的某些形式的结构化数据。</li>
<li>实证研究表明，新模型在估计底层流形维度和产生稀疏潜在表示方面表现优异，且不会增加重建误差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04859">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9f6051c5d2c288ae2566b210f8184d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2839c97697e1774bcba9aec0b0918924.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f04b18284bd0a7cfb8ce8dbf7d4b71ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04011231d485016c3e5bbd3ef9a608f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Smoothed-Preference-Optimization-via-ReNoise-Inversion-for-Aligning-Diffusion-Models-with-Varied-Human-Preferences"><a href="#Smoothed-Preference-Optimization-via-ReNoise-Inversion-for-Aligning-Diffusion-Models-with-Varied-Human-Preferences" class="headerlink" title="Smoothed Preference Optimization via ReNoise Inversion for Aligning   Diffusion Models with Varied Human Preferences"></a>Smoothed Preference Optimization via ReNoise Inversion for Aligning   Diffusion Models with Varied Human Preferences</h2><p><strong>Authors:Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, Min Zhang</strong></p>
<p>Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation models with human preferences using pairwise preference data. Although substantial resources are expended in collecting and labeling datasets, a critical aspect is often neglected: \textit{preferences vary across individuals and should be represented with more granularity.} To address this, we propose SmPO-Diffusion, a novel method for modeling preference distributions to improve the DPO objective, along with a numerical upper bound estimation for the diffusion optimization objective. First, we introduce a smoothed preference distribution to replace the original binary distribution. We employ a reward model to simulate human preferences and apply preference likelihood averaging to improve the DPO loss, such that the loss function approaches zero when preferences are similar. Furthermore, we utilize an inversion technique to simulate the trajectory preference distribution of the diffusion model, enabling more accurate alignment with the optimization objective. Our approach effectively mitigates issues of excessive optimization and objective misalignment present in existing methods through straightforward modifications. Our SmPO-Diffusion achieves state-of-the-art performance in preference evaluation, outperforming baselines across metrics with lower training costs. The project page is <a target="_blank" rel="noopener" href="https://jaydenlyh.github.io/SmPO-project-page/">https://jaydenlyh.github.io/SmPO-project-page/</a>. </p>
<blockquote>
<p>直接偏好优化（DPO）使用成对偏好数据将文本到图像（T2I）生成模型与人类偏好对齐。虽然收集和标注数据集需要耗费大量资源，但往往忽略了一个关键方面：*不同个体的偏好不同，应以更精细的方式表示。为了解决这一问题，我们提出了SmPO-Diffusion，这是一种改进DPO目标的新型偏好分布建模方法，以及扩散优化目标的数值上限估计。首先，我们引入平滑偏好分布来替换原始二进制分布。我们采用奖励模型来模拟人类偏好，并通过偏好可能性平均来改进DPO损失，使得当偏好相似时损失函数接近零。此外，我们利用反演技术模拟扩散模型的轨迹偏好分布，使与优化目标对齐得更准确。我们的方法通过直接修改有效地缓解了现有方法中过度优化和目标不匹配的问题。SmPO-Diffusion在偏好评估方面达到了最新技术水平，在各项指标上均优于基线，同时降低了训练成本。项目页面是<a target="_blank" rel="noopener" href="https://jaydenlyh.github.io/SmPO-project-page/%E3%80%82">https://jaydenlyh.github.io/SmPO-project-page/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02698v2">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了SmPO-Diffusion方法，通过引入平滑偏好分布来改进Direct Preference Optimization（DPO）的目标。该方法使用奖励模型模拟人类偏好，并采用偏好可能性平均法改进DPO损失函数。同时，利用扩散模型的轨迹偏好分布模拟技术，更有效地对齐优化目标。SmPO-Diffusion解决了现有方法中的过度优化和目标不对齐问题，实现了优异的偏好评估性能，并且在训练成本上优于基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmPO-Diffusion提出使用平滑偏好分布来改进DPO的目标，以更好地模拟人类偏好。</li>
<li>引入奖励模型来模拟人类偏好，提高DPO损失函数的准确性。</li>
<li>采用偏好可能性平均法，当偏好相似时，损失函数接近零。</li>
<li>利用扩散模型的轨迹偏好分布模拟技术，更有效地对齐优化目标。</li>
<li>SmPO-Diffusion解决了现有方法中的过度优化和目标不对齐问题。</li>
<li>SmPO-Diffusion在偏好评估方面达到最新性能水平，并在训练成本上优于基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02698">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6742da8d8e9b49b497896a518d81d79a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b03e0f1affb96ca12192b1373cd69733.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fa2116ae724c8ef509c8648351861e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdef7621a4b0601a72c49b8fe875c298.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Latent-Feature-Guided-Conditional-Diffusion-for-Generative-Image-Semantic-Communication"><a href="#Latent-Feature-Guided-Conditional-Diffusion-for-Generative-Image-Semantic-Communication" class="headerlink" title="Latent Feature-Guided Conditional Diffusion for Generative Image   Semantic Communication"></a>Latent Feature-Guided Conditional Diffusion for Generative Image   Semantic Communication</h2><p><strong>Authors:Zehao Chen, Xinfeng Wei, Haonan Tong, Zhaohui Yang, Changchuan Yin</strong></p>
<p>Semantic communication is proposed and expected to improve the efficiency of massive data transmission over sixth generation (6G) networks. However, existing image semantic communication schemes are primarily focused on optimizing pixel-level metrics, while neglecting the crucial aspect of region of interest (ROI) preservation. To address this issue, we propose an ROI-aware latent representation-oriented image semantic communication (LRISC) system. In particular, we first map the source image to latent features in a high-dimensional semantic space, these latent features are then fused with ROI mask through a feature-weighting mechanism. Subsequently, these features are encoded using a joint source and channel coding (JSCC) scheme with adaptive rate for efficient transmission over a wireless channel. At the receiver, a conditional diffusion model is developed by using the received latent features as conditional guidance to steer the reverse diffusion process, progressively reconstructing high-fidelity images while preserving semantic consistency. Moreover, we introduce a channel signal-to-noise ratio (SNR) adaptation mechanism, allowing one model to work across various channel states. Experiments show that the proposed method significantly outperforms existing methods, in terms of learned perceptual image patch similarity (LPIPS) and robustness against channel noise, with an average LPIPS reduction of 43.3% compared to DeepJSCC, while guaranteeing the semantic consistency. </p>
<blockquote>
<p>语义通信旨在提高第六代（6G）网络上大规模数据传输的效率。然而，现有的图像语义通信方案主要关注像素级指标的优化，忽视了感兴趣区域（ROI）保持的重要方面。为了解决这一问题，我们提出了一种面向潜在表示的感兴趣区域感知图像语义通信系统（LRISC）。具体而言，我们首先将源图像映射到高维语义空间中的潜在特征上，然后通过特征加权机制将这些潜在特征与ROI掩膜融合。随后，这些特征使用联合源和信道编码（JSCC）方案进行编码，并通过无线信道进行自适应速率的有效传输。在接收端，通过使用接收到的潜在特征作为条件指导，开发了一种条件扩散模型，以引导反向扩散过程，逐步重建高质量图像并保持语义一致性。此外，我们还引入了一种信道信噪比（SNR）自适应机制，使一个模型能够在各种信道状态下工作。实验表明，所提出的方法在感知图像补丁相似性（LPIPS）和对抗信道噪声的稳健性方面显著优于现有方法，与DeepJSCC相比平均LPIPS降低了43.3%，同时保证了语义一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21577v2">PDF</a> 6 pages, 6 figures, update title</p>
<p><strong>Summary</strong><br>    针对现有图像语义通信方案忽视感兴趣区域（ROI）保留的问题，提出一种基于ROI感知的潜在表示导向图像语义通信系统（LRISC）。该系统将源图像映射到高维语义空间的潜在特征，并与ROI掩膜融合。通过联合源信道编码（JSCC）方案自适应速率编码特征，实现无线信道上的高效传输。接收端采用条件扩散模型，以接收到的潜在特征作为条件指导，反向扩散过程逐步重建高保真图像，同时保持语义一致性。此外，引入信道信噪比（SNR）自适应机制，使模型能在各种信道状态下工作。实验表明，该方法在感知图像块相似性（LPIPS）和信道噪声鲁棒性方面显著优于现有方法，平均LPIPS降低43.3%，同时保证语义一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语义通信提高6G网络大数据传输效率。</li>
<li>现有图像语义通信方案主要优化像素级指标，忽视ROI保留。</li>
<li>提出LRISC系统，映射源图像到高维语义空间并融合ROI掩膜。</li>
<li>使用JSCC方案自适应速率编码特征，实现高效无线传输。</li>
<li>接收端采用条件扩散模型逐步重建高保真图像并保持语义一致性。</li>
<li>引入SNR自适应机制适应各种信道状态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21577">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c744f2eb37d3d5a3e79596c7eb32e18a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6e961db6bd6441be06a13f0a5db7bd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d63e92aea5424baa15256c2129ef4f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a6f6dbfe813cae95b2c73ecf7f3ba1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9def2bdb721e686d718bdf2dd6ade90c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92eb6a961a473b1cbba87907e70e7aac.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Diffusion-Models-A-Survey"><a href="#Efficient-Diffusion-Models-A-Survey" class="headerlink" title="Efficient Diffusion Models: A Survey"></a>Efficient Diffusion Models: A Survey</h2><p><strong>Authors:Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang</strong></p>
<p>Diffusion models have emerged as powerful generative models capable of producing high-quality contents such as images, videos, and audio, demonstrating their potential to revolutionize digital content creation. However, these capabilities come at the cost of their significant computational resources and lengthy generation time, underscoring the critical need to develop efficient techniques for practical deployment. In this survey, we provide a systematic and comprehensive review of research on efficient diffusion models. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient diffusion model topics from algorithm-level, system-level, and framework perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at <a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey</a>. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient diffusion model research and inspire them to contribute to this important and exciting field. </p>
<blockquote>
<p>扩散模型作为强大的生成模型已经崭露头角，能够产生高质量的图像、视频和音频等内容，显示出它们在数字内容创作领域具有颠覆性潜力。然而，这些功能需要大量的计算资源和漫长的生成时间，这凸显了开发高效技术用于实际部署的迫切需求。在本次调查中，我们对高效扩散模型的研究进行了系统而全面的综述。我们从算法层面、系统层面和框架视角对文献进行了分类，涵盖了三个主要类别中不同但相互关联的高效扩散模型主题。我们还创建了一个GitHub仓库，其中整理了本次调查中介绍的论文，地址为：<a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey%E3%80%82%E6%88%91%E4%BB%AC%E5%B9%B3%E5%B8%B8%E6%9C%AC%E6%9C%AF%E5%AF%BC%E5%AF%BC%E8%BF%BD%E6%B1%BD%E8%AF%AD%E7%A0%BA%E7%9A%84%E7%90%B3%E5%A4%B1%E6%B6%A6%E6%9D%A5%(Efficient-Diffusion-Model">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey。我们希望本次调查能为研究者和从业者提供对高效扩散模型的系统性理解，并激发他们对这一重要且激动人心的领域的贡献。</a>能作为有价值的资源，帮助研究者和从业者系统地了解高效扩散模型的研究，并激励他们为这一重要且令人兴奋的领域做出贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06805v3">PDF</a> Published in Transactions on Machine Learning Research (TMLR-2025)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型作为强大的生成模型在数字内容创作领域的潜力。虽然这些模型能够产生高质量的内容如图像、视频和音频，但它们需要大量的计算资源和长时间的生成时间。本文提供了一篇关于高效扩散模型的综合性调查，从算法、系统和框架三个角度组织了相关文献，并创建了一个GitHub仓库来整理这些论文。本文旨在为研究人员和实践者提供对高效扩散模型的系统性理解，并激发他们对这一重要且激动人心的领域的贡献。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已成为强大的生成模型，具有革命性数字内容创作的潜力。</li>
<li>扩散模型虽然能产生高质量内容，但需要大量计算资源和长时间的生成时间。</li>
<li>本文提供了一篇关于高效扩散模型的综合性调查。</li>
<li>调查从算法、系统和框架三个角度组织了相关文献。</li>
<li>创建一个GitHub仓库来整理论文，方便研究人员和实践者获取资源。</li>
<li>本文旨在为研究人员和实践者提供对高效扩散模型的系统性理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06805">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2bdafff256bc7ee316918dc59e1d6299.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d6512ec15151a5791afab41f34547ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2a9265e9ce5ffeaf4ca20905f46ee5d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Normalizing-Flows-are-Capable-Generative-Models"><a href="#Normalizing-Flows-are-Capable-Generative-Models" class="headerlink" title="Normalizing Flows are Capable Generative Models"></a>Normalizing Flows are Capable Generative Models</h2><p><strong>Authors:Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Josh Susskind</strong></p>
<p>Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/apple/ml-tarflow">https://github.com/apple/ml-tarflow</a>. </p>
<blockquote>
<p>标准化流（NFs）是基于连续输入的似然模型。它们在密度估计和生成建模任务上都显示出有前景的结果，但近年来受到的关注相对较少。在这项工作中，我们证明NFs比以前认为的更强大。我们提出了TarFlow：一种简单且可扩展的架构，能够实现高性能的NF模型。TarFlow可以被看作是基于变压器的掩码自回归流（MAF）的变体：它由图像补丁上的自回归变压器块堆叠而成，并在层之间交替自回归方向。TarFlow可以端到端进行训练，并且能够直接对像素进行建模和生成。我们还提出了三种提高样本质量的关键技术：训练过程中的高斯噪声增强、训练后的去噪程序，以及针对有条件和无条件设置的有效的指导方法。将这些结合起来，TarFlow在图像似然估计方面达到了新的最先进的水平，大大超过了以前最好的方法，并且生成的样本质量和多样性可与扩散模型相当，这是首次使用独立的NF模型实现。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/apple/ml-tarflow%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/apple/ml-tarflow获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06329v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了流模型（NFs）在密度估计和生成建模任务上的优异表现，并提出了一种简单且可扩展的架构TarFlow，它基于Transformer和Masked Autoregressive Flows（MAFs），能够构建高性能的NF模型。TarFlow通过三层技术改进样本质量，包括训练过程中的高斯噪声增强、训练后的去噪程序以及在有类和无条件下的有效指导方法。最终，TarFlow在图像的概率估计上达到新的最高水平，生成样本的质量和多样性首次与扩散模型相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流模型（NFs）在密度估计和生成建模任务上具有优异表现。</li>
<li>TarFlow是一种基于Transformer和MAFs的架构，使NF模型更加高效。</li>
<li>TarFlow通过堆叠的自回归Transformer块对图像块进行处理，并在层间交替自回归方向。</li>
<li>TarFlow采用三层技术提高样本质量：训练中的高斯噪声增强、训练后的去噪程序和有指导的生成方法。</li>
<li>TarFlow达到新的图像概率估计的最高水平，大幅度超越以前最好的方法。</li>
<li>TarFlow生成的样本质量和多样性首次与扩散模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06329">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cb21b054e9eb87d153862caa3d38e3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a449c5a515ca12aa486623921d989d94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-671334975173fbb4b6b15a96b63c1c0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a109ba9973f0bc49a09b2c5b78b3d319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33da3fea396089ae575cfb2780859afc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79255ee595e1c67b3e521691f7fa0b53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a032bae5e9fc4aa9184f5b7788b2436c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Birth-and-Death-of-a-Rose"><a href="#Birth-and-Death-of-a-Rose" class="headerlink" title="Birth and Death of a Rose"></a>Birth and Death of a Rose</h2><p><strong>Authors:Chen Geng, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu</strong></p>
<p>We study the problem of generating temporal object intrinsics – temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose – from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pre-trained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan. Project website: <a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d">https://chen-geng.com/rose4d</a> </p>
<blockquote>
<p>我们研究从预训练的2D基础模型生成时间对象固有体的问题——时间演化的对象几何、反射率和纹理序列，如盛开的玫瑰。不同于需要大量人工努力和专业知识的传统3D建模和动画技术，我们引入了一种方法，该方法使用从预训练的2D扩散模型中提炼的信号生成此类资产。为了确保对象固有体的时间一致性，我们提出了基于自监督学习图像特征自动派生的时间状态引导蒸馏的神经网络模板。我们的方法可以为多种自然现象生成高质量的时间对象固有体，并能够在任何生命周期的任何时间、任何环境照明条件下，从任何视角对这些动态对象进行采样和可控渲染。项目网站：<a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d">https://chen-geng.com/rose4d</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05278v2">PDF</a> CVPR 2025 Oral. Project website: <a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d">https://chen-geng.com/rose4d</a></p>
<p><strong>Summary</strong></p>
<p>该研究利用预训练的二维扩散模型生成时间性对象固有属性，如生长中的花朵等。该研究提出了一种从预训练的二维扩散模型中提炼信号生成此类资产的方法，无需传统三维建模和动画技术所需的大量手动工作和专业知识。通过使用神经模板进行时间状态引导的蒸馏，确保了对象固有属性的时间一致性。该方法可以生成高质量的时间性对象固有属性，为多种自然现象提供采样，并可从任何角度、在任何环境照明条件下、在对象的生命周期中的任何时间进行可控渲染。有关详细信息，请访问项目网站：<a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d%E3%80%82">https://chen-geng.com/rose4d。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究关注生成时间性对象固有属性，如生长中的花朵等。</li>
<li>利用预训练的二维扩散模型生成这些属性，简化了传统三维建模和动画技术的复杂流程。</li>
<li>引入神经模板确保对象固有属性的时间一致性。</li>
<li>该方法可以生成高质量的时间性对象固有属性，适用于多种自然现象。</li>
<li>可实现从任何角度、在任何环境照明条件下、在对象的生命周期中的任何时间的可控渲染。</li>
<li>项目详细信息可通过访问网站<a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d%E8%8E%B7%E5%8F%96%E3%80%82">https://chen-geng.com/rose4d获取。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05278">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fb93f895cb6ff7e8514c5e822792bd92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22775306ecbddeb5796e1613f273cefb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37f99c754ec1c1067f9c5c05e9893b7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c768f3a252f92e750442256486eb0082.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Understanding-Memorization-in-Generative-Models-via-Sharpness-in-Probability-Landscapes"><a href="#Understanding-Memorization-in-Generative-Models-via-Sharpness-in-Probability-Landscapes" class="headerlink" title="Understanding Memorization in Generative Models via Sharpness in   Probability Landscapes"></a>Understanding Memorization in Generative Models via Sharpness in   Probability Landscapes</h2><p><strong>Authors:Dongjae Jeon, Dueun Kim, Albert No</strong></p>
<p>In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term. </p>
<blockquote>
<p>在这篇论文中，我们引入了一个几何框架，通过对数概率密度的尖锐程度来分析扩散模型中的记忆能力。我们通过证明其在量化尖锐度方面的有效性，为之前提出的基于分数差异的记忆力指标提供了数学论证。此外，我们提出了一个新的记忆力指标，该指标能够捕捉潜在扩散模型的图像生成初始阶段的尖锐度，为潜在的记忆力提供早期见解。借助这一指标，我们开发了一种缓解策略，通过采用感知尖锐度的正则化项来优化生成过程的初始噪声。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04140v3">PDF</a> Accepted at ICML 2025 (Spotlight)</p>
<p><strong>Summary</strong>：<br>本文引入了一个几何框架，通过概率密度对数尖锐度分析扩散模型中的记忆能力。文章证明了先前提出的基于评分差异的量化记忆能力指标的数学有效性，并提出了一种新的量化图像生成初期记忆能力的指标。此外，基于这一新指标，提出了一种优化生成过程初始噪声的缓解策略。此策略通过引入一个尖锐度感知正则化项来实现。</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>引入几何框架分析扩散模型中记忆能力的机制。</li>
<li>通过概率密度对数尖锐度来衡量扩散模型的记忆能力。</li>
<li>证明了基于评分差异的量化记忆能力指标的数学有效性。</li>
<li>提出一种新型量化指标来衡量图像生成初期阶段模型对信息的记忆能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04140">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-72fd8a6c12256159a3543db45931be41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2456f7bcce1b2687d2077f7b0e83408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f8d303bf54cf2d273a6f5b383176b78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e692c9963094f7eef5a04af0ffab94f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2145a309848de8395686f607d60345dc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>扩散模型作为强大的生成模型，在解决图像重建问题方面有着广泛的应用和巨大潜力。一些作品尝试使用扩散模型解决MRI重建问题，但这些方法直接在像素空间操作，导致优化和推理的计算成本较高。潜伏扩散模型预先在自然图像上进行训练，具有丰富的视觉先验，有望通过在低维潜伏空间操作来解决MRI重建中的高计算成本问题。然而，直接应用于MRI重建面临三个关键挑战：(1)缺乏医疗准确性的明确控制机制；(2)自然图像与MR物理之间的领域差距；(3)潜伏空间中的数据一致性未定义。为了解决这些挑战，提出了一种新型的基于潜伏扩散先验的欠采样MRI重建（LDPM）方法。我们的LDPM框架通过以下方式应对这些挑战：(1)一个草图引导管道，采用两步重建策略，平衡感知质量和解剖准确性；(2)一个优化的MRI变分自编码器（MR-VAE），与SD-VAE相比，在欠采样MRI重建的PSNR中提高了约3.92 dB \cite{sd}；(3)双阶段采样器，是间隔DDPM采样器的改进版，强制潜伏空间的高保真重建。在fastMRI数据集\cite{fastmri}上的实验证明了该方法的最新性能以及在各种场景中的稳健性。各模块的有效性也通过消融实验得到了验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v3">PDF</a> accepted as oral presentation at EMBC 2025</p>
<p><strong>摘要</strong></p>
<p>扩散模型作为强大的生成模型，在图像重建问题中展现出了巨大的潜力，并得到了广泛的应用。尽管已有工作尝试用扩散模型解决MRI重建问题，但这些方法在像素空间直接操作，导致优化和推理的计算成本较高。预期潜伏扩散模型（在天然图像上预训练，具有丰富的视觉先验）能够在MRI重建中解决高计算成本问题，通过在低维潜伏空间操作。然而，直接应用于MRI重建面临三个关键挑战：（1）医学保真度的缺乏明确控制机制，（2）自然图像与MR物理之间的领域差距，以及（3）潜伏空间中未定义的数据一致性。为解决这些挑战，提出了一种新型的基于潜伏扩散先验的欠采样MRI重建（LDPM）方法。我们的LDPM框架通过以下方式解决这些挑战：（1）带有两步重建策略的草图引导管道，平衡感知质量和解剖保真度，（2）针对MRI优化的VAE（MR-VAE），与SD-VAE相比，在欠采样MRI重建的PSNR上提高了约3.92 dB \cite{sd}，以及（3）双阶段采样器，这是间隔DDPM采样器的改进版本，在潜伏空间中强制高保真重建。在fastMRI数据集上的实验\cite{fastmri}证明了该方法的先进性能及其在多种场景中的稳健性。通过消融实验也验证了每个模块的有效性。</p>
<p><strong>要点掌握</strong></p>
<ol>
<li>扩散模型在图像重建中展现出了巨大的潜力。</li>
<li>直接在像素空间操作导致MRI重建的计算成本较高。</li>
<li>潜伏扩散模型期望通过低维潜伏空间操作来解决高计算成本问题。</li>
<li>应用于MRI重建面临三大挑战：医学保真度的控制、领域差距和数据一致性问题。</li>
<li>LDPM方法通过草图引导管道、MRI优化的VAE和双阶段采样器来解决这些挑战。</li>
<li>LDPM在fastMRI数据集上的实验表现先进，且在多种场景中表现稳健。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-59c1a174dbd1f401ef1c046c5604925b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Flexiffusion-Segment-wise-Neural-Architecture-Search-for-Flexible-Denoising-Schedule"><a href="#Flexiffusion-Segment-wise-Neural-Architecture-Search-for-Flexible-Denoising-Schedule" class="headerlink" title="Flexiffusion: Segment-wise Neural Architecture Search for Flexible   Denoising Schedule"></a>Flexiffusion: Segment-wise Neural Architecture Search for Flexible   Denoising Schedule</h2><p><strong>Authors:Hongtao Huang, Xiaojun Chang, Lina Yao</strong></p>
<p>Diffusion models are cutting-edge generative models adept at producing diverse, high-quality images. Despite their effectiveness, these models often require significant computational resources owing to their numerous sequential denoising steps and the significant inference cost of each step. Recently, Neural Architecture Search (NAS) techniques have been employed to automatically search for faster generation processes. However, NAS for diffusion is inherently time-consuming as it requires estimating thousands of diffusion models to search for the optimal one. In this paper, we introduce Flexiffusion, a novel training-free NAS paradigm designed to accelerate diffusion models by concurrently optimizing generation steps and network structures. Specifically, we partition the generation process into isometric step segments, each sequentially composed of a full step, multiple partial steps, and several null steps. The full step computes all network blocks, while the partial step involves part of the blocks, and the null step entails no computation. Flexiffusion autonomously explores flexible step combinations for each segment, substantially reducing search costs and enabling greater acceleration compared to the state-of-the-art (SOTA) method for diffusion models. Our searched models reported speedup factors of $2.6\times$ and $1.5\times$ for the original LDM-4-G and the SOTA, respectively. The factors for Stable Diffusion V1.5 and the SOTA are $5.1\times$ and $2.0\times$. We also verified the performance of Flexiffusion on multiple datasets, and positive experiment results indicate that Flexiffusion can effectively reduce redundancy in diffusion models. </p>
<blockquote>
<p>扩散模型是前沿的生成模型，擅长生成多样化、高质量的图片。尽管这些模型非常有效，但由于其众多的连续去噪步骤和每个步骤的重大推理成本，它们通常需要大量的计算资源。最近，神经网络架构搜索（NAS）技术已被用于自动寻找更快的生成过程。然而，对扩散的NAS本质上是耗时的，因为它需要评估成千上万的扩散模型来寻找最佳模型。在本文中，我们介绍了Flexiffusion，这是一种新的无需训练的NAS范式，旨在通过同时优化生成步骤和网络结构来加速扩散模型。具体来说，我们将生成过程划分为等距的步骤段，每个段由完整的步骤、多个部分步骤和几个空步骤顺序组成。完整步骤计算所有网络块，部分步骤涉及部分块，而空步骤则不涉及计算。Flexiffusion自主地探索每个段的灵活步骤组合，大大降低了搜索成本，与扩散模型的最新方法相比，实现了更大的加速。我们搜索的模型报告了原始LDM-4-G和最新方法的加速倍数分别为2.6倍和1.5倍。对于Stable Diffusion V1.5和最新方法，加速倍数分别为5.1倍和2.0倍。我们还验证了Flexiffusion在多个数据集上的性能，积极的实验结果证明Flexiffusion可以有效地减少扩散模型中的冗余。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型是擅长生成多样、高质量图像的先进生成模型。然而，由于其众多的连续去噪步骤和每个步骤的高推理成本，这些模型通常需要大量的计算资源。最近，神经网络架构搜索（NAS）技术被用来自动寻找更快的生成过程。然而，对扩散的NAS本质上是耗时的，因为它需要评估成千上万的扩散模型来寻找最优模型。本文介绍了Flexiffusion，一种全新的无需训练的NAS范式，旨在通过同时优化生成步骤和网络结构来加速扩散模型。Flexiffusion自主探索每个段的灵活步骤组合，大大降低了搜索成本，与最先进的扩散模型相比实现了更大的加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够生成多样、高质量图像，但需要大量计算资源。</li>
<li>神经网络架构搜索（NAS）技术被用于自动寻找更快的扩散模型生成过程。</li>
<li>Flexiffusion是一种全新的无需训练的NAS方法，旨在加速扩散模型。</li>
<li>Flexiffusion通过优化生成步骤和网络结构来加速扩散模型。</li>
<li>Flexiffusion自主探索每个生成步骤段的灵活组合，降低搜索成本。</li>
<li>Flexiffusion实现的速度提升因素显著，例如对原始LDM-4-G和先进方法的加速分别为$2.6\times$和$1.5\times$。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4aa4d3422220603dc4ac4375c9c727b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de39a47b02a85cdc9b447eab01fffbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b68315dcc2c3727810895cf0147017f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c95a9c01cdc3431813de60abeb6b40c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46c2d8d7843ec9854580ae7a210443f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-16f5c5f351a9a28f3ab2c432f175292a.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-10  Integrating Complexity and Biological Realism High-Performance Spiking   Neural Networks for Breast Cancer Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3882ab7f62c5a0c00d382b681b2933a6.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-06-10  Dy3DGS-SLAM Monocular 3D Gaussian Splatting SLAM for Dynamic   Environments
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27768.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
