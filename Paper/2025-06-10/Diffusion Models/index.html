<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  STARFlow Scaling Latent Normalizing Flows for High-resolution Image   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fdef7621a4b0601a72c49b8fe875c298.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-10-æ›´æ–°"><a href="#2025-06-10-æ›´æ–°" class="headerlink" title="2025-06-10 æ›´æ–°"></a>2025-06-10 æ›´æ–°</h1><h2 id="STARFlow-Scaling-Latent-Normalizing-Flows-for-High-resolution-Image-Synthesis"><a href="#STARFlow-Scaling-Latent-Normalizing-Flows-for-High-resolution-Image-Synthesis" class="headerlink" title="STARFlow: Scaling Latent Normalizing Flows for High-resolution Image   Synthesis"></a>STARFlow: Scaling Latent Normalizing Flows for High-resolution Image   Synthesis</h2><p><strong>Authors:Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, Shuangfei Zhai</strong></p>
<p>We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. The core of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers. We first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce several key architectural and algorithmic innovations to significantly enhance scalability: (1) a deep-shallow design, wherein a deep Transformer block captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial; (2) modeling in the latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling; and (3) a novel guidance algorithm that significantly boosts sample quality. Crucially, our model remains an end-to-end normalizing flow, enabling exact maximum likelihood training in continuous spaces without discretization. STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality. To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†STARFlowï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè§„èŒƒåŒ–æµçš„å¯æ‰©å±•ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­å®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚STARFlowçš„æ ¸å¿ƒæ˜¯Transformerè‡ªå›å½’æµï¼ˆTARFlowï¼‰ï¼Œå®ƒç»“åˆäº†è§„èŒƒåŒ–æµçš„è¡¨è¾¾èƒ½åŠ›ä¸è‡ªå›å½’å˜å‹å™¨çš„ç»“æ„åŒ–å»ºæ¨¡èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†TARFlowå¯¹è¿ç»­åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡çš„ç†è®ºæ™®éæ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†å‡ é¡¹å…³é”®çš„æ¶æ„å’Œç®—æ³•åˆ›æ–°ï¼Œä»¥æ˜¾è‘—å¢å¼ºå¯æ‰©å±•æ€§ï¼š(1)æ·±æµ…è®¾è®¡ï¼Œå…¶ä¸­æ·±Transformerå—æ•è·æ¨¡å‹çš„å¤§éƒ¨åˆ†è¡¨ç¤ºèƒ½åŠ›ï¼Œè¾…ä»¥å‡ ä¸ªè®¡ç®—æ•ˆç‡é«˜ä½†å¯¹æ¨¡å‹å¤§æœ‰è£¨ç›Šçš„æµ…å±‚Transformerå—ï¼›(2)åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»ºæ¨¡ï¼Œè¿™è¢«è¯æ˜æ¯”ç›´æ¥åƒç´ çº§å»ºæ¨¡æ›´ä¸ºæœ‰æ•ˆï¼›(3)ä¸€ç§æ–°å‹æŒ‡å¯¼ç®—æ³•ï¼Œå¯å¤§å¹…æé«˜æ ·æœ¬è´¨é‡ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è§„èŒƒåŒ–æµï¼Œèƒ½å¤Ÿåœ¨è¿ç»­ç©ºé—´ä¸­è¿›è¡Œç²¾ç¡®çš„æœ€å¤§å¯èƒ½æ€§è®­ç»ƒï¼Œæ— éœ€ç¦»æ•£åŒ–ã€‚STARFlowåœ¨ç±»æ¡ä»¶å›¾åƒç”Ÿæˆå’Œä»»åŠ¡æ¡ä»¶æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­å®ç°äº†æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œæ¥è¿‘æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œæ˜¯é¦–æ¬¡æˆåŠŸå±•ç¤ºäº†åœ¨è¿™ç§è§„æ¨¡å’Œåˆ†è¾¨ç‡ä¸‹æœ‰æ•ˆè¿è¡Œçš„è§„èŒƒåŒ–æµã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06276v1">PDF</a> TLDR: We show for the first time that normalizing flows can be scaled   for high-resolution and text-conditioned image synthesis</p>
<p><strong>Summary</strong></p>
<p>STARFlowæ˜¯ä¸€ä¸ªåŸºäºæ ‡å‡†åŒ–æµçš„å¯æ‰©å±•ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚å…¶æ ¸å¿ƒæ˜¯Transformerè‡ªå›å½’æµï¼ˆTARFlowï¼‰ï¼Œç»“åˆäº†æ ‡å‡†åŒ–æµçš„è¡¨è¾¾åŠ›å’Œè‡ªå›å½’å˜å‹å™¨çš„ç»“æ„åŒ–å»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ·±åº¦æµ…å±‚è®¾è®¡ã€åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´è¿›è¡Œå»ºæ¨¡ä»¥åŠæ–°å‹æŒ‡å¯¼ç®—æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº†å…¶å¯æ‰©å±•æ€§ã€‚æ¨¡å‹åœ¨ç±»æ¡ä»¶å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ¥è¿‘æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>STARFlowæ˜¯ä¸€ä¸ªåŸºäºæ ‡å‡†åŒ–æµçš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆã€‚</li>
<li>æ ¸å¿ƒæ˜¯Transformerè‡ªå›å½’æµï¼ˆTARFlowï¼‰ï¼Œç»“åˆäº†æ ‡å‡†åŒ–æµå’Œè‡ªå›å½’å˜å‹å™¨çš„ä¼˜ç‚¹ã€‚</li>
<li>TARFlowçš„ç†è®ºæ™®éæ€§è¢«ç”¨æ¥å¯¹è¿ç»­åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>å¼•å…¥æ·±åº¦æµ…å±‚è®¾è®¡ä»¥æé«˜æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´è¿›è¡Œå»ºæ¨¡ï¼Œè¯æ˜å…¶æ¯”ç›´æ¥åƒç´ çº§å»ºæ¨¡æ›´æœ‰æ•ˆã€‚</li>
<li>æ–°å‹æŒ‡å¯¼ç®—æ³•æ˜¾è‘—æé«˜æ ·æœ¬è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ca70f10410bb81f97a96205eb98c0a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-914729d7a64cff8e3f4dec00ea0ff105.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a13187e2812f4da476cdebe90930b555.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3e3efebf3239180d34d0e966527b5a3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Feedback-Guidance-of-Diffusion-Models"><a href="#Feedback-Guidance-of-Diffusion-Models" class="headerlink" title="Feedback Guidance of Diffusion Models"></a>Feedback Guidance of Diffusion Models</h2><p><strong>Authors:Koulischer Felix, Handke Florian, Deleu Johannes, Demeester Thomas, Ambrogioni Luca</strong></p>
<p>While Classifier-Free Guidance (CFG) has become standard for improving sample fidelity in conditional diffusion models, it can harm diversity and induce memorization by applying constant guidance regardless of whether a particular sample needs correction. We propose FeedBack Guidance (FBG), which uses a state-dependent coefficient to self-regulate guidance amounts based on need. Our approach is derived from first principles by assuming the learned conditional distribution is linearly corrupted by the unconditional distribution, contrasting with CFGâ€™s implicit multiplicative assumption. Our scheme relies on feedback of its own predictions about the conditional signal informativeness to adapt guidance dynamically during inference, challenging the view of guidance as a fixed hyperparameter. The approach is benchmarked on ImageNet512x512, where it significantly outperforms Classifier-Free Guidance and is competitive to Limited Interval Guidance (LIG) while benefitting from a strong mathematical framework. On Text-To-Image generation, we demonstrate that, as anticipated, our approach automatically applies higher guidance scales for complex prompts than for simpler ones and that it can be easily combined with existing guidance schemes such as CFG or LIG. </p>
<blockquote>
<p>è™½ç„¶æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰å·²æˆä¸ºæé«˜æ¡ä»¶æ‰©æ•£æ¨¡å‹æ ·æœ¬ä¿çœŸåº¦çš„æ ‡å‡†æ–¹æ³•ï¼Œä½†å®ƒå¯èƒ½ä¼šæŸå®³å¤šæ ·æ€§å¹¶å¯¼è‡´è®°å¿†ï¼Œå› ä¸ºå®ƒä¼šåº”ç”¨å›ºå®šçš„æŒ‡å¯¼æ–¹å¼ï¼Œæ— è®ºç‰¹å®šçš„æ ·æœ¬æ˜¯å¦éœ€è¦ä¿®æ­£ã€‚æˆ‘ä»¬æå‡ºäº†åé¦ˆå¼•å¯¼ï¼ˆFBGï¼‰æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªçŠ¶æ€ä¾èµ–ç³»æ•°æ¥æ ¹æ®éœ€æ±‚è‡ªæˆ‘è°ƒèŠ‚å¼•å¯¼é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºåŸºæœ¬åŸç†ï¼Œå‡è®¾å­¦ä¹ çš„æ¡ä»¶åˆ†å¸ƒè¢«æ— æ¡ä»¶åˆ†å¸ƒçº¿æ€§ç ´åï¼Œè¿™ä¸CFGçš„éšå«ä¹˜æ³•å‡è®¾å½¢æˆå¯¹æ¯”ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆä¾èµ–äºå¯¹å…¶è‡ªèº«å…³äºæ¡ä»¶ä¿¡å·ä¿¡æ¯é¢„æµ‹çš„åé¦ˆï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é€‚åº”æŒ‡å¯¼ï¼ŒæŒ‘æˆ˜äº†å°†æŒ‡å¯¼è§†ä¸ºå›ºå®šè¶…å‚æ•°çš„è§‚ç‚¹ã€‚è¯¥æ–¹æ³•åœ¨ImageNet512x512ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ˜¾è‘—ä¼˜äºæ— åˆ†ç±»å™¨å¼•å¯¼ä¸”å…·å¤‡ç«äº‰åŠ›å¹¶ä¸æœ‰é™é—´éš”å¼•å¯¼ï¼ˆLIGï¼‰å…·æœ‰ç›¸å½“æ°´å¹³çš„è¡¨ç°åŒæ—¶å¾—ç›Šäºå¼ºå¤§çš„æ•°å­¦æ¡†æ¶æ”¯æŒã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ä¸ºå¤æ‚æç¤ºåº”ç”¨æ›´é«˜çš„æŒ‡å¯¼æ¯”ä¾‹è€Œä¸æ˜¯ç®€å•çš„æç¤ºï¼Œå¹¶ä¸”å®ƒå¯ä»¥å¾ˆå®¹æ˜“åœ°ä¸ç°æœ‰çš„å¼•å¯¼æ–¹æ¡ˆå¦‚CFGæˆ–LIGç»“åˆä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06085v1">PDF</a> Preprint. Article currently under review. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/FelixKoulischer/FBG_using_edm2">https://github.com/FelixKoulischer/FBG_using_edm2</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„åé¦ˆæŒ‡å¯¼ï¼ˆFBGï¼‰è¢«æå‡ºä»¥æé«˜æ ·æœ¬çš„ä¿çœŸåº¦å¹¶è‡ªé€‚åº”åœ°è°ƒæ•´æŒ‡å¯¼é‡ã€‚ç›¸è¾ƒäºClassifier-Free Guidanceï¼ˆCFGï¼‰çš„æ’å®šæŒ‡å¯¼ï¼ŒFBGèƒ½å¤Ÿæ ¹æ®æ ·æœ¬éœ€æ±‚è¿›è¡Œè‡ªæˆ‘è°ƒèŠ‚ï¼Œé¿å…æŸå®³å¤šæ ·æ€§å’Œäº§ç”Ÿè®°å¿†æ•ˆåº”ã€‚FBGåŸºäºé¢„æµ‹çš„æ¡ä»¶ä¿¡å·ä¿¡æ¯é‡çš„åé¦ˆæ¥åŠ¨æ€è°ƒæ•´æŒ‡å¯¼ï¼ŒæŒ‘æˆ˜äº†å°†æŒ‡å¯¼è§†ä¸ºå›ºå®šè¶…å‚æ•°çš„è§‚ç‚¹ã€‚åœ¨ImageNetå’Œæ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒFBGæ˜¾è‘—ä¼˜äºCFGï¼Œå¹¶ä¸Limited Interval Guidanceï¼ˆLIGï¼‰å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒFBGèƒ½å¤Ÿè½»æ¾ç»“åˆç°æœ‰æŒ‡å¯¼æ–¹æ¡ˆå¦‚CFGæˆ–LIGã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FBGæå‡ºä¸€ç§åŸºäºåé¦ˆçš„æ‰©æ•£æ¨¡å‹æŒ‡å¯¼æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´æŒ‡å¯¼é‡ä»¥æé«˜æ ·æœ¬ä¿çœŸåº¦ã€‚</li>
<li>FBGé€šè¿‡é¢„æµ‹çš„æ¡ä»¶ä¿¡å·ä¿¡æ¯é‡çš„åé¦ˆè¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼Œå…‹æœäº†æ’å®šæŒ‡å¯¼å¯èƒ½å¸¦æ¥çš„é—®é¢˜ã€‚</li>
<li>FBGä¸Classifier-Free Guidanceï¼ˆCFGï¼‰ç›¸æ¯”ï¼Œèƒ½æ›´å¥½åœ°é€‚åº”ä¸åŒæ ·æœ¬çš„éœ€æ±‚ï¼Œé¿å…æŸå®³å¤šæ ·æ€§å’Œäº§ç”Ÿè®°å¿†æ•ˆåº”ã€‚</li>
<li>FBGåœ¨ImageNetä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºCFGï¼Œä¸Limited Interval Guidanceï¼ˆLIGï¼‰ç›¸å½“å¹¶å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>FBGé€‚ç”¨äºæ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œèƒ½è‡ªåŠ¨ä¸ºå¤æ‚æç¤ºåº”ç”¨æ›´é«˜çš„æŒ‡å¯¼è§„æ¨¡ã€‚</li>
<li>FBGèƒ½å¤Ÿè½»æ¾ç»“åˆå…¶ä»–ç°æœ‰æŒ‡å¯¼æ–¹æ¡ˆï¼Œå¦‚CFGæˆ–LIGã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e6e9a24a14ede56ecaff2ea6c80dc313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d5f90e95dbf3cce057d275124aeeeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-788dfc0d069434d76a2ecc9f2319bd8c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c363b6a8eab76fe55fe6a52693ce03b0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Optimization-Free-Universal-Watermark-Forgery-with-Regenerative-Diffusion-Models"><a href="#Optimization-Free-Universal-Watermark-Forgery-with-Regenerative-Diffusion-Models" class="headerlink" title="Optimization-Free Universal Watermark Forgery with Regenerative   Diffusion Models"></a>Optimization-Free Universal Watermark Forgery with Regenerative   Diffusion Models</h2><p><strong>Authors:Chaoyi Zhu, Zaitang Li, Renyi Yang, Robert Birke, Pin-Yu Chen, Tsung-Yi Ho, Lydia Y. Chen</strong></p>
<p>Watermarking becomes one of the pivotal solutions to trace and verify the origin of synthetic images generated by artificial intelligence models, but it is not free of risks. Recent studies demonstrate the capability to forge watermarks from a target image onto cover images via adversarial optimization without knowledge of the target generative model and watermark schemes. In this paper, we uncover a greater risk of an optimization-free and universal watermark forgery that harnesses existing regenerative diffusion models. Our proposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and integrates the target watermark via regenerating the image, without needing any additional optimization routine. It allows for universal watermark forgery that works independently of the target imageâ€™s origin or the watermarking model used. We explore the watermarked latent extracted from the target image and visual-textual context of cover images as priors to guide sampling of the regenerative process. Extensive evaluation on 24 scenarios of model-data-watermark combinations demonstrates that PnP can successfully forge the watermark (up to 100% detectability and user attribution), and maintain the best visual perception. By bypassing model retraining and enabling adaptability to any image, our approach significantly broadens the scope of forgery attacks, presenting a greater challenge to the security of current watermarking techniques for diffusion models and the authority of watermarking schemes in synthetic data generation and governance. </p>
<blockquote>
<p>æ°´å°å·²æˆä¸ºè¿½è¸ªå’ŒéªŒè¯äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒæ¥æºçš„å…³é”®è§£å†³æ–¹æ¡ˆä¹‹ä¸€ï¼Œä½†å¹¶éæ²¡æœ‰é£é™©ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡å¯¹æŠ—æ€§ä¼˜åŒ–å°†ç›®æ ‡å›¾åƒçš„æ°´å°ä¼ªé€ åˆ°è¦†ç›–å›¾åƒä¸Šï¼Œè€Œæ— éœ€äº†è§£ç›®æ ‡ç”Ÿæˆæ¨¡å‹å’Œæ°´å°æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åˆ©ç”¨ç°æœ‰å†ç”Ÿæ‰©æ•£æ¨¡å‹è¿›è¡Œæ— ä¼˜åŒ–é€šç”¨æ°´å°ä¼ªé€ çš„æœ€å¤§é£é™©ã€‚æˆ‘ä»¬æå‡ºçš„ä¼ªé€ æ”»å‡»æ–¹æ³•PnPï¼ˆå³â€œå³æ’å³ç”¨â€ï¼‰é€šè¿‡å†ç”Ÿå›¾åƒæ— ç¼æå–å’Œé›†æˆç›®æ ‡æ°´å°ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„ä¼˜åŒ–æµç¨‹ã€‚å®ƒå…è®¸è¿›è¡Œé€šç”¨æ°´å°ä¼ªé€ ï¼Œç‹¬ç«‹äºç›®æ ‡å›¾åƒæ¥æºæˆ–æ°´å°æ¨¡å‹çš„ä½¿ç”¨æƒ…å†µã€‚æˆ‘ä»¬æ¢ç´¢ä»ç›®æ ‡å›¾åƒä¸­æå–çš„æ°´å°æ½œåŠ›å’Œè¦†ç›–å›¾åƒçš„è§†è§‰æ–‡æœ¬ä¸Šä¸‹æ–‡ä½œä¸ºå…ˆéªŒæ¥æŒ‡å¯¼å†ç”Ÿè¿‡ç¨‹çš„é‡‡æ ·ã€‚å¯¹24ç§æ¨¡å‹-æ•°æ®-æ°´å°ç»„åˆåœºæ™¯çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPnPå¯ä»¥æˆåŠŸä¼ªé€ æ°´å°ï¼ˆé«˜è¾¾100%çš„å¯æ£€æµ‹æ€§å’Œç”¨æˆ·å½’å±ï¼‰ï¼Œå¹¶ä¿æŒæœ€ä½³çš„è§†è§‰æ„ŸçŸ¥ã€‚é€šè¿‡ç»•è¿‡æ¨¡å‹é‡æ–°è®­ç»ƒå¹¶é€‚åº”ä»»ä½•å›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æ‰©å¤§äº†ä¼ªé€ æ”»å‡»çš„èŒƒå›´ï¼Œç»™å½“å‰æ°´å°æŠ€æœ¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„å®‰å…¨æ€§ä»¥åŠæ°´å°æ–¹æ¡ˆåœ¨åˆæˆæ•°æ®ç”Ÿæˆå’Œæ²»ç†ä¸­çš„æƒå¨æ€§å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ°´å°å·²æˆä¸ºè¿½è¸ªå’ŒéªŒè¯äººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒæ¥æºçš„å…³é”®è§£å†³æ–¹æ¡ˆä¹‹ä¸€ï¼Œä½†å¹¶éæ²¡æœ‰é£é™©ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥é€šè¿‡å¯¹æŠ—æ€§ä¼˜åŒ–å°†ç›®æ ‡æ°´å°ä¼ªé€ åˆ°è¦†ç›–å›¾åƒä¸Šï¼Œè€Œæ— éœ€äº†è§£ç›®æ ‡ç”Ÿæˆæ¨¡å‹å’Œæ°´å°æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åˆ©ç”¨ç°æœ‰å†ç”Ÿæ‰©æ•£æ¨¡å‹è¿›è¡Œæ— ä¼˜åŒ–å’Œé€šç”¨æ°´å°ä¼ªé€ çš„æ›´å¤§é£é™©ã€‚æˆ‘ä»¬æå‡ºçš„ä¼ªé€ æ”»å‡»PnPï¼ˆå³æ’å³ç”¨ï¼‰æ–¹æ³•ï¼Œé€šè¿‡å†ç”Ÿå›¾åƒæ— ç¼æå–å’Œé›†æˆç›®æ ‡æ°´å°ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„ä¼˜åŒ–æµç¨‹ã€‚å®ƒå…è®¸è¿›è¡Œç‹¬ç«‹äºç›®æ ‡å›¾åƒæ¥æºæˆ–æ°´å°æ¨¡å‹ä½¿ç”¨çš„é€šç”¨æ°´å°ä¼ªé€ ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä»ç›®æ ‡å›¾åƒä¸­æå–çš„æ°´å°æ½œä¼æœŸå’Œè¦†ç›–å›¾åƒçš„è§†è§‰æ–‡æœ¬ä¸Šä¸‹æ–‡ä½œä¸ºå…ˆéªŒï¼Œä»¥æŒ‡å¯¼å†ç”Ÿè¿‡ç¨‹çš„é‡‡æ ·ã€‚åœ¨24ç§æ¨¡å‹-æ•°æ®-æ°´å°ç»„åˆåœºæ™¯ä¸‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPnPå¯ä»¥æˆåŠŸä¼ªé€ æ°´å°ï¼ˆé«˜è¾¾100%çš„å¯æ£€æµ‹æ€§å’Œç”¨æˆ·å½’å±ï¼‰ï¼Œå¹¶ä¿æŒè‰¯å¥½çš„è§†è§‰æ„ŸçŸ¥ã€‚é€šè¿‡ç»•è¿‡æ¨¡å‹é‡æ–°è®­ç»ƒå¹¶é€‚åº”ä»»ä½•å›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§æ‰©å¤§äº†ä¼ªé€ æ”»å‡»çš„èŒƒå›´ï¼Œç»™æ‰©æ•£æ¨¡å‹çš„æ°´å°æŠ€æœ¯å®‰å…¨æ€§å’Œåˆæˆæ•°æ®ç”Ÿæˆã€æ²»ç†çš„æ°´å°æ–¹æ¡ˆæƒå¨æ€§å¸¦æ¥äº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´å°æ˜¯è¿½è¸ªå’ŒéªŒè¯AIç”Ÿæˆå›¾åƒæ¥æºçš„å…³é”®ï¼Œä½†å­˜åœ¨ä¼ªé€ é£é™©ã€‚</li>
<li>æœ€è¿‘ç ”ç©¶å±•ç¤ºäº†æ— éœ€äº†è§£ç›®æ ‡ç”Ÿæˆæ¨¡å‹å’Œæ°´å°æ–¹æ¡ˆçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¯¹æŠ—æ€§ä¼˜åŒ–ä¼ªé€ æ°´å°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¼ªé€ æ”»å‡»æ–¹æ³•PnPï¼Œèƒ½å¤Ÿæ— ç¼æå–å¹¶é›†æˆç›®æ ‡æ°´å°ï¼Œé€šè¿‡å†ç”Ÿå›¾åƒè¿›è¡Œã€‚</li>
<li>PnPæ–¹æ³•å…è®¸è¿›è¡Œé€šç”¨æ°´å°ä¼ªé€ ï¼Œç‹¬ç«‹äºç›®æ ‡å›¾åƒæ¥æºå’Œæ°´å°æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨ç›®æ ‡å›¾åƒçš„æ°´å°æ½œä¼æœŸå’Œè¦†ç›–å›¾åƒçš„è§†è§‰æ–‡æœ¬ä¸Šä¸‹æ–‡æ¥æé«˜ä¼ªé€ æ•ˆæœã€‚</li>
<li>å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒPnPæ–¹æ³•æˆåŠŸä¼ªé€ æ°´å°å¹¶ä¿æŒè‰¯å¥½è§†è§‰æ„ŸçŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a9812ac9f09cbc397d546ca80edd2ff8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b06987c1583f5abf701aaf6e79f762cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f47c1b90ddfcee85e0ae6da4458d8de7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="FADE-Frequency-Aware-Diffusion-Model-Factorization-for-Video-Editing"><a href="#FADE-Frequency-Aware-Diffusion-Model-Factorization-for-Video-Editing" class="headerlink" title="FADE: Frequency-Aware Diffusion Model Factorization for Video Editing"></a>FADE: Frequency-Aware Diffusion Model Factorization for Video Editing</h2><p><strong>Authors:Yixuan Zhu, Haolin Wang, Shilin Ma, Wenliang Zhao, Yansong Tang, Lei Chen, Jie Zhou</strong></p>
<p>Recent advancements in diffusion frameworks have significantly enhanced video editing, achieving high fidelity and strong alignment with textual prompts. However, conventional approaches using image diffusion models fall short in handling video dynamics, particularly for challenging temporal edits like motion adjustments. While current video diffusion models produce high-quality results, adapting them for efficient editing remains difficult due to the heavy computational demands that prevent the direct application of previous image editing techniques. To overcome these limitations, we introduce FADE, a training-free yet highly effective video editing approach that fully leverages the inherent priors from pre-trained video diffusion models via frequency-aware factorization. Rather than simply using these models, we first analyze the attention patterns within the video model to reveal how video priors are distributed across different components. Building on these insights, we propose a factorization strategy to optimize each componentâ€™s specialized role. Furthermore, we devise spectrum-guided modulation to refine the sampling trajectory with frequency domain cues, preventing information leakage and supporting efficient, versatile edits while preserving the basic spatial and temporal structure. Extensive experiments on real-world videos demonstrate that our method consistently delivers high-quality, realistic and temporally coherent editing results both qualitatively and quantitatively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/EternalEvan/FADE">https://github.com/EternalEvan/FADE</a> . </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¡†æ¶çš„è¿›å±•åœ¨è§†é¢‘ç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå®ç°äº†é«˜ä¿çœŸåº¦ä»¥åŠä¸æ–‡æœ¬æç¤ºçš„å¼ºçƒˆå¯¹é½ã€‚ç„¶è€Œï¼Œä½¿ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¼ ç»Ÿæ–¹æ³•åœ¨åº”å¯¹è§†é¢‘åŠ¨æ€æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œè°ƒæ•´ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„æ—¶é—´ç¼–è¾‘æ–¹é¢ã€‚è™½ç„¶å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½äº§ç”Ÿé«˜è´¨é‡çš„ç»“æœï¼Œä½†å°†å…¶é€‚åº”äºé«˜æ•ˆç¼–è¾‘ä»ç„¶å¾ˆå›°éš¾ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè¿™ä½¿å¾—æ— æ³•ç›´æ¥åº”ç”¨ä¹‹å‰çš„å›¾åƒç¼–è¾‘æŠ€æœ¯ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†FADEï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒä½†é«˜æ•ˆçš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå®ƒå……åˆ†åˆ©ç”¨äº†é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å†…åœ¨å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡é¢‘ç‡æ„ŸçŸ¥åˆ†è§£ã€‚æˆ‘ä»¬ä¸æ˜¯ç®€å•åœ°ä½¿ç”¨è¿™äº›æ¨¡å‹ï¼Œè€Œæ˜¯é¦–å…ˆåˆ†æè§†é¢‘æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œä»¥æ­ç¤ºè§†é¢‘å…ˆéªŒçŸ¥è¯†æ˜¯å¦‚ä½•åˆ†å¸ƒåœ¨ä¸åŒçš„ç»„ä»¶ä¸­çš„ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†è§£ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–æ¯ä¸ªç»„ä»¶çš„ä¸“é—¨ä½œç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†é¢‘è°±å¼•å¯¼è°ƒåˆ¶ï¼Œä»¥åˆ©ç”¨é¢‘ç‡åŸŸçº¿ç´¢æ¥å®Œå–„é‡‡æ ·è½¨è¿¹ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼Œå¹¶æ”¯æŒé«˜æ•ˆã€å¤šåŠŸèƒ½ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒåŸºæœ¬çš„ç©ºé—´å’Œæ—¶é—´ç»“æ„ã€‚åœ¨çœŸå®ä¸–ç•Œè§†é¢‘ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡ä¸Šéƒ½èƒ½æä¾›é«˜è´¨é‡ã€ç°å®æ„Ÿå¼ºä¸”æ—¶é—´è¿è´¯çš„ç¼–è¾‘ç»“æœã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EternalEvan/FADE%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EternalEvan/FADEä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05934v1">PDF</a> Accepted by IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR) 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ‰©æ•£æ¡†æ¶çš„è¿›å±•æå¤§æå‡äº†è§†é¢‘ç¼–è¾‘æ•ˆæœï¼Œå®ç°äº†é«˜ä¿çœŸå’Œä¸æ–‡æœ¬æç¤ºçš„å¼ºå¯¹é½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿå›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†è§†é¢‘åŠ¨æ€æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°¤å…¶åœ¨è¿åŠ¨è°ƒæ•´ç­‰æ—¶é—´ç¼–è¾‘æ–¹é¢æ›´å…·æŒ‘æˆ˜ã€‚å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡ç»“æœï¼Œä½†ç”±äºè®¡ç®—éœ€æ±‚å·¨å¤§ï¼Œéš¾ä»¥é«˜æ•ˆç¼–è¾‘ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºFADEï¼Œä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å†…åœ¨å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡é¢‘ç‡æ„ŸçŸ¥åˆ†è§£å®ç°ã€‚æˆ‘ä»¬åˆ†æè§†é¢‘æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œæ­ç¤ºè§†é¢‘å…ˆéªŒçŸ¥è¯†åœ¨ä¸åŒç»„ä»¶ä¸­çš„åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºä¼˜åŒ–å„ç»„ä»¶ä¸“ä¸šè§’è‰²çš„åˆ†è§£ç­–ç•¥ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘é¢‘è°±å¼•å¯¼è°ƒåˆ¶ï¼Œåˆ©ç”¨é¢‘ç‡åŸŸçº¿ç´¢ä¼˜åŒ–é‡‡æ ·è½¨è¿¹ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²ï¼Œæ”¯æŒé«˜æ•ˆã€å¤šæ ·åŒ–çš„ç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒåŸºæœ¬çš„æ—¶é—´å’Œç©ºé—´ç»“æ„ã€‚åœ¨çœŸå®è§†é¢‘ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ— è®ºåœ¨å®šæ€§è¿˜æ˜¯å®šé‡ä¸Šéƒ½èƒ½æä¾›é«˜è´¨é‡ã€é€¼çœŸçš„æ—¶é—´è¿è´¯ç¼–è¾‘ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¡†æ¶çš„æœ€æ–°è¿›å±•æ˜¾è‘—æé«˜äº†è§†é¢‘ç¼–è¾‘çš„è´¨é‡å’Œå¯¹æ–‡æœ¬æç¤ºçš„å“åº”åº¦ã€‚</li>
<li>ä¼ ç»Ÿå›¾åƒæ‰©æ•£æ¨¡å‹åœ¨å¤„ç†è§†é¢‘åŠ¨æ€æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„è¿åŠ¨ç¼–è¾‘æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è§†é¢‘æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆé«˜è´¨é‡ç»“æœï¼Œä½†éš¾ä»¥é«˜æ•ˆç¼–è¾‘ï¼Œè®¡ç®—éœ€æ±‚å¤§ã€‚</li>
<li>FADEæ–¹æ³•æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„å†…åœ¨å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>FADEé€šè¿‡åˆ†æè§†é¢‘æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›æ¨¡å¼æ¥æ­ç¤ºè§†é¢‘å…ˆéªŒçŸ¥è¯†çš„åˆ†å¸ƒã€‚</li>
<li>FADEé€šè¿‡é¢‘ç‡æ„ŸçŸ¥åˆ†è§£å’Œé¢‘è°±å¼•å¯¼è°ƒåˆ¶æŠ€æœ¯å®ç°é«˜è´¨é‡çš„è§†é¢‘ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee46da539fd149708a51be012d2a70d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9daa2e66ca08afbffd29071c324b28f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cce361e7fbb4cbe061fb9c9affc6af18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a33bd4a11613ef8daeda75d40465814.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Stealix-Model-Stealing-via-Prompt-Evolution"><a href="#Stealix-Model-Stealing-via-Prompt-Evolution" class="headerlink" title="Stealix: Model Stealing via Prompt Evolution"></a>Stealix: Model Stealing via Prompt Evolution</h2><p><strong>Authors:Zhixiong Zhuang, Hui-Po Wang, Maria-Irina Nicolae, Mario Fritz</strong></p>
<p>Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information. Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise. To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names. In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim modelâ€™s data distribution, and iteratively refines prompts through a genetic algorithm, progressively improving the precision and diversity of synthetic images. Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized. </p>
<blockquote>
<p>æ¨¡å‹çªƒå–å¯¹æœºå™¨å­¦ä¹ çš„å®‰å…¨æ„æˆé‡å¤§å¨èƒã€‚æ”»å‡»è€…èƒ½å¤Ÿæ— éœ€æ¥è§¦ç›®æ ‡æ¨¡å‹å³å¯å¤ç°é»‘ç›’æ¨¡å‹ï¼Œå¯¼è‡´çŸ¥è¯†äº§æƒå—æŸä¸”æš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚è¿‘æœŸä½¿ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®åˆæˆçš„æ–¹æ³•æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ï¼Œä½†ä¸¥é‡ä¾èµ–äººå·¥è®¾è®¡çš„æç¤ºï¼Œé™åˆ¶äº†è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œå°¤å…¶ä¸åˆ©äºç¼ºä¹ä¸“ä¸šçŸ¥è¯†çš„æ”»å‡»è€…ã€‚ä¸ºäº†è¯„ä¼°å¼€æºé¢„è®­ç»ƒæ¨¡å‹å¸¦æ¥çš„é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´ç°å®çš„å¨èƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ¶ˆé™¤äº†å¯¹æç¤ºè®¾è®¡æŠ€èƒ½æˆ–ç±»åçŸ¥è¯†çš„éœ€æ±‚ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Stealixï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢„è®¾æç¤ºå³å¯æ‰§è¡Œæ¨¡å‹çªƒå–çš„é¦–ä¸ªæ–¹æ³•ã€‚Stealixä½¿ç”¨ä¸¤ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹æ¥æ¨æ–­ç›®æ ‡æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒï¼Œå¹¶é€šè¿‡é—ä¼ ç®—æ³•è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œé€æ­¥æ”¹è¿›åˆæˆå›¾åƒçš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨ç›¸åŒçš„æŸ¥è¯¢é¢„ç®—ä¸‹ï¼ŒStealixä¹Ÿæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç”šè‡³ä¼˜äºé‚£äº›èƒ½å¤Ÿè®¿é—®ç±»åæˆ–ç²¾ç»†æç¤ºçš„æ–¹æ³•ã€‚è¿™äº›å‘ç°çªæ˜¾äº†æˆ‘ä»¬æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œå¹¶è¡¨æ˜é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹åœ¨æ¨¡å‹çªƒå–æ–¹é¢çš„é£é™©å¯èƒ½æ¯”ä»¥å‰è®¤è¯†åˆ°çš„æ›´å¤§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05867v1">PDF</a> Accepted at ICML 2025. The project page is at   <a target="_blank" rel="noopener" href="https://zhixiongzh.github.io/stealix/">https://zhixiongzh.github.io/stealix/</a></p>
<p><strong>Summary</strong></p>
<p>æ¨¡å‹çªƒå–å¯¹æœºå™¨å­¦ä¹ æ„æˆé‡å¤§å®‰å…¨é£é™©ï¼Œæ”»å‡»è€…å¯ä»¥åœ¨æ— éœ€æ¥è§¦è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹å¤åˆ¶é»‘ç›’æ¨¡å‹ï¼Œä»è€Œå±åŠçŸ¥è¯†äº§æƒå’Œæš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚å°½ç®¡åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®åˆæˆçš„æ–¹æ³•æé«˜äº†æ•ˆç‡å’Œæ€§èƒ½ï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–æ‰‹åŠ¨è®¾è®¡çš„æç¤ºï¼Œé™åˆ¶äº†è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•æ€§ï¼Œå°¤å…¶æ˜¯å¯¹é‚£äº›ç¼ºä¹ä¸“ä¸šçŸ¥è¯†çš„æ”»å‡»è€…ã€‚ä¸ºäº†è¯„ä¼°å¼€æºé¢„è®­ç»ƒæ¨¡å‹çš„é£é™©ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´ç°å®çš„å¨èƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ— éœ€æç¤ºè®¾è®¡æŠ€èƒ½æˆ–äº†è§£ç±»åå³å¯è¿è¡Œã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Stealixï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€é¢„å…ˆè®¾å®šæç¤ºå³å¯æ‰§è¡Œæ¨¡å‹çªƒå–çš„é¦–åˆ›æ–¹æ³•ã€‚Stealixä½¿ç”¨ä¸¤ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹æ¥æ¨æ–­å—å®³è€…æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒï¼Œå¹¶é€šè¿‡é—ä¼ ç®—æ³•è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œé€æ­¥æé«˜åˆæˆå›¾åƒçš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStealixæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå³ä½¿åœ¨ç›¸åŒçš„æŸ¥è¯¢é¢„ç®—ä¸‹ï¼Œå³ä½¿é‚£äº›èƒ½å¤Ÿè®¿é—®ç±»åæˆ–ç²¾ç»†æç¤ºçš„æ–¹æ³•ä¹Ÿè¢«å…¶è¶…è¶Šã€‚è¿™å¼ºè°ƒäº†æˆ‘ä»¬çš„æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œå¹¶è¡¨æ˜é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹åœ¨æ¨¡å‹çªƒå–æ–¹é¢çš„é£é™©å¯èƒ½æ¯”å…ˆå‰è®¤è¯†çš„è¦å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹çªƒå–å¯¹æœºå™¨å­¦ä¹ æ„æˆå®‰å…¨é£é™©ï¼Œæ”»å‡»è€…å¯å¤åˆ¶é»‘ç›’æ¨¡å‹ï¼Œå±åŠçŸ¥è¯†äº§æƒå’Œæš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®åˆæˆçš„æ–¹æ³•è™½æé«˜æ•ˆç‡å’Œæ€§èƒ½ï¼Œä½†ä¾èµ–æ‰‹åŠ¨æç¤ºï¼Œé™åˆ¶è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ›´ç°å®çš„å¨èƒæ¨¡å‹ï¼Œæ— éœ€æç¤ºè®¾è®¡æŠ€èƒ½æˆ–äº†è§£ç±»åå³å¯æ‰§è¡Œæ¨¡å‹çªƒå–ã€‚</li>
<li>æ¨å‡ºäº†Stealixæ–¹æ³•ï¼Œæ— éœ€é¢„è®¾æç¤ºå³å¯è¿›è¡Œæ¨¡å‹çªƒå–ã€‚</li>
<li>Stealixä½¿ç”¨ä¸¤ä¸ªå¼€æºé¢„è®­ç»ƒæ¨¡å‹æ¨æ–­å—å®³è€…æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>é€šè¿‡é—ä¼ ç®—æ³•è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œæé«˜åˆæˆå›¾åƒçš„ç²¾åº¦å’Œå¤šæ ·æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05867">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10ee92c6ccdd7f428068a093b2b15d7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e817da870e814597adda0d1e39b13a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-562e39916dd3996e67fb971b26775bbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-309a83cf82806e8c5a5ad57b91cb70d8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLIA-â€“-Enabling-Low-Latency-Interactive-Avatars-Real-Time-Audio-Driven-Portrait-Video-Generation-with-Diffusion-Models"><a href="#LLIA-â€“-Enabling-Low-Latency-Interactive-Avatars-Real-Time-Audio-Driven-Portrait-Video-Generation-with-Diffusion-Models" class="headerlink" title="LLIA â€“ Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven   Portrait Video Generation with Diffusion Models"></a>LLIA â€“ Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven   Portrait Video Generation with Diffusion Models</h2><p><strong>Authors:Haojie Yu, Zhaonian Wang, Yihan Pan, Meng Cheng, Hao Yang, Chao Wang, Tao Xie, Xiaoming Xu, Xiaoming Wei, Xunliang Cai</strong></p>
<p>Diffusion-based models have gained wide adoption in the virtual human generation due to their outstanding expressiveness. However, their substantial computational requirements have constrained their deployment in real-time interactive avatar applications, where stringent speed, latency, and duration requirements are paramount. We present a novel audio-driven portrait video generation framework based on the diffusion model to address these challenges. Firstly, we propose robust variable-length video generation to reduce the minimum time required to generate the initial video clip or state transitions, which significantly enhances the user experience. Secondly, we propose a consistency model training strategy for Audio-Image-to-Video to ensure real-time performance, enabling a fast few-step generation. Model quantization and pipeline parallelism are further employed to accelerate the inference speed. To mitigate the stability loss incurred by the diffusion process and model quantization, we introduce a new inference strategy tailored for long-duration video generation. These methods ensure real-time performance and low latency while maintaining high-fidelity output. Thirdly, we incorporate class labels as a conditional input to seamlessly switch between speaking, listening, and idle states. Lastly, we design a novel mechanism for fine-grained facial expression control to exploit our modelâ€™s inherent capacity. Extensive experiments demonstrate that our approach achieves low-latency, fluid, and authentic two-way communication. On an NVIDIA RTX 4090D, our model achieves a maximum of 78 FPS at a resolution of 384x384 and 45 FPS at a resolution of 512x512, with an initial video generation latency of 140 ms and 215 ms, respectively. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•åœ¨è™šæ‹Ÿäººç”Ÿæˆé¢†åŸŸå› å…¶å‡ºè‰²çš„è¡¨ç°åŠ›è€Œå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œå…¶å·¨å¤§çš„è®¡ç®—éœ€æ±‚é™åˆ¶äº†å…¶åœ¨å®æ—¶äº¤äº’å¼åŒ–èº«åº”ç”¨ç¨‹åºä¸­çš„éƒ¨ç½²ï¼Œè¿™äº›åº”ç”¨ç¨‹åºå¯¹é€Ÿåº¦ã€å»¶è¿Ÿå’ŒæŒç»­æ—¶é—´æœ‰ä¸¥æ ¼çš„è¦æ±‚ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºå¯å˜é•¿åº¦è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œä»¥å‡å°‘ç”Ÿæˆåˆå§‹è§†é¢‘ç‰‡æ®µæˆ–çŠ¶æ€è½¬æ¢æ‰€éœ€çš„æœ€çŸ­æ—¶é—´ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºç”¨æˆ·ä½“éªŒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ºéŸ³é¢‘å›¾åƒåˆ°è§†é¢‘çš„è½¬æ¢æå‡ºäº†ä¸€è‡´çš„æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¡®ä¿å®æ—¶æ€§èƒ½ï¼Œå®ç°å¿«é€Ÿå¤šæ­¥ç”Ÿæˆã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†æ¨¡å‹é‡åŒ–å’Œæµæ°´çº¿å¹¶è¡ŒåŒ–æ–¹æ³•æ¥åŠ é€Ÿæ¨ç†é€Ÿåº¦ã€‚ä¸ºäº†è§£å†³æ‰©æ•£è¿‡ç¨‹å’Œæ¨¡å‹é‡åŒ–å¸¦æ¥çš„ç¨³å®šæ€§æŸå¤±ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€‚ç”¨äºé•¿æŒç»­æ—¶é—´è§†é¢‘ç”Ÿæˆçš„æ–°å‹æ¨ç†ç­–ç•¥ã€‚è¿™äº›æ–¹æ³•ç¡®ä¿äº†å®æ—¶æ€§èƒ½å’Œä½å»¶è¿Ÿï¼ŒåŒæ—¶ä¿æŒäº†é«˜ä¿çœŸè¾“å‡ºã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬å°†ç±»åˆ«æ ‡ç­¾ä½œä¸ºæ¡ä»¶è¾“å…¥èå…¥å…¶ä¸­ï¼Œä»¥æ— ç¼åˆ‡æ¢è¯´è¯ã€å€¾å¬å’Œç©ºé—²çŠ¶æ€ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”¨äºç²¾ç»†é¢éƒ¨è¡¨æƒ…æ§åˆ¶çš„æ–°æœºåˆ¶ï¼Œä»¥åˆ©ç”¨æˆ‘ä»¬æ¨¡å‹çš„å†…åœ¨èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ä½å»¶è¿Ÿã€æµç•…å’Œé€¼çœŸçš„åŒå‘é€šä¿¡ã€‚åœ¨NVIDIA RTX 4090Dä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨384x384åˆ†è¾¨ç‡ä¸‹æœ€é«˜è¾¾åˆ°78å¸§æ¯ç§’ï¼ˆFPSï¼‰ï¼Œåœ¨512x512åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°45 FPSï¼Œåˆå§‹è§†é¢‘ç”Ÿæˆå»¶è¿Ÿåˆ†åˆ«ä¸º140æ¯«ç§’å’Œ215æ¯«ç§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05806v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ¡†æ¶è§£å†³äº†ç°æœ‰æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å®æ—¶äº’åŠ¨åŒ–èº«åº”ç”¨çš„ä¸¥æ ¼é€Ÿåº¦ã€å»¶è¿Ÿå’ŒæŒç»­æ—¶é—´è¦æ±‚ã€‚æå‡ºå¯å˜é•¿åº¦è§†é¢‘ç”Ÿæˆã€ä¸€è‡´æ€§æ¨¡å‹è®­ç»ƒç­–ç•¥ã€æ¨¡å‹é‡åŒ–å’Œç®¡é“å¹¶è¡ŒåŒ–ç­‰æ–¹æ³•ï¼Œä»¥æé«˜ç”¨æˆ·ä½“éªŒã€ä¿è¯å®æ—¶æ€§èƒ½å’Œä½å»¶è¿Ÿï¼ŒåŒæ—¶ç»´æŒé«˜ä¿çœŸè¾“å‡ºã€‚æ­¤å¤–ï¼Œçº³å…¥ç±»åˆ«æ ‡ç­¾ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå®ç°è¯´è¯ã€è†å¬å’Œç©ºé—²çŠ¶æ€é—´çš„æ— ç¼åˆ‡æ¢ï¼Œå¹¶è®¾è®¡ç²¾ç»†é¢éƒ¨è¡¨æƒ…æ§åˆ¶æœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å®ç°ä½å»¶è¿Ÿã€æµç•…å’ŒçœŸå®çš„åŒå‘äº¤æµã€‚åœ¨NVIDIA RTX 4090Dä¸Šï¼Œæ¨¡å‹æœ€é«˜è¾¾78å¸§&#x2F;ç§’ï¼ˆFPSï¼‰çš„å¸§ç‡ï¼Œåˆ†è¾¨ç‡åˆ†åˆ«ä¸º384x384å’Œ45 FPSåœ¨åˆ†è¾¨ç‡ä¸º512x512ã€‚åˆå§‹è§†é¢‘ç”Ÿæˆå»¶è¿Ÿåˆ†åˆ«ä¸º140æ¯«ç§’å’Œ215æ¯«ç§’ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿäººç”Ÿæˆä¸­çš„å‡ºè‰²è¡¨è¾¾åŠ›ã€‚</li>
<li>æå‡ºçš„éŸ³é¢‘é©±åŠ¨è‚–åƒè§†é¢‘ç”Ÿæˆæ¡†æ¶è§£å†³äº†å®æ—¶äº’åŠ¨åŒ–èº«åº”ç”¨çš„æŒ‘æˆ˜ã€‚</li>
<li>å˜é‡é•¿åº¦è§†é¢‘ç”Ÿæˆæé«˜äº†ç”¨æˆ·ä½“éªŒã€‚</li>
<li>ä¸€è‡´æ€§æ¨¡å‹è®­ç»ƒç­–ç•¥ç¡®ä¿äº†å®æ—¶æ€§èƒ½ï¼Œå®ç°äº†å¿«é€Ÿå‡ æ­¥ç”Ÿæˆã€‚</li>
<li>æ¨¡å‹é‡åŒ–å’Œç®¡é“å¹¶è¡ŒåŒ–æŠ€æœ¯åŠ é€Ÿäº†æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æ–°çš„æ¨ç†ç­–ç•¥æé«˜äº†é•¿æ—¶é•¿è§†é¢‘ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d6076a44b790d08c6a4cdedfbbf3341.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d07c6a332c72395c8c36293ba950bd1f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="FocusDiff-Advancing-Fine-Grained-Text-Image-Alignment-for-Autoregressive-Visual-Generation-through-RL"><a href="#FocusDiff-Advancing-Fine-Grained-Text-Image-Alignment-for-Autoregressive-Visual-Generation-through-RL" class="headerlink" title="FocusDiff: Advancing Fine-Grained Text-Image Alignment for   Autoregressive Visual Generation through RL"></a>FocusDiff: Advancing Fine-Grained Text-Image Alignment for   Autoregressive Visual Generation through RL</h2><p><strong>Authors:Kaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Recent studies extend the autoregression paradigm to text-to-image generation, achieving performance comparable to diffusion models. However, our new PairComp benchmark â€“ featuring test cases of paired prompts with similar syntax but different fine-grained semantics â€“ reveals that existing models struggle with fine-grained text-image alignment thus failing to realize precise control over visual tokens. To address this, we propose FocusDiff, which enhances fine-grained text-image semantic alignment by focusing on subtle differences between similar text-image pairs. We construct a new dataset of paired texts and images with similar overall expressions but distinct local semantics, further introducing a novel reinforcement learning algorithm to emphasize such fine-grained semantic differences for desired image generation. Our approach achieves state-of-the-art performance on existing text-to-image benchmarks and significantly outperforms prior methods on PairComp. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å°†è‡ªå›å½’èŒƒå¼æ‰©å±•åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œå…¶æ€§èƒ½ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ–°çš„PairCompåŸºå‡†æµ‹è¯•â€”â€”åŒ…å«å…·æœ‰ç›¸ä¼¼è¯­æ³•ä½†å…·æœ‰ä¸åŒç»†å¾®è¯­ä¹‰çš„é…å¯¹æç¤ºæµ‹è¯•ç”¨ä¾‹â€”â€”è¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨ç»†å¾®çš„æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— æ³•å®ç°è§†è§‰æ ‡è®°çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FocusDiffï¼Œå®ƒé€šè¿‡å…³æ³¨ç›¸ä¼¼æ–‡æœ¬å›¾åƒå¯¹ä¹‹é—´çš„ç»†å¾®å·®å¼‚ï¼Œæé«˜äº†æ–‡æœ¬å›¾åƒçš„ç²¾ç»†è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°çš„é…å¯¹æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ï¼Œå…·æœ‰ç›¸ä¼¼çš„æ•´ä½“è¡¨è¾¾ä½†å±€éƒ¨è¯­ä¹‰ä¸åŒï¼Œå¹¶è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥å¼ºè°ƒè¿™ç§ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ï¼Œä»¥å®ç°æ‰€éœ€çš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨PairCompä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05501v1">PDF</a> 15 pages, 8 figures. Project Page: <a target="_blank" rel="noopener" href="https://focusdiff.github.io/">https://focusdiff.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶å°†è‡ªå›å½’èŒƒå¼æ‰©å±•åˆ°æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸï¼Œæ€§èƒ½ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚ç„¶è€Œï¼Œæ–°çš„PairCompåŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨ç²¾ç»†æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— æ³•å®ç°è§†è§‰ç¬¦å·çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºFocusDiffï¼Œé€šè¿‡å…³æ³¨ç›¸ä¼¼æ–‡æœ¬å›¾åƒå¯¹ä¹‹é—´çš„ç»†å¾®å·®å¼‚ï¼Œæé«˜ç²¾ç»†æ–‡æœ¬å›¾åƒè¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†æ–°çš„é…å¯¹æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ï¼Œæ•´ä½“è¡¨è¾¾ç›¸ä¼¼ä½†å±€éƒ¨è¯­ä¹‰ä¸åŒï¼Œå¹¶å¼•å…¥æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥å¼ºè°ƒè¿™ç§ç²¾ç»†è¯­ä¹‰å·®å¼‚ä»¥å®ç°æ‰€éœ€çš„å›¾åƒç”Ÿæˆã€‚FocusDiffåœ¨ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œå¹¶åœ¨PairCompä¸Šæ˜¾è‘—ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸç ”ç©¶å°†è‡ªå›å½’èŒƒå¼åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆï¼Œæ€§èƒ½ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ç²¾ç»†æ–‡æœ¬å›¾åƒå¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>PairCompåŸºå‡†æµ‹è¯•æ­ç¤ºäº†è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼ºè°ƒå¯¹è§†è§‰ç¬¦å·çš„ç²¾ç¡®æ§åˆ¶çš„é‡è¦æ€§ã€‚</li>
<li>FocusDiffé€šè¿‡å…³æ³¨ç»†å¾®å·®å¼‚æé«˜ç²¾ç»†æ–‡æœ¬å›¾åƒè¯­ä¹‰å¯¹é½ã€‚</li>
<li>FocusDiffæ„å»ºäº†æ–°çš„é…å¯¹æ–‡æœ¬å’Œå›¾åƒæ•°æ®é›†ï¼Œæ•´ä½“è¡¨è¾¾ç›¸ä¼¼ä½†å±€éƒ¨è¯­ä¹‰ä¸åŒã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ç®—æ³•è¢«ç”¨äºFocusDiffï¼Œä»¥å¼ºè°ƒç²¾ç»†è¯­ä¹‰å·®å¼‚å®ç°æ‰€éœ€å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13a1040f0fa0913245e08a4dcfe8602c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72eb9f96c74f459adce38dd969cdb0b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea69ce3433f61c8c19286bf7b985839f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-668c520498981fa1c3d5219b83100283.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83c685c14eb0d04edbef449491094a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d898e9e177f88855dc061ea84a6bb3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations"><a href="#Towards-Reliable-Identification-of-Diffusion-based-Image-Manipulations" class="headerlink" title="Towards Reliable Identification of Diffusion-based Image Manipulations"></a>Towards Reliable Identification of Diffusion-based Image Manipulations</h2><p><strong>Authors:Alex Costanzino, Woody Bayliss, Juil Sock, Marc Gorriz Blanch, Danijela Horak, Ivan Laptev, Philip Torr, Fabio Pizzati</strong></p>
<p>Changing facial expressions, gestures, or background details may dramatically alter the meaning conveyed by an image. Notably, recent advances in diffusion models greatly improve the quality of image manipulation while also opening the door to misuse. Identifying changes made to authentic images, thus, becomes an important task, constantly challenged by new diffusion-based editing tools. To this end, we propose a novel approach for ReliAble iDentification of inpainted AReas (RADAR). RADAR builds on existing foundation models and combines features from different image modalities. It also incorporates an auxiliary contrastive loss that helps to isolate manipulated image patches. We demonstrate these techniques to significantly improve both the accuracy of our method and its generalisation to a large number of diffusion models. To support realistic evaluation, we further introduce BBC-PAIR, a new comprehensive benchmark, with images tampered by 28 diffusion models. Our experiments show that RADAR achieves excellent results, outperforming the state-of-the-art in detecting and localising image edits made by both seen and unseen diffusion models. Our code, data and models will be publicly available at alex-costanzino.github.io&#x2F;radar. </p>
<blockquote>
<p>æ”¹å˜é¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿æˆ–èƒŒæ™¯ç»†èŠ‚å¯èƒ½ä¼šæå¤§åœ°æ”¹å˜å›¾åƒæ‰€ä¼ è¾¾çš„å«ä¹‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•åœ¨æå¤§åœ°æé«˜äº†å›¾åƒæ“ä½œè´¨é‡çš„åŒæ—¶ï¼Œä¹Ÿæ‰“å¼€äº†æ»¥ç”¨çš„å¤§é—¨ã€‚å› æ­¤ï¼Œè¯†åˆ«å¯¹çœŸå®å›¾åƒæ‰€åšçš„æ›´æ”¹æˆä¸ºäº†ä¸€é¡¹é‡è¦ä»»åŠ¡ï¼Œè¿™é¡¹ä»»åŠ¡ä¸æ–­å—åˆ°æ–°çš„åŸºäºæ‰©æ•£çš„ç¼–è¾‘å·¥å…·çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•â€”â€”å¯é æ ‡è¯†å›¾åƒå†…å¡«å……åŒºåŸŸï¼ˆRADARï¼‰ã€‚RADARå»ºç«‹åœ¨ç°æœ‰çš„åŸºç¡€æ¨¡å‹ä¸Šï¼Œç»“åˆäº†ä¸åŒå›¾åƒæ¨¡æ€çš„ç‰¹å¾ã€‚å®ƒè¿˜èå…¥äº†ä¸€ç§è¾…åŠ©å¯¹æ¯”æŸå¤±ï¼Œæœ‰åŠ©äºéš”ç¦»æ“çºµè¿‡çš„å›¾åƒåŒºå—ã€‚æˆ‘ä»¬å±•ç¤ºçš„è¿™äº›æŠ€æœ¯æå¤§åœ°æé«˜äº†æˆ‘ä»¬æ–¹æ³•çš„å‡†ç¡®æ€§åŠå…¶å¯¹å¤§é‡æ‰©æ•£æ¨¡å‹çš„æ¨å¹¿èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒç°å®è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†BBC-PAIRï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ç”±28ç§æ‰©æ•£æ¨¡å‹ç¯¡æ”¹è¿‡çš„å›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒRADARå–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œåœ¨æ£€æµ‹å’Œå®šä½å·²çŸ¥å’ŒæœªçŸ¥çš„æ‰©æ•£æ¨¡å‹æ‰€åšçš„å›¾åƒç¼–è¾‘æ–¹é¢è¶…è¿‡äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†åœ¨alex-costanzino.github.io&#x2F;radarä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05466v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒæ“ä½œçš„æ–°æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºRADARçš„æ–°æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å›¾åƒä¸­çš„ç¼–è¾‘åŒºåŸŸã€‚RADARç»“åˆä¸åŒå›¾åƒæ¨¡æ€çš„ç‰¹å¾ï¼Œå¹¶å¼•å…¥è¾…åŠ©å¯¹æ¯”æŸå¤±æ¥éš”ç¦»ç¼–è¾‘è¿‡çš„å›¾åƒåŒºåŸŸã€‚åŒæ—¶ï¼Œä¸ºäº†æ”¯æŒçœŸå®è¯„ä¼°ï¼Œè¿˜å¼•å…¥äº†BBC-PAIRæ–°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ç”±28ç§æ‰©æ•£æ¨¡å‹ä¿®æ”¹çš„å›¾åƒã€‚å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨æ£€æµ‹å’Œå®šä½ç”±å·²çŸ¥å’ŒæœªçŸ¥æ‰©æ•£æ¨¡å‹è¿›è¡Œçš„å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ”¹è¿›å›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œä¹Ÿå¸¦æ¥äº†æ–°çš„å›¾åƒæ“ä½œæ»¥ç”¨é—®é¢˜ã€‚</li>
<li>RADARæ–¹æ³•ç»“åˆäº†ä¸åŒå›¾åƒæ¨¡æ€çš„ç‰¹å¾ï¼Œä»¥è¯†åˆ«å›¾åƒä¸­çš„ç¼–è¾‘åŒºåŸŸã€‚</li>
<li>RADARé€šè¿‡å¼•å…¥è¾…åŠ©å¯¹æ¯”æŸå¤±ï¼Œæé«˜äº†æ£€æµ‹å›¾åƒç¼–è¾‘åŒºåŸŸçš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>BBC-PAIRæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæ”¯æŒå¯¹å›¾åƒæ“ä½œæ£€æµ‹æ–¹æ³•çš„çœŸå®è¯„ä¼°ã€‚</li>
<li>RADARåœ¨BBC-PAIRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¯¹å·²çŸ¥å’ŒæœªçŸ¥çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œçš„å›¾åƒç¼–è¾‘éƒ½æœ‰å¾ˆå¥½çš„æ£€æµ‹æ•ˆæœã€‚</li>
<li>RADARçš„æºä»£ç ã€æ•°æ®å’Œæ¨¡å‹å°†å…¬å¼€æä¾›ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05466">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69663f8357c48500036575130339d759.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af2166a234b6d1a78d8fac83caa42cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84359b69dca16787536bd8d3beb9726.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6eaf4c668fffd453db25aedf5883b7f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-Diffusion-Transformer-Designs-via-Grafting"><a href="#Exploring-Diffusion-Transformer-Designs-via-Grafting" class="headerlink" title="Exploring Diffusion Transformer Designs via Grafting"></a>Exploring Diffusion Transformer Designs via Grafting</h2><p><strong>Authors:Keshigeyan Chandrasegaran, Michael Poli, Daniel Y. Fu, Dongjun Kim, Lea M. Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, Stefano Ermon, Li Fei-Fei</strong></p>
<p>Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present grafting, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL&#x2F;2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for DiT-XL&#x2F;2) using &lt;2% pretraining compute. We then graft a text-to-image model (PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL&#x2F;2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2x and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: <a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu</a> </p>
<blockquote>
<p>è®¾è®¡æ¨¡å‹æ¶æ„éœ€è¦è¿›è¡Œè¯¸å¦‚é€‰æ‹©è¿ç®—ç¬¦ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›ã€å·ç§¯ï¼‰å’Œé…ç½®ï¼ˆä¾‹å¦‚æ·±åº¦ã€å®½åº¦ï¼‰ä¹‹ç±»çš„å†³ç­–ã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›å†³ç­–å¯¹æ¨¡å‹è´¨é‡çš„å½±å“éœ€è¦å¤§é‡çš„é¢„è®­ç»ƒæˆæœ¬ï¼Œè¿™é™åˆ¶äº†æ¶æ„çš„ç ”ç©¶ã€‚æˆ‘ä»¬å—å¯å‘äºå¦‚ä½•åœ¨ç°æœ‰ä»£ç ä¸Šæ„å»ºæ–°è½¯ä»¶ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“ï¼šå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥ç ”ç©¶æ–°çš„æ¶æ„è®¾è®¡å—ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å«æ¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ç¼–è¾‘é¢„è®­ç»ƒæ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰çš„æ–¹æ³•ï¼Œä»¥åœ¨è¾ƒå°çš„è®¡ç®—é¢„ç®—ä¸‹å®ç°æ–°çš„æ¶æ„ã€‚æ ¹æ®æˆ‘ä»¬å¯¹æ¿€æ´»è¡Œä¸ºå’Œæ³¨æ„åŠ›å±€éƒ¨æ€§çš„åˆ†æï¼Œæˆ‘ä»¬ä»¥DiT-XL&#x2F;2è®¾è®¡ä¸ºåŸºç¡€æ„å»ºäº†ä¸€ä¸ªæµ‹è¯•å¹³å°ï¼Œä»¥ç ”ç©¶å«æ¥å¯¹æ¨¡å‹è´¨é‡çš„å½±å“ã€‚ä½¿ç”¨è¿™ä¸ªæµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬é€šè¿‡å«æ¥å¼€å‘äº†ä¸€ç³»åˆ—æ··åˆè®¾è®¡ï¼šç”¨é—¨æ§å·ç§¯ã€å±€éƒ¨æ³¨æ„åŠ›å’Œçº¿æ€§æ³¨æ„åŠ›æ›¿æ¢softmaxæ³¨æ„åŠ›ï¼Œå¹¶ç”¨å¯å˜æ‰©å±•ç‡å’Œå·ç§¯å˜ä½“æ›¿æ¢MLPã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®¸å¤šæ··åˆè®¾è®¡åœ¨é¢„è®­ç»ƒè®¡ç®—é‡ä¸åˆ°2%çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨FIDï¼ˆ2.38-2.64ä¸DiT-XL&#x2F;2çš„2.27ï¼‰è¾¾åˆ°äº†è‰¯å¥½çš„è´¨é‡ã€‚ç„¶åæˆ‘ä»¬å°†ä¸€ä¸ªæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆPixArt-Sigmaï¼‰è¿›è¡Œå«æ¥ï¼Œå®ç°äº†1.43å€çš„åŠ é€Ÿï¼ŒåŒæ—¶GenEvalåˆ†æ•°ä¸‹é™ä¸åˆ°2%ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œé€šè¿‡å«æ¥é‡æ–°æ„å»ºDiT-XL&#x2F;2ï¼Œå°†æ¯ä¸€å¯¹è¿ç»­çš„å˜å‹å™¨å—è½¬æ¢ä¸ºå¹¶è¡Œå—ã€‚è¿™å‡å°‘äº†æ¨¡å‹æ·±åº¦çš„ä¸€åŠï¼Œå¹¶äº§ç”Ÿäº†æ¯”å…¶ä»–å…·æœ‰ç›¸ä¼¼æ·±åº¦çš„æ¨¡å‹æ›´å¥½çš„è´¨é‡ï¼ˆFIDï¼š2.77ï¼‰ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯ä»¥é€šè¿‡å«æ¥é¢„è®­ç»ƒçš„DiTsæ¥æ¢ç´¢æ–°çš„æ‰©æ•£æ¨¡å‹è®¾è®¡ï¼Œç¼–è¾‘èŒƒå›´ä»æ“ä½œç¬¦æ›¿æ¢åˆ°æ¶æ„é‡ç»„ã€‚ä»£ç å’Œå«æ¥æ¨¡å‹ï¼š<a target="_blank" rel="noopener" href="https://grafting.stanford.edu./">https://grafting.stanford.eduã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05340v2">PDF</a> 22 pages; Project website: <a target="_blank" rel="noopener" href="https://grafting.stanford.edu/">https://grafting.stanford.edu</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é€šè¿‡å€Ÿé‰´è½¯ä»¶å¼€å‘ä¸­çš„å¤ç”¨æ€æƒ³ï¼Œæå‡ºäº†æ‰©æ•£æ¨¡å‹æ¶æ„è®¾è®¡çš„æ–°æ–¹æ³•â€”â€”å«æ¥é¢„è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£è½¬æ¢å™¨è¿›è¡Œç®€å•ç¼–è¾‘ï¼Œå®ç°æ–°æ¶æ„åœ¨æœ‰é™è®¡ç®—é¢„ç®—ä¸‹çš„ç ”ç©¶ã€‚é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šç§»æ¤ä¸åŒçš„è®¾è®¡ï¼Œè¯æ˜äº†è¿™ç§æ–¹æ³•çš„é«˜æ•ˆæ€§ï¼Œåªéœ€è¾ƒå°‘çš„è®¡ç®—å°±èƒ½å¾—åˆ°è‰¯å¥½è´¨é‡çš„æ¨¡å‹è®¾è®¡ã€‚è¯¥ç ”ç©¶å‘ç°å·²åœ¨å®˜æ–¹ç½‘ç«™ä¸Šå…¬å¸ƒï¼Œä»¥æ¨è¿›ç ”ç©¶ã€‚ç®€å•è€Œè¨€ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ‰©æ•£æ¨¡å‹æ¶æ„è®¾è®¡çš„æ–°æ€è·¯ï¼Œä¸ºå¿«é€Ÿå¼€å‘æ–°æ¨¡å‹æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶è€…æå‡ºäº†é€šè¿‡å«æ¥é¢„è®­ç»ƒæ¨¡å‹æ¥è®¾è®¡æ–°çš„æ‰©æ•£æ¨¡å‹æ¶æ„çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç¼–è¾‘é¢„è®­ç»ƒçš„æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTsï¼‰ï¼Œå®ç°äº†åœ¨å°è®¡ç®—é¢„ç®—ä¸‹ç ”ç©¶æ–°æ¶æ„çš„ç›®çš„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d046a83c26f21ff929ad26e2576f1a9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89076046b741e0b58281d672d44906d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d46bc3806e04a5d5868564af91a233b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d3940bf603b77c759e8b1293fa88b839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b8d1b02559903edd1458b51dfa1a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb4c6c4074112e116ac2560fed9daca0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SeedEdit-3-0-Fast-and-High-Quality-Generative-Image-Editing"><a href="#SeedEdit-3-0-Fast-and-High-Quality-Generative-Image-Editing" class="headerlink" title="SeedEdit 3.0: Fast and High-Quality Generative Image Editing"></a>SeedEdit 3.0: Fast and High-Quality Generative Image Editing</h2><p><strong>Authors:Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang</strong></p>
<p>We introduce SeedEdit 3.0, in companion with our T2I model Seedream 3.0, which significantly improves over our previous SeedEdit versions in both aspects of edit instruction following and image content (e.g., ID&#x2F;IP) preservation on real image inputs. Additional to model upgrading with T2I, in this report, we present several key improvements. First, we develop an enhanced data curation pipeline with a meta-info paradigm and meta-info embedding strategy that help mix images from multiple data sources. This allows us to scale editing data effectively, and meta information is helpfult to connect VLM with diffusion model more closely. Second, we introduce a joint learning pipeline for computing a diffusion loss and reward losses. Finally, we evaluate SeedEdit 3.0 on our testing benchmarks, for real&#x2F;synthetic image editing, where it achieves a best trade-off between multiple aspects, yielding a high usability rate of 56.1%, compared to SeedEdit 1.6 (38.4%), GPT4o (37.1%) and Gemini 2.0 (30.3%). </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SeedEdit 3.0ç‰ˆæœ¬ï¼Œä¸æˆ‘ä»¬çš„T2Iæ¨¡å‹Seedream 3.0ä¸€åŒä»‹ç»ã€‚ç›¸è¾ƒäºæˆ‘ä»¬ä¹‹å‰çš„SeedEditç‰ˆæœ¬ï¼Œå®ƒåœ¨éµå¾ªç¼–è¾‘æŒ‡ä»¤å’Œä¿ç•™å›¾åƒå†…å®¹ï¼ˆä¾‹å¦‚ID&#x2F;IPï¼‰æ–¹é¢å¯¹çœŸå®å›¾åƒè¾“å…¥æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚é™¤äº†ä½¿ç”¨T2Iè¿›è¡Œæ¨¡å‹å‡çº§ï¼Œæœ¬æŠ¥å‘Šè¿˜å±•ç¤ºäº†è‹¥å¹²é¡¹é‡è¦æ”¹è¿›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¢å¼ºçš„æ•°æ®æ•´ç†ç®¡é“ï¼Œé‡‡ç”¨å…ƒä¿¡æ¯èŒƒå¼å’Œå…ƒä¿¡æ¯åµŒå…¥ç­–ç•¥ï¼Œæœ‰åŠ©äºæ··åˆæ¥è‡ªå¤šä¸ªæ•°æ®æºçš„å›¾ç‰‡ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°æ‰©å±•ç¼–è¾‘æ•°æ®ï¼Œè€Œå…ƒä¿¡æ¯æœ‰åŠ©äºæ›´ç´§å¯†åœ°å°†VLMä¸æ‰©æ•£æ¨¡å‹è¿æ¥èµ·æ¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè”åˆå­¦ä¹ ç®¡é“ï¼Œç”¨äºè®¡ç®—æ‰©æ•£æŸå¤±å’Œå¥–åŠ±æŸå¤±ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•åŸºå‡†ä¸Šå¯¹SeedEdit 3.0è¿›è¡Œäº†çœŸå®&#x2F;åˆæˆå›¾åƒç¼–è¾‘çš„è¯„ä¼°ï¼Œå®ƒåœ¨å¤šæ–¹é¢è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œä½¿ç”¨æ€§ç‡é«˜è¾¾56.1%ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒSeedEdit 1.6ä¸º38.4%ï¼ŒGPT4oä¸º37.1%ï¼ŒGemini 2.0ä¸º30.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05083v2">PDF</a> Website: <a target="_blank" rel="noopener" href="https://seed.bytedance.com/tech/seededit">https://seed.bytedance.com/tech/seededit</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–°æ¨å‡ºçš„SeedEdit 3.0ä¸å…¶é…å¥—çš„T2Iæ¨¡å‹Seedream 3.0ï¼Œç›¸è¾ƒäºä¹‹å‰çš„ç‰ˆæœ¬ï¼Œå®ƒåœ¨éµå¾ªç¼–è¾‘æŒ‡ä»¤å’Œä¿ç•™å›¾åƒå†…å®¹ï¼ˆå¦‚ID&#x2F;IPï¼‰æ–¹é¢æœ‰äº†æ˜¾è‘—æå‡ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å‡ ä¸ªå…³é”®æ”¹è¿›ï¼ŒåŒ…æ‹¬é‡‡ç”¨å…ƒä¿¡æ¯èŒƒå¼å’Œå…ƒä¿¡æ¯åµŒå…¥ç­–ç•¥å¢å¼ºæ•°æ®æ•´ç†æµç¨‹ã€å¼•å…¥è”åˆå­¦ä¹ ç®¡é“è®¡ç®—æ‰©æ•£æŸå¤±å’Œå¥–åŠ±æŸå¤±ï¼Œå¹¶åœ¨æµ‹è¯•åŸºå‡†ä¸Šè¯„ä¼°äº†SeedEdit 3.0åœ¨çœŸå®&#x2F;åˆæˆå›¾åƒç¼–è¾‘æ–¹é¢çš„è¡¨ç°ï¼Œå®ç°äº†å¤šæ–¹é¢çš„æœ€ä½³å¹³è¡¡ï¼Œä½¿ç”¨ç‡è¾¾åˆ°56.1%ï¼Œç›¸è¾ƒäºå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SeedEdit 3.0ä¸å…¶é…å¥—çš„T2Iæ¨¡å‹Seedream 3.0æ¨å‡ºï¼Œæ˜¾è‘—æå‡äº†ç¼–è¾‘æŒ‡ä»¤éµå¾ªå’Œå›¾åƒå†…å®¹ä¿ç•™çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨äº†å…ƒä¿¡æ¯èŒƒå¼å’Œå…ƒä¿¡æ¯åµŒå…¥ç­–ç•¥çš„å¢å¼ºæ•°æ®æ•´ç†æµç¨‹ï¼Œèƒ½æœ‰æ•ˆæ•´åˆå¤šæºå›¾åƒæ•°æ®ã€‚</li>
<li>å¼•å…¥äº†è”åˆå­¦ä¹ ç®¡é“ï¼Œè®¡ç®—æ‰©æ•£æŸå¤±å’Œå¥–åŠ±æŸå¤±ï¼Œä¿ƒè¿›äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</li>
<li>SeedEdit 3.0åœ¨çœŸå®&#x2F;åˆæˆå›¾åƒç¼–è¾‘æ–¹é¢çš„è¯„ä¼°è¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†å¤šæ–¹é¢çš„æœ€ä½³å¹³è¡¡ã€‚</li>
<li>SeedEdit 3.0çš„ä½¿ç”¨ç‡è¾¾åˆ°56.1%ï¼Œç›¸è¾ƒäºä¹‹å‰ç‰ˆæœ¬å’Œå…¶ä»–æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>æ¨¡å‹çš„å‡çº§å’Œæ”¹è¿›æœ‰åŠ©äºæå‡å›¾åƒç¼–è¾‘ä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05083">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e3f44fbe8067708c38f62b117d6dd8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fb4600dc35b860b01b8ad838e2f0ae0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2934c8258ac1cc2a447e3edeff776025.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7ef819710f5a2be0462d56c5b563e6cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1943abbace22e3888206647fb27b7fe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoders-Again"><a href="#Sparse-Autoencoders-Again" class="headerlink" title="Sparse Autoencoders, Again?"></a>Sparse Autoencoders, Again?</h2><p><strong>Authors:Yin Lu, Xuening Zhu, Tong He, David Wipf</strong></p>
<p>Is there really much more to say about sparse autoencoders (SAEs)? Autoencoders in general, and SAEs in particular, represent deep architectures that are capable of modeling low-dimensional latent structure in data. Such structure could reflect, among other things, correlation patterns in large language model activations, or complex natural image manifolds. And yet despite the wide-ranging applicability, there have been relatively few changes to SAEs beyond the original recipe from decades ago, namely, standard deep encoder&#x2F;decoder layers trained with a classical&#x2F;deterministic sparse regularizer applied within the latent space. One possible exception is the variational autoencoder (VAE), which adopts a stochastic encoder module capable of producing sparse representations when applied to manifold data. In this work we formalize underappreciated weaknesses with both canonical SAEs, as well as analogous VAEs applied to similar tasks, and propose a hybrid alternative model that circumvents these prior limitations. In terms of theoretical support, we prove that global minima of our proposed model recover certain forms of structured data spread across a union of manifolds. Meanwhile, empirical evaluations on synthetic and real-world datasets substantiate the efficacy of our approach in accurately estimating underlying manifold dimensions and producing sparser latent representations without compromising reconstruction error. In general, we are able to exceed the performance of equivalent-capacity SAEs and VAEs, as well as recent diffusion models where applicable, within domains such as images and language model activation patterns. </p>
<blockquote>
<p>å…³äºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰è¿˜æœ‰ä»€ä¹ˆæ›´å¤šå¯è¯´çš„å—ï¼Ÿæ€»çš„æ¥è¯´ï¼Œè‡ªç¼–ç å™¨ï¼Œå°¤å…¶æ˜¯SAEsï¼Œä»£è¡¨äº†èƒ½å¤Ÿå»ºæ¨¡æ•°æ®ä¸­çš„ä½ç»´æ½œåœ¨ç»“æ„çš„æ·±åº¦æ¶æ„ã€‚è¿™ç§ç»“æ„å¯èƒ½åæ˜ äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¿€æ´»ä¸­çš„ç›¸å…³æ€§æ¨¡å¼ï¼Œæˆ–å¤æ‚çš„è‡ªç„¶å›¾åƒæµå½¢ã€‚å°½ç®¡SAEså…·æœ‰å¹¿æ³›çš„åº”ç”¨èŒƒå›´ï¼Œä½†é™¤äº†å‡ åå¹´å‰çš„åŸå§‹é…æ–¹ä¹‹å¤–ï¼Œå‡ ä¹æ²¡æœ‰å¤ªå¤šå˜åŒ–ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç”¨ç»å…¸&#x2F;ç¡®å®šæ€§ç¨€ç–æ­£åˆ™åŒ–åœ¨æ½œåœ¨ç©ºé—´å†…è®­ç»ƒçš„æ·±åº¦ç¼–ç å™¨&#x2F;è§£ç å™¨å±‚ã€‚ä¸€ä¸ªå¯èƒ½çš„ä¾‹å¤–æ˜¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§éšæœºç¼–ç å™¨æ¨¡å—ï¼Œåœ¨åº”ç”¨äºæµå½¢æ•°æ®æ—¶èƒ½å¤Ÿäº§ç”Ÿç¨€ç–è¡¨ç¤ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­£å¼æå‡ºäº†è¢«å¿½è§†çš„å¼±ç‚¹ï¼Œæ— è®ºæ˜¯å…¸å‹çš„SAEsï¼Œè¿˜æ˜¯ç±»ä¼¼ä»»åŠ¡çš„VAEsï¼Œå¹¶æå‡ºäº†ä¸€ç§æ··åˆçš„æ›¿ä»£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ç»•è¿‡è¿™äº›å…ˆå‰çš„é™åˆ¶ã€‚åœ¨ç†è®ºæ”¯æŒæ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹å…¨å±€æœ€å°å€¼èƒ½å¤Ÿæ¢å¤è·¨å¤šä¸ªæµå½¢çš„æŸäº›å½¢å¼çš„ç»“æ„åŒ–æ•°æ®åˆ†å¸ƒã€‚åŒæ—¶ï¼Œåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®è¯è¯„ä¼°è¯å®äº†æˆ‘ä»¬æ–¹æ³•åœ¨å‡†ç¡®ä¼°è®¡æ½œåœ¨æµå½¢ç»´åº¦å’Œäº§ç”Ÿç¨€ç–æ½œåœ¨è¡¨ç¤ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè€Œä¸ä¼šæŸå®³é‡å»ºè¯¯å·®ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å›¾åƒå’Œè¯­è¨€æ¨¡å‹æ¿€æ´»æ¨¡å¼ç­‰é¢†åŸŸè¶…è¿‡äº†åŒç­‰å®¹é‡çš„SAEså’ŒVAEsçš„æ€§èƒ½è¡¨ç°ï¼Œä»¥åŠåœ¨é€‚ç”¨çš„æœ€æ–°æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04859v2">PDF</a> Accepted to the International Conference on Machine Learning (ICML)   2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ··åˆæ¨¡å‹æ¥å…‹æœè¿™äº›å±€é™ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡SAEåœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç»“æ„å’ŒæŠ€æœ¯è‡ªå‡ åå¹´å‰çš„åŸå§‹é…æ–¹ä»¥æ¥å¹¶æ²¡æœ‰å¤ªå¤§å˜åŒ–ã€‚æ–‡ç« è¿˜ä»‹ç»äº†å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰çš„æŸäº›ç‰¹æ€§ï¼Œå¹¶å±•ç¤ºäº†æ–°çš„æ··åˆæ¨¡å‹åœ¨ç†è®ºæ”¯æŒå’Œå®è¯ç ”ç©¶ä¸Šçš„ä¼˜åŠ¿ï¼Œä¾‹å¦‚åœ¨ä¼°è®¡åº•å±‚æµå½¢ç»´åº¦å’Œäº§ç”Ÿç¨€ç–æ½œåœ¨è¡¨ç¤ºæ–¹é¢çš„ä¼˜åŠ¿ã€‚æ€»ä½“è€Œè¨€ï¼Œæ–°æ¨¡å‹èƒ½åœ¨å›¾åƒå’Œè¯­è¨€æ¨¡å‹æ¿€æ´»æ¨¡å¼ç­‰é¢†åŸŸè¶…è¶Šç­‰æ•ˆå®¹é‡çš„SAEå’ŒVAEä»¥åŠæœ€è¿‘çš„æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ˜¯ä¸€ç§èƒ½å¤Ÿå»ºæ¨¡æ•°æ®ä½ç»´æ½œåœ¨ç»“æ„çš„æ·±åº¦æ¶æ„ã€‚</li>
<li>SAEåœ¨å¤šç§é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç»“æ„å’ŒæŠ€æœ¯è‡ªæå‡ºä»¥æ¥å˜åŒ–è¾ƒå°‘ã€‚</li>
<li>å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰é‡‡ç”¨éšæœºç¼–ç å™¨æ¨¡å—ï¼Œèƒ½åº”ç”¨äºæµå½¢æ•°æ®äº§ç”Ÿç¨€ç–è¡¨ç¤ºã€‚</li>
<li>æœ¬æ–‡æŒ‡å‡ºäº†SAEå’Œç±»ä¼¼ä»»åŠ¡çš„VAEçš„ä¸è¶³ä¹‹å¤„ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°å‹æ··åˆæ¨¡å‹æ¥å…‹æœè¿™äº›å±€é™ã€‚</li>
<li>æ–°æ¨¡å‹åœ¨ç†è®ºä¸Šæœ‰è¯æ˜ï¼Œèƒ½å¤Ÿæ¢å¤è·¨å¤šä¸ªæµå½¢çš„æŸäº›å½¢å¼çš„ç»“æ„åŒ–æ•°æ®ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ–°æ¨¡å‹åœ¨ä¼°è®¡åº•å±‚æµå½¢ç»´åº¦å’Œäº§ç”Ÿç¨€ç–æ½œåœ¨è¡¨ç¤ºæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¸”ä¸ä¼šå¢åŠ é‡å»ºè¯¯å·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04859">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9f6051c5d2c288ae2566b210f8184d14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2839c97697e1774bcba9aec0b0918924.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f04b18284bd0a7cfb8ce8dbf7d4b71ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04011231d485016c3e5bbd3ef9a608f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Smoothed-Preference-Optimization-via-ReNoise-Inversion-for-Aligning-Diffusion-Models-with-Varied-Human-Preferences"><a href="#Smoothed-Preference-Optimization-via-ReNoise-Inversion-for-Aligning-Diffusion-Models-with-Varied-Human-Preferences" class="headerlink" title="Smoothed Preference Optimization via ReNoise Inversion for Aligning   Diffusion Models with Varied Human Preferences"></a>Smoothed Preference Optimization via ReNoise Inversion for Aligning   Diffusion Models with Varied Human Preferences</h2><p><strong>Authors:Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, Min Zhang</strong></p>
<p>Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation models with human preferences using pairwise preference data. Although substantial resources are expended in collecting and labeling datasets, a critical aspect is often neglected: \textit{preferences vary across individuals and should be represented with more granularity.} To address this, we propose SmPO-Diffusion, a novel method for modeling preference distributions to improve the DPO objective, along with a numerical upper bound estimation for the diffusion optimization objective. First, we introduce a smoothed preference distribution to replace the original binary distribution. We employ a reward model to simulate human preferences and apply preference likelihood averaging to improve the DPO loss, such that the loss function approaches zero when preferences are similar. Furthermore, we utilize an inversion technique to simulate the trajectory preference distribution of the diffusion model, enabling more accurate alignment with the optimization objective. Our approach effectively mitigates issues of excessive optimization and objective misalignment present in existing methods through straightforward modifications. Our SmPO-Diffusion achieves state-of-the-art performance in preference evaluation, outperforming baselines across metrics with lower training costs. The project page is <a target="_blank" rel="noopener" href="https://jaydenlyh.github.io/SmPO-project-page/">https://jaydenlyh.github.io/SmPO-project-page/</a>. </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½¿ç”¨æˆå¯¹åå¥½æ•°æ®å°†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è™½ç„¶æ”¶é›†å’Œæ ‡æ³¨æ•°æ®é›†éœ€è¦è€—è´¹å¤§é‡èµ„æºï¼Œä½†å¾€å¾€å¿½ç•¥äº†ä¸€ä¸ªå…³é”®æ–¹é¢ï¼š*ä¸åŒä¸ªä½“çš„åå¥½ä¸åŒï¼Œåº”ä»¥æ›´ç²¾ç»†çš„æ–¹å¼è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmPO-Diffusionï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›DPOç›®æ ‡çš„æ–°å‹åå¥½åˆ†å¸ƒå»ºæ¨¡æ–¹æ³•ï¼Œä»¥åŠæ‰©æ•£ä¼˜åŒ–ç›®æ ‡çš„æ•°å€¼ä¸Šé™ä¼°è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥å¹³æ»‘åå¥½åˆ†å¸ƒæ¥æ›¿æ¢åŸå§‹äºŒè¿›åˆ¶åˆ†å¸ƒã€‚æˆ‘ä»¬é‡‡ç”¨å¥–åŠ±æ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»åå¥½ï¼Œå¹¶é€šè¿‡åå¥½å¯èƒ½æ€§å¹³å‡æ¥æ”¹è¿›DPOæŸå¤±ï¼Œä½¿å¾—å½“åå¥½ç›¸ä¼¼æ—¶æŸå¤±å‡½æ•°æ¥è¿‘é›¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨åæ¼”æŠ€æœ¯æ¨¡æ‹Ÿæ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒï¼Œä½¿ä¸ä¼˜åŒ–ç›®æ ‡å¯¹é½å¾—æ›´å‡†ç¡®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ç›´æ¥ä¿®æ”¹æœ‰æ•ˆåœ°ç¼“è§£äº†ç°æœ‰æ–¹æ³•ä¸­è¿‡åº¦ä¼˜åŒ–å’Œç›®æ ‡ä¸åŒ¹é…çš„é—®é¢˜ã€‚SmPO-Diffusionåœ¨åå¥½è¯„ä¼°æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://jaydenlyh.github.io/SmPO-project-page/%E3%80%82">https://jaydenlyh.github.io/SmPO-project-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02698v2">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SmPO-Diffusionæ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¹³æ»‘åå¥½åˆ†å¸ƒæ¥æ”¹è¿›Direct Preference Optimizationï¼ˆDPOï¼‰çš„ç›®æ ‡ã€‚è¯¥æ–¹æ³•ä½¿ç”¨å¥–åŠ±æ¨¡å‹æ¨¡æ‹Ÿäººç±»åå¥½ï¼Œå¹¶é‡‡ç”¨åå¥½å¯èƒ½æ€§å¹³å‡æ³•æ”¹è¿›DPOæŸå¤±å‡½æ•°ã€‚åŒæ—¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒæ¨¡æ‹ŸæŠ€æœ¯ï¼Œæ›´æœ‰æ•ˆåœ°å¯¹é½ä¼˜åŒ–ç›®æ ‡ã€‚SmPO-Diffusionè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­çš„è¿‡åº¦ä¼˜åŒ–å’Œç›®æ ‡ä¸å¯¹é½é—®é¢˜ï¼Œå®ç°äº†ä¼˜å¼‚çš„åå¥½è¯„ä¼°æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæˆæœ¬ä¸Šä¼˜äºåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmPO-Diffusionæå‡ºä½¿ç”¨å¹³æ»‘åå¥½åˆ†å¸ƒæ¥æ”¹è¿›DPOçš„ç›®æ ‡ï¼Œä»¥æ›´å¥½åœ°æ¨¡æ‹Ÿäººç±»åå¥½ã€‚</li>
<li>å¼•å…¥å¥–åŠ±æ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»åå¥½ï¼Œæé«˜DPOæŸå¤±å‡½æ•°çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨åå¥½å¯èƒ½æ€§å¹³å‡æ³•ï¼Œå½“åå¥½ç›¸ä¼¼æ—¶ï¼ŒæŸå¤±å‡½æ•°æ¥è¿‘é›¶ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è½¨è¿¹åå¥½åˆ†å¸ƒæ¨¡æ‹ŸæŠ€æœ¯ï¼Œæ›´æœ‰æ•ˆåœ°å¯¹é½ä¼˜åŒ–ç›®æ ‡ã€‚</li>
<li>SmPO-Diffusionè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­çš„è¿‡åº¦ä¼˜åŒ–å’Œç›®æ ‡ä¸å¯¹é½é—®é¢˜ã€‚</li>
<li>SmPO-Diffusionåœ¨åå¥½è¯„ä¼°æ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨è®­ç»ƒæˆæœ¬ä¸Šä¼˜äºåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6742da8d8e9b49b497896a518d81d79a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b03e0f1affb96ca12192b1373cd69733.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fa2116ae724c8ef509c8648351861e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdef7621a4b0601a72c49b8fe875c298.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Latent-Feature-Guided-Conditional-Diffusion-for-Generative-Image-Semantic-Communication"><a href="#Latent-Feature-Guided-Conditional-Diffusion-for-Generative-Image-Semantic-Communication" class="headerlink" title="Latent Feature-Guided Conditional Diffusion for Generative Image   Semantic Communication"></a>Latent Feature-Guided Conditional Diffusion for Generative Image   Semantic Communication</h2><p><strong>Authors:Zehao Chen, Xinfeng Wei, Haonan Tong, Zhaohui Yang, Changchuan Yin</strong></p>
<p>Semantic communication is proposed and expected to improve the efficiency of massive data transmission over sixth generation (6G) networks. However, existing image semantic communication schemes are primarily focused on optimizing pixel-level metrics, while neglecting the crucial aspect of region of interest (ROI) preservation. To address this issue, we propose an ROI-aware latent representation-oriented image semantic communication (LRISC) system. In particular, we first map the source image to latent features in a high-dimensional semantic space, these latent features are then fused with ROI mask through a feature-weighting mechanism. Subsequently, these features are encoded using a joint source and channel coding (JSCC) scheme with adaptive rate for efficient transmission over a wireless channel. At the receiver, a conditional diffusion model is developed by using the received latent features as conditional guidance to steer the reverse diffusion process, progressively reconstructing high-fidelity images while preserving semantic consistency. Moreover, we introduce a channel signal-to-noise ratio (SNR) adaptation mechanism, allowing one model to work across various channel states. Experiments show that the proposed method significantly outperforms existing methods, in terms of learned perceptual image patch similarity (LPIPS) and robustness against channel noise, with an average LPIPS reduction of 43.3% compared to DeepJSCC, while guaranteeing the semantic consistency. </p>
<blockquote>
<p>è¯­ä¹‰é€šä¿¡æ—¨åœ¨æé«˜ç¬¬å…­ä»£ï¼ˆ6Gï¼‰ç½‘ç»œä¸Šå¤§è§„æ¨¡æ•°æ®ä¼ è¾“çš„æ•ˆç‡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒè¯­ä¹‰é€šä¿¡æ–¹æ¡ˆä¸»è¦å…³æ³¨åƒç´ çº§æŒ‡æ ‡çš„ä¼˜åŒ–ï¼Œå¿½è§†äº†æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä¿æŒçš„é‡è¦æ–¹é¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢å‘æ½œåœ¨è¡¨ç¤ºçš„æ„Ÿå…´è¶£åŒºåŸŸæ„ŸçŸ¥å›¾åƒè¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼ˆLRISCï¼‰ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æºå›¾åƒæ˜ å°„åˆ°é«˜ç»´è¯­ä¹‰ç©ºé—´ä¸­çš„æ½œåœ¨ç‰¹å¾ä¸Šï¼Œç„¶åé€šè¿‡ç‰¹å¾åŠ æƒæœºåˆ¶å°†è¿™äº›æ½œåœ¨ç‰¹å¾ä¸ROIæ©è†œèåˆã€‚éšåï¼Œè¿™äº›ç‰¹å¾ä½¿ç”¨è”åˆæºå’Œä¿¡é“ç¼–ç ï¼ˆJSCCï¼‰æ–¹æ¡ˆè¿›è¡Œç¼–ç ï¼Œå¹¶é€šè¿‡æ— çº¿ä¿¡é“è¿›è¡Œè‡ªé€‚åº”é€Ÿç‡çš„æœ‰æ•ˆä¼ è¾“ã€‚åœ¨æ¥æ”¶ç«¯ï¼Œé€šè¿‡ä½¿ç”¨æ¥æ”¶åˆ°çš„æ½œåœ¨ç‰¹å¾ä½œä¸ºæ¡ä»¶æŒ‡å¯¼ï¼Œå¼€å‘äº†ä¸€ç§æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥å¼•å¯¼åå‘æ‰©æ•£è¿‡ç¨‹ï¼Œé€æ­¥é‡å»ºé«˜è´¨é‡å›¾åƒå¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ä¿¡é“ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰è‡ªé€‚åº”æœºåˆ¶ï¼Œä½¿ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿåœ¨å„ç§ä¿¡é“çŠ¶æ€ä¸‹å·¥ä½œã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ„ŸçŸ¥å›¾åƒè¡¥ä¸ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰å’Œå¯¹æŠ—ä¿¡é“å™ªå£°çš„ç¨³å¥æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸DeepJSCCç›¸æ¯”å¹³å‡LPIPSé™ä½äº†43.3%ï¼ŒåŒæ—¶ä¿è¯äº†è¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21577v2">PDF</a> 6 pages, 6 figures, update title</p>
<p><strong>Summary</strong><br>    é’ˆå¯¹ç°æœ‰å›¾åƒè¯­ä¹‰é€šä¿¡æ–¹æ¡ˆå¿½è§†æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ä¿ç•™çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºROIæ„ŸçŸ¥çš„æ½œåœ¨è¡¨ç¤ºå¯¼å‘å›¾åƒè¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼ˆLRISCï¼‰ã€‚è¯¥ç³»ç»Ÿå°†æºå›¾åƒæ˜ å°„åˆ°é«˜ç»´è¯­ä¹‰ç©ºé—´çš„æ½œåœ¨ç‰¹å¾ï¼Œå¹¶ä¸ROIæ©è†œèåˆã€‚é€šè¿‡è”åˆæºä¿¡é“ç¼–ç ï¼ˆJSCCï¼‰æ–¹æ¡ˆè‡ªé€‚åº”é€Ÿç‡ç¼–ç ç‰¹å¾ï¼Œå®ç°æ— çº¿ä¿¡é“ä¸Šçš„é«˜æ•ˆä¼ è¾“ã€‚æ¥æ”¶ç«¯é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥æ¥æ”¶åˆ°çš„æ½œåœ¨ç‰¹å¾ä½œä¸ºæ¡ä»¶æŒ‡å¯¼ï¼Œåå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥é‡å»ºé«˜ä¿çœŸå›¾åƒï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥ä¿¡é“ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰è‡ªé€‚åº”æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½åœ¨å„ç§ä¿¡é“çŠ¶æ€ä¸‹å·¥ä½œã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥å›¾åƒå—ç›¸ä¼¼æ€§ï¼ˆLPIPSï¼‰å’Œä¿¡é“å™ªå£°é²æ£’æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹³å‡LPIPSé™ä½43.3%ï¼ŒåŒæ—¶ä¿è¯è¯­ä¹‰ä¸€è‡´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­ä¹‰é€šä¿¡æé«˜6Gç½‘ç»œå¤§æ•°æ®ä¼ è¾“æ•ˆç‡ã€‚</li>
<li>ç°æœ‰å›¾åƒè¯­ä¹‰é€šä¿¡æ–¹æ¡ˆä¸»è¦ä¼˜åŒ–åƒç´ çº§æŒ‡æ ‡ï¼Œå¿½è§†ROIä¿ç•™ã€‚</li>
<li>æå‡ºLRISCç³»ç»Ÿï¼Œæ˜ å°„æºå›¾åƒåˆ°é«˜ç»´è¯­ä¹‰ç©ºé—´å¹¶èåˆROIæ©è†œã€‚</li>
<li>ä½¿ç”¨JSCCæ–¹æ¡ˆè‡ªé€‚åº”é€Ÿç‡ç¼–ç ç‰¹å¾ï¼Œå®ç°é«˜æ•ˆæ— çº¿ä¼ è¾“ã€‚</li>
<li>æ¥æ”¶ç«¯é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹é€æ­¥é‡å»ºé«˜ä¿çœŸå›¾åƒå¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥SNRè‡ªé€‚åº”æœºåˆ¶é€‚åº”å„ç§ä¿¡é“çŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c744f2eb37d3d5a3e79596c7eb32e18a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6e961db6bd6441be06a13f0a5db7bd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d63e92aea5424baa15256c2129ef4f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a6f6dbfe813cae95b2c73ecf7f3ba1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9def2bdb721e686d718bdf2dd6ade90c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92eb6a961a473b1cbba87907e70e7aac.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Efficient-Diffusion-Models-A-Survey"><a href="#Efficient-Diffusion-Models-A-Survey" class="headerlink" title="Efficient Diffusion Models: A Survey"></a>Efficient Diffusion Models: A Survey</h2><p><strong>Authors:Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, Chaofan Tao, Yongfeng Huang, Ye Yuan, Mi Zhang</strong></p>
<p>Diffusion models have emerged as powerful generative models capable of producing high-quality contents such as images, videos, and audio, demonstrating their potential to revolutionize digital content creation. However, these capabilities come at the cost of their significant computational resources and lengthy generation time, underscoring the critical need to develop efficient techniques for practical deployment. In this survey, we provide a systematic and comprehensive review of research on efficient diffusion models. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient diffusion model topics from algorithm-level, system-level, and framework perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at <a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey</a>. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient diffusion model research and inspire them to contribute to this important and exciting field. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹å·²ç»å´­éœ²å¤´è§’ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ç­‰å†…å®¹ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬åœ¨æ•°å­—å†…å®¹åˆ›ä½œé¢†åŸŸå…·æœ‰é¢ è¦†æ€§æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›åŠŸèƒ½éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ¼«é•¿çš„ç”Ÿæˆæ—¶é—´ï¼Œè¿™å‡¸æ˜¾äº†å¼€å‘é«˜æ•ˆæŠ€æœ¯ç”¨äºå®é™…éƒ¨ç½²çš„è¿«åˆ‡éœ€æ±‚ã€‚åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬å¯¹é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶è¿›è¡Œäº†ç³»ç»Ÿè€Œå…¨é¢çš„ç»¼è¿°ã€‚æˆ‘ä»¬ä»ç®—æ³•å±‚é¢ã€ç³»ç»Ÿå±‚é¢å’Œæ¡†æ¶è§†è§’å¯¹æ–‡çŒ®è¿›è¡Œäº†åˆ†ç±»ï¼Œæ¶µç›–äº†ä¸‰ä¸ªä¸»è¦ç±»åˆ«ä¸­ä¸åŒä½†ç›¸äº’å…³è”çš„é«˜æ•ˆæ‰©æ•£æ¨¡å‹ä¸»é¢˜ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªGitHubä»“åº“ï¼Œå…¶ä¸­æ•´ç†äº†æœ¬æ¬¡è°ƒæŸ¥ä¸­ä»‹ç»çš„è®ºæ–‡ï¼Œåœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey%E3%80%82%E6%88%91%E4%BB%AC%E5%B9%B3%E5%B8%B8%E6%9C%AC%E6%9C%AF%E5%AF%BC%E5%AF%BC%E8%BF%BD%E6%B1%BD%E8%AF%AD%E7%A0%BA%E7%9A%84%E7%90%B3%E5%A4%B1%E6%B6%A6%E6%9D%A5%(Efficient-Diffusion-Model">https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Surveyã€‚æˆ‘ä»¬å¸Œæœ›æœ¬æ¬¡è°ƒæŸ¥èƒ½ä¸ºç ”ç©¶è€…å’Œä»ä¸šè€…æä¾›å¯¹é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç³»ç»Ÿæ€§ç†è§£ï¼Œå¹¶æ¿€å‘ä»–ä»¬å¯¹è¿™ä¸€é‡è¦ä¸”æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸçš„è´¡çŒ®ã€‚</a>èƒ½ä½œä¸ºæœ‰ä»·å€¼çš„èµ„æºï¼Œå¸®åŠ©ç ”ç©¶è€…å’Œä»ä¸šè€…ç³»ç»Ÿåœ°äº†è§£é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ï¼Œå¹¶æ¿€åŠ±ä»–ä»¬ä¸ºè¿™ä¸€é‡è¦ä¸”ä»¤äººå…´å¥‹çš„é¢†åŸŸåšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06805v3">PDF</a> Published in Transactions on Machine Learning Research (TMLR-2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹åœ¨æ•°å­—å†…å®¹åˆ›ä½œé¢†åŸŸçš„æ½œåŠ›ã€‚è™½ç„¶è¿™äº›æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„å†…å®¹å¦‚å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œé•¿æ—¶é—´çš„ç”Ÿæˆæ—¶é—´ã€‚æœ¬æ–‡æä¾›äº†ä¸€ç¯‡å…³äºé«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç»¼åˆæ€§è°ƒæŸ¥ï¼Œä»ç®—æ³•ã€ç³»ç»Ÿå’Œæ¡†æ¶ä¸‰ä¸ªè§’åº¦ç»„ç»‡äº†ç›¸å…³æ–‡çŒ®ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªGitHubä»“åº“æ¥æ•´ç†è¿™äº›è®ºæ–‡ã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›å¯¹é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç³»ç»Ÿæ€§ç†è§£ï¼Œå¹¶æ¿€å‘ä»–ä»¬å¯¹è¿™ä¸€é‡è¦ä¸”æ¿€åŠ¨äººå¿ƒçš„é¢†åŸŸçš„è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰é©å‘½æ€§æ•°å­—å†…å®¹åˆ›ä½œçš„æ½œåŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è™½ç„¶èƒ½äº§ç”Ÿé«˜è´¨é‡å†…å®¹ï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œé•¿æ—¶é—´çš„ç”Ÿæˆæ—¶é—´ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ä¸€ç¯‡å…³äºé«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç»¼åˆæ€§è°ƒæŸ¥ã€‚</li>
<li>è°ƒæŸ¥ä»ç®—æ³•ã€ç³»ç»Ÿå’Œæ¡†æ¶ä¸‰ä¸ªè§’åº¦ç»„ç»‡äº†ç›¸å…³æ–‡çŒ®ã€‚</li>
<li>åˆ›å»ºä¸€ä¸ªGitHubä»“åº“æ¥æ•´ç†è®ºæ–‡ï¼Œæ–¹ä¾¿ç ”ç©¶äººå‘˜å’Œå®è·µè€…è·å–èµ„æºã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›å¯¹é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„ç³»ç»Ÿæ€§ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2bdafff256bc7ee316918dc59e1d6299.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d6512ec15151a5791afab41f34547ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2a9265e9ce5ffeaf4ca20905f46ee5d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Normalizing-Flows-are-Capable-Generative-Models"><a href="#Normalizing-Flows-are-Capable-Generative-Models" class="headerlink" title="Normalizing Flows are Capable Generative Models"></a>Normalizing Flows are Capable Generative Models</h2><p><strong>Authors:Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, Josh Susskind</strong></p>
<p>Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/apple/ml-tarflow">https://github.com/apple/ml-tarflow</a>. </p>
<blockquote>
<p>æ ‡å‡†åŒ–æµï¼ˆNFsï¼‰æ˜¯åŸºäºè¿ç»­è¾“å…¥çš„ä¼¼ç„¶æ¨¡å‹ã€‚å®ƒä»¬åœ¨å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šéƒ½æ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„ç»“æœï¼Œä½†è¿‘å¹´æ¥å—åˆ°çš„å…³æ³¨ç›¸å¯¹è¾ƒå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜NFsæ¯”ä»¥å‰è®¤ä¸ºçš„æ›´å¼ºå¤§ã€‚æˆ‘ä»¬æå‡ºäº†TarFlowï¼šä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ¶æ„ï¼Œèƒ½å¤Ÿå®ç°é«˜æ€§èƒ½çš„NFæ¨¡å‹ã€‚TarFlowå¯ä»¥è¢«çœ‹ä½œæ˜¯åŸºäºå˜å‹å™¨çš„æ©ç è‡ªå›å½’æµï¼ˆMAFï¼‰çš„å˜ä½“ï¼šå®ƒç”±å›¾åƒè¡¥ä¸ä¸Šçš„è‡ªå›å½’å˜å‹å™¨å—å †å è€Œæˆï¼Œå¹¶åœ¨å±‚ä¹‹é—´äº¤æ›¿è‡ªå›å½’æ–¹å‘ã€‚TarFlowå¯ä»¥ç«¯åˆ°ç«¯è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”èƒ½å¤Ÿç›´æ¥å¯¹åƒç´ è¿›è¡Œå»ºæ¨¡å’Œç”Ÿæˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸‰ç§æé«˜æ ·æœ¬è´¨é‡çš„å…³é”®æŠ€æœ¯ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ–¯å™ªå£°å¢å¼ºã€è®­ç»ƒåçš„å»å™ªç¨‹åºï¼Œä»¥åŠé’ˆå¯¹æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶è®¾ç½®çš„æœ‰æ•ˆçš„æŒ‡å¯¼æ–¹æ³•ã€‚å°†è¿™äº›ç»“åˆèµ·æ¥ï¼ŒTarFlowåœ¨å›¾åƒä¼¼ç„¶ä¼°è®¡æ–¹é¢è¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¤§å¤§è¶…è¿‡äº†ä»¥å‰æœ€å¥½çš„æ–¹æ³•ï¼Œå¹¶ä¸”ç”Ÿæˆçš„æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§å¯ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“ï¼Œè¿™æ˜¯é¦–æ¬¡ä½¿ç”¨ç‹¬ç«‹çš„NFæ¨¡å‹å®ç°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/apple/ml-tarflow%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/apple/ml-tarflowè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06329v3">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æµæ¨¡å‹ï¼ˆNFsï¼‰åœ¨å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ¶æ„TarFlowï¼Œå®ƒåŸºäºTransformerå’ŒMasked Autoregressive Flowsï¼ˆMAFsï¼‰ï¼Œèƒ½å¤Ÿæ„å»ºé«˜æ€§èƒ½çš„NFæ¨¡å‹ã€‚TarFlowé€šè¿‡ä¸‰å±‚æŠ€æœ¯æ”¹è¿›æ ·æœ¬è´¨é‡ï¼ŒåŒ…æ‹¬è®­ç»ƒè¿‡ç¨‹ä¸­çš„é«˜æ–¯å™ªå£°å¢å¼ºã€è®­ç»ƒåçš„å»å™ªç¨‹åºä»¥åŠåœ¨æœ‰ç±»å’Œæ— æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæŒ‡å¯¼æ–¹æ³•ã€‚æœ€ç»ˆï¼ŒTarFlowåœ¨å›¾åƒçš„æ¦‚ç‡ä¼°è®¡ä¸Šè¾¾åˆ°æ–°çš„æœ€é«˜æ°´å¹³ï¼Œç”Ÿæˆæ ·æœ¬çš„è´¨é‡å’Œå¤šæ ·æ€§é¦–æ¬¡ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµæ¨¡å‹ï¼ˆNFsï¼‰åœ¨å¯†åº¦ä¼°è®¡å’Œç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ä¸Šå…·æœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>TarFlowæ˜¯ä¸€ç§åŸºäºTransformerå’ŒMAFsçš„æ¶æ„ï¼Œä½¿NFæ¨¡å‹æ›´åŠ é«˜æ•ˆã€‚</li>
<li>TarFlowé€šè¿‡å †å çš„è‡ªå›å½’Transformerå—å¯¹å›¾åƒå—è¿›è¡Œå¤„ç†ï¼Œå¹¶åœ¨å±‚é—´äº¤æ›¿è‡ªå›å½’æ–¹å‘ã€‚</li>
<li>TarFlowé‡‡ç”¨ä¸‰å±‚æŠ€æœ¯æé«˜æ ·æœ¬è´¨é‡ï¼šè®­ç»ƒä¸­çš„é«˜æ–¯å™ªå£°å¢å¼ºã€è®­ç»ƒåçš„å»å™ªç¨‹åºå’Œæœ‰æŒ‡å¯¼çš„ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>TarFlowè¾¾åˆ°æ–°çš„å›¾åƒæ¦‚ç‡ä¼°è®¡çš„æœ€é«˜æ°´å¹³ï¼Œå¤§å¹…åº¦è¶…è¶Šä»¥å‰æœ€å¥½çš„æ–¹æ³•ã€‚</li>
<li>TarFlowç”Ÿæˆçš„æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§é¦–æ¬¡ä¸æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb21b054e9eb87d153862caa3d38e3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a449c5a515ca12aa486623921d989d94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-671334975173fbb4b6b15a96b63c1c0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a109ba9973f0bc49a09b2c5b78b3d319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33da3fea396089ae575cfb2780859afc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79255ee595e1c67b3e521691f7fa0b53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a032bae5e9fc4aa9184f5b7788b2436c.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Birth-and-Death-of-a-Rose"><a href="#Birth-and-Death-of-a-Rose" class="headerlink" title="Birth and Death of a Rose"></a>Birth and Death of a Rose</h2><p><strong>Authors:Chen Geng, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu</strong></p>
<p>We study the problem of generating temporal object intrinsics â€“ temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose â€“ from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pre-trained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan. Project website: <a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d">https://chen-geng.com/rose4d</a> </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶ä»é¢„è®­ç»ƒçš„2DåŸºç¡€æ¨¡å‹ç”Ÿæˆæ—¶é—´å¯¹è±¡å›ºæœ‰ä½“çš„é—®é¢˜â€”â€”æ—¶é—´æ¼”åŒ–çš„å¯¹è±¡å‡ ä½•ã€åå°„ç‡å’Œçº¹ç†åºåˆ—ï¼Œå¦‚ç››å¼€çš„ç«ç‘°ã€‚ä¸åŒäºéœ€è¦å¤§é‡äººå·¥åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†çš„ä¼ ç»Ÿ3Då»ºæ¨¡å’ŒåŠ¨ç”»æŠ€æœ¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä»é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹ä¸­æç‚¼çš„ä¿¡å·ç”Ÿæˆæ­¤ç±»èµ„äº§ã€‚ä¸ºäº†ç¡®ä¿å¯¹è±¡å›ºæœ‰ä½“çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè‡ªç›‘ç£å­¦ä¹ å›¾åƒç‰¹å¾è‡ªåŠ¨æ´¾ç”Ÿçš„æ—¶é—´çŠ¶æ€å¼•å¯¼è’¸é¦çš„ç¥ç»ç½‘ç»œæ¨¡æ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸ºå¤šç§è‡ªç„¶ç°è±¡ç”Ÿæˆé«˜è´¨é‡çš„æ—¶é—´å¯¹è±¡å›ºæœ‰ä½“ï¼Œå¹¶èƒ½å¤Ÿåœ¨ä»»ä½•ç”Ÿå‘½å‘¨æœŸçš„ä»»ä½•æ—¶é—´ã€ä»»ä½•ç¯å¢ƒç…§æ˜æ¡ä»¶ä¸‹ï¼Œä»ä»»ä½•è§†è§’å¯¹è¿™äº›åŠ¨æ€å¯¹è±¡è¿›è¡Œé‡‡æ ·å’Œå¯æ§æ¸²æŸ“ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d">https://chen-geng.com/rose4d</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05278v2">PDF</a> CVPR 2025 Oral. Project website: <a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d">https://chen-geng.com/rose4d</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ—¶é—´æ€§å¯¹è±¡å›ºæœ‰å±æ€§ï¼Œå¦‚ç”Ÿé•¿ä¸­çš„èŠ±æœµç­‰ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä»é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹ä¸­æç‚¼ä¿¡å·ç”Ÿæˆæ­¤ç±»èµ„äº§çš„æ–¹æ³•ï¼Œæ— éœ€ä¼ ç»Ÿä¸‰ç»´å»ºæ¨¡å’ŒåŠ¨ç”»æŠ€æœ¯æ‰€éœ€çš„å¤§é‡æ‰‹åŠ¨å·¥ä½œå’Œä¸“ä¸šçŸ¥è¯†ã€‚é€šè¿‡ä½¿ç”¨ç¥ç»æ¨¡æ¿è¿›è¡Œæ—¶é—´çŠ¶æ€å¼•å¯¼çš„è’¸é¦ï¼Œç¡®ä¿äº†å¯¹è±¡å›ºæœ‰å±æ€§çš„æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ—¶é—´æ€§å¯¹è±¡å›ºæœ‰å±æ€§ï¼Œä¸ºå¤šç§è‡ªç„¶ç°è±¡æä¾›é‡‡æ ·ï¼Œå¹¶å¯ä»ä»»ä½•è§’åº¦ã€åœ¨ä»»ä½•ç¯å¢ƒç…§æ˜æ¡ä»¶ä¸‹ã€åœ¨å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸä¸­çš„ä»»ä½•æ—¶é—´è¿›è¡Œå¯æ§æ¸²æŸ“ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d%E3%80%82">https://chen-geng.com/rose4dã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨ç”Ÿæˆæ—¶é—´æ€§å¯¹è±¡å›ºæœ‰å±æ€§ï¼Œå¦‚ç”Ÿé•¿ä¸­çš„èŠ±æœµç­‰ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿™äº›å±æ€§ï¼Œç®€åŒ–äº†ä¼ ç»Ÿä¸‰ç»´å»ºæ¨¡å’ŒåŠ¨ç”»æŠ€æœ¯çš„å¤æ‚æµç¨‹ã€‚</li>
<li>å¼•å…¥ç¥ç»æ¨¡æ¿ç¡®ä¿å¯¹è±¡å›ºæœ‰å±æ€§çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ—¶é—´æ€§å¯¹è±¡å›ºæœ‰å±æ€§ï¼Œé€‚ç”¨äºå¤šç§è‡ªç„¶ç°è±¡ã€‚</li>
<li>å¯å®ç°ä»ä»»ä½•è§’åº¦ã€åœ¨ä»»ä½•ç¯å¢ƒç…§æ˜æ¡ä»¶ä¸‹ã€åœ¨å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸä¸­çš„ä»»ä½•æ—¶é—´çš„å¯æ§æ¸²æŸ“ã€‚</li>
<li>é¡¹ç›®è¯¦ç»†ä¿¡æ¯å¯é€šè¿‡è®¿é—®ç½‘ç«™<a target="_blank" rel="noopener" href="https://chen-geng.com/rose4d%E8%8E%B7%E5%8F%96%E3%80%82">https://chen-geng.com/rose4dè·å–ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb93f895cb6ff7e8514c5e822792bd92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22775306ecbddeb5796e1613f273cefb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37f99c754ec1c1067f9c5c05e9893b7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c768f3a252f92e750442256486eb0082.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Understanding-Memorization-in-Generative-Models-via-Sharpness-in-Probability-Landscapes"><a href="#Understanding-Memorization-in-Generative-Models-via-Sharpness-in-Probability-Landscapes" class="headerlink" title="Understanding Memorization in Generative Models via Sharpness in   Probability Landscapes"></a>Understanding Memorization in Generative Models via Sharpness in   Probability Landscapes</h2><p><strong>Authors:Dongjae Jeon, Dueun Kim, Albert No</strong></p>
<p>In this paper, we introduce a geometric framework to analyze memorization in diffusion models through the sharpness of the log probability density. We mathematically justify a previously proposed score-difference-based memorization metric by demonstrating its effectiveness in quantifying sharpness. Additionally, we propose a novel memorization metric that captures sharpness at the initial stage of image generation in latent diffusion models, offering early insights into potential memorization. Leveraging this metric, we develop a mitigation strategy that optimizes the initial noise of the generation process using a sharpness-aware regularization term. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ•°æ¦‚ç‡å¯†åº¦çš„å°–é”ç¨‹åº¦æ¥åˆ†ææ‰©æ•£æ¨¡å‹ä¸­çš„è®°å¿†èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡è¯æ˜å…¶åœ¨é‡åŒ–å°–é”åº¦æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºä¹‹å‰æå‡ºçš„åŸºäºåˆ†æ•°å·®å¼‚çš„è®°å¿†åŠ›æŒ‡æ ‡æä¾›äº†æ•°å­¦è®ºè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„è®°å¿†åŠ›æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡èƒ½å¤Ÿæ•æ‰æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„å›¾åƒç”Ÿæˆåˆå§‹é˜¶æ®µçš„å°–é”åº¦ï¼Œä¸ºæ½œåœ¨çš„è®°å¿†åŠ›æä¾›æ—©æœŸè§è§£ã€‚å€ŸåŠ©è¿™ä¸€æŒ‡æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç¼“è§£ç­–ç•¥ï¼Œé€šè¿‡é‡‡ç”¨æ„ŸçŸ¥å°–é”åº¦çš„æ­£åˆ™åŒ–é¡¹æ¥ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹çš„åˆå§‹å™ªå£°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04140v3">PDF</a> Accepted at ICML 2025 (Spotlight)</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•æ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡å¯†åº¦å¯¹æ•°å°–é”åº¦åˆ†ææ‰©æ•£æ¨¡å‹ä¸­çš„è®°å¿†èƒ½åŠ›ã€‚æ–‡ç« è¯æ˜äº†å…ˆå‰æå‡ºçš„åŸºäºè¯„åˆ†å·®å¼‚çš„é‡åŒ–è®°å¿†èƒ½åŠ›æŒ‡æ ‡çš„æ•°å­¦æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–å›¾åƒç”ŸæˆåˆæœŸè®°å¿†èƒ½åŠ›çš„æŒ‡æ ‡ã€‚æ­¤å¤–ï¼ŒåŸºäºè¿™ä¸€æ–°æŒ‡æ ‡ï¼Œæå‡ºäº†ä¸€ç§ä¼˜åŒ–ç”Ÿæˆè¿‡ç¨‹åˆå§‹å™ªå£°çš„ç¼“è§£ç­–ç•¥ã€‚æ­¤ç­–ç•¥é€šè¿‡å¼•å…¥ä¸€ä¸ªå°–é”åº¦æ„ŸçŸ¥æ­£åˆ™åŒ–é¡¹æ¥å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>å¼•å…¥å‡ ä½•æ¡†æ¶åˆ†ææ‰©æ•£æ¨¡å‹ä¸­è®°å¿†èƒ½åŠ›çš„æœºåˆ¶ã€‚</li>
<li>é€šè¿‡æ¦‚ç‡å¯†åº¦å¯¹æ•°å°–é”åº¦æ¥è¡¡é‡æ‰©æ•£æ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚</li>
<li>è¯æ˜äº†åŸºäºè¯„åˆ†å·®å¼‚çš„é‡åŒ–è®°å¿†èƒ½åŠ›æŒ‡æ ‡çš„æ•°å­¦æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹é‡åŒ–æŒ‡æ ‡æ¥è¡¡é‡å›¾åƒç”ŸæˆåˆæœŸé˜¶æ®µæ¨¡å‹å¯¹ä¿¡æ¯çš„è®°å¿†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72fd8a6c12256159a3543db45931be41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2456f7bcce1b2687d2077f7b0e83408.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f8d303bf54cf2d273a6f5b383176b78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e692c9963094f7eef5a04af0ffab94f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2145a309848de8395686f607d60345dc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨è§£å†³å›¾åƒé‡å»ºé—®é¢˜æ–¹é¢æœ‰ç€å¹¿æ³›çš„åº”ç”¨å’Œå·¨å¤§æ½œåŠ›ã€‚ä¸€äº›ä½œå“å°è¯•ä½¿ç”¨æ‰©æ•£æ¨¡å‹è§£å†³MRIé‡å»ºé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•ç›´æ¥åœ¨åƒç´ ç©ºé—´æ“ä½œï¼Œå¯¼è‡´ä¼˜åŒ–å’Œæ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æ½œä¼æ‰©æ•£æ¨¡å‹é¢„å…ˆåœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰ä¸°å¯Œçš„è§†è§‰å…ˆéªŒï¼Œæœ‰æœ›é€šè¿‡åœ¨ä½ç»´æ½œä¼ç©ºé—´æ“ä½œæ¥è§£å†³MRIé‡å»ºä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼š(1)ç¼ºä¹åŒ»ç–—å‡†ç¡®æ€§çš„æ˜ç¡®æ§åˆ¶æœºåˆ¶ï¼›(2)è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼›(3)æ½œä¼ç©ºé—´ä¸­çš„æ•°æ®ä¸€è‡´æ€§æœªå®šä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ½œä¼æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„LDPMæ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼š(1)ä¸€ä¸ªè‰å›¾å¼•å¯¼ç®¡é“ï¼Œé‡‡ç”¨ä¸¤æ­¥é‡å»ºç­–ç•¥ï¼Œå¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–å‡†ç¡®æ€§ï¼›(2)ä¸€ä¸ªä¼˜åŒ–çš„MRIå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆMR-VAEï¼‰ï¼Œä¸SD-VAEç›¸æ¯”ï¼Œåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸­æé«˜äº†çº¦3.92 dB \cite{sd}ï¼›(3)åŒé˜¶æ®µé‡‡æ ·å™¨ï¼Œæ˜¯é—´éš”DDPMé‡‡æ ·å™¨çš„æ”¹è¿›ç‰ˆï¼Œå¼ºåˆ¶æ½œä¼ç©ºé—´çš„é«˜ä¿çœŸé‡å»ºã€‚åœ¨fastMRIæ•°æ®é›†\cite{fastmri}ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ€æ–°æ€§èƒ½ä»¥åŠåœ¨å„ç§åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚å„æ¨¡å—çš„æœ‰æ•ˆæ€§ä¹Ÿé€šè¿‡æ¶ˆèå®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v3">PDF</a> accepted as oral presentation at EMBC 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒé‡å»ºé—®é¢˜ä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚å°½ç®¡å·²æœ‰å·¥ä½œå°è¯•ç”¨æ‰©æ•£æ¨¡å‹è§£å†³MRIé‡å»ºé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨åƒç´ ç©ºé—´ç›´æ¥æ“ä½œï¼Œå¯¼è‡´ä¼˜åŒ–å’Œæ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚é¢„æœŸæ½œä¼æ‰©æ•£æ¨¡å‹ï¼ˆåœ¨å¤©ç„¶å›¾åƒä¸Šé¢„è®­ç»ƒï¼Œå…·æœ‰ä¸°å¯Œçš„è§†è§‰å…ˆéªŒï¼‰èƒ½å¤Ÿåœ¨MRIé‡å»ºä¸­è§£å†³é«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼Œé€šè¿‡åœ¨ä½ç»´æ½œä¼ç©ºé—´æ“ä½œã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åŒ»å­¦ä¿çœŸåº¦çš„ç¼ºä¹æ˜ç¡®æ§åˆ¶æœºåˆ¶ï¼Œï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠï¼ˆ3ï¼‰æ½œä¼ç©ºé—´ä¸­æœªå®šä¹‰çš„æ•°æ®ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæ½œä¼æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„LDPMæ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¸¦æœ‰ä¸¤æ­¥é‡å»ºç­–ç•¥çš„è‰å›¾å¼•å¯¼ç®¡é“ï¼Œå¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ä¿çœŸåº¦ï¼Œï¼ˆ2ï¼‰é’ˆå¯¹MRIä¼˜åŒ–çš„VAEï¼ˆMR-VAEï¼‰ï¼Œä¸SD-VAEç›¸æ¯”ï¼Œåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šæé«˜äº†çº¦3.92 dB \cite{sd}ï¼Œä»¥åŠï¼ˆ3ï¼‰åŒé˜¶æ®µé‡‡æ ·å™¨ï¼Œè¿™æ˜¯é—´éš”DDPMé‡‡æ ·å™¨çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œåœ¨æ½œä¼ç©ºé—´ä¸­å¼ºåˆ¶é«˜ä¿çœŸé‡å»ºã€‚åœ¨fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒ\cite{fastmri}è¯æ˜äº†è¯¥æ–¹æ³•çš„å…ˆè¿›æ€§èƒ½åŠå…¶åœ¨å¤šç§åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚é€šè¿‡æ¶ˆèå®éªŒä¹ŸéªŒè¯äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé‡å»ºä¸­å±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚</li>
<li>ç›´æ¥åœ¨åƒç´ ç©ºé—´æ“ä½œå¯¼è‡´MRIé‡å»ºçš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>æ½œä¼æ‰©æ•£æ¨¡å‹æœŸæœ›é€šè¿‡ä½ç»´æ½œä¼ç©ºé—´æ“ä½œæ¥è§£å†³é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šåŒ»å­¦ä¿çœŸåº¦çš„æ§åˆ¶ã€é¢†åŸŸå·®è·å’Œæ•°æ®ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>LDPMæ–¹æ³•é€šè¿‡è‰å›¾å¼•å¯¼ç®¡é“ã€MRIä¼˜åŒ–çš„VAEå’ŒåŒé˜¶æ®µé‡‡æ ·å™¨æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>LDPMåœ¨fastMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å…ˆè¿›ï¼Œä¸”åœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59c1a174dbd1f401ef1c046c5604925b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Flexiffusion-Segment-wise-Neural-Architecture-Search-for-Flexible-Denoising-Schedule"><a href="#Flexiffusion-Segment-wise-Neural-Architecture-Search-for-Flexible-Denoising-Schedule" class="headerlink" title="Flexiffusion: Segment-wise Neural Architecture Search for Flexible   Denoising Schedule"></a>Flexiffusion: Segment-wise Neural Architecture Search for Flexible   Denoising Schedule</h2><p><strong>Authors:Hongtao Huang, Xiaojun Chang, Lina Yao</strong></p>
<p>Diffusion models are cutting-edge generative models adept at producing diverse, high-quality images. Despite their effectiveness, these models often require significant computational resources owing to their numerous sequential denoising steps and the significant inference cost of each step. Recently, Neural Architecture Search (NAS) techniques have been employed to automatically search for faster generation processes. However, NAS for diffusion is inherently time-consuming as it requires estimating thousands of diffusion models to search for the optimal one. In this paper, we introduce Flexiffusion, a novel training-free NAS paradigm designed to accelerate diffusion models by concurrently optimizing generation steps and network structures. Specifically, we partition the generation process into isometric step segments, each sequentially composed of a full step, multiple partial steps, and several null steps. The full step computes all network blocks, while the partial step involves part of the blocks, and the null step entails no computation. Flexiffusion autonomously explores flexible step combinations for each segment, substantially reducing search costs and enabling greater acceleration compared to the state-of-the-art (SOTA) method for diffusion models. Our searched models reported speedup factors of $2.6\times$ and $1.5\times$ for the original LDM-4-G and the SOTA, respectively. The factors for Stable Diffusion V1.5 and the SOTA are $5.1\times$ and $2.0\times$. We also verified the performance of Flexiffusion on multiple datasets, and positive experiment results indicate that Flexiffusion can effectively reduce redundancy in diffusion models. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯å‰æ²¿çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ“…é•¿ç”Ÿæˆå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„å›¾ç‰‡ã€‚å°½ç®¡è¿™äº›æ¨¡å‹éå¸¸æœ‰æ•ˆï¼Œä½†ç”±äºå…¶ä¼—å¤šçš„è¿ç»­å»å™ªæ­¥éª¤å’Œæ¯ä¸ªæ­¥éª¤çš„é‡å¤§æ¨ç†æˆæœ¬ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æœ€è¿‘ï¼Œç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯å·²è¢«ç”¨äºè‡ªåŠ¨å¯»æ‰¾æ›´å¿«çš„ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå¯¹æ‰©æ•£çš„NASæœ¬è´¨ä¸Šæ˜¯è€—æ—¶çš„ï¼Œå› ä¸ºå®ƒéœ€è¦è¯„ä¼°æˆåƒä¸Šä¸‡çš„æ‰©æ•£æ¨¡å‹æ¥å¯»æ‰¾æœ€ä½³æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Flexiffusionï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„NASèŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡åŒæ—¶ä¼˜åŒ–ç”Ÿæˆæ­¥éª¤å’Œç½‘ç»œç»“æ„æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆè¿‡ç¨‹åˆ’åˆ†ä¸ºç­‰è·çš„æ­¥éª¤æ®µï¼Œæ¯ä¸ªæ®µç”±å®Œæ•´çš„æ­¥éª¤ã€å¤šä¸ªéƒ¨åˆ†æ­¥éª¤å’Œå‡ ä¸ªç©ºæ­¥éª¤é¡ºåºç»„æˆã€‚å®Œæ•´æ­¥éª¤è®¡ç®—æ‰€æœ‰ç½‘ç»œå—ï¼Œéƒ¨åˆ†æ­¥éª¤æ¶‰åŠéƒ¨åˆ†å—ï¼Œè€Œç©ºæ­¥éª¤åˆ™ä¸æ¶‰åŠè®¡ç®—ã€‚Flexiffusionè‡ªä¸»åœ°æ¢ç´¢æ¯ä¸ªæ®µçš„çµæ´»æ­¥éª¤ç»„åˆï¼Œå¤§å¤§é™ä½äº†æœç´¢æˆæœ¬ï¼Œä¸æ‰©æ•£æ¨¡å‹çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†æ›´å¤§çš„åŠ é€Ÿã€‚æˆ‘ä»¬æœç´¢çš„æ¨¡å‹æŠ¥å‘Šäº†åŸå§‹LDM-4-Gå’Œæœ€æ–°æ–¹æ³•çš„åŠ é€Ÿå€æ•°åˆ†åˆ«ä¸º2.6å€å’Œ1.5å€ã€‚å¯¹äºStable Diffusion V1.5å’Œæœ€æ–°æ–¹æ³•ï¼ŒåŠ é€Ÿå€æ•°åˆ†åˆ«ä¸º5.1å€å’Œ2.0å€ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Flexiffusionåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œç§¯æçš„å®éªŒç»“æœè¯æ˜Flexiffusionå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘æ‰©æ•£æ¨¡å‹ä¸­çš„å†—ä½™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.17566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹æ˜¯æ“…é•¿ç”Ÿæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒçš„å…ˆè¿›ç”Ÿæˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå…¶ä¼—å¤šçš„è¿ç»­å»å™ªæ­¥éª¤å’Œæ¯ä¸ªæ­¥éª¤çš„é«˜æ¨ç†æˆæœ¬ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚æœ€è¿‘ï¼Œç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯è¢«ç”¨æ¥è‡ªåŠ¨å¯»æ‰¾æ›´å¿«çš„ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå¯¹æ‰©æ•£çš„NASæœ¬è´¨ä¸Šæ˜¯è€—æ—¶çš„ï¼Œå› ä¸ºå®ƒéœ€è¦è¯„ä¼°æˆåƒä¸Šä¸‡çš„æ‰©æ•£æ¨¡å‹æ¥å¯»æ‰¾æœ€ä¼˜æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»äº†Flexiffusionï¼Œä¸€ç§å…¨æ–°çš„æ— éœ€è®­ç»ƒçš„NASèŒƒå¼ï¼Œæ—¨åœ¨é€šè¿‡åŒæ—¶ä¼˜åŒ–ç”Ÿæˆæ­¥éª¤å’Œç½‘ç»œç»“æ„æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚Flexiffusionè‡ªä¸»æ¢ç´¢æ¯ä¸ªæ®µçš„çµæ´»æ­¥éª¤ç»„åˆï¼Œå¤§å¤§é™ä½äº†æœç´¢æˆæœ¬ï¼Œä¸æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”å®ç°äº†æ›´å¤§çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¤šæ ·ã€é«˜è´¨é‡å›¾åƒï¼Œä½†éœ€è¦å¤§é‡è®¡ç®—èµ„æºã€‚</li>
<li>ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯è¢«ç”¨äºè‡ªåŠ¨å¯»æ‰¾æ›´å¿«çš„æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>Flexiffusionæ˜¯ä¸€ç§å…¨æ–°çš„æ— éœ€è®­ç»ƒçš„NASæ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Flexiffusioné€šè¿‡ä¼˜åŒ–ç”Ÿæˆæ­¥éª¤å’Œç½‘ç»œç»“æ„æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Flexiffusionè‡ªä¸»æ¢ç´¢æ¯ä¸ªç”Ÿæˆæ­¥éª¤æ®µçš„çµæ´»ç»„åˆï¼Œé™ä½æœç´¢æˆæœ¬ã€‚</li>
<li>Flexiffusionå®ç°çš„é€Ÿåº¦æå‡å› ç´ æ˜¾è‘—ï¼Œä¾‹å¦‚å¯¹åŸå§‹LDM-4-Gå’Œå…ˆè¿›æ–¹æ³•çš„åŠ é€Ÿåˆ†åˆ«ä¸º$2.6\times$å’Œ$1.5\times$ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.17566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4aa4d3422220603dc4ac4375c9c727b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de39a47b02a85cdc9b447eab01fffbf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b68315dcc2c3727810895cf0147017f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c95a9c01cdc3431813de60abeb6b40c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e46c2d8d7843ec9854580ae7a210443f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-16f5c5f351a9a28f3ab2c432f175292a.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Integrating Complexity and Biological Realism High-Performance Spiking   Neural Networks for Breast Cancer Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3882ab7f62c5a0c00d382b681b2933a6.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Dy3DGS-SLAM Monocular 3D Gaussian Splatting SLAM for Dynamic   Environments
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
