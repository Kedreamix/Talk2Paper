<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f32a620e1dcc6c1169b3c854b451e2bd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-10-æ›´æ–°"><a href="#2025-06-10-æ›´æ–°" class="headerlink" title="2025-06-10 æ›´æ–°"></a>2025-06-10 æ›´æ–°</h1><h2 id="Eigenspectrum-Analysis-of-Neural-Networks-without-Aspect-Ratio-Bias"><a href="#Eigenspectrum-Analysis-of-Neural-Networks-without-Aspect-Ratio-Bias" class="headerlink" title="Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias"></a>Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias</h2><p><strong>Authors:Yuanzhe Hu, Kinshuk Goel, Vlad Killiakov, Yaoqing Yang</strong></p>
<p>Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight matrices has been an active area of research in recent years. At a high level, eigenspectrum analysis of DNNs involves measuring the heavytailness of the empirical spectral densities (ESD) of weight matrices. It provides insight into how well a model is trained and can guide decisions on assigning better layer-wise training hyperparameters. In this paper, we address a challenge associated with such eigenspectrum methods: the impact of the aspect ratio of weight matrices on estimated heavytailness metrics. We demonstrate that matrices of varying sizes (and aspect ratios) introduce a non-negligible bias in estimating heavytailness metrics, leading to inaccurate model diagnosis and layer-wise hyperparameter assignment. To overcome this challenge, we propose FARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the weight matrices by subsampling submatrices with a fixed aspect ratio. Instead of measuring the heavytailness of the original ESD, we measure the average ESD of these subsampled submatrices. We show that measuring the heavytailness of these submatrices with the fixed aspect ratio can effectively mitigate the aspect ratio bias. We validate our approach across various optimization techniques and application domains that involve eigenspectrum analysis of weights, including image classification in computer vision (CV) models, scientific machine learning (SciML) model training, and large language model (LLM) pruning. Our results show that despite its simplicity, FARMS uniformly improves the accuracy of eigenspectrum analysis while enabling more effective layer-wise hyperparameter assignment in these application domains. In one of the LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model by 17.3% when compared with the state-of-the-art method. </p>
<blockquote>
<p>é€šè¿‡æƒé‡çŸ©é˜µçš„ç‰¹å¾è°±è¯Šæ–­æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è¿‘å¹´æ¥å·²æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚ä»é«˜å±‚æ¬¡ä¸Šçœ‹ï¼ŒDNNçš„ç‰¹å¾è°±åˆ†ææ¶‰åŠæµ‹é‡å®è¯è°±å¯†åº¦ï¼ˆESDï¼‰çš„é‡å°¾æ€§ã€‚å®ƒæä¾›äº†æ¨¡å‹è®­ç»ƒçŠ¶å†µçš„æ´å¯Ÿï¼Œå¹¶å¯ä»¥æŒ‡å¯¼åˆ†é…æ›´å¥½çš„é€å±‚è®­ç»ƒè¶…å‚æ•°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†ä¸è¿™ç§ç‰¹å¾è°±æ–¹æ³•ç›¸å…³çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼šæƒé‡çŸ©é˜µçš„çºµæ¨ªæ¯”ä¼°è®¡é‡å°¾åº¦æŒ‡æ ‡çš„å½±å“ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä¸åŒå¤§å°ï¼ˆå’Œçºµæ¨ªæ¯”ï¼‰çš„çŸ©é˜µåœ¨ä¼°è®¡é‡å°¾åº¦æŒ‡æ ‡æ—¶ä¼šäº§ç”Ÿä¸å¯å¿½ç•¥çš„åå·®ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹è¯Šæ–­ä¸å‡†ç¡®å’Œé€å±‚è¶…å‚æ•°åˆ†é…ä¸å½“ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FARMSï¼ˆå›ºå®šçºµæ¨ªæ¯”çŸ©é˜µå­é‡‡æ ·ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å­é‡‡æ ·å…·æœ‰å›ºå®šçºµæ¨ªæ¯”çš„å­çŸ©é˜µæ¥æ ‡å‡†åŒ–æƒé‡çŸ©é˜µçš„æ–¹æ³•ã€‚æˆ‘ä»¬ä¸æ˜¯æµ‹é‡åŸå§‹ESDçš„é‡å°¾æ€§ï¼Œè€Œæ˜¯æµ‹é‡è¿™äº›å­é‡‡æ ·çš„å­çŸ©é˜µçš„å¹³å‡ESDã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡æµ‹é‡å…·æœ‰å›ºå®šçºµæ¨ªæ¯”çš„è¿™äº›å­çŸ©é˜µçš„é‡å°¾æ€§å¯ä»¥æœ‰æ•ˆåœ°å‡è½»çºµæ¨ªæ¯”åå·®ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠæƒé‡ç‰¹å¾è°±åˆ†æçš„å„ä¸ªé¢†åŸŸéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ¨¡å‹ä¸­çš„å›¾åƒåˆ†ç±»ã€ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰æ¨¡å‹è®­ç»ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¿®å‰ªã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡å…¶ç®€å•æ€§ï¼ŒFARMSåœ¨ç»Ÿä¸€æé«˜ç‰¹å¾è°±åˆ†æçš„å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œåœ¨è¿™äº›é¢†åŸŸä½¿æ›´æœ‰æ•ˆçš„é€å±‚è¶…å‚æ•°åˆ†é…æˆä¸ºå¯èƒ½ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¿®å‰ªå®éªŒä¸­ä¹‹ä¸€ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒFARMSå°†LLaMA-7Bæ¨¡å‹çš„å›°æƒ‘åº¦é™ä½äº†17.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06280v1">PDF</a> 30 pages, 14 figures, published to ICML 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é€šè¿‡æƒé‡çŸ©é˜µçš„é¢‘è°±å¯¹æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è¿›è¡Œè¯Šæ–­æ˜¯è¿‘å¹´æ¥çš„ç ”ç©¶çƒ­ç‚¹ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡æµ‹é‡æƒé‡çŸ©é˜µçš„å®è¯è°±å¯†åº¦ï¼ˆESDï¼‰çš„é‡å°¾ç¨‹åº¦æ¥åˆ†æDNNã€‚ç„¶è€Œï¼Œå½“é¢ä¸´ä¸åŒå°ºå¯¸å’Œçºµæ¨ªæ¯”çš„çŸ©é˜µæ—¶ï¼Œé‡å°¾åº¦é‡ä¼šå‡ºç°åå·®ï¼Œå¯¼è‡´æ¨¡å‹è¯Šæ–­å’Œé€å±‚è¶…å‚æ•°åˆ†é…ä¸å‡†ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFARMSï¼ˆå›ºå®šçºµæ¨ªæ¯”çŸ©é˜µå­é‡‡æ ·ï¼‰çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡å­é‡‡æ ·å…·æœ‰å›ºå®šçºµæ¨ªæ¯”çš„å­çŸ©é˜µæ¥å½’ä¸€åŒ–æƒé‡çŸ©é˜µï¼Œå¹¶æµ‹é‡è¿™äº›å­çŸ©é˜µçš„å¹³å‡ESDçš„é‡å°¾ç¨‹åº¦ï¼Œä»è€Œæœ‰æ•ˆåœ°å‡è½»çºµæ¨ªæ¯”åå·®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ¨¡å‹ã€ç§‘å­¦æœºå™¨å­¦ä¹ ï¼ˆSciMLï¼‰æ¨¡å‹è®­ç»ƒè¿˜æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‰ªæç­‰é¢†åŸŸï¼Œè¯¥æ–¹æ³•éƒ½èƒ½æé«˜é¢‘è°±åˆ†æå‡†ç¡®æ€§ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°è¿›è¡Œé€å±‚è¶…å‚æ•°åˆ†é…ã€‚åœ¨LLMå‰ªæå®éªŒä¸­ï¼Œä¸ç°æœ‰æœ€ä½³æ–¹æ³•ç›¸æ¯”ï¼ŒFARMSå¯å°†LLaMA-7Bæ¨¡å‹çš„å›°æƒ‘åº¦é™ä½17.3%ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>é€šè¿‡æƒé‡çŸ©é˜µçš„é¢‘è°±åˆ†æå¯¹DNNè¿›è¡Œè¯Šæ–­æ˜¯å½“å‰çš„çƒ­é—¨ç ”ç©¶é¢†åŸŸã€‚</li>
<li>eigenspectrumåˆ†æé€šè¿‡æµ‹é‡æƒé‡çŸ©é˜µçš„å®è¯è°±å¯†åº¦ï¼ˆESDï¼‰çš„é‡å°¾ç¨‹åº¦æ¥äº†è§£æ¨¡å‹çš„è®­ç»ƒæƒ…å†µå¹¶æŒ‡å¯¼é€å±‚è¶…å‚æ•°åˆ†é…ã€‚</li>
<li>ä¸åŒå°ºå¯¸å’Œçºµæ¨ªæ¯”çš„çŸ©é˜µåœ¨ä¼°è®¡é‡å°¾åº¦é‡æ—¶ä¼šäº§ç”Ÿåå·®ï¼Œå½±å“æ¨¡å‹è¯Šæ–­å’Œé€å±‚è¶…å‚æ•°åˆ†é…çš„å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºFARMSçš„æ–¹æ³•ï¼Œé€šè¿‡å­é‡‡æ ·å…·æœ‰å›ºå®šçºµæ¨ªæ¯”çš„å­çŸ©é˜µæ¥å½’ä¸€åŒ–æƒé‡çŸ©é˜µï¼Œä»¥å‡è½»çºµæ¨ªæ¯”åå·®ã€‚</li>
<li>FARMSæ–¹æ³•ä¸ä»…æé«˜äº†é¢‘è°±åˆ†æçš„å‡†ç¡®æ€§ï¼Œè€Œä¸”åœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸï¼ˆå¦‚è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€ç§‘å­¦æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå’Œè¯­è¨€æ¨¡å‹å‰ªæï¼‰ä¸­å®ç°äº†æ›´æœ‰æ•ˆçš„é€å±‚è¶…å‚æ•°åˆ†é…ã€‚</li>
<li>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å‰ªæå®éªŒä¸­ï¼ŒFARMSç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ˜¾è‘—é™ä½äº†æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06280">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0558153666b613a484fd26eadb1bdb3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb93cb2f37944f3446b8601420d0187d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-001c1a70bb59a134a38805195fc11963.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoMemo-LVLMs-Need-Image-Context-with-Image-Memory"><a href="#CoMemo-LVLMs-Need-Image-Context-with-Image-Memory" class="headerlink" title="CoMemo: LVLMs Need Image Context with Image Memory"></a>CoMemo: LVLMs Need Image Context with Image Memory</h2><p><strong>Authors:Shi Liu, Weijie Su, Xizhou Zhu, Wenhai Wang, Jifeng Dai</strong></p>
<p>Recent advancements in Large Vision-Language Models built upon Large Language Models have established aligning visual features with LLM representations as the dominant paradigm. However, inherited LLM architectural designs introduce suboptimal characteristics for multimodal processing. First, LVLMs exhibit a bimodal distribution in attention allocation, leading to the progressive neglect of middle visual content as context expands. Second, conventional positional encoding schemes fail to preserve vital 2D structural relationships when processing dynamic high-resolution images. To address these limitations, we propose CoMemo - a dual-path architecture that combines a Context image path with an image Memory path for visual processing, effectively alleviating visual information neglect. Additionally, we introduce RoPE-DHR, a novel positional encoding mechanism that employs thumbnail-based positional aggregation to maintain 2D spatial awareness while mitigating remote decay in extended sequences. Evaluations across seven benchmarks,including long-context comprehension, multi-image reasoning, and visual question answering, demonstrate CoMemoâ€™s superior performance compared to conventional LVLM architectures. Project page is available at <a target="_blank" rel="noopener" href="https://lalbj.github.io/projects/CoMemo/">https://lalbj.github.io/projects/CoMemo/</a>. </p>
<blockquote>
<p>æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰çš„è¿›æ­¥å·²ç»å»ºç«‹äº†å°†è§†è§‰ç‰¹å¾ä¸LLMè¡¨ç¤ºå¯¹é½çš„ä¸»å¯¼èŒƒå¼ã€‚ç„¶è€Œï¼Œç»§æ‰¿çš„LLMæ¶æ„è®¾è®¡ä¸ºå¤šæ¨¡æ€å¤„ç†å¼•å…¥äº†æ¬¡ä¼˜ç‰¹æ€§ã€‚é¦–å…ˆï¼ŒLVLMåœ¨æ³¨æ„åŠ›åˆ†é…ä¸Šè¡¨ç°å‡ºåŒå³°åˆ†å¸ƒï¼Œéšç€ä¸Šä¸‹æ–‡æ‰©å±•ï¼Œé€æ¸å¿½è§†ä¸­é—´è§†è§‰å†…å®¹ã€‚å…¶æ¬¡ï¼Œä¼ ç»Ÿçš„ä½ç½®ç¼–ç æ–¹æ¡ˆåœ¨å¤„ç†åŠ¨æ€é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶ï¼Œæ— æ³•ä¿æŒé‡è¦çš„äºŒç»´ç»“æ„å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†CoMemoâ€”â€”ä¸€ç§ç»“åˆä¸Šä¸‹æ–‡å›¾åƒè·¯å¾„å’Œå›¾åƒè®°å¿†è·¯å¾„è¿›è¡Œè§†è§‰å¤„ç†çš„åŒè·¯å¾„æ¶æ„ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†è§†è§‰ä¿¡æ¯è¢«å¿½è§†çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†RoPE-DHRï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä½ç½®ç¼–ç æœºåˆ¶ï¼Œé‡‡ç”¨åŸºäºç¼©ç•¥å›¾çš„ä½ç½®èšåˆï¼Œä»¥ç»´æŒäºŒç»´ç©ºé—´æ„ŸçŸ¥ï¼ŒåŒæ—¶ç¼“è§£é•¿åºåˆ—ä¸­çš„è¿œç¨‹è¡°å‡é—®é¢˜ã€‚åœ¨åŒ…æ‹¬é•¿ä¸Šä¸‹æ–‡ç†è§£ã€å¤šå›¾åƒæ¨ç†å’Œè§†è§‰é—®ç­”ç­‰ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ï¼Œå±•ç¤ºäº†CoMemoç›¸è¾ƒäºä¼ ç»ŸLVLMæ¶æ„çš„å“è¶Šæ€§èƒ½ã€‚é¡¹ç›®é¡µé¢å¯åœ¨[<a target="_blank" rel="noopener" href="https://lalbj.github.io/projects/CoMemo/]%E4%B8%8A%E6%9F%A5%E7%9C%8B%E3%80%82">https://lalbj.github.io/projects/CoMemo/]ä¸ŠæŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06279v1">PDF</a> ICML 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹å·²ç»å»ºç«‹äº†ä¸€ç§ä¸»æµèŒƒå¼ï¼Œå³é€šè¿‡èåˆè§†è§‰ç‰¹å¾ä¸LLMè¡¨ç¤ºæ¥å¯¹é½ä¿¡æ¯ã€‚ç„¶è€Œï¼Œç»§æ‰¿çš„LLMæ¶æ„è®¾è®¡å¯¹äºå¤šæ¨¡æ€å¤„ç†è¡¨ç°å‡ºæ¬¡ä¼˜ç‰¹æ€§ã€‚LVLMåœ¨æ³¨æ„åŠ›åˆ†é…ä¸Šå±•ç°å‡ºåŒå³°åˆ†å¸ƒæ¨¡å¼ï¼Œéšç€è¯­å¢ƒæ‰©å±•ï¼Œä¼šé€æ¸å¿½ç•¥ä¸­é—´è§†è§‰å†…å®¹ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„å®šä½ç¼–ç æ–¹æ¡ˆåœ¨å¤„ç†åŠ¨æ€é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶æ— æ³•ä¿æŒå…³é”®çš„äºŒç»´ç»“æ„å…³ç³»ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoMemoâ€”â€”ä¸€ç§ç»“åˆä¸Šä¸‹æ–‡å›¾åƒè·¯å¾„å’Œå›¾åƒè®°å¿†è·¯å¾„çš„åŒè·¯å¾„æ¶æ„è¿›è¡Œè§†è§‰å¤„ç†ï¼Œæœ‰æ•ˆå‡è½»äº†è§†è§‰ä¿¡æ¯å¿½ç•¥é—®é¢˜ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†RoPE-DHRè¿™ä¸€æ–°å‹å®šä½ç¼–ç æœºåˆ¶ï¼Œé€šè¿‡ç¼©ç•¥å›¾å®šä½èšåˆä¿æŒäºŒç»´ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å¹¶å‡è½»é•¿æœŸåºåˆ—ä¸­çš„è¿œç¨‹è¡°å‡é—®é¢˜ã€‚åœ¨åŒ…æ‹¬é•¿è¯­å¢ƒç†è§£ã€å¤šå›¾åƒæ¨ç†å’Œè§†è§‰é—®ç­”ç­‰ä¸ƒä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCoMemoç›¸è¾ƒäºä¼ ç»ŸLVLMæ¶æ„å…·æœ‰å“è¶Šæ€§èƒ½ã€‚é¡¹ç›®é¡µé¢å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://lalbj.github.io/projects/CoMemo/">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰å·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œé€šè¿‡èåˆè§†è§‰ç‰¹å¾å’Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç¤ºè¿›è¡Œä¿¡æ¯å¯¹é½ã€‚</li>
<li>ç°æœ‰LLMæ¶æ„å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šæ³¨æ„åŠ›åˆ†é…çš„â€œåŒå³°åˆ†å¸ƒâ€ï¼Œä»¥åŠéšç€è¯­å¢ƒæ‰©å±•å¯¹ä¸­é—´è§†è§‰å†…å®¹çš„å¿½è§†ï¼›ä¼ ç»Ÿå®šä½ç¼–ç æ–¹æ¡ˆåœ¨å¤„ç†åŠ¨æ€é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶æ— æ³•ä¿æŒå…³é”®çš„äºŒç»´ç»“æ„å…³ç³»ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†CoMemoåŒè·¯å¾„æ¶æ„å’ŒRoPE-DHRå®šä½ç¼–ç æœºåˆ¶ã€‚CoMemoç»“åˆäº†ä¸Šä¸‹æ–‡å›¾åƒè·¯å¾„å’Œå›¾åƒè®°å¿†è·¯å¾„ï¼Œæœ‰æ•ˆå‡è½»è§†è§‰ä¿¡æ¯å¿½ç•¥é—®é¢˜ï¼›RoPE-DHRåˆ™é€šè¿‡ç¼©ç•¥å›¾å®šä½èšåˆä¿æŒäºŒç»´ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›å¹¶å‡è½»è¿œç¨‹è¡°å‡é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e0f2974980efb8ad9ff2ddda9237053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a3c32ae4ed34e1fb22cf5e546ded6b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d8a2d98acc94a1481a276718a50f5a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fd5ad2db4cbb6ffa664cd7436c0a103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-288d99cd78dcc03e2ceabd86bc22ec9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f32874756211ac379273c004d2d82a86.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PersonaAgent-When-Large-Language-Model-Agents-Meet-Personalization-at-Test-Time"><a href="#PersonaAgent-When-Large-Language-Model-Agents-Meet-Personalization-at-Test-Time" class="headerlink" title="PersonaAgent: When Large Language Model Agents Meet Personalization at   Test Time"></a>PersonaAgent: When Large Language Model Agents Meet Personalization at   Test Time</h2><p><strong>Authors:Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, Xian Li</strong></p>
<p>Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to usersâ€™ varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences. </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹èƒ½çš„ä»£ç†æ¶Œç°ä¸ºå…ˆè¿›èŒƒå¼ï¼Œåœ¨å¹¿æ³›é¢†åŸŸå’Œä»»åŠ¡ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬å…·æœ‰æ½œåŠ›ï¼Œä½†å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†é€šå¸¸é‡‡ç”¨ä¸€åˆ€åˆ‡çš„æ–¹æ³•ï¼Œç¼ºä¹åº”å¯¹ç”¨æˆ·ä¸åŒéœ€æ±‚å’Œåå¥½çš„çµæ´»æ€§ã€‚è¿™ä¸€å±€é™æ€§ä¿ƒä½¿æˆ‘ä»¬å¼€å‘PersonaAgentï¼Œé¦–ä¸ªè®¾è®¡ç”¨äºå¤„ç†å¤šæ ·åŒ–ä¸ªæ€§åŒ–ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒPersonaAgenté›†æˆäº†ä¸¤ä¸ªäº’è¡¥çš„ç»„ä»¶ï¼šåŒ…æ‹¬äº‹ä»¶è®°å¿†å’Œè¯­ä¹‰è®°å¿†æœºåˆ¶çš„ä¸ªäººåŒ–è®°å¿†æ¨¡å—ï¼›ä½¿ä»£ç†èƒ½å¤Ÿæ‰§è¡Œé’ˆå¯¹ç”¨æˆ·å®šåˆ¶çš„å·¥å…·åŠ¨ä½œçš„ä¸ªäººåŒ–è¡ŒåŠ¨æ¨¡å—ã€‚å…¶æ ¸å¿ƒäººæ ¼ï¼ˆå®šä¹‰ä¸ºæ¯ä¸ªç”¨æˆ·çš„ç‹¬ç‰¹ç³»ç»Ÿæç¤ºï¼‰å……å½“ä¸­ä»‹ï¼šå®ƒåˆ©ç”¨æ¥è‡ªä¸ªæ€§åŒ–è®°å¿†çš„è§è§£æ¥æ§åˆ¶ä»£ç†è¡ŒåŠ¨ï¼Œè€Œè¿™äº›è¡ŒåŠ¨çš„ç»“æœåè¿‡æ¥åˆå®Œå–„è®°å¿†ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æµ‹è¯•æ—¶çš„ç”¨æˆ·åå¥½å¯¹é½ç­–ç•¥ï¼Œæ¨¡æ‹Ÿæœ€è¿‘çš„næ¬¡äº¤äº’ä»¥ä¼˜åŒ–äººæ ¼æç¤ºï¼Œé€šè¿‡æ¨¡æ‹Ÿå“åº”å’ŒçœŸå®å“åº”ä¹‹é—´çš„æ–‡æœ¬æŸå¤±åé¦ˆï¼Œç¡®ä¿å®æ—¶ç”¨æˆ·åå¥½å¯¹é½ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPersonaAgentä¸ä»…æœ‰æ•ˆåœ°ä¸ªæ€§åŒ–è¡ŒåŠ¨ç©ºé—´ï¼Œè€Œä¸”åœ¨æµ‹è¯•æ—¶çš„ç°å®ä¸–ç•Œåº”ç”¨ä¸­å®ç°æ‰©å±•ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–åŸºå‡†æ–¹æ³•ã€‚è¿™äº›ç»“æœçªæ˜¾äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æä¾›å®šåˆ¶ã€åŠ¨æ€ç”¨æˆ·ä½“éªŒæ–¹é¢çš„å¯è¡Œæ€§å’Œæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06254v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šLLMèµ‹èƒ½çš„ä»£ç†å·²å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„è·¨é¢†åŸŸä»»åŠ¡èƒ½åŠ›ï¼Œä½†ç¼ºä¹çµæ´»æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†PersonaAgentï¼Œä¸€ä¸ªä¸ªæ€§åŒ–LLMä»£ç†æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–è®°å¿†æ¨¡å—å’ŒåŠ¨ä½œæ¨¡å—ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆä¼˜åŒ–ä¸ªæ€§åŒ–æç¤ºï¼Œå®ç°å®æ—¶ç”¨æˆ·åå¥½å¯¹é½ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPersonaAgentæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMä»£ç†åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œä½†ç¼ºä¹çµæ´»æ€§åº”å¯¹ç”¨æˆ·ä¸åŒéœ€æ±‚ã€‚</li>
<li>PersonaAgentæ˜¯ä¸€ä¸ªä¸ªæ€§åŒ–LLMä»£ç†æ¡†æ¶ï¼ŒåŒ…å«ä¸ªæ€§åŒ–è®°å¿†å’ŒåŠ¨ä½œæ¨¡å—ã€‚</li>
<li>ä¸ªæ€§åŒ–è®°å¿†æ¨¡å—åŒ…æ‹¬æƒ…æ™¯å’Œè¯­ä¹‰è®°å¿†æœºåˆ¶ã€‚</li>
<li>ä¸ªæ€§åŒ–åŠ¨ä½œæ¨¡å—ä½¿ä»£ç†èƒ½æ‰§è¡Œé’ˆå¯¹ç”¨æˆ·çš„å·¥å…·åŠ¨ä½œã€‚</li>
<li>äººæ ¼ä½œä¸ºç³»ç»Ÿæç¤ºçš„ä¸­ä»‹ï¼Œåˆ©ç”¨ä¸ªæ€§åŒ–è®°å¿†æ§åˆ¶ä»£ç†åŠ¨ä½œï¼ŒåŒæ—¶ä»£ç†åŠ¨ä½œçš„æˆæœä¼šä¼˜åŒ–è®°å¿†ã€‚</li>
<li>PersonaAgenté‡‡ç”¨æµ‹è¯•æ—¶ç”¨æˆ·åå¥½å¯¹é½ç­–ç•¥ï¼Œé€šè¿‡æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆä¼˜åŒ–ä¸ªæ€§åŒ–æç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d32219107555db62f852e98a26aa0390.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f1f51f09d1ad4d55a71484ea686678f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fa7ad1fbfeec0796025879246ba36f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f32a620e1dcc6c1169b3c854b451e2bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c44ff8a7aa701984277e2fd8e778391c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DesignBench-A-Comprehensive-Benchmark-for-MLLM-based-Front-end-Code-Generation"><a href="#DesignBench-A-Comprehensive-Benchmark-for-MLLM-based-Front-end-Code-Generation" class="headerlink" title="DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code   Generation"></a>DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code   Generation</h2><p><strong>Authors:Jingyu Xiao, Ming Wang, Man Ho Lam, Yuxuan Wan, Junliang Liu, Yintong Huo, Michael R. Lyu</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in automated front-end engineering, e.g., generating UI code from visual designs. However, existing front-end UI code generation benchmarks have the following limitations: (1) While framework-based development becomes predominant in modern front-end programming, current benchmarks fail to incorporate mainstream development frameworks. (2) Existing evaluations focus solely on the UI code generation task, whereas practical UI development involves several iterations, including refining editing, and repairing issues. (3) Current benchmarks employ unidimensional evaluation, lacking investigation into influencing factors like task difficulty, input context variations, and in-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a multi-framework, multi-task evaluation benchmark for assessing MLLMsâ€™ capabilities in automated front-end engineering. DesignBench encompasses three widely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML&#x2F;CSS, and evaluates on three essential front-end tasks (generation, edit, and repair) in real-world development workflows. DesignBench contains 900 webpage samples spanning over 11 topics, 9 edit types, and 6 issue categories, enabling detailed analysis of MLLM performance across multiple dimensions. Our systematic evaluation reveals critical insights into MLLMsâ€™ framework-specific limitations, task-related bottlenecks, and performance variations under different conditions, providing guidance for future research in automated front-end development. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/WebPAI/DesignBench">https://github.com/WebPAI/DesignBench</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å‰ç«¯å·¥ç¨‹é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä¾‹å¦‚ä»è§†è§‰è®¾è®¡ä¸­ç”ŸæˆUIä»£ç ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å‰ç«¯UIä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ï¼šï¼ˆ1ï¼‰è™½ç„¶åŸºäºæ¡†æ¶çš„å¼€å‘åœ¨ç°ä»£å‰ç«¯ç¼–ç¨‹ä¸­æˆä¸ºä¸»æµï¼Œä½†å½“å‰åŸºå‡†æµ‹è¯•æœªèƒ½èå…¥ä¸»æµå¼€å‘æ¡†æ¶ã€‚ï¼ˆ2ï¼‰ç°æœ‰è¯„ä¼°ä»…ä¸“æ³¨äºUIä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œè€Œå®é™…UIå¼€å‘æ¶‰åŠå¤šæ¬¡è¿­ä»£ï¼ŒåŒ…æ‹¬ç²¾ç»†ç¼–è¾‘å’Œä¿®å¤é—®é¢˜ã€‚ï¼ˆ3ï¼‰å½“å‰åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¸€ç»´è¯„ä¼°ï¼Œç¼ºä¹ä»»åŠ¡éš¾åº¦ã€è¾“å…¥ä¸Šä¸‹æ–‡å˜åŒ–ç­‰å½±å“å› ç´ çš„æ·±å…¥æ¢ç©¶å’Œä»£ç å±‚é¢çš„æ·±å…¥åˆ†æã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†DesignBenchï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsåœ¨è‡ªåŠ¨åŒ–å‰ç«¯å·¥ç¨‹é¢†åŸŸèƒ½åŠ›çš„æ–°åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ”¯æŒå¤šæ¡†æ¶å¤šä»»åŠ¡ã€‚DesignBenchæ¶µç›–äº†ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„UIæ¡†æ¶ï¼ˆReactã€Vueå’ŒAngularï¼‰ï¼Œä»¥åŠçº¯HTML&#x2F;CSSï¼Œå¹¶è¯„ä¼°äº†çœŸå®ä¸–ç•Œå¼€å‘å·¥ä½œæµç¨‹ä¸­çš„ä¸‰ä¸ªåŸºæœ¬ä»»åŠ¡ï¼ˆç”Ÿæˆã€ç¼–è¾‘å’Œä¿®å¤ï¼‰ã€‚DesignBenchåŒ…å«900ä¸ªç½‘é¡µæ ·æœ¬ï¼Œæ¶µç›–11ä¸ªä¸»é¢˜ã€9ç§ç¼–è¾‘ç±»å‹å’Œ6ä¸ªé—®é¢˜ç±»åˆ«ï¼Œå¯å¯¹MLLMåœ¨å¤šä¸ªç»´åº¦ä¸Šçš„æ€§èƒ½è¿›è¡Œè¯¦å°½åˆ†æã€‚æˆ‘ä»¬çš„ç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†MLLMsåœ¨ç‰¹å®šæ¡†æ¶ä¸‹çš„å±€é™æ€§ã€ä»»åŠ¡ç›¸å…³ç“¶é¢ˆä»¥åŠä¸åŒæ¡ä»¶ä¸‹çš„æ€§èƒ½å˜åŒ–ï¼Œä¸ºè‡ªåŠ¨åŒ–å‰ç«¯å¼€å‘çš„æœªæ¥ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WebPAI/DesignBench%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WebPAI/DesignBenchæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06251v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–å‰ç«¯å·¥ç¨‹ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”ŸæˆUIä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å‰ç«¯UIä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•å­˜åœ¨ä»¥ä¸‹å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†DesignBenchè¿™ä¸€è·¨å¤šæ¡†æ¶ã€å¤šä»»åŠ¡è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°MLLMsåœ¨è‡ªåŠ¨åŒ–å‰ç«¯å·¥ç¨‹ä¸­çš„èƒ½åŠ›ã€‚DesignBenchæ¶µç›–äº†ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„UIæ¡†æ¶ï¼ˆReactã€Vueå’ŒAngularï¼‰ï¼ŒåŒæ—¶å¯¹çœŸå®ä¸–ç•Œå¼€å‘æµç¨‹ä¸­çš„ä¸‰ä¸ªå…³é”®å‰ç«¯ä»»åŠ¡ï¼ˆç”Ÿæˆã€ç¼–è¾‘å’Œä¿®å¤ï¼‰è¿›è¡Œè¯„ä¼°ã€‚è¯¥ç ”ç©¶ä¸ºæˆ‘ä»¬æ·±å…¥äº†è§£äº†MLLMsåœ¨ç‰¹å®šæ¡†æ¶ä¸‹çš„å±€é™æ€§ã€ä»»åŠ¡ç›¸å…³ç“¶é¢ˆä»¥åŠä¸åŒæ¡ä»¶ä¸‹çš„æ€§èƒ½å˜åŒ–æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è‡ªåŠ¨åŒ–å‰ç«¯å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯ç”ŸæˆUIä»£ç æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å½“å‰çš„å‰ç«¯UIä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•å­˜åœ¨å±€é™æ€§ï¼Œæœªèƒ½èå…¥ä¸»æµå¼€å‘æ¡†æ¶ã€‚</li>
<li>å®ç”¨UIå¼€å‘åŒ…å«å¤šæ¬¡è¿­ä»£ï¼Œå¦‚ä¿®æ”¹å’Œä¿®å¤é—®é¢˜ï¼Œè€Œç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨UIä»£ç ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¸€ç»´è¯„ä¼°ï¼Œç¼ºä¹ä»»åŠ¡éš¾åº¦ã€è¾“å…¥ä¸Šä¸‹æ–‡å˜åŒ–ç­‰å› ç´ çš„æ·±å…¥ç ”ç©¶ã€‚</li>
<li>DesignBenchåŸºå‡†æ¶µç›–äº†ä¸‰ä¸ªæµè¡Œçš„UIæ¡†æ¶å’ŒçœŸå®ä¸–ç•Œå¼€å‘æµç¨‹ä¸­çš„ä¸‰ä¸ªå…³é”®å‰ç«¯ä»»åŠ¡ã€‚</li>
<li>DesignBenchç³»ç»Ÿè¯„ä¼°æ­ç¤ºäº†MLLMsçš„æ¡†æ¶ç‰¹å®šå±€é™æ€§ã€ä»»åŠ¡ç›¸å…³ç“¶é¢ˆä»¥åŠä¸åŒæ¡ä»¶ä¸‹çš„æ€§èƒ½å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06251">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e88e1ad97b4db2e18731b888cd1b9f5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3351bf5956b337117a1967585eaf1fce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-919fdaeb1d963eb6a3be3714e92cb9c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8871962c54d8c9b1e9fae9de8cc2e04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59cf7114d1b44a29e9be987096d3386e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dee4b556fb4746e1b90f8e275700a3d4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Visual-Graph-Arena-Evaluating-Visual-Conceptualization-of-Vision-and-Multimodal-Large-Language-Models"><a href="#Visual-Graph-Arena-Evaluating-Visual-Conceptualization-of-Vision-and-Multimodal-Large-Language-Models" class="headerlink" title="Visual Graph Arena: Evaluating Visual Conceptualization of Vision and   Multimodal Large Language Models"></a>Visual Graph Arena: Evaluating Visual Conceptualization of Vision and   Multimodal Large Language Models</h2><p><strong>Authors:Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu</strong></p>
<p>Recent advancements in multimodal large language models have driven breakthroughs in visual question answering. Yet, a critical gap persists, &#96;conceptualizationâ€™-the ability to recognize and reason about the same concept despite variations in visual form, a basic ability of human reasoning. To address this challenge, we introduce the Visual Graph Arena (VGA), a dataset featuring six graph-based tasks designed to evaluate and improve AI systemsâ€™ capacity for visual abstraction. VGA uses diverse graph layouts (e.g., Kamada-Kawai vs. planar) to test reasoning independent of visual form. Experiments with state-of-the-art vision models and multimodal LLMs reveal a striking divide: humans achieved near-perfect accuracy across tasks, while models totally failed on isomorphism detection and showed limited success in path&#x2F;cycle tasks. We further identify behavioral anomalies suggesting pseudo-intelligent pattern matching rather than genuine understanding. These findings underscore fundamental limitations in current AI models for visual understanding. By isolating the challenge of representation-invariant reasoning, the VGA provides a framework to drive progress toward human-like conceptualization in AI visual models. The Visual Graph Arena is available at: \href{<a target="_blank" rel="noopener" href="https://vga.csail.mit.edu/%7D%7Bvga.csail.mit.edu%7D">https://vga.csail.mit.edu/}{vga.csail.mit.edu}</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•æ¨åŠ¨äº†è§†è§‰é—®ç­”çš„çªç ´ã€‚ç„¶è€Œï¼Œä»ç„¶å­˜åœ¨ä¸€ä¸ªå…³é”®å·®è·ï¼Œå³â€œæ¦‚å¿µåŒ–â€â€”â€”å³ä½¿è§†è§‰å½¢å¼å­˜åœ¨å˜åŒ–ï¼Œä¹Ÿèƒ½è¯†åˆ«å’Œæ¨ç†åŒä¸€æ¦‚å¿µçš„èƒ½åŠ›ï¼Œè¿™æ˜¯äººç±»æ¨ç†çš„åŸºæœ¬èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰å›¾ç«æŠ€åœºï¼ˆVGAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å…­ç§åŸºäºå›¾çš„ä»»åŠ¡ä¸ºç‰¹è‰²çš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜AIç³»ç»Ÿåœ¨è§†è§‰æŠ½è±¡æ–¹é¢çš„èƒ½åŠ›ã€‚VGAä½¿ç”¨å„ç§å›¾å½¢å¸ƒå±€ï¼ˆä¾‹å¦‚ï¼ŒKamada-Kawaiå’Œå¹³é¢å¸ƒå±€ï¼‰æ¥æµ‹è¯•ç‹¬ç«‹äºè§†è§‰å½¢å¼çš„æ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„è§†è§‰æ¨¡å‹å’Œå¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹çš„å®éªŒæ­ç¤ºäº†ä¸€ä¸ªæƒŠäººçš„å·®è·ï¼šäººç±»åœ¨å„é¡¹ä»»åŠ¡ä¸­å‡ ä¹è¾¾åˆ°äº†å®Œç¾çš„å‡†ç¡®ç‡ï¼Œè€Œæ¨¡å‹åœ¨åŒæ„æ£€æµ‹ä¸Šå®Œå…¨å¤±è´¥ï¼Œå¹¶åœ¨è·¯å¾„&#x2F;å¾ªç¯ä»»åŠ¡ä¸­å–å¾—äº†æœ‰é™çš„æˆåŠŸã€‚æˆ‘ä»¬è¿˜å‘ç°äº†è¡Œä¸ºå¼‚å¸¸ï¼Œè¿™è¡¨æ˜æ˜¯ä¼ªæ™ºèƒ½çš„æ¨¡å¼åŒ¹é…è€Œä¸æ˜¯çœŸæ­£çš„ç†è§£ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å½“å‰AIæ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ã€‚é€šè¿‡è§£å†³è¡¨ç¤ºä¸å˜æ¨ç†çš„æŒ‘æˆ˜ï¼ŒVGAæä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œä»¥æ¨åŠ¨äººå·¥æ™ºèƒ½è§†è§‰æ¨¡å‹å‘äººç±»æ¦‚å¿µåŒ–æ–¹å‘å‘å±•ã€‚è§†è§‰å›¾ç«æŠ€åœºå¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://vga.csail.mit.edu/">vga.csail.mit.edu</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06242v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•æ¨åŠ¨äº†è§†è§‰é—®ç­”çš„çªç ´ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸€ä¸ªå…³é”®å·®è·ï¼Œå³â€œæ¦‚å¿µåŒ–â€â€”â€”å³ä½¿è§†è§‰å½¢å¼æœ‰å˜åŒ–ï¼Œä¹Ÿèƒ½è¯†åˆ«å’Œæ¨ç†åŒä¸€æ¦‚å¿µçš„èƒ½åŠ›ï¼Œè¿™æ˜¯äººç±»æ¨ç†çš„åŸºæœ¬èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œå¼•å…¥äº†è§†è§‰å›¾åœºï¼ˆVGAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±å…­ä¸ªåŸºäºå›¾å½¢çš„ä»»åŠ¡ç»„æˆçš„æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæé«˜AIç³»ç»Ÿåœ¨è§†è§‰æŠ½è±¡æ–¹é¢çš„èƒ½åŠ›ã€‚VGAä½¿ç”¨å„ç§å›¾å½¢å¸ƒå±€ï¼ˆä¾‹å¦‚ï¼ŒKamada-Kawaiå’Œå¹³é¢å¸ƒå±€ï¼‰æ¥æµ‹è¯•ç‹¬ç«‹äºè§†è§‰å½¢å¼çš„æ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨æœ€æ–°å‰æ²¿çš„è§†è§‰æ¨¡å‹å’Œè·¨æ¨¡æ€LLMè¿›è¡Œçš„å®éªŒæ˜¾ç¤ºï¼Œäººç±»åœ¨å„é¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ¥è¿‘å®Œç¾çš„å‡†ç¡®ç‡ï¼Œè€Œæ¨¡å‹åœ¨æ£€æµ‹åŒæ„æ–¹é¢å®Œå…¨å¤±è´¥ï¼Œå¹¶ä¸”åœ¨è·¯å¾„&#x2F;å¾ªç¯ä»»åŠ¡ä¸­å–å¾—æœ‰é™æˆåŠŸã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å½“å‰AIæ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ã€‚é€šè¿‡è§£å†³è¡¨ç¤ºä¸å˜æ¨ç†çš„æŒ‘æˆ˜ï¼ŒVGAæä¾›äº†ä¸€ä¸ªæ¨åŠ¨AIè§†è§‰æ¨¡å‹å®ç°ç±»ä¼¼äººç±»çš„æ¦‚å¿µåŒ–çš„æ¡†æ¶ã€‚<strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²æ¨åŠ¨è§†è§‰é—®ç­”é¢†åŸŸçš„çªç ´ã€‚</li>
<li>â€œæ¦‚å¿µåŒ–â€ä»æ˜¯AIçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒæŒ‡çš„æ˜¯å³ä½¿è§†è§‰å½¢å¼å˜åŒ–ï¼Œä¹Ÿèƒ½è¯†åˆ«å’Œæ¨ç†åŒä¸€æ¦‚å¿µçš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è§†è§‰å›¾åœºï¼ˆVGAï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å…­ä¸ªåŸºäºå›¾å½¢çš„ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ”¹è¿›AIåœ¨è§†è§‰æŠ½è±¡æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>VGAä½¿ç”¨ä¸åŒçš„å›¾å½¢å¸ƒå±€æ¥æµ‹è¯•ç‹¬ç«‹äºè§†è§‰å½¢å¼çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œäººç±»åœ¨VGAä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡æ¥è¿‘å®Œç¾ï¼Œè€Œå½“å‰AIæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æœ‰é™æˆ–å¤±è´¥ã€‚</li>
<li>å½“å‰AIæ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢å­˜åœ¨æ ¹æœ¬å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8fb715ef43575a6b640af6255822743.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd2fed38a4fc25304e279ebfb821b179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4e2889a5f7251ab1b514d3a471cc3d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e5fd45164b4b07fcc602b4a88a8e4e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a8de7da1eea9905e8d3bbea629a156a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43efa62e63abed114e97043ec5cdebec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f33e37df23f76cd4f65afcb4a241a0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff7376e2412d2ba1c63f7fdc1d4cc96c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6290fafbcd2251173b90639fa3b98f31.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-Lock-in-Hypothesis-Stagnation-by-Algorithm"><a href="#The-Lock-in-Hypothesis-Stagnation-by-Algorithm" class="headerlink" title="The Lock-in Hypothesis: Stagnation by Algorithm"></a>The Lock-in Hypothesis: Stagnation by Algorithm</h2><p><strong>Authors:Tianyi Alex Qiu, Zhonghao He, Tejasveer Chugh, Max Kleiman-Weiner</strong></p>
<p>The training and deployment of large language models (LLMs) create a feedback loop with human users: models learn human beliefs from data, reinforce these beliefs with generated content, reabsorb the reinforced beliefs, and feed them back to users again and again. This dynamic resembles an echo chamber. We hypothesize that this feedback loop entrenches the existing values and beliefs of users, leading to a loss of diversity and potentially the lock-in of false beliefs. We formalize this hypothesis and test it empirically with agent-based LLM simulations and real-world GPT usage data. Analysis reveals sudden but sustained drops in diversity after the release of new GPT iterations, consistent with the hypothesized human-AI feedback loop. Code and data available at <a target="_blank" rel="noopener" href="https://thelockinhypothesis.com/">https://thelockinhypothesis.com</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå’Œéƒ¨ç½²ä¸äººç±»ç”¨æˆ·å½¢æˆäº†ä¸€ä¸ªåé¦ˆå¾ªç¯ï¼šæ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ äººç±»ä¿¡å¿µï¼Œé€šè¿‡ç”Ÿæˆå†…å®¹å¼ºåŒ–è¿™äº›ä¿¡å¿µï¼Œé‡æ–°å¸æ”¶å¼ºåŒ–çš„ä¿¡å¿µï¼Œç„¶åä¸€æ¬¡åˆä¸€æ¬¡åœ°åé¦ˆç»™äººç±»ç”¨æˆ·ã€‚è¿™ç§åŠ¨æ€ç±»ä¼¼äºå›å£°å®¤ã€‚æˆ‘ä»¬å‡è®¾è¿™ç§åé¦ˆå¾ªç¯ä½¿ç°æœ‰ç”¨æˆ·çš„ä»·å€¼è§‚å’Œä¿¡å¿µæ ¹æ·±è’‚å›ºï¼Œå¯¼è‡´å¤šæ ·æ€§ä¸§å¤±ï¼Œå¹¶å¯èƒ½é”å®šé”™è¯¯çš„ä¿¡å¿µã€‚æˆ‘ä»¬ä»¥å®è¯çš„æ–¹å¼é€šè¿‡åŸºäºä»£ç†çš„LLMæ¨¡æ‹Ÿå’Œç°å®ä¸–ç•ŒGPTä½¿ç”¨æ•°æ®å¯¹è¿™ç§å‡è®¾è¿›è¡Œäº†æµ‹è¯•ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨æ–°çš„GPTè¿­ä»£å‘å¸ƒåï¼Œå¤šæ ·æ€§ä¼šå‡ºç°çªç„¶è€ŒæŒç»­çš„ä¸‹é™ï¼Œè¿™ä¸å‡è®¾çš„äººç±»-äººå·¥æ™ºèƒ½åé¦ˆå¾ªç¯ä¸€è‡´ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://thelockinhypothesis.comæ‰¾åˆ°./">https://thelockinhypothesis.comæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06166v1">PDF</a> ICML 2025, 46 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒä¸éƒ¨ç½²å½¢æˆäº†ä¸€ä¸ªä¸äººç±»ç”¨æˆ·äº’åŠ¨çš„åé¦ˆå¾ªç¯ï¼šæ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ äººç±»ä¿¡ä»°ï¼Œé€šè¿‡ç”Ÿæˆå†…å®¹å¼ºåŒ–è¿™äº›ä¿¡ä»°ï¼Œé‡æ–°å¸æ”¶å¼ºåŒ–åçš„ä¿¡ä»°ï¼Œç„¶åå†æ¬¡åé¦ˆç»™äººç±»ç”¨æˆ·ã€‚è¿™ä¸€è¿‡ç¨‹ç±»ä¼¼äºå›å£°å®¤æ•ˆåº”ï¼Œå‡è®¾è¿™ç§åé¦ˆå¾ªç¯åŠ æ·±äº†ç”¨æˆ·ç°æœ‰çš„ä»·å€¼è§‚å’Œä¿¡ä»°ï¼Œå¯¼è‡´å¤šæ ·æ€§ä¸§å¤±ï¼Œå¹¶å¯èƒ½é”å®šé”™è¯¯çš„ä¿¡ä»°ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºä»£ç†çš„LLMæ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„GPTä½¿ç”¨æ•°æ®å®è¯æ£€éªŒäº†è¿™ä¸€å‡è®¾ï¼Œåˆ†ææ˜¾ç¤ºæ–°GPTç‰ˆæœ¬å‘å¸ƒåå¤šæ ·æ€§å‡ºç°çªç„¶ä½†æŒç»­çš„ä¸‹é™ï¼Œè¿™ä¸å‡è®¾çš„äººæœºåé¦ˆå¾ªç¯ç›¸ä¸€è‡´ã€‚è¯¦æƒ…å¯è®¿é—®thelockinhypothesis.comäº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså½¢æˆäººæœºäº’åŠ¨åé¦ˆå¾ªç¯ã€‚</li>
<li>LLMsä»æ•°æ®ä¸­å­¦ä¹ äººç±»ä¿¡ä»°å¹¶é€šè¿‡ç”Ÿæˆå†…å®¹å¼ºåŒ–è¿™äº›ä¿¡ä»°ã€‚</li>
<li>åé¦ˆå¾ªç¯å¯èƒ½å¯¼è‡´ä»·å€¼è§‚å’Œä¿¡ä»°çš„æ·±åŒ–ï¼Œå¼•å‘å¤šæ ·æ€§ä¸§å¤±ã€‚</li>
<li>å­˜åœ¨é”å®šé”™è¯¯ä¿¡ä»°çš„é£é™©ã€‚</li>
<li>é€šè¿‡ä»£ç†æ¨¡æ‹Ÿå’ŒçœŸå®GPTæ•°æ®å®è¯æ£€éªŒäº†ä¸Šè¿°å‡è®¾ã€‚</li>
<li>æ–°GPTç‰ˆæœ¬å‘å¸ƒåï¼Œå¤šæ ·æ€§å‡ºç°çªç„¶ä½†æŒç»­çš„ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-067de0b0d4437037ff18f5a45024866e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f45fc1921f5043fd1a3e5068302b45c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc3fc4cd455db3506423e9ba50eed6db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06b65089647d29033391fc26d55ae6e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c932f186cceb9baf9c799cbb4c100942.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Masked-Language-Models-are-Good-Heterogeneous-Graph-Generalizers"><a href="#Masked-Language-Models-are-Good-Heterogeneous-Graph-Generalizers" class="headerlink" title="Masked Language Models are Good Heterogeneous Graph Generalizers"></a>Masked Language Models are Good Heterogeneous Graph Generalizers</h2><p><strong>Authors:Jinyu Yang, Cheng Yang, Shanyuan Cui, Zeyuan Guo, Liangwei Yang, Muhan Zhang, Chuan Shi</strong></p>
<p>Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. Recently, some researchers have turned to integrating HGNNs with large language models (LLMs) for more generalizable heterogeneous graph learning. However, these approaches typically extract structural information via HGNNs as HG tokens, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLMâ€™s comprehension of HGs. Moreover, as these HG tokens are often derived from node-level tasks, the modelâ€™s ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style â€œmaskâ€ token prediction paradigm. Specifically, MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/MLM4HG">https://github.com/BUPT-GAMMA/MLM4HG</a>. </p>
<blockquote>
<p>å¼‚è´¨å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰æ“…é•¿æ•æ‰å¼‚è´¨å›¾ï¼ˆHGï¼‰ä¸­çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä½†åœ¨è·¨åŸŸå’Œä»»åŠ¡é—´æ³›åŒ–æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æœ€è¿‘ï¼Œä¸€äº›ç ”ç©¶äººå‘˜å°è¯•å°†HGNNsä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆï¼Œä»¥å®ç°æ›´å…·é€šç”¨æ€§çš„å¼‚è´¨å›¾å­¦ä¹ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡HGNNsæå–ç»“æ„ä¿¡æ¯ä½œä¸ºHGæ ‡è®°ï¼ŒHGNNså’ŒLLMä¹‹é—´çš„åµŒå…¥ç©ºé—´å·®å¼‚å·²è¢«è¯æ˜ä¼šåå‘LLMå¯¹HGçš„ç†è§£ã€‚æ­¤å¤–ï¼Œç”±äºè¿™äº›HGæ ‡è®°é€šå¸¸æ¥æºäºèŠ‚ç‚¹çº§ä»»åŠ¡ï¼Œæ¨¡å‹åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„åŸºäºMasked Language Modelingçš„æ–¹æ³•ï¼Œç§°ä¸ºMLM4HGã€‚MLM4HGå¼•å…¥åŸºäºå…ƒè·¯å¾„çš„æ–‡æœ¬åºåˆ—ï¼Œè€Œä¸æ˜¯HGæ ‡è®°ï¼Œä»¥æå–HGä¸­å›ºæœ‰çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶è®¾è®¡å®šåˆ¶çš„æ–‡æœ¬æ¨¡æ¿ï¼Œå°†ä¸åŒçš„å›¾å½¢ä»»åŠ¡ç»Ÿä¸€ä¸ºä¸€ä¸ªè¿è´¯çš„å¡«ç©ºå¼â€œæ©ç â€æ ‡è®°é¢„æµ‹èŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼ŒMLM4HGé¦–å…ˆæ ¹æ®å…ƒè·¯å¾„å°†æ¥è‡ªä¸åŒé¢†åŸŸçš„HGè½¬æ¢ä¸ºæ–‡æœ¬ï¼Œç„¶åå°†å…¶ä¸ç»Ÿä¸€çš„ä»»åŠ¡æ–‡æœ¬ç»“åˆï¼Œå½¢æˆåŸºäºHGçš„è¯­æ–™åº“ã€‚æ­¤å¤–ï¼Œè¯¥è¯­æ–™åº“è¢«è¾“å…¥åˆ°é¢„è®­ç»ƒçš„LMä¸­è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨å—é™åˆ¶çš„ç›®æ ‡è¯æ±‡è¡¨ï¼Œä½¿å¾®è°ƒåçš„LMèƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„ç›®æ ‡HGã€‚åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è·¨åŸŸå’Œå¤šä»»åŠ¡å®éªŒå¹¿æ³›è¯æ˜ï¼Œåœ¨å°‘é‡æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­ï¼ŒMLM4HGçš„æ³›åŒ–æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUPT-GAMMA/MLM4HG%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BUPT-GAMMA/MLM4HGä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼‚æ„å›¾ç¥ç»ç½‘ç»œï¼ˆHGNNsï¼‰æ“…é•¿æ•æ‰å¼‚æ„å›¾ï¼ˆHGï¼‰çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶ï¼Œå®ƒä»¬åœ¨è·¨åŸŸå’Œä»»åŠ¡é—´çš„æ³›åŒ–èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ‰ç ”ç©¶è€…å°è¯•å°†HGNNsä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œè¿›è¡Œæ›´å…·æ³›åŒ–èƒ½åŠ›çš„å¼‚æ„å›¾å­¦ä¹ ã€‚ç„¶è€Œï¼ŒHGNNsä¸LLMsåµŒå…¥ç©ºé—´ä¸­çš„å·®å¼‚å½±å“äº†LLMå¯¹HGçš„ç†è§£ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºMasked Language Modelingçš„æ–¹æ³•ï¼Œåä¸ºMLM4HGã€‚å®ƒé€šè¿‡å…ƒè·¯å¾„æ–‡æœ¬åºåˆ—æå–HGçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å®šåˆ¶æ–‡æœ¬æ¨¡æ¿å°†ä¸åŒå›¾ä»»åŠ¡ç»Ÿä¸€ä¸ºè¿è´¯çš„å¡«å……è¯â€œmaskâ€æ ‡è®°é¢„æµ‹èŒƒå¼ã€‚å®éªŒè¯æ˜ï¼ŒMLM4HGåœ¨è·¨åŸŸå¤šä»»åŠ¡åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HGNNsæ“…é•¿å¤„ç†å¼‚æ„å›¾çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä½†åœ¨è·¨åŸŸå’Œä»»åŠ¡é—´çš„æ³›åŒ–æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç»“åˆHGNNså’ŒLLMsçš„æ–¹æ³•è¢«æå‡ºä»¥è§£å†³æ³›åŒ–é—®é¢˜ï¼Œä½†å­˜åœ¨åµŒå…¥ç©ºé—´å·®å¼‚çš„é—®é¢˜ã€‚</li>
<li>MLM4HGæ–¹æ³•é€šè¿‡å…ƒè·¯å¾„æ–‡æœ¬åºåˆ—æå–HGçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œé¿å…ä½¿ç”¨HG tokensã€‚</li>
<li>MLM4HGä½¿ç”¨å®šåˆ¶æ–‡æœ¬æ¨¡æ¿ç»Ÿä¸€ä¸åŒå›¾ä»»åŠ¡ï¼Œå½¢æˆå¡«å……è¯â€œmaskâ€æ ‡è®°é¢„æµ‹èŒƒå¼ã€‚</li>
<li>MLM4HGå°†å¼‚æ„å›¾è½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œå¹¶ç»“åˆç»Ÿä¸€ä»»åŠ¡æ–‡æœ¬å½¢æˆåŸºäºå¼‚æ„å›¾çš„è¯­æ–™åº“ã€‚</li>
<li>MLM4HGé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæ³›åŒ–åˆ°æœªè§è¿‡çš„å¼‚æ„å›¾ã€‚</li>
<li>åœ¨å››ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„è·¨åŸŸå¤šä»»åŠ¡å®éªŒè¯æ˜MLM4HGåœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31fa2dabe71dd56c8302f460befba4f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-921bc2b43ffa46547f1ed62d745eac82.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Joint-GCG-Unified-Gradient-Based-Poisoning-Attacks-on-Retrieval-Augmented-Generation-Systems"><a href="#Joint-GCG-Unified-Gradient-Based-Poisoning-Attacks-on-Retrieval-Augmented-Generation-Systems" class="headerlink" title="Joint-GCG: Unified Gradient-Based Poisoning Attacks on   Retrieval-Augmented Generation Systems"></a>Joint-GCG: Unified Gradient-Based Poisoning Attacks on   Retrieval-Augmented Generation Systems</h2><p><strong>Authors:Haowei Wang, Rupeng Zhang, Junjie Wang, Mingyang Li, Yuekai Huang, Dandan Wang, Qing Wang</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by retrieving relevant documents from external corpora before generating responses. This approach significantly expands LLM capabilities by leveraging vast, up-to-date external knowledge. However, this reliance on external knowledge makes RAG systems vulnerable to corpus poisoning attacks that manipulate generated outputs via poisoned document injection. Existing poisoning attack strategies typically treat the retrieval and generation stages as disjointed, limiting their effectiveness. We propose Joint-GCG, the first framework to unify gradient-based attacks across both retriever and generator models through three innovations: (1) Cross-Vocabulary Projection for aligning embedding spaces, (2) Gradient Tokenization Alignment for synchronizing token-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically balancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves at most 25% and an average of 5% higher attack success rate than previous methods across multiple retrievers and generators. While optimized under a white-box assumption, the generated poisons show unprecedented transferability to unseen models. Joint-GCGâ€™s innovative unification of gradient-based attacks across retrieval and generation stages fundamentally reshapes our understanding of vulnerabilities within RAG systems. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/NicerWang/Joint-GCG">https://github.com/NicerWang/Joint-GCG</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿé€šè¿‡ä»å¤–éƒ¨è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå“åº”èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•é€šè¿‡åˆ©ç”¨åºå¤§ä¸”æœ€æ–°çš„å¤–éƒ¨çŸ¥è¯†ï¼Œæ˜¾è‘—æ‰©å±•äº†LLMçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹å¤–éƒ¨çŸ¥è¯†çš„ä¾èµ–ä½¿å¾—RAGç³»ç»Ÿå®¹æ˜“å—åˆ°è¯­æ–™åº“ä¸­æ¯’æ”»å‡»ï¼Œåè€…é€šè¿‡æ³¨å…¥æœ‰æ¯’æ–‡æ¡£æ¥æ“çºµç”Ÿæˆè¾“å‡ºã€‚ç°æœ‰çš„ä¸­æ¯’æ”»å‡»ç­–ç•¥é€šå¸¸å°†æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µè§†ä¸ºç›¸äº’ç‹¬ç«‹çš„ï¼Œè¿™é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†Joint-GCGï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€šè¿‡ä¸‰é¡¹åˆ›æ–°ç»Ÿä¸€æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨æ¨¡å‹çš„åŸºäºæ¢¯åº¦çš„æ”»å‡»æ¡†æ¶ï¼šï¼ˆ1ï¼‰è·¨è¯æ±‡æŠ•å½±ä»¥å¯¹é½åµŒå…¥ç©ºé—´ï¼Œï¼ˆ2ï¼‰æ¢¯åº¦ä»¤ç‰ŒåŒ–å¯¹é½ä»¥åŒæ­¥ä»¤ç‰Œçº§æ¢¯åº¦ä¿¡å·ï¼Œä»¥åŠï¼ˆ3ï¼‰è‡ªé€‚åº”åŠ æƒèåˆä»¥åŠ¨æ€å¹³è¡¡æ”»å‡»ç›®æ ‡ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒJoint-GCGåœ¨å¤šä¸ªæ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ä¸Šæœ€é«˜å®ç°äº†é«˜è¾¾25%ã€å¹³å‡æé«˜äº†5%çš„æ”»å‡»æˆåŠŸç‡ã€‚è™½ç„¶åœ¨ç™½ç›’å‡è®¾ä¸‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½†ç”Ÿæˆçš„æ¯’æ–‡åœ¨æœªçŸ¥æ¨¡å‹ä¸Šæ˜¾ç¤ºå‡ºå‰æ‰€æœªæœ‰çš„å¯è½¬ç§»æ€§ã€‚Joint-GCGåˆ›æ–°åœ°ç»Ÿä¸€äº†æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µåŸºäºæ¢¯åº¦çš„æ”»å‡»ï¼Œä»æ ¹æœ¬ä¸Šæ”¹å˜äº†æˆ‘ä»¬å¯¹RAGç³»ç»Ÿå†…æ¼æ´çš„ç†è§£ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NicerWang/Joint-GCG">https://github.com/NicerWang/Joint-GCG</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06151v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RAGç³»ç»Ÿé€šè¿‡ä»å¤–éƒ¨è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§ä¾èµ–å¤–éƒ¨çŸ¥è¯†çš„æ–¹å¼ä½¿å¾—RAGç³»ç»Ÿå®¹æ˜“å—åˆ°è¯­æ–™åº“æ±¡æŸ“æ”»å‡»ã€‚æœ¬æ–‡æå‡ºJoint-GCGæ¡†æ¶ï¼Œé€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯ç»Ÿä¸€æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µçš„æ¢¯åº¦æ”»å‡»ï¼ŒåŒ…æ‹¬è·¨è¯æ±‡æŠ•å½±ã€æ¢¯åº¦ä»¤ç‰ŒåŒ–å¯¹é½å’Œè‡ªé€‚åº”åŠ æƒèåˆã€‚è¯¥æ¡†æ¶å®ç°äº†è¾ƒé«˜çš„æ”»å‡»æˆåŠŸç‡ï¼Œå¹¶å…·æœ‰è¾ƒå¼ºçš„å¯è½¬ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGç³»ç»Ÿé€šè¿‡æ£€ç´¢å¤–éƒ¨ç›¸å…³æ–‡æ¡£å¢å¼ºLLMåŠŸèƒ½ï¼Œä½†è¿™ä¹Ÿä½¿å…¶å®¹æ˜“å—åˆ°è¯­æ–™åº“æ±¡æŸ“æ”»å‡»ã€‚</li>
<li>ç°æœ‰æ±¡æŸ“æ”»å‡»ç­–ç•¥é€šå¸¸å°†æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µè§†ä¸ºç‹¬ç«‹ï¼Œé™åˆ¶äº†å…¶æ•ˆæœã€‚</li>
<li>Joint-GCGæ¡†æ¶é¦–æ¬¡ç»Ÿä¸€äº†æ£€ç´¢å’Œç”Ÿæˆé˜¶æ®µçš„æ¢¯åº¦æ”»å‡»ã€‚</li>
<li>Joint-GCGé€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯å®ç°æ”»å‡»ï¼šè·¨è¯æ±‡æŠ•å½±ã€æ¢¯åº¦ä»¤ç‰ŒåŒ–å¯¹é½å’Œè‡ªé€‚åº”åŠ æƒèåˆã€‚</li>
<li>ä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼ŒJoint-GCGåœ¨å¤šä¸ªæ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ä¸Šå®ç°äº†æœ€é«˜çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>è¯¥æ¡†æ¶ç”Ÿæˆçš„æ¯’ç´ å…·æœ‰å¼ºå¤§çš„å¯è½¬ç§»æ€§ï¼Œå³ä½¿å¯¹æœªè§è¿‡çš„æ¨¡å‹ä¹Ÿèƒ½å‘æŒ¥ä½œç”¨ã€‚</li>
<li>Joint-GCGæ¡†æ¶é‡æ–°è®¤è¯†äº†RAGç³»ç»Ÿä¸­çš„æ¼æ´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa51fba76517045b10e761b3c62452ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eaccf0c7830d1491ab7b279b3277d3a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text-to-LoRA-Instant-Transformer-Adaption"><a href="#Text-to-LoRA-Instant-Transformer-Adaption" class="headerlink" title="Text-to-LoRA: Instant Transformer Adaption"></a>Text-to-LoRA: Instant Transformer Adaption</h2><p><strong>Authors:Rujikorn Charakorn, Edoardo Cetin, Yujin Tang, Robert Tjarko Lange</strong></p>
<p>While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyper-parameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting Large Language Models on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/SakanaAI/text-to-lora">https://github.com/SakanaAI/text-to-lora</a> </p>
<blockquote>
<p>è™½ç„¶åŸºç¡€æ¨¡å‹ä¸ºå¿«é€Ÿå†…å®¹åˆ›å»ºæä¾›äº†é€šç”¨å·¥å…·ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œé€‚åº”ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹éœ€è¦ä»”ç»†ç­›é€‰æ•°æ®é›†å¹¶é‡å¤å¾®è°ƒåŸºç¡€æ¨¡å‹ã€‚å¾®è°ƒæŠ€æœ¯ä½¿ä»ä¸šè€…èƒ½å¤Ÿä¸ºè®¸å¤šæ–°åº”ç”¨é€‚åº”åŸºç¡€æ¨¡å‹ï¼Œä½†éœ€è¦æ˜‚è´µä¸”å†—é•¿çš„è®­ç»ƒï¼ŒåŒæ—¶å¯¹è¶…å‚æ•°é€‰æ‹©éå¸¸æ•æ„Ÿã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Text-to-LoRAï¼ˆT2Lï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»…æ ¹æ®ç›®æ ‡ä»»åŠ¡çš„è‡ªç„¶è¯­è¨€æè¿°å®æ—¶é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ã€‚T2Læ˜¯ä¸€ç§è¶…ç½‘ç»œï¼Œç»è¿‡è®­ç»ƒèƒ½å¤Ÿä¸€æ¬¡æ€§ä½æˆæœ¬å‰å‘ä¼ é€’ä¸­æ„å»ºLoRAsã€‚åœ¨åŸºäº9ä¸ªé¢„è®­ç»ƒLoRAé€‚é…å™¨ï¼ˆGSM8Kã€Arcç­‰ï¼‰çš„å¥—ä»¶ä¸Šè®­ç»ƒT2Låï¼Œæˆ‘ä»¬æ˜¾ç¤ºå‡ºç‰¹å®šé‡æ„çš„LoRAå®ä¾‹åœ¨ç›¸åº”æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¸é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é€‚é…å™¨ç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼ŒT2Lå¯ä»¥å‹ç¼©æ•°ç™¾ä¸ªLoRAå®ä¾‹å¹¶é›¶å°„æ³›åŒ–åˆ°å®Œå…¨æœªè§è¿‡çš„ä»»åŠ¡ã€‚æ­¤æ–¹æ³•ä¸ºæ°‘ä¸»åŒ–åŸºç¡€æ¨¡å‹çš„ä¸“ä¸šåŒ–è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œå¹¶é€šè¿‡æœ€å°çš„è®¡ç®—è¦æ±‚å®ç°äº†åŸºäºè¯­è¨€çš„é€‚åº”ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SakanaAI/text-to-lora%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SakanaAI/text-to-loraæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06105v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£æ¨¡å‹é€‚åº”ç­–ç•¥ï¼šT2Lï¼ˆæ–‡æœ¬è½¬LoRAï¼‰æ–¹æ³•ï¼Œèƒ½åœ¨æ— éœ€ç‰¹å®šæ•°æ®é›†å’Œç²¾ç»†è°ƒæ•´æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä»…é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç›®æ ‡ä»»åŠ¡ï¼Œå¿«é€Ÿé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ­¤æ–¹æ³•é€šè¿‡è®­ç»ƒä¸€ä¸ªè¶…ç½‘ç»œæ¥æ„å»ºLoRAé€‚é…å™¨ï¼Œå®ç°ä¸€æ¬¡ä½æˆæœ¬å‰å‘ä¼ é€’ã€‚è®­ç»ƒåçš„T2Lèƒ½åœ¨ä¸åŒæµ‹è¯•é›†ä¸Šè¾¾åˆ°ç‰¹å®šä»»åŠ¡é€‚é…å™¨çš„æ€§èƒ½ï¼Œå¹¶èƒ½å‹ç¼©å¤šä¸ªLoRAå®ä¾‹ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–è‡³æœªè§ä»»åŠ¡ã€‚æ­¤æ–¹æ³•å¤§å¤§ç®€åŒ–äº†ä¸“ä¸šæ¨¡å‹çš„ç‰¹æ®ŠåŒ–è¿‡ç¨‹ï¼Œé™ä½äº†è®¡ç®—éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Læ˜¯ä¸€ç§æ–°å‹æ¨¡å‹é€‚åº”ç­–ç•¥ï¼Œå¯å¿«é€Ÿé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è€Œæ— éœ€ç‰¹å®šæ•°æ®é›†å’Œç²¾ç»†è°ƒæ•´ã€‚</li>
<li>T2Lé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç›®æ ‡ä»»åŠ¡è¿›è¡Œæ¨¡å‹é€‚åº”ï¼Œæ— éœ€å¤æ‚çš„æ•°æ®é›†é€‰æ‹©å’Œæ¨¡å‹ç²¾ç»†è°ƒæ•´è¿‡ç¨‹ã€‚</li>
<li>T2Låˆ©ç”¨è¶…ç½‘ç»œæ„å»ºLoRAé€‚é…å™¨ï¼Œå®ç°ä½æˆæœ¬çš„ä¸€æ¬¡æ€§å‰å‘ä¼ é€’è®­ç»ƒã€‚</li>
<li>è®­ç»ƒåçš„T2Læ€§èƒ½ä¸ç‰¹å®šä»»åŠ¡é€‚é…å™¨ç›¸å½“ï¼Œå¯åœ¨ä¸åŒæµ‹è¯•é›†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>T2Lèƒ½å¤Ÿå‹ç¼©å¤šä¸ªLoRAå®ä¾‹ï¼Œå¹¶å®ç°é›¶æ ·æœ¬æ³›åŒ–è‡³æœªè§ä»»åŠ¡ã€‚</li>
<li>T2Læ–¹æ³•ç®€åŒ–äº†ä¸“ä¸šæ¨¡å‹çš„ç‰¹æ®ŠåŒ–è¿‡ç¨‹ï¼Œä½¿è¯­è¨€æ¨¡å‹çš„é€‚åº”å˜å¾—æ›´åŠ å®¹æ˜“å’Œé«˜æ•ˆã€‚</li>
<li>T2Lçš„ä»£ç å·²åœ¨æŒ‡å®šé“¾æ¥å…¬å¼€ï¼Œä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06105">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6c3013ab1c502cc8279a98b2068a5f40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbc554d240b0131c2c48c183c8170ca2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c2c1a83c2e60e01d898dd28e8107e42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2c88d10a1c784a7d2422fc1f454fab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b6f5826e111ac17e59c2b50ce09b1e1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU"><a href="#Flexible-Operator-Fusion-for-Fast-Sparse-Transformer-with-Diverse-Masking-on-GPU" class="headerlink" title="Flexible Operator Fusion for Fast Sparse Transformer with Diverse   Masking on GPU"></a>Flexible Operator Fusion for Fast Sparse Transformer with Diverse   Masking on GPU</h2><p><strong>Authors:Wenhao Dai, Haodong Deng, Mengfei Rong, Xinyu Yang, Hongyu Liu, Fangxin Liu, Hailong Yang, Weifeng Liu, Qingxiao Sun</strong></p>
<p>Large language models are popular around the world due to their powerful understanding capabilities. As the core component of LLMs, accelerating Transformer through parallelization has gradually become a hot research topic. Mask layers introduce sparsity into Transformer to reduce calculations. However, previous works rarely focus on the performance optimization of sparse Transformer. Moreover, rule-based mechanisms ignore the fusion opportunities of mixed-type operators and fail to adapt to various sequence lengths. To address the above problems, we propose STOF, a framework that incorporates optimizations for Sparse Transformer via flexible masking and operator fusion on GPU. We firstly unify the storage format and kernel implementation for the multi-head attention. Then, we map fusion schemes to compilation templates and determine the optimal parameter setting through a two-stage search engine. The experimental results show that compared to the state-of-the-art work, STOF achieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end inference. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å› å…¶å¼ºå¤§çš„ç†è§£èƒ½åŠ›è€Œå¤‡å—å…¨çƒå…³æ³¨ã€‚ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–åŠ é€ŸTransformeré€æ¸æˆä¸ºä¸€ä¸ªçƒ­é—¨çš„ç ”ç©¶è¯¾é¢˜ã€‚æ©ç å±‚å°†ç¨€ç–æ€§å¼•å…¥Transformerä¸­ï¼Œä»¥å‡å°‘è®¡ç®—é‡ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œå¾ˆå°‘å…³æ³¨ç¨€ç–Transformerçš„æ€§èƒ½ä¼˜åŒ–ã€‚æ­¤å¤–ï¼ŒåŸºäºè§„åˆ™çš„æ–¹æ³•å¿½è§†äº†æ··åˆç±»å‹æ“ä½œç¬¦çš„èåˆæœºä¼šï¼Œå¹¶ä¸”æ— æ³•é€‚åº”å„ç§åºåˆ—é•¿åº¦ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STOFï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡çµæ´»çš„æ©ç å’ŒGPUä¸Šçš„æ“ä½œç¬¦èåˆå¯¹ç¨€ç–Transformerè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬é¦–å…ˆç»Ÿä¸€äº†å¤šå¤´æ³¨æ„åŠ›çš„å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†èåˆæ–¹æ¡ˆæ˜ å°„åˆ°ç¼–è¯‘æ¨¡æ¿ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæœç´¢å¼•æ“ç¡®å®šæœ€ä½³å‚æ•°è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°å·¥ä½œç›¸æ¯”ï¼ŒSTOFåœ¨å¤šå¤´æ³¨æ„åŠ›è®¡ç®—ä¸­å®ç°äº†æœ€é«˜è¾¾1.7å€çš„åŠ é€Ÿï¼Œåœ¨ç«¯åˆ°ç«¯æ¨ç†ä¸­å®ç°äº†æœ€é«˜è¾¾1.5å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06095v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒç»„ä»¶Transformerï¼Œæå‡ºé€šè¿‡å¹¶è¡ŒåŒ–åŠ é€Ÿçš„æ–¹æ³•ã€‚å…¶ä¸­ç¨€ç–Transformerå¼•å…¥æ©å±‚ä»¥å‡å°‘è®¡ç®—ï¼Œä½†æ€§èƒ½ä¼˜åŒ–è¢«å¿½è§†ã€‚STOFæ¡†æ¶é€šè¿‡çµæ´»çš„æ©ç å’ŒGPUä¸Šçš„æ“ä½œç¬¦èåˆä¼˜åŒ–Sparse Transformerï¼Œå®ç°å¤šå¤´æ³¨æ„åŠ›çš„å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°çš„ç»Ÿä¸€ï¼Œæ˜ å°„èåˆæ–¹æ¡ˆåˆ°ç¼–è¯‘æ¨¡æ¿ï¼Œå¹¶é€šè¿‡ä¸¤é˜¶æ®µæœç´¢å¼•æ“ç¡®å®šæœ€ä½³å‚æ•°è®¾ç½®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°å·¥ä½œç›¸æ¯”ï¼ŒSTOFåœ¨MHAè®¡ç®—å’Œç«¯åˆ°ç«¯æ¨ç†ä¸Šåˆ†åˆ«å®ç°äº†æœ€å¤§1.7å€å’Œ1.5å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å› å¼ºå¤§çš„ç†è§£åŠ›è€Œå…¨çƒæµè¡Œï¼ŒTransformerä½œä¸ºå…¶æ ¸å¿ƒç»„ä»¶çš„åŠ é€Ÿæˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚</li>
<li>ç¨€ç–Transformerå¼•å…¥æ©å±‚ä»¥å‡å°‘è®¡ç®—ï¼Œä½†ä¹‹å‰çš„ç ”ç©¶å¾ˆå°‘å…³æ³¨å…¶æ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>STOFæ¡†æ¶é€šè¿‡çµæ´»çš„æ©ç å’Œæ“ä½œç¬¦èåˆä¼˜åŒ–Sparse Transformerã€‚</li>
<li>STOFå®ç°äº†å¤šå¤´æ³¨æ„åŠ›çš„å­˜å‚¨æ ¼å¼å’Œå†…æ ¸å®ç°çš„ç»Ÿä¸€ã€‚</li>
<li>STOFå°†èåˆæ–¹æ¡ˆæ˜ å°„åˆ°ç¼–è¯‘æ¨¡æ¿ã€‚</li>
<li>STOFé€šè¿‡ä¸¤é˜¶æ®µæœç´¢å¼•æ“ç¡®å®šæœ€ä½³å‚æ•°è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5cff390cac7368c31909ac7b1c8ed6ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34f2f2c14ee205bcc1a9f17a65c74808.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55005a11a82aa1d9f054e0cc15e96057.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78ad09ade3ce27009ffc43e2eda1bafb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-855867af03860e31ea8a5bc088007932.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59d0fb4ad9ecf0c453a6007bb5033681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bf1353b108ca40d80c15f7d9ce7ece3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01cb32476982d4124548cd4db65b355a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8af1a560e6551eb4d4f62ebb6ebbbdc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Efficient-Online-RFT-with-Plug-and-Play-LLM-Judges-Unlocking-State-of-the-Art-Performance"><a href="#Efficient-Online-RFT-with-Plug-and-Play-LLM-Judges-Unlocking-State-of-the-Art-Performance" class="headerlink" title="Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking   State-of-the-Art Performance"></a>Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking   State-of-the-Art Performance</h2><p><strong>Authors:Rudransh Agnihotri, Ananya Pandey</strong></p>
<p>Reward-model training is the cost bottleneck in modern Reinforcement Learning Human Feedback (RLHF) pipelines, often requiring tens of billions of parameters and an offline preference-tuning phase. In the proposed method, a frozen, instruction-tuned 7B LLM is augmented with only a one line JSON rubric and a rank-16 LoRA adapter (affecting just 0.8% of the modelâ€™s parameters), enabling it to serve as a complete substitute for the previously used heavyweight evaluation models. The plug-and-play judge achieves 96.2% accuracy on RewardBench, outperforming specialized reward networks ranging from 27B to 70B parameters. Additionally, it allows a 7B actor to outperform the top 70B DPO baseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K utilizing online PPO. Thorough ablations indicate that (i) six in context demonstrations deliver the majority of the zero-to-few-shot improvements (+2pp), and (ii) the LoRA effectively addresses the remaining disparity, particularly in the safety and adversarial Chat-Hard segments. The proposed model introduces HH-Rationales, a subset of 10,000 pairs from Anthropic HH-RLHF, to examine interpretability, accompanied by human generated justifications. GPT-4 scoring indicates that our LoRA judge attains approximately &#x3D; 9&#x2F;10 in similarity to human explanations, while zero-shot judges score around &#x3D;5&#x2F;10. These results indicate that the combination of prompt engineering and tiny LoRA produces a cost effective, transparent, and easily adjustable reward function, removing the offline phase while achieving new state-of-the-art outcomes for both static evaluation and online RLHF. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹è®­ç»ƒæ˜¯ç°ä»£å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ç®¡é“ä¸­çš„æˆæœ¬ç“¶é¢ˆï¼Œé€šå¸¸éœ€è¦æ•°åäº¿ä¸ªå‚æ•°å’Œç¦»çº¿åå¥½è°ƒæ•´é˜¶æ®µã€‚åœ¨æå‡ºçš„æ–¹æ³•ä¸­ï¼Œä¸€ä¸ªå†»ç»“çš„ã€æŒ‡ä»¤è°ƒè°çš„7B LLMä»…é€šè¿‡ä¸€è¡ŒJSONæŒ‡å—å’Œä¸€ä¸ªæ’å16çš„LoRAé€‚é…å™¨ï¼ˆä»…å½±å“æ¨¡å‹å‚æ•°çš„0.8%ï¼‰è¿›è¡Œå¢å¼ºï¼Œèƒ½å¤Ÿä½œä¸ºä¹‹å‰ä½¿ç”¨çš„é‡å‹è¯„ä¼°æ¨¡å‹çš„ç»¼åˆæ›¿ä»£å“ã€‚å³æ’å³ç”¨åˆ¤å®˜åœ¨RewardBenchä¸Šè¾¾åˆ°äº†96.2%çš„å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†å‚æ•°èŒƒå›´ä»27Båˆ°70Bçš„ä¸“ç”¨å¥–åŠ±ç½‘ç»œã€‚æ­¤å¤–ï¼Œå®ƒå…è®¸ä¸€ä¸ª7Bæ¼”å‘˜è¶…è¶Šé¡¶çº§70B DPOåŸºçº¿ï¼Œåè€…å¾—åˆ†ä¸º61.8%ï¼Œåœ¨GSM-8Kä¸Šå®ç°92%çš„ç²¾ç¡®åŒ¹é…ç‡ï¼Œé‡‡ç”¨åœ¨çº¿PPOã€‚å½»åº•çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œï¼ˆiï¼‰å…­ä¸ªä¸Šä¸‹æ–‡æ¼”ç¤ºå®ç°äº†ä»é›¶åˆ°å°‘æ•°é•œå¤´çš„å¤§éƒ¨åˆ†æ”¹è¿›ï¼ˆ+2ppï¼‰ï¼Œï¼ˆiiï¼‰LoRAæœ‰æ•ˆåœ°è§£å†³äº†å‰©ä½™çš„å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨å’Œå¯¹æŠ—æ€§Chat-Hardæ®µè½ä¸­ã€‚æ‰€æå‡ºçš„æ¨¡å‹å¼•å…¥äº†HH-Rationalesï¼Œå®ƒæ˜¯ä»Anthropic HH-RLHFä¸­çš„10000å¯¹å­é›†ä¸­æå–çš„ï¼Œä»¥æ£€æŸ¥è§£é‡Šæ€§ï¼Œè¾…ä»¥äººç±»ç”Ÿæˆçš„æ­£å½“ç†ç”±ã€‚GPT-4è¯„åˆ†æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„LoRAåˆ¤å®˜åœ¨ç›¸ä¼¼åº¦æ–¹é¢è¾¾åˆ°çº¦9&#x2F;10ï¼Œè€Œé›¶é•œå¤´åˆ¤å®˜å¾—åˆ†çº¦ä¸º5&#x2F;10ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæç¤ºå·¥ç¨‹å’Œå¾®å°çš„LoRAç›¸ç»“åˆäº§ç”Ÿäº†æˆæœ¬æ•ˆç›Šé«˜ã€é€æ˜ã€æ˜“äºè°ƒæ•´çš„å¥–åŠ±å‡½æ•°ï¼Œå»é™¤äº†ç¦»çº¿é˜¶æ®µï¼ŒåŒæ—¶ä¸ºå®ç°é™æ€è¯„ä¼°å’Œåœ¨çº¿RLHFçš„æœ€æ–°æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05748v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„æŒ‡ä»¤ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œåœ¨çº¿çš„å°‘é‡è°ƒæ•´å™¨ï¼ˆLoRAï¼‰ï¼Œä»¥è¾ƒä½çš„å‚æ•°è°ƒæ•´æˆæœ¬å®ç°é«˜æ•ˆèƒ½ã€‚è¿™ç§æ–¹æ³•é‡‡ç”¨ä¸€ä¸ªç®€åŒ–çš„å¥–åŠ±æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶è¾¾åˆ°äº†å‰æ²¿çš„å‡†ç¡®åº¦å’Œæ€§èƒ½ã€‚å®ƒå‡å°‘äº†ç¦»çº¿åå¥½è°ƒæ•´é˜¶æ®µï¼Œæå‡äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„çµæ´»æ€§ã€‚å®éªŒè¯æ˜å…¶å…·æœ‰é«˜å‡†ç¡®åº¦å’Œå¼ºæ³›åŒ–èƒ½åŠ›ã€‚å…³é”®å…ƒç´ åœ¨äºåŸºäºæç¤ºå·¥ç¨‹æŠ€æœ¯çš„ç®€å•è§„åˆ™é›†æˆå’Œå¯¹å°å‹è°ƒæ•´å™¨çš„å·§å¦™è¿ç”¨ã€‚è¿™ä¸ºè®­ç»ƒæ•ˆç‡çš„æé«˜å¸¦æ¥äº†å…¨æ–°çš„å¯èƒ½ã€‚å…·ä½“ä¸ºè½»é‡çº§è¯„ä»·æ¨¡å‹æ›¿ä»£äº†ä¼ ç»Ÿçš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œæé«˜äº†æ•ˆç‡å¹¶å®ç°äº†ä¼˜å¼‚æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•å±•ç¤ºäº†è‰¯å¥½çš„è§£é‡Šæ€§ï¼ŒåŒæ—¶å®ç°äº†é«˜æ•ˆã€é€æ˜å’Œçµæ´»çš„å¥–åŠ±åŠŸèƒ½ã€‚<strong>Key Takeaways</strong>:</p>
<ol>
<li>æ–°æ–¹æ³•ä½¿ç”¨è½»é‡çº§å¥–åŠ±æ¨¡å‹å–ä»£ä¼ ç»Ÿçš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œæ˜¾è‘—é™ä½äº†è®­ç»ƒæˆæœ¬ã€‚</li>
<li>ç»“åˆé¢„è®­ç»ƒçš„æŒ‡ä»¤ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œåœ¨çº¿çš„å°‘é‡è°ƒæ•´å™¨ï¼ˆLoRAï¼‰ï¼Œæå‡äº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„çµæ´»æ€§ã€‚é€šè¿‡å¼•å…¥ä»…å½±å“æ¨¡å‹æå°‘éƒ¨åˆ†å‚æ•°çš„LoRAé€‚é…å™¨å®ç°é«˜æ•ˆç‡æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚å…¶åœ¨é«˜éš¾åº¦çš„åœ¨çº¿è¯„ä»·åœºæ™¯ä¸­ä¹Ÿæœ‰å“è¶Šè¡¨ç°ã€‚å› æ­¤åŒæ—¶èŠ‚çœäº†ç¡¬ä»¶èµ„æºå’Œè¿ç®—æ—¶é—´ï¼Œå…·æœ‰å¾ˆé«˜çš„å®é™…åº”ç”¨ä»·å€¼ã€‚å¹¶ä¸”åœ¨å›æŠ¥æ¨¡å‹çš„è®­ç»ƒä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨å‡å°‘å¤§é‡å‚æ•°éœ€æ±‚çš„åŒæ—¶ç»´æŒé«˜æ°´å¹³çš„æ€§èƒ½è¡¨ç°ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„æ¨¡å‹è®­ç»ƒæ–¹å¼å…·æœ‰æ˜¾è‘—çš„æˆæœ¬ä¼˜åŠ¿ã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å…¶æˆä¸ºæ½œåœ¨çš„æœªæ¥æŠ€æœ¯è¶‹åŠ¿ã€‚åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜åŠ¿å’Œæ½œåŠ›ã€‚å¯¹æå‡è®­ç»ƒæ•ˆç‡ä»¥åŠæ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰ç€æ˜¾è‘—çš„å½±å“ã€‚è¯¥æ–¹æ³•å®ç°äº†åœ¨ä¿æŒæ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒæ—¶é—´çš„åŸºç¡€ä¸Šæå‡å…¶æ•ˆèƒ½çš„å£®ä¸¾æ»¡è¶³äº†äººä»¬å¯¹æ›´é«˜æ•ˆç‡çš„ç®—æ³•å’Œæ›´é«˜è´¨é‡çš„è¾“å‡ºå†…å®¹çš„è¿«åˆ‡éœ€æ±‚ä¸ºäººä»¬æä¾›äº†ä¸€ä¸ªæ–°é¢–ã€å®ç”¨ä¸”å…·æœ‰å¼ºå¤§æ½œåŠ›çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆè¿˜æ³¨é‡å®é™…åº”ç”¨ä»·å€¼åœ¨ä¿è¯æ•ˆèƒ½çš„åŒæ—¶æä¾›äº†ä¸°å¯Œçš„å¯è§†åŒ–å·¥å…·å’Œæ”¯æŒå¼€å‘çš„åŠŸèƒ½æ›´ä¸°å¯Œçš„è¯„ä»·æ¡†æ¶å’Œæ•°æ®ç®¡ç†æ–¹å¼å¯ä»¥åœ¨æ›´å¤šçš„é¢†åŸŸä¸­å®ç°è‰¯å¥½çš„å®é™…åº”ç”¨å¯¹å¼€å‘ç¯å¢ƒçš„æŠ€æœ¯äººå‘˜ä¹Ÿå…·æœ‰å‚è€ƒä»·å€¼å¯¹æ¯”å·²æœ‰çš„æ–¹æ³•å’Œæ‰‹æ®µæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–äº†æŠ€æœ¯å±‚é¢çš„æ“ä½œæµç¨‹è€Œä¸”æœ‰æœ›æ”¹å–„è¿™ä¸€ç°çŠ¶å¹¶èƒ½å¼•èµ·æœªæ¥çš„æŠ€æœ¯åº”ç”¨é©å‘½çš„æ–¹æ³•å’Œé€”å¾„è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½é€šè¿‡ä¼˜åŒ–ç®—æ³•ç»“æ„å’Œå¼•å…¥æ–°çš„æŠ€æœ¯æ‰‹æ®µç®€åŒ–äº†æ¨¡å‹å¤æ‚åº¦æé«˜äº†è®­ç»ƒæ•ˆç‡åŒæ—¶ä¹Ÿå¸¦æ¥äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ä½¿å¾—è¯¥æŠ€æœ¯åœ¨æœªæ¥å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå‘å±•æ½œåŠ›ã€‚å¹¶ä¸”å¯¹äºæœªæ¥çš„æŠ€æœ¯å‘å±•å…·æœ‰å¯ç¤ºä½œç”¨é€šè¿‡å¼•å…¥åˆ›æ–°çš„æŠ€æœ¯æ‰‹æ®µå’ŒæŠ€æœ¯æ–¹æ³•è¯¥æŠ€æœ¯åœ¨æœªæ¥çš„åº”ç”¨å‰æ™¯å¹¿é˜”å…·å¤‡å¼ºå¤§çš„å‘å±•æ½œåŠ›å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯å°¤å…¶æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸæœ‰æ½œåŠ›å®ç°æ›´å¤§çš„çªç ´å’Œæ”¹è¿›å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„ä¼˜ç§€è¡¨ç°åŒæ—¶ä¹Ÿé€šè¿‡è¿›ä¸€æ­¥çš„æ¢ç´¢å’Œç ”ç©¶ä»¥å®ç°æ›´åŠ ä¼˜ç§€çš„ç»“æœå¹¶å¯¹æŠ€æœ¯çš„å‘å±•è¶‹åŠ¿æå‡ºäº†é¢„è§æ€§å’Œå¼•å¯¼æ€§å…·æœ‰å¹¿æ³›çš„ç†è®ºç ”ç©¶ä»·å€¼å¹¶å¯ä»¥ä¸ºå®é™…åº”ç”¨æä¾›æœ‰ç›Šçš„å‚è€ƒå’ŒæŠ€æœ¯æ”¯æŒå®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ä½¿å…¶æˆä¸ºè¯¥é¢†åŸŸæŠ€æœ¯è¿›æ­¥çš„å¼•é¢†è€…æå‡ºäº†å…·ä½“çš„åº”ç”¨å‰æ™¯å’ŒæŠ€æœ¯å‘å±•æ–¹å‘å°†å¤§å¤§æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œåº”ç”¨å‘å±•å¸¦æ¥äº†é‡å¤§çš„ä»·å€¼å‰æ™¯å¯¹äºç›¸å…³çš„æŠ€æœ¯å’Œåº”ç”¨é¢†åŸŸå…·æœ‰é‡è¦çš„æ¨åŠ¨ä½œç”¨å¹¶æ¨åŠ¨äº†æ•´ä¸ªè¡Œä¸šçš„è¿›æ­¥å’Œå‘å±•ã€‚<strong>Summary</strong>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæ–¹æ³•ï¼Œç”¨è½»é‡çº§å¥–åŠ±æ¨¡å‹å–ä»£ä¼ ç»Ÿçš„å¤§è§„æ¨¡æ¨¡å‹æ¥é™ä½æˆæœ¬å’Œæé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒçš„LLMæ¨¡å‹å’Œåœ¨çº¿çš„LoRAé€‚é…å™¨ï¼Œå®ç°äº†é«˜æ•ˆã€çµæ´»å’Œé€æ˜çš„å¥–åŠ±åŠŸèƒ½ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚<strong>Key Takeaways</strong>:</li>
</ol>
<ul>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆæ–¹æ³•ï¼Œä½¿ç”¨è½»é‡çº§å¥–åŠ±æ¨¡å‹å–ä»£ä¼ ç»Ÿçš„å¤§è§„æ¨¡æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé™ä½äº†æˆæœ¬å¹¶æé«˜äº†æ•ˆç‡ã€‚ </li>
<li>ç»“åˆé¢„è®­ç»ƒçš„LLMæ¨¡å‹å’Œåœ¨çº¿çš„LoRAé€‚é…å™¨ï¼Œå¢å¼ºäº†æ¨¡å‹çš„å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ç®€åŒ–äº†æ¨¡å‹çš„å¤æ‚åº¦å¹¶æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚ </li>
<li>è¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„è§£é‡Šæ€§ï¼Œå¹¶ä¸”é€šè¿‡å®éªŒè¯æ˜äº†å…¶åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„ä¼˜ç§€è¡¨ç°å’Œå¯¹æœªæ¥æŠ€æœ¯å‘å±•çš„æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23d60b20577b59ddb38a656bb9f575ff.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="FaCTR-Factorized-Channel-Temporal-Representation-Transformers-for-Efficient-Time-Series-Forecasting"><a href="#FaCTR-Factorized-Channel-Temporal-Representation-Transformers-for-Efficient-Time-Series-Forecasting" class="headerlink" title="FaCTR: Factorized Channel-Temporal Representation Transformers for   Efficient Time Series Forecasting"></a>FaCTR: Factorized Channel-Temporal Representation Transformers for   Efficient Time Series Forecasting</h2><p><strong>Authors:Yash Vijay, Harini Subramanyan</strong></p>
<p>While Transformers excel in language and vision-where inputs are semantically rich and exhibit univariate dependency structures-their architectural complexity leads to diminishing returns in time series forecasting. Time series data is characterized by low per-timestep information density and complex dependencies across channels and covariates, requiring conditioning on structured variable interactions. To address this mismatch and overparameterization, we propose FaCTR, a lightweight spatiotemporal Transformer with an explicitly structural design. FaCTR injects dynamic, symmetric cross-channel interactions-modeled via a low-rank Factorization Machine into temporally contextualized patch embeddings through a learnable gating mechanism. It further encodes static and dynamic covariates for multivariate conditioning. Despite its compact design, FaCTR achieves state-of-the-art performance on eleven public forecasting benchmarks spanning both short-term and long-term horizons, with its largest variant using close to only 400K parameters-on average 50x smaller than competitive spatiotemporal transformer baselines. In addition, its structured design enables interpretability through cross-channel influence scores-an essential requirement for real-world decision-making. Finally, FaCTR supports self-supervised pretraining, positioning it as a compact yet versatile foundation for downstream time series tasks. </p>
<blockquote>
<p>å°½ç®¡Transformeråœ¨è‡ªç„¶è¯­è¨€å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å…¥è¯­ä¹‰ä¸°å¯Œä¸”è¡¨ç°å‡ºå•å˜é‡ä¾èµ–ç»“æ„çš„åœºæ™¯ä¸‹ï¼Œä½†å…¶æ¶æ„çš„å¤æ‚æ€§å¯¼è‡´å…¶åœ¨æ—¶é—´åºåˆ—é¢„æµ‹æ–¹é¢çš„æ”¶ç›Šé€’å‡ã€‚æ—¶é—´åºåˆ—æ•°æ®çš„ç‰¹ç‚¹æ˜¯æ¯ä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯å¯†åº¦ä½ï¼Œå„é€šé“å’Œåå˜é‡ä¹‹é—´çš„å¤æ‚ä¾èµ–æ€§ï¼Œéœ€è¦åŸºäºç»“æ„åŒ–å˜é‡äº¤äº’è¿›è¡Œæ¡ä»¶è®¾å®šã€‚ä¸ºäº†è§£å†³è¿™ç§ä¸åŒ¹é…å’Œè¿‡åº¦å‚æ•°åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FaCTRï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æ˜ç¡®ç»“æ„è®¾è®¡çš„è½»é‡çº§æ—¶ç©ºTransformerã€‚FaCTRé€šè¿‡å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶ï¼Œå°†åŠ¨æ€å¯¹ç§°çš„è·¨é€šé“äº¤äº’ï¼ˆé€šè¿‡ä½é˜¶åˆ†è§£æœºå»ºæ¨¡ï¼‰æ³¨å…¥åˆ°å…·æœ‰æ—¶é—´ä¸Šä¸‹æ–‡çš„è¡¥ä¸åµŒå…¥ä¸­ã€‚å®ƒè¿›ä¸€æ­¥ç¼–ç é™æ€å’ŒåŠ¨æ€åå˜é‡ä»¥è¿›è¡Œå¤šå…ƒæ¡ä»¶è®¾å®šã€‚å°½ç®¡å…¶è®¾è®¡ç´§å‡‘ï¼ŒFaCTRåœ¨åŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸè§†é‡åœ¨å†…çš„åä¸€ä¸ªå…¬å…±é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…¶æœ€å¤§å˜ä½“ä»…ä½¿ç”¨è¿‘40ä¸‡ä¸ªå‚æ•°â€”â€”å¹³å‡æ¯”ç«äº‰æ€§çš„æ—¶ç©ºTransformeråŸºå‡†æµ‹è¯•å°50å€ã€‚æ­¤å¤–ï¼Œå…¶ç»“æ„åŒ–è®¾è®¡é€šè¿‡è·¨é€šé“å½±å“åˆ†æ•°å®ç°äº†å¯è§£é‡Šæ€§ï¼Œè¿™æ˜¯ç°å®ä¸–ç•Œå†³ç­–çš„ä¸€ä¸ªåŸºæœ¬è¦æ±‚ã€‚æœ€åï¼ŒFaCTRæ”¯æŒè‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªç´§å‡‘è€Œé€šç”¨çš„æ—¶é—´åºåˆ—ä»»åŠ¡åŸºç¡€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05597v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFaCTRçš„è½»é‡çº§æ—¶ç©ºTransformeræ¨¡å‹ï¼Œé’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡è¿›è¡Œè®¾è®¡ã€‚é€šè¿‡æ³¨å…¥åŠ¨æ€å¯¹ç§°çš„è·¨é€šé“äº¤äº’ã€å¯¹ç»“æ„åŒ–å˜é‡äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œä»¥åŠç¼–ç é™æ€å’ŒåŠ¨æ€åå˜é‡è¿›è¡Œå¤šå…ƒæ¡ä»¶å¤„ç†ï¼ŒFaCTRæ¨¡å‹åœ¨å¤šä¸ªå…¬å…±é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚å…¶ç»“æ„åŒ–çš„è®¾è®¡è¿˜æä¾›äº†å¯è§£é‡Šæ€§ï¼Œå¹¶å¯é€šè¿‡è·¨é€šé“å½±å“åˆ†æ•°è¿›è¡Œå®é™…åº”ç”¨ä¸­çš„å†³ç­–æ”¯æŒã€‚æ­¤å¤–ï¼ŒFaCTRæ”¯æŒè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒï¼Œä½¿å…¶æˆä¸ºé’ˆå¯¹ä¸‹æ¸¸æ—¶é—´åºåˆ—ä»»åŠ¡çš„ç´§å‡‘è€Œé€šç”¨çš„åŸºç¡€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FaCTRæ˜¯ä¸€ä¸ªé’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹ä»»åŠ¡çš„è½»é‡çº§æ—¶ç©ºTransformeræ¨¡å‹ã€‚</li>
<li>FaCTRé€šè¿‡åŠ¨æ€å¯¹ç§°çš„è·¨é€šé“äº¤äº’ï¼Œç»“åˆç»“æ„åŒ–å˜é‡äº¤äº’å»ºæ¨¡æ¥å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ã€‚</li>
<li>FaCTRåœ¨å¤šä¸ªå…¬å…±é¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œä¸”æ¨¡å‹å‚æ•°æ•°é‡è¾ƒå°ã€‚</li>
<li>FaCTRçš„ç»“æ„åŒ–è®¾è®¡æä¾›äº†å¯è§£é‡Šæ€§ï¼Œä¾¿äºå®é™…å†³ç­–æ”¯æŒã€‚</li>
<li>FaCTRæ”¯æŒè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒï¼Œé€‚ç”¨äºå¤šç§ä¸‹æ¸¸æ—¶é—´åºåˆ—ä»»åŠ¡ã€‚</li>
<li>FaCTRèƒ½å¤Ÿå¤„ç†çŸ­æ—¶é—´åˆ°é•¿æ—¶é—´çš„é¢„æµ‹ä»»åŠ¡ï¼Œå…·æœ‰è‰¯å¥½çš„æ—¶é—´è·¨åº¦é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5b79ad4d6afbb9ccfaf24c03944c4218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9fafaa298732ddb7f7a170b4df879f53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0480c2cf5a63b735e8c4857f21e9d6f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-ChatGPT-Perform-Image-Splicing-Detection-A-Preliminary-Study"><a href="#Can-ChatGPT-Perform-Image-Splicing-Detection-A-Preliminary-Study" class="headerlink" title="Can ChatGPT Perform Image Splicing Detection? A Preliminary Study"></a>Can ChatGPT Perform Image Splicing Detection? A Preliminary Study</h2><p><strong>Authors:Souradip Nath</strong></p>
<p>Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning across text and image modalities, showing promise in a variety of complex vision-language tasks. In this preliminary study, we investigate the out-of-the-box capabilities of GPT-4V in the domain of image forensics, specifically, in detecting image splicing manipulations. Without any task-specific fine-tuning, we evaluate GPT-4V using three prompting strategies: Zero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a curated subset of the CASIA v2.0 splicing dataset.   Our results show that GPT-4V achieves competitive detection performance in zero-shot settings (more than 85% accuracy), with CoT prompting yielding the most balanced trade-off across authentic and spliced images. Qualitative analysis further reveals that the model not only detects low-level visual artifacts but also draws upon real-world contextual knowledge such as object scale, semantic consistency, and architectural facts, to identify implausible composites. While GPT-4V lags behind specialized state-of-the-art splicing detection models, its generalizability, interpretability, and encyclopedic reasoning highlight its potential as a flexible tool in image forensics. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-4Vï¼‰èƒ½å¤Ÿè·¨æ–‡æœ¬å’Œå›¾åƒæ¨¡å¼è¿›è¡Œæ¨ç†ï¼Œåœ¨å„ç§å¤æ‚çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚åœ¨è¿™é¡¹åˆæ­¥ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†GPT-4Våœ¨å›¾åƒå–è¯é¢†åŸŸå³æ£€æµ‹å›¾åƒæ‹¼æ¥æ“ä½œçš„èƒ½åŠ›ã€‚æ²¡æœ‰ä»»ä½•ç‰¹å®šçš„å¾®è°ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‰ç§æç¤ºç­–ç•¥å¯¹GPT-4Vè¿›è¡Œäº†è¯„ä¼°ï¼šé›¶æ ·æœ¬ï¼ˆZSï¼‰ã€å°‘æ ·æœ¬ï¼ˆFSï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œè¿™äº›ç­–ç•¥åº”ç”¨äºCASIA v2.0æ‹¼æ¥æ•°æ®é›†çš„ç²¾é€‰å­é›†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒGPT-4Våœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰ç«äº‰åŠ›æ£€æµ‹æ€§èƒ½ï¼ˆå‡†ç¡®ç‡è¶…è¿‡85%ï¼‰ï¼Œæ€ç»´é“¾æç¤ºåœ¨çœŸå®å’Œæ‹¼æ¥å›¾åƒä¹‹é—´å–å¾—äº†æœ€å¹³è¡¡çš„äº¤æ˜“ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¸ä»…æ£€æµ‹ä½çº§åˆ«çš„è§†è§‰ä¼ªå½±ï¼Œè€Œä¸”è¿˜åˆ©ç”¨ç°å®ä¸–ç•Œä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œå¦‚å¯¹è±¡æ¯”ä¾‹ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œå»ºç­‘äº‹å®ï¼Œæ¥è¯†åˆ«ä¸å¯ä¿¡çš„åˆæˆå›¾åƒã€‚è™½ç„¶GPT-4Våœ¨æ£€æµ‹å›¾åƒæ‹¼æ¥æ–¹é¢è½åäºä¸“ä¸šçš„æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä½†å…¶é€šç”¨æ€§ã€å¯è§£é‡Šæ€§å’Œç™¾ç§‘å…¨ä¹¦èˆ¬çš„æ¨ç†èƒ½åŠ›çªæ˜¾äº†å…¶åœ¨å›¾åƒå–è¯é¢†åŸŸä½œä¸ºçµæ´»å·¥å…·çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.05358v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>GPT-4Vç­‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·¨æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„æ¨ç†ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œåˆæ­¥ç ”ç©¶è¡¨æ˜å…¶åœ¨å›¾åƒå–è¯é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€æµ‹å›¾åƒæ‹¼æ¥æ“çºµæ–¹é¢è¡¨ç°å‡ºå‡ºè‰²çš„èƒ½åŠ›ã€‚æ— éœ€ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œæˆ‘ä»¬åœ¨CASIA v2.0æ‹¼æ¥æ•°æ®é›†çš„ä¸€ä¸ªç²¾é€‰å­é›†ä¸Šåº”ç”¨äº†é›¶æ ·æœ¬ï¼ˆZSï¼‰ã€å°‘æ ·æœ¬ï¼ˆFSï¼‰å’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä¸‰ç§æç¤ºç­–ç•¥ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4Våœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­çš„æ£€æµ‹æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼ˆå‡†ç¡®ç‡è¶…è¿‡85%ï¼‰ï¼Œå…¶ä¸­é“¾å¼æ€ç»´æç¤ºåœ¨çœŸå®å’Œæ‹¼æ¥å›¾åƒä¹‹é—´æä¾›äº†æœ€å¹³è¡¡çš„äº¤æ˜“ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè¯¥æ¨¡å‹ä¸ä»…æ£€æµ‹ä½çº§åˆ«çš„è§†è§‰ä¼ªå½±ï¼Œè€Œä¸”åˆ©ç”¨ç°å®ä¸–ç•Œä¸Šä¸‹æ–‡çŸ¥è¯†å¦‚ç‰©ä½“å°ºåº¦ã€è¯­ä¹‰ä¸€è‡´æ€§å’Œå»ºç­‘äº‹å®æ¥è¯†åˆ«ä¸åˆç†çš„åˆæˆå›¾åƒã€‚è™½ç„¶GPT-4Våœ¨ä¸“é—¨çš„æ‹¼æ¥æ£€æµ‹æ¨¡å‹é¢å‰ä»æœ‰ä¸è¶³ï¼Œä½†å…¶æ³›åŒ–èƒ½åŠ›ã€å¯è§£é‡Šæ€§å’Œç™¾ç§‘å…¨ä¹¦èˆ¬çš„æ¨ç†èƒ½åŠ›çªæ˜¾äº†å…¶åœ¨å›¾åƒå–è¯ä¸­çš„æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GPT-4Vç­‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å…·å¤‡è·¨æ–‡æœ¬å’Œå›¾åƒæ¨¡æ€çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨å›¾åƒå–è¯é¢†åŸŸï¼ŒGPT-4Våœ¨æ£€æµ‹å›¾åƒæ‹¼æ¥æ“çºµæ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>GPT-4Våœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹å®ç°äº†è¶…è¿‡85%çš„å‡†ç¡®ç‡ã€‚</li>
<li>é“¾å¼æ€ç»´æç¤ºç­–ç•¥åœ¨çœŸå®å’Œæ‹¼æ¥å›¾åƒä¹‹é—´æä¾›äº†æœ€ä½³çš„å¹³è¡¡ã€‚</li>
<li>GPT-4Vä¸ä»…èƒ½æ£€æµ‹ä½çº§åˆ«çš„è§†è§‰ä¼ªå½±ï¼Œè¿˜èƒ½åˆ©ç”¨ç°å®ä¸–ç•Œä¸Šä¸‹æ–‡çŸ¥è¯†è¯†åˆ«ä¸åˆç†çš„åˆæˆå›¾åƒã€‚</li>
<li>GPT-4Vçš„æ³›åŒ–èƒ½åŠ›ã€å¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›åœ¨å›¾åƒå–è¯ä¸­å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.05358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e94a6fc7da12b06375c5a13a3d97607a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b01fd72184fb2acc0f8effa455cbea3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72bdadc995e1d0935240e827eb46ae61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adb5a8c48011bae5c2b66d7a76fbffd3.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MEDAL-A-Framework-for-Benchmarking-LLMs-as-Multilingual-Open-Domain-Chatbots-and-Dialogue-Evaluators"><a href="#MEDAL-A-Framework-for-Benchmarking-LLMs-as-Multilingual-Open-Domain-Chatbots-and-Dialogue-Evaluators" class="headerlink" title="MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain   Chatbots and Dialogue Evaluators"></a>MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain   Chatbots and Dialogue Evaluators</h2><p><strong>Authors:John MendonÃ§a, Alon Lavie, Isabel Trancoso</strong></p>
<p>As the capabilities of chatbots and their underlying LLMs continue to dramatically improve, evaluating their performance has increasingly become a major blocker to their further development. A major challenge is the available benchmarking datasets, which are largely static, outdated, and lacking in multilingual coverage, limiting their ability to capture subtle linguistic and cultural variations. This paper introduces MEDAL, an automated multi-agent framework for generating, evaluating, and curating more representative and diverse open-domain dialogue evaluation benchmarks. Our approach leverages several state-of-the-art LLMs to generate user-chatbot multilingual dialogues, conditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a multidimensional analysis of the performance of the chatbots, uncovering noticeable cross-lingual performance differences. Guided by this large-scale evaluation, we curate a new meta-evaluation multilingual benchmark and human-annotate samples with nuanced quality judgments. This benchmark is then used to assess the ability of several reasoning and non-reasoning LLMs to act as evaluators of open-domain dialogues. We find that current LLMs struggle to detect nuanced issues, particularly those involving empathy and reasoning. </p>
<blockquote>
<p>éšç€èŠå¤©æœºå™¨äººå’Œå…¶åº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æŒç»­æ˜¾è‘—æé«˜ï¼Œè¯„ä¼°å®ƒä»¬çš„æ€§èƒ½å·²æˆä¸ºè¿›ä¸€æ­¥å‘å±•çš„ä¸»è¦éšœç¢ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜åœ¨äºå¯ç”¨çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¤§å¤šé™æ€ã€è¿‡æ—¶ï¼Œå¹¶ä¸”ç¼ºä¹å¤šè¯­è¨€è¦†ç›–ï¼Œé™åˆ¶äº†å®ƒä»¬æ•æ‰ç»†å¾®çš„è¯­è¨€å’Œæ–‡åŒ–å˜åŒ–çš„èƒ½åŠ›ã€‚æœ¬æ–‡ä»‹ç»äº†MEDALï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆã€è¯„ä¼°å’Œç­–åˆ’æ›´å…·ä»£è¡¨æ€§å’Œå¤šæ ·åŒ–çš„å¼€æ”¾é¢†åŸŸå¯¹è¯è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å‡ ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆç”¨æˆ·ä¸èŠå¤©æœºå™¨äººçš„å¤šè¯­è¨€å¯¹è¯ï¼Œè¿™äº›å¯¹è¯æ˜¯åœ¨ä¸åŒçš„ç§å­è¯­å¢ƒä¸‹è¿›è¡Œçš„ã€‚ç„¶åä½¿ç”¨ä¸€ä¸ªå¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹GPT-4.1å¯¹èŠå¤©æœºå™¨äººçš„æ€§èƒ½è¿›è¡Œå¤šç»´åˆ†æï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„è¯­è¨€é—´æ€§èƒ½å·®å¼‚ã€‚ä»¥æ­¤å¤§è§„æ¨¡è¯„ä¼°ä¸ºæŒ‡å¯¼ï¼Œæˆ‘ä»¬ç­–åˆ’äº†ä¸€ä¸ªæ–°çš„å…ƒè¯„ä¼°å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹æ ·æœ¬è¿›è¡Œå¾®å¦™çš„å“è´¨åˆ¤æ–­è¿›è¡Œäººå·¥æ ‡æ³¨ã€‚è¿™ä¸ªåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤šä¸ªæ¨ç†å’Œéæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¼€æ”¾é¢†åŸŸå¯¹è¯ä¸­çš„è¯„ä¼°èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹å¾®å¦™é—®é¢˜æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯æ¶‰åŠåŒç†å¿ƒå’Œæ¨ç†çš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22777v2">PDF</a> May ARR</p>
<p><strong>Summary</strong></p>
<p>éšç€èŠå¤©æœºå™¨äººåŠå…¶åº•å±‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æŒç»­æ˜¾è‘—æé«˜ï¼Œè¯„ä¼°å®ƒä»¬çš„æ€§èƒ½å·²æˆä¸ºè¿›ä¸€æ­¥å¼€å‘çš„ä¸»è¦éšœç¢ã€‚å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€æ˜¯å¯ç”¨çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼Œå®ƒä»¬å¤§å¤šé™æ€ã€è¿‡æ—¶ä¸”ç¼ºä¹å¤šè¯­è¨€è¦†ç›–ï¼Œæ— æ³•æ•æ‰è¯­è¨€å’Œæ–‡åŒ–çš„ç»†å¾®å·®å¼‚ã€‚æœ¬æ–‡ä»‹ç»äº† MEDALï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆã€è¯„ä¼°å’Œæ•´ç†æ›´å…·ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„å¼€æ”¾åŸŸå¯¹è¯è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šä¸ªå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç”¨æˆ·ä¸èŠå¤©æœºå™¨äººçš„å¤šè¯­è¨€å¯¹è¯ï¼ŒåŸºäºä¸åŒçš„ç§å­ä¸Šä¸‹æ–‡ã€‚éšåä½¿ç”¨å¼ºå¤§çš„GPT-4.1è¿›è¡Œå¤šç»´åº¦çš„èŠå¤©æœºå™¨äººæ€§èƒ½åˆ†æï¼Œå‘ç°æ˜æ˜¾çš„è·¨è¯­è¨€æ€§èƒ½å·®å¼‚ã€‚å€ŸåŠ©å¤§è§„æ¨¡è¯„ä¼°ç»“æœï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªæ–°çš„å¤šå…ƒè¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œå¹¶ç”¨äººæ¥æ ‡æ³¨æ ·æœ¬ä»¥è¿›è¡Œå¾®å¦™çš„å“è´¨åˆ¤æ–­ã€‚è¯¥åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤šä¸ªæ¨ç†å’Œéæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å¼€æ”¾åŸŸå¯¹è¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹å¾®å¦™é—®é¢˜æ–¹é¢è¡¨ç°æŒ£æ‰ï¼Œå°¤å…¶æ˜¯åœ¨æƒ…æ„Ÿå’Œæ¨ç†æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€èŠå¤©æœºå™¨äººåŠLLMæ€§èƒ½çš„æå‡ï¼Œè¯„ä¼°å®ƒä»¬çš„æ€§èƒ½æˆä¸ºäº†ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•æ•°æ®é›†å­˜åœ¨å±€é™æ€§ï¼Œå¦‚é™æ€ã€è¿‡æ—¶ä»¥åŠç¼ºä¹å¤šè¯­è¨€è¦†ç›–ã€‚</li>
<li>MEDALæ¡†æ¶è¢«å¼•å…¥ï¼Œç”¨äºç”Ÿæˆã€è¯„ä¼°å’Œæ•´ç†æ›´å…·ä»£è¡¨æ€§å’Œå¤šæ ·æ€§çš„å¼€æ”¾åŸŸå¯¹è¯è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚</li>
<li>MEDALåˆ©ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥ç”Ÿæˆå¤šè¯­è¨€å¯¹è¯ï¼Œå¹¶åŸºäºä¸åŒçš„ç§å­ä¸Šä¸‹æ–‡è¿›è¡Œã€‚</li>
<li>GPT-4.1è¢«ç”¨äºå¤šç»´åº¦çš„èŠå¤©æœºå™¨äººæ€§èƒ½åˆ†æï¼Œæ­ç¤ºè·¨è¯­è¨€æ€§èƒ½å·®å¼‚ã€‚</li>
<li>æ–°æ•´ç†çš„å¤šå…ƒè¯­è¨€åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°å¼€æ”¾åŸŸå¯¹è¯çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40950214caae7bc1e86fa7d7691a268b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-896314195a785547113d63c808070318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19ffc29965317a33915b16d94846495e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480d499b9ee2e5fd4b53271d40640d6b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯ä¸­ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿï¼Œéœ€è¦å¤§é‡çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†éå¸¸å›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆåº”è¿è€Œç”Ÿï¼Œå®ƒå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ¨¡å‹â€”â€”Ophoraï¼Œå®ƒå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºäº†æ„å»ºOphoraï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™è¿°æ€§çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16ä¸‡å¯¹è§†é¢‘æŒ‡ä»¤å¯¹ï¼Œå³Ophora-160Kã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»¥ä»ä¸€ä¸ªåœ¨è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„T2Væ¨¡å‹è½¬ç§»ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ï¼Œç”¨äºåŸºäºOphora-160Kçš„éšç§ä¿æŠ¤çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å¯¹è§†é¢‘è´¨é‡çš„å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆç°å®å’Œå¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Ophoraåœ¨æ‰§è¡Œçœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v4">PDF</a> Early accepted in MICCAI25</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„æ–°å‹æ¨¡å‹â€”â€”Ophoraã€‚é€šè¿‡å¤§è§„æ¨¡æ•°æ®é›†æ„å»ºåŠæ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒä¼˜æ–¹æ¡ˆï¼Œå®ç°äº†é«˜è´¨é‡çœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„ç”Ÿæˆã€‚æ­¤æ¨¡å‹èƒ½ä¾æ®åŒ»ç”ŸæŒ‡ä»¤ç”ŸæˆçœŸå®å¯é çš„æ‰‹æœ¯è§†é¢‘ï¼Œå¹¶æœ‰åŠ©äºä¸‹æ¸¸çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ä»»åŠ¡ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ophoraæ˜¯ä¸€ä¸ªåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘çš„é¢†å…ˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç»¼åˆæ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™è¿°æ€§çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†â€”â€”Ophora-160Kã€‚</li>
<li>æå‡ºæ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒä¼˜æ–¹æ¡ˆï¼Œä»è‡ªç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸­é¢„è®­ç»ƒT2Væ¨¡å‹ï¼Œå®ç°åŸºäºOphora-160Kçš„éšç§ä¿æŠ¤çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒOphoraèƒ½æ ¹æ®åŒ»ç”ŸæŒ‡ä»¤ç”ŸæˆçœŸå®ä¸”å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚</li>
<li>Ophoraåœ¨çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å·²ç»å…¬å¼€ï¼Œæ–¹ä¾¿åç»­ç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c3cb97d0ac5fdfce32d4a1ff1af58996.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4ec3b71a8c06c8f6d824bb3db7290c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Sparse-Autoencoders-Learn-Monosemantic-Features-in-Vision-Language-Models"><a href="#Sparse-Autoencoders-Learn-Monosemantic-Features-in-Vision-Language-Models" class="headerlink" title="Sparse Autoencoders Learn Monosemantic Features in Vision-Language   Models"></a>Sparse Autoencoders Learn Monosemantic Features in Vision-Language   Models</h2><p><strong>Authors:Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata</strong></p>
<p>Given that interpretability and steerability are crucial to AI safety, Sparse Autoencoders (SAEs) have emerged as a tool to enhance them in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in vision representations. To ensure that our evaluation aligns with human perception, we propose a benchmark derived from a large-scale user study. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons, with sparsity and wide latents being the most influential factors. Notably, we demonstrate that applying SAE interventions on CLIPâ€™s vision encoder directly steers multimodal LLM outputs (e.g., LLaVA), without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised tool for enhancing both interpretability and control of VLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/sae-for-vlm">https://github.com/ExplainableML/sae-for-vlm</a>. </p>
<blockquote>
<p>è€ƒè™‘åˆ°å¯è§£é‡Šæ€§å’Œå¯æ§åˆ¶æ€§å¯¹äºäººå·¥æ™ºèƒ½å®‰å…¨è‡³å…³é‡è¦ï¼Œç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å·²ç»æˆä¸ºä¸€ç§å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¿™ä¸¤è€…çš„å·¥å…·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†SAEçš„åº”ç”¨æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œä¾‹å¦‚CLIPï¼Œå¹¶å¼•å…¥ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¡¨ç¤ºä¸­ç¥ç»å…ƒçº§åˆ«çš„å•è¯­ä¹‰æ€§ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„è¯„ä¼°ä¸äººç±»æ„ŸçŸ¥ç›¸ä¸€è‡´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»å¤§è§„æ¨¡ç”¨æˆ·ç ”ç©¶ä¸­å¾—å‡ºçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨VLMä¸Šè®­ç»ƒçš„SAEæ˜¾è‘—å¢å¼ºäº†å•ä¸ªç¥ç»å…ƒçš„å•è¯­ä¹‰æ€§ï¼Œå…¶ä¸­ç¨€ç–æ€§å’Œå®½æ½œå˜é‡æ˜¯æœ€å…·å½±å“åŠ›çš„å› ç´ ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯¹CLIPè§†è§‰ç¼–ç å™¨ç›´æ¥åº”ç”¨SAEå¹²é¢„å¯ä»¥å¼•å¯¼å¤šæ¨¡æ€LLMè¾“å‡ºï¼ˆä¾‹å¦‚LLaVAï¼‰ï¼Œè€Œæ— éœ€å¯¹åº•å±‚æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†SAEä½œä¸ºå¢å¼ºVLMå¯è§£é‡Šæ€§å’Œæ§åˆ¶æ€§çš„æ— ç›‘ç£å·¥å…·çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/sae-for-vlm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/sae-for-vlmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02821v2">PDF</a> Preprint</p>
<p><strong>æ€»ç»“</strong><br>    æœ¬è®ºæ–‡å°†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¦‚CLIPï¼Œä»¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ§åˆ¶æ€§ã€‚è®ºæ–‡æå‡ºä¸€ä¸ªè¯„ä¼°è§†è§‰è¡¨ç¤ºç¥ç»å…ƒçº§åˆ«å•è¯­ä¹‰æ€§çš„å…¨é¢æ¡†æ¶ï¼Œå¹¶åŸºäºå¤§è§„æ¨¡ç”¨æˆ·ç ”ç©¶æå‡ºä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„åŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨VLMä¸Šè®­ç»ƒçš„SAEèƒ½æ˜¾è‘—æå‡å•ä¸ªç¥ç»å…ƒçš„å•è¯­ä¹‰æ€§ï¼Œå…¶ä¸­ç¨€ç–æ€§å’Œå®½æ½œåœ¨å› ç´ æ˜¯æœ€å…·å½±å“åŠ›çš„å› ç´ ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¯¹CLIPçš„è§†è§‰ç¼–ç å™¨åº”ç”¨SAEå¹²é¢„èƒ½ç›´æ¥æ§åˆ¶å¤šæ¨¡æ€LLMçš„è¾“å‡ºï¼Œæ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹ã€‚è¿™è¡¨æ˜SAEä½œä¸ºä¸€ç§æ— ç›‘ç£å·¥å…·ï¼Œåœ¨æå‡VLMçš„å¯è§£é‡Šæ€§å’Œæ§åˆ¶æ€§æ–¹é¢éå¸¸å®ç”¨æœ‰æ•ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰è¢«æ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œç”¨äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯è§£é‡Šæ€§å’Œå¯æ§åˆ¶æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°è§†è§‰è¡¨ç¤ºç¥ç»å…ƒçº§åˆ«çš„å•è¯­ä¹‰æ€§ã€‚</li>
<li>åŸºäºå¤§è§„æ¨¡ç”¨æˆ·ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ä¸ªä¸å•è¯­ä¹‰æ€§è¯„ä¼°ç›¸å¯¹åº”çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥ç¡®ä¿è¯„ä¼°ä¸äººç±»æ„ŸçŸ¥ä¸€è‡´ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSAEèƒ½æ˜¾è‘—æå‡å•ä¸ªç¥ç»å…ƒçš„å•è¯­ä¹‰æ€§ï¼Œå…¶ä¸­ç¨€ç–æ€§å’Œå®½æ½œåœ¨å› ç´ æœ€é‡è¦ã€‚</li>
<li>SAEå¹²é¢„èƒ½ç›´æ¥æ§åˆ¶å¤šæ¨¡æ€LLMçš„è¾“å‡ºï¼Œè€Œæ— éœ€ä¿®æ”¹åº•å±‚æ¨¡å‹ã€‚</li>
<li>SAEä½œä¸ºä¸€ç§æ— ç›‘ç£å·¥å…·ï¼Œåœ¨æå‡VLMçš„å¯è§£é‡Šæ€§å’Œæ§åˆ¶æ€§æ–¹é¢éå¸¸å®ç”¨ã€‚</li>
<li>è®ºæ–‡æä¾›äº†ç›¸å…³çš„ä»£ç ï¼Œå¯ä¾›è¿›ä¸€æ­¥ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47f73b246ffd7af0fb8c27ab8715f9d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5535f463b323ce586efb47e65ea0e463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a41f7e0a2e41b1c5defb3b0ca16bbda0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b0bc8993766da408d82c9f86a908d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b766bf7f21173b4d95aca284029b8e4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MimeQA-Towards-Socially-Intelligent-Nonverbal-Foundation-Models"><a href="#MimeQA-Towards-Socially-Intelligent-Nonverbal-Foundation-Models" class="headerlink" title="MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models"></a>MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</h2><p><strong>Authors:Hengzhi Li, Megan Tjandrasuwita, Yi R. Fung, Armando Solar-Lezama, Paul Pu Liang</strong></p>
<p>As AI becomes more closely integrated with peoplesâ€™ daily activities, socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important. However, current works in AI social reasoning all rely on language-only or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel data source rich in nonverbal social interactions â€“ mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting nonverbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing 8 hours of videos clips from YouTube and developing a comprehensive video question-answering benchmark comprising 806 carefully annotated and verified question-answer pairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA, we evaluate state-of-the-art video large language models (vLLMs) and find that they achieve low overall accuracy, ranging from 20-30%, while humans score 86%. Our analysis reveals that vLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. We hope to inspire future work in AI models that embody true social intelligence capable of interpreting non-verbal human interactions. </p>
<blockquote>
<p>éšç€äººå·¥æ™ºèƒ½æ—¥ç›Šèå…¥äººä»¬çš„æ—¥å¸¸æ´»åŠ¨ï¼Œèƒ½å¤Ÿæ— ç¼åœ°ç†è§£å’Œä¸äººç±»è¿›è¡Œæ—¥å¸¸äº¤äº’çš„ç¤¾ä¼šæ™ºèƒ½äººå·¥æ™ºèƒ½å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„äººå·¥æ™ºèƒ½ç¤¾ä¼šæ¨ç†ç ”ç©¶éƒ½ä¾èµ–äºä»…ä½¿ç”¨è¯­è¨€æˆ–è¯­è¨€ä¸»å¯¼çš„æ–¹æ³•æ¥è¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹è®­ç»ƒï¼Œè¿™å¯¼è‡´ç³»ç»Ÿåœ¨å£å¤´æ²Ÿé€šæ–¹é¢æœ‰æ‰€æé«˜ï¼Œä½†åœ¨éè¨€è¯­ç¤¾ä¼šç†è§£æ–¹é¢å´é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ç§å¯Œå«éè¨€è¯­ç¤¾äº¤äº’åŠ¨çš„æ–°æ•°æ®æºâ€”â€”å“‘å‰§è§†é¢‘ã€‚å“‘å‰§æ˜¯æŒ‡é€šè¿‡æ‰‹åŠ¿å’ŒåŠ¨ä½œè¡¨è¾¾æƒ…æ„Ÿè€Œæ— éœ€è¨€è¯­ï¼Œè¿™ä¸ºè§£é‡Šéè¨€è¯­ç¤¾ä¼šæ²Ÿé€šå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæœºä¼šã€‚æˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œåä¸ºMimeQAï¼Œè¯¥æ•°æ®é›†é€šè¿‡ä»YouTubeè·å–8å°æ—¶çš„è§†é¢‘ç‰‡æ®µï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«806ä¸ªç»è¿‡ä»”ç»†æ ‡æ³¨å’ŒéªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ—¨åœ¨æ£€æµ‹éè¨€è¯­ç¤¾ä¼šæ¨ç†èƒ½åŠ›ã€‚ä½¿ç”¨MimeQAï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆvLLMï¼‰ï¼Œå‘ç°å®ƒä»¬çš„æ€»ä½“å‡†ç¡®ç‡è¾ƒä½ï¼ŒèŒƒå›´åœ¨20-30%ï¼Œè€Œäººç±»çš„å‡†ç¡®ç‡ä¸º86%ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒvLLMå¾€å¾€æ— æ³•å¯¹æƒ³è±¡ä¸­çš„ç‰©ä½“è¿›è¡Œå®šä½ï¼Œè¿‡äºä¾èµ–æ–‡æœ¬æç¤ºï¼Œè€Œå¿½ç•¥äº†å¾®å¦™çš„éè¨€è¯­äº’åŠ¨ã€‚æˆ‘ä»¬å¸Œæœ›æ¿€å‘æœªæ¥ç ”ç©¶å‡ºèƒ½å¤Ÿè§£é‡Šéè¨€è¯­äººç±»äº’åŠ¨çš„çœŸæ­£ç¤¾ä¼šæ™ºèƒ½çš„AIæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16671v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€äººå·¥æ™ºèƒ½åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ—¥ç›Šæ™®åŠï¼Œå…·å¤‡ç†è§£å¹¶æ— ç¼ä¸äººç±»äº’åŠ¨çš„èƒ½åŠ›çš„ç¤¾ä¼šæ™ºèƒ½AIå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„äººå·¥æ™ºèƒ½ç¤¾ä¼šæ¨ç†ç ”ç©¶ä¸»è¦ä¾èµ–äºè¯­è¨€æˆ–è¯­è¨€ä¸»å¯¼çš„è¯„ä¼°æ–¹æ³•å’Œè®­ç»ƒæ¨¡å‹ï¼Œå¯¼è‡´ç³»ç»Ÿè™½ç„¶å£å¤´æ²Ÿé€šèƒ½åŠ›æœ‰æ‰€æå‡ï¼Œä½†åœ¨éè¯­è¨€ç¤¾äº¤ç†è§£æ–¹é¢ä»æœ‰å›°éš¾ã€‚ä¸ºè§£å†³è¿™ä¸€å±€é™ï¼Œæœ¬ç ”ç©¶åˆ©ç”¨ä¸°å¯Œçš„éè¯­è¨€ç¤¾äº¤äº’åŠ¨æ•°æ®â€”â€”å“‘å‰§è§†é¢‘ï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†MimeQAã€‚è¯¥æ•°æ®é›†åŒ…å«ä»YouTubeæ”¶é›†çš„8å°æ—¶è§†é¢‘ç‰‡æ®µï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªåŒ…å«806ç»„ç²¾å¿ƒæ ‡æ³¨å’ŒéªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ—¨åœ¨æµ‹è¯•éè¯­è¨€ç¤¾äº¤æ¨ç†èƒ½åŠ›ã€‚åˆ©ç”¨MimeQAè¯„ä¼°å½“å‰å…ˆè¿›çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆvLLMï¼‰ï¼Œå‘ç°å…¶æ•´ä½“å‡†ç¡®ç‡è¾ƒä½ï¼Œä»…ä¸º20-30%ï¼Œè€Œäººç±»å‡†ç¡®ç‡åˆ™é«˜è¾¾86%ã€‚åˆ†æè¡¨æ˜ï¼ŒvLLMå¾€å¾€æ— æ³•å‡†ç¡®ç†è§£è™šæ„ç‰©ä½“å¹¶è¿‡åº¦ä¾èµ–æ–‡æœ¬æç¤ºè€Œå¿½è§†å¾®å¦™çš„éè¯­è¨€äº’åŠ¨ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¿€å‘æœªæ¥ç ”ç©¶èƒ½å¤ŸçœŸæ­£è§£è¯»éè¯­è¨€äººç±»äº’åŠ¨çš„ç¤¾ä¼šæ™ºèƒ½AIæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰AIç¤¾ä¼šæ¨ç†ç ”ç©¶ä¸»è¦ä¾§é‡äºè¯­è¨€æ²Ÿé€šï¼Œä½†åœ¨éè¯­è¨€ç¤¾äº¤ç†è§£æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ä¸ºæé«˜AIåœ¨éè¯­è¨€ç¤¾äº¤ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†MimeQAï¼ŒåŒ…å«å“‘å‰§è§†é¢‘ä¸­çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œæ—¨åœ¨æµ‹è¯•éè¯­è¨€ç¤¾äº¤æ¨ç†ã€‚</li>
<li>åˆ©ç”¨MimeQAè¯„ä¼°å½“å‰å…ˆè¿›çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆvLLMï¼‰ï¼Œå‘ç°å…¶æ•´ä½“å‡†ç¡®ç‡è¾ƒä½ã€‚</li>
<li>vLLMåœ¨ç†è§£è™šæ„ç‰©ä½“å’Œå¾®å¦™éè¯­è¨€äº’åŠ¨æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ä¸äººç±»ç›¸æ¯”ï¼ŒvLLMçš„å‡†ç¡®ç‡è¾ƒä½ï¼Œè¡¨æ˜å…¶åœ¨éè¯­è¨€ç¤¾äº¤ç†è§£æ–¹é¢ä»æœ‰è¾ƒå¤§æå‡ç©ºé—´ã€‚</li>
<li>æœ¬ç ”ç©¶æ—¨åœ¨æ¿€å‘æœªæ¥ç ”ç©¶èƒ½å¤ŸçœŸæ­£è§£è¯»éè¯­è¨€äººç±»äº’åŠ¨çš„ç¤¾ä¼šæ™ºèƒ½AIæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b1e9f1e3643292be31c7629a954b5707.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-256e9b928404dfb753c1ca61a256a06a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55a07b0c0154d820037bb07a9f81bb48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1541d7161bf7b964226627f27f36b400.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7817dfebf9895c184f90d0959b91297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c167de04d2239d6b9ae63d681453d6ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cad19e001f684727f94f19935681f656.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLMs-on-the-Line-Data-Determines-Loss-to-Loss-Scaling-Laws"><a href="#LLMs-on-the-Line-Data-Determines-Loss-to-Loss-Scaling-Laws" class="headerlink" title="LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws"></a>LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws</h2><p><strong>Authors:Prasanna Mayilvahanan, ThaddÃ¤us Wiedemer, Sayak Mallick, Matthias Bethge, Wieland Brendel</strong></p>
<p>Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency. </p>
<blockquote>
<p>æ‰©å±•å®šå¾‹é€šè¿‡ä¸ºæ¨¡å‹å¤§å°ã€ä»¤ç‰Œå’Œè®¡ç®—ä¹‹é—´çš„æœ€ä½³å¹³è¡¡æä¾›ä¼°è®¡ï¼ŒæŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚æœ€è¿‘ï¼ŒæŸå¤±è·¨é¢„è®­ç»ƒæ•°æ®é›†å’Œä¸‹æ¸¸ä»»åŠ¡ä¹‹é—´çš„æŸå¤±ç¼©æ”¾å®šå¾‹ä½œä¸ºç†è§£å’Œæ”¹è¿›LLMæ€§èƒ½çš„å¼ºå¤§å·¥å…·è€Œå‡ºç°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å“ªäº›å› ç´ æœ€å¼ºçƒˆåœ°å½±å“æŸå¤±åˆ°æŸå¤±çš„ç¼©æ”¾ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé¢„è®­ç»ƒæ•°æ®å’Œåˆ†è¯å™¨å†³å®šäº†ç¼©æ”¾è¶‹åŠ¿ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¨¡å‹å¤§å°ã€ä¼˜åŒ–è¶…å‚æ•°ï¼Œç”šè‡³æ˜¯åƒLlamaçš„åŸºäºè½¬æ¢å™¨æ¨¡å‹å’ŒMambaçš„çŠ¶æ€ç©ºé—´æ¨¡å‹è¿™æ ·çš„é‡å¤§æ¶æ„å·®å¼‚ï¼Œå½±å“æœ‰é™ã€‚å› æ­¤ï¼Œå®è·µè€…åº”è¯¥ä»”ç»†æŒ‘é€‰åˆé€‚çš„é¢„è®­ç»ƒæ•°æ®é›†ä»¥è·å¾—æœ€ä½³çš„ä¸‹æ¸¸æ€§èƒ½ï¼Œè€Œæ¶æ„å’Œå…¶ä»–è®¾ç½®å¯ä»¥è‡ªç”±ä¼˜åŒ–ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12120v2">PDF</a> ICML 2025 camera-ready version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å½±å“æŸå¤±-æŸå¤±ç¼©æ”¾çš„å…³é”®å› ç´ ï¼Œå‘ç°é¢„è®­ç»ƒæ•°æ®å’Œåˆ†è¯å™¨å¯¹ç¼©æ”¾è¶‹åŠ¿æœ‰å†³å®šæ€§å½±å“ï¼Œè€Œæ¨¡å‹å¤§å°ã€ä¼˜åŒ–è¶…å‚æ•°ä»¥åŠæ¶æ„å·®å¼‚ï¼ˆå¦‚åŸºäºè½¬æ¢æ¨¡å‹çš„Llamaå’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹çš„Mambaï¼‰çš„å½±å“æœ‰é™ã€‚å› æ­¤ï¼Œå®è·µè€…åº”ä¸ºè·å¾—æœ€ä½³ä¸‹æ¸¸æ€§èƒ½è€Œè°¨æ…é€‰æ‹©é€‚å½“çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œè€Œæ¶æ„å’Œå…¶ä»–è®¾ç½®åˆ™å¯ä»¥è‡ªç”±ä¼˜åŒ–ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸå¤±-æŸå¤±ç¼©æ”¾è§„å¾‹æ˜¯ç†è§£å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„é‡è¦å·¥å…·ã€‚</li>
<li>é¢„è®­ç»ƒæ•°æ®å’Œåˆ†è¯å™¨æ˜¯å½±å“æŸå¤±-æŸå¤±ç¼©æ”¾è¶‹åŠ¿çš„å…³é”®å› ç´ ã€‚</li>
<li>æ¨¡å‹å¤§å°ã€ä¼˜åŒ–è¶…å‚æ•°å’Œæ¶æ„å·®å¼‚å¯¹æŸå¤±-æŸå¤±ç¼©æ”¾çš„å½±å“æœ‰é™ã€‚</li>
<li>å®è·µè€…åº”è¯¥ä¸ºæœ€ä½³ä¸‹æ¸¸æ€§èƒ½è°¨æ…é€‰æ‹©é€‚å½“çš„é¢„è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>æ¶æ„å’Œå…¶ä»–è®¾ç½®å¯ä»¥ä¼˜åŒ–ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚Llamaå’ŒMambaï¼‰åœ¨æŸå¤±-æŸå¤±ç¼©æ”¾æ–¹é¢çš„è¡¨ç°å·®å¼‚ä¸å¤§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-309ec74c32308ca2ea01595bd43c60e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0576686751c83aecc1680a33bb4ba7be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-103ef569317f338ed982513c0d557e2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d157b76db35afb726956b70417e515fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beffac34df67309b2e07d229e748d46b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-Effective-Extraction-and-Evaluation-of-Factual-Claims"><a href="#Towards-Effective-Extraction-and-Evaluation-of-Factual-Claims" class="headerlink" title="Towards Effective Extraction and Evaluation of Factual Claims"></a>Towards Effective Extraction and Evaluation of Factual Claims</h2><p><strong>Authors:Dasha Metropolitansky, Jonathan Larson</strong></p>
<p>A common strategy for fact-checking long-form content generated by Large Language Models (LLMs) is extracting simple claims that can be verified independently. Since inaccurate or incomplete claims compromise fact-checking results, ensuring claim quality is critical. However, the lack of a standardized evaluation framework impedes assessment and comparison of claim extraction methods. To address this gap, we propose a framework for evaluating claim extraction in the context of fact-checking along with automated, scalable, and replicable methods for applying this framework, including novel approaches for measuring coverage and decontextualization. We also introduce Claimify, an LLM-based claim extraction method, and demonstrate that it outperforms existing methods under our evaluation framework. A key feature of Claimify is its ability to handle ambiguity and extract claims only when there is high confidence in the correct interpretation of the source text. </p>
<blockquote>
<p>é’ˆå¯¹ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„é•¿ç¯‡å†…å®¹çš„äº‹å®æ ¸æŸ¥ï¼Œä¸€ç§å¸¸è§ç­–ç•¥æ˜¯æå–å¯ä»¥ç‹¬ç«‹éªŒè¯çš„ç®€å•å£°æ˜ã€‚ç”±äºä¸å‡†ç¡®æˆ–ä¸å®Œæ•´çš„å£°æ˜ä¼šæŸå®³äº‹å®æ ¸æŸ¥ç»“æœï¼Œå› æ­¤ç¡®ä¿å£°æ˜çš„è´¨é‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶é˜»ç¢äº†å£°æ˜æå–æ–¹æ³•çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºäº‹å®æ ¸æŸ¥èƒŒæ™¯ä¸‹è¯„ä¼°å£°æ˜æå–çš„æ¡†æ¶ï¼Œä»¥åŠåº”ç”¨æ­¤æ¡†æ¶çš„è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•å’Œå¯å¤åˆ¶çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æµ‹é‡è¦†ç›–ç‡å’Œè„±ç¦»è¯­å¢ƒçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºLLMçš„å£°æ˜æå–æ–¹æ³•Claimifyï¼Œå¹¶è¯æ˜åœ¨æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶ä¸‹ï¼Œå®ƒçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚Claimifyçš„ä¸€ä¸ªå…³é”®åŠŸèƒ½æ˜¯å®ƒèƒ½å¤Ÿå¤„ç†æ­§ä¹‰ï¼Œä»…åœ¨æ­£ç¡®è§£é‡Šæºæ–‡æœ¬æ—¶è¡¨ç°å‡ºé«˜ä¿¡å¿ƒæ—¶æ‰æå–å£°æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.10855v2">PDF</a> ACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„é•¿å½¢å¼å†…å®¹çš„å¸¸è§äº‹å®æ ¸æŸ¥ç­–ç•¥æ˜¯æå–å¯ä»¥ç‹¬ç«‹éªŒè¯çš„ç®€å•å£°æ˜ã€‚ä¸ºç¡®ä¿äº‹å®æ ¸æŸ¥ç»“æœçš„å‡†ç¡®æ€§ï¼Œç¡®ä¿å£°æ˜è´¨é‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œé˜»ç¢äº†å£°æ˜æå–æ–¹æ³•çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºäº‹å®æ ¸æŸ¥èƒŒæ™¯ä¸‹è¯„ä¼°å£°æ˜æå–çš„æ¡†æ¶ï¼Œä»¥åŠå¯è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•å’Œå¯å¤åˆ¶çš„å®æ–½è¿™ä¸€æ¡†æ¶çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æµ‹é‡è¦†ç›–ç‡å’Œè„±ç¦»ä¸Šä¸‹æ–‡çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†åŸºäºLLMçš„å£°æ˜æå–æ–¹æ³•Claimifyï¼Œå¹¶åœ¨æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶ä¸‹è¯æ˜å…¶è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚Claimifyçš„å…³é”®åŠŸèƒ½ä¹‹ä¸€æ˜¯å®ƒèƒ½å¤Ÿå¤„ç†æ­§ä¹‰ï¼Œå¹¶åœ¨å¯¹æºæ–‡æœ¬çš„æ­£ç¡®è§£é‡Šæœ‰é«˜åº¦ä¿¡å¿ƒæ—¶ä»…æå–å£°æ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨äº‹å®æ ¸æŸ¥ä¸­ï¼Œç¡®ä¿å£°æ˜è´¨é‡è‡³å…³é‡è¦ï¼Œå› ä¸ºä¸å‡†ç¡®æˆ–ä¸å®Œæ•´çš„å£°æ˜ä¼šå½±å“äº‹å®æ ¸æŸ¥ç»“æœçš„å‡†ç¡®æ€§ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶é˜»ç¢äº†å£°æ˜æå–æ–¹æ³•çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚</li>
<li>æå‡ºä¸€ä¸ªç”¨äºè¯„ä¼°äº‹å®æ ¸æŸ¥ä¸­å£°æ˜æå–çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬æµ‹é‡è¦†ç›–ç‡å’Œè„±ç¦»ä¸Šä¸‹æ–‡çš„æ–°æ–¹æ³•ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å£°æ˜æå–æ–¹æ³•Claimifyã€‚</li>
<li>Claimifyå…·æœ‰å¤„ç†æ­§ä¹‰çš„èƒ½åŠ›ï¼Œåªåœ¨æœ‰é«˜åº¦ä¿¡å¿ƒæ­£ç¡®è§£é‡Šæºæ–‡æœ¬æ—¶æå–å£°æ˜ã€‚</li>
<li>åœ¨è¯„ä¼°æ¡†æ¶ä¸‹ï¼ŒClaimifyçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.10855">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b6900dae7fb00c65095527ca3e54de3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d85c305495e14c11be52e2c8acbafae.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Emergent-Response-Planning-in-LLMs"><a href="#Emergent-Response-Planning-in-LLMs" class="headerlink" title="Emergent Response Planning in LLMs"></a>Emergent Response Planning in LLMs</h2><p><strong>Authors:Zhichen Dong, Zhanhui Zhou, Zhixuan Liu, Chao Yang, Chaochao Lu</strong></p>
<p>In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\textit{structure attributes}$ (e.g., response length, reasoning steps), $\textit{content attributes}$ (e.g., character choices in storywriting, multiple-choice answers at the end of response), and $\textit{behavior attributes}$ (e.g., answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggest potential applications for improving transparency and generation control. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶è¢«è®­ç»ƒç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œä½†è¡¨ç°å‡ºæ–°å…´çš„è®¡åˆ’è¡Œä¸ºï¼š<strong>ä»–ä»¬çš„éšè—è¡¨ç¤ºç¼–ç äº†ä¸‹ä¸€ä¸ªæ ‡è®°ä¹‹å¤–çš„æœªæ¥è¾“å‡º</strong>ã€‚é€šè¿‡ç®€å•çš„æ¢æµ‹ï¼Œæˆ‘ä»¬è¯æ˜äº†LLMæç¤ºè¡¨ç¤ºåŒ…å«äº†å…¶æ•´ä¸ªå“åº”çš„å…¨å±€å±æ€§ï¼ŒåŒ…æ‹¬<strong>ç»“æ„å±æ€§</strong>ï¼ˆå¦‚å“åº”é•¿åº¦ã€æ¨ç†æ­¥éª¤ï¼‰ã€<strong>å†…å®¹å±æ€§</strong>ï¼ˆå¦‚æ•…äº‹å†™ä½œä¸­çš„å­—ç¬¦é€‰æ‹©ã€å“åº”æœ«å°¾çš„å¤šé¡¹é€‰æ‹©é¢˜ç­”æ¡ˆï¼‰ï¼Œä»¥åŠ<strong>è¡Œä¸ºå±æ€§</strong>ï¼ˆå¦‚ç­”æ¡ˆä¿¡å¿ƒã€äº‹å®ä¸€è‡´æ€§ï¼‰ã€‚é™¤äº†è¯†åˆ«å“åº”è®¡åˆ’å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å®ƒåœ¨ä¸åŒä»»åŠ¡ä¸­å¦‚ä½•éšæ¨¡å‹å¤§å°å˜åŒ–ä»¥åŠå¦‚ä½•åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‘å±•ã€‚LLMåœ¨éšè—è¡¨ç¤ºä¸­ä¸ºæœªæ¥è®¡åˆ’çš„äº‹å®ä¸ºæ”¹è¿›é€æ˜åº¦å’Œç”Ÿæˆæ§åˆ¶æä¾›äº†æ½œåœ¨çš„åº”ç”¨å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06258v2">PDF</a> ICML 2025</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„åŒæ—¶ï¼Œå…¶éšè—è¡¨å¾ä¼šè¡¨ç°å‡ºæœªæ¥è¾“å‡ºçš„è¡Œä¸ºè§„åˆ’ã€‚é€šè¿‡ç®€å•çš„æ¢æµ‹æ–¹æ³•ï¼Œå‘ç°LLMçš„æç¤ºè¡¨å¾åŒ…å«äº†å…¶æ•´ä½“å“åº”çš„å…¨å±€å±æ€§ï¼Œå¦‚ç»“æ„å±æ€§ã€å†…å®¹å±æ€§å’Œè¡Œä¸ºå±æ€§ç­‰ã€‚è¯¥ç ”ç©¶æ¢è®¨äº†è¿™ç§è§„åˆ’èƒ½åŠ›åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ¨¡å‹è§„æ¨¡å˜åŒ–ä»¥åŠåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¼”å˜æƒ…å†µã€‚è¿™ä¸ºæé«˜é€æ˜åº¦å’Œç”Ÿæˆæ§åˆ¶æä¾›äº†æ½œåœ¨çš„åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMçš„éšè—è¡¨å¾å¯ä»¥ç¼–ç æœªæ¥çš„è¾“å‡ºï¼Œè¿™æ˜¾ç¤ºäº†å…¶è®¡åˆ’è¡Œä¸ºã€‚</li>
<li>é€šè¿‡ç®€å•æ¢æµ‹å¯ä»¥å‘ç°LLMçš„æç¤ºè¡¨å¾åŒ…å«å…¶å“åº”çš„å…¨å±€å±æ€§ã€‚</li>
<li>LLMçš„è®¡åˆ’èƒ½åŠ›æ¶‰åŠç»“æ„å±æ€§ï¼ˆå¦‚å“åº”é•¿åº¦å’Œæ¨ç†æ­¥éª¤ï¼‰ã€‚</li>
<li>å†…å®¹å±æ€§åŒ…æ‹¬æ•…äº‹å†™ä½œä¸­çš„å­—ç¬¦é€‰æ‹©å’Œå“åº”ç»“æŸæ—¶çš„å¤šé¡¹é€‰æ‹©ç­”æ¡ˆã€‚</li>
<li>è¡Œä¸ºå±æ€§åŒ…æ‹¬ç­”æ¡ˆçš„è‡ªä¿¡åº¦å’Œäº‹å®ä¸€è‡´æ€§ã€‚</li>
<li>LLMçš„è§„åˆ’èƒ½åŠ›éšæ¨¡å‹è§„æ¨¡çš„å˜åŒ–è€Œå˜åŒ–ï¼Œå¹¶éšç€ç”Ÿæˆè¿‡ç¨‹çš„å‘å±•è€Œæ¼”å˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5b3857c87c26151c325ba9a683ccca6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebc02921b5cfb4420d3cc41f7d68a190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2749f5ba619621df548dd5fc47cfac3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3c3e2b54c30dffe39833c24f8e3c54c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5384bbb0da36833e0eba442446e80c7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-10/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f32a620e1dcc6c1169b3c854b451e2bd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  PersonaAgent When Large Language Model Agents Meet Personalization at   Test Time
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-10/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-866a0195fc79c9b0f61efb2762a43361.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-10  PuzzleWorld A Benchmark for Multimodal, Open-Ended Reasoning in   Puzzlehunts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32102k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
