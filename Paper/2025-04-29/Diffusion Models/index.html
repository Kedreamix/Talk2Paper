<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-29  Optimizing Multi-Round Enhanced Training in Diffusion Models for   Improved Preference Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-48564c5809f4b7117d483ec581a40f88.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-01
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-29-更新"><a href="#2025-04-29-更新" class="headerlink" title="2025-04-29 更新"></a>2025-04-29 更新</h1><h2 id="Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Preference-Understanding"><a href="#Optimizing-Multi-Round-Enhanced-Training-in-Diffusion-Models-for-Improved-Preference-Understanding" class="headerlink" title="Optimizing Multi-Round Enhanced Training in Diffusion Models for   Improved Preference Understanding"></a>Optimizing Multi-Round Enhanced Training in Diffusion Models for   Improved Preference Understanding</h2><p><strong>Authors:Kun Li, Jianhui Wang, Yangfan He, Xinyuan Song, Ruoyu Wang, Hongyang He, Wenxin Zhang, Jiaqi Chen, Keqin Li, Sida Li, Miao Zhang, Tianyu Shi, Xueqian Wang</strong></p>
<p>Generative AI has significantly changed industries by enabling text-driven image generation, yet challenges remain in achieving high-resolution outputs that align with fine-grained user preferences. Consequently, multi-round interactions are necessary to ensure the generated images meet expectations. Previous methods enhanced prompts via reward feedback but did not optimize over a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation (VCA) framework incorporating human-in-the-loop feedback, leveraging a well-trained reward model aligned with human preferences. Using a diverse multi-turn dialogue dataset, our framework applies multiple reward functions, such as diversity, consistency, and preference feedback, while fine-tuning the diffusion model through LoRA, thus optimizing image generation based on user input. We also construct multi-round dialogue datasets of prompts and image pairs aligned with user intent. Experiments demonstrate that our method outperforms state-of-the-art baselines, significantly improving image consistency and alignment with user intent. Our approach consistently surpasses competing models in user satisfaction, especially in multi-turn dialogue scenarios. </p>
<blockquote>
<p>生成式人工智能已经通过文本驱动的图片生成显著改变了各行各业，然而，在实现与用户精细偏好对齐的高分辨率输出方面仍存在挑战。因此，多轮交互是必要的，以确保生成的图像满足期望。之前的方法通过奖励反馈来增强提示，但并没有在多个轮次对话数据集上进行优化。在这项工作中，我们提出了一个视觉协同适应（VCA）框架，该框架结合了人类反馈回路，利用训练良好的奖励模型与人类偏好对齐。使用多样化的多轮对话数据集，我们的框架应用多个奖励函数，如多样性、一致性和偏好反馈等，同时通过LoRA微调扩散模型，从而基于用户输入优化图像生成。我们还构建了与用户意图对齐的多轮对话数据集，包含提示和图像对。实验表明，我们的方法优于最先进的基线方法，显著提高了图像的一致性和与用户意图的对齐程度。我们的方法在用户满意度方面始终超过竞争模型，特别是在多轮对话场景中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18204v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2503.17660</p>
<p><strong>Summary</strong><br>     本文介绍了生成式AI如何通过文本驱动图像生成来改变行业，但仍然存在实现高分辨率输出和满足精细用户偏好方面的挑战。因此，需要多轮交互来确保生成的图像符合预期。之前的方法通过奖励反馈增强了提示，但并没有在一个多轮对话数据集上进行优化。本文提出了一个视觉协同适应（VCA）框架，该框架结合了人类反馈循环，利用训练良好的奖励模型与人类偏好对齐。使用多样化的多轮对话数据集，我们的框架应用了多个奖励函数，如多样性、一致性和偏好反馈，同时通过LoRA微调扩散模型，从而基于用户输入优化图像生成。实验证明，我们的方法在图像一致性、用户意图对齐方面优于其他模型。特别是在多轮对话场景中，我们的方法在用户满意度方面始终超越竞争对手模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式AI通过文本驱动图像生成改变了行业，但仍面临实现高分辨率输出和满足精细用户偏好的挑战。</li>
<li>多轮交互是必要的，以确保生成的图像符合用户期望和需求。</li>
<li>之前的方法主要侧重于通过奖励反馈增强提示，但没有在多轮对话数据集上进行优化。</li>
<li>提出的视觉协同适应（VCA）框架结合了人类反馈循环，利用训练良好的奖励模型与人类偏好对齐。</li>
<li>该框架使用多个奖励函数（如多样性、一致性和偏好反馈），并通过LoRA微调扩散模型。</li>
<li>实验证明，该方法在图像一致性、用户意图对齐方面优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18204">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14dc3b23b29b905ebce9f68056a6155d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f883335208dd67635b17478941be23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f902e8f9d365ea97005d5e50ee543456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4081a9344132c11648479f5f52492f6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54b3213f4f3d571f3d79be8021e61541.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-66eaaa6d51421f4656a0d7b625757cb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30ea5d398c0c9730489667b1a39ebe77.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Enhancing-Privacy-Utility-Trade-offs-to-Mitigate-Memorization-in-Diffusion-Models"><a href="#Enhancing-Privacy-Utility-Trade-offs-to-Mitigate-Memorization-in-Diffusion-Models" class="headerlink" title="Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in   Diffusion Models"></a>Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in   Diffusion Models</h2><p><strong>Authors:Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu</strong></p>
<p>Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art. </p>
<blockquote>
<p>文本到图像的扩散模型在创建与用户提示高度匹配的图片方面表现出了显著的能力，但它们倾向于记忆训练集图片，引发了人们对生成图片原创性和隐私问题的担忧，这可能导致模型所有者与用户双方面临法律和隐私问题，特别是当记忆的图片包含专有内容时。虽然已有方法建议缓解这些问题，但增强隐私往往会导致输出结果的实用性显著降低，如文本对齐分数所示。为了弥补研究空白，我们提出了一种新方法PRSS，它通过整合提示重新锚定（PR）来提高隐私，并融入语义提示搜索（SS）以增强实用性，对扩散模型中的无分类器引导方法进行了改进。在不同隐私级别的广泛实验表明，我们的方法持续改善了隐私与实用性的权衡，创下了新的最先进的记录。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18032v1">PDF</a> Accepted at CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://chenchen-usyd.github.io/PRSS-Project-Page/">https://chenchen-usyd.github.io/PRSS-Project-Page/</a></p>
<p><strong>摘要</strong></p>
<p>文本到图像的扩散模型在创建与用户提示高度匹配的图片方面表现出卓越的能力，但其倾向于记忆训练集图片的问题引发了人们对生成图片原创性和隐私的担忧，可能导致模型所有者与用户面临法律和隐私问题，特别是当记忆中的图片包含专有内容时。虽然已有方法建议缓解这些问题，但增强隐私往往会导致输出效用显著降低，如文本对齐分数所示。为了弥补研究空白，我们提出了一种新方法PRSS，它通过改进扩散模型中的无分类器引导方法，通过集成提示重新锚定（PR）提高隐私性，并结合语义提示搜索（SS）提高实用性。跨越各种隐私级别的广泛实验表明，我们的方法持续改善了隐私与实用性的权衡，建立了新的技术前沿。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本到图像的扩散模型具备出色的创建与用户提示匹配图像的能力。</li>
<li>扩散模型存在记忆训练集图片的问题，引发关于原创性和隐私的担忧。</li>
<li>增强隐私可能会导致输出效用显著降低。</li>
<li>提出了一种新方法PRSS，通过集成提示重新锚定和语义提示搜索，改进扩散模型的隐私性和实用性。</li>
<li>PRSS方法提高了隐私与实用性的权衡。</li>
<li>PRSS方法建立了新的技术前沿。</li>
<li>通过广泛实验验证了PRSS方法的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18032">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-927da7066ccecda6d2679380d1a4887d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36b1f59075f2f45345219e7f8831a9fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa292bc17745392834ca8a6080f42b33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbe5ebecc5f77558200d869b8b70de6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a825d7f998831ccade728aea7b959981.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Diffusion-Driven-Universal-Model-Inversion-Attack-for-Face-Recognition"><a href="#Diffusion-Driven-Universal-Model-Inversion-Attack-for-Face-Recognition" class="headerlink" title="Diffusion-Driven Universal Model Inversion Attack for Face Recognition"></a>Diffusion-Driven Universal Model Inversion Attack for Face Recognition</h2><p><strong>Authors:Hanrui Wang, Shuo Wang, Chun-Shien Lu, Isao Echizen</strong></p>
<p>Facial recognition technology poses significant privacy risks, as it relies on biometric data that is inherently sensitive and immutable if compromised. To mitigate these concerns, face recognition systems convert raw images into embeddings, traditionally considered privacy-preserving. However, model inversion attacks pose a significant privacy threat by reconstructing these private facial images, making them a crucial tool for evaluating the privacy risks of face recognition systems. Existing methods usually require training individual generators for each target model, a computationally expensive process. In this paper, we propose DiffUMI, a training-free diffusion-driven universal model inversion attack for face recognition systems. DiffUMI is the first approach to apply a diffusion model for unconditional image generation in model inversion. Unlike other methods, DiffUMI is universal, eliminating the need for training target-specific generators. It operates within a fixed framework and pretrained diffusion model while seamlessly adapting to diverse target identities and models. DiffUMI breaches privacy-preserving face recognition systems with state-of-the-art success, demonstrating that an unconditional diffusion model, coupled with optimized adversarial search, enables efficient and high-fidelity facial reconstruction. Additionally, we introduce a novel application of out-of-domain detection (OODD), marking the first use of model inversion to distinguish non-face inputs from face inputs based solely on embeddings. </p>
<blockquote>
<p>人脸识别技术存在重大的隐私风险，因为它依赖于生物特征数据，这些数据本质上是敏感的，一旦泄露，具有不可变性。为了缓解这些担忧，人脸识别系统将原始图像转化为嵌入形式，这被认为是隐私保护的。然而，模型反转攻击通过重建这些私有面部图像构成重大隐私威胁，成为评估人脸识别系统隐私风险的重要工具。现有方法通常需要针对每个目标模型进行个体生成器的训练，这是一个计算成本高昂的过程。在本文中，我们提出了DiffUMI，这是一种无需训练的人脸识别系统扩散驱动通用模型反转攻击。DiffUMI是第一个将扩散模型应用于无条件图像生成的方法。与其他方法不同，DiffUMI具有通用性，无需针对目标进行训练生成器。它在固定的框架和预先训练的扩散模型内运行，无缝适应不同的目标身份和模型。DiffUMI以最先进的成功率突破了隐私保护人脸识别系统，证明无条件扩散模型结合优化的对抗性搜索可实现高效、高保真度的面部重建。此外，我们引入了域外检测（OODD）的新应用，首次使用模型反转来仅根据嵌入来区分非面部输入和面部输入。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18015v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>面部识别技术存在重大隐私风险，因为该技术依赖于生物特征数据，一旦泄露即可能造成敏感信息泄露。为缓解这些担忧，面部识别系统将原始图像转化为嵌入数据，一般认为这一过程能保护隐私。然而，模型逆向攻击通过重建这些私人面部图像，对隐私构成严重威胁，成为评估面部识别系统隐私风险的重要工具。现有方法通常需要针对每个目标模型进行个体生成器训练，这一过程计算成本高昂。本文提出一种无需训练的扩散驱动通用模型逆向攻击方法DiffUMI。DiffUMI首次将扩散模型应用于无条件图像生成中的模型逆向。与其他方法不同，DiffUMI具有通用性，无需针对目标进行特定生成器训练。它在固定框架和预训练扩散模型内运行，并能轻松适应不同目标身份和模型。DiffUMI以高成功率突破了隐私保护面部识别系统，证明无条件扩散模型结合优化的对抗搜索可实现高效、高保真度的面部重建。此外，我们还引入了域外检测的新应用（OODD），首次使用模型逆向技术仅根据嵌入来区分非面部输入和面部输入。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>面部识别技术存在隐私泄露风险，因为依赖的生物特征数据敏感且一旦泄露后果严重。</li>
<li>为保护隐私，面部识别系统通常将原始图像转化为嵌入数据。</li>
<li>模型逆向攻击能重建面部图像，成为评估面部识别系统隐私风险的重要工具。</li>
<li>现有模型逆向攻击方法训练成本高昂，需要针对每个目标模型进行个体生成器训练。</li>
<li>DiffUMI是一种无需训练的扩散驱动通用模型逆向攻击方法，首次将扩散模型应用于无条件图像生成。</li>
<li>DiffUMI具有通用性，能在固定框架和预训练扩散模型内运行，适应不同目标身份和模型。</li>
<li>DiffUMI成功突破了隐私保护面部识别系统，并引入了域外检测的新应用（OODD）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18015">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2622eab451ae9986e0f158dacad0e4ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1e0b01c5c6c4809eea98aaaf3122fbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf3b3bd0fde15f3f484445c00f5ca855.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af736918611795d8d53839a090c1dd44.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Dual-Prompting-Image-Restoration-with-Diffusion-Transformers"><a href="#Dual-Prompting-Image-Restoration-with-Diffusion-Transformers" class="headerlink" title="Dual Prompting Image Restoration with Diffusion Transformers"></a>Dual Prompting Image Restoration with Diffusion Transformers</h2><p><strong>Authors:Dehong Kong, Fan Li, Zhixin Wang, Jiaqi Xu, Renjing Pei, Wenbo Li, WenQi Ren</strong></p>
<p>Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance. </p>
<blockquote>
<p>最新的先进图像恢复方法主要采用了基于U-Net的潜在扩散模型，但由于其能力有限，仍面临着实现高质量恢复的挑战。扩散变压器（DiTs）如SD3因其可扩展性和更好的质量而成为有前途的替代品。本文介绍了DPIR（双提示图像恢复），这是一种新型图像恢复方法，能够从多个角度有效地提取低质量图像的条件信息。具体来说，DPIR包括两个分支：低质量图像条件分支和双提示控制分支。第一个分支利用轻量级模块高效地将图像先验知识融入到DiT中。更重要的是，我们认为在图像恢复中，单纯的文本描述无法充分捕捉其丰富的视觉特征。因此，设计了双提示模块，为DiT提供额外的视觉线索，捕捉全局上下文和局部外观。提取的全局-局部视觉提示作为额外的条件控制，与文本提示一起形成双提示，极大地提高了恢复的质量。大量的实验结果证明，DPIR在图像恢复性能方面表现出卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17825v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong></p>
<p>最新图像修复方法主要采用了带有U-Net骨干的潜在扩散模型，但仍面临因能力有限而无法实现高质量修复的难题。扩散变压器（DiTs）如SD3因其较好的质量和可扩展性而崭露头角。本文介绍了一种新型图像修复方法DPIR（双提示图像修复），它通过多个角度有效地提取低质量图像的条件信息。DPIR包括两个分支：低质量图像条件分支和双提示控制分支。第一个分支利用轻量级模块高效地将图像先验知识融入DiT。更重要的是，我们认为在图像修复中，仅依靠文本描述无法完全捕捉其丰富的视觉特征。因此，设计了双提示模块，为DiT提供额外的视觉线索，捕捉全局上下文和局部外观。结合文本提示形成双重提示，极大地提高了修复质量。实验结果表明，DPIR在图像修复方面具有卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像修复领域广泛应用，但仍存在高质量修复的挑战。</li>
<li>介绍了DPIR方法，包含两个分支：低质量图像条件分支和双提示控制分支。</li>
<li>低质量图像条件分支利用轻量级模块高效融入图像先验知识。</li>
<li>仅依靠文本描述在图像修复中无法完全捕捉视觉特征。</li>
<li>双提示模块提供额外的视觉线索，同时捕捉全局上下文和局部外观。</li>
<li>结合文本提示形成的双重提示能显著提高图像修复的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-093bf89a3068a75212a5fa48c89fb324.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48564c5809f4b7117d483ec581a40f88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e9d6b16b869624a185252336c771e02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a478ac22623c490ba220499c246d3f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43bfc7efb6eb886fa47e4f32c63d3acb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a384da7b536cda2a4b63486b78c33dc5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DreamID-High-Fidelity-and-Fast-diffusion-based-Face-Swapping-via-Triplet-ID-Group-Learning"><a href="#DreamID-High-Fidelity-and-Fast-diffusion-based-Face-Swapping-via-Triplet-ID-Group-Learning" class="headerlink" title="DreamID: High-Fidelity and Fast diffusion-based Face Swapping via   Triplet ID Group Learning"></a>DreamID: High-Fidelity and Fast diffusion-based Face Swapping via   Triplet ID Group Learning</h2><p><strong>Authors:Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao, Qian He, Xinglong Wu</strong></p>
<p>In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achieve satisfactory results. DreamID establishes explicit supervision for face swapping by constructing Triplet ID Group data, significantly enhancing identity similarity and attribute preservation. The iterative nature of diffusion models poses challenges for utilizing efficient image-space loss functions, as performing time-consuming multi-step sampling to obtain the generated image during training is impractical. To address this issue, we leverage the accelerated diffusion model SD Turbo, reducing the inference steps to a single iteration, enabling efficient pixel-level end-to-end training with explicit Triplet ID Group supervision. Additionally, we propose an improved diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter. This robust architecture fully unlocks the power of the Triplet ID Group explicit supervision. Finally, to further extend our method, we explicitly modify the Triplet ID Group data during training to fine-tune and preserve specific attributes, such as glasses and face shape. Extensive experiments demonstrate that DreamID outperforms state-of-the-art methods in terms of identity similarity, pose and expression preservation, and image fidelity. Overall, DreamID achieves high-quality face swapping results at 512*512 resolution in just 0.6 seconds and performs exceptionally well in challenging scenarios such as complex lighting, large angles, and occlusions. </p>
<blockquote>
<p>本文介绍了DreamID，这是一款基于扩散的面貌替换模型，实现了高水平的身份相似性、属性保留、图像保真度和快速推理速度。不同于典型的面貌替换训练过程，它通常依赖于隐式监督，且难以实现令人满意的结果。DreamID通过构建Triplet ID Group数据实现对面貌替换的显式监督，从而显著提高身份相似性和属性保留。扩散模型的迭代性质给利用高效的图像空间损失函数带来了挑战，因为在训练过程中进行耗时的多步采样以获得生成图像是不切实际的。为了解决这个问题，我们利用了加速扩散模型SD Turbo，将推理步骤减少到单次迭代，能够在显式Triplet ID Group监督下进行高效的像素级端到端训练。此外，我们提出了改进的基于扩散的模型架构，包括SwapNet、FaceNet和ID适配器。这一稳健的架构充分释放了Triplet ID Group显式监督的威力。最后，为了进一步完善我们的方法，我们在训练过程中显式修改了Triplet ID Group数据，以微调并保留特定属性，如眼镜和脸型。大量实验表明，DreamID在身份相似性、姿势和表情保留以及图像保真度方面均优于最新技术方法。总的来说，DreamID在512*512分辨率下实现了高质量的面貌替换结果，仅需0.6秒，且在复杂光照、大角度和遮挡等挑战场景下表现尤为出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14509v3">PDF</a> Project: <a target="_blank" rel="noopener" href="https://superhero-7.github.io/DreamID/">https://superhero-7.github.io/DreamID/</a></p>
<p><strong>摘要</strong></p>
<p>本文介绍了DreamID，一种基于扩散技术的换脸模型，具有身份相似度高、属性保留完整、图像保真度高和推理速度快的特点。与传统的换脸训练过程不同，DreamID通过构建Triplet ID Group数据实现显式监督，显著提高了身份相似性和属性保留。为解决扩散模型的迭代性质带来的挑战，我们采用加速扩散模型SD Turbo，将推理步骤减少到单次迭代，实现了高效的像素级端到端训练，同时辅以Triplet ID Group的显式监督。此外，我们提出了改进型的基于扩散的模型架构，包括SwapNet、FaceNet和ID Adapter，充分发挥Triplet ID Group显式监督的威力。最后，我们在训练过程中明确修改了Triplet ID Group数据，以精细调整并保留特定属性，如眼镜和脸型。实验表明，DreamID在身份相似性、姿势和表情保留以及图像保真度方面优于现有技术。总的来说，DreamID在512*512分辨率下实现了高质量换脸，只需0.6秒即可完成，并在复杂光照、大角度和遮挡等挑战场景下表现优异。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>DreamID是基于扩散技术的换脸模型，实现高身份相似性、属性保留、图像保真度和快速推理。</li>
<li>通过构建Triplet ID Group数据实现显式监督，提高身份相似性和属性保留。</li>
<li>采用加速扩散模型SD Turbo，减少推理步骤至单次迭代，实现高效像素级端到端训练。</li>
<li>改进的基于扩散的模型架构包括SwapNet、FaceNet和ID Adapter，充分发挥显式监督的威力。</li>
<li>在训练过程中修改Triplet ID Group数据，以保留和调整特定属性，如眼镜和脸型。</li>
<li>DreamID在多项实验中表现出色，尤其是在身份相似性、姿势和表情保留以及图像质量方面。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14509">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7314fb6321a58289a81fae1eb843e45b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d048785f516ac3def442343aa72cec92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df1305be8367a6cc18f035376700d60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5c2281c747ef6711f0bef06217fa92f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83efa0a045974afbd481f4861decccf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Importance-Based-Token-Merging-for-Efficient-Image-and-Video-Generation"><a href="#Importance-Based-Token-Merging-for-Efficient-Image-and-Video-Generation" class="headerlink" title="Importance-Based Token Merging for Efficient Image and Video Generation"></a>Importance-Based Token Merging for Efficient Image and Video Generation</h2><p><strong>Authors:Haoyu Wu, Jingyi Xu, Hieu Le, Dimitris Samaras</strong></p>
<p>Token merging can effectively accelerate various vision systems by processing groups of similar tokens only once and sharing the results across them. However, existing token grouping methods are often ad hoc and random, disregarding the actual content of the samples. We show that preserving high-information tokens during merging - those essential for semantic fidelity and structural details - significantly improves sample quality, producing finer details and more coherent, realistic generations. Despite being simple and intuitive, this approach remains underexplored.   To do so, we propose an importance-based token merging method that prioritizes the most critical tokens in computational resource allocation, leveraging readily available importance scores, such as those from classifier-free guidance in diffusion models. Experiments show that our approach significantly outperforms baseline methods across multiple applications, including text-to-image synthesis, multi-view image generation, and video generation with various model architectures such as Stable Diffusion, Zero123++, AnimateDiff, or PixArt-$\alpha$. </p>
<blockquote>
<p>令牌合并通过仅处理一次相似的令牌组并在它们之间共享结果，可以有效地加速各种视觉系统。然而，现有的令牌分组方法往往是临时的和随机的，忽略了样本的实际内容。我们证明，在合并过程中保留高信息令牌——对语义保真和结构细节至关重要的令牌——可以显著提高样本质量，产生更精细的细节和更连贯、更逼真的生成内容。尽管这种方法简单直观，但仍被研究得不够深入。因此，我们提出了一种基于重要性的令牌合并方法，该方法在分配计算资源时优先考虑最重要的令牌，并利用现有的重要性评分，如扩散模型中的无分类器引导的重要性评分。实验表明，我们的方法在多个应用程序上的表现都优于基准方法，包括文本到图像合成、多视图图像生成和视频生成，以及使用各种模型架构（如Stable Diffusion、Zero123++、AnimateDiff或PixArt-α）的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16720v2">PDF</a> </p>
<p><strong>Summary</strong><br>     通过利用分组的重要性分数实现令牌合并方法，可以有效提高各类视觉系统的处理速度。现有的令牌合并方法常常忽视样本的实际内容，仅随机处理令牌分组。我们提出一种基于重要性的令牌合并方法，在资源分配中优先处理最关键的令牌，并借助现成的扩散模型中的分类器免费指导来实现。实验表明，该方法在多个应用领域中均优于基线方法，包括文本到图像合成、多视图图像生成和视频生成等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>令牌合并技术可以加速视觉系统的处理速度。</li>
<li>当前令牌分组方法常常忽视样本的实际内容，导致随机处理令牌分组。</li>
<li>通过保留高信息令牌（对语义保真和结构细节至关重要的令牌）在合并过程中可以提高样本质量。</li>
<li>我们提出了一种基于重要性的令牌合并方法，利用现有模型中的重要性分数进行优化。</li>
<li>该方法在多种应用领域中显著优于基线方法，包括文本到图像合成、多视图图像生成和视频生成等。</li>
<li>该方法适用于多种模型架构，如Stable Diffusion、Zero123++、AnimateDiff和PixArt-$\alpha$等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16720">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8fc3584a9470f5103f702f13cad82b6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc216558a2439132a301104fc642a339.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86dee26d33974aa2d067aa31d2e3f3ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d31a5dfc884ad28c62373492831c0773.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d5ca38362ba6327a6f357b2a9bbfca7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Investigating-Memorization-in-Video-Diffusion-Models"><a href="#Investigating-Memorization-in-Video-Diffusion-Models" class="headerlink" title="Investigating Memorization in Video Diffusion Models"></a>Investigating Memorization in Video Diffusion Models</h2><p><strong>Authors:Chen Chen, Enhuai Liu, Daochang Liu, Mubarak Shah, Chang Xu</strong></p>
<p>Diffusion models, widely used for image and video generation, face a significant limitation: the risk of memorizing and reproducing training data during inference, potentially generating unauthorized copyrighted content. While prior research has focused on image diffusion models (IDMs), video diffusion models (VDMs) remain underexplored. To address this gap, we first formally define the two types of memorization in VDMs (content memorization and motion memorization) in a practical way that focuses on privacy preservation and applies to all generation types. We then introduce new metrics specifically designed to separately assess content and motion memorization in VDMs. Additionally, we curate a dataset of text prompts that are most prone to triggering memorization when used as conditioning in VDMs. By leveraging these prompts, we generate diverse videos from various open-source VDMs, successfully extracting numerous training videos from each tested model. Through the application of our proposed metrics, we systematically analyze memorization across various pretrained VDMs, including text-conditional and unconditional models, on a variety of datasets. Our comprehensive study reveals that memorization is widespread across all tested VDMs, indicating that VDMs can also memorize image training data in addition to video datasets. Finally, we propose efficient and effective detection strategies for both content and motion memorization, offering a foundational approach for improving privacy in VDMs. </p>
<blockquote>
<p>扩散模型广泛应用于图像和视频生成，但它们面临一个重大局限：在推理过程中存在记忆和重现训练数据的风险，可能会生成未经授权的版权内容。虽然之前的研究主要集中在图像扩散模型（IDMs）上，但视频扩散模型（VDMs）的研究仍然不足。为了弥补这一空白，我们首先以实用方式正式定义VDMs中的两种记忆类型（内容记忆和运动记忆），重点关注隐私保护并适用于所有生成类型。然后，我们引入了专门用于单独评估VDMs中内容和运动记忆的新指标。此外，我们还整理了一组文本提示，当用作VDMs的条件时，最有可能触发记忆。通过利用这些提示，我们从各种开源VDMs生成了多样化的视频，成功地从每个测试模型中提取了大量训练视频。通过应用我们提出的指标，我们系统地分析了各种预训练VDMs中的记忆情况，包括文本条件和无条件模型，以及各种数据集。我们的综合研究表明，所有测试过的VDMs都存在广泛的记忆问题，这表明VDMs除了视频数据集外，还能记住图像训练数据。最后，我们为内容和运动记忆提出了高效且有效的检测策略，为改善VDMs中的隐私提供了基础方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21669v2">PDF</a> Accepted at DATA-FM Workshop @ ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文研究了视频扩散模型（VDMs）中的记忆化问题，包括内容记忆化和动作记忆化。文章正式定义了这两种记忆化类型，并引入专门评估VDMs中内容和动作记忆化的新指标。通过利用特定文本提示，文章成功从各种预训练VDMs中提取了训练视频。研究表明，记忆化在各类预训练VDMs中普遍存在，不仅存在于视频数据集，也存在于图像训练数据中。最后，文章提出了针对内容和动作记忆化的高效检测策略，为提高VDMs的隐私性提供了基础方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像和视频生成中广泛应用，但存在记忆化风险，可能生成未经授权版权内容。</li>
<li>视频扩散模型（VDMs）面临内容记忆化和动作记忆化问题，这两种类型被正式定义并引入新指标进行评估。</li>
<li>通过特定文本提示成功从预训练VDMs中提取训练视频，表明记忆化问题普遍存在。</li>
<li>记忆化不仅存在于视频数据集，也存在于图像训练数据中。</li>
<li>引入评估指标后发现记忆化现象在各种预训练VDMs中普遍存在。</li>
<li>文章提供了针对内容和动作记忆化的高效检测策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cc134d8c895dca306eb705642449eac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8d2f54a37e0eb2b887de2bf20f98daa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b49dce5b8edb24533db87bd5fc36a078.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16fc4055e9fef12aeaf5ac939d359b9b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Local-Memorization-in-Diffusion-Models-via-Bright-Ending-Attention"><a href="#Exploring-Local-Memorization-in-Diffusion-Models-via-Bright-Ending-Attention" class="headerlink" title="Exploring Local Memorization in Diffusion Models via Bright Ending   Attention"></a>Exploring Local Memorization in Diffusion Models via Bright Ending   Attention</h2><p><strong>Authors:Chen Chen, Daochang Liu, Mubarak Shah, Chang Xu</strong></p>
<p>Text-to-image diffusion models have achieved unprecedented proficiency in generating realistic images. However, their inherent tendency to memorize and replicate training data during inference raises significant concerns, including potential copyright infringement. In response, various methods have been proposed to evaluate, detect, and mitigate memorization. Our analysis reveals that existing approaches significantly underperform in handling local memorization, where only specific image regions are memorized, compared to global memorization, where the entire image is replicated. Also, they cannot locate the local memorization regions, making it hard to investigate locally. To address these, we identify a novel “bright ending” (BE) anomaly in diffusion models prone to memorizing training images. BE refers to a distinct cross-attention pattern observed in text-to-image diffusion models, where memorized image patches exhibit significantly greater attention to the final text token during the last inference step than non-memorized patches. This pattern highlights regions where the generated image replicates training data and enables efficient localization of memorized regions. Equipped with this, we propose a simple yet effective method to integrate BE into existing frameworks, significantly improving their performance by narrowing the performance gap caused by local memorization. Our results not only validate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon. </p>
<blockquote>
<p>文本到图像的扩散模型在生成真实图像方面取得了前所未有的熟练度。然而，它们在推理过程中固有地倾向于记忆和复制训练数据，这引发了重大担忧，包括潜在的知识产权侵犯。为了应对这一问题，已经提出了各种方法来评估、检测和缓解记忆问题。我们的分析表明，与全局记忆相比，现有方法在处理局部记忆方面存在显著不足，全局记忆是指复制整个图像，而局部记忆仅指特定图像区域的记忆。此外，它们无法定位局部记忆区域，使得难以进行局部调查。为了解决这些问题，我们确定了扩散模型中容易记忆训练图像的一种新型“明亮结束”（BE）异常。BE是指文本到图像扩散模型中观察到的一种特殊的跨注意力模式，其中记忆的图像斑块在最后一步推理中对最终文本标记的注意力显著大于非记忆斑块。这种模式突出了生成图像复制训练数据的地方，并能够有效地定位记忆的图像区域。通过运用这一模式，我们提出了一种简单而有效的方法，将BE整合到现有框架中，通过缩小局部记忆造成的性能差距，显著提高现有框架的性能。我们的结果不仅验证了新定位任务的成功执行，而且确立了所有现有任务中的最新性能水平，突显了BE现象的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21665v2">PDF</a> Accepted at ICLR 2025 (Spotlight). Project page:   <a target="_blank" rel="noopener" href="https://chenchen-usyd.github.io/BE-Project-Page/">https://chenchen-usyd.github.io/BE-Project-Page/</a></p>
<p><strong>Summary</strong></p>
<p>文本转图像扩散模型在生成真实图像方面取得了前所未有的熟练程度。然而，它们在推理过程中的固有倾向是记忆和复制训练数据，这引发了包括潜在版权侵犯在内的担忧。针对这一问题，已经提出了各种方法来评估、检测和缓解记忆问题。我们的分析发现，现有方法在局部记忆方面的表现显著较差，其中只有特定的图像区域被记忆，与整个图像都被复制的全局记忆相比。此外，它们无法定位局部记忆区域，使得本地调查变得困难。为了解决这些问题，我们确定了扩散模型中易于记忆训练图像的一种新型“明亮结束”（BE）异常现象。BE指的是文本转图像扩散模型中观察到的交叉注意力模式的独特现象，其中记忆的图像斑块在最后推理步骤中对最终文本令牌的注意力显著高于非记忆斑块。这种模式突出了生成图像复制训练数据的区域，并能够有效地定位记忆区域。基于此，我们提出了一种简单而有效的方法，将BE集成到现有框架中，通过缩小局部记忆造成的性能差距，显著提高它们的性能。我们的结果不仅验证了新定位任务的成功执行，而且在所有现有任务中建立了新的最先进的性能表现，突显了BE现象的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本转图像扩散模型能够生成高度逼真的图像，但在推理过程中存在记忆和复制训练数据的倾向。</li>
<li>这种倾向可能导致版权问题和其他相关问题。</li>
<li>现有方法在检测和处理局部记忆问题方面表现不佳，无法定位局部记忆区域。</li>
<li>提出了一种名为“明亮结束”（BE）的新现象，表现为一种独特的交叉注意力模式，有助于识别记忆的图像区域。</li>
<li>通过将BE集成到现有框架中，可以有效定位和缓解局部记忆问题，提高模型性能。</li>
<li>新方法的性能在多个任务上达到了新的最先进的水平，验证了其有效性和实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2a4edb13fdd90b3c79b08e52226a7a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d59cc7c3c51556fccefb5890ac0b2c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc5a29051210962c3498696fd9f87627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33d44aae5db2b60b6c97e6709deb000a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51e8b80ce082ced56e8e375608d1bc9f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Improving-Consistency-in-Diffusion-Models-for-Image-Super-Resolution"><a href="#Improving-Consistency-in-Diffusion-Models-for-Image-Super-Resolution" class="headerlink" title="Improving Consistency in Diffusion Models for Image Super-Resolution"></a>Improving Consistency in Diffusion Models for Image Super-Resolution</h2><p><strong>Authors:Junhao Gu, Peng-Tao Jiang, Hao Zhang, Mi Zhou, Jinwei Chen, Wenming Yang, Bo Li</strong></p>
<p>Recent methods exploit the powerful text-to-image (T2I) diffusion models for real-world image super-resolution (Real-ISR) and achieve impressive results compared to previous models. However, we observe two kinds of inconsistencies in diffusion-based methods which hinder existing models from fully exploiting diffusion priors. The first is the semantic inconsistency arising from diffusion guidance. T2I generation focuses on semantic-level consistency with text prompts, while Real-ISR emphasizes pixel-level reconstruction from low-quality (LQ) images, necessitating more detailed semantic guidance from LQ inputs. The second is the training-inference inconsistency stemming from the DDPM, which improperly assumes high-quality (HQ) latent corrupted by Gaussian noise as denoising inputs for each timestep. To address these issues, we introduce ConsisSR to handle both semantic and training-inference consistencies. On the one hand, to address the semantic inconsistency, we proposed a Hybrid Prompt Adapter (HPA). Instead of text prompts with coarse-grained classification information, we leverage the more powerful CLIP image embeddings to explore additional color and texture guidance. On the other hand, we introduce Time-Aware Latent Augmentation (TALA) to bridge the training-inference inconsistency. Based on the probability function p(t), we accordingly enhance the SDSR training strategy. With LQ latent with Gaussian noise as inputs, our TALA not only focuses on diffusion noise but also refine the LQ latent towards the HQ counterpart. Our method demonstrates state-of-the-art performance among existing diffusion models. The code will be made publicly available. </p>
<blockquote>
<p>最近的方法利用强大的文本到图像（T2I）扩散模型进行现实世界图像超分辨率（Real-ISR）处理，与之前的模型相比取得了令人印象深刻的结果。然而，我们观察到扩散模型中存在两种不一致性，阻碍了现有模型充分利用扩散先验知识。第一种是扩散引导产生的语义不一致性。T2I生成侧重于与文本提示的语义级别一致性，而Real-ISR强调从低质量（LQ）图像进行像素级重建，需要LQ输入提供更详细的语义引导。第二种是DDPM产生的训练推理不一致性，它错误地假设高质量（HQ）潜在变量受到高斯噪声的破坏，作为每个时间步长的去噪输入。为了解决这些问题，我们引入了ConsisSR来处理语义和训练推理一致性。一方面，为了解决语义不一致性，我们提出了混合提示适配器（HPA）。我们不再使用带有粗粒度分类信息的文本提示，而是利用更强大的CLIP图像嵌入来探索额外的颜色和纹理指导。另一方面，我们引入时间感知潜在增强（TALA）来弥合训练推理不一致性的鸿沟。基于概率函数p(t)，我们相应地增强了SDSR训练策略。使用LQ潜在带有高斯噪声作为输入，我们的TALA不仅关注扩散噪声，而且还使LQ潜在向HQ对应物进行精炼。我们的方法在现有的扩散模型中达到了最先进的性能。代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13807v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>最新方法利用强大的文本到图像（T2I）扩散模型进行现实世界图像超分辨率（Real-ISR），与之前的模型相比取得了令人印象深刻的结果。然而，我们观察到扩散模型中的两种不一致性，阻碍了现有模型充分利用扩散先验。首先是语义不一致性，源于扩散指导。T2I生成侧重于与文本提示的语义级一致性，而Real-ISR强调从低质量（LQ）图像进行像素级重建，需要更多详细的语义指导来自LQ输入。其次是DDPM产生的训练推理不一致性，它错误地假设高质量（HQ）潜在变量被高斯噪声腐蚀作为每个时间步的降噪输入。为了解决这些问题，我们引入了ConsisSR来处理语义和训练推理的一致性。一方面，为了解决语义不一致性，我们提出了混合提示适配器（HPA）。我们利用更强大的CLIP图像嵌入来探索额外的颜色和纹理指导，而不是带有粗略分类信息的文本提示。另一方面，我们引入时间感知潜在增强（TALA）来弥合训练推理的不一致性。基于概率函数p(t)，我们相应地增强了SDSR训练策略。使用LQ潜在带有高斯噪声作为输入，我们的TALA不仅关注扩散噪声，还优化LQ潜在变量向HQ对应物靠近。我们的方法在现有扩散模型中达到了领先水平，相关代码将公开。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>文本到图像（T2I）扩散模型在现实世界图像超分辨率（Real-ISR）任务中取得显著成果。</li>
<li>现有扩散模型存在语义不一致和训练推理不一致的问题。</li>
<li>ConsisSR被提出来处理这两种不一致性，包括混合提示适配器（HPA）和时间感知潜在增强（TALA）。</li>
<li>HPA利用CLIP图像嵌入来提供额外的颜色和纹理指导，解决语义不一致问题。</li>
<li>TALA基于概率函数增强SDSR训练策略，以缩小训练与推理之间的差距。</li>
<li>该方法不仅在像素级重建方面表现出色，而且在处理低质量图像时提供了强大的语义指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13807">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e346da3e4319c31fc71877bc65d72d8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd03653aa4185212c3b445c5dba58733.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f48d10b9c78f5008be9a10ecb91567d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1e60b4a23846bdc9ef2a2bb4d396f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5b8b0030aa0d3ff2484f1300b5ecd16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6f7bca529fbe5dcbdf7d2c96c35fb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1653cda80b8d0781cf2a9a641f80c613.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60b30a9f2144f294f13c1024cd533337.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-0c7c2a2d7f366b6cf266a0da24959227.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-29  RSFR A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor   Cardiac MRI with Semantic-Aware Refinement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-636de62659fd962c181f7bd36cdeb9d9.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-29  RGS-DR Reflective Gaussian Surfels with Deferred Rendering for Shiny   Objects
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17012.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
