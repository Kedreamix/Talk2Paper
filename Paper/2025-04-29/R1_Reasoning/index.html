<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-29  TRACE Back from the Future A Probabilistic Reasoning Approach to   Controllable Language Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-d8c7bed048e1aaa024c2d92f6d0a1179.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-29-æ›´æ–°"><a href="#2025-04-29-æ›´æ–°" class="headerlink" title="2025-04-29 æ›´æ–°"></a>2025-04-29 æ›´æ–°</h1><h2 id="TRACE-Back-from-the-Future-A-Probabilistic-Reasoning-Approach-to-Controllable-Language-Generation"><a href="#TRACE-Back-from-the-Future-A-Probabilistic-Reasoning-Approach-to-Controllable-Language-Generation" class="headerlink" title="TRACE Back from the Future: A Probabilistic Reasoning Approach to   Controllable Language Generation"></a>TRACE Back from the Future: A Probabilistic Reasoning Approach to   Controllable Language Generation</h2><p><strong>Authors:Gwen Yidou Weng, Benjie Wang, Guy Van den Broeck</strong></p>
<p>As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMMâ€™s predicted futures. This EAP is then used to reweigh the LMâ€™s next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„ä¸æ–­å‘å±•ï¼Œè¶Šæ¥è¶Šéœ€è¦æ§åˆ¶å…¶è¾“å‡ºä»¥ç¬¦åˆäººç±»ä»·å€¼è§‚ï¼ˆä¾‹å¦‚å»æ¯’ï¼‰æˆ–æ‰€éœ€å±æ€§ï¼ˆä¾‹å¦‚ä¸ªæ€§åŒ–ã€ä¸»é¢˜ï¼‰ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¨¡å‹ä¸“æ³¨äºä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„æµ‹ï¼Œå¹¶åŠªåŠ›åº”å¯¹éœ€è¦å‰ç»æ€§çš„å…¨å±€å±æ€§ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆè¦ä¹ˆé’ˆå¯¹æ¯ä¸ªæ–°å±æ€§è°ƒæ•´æˆ–è¿›è¡Œåè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹â€”â€”è¿™æ—¢æ˜‚è´µåˆç¼ºä¹çµæ´»æ€§â€”â€”è¦ä¹ˆé€šè¿‡æŠ½æ ·æˆ–è®­ç»ƒæ¥è¿‘ä¼¼æœªæ¥åºåˆ—çš„é¢„æœŸå±æ€§æ¦‚ç‡ï¼ˆEAPï¼‰ï¼Œè¿™å¯¹äºç½•è§å±æ€§è€Œè¨€æ—¢ç¼“æ…¢åˆä¸å¯é ã€‚æˆ‘ä»¬å¼•å…¥äº†TRACEï¼ˆç”¨äºå¯æ§å¯é€‚åº”ç”Ÿæˆçš„å¯é æ¦‚ç‡æ¨ç†æ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆè®¡ç®—EAPå¹¶æ ¹æ®æ–°çš„å±æ€§è¿›è¡Œé€‚åº”ï¼Œé€šè¿‡å¯é çš„æ¦‚ç‡æ¨ç†å’Œè½»é‡çº§æ§åˆ¶æ¥å®ç°ã€‚TRACEé€šè¿‡è¯­è¨€æ¨¡å‹è’¸é¦å‡ºéšè—é©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰ï¼Œå¹¶å°†å…¶ä¸å°å‹åˆ†ç±»å™¨é…å¯¹ä»¥ä¼°è®¡å±æ€§æ¦‚ç‡ï¼Œä»è€Œåœ¨HMMé¢„æµ‹çš„æœªæ¥å‘å±•ä¸Šå®ç°ç²¾ç¡®çš„EAPè®¡ç®—ã€‚ç„¶åï¼Œä½¿ç”¨æ­¤EAPé‡æ–°æƒè¡¡è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡ï¼Œä»¥ç”Ÿæˆå…¨å±€åˆè§„çš„å»¶ç»­æ–‡æœ¬ã€‚ç»éªŒè¡¨æ˜ï¼ŒTRACEåœ¨ä»…10%çš„è§£ç å¼€é”€ä¸‹å®ç°äº†æœ€å…ˆè¿›çš„å»æ¯’æ•ˆæœï¼Œå¯åœ¨å‡ ç§’å†…é€‚åº”76ä¸ªä½èµ„æºä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ— ç¼æ‰©å±•åˆ°å¤åˆå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„å‘å±•ï¼Œå¦‚ä½•æ§åˆ¶å…¶è¾“å‡ºä»¥ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›å±æ€§ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦å‰ç»çš„å…¨å±€å±æ€§æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸”è°ƒæ•´æˆ–åè®­ç»ƒæ¯ä¸ªæ–°å±æ€§æ—¢æ˜‚è´µåˆä¸çµæ´»ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†TRACEæ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯è¡Œçš„æ¦‚ç‡æ¨ç†å’Œè½»é‡çº§æ§åˆ¶æ¥é€‚åº”å¯æ§ç”Ÿæˆã€‚TRACEé€šè¿‡ä»LMä¸­æç‚¼éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å¹¶é…ä»¥å°å‹åˆ†ç±»å™¨æ¥ä¼°è®¡å±æ€§æ¦‚ç‡ï¼Œèƒ½å¤Ÿç²¾ç¡®è®¡ç®—HMMé¢„æµ‹æœªæ¥çš„æœŸæœ›å±æ€§æ¦‚ç‡ï¼ˆEAPï¼‰ã€‚éšåï¼Œè¯¥EAPè¢«ç”¨äºé‡æ–°è°ƒæ•´LMçš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡ï¼Œä»¥ç”Ÿæˆå…¨å±€åˆè§„çš„å»¶ç»­æ–‡æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒTRACEåœ¨è§£æ¯’ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ•ˆæœï¼Œä»…å¢åŠ äº†10%çš„è§£ç å¼€é”€ï¼Œå¹¶èƒ½å¿«é€Ÿé€‚åº”ä½èµ„æºä¸ªæ€§åŒ–LLMï¼Œä¸”èƒ½æ— ç¼æ‰©å±•åˆ°å¤åˆå±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„è¾“å‡ºéœ€è¦æ§åˆ¶ï¼Œä»¥ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›å±æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†éœ€è¦å‰ç»çš„å…¨å±€å±æ€§æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸”è°ƒæ•´æˆ–åè®­ç»ƒæ¯ä¸ªæ–°å±æ€§æˆæœ¬é«˜æ˜‚ä¸”ä¸çµæ´»ã€‚</li>
<li>TRACEæ¡†æ¶é€šè¿‡éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å’Œåˆ†ç±»å™¨æ¥ä¼°è®¡å±æ€§æ¦‚ç‡ï¼Œç²¾ç¡®è®¡ç®—æœŸæœ›å±æ€§æ¦‚ç‡ï¼ˆEAPï¼‰ã€‚</li>
<li>TRACEå®ç°äº†å¯¹å…¨å±€åˆè§„å»¶ç»­æ–‡æœ¬çš„æ§åˆ¶ï¼Œä½¿ç”¨EAPé‡æ–°è°ƒæ•´LMçš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡ã€‚</li>
<li>TRACEåœ¨è§£æ¯’ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œä¸”ä»…æœ‰10%çš„è§£ç å¼€é”€ã€‚</li>
<li>TRACEèƒ½å¿«é€Ÿé€‚åº”ä½èµ„æºä¸ªæ€§åŒ–LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e00ec60b4484319ca8bd6df7c5ea3812.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d59b580197ea6c88a18534adefdc458.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fast-Slow-Thinking-for-Large-Vision-Language-Model-Reasoning"><a href="#Fast-Slow-Thinking-for-Large-Vision-Language-Model-Reasoning" class="headerlink" title="Fast-Slow Thinking for Large Vision-Language Model Reasoning"></a>Fast-Slow Thinking for Large Vision-Language Model Reasoning</h2><p><strong>Authors:Wenyi Xiao, Leilei Gan, Weilong Dai, Wanggui He, Ziwei Huang, Haoyuan Li, Fangxun Shu, Zhelun Yu, Peng Zhang, Hao Jiang, Fei Wu</strong></p>
<p>Recent advances in large vision-language models (LVLMs) have revealed an \textit{overthinking} phenomenon, where models generate verbose reasoning across all tasks regardless of questions. To address this issue, we present \textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. We develop FAST-GRPO with three components: model-based metrics for question characterization, an adaptive thinking reward mechanism, and difficulty-aware KL regularization. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10% relative improvement compared to the base model, while reducing token usage by 32.7-67.3% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„è¿›æ­¥æ­ç¤ºäº†ä¸€ç§â€œè¿‡åº¦æ€è€ƒâ€ç°è±¡ï¼Œå³æ¨¡å‹ä¼šåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ç”Ÿæˆå†—é•¿çš„æ¨ç†ï¼Œè€Œæ— è®ºé—®é¢˜å¦‚ä½•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FASTï¼Œä¸€ç§æ–°å‹çš„Fast-Slow Thinkingæ¡†æ¶ï¼Œå®ƒæ ¹æ®é—®é¢˜çš„ç‰¹æ€§åŠ¨æ€é€‚åº”æ¨ç†æ·±åº¦ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ç ”ç©¶å“åº”é•¿åº¦å’Œæ•°æ®åˆ†å¸ƒå¦‚ä½•å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œè¯å®äº†å¿«é€Ÿ-æ…¢é€Ÿæ€è€ƒåœ¨LVLMsä¸­çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬å¼€å‘äº†FAST-GRPOï¼ŒåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šåŸºäºæ¨¡å‹çš„åº¦é‡è¿›è¡Œé—®é¢˜ç‰¹å¾åŒ–ã€è‡ªé€‚åº”æ€è€ƒå¥–åŠ±æœºåˆ¶å’Œéš¾åº¦æ„ŸçŸ¥KLæ­£åˆ™åŒ–ã€‚åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFASTå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”æé«˜äº†è¶…è¿‡10%çš„ç›¸å¯¹æ”¹å–„ç‡ã€‚ä¸ä¹‹å‰éœ€è¦ç¼“æ…¢æ€è€ƒçš„æ¨ç†æ–¹æ³•ç›¸æ¯”ï¼ŒFASTå‡å°‘äº†32.7-67.3%çš„ä»¤ç‰Œä½¿ç”¨ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ¨ç†é•¿åº¦å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18458v1">PDF</a> 16 pages, 5 figures, and 12 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰ä¸­çš„â€œè¿‡åº¦æ€è€ƒâ€ç°è±¡å¼•å‘äº†å¹¿æ³›å…³æ³¨ï¼Œè¯¥é—®é¢˜å¯¼è‡´æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ç”Ÿæˆå†—é•¿çš„æ¨ç†ï¼Œæ— è§†é—®é¢˜çš„å®é™…éœ€æ±‚ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºâ€œFASTâ€çš„æ–°å‹å¿«æ…¢æ€è€ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®é—®é¢˜çš„ç‰¹æ€§åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚é€šè¿‡å®è¯ç ”ç©¶ï¼Œæœ¬æ–‡éªŒè¯äº†å¿«æ…¢æ€è€ƒåœ¨LVLMsä¸­çš„å¯è¡Œæ€§ï¼Œå¹¶æ¢è®¨äº†å“åº”é•¿åº¦å’Œæ•°æ®åˆ†å¸ƒå¯¹æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼€å‘äº†åŒ…å«ä¸‰ä¸ªç»„ä»¶çš„FAST-GRPOï¼šåŸºäºæ¨¡å‹çš„æŒ‡æ ‡è¿›è¡Œé—®é¢˜ç‰¹å¾åŒ–ã€è‡ªé€‚åº”æ€è€ƒå¥–åŠ±æœºåˆ¶å’Œéš¾åº¦æ„ŸçŸ¥KLæ­£åˆ™åŒ–ã€‚åœ¨ä¸ƒä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFASTå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰è¶…è¿‡10%çš„ç›¸å¯¹æ”¹è¿›ï¼ŒåŒæ—¶ç›¸è¾ƒäºä¹‹å‰çš„æ…¢æ€è€ƒæ–¹æ³•å‡å°‘äº†32.7-67.3%çš„ä»¤ç‰Œä½¿ç”¨ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ¨ç†é•¿åº¦å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å­˜åœ¨â€œè¿‡åº¦æ€è€ƒâ€ç°è±¡ï¼Œå³ç”Ÿæˆå†—é•¿æ¨ç†çš„é—®é¢˜ã€‚</li>
<li>FASTæ¡†æ¶æ—¨åœ¨æ ¹æ®é—®é¢˜çš„ç‰¹æ€§åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚</li>
<li>å®è¯ç ”ç©¶éªŒè¯äº†å¿«æ…¢æ€è€ƒåœ¨LVLMsä¸­çš„å¯è¡Œæ€§ã€‚</li>
<li>FAST-GRPOåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šé—®é¢˜ç‰¹å¾åŒ–ã€è‡ªé€‚åº”æ€è€ƒå¥–åŠ±æœºåˆ¶å’Œéš¾åº¦æ„ŸçŸ¥KLæ­£åˆ™åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒFASTåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å…ˆè¿›å‡†ç¡®æ€§ã€‚</li>
<li>FASTç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æœ‰æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œç›¸å¯¹æ”¹è¿›è¶…è¿‡10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-735d1fdb28360cdc8c39a32feb79ba6d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8c7bed048e1aaa024c2d92f6d0a1179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e200c94edd4fe6ad9fdc8adf016d4147.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reason-Like-a-Radiologist-Chain-of-Thought-and-Reinforcement-Learning-for-Verifiable-Report-Generation"><a href="#Reason-Like-a-Radiologist-Chain-of-Thought-and-Reinforcement-Learning-for-Verifiable-Report-Generation" class="headerlink" title="Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning   for Verifiable Report Generation"></a>Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning   for Verifiable Report Generation</h2><p><strong>Authors:Peiyuan Jing, Kinhei Lee, Zhenxuan Zhang, Huichi Zhou, Zhengqing Yuan, Zhifan Gao, Lei Zhu, Giorgos Papanastasiou, Yingying Fang, Guang Yang</strong></p>
<p>Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologistsâ€™ workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RLâ€™s robustness in generating high-quality radiology reports. </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¯¹äºæ•ˆç‡è‡³å…³é‡è¦ï¼Œä½†å½“å‰æ¨¡å‹ç¼ºä¹ä¸“å®¶çš„ç»“æ„åŒ–æ¨ç†ï¼Œæ— æ³•é€šè¿‡è”ç³»è§†è§‰å‘ç°ä¸ç²¾ç¡®è§£å‰–ä½ç½®æ¥å»ºç«‹ä¸´åºŠä¿¡ä»»å’Œè§£é‡Šæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†BoxMed-RLï¼Œä¸€ä¸ªç”¨äºç”Ÿæˆç©ºé—´å¯éªŒè¯å’Œå¯è§£é‡Šæ”¾å°„å­¦æŠ¥å‘Šçš„åˆ›æ–°æ€§ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ã€‚BoxMed-RLå»ºç«‹åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¹‹ä¸Šï¼Œé€šè¿‡ä¸¤ä¸ªé›†æˆé˜¶æ®µå®ç°æŠ¥å‘Šçš„ç”Ÿæˆé©æ–°ï¼šï¼ˆ1ï¼‰åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡åŒ»ç–—æ¦‚å¿µå­¦ä¹ ç²¾ç‚¼æ¨¡å‹ï¼Œä½¿ç”¨â€œæ€ç»´é“¾â€ç›‘ç£æ¥å†…åŒ–ç±»ä¼¼æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œæµç¨‹ï¼Œç„¶åæ˜¯ç©ºé—´å¯éªŒè¯å¼ºåŒ–ï¼Œå°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºå°†åŒ»ç–—å‘ç°ä¸è¾¹ç•Œæ¡†å¯¹é½ã€‚ï¼ˆ2ï¼‰åœ¨ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µï¼Œæˆ‘ä»¬å†»ç»“é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªä¸‹æ¸¸é€‚é…å™¨ï¼Œä»¥ç¡®ä¿æŠ¥å‘Šæµç•…ä¸”ä¸´åºŠå¯ä¿¡ã€‚è¯¥æ¡†æ¶ç²¾ç¡®åœ°æ¨¡ä»¿äº†æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œæµç¨‹ï¼Œä¿ƒä½¿æ¨¡å‹å°†é«˜çº§åŒ»ç–—æ¦‚å¿µä¸æ˜ç¡®çš„è§£å‰–è¯æ®è”ç³»èµ·æ¥ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBoxMed-RLåœ¨METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†7%ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡å¹³å‡æé«˜äº†5%ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†BoxMed-RLåœ¨ç”Ÿæˆé«˜è´¨é‡æ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18453v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåˆ›æ–°çš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶BoxMed-RLï¼Œç”¨äºç”Ÿæˆå¯ç©ºé—´éªŒè¯å’Œå¯è§£é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µå®ç°ï¼šé¢„è®­ç»ƒé˜¶æ®µé€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ å’Œç©ºé—´å¯éªŒè¯å¼ºåŒ–æ¥æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹ï¼›ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µåˆ™ç¡®ä¿æŠ¥å‘Šçš„æµç•…æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBoxMed-RLç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œåœ¨METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†7%ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶åœ¨ç”Ÿæˆé«˜è´¨é‡æ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>BoxMed-RLæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æŠ¥å‘Šçš„æ•ˆç‡å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’Œä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µé€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ å’Œç©ºé—´å¯éªŒè¯å¼ºåŒ–æ¥æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹ã€‚</li>
<li>ç©ºé—´å¯éªŒè¯å¼ºåŒ–åº”ç”¨å¼ºåŒ–å­¦ä¹ å°†åŒ»ç–—å‘ç°ä¸è¾¹ç•Œæ¡†å¯¹é½ã€‚</li>
<li>ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µç¡®ä¿æŠ¥å‘Šçš„æµç•…æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
<li>BoxMed-RLåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶åœ¨METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ‰€æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-819f69991ca333c6b6bbb5c552c5ae09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aa1fb71b7b66167a479d3efff4c711b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a6eb952a6bf902ea6f1f4383f4b9c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52e6437e91f50b9bba7a9a58ca3bfbbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b210393d3fcb02840b6f680907c01a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84d271d1ca1ca74ffd3901c7fb16abbc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PolyMath-Evaluating-Mathematical-Reasoning-in-Multilingual-Contexts"><a href="#PolyMath-Evaluating-Mathematical-Reasoning-in-Multilingual-Contexts" class="headerlink" title="PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"></a>PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts</h2><p><strong>Authors:Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, Jingren Zhou</strong></p>
<p>In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†PolyMathï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–18ç§è¯­è¨€å’Œ4ä¸ªéš¾åº¦ç­‰çº§ï¼ˆç”±æ˜“åˆ°éš¾ï¼‰çš„å¤šè¯­è¨€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç¡®ä¿äº†éš¾åº¦å…¨é¢æ€§ã€è¯­è¨€å¤šæ ·æ€§å’Œé«˜è´¨é‡ç¿»è¯‘ï¼Œä½¿å…¶æˆä¸ºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ä»£çš„é«˜åº¦åŒºåˆ†æ€§çš„å¤šè¯­è¨€æ•°å­¦åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬å¯¹å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå‘ç°å³ä½¿æ˜¯Deepseek-R1-671Bå’ŒQw-QwQ-Qen 32Bæ¨¡å‹ï¼Œåœ¨æœ€é«˜éš¾åº¦çº§åˆ«ä¸‹çš„å‡†ç¡®ç‡ä¹Ÿä½äº30%ï¼Œåˆ†åˆ«è·å¾—43.4å’Œ41.8çš„åŸºå‡†æµ‹è¯•æˆç»©ã€‚ä»è¯­è¨€è§’åº¦çœ‹ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ­ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€æ¨ç†æ–¹é¢çš„å‡ ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½åœ¨ä¸åŒè¯­è¨€ä¸­å·®å¼‚å¾ˆå¤§ï¼›ï¼ˆ2ï¼‰æ¨ç†å‹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¾“å…¥è¾“å‡ºè¯­è¨€ä¸€è‡´æ€§è¾ƒä½ï¼Œå¯èƒ½ä¸æ€§èƒ½æœ‰å…³ï¼›ï¼ˆ3ï¼‰å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€è€ƒé•¿åº¦åœ¨ä¸åŒè¯­è¨€ä¸­å·®å¼‚æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜ï¼ŒæŒ‡ä»¤ä¸­çš„è¾“å‡ºè¯­è¨€æ§åˆ¶å¯¹æ¨ç†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯å¯¹æŸäº›ä½èµ„æºè¯­è¨€ï¼Œå…·æœ‰æ½œåœ¨å½±å“ï¼Œè¿™ä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›æä¾›äº†æœ‰å¸Œæœ›çš„æ”¹è¿›æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18428v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PolyMathå¤šè¯­è¨€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–18ç§è¯­è¨€å’Œ4ä¸ªéš¾åº¦çº§åˆ«ã€‚è¯¥åŸºå‡†æµ‹è¯•ç¡®ä¿äº†éš¾åº¦å…¨é¢æ€§ã€è¯­è¨€å¤šæ ·æ€§å’Œé«˜è´¨é‡ç¿»è¯‘ï¼Œæ˜¯å½“ä¸‹æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„é«˜åº¦åŒºåˆ†æ€§å¤šè¯­è¨€æ•°å­¦åŸºå‡†æµ‹è¯•ã€‚å¯¹å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°åå‘ç°ï¼Œå³ä½¿åœ¨æœ€é«˜éš¾åº¦ä¸‹ï¼ŒDeepseek-R1-671Bå’ŒQwen-QwQ-32Bç­‰æ¨¡å‹ä¹Ÿåªè¾¾åˆ°43.4å’Œ41.8çš„åŸºå‡†æµ‹è¯•åˆ†æ•°ï¼Œå‡†ç¡®ç‡ä½äº30%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PolyMathæ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œè¦†ç›–18ç§è¯­è¨€å’Œ4ä¸ªéš¾åº¦çº§åˆ«ï¼Œç¡®ä¿éš¾åº¦å…¨é¢æ€§ã€è¯­è¨€å¤šæ ·æ€§å’Œé«˜è´¨é‡ç¿»è¯‘ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨PolyMathåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå³ä½¿åœ¨ç›¸å¯¹ç®€å•çš„ä»»åŠ¡ä¸­ä¹Ÿå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®å¼‚ã€‚</li>
<li>ä¸åŒè¯­è¨€ä¹‹é—´çš„æ¨ç†æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œè¡¨æ˜å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€æ¨ç†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>è¾“å…¥è¾“å‡ºè¯­è¨€ä¸€è‡´æ€§åœ¨æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¾ƒä½ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ§åˆ¶æŒ‡ä»¤çš„è¾“å‡ºè¯­è¨€æœ‰å¯èƒ½å½±å“æ¨ç†æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä½èµ„æºè¯­è¨€æ–¹é¢ï¼Œè¿™ä¸ºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›æä¾›äº†æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€è€ƒé•¿åº¦åœ¨ä¸åŒè¯­è¨€ä¸­å·®å¼‚æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18428">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b78f1539576c0923f21ecb3bbe33d53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42054d723cdd7946b52032c6cbe48b99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57b272e2c62248ec6d14e19aed222f44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1073b7a28568edfaa8c772e7eec09fae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8ec32a75c93dc8e3aced2b9da137ead.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7843ab0c760e41c1b8ee0a389f22667c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Visual-Chain-of-Thought-Reasoning-via-Preference-Optimization"><a href="#Unsupervised-Visual-Chain-of-Thought-Reasoning-via-Preference-Optimization" class="headerlink" title="Unsupervised Visual Chain-of-Thought Reasoning via Preference   Optimization"></a>Unsupervised Visual Chain-of-Thought Reasoning via Preference   Optimization</h2><p><strong>Authors:Kesen Zhao, Beier Zhu, Qianru Sun, Hanwang Zhang</strong></p>
<p>Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perceptionâ€“identifying key regions and reasoning based on themâ€“UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in <a target="_blank" rel="noopener" href="https://github.com/kesenzhao/UV-CoT">https://github.com/kesenzhao/UV-CoT</a>. </p>
<blockquote>
<p>è®¤çŸ¥é“¾ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†æå¤§åœ°æé«˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬CoTï¼Œé™åˆ¶äº†å®ƒä»¬åˆ©ç”¨è§†è§‰çº¿ç´¢çš„èƒ½åŠ›ã€‚è§†è§‰CoTä»ç„¶æœªè¢«å……åˆ†ç ”ç©¶ï¼Œç°æœ‰çš„å·¥ä½œéƒ½æ˜¯åŸºäºæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™ä¾èµ–äºå¤§é‡çš„æ ‡è®°è¾¹ç•Œæ¡†æ•°æ®ï¼Œå¹¶ä¸”å¾ˆéš¾æ³›åŒ–åˆ°æœªè§è¿‡çš„æ¡ˆä¾‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ— ç›‘ç£è§†è§‰CoTï¼ˆUV-CoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åå¥½ä¼˜åŒ–è¿›è¡Œå›¾åƒçº§CoTæ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚UV-CoTé€šè¿‡å¯¹æ¨¡å‹ç”Ÿæˆçš„è¾¹ç•Œæ¡†è¿›è¡Œåå¥½æ¯”è¾ƒï¼ˆä¸€ä¸ªè¢«ä¼˜å…ˆé€‰ä¸­ï¼Œå¦ä¸€ä¸ªåˆ™è¢«æ’é™¤ï¼‰ï¼Œä»è€Œæ— éœ€è¾¹ç•Œæ¡†æ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥è‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡é“æ¥è·å¾—è¿™ç§åå¥½æ•°æ®ã€‚ç»™å®šä¸€å¼ å›¾åƒï¼Œæˆ‘ä»¬çš„ç›®æ ‡MLLMï¼ˆä¾‹å¦‚LLaVA-1.5-7Bï¼‰ä½¿ç”¨æ¨¡æ¿æç¤ºç”Ÿæˆç§å­è¾¹ç•Œæ¡†ï¼Œç„¶åä½¿ç”¨æ¯ä¸ªè¾¹ç•ŒåŒºåŸŸä½œä¸ºè¾“å…¥å›ç­”é—®é¢˜ã€‚è¯„ä¼°å™¨MLLMï¼ˆä¾‹å¦‚OmniLLM-12Bï¼‰å¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼Œè¿™äº›æ’åä½œä¸ºç›‘ç£ä¿¡æ¯æ¥è®­ç»ƒç›®æ ‡MLLMçš„UV-CoTï¼Œé€šè¿‡æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥â€”â€”è¯†åˆ«å…³é”®åŒºåŸŸå¹¶åŸºäºå®ƒä»¬è¿›è¡Œæ¨ç†â€”â€”UV-CoTå¯ä»¥æ”¹å–„è§†è§‰ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ï¼Œä»…å‡­æ–‡æœ¬æè¿°æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–‡æœ¬å’Œè§†è§‰CoTæ–¹æ³•ç›¸æ¯”ï¼ŒUV-CoTå…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæœªè§è¿‡çš„æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æµ‹è¯•æ˜¾ç¤ºäº†UV-CoTçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kesenzhao/UV-CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kesenzhao/UV-CoTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18397v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºUV-CoTçš„æ— ç›‘ç£è§†è§‰é“¾å¼æ€ç»´ï¼ˆVisual Chain-of-Thought, CoTï¼‰æ¡†æ¶ï¼Œç”¨äºå›¾åƒçº§åˆ«çš„CoTæ¨ç†ã€‚é€šè¿‡åå¥½ä¼˜åŒ–ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— éœ€è¾¹ç•Œæ¡†æ ‡æ³¨çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹ç”Ÿæˆçš„è¾¹ç•Œæ¡†ä¹‹é—´çš„åå¥½æ¯”è¾ƒã€‚å¼•å…¥è‡ªåŠ¨æ•°æ®ç”Ÿæˆæµç¨‹æ¥è·å–åå¥½æ•°æ®ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–è´Ÿå¯¹æ•°æŸå¤±æ¥è®­ç»ƒç›®æ ‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚UV-CoTé€šè¿‡æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥â€”â€”è¯†åˆ«å…³é”®åŒºåŸŸå¹¶åŸºäºè¿™äº›åŒºåŸŸè¿›è¡Œæ¨ç†ï¼Œæé«˜äº†è§†è§‰ç†è§£èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUV-CoTåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–‡æœ¬å’Œè§†è§‰CoTæ–¹æ³•ï¼Œä¸”åœ¨å››ä¸ªæœªè§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æµ‹è¯•è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UV-CoTæ˜¯ä¸€ä¸ªæ— ç›‘ç£çš„è§†è§‰é“¾å¼æ€ç»´ï¼ˆVisual CoTï¼‰æ¡†æ¶ï¼Œç”¨äºå›¾åƒçº§åˆ«çš„æ¨ç†ã€‚</li>
<li>UV-CoTé€šè¿‡åå¥½ä¼˜åŒ–æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„è¾¹ç•Œæ¡†ï¼Œæ— éœ€è¾¹ç•Œæ¡†æ ‡æ³¨ã€‚</li>
<li>è‡ªåŠ¨æ•°æ®ç”Ÿæˆæµç¨‹ç”¨äºè·å–åå¥½æ•°æ®ã€‚</li>
<li>UV-CoTé€šè¿‡æœ€å°åŒ–è´Ÿå¯¹æ•°æŸå¤±æ¥è®­ç»ƒç›®æ ‡MLLMã€‚</li>
<li>UV-CoTæ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥ï¼Œæé«˜è§†è§‰ç†è§£èƒ½åŠ›ï¼Œå°¤å…¶åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å®éªŒè¯æ˜UV-CoTåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>UV-CoTå…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨æœªè§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æµ‹è¯•è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac36512d8491865f5c62c835c4758710.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-204ac322ec9aafa810cb593bdc201494.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9bd7e5dbdf961f0239cf0df2caa7711a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c54236e4bb21286c248f2614811dcf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f50a4e4870a9008a6e0ebff4aebdc3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Spatial-Reasoner-A-3D-Inference-Pipeline-for-XR-Applications"><a href="#Spatial-Reasoner-A-3D-Inference-Pipeline-for-XR-Applications" class="headerlink" title="Spatial Reasoner: A 3D Inference Pipeline for XR Applications"></a>Spatial Reasoner: A 3D Inference Pipeline for XR Applications</h2><p><strong>Authors:Steven HÃ¤sler, Philipp Ackermann</strong></p>
<p>Modern extended reality XR systems provide rich analysis of image data and fusion of sensor input and demand AR&#x2F;VR applications that can reason about 3D scenes in a semantic manner. We present a spatial reasoning framework that bridges geometric facts with symbolic predicates and relations to handle key tasks such as determining how 3D objects are arranged among each other (â€˜onâ€™, â€˜behindâ€™, â€˜nearâ€™, etc.). Its foundation relies on oriented 3D bounding box representations, enhanced by a comprehensive set of spatial predicates, ranging from topology and connectivity to directionality and orientation, expressed in a formalism related to natural language. The derived predicates form a spatial knowledge graph and, in combination with a pipeline-based inference model, enable spatial queries and dynamic rule evaluation. Implementations for client- and server-side processing demonstrate the frameworkâ€™s capability to efficiently translate geometric data into actionable knowledge, ensuring scalable and technology-independent spatial reasoning in complex 3D environments. The Spatial Reasoner framework is fostering the creation of spatial ontologies, and seamlessly integrates with and therefore enriches machine learning, natural language processing, and rule systems in XR applications. </p>
<blockquote>
<p>ç°ä»£æ‰©å±•ç°å®ï¼ˆXRï¼‰ç³»ç»Ÿæä¾›äº†å›¾åƒæ•°æ®çš„ä¸°å¯Œåˆ†æå’Œä¼ æ„Ÿå™¨è¾“å…¥çš„èåˆï¼Œå¹¶éœ€è¦AR&#x2F;VRåº”ç”¨ç¨‹åºèƒ½å¤Ÿä»¥è¯­ä¹‰æ–¹å¼æ¨ç†3Dåœºæ™¯ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªç©ºé—´æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å‡ ä½•äº‹å®ä¸ç¬¦å·è°“è¯å’Œå…³ç³»è”ç³»èµ·æ¥ï¼Œä»¥å¤„ç†å…³é”®ä»»åŠ¡ï¼Œä¾‹å¦‚ç¡®å®š3Då¯¹è±¡ä¹‹é—´æ˜¯å¦‚ä½•ç›¸äº’æ’åˆ—çš„ï¼ˆâ€œåœ¨â€¦â€¦ä¸Šâ€ï¼Œâ€œåœ¨â€¦â€¦åé¢â€ï¼Œâ€œé è¿‘â€ç­‰ï¼‰ã€‚å®ƒçš„åŸºç¡€ä¾èµ–äºå¸¦æœ‰æ–¹å‘æ€§çš„3Dè¾¹ç•Œæ¡†è¡¨ç¤ºï¼Œé€šè¿‡ä¸€å¥—å…¨é¢çš„ç©ºé—´è°“è¯å¢å¼ºï¼Œè¿™äº›è°“è¯ä»æ‹“æ‰‘å’Œè¿é€šæ€§åˆ°æ–¹å‘æ€§å’Œæ–¹ä½ï¼Œä»¥ä¸è‡ªç„¶è¯­è¨€ç›¸å…³çš„å½¢å¼åŒ–è¡¨è¾¾ã€‚æ‰€å¾—åˆ°çš„è°“è¯å½¢æˆç©ºé—´çŸ¥è¯†å›¾ï¼Œç»“åˆåŸºäºç®¡é“æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°ç©ºé—´æŸ¥è¯¢å’ŒåŠ¨æ€è§„åˆ™è¯„ä¼°ã€‚å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯å¤„ç†çš„å®ç°è¯æ˜äº†è¯¥æ¡†æ¶èƒ½å¤Ÿé«˜æ•ˆåœ°å°†å‡ ä½•æ•°æ®è½¬åŒ–ä¸ºå¯æ“ä½œçŸ¥è¯†çš„èƒ½åŠ›ï¼Œç¡®ä¿åœ¨å¤æ‚çš„3Dç¯å¢ƒä¸­è¿›è¡Œå¯ä¼¸ç¼©å’ŒæŠ€æœ¯ç‹¬ç«‹çš„ç©ºé—´æ¨ç†ã€‚ç©ºé—´æ¨ç†å™¨æ¡†æ¶æ­£ä¿ƒè¿›ç©ºé—´æœ¬ä½“è®ºçš„äº§ç”Ÿï¼Œå¹¶æ— ç¼é›†æˆå’Œä¸°å¯Œäº†æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè§„åˆ™ç³»ç»Ÿï¼Œåœ¨XRåº”ç”¨ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18380v1">PDF</a> 11 pages, preprint of ICVARS 2025 paper</p>
<p><strong>Summary</strong></p>
<p>ç°ä»£æ‰©å±•ç°å®XRç³»ç»Ÿéœ€è¦èƒ½å¤Ÿç†è§£å’Œå¤„ç†è¯­ä¹‰çš„AR&#x2F;VRåº”ç”¨ç¨‹åºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç©ºé—´æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å‡ ä½•äº‹å®ã€ç¬¦å·è°“è¯å’Œå…³ç³»ï¼Œèƒ½å¤Ÿå¤„ç†è¯¸å¦‚ç¡®å®šä¸‰ç»´ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ç­‰å…³é”®ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŸºäºå®šå‘ä¸‰ç»´è¾¹ç•Œæ¡†è¡¨ç¤ºï¼Œé€šè¿‡ä¸€ç³»åˆ—ç©ºé—´è°“è¯ï¼ˆå¦‚æ‹“æ‰‘ã€è¿æ¥æ€§ã€æ–¹å‘æ€§å’Œæ–¹ä½ç­‰ï¼‰è¿›è¡Œå¢å¼ºï¼Œå¹¶ä»¥ä¸è‡ªç„¶è¯­è¨€ç›¸å…³çš„å½¢å¼åŒ–è¡¨è¾¾ã€‚æ´¾ç”Ÿå‡ºçš„è°“è¯å½¢æˆç©ºé—´çŸ¥è¯†å›¾è°±ï¼Œä¸ç®¡é“å¼æ¨ç†æ¨¡å‹ç›¸ç»“åˆï¼Œå¯å®ç°ç©ºé—´æŸ¥è¯¢å’ŒåŠ¨æ€è§„åˆ™è¯„ä¼°ã€‚å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç«¯å¤„ç†çš„å®ç°è¯æ˜äº†è¯¥æ¡†æ¶å°†å‡ ä½•æ•°æ®é«˜æ•ˆè½¬åŒ–ä¸ºå¯æ“ä½œçŸ¥è¯†çš„èƒ½åŠ›ï¼Œç¡®ä¿åœ¨å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œå¯æ‰©å±•å’ŒæŠ€æœ¯ç‹¬ç«‹çš„ç©ºé—´æ¨ç†ã€‚ç©ºé—´æ¨ç†æ¡†æ¶ä¿ƒè¿›äº†ç©ºé—´æœ¬ä½“è®ºçš„äº§ç”Ÿï¼Œå¹¶ä¸°å¯Œæœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’ŒXRåº”ç”¨è§„åˆ™ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£XRç³»ç»Ÿè¦æ±‚åº”ç”¨ç¨‹åºå…·å¤‡è¯­ä¹‰å¤„ç†åŠŸèƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªç©ºé—´æ¨ç†æ¡†æ¶ï¼Œèåˆäº†å‡ ä½•äº‹å®ã€ç¬¦å·è°“è¯å’Œå…³ç³»ã€‚</li>
<li>æ¡†æ¶åŸºäºå®šå‘ä¸‰ç»´è¾¹ç•Œæ¡†è¡¨ç¤ºï¼Œé€šè¿‡ä¸€ç³»åˆ—ç©ºé—´è°“è¯å¢å¼ºã€‚</li>
<li>æ¡†æ¶å¯ä»¥å¤„ç†ç¡®å®šä¸‰ç»´ç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ä½ç½®å…³ç³»ç­‰ä»»åŠ¡ã€‚</li>
<li>æ¡†æ¶å½¢æˆç©ºé—´çŸ¥è¯†å›¾è°±ï¼Œç»“åˆç®¡é“å¼æ¨ç†æ¨¡å‹å®ç°ç©ºé—´æŸ¥è¯¢å’ŒåŠ¨æ€è§„åˆ™è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶å°†å‡ ä½•æ•°æ®è½¬åŒ–ä¸ºå¯æ“ä½œçŸ¥è¯†ï¼Œé€‚åº”å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f8ee68330e7adf3c26c511bda5ff5b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d855049a2a2818e4b08c5b91cf1a365a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97f0f7162f9430647337681ff227a258.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a088f7128c7c4e7ecec40f28edcc9fc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e007709961e01516e0ab5cba1ddb27.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pushing-the-boundary-on-Natural-Language-Inference"><a href="#Pushing-the-boundary-on-Natural-Language-Inference" class="headerlink" title="Pushing the boundary on Natural Language Inference"></a>Pushing the boundary on Natural Language Inference</h2><p><strong>Authors:Pablo Miralles-GonzÃ¡lez, Javier Huertas-Tato, Alejandro MartÃ­n, David Camacho</strong></p>
<p>Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\unicode{x2013}$or on all of them considering our replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality. </p>
<blockquote>
<p>è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ˜¯è‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„ä¸€é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œåœ¨äº‹å®æ ¸æŸ¥ã€é—®ç­”å’Œä¿¡æ¯æ£€ç´¢ç­‰æ–¹é¢æœ‰å¹¿æ³›åº”ç”¨ã€‚å°½ç®¡å…¶é‡è¦æ€§æ˜¾è‘—ï¼Œå½“å‰çš„NLIç³»ç»Ÿä¸¥é‡ä¾èµ–äºç›‘ç£å­¦ä¹ ï¼Œè€Œæ•°æ®é›†é€šå¸¸åŒ…å«æ ‡æ³¨ä¼ªè¿¹å’Œåè§ï¼Œè¿™é™åˆ¶äº†å…¶æ³›åŒ–å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„Group Relative Policy Optimization (GRPO)æ–¹æ³•ï¼Œç”¨äºè‡ªç„¶è¯­è¨€æ¨ç†ä¸­çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰ï¼ˆCoTï¼‰å­¦ä¹ ï¼Œæ¶ˆé™¤äº†å¯¹æ ‡æ³¨ç†ç”±çš„éœ€æ±‚ï¼Œå¹¶èƒ½åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼ˆå¦‚ANLIï¼‰ä¸Šè¿›è¡Œæ­¤ç±»åŸ¹è®­ã€‚æˆ‘ä»¬ä½¿ç”¨å‚æ•°é«˜æ•ˆæŠ€æœ¯ï¼ˆLoRAå’ŒQLoRAï¼‰å¯¹7Bã€14Bå’Œ32Bè¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨æ ‡å‡†å’Œå¯¹æŠ—æ€§NLIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„32B AWQé‡åŒ–æ¨¡å‹åœ¨11ä¸ªå¯¹æŠ—é›†åˆä¸­çš„7ä¸ªä¸Šè¶…è¶Šäº†æœ€æ–°ç»“æœâ€”â€”å¦‚æœæˆ‘ä»¬è¿›è¡Œå¤åˆ¶çš„è¯ç”šè‡³å¯èƒ½è¶…è¶Šå…¨éƒ¨â€”â€”åœ¨ä»…å ç”¨çš„å†…å­˜å¤§å°ä¸ºå†…å­˜ä¸ºä¿ç•™å‹ç¼©ç‡çš„é›†åˆä¿ç•™å¼ºåº¦çš„é€»è¾‘æ¨ç†ååˆ é™¤äº†ç»“åˆè½¬å½•å±çŠ¶æ€çš„å¼ºåº¦å’Œå…ˆéªŒé‡çš„æ˜æ˜¾å˜å¼‚ç”Ÿæˆå’Œä½¿ç”¨å¹¿ä¹‰çš„æœ‰æœºè¾“å‡ºç•Œé¢è§£å±€æ¨è¿›å¯Œå…‹åŠ³è¡Œçš„å¿…è¦ä¸»ä½“åœ¨ä»…æœ‰ 22GB çš„å†…å­˜å ç”¨ä¸‹ï¼Œæ˜¾ç¤ºå‡ºå³ä½¿åœ¨æ¿€çƒˆçš„é‡åŒ–ä¸‹ä¹Ÿèƒ½ä¿æŒç¨³å¥çš„æ¨ç†èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”å®ç”¨çš„æ¡†æ¶ï¼Œç”¨äºæ„å»ºç¨³å¥çš„NLIç³»ç»Ÿï¼Œè€Œä¸ç‰ºç‰²æ¨ç†è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18376v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€æ¨æ–­ä¸­çš„åº”ç”¨å·²ç»å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚è¯¥ç ”ç©¶ä½¿ç”¨åŸºäºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰å­¦ä¹ ï¼Œå‡å°‘äº†æ ‡ç­¾ä¾èµ–ï¼Œèƒ½å¤Ÿåœ¨æ›´å¤æ‚çš„æ•°æ®é›†å¦‚ANLIä¸Šè¿›è¡Œè®­ç»ƒã€‚è¯¥ç ”ç©¶è¿˜å±•ç¤ºäº†å‚æ•°é«˜æ•ˆæŠ€æœ¯ï¼ˆLoRAå’ŒQLoRAï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚7Bã€14Bå’Œ32Bæ¨¡å‹ï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶åœ¨æ ‡å‡†å’Œå¯¹æŠ—æ€§çš„NLIåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œç»è¿‡AWQé‡åŒ–çš„32Bæ¨¡å‹åœ¨å¤šä¸ªå¯¹æŠ—æ€§æµ‹è¯•é›†ä¸Šè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨é‡åŒ–å¤„ç†ä¸‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæ„å»ºç¨³å¥çš„è‡ªç„¶è¯­è¨€æ¨æ–­ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•å’Œå®ç”¨çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨NLIä¸­çš„åº”ç”¨å¼•äººå…³æ³¨ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨GRPOå¼ºåŒ–å­¦ä¹ æ–¹æ³•è¿›è¡ŒCoTå­¦ä¹ ï¼Œå‡å°‘äº†æ ‡ç­¾ä¾èµ–ï¼Œæé«˜äº†æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>LoRAå’ŒQLoRAç­‰å‚æ•°é«˜æ•ˆæŠ€æœ¯æˆåŠŸåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶çš„é‡ç‚¹ä¹‹ä¸€æ˜¯æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ANLIä¸Šçš„åº”ç”¨ã€‚</li>
<li>ç ”ç©¶ç»“æœå±•ç¤ºå‡ºåœ¨å¤šä¸ªå¯¹æŠ—æ€§æµ‹è¯•é›†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡åŒ–å¤„ç†ä¸‹çš„å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12ce90da5adae47935ac0e795e634976.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-214c75348d0b9ecb227937c796821c10.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Stabilizing-Reasoning-in-Medical-LLMs-with-Continued-Pretraining-and-Reasoning-Preference-Optimization"><a href="#Stabilizing-Reasoning-in-Medical-LLMs-with-Continued-Pretraining-and-Reasoning-Preference-Optimization" class="headerlink" title="Stabilizing Reasoning in Medical LLMs with Continued Pretraining and   Reasoning Preference Optimization"></a>Stabilizing Reasoning in Medical LLMs with Continued Pretraining and   Reasoning Preference Optimization</h2><p><strong>Authors:Wataru Kawakami, Keita Suzuki, Junichiro Iwasawa</strong></p>
<p>Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations â€“ a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5% and 3.8% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPOâ€™s effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»å­¦é¢†åŸŸæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶åœ¨ä¸´åºŠä¸Šçš„åº”ç”¨å—åˆ°äº‹å®å‡†ç¡®æ€§ã€ç‰¹å®šè¯­è¨€å±€é™æ€§ï¼ˆä¾‹å¦‚æ—¥è¯­ï¼‰ä»¥åŠå½“éœ€è¦ç”Ÿæˆæ¨ç†è§£é‡Šæ—¶çš„å¯é æ€§ç­‰é—®é¢˜çš„é˜»ç¢â€”â€”è¿™æ˜¯å»ºç«‹ä¿¡ä»»çš„å¿…è¦æ¡ä»¶ã€‚æœ¬æ–‡ä»‹ç»äº†Preferred-MedLLM-Qwen-72Bï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ—¥æœ¬åŒ»å­¦é¢†åŸŸä¼˜åŒ–çš„72Bå‚æ•°æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜å‡†ç¡®æ€§å’Œç¨³å®šæ¨ç†ã€‚æˆ‘ä»¬å¯¹Qwen2.5-72BåŸºç¡€æ¨¡å‹é‡‡ç”¨äº†ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹ï¼šé¦–å…ˆï¼Œåœ¨å…¨é¢çš„æ—¥æœ¬åŒ»å­¦è¯­æ–™åº“ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰ï¼Œä»¥çŒè¾“æ·±å±‚é¢†åŸŸçŸ¥è¯†ã€‚å…¶æ¬¡ï¼Œé‡‡ç”¨åŸºäºåå¥½çš„æ–¹æ³•ï¼Œå³æ¨ç†åå¥½ä¼˜åŒ–ï¼ˆRPOï¼‰ï¼Œåœ¨æé«˜æ¨ç†è·¯å¾„å¯é æ€§çš„åŒæ—¶ä¿æŒé«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€‚åœ¨æ—¥æœ¬åŒ»å­¦æ‰§ç…§è€ƒè¯•åŸºå‡†ï¼ˆIgakuQAï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPreferred-MedLLM-Qwen-72Bè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆå‡†ç¡®ç‡ä¸º0.868ï¼‰ï¼Œè¶…è¶Šäº†GPT-4oç­‰å¼ºå¤§ä¸“æœ‰æ¨¡å‹ï¼ˆå‡†ç¡®ç‡ä¸º0.866ï¼‰ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä¸åŸºçº¿æˆ–ä»…ä½¿ç”¨CPTçš„æ¨¡å‹ç›¸æ¯”ï¼Œå½“æç¤ºéœ€è¦è§£é‡Šæ—¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨IgakuQAä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼ˆæœ€é«˜è¾¾11.5%å’Œ3.8%ï¼‰ï¼Œè€Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ­¤æ¡ä»¶ä¸‹ä»èƒ½ä¿æŒå…¶é«˜å‡†ç¡®ç‡ï¼ˆ0.868ï¼‰ã€‚è¿™çªå‡ºäº†RPOåœ¨ç¨³å®šæ¨ç†ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ä¼˜åŒ–å¯é è§£é‡Šä¸å‡†ç¡®æ€§åŒç­‰é‡è¦çš„è§‚ç‚¹ã€‚æˆ‘ä»¬å‘å¸ƒPreferred-MedLLM-Qwen-72Bæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›å¯¹å¯ä¿¡LLMåœ¨ä¸“é¡¹ã€é«˜é£é™©åº”ç”¨æ–¹é¢çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18080v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ—¥æœ¬åŒ»ç–—é¢†åŸŸçš„ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”Preferred-MedLLM-Qwen-72Bã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹ï¼Œåœ¨Qwen2.5-72BåŸºç¡€æ¨¡å‹ä¸Šå®ç°äº†é«˜å‡†ç¡®æ€§å’Œç¨³å®šæ¨ç†ã€‚é¦–å…ˆï¼Œåœ¨å…¨é¢çš„æ—¥æœ¬åŒ»ç–—è¯­æ–™åº“ä¸Šè¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰ï¼Œèµ‹äºˆæ¨¡å‹æ·±åšçš„é¢†åŸŸçŸ¥è¯†ã€‚ç„¶åï¼Œé‡‡ç”¨åŸºäºåå¥½æ–¹æ³•çš„æ¨ç†åå¥½ä¼˜åŒ–ï¼ˆRPOï¼‰ï¼Œæé«˜äº†å¯é æ¨ç†è·¯å¾„çš„ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†é«˜ç­”æ¡ˆå‡†ç¡®æ€§ã€‚åœ¨æ—¥æœ¬åŒ»å­¦æ‰§ç…§è€ƒè¯•åŸºå‡†æµ‹è¯•ï¼ˆIgakuQAï¼‰ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒPreferred-MedLLM-Qwen-72Bè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†GPT-4oç­‰å¼ºå¤§æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼Œå½“è¦æ±‚æä¾›è§£é‡Šæ—¶ï¼Œè¯¥æ¨¡å‹èƒ½ç»´æŒå…¶é«˜å‡†ç¡®æ€§ï¼Œçªæ˜¾äº†RPOåœ¨ç¨³å®šæ¨ç†ç”Ÿæˆæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¼˜åŒ–å¯é è§£é‡Šä¸å‡†ç¡®æ€§åŒæ ·é‡è¦ã€‚æˆ‘ä»¬å‘å¸ƒPreferred-MedLLM-Qwen-72Bæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›å¯¹å¯é å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶ï¼Œä¸“é—¨ç”¨äºé«˜é£é™©çš„ç‰¹æ®Šåº”ç”¨é¢†åŸŸã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œä½†ä¸´åºŠé‡‡çº³é¢ä¸´äº‹å®å‡†ç¡®æ€§ã€è¯­è¨€ç‰¹å®šé™åˆ¶å’Œæ¨ç†è§£é‡Šå¯é æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§é’ˆå¯¹æ—¥æœ¬åŒ»ç–—é¢†åŸŸçš„ä¼˜åŒ–æ¨¡å‹â€”â€”Preferred-MedLLM-Qwen-72Bï¼Œè¯¥æ¨¡å‹å…·æœ‰é«˜å‡†ç¡®æ€§å’Œç¨³å®šæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬é€šè¿‡æŒç»­é¢„è®­ç»ƒæ·±åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œå¹¶é€šè¿‡æ¨ç†åå¥½ä¼˜åŒ–æé«˜å¯é æ¨ç†è·¯å¾„çš„ç”Ÿæˆã€‚</li>
<li>åœ¨æ—¥æœ¬åŒ»å­¦æ‰§ç…§è€ƒè¯•åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½åœ¨è¦æ±‚æä¾›è§£é‡Šæ—¶ç»´æŒé«˜å‡†ç¡®æ€§ã€‚</li>
<li>æ¨ç†åå¥½ä¼˜åŒ–æ–¹æ³•ï¼ˆRPOï¼‰åœ¨ç¨³å®šæ¨ç†ç”Ÿæˆæ–¹é¢éå¸¸æœ‰æ•ˆã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¼˜åŒ–å¯é è§£é‡Šä¸å‡†ç¡®æ€§åŒæ ·é‡è¦ï¼Œè¿™å¯¹äºé«˜é£é™©çš„ç‰¹æ®Šåº”ç”¨é¢†åŸŸè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a488178c830cd363ac0d73f8ec9c5569.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08f83bd4db6fd8cbde144a3108deadba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bcfa4601012044f5af23c6c7c41fb43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65fd69c113f4d6e3ccdafc49e22f0042.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DREAM-Disentangling-Risks-to-Enhance-Safety-Alignment-in-Multimodal-Large-Language-Models"><a href="#DREAM-Disentangling-Risks-to-Enhance-Safety-Alignment-in-Multimodal-Large-Language-Models" class="headerlink" title="DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal   Large Language Models"></a>DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal   Large Language Models</h2><p><strong>Authors:Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu</strong></p>
<p>Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \textbf{DREAM} (\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17% improvement in the SIUO safe&amp;effective score compared to GPT-4V. The data and code are available at <a target="_blank" rel="noopener" href="https://github.com/Kizna1ver/DREAM">https://github.com/Kizna1ver/DREAM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå…¶èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œå¸¦æ¥äº†ç‹¬ç‰¹çš„å®‰å…¨æŒ‘æˆ˜ï¼Œä»è€Œå¼•å…¥äº†æ–°çš„æ½œåœ¨æ”»å‡»ç»´åº¦å’Œå¤æ‚çš„é£é™©ç»„åˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»è¯¦ç»†åˆ†æå¼€å§‹ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ¨¡æ€è¾“å…¥ä¸­çš„é€æ­¥æ¨ç†æ¥ç†æ¸…é£é™©ã€‚æˆ‘ä»¬å‘ç°ç³»ç»Ÿçš„å¤šæ¨¡æ€é£é™©åˆ†è§£æ˜¾è‘—æé«˜äº†å¯¹MLLMsçš„é£é™©æ„è¯†ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€é£é™©åˆ†è§£çš„å¼ºå¤§è¾¨åˆ«èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†<strong>DREAM</strong>ï¼ˆ<strong>D</strong>isentangling <strong>R</strong>isks to <strong>E</strong>nhance <strong>A</strong>lignment in <strong>M</strong>LLMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç›‘ç£å¾®è°ƒä»¥åŠæ¥è‡ªAIåé¦ˆçš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼ˆRLAIFï¼‰å¢å¼ºMLLMså®‰å…¨å¯¹é½çš„æ–°æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDREAMåœ¨æ¨ç†å’Œè®­ç»ƒé˜¶æ®µéƒ½èƒ½æ˜¾è‘—æé«˜å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¸å½±å“æ­£å¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼ˆå³è¿‡åº¦å®‰å…¨ï¼‰ï¼Œä¸GPT-4Vç›¸æ¯”ï¼Œåœ¨SIUOå®‰å…¨&amp;æœ‰æ•ˆå¾—åˆ†æ–¹é¢æé«˜äº†16.17%ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Kizna1ver/DREAM">https://github.com/Kizna1ver/DREAM</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18053v1">PDF</a> [NAACL 2025] The first four authors contribute equally, 23 pages,   repo at <a target="_blank" rel="noopener" href="https://github.com/Kizna1ver/DREAM">https://github.com/Kizna1ver/DREAM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é¢ä¸´çš„å®‰å…¨æŒ‘æˆ˜ï¼Œç”±äºå®ƒä»¬èåˆäº†è§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œå¼•å…¥äº†æ–°çš„æ½œåœ¨æ”»å‡»ç»´åº¦å’Œå¤æ‚é£é™©ç»„åˆã€‚æ–‡ç« é€šè¿‡è¯¦ç»†åˆ†æå’Œé€æ­¥æ¨ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œå‘ç°ç³»ç»Ÿæ€§å¤šæ¨¡æ€é£é™©æ‹†è§£èƒ½æ˜¾è‘—æé«˜MLLMsçš„é£é™©æ„è¯†ã€‚åŸºäºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç§åä¸ºDREAMçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒä¸æ¥è‡ªAIåé¦ˆçš„è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜MLLMsçš„å®‰å…¨æ€§å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDREAMåœ¨æ¨ç†å’Œè®­ç»ƒé˜¶æ®µéƒ½èƒ½æ˜¾è‘—æé«˜å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¸å½±å“æ­£å¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œä¸GPT-4Vç›¸æ¯”ï¼ŒSIUOå®‰å…¨&amp;æœ‰æ•ˆå¾—åˆ†æé«˜äº†16.17%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èåˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®ï¼Œå¸¦æ¥ç‹¬ç‰¹çš„å®‰å…¨æŒ‘æˆ˜ã€‚</li>
<li>ç³»ç»Ÿæ€§å¤šæ¨¡æ€é£é™©æ‹†è§£æœ‰åŠ©äºå¢å¼ºMLLMsçš„é£é™©æ„è¯†ã€‚</li>
<li>DREAMæ–¹æ³•é€šè¿‡ç›‘ç£å¾®è°ƒä¸è¿­ä»£å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜MLLMsçš„å®‰å…¨æ€§å¯¹é½ã€‚</li>
<li>DREAMæ–¹æ³•åœ¨æ¨ç†å’Œè®­ç»ƒé˜¶æ®µéƒ½èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„å®‰å…¨æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDREAMæ–¹æ³•åœ¨æé«˜å®‰å…¨æ€§çš„åŒæ—¶ï¼Œä¸å½±å“æ¨¡å‹åœ¨æ­£å¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä¸GPT-4Vç›¸æ¯”ï¼Œé‡‡ç”¨DREAMæ–¹æ³•çš„æ¨¡å‹åœ¨SIUOå®‰å…¨&amp;æœ‰æ•ˆå¾—åˆ†ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>ç›¸å…³æ•°æ®å’Œä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Kizna1ver/DREAM%E3%80%82">https://github.com/Kizna1ver/DREAMã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-65a0fa66690ef697c6f0dc3528e3f0da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1989da3e210c43bc1f69dd8dd23d3394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f8ed7b92acc34c05baf3a9d1cb674cb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8669d30dd354840f06b6835d1db4a7ad.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Evaluating-Machine-Expertise-How-Graduate-Students-Develop-Frameworks-for-Assessing-GenAI-Content"><a href="#Evaluating-Machine-Expertise-How-Graduate-Students-Develop-Frameworks-for-Assessing-GenAI-Content" class="headerlink" title="Evaluating Machine Expertise: How Graduate Students Develop Frameworks   for Assessing GenAI Content"></a>Evaluating Machine Expertise: How Graduate Students Develop Frameworks   for Assessing GenAI Content</h2><p><strong>Authors:Celia Chen, Alex Leitch</strong></p>
<p>This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we identify patterns in how these emerging professionals assess and engage with AI-generated content. Our findings reveal that students construct evaluation frameworks shaped by three main factors: professional identity, verification capabilities, and system navigation experience. Rather than uniformly accepting or rejecting LLM outputs, students protect domains central to their professional identities while delegating othersâ€“with managers preserving conceptual work, designers safeguarding creative processes, and programmers maintaining control over core technical expertise. These evaluation frameworks are further influenced by studentsâ€™ ability to verify different types of content and their experience navigating complex systems. This research contributes to web science by highlighting emerging human-genAI interaction patterns and suggesting how platforms might better support users in developing effective frameworks for evaluating machine-generated expertise signals in AI-mediated web environments. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†ç ”ç©¶ç”Ÿå¦‚ä½•åœ¨åŸºäºç½‘ç»œçš„ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº’åŠ¨ä¸­ï¼Œæ„å»ºè¯„ä¼°æœºå™¨ç”Ÿæˆä¸“ä¸šçŸ¥è¯†çš„æ¡†æ¶ã€‚é€šè¿‡ç»“åˆè°ƒæŸ¥ã€LLMäº¤äº’è®°å½•ä»¥åŠ14åç ”ç©¶ç”Ÿçš„æ·±å…¥è®¿è°ˆçš„å®šæ€§ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†è¿™äº›æ–°å…´ä¸“ä¸šäººå£«å¦‚ä½•è¯„ä¼°å’Œå‚ä¸äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå­¦ç”Ÿæ„å»ºçš„è¯„ä¼°æ¡†æ¶å—åˆ°ä¸‰ä¸ªä¸»è¦å› ç´ çš„å½±å“ï¼šä¸“ä¸šèº«ä»½ã€éªŒè¯èƒ½åŠ›å’Œç³»ç»Ÿå¯¼èˆªç»éªŒã€‚å­¦ç”Ÿå¹¶ä¸ä¼šå…¨ç›˜æ¥å—æˆ–æ‹’ç»LLMçš„è¾“å‡ºï¼Œè€Œæ˜¯ä¿æŠ¤å…¶ä¸“ä¸šèº«ä»½çš„æ ¸å¿ƒé¢†åŸŸï¼ŒåŒæ—¶å§”æ´¾å…¶ä»–é¢†åŸŸâ€”â€”ç»ç†ä¿ç•™æ¦‚å¿µå·¥ä½œï¼Œè®¾è®¡å¸ˆä¿æŠ¤åˆ›æ„è¿‡ç¨‹ï¼Œç¨‹åºå‘˜æ§åˆ¶æ ¸å¿ƒæŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ã€‚è¿™äº›è¯„ä¼°æ¡†æ¶è¿˜å—åˆ°å­¦ç”ŸéªŒè¯ä¸åŒç±»å‹å†…å®¹çš„èƒ½åŠ›ä»¥åŠä»–ä»¬å¯¼èˆªå¤æ‚ç³»ç»Ÿç»éªŒçš„å½±å“ã€‚æœ¬ç ”ç©¶é€šè¿‡å¼ºè°ƒæ–°å…´çš„äººæœºäº¤äº’æ¨¡å¼ï¼Œä¸ºç½‘ç»œç§‘å­¦åšå‡ºäº†è´¡çŒ®ï¼Œå¹¶æ¢è®¨äº†å¹³å°å¦‚ä½•æ›´å¥½åœ°æ”¯æŒç”¨æˆ·å¼€å‘æœ‰æ•ˆçš„è¯„ä¼°æœºå™¨ç”Ÿæˆä¸“ä¸šçŸ¥è¯†æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨äººå·¥æ™ºèƒ½ä»‹å¯¼çš„åœ¨çº¿ç¯å¢ƒä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17964v1">PDF</a> Under review at ACM Web Science Conference 2025â€™s Human-GenAI   Interactions Workshop, 4 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æ¢è®¨ç ”ç©¶ç”Ÿå¦‚ä½•åœ¨åŸºäºç½‘ç»œçš„ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„äº¤äº’ä¸­ï¼Œæ„å»ºè¯„ä¼°æœºå™¨ç”Ÿæˆä¸“ä¸šçŸ¥è¯†çš„æ–¹æ³•è®ºæ¡†æ¶ã€‚é€šè¿‡ç»“åˆé—®å·è°ƒæŸ¥ã€LLMäº¤äº’è®°å½•ä»¥åŠæ·±åº¦è®¿è°ˆç­‰å®šæ€§ç ”ç©¶æ–¹æ³•ï¼Œç ”ç©¶å‘ç°äº†æ–°å…´ä¸“ä¸šäººå£«å¦‚ä½•è¯„ä¼°å¹¶èå…¥AIç”Ÿæˆå†…å®¹çš„æ¨¡å¼ã€‚è¯„ä»·æ¡†æ¶ä¸»è¦å—åˆ°èŒä¸šèº«ä»½ã€éªŒè¯èƒ½åŠ›å’Œç³»ç»Ÿå¯¼èˆªç»éªŒä¸‰ä¸ªå› ç´ çš„å½±å“ã€‚ç ”ç©¶ç»“æœå¯¹äºå¹³å°å¦‚ä½•æ›´å¥½åœ°æ”¯æŒç”¨æˆ·åœ¨AIä¸­ä»‹çš„Webç¯å¢ƒä¸­å¼€å‘æœ‰æ•ˆçš„è¯„ä¼°æœºå™¨ç”Ÿæˆä¸“ä¸šçŸ¥è¯†ä¿¡å·çš„æ¡†æ¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶é€šè¿‡è°ƒæŸ¥ç ”ç©¶ç”Ÿå¦‚ä½•è¯„ä¼°ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„äº¤äº’ä¸­ç”Ÿæˆçš„AIå†…å®¹ï¼Œå‘ç°å­¦ç”Ÿæ„å»ºäº†è¯„ä¼°æ¡†æ¶ï¼Œè¿™äº›æ¡†æ¶å—ä¸“ä¸šèº«ä»½ã€éªŒè¯èƒ½åŠ›å’Œç³»ç»Ÿå¯¼èˆªç»éªŒçš„å½±å“ã€‚</li>
<li>åœ¨è¯„ä¼°AIç”Ÿæˆçš„å†…å®¹æ—¶ï¼Œå­¦ç”Ÿå¹¶éå…¨ç›˜æ¥å—æˆ–æ‹’ç»LLMè¾“å‡ºï¼Œè€Œæ˜¯ä¿æŠ¤å…¶ä¸“ä¸šèº«ä»½çš„æ ¸å¿ƒé¢†åŸŸï¼ŒåŒæ—¶å°†å…¶ä»–é¢†åŸŸå§”æ´¾ç»™ä»–äººã€‚</li>
<li>ç®¡ç†å±‚ä¿æŠ¤æ¦‚å¿µæ€§å·¥ä½œï¼Œè®¾è®¡å¸ˆä¿éšœåˆ›æ„è¿‡ç¨‹ï¼Œç¨‹åºå‘˜æŒæ¡æ ¸å¿ƒæŠ€æœ¯ä¸“ä¸šçŸ¥è¯†ï¼Œè¿™äº›èŒä¸šè§’è‰²åœ¨è¯„ä¼°æ¡†æ¶ä¸­å±•ç°å‡ºä¸åŒçš„è¯„ä»·æ ‡å‡†å’Œä¾§é‡ã€‚</li>
<li>å­¦ç”Ÿçš„éªŒè¯èƒ½åŠ›å’Œç³»ç»Ÿå¯¼èˆªç»éªŒå¯¹è¯„ä»·æ¡†æ¶äº§ç”Ÿäº†è¿›ä¸€æ­¥çš„å½±å“ã€‚ä»–ä»¬èƒ½å¤Ÿæ ¹æ®è¿™äº›èƒ½åŠ›è¯„ä¼°ä¸åŒç±»å‹çš„å†…å®¹å’Œå¤æ‚çš„ç³»ç»Ÿã€‚</li>
<li>ç ”ç©¶ç»“æœæ­ç¤ºäº†æ–°å…´çš„äººæœºäº¤äº’æ¨¡å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°æœºå™¨ç”Ÿæˆçš„ä¸“å®¶çŸ¥è¯†æ–¹é¢ã€‚</li>
<li>æœ¬ç ”ç©¶å¯¹Webç§‘å­¦é¢†åŸŸå…·æœ‰è´¡çŒ®ï¼Œå¼ºè°ƒäº†äººç±»ä¸AIäº’åŠ¨ä¸­çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>å¯¹äºå¹³å°è€Œè¨€ï¼Œäº†è§£è¿™äº›è¯„ä¼°æ¡†æ¶æœ‰åŠ©äºæ›´å¥½åœ°æ”¯æŒç”¨æˆ·åœ¨AIä¸­ä»‹çš„Webç¯å¢ƒä¸­ä½¿ç”¨æœºå™¨ç”Ÿæˆçš„ä¸“ä¸šçŸ¥è¯†ä¿¡å·ã€‚è¿™å¯èƒ½å¼•å¯¼å¹³å°è®¾è®¡æ›´äººæ€§åŒ–çš„AIäº¤äº’ç•Œé¢å’ŒåŠŸèƒ½ï¼Œä»¥æé«˜ç”¨æˆ·å¯¹AIç”Ÿæˆå†…å®¹çš„ä¿¡ä»»åº¦å’Œæ»¡æ„åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17964">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f8650a46d840ec83485dec070f300b88.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Collaborating-Action-by-Action-A-Multi-agent-LLM-Framework-for-Embodied-Reasoning"><a href="#Collaborating-Action-by-Action-A-Multi-agent-LLM-Framework-for-Embodied-Reasoning" class="headerlink" title="Collaborating Action by Action: A Multi-agent LLM Framework for Embodied   Reasoning"></a>Collaborating Action by Action: A Multi-agent LLM Framework for Embodied   Reasoning</h2><p><strong>Authors:Isadora White, Kolby Nottingham, Ayush Maniar, Max Robinson, Hansen Lillemark, Mehul Maheshwari, Lianhui Qin, Prithviraj Ammanabrolu</strong></p>
<p>Collaboration is ubiquitous and essential in day-to-day life â€“ from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: <a target="_blank" rel="noopener" href="https://mindcraft-minecollab.github.io/">https://mindcraft-minecollab.github.io/</a> </p>
<blockquote>
<p>åä½œåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„æ— å¤„ä¸åœ¨ä¸”è‡³å…³é‡è¦ï¼Œæ— è®ºæ˜¯äº¤æµæƒ³æ³•ã€åˆ†é…ä»»åŠ¡è¿˜æ˜¯å…±åŒåˆ¶å®šè®¡åˆ’ã€‚æœ¬ç ”ç©¶æ¢è®¨LLMå¦‚ä½•é€‚åº”åä½œä»¥æ‰§è¡Œå¤æ‚çš„å®ä½“æ¨ç†ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MINDcraftå¹³å°ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¹³å°ï¼Œæ—¨åœ¨ä½¿LLMä»£ç†èƒ½å¤Ÿæ§åˆ¶Minecraftè¿™æ¬¾å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­çš„è§’è‰²ï¼›ä»¥åŠMineCollabåŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•å®ä½“å’Œåä½œæ¨ç†çš„ä¸åŒæ–¹é¢ã€‚ä¸€é¡¹å®éªŒç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„ä»£ç†è¿›è¡Œæœ‰æ•ˆåä½œçš„ä¸»è¦ç“¶é¢ˆæ˜¯é«˜æ•ˆçš„è‡ªç„¶è¯­è¨€é€šä¿¡ï¼Œå½“éœ€è¦ä¼ è¾¾è¯¦ç»†çš„ä»»åŠ¡å®Œæˆè®¡åˆ’æ—¶ï¼Œä»£ç†æ€§èƒ½ä¼šä¸‹é™é«˜è¾¾15%ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œç°æœ‰çš„LLMä»£ç†åœ¨å¤šæ™ºèƒ½ä½“åä½œæ–¹é¢ä¼˜åŒ–ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨å®ä½“åœºæ™¯ä¸­ï¼Œå¹¶å¼ºè°ƒéœ€è¦é‡‡ç”¨è¶…å‡ºä¸Šä¸‹æ–‡å’Œæ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç½‘ç«™å¯åœ¨æ­¤æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://mindcraft-minecollab.github.io/%E3%80%82">https://mindcraft-minecollab.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17950v1">PDF</a> 9 pages of main paper with 6 main figures, overall 28 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†LLMå¦‚ä½•é€‚åº”æ€§åœ°åä½œæ‰§è¡Œå¤æ‚çš„ä½“éªŒå¼æ¨ç†ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†MINDcraftå¹³å°å’ŒMineCollabåŸºå‡†æµ‹è¯•ï¼Œåˆ†åˆ«ç”¨äºLLMä»£ç†æ§åˆ¶Minecraftæ¸¸æˆä¸­çš„è§’è‰²å’Œæµ‹è¯•ä»£ç†çš„åä½œå’Œèº«ä½“æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„ä»£ç†åœ¨æœ‰æ•ˆåä½œæ–¹é¢çš„ä¸»è¦ç“¶é¢ˆæ˜¯è‡ªç„¶è¯­è¨€æ²Ÿé€šçš„æ•ˆç‡ï¼Œå½“éœ€è¦æ²Ÿé€šè¯¦ç»†çš„å®Œæˆè®¡åˆ’æ—¶ï¼Œä»£ç†æ€§èƒ½ä¼šä¸‹é™é«˜è¾¾15%ã€‚å› æ­¤ï¼Œç°æœ‰LLMä»£ç†åœ¨å¤šæ™ºèƒ½ä½“åä½œæ–¹é¢çš„ä¼˜åŒ–ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨ä½“éªŒå¼åœºæ™¯ä¸­ï¼Œå¹¶å¼ºè°ƒéœ€è¦é‡‡ç”¨è¶…è¶Šä¸Šä¸‹æ–‡å’Œæ¨¡ä»¿å­¦ä¹ çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨é€‚åº”åä½œæ‰§è¡Œå¤æ‚ä½“éªŒå¼æ¨ç†ä»»åŠ¡æ–¹é¢éœ€è¦è¿›è¡Œæ›´å¤šç ”ç©¶ã€‚</li>
<li>MINDcraftå¹³å°ç”¨äºLLMä»£ç†æ§åˆ¶Minecraftä¸­çš„è§’è‰²ã€‚</li>
<li>MineCollabåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°ä»£ç†çš„åä½œå’Œèº«ä½“æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰LLMä»£ç†åœ¨æœ‰æ•ˆåä½œæ–¹é¢çš„ä¸»è¦ç“¶é¢ˆæ˜¯è‡ªç„¶è¯­è¨€æ²Ÿé€šçš„æ•ˆç‡ã€‚</li>
<li>å½“éœ€è¦è¯¦ç»†æ²Ÿé€šä»»åŠ¡å®Œæˆè®¡åˆ’æ—¶ï¼Œä»£ç†æ€§èƒ½ä¸‹é™æ˜¾è‘—ã€‚</li>
<li>ç°æœ‰LLMä»£ç†åœ¨å¤šæ™ºèƒ½ä½“åä½œæ–¹é¢çš„ä¼˜åŒ–ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b0c665af264fd9b8ef89a85bb54de0ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b7adbaec14719848db5914556a64c71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-387e6cddb5042fb16211578fb1b5ec89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-67ee93ceb7d494f4963e24d343742f2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebb32bb5bb534dcc996af4bc33a7fb1b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training"><a href="#DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training" class="headerlink" title="DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training"></a>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training</h2><p><strong>Authors:Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li</strong></p>
<p>Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å„ç§å¤æ‚çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å­¦æœ¯ç•Œä»ç„¶ç¼ºä¹å¯¹åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„æ·±å…¥äº†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«çº¦334ä¸‡ä¸ªä¸åŒéš¾åº¦çº§åˆ«çš„å”¯ä¸€æŸ¥è¯¢å’Œçº¦ç”±å¤šä¸ªæ¨¡å‹ç»è¿‡å¤šæ¬¡ä¼ é€’ç”Ÿæˆçš„4000ä¸‡ä¸ªè’¸é¦å“åº”ã€‚æˆ‘ä»¬åˆ©ç”¨é€šè¿‡ç‡å’Œå˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ç²¾ç¡®é€‰æ‹©äº†æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼çš„è½¬å˜ï¼Œè¿™è¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†å¯¼å‘è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡æ‰èƒ½è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ä½¿ç”¨ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„é€šè¿‡ç‡è¾¾åˆ°79.2%ã€‚è¿™ä¸€ç»“æœè¶…è¶Šäº†å¤§å¤šæ•°å½“å‰çš„è’¸é¦æ¨¡å‹ï¼Œå¹¶æ¥è¿‘äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°å’ŒåŸ¹è®­æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒäº†æ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚æ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17565v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ•°æ®é›†æ„å»ºæ–¹æ³•ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦334ä¸‡æ¡ç‹¬ç‰¹æŸ¥è¯¢å’Œçº¦4åƒä¸‡æ¡ç»è¿‡è’¸é¦çš„å“åº”ï¼Œé€šè¿‡éš¾åº¦åˆ†çº§å’Œå¤šæ¨¡å‹å¤šæ¬¡è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åˆ©ç”¨é€šè¿‡ç‡åŠå˜å¼‚ç³»æ•°ç²¾ç¡®ç­›é€‰æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼çš„è½¬å˜ã€‚é€šè¿‡è°ƒæ•´å­¦ä¹ ç‡ï¼Œå®ç°äº†å¯¹åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ˜¾è‘—æå‡ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°79.2%çš„é€šè¿‡ç‡ï¼Œæ¥è¿‘å½“å‰æœ€ä½³æ€§èƒ½ã€‚æ•°æ®é›†å·²å…¬å¼€ä¾›å¼€æºä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«æ•°ç™¾ä¸‡æ¡ç‹¬ç‰¹æŸ¥è¯¢å’Œå“åº”ã€‚</li>
<li>åˆ©ç”¨é€šè¿‡ç‡å’Œå˜å¼‚ç³»æ•°ç²¾ç¡®é€‰æ‹©æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>å‘ç°åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚</li>
<li>é€šè¿‡è°ƒæ•´å­¦ä¹ ç‡ï¼Œæ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°79.2%çš„é€šè¿‡ç‡ã€‚</li>
<li>æ•°æ®é›†å·²å…¬å¼€å¹¶å¯ä¾›å…¬ä¼—ä½¿ç”¨ï¼Œæ—¨åœ¨æ¨åŠ¨å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43bbdf4109b00c3cf285ff26fdce5789.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Skywork-R1V2-Multimodal-Hybrid-Reinforcement-Learning-for-Reasoning"><a href="#Skywork-R1V2-Multimodal-Hybrid-Reinforcement-Learning-for-Reasoning" class="headerlink" title="Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning"></a>Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning</h2><p><strong>Authors: Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou</strong></p>
<p>We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that jointly leverages the Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization (GRPO), which harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we introduce the Selective Sample Buffer (SSB) mechanism, which effectively counters the &#96;&#96;Vanishing Advantagesâ€™â€™ dilemma inherent in GRPO by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinationsâ€“a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and 73.6 on MMMU. These results underscore R1V2â€™s superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility <a target="_blank" rel="noopener" href="https://huggingface.co/Skywork/Skywork-R1V2-38B">https://huggingface.co/Skywork/Skywork-R1V2-38B</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºSkywork R1V2ï¼Œè¿™æ˜¯ä¸‹ä¸€ä»£å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œä¹Ÿæ˜¯å…¶å‰èº«Skywork R1Vçš„é‡å¤§é£è·ƒã€‚R1V2çš„æ ¸å¿ƒå¼•å…¥äº†ä¸€ç§æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œè¯¥èŒƒå¼è”åˆåˆ©ç”¨äº†æ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œå°†å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ä¸åŸºäºè§„åˆ™çš„ç­–ç•¥ç›¸ç»“åˆï¼Œä»è€Œè§£å†³äº†é•¿æœŸå­˜åœ¨çš„åœ¨å¹³è¡¡å¤æ‚æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€‰æ‹©æ€§æ ·æœ¬ç¼“å†²ï¼ˆSSBï¼‰æœºåˆ¶ï¼Œé€šè¿‡ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„é«˜ä»·å€¼æ ·æœ¬ä¼˜å…ˆåŒ–ï¼Œæœ‰æ•ˆåº”å¯¹GRPOå›ºæœ‰çš„â€œä¼˜åŠ¿æ¶ˆå¤±â€å›°å¢ƒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿‡å¤šçš„å¼ºåŒ–ä¿¡å·å¯èƒ½å¯¼è‡´è§†è§‰å¹»è§‰â€”â€”æˆ‘ä»¬é€šè¿‡è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ ¡å‡†å¥–åŠ±é˜ˆå€¼æ¥ç³»ç»Ÿç›‘æµ‹å’Œç¼“è§£è¿™ä¸€ç°è±¡ã€‚å®è¯ç»“æœè¯å®äº†R1V2çš„å“è¶Šèƒ½åŠ›ï¼Œå…¶åœ¨OlympiadBenchä¸Šè¾¾åˆ°62.6ï¼ŒAIME2024ä¸Šè¾¾åˆ°78.9ï¼ŒLiveCodeBenchä¸Šè¾¾åˆ°63.6ï¼ŒMMMUä¸Šè¾¾åˆ°73.6ã€‚è¿™äº›ç»“æœçªæ˜¾äº†R1V2åœ¨ç°æœ‰å¼€æºæ¨¡å‹ä¸­çš„ä¼˜è¶Šæ€§ï¼Œå¹¶è¡¨æ˜åœ¨ç¼©å°ä¸é¡¶å°–ä¸“æœ‰ç³»ç»Ÿï¼ˆåŒ…æ‹¬Gemini 2.5å’ŒOpenAI-o4-miniï¼‰çš„æ€§èƒ½å·®è·æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚Skywork R1V2æ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¼€æ”¾æ€§å’Œå¯é‡å¤æ€§ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/Skywork/Skywork-R1V2-38B">https://huggingface.co/Skywork/Skywork-R1V2-38B</a> æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16656v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Skywork R1V2æ˜¯ä¸€æ¬¾å…ˆè¿›çš„è·¨æ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œç›¸æ¯”å…¶å‰èº«Skywork R1Væœ‰äº†é‡å¤§çªç ´ã€‚å®ƒé‡‡ç”¨æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œç»“åˆMixed Preference Optimization (MPO)å’ŒGroup Relative Policy Optimization (GRPO)ï¼Œå¹³è¡¡äº†é«˜çº§æ¨ç†èƒ½åŠ›ä¸å¹¿æ³›æ³›åŒ–ã€‚ä¸ºæé«˜è®­ç»ƒæ•ˆç‡ï¼Œå¼•å…¥äº†Selective Sample Buffer (SSB)æœºåˆ¶ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹èƒ½æœ‰æ•ˆåº”å¯¹è¿‡åº¦å¼ºåŒ–ä¿¡å·å¯¼è‡´çš„è§†è§‰å¹»è§‰ï¼Œå¹¶é€šè¿‡æ ¡å‡†å¥–åŠ±é˜ˆå€¼è¿›è¡Œç¼“è§£ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼ŒR1V2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šï¼Œå¦‚OlympiadBenchã€AIME2024ã€LiveCodeBenchå’ŒMMMUç­‰ï¼Œå¹¶å…¬å¼€äº†æ¨¡å‹æƒé‡ä»¥ä¿ƒè¿›å¼€æ”¾æ€§å’Œå¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Skywork R1V2æ˜¯è·¨æ¨¡æ€æ¨ç†æ¨¡å‹çš„ä¸‹ä¸€ä»£ç‰ˆæœ¬ï¼Œç›¸æ¯”å‰ä»£æœ‰æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œç»“åˆMPOå’ŒGRPOï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥SSBæœºåˆ¶æé«˜è®­ç»ƒæ•ˆç‡ï¼Œè§£å†³GRPOä¸­çš„â€œä¼˜åŠ¿æ¶ˆå¤±â€éš¾é¢˜ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿåº”å¯¹è¿‡åº¦å¼ºåŒ–ä¿¡å·å¯¼è‡´çš„è§†è§‰å¹»è§‰ï¼Œé€šè¿‡æ ¡å‡†å¥–åŠ±é˜ˆå€¼è¿›è¡Œç¼“è§£ã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºSkywork R1V2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>æ¨¡å‹æƒé‡å·²å…¬å¼€ï¼Œä»¥ä¿ƒè¿›å¼€æ”¾æ€§å’Œå¯é‡å¤æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16656">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76f08ef0c956678cc2a0872e2f66821f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dda8f93ba3a34c5d51cf9723a2722154.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ReasoningV-Efficient-Verilog-Code-Generation-with-Adaptive-Hybrid-Reasoning-Model"><a href="#ReasoningV-Efficient-Verilog-Code-Generation-with-Adaptive-Hybrid-Reasoning-Model" class="headerlink" title="ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model"></a>ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model</h2><p><strong>Authors:Haiyan Qin, Zhiwei Xie, Jingjing Li, Liangchen Li, Xiaotong Feng, Junzhan Liu, Wang Kang</strong></p>
<p>Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75% while preserving performance. Experimental results demonstrate ReasoningVâ€™s effectiveness with a pass@1 accuracy of 57.8% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at <a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/ReasoningV">https://github.com/BUAA-CLab/ReasoningV</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå¤§åœ°æ¨åŠ¨äº†Verilogä»£ç ç”Ÿæˆçš„å‘å±•ï¼Œä½†åœ¨æ•°æ®è´¨é‡ã€æ¨ç†èƒ½åŠ›å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReasoningVçš„æ–°å‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œå°†è®­ç»ƒçš„å†…åœ¨èƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ç›¸ç»“åˆï¼Œç”¨äºVerilogä»£ç ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸‰é¡¹äº’è¡¥åˆ›æ–°ï¼š1ï¼‰ReasoningV-5Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«5000ä¸ªåŠŸèƒ½éªŒè¯å®ä¾‹çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œé€šè¿‡PyraNetæ ·æœ¬çš„å¤šç»´è¿‡æ»¤åˆ›å»ºæ¨ç†è·¯å¾„ï¼›2ï¼‰ç»“åˆå‚æ•°é«˜æ•ˆå¾®è°ƒåŸºç¡€çŸ¥è¯†ä¸å…¨å‚æ•°ä¼˜åŒ–å¢å¼ºæ¨ç†èƒ½åŠ›çš„ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼›3ï¼‰è‡ªé€‚åº”æ¨ç†æœºåˆ¶ï¼Œæ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œæœ€å¤šå¯å‡å°‘75%çš„ä»¤ç‰Œæ¶ˆè€—ã€‚å®éªŒç»“æœè¯æ˜äº†ReasoningVçš„æœ‰æ•ˆæ€§ï¼Œåœ¨VerilogEval-humanä¸Šçš„pass@1å‡†ç¡®ç‡ä¸º57.8%ï¼Œæ€§èƒ½ä¸é¢†å…ˆçš„å•†ä¸šæ¨¡å‹ï¼ˆå¦‚Gemini-2.0-flashçš„59.5%ï¼‰ç›¸ç«äº‰ï¼Œå¹¶è¶…è¿‡äº†ä¹‹å‰æœ€ä½³å¼€æºæ¨¡å‹çš„10.4ä¸ªç™¾åˆ†ç‚¹ã€‚ReasoningVä¸ºæ¨è¿›AIé©±åŠ¨çš„ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–æä¾›äº†æ›´å¯é ã€æ›´å¯è¡Œçš„é€”å¾„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BUAA-CLab/ReasoningV%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/BUAA-CLab/ReasoningVè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14560v2">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ReasoningVï¼Œä¸€ä¸ªç”¨äºVerilogä»£ç ç”Ÿæˆçš„æ–°å‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ï¼Œèåˆäº†è®­ç»ƒçš„å†…åœ¨èƒ½åŠ›ä¸åŠ¨æ€æ¨ç†é€‚åº”ã€‚è®ºæ–‡æå‡ºäº†ä¸‰é¡¹åˆ›æ–°ï¼šä¸€æ˜¯ReasoningV-5Kæ•°æ®é›†ï¼ŒåŒ…å«5000ä¸ªç»è¿‡åŠŸèƒ½éªŒè¯çš„å®ä¾‹ï¼›äºŒæ˜¯ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼Œç»“åˆäº†å‚æ•°æ•ˆç‡å¾®è°ƒä¸å…¨å‚æ•°ä¼˜åŒ–ï¼›ä¸‰æ˜¯è‡ªé€‚åº”æ¨ç†æœºåˆ¶ï¼Œèƒ½æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ã€‚å®éªŒç»“æœè¯æ˜ReasoningVçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨VerilogEval-humanä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œè¶…è¿‡äº†å…ˆå‰çš„æœ€ä½³å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ReasoningVæ˜¯ä¸€ä¸ªç”¨äºVerilogä»£ç ç”Ÿæˆçš„æ–°å‹æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆæ¨ç†ç­–ç•¥ã€‚</li>
<li>è¯¥æ¨¡å‹æå‡ºäº†ReasoningV-5Kæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„åŠŸèƒ½éªŒè¯å®ä¾‹ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒæ³•ç»“åˆäº†å‚æ•°æ•ˆç‡å¾®è°ƒä¸å…¨å‚æ•°ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚<br>4.è‡ªé€‚åº”æ¨ç†æœºåˆ¶èƒ½æ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œå‡å°‘è®¡ç®—æ¶ˆè€—ã€‚</li>
<li>ReasoningVåœ¨VerilogEval-humanä¸Šçš„è¡¨ç°å…·æœ‰ç«äº‰åŠ›ï¼Œè¶…è¿‡äº†æŸäº›å•†ä¸šæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹æé«˜äº†AIé©±åŠ¨ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–çš„å¯é æ€§ã€‚</li>
<li>æ¨¡å‹ã€æ•°æ®å’Œä»£ç å·²å…¬å¼€ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-63d1622581890800cb7d8ebfc77a2e77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900e1fe231abade6cd7d6321d277566f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adc86ca0cc94433e509d30c6cb047403.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f520ec0ccaa1ba139c65834ee18ba91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8da43966b775ad9920ea359f4a949617.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30a95a460187187a62ce50b7d6216835.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72bfa5579921ebc1397cc2a763799b1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-49316afee573591163a4d109bb954243.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Can-Reasoning-LLMs-Enhance-Clinical-Document-Classification"><a href="#Can-Reasoning-LLMs-Enhance-Clinical-Document-Classification" class="headerlink" title="Can Reasoning LLMs Enhance Clinical Document Classification?"></a>Can Reasoning LLMs Enhance Clinical Document Classification?</h2><p><strong>Authors:Akram Mustafa, Usman Naseem, Mostafa Rahimi Azghadi</strong></p>
<p>Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications. </p>
<blockquote>
<p>ä¸´åºŠæ–‡æ¡£åˆ†ç±»å¯¹äºå°†éç»“æ„åŒ–çš„åŒ»å­¦æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„ICD-10è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç”±äºå¤æ‚çš„åŒ»ç–—è¯­è¨€ã€éšç§çº¦æŸå’Œæœ‰é™çš„æœ‰æ ‡æ³¨æ•°æ®é›†ï¼Œå®ƒé¢ä¸´ç€æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè¿™ä¸ªä»»åŠ¡æä¾›äº†æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡çš„æœ‰å‰é€”çš„æ”¹è¿›æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†8ç§LLMçš„æ€§èƒ½å’Œä¸€è‡´æ€§ï¼›å…¶ä¸­å››ç§æ˜¯æ¨ç†æ¨¡å‹ï¼ˆQwen QWQã€Deepseek Reasonerã€GPT o3 Miniã€Gemini 2.0 Flash Thinkingï¼‰ï¼Œå››ç§æ˜¯éæ¨ç†æ¨¡å‹ï¼ˆLlama 3.3ã€GPT 4o Miniã€Gemini 2.0 Flashã€Deepseek Chatï¼‰ï¼›ä½¿ç”¨MIMIC-IVæ•°æ®é›†å¯¹ä¸´åºŠå‡ºé™¢æ€»ç»“è¿›è¡Œåˆ†ç±»ã€‚ä½¿ç”¨cTAKESå¯¹ä¸´åºŠå™è¿°è¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œäº†ä¸‰æ¬¡å®éªŒè¿è¡Œè¯„ä¼°ï¼Œä»¥å¤šæ•°æŠ•ç¥¨å†³å®šæœ€ç»ˆé¢„æµ‹ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹çš„å‡†ç¡®æ€§å’ŒF1åˆ†æ•°å‡ä¼˜äºéæ¨ç†æ¨¡å‹ï¼ˆå‡†ç¡®ç‡71% vs 68%ï¼ŒF1åˆ†æ•°67% vs 60%ï¼‰ï¼Œå…¶ä¸­Gemini 2.0 Flash Thinkingçš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°æœ€é«˜ï¼ˆåˆ†åˆ«ä¸º75%å’Œ76%ï¼‰ã€‚ç„¶è€Œï¼Œéæ¨ç†æ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„ç¨³å®šæ€§ï¼ˆä¸€è‡´æ€§91% vs 84%ï¼‰ã€‚ä¸åŒICD-10ä»£ç çš„æ€§èƒ½å­˜åœ¨å·®å¼‚ï¼Œæ¨ç†æ¨¡å‹åœ¨å¤æ‚ç—…ä¾‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æŠ½è±¡ç±»åˆ«ä¸­é‡åˆ°å›°éš¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼Œæç¤ºé‡‡ç”¨æ··åˆæ–¹æ³•å¯èƒ½ä¼˜åŒ–ä¸´åºŠç¼–ç ã€‚æœªæ¥çš„ç ”ç©¶åº”æ¢ç´¢å¤šæ ‡ç­¾åˆ†ç±»ã€ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒä»¥åŠé›†æˆæ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08040v2">PDF</a> 27 pages</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶è¯„ä¼°äº†å…«ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŸºäºMIMIC-IVæ•°æ®é›†çš„ä¸´åºŠå‡ºé™¢æ‘˜è¦åˆ†ç±»ä¸­çš„æ€§èƒ½å’Œä¸€è‡´æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹åœ¨å‡†ç¡®æ€§å’ŒF1åˆ†æ•°ä¸Šä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œä½†åœ¨ä¸€è‡´æ€§æ–¹é¢ç•¥æ˜¾ä¸è¶³ã€‚æœ€ä½³æ¨¡å‹ä¸ºGemini 2.0 Flash Thinkingï¼Œä½†å®é™…åº”ç”¨ä¸­ä»éœ€è€ƒè™‘å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¸´åºŠæ–‡æ¡£åˆ†ç±»å¯¹äºå°†éç»“æ„åŒ–åŒ»å­¦æ–‡æœ¬è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„ICD-10è¯Šæ–­è‡³å…³é‡è¦ï¼Œé¢ä¸´å¤æ‚åŒ»å­¦è¯­è¨€ã€éšç§é™åˆ¶å’Œæœ‰é™æ³¨é‡Šæ•°æ®é›†çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æ¨ç†æ¨¡å‹åœ¨åˆ†ç±»ä¸´åºŠå‡ºé™¢æ‘˜è¦æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§å’ŒF1åˆ†æ•°ï¼Œä½†éæ¨ç†æ¨¡å‹æ›´ç¨³å®šã€‚</li>
<li>Gemini 2.0 Flash Thinkingåœ¨å‡†ç¡®æ€§æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†å®é™…åº”ç”¨ä¸­éœ€è¦æƒè¡¡å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ä¸åŒICD-10ä»£ç çš„æ€§èƒ½æœ‰æ‰€ä¸åŒï¼Œæ¨ç†æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¡ˆä¾‹æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œä½†åœ¨æŠ½è±¡ç±»åˆ«æ–¹é¢é‡åˆ°å›°éš¾ã€‚</li>
<li>ç ”ç©¶å»ºè®®é‡‡ç”¨æ··åˆæ–¹æ³•ä¼˜åŒ–ä¸´åºŠç¼–ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0293746784088d707a360ee990376f5d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generative-Evaluation-of-Complex-Reasoning-in-Large-Language-Models"><a href="#Generative-Evaluation-of-Complex-Reasoning-in-Large-Language-Models" class="headerlink" title="Generative Evaluation of Complex Reasoning in Large Language Models"></a>Generative Evaluation of Complex Reasoning in Large Language Models</h2><p><strong>Authors:Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang</strong></p>
<p>With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMOâ€™s value as a robust, enduring assessment tool for genuine LLM reasoning capabilities. </p>
<blockquote>
<p>éšç€å…·æœ‰è¶…äººç±»æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ï¼Œä¸€ä¸ªé‡è¦çš„é—®é¢˜å‡ºç°äº†ï¼šLLMsæ˜¯å¦çœŸçš„è¿›è¡Œæ¨ç†ï¼Œè¿˜æ˜¯ä»…ä»…ä»ä»–ä»¬å¹¿æ³›é‡‡é›†çš„ã€æ¥è‡ªç½‘ç»œçˆ¬å–çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆï¼Ÿä¸€æ—¦çº³å…¥åç»­çš„LLMè®­ç»ƒé›†ï¼Œå…¬å¼€å‘å¸ƒçš„åŸºå‡†æµ‹è¯•ä¸å¯é¿å…åœ°ä¼šè¢«æ±¡æŸ“ï¼Œä»è€Œç ´åäº†å®ƒä»¬ä½œä¸ºå¿ å®è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†KUMOï¼Œä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMä¸­æ¨ç†èƒ½åŠ›çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚KUMOååŒåœ°å°†LLMsä¸ç¬¦å·å¼•æ“ç›¸ç»“åˆï¼Œä»¥åŠ¨æ€ç”Ÿæˆå¤šæ ·ä¸”å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ˜¯éƒ¨åˆ†å¯è§‚å¯Ÿçš„ï¼Œå¹¶ä¸”éš¾åº¦å¯è°ƒã€‚é€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“ï¼ŒKUMOæŒç»­åœ°åœ¨æ— é™åˆ¶çš„é¢†åŸŸç”Ÿæˆæ–°ä»»åŠ¡ï¼Œä¿ƒä½¿æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›è€Œéè®°å¿†åŠ›ã€‚æˆ‘ä»¬åœ¨KUMOåˆ›å»ºçš„5000ä¸ªä»»åŠ¡ã€100ä¸ªé¢†åŸŸä¸­å¯¹23æ¬¾æœ€å…ˆè¿›LLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥è¯„ä¼°ä»–ä»¬çš„æ¨ç†èƒ½åŠ›ä¸å¤§å­¦ç”Ÿçš„æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè®¸å¤šLLMsåœ¨ç®€å•çš„æ¨ç†ä»»åŠ¡ä¸Šå·²è¶…è¶Šå¤§å­¦æ°´å¹³ï¼Œè€Œç»è¿‡æ¨ç†èƒ½åŠ›è¯„ä¼°çš„LLMsåœ¨å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ä¸Šè¾¾åˆ°äº†å¤§å­¦æ°´å¹³ã€‚æ­¤å¤–ï¼ŒLLMåœ¨KUMOä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•çš„ç»“æœå…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ï¼Œè¿™å‡¸æ˜¾äº†KUMOä½œä¸ºè¯„ä¼°LLMçœŸå®æ¨ç†èƒ½åŠ›çš„ç¨³å¥ã€æŒä¹…è¯„ä¼°å·¥å…·çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02810v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å¼•å‘äº†å…³äºå…¶æ˜¯å¦çœŸæ­£è¿›è¡Œæ¨ç†è¿˜æ˜¯ä»…ä»å¤§è§„æ¨¡ç½‘ç»œæŠ“å–çš„è®­ç»ƒæ•°æ®é›†ä¸­å›å¿†ç­”æ¡ˆçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†KUMOè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ï¼Œå¹¶å¯ä»¥éƒ¨åˆ†è§‚å¯Ÿå’Œè°ƒæ•´éš¾åº¦ï¼Œä»¥è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹23ä¸ªæœ€å…ˆè¿›çš„LLMåœ¨KUMOç”Ÿæˆçš„5000ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°è®¸å¤šLLMåœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å·²ç»è¶…è¶Šäº†å¤§å­¦æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ä¸­ä¹Ÿèƒ½è¾¾åˆ°å¤§å­¦æ°´å¹³ã€‚æ­¤å¤–ï¼ŒKUMOä»»åŠ¡ä¸Šçš„LLMè¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•ç»“æœé«˜åº¦ç›¸å…³ï¼Œè¯æ˜äº†KUMOä½œä¸ºè¯„ä¼°LLMçœŸæ­£æ¨ç†èƒ½åŠ›çš„ç¨³å¥å·¥å…·çš„å·¨å¤§ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›å—åˆ°è´¨ç–‘ï¼Œéœ€è¦è¯„ä¼°æ¡†æ¶æ¥åŒºåˆ†çœŸæ­£çš„æ¨ç†å’Œè®°å¿†å›ç­”ã€‚</li>
<li>KUMOæ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMæ¨ç†èƒ½åŠ›çš„ç”Ÿæˆè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>KUMOèƒ½å¤Ÿç»“åˆLLMså’Œç¬¦å·å¼•æ“ï¼ŒåŠ¨æ€ç”Ÿæˆå¤šæ ·åŒ–ã€å¤šå›åˆçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>KUMOä»»åŠ¡éƒ¨åˆ†å¯è§‚å¯Ÿå’Œå¯è°ƒæ•´éš¾åº¦ï¼Œé¼“åŠ±æ¨¡å‹å±•ç¤ºçœŸæ­£çš„æ³›åŒ–èƒ½åŠ›è€Œéè®°å¿†åŠ›ã€‚</li>
<li>åœ¨KUMOæ¡†æ¶ä¸Šè¯„ä¼°çš„LLMè¡¨ç°å·²è¶…è¶Šå¤§å­¦æ°´å¹³åœ¨ç®€å•æ¨ç†ä»»åŠ¡ä¸Šï¼Œå¹¶åœ¨å¤æ‚æ¨ç†æŒ‘æˆ˜ä¸­è¾¾åˆ°å¤§å­¦æ°´å¹³ã€‚</li>
<li>LLMåœ¨KUMOä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸æ–°å‘å¸ƒçš„ç°å®ä¸–ç•Œæ¨ç†åŸºå‡†æµ‹è¯•ç»“æœé«˜åº¦ç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e848cc4830d33505126d232a020756f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaa063af0eaac41471c9445e18dc9946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63df7cecd8ef5ea2db485abcbea9c441.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models"><a href="#Med-R1-Reinforcement-Learning-for-Generalizable-Medical-Reasoning-in-Vision-Language-Models" class="headerlink" title="Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models"></a>Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in   Vision-Language Models</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, Xiaofeng Yang</strong></p>
<p>Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional supervised fine-tuning (SFT) and Chain-of-Thought (CoT) strategies that work well in general domains. To address these challenges, we propose Med-R1, a reinforcement learning (RL)-enhanced vision-language model designed to improve generalization and reliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts Group Relative Policy Optimization (GRPO) to encourage reward-guided learning beyond static annotations. We comprehensively evaluate Med-R1 across eight distinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in average accuracy over its base model Qwen2-VL-2B, and even outperforms Qwen2-VL-72B-a model with 36x more parameters. To assess cross-task generalization, we further evaluate Med-R1 on five question types. Med-R1 outperforms Qwen2-VL-2B by 32.06% in question-type generalization, also surpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a crucial component for the success of Deepseek-R1. Our results show that omitting intermediate rationales (No-Thinking-Med-R1) not only improves in-domain and cross-domain generalization with less training, but also challenges the assumption that more reasoning always helps. These findings suggest that in medical VQA, it is not reasoning itself, but its quality and domain alignment, that determine effectiveness. Together, these results highlight that RL improves medical reasoning and generalization, enabling efficient and reliable VLMs for real-world deployment. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªç„¶å›¾åƒæ¨ç†æ–¹é¢å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦æˆåƒæ–¹é¢çš„æ½œåŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åŒ»å­¦è§†è§‰è¯­è¨€ä»»åŠ¡éœ€è¦ç²¾ç¡®çš„ç†è§£å’Œä¸´åºŠè¿è´¯çš„ç­”æ¡ˆï¼Œè¿™å¾ˆéš¾å®ç°ï¼Œå› ä¸ºåŒ»å­¦æ•°æ®çš„å¤æ‚æ€§å’Œé«˜è´¨é‡ä¸“å®¶æ³¨é‡Šçš„ç¨€ç¼ºæ€§ã€‚è¿™äº›æŒ‘æˆ˜é™åˆ¶äº†ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰ç­–ç•¥åœ¨ä¸€èˆ¬é¢†åŸŸä¸­çš„è‰¯å¥½è¡¨ç°ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Med-R1ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºç°å®å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦æ¨ç†ä¸­çš„é€šç”¨æ€§å’Œå¯é æ€§ã€‚åŸºäºDeepSeekç­–ç•¥ï¼ŒMed-R1é‡‡ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥é¼“åŠ±è¶…è¶Šé™æ€æ³¨é‡Šçš„å¥–åŠ±æŒ‡å¯¼å­¦ä¹ ã€‚æˆ‘ä»¬å…¨é¢è¯„ä¼°äº†Med-R1åœ¨å…«ç§ä¸åŒçš„åŒ»å­¦æˆåƒæ¨¡å¼ä¸Šçš„è¡¨ç°ã€‚Med-R1åœ¨å…¶åŸºç¡€æ¨¡å‹Qwen2-VL-2Bä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†29.94%ï¼Œç”šè‡³è¶…è¶Šäº†å…·æœ‰36å€å‚æ•°çš„Qwen2-VL-72Bæ¨¡å‹ã€‚ä¸ºäº†è¯„ä¼°è·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åœ¨äº”ç§é—®é¢˜ç±»å‹ä¸Šè¯„ä¼°äº†Med-R1ã€‚Med-R1åœ¨é—®é¢˜ç±»å‹æ³›åŒ–æ–¹é¢æ¯”Qwen2-VL-2Bé«˜å‡º32.06%ï¼Œä¹Ÿè¶…è¶Šäº†Qwen2-VL-79Bæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥æ¢ç´¢äº†Med-R1ä¸­çš„æ€è€ƒè¿‡ç¨‹ï¼Œè¿™æ˜¯Deepseek-R1æˆåŠŸçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œçœç•¥ä¸­é—´ç†æ€§ï¼ˆæ— æ€è€ƒMed-R1ï¼‰ä¸ä»…èƒ½åœ¨åŸŸå†…å’Œè·¨åŸŸæ³›åŒ–æ–¹é¢æœ‰æ‰€æé«˜ä¸”è®­ç»ƒæ›´å°‘ï¼Œè€Œä¸”è¿˜æŒ‘æˆ˜äº†è¿™æ ·ä¸€ä¸ªå‡è®¾ï¼šæ›´å¤šçš„æ¨ç†æ€»æ˜¯æœ‰å¸®åŠ©çš„ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåœ¨åŒ»å­¦é—®ç­”ä¸­ï¼Œä¸æ˜¯æ¨ç†æœ¬èº«ï¼Œè€Œæ˜¯å…¶è´¨é‡å’Œé¢†åŸŸåŒ¹é…åº¦å†³å®šäº†å…¶æœ‰æ•ˆæ€§ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœå¼ºè°ƒç°å®å­¦ä¹ æ”¹è¿›äº†åŒ»å­¦æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå¯é åœ°ç”¨äºç°å®ä¸–ç•Œéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13939v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»ç–—å›¾åƒæ¨ç†ä¸­ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¢å¼ºçš„è§†è§‰è¯­è¨€æ¨¡å‹Med-R1çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æé«˜åœ¨åŒ»ç–—æ¨ç†ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œå¯é æ€§ã€‚é€šè¿‡é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç­–ç•¥ï¼ŒMed-R1åœ¨å…«ç§ä¸åŒçš„åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œé«˜è´¨é‡çš„æ¨ç†å’Œé¢†åŸŸå¯¹é½å¯¹äºåŒ»ç–—é—®ç­”çš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚æ€»ä½“è€Œè¨€ï¼Œå¼ºåŒ–å­¦ä¹ æœ‰åŠ©äºæ”¹å–„åŒ»ç–—æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—VLMsåœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­æ›´åŠ é«˜æ•ˆå’Œå¯é ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-R1æ¨¡å‹æ˜¯ä¸“ä¸ºåŒ»ç–—æ¨ç†è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ³›åŒ–å’Œå¯é æ€§ã€‚</li>
<li>Med-R1é‡‡ç”¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç­–ç•¥ï¼Œä»¥æå‡æ€§èƒ½ã€‚</li>
<li>Med-R1åœ¨å…«ç§ä¸åŒçš„åŒ»å­¦æˆåƒæ¨¡æ€ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡å‡†ç¡®åº¦æé«˜äº†29.94%ã€‚</li>
<li>Med-R1åœ¨è·¨ä»»åŠ¡æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„è¡¨ç°ï¼Œåœ¨æŸäº›é—®é¢˜ç±»å‹ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å¤§å‹æ¨¡å‹ã€‚</li>
<li>æ·±å…¥ç ”ç©¶æ˜¾ç¤ºï¼Œé«˜è´¨é‡æ¨ç†å’Œé¢†åŸŸå¯¹é½å¯¹äºåŒ»ç–—é—®ç­”çš„æœ‰æ•ˆæ€§è‡³å…³é‡è¦ã€‚</li>
<li>å»é™¤ä¸­é—´æ¨ç†è¿‡ç¨‹ï¼ˆNo-Thinking-Med-R1ï¼‰å¯æé«˜åŸŸå†…å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æŒ‘æˆ˜äº†æ›´å¤šæ¨ç†æ€»æ˜¯æœ‰å¸®åŠ©çš„å‡è®¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13939">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e7547742b384af13f65e5c5c2f564c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc3d722993e59ebdefa178d6c9cfdb12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e4aa6b2b3ab886892f571565849ea01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f82cc8ea9d52fb254adad5f91f029b6e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-895787994dd3884ef8dd6d53226aa9f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a65e2ecffb2b12849006ef59f15d672f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-based-Threat-Assessment"><a href="#Reinforcement-Learning-based-Threat-Assessment" class="headerlink" title="Reinforcement Learning-based Threat Assessment"></a>Reinforcement Learning-based Threat Assessment</h2><p><strong>Authors:Wuzhou Sun, Siyi Li, Qingxiang Zou, Zixing Liao</strong></p>
<p>In some game scenarios, due to the uncertainty of the number of enemy units and the priority of various attributes, the evaluation of the threat level of enemy units as well as the screening has been a challenging research topic, and the core difficulty lies in how to reasonably set the priority of different attributes in order to achieve quantitative evaluation of the threat. In this paper, we innovatively transform the problem of threat assessment into a reinforcement learning problem, and through systematic reinforcement learning training, we successfully construct an efficient neural network evaluator. The evaluator can not only comprehensively integrate the multidimensional attribute features of the enemy, but also effectively combine our state information, thus realizing a more accurate and scientific threat assessment. </p>
<blockquote>
<p>åœ¨æŸäº›æ¸¸æˆåœºæ™¯ä¸­ï¼Œç”±äºæ•Œæ–¹å•ä½æ•°é‡å’Œå„å±æ€§çš„ä¼˜å…ˆçº§çš„ä¸ç¡®å®šæ€§ï¼Œæ•Œæ–¹å•ä½çš„å¨èƒç­‰çº§è¯„ä¼°åŠç­›é€‰ä¸€ç›´æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç ”ç©¶è¯¾é¢˜ï¼Œå…¶æ ¸ä¹Ÿéš¾ç‚¹åœ¨äºå¦‚ä½•åˆç†è®¾ç½®ä¸åŒå±æ€§çš„ä¼˜å…ˆçº§ï¼Œä»¥å®ç°å¨èƒçš„å®šé‡è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°å°†å¨èƒè¯„ä¼°é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ç¥ç»ç½‘ç»œè¯„ä¼°å™¨ã€‚è¯¥è¯„ä¼°å™¨ä¸ä»…èƒ½å…¨é¢æ•´åˆæ•Œæ–¹çš„å¤šç»´å±æ€§ç‰¹å¾ï¼Œè¿˜èƒ½æœ‰æ•ˆåœ°ç»“åˆæˆ‘ä»¬çš„çŠ¶æ€ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®ã€æ›´ç§‘å­¦çš„å¨èƒè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02612v2">PDF</a> The research content is not yet complete and requires further   supplementation and improvement</p>
<p><strong>Summary</strong>ï¼š<br>åœ¨æŸäº›æ¸¸æˆåœºæ™¯ä¸­ï¼Œè¯„ä¼°æ•Œæ–¹å•ä½çš„å¨èƒçº§åˆ«å’Œç­›é€‰å·¥ä½œå› æ•Œæ–¹å•ä½æ•°é‡çš„ä¸ç¡®å®šæ€§å’Œå„ç§å±æ€§çš„ä¼˜å…ˆçº§è€Œå……æ»¡æŒ‘æˆ˜ã€‚æœ¬æ–‡åˆ›æ–°åœ°å°†å¨èƒè¯„ä¼°é—®é¢˜è½¬åŒ–ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œé€šè¿‡ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ç¥ç»ç½‘ç»œè¯„ä¼°å™¨ã€‚è¯¥è¯„ä¼°å™¨èƒ½å¤Ÿç»¼åˆæ•Œæ–¹å¤šç»´å±æ€§ç‰¹å¾ï¼Œå¹¶ç»“åˆè‡ªèº«çŠ¶æ€ä¿¡æ¯ï¼Œå®ç°æ›´å‡†ç¡®ç§‘å­¦çš„å¨èƒè¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ¸¸æˆåœºæ™¯ä¸­æ•Œæ–¹å•ä½å¨èƒè¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› æ¶‰åŠæ•Œæ–¹å•ä½æ•°é‡çš„ä¸ç¡®å®šæ€§å’Œå±æ€§ä¼˜å…ˆçº§çš„è®¾å®šã€‚</li>
<li>æœ¬æ–‡åˆ›æ–°æ€§åœ°é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•è§£å†³å¨èƒè¯„ä¼°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼ŒæˆåŠŸæ„å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ç¥ç»ç½‘ç»œè¯„ä¼°å™¨ã€‚</li>
<li>è¯„ä¼°å™¨èƒ½å¤Ÿç»¼åˆæ•Œæ–¹å¤šç»´å±æ€§ç‰¹å¾ã€‚</li>
<li>è¯„ä¼°å™¨ç»“åˆè‡ªèº«çŠ¶æ€ä¿¡æ¯ï¼Œæé«˜å¨èƒè¯„ä¼°çš„å‡†ç¡®æ€§å’Œç§‘å­¦æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¨èƒè¯„ä¼°ä¸­çš„åº”ç”¨å±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚é—®é¢˜çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87b480f4343d484e55f29269eb9617a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a16527f9a03f27966c2df1faa5cead6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6410b3d8dab318cfff0b82306f7c51da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66dbd9bd7a431e3b77d081bbf0090e1a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models"><a href="#From-System-1-to-System-2-A-Survey-of-Reasoning-Large-Language-Models" class="headerlink" title="From System 1 to System 2: A Survey of Reasoning Large Language Models"></a>From System 1 to System 2: A Survey of Reasoning Large Language Models</h2><p><strong>Authors:Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Zhijiang Guo, Le Song, Cheng-Lin Liu</strong></p>
<p>Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAIâ€™s o1&#x2F;o3 and DeepSeekâ€™s R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \href{<a target="_blank" rel="noopener" href="https://github.com/zzli2022/Awesome-Slow-Reason-System%7D%7BGitHub">https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub</a> Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field. </p>
<blockquote>
<p>å®ç°äººç±»æ°´å¹³çš„æ™ºèƒ½éœ€è¦å®Œå–„ä»å¿«é€Ÿç›´è§‰ç³»ç»Ÿ1åˆ°è¾ƒæ…¢ã€æ›´æ…é‡çš„ç³»ç»Ÿ2æ¨ç†çš„è½¬å˜ã€‚ç³»ç»Ÿ1æ“…é•¿å¿«é€Ÿå¯å‘å¼å†³ç­–ï¼Œè€Œç³»ç»Ÿ2åˆ™ä¾èµ–äºé€»è¾‘æ¨ç†ä»¥åšå‡ºæ›´å‡†ç¡®çš„åˆ¤æ–­å’Œå‡å°‘åè§ã€‚åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å¿«é€Ÿå†³ç­–åˆ¶å®šï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢ç¼ºä¹æ·±åº¦ï¼Œå› ä¸ºå®ƒä»¬å°šæœªå®Œå…¨æ¥å—ç³»ç»Ÿ2æ€ç»´æ‰€å…·æœ‰çš„é€æ­¥åˆ†æç‰¹å¾ã€‚æœ€è¿‘ï¼ŒåƒOpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1ç­‰æ¨ç†LLMå·²ç»åœ¨æ•°å­¦å’Œç¼–ç ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†ä¸“å®¶çº§çš„æ€§èƒ½ï¼Œå®ƒä»¬æ¨¡ä»¿äº†ç³»ç»Ÿ2çš„æ…é‡æ¨ç†ï¼Œå±•ç¤ºäº†äººç±»èˆ¬çš„è®¤çŸ¥èƒ½åŠ›ã€‚è¿™ç¯‡ç»¼è¿°é¦–å…ˆç®€è¦æ¦‚è¿°äº†åŸºç¡€LLMå’Œç³»ç»Ÿ2æŠ€æœ¯çš„æ—©æœŸå‘å±•çš„è¿›å±•ï¼Œæ¢è®¨äº†å®ƒä»¬çš„ç»“åˆå¦‚ä½•ä¸ºæ¨ç†LLMé“ºå¹³é“è·¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¨è®ºå¦‚ä½•æ„å»ºæ¨ç†LLMï¼Œåˆ†æå®ƒä»¬çš„ç‰¹æ€§ã€æ”¯æŒé«˜çº§æ¨ç†çš„æ ¸å¿ƒæ–¹æ³•ä»¥åŠå„ç§æ¨ç†LLMçš„æ¼”å˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¦‚è¿°äº†æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ·±å…¥æ¯”è¾ƒäº†ä»£è¡¨æ€§æ¨ç†LLMçš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¨è¿›æ¨ç†LLMçš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¹¶ç»´æŠ¤ä¸€ä¸ªå®æ—¶GitHubä»“åº“æ¥è·Ÿè¸ªæœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡ç»¼è¿°èƒ½æˆä¸ºè¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„å®è´µèµ„æºï¼Œä»¥æ¿€å‘åˆ›æ–°å’Œæ¨åŠ¨è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17419v3">PDF</a> Slow-thinking, Large Language Models, Human-like Reasoning, Decision   Making in AI, AGI</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å®ç°äººç±»æ™ºèƒ½æ°´å¹³çš„ä¸¤ä¸ªå…³é”®ç³»ç»Ÿâ€”â€”å¿«é€Ÿç›´è§‰ç³»ç»Ÿä¸€å’Œæ…¢é€Ÿæ·±æ€ç†Ÿè™‘ç³»ç»ŸäºŒä¹‹é—´çš„è¿‡æ¸¡ã€‚æ–‡ç« æŒ‡å‡ºï¼ŒåŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿å¿«é€Ÿå†³ç­–ä½†ç¼ºä¹å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä»åœç•™åœ¨ç³»ç»Ÿä¸€çº§çš„ç›´è§‰å†³ç­–ä¸Šã€‚è€Œæ–°è¿‘å‘å±•çš„æ¨ç†å‹LLMï¼Œå¦‚OpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1ï¼Œå¼€å§‹æ¨¡ä»¿ç³»ç»ŸäºŒçš„æ·±æ€ç†Ÿè™‘æ¨ç†æ–¹å¼ï¼Œå±•ç¤ºå‡ºä¸äººç±»ç±»ä¼¼çš„è®¤çŸ¥èƒ½åŠ›ã€‚æœ¬æ–‡ä¸»è¦å›é¡¾äº†åŸºç¡€LLMå’Œç³»ç»ŸäºŒæŠ€æœ¯çš„æ—©æœŸå‘å±•ï¼Œæ¢è®¨äº†å¦‚ä½•æ„å»ºæ¨ç†å‹LLMã€åˆ†æä»–ä»¬çš„ç‰¹æ€§å’Œæ ¸å¿ƒæ–¹æ³•ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒæ¨ç†å‹LLMçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜å±•æœ›äº†æœªæ¥æ¨åŠ¨æ¨ç†å‹LLMå‘å±•çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®ç°äººç±»æ™ºèƒ½æ°´å¹³çš„è¿‡æ¸¡éœ€è¦ä»ç›´è§‰å†³ç­–ç³»ç»Ÿä¸€ï¼ˆå¿«é€Ÿå¯å‘å¼å†³ç­–ï¼‰å‘æ·±æ€ç†Ÿè™‘å†³ç­–ç³»ç»ŸäºŒï¼ˆåŸºäºé€»è¾‘æ¨ç†è¿›è¡Œæ›´ç²¾ç¡®çš„åˆ¤æ–­å’Œå‡å°‘åè§ï¼‰çš„è½¬å˜ã€‚</li>
<li>åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶åœ¨å¿«é€Ÿå†³ç­–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æ¨ç†æ–¹é¢ç¼ºä¹æ·±åº¦ï¼Œå°šæœªå®Œå…¨èå…¥ç³»ç»ŸäºŒçš„é€æ­¥åˆ†æç‰¹æ€§ã€‚</li>
<li>æ–°ä¸€ä»£æ¨ç†å‹LLMå¦‚OpenAIçš„o1&#x2F;o3å’ŒDeepSeekçš„R1å·²ç»å±•ç°å‡ºä¸“å®¶çº§åˆ«çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œç¼–ç ç­‰é¢†åŸŸï¼Œä»–ä»¬æ¨¡æ‹Ÿäº†ç³»ç»ŸäºŒçš„æ·±æ€ç†Ÿè™‘æ¨ç†è¿‡ç¨‹å¹¶è¡¨ç°å‡ºç±»ä¼¼äººç±»çš„è®¤çŸ¥èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡æ¦‚è¿°äº†åŸºç¡€LLMå’Œç³»ç»ŸäºŒæŠ€æœ¯çš„æ—©æœŸå‘å±•ä»¥åŠå¦‚ä½•å°†ä¸¤è€…ç»“åˆä»¥åˆ›å»ºæ¨ç†å‹LLMã€‚</li>
<li>è®ºæ–‡è®¨è®ºäº†å¦‚ä½•æ„å»ºæ¨ç†å‹LLMï¼ŒåŒ…æ‹¬å…¶ç‰¹æ€§ã€æ ¸å¿ƒæ–¹æ³•å’Œä¸åŒæ¨ç†å‹LLMçš„å‘å±•æ¼”å˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17419">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6515e666ae29fe7db2b5cb4dd084660.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5afb7470b568b910712ac368d85e72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-272ed77ea83dc422eb850ac5c80a9a6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8220c76d25d552cb6f25d4eca1848826.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning"><a href="#EPO-Explicit-Policy-Optimization-for-Strategic-Reasoning-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning"></a>EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang</strong></p>
<p>Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPOâ€™s ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æ˜ç¡®è§£å†³æ–¹æ¡ˆçš„æ˜ç¡®å®šä¹‰çš„é—®é¢˜ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨æ•°å­¦å’Œç¼–ç æ–¹é¢ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å¤æ‚çš„ç°å®ä¸–ç•Œåœºæ™¯ï¼ˆå¦‚å•†åŠ¡è°ˆåˆ¤ï¼‰æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›åœºæ™¯éœ€è¦ç­–ç•¥æ¨ç†èƒ½åŠ›ï¼Œå³åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªå¹¶åœ¨ä¸ç¡®å®šæ€§ä¸­å®ç°é•¿æœŸç›®æ ‡çš„èƒ½åŠ›ã€‚ç°æœ‰çš„ç­–ç•¥æ¨ç†æ–¹æ³•é¢ä¸´é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œå°†ç­–ç•¥è½¬ç§»åˆ°æ–°æƒ…å¢ƒçš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºç­–ç•¥æ¨ç†çš„æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰ï¼Œå…¶ç‰¹ç‚¹æ˜¯LLMèƒ½åœ¨å¼€æ”¾å¼è¡ŒåŠ¨ç©ºé—´ä¸­æä¾›ç­–ç•¥ï¼Œå¹¶ä¸”å¯ä»¥æ’å…¥åˆ°ä»»æ„çš„LLMä»£ç†ä¸­ä»¥æ¿€åŠ±ç›®æ ‡å¯¼å‘çš„è¡Œä¸ºã€‚ä¸ºäº†æé«˜é€‚åº”æ€§å’Œç­–ç•¥å¯è½¬ç§»æ€§ï¼Œæˆ‘ä»¬é€šè¿‡å¤šå›åˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒç­–ç•¥æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘å¯¹æŠ—ï¼Œæ— éœ€é¢„å…ˆçš„ç›‘ç£å’Œå¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºåˆæ­¥æ­¥éª¤ã€‚åœ¨ç¤¾ä¼šå’Œç‰©ç†é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒEPOèƒ½å¤Ÿé€šè¿‡å¢å¼ºçš„ç­–ç•¥æ¨ç†å®ç°é•¿æœŸç›®æ ‡å¯¹é½çš„èƒ½åŠ›ï¼Œåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†EPOä¸­æµ®ç°çš„å„ç§åä½œæ¨ç†æœºåˆ¶åŠå…¶åœ¨ç”Ÿæˆæ–°ç­–ç•¥æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ç­–ç•¥æ¨ç†æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPOè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12486v5">PDF</a> 22 pages, 4 figures</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰æ˜ç¡®è§£å†³æ–¹æ¡ˆçš„æ˜ç¡®é—®é¢˜ä¸­å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æ•°å­¦å’Œç¼–ç¨‹ã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†éœ€è¦ç­–ç•¥æ€§æ¨ç†çš„å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯ï¼ˆå¦‚å•†åŠ¡è°ˆåˆ¤ï¼‰æ—¶ï¼Œå®ƒä»¬ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç­–ç•¥æ€§æ¨ç†è¦æ±‚èƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­å¯¼èˆªå¹¶é•¿æœŸç›®æ ‡å¯¹é½ã€‚ä¸ºè§£å†³ç°æœ‰ç­–ç•¥æ€§æ¨ç†æ–¹æ³•çš„é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘æ¸¸æˆï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä½œä¸ºåˆæ­¥æ­¥éª¤ã€‚å®éªŒè¡¨æ˜ï¼ŒEPOåœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå…·æœ‰é•¿æœŸç›®æ ‡å¯¹é½çš„èƒ½åŠ›ã€‚å…¶å‘ç°äº†å¤šç§åä½œæ¨ç†æœºåˆ¶å’Œç”Ÿæˆæ–°ç­–ç•¥çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­ç­–ç•¥æ¨ç†åº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨æ˜ç¡®çš„é—®é¢˜è§£å†³ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯å¦‚å•†åŠ¡è°ˆåˆ¤ä¸­çš„ç­–ç•¥æ€§æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„ç­–ç•¥æ¨ç†æ–¹æ³•å­˜åœ¨é€‚åº”æ€§ã€å¯æ‰©å±•æ€§å’Œç­–ç•¥è½¬ç§»çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ˜¾å¼ç­–ç•¥ä¼˜åŒ–ï¼ˆEPOï¼‰æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒæˆ˜ç•¥æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±å’Œè¿­ä»£è‡ªæˆ‘æ¸¸æˆï¼Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚</li>
<li>EPOåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾ä¼šå¯¹è¯å’Œç½‘é¡µå¯¼èˆªä»»åŠ¡ä¸Šã€‚</li>
<li>å®éªŒå‘ç°EPOå…·æœ‰é•¿æœŸç›®æ ‡å¯¹é½çš„èƒ½åŠ›å’Œå¤šç§åä½œæ¨ç†æœºåˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12486">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f985cc3c0591e22c13a8103722b76265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b362a1bd20af45a1de73d92c66aa409.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b648eac68cb9a111a9bbc9c7c8cacbb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c9977c767b3fceb5b7122b8173e95db9.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-591289440c0df5250c56e70c3dac38bd.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-29  TRACE Back from the Future A Probabilistic Reasoning Approach to   Controllable Language Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-27/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d6bcdc2ca24a01f86b13faf0d3af82d0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-27  ReaL Efficient RLHF Training of Large Language Models with Parameter   Reallocation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-27
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
