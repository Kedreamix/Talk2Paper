<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-29  TRACE Back from the Future A Probabilistic Reasoning Approach to   Controllable Language Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-591289440c0df5250c56e70c3dac38bd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    71 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-29-æ›´æ–°"><a href="#2025-04-29-æ›´æ–°" class="headerlink" title="2025-04-29 æ›´æ–°"></a>2025-04-29 æ›´æ–°</h1><h2 id="TRACE-Back-from-the-Future-A-Probabilistic-Reasoning-Approach-to-Controllable-Language-Generation"><a href="#TRACE-Back-from-the-Future-A-Probabilistic-Reasoning-Approach-to-Controllable-Language-Generation" class="headerlink" title="TRACE Back from the Future: A Probabilistic Reasoning Approach to   Controllable Language Generation"></a>TRACE Back from the Future: A Probabilistic Reasoning Approach to   Controllable Language Generation</h2><p><strong>Authors:Gwen Yidou Weng, Benjie Wang, Guy Van den Broeck</strong></p>
<p>As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMMâ€™s predicted futures. This EAP is then used to reweigh the LMâ€™s next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„å‘å±•ï¼Œè¶Šæ¥è¶Šéœ€è¦æ§åˆ¶å®ƒä»¬çš„è¾“å‡ºä»¥ç¬¦åˆäººç±»ä»·å€¼è§‚ï¼ˆä¾‹å¦‚ï¼Œå»æ¯’ï¼‰æˆ–æœŸæœ›çš„å±æ€§ï¼ˆä¾‹å¦‚ï¼Œä¸ªæ€§åŒ–ã€ä¸»é¢˜ï¼‰ã€‚ç„¶è€Œï¼Œè‡ªå›å½’æ¨¡å‹ä¸“æ³¨äºä¸‹ä¸€ä¸ªæ ‡è®°çš„é¢„æµ‹ï¼Œå¹¶åŠªåŠ›å¤„ç†éœ€è¦å‰ç»æ€§çš„å…¨å±€å±æ€§ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆè¦ä¹ˆé’ˆå¯¹æ¯ä¸ªæ–°å±æ€§è¿›è¡Œè°ƒä¼˜æˆ–åæœŸè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™æ—¢æ˜‚è´µåˆä¸çµæ´»ï¼Œè¦ä¹ˆé€šè¿‡é‡‡æ ·æˆ–è®­ç»ƒæ¥è¿‘ä¼¼æœªæ¥åºåˆ—çš„é¢„æœŸå±æ€§æ¦‚ç‡ï¼ˆEAPï¼‰ï¼Œè¿™å¯¹äºç¨€æœ‰å±æ€§è€Œè¨€æ—¢ç¼“æ…¢åˆä¸å¯é ã€‚æˆ‘ä»¬ä»‹ç»äº†TRACEï¼ˆç”¨äºå¯é€‚åº”å¯æ§ç”Ÿæˆçš„å¯è¡Œæ¦‚ç‡æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆè®¡ç®—EAPå¹¶é€šè¿‡å¯è¡Œçš„æ¦‚ç‡æ¨ç†å’Œè½»é‡çº§æ§åˆ¶æ¥é€‚åº”æ–°å±æ€§ã€‚TRACEé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹æç‚¼å‡ºéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰ï¼Œå¹¶å°†å…¶ä¸å°å‹åˆ†ç±»å™¨é…å¯¹ä»¥ä¼°ç®—å±æ€§æ¦‚ç‡ï¼Œä»è€Œèƒ½å¤Ÿåœ¨HMMé¢„æµ‹çš„æœªæ¥ä¸Šè¿›è¡Œç²¾ç¡®EAPè®¡ç®—ã€‚ç„¶åï¼Œä½¿ç”¨è¿™ä¸ªEAPæ¥é‡æ–°è¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä¸ªæ ‡è®°æ¦‚ç‡ï¼Œä»¥ä¾¿ç”Ÿæˆå…¨å±€åˆè§„çš„å»¶ç»­æ–‡æœ¬ã€‚å®è¯è¡¨æ˜ï¼ŒTRACEåœ¨ä»…å¢åŠ 10%è§£ç å¼€é”€çš„æƒ…å†µä¸‹å®ç°äº†æœ€å…ˆè¿›çš„å»æ¯’æ•ˆæœï¼Œå¯åœ¨å‡ ç§’å†…é€‚åº”76ä¸ªä½èµ„æºä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ— ç¼æ‰©å±•åˆ°ç»„åˆå±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18535v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„å‘å±•æ¨åŠ¨äº†å¯¹å…¶è¾“å‡ºæ§åˆ¶çš„éœ€æ±‚ï¼Œä»¥ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›å±æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨å¤„ç†å…¨å±€å±æ€§æ—¶å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦å‰è§†åŠŸèƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTRACEçš„æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ˜“å¤„ç†çš„æ¦‚ç‡æ¨ç†å’Œè½»é‡çº§æ§åˆ¶æ¥é«˜æ•ˆè®¡ç®—é¢„æœŸå±æ€§æ¦‚ç‡ï¼ˆEAPï¼‰ï¼Œå¹¶é€‚åº”æ–°å±æ€§ã€‚TRACEé€šè¿‡ä»LMä¸­æç‚¼éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å¹¶å°†å…¶ä¸å°å‹åˆ†ç±»å™¨é…å¯¹æ¥ä¼°è®¡å±æ€§æ¦‚ç‡ï¼Œä»è€Œåœ¨HMMé¢„æµ‹çš„æœªæœªæ¥ä¸Šå®ç°ç²¾ç¡®çš„EAPè®¡ç®—ã€‚ç„¶åï¼Œä½¿ç”¨æ­¤EAPé‡æ–°æƒè¡¡LMçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¦‚ç‡ï¼Œä»¥ç”Ÿæˆå…¨å±€åˆè§„çš„å»¶ç»­å†…å®¹ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒTRACEåœ¨è§£æ¯’ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ•ˆæœï¼Œåªéœ€10%çš„è§£ç å¼€é”€ï¼Œè¿˜èƒ½åœ¨å‡ ç§’é’Ÿå†…é€‚åº”76ä¸ªä½èµ„æºä¸ªæ€§åŒ–LLMï¼Œå¹¶è½»æ¾æ‰©å±•åˆ°ç»„åˆå±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰éœ€è¦æ§åˆ¶å…¶è¾“å‡ºä»¥ç¬¦åˆäººç±»ä»·å€¼è§‚å’ŒæœŸæœ›å±æ€§ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨å¤„ç†éœ€è¦å‰è§†åŠŸèƒ½çš„å…¨çƒå±æ€§æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>TRACEæ¡†æ¶é€šè¿‡éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHMMï¼‰å’Œåˆ†ç±»å™¨ä¼°è®¡å±æ€§æ¦‚ç‡ï¼Œå®ç°ç²¾ç¡®çš„é¢„æœŸå±æ€§æ¦‚ç‡ï¼ˆEAPï¼‰è®¡ç®—ã€‚</li>
<li>TRACEæ¡†æ¶èƒ½é«˜æ•ˆé€‚åº”æ–°å±æ€§ï¼Œå¹¶å…·æœ‰ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>TRACEæ¡†æ¶åªéœ€å°‘é‡è§£ç å¼€é”€å³å¯è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚</li>
<li>TRACEèƒ½å¿«é€Ÿé€‚åº”å¤šç§ä½èµ„æºä¸ªæ€§åŒ–LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e00ec60b4484319ca8bd6df7c5ea3812.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d59b580197ea6c88a18534adefdc458.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Eval3D-Interpretable-and-Fine-grained-Evaluation-for-3D-Generation"><a href="#Eval3D-Interpretable-and-Fine-grained-Evaluation-for-3D-Generation" class="headerlink" title="Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation"></a>Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation</h2><p><strong>Authors:Shivam Duggal, Yushi Hu, Oscar Michel, Aniruddha Kembhavi, William T. Freeman, Noah A. Smith, Ranjay Krishna, Antonio Torralba, Ali Farhadi, Wei-Chiu Ma</strong></p>
<p>Despite the unprecedented progress in the field of 3D generation, current systems still often fail to produce high-quality 3D assets that are visually appealing and geometrically and semantically consistent across multiple viewpoints. To effectively assess the quality of the generated 3D data, there is a need for a reliable 3D evaluation tool. Unfortunately, existing 3D evaluation metrics often overlook the geometric quality of generated assets or merely rely on black-box multimodal large language models for coarse assessment. In this paper, we introduce Eval3D, a fine-grained, interpretable evaluation tool that can faithfully evaluate the quality of generated 3D assets based on various distinct yet complementary criteria. Our key observation is that many desired properties of 3D generation, such as semantic and geometric consistency, can be effectively captured by measuring the consistency among various foundation models and tools. We thus leverage a diverse set of models and tools as probes to evaluate the inconsistency of generated 3D assets across different aspects. Compared to prior work, Eval3D provides pixel-wise measurement, enables accurate 3D spatial feedback, and aligns more closely with human judgments. We comprehensively evaluate existing 3D generation models using Eval3D and highlight the limitations and challenges of current models. </p>
<blockquote>
<p>å°½ç®¡ä¸‰ç»´ç”Ÿæˆé¢†åŸŸå–å¾—äº†å‰æ‰€æœªæœ‰çš„è¿›å±•ï¼Œä½†å½“å‰çš„ç³»ç»Ÿä»ç„¶ç»å¸¸æ— æ³•ç”Ÿæˆé«˜è´¨é‡çš„ä¸‰ç»´èµ„äº§ï¼Œè¿™äº›èµ„äº§åœ¨è§†è§‰ä¸Šå¸å¼•äººï¼Œå¹¶ä¸”åœ¨å¤šä¸ªè§†è§’åœ¨å‡ ä½•å’Œè¯­ä¹‰ä¸Šå…·æœ‰ä¸€è‡´æ€§ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è¯„ä¼°ç”Ÿæˆçš„ä¸‰ç»´æ•°æ®çš„è´¨é‡ï¼Œéœ€è¦ä¸€ç§å¯é çš„çš„ä¸‰ç»´è¯„ä¼°å·¥å…·ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç°æœ‰çš„ä¸‰ç»´è¯„ä¼°æŒ‡æ ‡å¾€å¾€å¿½è§†äº†ç”Ÿæˆèµ„äº§çš„å‡ ä½•è´¨é‡ï¼Œæˆ–è€…ä»…ä»…ä¾èµ–äºé»‘ç®±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç²—ç•¥è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Eval3Dï¼Œè¿™æ˜¯ä¸€ç§ç²¾ç»†ä¸”å¯è§£é‡Šæ€§çš„è¯„ä¼°å·¥å…·ï¼Œå¯ä»¥åŸºäºå„ç§ç‹¬ç‰¹è€Œäº’è¡¥çš„æ ‡å‡†å¿ å®è¯„ä¼°ç”Ÿæˆçš„ä¸‰ç»´èµ„äº§çš„è´¨é‡ã€‚æˆ‘ä»¬çš„å…³é”®è§‚å¯Ÿæ˜¯ï¼Œé€šè¿‡æµ‹é‡å„ç§åŸºç¡€æ¨¡å‹å’Œå·¥å…·ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•è·ä¸‰ç»´ç”Ÿæˆçš„è®¸å¤šæ‰€éœ€å±æ€§ï¼Œå¦‚è¯­ä¹‰å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ç»„å¤šæ ·åŒ–çš„æ¨¡å‹å’Œå·¥å…·ä½œä¸ºæ¢é’ˆæ¥è¯„ä¼°ç”Ÿæˆçš„ä¸‰ç»´èµ„äº§åœ¨ä¸åŒæ–¹é¢çš„ä¸ä¸€è‡´æ€§ã€‚ä¸ä¹‹å‰çš„å·¥ä½œç›¸æ¯”ï¼ŒEval3Dæä¾›äº†åƒç´ çº§çš„æµ‹é‡ï¼Œå®ç°äº†ç²¾ç¡®çš„3Dç©ºé—´åé¦ˆï¼Œå¹¶æ›´ç´§å¯†åœ°ä¸äººç±»åˆ¤æ–­ä¿æŒä¸€è‡´ã€‚æˆ‘ä»¬ä½¿ç”¨Eval3Då…¨é¢è¯„ä¼°ç°æœ‰çš„ä¸‰ç»´ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å¼ºè°ƒå½“å‰æ¨¡å‹çš„å±€é™æ€§å’ŒæŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18509v1">PDF</a> CVPR 2025. Project page and codes: <a target="_blank" rel="noopener" href="https://eval3d.github.io/">https://eval3d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å½“å‰3Dç”ŸæˆæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³ç”Ÿæˆçš„3Dèµ„äº§åœ¨è§†è§‰ã€å‡ ä½•å’Œè¯­ä¹‰ä¸Šçš„ä¸€è‡´æ€§ä¸è¶³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºEval3Dçš„ç²¾ç»†ã€å¯è§£é‡Šçš„è¯„ä¼°å·¥å…·ï¼Œè¯¥å·¥å…·å¯ä»¥é€šè¿‡æµ‹é‡ä¸åŒåŸºç¡€æ¨¡å‹å’Œå·¥å…·ä¹‹é—´çš„ä¸€è‡´æ€§æ¥å‡†ç¡®è¯„ä¼°ç”Ÿæˆçš„3Dèµ„äº§çš„è´¨é‡ã€‚ç›¸è¾ƒäºç°æœ‰è¯„ä¼°æ–¹æ³•ï¼ŒEval3Dæä¾›äº†åƒç´ çº§çš„æµ‹é‡å’Œå‡†ç¡®çš„3Dç©ºé—´åé¦ˆï¼Œæ›´è´´è¿‘äººç±»åˆ¤æ–­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰3Dç”ŸæˆæŠ€æœ¯åœ¨ç”Ÿæˆé«˜è´¨é‡ã€è§†è§‰ã€å‡ ä½•å’Œè¯­ä¹‰ä¸€è‡´çš„3Dèµ„äº§æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç”Ÿæˆçš„3Dèµ„äº§çš„è´¨é‡è¯„ä¼°éœ€è¦ä¸€ç§å¯é çš„3Dè¯„ä¼°å·¥å…·ã€‚</li>
<li>ç°æœ‰3Dè¯„ä¼°æŒ‡æ ‡å¸¸å¸¸å¿½è§†èµ„äº§çš„å‡ ä½•è´¨é‡ï¼Œæˆ–ä»…ä¾èµ–é»‘ç›’å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œç²—ç•¥è¯„ä¼°ã€‚</li>
<li>Eval3Dæ˜¯ä¸€ç§ç²¾ç»†ã€å¯è§£é‡Šçš„è¯„ä¼°å·¥å…·ï¼Œèƒ½å¤ŸåŸºäºå¤šç§ä¸åŒä½†äº’è¡¥çš„æ ‡å‡†æ¥è¯„ä¼°ç”Ÿæˆçš„3Dèµ„äº§è´¨é‡ã€‚</li>
<li>Eval3Dé€šè¿‡æµ‹é‡ä¸åŒåŸºç¡€æ¨¡å‹å’Œå·¥å…·ä¹‹é—´çš„ä¸€è‡´æ€§æ¥æ•æ‰3Dç”Ÿæˆçš„å¤šä¸ªæ‰€éœ€å±æ€§ï¼Œå¦‚è¯­ä¹‰å’Œå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>Eval3Dæä¾›åƒç´ çº§çš„æµ‹é‡å’Œå‡†ç¡®çš„3Dç©ºé—´åé¦ˆï¼Œæ›´è´´è¿‘äººç±»åˆ¤æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5cc60c68705285723b32f7aa8fd9c30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-571fbb25b847f19bae358c1c3b41cd41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0248eaa18e69c68b370c9eb05c01a64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e85fa4ef9f5aec17803769080815eb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34d4194bb704aef8f3419aeca92c6727.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Reason-Like-a-Radiologist-Chain-of-Thought-and-Reinforcement-Learning-for-Verifiable-Report-Generation"><a href="#Reason-Like-a-Radiologist-Chain-of-Thought-and-Reinforcement-Learning-for-Verifiable-Report-Generation" class="headerlink" title="Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning   for Verifiable Report Generation"></a>Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning   for Verifiable Report Generation</h2><p><strong>Authors:Peiyuan Jing, Kinhei Lee, Zhenxuan Zhang, Huichi Zhou, Zhengqing Yuan, Zhifan Gao, Lei Zhu, Giorgos Papanastasiou, Yingying Fang, Guang Yang</strong></p>
<p>Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologistsâ€™ workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RLâ€™s robustness in generating high-quality radiology reports. </p>
<blockquote>
<p>æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆå¯¹äºæ•ˆç‡è‡³å…³é‡è¦ï¼Œä½†å½“å‰æ¨¡å‹ç¼ºä¹ä¸“å®¶çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ï¼Œæ— æ³•é€šè¿‡å…³è”è§†è§‰æ£€æŸ¥ç»“æœä¸ç²¾ç¡®è§£å‰–éƒ¨ä½æ¥è§£é‡ŠæŠ¥å‘Šçš„å¯é æ€§ï¼Œé˜»ç¢äº†ä¸´åºŠä¿¡ä»»åº¦å’Œè§£é‡Šæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†BoxMed-RLï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆç©ºé—´å¯éªŒè¯å’Œå¯è§£é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šçš„åˆ›æ–°ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ã€‚BoxMed-RLå»ºç«‹åœ¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šï¼Œé€šè¿‡ä¸¤ä¸ªé˜¶æ®µå®ç°äº†æŠ¥å‘Šçš„ç”Ÿæˆé©å‘½ï¼šï¼ˆ1ï¼‰åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ ç²¾ç‚¼æ¨¡å‹ï¼Œä½¿ç”¨æ€ç»´é“¾å¼ç›‘ç£æ¥å†…åŒ–ç±»ä¼¼æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œæµç¨‹ï¼Œéšåè¿›è¡Œç©ºé—´å¯éªŒè¯å¼ºåŒ–è®­ç»ƒï¼Œåº”ç”¨å¼ºåŒ–å­¦ä¹ å°†åŒ»å­¦æ£€æŸ¥ç»“æœä¸è¾¹ç•Œæ¡†å¯¹é½ã€‚ï¼ˆ2ï¼‰åœ¨ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µï¼Œæˆ‘ä»¬å†»ç»“é¢„è®­ç»ƒæƒé‡å¹¶è®­ç»ƒä¸‹æ¸¸é€‚é…å™¨ä»¥ç¡®ä¿æŠ¥å‘Šçš„æµç•…æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚è¯¥æ¡†æ¶ç²¾ç¡®åœ°æ¨¡ä»¿äº†æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œæµç¨‹ï¼Œä¿ƒä½¿æ¨¡å‹å°†é«˜çº§åŒ»å­¦æ¦‚å¿µä¸æ˜ç¡®çš„è§£å‰–å­¦è¯æ®è”ç³»èµ·æ¥ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒBoxMed-RLåœ¨METEORå’ŒROUGE-LæŒ‡æ ‡ä¸Šå¹³å‡æé«˜äº†7%ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡æ ‡å¹³å‡æé«˜äº†5%ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†BoxMed-RLåœ¨ç”Ÿæˆé«˜è´¨é‡æ”¾å°„å­¦æŠ¥å‘Šæ–¹é¢çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18453v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåˆ›æ–°çš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶BoxMed-RLï¼Œç”¨äºç”Ÿæˆç©ºé—´å¯éªŒè¯å’Œå¯è§£é‡Šçš„æ”¾å°„å­¦æŠ¥å‘Šã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªé˜¶æ®µå®ç°ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¯¹é½åŒ»å­¦å‘ç°ä¸è¾¹ç•Œæ¡†ï¼›ç¬¬äºŒé˜¶æ®µæ˜¯ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µï¼Œè®­ç»ƒä¸‹æ¸¸é€‚é…å™¨ä»¥ç¡®ä¿æŠ¥å‘Šçš„æµç•…æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚BoxMed-RLç²¾ç¡®æ¨¡æ‹Ÿäº†æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹ï¼Œå°†é«˜çº§åŒ»å­¦æ¦‚å¿µä¸æ˜ç¡®çš„è§£å‰–å­¦è¯æ®è”ç³»èµ·æ¥ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒBoxMed-RLç›¸è¾ƒäºç°æœ‰æ–¹æ³•å¹³å‡æé«˜äº†7%çš„METEORå’ŒROUGE-LæŒ‡æ ‡å¾—åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BoxMed-RLæ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šçš„ç»Ÿä¸€è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æŠ¥å‘Šçš„æ•ˆç‡å’Œä¸´åºŠä¿¡ä»»åº¦ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒé˜¶æ®µå’Œä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œé€šè¿‡åŒ»å­¦æ¦‚å¿µå­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ å¯¹é½åŒ»å­¦å‘ç°ä¸è¾¹ç•Œæ¡†ï¼Œä»¥æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œæµç¨‹ã€‚</li>
<li>ä¸‹æ¸¸é€‚é…å™¨é˜¶æ®µç¡®ä¿æŠ¥å‘Šçš„æµç•…æ€§å’Œä¸´åºŠå¯ä¿¡åº¦ã€‚</li>
<li>BoxMed-RLå®ç°äº†å¯¹æ”¾å°„å­¦æŠ¥å‘Šçš„é«˜è´¨é‡ç”Ÿæˆï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¹³å‡æé«˜äº†METEORå’ŒROUGE-LæŒ‡æ ‡å¾—åˆ†ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ç»“åˆé«˜çº§åŒ»å­¦æ¦‚å¿µå’Œè§£å‰–å­¦è¯æ®ï¼Œæé«˜äº†æŠ¥å‘Šçš„å¯è§£é‡Šæ€§å’Œç©ºé—´éªŒè¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-819f69991ca333c6b6bbb5c552c5ae09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aa1fb71b7b66167a479d3efff4c711b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7a6eb952a6bf902ea6f1f4383f4b9c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52e6437e91f50b9bba7a9a58ca3bfbbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b210393d3fcb02840b6f680907c01a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84d271d1ca1ca74ffd3901c7fb16abbc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-Evaluating-Long-form-Question-Answering"><a href="#An-Empirical-Study-of-Evaluating-Long-form-Question-Answering" class="headerlink" title="An Empirical Study of Evaluating Long-form Question Answering"></a>An Empirical Study of Evaluating Long-form Question Answering</h2><p><strong>Authors:Ning Xian, Yixing Fan, Ruqing Zhang, Maarten de Rijke, Jiafeng Guo</strong></p>
<p>\Ac{LFQA} aims to generate lengthy answers to complex questions. This scenario presents great flexibility as well as significant challenges for evaluation. Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored. We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations? (ii) What are the limitations of existing evaluation metrics compared to human evaluations? (iii) How can the effectiveness and robustness of existing evaluation methods be improved? We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness. Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations. We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics. However, fine-grained evaluation helps mitigate this issue on some metrics. Our findings have important implications for the use of large language models for evaluating long-form question answering. All code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/bugtig6351/lfqa_evaluation">https://github.com/bugtig6351/lfqa_evaluation</a>. </p>
<blockquote>
<p>\Ac{LFQA}æ—¨åœ¨ç”Ÿæˆå¯¹å¤æ‚é—®é¢˜çš„é•¿ç¯‡ç­”æ¡ˆã€‚è¿™ä¸€åœºæ™¯ä¸ºè¯„ä¼°æä¾›äº†å¾ˆå¤§çš„çµæ´»æ€§ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚å¤§å¤šæ•°è¯„ä¼°ä¾èµ–äºä¾èµ–äºå­—ç¬¦ä¸²æˆ–nå…ƒç»„åŒ¹é…çš„ç¡®å®šæ€§æŒ‡æ ‡ï¼Œè€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æœŸç­”æ¡ˆè¯„ä¼°çš„å¯é æ€§ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬é€šè¿‡æ·±å…¥ç ”ç©¶é•¿ç¯‡ç­”æ¡ˆè¯„ä¼°æ¥è§£å†³è¿™ä¸€å·®è·ï¼Œæå‡ºä»¥ä¸‹ç ”ç©¶é—®é¢˜ï¼šï¼ˆiï¼‰ç°æœ‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯ä»¥ä½œä¸ºäººç±»è¯„ä¼°çš„æ›¿ä»£å“ï¼Ÿï¼ˆiiï¼‰ä¸äººå·¥è¯„ä¼°ç›¸æ¯”ï¼Œç°æœ‰è¯„ä¼°æŒ‡æ ‡çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆiiiï¼‰å¦‚ä½•æ”¹è¿›ç°æœ‰è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Ÿæˆ‘ä»¬æ”¶é›†äº†ç”±ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„5236ä¸ªäº‹å®æ€§å’Œéäº‹å®æ€§çš„é•¿ç¯‡ç­”æ¡ˆï¼Œå¹¶å¯¹å…¶ä¸­çš„2079ä¸ªè¿›è¡Œäº†äººå·¥è¯„ä¼°ï¼Œé‡ç‚¹è¯„ä¼°å…¶æ­£ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚éšåï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°è¿™äº›ç­”æ¡ˆæ¥è°ƒæŸ¥è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„æ€§èƒ½ï¼Œå¹¶åˆ†æè¿™äº›æŒ‡æ ‡ä¸äººç±»è¯„ä¼°ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œç­”æ¡ˆçš„é£æ ¼ã€é•¿åº¦å’Œé—®é¢˜çš„ç±»åˆ«éƒ½å¯èƒ½å¯¹è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡äº§ç”Ÿåè§ã€‚ç„¶è€Œï¼Œç²¾ç»†çš„è¯„ä¼°æœ‰åŠ©äºç¼“è§£æŸäº›æŒ‡æ ‡ä¸Šçš„é—®é¢˜ã€‚æˆ‘ä»¬çš„å‘ç°å¯¹äºä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé•¿æœŸé—®ç­”è¯„ä¼°å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bugtig6351/lfqa_evaluation%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bugtig6351/lfqa_evaluationæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é•¿ç¯‡é—®ç­”çš„è¯„ä¼°é—®é¢˜ï¼Œä¸»è¦å…³æ³¨ç°æœ‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡åœ¨å¤šå¤§ç¨‹åº¦ä¸Šèƒ½å¤Ÿæ›¿ä»£äººå·¥è¯„ä¼°ï¼Œä»¥åŠå®ƒä»¬ç›¸è¾ƒäºäººå·¥è¯„ä¼°çš„å±€é™æ€§ã€‚ç ”ç©¶é€šè¿‡æ”¶é›†5236ä¸ªç”±ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„äº‹å®å’Œéäº‹å®é•¿ç¯‡ç­”æ¡ˆï¼Œå¹¶è¿›è¡Œäººå·¥è¯„ä¼°ï¼Œå†å¯¹æ¯”è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„æ•ˆèƒ½ï¼Œå‘ç°ç­”æ¡ˆçš„é£æ ¼ã€é•¿åº¦ã€é—®é¢˜ç±»åˆ«éƒ½ä¼šå½±å“è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å‡†ç¡®æ€§ã€‚ç²¾ç»†åŒ–çš„è¯„ä¼°æœ‰åŠ©äºç¼“è§£è¿™ä¸€é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>\Ac{LFQA}æ—¨åœ¨ç”Ÿæˆå¯¹å¤æ‚é—®é¢˜çš„é•¿ç¯‡ç­”æ¡ˆï¼Œè¿™ä¸ºè¯„ä¼°å¸¦æ¥äº†æŒ‘æˆ˜å’Œçµæ´»æ€§ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦ä¾èµ–ç¡®å®šæ€§æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡ä¾èµ–äºå­—ç¬¦ä¸²æˆ–n-gramåŒ¹é…ï¼Œè€ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿ç¯‡ç­”æ¡ˆçš„è¯„ä¼°å¯é æ€§ä»ç„¶ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜æ¥æ·±å…¥ç ”ç©¶é•¿ç¯‡ç­”æ¡ˆçš„è¯„ä¼°é—®é¢˜ï¼šç°æœ‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¯ä»¥æ›¿ä»£äººå·¥è¯„ä¼°ï¼Ÿè¿™äº›ç°æœ‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•æ”¹è¿›ç°æœ‰è¯„ä¼°æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Ÿ</li>
<li>é€šè¿‡æ”¶é›†å’Œåˆ†æ5236ä¸ªé•¿ç¯‡ç­”æ¡ˆï¼Œç ”ç©¶è¿›è¡Œäº†äººå·¥è¯„ä¼°ï¼Œé‡ç‚¹å…³æ³¨ç­”æ¡ˆçš„æ­£ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚</li>
<li>è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„æ•ˆèƒ½è¿›è¡Œäº†ç ”ç©¶ï¼Œå‘ç°ç­”æ¡ˆçš„é£æ ¼ã€é•¿åº¦å’Œé—®é¢˜ç±»åˆ«å¯èƒ½å½±å“è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œç²¾ç»†åŒ–çš„è¯„ä¼°æœ‰åŠ©äºç¼“è§£æŸäº›è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2edaa8306a6960feeec2202382147b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-299571cb0e3c6ac907f1b2aa3d100669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-952db4c9253a55ae43163f99ee83eb45.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e4629a6b83248eb68804680ed8c1c4c9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f713b8f0c72b4a36df7a1f945d08135.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b25827ce8ee0fdc48254c198ee32248.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Expressing-stigma-and-inappropriate-responses-prevents-LLMs-from-safely-replacing-mental-health-providers"><a href="#Expressing-stigma-and-inappropriate-responses-prevents-LLMs-from-safely-replacing-mental-health-providers" class="headerlink" title="Expressing stigma and inappropriate responses prevents LLMs from safely   replacing mental health providers"></a>Expressing stigma and inappropriate responses prevents LLMs from safely   replacing mental health providers</h2><p><strong>Authors:Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber</strong></p>
<p>Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to <em>replace</em> mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as <code>gpt-4o</code>. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings â€“ e.g., LLMs encourage clientsâ€™ delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”è¯¥è¢«ç”¨ä½œå¿ƒç†æ²»ç–—å¸ˆå—ï¼Ÿåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨LLMåœ¨å–ä»£ç²¾ç¥å¥åº·ä»ä¸šè€…æ–¹é¢çš„åº”ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ç§‘æŠ€åˆ›ä¸šå’Œç ”ç©¶é¢†åŸŸå¾—åˆ°æ¨å¹¿çš„ä½¿ç”¨åœºæ™¯ã€‚æˆ‘ä»¬å¯¹ä¸»è¦åŒ»ç–—æœºæ„ä½¿ç”¨çš„æ²»ç–—æŒ‡å—è¿›è¡Œäº†æ˜ å°„å›é¡¾ï¼Œä»¥è¯†åˆ«æ²»ç–—å…³ç³»çš„å…³é”®æ–¹é¢ï¼Œå¦‚æ²»ç–—å¸ˆå’Œæ‚£è€…ä¹‹é—´æ²»ç–—è”ç›Ÿçš„é‡è¦æ€§ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡å¼€å±•å‡ é¡¹å®éªŒï¼Œè¯„ä¼°äº†LLMåœ¨å¤åˆ¶å’Œéµå®ˆè¿™äº›æ²»ç–—å…³ç³»æ–¹é¢çš„èƒ½åŠ›ï¼Œå®éªŒå¯¹è±¡åŒ…æ‹¬å½“å‰çš„LLMå¦‚GPT-4oç­‰ã€‚ä¸åŒ»å­¦ç•Œçš„æœ€ä½³å®è·µç›¸åï¼ŒLLMè¡¨ç°å‡ºå¯¹ç²¾ç¥å¥åº·æ‚£è€…çš„åè§å’Œä¸æ°å½“çš„å›åº”ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶ç–—æ³•ç¯å¢ƒä¸­å¯¹æŸäº›å¸¸è§ï¼ˆä¸”å…³é”®ï¼‰çŠ¶å†µçš„å›åº”ä¸å½“â€”â€”ä¾‹å¦‚ï¼ŒLLMé¼“åŠ±å®¢æˆ·çš„å¦„æƒ³æ€ç»´ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºå…¶å¥‰æ‰¿æ€§æ ¼é€ æˆçš„ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨æ›´å¤§ã€æ›´æ–°çš„LLMä¸­ï¼Œè¡¨æ˜å½“å‰çš„å®‰å…¨å®è·µå¯èƒ½æ— æ³•å¼¥è¡¥è¿™äº›å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†é‡‡ç”¨LLMä½œä¸ºæ²»ç–—å¸ˆçš„åŸºæœ¬å’Œå®è·µéšœç¢ï¼Œå¦‚æ²»ç–—è”ç›Ÿéœ€è¦äººç±»ç‰¹å¾ï¼ˆä¾‹å¦‚èº«ä»½å’Œåˆ©å®³å…³ç³»ï¼‰ã€‚åŸºäºè¿™äº›åŸå› ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼ŒLLMä¸åº”å–ä»£å¿ƒç†æ²»ç–—å¸ˆï¼Œæˆ‘ä»¬è®¨è®ºäº†LLMåœ¨ä¸´åºŠæ²»ç–—ä¸­çš„æ›¿ä»£è§’è‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18412v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå…³äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ²»ç–—å¸ˆçš„ç ”ç©¶æ¢è®¨ã€‚æœ¬æ–‡è°ƒæŸ¥äº†LLMåœ¨æ›¿ä»£ç²¾ç¥å¥åº·æä¾›è€…æ–¹é¢çš„åº”ç”¨ï¼Œå¹¶å¯¹æ­¤è¿›è¡Œäº†å®éªŒç ”ç©¶ã€‚ç»“æœå‘ç°LLMåœ¨æ¨¡æ‹Ÿæ²»ç–—å…³ç³»ä¸­å­˜åœ¨é—®é¢˜ï¼Œå¦‚è¡¨è¾¾å¯¹æ‚£è€…æ¡ä»¶çš„åè§å’Œå›åº”ä¸å½“ç­‰ã€‚å› æ­¤ï¼Œæœ¬æ–‡è®¤ä¸ºLLMä¸èƒ½æ›¿ä»£æ²»ç–—å¸ˆçš„è§’è‰²ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMè¢«ç ”ç©¶ç”¨äºæ›¿ä»£ç²¾ç¥å¥åº·æä¾›è€…ï¼Œä½†åœ¨æ¨¡æ‹Ÿæ²»ç–—å…³ç³»ä¸­å­˜åœ¨é—®é¢˜ã€‚</li>
<li>é€šè¿‡å®éªŒç ”ç©¶å‘ç°LLMå¯¹ç²¾ç¥å¥åº·æ¡ä»¶çš„åè§å’Œå›åº”ä¸å½“ã€‚</li>
<li>LLMåœ¨é¼“åŠ±å®¢æˆ·å¦„æƒ³æ€è€ƒæ–¹é¢å­˜åœ¨é—®é¢˜ï¼Œå¯èƒ½æ˜¯ç”±äºå…¶å¥‰æ‰¿æ€§è´¨ã€‚</li>
<li>å³ä½¿å¯¹äºè¾ƒå¤§å’Œè¾ƒæ–°çš„LLMï¼Œä¹Ÿå­˜åœ¨å®‰å…¨é—®é¢˜ï¼Œå½“å‰çš„å®‰å…¨å®è·µå¯èƒ½æ— æ³•è§£å†³è¿™äº›å·®è·ã€‚</li>
<li>é‡‡ç”¨LLMä½œä¸ºæ²»ç–—å¸ˆå­˜åœ¨åŸºç¡€å’Œå®è·µä¸Šçš„éšœç¢ï¼Œå¦‚æ²»ç–—è”ç›Ÿéœ€è¦äººç±»ç‰¹å¾ï¼ˆä¾‹å¦‚èº«ä»½å’Œåˆ©å®³å…³ç³»ï¼‰ã€‚</li>
<li>LLMä¸åº”æ›¿ä»£æ²»ç–—å¸ˆçš„è§’è‰²ï¼Œä½†å¯æ¢è®¨å…¶åœ¨ä¸´åºŠæ²»ç–—ä¸­çš„å…¶ä»–è§’è‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2b713f20a9c00eb2bb12b9a6f6373b84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c37a58007f6a4d5364ce74f08778114.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b1230a4371230721d3f365cd1c50410.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Visual-Chain-of-Thought-Reasoning-via-Preference-Optimization"><a href="#Unsupervised-Visual-Chain-of-Thought-Reasoning-via-Preference-Optimization" class="headerlink" title="Unsupervised Visual Chain-of-Thought Reasoning via Preference   Optimization"></a>Unsupervised Visual Chain-of-Thought Reasoning via Preference   Optimization</h2><p><strong>Authors:Kesen Zhao, Beier Zhu, Qianru Sun, Hanwang Zhang</strong></p>
<p>Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perceptionâ€“identifying key regions and reasoning based on themâ€“UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in <a target="_blank" rel="noopener" href="https://github.com/kesenzhao/UV-CoT">https://github.com/kesenzhao/UV-CoT</a>. </p>
<blockquote>
<p>æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æå¤§åœ°æé«˜äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¯è§£é‡Šæ€§å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬CoTï¼Œé™åˆ¶äº†å®ƒä»¬åˆ©ç”¨è§†è§‰çº¿ç´¢çš„èƒ½åŠ›ã€‚è§†è§‰CoTä»ç„¶æœªè¢«å……åˆ†ç ”ç©¶ï¼Œç°æœ‰çš„å·¥ä½œéƒ½æ˜¯åŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™ä¾èµ–äºå¤§é‡çš„æ ‡è®°è¾¹ç•Œæ¡†æ•°æ®ï¼Œå¹¶ä¸”å¾ˆéš¾æ¨å¹¿åˆ°æœªè§çš„æƒ…å†µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ— ç›‘ç£è§†è§‰CoTï¼ˆUV-CoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡åå¥½ä¼˜åŒ–è¿›è¡Œå›¾åƒçº§CoTæ¨ç†çš„æ–°å‹æ¡†æ¶ã€‚UV-CoTå¯¹æ¨¡å‹ç”Ÿæˆçš„è¾¹ç•Œæ¡†è¿›è¡Œåå¥½æ¯”è¾ƒï¼ˆä¸€ä¸ªè¢«åå¥½ï¼Œå¦ä¸€ä¸ªè¢«ä¸å–œæ¬¢ï¼‰ï¼Œä»è€Œæ— éœ€è¾¹ç•Œæ¡†æ³¨é‡Šã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥è‡ªåŠ¨æ•°æ®ç”Ÿæˆç®¡é“æ¥è·å¾—è¿™ç§åå¥½æ•°æ®ã€‚ç»™å®šä¸€å¼ å›¾åƒï¼Œæˆ‘ä»¬çš„ç›®æ ‡MLLMï¼ˆä¾‹å¦‚LLaVA-1.5-7Bï¼‰ä½¿ç”¨æ¨¡æ¿æç¤ºç”Ÿæˆç§å­è¾¹ç•Œæ¡†ï¼Œç„¶åä½¿ç”¨æ¯ä¸ªè¾¹ç•ŒåŒºåŸŸä½œä¸ºè¾“å…¥å›ç­”é—®é¢˜ã€‚è¯„ä¼°å™¨MLLMï¼ˆä¾‹å¦‚OmniLLM-12Bï¼‰å¯¹ç­”æ¡ˆè¿›è¡Œæ’åï¼Œè¿™äº›æ’åä½œä¸ºç›‘ç£ä¿¡æ¯æ¥è®­ç»ƒç›®æ ‡MLLMä½¿ç”¨UV-CoTï¼Œé€šè¿‡æœ€å°åŒ–è´Ÿå¯¹æ•°æŸå¤±æ¥å®ç°ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»æ„ŸçŸ¥â€”â€”è¯†åˆ«å…³é”®åŒºåŸŸå¹¶åŸºäºå®ƒä»¬è¿›è¡Œæ¨ç†â€”â€”UV-CoTå¯ä»¥æ”¹å–„è§†è§‰ç†è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­ï¼Œä»…ä½¿ç”¨æ–‡æœ¬æè¿°æ˜¯ä¸å¤Ÿçš„ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–‡æœ¬å’Œè§†è§‰CoTæ–¹æ³•ç›¸æ¯”ï¼ŒUV-CoTå…·æœ‰ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæœªè§æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬æµ‹è¯•æ˜¾ç¤ºäº†UV-CoTçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kesenzhao/UV-CoT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kesenzhao/UV-CoTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18397v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºè§†è§‰é“¾å¼æ€ç»´ï¼ˆVisual CoTï¼‰çš„ç°æœ‰ç ”ç©¶å±€é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ— ç›‘ç£è§†è§‰é“¾å¼æ€ç»´ï¼ˆUV-CoTï¼‰æ¡†æ¶ï¼Œç”¨äºå›¾åƒçº§åˆ«çš„CoTæ¨ç†ã€‚UV-CoTé€šè¿‡åå¥½ä¼˜åŒ–æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„è¾¹ç•Œæ¡†ï¼Œæ— éœ€è¾¹ç•Œæ¡†æ ‡æ³¨ã€‚å®éªŒè¯æ˜ï¼ŒUV-CoTåœ¨è§†è§‰ç†è§£æ–¹é¢ï¼Œç‰¹åˆ«æ˜¯åœ¨ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å¢å¼ºäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šèƒ½åŠ›å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ–‡æœ¬CoTï¼Œå¿½ç•¥äº†è§†è§‰çº¿ç´¢çš„åˆ©ç”¨ã€‚</li>
<li>è§†è§‰é“¾å¼æ€ç»´ï¼ˆVisual CoTï¼‰ä»å¤„äºæ¢ç´¢é˜¶æ®µï¼Œç°æœ‰å·¥ä½œåŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨çš„è¾¹ç•Œæ¡†æ•°æ®ï¼Œéš¾ä»¥æ³›åŒ–åˆ°æœªè§æƒ…å†µã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æ— ç›‘ç£è§†è§‰é“¾å¼æ€ç»´ï¼ˆUV-CoTï¼‰æ¡†æ¶ï¼Œé€šè¿‡åå¥½ä¼˜åŒ–è¿›è¡Œå›¾åƒçº§åˆ«çš„CoTæ¨ç†ã€‚</li>
<li>UV-CoTé€šè¿‡æ¯”è¾ƒæ¨¡å‹ç”Ÿæˆçš„è¾¹ç•Œæ¡†è¿›è¡Œåå¥½æ¯”è¾ƒï¼Œé™ä½äº†å¯¹æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>UV-CoTåœ¨è§†è§‰ç†è§£ï¼Œç‰¹åˆ«æ˜¯ç©ºé—´æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac36512d8491865f5c62c835c4758710.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-204ac322ec9aafa810cb593bdc201494.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bd7e5dbdf961f0239cf0df2caa7711a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c54236e4bb21286c248f2614811dcf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f50a4e4870a9008a6e0ebff4aebdc3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Auto-SLURP-A-Benchmark-Dataset-for-Evaluating-Multi-Agent-Frameworks-in-Smart-Personal-Assistant"><a href="#Auto-SLURP-A-Benchmark-Dataset-for-Evaluating-Multi-Agent-Frameworks-in-Smart-Personal-Assistant" class="headerlink" title="Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in   Smart Personal Assistant"></a>Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in   Smart Personal Assistant</h2><p><strong>Authors:Lei Shen, Xiaoyu Shen</strong></p>
<p>In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset â€“ initially developed for natural language understanding tasks â€“ by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at <a target="_blank" rel="noopener" href="https://github.com/lorashen/Auto-SLURP/">https://github.com/lorashen/Auto-SLURP/</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶å‘å±•è¿…é€Ÿã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†ä»ç¼ºä¹ä¸“é—¨ç”¨äºè¯„ä¼°å…¶æ€§èƒ½çš„åŸºå‡†æ•°æ®é›†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†Auto-SLURPï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨æ™ºèƒ½ä¸ªäººåŠ©ç†èƒŒæ™¯ä¸‹çš„åŸºå‡†æ•°æ®é›†ã€‚Auto-SLURPæ‰©å±•äº†åŸå§‹çš„SLURPæ•°æ®é›†â€”â€”æœ€åˆæ˜¯ä¸ºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡è€Œå¼€å‘çš„â€”â€”é€šè¿‡é‡æ–°æ ‡è®°æ•°æ®å¹¶é›†æˆæ¨¡æ‹ŸæœåŠ¡å™¨å’Œå¤–éƒ¨æœåŠ¡æ¥å¢å¼ºæ•°æ®é›†çš„åŠŸèƒ½ã€‚è¿™ä¸€å¢å¼ºåŠŸèƒ½èƒ½å¤Ÿæä¾›ä¸€ä¸ªå…¨é¢çš„ç«¯åˆ°ç«¯è¯„ä¼°æµç¨‹ï¼Œæ¶µç›–è¯­è¨€ç†è§£ã€ä»»åŠ¡æ‰§è¡Œå’Œå“åº”ç”Ÿæˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒAuto-SLURPå¯¹å½“å‰æœ€å…ˆè¿›çš„æ¡†æ¶æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå¼ºè°ƒçœŸæ­£å¯é å’Œæ™ºèƒ½çš„å¤šæ™ºèƒ½ä¸ªäººåŠ©ç†ä»åœ¨å‘å±•ä¸­ã€‚æ•°æ®é›†å’Œç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lorashen/Auto-SLURP/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lorashen/Auto-SLURP/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18373v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMå¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨è¿‘å¹´æ¥å‘å±•è¿…çŒ›ï¼Œä½†ä»ç¼ºä¹ä¸“é—¨ç”¨äºè¯„ä¼°å…¶æ€§èƒ½çš„åŸºå‡†æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºAuto-SLURPæ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä¸ªäººåŠ©ç†é¢†åŸŸçš„LLMå¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚Auto-SLURPæ‰©å±•äº†æœ€åˆç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡çš„SLURPæ•°æ®é›†ï¼Œé€šè¿‡é‡æ–°æ ‡è®°æ•°æ®å’Œé›†æˆæ¨¡æ‹ŸæœåŠ¡å™¨å’Œå¤–éƒ¨æœåŠ¡ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å…¨é¢è¯„ä¼°æµç¨‹ï¼Œæ¶µç›–è¯­è¨€ç†è§£ã€ä»»åŠ¡æ‰§è¡Œå’Œå“åº”ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒAuto-SLURPå¯¹å½“å‰å…ˆè¿›æ¡†æ¶æå‡ºäº†é‡å¤§æŒ‘æˆ˜ï¼Œå‡¸æ˜¾å‡ºçœŸæ­£å¯é ä¸”æ™ºèƒ½çš„å¤šæ™ºèƒ½ä½“ä¸ªäººåŠ©ç†ä»åœ¨å‘å±•ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå¤šæ™ºèƒ½ä½“æ¡†æ¶å‘å±•è¿…çŒ›ï¼Œä½†ç¼ºä¹ä¸“é—¨çš„åŸºå‡†æ•°æ®é›†æ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚</li>
<li>Auto-SLURPæ•°æ®é›†æ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä¸ªäººåŠ©ç†é¢†åŸŸçš„LLMå¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚</li>
<li>Auto-SLURPæ‰©å±•äº†SLURPæ•°æ®é›†ï¼Œç”¨äºå…¨é¢çš„ç«¯åˆ°ç«¯è¯„ä¼°ï¼ŒåŒ…æ‹¬è¯­è¨€ç†è§£ã€ä»»åŠ¡æ‰§è¡Œå’Œå“åº”ç”Ÿæˆã€‚</li>
<li>Auto-SLURPæ•°æ®é›†é€šè¿‡é‡æ–°æ ‡è®°æ•°æ®å’Œé›†æˆæ¨¡æ‹ŸæœåŠ¡å™¨ä¸å¤–éƒ¨æœåŠ¡å®ç°å¢å¼ºã€‚</li>
<li>å½“å‰å…ˆè¿›æ¡†æ¶åœ¨Auto-SLURPä¸Šé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>çœŸæ­£çš„å¯é ä¸”æ™ºèƒ½çš„å¤šæ™ºèƒ½ä½“ä¸ªäººåŠ©ç†ä»åœ¨å‘å±•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18373">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b13a9c54c36976f9ca70b81e1e576dbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b0b6ed4ead1dd102fbc97b71132e60c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a7423f074dc437a3d2c90ff507f3bf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f458a17f081e8c564a1bc4466bc118b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee6c600abf05b734ff6b9679488b2c27.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Comparing-Uncertainty-Measurement-and-Mitigation-Methods-for-Large-Language-Models-A-Systematic-Review"><a href="#Comparing-Uncertainty-Measurement-and-Mitigation-Methods-for-Large-Language-Models-A-Systematic-Review" class="headerlink" title="Comparing Uncertainty Measurement and Mitigation Methods for Large   Language Models: A Systematic Review"></a>Comparing Uncertainty Measurement and Mitigation Methods for Large   Language Models: A Systematic Review</h2><p><strong>Authors:Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei</strong></p>
<p>Large Language Models (LLMs) have been transformative across many domains. However, hallucination â€“ confidently outputting incorrect information â€“ remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨è®¸å¤šé¢†åŸŸäº§ç”Ÿäº†å˜é©æ€§çš„å½±å“ã€‚ç„¶è€Œï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯çš„é—®é¢˜â€”â€”å³â€œå¹»æƒ³â€â€”â€”ä»ç„¶æ˜¯LLMé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚è¿™å¼•å‘äº†å¦‚ä½•å‡†ç¡®è¯„ä¼°å’Œé‡åŒ–LLMä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚å…³äºä¼ ç»Ÿæ¨¡å‹çš„å¹¿æ³›æ–‡çŒ®å·²ç»æ¢è®¨äº†ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æ¥æµ‹é‡ä¸ç¡®å®šæ€§ï¼Œå¹¶é‡‡ç”¨äº†æ ¡å‡†æŠ€æœ¯æ¥è§£å†³ä¸ç¡®å®šæ€§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ã€‚è™½ç„¶è¿™äº›æ–¹æ³•ä¸­çš„ä¸€äº›å·²ç»é€‚åº”äºLLMï¼Œä½†æ–‡çŒ®ç¼ºä¹å¯¹å…¶æœ‰æ•ˆæ€§çš„æ·±å…¥åˆ†æï¼Œå¹¶ä¸”æ²¡æœ‰æä¾›ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æ¥æ´å¯Ÿç°æœ‰è§£å†³æ–¹æ¡ˆä¹‹é—´çš„æ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç³»ç»Ÿåœ°è°ƒæŸ¥UQå’ŒLLMæ ¡å‡†çš„ä»£è¡¨æ€§å‰æœŸå·¥ä½œæ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªä¸¥æ ¼çš„åŸºå‡†ã€‚ä½¿ç”¨ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¯é æ€§æ•°æ®é›†ï¼Œæˆ‘ä»¬å®è¯è¯„ä¼°äº†å…­ç§ç›¸å…³æ–¹æ³•ï¼Œè¯æ˜äº†æˆ‘ä»¬å®¡æŸ¥çš„é‡è¦å‘ç°ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºæœªæ¥çš„å…³é”®æ–¹å‘æä¾›äº†å±•æœ›å¹¶æ¦‚è¿°äº†å¼€æ”¾æŒ‘æˆ˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹è°ƒæŸ¥æ˜¯é¦–æ¬¡ä¸“é—¨é’ˆå¯¹LLMçš„æ ¡å‡†æ–¹æ³•å’Œç›¸å…³æŒ‡æ ‡è¿›è¡Œçš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18346v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMsï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰åœ¨ä¼—å¤šé¢†åŸŸå¸¦æ¥äº†å˜é©ï¼Œä½†æ¨¡å‹è¾“å‡ºé”™è¯¯ä¿¡æ¯çš„é—®é¢˜â€”â€”å³â€œhallucinationâ€ç°è±¡ï¼Œä»æ˜¯ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚å¦‚ä½•å‡†ç¡®è¯„ä¼°å’Œé‡åŒ–LLMçš„ä¸ç¡®å®šæ€§æ˜¯å½“å‰é‡è¦è®®é¢˜ã€‚å°½ç®¡ä¼ ç»Ÿæ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰å·²æœ‰ä¸°å¯Œæ–‡çŒ®ï¼Œä¸”é‡‡ç”¨æ ¡å‡†æŠ€æœ¯è§£å†³ä¸ç¡®å®šæ€§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„ä¸åŒ¹é…é—®é¢˜ï¼Œä½†é’ˆå¯¹LLMçš„è¿™äº›æ–¹æ³•çš„æ•ˆæœç¼ºä¹æ·±å…¥åˆ†æå’Œç»¼åˆåŸºå‡†æ¯”è¾ƒã€‚æœ¬æ–‡å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œå¯¹LLMçš„UQå’Œæ ¡å‡†æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿè°ƒæŸ¥ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªä¸¥æ ¼çš„åŸºå‡†ã€‚é€šè¿‡ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¯é æ€§æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹å…­ç§ç›¸å…³æ–¹æ³•è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼ŒéªŒè¯äº†è°ƒç ”çš„é‡å¤§æˆæœã€‚æœ€åï¼Œæˆ‘ä»¬å±•æœ›äº†å…³é”®æœªæ¥å‘å±•æ–¹å‘å¹¶æ¦‚è¿°äº†å¼€æ”¾æŒ‘æˆ˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹LLMæ ¡å‡†æ–¹æ³•å’Œç›¸å…³æŒ‡æ ‡çš„ä¸“é¡¹ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨å¤šä¸ªé¢†åŸŸå…·æœ‰å˜é©æ€§å½±å“ï¼Œä½†hallucinationç°è±¡ä»æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰å¯¹äºè¯„ä¼°LLMè‡³å…³é‡è¦ã€‚</li>
<li>å°½ç®¡æœ‰å¯¹ä¼ ç»Ÿæ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–çš„ç ”ç©¶ï¼Œä½†å¯¹LLMçš„æ•ˆæœç¼ºä¹æ·±å…¥åˆ†æã€‚</li>
<li>ç›®å‰ç¼ºä¹é’ˆå¯¹LLMæ ¡å‡†æ–¹æ³•çš„ç»¼åˆåŸºå‡†æ¯”è¾ƒã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿè°ƒæŸ¥äº†LLMçš„UQå’Œæ ¡å‡†æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å®è¯è¯„ä¼°ï¼ŒéªŒè¯äº†å…­ç§ç›¸å…³æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7164e817ed31e430f9715fd94612d092.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac5ec95627361cf8d5f900ecb2e57cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1e88b5b855b379db011987fe0604a07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b82e4c2285bb70a28994a16bad15dac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c637d009d7ce77c460b6c5800b830222.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimising-ChatGPT-for-creativity-in-literary-translation-A-case-study-from-English-into-Dutch-Chinese-Catalan-and-Spanish"><a href="#Optimising-ChatGPT-for-creativity-in-literary-translation-A-case-study-from-English-into-Dutch-Chinese-Catalan-and-Spanish" class="headerlink" title="Optimising ChatGPT for creativity in literary translation: A case study   from English into Dutch, Chinese, Catalan and Spanish"></a>Optimising ChatGPT for creativity in literary translation: A case study   from English into Dutch, Chinese, Catalan and Spanish</h2><p><strong>Authors:Shuxiang Du, Ana Guerberof Arenas, Antonio Toral, Kyo Gerrits, Josep Marco Borillo</strong></p>
<p>This study examines the variability of Chat-GPT machine translation (MT) outputs across six different configurations in four languages,with a focus on creativity in a literary text. We evaluate GPT translations in different text granularity levels, temperature settings and prompting strategies with a Creativity Score formula. We found that prompting ChatGPT with a minimal instruction yields the best creative translations, with â€œTranslate the following text into [TG] creativelyâ€ at the temperature of 1.0 outperforming other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless, ChatGPT consistently underperforms compared to human translation (HT). </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨Chat-GPTæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰è¾“å‡ºåœ¨å››ç§è¯­è¨€çš„å…­ç§ä¸åŒé…ç½®ä¸­çš„å˜åŒ–æ€§ï¼Œé‡ç‚¹å…³æ³¨æ–‡å­¦æ–‡æœ¬ä¸­çš„åˆ›é€ æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸åŒæ–‡æœ¬ç²’åº¦çº§åˆ«ã€æ¸©åº¦è®¾ç½®å’Œæç¤ºç­–ç•¥æ¥è¯„ä¼°GPTç¿»è¯‘ï¼Œå¹¶ä½¿ç”¨åˆ›é€ åŠ›å¾—åˆ†å…¬å¼è¿›è¡Œè¯„ä»·ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”¨æœ€ç®€å•çš„æŒ‡ä»¤æç¤ºChatGPTä¼šè·å¾—æœ€å…·åˆ›æ„çš„ç¿»è¯‘ã€‚åœ¨æ¸©åº¦ä¸º1.0çš„æƒ…å†µä¸‹ï¼Œæç¤ºâ€œå°†ä»¥ä¸‹æ–‡æœ¬åˆ›é€ æ€§åœ°ç¿»è¯‘æˆ[TG]â€æ¯”å…¶ä»–é…ç½®å’ŒDeepLåœ¨è¥¿ç­ç‰™è¯­ã€è·å…°è¯­å’Œä¸­æ–‡æ–¹é¢çš„è¡¨ç°éƒ½è¦å¥½ã€‚ç„¶è€Œï¼Œä¸äººå·¥ç¿»è¯‘ï¼ˆHTï¼‰ç›¸æ¯”ï¼ŒChatGPTçš„è¡¨ç°å§‹ç»ˆè¾ƒå·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18221v1">PDF</a> This paper has been accepted to the MT Summit 2025 to be held in   Geneva on June 23-27 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¸åŒé…ç½®ä¸‹ChatGPTæœºå™¨ç¿»è¯‘è¾“å‡ºçš„å˜åŒ–ï¼Œæ¶‰åŠå››ç§è¯­è¨€ä¸­çš„å…­ç§ä¸åŒé…ç½®ï¼Œå¹¶é‡ç‚¹å…³æ³¨æ–‡å­¦æ–‡æœ¬ä¸­çš„åˆ›é€ æ€§ã€‚ç ”ç©¶è¯„ä»·äº†GPTåœ¨ä¸åŒæ–‡æœ¬ç²’åº¦çº§åˆ«ã€æ¸©åº¦è®¾ç½®å’Œæç¤ºç­–ç•¥ä¸‹çš„ç¿»è¯‘è¡¨ç°ï¼Œå¹¶é‡‡ç”¨åˆ›é€ åŠ›è¯„åˆ†å…¬å¼è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œç”¨ç®€æ´æŒ‡ä»¤æç¤ºChatGPTå¯è·å¾—æœ€ä½³åˆ›æ„ç¿»è¯‘ã€‚åœ¨æ¸©åº¦ä¸º1.0çš„æƒ…å†µä¸‹ï¼Œæç¤ºâ€œå°†ä»¥ä¸‹æ–‡æœ¬åˆ›é€ æ€§åœ°ç¿»è¯‘æˆ[TG]â€ä¼˜äºå…¶ä»–é…ç½®å’ŒDeepLåœ¨è¥¿ç­ç‰™è¯­ã€è·å…°è¯­å’Œä¸­æ–‡çš„ç¿»è¯‘è¡¨ç°ã€‚ç„¶è€Œï¼ŒChatGPTä¸äººç±»ç¿»è¯‘ç›¸æ¯”å§‹ç»ˆè¡¨ç°æ¬ ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨ChatGPTæœºå™¨ç¿»è¯‘åœ¨å››ç§è¯­è¨€ä¸­çš„åˆ›é€ æ€§è¡¨ç°ã€‚</li>
<li>åœ¨ä¸åŒé…ç½®ï¼ˆæ–‡æœ¬ç²’åº¦ã€æ¸©åº¦è®¾ç½®å’Œæç¤ºç­–ç•¥ï¼‰ä¸‹è¯„ä¼°GPTçš„ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>â€œTranslate the following text into [TG] creativelyâ€çš„æç¤ºæ–¹å¼åœ¨æ¸©åº¦1.0æ—¶è¡¨ç°æœ€ä½³ã€‚</li>
<li>ChatGPTåœ¨æŸäº›è¯­è¨€ï¼ˆå¦‚è¥¿ç­ç‰™è¯­ã€è·å…°è¯­ã€ä¸­æ–‡ï¼‰çš„ç¿»è¯‘è¡¨ç°ä¼˜äºDeepLã€‚</li>
<li>ChatGPTçš„ç¿»è¯‘åˆ›é€ åŠ›è¯„åˆ†ä½äºäººç±»ç¿»è¯‘ã€‚</li>
<li>ç®€æ´çš„æŒ‡ä»¤æç¤ºæœ‰åŠ©äºè·å¾—æœ€ä½³åˆ›æ„ç¿»è¯‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b469d619ff04753a4395c8bdd252e14e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a54a58fd02540acef01d1f20b9ec94d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-374ecd6130b27504003a1be328eaa184.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training"><a href="#DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training" class="headerlink" title="DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training"></a>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training</h2><p><strong>Authors:Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li</strong></p>
<p>Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å„ç§å¤æ‚çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å­¦æœ¯ç•Œä»ç„¶ç¼ºä¹å¯¹åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„æ·±å…¥äº†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«çº¦334ä¸‡ä¸ªå”¯ä¸€æŸ¥è¯¢å’Œå¤§çº¦4äº¿ä¸ªè’¸é¦å“åº”ï¼Œè¿™äº›å“åº”ç”±å¤šä¸ªæ¨¡å‹å¤šæ¬¡è¿­ä»£ç”Ÿæˆã€‚æˆ‘ä»¬åˆ©ç”¨é€šè¿‡ç‡å’Œå˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ç²¾ç¡®é€‰æ‹©æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼å‘ç”Ÿäº†å˜åŒ–ï¼Œè¿™è¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡æ‰èƒ½è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ä½¿ç”¨è¿™äº›ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„é€šè¿‡ç‡è¾¾åˆ°79.2%ã€‚è¿™ä¸€ç»“æœè¶…è¿‡äº†å¤§å¤šæ•°å½“å‰çš„è’¸é¦æ¨¡å‹ï¼Œå¹¶æ¥è¿‘æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†æ•°æ®å¤„ç†çš„è¯¦ç»†æè¿°ã€éš¾åº¦è¯„ä¼°å’ŒåŸ¹è®­æ–¹æ³•ï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚æ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17565v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§å¤æ‚æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œä½†å­¦æœ¯ç•Œä»ç¼ºä¹å¯¹åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡æ·±å…¥äº†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«çº¦334ä¸‡æ¡ç‹¬ç‰¹æŸ¥è¯¢å’Œçº¦4åƒä¸‡æ¡è’¸é¦å“åº”ã€‚é€šè¿‡åˆ©ç”¨é€šè¿‡ç‡å’Œç›¸å…³ç³»æ•°ï¼ˆCVï¼‰ï¼Œæˆ‘ä»¬ç²¾ç¡®é€‰æ‹©äº†æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼è½¬å˜ï¼Œè¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜å­¦ä¹ ç‡è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ä½¿ç”¨æ­¤æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„é€šè¿‡ç‡è¾¾åˆ°äº†79.2%ï¼Œè¶…è¶Šå¤§å¤šæ•°ç°æœ‰è’¸é¦æ¨¡å‹ï¼Œæ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„æ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°åŠè®­ç»ƒæ–¹æ³•çš„æè¿°ï¼Œå¹¶å·²å…¬å¼€æ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼Œç”¨äºæå‡LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨é€šè¿‡ç‡å’Œç›¸å…³ç³»æ•°ï¼ˆCVï¼‰ç²¾ç¡®é€‰æ‹©æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>è§‚å¯Ÿåˆ°äº†åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜å­¦ä¹ ç‡çš„è®­ç»ƒæ¨¡å¼è½¬å˜ã€‚</li>
<li>ä½¿ç”¨ç²¾é€‰æ•°æ®æ˜¾è‘—æå‡äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—é«˜é€šè¿‡ç‡ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†è®¸å¤šç°æœ‰çš„è’¸é¦æ¨¡å‹ï¼Œå¹¶æ¥è¿‘äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>å…¬å¼€æ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43bbdf4109b00c3cf285ff26fdce5789.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference"><a href="#COBRA-Algorithm-Architecture-Co-optimized-Binary-Transformer-Accelerator-for-Edge-Inference" class="headerlink" title="COBRA: Algorithm-Architecture Co-optimized Binary Transformer   Accelerator for Edge Inference"></a>COBRA: Algorithm-Architecture Co-optimized Binary Transformer   Accelerator for Edge Inference</h2><p><strong>Authors:Ye Qiao, Zhiheng Chen, Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang</strong></p>
<p>Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers perform inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accelerator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and +1 values, surpassing ternary methods. With further hardware-friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS&#x2F;Watt energy efficiency on edge FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åºå¤§çš„æ¨¡å‹è§„æ¨¡ä»¥åŠå¯¹è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡çš„é«˜è¦æ±‚ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è¾¹ç¼˜å¹³å°ä¸Šçš„æœ¬åœ°å®‰å…¨æ¨ç†éƒ¨ç½²ã€‚äºŒè¿›åˆ¶å˜å‹å™¨ï¼ˆBinary Transformersï¼‰ä»¥ç´§å‡‘ã€ä½å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆæ»¡è¶³è¾¹ç¼˜éƒ¨ç½²çš„éœ€æ±‚ï¼Œé™ä½äº†å¸¦å®½éœ€æ±‚å¹¶ä¿æŒäº†å¯æ¥å—çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘é’ˆå¯¹äºŒè¿›åˆ¶çš„ç‰¹å®šä¼˜åŒ–ï¼Œç°æœ‰çš„äºŒè¿›åˆ¶å˜å‹å™¨åœ¨å½“å‰ç¡¬ä»¶ä¸Šçš„è¿è¡Œæ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†COBRAï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¾¹ç¼˜è®¡ç®—çš„ååŒä¼˜åŒ–çš„äºŒè¿›åˆ¶TransformeråŠ é€Ÿå™¨ã€‚COBRAé…å¤‡äº†çœŸæ­£çš„1ä½äºŒè¿›åˆ¶ä¹˜æ³•å•å…ƒï¼Œèƒ½å¤Ÿä½¿ç”¨-1ã€0å’Œ+1çš„å€¼è¿›è¡ŒçŸ©é˜µæ“ä½œï¼Œè¶…è¶Šäº†ä¸‰å…ƒæ–¹æ³•ã€‚é€šè¿‡å¯¹æ³¨æ„åŠ›æ¨¡å—çš„è¿›ä¸€æ­¥é¢å‘ç¡¬ä»¶çš„ä¼˜åŒ–ï¼ŒCOBRAåœ¨è¾¹ç¼˜FPGAä¸Šå®ç°äº†é«˜è¾¾3894.7 GOPSçš„ååé‡å’Œ448.7 GOPS&#x2F;Wattçš„èƒ½æ•ˆï¼Œç›¸å¯¹äºGPUå®ç°äº†311å€èƒ½æ•ˆæå‡ï¼Œç›¸å¯¹äºæœ€æ–°çš„äºŒè¿›åˆ¶åŠ é€Ÿå™¨å®ç°äº†3.5å€çš„ååé‡æå‡ï¼ŒåŒæ—¶æ¨ç†ç²¾åº¦ä»…å‡ºç°å¾®å°çš„ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16269v2">PDF</a> </p>
<p><strong>Summary</strong><br>    äºŒè¿›åˆ¶TransformeråŠ é€Ÿå™¨COBRAé’ˆå¯¹è¾¹ç¼˜è®¡ç®—è¿›è¡Œä¼˜åŒ–ï¼Œé‡‡ç”¨çœŸå®çš„1ä½äºŒè¿›åˆ¶ä¹˜æ³•å•å…ƒï¼Œå®ç°çŸ©é˜µæ“ä½œï¼Œæé«˜èƒ½æºæ•ˆç‡å’Œååé‡ï¼Œé™ä½æ¨ç†ç²¾åº¦æŸå¤±ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based modelsåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨è¾¹ç¼˜å¹³å°ä¸Šéƒ¨ç½²è¿›è¡Œæœ¬åœ°å®‰å…¨æ¨ç†æ—¶é¢ä¸´è®¡ç®—ã€å†…å­˜å’Œé€šä¿¡éœ€æ±‚å·¨å¤§çš„é—®é¢˜ã€‚</li>
<li>äºŒè¿›åˆ¶å˜å‹å™¨ä¸ºè§£å†³è¾¹ç¼˜éƒ¨ç½²é—®é¢˜æä¾›äº†ç´§å‡‘ã€ä½å¤æ‚åº¦çš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å‡å°‘å¸¦å®½éœ€æ±‚å’Œä¿æŒå¯æ¥å—ç²¾åº¦çš„å¥½å¤„ã€‚</li>
<li>ç°æœ‰äºŒè¿›åˆ¶å˜å‹å™¨åœ¨å½“å‰ç¡¬ä»¶ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œç¼ºä¹äºŒè¿›åˆ¶ç‰¹å®šä¼˜åŒ–ã€‚</li>
<li>COBRAæ˜¯ä¸€ä¸ªé’ˆå¯¹è¾¹ç¼˜è®¡ç®—çš„ç®—æ³•-æ¶æ„ååŒä¼˜åŒ–çš„äºŒè¿›åˆ¶TransformeråŠ é€Ÿå™¨ã€‚</li>
<li>COBRAé‡‡ç”¨çœŸå®çš„1ä½äºŒè¿›åˆ¶ä¹˜æ³•å•å…ƒï¼Œè¶…è¶Šä¸‰å…ƒæ–¹æ³•ï¼Œå®ç°çŸ©é˜µæ“ä½œã€‚</li>
<li>é€šè¿‡åœ¨æ³¨æ„åŠ›å—ä¸­è¿›è¡Œç¡¬ä»¶å‹å¥½çš„è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ŒCOBRAåœ¨è¾¹ç¼˜FPGAä¸Šå®ç°äº†é«˜è¾¾3,894.7 GOPSçš„ååé‡ï¼Œä»¥åŠ448.7 GOPS&#x2F;Wattçš„èƒ½æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb6acc141e4b93f016598c65a1c30e36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcc253801077ecf6c67848506e72e235.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-269d2b206a1831a0bdaad93cdddcd488.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73ecb18e9a038a0704b6821ed52cbff5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb387527664c227993eca561800e0226.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3df1f9c10a7fe870730eccfbe44ba18b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CAPO-Cost-Aware-Prompt-Optimization"><a href="#CAPO-Cost-Aware-Prompt-Optimization" class="headerlink" title="CAPO: Cost-Aware Prompt Optimization"></a>CAPO: Cost-Aware Prompt Optimization</h2><p><strong>Authors:Tom Zehle, Moritz Schlager, Timo HeiÃŸ, Matthias Feurer</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11&#x2F;15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç®€å•çš„æç¤ºæŒ‡å¯¼è§£å†³äº†å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä»è€Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„å±€é¢ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å¯¹æç¤ºçš„åˆ¶å®šéå¸¸æ•æ„Ÿã€‚è™½ç„¶è‡ªåŠ¨æç¤ºä¼˜åŒ–å¯ä»¥é€šè¿‡æ‰¾åˆ°æœ€ä½³æç¤ºæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†å½“å‰çš„æ–¹æ³•éœ€è¦å¤§é‡çš„LLMè°ƒç”¨å’Œè¾“å…¥ä»¤ç‰Œï¼Œè¿™ä½¿å¾—æç¤ºä¼˜åŒ–æˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬å¼•å…¥äº†CAPOï¼ˆåŸºäºæˆæœ¬çš„æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡çš„ç®—æ³•ã€‚CAPOæ˜¯ä¸€ç§è¿›åŒ–æ–¹æ³•ï¼Œä»¥LLMä½œä¸ºæ“ä½œå‘˜ï¼Œèå…¥ç«èµ›ä»¥èŠ‚çœè¯„ä¼°å’Œå¤šç›®æ ‡ä¼˜åŒ–æ¥å¹³è¡¡æ€§èƒ½å’Œæç¤ºé•¿åº¦ã€‚å®ƒè”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡æè¿°æ¥æé«˜ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†å’ŒLLMä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨15ç§æƒ…å†µä¸‹ï¼ŒCAPOåœ¨11ç§æƒ…å†µä¸‹ä¼˜äºæœ€å…ˆè¿›çš„ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œæ€§èƒ½æé«˜äº†é«˜è¾¾21%ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å³å¯å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œé€šè¿‡ç«èµ›èŠ‚çœè¯„ä¼°ï¼Œå¹¶é€šè¿‡é•¿åº¦æƒ©ç½šå‡å°‘å¹³å‡æç¤ºé•¿åº¦ï¼Œä½¿å…¶æ—¢ç»æµåˆæ³¨é‡æˆæœ¬ã€‚å³ä½¿æ²¡æœ‰å°‘é‡ç¤ºä¾‹ï¼ŒCAPOä¹Ÿèƒ½è¶…è¶Šç«äº‰å¯¹æ‰‹ï¼Œå¹¶ä¸”å¯¹åˆå§‹æç¤ºä¿æŒç¨³å¥ã€‚CAPOæœç€é€šè¿‡æé«˜æˆæœ¬æ•ˆç‡æ¥ä½¿æç¤ºä¼˜åŒ–æ›´å¼ºå¤§ã€æ›´æ˜“äºè®¿é—®è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16005v3">PDF</a> Submitted to AutoML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æç¤ºæŒ‡å¯¼è§£å†³äº†å¤šç§ä»»åŠ¡ï¼Œä½†æ€§èƒ½å¯¹æç¤ºåˆ¶å®šé«˜åº¦æ•æ„Ÿã€‚å½“å‰è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–æ–¹æ³•éœ€è¦å¤§é‡LLMè°ƒç”¨å’Œè¾“å…¥ä»¤ç‰Œï¼Œæˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬å¼•å…¥CAPOï¼ˆæˆæœ¬æ„ŸçŸ¥æç¤ºä¼˜åŒ–ï¼‰ç®—æ³•ï¼Œé€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡ã€‚CAPOé‡‡ç”¨è¿›åŒ–æ–¹æ³•ï¼Œå°†LLMä½œä¸ºæ“ä½œå‘˜ï¼Œç»“åˆç«èµ›ä»¥èŠ‚çœè¯„ä¼°å’Œå¤šç›®æ ‡ä¼˜åŒ–ä»¥å¹³è¡¡æ€§èƒ½å’Œæç¤ºé•¿åº¦ã€‚å®ƒè”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡æè¿°æé«˜ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCAPOåœ¨å¤šæ•°æƒ…å†µä¸‹ä¼˜äºæœ€æ–°ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œæ”¹è¿›å¹…åº¦è¾¾21%ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨è¾ƒå°é¢„ç®—ä¸‹å³å¯å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œé€šè¿‡ç«èµ›èŠ‚çœè¯„ä¼°ï¼Œå¹¶é€šè¿‡é•¿åº¦æƒ©ç½šå‡å°‘å¹³å‡æç¤ºé•¿åº¦ï¼Œå®ç°äº†æˆæœ¬æ•ˆç›Šå’Œæ„ŸçŸ¥æˆæœ¬çš„ä¼˜åŒ–ã€‚å³ä½¿åœ¨æ²¡æœ‰å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼ŒCAPOä¹Ÿèƒ½è¶…è¶Šç«äº‰å¯¹æ‰‹ï¼Œå¹¶ä¸”å¯¹åˆå§‹æç¤ºä¿æŒç¨³å¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡æç¤ºæŒ‡å¯¼è§£å†³å¤šç§ä»»åŠ¡ï¼Œä½†æ€§èƒ½å—æç¤ºåˆ¶å®šå½±å“ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–æç¤ºä¼˜åŒ–æ–¹æ³•æˆæœ¬é«˜ï¼Œéœ€è¦æ”¹è¿›æ•ˆç‡ã€‚</li>
<li>å¼•å…¥CAPOç®—æ³•ï¼Œé›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡ã€‚</li>
<li>CAPOé‡‡ç”¨è¿›åŒ–æ–¹æ³•ï¼Œç»“åˆç«èµ›ä»¥èŠ‚çœè¯„ä¼°å’Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚</li>
<li>CAPOè”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œæé«˜ç¨³å¥æ€§ã€‚</li>
<li>CAPOåœ¨å¤šæ•°æƒ…å†µä¸‹ä¼˜äºæœ€æ–°ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œæ”¹è¿›å¹…åº¦è¾¾21%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dd67d6de95c62c7869474bf18b96595.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b116fd3254b73260dad8b5aba0aa636b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fba975ea5ea648e831c5bf75967937cd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Using-customized-GPT-to-develop-prompting-proficiency-in-architectural-AI-generated-images"><a href="#Using-customized-GPT-to-develop-prompting-proficiency-in-architectural-AI-generated-images" class="headerlink" title="Using customized GPT to develop prompting proficiency in architectural   AI-generated images"></a>Using customized GPT to develop prompting proficiency in architectural   AI-generated images</h2><p><strong>Authors:Juan David Salazar Rodriguez, Sam Conrad Joyce,  Julfendi</strong></p>
<p>This research investigates the use of customized GPT models to enhance prompting proficiency among architecture students when generating AI-driven images. Prompt engineering is increasingly essential in architectural education due to the widespread adoption of generative AI tools. This study utilized a mixed-methods experimental design involving architecture students divided into three distinct groups: a control group receiving no structured support, a second group provided with structured prompting guides, and a third group supported by both structured guides and interactive AI personas. Students engaged in reverse engineering tasks, first guessing provided image prompts and then generating their own prompts, aiming to boost critical thinking and prompting skills. Variables examined included time spent prompting, word count, prompt similarity, and concreteness. Quantitative analysis involved correlation assessments between these variables and a one-way ANOVA to evaluate differences across groups. While several correlations showed meaningful relationships, not all were statistically significant. ANOVA results indicated statistically significant improvements in word count, similarity, and concreteness, especially in the group supported by AI personas and structured prompting guides. Qualitative feedback complemented these findings, revealing enhanced confidence and critical thinking skills in students. These results suggest tailored GPT interactions substantially improve studentsâ€™ ability to communicate architectural concepts clearly and effectively. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å®šåˆ¶GPTæ¨¡å‹åœ¨å¢å¼ºå»ºç­‘å­¦å­¦ç”Ÿåœ¨ç”Ÿæˆäººå·¥æ™ºèƒ½é©±åŠ¨å›¾åƒæ—¶çš„æç¤ºèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚ç”±äºç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥å…·çš„å¹¿æ³›åº”ç”¨ï¼Œæç¤ºå·¥ç¨‹åœ¨å»ºç­‘æ•™è‚²ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨æ··åˆæ–¹æ³•å®éªŒè®¾è®¡ï¼Œå°†å»ºç­‘å­¦å­¦ç”Ÿåˆ†ä¸ºä¸‰ç»„ï¼šå¯¹ç…§ç»„æ— ç»“æ„åŒ–æ”¯æŒï¼Œç¬¬äºŒç»„æä¾›ç»“æ„åŒ–æç¤ºæŒ‡å—ï¼Œç¬¬ä¸‰ç»„ç”±ç»“æ„åŒ–æŒ‡å—å’Œäº¤äº’å¼AIäººæ ¼æä¾›æ”¯æŒã€‚å­¦ç”Ÿä»¬å‚ä¸äº†é€†å‘å·¥ç¨‹ä»»åŠ¡ï¼Œé¦–å…ˆçŒœæµ‹æä¾›çš„å›¾åƒæç¤ºï¼Œç„¶åç”Ÿæˆè‡ªå·±çš„æç¤ºï¼Œæ—¨åœ¨æé«˜æ‰¹åˆ¤æ€ç»´å’Œæç¤ºæŠ€èƒ½ã€‚ç ”ç©¶çš„å˜é‡åŒ…æ‹¬æç¤ºæ‰€èŠ±è´¹çš„æ—¶é—´ã€å­—æ•°ã€æç¤ºçš„ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§ã€‚å®šé‡åˆ†ææ¶‰åŠè¿™äº›å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§è¯„ä¼°ï¼Œä»¥åŠå•å› ç´ æ–¹å·®åˆ†æä»¥è¯„ä¼°å„ç»„é—´å·®å¼‚ã€‚è™½ç„¶å‡ ä¸ªç›¸å…³æ€§æ˜¾ç¤ºäº†æœ‰æ„ä¹‰çš„å…³ç³»ï¼Œä½†å¹¶ä¸æ˜¯éƒ½æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ„ä¹‰ã€‚æ–¹å·®åˆ†æç»“æœæŒ‡å‡ºï¼Œåœ¨å­—æ•°ã€ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨AIäººæ ¼å’Œç»“æ„åŒ–æç¤ºæŒ‡å—çš„æ”¯æŒä¸‹çš„å°ç»„ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚å®šæ€§åé¦ˆè¡¥å……äº†è¿™äº›å‘ç°ï¼Œæ˜¾ç¤ºå­¦ç”Ÿçš„è‡ªä¿¡å’Œæ‰¹åˆ¤æ€ç»´èƒ½åŠ›æœ‰æ‰€æé«˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæœ‰é’ˆå¯¹æ€§çš„GPTäº’åŠ¨èƒ½æ˜¾è‘—æé«˜å­¦ç”Ÿæ¸…æ™°æœ‰æ•ˆåœ°ä¼ è¾¾å»ºç­‘æ¦‚å¿µçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13948v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨å®šåˆ¶çš„GPTæ¨¡å‹åœ¨æé«˜å»ºç­‘å­¦ç”Ÿåœ¨ä½¿ç”¨äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒæ—¶çš„æç¤ºèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚éšç€ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å·¥å…·åœ¨å»ºç­‘è®¾è®¡é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œæç¤ºå·¥ç¨‹åœ¨å»ºç­‘è®¾è®¡æ•™è‚²ä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ã€‚è¯¥ç ”ç©¶é‡‡ç”¨æ··åˆæ–¹æ³•å®éªŒè®¾è®¡ï¼Œå°†å»ºç­‘å­¦ç”Ÿåˆ†ä¸ºä¸‰ç»„ï¼šå¯¹ç…§ç»„ï¼ˆæ— ç»“æ„åŒ–æ”¯æŒï¼‰ã€ç¬¬äºŒç»„ï¼ˆæä¾›ç»“æ„åŒ–æç¤ºæŒ‡å—ï¼‰å’Œç¬¬ä¸‰ç»„ï¼ˆç”±ç»“æ„åŒ–æŒ‡å—å’Œäº¤äº’å¼AIäººæ ¼æä¾›æ”¯æŒï¼‰ã€‚å­¦ç”Ÿä»¬å‚ä¸äº†é€†å‘å·¥ç¨‹ä»»åŠ¡ï¼Œé¦–å…ˆçŒœæµ‹æä¾›çš„å›¾åƒæç¤ºï¼Œç„¶åç”Ÿæˆè‡ªå·±çš„æç¤ºï¼Œæ—¨åœ¨æé«˜æ‰¹åˆ¤æ€ç»´å’Œæç¤ºæŠ€èƒ½ã€‚è€ƒå¯Ÿçš„å˜é‡åŒ…æ‹¬æç¤ºæ—¶é—´ã€å­—æ•°ã€æç¤ºç›¸ä¼¼æ€§å’Œå…·ä½“æ€§ã€‚å®šé‡åˆ†æåŒ…æ‹¬è¿™äº›å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§è¯„ä¼°ä»¥åŠè¯„ä¼°å„ç»„é—´å·®å¼‚çš„ä¸€å‘æ–¹å·®åˆ†æã€‚è™½ç„¶ä¸€äº›ç›¸å…³æ€§æ˜¾ç¤ºå‡ºæœ‰æ„ä¹‰çš„å…³ç³»ï¼Œä½†å¹¶ééƒ½å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚æ–¹å·®åˆ†æç»“æœæŒ‡å‡ºï¼Œåœ¨å­—æ•°ã€ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨AIäººæ ¼å’Œç»“æ„åŒ–æç¤ºæŒ‡å—æ”¯æŒçš„å°ç»„ä¸­ã€‚å®šæ€§åé¦ˆè¡¥å……äº†è¿™äº›å‘ç°ï¼Œæ˜¾ç¤ºå‡ºå­¦ç”Ÿä»¬çš„è‡ªä¿¡å’Œæ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›æœ‰æ‰€æé«˜ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå®šåˆ¶GPTäº¤äº’èƒ½æ˜¾è‘—æé«˜å­¦ç”Ÿæ¸…æ™°æœ‰æ•ˆåœ°ä¼ è¾¾å»ºç­‘æ¦‚å¿µçš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶æ¢ç´¢äº†GPTæ¨¡å‹åœ¨å¢å¼ºå»ºç­‘å­¦ç”Ÿä½¿ç”¨AIç”Ÿæˆå›¾åƒæ—¶çš„æç¤ºèƒ½åŠ›æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æç¤ºå·¥ç¨‹åœ¨å»ºç­‘è®¾è®¡æ•™è‚²ä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ï¼Œå› ä¸ºç”Ÿæˆå¼AIå·¥å…·è¢«å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å­¦ç”Ÿåˆ†ä¸ºä¸‰ç»„ï¼Œåˆ†åˆ«æ¥å—ä¸åŒçš„æ”¯æŒå’ŒæŒ‡å¯¼æ–¹å¼ï¼Œä»¥è¯„ä¼°å…¶å¯¹æç¤ºèƒ½åŠ›çš„å½±å“ã€‚</li>
<li>å®šé‡åˆ†æè¡¨æ˜ï¼ŒæŸäº›å˜é‡ä¹‹é—´å­˜åœ¨æœ‰æ„ä¹‰çš„ç›¸å…³æ€§ï¼Œä½†å¹¶éæ‰€æœ‰ç›¸å…³æ€§éƒ½å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚</li>
<li>ä¸€å‘æ–¹å·®åˆ†ææ˜¾ç¤ºï¼Œåœ¨ç‰¹å®šå˜é‡ï¼ˆå¦‚å­—æ•°ã€ç›¸ä¼¼æ€§å’Œå…·ä½“æ€§ï¼‰æ–¹é¢å­˜åœ¨æ˜¾è‘—çš„ç»„é—´å·®å¼‚ã€‚</li>
<li>å®šæ€§åé¦ˆè¿›ä¸€æ­¥è¯å®äº†å­¦ç”Ÿåœ¨æç¤ºèƒ½åŠ›æ–¹é¢çš„æé«˜ï¼ŒåŒ…æ‹¬è‡ªä¿¡å’Œæ‰¹åˆ¤æ€§æ€ç»´ã€‚</li>
<li>å®šåˆ¶GPTäº¤äº’æœ‰åŠ©äºæé«˜å­¦ç”Ÿæ¸…æ™°æœ‰æ•ˆåœ°ä¼ è¾¾å»ºç­‘æ¦‚å¿µçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c09e812b987021ad056662f7e2b8d791.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Machine-generated-text-detection-prevents-language-model-collapse"><a href="#Machine-generated-text-detection-prevents-language-model-collapse" class="headerlink" title="Machine-generated text detection prevents language model collapse"></a>Machine-generated text detection prevents language model collapse</h2><p><strong>Authors:George Drayson, Emine Yilmaz, Vasileios Lampos</strong></p>
<p>As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. We release our code at <a target="_blank" rel="noopener" href="https://github.com/GeorgeDrayson/model_collapse">https://github.com/GeorgeDrayson/model_collapse</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šæ™®åŠï¼Œå®ƒä»¬ç”Ÿæˆçš„è¾“å‡ºåœ¨ç½‘ä¸Šä¸æ–­å¢å¤šï¼Œæœªæ¥å¯èƒ½å¯¼è‡´æœºå™¨ç”Ÿæˆå†…å®¹ç¨€é‡Šäººç±»åˆ›ä½œçš„æ–‡æœ¬ã€‚ç”±äºåœ¨çº¿æ•°æ®æ˜¯LLMé¢„è®­ç»ƒçš„ä¸»è¦èµ„æºï¼Œåç»­æ¨¡å‹å¯èƒ½ä¼šåœ¨æœªçŸ¥æ¯”ä¾‹çš„äººå·¥åˆæˆæ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒã€‚è¿™å°†å¯¼è‡´æ¨¡å‹å´©æºƒï¼Œè¿™æ˜¯ä¸€ä¸ªé€€åŒ–è¿‡ç¨‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼ŒLLMä¼šå¼ºåŒ–è‡ªèº«çš„é”™è¯¯ï¼Œå¹¶æœ€ç»ˆå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†è§£ç ç­–ç•¥å¯¹æ¨¡å‹å´©æºƒçš„å½±å“ï¼Œåˆ†ææ¯ä¸ªæ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ç‰¹è´¨ã€ä¸äººç±»å‚è€ƒçš„ç›¸ä¼¼åº¦ï¼Œä»¥åŠç”±æ­¤å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ç»“æœã€‚æˆ‘ä»¬åˆ©ç”¨å¯¼è‡´é™è§£æœ€ä¸¥é‡çš„è§£ç ç­–ç•¥ï¼Œè¯„ä¼°åœ¨æ•°æ®ï¼ˆäººç±»æˆ–åˆæˆï¼‰æ¥æºæœªçŸ¥çš„æ›´ç°å®åœºæ™¯ä¸­æ¨¡å‹å´©æºƒçš„æƒ…å†µã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§é‡è¦æ€§é‡‡æ ·æ–¹æ³•æ¥ç¼“è§£æ¨¡å‹å´©æºƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸¤ä¸ªLLMå˜ä½“ï¼ˆGPT-2å’ŒSmolLM2ï¼‰çš„å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸Šå¾—åˆ°äº†éªŒè¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå®ƒä¸ä»…å¯ä»¥é˜²æ­¢æ¨¡å‹å´©æºƒï¼Œè€Œä¸”åœ¨å­˜åœ¨è¶³å¤Ÿçš„äººç±»åˆ›ä½œæ ·æœ¬æ—¶è¿˜å¯ä»¥æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/GeorgeDrayson/model_collapse">https://github.com/GeorgeDrayson/model_collapse</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.15654v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè¾“å‡ºæ­£åœ¨ç½‘ç»œä¸Šå¤§é‡å‡ºç°ï¼Œè¿™å¯èƒ½ä¼šç¨€é‡Šäººç±»åˆ›ä½œçš„å†…å®¹å¹¶å¸¦æ¥æ¨¡å‹å´©æºƒçš„é£é™©ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è§£ç ç­–ç•¥å¯¹æ¨¡å‹å´©æºƒçš„å½±å“ï¼Œåˆ†æäº†ä¸åŒæ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ç‰¹æ€§å’Œä¸äººç±»å‚è€ƒæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼Œä»¥åŠæ¨¡å‹æ€§èƒ½çš„å˜åŒ–ã€‚é€šè¿‡è¯„ä¼°åœ¨æ— æ³•ç¡®å®šæ•°æ®æ¥æºäºäººç±»è¿˜æ˜¯åˆæˆçš„æƒ…å†µä¸‹æ›´ç°å®çš„åœºæ™¯ä¸­æ¨¡å‹å´©æºƒçš„æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡‡ç”¨æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨å’Œé‡è¦æ€§é‡‡æ ·æ–¹æ³•çš„æ–¹æ³•ï¼Œä¸ä»…å¯é˜²æ­¢æ¨¡å‹å´©æºƒï¼Œè€Œä¸”å¯åœ¨å­˜åœ¨è¶³å¤Ÿäººç±»åˆ›ä½œæ ·æœ¬çš„æƒ…å†µä¸‹æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè¾“å‡ºåœ¨ç½‘ç»œä¸Šå¤§é‡å‡ºç°ï¼Œå¯èƒ½ä¼šç¨€é‡Šäººç±»åˆ›ä½œçš„å†…å®¹å¹¶å¸¦æ¥æ¨¡å‹å´©æºƒé£é™©ã€‚</li>
<li>è§£ç ç­–ç•¥å¯¹LLMæ¨¡å‹å´©æºƒæœ‰é‡è¦å½±å“ã€‚</li>
<li>æ¨¡å‹å´©æºƒæ˜¯ä¸€ç§é€€åŒ–è¿‡ç¨‹ï¼ŒLLMsä¼šå¼ºåŒ–è‡ªå·±çš„é”™è¯¯ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>åœ¨æ— æ³•ç¡®å®šæ•°æ®æ¥æºäºäººç±»è¿˜æ˜¯åˆæˆçš„æƒ…å†µä¸‹è¯„ä¼°æ¨¡å‹æ€§èƒ½æ›´åŠ ç°å®ã€‚</li>
<li>é‡‡ç”¨æœºå™¨ç”Ÿæˆæ–‡æœ¬æ£€æµ‹å™¨å¯ä»¥æœ‰æ•ˆè¯†åˆ«åˆæˆæ ·æœ¬ã€‚</li>
<li>é‡è¦æ€§é‡‡æ ·æ–¹æ³•å¯ä»¥ç¼“è§£æ¨¡å‹å´©æºƒé—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç»è¿‡åœ¨ä¸¤ç§LLMå˜ä½“ï¼ˆGPT-2å’ŒSmolLM2ï¼‰ä¸Šçš„éªŒè¯ï¼Œè¡¨æ˜å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.15654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c5b84621de3213e90919c325477fb11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b0a2440462e76208b79b12069f64205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2e678cca0b4ccc16a3d8f6247321ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-591289440c0df5250c56e70c3dac38bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5751bc7cc62b7d4d1341aca74ed54cf6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="VisTabNet-Adapting-Vision-Transformers-for-Tabular-Data"><a href="#VisTabNet-Adapting-Vision-Transformers-for-Tabular-Data" class="headerlink" title="VisTabNet: Adapting Vision Transformers for Tabular Data"></a>VisTabNet: Adapting Vision Transformers for Tabular Data</h2><p><strong>Authors:Witold WydmaÅ„ski, Ulvi Movsum-zada, Jacek Tabor, Marek Åšmieja</strong></p>
<p>Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet â€“ a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNetâ€™s superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. We share our example implementation as a GitHub repository available at <a target="_blank" rel="noopener" href="https://github.com/wwydmanski/VisTabNet">https://github.com/wwydmanski/VisTabNet</a>. </p>
<blockquote>
<p>å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤„ç†è¡¨æ ¼æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è§‚å¯Ÿåˆ°ç›¸åº”çš„æ”¹è¿›ã€‚è¡¨æ ¼æ•°æ®ä»ç„¶æ˜¯ç”Ÿç‰©ã€å·¥ä¸šå’Œé‡‘èåº”ç”¨ä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç±»å‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°†å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°åœ¨å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šå®šä¹‰çš„ä¸‹æ¸¸ä»»åŠ¡æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisTabNetâ€”â€”ä¸€ç§è·¨æ¨¡æ€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå…è®¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡å¯¹Vision Transformerï¼ˆViTï¼‰è¿›è¡Œé€‚é…ä»¥å¤„ç†è¡¨æ ¼æ•°æ®ã€‚é€šè¿‡å°†è¡¨æ ¼è¾“å…¥æŠ•å½±åˆ°ViTå¯æ¥å—çš„è¡¥ä¸åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†é¢„è®­ç»ƒçš„Transformerç¼–ç å™¨åº”ç”¨äºè¡¨æ ¼è¾“å…¥ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†ä¸ºå¤„ç†è¡¨æ ¼æ•°æ®è€Œè®¾è®¡åˆé€‚æ¶æ„çš„æ¦‚å¿µæˆæœ¬ï¼ŒåŒæ—¶é™ä½äº†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æˆæœ¬ã€‚åœ¨å¤šä¸ªå°å‹è¡¨æ ¼æ•°æ®é›†ï¼ˆå°‘äº1kæ ·æœ¬ï¼‰ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†VisTabNetçš„ä¼˜è¶Šæ€§ï¼Œå®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„é›†æˆæ–¹æ³•å’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¶…è¶Šäº†ä¼ ç»Ÿçš„è¿ç§»å­¦ä¹ å®è·µï¼Œå¹¶è¡¨æ˜é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹å¯ä»¥è½¬ç§»åˆ°è§£å†³è¡¨æ ¼é—®é¢˜ï¼Œæ‰©å±•äº†è¿ç§»å­¦ä¹ çš„è¾¹ç•Œã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/wwydmanski/VisTabNet">https://github.com/wwydmanski/VisTabNet</a>æä¾›äº†ç¤ºä¾‹å®ç°çš„GitHubä»“åº“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00057v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VisTabNetâ€”â€”ä¸€ç§è·¨æ¨¡æ€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå¯å°†é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åº”ç”¨äºè¡¨æ ¼æ•°æ®å¤„ç†ã€‚é€šè¿‡æŠŠè¡¨æ ¼æ•°æ®è½¬åŒ–ä¸ºViTå¯æ¥å—çš„patch embeddingsï¼Œæˆ‘ä»¬å¯ç›´æ¥åº”ç”¨é¢„è®­ç»ƒçš„Transformer Encoderæ¥å¤„ç†è¡¨æ ¼æ•°æ®ã€‚è¿™ç§æ–¹æ³•é™ä½äº†è®¾è®¡é€‚åˆå¤„ç†è¡¨æ ¼æ•°æ®çš„æ¶æ„çš„æ¦‚å¿µæˆæœ¬ï¼ŒåŒæ—¶å‡å°‘äº†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æˆæœ¬ã€‚åœ¨å¤šä¸ªå°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒVisTabNetä¼˜äºä¼ ç»Ÿçš„é›†æˆæ–¹æ³•å’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¯¥æ–¹æ³•çªç ´äº†ä¼ ç»Ÿçš„è¿ç§»å­¦ä¹ å®è·µï¼Œå±•ç¤ºäº†é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹å¯ä»¥è§£å†³è¡¨æ ¼é—®é¢˜ï¼Œæ‰©å±•äº†è¿ç§»å­¦ä¹ çš„è¾¹ç•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ–¹é¢ä»æœ‰å¾…æé«˜ï¼Œå°¤å…¶æ˜¯åœ¨å°†å¤§å‹é¢„è®­ç»ƒæ¨¡å‹è¿ç§»åˆ°å°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šå®šä¹‰çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>VisTabNetæ˜¯ä¸€ç§è·¨æ¨¡æ€è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œèƒ½å¤Ÿå°†é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¨¡å‹åº”ç”¨äºè¡¨æ ¼æ•°æ®å¤„ç†ã€‚</li>
<li>VisTabNeté€šè¿‡å°†è¡¨æ ¼æ•°æ®è½¬åŒ–ä¸ºpatch embeddingsï¼Œä½¿å¾—é¢„è®­ç»ƒçš„Transformer Encoderå¯ä»¥ç›´æ¥åº”ç”¨äºè¡¨æ ¼æ•°æ®ã€‚</li>
<li>VisTabNeté™ä½äº†è®¾è®¡å¤„ç†è¡¨æ ¼æ•°æ®çš„æ¶æ„çš„æ¦‚å¿µæˆæœ¬ï¼Œå¹¶å‡å°‘äº†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹æ‰€éœ€çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒVisTabNetåœ¨å¤šä¸ªå°å‹è¡¨æ ¼æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿçš„é›†æˆæ–¹æ³•å’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>VisTabNetçªç ´äº†ä¼ ç»Ÿçš„è¿ç§»å­¦ä¹ å®è·µï¼Œå±•ç¤ºäº†é¢„è®­ç»ƒçš„å›¾åƒæ¨¡å‹åœ¨è§£å†³è¡¨æ ¼é—®é¢˜æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4690663b02d6ea2b0e058601669166af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e70deaf14ab282113bd6379bcf3fff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d7e85482d17a245e9d0520c8ea03b6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c06fde064289fcddc45dc797673c17b7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MeTHanol-Modularized-Thinking-Language-Models-with-Intermediate-Layer-Thinking-Decoding-and-Bootstrapping-Reasoning"><a href="#MeTHanol-Modularized-Thinking-Language-Models-with-Intermediate-Layer-Thinking-Decoding-and-Bootstrapping-Reasoning" class="headerlink" title="MeTHanol: Modularized Thinking Language Models with Intermediate Layer   Thinking, Decoding and Bootstrapping Reasoning"></a>MeTHanol: Modularized Thinking Language Models with Intermediate Layer   Thinking, Decoding and Bootstrapping Reasoning</h2><p><strong>Authors:Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji</strong></p>
<p>Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/TadE">https://anonymous.4open.science/r/TadE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆç†åœ°ç†è§£å’Œç”Ÿæˆäººç±»è¡¨è¾¾ï¼Œä½†å¯èƒ½ç¼ºä¹æ·±å…¥çš„æ€è€ƒå’Œæ¨ç†æœºåˆ¶ã€‚æœ€è¿‘æœ‰å‡ é¡¹ç ”ç©¶æ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„æ€è€ƒèƒ½åŠ›ï¼Œä½†å¤§å¤šæ•°å¹¶éæ•°æ®é©±åŠ¨æˆ–åŸºäºè®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å—åˆ°è‡ªç„¶è®¤çŸ¥æœºåˆ¶çš„å¯å‘ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹æ¨¡å‹æ¶æ„ï¼Œç§°ä¸ºTaSï¼Œå®ƒå…è®¸æ¨¡å‹é¦–å…ˆè€ƒè™‘æ€æƒ³ï¼Œç„¶åæ ¹æ®æŸ¥è¯¢è¡¨è¾¾å“åº”ã€‚æˆ‘ä»¬è®¾è®¡äº†å‡ ä¸ªç®¡é“æ¥ä»æç¤º-å“åº”æ ·æœ¬ä¸­æ ‡æ³¨æˆ–ç”Ÿæˆæ€æƒ³å†…å®¹ï¼Œç„¶ååœ¨ä¸­é—´å±‚æ·»åŠ è¯­è¨€å¤´ï¼Œå……å½“æ€è€ƒå±‚ã€‚æˆ‘ä»¬é€šè¿‡æ€æƒ³å¢å¼ºæ•°æ®è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ŒæˆåŠŸè®©æ€è€ƒå±‚è‡ªåŠ¨äº§ç”Ÿåˆç†æ€æƒ³ï¼Œå¹¶æœ€ç»ˆè¾“å‡ºæ›´åˆç†çš„å“åº”ã€‚å®šæ€§å’Œå®šé‡ç»“æœå‡éªŒè¯äº†TaSçš„æœ‰æ•ˆæ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r%2FTadE%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/TadEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.12059v4">PDF</a> 19 pages, 7 figures</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹å¯ä»¥ç†è§£å¹¶ç”Ÿæˆäººç±»è¡¨è¾¾ï¼Œä½†å¯èƒ½ç¼ºä¹æ·±å…¥çš„æ€è€ƒå’Œæ¨ç†æœºåˆ¶ã€‚æœ¬æ–‡è®¾è®¡äº†ä¸€ç§æ–°å‹æ¨¡å‹æ¶æ„TaSï¼Œå®ƒé¦–å…ˆä¼šè€ƒè™‘æ€è€ƒå†…å®¹ï¼Œç„¶åæ ¹æ®æŸ¥è¯¢è¡¨è¾¾å“åº”ã€‚é€šè¿‡æ ‡æ³¨æˆ–ç”Ÿæˆæ€æƒ³å†…å®¹æ ·æœ¬ï¼Œåœ¨ä¸­é—´çš„å±‚åŠ å…¥è¯­è¨€å¤´ä½œä¸ºæ€è€ƒå±‚ã€‚ä½¿ç”¨æ€æƒ³å¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒåï¼Œå¯ä»¥è®©æ€è€ƒå±‚è‡ªåŠ¨ç”Ÿæˆåˆç†çš„æ€æƒ³ï¼Œæœ€ç»ˆè¾“å‡ºæ›´åˆç†çš„å“åº”ã€‚å®éªŒç»“æœéªŒè¯äº†TaSçš„æœ‰æ•ˆæ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹è™½ç„¶èƒ½ç†è§£å’Œç”Ÿæˆäººç±»è¡¨è¾¾ï¼Œä½†ç¼ºä¹æ·±å…¥çš„æ€è€ƒå’Œæ¨ç†æœºåˆ¶ã€‚</li>
<li>æ–°å‹æ¨¡å‹æ¶æ„TaSè®¾è®¡æ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé¦–å…ˆè€ƒè™‘æ€è€ƒå†…å®¹ï¼Œå†åŸºäºæŸ¥è¯¢è¡¨è¾¾å“åº”ã€‚</li>
<li>TaSé€šè¿‡æ ‡æ³¨æˆ–ç”Ÿæˆæ€æƒ³å†…å®¹æ ·æœ¬ï¼Œåœ¨ä¸­é—´çš„å±‚åŠ å…¥è¯­è¨€å¤´ä½œä¸ºæ€è€ƒå±‚ã€‚</li>
<li>ä½¿ç”¨æ€æƒ³å¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œèƒ½è®©æ€è€ƒå±‚è‡ªåŠ¨ç”Ÿæˆåˆç†çš„æ€æƒ³ã€‚</li>
<li>TaSæ¨¡å‹èƒ½è¾“å‡ºæ›´åˆç†çš„å“åº”ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œæ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.12059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db80c722225860d7a6b5cd6dd0a5fe71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a183e7f330f925044b4554a936d978b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36d64b43de0d63b1fd2461b032f25b96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57dd379389f56d83aa64a12546983061.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-632f54752b561a3a6974c6d7219e1875.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b028dccf38005f2391366daeee0a19.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Automatically-Generating-UI-Code-from-Screenshot-A-Divide-and-Conquer-Based-Approach"><a href="#Automatically-Generating-UI-Code-from-Screenshot-A-Divide-and-Conquer-Based-Approach" class="headerlink" title="Automatically Generating UI Code from Screenshot: A   Divide-and-Conquer-Based Approach"></a>Automatically Generating UI Code from Screenshot: A   Divide-and-Conquer-Based Approach</h2><p><strong>Authors:Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, Michael R. Lyu</strong></p>
<p>Websites are critical in todayâ€™s digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process.   In this paper, we propose DCGen, a divide-and-conquer-based approach to automate the translation of webpage design to UI code. DCGen starts by dividing screenshots into manageable segments, generating code for each segment, and then reassembling them into complete UI code for the entire screenshot. We conduct extensive testing with a dataset comprised of real-world websites and various MLLMs and demonstrate that DCGen achieves up to a 15% improvement in visual similarity and 8% in code similarity for large input images. Human evaluations show that DCGen can help developers implement webpages significantly faster and more similar to the UI designs. To the best of our knowledge, DCGen is the first segment-aware MLLM-based approach for generating UI code directly from screenshots. </p>
<blockquote>
<p>åœ¨å½“ä»Šæ•°å­—åŒ–ä¸–ç•Œä¸­ï¼Œç½‘ç«™æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå½“å‰æœ‰è¶…è¿‡11.1äº¿ä¸ªæ´»è·ƒç½‘ç«™ï¼Œæ¯å¤©å¤§çº¦ä¼šæœ‰25ä¸‡2åƒä¸ªæ–°ç½‘ç«™ä¸Šçº¿ã€‚å°†ç½‘ç«™å¸ƒå±€è®¾è®¡è½¬åŒ–ä¸ºåŠŸèƒ½æ€§çš„UIä»£ç æ˜¯ç½‘ç«™å¼€å‘ä¸­ä¸€ä¸ªæ—¢è€—æ—¶åˆå¿…ä¸å¯å°‘çš„æ­¥éª¤ã€‚ä½¿ç”¨äººå·¥æ–¹æ³•å°†è§†è§‰è®¾è®¡è½¬åŒ–ä¸ºåŠŸèƒ½æ€§ä»£ç é¢ä¸´ç€é‡å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›éä¸“ä¸šäººå£«æ¥è¯´ã€‚ä¸ºäº†æ¢ç´¢è‡ªåŠ¨è®¾è®¡åˆ°ä»£ç çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬é¦–å…ˆé’ˆå¯¹GPT-4oè¿›è¡Œäº†ä¸€é¡¹æ¿€åŠ±ç ”ç©¶ï¼Œå¹¶å‘ç°äº†ç”ŸæˆUIä»£ç æ—¶å­˜åœ¨çš„ä¸‰ç§é—®é¢˜ï¼šå…ƒç´ é—æ¼ã€å…ƒç´ å˜å½¢å’Œå…ƒç´ é”™æ’ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ­ç¤ºï¼Œå…³æ³¨è¾ƒå°çš„è§†è§‰ç‰‡æ®µå¯ä»¥å¸®åŠ©å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç¼“è§£è¿™äº›å¤±è´¥é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16386v3">PDF</a> Accepted by FSE 2025</p>
<p><strong>Summary</strong>ï¼šåœ¨æ•°å­—æ—¶ä»£ï¼Œç½‘ç«™çš„æ•°é‡æŒç»­å¢é•¿ï¼Œå¦‚ä½•å°†ç½‘é¡µå¸ƒå±€è®¾è®¡è½¬åŒ–ä¸ºåŠŸèƒ½æ€§ç”¨æˆ·ç•Œé¢ä»£ç æˆä¸ºäº†é‡è¦æ­¥éª¤ã€‚å­˜åœ¨GPT-4oç­‰è‡ªåŠ¨è®¾è®¡è½¬ä»£ç è§£å†³æ–¹æ¡ˆçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºDCGenæ–¹æ³•ï¼Œé€šè¿‡åˆ†å‰²æˆªå›¾ç”Ÿæˆä»£ç ç‰‡æ®µå†ç»„åˆæˆå®Œæ•´ä»£ç ï¼Œå®ç°ç½‘é¡µè®¾è®¡è‡ªåŠ¨åŒ–è½¬ç ã€‚æµ‹è¯•è¡¨æ˜ï¼ŒDCGenåœ¨è§†è§‰å’Œä»£ç ç›¸ä¼¼æ€§ä¸Šæœ‰æ‰€æå‡ï¼Œå¹¶èƒ½å¸®åŠ©å¼€å‘è€…æ›´å¿«å®ç°ä¸UIè®¾è®¡ç›¸ä¼¼çš„ç½‘é¡µã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½‘ç«™åœ¨æ•°å­—æ—¶ä»£å…·æœ‰é‡è¦æ€§ï¼Œè‡ªåŠ¨è®¾è®¡è½¬ä»£ç è§£å†³æ–¹æ¡ˆéœ€æ±‚å¢åŠ ã€‚</li>
<li>GPT-4oç­‰ç°æœ‰è§£å†³æ–¹æ¡ˆå­˜åœ¨å…ƒç´ é—æ¼ã€å¤±çœŸå’Œé”™ä½ç­‰é—®é¢˜ã€‚</li>
<li>DCGenæ–¹æ³•åŸºäºåˆ†æ®µå¾æœç­–ç•¥ï¼Œä»æˆªå›¾ç”Ÿæˆä»£ç ç‰‡æ®µå¹¶ç»„åˆæˆå®Œæ•´ä»£ç ã€‚</li>
<li>DCGenåœ¨å¤§å‹è¾“å…¥å›¾åƒä¸Šå®ç°äº†è§†è§‰å’Œä»£ç ç›¸ä¼¼æ€§çš„æå‡ã€‚</li>
<li>DCGenæœ‰åŠ©äºå¼€å‘è€…æ›´å¿«ã€æ›´å‡†ç¡®åœ°å®ç°ç½‘é¡µå¼€å‘ã€‚</li>
<li>æ®æ‚‰ï¼ŒDCGenæ˜¯é¦–ä¸ªåŸºäºMLLMçš„æ®µæ„ŸçŸ¥æ–¹æ³•ï¼Œå¯ç›´æ¥ä»æˆªå›¾ç”ŸæˆUIä»£ç ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16386">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e78ed248625d0dae6dfbfd80852a2b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b99924f4fb8bd70e351596184e5e443.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e32e45daecebffb48a9b27ab69515dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-366d0b7f71f9d179fb929eae7cbc6012.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FoC-Figure-out-the-Cryptographic-Functions-in-Stripped-Binaries-with-LLMs"><a href="#FoC-Figure-out-the-Cryptographic-Functions-in-Stripped-Binaries-with-LLMs" class="headerlink" title="FoC: Figure out the Cryptographic Functions in Stripped Binaries with   LLMs"></a>FoC: Figure out the Cryptographic Functions in Stripped Binaries with   LLMs</h2><p><strong>Authors:Xiuwei Shang, Guoqiang Chen, Shaoyin Cheng, Shikai Guo, Yanming Zhang, Weiming Zhang, Nenghai Yu</strong></p>
<p>Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoC-BinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection. </p>
<blockquote>
<p>åˆ†æå‰¥ç¦»äºŒè¿›åˆ¶æ–‡ä»¶ä¸­çš„åŠ å¯†å‡½æ•°è¡Œä¸ºæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§ä½†è‡³å…³é‡è¦çš„ä»»åŠ¡ã€‚ä¸å…¸å‹ä»£ç ç›¸æ¯”ï¼ŒåŠ å¯†ç®—æ³•è¡¨ç°å‡ºæ›´é«˜çš„é€»è¾‘å¤æ‚æ€§ï¼Œä½†åœ¨ç—…æ¯’åˆ†æå’Œé—ç•™ä»£ç æ£€æŸ¥ç­‰é¢†åŸŸï¼Œå…¶åˆ†ææ˜¯å¿…ä¸å¯å°‘çš„ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºæ•°æ®æˆ–ç»“æ„æ¨¡å¼åŒ¹é…ï¼Œå¯¼è‡´é€šç”¨æ€§ä¸è¶³ï¼Œä¸”éœ€è¦å¤§é‡æ‰‹åŠ¨æ“ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œåä¸ºFoCï¼Œä»¥åœ¨å‰¥ç¦»çš„äºŒè¿›åˆ¶æ–‡ä»¶ä¸­æ‰¾å‡ºåŠ å¯†å‡½æ•°ã€‚åœ¨FoCä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªäºŒè¿›åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆFoC-BinLLMï¼‰ï¼Œä»¥è‡ªç„¶è¯­è¨€æ€»ç»“åŠ å¯†å‡½æ•°çš„è¯­ä¹‰ã€‚FoC-BinLLMçš„é¢„æµ‹å¯¹å¾®å°çš„å˜åŒ–ï¼ˆå¦‚æ¼æ´è¡¥ä¸ï¼‰å¹¶ä¸æ•æ„Ÿã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥åœ¨FoC-BinLLMä¹‹ä¸Šæ„å»ºäº†ä¸€ä¸ªäºŒè¿›åˆ¶ä»£ç ç›¸ä¼¼æ€§æ¨¡å‹ï¼ˆFoC-Simï¼‰ï¼Œä»¥åˆ›å»ºå¯¹å˜åŒ–æ•æ„Ÿçš„è¡¨ç¤ºï¼Œå¹¶å°†å…¶ç”¨äºåœ¨æ•°æ®åº“ä¸­æ£€ç´¢æœªçŸ¥çš„åŠ å¯†å‡½æ•°çš„ç›¸ä¼¼å®ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŠ å¯†äºŒè¿›åˆ¶æ•°æ®é›†ï¼Œä»¥è¿›è¡Œè¯„ä¼°å¹¶ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚è¿˜è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•æ¥ä¸ºå¹¿æ³›çš„äºŒè¿›åˆ¶å‡½æ•°åˆ›å»ºè¯­ä¹‰æ ‡ç­¾ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒFoC-BinLLMåœ¨ROUGE-Låˆ†æ•°ä¸Šæ¯”ChatGPTé«˜å‡º14.61%ã€‚FoC-Simçš„å¬å›ç‡æ¯”ä¹‹å‰çš„æœ€ä½³æ–¹æ³•é«˜å‡º52%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç—…æ¯’åˆ†æå’Œ1å¤©æ¼æ´æ£€æµ‹æ–¹é¢ä¹Ÿæ˜¾ç¤ºå‡ºå®é™…èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.18403v3">PDF</a> 38 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºFoCçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«å‰¥ç¦»äºŒè¿›åˆ¶æ–‡ä»¶ä¸­çš„åŠ å¯†å‡½æ•°ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šFoC-BinLLMå’ŒFoC-Simã€‚FoC-BinLLMæ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç”¨äºä»¥è‡ªç„¶è¯­è¨€å½¢å¼æ€»ç»“åŠ å¯†å‡½æ•°çš„è¯­ä¹‰ã€‚FoC-Simåˆ™æ˜¯ä¸€ä¸ªåŸºäºFoC-BinLLMæ„å»ºçš„äºŒè¿›åˆ¶ä»£ç ç›¸ä¼¼æ€§æ¨¡å‹ï¼Œç”¨äºåˆ›å»ºå¯¹å˜åŒ–æ•æ„Ÿçš„è¡¨ç¤ºï¼Œå¹¶ç”¨äºæ£€ç´¢æ•°æ®åº“ä¸­æœªçŸ¥çš„åŠ å¯†å‡½æ•°çš„ç›¸ä¼¼å®ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ„å»ºäº†ç”¨äºè¯„ä¼°çš„åŠ å¯†äºŒè¿›åˆ¶æ•°æ®é›†ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨åŒ–æ–¹æ³•ä¸ºå¹¿æ³›çš„äºŒè¿›åˆ¶å‡½æ•°åˆ›å»ºè¯­ä¹‰æ ‡ç­¾ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒFoC-BinLLMåœ¨ROUGE-Låˆ†æ•°ä¸Šè¾ƒChatGPTé«˜å‡º14.6%ï¼ŒFoC-Simçš„å¬å›ç‡è¾ƒä¹‹å‰æœ€ä½³æ–¹æ³•é«˜å‡º52%ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•åœ¨å®é™…ç—…æ¯’åˆ†æå’Œä¸€æ—¥æ¼æ´æ£€æµ‹ä¸­å±•ç°å‡ºå®ç”¨èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ†æå‰¥ç¦»äºŒè¿›åˆ¶æ–‡ä»¶ä¸­çš„åŠ å¯†å‡½æ•°è¡Œä¸ºæ˜¯ä¸€é¡¹é‡è¦ä¸”å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå°¤å…¶åœ¨ç—…æ¯’åˆ†æå’Œé—ç•™ä»£ç æ£€æŸ¥ç­‰é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ•°æ®æˆ–ç»“æ„æ¨¡å¼åŒ¹é…ï¼Œå­˜åœ¨é€šç”¨æ€§ä¸è¶³å’Œæ‰‹å·¥æ“ä½œç¹ççš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶FoCï¼ŒåŒ…æ‹¬FoC-BinLLMå’ŒFoC-Simä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œä»¥è¯†åˆ«äºŒè¿›åˆ¶æ–‡ä»¶ä¸­çš„åŠ å¯†å‡½æ•°ã€‚</li>
<li>FoC-BinLLMæ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ€»ç»“åŠ å¯†å‡½æ•°çš„è‡ªç„¶è¯­è¨€è¯­ä¹‰ï¼Œå¯¹å¾®å°å˜åŒ–å…·æœ‰é¢„æµ‹ä¸æ•æ„Ÿæ€§ã€‚</li>
<li>FoC-Simæ˜¯ä¸€ä¸ªåŸºäºFoC-BinLLMæ„å»ºçš„äºŒè¿›åˆ¶ä»£ç ç›¸ä¼¼æ€§æ¨¡å‹ï¼Œç”¨äºåˆ›å»ºå¯¹å˜åŒ–æ•æ„Ÿçš„è¡¨ç¤ºï¼Œå¹¶æ£€ç´¢æœªçŸ¥åŠ å¯†å‡½æ•°çš„ç›¸ä¼¼å®ç°ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†åŠ å¯†äºŒè¿›åˆ¶æ•°æ®é›†ï¼Œä¸ºé¢†åŸŸå†…çš„è¯„ä¼°å’Œç ”ç©¶æä¾›äº†ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.18403">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3ebc66586a9a105b44da5e04731b5f1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85a3958f3e6c23c1949928fc0d009729.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e2a782c089fed3b00ece0d1393a66cb8.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-29  Auto-SLURP A Benchmark Dataset for Evaluating Multi-Agent Frameworks in   Smart Personal Assistant
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-d8c7bed048e1aaa024c2d92f6d0a1179.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-29  TRACE Back from the Future A Probabilistic Reasoning Approach to   Controllable Language Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
