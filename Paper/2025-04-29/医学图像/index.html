<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-29  RSFR A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor   Cardiac MRI with Semantic-Aware Refinement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0c7c2a2d7f366b6cf266a0da24959227.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-29-更新"><a href="#2025-04-29-更新" class="headerlink" title="2025-04-29 更新"></a>2025-04-29 更新</h1><h2 id="RSFR-A-Coarse-to-Fine-Reconstruction-Framework-for-Diffusion-Tensor-Cardiac-MRI-with-Semantic-Aware-Refinement"><a href="#RSFR-A-Coarse-to-Fine-Reconstruction-Framework-for-Diffusion-Tensor-Cardiac-MRI-with-Semantic-Aware-Refinement" class="headerlink" title="RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor   Cardiac MRI with Semantic-Aware Refinement"></a>RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor   Cardiac MRI with Semantic-Aware Refinement</h2><p><strong>Authors:Jiahao Huang, Fanwen Wang, Pedro F. Ferreira, Haosen Zhang, Yinzhe Wu, Zhifan Gao, Lei Zhu, Angelica I. Aviles-Rivero, Carola-Bibiane Schonlieb, Andrew D. Scott, Zohya Khalique, Maria Dwornik, Ramyah Rajakulasingam, Ranil De Silva, Dudley J. Pennell, Guang Yang, Sonia Nielles-Vallespin</strong></p>
<p>Cardiac diffusion tensor imaging (DTI) offers unique insights into cardiomyocyte arrangements, bridging the gap between microscopic and macroscopic cardiac function. However, its clinical utility is limited by technical challenges, including a low signal-to-noise ratio, aliasing artefacts, and the need for accurate quantitative fidelity. To address these limitations, we introduce RSFR (Reconstruction, Segmentation, Fusion &amp; Refinement), a novel framework for cardiac diffusion-weighted image reconstruction. RSFR employs a coarse-to-fine strategy, leveraging zero-shot semantic priors via the Segment Anything Model and a robust Vision Mamba-based reconstruction backbone. Our framework integrates semantic features effectively to mitigate artefacts and enhance fidelity, achieving state-of-the-art reconstruction quality and accurate DT parameter estimation under high undersampling rates. Extensive experiments and ablation studies demonstrate the superior performance of RSFR compared to existing methods, highlighting its robustness, scalability, and potential for clinical translation in quantitative cardiac DTI. </p>
<blockquote>
<p>心脏扩散张量成像（DTI）为心肌细胞排列提供了独特的见解，架起了微观和宏观心脏功能之间的桥梁。然而，由于其信号噪声比较低、存在混叠伪影以及需要准确的定量保真等技术挑战，其临床应用受到限制。为了克服这些局限性，我们引入了RSFR（重建、分割、融合与细化）——一种用于心脏扩散加权图像重建的新型框架。RSFR采用由粗到细的策略，利用基于Segment Anything模型的零样本语义先验和基于Vision Mamba的稳健重建主干。我们的框架有效地集成了语义特征，以减轻伪影并提高保真度，实现了在高欠采样率下的最先进的重建质量和准确的DT参数估计。广泛的实验和消融研究证明了RSFR相较于现有方法的卓越性能，凸显了其稳健性、可扩展性以及定量心脏DTI临床转化的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18520v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>心脏扩散张量成像（DTI）能够深入了解心肌组织排列，为微观和宏观心脏功能之间的联系提供桥梁。然而，其临床应用受限于技术挑战，如低信噪比、混叠伪影和定量准确性需求。为解决这些问题，我们提出了RSFR（重建、分割、融合与细化）这一新型心脏扩散加权图像重建框架。RSFR采用由粗到细的策略，通过Segment Anything模型和基于Vision Mamba的稳健重建后盾，有效利用语义特征来减少伪影并增强准确性。该框架实现了最先进的重建质量和准确的DT参数估计，在高欠采样率下表现优异。广泛实验和消融研究证明了RSFR相较于现有方法的优越性，凸显了其稳健性、可扩展性和在定量心脏DTI中临床转化的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>心脏扩散张量成像（DTI）可揭示心肌组织排列，为微观与宏观心脏功能研究提供桥梁。</li>
<li>当前临床应用受限于技术挑战，如低信噪比和混叠伪影。</li>
<li>RSFR框架旨在解决这些技术挑战，通过重建、分割、融合与细化过程优化图像质量。</li>
<li>RSFR利用语义特征减少伪影，增强图像准确性。</li>
<li>RSFR框架实现了先进的重建质量和DT参数估计。</li>
<li>大量实验和消融研究证明了RSFR的优越性，凸显其临床转化的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18520">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0657f851d4825bf0b6f566afb90996c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-066cff8f689078a2e1b723c1ab305a3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de0eaf1b663688c7a91737b21ff8e537.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MROP-Modulated-Rank-One-Projections-for-compressive-radio-interferometric-imaging"><a href="#MROP-Modulated-Rank-One-Projections-for-compressive-radio-interferometric-imaging" class="headerlink" title="MROP: Modulated Rank-One Projections for compressive radio   interferometric imaging"></a>MROP: Modulated Rank-One Projections for compressive radio   interferometric imaging</h2><p><strong>Authors:Olivier Leblanc, Chung San Chu, Laurent Jacques, Yves Wiaux</strong></p>
<p>The emerging generation of radio-interferometric (RI) arrays are set to form images of the sky with a new regime of sensitivity and resolution. This implies a significant increase in visibility data volumes, scaling as $\mathcal{O}(Q^{2}B)$ for $Q$ antennas and $B$ short-time integration intervals (or batches), calling for efficient data dimensionality reduction techniques. This paper proposes a new approach to data compression during acquisition, coined modulated rank-one projection (MROP). MROP compresses the $Q\times Q$ batchwise covariance matrix into a smaller number $P$ of random rank-one projections and compresses across time by trading $B$ for a smaller number $M$ of random modulations of the ROP measurement vectors. Firstly, we introduce a dual perspective on the MROP acquisition, which can either be understood as random beamforming, or as a post-correlation compression. Secondly, we analyse the noise statistics of MROPs and demonstrate that the random projections induce a uniform noise level across measurements independently of the visibility-weighting scheme used. Thirdly, we propose a detailed analysis of the memory and computational cost requirements across the data acquisition and image reconstruction stages, with comparison to state-of-the-art dimensionality reduction approaches. Finally, the MROP model is validated in simulation for monochromatic intensity imaging, with comparison to the classical and baseline-dependent averaging (BDA) models, and using the uSARA optimisation algorithm for image formation. An extensive experimental setup is considered, with ground-truth images containing diffuse and faint emission and spanning a wide variety of dynamic ranges, and for a range of $uv$-coverages corresponding to VLA and MeerKAT observation. </p>
<blockquote>
<p>新一代射电干涉测量（RI）阵列将以新的灵敏度和分辨率机制形成天空图像。这意味着可见数据量大幅增加，随着天线数量Q和短时间积分间隔（或批次）B的增加，数据量的增长量级为$\mathcal{O}(Q^{2}B)$，因此需要高效的数据降维技术。本文提出了一种新的数据压缩采集方法，称为调制秩一投影（MROP）。MROP将$Q\times Q$批处理协方差矩阵压缩成较小数量的随机秩一投影P，并通过用较小的调制数M代替时间压缩，对随机调制的ROP测量向量进行压缩。首先，我们对MROP采集进行了双重角度的介绍，可以将其理解为随机波束形成，也可以理解为后关联压缩。其次，我们分析了MROP的噪声统计，并证明随机投影会在测量中引起均匀噪声水平，与使用可见性加权方案无关。再次，我们详细分析了数据采集和图像重建阶段所需的内存和计算成本要求，并与最新的降维方法进行了比较。最后，通过模拟对MROP模型进行了单色强度成像的验证，并与经典模型和基线依赖平均（BDA）模型进行了比较，使用uSARA优化算法进行成像。实验考虑了广泛的设置，包括包含漫射和微弱发射的真实图像以及各种各样的动态范围，以及对应于VLA和MeerKAT观测的一系列uv覆盖。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18446v1">PDF</a> </p>
<p><strong>Summary</strong><br>    新一代射电干涉测量阵列（RI arrays）将形成天空图像，具有新的敏感性和分辨率。随着数据量的增长，需要高效的数据降维技术。本文提出了一种新的数据压缩采集方法——调制秩一投影（MROP）。MROP将QQ批处理协方差矩阵压缩成较小的随机秩一投影，并通过时间交换压缩。本文介绍了MROP采集的双重视角，分析了MROP的噪声统计，并与其他降维方法比较了内存和计算成本要求。最后，通过模拟验证了MROP模型在单色强度成像中的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>新一代RI arrays将提高图像敏感性和分辨率，导致数据量大幅增加。</li>
<li>数据降维技术对于处理大规模数据至关重要。</li>
<li>MROP是一种新的数据压缩采集方法，通过随机秩一投影和调制来压缩数据。</li>
<li>MROP采集具有双重视角：随机波束形成或后关联压缩。</li>
<li>MROP的噪声统计特性被分析，表明随机投影在所有测量中产生均匀噪声水平。</li>
<li>相较于其他降维方法，MROP在内存和计算成本方面具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9ba7ef76613e0125503a51fae144c7af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3032e84d47d054cbb8ce322ff2fba8b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HepatoGEN-Generating-Hepatobiliary-Phase-MRI-with-Perceptual-and-Adversarial-Models"><a href="#HepatoGEN-Generating-Hepatobiliary-Phase-MRI-with-Perceptual-and-Adversarial-Models" class="headerlink" title="HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and   Adversarial Models"></a>HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and   Adversarial Models</h2><p><strong>Authors:Jens Hooge, Gerard Sanroma-Guell, Faidra Stavropoulou, Alexander Ullmann, Gesine Knobloch, Mark Klemens, Carola Schmidt, Sabine Weckbach, Andreas Bolz</strong></p>
<p>Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a crucial role in the detection and characterization of focal liver lesions, with the hepatobiliary phase (HBP) providing essential diagnostic information. However, acquiring HBP images requires prolonged scan times, which may compromise patient comfort and scanner throughput. In this study, we propose a deep learning based approach for synthesizing HBP images from earlier contrast phases (precontrast and transitional) and compare three generative models: a perceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion probabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from diverse clinical settings and introduced a contrast evolution score (CES) to assess training data quality, enhancing model performance. Quantitative evaluation using pixel-wise and perceptual metrics, combined with qualitative assessment through blinded radiologist reviews, showed that pGAN achieved the best quantitative performance but introduced heterogeneous contrast in out-of-distribution cases. In contrast, the U-Net produced consistent liver enhancement with fewer artifacts, while DDPM underperformed due to limited preservation of fine structural details. These findings demonstrate the feasibility of synthetic HBP image generation as a means to reduce scan time without compromising diagnostic utility, highlighting the clinical potential of deep learning for dynamic contrast enhancement in liver MRI. A project demo is available at: <a target="_blank" rel="noopener" href="https://jhooge.github.io/hepatogen">https://jhooge.github.io/hepatogen</a> </p>
<blockquote>
<p>动态增强磁共振成像（DCE-MRI）在检测和识别局部肝脏病变中起着关键作用，其中肝胆相（HBP）提供了关键的诊断信息。然而，获取HBP图像需要长时间的扫描，可能会影响患者的舒适度和扫描器的通过率。本研究提出了一种基于深度学习的方法，用于从早期的对比阶段（预对比和过渡阶段）合成HBP图像，并比较了三种生成模型：感知U-Net、感知GAN（pGAN）和去噪扩散概率模型（DDPM）。我们从不同的临床环境中整理了一个多站点DCE-MRI数据集，并引入了一个对比演化分数（CES）来评估训练数据的质量，以提高模型的性能。利用像素级和感知指标的定量评估，结合通过盲放射科医生审查的定性评估，结果显示pGAN在定量性能上表现最佳，但在非分布案例中引入了异质对比。相比之下，U-Net产生的肝脏增强效果一致且伪影较少，而DDPM由于精细结构细节保存有限而表现不佳。这些发现证明了合成HBP图像生成的可行性，作为一种减少扫描时间而不影响诊断效用的手段，突出了深度学习在肝脏MRI动态对比增强中的临床潜力。项目演示可在：[<a target="_blank" rel="noopener" href="https://jhooge.github.io/hepatogen]">https://jhooge.github.io/hepatogen]</a> 处查看。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18405v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了动态对比增强磁共振成像（DCE-MRI）中肝胆相（HBP）图像合成的新方法，利用深度学习技术从早期对比相合成HBP图像。研究中比较了三种生成模型：感知U-Net、感知GAN（pGAN）和去噪扩散概率模型（DDPM）。通过对比定量评估和放射科医生盲审结果，发现pGAN在定量性能上表现最佳，但在异常情况下引入了异质对比。U-Net在保持肝脏增强一致性方面表现出优势，DDPM则因细节保留不足而表现较差。研究结果表明，合成HBP图像是减少扫描时间而不损失诊断价值的有效手段，突显深度学习在肝脏MRI动态对比增强中的临床潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DCE-MRI中HBP图像对焦点肝病变的检测和特征表征至关重要。</li>
<li>HBP图像的获取需要长时间扫描，可能影响患者舒适度和扫描效率。</li>
<li>深度学习可用于从早期对比阶段合成HBP图像，减少扫描时间。</li>
<li>对比了三种生成模型：感知U-Net、感知GAN（pGAN）和DDPM。</li>
<li>pGAN在定量性能上表现最佳，但在异常情况下引入异质对比。</li>
<li>U-Net在保持肝脏增强一致性方面表现出优势，且较少出现伪影。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18405">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-547cef32d1f32460f8232c19f800711d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9bef3cff7c330ba392a0ee5f6493c04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf76e504ce111178bf9a726ed6afb24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2750d63ee69b4b95686a9dde45922b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ac6191106ca499bf8cd26e7bec987cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24327b92d9ea00cb6f12f199f2069f3a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Multimodal-Deep-Learning-Approach-for-White-Matter-Shape-Prediction-in-Diffusion-MRI-Tractography"><a href="#A-Multimodal-Deep-Learning-Approach-for-White-Matter-Shape-Prediction-in-Diffusion-MRI-Tractography" class="headerlink" title="A Multimodal Deep Learning Approach for White Matter Shape Prediction in   Diffusion MRI Tractography"></a>A Multimodal Deep Learning Approach for White Matter Shape Prediction in   Diffusion MRI Tractography</h2><p><strong>Authors:Yui Lo, Yuqian Chen, Dongnan Liu, Leo Zekelman, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Fan Zhang, Weidong Cai, Lauren J. O’Donnell</strong></p>
<p>Shape measures have emerged as promising descriptors of white matter tractography, offering complementary insights into anatomical variability and associations with cognitive and clinical phenotypes. However, conventional methods for computing shape measures are computationally expensive and time-consuming for large-scale datasets due to reliance on voxel-based representations. We propose Tract2Shape, a novel multimodal deep learning framework that leverages geometric (point cloud) and scalar (tabular) features to predict ten white matter tractography shape measures. To enhance model efficiency, we utilize a dimensionality reduction algorithm for the model to predict five primary shape components. The model is trained and evaluated on two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset. We evaluate the performance of Tract2Shape by training and testing it on the HCP-YA dataset and comparing the results with state-of-the-art models. To further assess its robustness and generalization ability, we also test Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep learning models across all ten shape measures, achieving the highest average Pearson’s r and the lowest nMSE on the HCP-YA dataset. The ablation study shows that both multimodal input and PCA contribute to performance gains. On the unseen testing PPMI dataset, Tract2Shape maintains a high Pearson’s r and low nMSE, demonstrating strong generalizability in cross-dataset evaluation. Tract2Shape enables fast, accurate, and generalizable prediction of white matter shape measures from tractography data, supporting scalable analysis across datasets. This framework lays a promising foundation for future large-scale white matter shape analysis. </p>
<blockquote>
<p>形态测量已作为有前途的白质轨迹描述方法出现，为解剖变异性提供了补充见解，并与认知和临床表型相关联。然而，由于传统计算形态测量的方法依赖于基于体素（voxel-based）的表示，因此在大规模数据集上计算成本高昂且耗时。我们提出了Tract2Shape，这是一种新型的多模式深度学习框架，它利用几何（点云）和标量（表格）特征来预测十个白质轨迹形态测量方法。为了提高模型效率，我们使用降维算法来预测五个主要形态成分。该模型在两个独立获取的数据集上进行训练和评估，即HCP-YA数据集和PPMI数据集。我们通过训练Tract2Shape并在HCP-YA数据集上测试它，并与最先进的模型比较结果来评估其性能。为了进一步评估其稳健性和泛化能力，我们还对未见过的PPMI数据集进行了Tract2Shape测试。Tract2Shape在所有十个形态测量指标上均优于SOTA深度学习模型，在HCP-YA数据集上取得了最高的平均Pearson r值和最低的平均平方误差（nMSE）。消融研究表明，多模式输入和主成分分析（PCA）都有助于性能提升。在未见的测试数据集PPMI上，Tract2Shape保持了较高的Pearson r值和较低的nMSE值，表现出良好的跨数据集评估泛化能力。Tract2Shape能够从轨迹数据快速、准确、通用地预测白质形态测量指标，支持跨数据集的可扩展性分析。这一框架为未来大规模白质形态分析奠定了有前途的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18400v1">PDF</a> 21 pages, 3 figures, 6 tables</p>
<p><strong>Summary</strong><br>     提出一种名为Tract2Shape的多模态深度学习框架，利用几何点云和标量特征预测白质轨迹图形状指标。通过降低维度的方法提升模型效率并预测主要形状成分。该模型在两个独立数据集上的训练和评估表现优异，且在未见数据集上具有良好的泛化能力。Tract2Shape为快速、准确和通用的白质形状指标预测提供了支持，为大规模数据集的白质形状分析提供了前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>白质轨迹图形状指标作为认知和临床现象的重要补充信息。</li>
<li>常规计算形状指标的方法对于大规模数据集存在计算昂贵和时间消耗的问题。</li>
<li>提出Tract2Shape多模态深度学习框架，利用点云和标量特征预测白质轨迹图形状指标。</li>
<li>通过降低维度提高模型效率，预测主要形状成分。</li>
<li>在两个独立数据集上的训练和评估表现优异。</li>
<li>Tract2Shape在未见数据集上具有良好的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18400">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0c7c2a2d7f366b6cf266a0da24959227.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e305238ba42c7dc7b1b0f7787cedf57.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-a-deep-learning-approach-for-classifying-treatment-response-in-glioblastomas"><a href="#Towards-a-deep-learning-approach-for-classifying-treatment-response-in-glioblastomas" class="headerlink" title="Towards a deep learning approach for classifying treatment response in   glioblastomas"></a>Towards a deep learning approach for classifying treatment response in   glioblastomas</h2><p><strong>Authors:Ana Matoso, Catarina Passarinho, Marta P. Loureiro, José Maria Moreira, Patrícia Figueiredo, Rita G. Nunes</strong></p>
<p>Glioblastomas are the most aggressive type of glioma, having a 5-year survival rate of 6.9%. Treatment typically involves surgery, followed by radiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI) scans to monitor disease progression. To assess treatment response, radiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to categorize the tumor into one of four labels based on imaging and clinical features: complete response, partial response, stable disease, and progressive disease. This assessment is very complex and time-consuming. Since deep learning (DL) has been widely used to tackle classification problems, this work aimed to implement the first DL pipeline for the classification of RANO criteria based on two consecutive MRI acquisitions. The models were trained and tested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction of input images, 2) different combinations of modalities, 3) different model architectures, 4) different pretraining tasks, and 5) adding clinical data. The pipeline that achieved the best performance used a Densenet264 considering only T1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR) images as input without any pretraining. A median Balanced Accuracy of 50.96% was achieved. Additionally, explainability methods were applied. Using Saliency Maps, the tumor region was often successfully highlighted. In contrast, Grad-CAM typically failed to highlight the tumor region, with some exceptions observed in the Complete Response and Progressive Disease classes, where it effectively identified the tumor region. These results set a benchmark for future studies on glioblastoma treatment response assessment based on the RANO criteria while emphasizing the heterogeneity of factors that might play a role when assessing the tumor’s response to treatment. </p>
<blockquote>
<p>胶质母细胞瘤是最具侵袭性的胶质瘤类型，5年存活率为6.9%。治疗通常包括手术，然后是放疗和化疗，以及经常进行磁共振成像（MRI）扫描以监测疾病进展。为了评估治疗反应，放射科医生使用神经肿瘤学反应评估（RANO）标准，根据影像学和临床特征将肿瘤分为四种标签：完全反应、部分反应、稳定性疾病和进展性疾病。这种评估非常复杂且耗时。由于深度学习（DL）已广泛应用于解决分类问题，因此，这项工作旨在实施基于两次连续MRI采集的RANO标准分类的第一个DL管道。模型在公开数据集LUMIERE上进行训练和测试。测试了五种方法：1）输入图像的减法，2）不同模态的组合，3）不同的模型架构，4）不同的预训练任务，以及5）添加临床数据。表现最佳的管道仅使用Densenet264，仅考虑T1加权、T2加权和液体衰减反转恢复（FLAIR）图像作为输入，无需任何预训练。取得了50.96%的中位数平衡精度。此外，还应用了可解释性方法。使用显著性图（Saliency Maps），肿瘤区域通常被成功突出显示。相反，Grad-CAM通常未能突出显示肿瘤区域，但在完全反应和进展性疾病类别中观察到一些例外，在这些类别中，它有效地识别了肿瘤区域。这些结果为基于RANO标准的胶质母细胞瘤治疗反应评估的未来研究设定了基准，同时强调了评估肿瘤对治疗反应时可能起作用的因素的异质性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18268v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了胶质母细胞瘤的治疗反应评估。传统评估方法复杂耗时，现采用深度学习技术建立分类模型。最佳模型基于Densenet264架构，使用T1加权、T2加权和FLAIR图像为输入，无需预训练，达到平衡精度为50.96%。利用解释性方法，如显著性图（Saliency Maps），成功突出肿瘤区域。不过，Grad-CAM在多数情况下未能有效标识肿瘤区域，但在完全反应和疾病进展类别中有较好表现。这为基于RANO标准的胶质母细胞瘤治疗反应评估提供了基准，并强调了评估肿瘤反应时可能存在的因素异质性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>胶质母细胞瘤是胶质瘤中最具侵袭性的类型，其五年存活率为6.9%。</li>
<li>治疗通常包括手术、放疗和化疗，以及通过MRI扫描监测疾病进展。</li>
<li>使用RANO标准评估治疗反应涉及复杂的分类问题。</li>
<li>研究采用深度学习技术建立模型，以MRI图像为基础进行RANO标准的分类。</li>
<li>最佳模型使用Densenet264架构，仅考虑T1、T2加权和FLAIR图像作为输入，无需预训练，达到平衡精度为50.96%。</li>
<li>通过解释性方法，如显著性图（Saliency Maps），能够突出肿瘤区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18268">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-da258d68152885ac494ca45a9309992e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-476a19b4c02c9e7bf9f145d1382e764c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-033f995ca453573766e995c7a519d5d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e7e121ad2f411c9ac565468950d6445.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8956dcd16381af8d5f241161e0a753e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Physics-Driven-Neural-Compensation-For-Electrical-Impedance-Tomography"><a href="#Physics-Driven-Neural-Compensation-For-Electrical-Impedance-Tomography" class="headerlink" title="Physics-Driven Neural Compensation For Electrical Impedance Tomography"></a>Physics-Driven Neural Compensation For Electrical Impedance Tomography</h2><p><strong>Authors:Chuyu Wang, Huiting Deng, Dong Liu</strong></p>
<p>Electrical Impedance Tomography (EIT) provides a non-invasive, portable imaging modality with significant potential in medical and industrial applications. Despite its advantages, EIT encounters two primary challenges: the ill-posed nature of its inverse problem and the spatially variable, location-dependent sensitivity distribution. Traditional model-based methods mitigate ill-posedness through regularization but overlook sensitivity variability, while supervised deep learning approaches require extensive training data and lack generalization. Recent developments in neural fields have introduced implicit regularization techniques for image reconstruction, but these methods typically neglect the physical principles underlying EIT, thus limiting their effectiveness. In this study, we propose PhyNC (Physics-driven Neural Compensation), an unsupervised deep learning framework that incorporates the physical principles of EIT. PhyNC addresses both the ill-posed inverse problem and the sensitivity distribution by dynamically allocating neural representational capacity to regions with lower sensitivity, ensuring accurate and balanced conductivity reconstructions. Extensive evaluations on both simulated and experimental data demonstrate that PhyNC outperforms existing methods in terms of detail preservation and artifact resistance, particularly in low-sensitivity regions. Our approach enhances the robustness of EIT reconstructions and provides a flexible framework that can be adapted to other imaging modalities with similar challenges. </p>
<blockquote>
<p>电阻抗断层扫描（EIT）提供了一种非侵入性、便携式的成像方式，在医疗和工业应用中具有巨大潜力。尽管具有优势，但EIT面临两个主要挑战：其反问题的病态性质和空间变化、位置依赖的灵敏度分布。传统基于模型的方法通过正则化来缓解不适定性，但忽略了灵敏度的变化，而监督深度学习的方法需要大量的训练数据且缺乏泛化能力。最近神经网络领域的发展引入了隐式正则化技术用于图像重建，但这些方法通常忽略了EIT背后的物理原理，从而限制了其有效性。本研究中，我们提出了PhyNC（物理驱动神经网络补偿），这是一个结合了EIT物理原理的无监督深度学习框架。PhyNC通过动态分配神经表征容量到灵敏度较低的区域，解决了不适定的反问题和灵敏度分布问题，确保了精确且平衡的导电率重建。对模拟和实验数据的广泛评估表明，PhyNC在细节保留和抗伪影方面优于现有方法，特别是在低灵敏度区域。我们的方法提高了EIT重建的稳健性，并提供了一个灵活的框架，可以适应具有类似挑战的其他成像模式。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18067v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>EIT面临两大挑战：其反问题的病态性和灵敏度分布的空间变化性。本研究提出一种融合物理原理的无监督深度学习框架PhyNC，解决EIT的这两个问题。PhyNC通过动态分配神经网络表征容量来应对低灵敏度区域的需求，确保导电率重建的准确性和平衡性。评估和实验数据表明，PhyNC在细节保留和抗伪影方面优于现有方法，特别是在低灵敏度区域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EIT面临两大挑战：反问题的病态性和灵敏度分布的空间变化性。</li>
<li>传统方法通过正则化解决病态性问题，但忽略灵敏度变化。</li>
<li>深度学习方法需要大量训练数据，并缺乏泛化能力。</li>
<li>最近神经场的发展引入了隐式正则化技术，但忽略了EIT的物理原理。</li>
<li>PhyNC是一种无监督深度学习框架，结合EIT的物理原理。</li>
<li>PhyNC通过动态分配神经网络表征容量，应对低灵敏度区域，确保导电率重建的准确性和平衡性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-07fca6d6a1f4ea7407923e6864254d6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a605a63bc21d33aa1c38145ea7fd1c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-173fa8df783555d04f9e9da8c531b58d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ed6939d700cd994ab588a4f988905a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66c17d7c27908ba31f21bd4fd4234ea9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Federated-Client-tailored-Adapter-for-Medical-Image-Segmentation"><a href="#Federated-Client-tailored-Adapter-for-Medical-Image-Segmentation" class="headerlink" title="Federated Client-tailored Adapter for Medical Image Segmentation"></a>Federated Client-tailored Adapter for Medical Image Segmentation</h2><p><strong>Authors:Guyue Hu, Siyuan Song, Yukun Kang, Zhu Yin, Gangming Zhao, Chenglong Li, Jin Tang</strong></p>
<p>Medical image segmentation in X-ray images is beneficial for computer-aided diagnosis and lesion localization. Existing methods mainly fall into a centralized learning paradigm, which is inapplicable in the practical medical scenario that only has access to distributed data islands. Federated Learning has the potential to offer a distributed solution but struggles with heavy training instability due to client-wise domain heterogeneity (including distribution diversity and class imbalance). In this paper, we propose a novel Federated Client-tailored Adapter (FCA) framework for medical image segmentation, which achieves stable and client-tailored adaptive segmentation without sharing sensitive local data. Specifically, the federated adapter stirs universal knowledge in off-the-shelf medical foundation models to stabilize the federated training process. In addition, we develop two client-tailored federated updating strategies that adaptively decompose the adapter into common and individual components, then globally and independently update the parameter groups associated with common client-invariant and individual client-specific units, respectively. They further stabilize the heterogeneous federated learning process and realize optimal client-tailored instead of sub-optimal global-compromised segmentation models. Extensive experiments on three large-scale datasets demonstrate the effectiveness and superiority of the proposed FCA framework for federated medical segmentation. </p>
<blockquote>
<p>医学图像中的X光图像分割对于计算机辅助诊断和病灶定位非常有益。现有的方法主要遵循集中式学习模式，这在只能访问分布式数据孤岛的实际医疗场景中并不适用。联邦学习（Federated Learning）有潜力提供分布式解决方案，但由于客户端域异构性（包括分布多样性和类别不平衡）而面临训练不稳定的挑战。在本文中，我们提出了一种新型的针对医疗图像分割的联邦客户端定制适配器（FCA）框架，实现了在不共享敏感本地数据的情况下稳定和适应客户端的定制化分割。具体来说，联邦适配器将现成的医学基础模型中的通用知识混合在一起，以稳定联邦训练过程。此外，我们开发了两项针对客户端定制的联邦更新策略，自适应地将适配器分解为通用和个性化组件，然后分别全局和独立地更新与通用客户端不变和个性化客户端相关的参数组。它们进一步稳定了异构的联邦学习过程，实现了针对客户端的最优而非次优全局妥协分割模型。在三个大规模数据集上的广泛实验证明了所提出的FCA框架在医学图像联邦分割中的有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18020v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像分割在X光图像中有助于计算机辅助诊断和病灶定位。现有方法主要采用集中式学习范式，不适用于仅访问分布式数据岛的实际医学场景。联邦学习有潜力提供分布式解决方案，但由于客户端领域异质性（包括分布多样性和类别不平衡）而面临训练不稳定的挑战。本文提出一种用于医学图像分割的新型联邦客户端定制适配器（FCA）框架，可在不共享敏感本地数据的情况下实现稳定且客户定制的适应性分割。具体来说，联邦适配器搅拌现成的医学基础模型中的通用知识以稳定联邦训练过程。此外，我们开发了两种客户定制的联邦更新策略，自适应地将适配器分解为通用和个体组件，然后全局和独立地更新与通用客户端不变和个体客户端特定单元相关的参数组，进一步稳定了异构联邦学习过程，实现了优化的客户定制而非次优的全局妥协分割模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割在X光图像中有助于计算机辅助诊断和病灶定位。</li>
<li>现有方法主要采用集中式学习范式，不适用于分布式数据环境。</li>
<li>联邦学习面临因客户端领域异质性导致的训练不稳定问题。</li>
<li>提出了联邦客户端定制适配器（FCA）框架，实现稳定且客户定制的适应性分割。</li>
<li>联邦适配器利用通用知识稳定联邦训练过程。</li>
<li>开发两种客户定制的联邦更新策略，自适应分解适配器并更新参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.18020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d5a3d55d6083e2699431a396402a623a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b656546ecfbfdc349aad0a0b08dbca62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10216fcb7e8f08646bc4afa64b97784d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42615c12a1aaab45d51dcdb1aa8d2a82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85607ab0518d0d30b0281c94f619257b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29b52789ea917a6c66cf7f04e9d7948f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DCFormer-Efficient-3D-Vision-Language-Modeling-with-Decomposed-Convolutions"><a href="#DCFormer-Efficient-3D-Vision-Language-Modeling-with-Decomposed-Convolutions" class="headerlink" title="DCFormer: Efficient 3D Vision-Language Modeling with Decomposed   Convolutions"></a>DCFormer: Efficient 3D Vision-Language Modeling with Decomposed   Convolutions</h2><p><strong>Authors:Gorkem Can Ates, Yu Xin, Kuang Gong, Wei Shao</strong></p>
<p>Vision-language models (VLMs) have been widely applied to 2D medical image analysis due to their ability to align visual and textual representations. However, extending VLMs to 3D imaging remains computationally challenging. Existing 3D VLMs often rely on Vision Transformers (ViTs), which are computationally expensive due to the quadratic complexity of self-attention, or on 3D convolutions, which require large numbers of parameters and FLOPs as kernel size increases. We introduce DCFormer, an efficient 3D image encoder that factorizes 3D convolutions into three parallel 1D convolutions along the depth, height, and width dimensions. This design preserves spatial information while significantly reducing computational cost. Integrated into a CLIP-based vision-language framework, DCFormer is trained and evaluated on CT-RATE, a dataset of 50,188 paired 3D chest CT volumes and radiology reports. In zero-shot and fine-tuned detection of 18 pathologies, as well as in image-text retrieval tasks, DCFormer consistently outperforms state-of-the-art 3D vision encoders, including CT-ViT, ViT, ConvNeXt, PoolFormer, and TransUNet. These results highlight DCFormer’s potential for scalable, clinically deployable 3D medical VLMs. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/mirthAI/DCFormer">https://github.com/mirthAI/DCFormer</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）由于其视觉和文本表示对齐的能力，已广泛应用于二维医学图像分析。然而，将VLMs扩展到三维成像在计算上仍然具有挑战性。现有的三维VLMs通常依赖于视觉转换器（ViTs），由于自注意力的二次复杂性，其计算成本高昂，或者依赖于三维卷积，随着内核大小的增加，需要大量的参数和浮点运算（FLOPs）。我们引入了DCFormer，这是一种高效的三维图像编码器，它将三维卷积分解为沿深度、高度和宽度方向的三个并行一维卷积。这种设计保留了空间信息，同时大大降低了计算成本。DCFormer被集成到一个基于CLIP的视觉语言框架中，并在CT-RATE数据集上进行训练和评估，该数据集包含50,188对三维胸部CT体积和放射学报告。在零样本和微调检测18种病理情况以及图像文本检索任务中，DCFormer持续优于最新的三维视觉编码器，包括CT-ViT、ViT、ConvNeXt、PoolFormer和TransUNet。这些结果突出了DCFormer在可扩展、可部署的临床三维医学VLMs中的潜力。我们的代码位于：<a target="_blank" rel="noopener" href="https://github.com/mirthAI/DCFormer%E3%80%82">https://github.com/mirthAI/DCFormer。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05091v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于视觉-语言模型（VLMs）对二维医学影像分析的广泛应用及其强大的视觉与文本对齐能力，人们正试图将其扩展到三维成像领域。然而，由于计算上的挑战，现有三维VLMs通常采用视觉转换器（ViTs），其自注意力的二次复杂性导致计算量大；或是采用三维卷积，随着内核大小增加，其所需的参数和浮点运算量也急剧增长。本文提出DCFormer，一种高效的三维图像编码器，将三维卷积分解为沿深度、高度和宽度方向的三个并行一维卷积。这种设计在保留空间信息的同时显著降低了计算成本。DCFormer被集成到一个基于CLIP的视野语言框架中，并在CT-RATE数据集（包含50,188对三维胸部CT体积和放射学报告）上进行训练和评估。在零样本和微调检测18种病理情况以及图像文本检索任务中，DCFormer始终优于最新的三维视觉编码器，包括CT-ViT、ViT、ConvNeXt、PoolFormer和TransUNet。这些结果突显了DCFormer在可扩展的临床部署三维医学VLMs方面的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>VLMs已广泛应用于二维医学影像分析，但扩展到三维成像仍存在计算挑战。</li>
<li>现有三维VLMs多采用ViTs或三维卷积，前者自注意力机制计算量大，后者参数和计算量大。</li>
<li>DCFormer是一种高效的三维图像编码器，通过分解三维卷积以降低计算成本并保留空间信息。</li>
<li>DCFormer在CLIP基础上被集成到视野语言框架中，并在大规模CT-RATE数据集上进行训练和评估。</li>
<li>在零样本学习和微调检测病理情况，以及图像文本检索任务中，DCFormer表现优于其他先进的三维视觉编码器。</li>
<li>DCFormer具有潜力成为可扩展的、可临床部署的三维医学VLMs解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05091">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0173403a8b9f8356896654b82ef9bc45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a46abe86d41b8d0e78204771e3cd65d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e498c7700b4337837aae0345363f49a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b297226edd815280e93e0610b483807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96bfd104c3d3841e529821b9c83f15a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b54cd4e09288a221ff50c3f742dbb572.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86a13eb956e6aa9358b755623906e2ee.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Synchronous-Memorizability-and-Generalizability-with-Site-Modulated-Diffusion-Replay-for-Cross-Site-Continual-Segmentation"><a href="#Towards-Synchronous-Memorizability-and-Generalizability-with-Site-Modulated-Diffusion-Replay-for-Cross-Site-Continual-Segmentation" class="headerlink" title="Towards Synchronous Memorizability and Generalizability with   Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation"></a>Towards Synchronous Memorizability and Generalizability with   Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation</h2><p><strong>Authors:Dunyuan Xu, Xi Wang, Jingyang Zhang, Pheng-Ann Heng</strong></p>
<p>The ability to learn sequentially from different data sites is crucial for a deep network in solving practical medical image diagnosis problems due to privacy restrictions and storage limitations. However, adapting on incoming site leads to catastrophic forgetting on past sites and decreases generalizablity on unseen sites. Existing Continual Learning (CL) and Domain Generalization (DG) methods have been proposed to solve these two challenges respectively, but none of them can address both simultaneously. Recognizing this limitation, this paper proposes a novel training paradigm, learning towards Synchronous Memorizability and Generalizability (SMG-Learning). To achieve this, we create the orientational gradient alignment to ensure memorizability on previous sites, and arbitrary gradient alignment to enhance generalizability on unseen sites. This approach is named as Parallel Gradient Alignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives using the first-order Taylor expansion to reduce computational cost of aligning gradients. Considering that performing gradient alignments, especially for previous sites, is not feasible due to the privacy constraints, we design a Site-Modulated Diffusion (SMD) model to generate images with site-specific learnable prompts, replaying images have similar data distributions as previous sites. We evaluate our method on two medical image segmentation tasks, where data from different sites arrive sequentially. Experimental results show that our method efficiently enhances both memorizability and generalizablity better than other state-of-the-art methods, delivering satisfactory performance across all sites. Our code will be available at: <a target="_blank" rel="noopener" href="https://github.com/dyxu-cuhkcse/SMG-Learning">https://github.com/dyxu-cuhkcse/SMG-Learning</a>. </p>
<blockquote>
<p>能够从不同的数据站点进行顺序学习对于深度网络解决实际的医学图像诊断问题至关重要，因为存在隐私限制和存储限制。然而，适应新站点会导致对过去站点的灾难性遗忘，并降低对未见站点的泛化能力。现有的持续学习（CL）和域泛化（DG）方法分别被提出来解决这两个挑战，但没有任何一种方法可以同时解决这两个问题。本文认识到了这一局限性，提出了一种新的训练范式，即学习面向同步记忆能力和泛化能力（SMG-Learning）。为实现这一目标，我们创建方向梯度对齐以确保对过去站点的记忆能力，并创建任意梯度对齐以增强对未见站点的泛化能力。这种方法被称为并行梯度对齐（PGA）。此外，我们将PGA近似为双元目标使用一阶泰勒展开，以减少梯度对齐的计算成本。考虑到进行梯度对齐，尤其是对以前的站点，由于隐私约束而不可行，我们设计了一个站点调制扩散（SMD）模型，以生成具有站点特定可学习提示的图像，回放图像的 数据分布与以前的站点相似。我们在两个医学图像分割任务上评估了我们的方法，其中来自不同站点的数据会顺序到达。实验结果表明，我们的方法有效地增强了记忆能力和泛化能力，比其他最先进的方法表现更好，并在所有站点上表现出令人满意的性能。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/dyxu-cuhkcse/SMG-Learning">https://github.com/dyxu-cuhkcse/SMG-Learning</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18037v3">PDF</a> This paper is not proper to be published on arXiv, since we think   some method are quite similar with one other paper</p>
<p><strong>摘要</strong></p>
<p>这篇论文针对因隐私限制和存储限制导致的医学图像诊断问题，提出了一种新的训练范式——同步记忆性和泛化性学习（SMG-Learning）。为解决这个问题，创建了方向梯度对齐以确保对之前站点的记忆能力，任意梯度对齐以增强对未见站点的泛化能力。这种方法被称为并行梯度对齐（PGA）。此外，通过将PGA近似为双重元目标使用一阶泰勒展开，减少了梯度对齐的计算成本。考虑到梯度对齐特别是针对之前站点不可行是由于隐私约束，设计了一种名为Site-Modulated Diffusion（SMD）的模型来生成具有特定学习提示的图像回放，这些图像具有与之前站点相似的数据分布。在两项医学图像分割任务上评估了该方法，不同站点的数据按顺序到达。实验结果表明，该方法在记忆性和泛化性方面均优于其他最新方法，在所有站点上均表现出令人满意的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文强调了从多个数据站点顺序学习对于解决因隐私和存储限制导致的医学图像诊断问题的深度网络的重要性。</li>
<li>存在持续学习（CL）和域泛化（DG）方法分别应对顺序学习和泛化挑战，但没有一种方法可以同时解决这两个问题。</li>
<li>论文提出了一种新的训练范式——同步记忆性和泛化性学习（SMG-Learning）来解决上述问题。</li>
<li>通过创建方向梯度对齐和任意梯度对齐来提升记忆性和泛化性，提出了并行梯度对齐（PGA）方法。</li>
<li>使用一阶泰勒展开将PGA近似为双重元目标以降低计算成本。</li>
<li>考虑隐私约束，设计了一种名为Site-Modulated Diffusion（SMD）的模型来生成具有特定学习提示的图像回放。</li>
<li>在医学图像分割任务上的实验结果表明，该方法在记忆性和泛化性方面表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.18037">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9eb07fa660a2a1c0a519395ce796a17b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d89db0cf234eb6597cf1b4dd7c956fdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64c7a8e2544aef3ba38e54f42dd6002b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06a94f68e8019bc8288835cd50632db8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-29/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a156ef5944afd968a789ed1ad5fc32cd.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-04-29  Generative Induction of Dialogue Task Schemas with Streaming Refinement   and Simulated Interactions
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-29/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-48564c5809f4b7117d483ec581a40f88.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-29  Optimizing Multi-Round Enhanced Training in Diffusion Models for   Improved Preference Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">20064.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
