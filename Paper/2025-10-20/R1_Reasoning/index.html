<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-20  Multimodal Retrieval-Augmented Generation with Large Language Models for   Medical VQA">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-5c6664a5421551d43893bc3404c68c9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896176&auth_key=1760896176-0-0-79689baab1883fa68e3ea62977776882&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-20-æ›´æ–°"><a href="#2025-10-20-æ›´æ–°" class="headerlink" title="2025-10-20 æ›´æ–°"></a>2025-10-20 æ›´æ–°</h1><h2 id="Multimodal-Retrieval-Augmented-Generation-with-Large-Language-Models-for-Medical-VQA"><a href="#Multimodal-Retrieval-Augmented-Generation-with-Large-Language-Models-for-Medical-VQA" class="headerlink" title="Multimodal Retrieval-Augmented Generation with Large Language Models for   Medical VQA"></a>Multimodal Retrieval-Augmented Generation with Large Language Models for   Medical VQA</h2><p><strong>Authors:A H M Rezaul Karim, Ozlem Uzuner</strong></p>
<p>Medical Visual Question Answering (MedVQA) enables natural language queries over medical images to support clinical decision-making and patient care. The MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to generate free-text responses and structured wound attributes from images and patient queries. We present the MasonNLP system, which employs a general-domain, instruction-tuned large language model with a retrieval-augmented generation (RAG) framework that incorporates textual and visual examples from in-domain data. This approach grounds outputs in clinically relevant exemplars, improving reasoning, schema adherence, and response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our best-performing system ranked 3rd among 19 teams and 51 submissions with an average score of 41.37%, demonstrating that lightweight RAG with general-purpose LLMs â€“ a minimal inference-time layer that adds a few relevant exemplars via simple indexing and fusion, with no extra training or complex re-ranking â€“ provides a simple and effective baseline for multimodal clinical NLP tasks. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMedVQAï¼‰æ”¯æŒé€šè¿‡åŒ»ç–—å›¾åƒè¿›è¡Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œä»¥è¾…åŠ©ä¸´åºŠå†³ç­–å’Œç—…äººæŠ¤ç†ã€‚MEDIQA-WV 2025å…±äº«ä»»åŠ¡é’ˆå¯¹ä¼¤å£æŠ¤ç†é—®ç­”ï¼ˆVQAï¼‰ï¼Œè¦æ±‚ç³»ç»Ÿæ ¹æ®å›¾åƒå’Œæ‚£è€…æŸ¥è¯¢ç”Ÿæˆè‡ªç”±æ–‡æœ¬å›ç­”å’Œç»“æ„åŒ–ä¼¤å£å±æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†MasonNLPç³»ç»Ÿï¼Œå®ƒé‡‡ç”¨é€šç”¨æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é¢†åŸŸå†…çš„æ–‡æœ¬å’Œè§†è§‰ç¤ºä¾‹ã€‚è¿™ç§æ–¹æ³•ä»¥ä¸´åºŠç›¸å…³çš„èŒƒä¾‹ä¸ºåŸºç¡€ï¼Œæé«˜äº†æ¨ç†ã€æ¨¡å¼éµå¾ªå’Œå“åº”è´¨é‡ï¼Œæ¶‰åŠdBLEUã€ROUGEã€BERTScoreå’ŒåŸºäºå¤§å‹æ¨¡å‹çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„ç³»ç»Ÿåœ¨19æ”¯é˜Ÿä¼å’Œ51æ¬¡æäº¤ä¸­æ’åç¬¬3ï¼Œå¹³å‡å¾—åˆ†ä¸º41.37%ï¼Œè¿™è¡¨æ˜å¸¦æœ‰é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„è½»é‡åŒ–RAGâ€”â€”ä¸€ä¸ªæœ€å°æ¨ç†æ—¶é—´å±‚ï¼Œé€šè¿‡ç®€å•ç´¢å¼•å’Œèåˆæ·»åŠ å°‘é‡ç›¸å…³èŒƒä¾‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚æ’åºâ€”â€”ä¸ºå¤šæ¨¡æ€ä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æä¾›äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13856v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMedVQAï¼‰èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ”¯æŒä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†ã€‚MEDIQA-WV 2025å…±äº«ä»»åŠ¡å…³æ³¨ä¼¤å£æŠ¤ç†é¢†åŸŸçš„é—®ç­”ç³»ç»Ÿï¼Œè¦æ±‚ç³»ç»Ÿæ ¹æ®å›¾åƒå’Œæ‚£è€…æŸ¥è¯¢ç”Ÿæˆè‡ªç”±æ–‡æœ¬å›ç­”å’Œç»“æ„åŒ–ä¼¤å£å±æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†MasonNLPç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨é€šç”¨æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆé¢†åŸŸå†…çš„æ–‡æœ¬å’Œè§†è§‰ç¤ºä¾‹ï¼Œä»¥æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ä¸ºåŸºç¡€ï¼Œè¾“å‡ºä¸ä¸´åºŠç›¸å…³çš„èŒƒä¾‹ï¼Œæé«˜æ¨ç†ã€æ¨¡å¼éµå®ˆå’Œå“åº”è´¨é‡ã€‚MasonNLPç³»ç»Ÿåœ¨å¤šé¡¹æŒ‡æ ‡ä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œåœ¨æœ€ä½³æ¨¡å‹è¯„ä¼°ä¸‹æ’åç¬¬3ã€‚è¿™ä¸€ç ”ç©¶è¯æ˜é‡‡ç”¨é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè½»é‡åŒ–å¢å¼ºç”ŸæˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¤æ‚æ’åºçš„æƒ…å†µä¸‹ï¼Œä¸ºä¸´åºŠè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡æä¾›ç®€å•æœ‰æ•ˆçš„åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MedVQAæŠ€æœ¯é€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ”¯æŒåŒ»ç–—å›¾åƒçš„ä¸´åºŠå†³ç­–å’Œæ‚£è€…æŠ¤ç†ã€‚</li>
<li>MEDIQA-WV 2025å…±äº«ä»»åŠ¡é›†ä¸­åœ¨ä¼¤å£æŠ¤ç†é—®ç­”ç³»ç»Ÿä¸Šã€‚</li>
<li>MasonNLPç³»ç»Ÿä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>ç³»ç»Ÿç»“åˆäº†é¢†åŸŸå†…çš„æ–‡æœ¬å’Œè§†è§‰ç¤ºä¾‹æ¥ä¼˜åŒ–è¾“å‡ºï¼Œä½¿ä¹‹æ›´è´´è¿‘ä¸´åºŠå®é™…æƒ…å†µã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨å¤šé¡¹è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†æ–¹æ³•çš„å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-735b224148c3c35790efdc2cf9775457~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895864&auth_key=1760895864-0-0-7a3565b0b2ff0241f23e4158e6d3245e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-304b3e342090a6d0ac7c6d60bf46ccd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895871&auth_key=1760895871-0-0-e0654eee23815aa001396f39d582be22&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f7748c423639ab014503946895919ec3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895878&auth_key=1760895878-0-0-a26f343552f4cc9e0c7cf26b17564a58&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3e7fc0671334d2b02fb29fa174f48090~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895884&auth_key=1760895884-0-0-51dd0a8493f533f5630b1b8de5bed980&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adaptive-Selection-of-Symbolic-Languages-for-Improving-LLM-Logical-Reasoning"><a href="#Adaptive-Selection-of-Symbolic-Languages-for-Improving-LLM-Logical-Reasoning" class="headerlink" title="Adaptive Selection of Symbolic Languages for Improving LLM Logical   Reasoning"></a>Adaptive Selection of Symbolic Languages for Improving LLM Logical   Reasoning</h2><p><strong>Authors:Xiangyu Wang, Haocheng Yang, Fengxiang Cheng, Fenrong Liu</strong></p>
<p>Large Language Models (LLMs) still struggle with complex logical reasoning. While previous works achieve remarkable improvements, their performance is highly dependent on the correctness of translating natural language (NL) problems into a symbolic language (SL). Though numerous works focusing on improving this translation accuracy, they only consider the similarity between the meaning of SL and NL, overlooking another crucial influencing factor, the selection of the target SL type itself. For example, first-order logic language specializes in logical reasoning with categorical syllogisms and complex quantifiers, while Boolean satisfiability formalism excels at representing constraint satisfaction like partial problems. To our knowledge, this is the first paper to claim and verify that different NL logical reasoning problem corresponds to different optimal SL formalization for translation. Based on this, we propose a methods to improve the logical reasoning performance of LLMs by adaptively selecting the most suitable SL for each problem prior to translation. Specifically, we leverage LLMs to select the target SL among first-order logic, logic programming and Boolean satisfiability and then translate the problem in NL to target SL expressions as well as employ the corresponding logical solver to derive the final answer. Experimental results on benchmarks show that our adaptive selection method significantly outperforms translating all into single SL and randomly selecting the SL. On a mixed dataset of these benchmarks, our approach achieves 96% accuracy, which improving performance by 25% compared to the second highest accuracy from the first-order logic translation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚é€»è¾‘æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œå®ƒä»¬çš„æ€§èƒ½é«˜åº¦ä¾èµ–äºå°†è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰é—®é¢˜æ­£ç¡®ç¿»è¯‘ä¸ºç›®æ ‡ç¬¦å·è¯­è¨€ï¼ˆSLï¼‰ã€‚å°½ç®¡è®¸å¤šç ”ç©¶è‡´åŠ›äºæé«˜è¿™ç§ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬åªè€ƒè™‘äº†ç¬¦å·è¯­è¨€ä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè€Œå¿½è§†äº†å¦ä¸€ä¸ªå…³é”®å½±å“å› ç´ ï¼Œå³ç›®æ ‡ç¬¦å·è¯­è¨€ç±»å‹æœ¬èº«çš„é€‰å–ã€‚ä¾‹å¦‚ï¼Œä¸€é˜¶é€»è¾‘è¯­è¨€æ“…é•¿äºé€»è¾‘æ¨ç†ï¼ŒåŒ…æ‹¬åˆ†ç±»å½’çº³å’Œå¤æ‚é‡è¯ï¼Œè€Œå¸ƒå°”å¯æ»¡è¶³æ€§å½¢å¼åŒ–åˆ™æ“…é•¿äºè¡¨ç¤ºçº¦æŸæ»¡è¶³é—®é¢˜ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡å£°ç§°å¹¶éªŒè¯ä¸åŒçš„è‡ªç„¶è¯­è¨€é€»è¾‘æ¨ç†é—®é¢˜å¯¹åº”äºä¸åŒçš„æœ€ä½³ç¬¦å·è¯­è¨€å½¢å¼åŒ–çš„è®ºæ–‡ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©æ¯ä¸ªé—®é¢˜æœ€åˆé€‚çš„ç¬¦å·è¯­è¨€æ¥è¿›è¡Œç¿»è¯‘ï¼Œä»¥æé«˜LLMçš„é€»è¾‘æ¨ç†æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é—®é¢˜ç¡®å®šç›®æ ‡ç¬¦å·è¯­è¨€åç¿»è¯‘è‡³ç¬¬ä¸€é˜¶é€»è¾‘è¯­è¨€ã€é€»è¾‘ç¼–ç¨‹å’Œå¸ƒå°”å¯æ»¡è¶³æ€§é—´ï¼Œå¹¶ä½¿ç”¨ç›¸åº”çš„é€»è¾‘æ±‚è§£å™¨æ¨å¯¼å‡ºæœ€ç»ˆç­”æ¡ˆã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è‡ªé€‚åº”é€‰æ‹©æ–¹æ³•æ˜¾è‘—ä¼˜äºå°†æ‰€æœ‰é—®é¢˜éƒ½ç¿»è¯‘æˆå•ä¸€ç¬¦å·è¯­è¨€å’Œéšæœºé€‰æ‹©ç¬¦å·è¯­è¨€çš„æ–¹æ³•ã€‚åœ¨è¿™äº›åŸºå‡†æµ‹è¯•çš„æ··åˆæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºç¬¬ä¸€é˜¶é€»è¾‘ç¿»è¯‘çš„ç¬¬äºŒé«˜å‡†ç¡®ç‡æé«˜äº†25%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10703v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚çš„é€»è¾‘æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºå°†è‡ªç„¶è¯­è¨€é—®é¢˜æ­£ç¡®ç¿»è¯‘ä¸ºç¬¦å·è¯­è¨€çš„èƒ½åŠ›ã€‚è®¸å¤šç ”ç©¶é›†ä¸­åœ¨æé«˜è¿™ç§ç¿»è¯‘çš„å‡†ç¡®æ€§ä¸Šï¼Œä½†å¾€å¾€å¿½è§†äº†ç›®æ ‡ç¬¦å·è¯­è¨€ç±»å‹æœ¬èº«çš„é€‰æ‹©ä¹Ÿæ˜¯å…³é”®å½±å“å› ç´ ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºå¹¶éªŒè¯äº†ä¸åŒçš„è‡ªç„¶è¯­è¨€é€»è¾‘æ¨ç†é—®é¢˜å¯¹åº”ä¸åŒçš„æœ€ä½³ç¬¦å·è¯­è¨€å½¢å¼åŒ–ç¿»è¯‘ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è‡ªé€‚åº”é€‰æ‹©æ¯ä¸ªé—®é¢˜æœ€åˆé€‚çš„ç¬¦å·è¯­è¨€è¿›è¡Œç¿»è¯‘ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„é€»è¾‘æ¨ç†æ€§èƒ½ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸å°†æ‰€æœ‰é—®é¢˜ç¿»è¯‘æˆå•ä¸€çš„ç¬¦å·è¯­è¨€ä»¥åŠéšæœºé€‰æ‹©ç¬¦å·è¯­è¨€çš„æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„è‡ªé€‚åº”é€‰æ‹©æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ··åˆæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºæ­¤å‰æœ€ä½³çš„ä¸€é˜¶é€»è¾‘ç¿»è¯‘æ–¹æ³•æå‡äº†25%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é€»è¾‘æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è‡ªç„¶è¯­è¨€é—®é¢˜æ­£ç¡®ç¿»è¯‘æˆç¬¦å·è¯­è¨€å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç¬¦å·è¯­è¨€ç±»å‹é€‰æ‹©å¯¹é€»è¾‘æ¨ç†æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºå¹¶éªŒè¯äº†ä¸åŒçš„è‡ªç„¶è¯­è¨€é€»è¾‘é—®é¢˜å¯¹åº”ä¸åŒçš„æœ€ä½³ç¬¦å·è¯­è¨€å½¢å¼åŒ–ç¿»è¯‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é€‰æ‹©æœ€é€‚åˆçš„ç¬¦å·è¯­è¨€è¿›è¡Œç¿»è¯‘çš„æ–¹æ³•ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè‡ªé€‚åº”é€‰æ‹©æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d1946bb0fe1288386539750f4f269222~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895892&auth_key=1760895892-0-0-ff022797233e36f4fa6d11c40721fb15&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff3dbb2c1a1104803da390160bb35ad2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895899&auth_key=1760895899-0-0-6d47f3c9a429da51cfa62351024c9060&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c1208fbe41d77b63060bf51bf7a10c17~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895906&auth_key=1760895906-0-0-66e0a7bd9234167227b05f758a96013e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning"><a href="#VR-Thinker-Boosting-Video-Reward-Models-through-Thinking-with-Image-Reasoning" class="headerlink" title="VR-Thinker: Boosting Video Reward Models through Thinking-with-Image   Reasoning"></a>VR-Thinker: Boosting Video Reward Models through Thinking-with-Image   Reasoning</h2><p><strong>Authors:Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, Jiaheng Liu</strong></p>
<p>Recent advancements in multimodal reward models (RMs) have substantially improved post-training for visual generative models. However, current RMs face inherent limitations: (1) visual inputs consume large context budgets, forcing fewer frames and causing loss of fine-grained details; and (2) all visual information is packed into the initial prompt, exacerbating hallucination and forgetting during chain-of-thought reasoning. To overcome these issues, we introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework that equips the RM with visual reasoning operations (e.g., select frame) and a configurable visual memory window. This allows the RM to actively acquire and update visual evidence within context limits, improving reasoning fidelity and reliability. We activate visual reasoning via a reinforcement fine-tuning pipeline: (i) Cold Start with curated visual chain-of-thought data to distill basic reasoning skills and operation formatting; (ii) select samples whose per-dimension and overall judgments are all correct, then conduct Rejection sampling Fine-Tuning on these high-quality traces to further enhance reasoning; and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen reasoning. Our approach delivers state-of-the-art accuracy among open-source models on video preference benchmarks, especially for longer videos: a 7B VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6% on MJ-Bench-Video. These results validate the effectiveness and promise of thinking-with-image multimodal reward modeling. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰çš„è¿›å±•ä¸ºè§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒåè¡¨ç°å¸¦æ¥äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œå½“å‰çš„RMså­˜åœ¨ä¸€äº›å›ºæœ‰çš„å±€é™æ€§ï¼šï¼ˆ1ï¼‰è§†è§‰è¾“å…¥éœ€è¦å¤§é‡ä¸Šä¸‹æ–‡é¢„ç®—ï¼Œå¯¼è‡´æ¡†æ¶æ•°é‡å‡å°‘å¹¶é€ æˆç²¾ç»†ç»†èŠ‚çš„æŸå¤±ï¼›ï¼ˆ2ï¼‰æ‰€æœ‰è§†è§‰ä¿¡æ¯éƒ½è¢«æ‰“åŒ…åˆ°åˆå§‹æç¤ºä¸­ï¼ŒåŠ å‰§äº†æ€ç»´é“¾æ¨ç†è¿‡ç¨‹ä¸­çš„å¹»è§‰å’Œé—å¿˜ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VideoReward Thinkerï¼ˆVR-Thinkerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸å›¾åƒæ€è€ƒæ¡†æ¶ç›¸ç»“åˆçš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ã€‚å®ƒé…å¤‡äº†è§†è§‰æ¨ç†æ“ä½œï¼ˆä¾‹å¦‚é€‰æ‹©æ¡†æ¶ï¼‰å’Œå¯é…ç½®è§†è§‰è®°å¿†çª—å£ã€‚è¿™å…è®¸RMåœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…ä¸»åŠ¨è·å–å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼ºåŒ–å¾®è°ƒç®¡é“æ¿€æ´»è§†è§‰æ¨ç†ï¼šï¼ˆiï¼‰ä½¿ç”¨ç²¾é€‰çš„è§†è§‰æ€ç»´é“¾æ•°æ®å¯åŠ¨ï¼Œä»¥æç‚¼åŸºæœ¬çš„æ¨ç†æŠ€èƒ½å’Œæ“ä½œæ ¼å¼åŒ–ï¼›ï¼ˆiiï¼‰é€‰æ‹©æ¯ä¸ªç»´åº¦å’Œæ•´ä½“åˆ¤æ–­éƒ½æ­£ç¡®çš„æ ·æœ¬ï¼Œç„¶åå¯¹è¿™äº›é«˜è´¨é‡çš„è½¨è¿¹è¿›è¡Œæ‹’ç»é‡‡æ ·å¾®è°ƒï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨ç†èƒ½åŠ›ï¼›ï¼ˆiiiï¼‰åº”ç”¨ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†é¢‘åå¥½åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¼€æºæ¨¡å‹ä¸­çš„æœ€æ–°å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¾ƒé•¿çš„è§†é¢‘ï¼šä¸€ä¸ªè§„æ¨¡ä¸º7Bçš„VR-Thinkeråœ¨VideoGen Rewardä¸Šå®ç°äº†80.5%ï¼Œåœ¨GenAI-Benchä¸Šå®ç°äº†82.3%ï¼Œåœ¨MJ-Bench-Videoä¸Šå®ç°äº†75.6%ã€‚è¿™äº›ç»“æœéªŒè¯äº†ä¸å›¾åƒæ€è€ƒç›¸ç»“åˆçš„å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§å’Œå‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10518v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰ç”Ÿæˆæ¨¡å‹çš„åè®­ç»ƒæ”¹è¿›ä¸­é‡åˆ°çš„æŒ‘æˆ˜ï¼Œå¦‚è§†è§‰è¾“å…¥æ¶ˆè€—å¤§é‡ä¸Šä¸‹æ–‡é¢„ç®—å’Œè§†è§‰ä¿¡æ¯è¿‡äºé›†ä¸­å¯¼è‡´çš„å¹»è§‰å’Œé—å¿˜é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†VideoReward Thinkerï¼ˆVR-Thinkerï¼‰æ¡†æ¶ï¼Œé…å¤‡äº†è§†è§‰æ¨ç†æ“ä½œï¼ŒåŒ…æ‹¬é€‰æ‹©å¸§ç­‰åŠŸèƒ½ã€‚æ­¤æ¡†æ¶å…è®¸æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…ä¸»åŠ¨è·å–å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é€šè¿‡å¼ºåŒ–å¾®è°ƒç®¡é“æ¿€æ´»è§†è§‰æ¨ç†ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨é˜¶æ®µã€æ‹’ç»é‡‡æ ·å¾®è°ƒä»¥åŠç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç­‰æ­¥éª¤ã€‚åœ¨è§†é¢‘åå¥½åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•çš„å‡†ç¡®æ€§è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿è§†é¢‘æ—¶è¡¨ç°ä¼˜å¼‚ã€‚è¿™ä¸€ç»“æœéªŒè¯äº†æ€è€ƒå‹å›¾åƒå¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡çš„æœ‰æ•ˆæ€§å’Œå‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨è§†è§‰ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒåæ”¹è¿›é¢ä¸´ä¸¤å¤§é—®é¢˜ï¼šä¸€æ˜¯è§†è§‰è¾“å…¥å ç”¨å¤§é‡ä¸Šä¸‹æ–‡é¢„ç®—å¯¼è‡´ç¼ºå°‘ç²¾ç»†ç²’åº¦çš„ç»†èŠ‚ï¼ŒäºŒæ˜¯æ‰€æœ‰è§†è§‰ä¿¡æ¯éƒ½é›†ä¸­äºåˆå§‹æç¤ºå¯¼è‡´å¹»è§‰å’Œé—å¿˜é—®é¢˜ã€‚</li>
<li>VR-Thinkeræ¡†æ¶é€šè¿‡é…å¤‡è§†è§‰æ¨ç†æ“ä½œï¼ˆå¦‚é€‰æ‹©å¸§ï¼‰å’Œå¯é…ç½®çš„è§†è§‰è®°å¿†çª—å£æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®ƒå…è®¸æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡é™åˆ¶å†…ä¸»åŠ¨è·å–å’Œæ›´æ–°è§†è§‰è¯æ®ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>VR-Thinkeræ¡†æ¶é‡‡ç”¨å¼ºåŒ–å¾®è°ƒç®¡é“æ¥æ¿€æ´»è§†è§‰æ¨ç†ï¼ŒåŒ…æ‹¬å†·å¯åŠ¨é˜¶æ®µã€æ‹’ç»é‡‡æ ·å¾®è°ƒä»¥åŠç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç­‰æ­¥éª¤ã€‚è¿™ä¸‰ä¸ªæ­¥éª¤å…±åŒæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10518">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d9578ed00fb4d6ba02bf482c0572346d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895914&auth_key=1760895914-0-0-96a5fe9060780c59170ba30e534a2281&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bbbeb2ae5c05f7bc6aad0df0f6cd136f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895922&auth_key=1760895922-0-0-1184ed0fee51da282ab86e587ed58dcf&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8c5cc8c279d2c09f70461e54c741063b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895929&auth_key=1760895929-0-0-dcdc55b4acc5150e8b7ecb282fb2bb64&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Adaptive-Dual-Reasoner-Large-Reasoning-Models-Can-Think-Efficiently-by-Hybrid-Reasoning"><a href="#Adaptive-Dual-Reasoner-Large-Reasoning-Models-Can-Think-Efficiently-by-Hybrid-Reasoning" class="headerlink" title="Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by   Hybrid Reasoning"></a>Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by   Hybrid Reasoning</h2><p><strong>Authors:Yujian Zhang, Keyu Chen, Zhifeng Shen, Ruizhi Qiao, Xing Sun</strong></p>
<p>Although Long Reasoning Models (LRMs) have achieved superior performance on various reasoning scenarios, they often suffer from increased computational costs and inference latency caused by overthinking. To address these limitations, we propose Adaptive Dual Reasoner, which supports two reasoning modes: fast thinking and slow thinking. ADR dynamically alternates between these modes based on the contextual complexity during reasoning. ADR is trained in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to equip the model with the ability to integrate both fast and slow reasoning modes, in which we construct a hybrid reasoning dataset through a dedicated pipeline to provide large-scale supervision. (2) A reinforcement learning stage for optimizing reasoning effort, where we introduce Entropy-guided Hybrid Policy Optimization EHPO, an RL training framework employing an entropy-guided dynamic rollout strategy for branching at high-entropy units and a difficulty-aware penalty to balance fast and slow reasoning. Across challenging mathematical reasoning benchmarks, ADR achieves an effective balance between reasoning performance and efficiency among state-of-the-art approaches. Specifically, ADR yields a performance gain of up to 6.1%, while reducing the reasoning output length by 49.5% to 59.3%. </p>
<blockquote>
<p>å°½ç®¡é•¿æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å„ç§æ¨ç†åœºæ™¯ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¸¸å¸¸å› ä¸ºè¿‡åº¦æ€è€ƒè€Œé¢ä¸´è®¡ç®—æˆæœ¬å¢åŠ å’Œæ¨ç†å»¶è¿Ÿçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”åŒæ¨ç†å™¨ï¼ˆADRï¼‰ï¼Œå®ƒæ”¯æŒä¸¤ç§æ¨ç†æ¨¡å¼ï¼šå¿«é€Ÿæ€è€ƒå’Œæ…¢é€Ÿæ€è€ƒã€‚ADRä¼šæ ¹æ®æ¨ç†è¿‡ç¨‹ä¸­çš„ä¸Šä¸‹æ–‡å¤æ‚æ€§åŠ¨æ€åœ°åœ¨è¿™äº›æ¨¡å¼ä¹‹é—´äº¤æ›¿ã€‚ADRçš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å†·å¯åŠ¨é˜¶æ®µï¼Œé€šè¿‡ä¸“ç”¨ç®¡é“æ„å»ºæ··åˆæ¨ç†æ•°æ®é›†ï¼Œä¸ºæ¨¡å‹æä¾›é›†æˆå¿«é€Ÿå’Œæ…¢é€Ÿæ¨ç†æ¨¡å¼çš„èƒ½åŠ›ï¼Œä»¥æä¾›å¤§è§„æ¨¡ç›‘ç£ã€‚ï¼ˆ2ï¼‰ä¼˜åŒ–æ¨ç†åŠªåŠ›çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ç†µå¼•å¯¼æ··åˆç­–ç•¥ä¼˜åŒ–ï¼ˆEHPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§RLè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ç†µå¼•å¯¼çš„åŠ¨æ€æ»šåŠ¨ç­–ç•¥åœ¨é«˜ç†µå•ä½è¿›è¡Œåˆ†æ”¯ï¼Œä»¥åŠéš¾åº¦æ„ŸçŸ¥æƒ©ç½šæ¥å¹³è¡¡å¿«é€Ÿå’Œæ…¢é€Ÿæ¨ç†ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒADRåœ¨æœ€æ–°æ–¹æ³•ä¹‹é—´å®ç°äº†æ¨ç†æ€§èƒ½å’Œæ•ˆç‡çš„æœ‰æ•ˆå¹³è¡¡ã€‚å…·ä½“æ¥è¯´ï¼ŒADRçš„æ€§èƒ½æé«˜äº†é«˜è¾¾6.1%ï¼ŒåŒæ—¶å‡å°‘äº†æ¨ç†è¾“å‡ºé•¿åº¦è¾¾49.5%è‡³59.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10207v2">PDF</a> Accepted to NeurIPS 2025 Workshop on Efficient Reasoning</p>
<p><strong>Summary</strong></p>
<p>è‡ªé€‚åº”åŒæ¨ç†å™¨ï¼ˆADRï¼‰æ˜¯ä¸€ç§é’ˆå¯¹é•¿æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿé—®é¢˜æå‡ºçš„è§£å†³æ–¹æ¡ˆã€‚å®ƒæ”¯æŒå¿«é€Ÿå’Œæ…¢é€Ÿä¸¤ç§æ¨ç†æ¨¡å¼ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡å¤æ‚æ€§åŠ¨æ€åˆ‡æ¢ã€‚ADRçš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å†·å¯åŠ¨é˜¶æ®µï¼Œèµ‹äºˆæ¨¡å‹ç»“åˆä¸¤ç§æ¨ç†æ¨¡å¼çš„èƒ½åŠ›ï¼›å…¶æ¬¡æ˜¯å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œä¼˜åŒ–æ¨ç†åŠªåŠ›ã€‚EHPOæ˜¯ä¸€ç§RLè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ç†µå¼•å¯¼çš„åŠ¨æ€æ»šåŠ¨ç­–ç•¥è¿›è¡Œåˆ†æ”¯ï¼Œå¹¶å¼•å…¥éš¾åº¦æ„ŸçŸ¥æƒ©ç½šæ¥å¹³è¡¡å¿«é€Ÿå’Œæ…¢é€Ÿæ¨ç†ã€‚åœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒADRåœ¨æœ€æ–°æ–¹æ³•ä¹‹é—´å®ç°äº†æ¨ç†æ€§èƒ½å’Œæ•ˆç‡çš„æœ‰æ•ˆå¹³è¡¡ï¼Œæ€§èƒ½æå‡è¾¾6.1%ï¼ŒåŒæ—¶å‡å°‘æ¨ç†è¾“å‡ºé•¿åº¦è¾¾49.5%è‡³59.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªé€‚åº”åŒæ¨ç†å™¨ï¼ˆADRï¼‰æ—¨åœ¨è§£å†³é•¿æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>ADRæ”¯æŒå¿«é€Ÿå’Œæ…¢é€Ÿä¸¤ç§æ¨ç†æ¨¡å¼ï¼Œå¹¶å¯æ ¹æ®ä¸Šä¸‹æ–‡å¤æ‚æ€§åŠ¨æ€åˆ‡æ¢ã€‚</li>
<li>ADRçš„è®­ç»ƒåˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å†·å¯åŠ¨é˜¶æ®µå’Œå¼ºåŒ–å­¦ä¹ é˜¶æ®µã€‚</li>
<li>EHPOæ˜¯ä¸€ç§RLè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨ç†µå¼•å¯¼çš„åŠ¨æ€æ»šåŠ¨ç­–ç•¥ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œåˆ†æ”¯å†³ç­–ã€‚</li>
<li>ADRåœ¨éš¾åº¦æ„ŸçŸ¥æƒ©ç½šæœºåˆ¶ä¸‹å¹³è¡¡äº†å¿«é€Ÿå’Œæ…¢é€Ÿæ¨ç†ï¼Œå®ç°äº†æ¨ç†æ€§èƒ½å’Œæ•ˆç‡çš„ä¼˜åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-94d2fba0b146503df0dff0d927a854e2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895936&auth_key=1760895936-0-0-1d6d6b26377b4faaee379500f5d6c757&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d56f897bc055d1f0f023a58351ce1f8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895943&auth_key=1760895943-0-0-d647602d15d69a0a3a5206382495f366&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e29b7ec172762e30eea102854d8c8ff6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895950&auth_key=1760895950-0-0-fc0c05b5f1c1561c88486062d1610034&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Idola-Tribus-of-AI-Large-Language-Models-tend-to-perceive-order-where-none-exists"><a href="#The-Idola-Tribus-of-AI-Large-Language-Models-tend-to-perceive-order-where-none-exists" class="headerlink" title="The Idola Tribus of AI: Large Language Models tend to perceive order   where none exists"></a>The Idola Tribus of AI: Large Language Models tend to perceive order   where none exists</h2><p><strong>Authors:Shin-nosuke Ishikawa, Masato Todo, Taiki Ogihara, Hirotsugu Ohba</strong></p>
<p>We present a tendency of large language models (LLMs) to generate absurd patterns despite their clear inappropriateness in a simple task of identifying regularities in number series. Several approaches have been proposed to apply LLMs to complex real-world tasks, such as providing knowledge through retrieval-augmented generation and executing multi-step tasks using AI agent frameworks. However, these approaches rely on the logical consistency and self-coherence of LLMs, making it crucial to evaluate these aspects and consider potential countermeasures. To identify cases where LLMs fail to maintain logical consistency, we conducted an experiment in which LLMs were asked to explain the patterns in various integer sequences, ranging from arithmetic sequences to randomly generated integer series. While the models successfully identified correct patterns in arithmetic and geometric sequences, they frequently over-recognized patterns that were inconsistent with the given numbers when analyzing randomly generated series. This issue was observed even in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini 2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can be interpreted as the AI model equivalent of Idola Tribus and highlights potential limitations in their capability for applied tasks requiring logical reasoning, even when employing chain-of-thought reasoning mechanisms. </p>
<blockquote>
<p>æˆ‘ä»¬æ³¨æ„åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯†åˆ«æ•°å­—åºåˆ—è§„å¾‹è¿™ä¸€ç®€å•ä»»åŠ¡ä¸­ï¼Œå°½ç®¡æ˜æ˜¾ä¸åˆé€‚ï¼Œä½†ä»å€¾å‘äºç”Ÿæˆè’è°¬çš„æ¨¡å¼ã€‚å°½ç®¡å·²ç»æå‡ºäº†å°†LLMåº”ç”¨äºå¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡çš„å‡ ç§æ–¹æ³•ï¼Œä¾‹å¦‚é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆæä¾›çŸ¥è¯†ï¼Œä»¥åŠä½¿ç”¨äººå·¥æ™ºèƒ½ä»£ç†æ¡†æ¶æ‰§è¡Œå¤šæ­¥éª¤ä»»åŠ¡ï¼Œä½†è¿™äº›æ–¹æ³•ä¾èµ–äºLLMçš„é€»è¾‘è¿è´¯æ€§å’Œè‡ªæˆ‘ä¸€è‡´æ€§ï¼Œå› æ­¤è¯„ä¼°è¿™äº›æ–¹é¢å¹¶è€ƒè™‘æ½œåœ¨çš„å¯¹ç­–è‡³å…³é‡è¦ã€‚ä¸ºäº†ç¡®å®šLLMåœ¨å“ªäº›æƒ…å†µä¸‹æ— æ³•ä¿æŒé€»è¾‘ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å®éªŒï¼Œè¦æ±‚LLMè§£é‡Šå„ç§æ•´æ•°åºåˆ—çš„æ¨¡å¼ï¼Œè¿™äº›åºåˆ—ä»ç®—æœ¯åºåˆ—åˆ°éšæœºç”Ÿæˆçš„æ•´æ•°ç³»åˆ—ä¸ç­‰ã€‚è™½ç„¶è¿™äº›æ¨¡å‹åœ¨ç®—æœ¯å’Œå‡ ä½•åºåˆ—ä¸­æˆåŠŸè¯†åˆ«å‡ºäº†æ­£ç¡®çš„æ¨¡å¼ï¼Œä½†åœ¨åˆ†æéšæœºç”Ÿæˆçš„ç³»åˆ—æ—¶ï¼Œå®ƒä»¬ç»å¸¸è¿‡åº¦è¯†åˆ«ä¸ç»™å®šæ•°å­—ä¸ä¸€è‡´çš„æ¨¡å¼ã€‚è¿™ä¸€é—®é¢˜åœ¨å¤šæ­¥æ¨ç†æ¨¡å‹ä¸­ä¹Ÿè¢«è§‚å¯Ÿåˆ°ï¼ŒåŒ…æ‹¬OpenAI o3ã€o4-miniå’ŒGoogle Gemini 2.5 Flash Preview Thinkingç­‰ã€‚è¿™ç§æ„ŸçŸ¥ä¸å­˜åœ¨çš„æ¨¡å¼çš„å€¾å‘å¯ä»¥è¢«è§£é‡Šä¸ºç›¸å½“äºIdola Tribusçš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå¹¶çªå‡ºäº†å®ƒä»¬åœ¨åº”ç”¨ä»»åŠ¡ä¸­çš„æ½œåœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é€»è¾‘æ¨ç†çš„ä»»åŠ¡ä¸­ï¼Œå³ä½¿é‡‡ç”¨æ€ç»´é“¾æ¨ç†æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09709v1">PDF</a> 14 pages, 3 figures, accepted to Findings of EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯†åˆ«æ•°å­—åºåˆ—è§„å¾‹çš„ä»»åŠ¡ä¸­ï¼Œå€¾å‘äºç”Ÿæˆè’è°¬çš„æ¨¡å¼ã€‚å°½ç®¡åœ¨ç®—æœ¯åºåˆ—å’Œå‡ ä½•åºåˆ—ä¸­ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤ŸæˆåŠŸè¯†åˆ«æ­£ç¡®çš„æ¨¡å¼ï¼Œä½†åœ¨åˆ†æéšæœºç”Ÿæˆçš„ç³»åˆ—æ—¶ï¼Œå®ƒä»¬ç»å¸¸è¿‡åº¦è¯†åˆ«ä¸ç»™å®šæ•°å­—ä¸ä¸€è‡´çš„æ¨¡å¼ã€‚è¿™åæ˜ å‡ºLLMsåœ¨éœ€è¦é€»è¾‘æ¨ç†çš„åº”ç”¨ä»»åŠ¡ä¸­çš„æ½œåœ¨å±€é™æ€§ï¼Œå³ä½¿é‡‡ç”¨æ€ç»´é“¾æ¨ç†æœºåˆ¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨è¯†åˆ«æ•°å­—åºåˆ—è§„å¾‹çš„ä»»åŠ¡ä¸­æœ‰ç”Ÿæˆè’è°¬æ¨¡å¼çš„å€¾å‘ã€‚</li>
<li>LLMsåœ¨åˆ†æéšæœºç”Ÿæˆçš„æ•°å­—ç³»åˆ—æ—¶ï¼Œä¼šè¿‡åº¦è¯†åˆ«ä¸ç»™å®šæ•°å­—ä¸ä¸€è‡´çš„æ¨¡å¼ã€‚</li>
<li>LLMsåœ¨é€»è¾‘ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æ½œåœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é€»è¾‘æ¨ç†çš„åº”ç”¨ä»»åŠ¡ä¸­ã€‚</li>
<li>å³ä½¿æ˜¯é‡‡ç”¨å¤šæ­¥æ¨ç†æœºåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿå¯èƒ½ä¼šå‡ºç°é€»è¾‘ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>LLMsçš„æˆåŠŸåº”ç”¨ä¾èµ–äºå…¶é€»è¾‘ä¸€è‡´æ€§å’Œè‡ªæˆ‘è¿è´¯æ€§ã€‚</li>
<li>Idola Tribusçš„æ¦‚å¿µå¯ä»¥ç±»æ¯”ç†è§£LLMsçš„è¿™ç§å€¾å‘ï¼Œå³è¯¯è¯†éå­˜åœ¨çš„æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-7c27e3d21701c119f7663250f43a3d22~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895958&auth_key=1760895958-0-0-67bb02552227be2b67d11f2ddf52879c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-554a0f2efac3db1bbb3a0889e5b16296~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895968&auth_key=1760895968-0-0-cc2943bdf377f7a5aa2eae58caf6066c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5053cf623fcb24f103bb29813b3306d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895975&auth_key=1760895975-0-0-d03b6505d12c05a2700448f97bcbbbe7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Models-A-Survey-of-26K-Papers"><a href="#Vision-Language-Models-A-Survey-of-26K-Papers" class="headerlink" title="Vision Language Models: A Survey of 26K Papers"></a>Vision Language Models: A Survey of 26K Papers</h2><p><strong>Authors:Fengming Lin</strong></p>
<p>We present a transparent, reproducible measurement of research trends across 26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles and abstracts are normalized, phrase-protected, and matched against a hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained cues about tasks, architectures, training regimes, objectives, datasets, and co-mentioned modalities. The analysis quantifies three macro shifts: (1) a sharp rise of multimodal vision-language-LLM work, which increasingly reframes classic perception as instruction following and multi-step reasoning; (2) steady expansion of generative methods, with diffusion research consolidating around controllability, distillation, and speed; and (3) resilient 3D and video activity, with composition moving from NeRFs to Gaussian splatting and a growing emphasis on human- and agent-centric understanding. Within VLMs, parameter-efficient adaptation like prompting&#x2F;adapters&#x2F;LoRA and lightweight vision-language bridges dominate; training practice shifts from building encoders from scratch to instruction tuning and finetuning strong backbones; contrastive objectives recede relative to cross-entropy&#x2F;ranking and distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and ICLR the highest VLM share, while reliability themes such as efficiency or robustness diffuse across areas. We release the lexicon and methodology to enable auditing and extension. Limitations include lexicon recall and abstract-only scope, but the longitudinal signals are consistent across venues and years. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹CVPRã€ICLRå’ŒNeurIPSåœ¨2023-2025å¹´é—´æ¥å—çš„26,104ç¯‡è®ºæ–‡çš„ç ”ç©¶è¶‹åŠ¿è¿›è¡Œäº†é€æ˜ã€å¯å¤åˆ¶çš„æµ‹é‡ã€‚æˆ‘ä»¬å¯¹æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œè§„èŒƒåŒ–ã€çŸ­è¯­ä¿æŠ¤ï¼Œå¹¶ä¸æ‰‹å·¥åˆ¶ä½œçš„è¯æ±‡è¡¨è¿›è¡ŒåŒ¹é…ï¼Œæœ€å¤šåˆ†é…äº†35ä¸ªä¸»é¢˜æ ‡ç­¾ï¼ŒæŒ–æ˜äº†å…³äºä»»åŠ¡ã€æ¶æ„ã€è®­ç»ƒæ–¹æ¡ˆã€ç›®æ ‡ã€æ•°æ®é›†å’Œå…±åŒæåŠçš„æ¨¡æ€çš„ç²¾ç»†çº¿ç´¢ã€‚åˆ†æé‡åŒ–äº†ä¸‰ä¸ªå®è§‚å˜åŒ–ï¼šï¼ˆ1ï¼‰å¤šæ¨¡æ€è§†è§‰è¯­è¨€-å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·¥ä½œçš„æ€¥å‰§å¢é•¿ï¼Œå®ƒè¶Šæ¥è¶Šå¤šåœ°å°†ç»å…¸æ„ŸçŸ¥é‡æ–°å®šä¹‰ä¸ºéµå¾ªæŒ‡ä»¤å’Œåˆ†æ­¥æ¨ç†ï¼›ï¼ˆ2ï¼‰ç”Ÿæˆæ–¹æ³•çš„ç¨³æ­¥æ‰©å±•ï¼Œæ‰©æ•£ç ”ç©¶å›´ç»•å¯æ§æ€§ã€è’¸é¦å’Œé€Ÿåº¦è¿›è¡Œæ•´åˆï¼›ï¼ˆ3ï¼‰3Då’Œè§†é¢‘æ´»åŠ¨å…·æœ‰éŸ§æ€§ï¼Œç»„åˆä»NeRFsè½¬å‘é«˜æ–¯è´´å›¾ï¼Œå¹¶å¯¹äººç±»å’Œä»£ç†ä¸ºä¸­å¿ƒçš„ç†è§£è¶Šæ¥è¶Šå¼ºè°ƒã€‚åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å†…éƒ¨ï¼Œå‚æ•°æœ‰æ•ˆçš„é€‚åº”æ–¹æ³•å¦‚æç¤º&#x2F;é€‚é…å™¨&#x2F;LoRAå’Œè½»é‡çº§è§†è§‰è¯­è¨€æ¡¥æ¢å ä¸»å¯¼åœ°ä½ï¼›è®­ç»ƒå®è·µä»ä»å¤´å¼€å§‹æ„å»ºç¼–ç å™¨è½¬å‘æŒ‡ä»¤è°ƒæ•´å’Œå¾®è°ƒå¼ºå¤§çš„ä¸»å¹²ï¼›å¯¹æ¯”ç›®æ ‡ç›¸å¯¹äºäº¤å‰ç†µ&#x2F;æ’åå’Œè’¸é¦è€Œè¨€æœ‰æ‰€å‡å¼±ã€‚è·¨åœºåœ°æ¯”è¾ƒæ˜¾ç¤ºï¼ŒCVPRåœ¨3Dè¶³è¿¹æ–¹é¢å…·æœ‰æ›´å¼ºçš„åœ°ä½ï¼Œè€ŒICLRåœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ–¹é¢å æ¯”æœ€é«˜ï¼ŒåŒæ—¶æ•ˆç‡æˆ–ç¨³å¥æ€§ç­‰å¯é æ€§ä¸»é¢˜æ‰©æ•£åˆ°å„ä¸ªåŒºåŸŸã€‚æˆ‘ä»¬å‘å¸ƒè¯æ±‡è¡¨å’Œæ–¹æ³•ï¼Œä»¥ä¾¿è¿›è¡Œå®¡è®¡å’Œæ‰©å±•ã€‚å±€é™æ€§åŒ…æ‹¬è¯æ±‡è¡¨å¬å›å’Œä»…æ¶µç›–æ‘˜è¦èŒƒå›´ï¼Œä½†çºµå‘ä¿¡å·åœ¨åœºåœ°å’Œå¹´ä»½ä¹‹é—´æ˜¯ä¸€è‡´çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09586v1">PDF</a> VLM&#x2F;LLM Learning Notes</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹CVPRã€ICLRå’ŒNeurIPSä¼šè®®åœ¨2023-2025å¹´é—´æ‰€æ¥å—çš„26,104ç¯‡è®ºæ–‡è¶‹åŠ¿çš„é€æ˜ã€å¯å¤ç°çš„æµ‹é‡ç»“æœã€‚é€šè¿‡å¯¹æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œè§„èŒƒåŒ–ã€çŸ­è¯­ä¿æŠ¤å’Œä¸æ‰‹å·¥è¯å…¸åŒ¹é…ï¼Œå¯¹è®ºæ–‡è¿›è¡Œä¸»é¢˜æ ‡ç­¾åˆ†é…å’Œç²¾ç»†çº¿ç´¢æŒ–æ˜ï¼Œåˆ†æåŒ…æ‹¬ä»»åŠ¡ã€æ¶æ„ã€è®­ç»ƒåˆ¶åº¦ã€ç›®æ ‡ã€æ•°æ®é›†å’Œå…±åŒæåŠçš„æ¨¡å¼ç­‰æ–¹é¢çš„å˜åŒ–ã€‚æœ¬æ–‡é‡åŒ–ä¸‰ä¸ªå®è§‚å˜åŒ–ï¼šä¸€æ˜¯å¤šæ¨¡æ€è§†è§‰è¯­è¨€ä¸å¤§å‹è¯­è¨€æ¨¡å‹å·¥ä½œçš„æ€¥å‰§å¢åŠ ï¼Œé‡æ–°å®šä¹‰äº†ä¼ ç»Ÿæ„ŸçŸ¥ä¸ºæŒ‡ä»¤è·Ÿéšå’Œå¤šæ­¥æ¨ç†ï¼›äºŒæ˜¯ç”Ÿæˆæ–¹æ³•çš„ç¨³æ­¥æ‰©å±•ï¼Œæ‰©æ•£ç ”ç©¶é›†ä¸­åœ¨å¯æ§æ€§ã€æç‚¼å’Œé€Ÿåº¦æ–¹é¢ï¼›ä¸‰æ˜¯3Då’Œæ´»åŠ¨è§†é¢‘æŒç»­æ´»è·ƒï¼Œç»„åˆä»NeRFè½¬å‘é«˜æ–¯æ‹¼è´´ï¼Œå¹¶è¶Šæ¥è¶Šå¼ºè°ƒäººç±»å’Œä»£ç†ä¸ºä¸­å¿ƒçš„ç†è§£ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†å‚æ•°æ•ˆç‡é€‚åº”å¦‚æç¤º&#x2F;é€‚é…å™¨&#x2F;LoRAå’Œè½»é‡çº§è§†è§‰è¯­è¨€æ¡¥æ¢çš„ä¸»å¯¼åœ°ä½ã€è®­ç»ƒå®è·µçš„è½¬å˜ä»¥åŠç›®æ ‡å‡½æ•°çš„å˜åŒ–ç­‰ã€‚é€šè¿‡è·¨åœºåœ°æ¯”è¾ƒæ˜¾ç¤ºCVPRåœ¨3Dè¶³è¿¹æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œè€ŒICLRåœ¨VLMä»½é¢æ–¹é¢æœ€é«˜ã€‚åŒæ—¶ï¼Œæœ¬æ–‡å¼ºè°ƒäº†æ•ˆç‡å’Œé²æ£’æ€§ç­‰å¯é æ€§ä¸»é¢˜åœ¨æ•´ä¸ªé¢†åŸŸçš„æ™®åŠã€‚æœ€åï¼Œæœ¬æ–‡å‘å¸ƒäº†è¯å…¸å’Œæ–¹æ³•è®ºä»¥ä¿ƒè¿›å®¡è®¡å’Œæ‰©å±•ã€‚å°½ç®¡å­˜åœ¨è¯å…¸å¬å›å’Œä»…æ¶µç›–æ‘˜è¦ç­‰å±€é™æ€§ï¼Œä½†å„ä¼šåœºå’Œå¹´ä»½çš„é•¿æœŸä¿¡å·æ˜¯ä¸€è‡´çš„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é€šè¿‡åˆ†æCVPRã€ICLRå’ŒNeurIPSä¼šè®®è®ºæ–‡ï¼Œè¦†ç›–2023-2025å¹´çš„ç ”ç©¶è¶‹åŠ¿å˜å¾—é€æ˜å’Œå¯å¤ç°ã€‚</li>
<li>è§‚å¯Ÿåˆ°ä¸‰ä¸ªä¸»è¦å®è§‚å˜åŒ–ï¼šå¤šæ¨¡æ€è§†è§‰è¯­è¨€ä¸å¤§å‹è¯­è¨€æ¨¡å‹å·¥ä½œçš„å¢åŠ ï¼Œç”Ÿæˆæ–¹æ³•çš„æ‰©å±•ä»¥åŠ3Då’Œè§†é¢‘æ´»åŠ¨é¢†åŸŸçš„æŒç»­å‘å±•ã€‚</li>
<li>è¯æ±‡è¡¨å’Œæ–¹æ³•è®ºè¢«å‘å¸ƒä»¥æ¨åŠ¨å®¡è®¡å’Œæ‰©å±•ã€‚</li>
<li>å¤šæ¨¡æ€èåˆçš„å·¥ä½œé€æ¸æˆä¸ºä¸€ç§è¶‹åŠ¿ï¼Œè¶Šæ¥è¶Šå¤šçš„å°†ä¼ ç»Ÿæ„ŸçŸ¥ä»»åŠ¡é‡å¡‘ä¸ºæŒ‡ä»¤è·Ÿéšå’Œå¤šæ­¥æ¨ç†ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç ”ç©¶å¼€å§‹é‡è§†å¯æ§æ€§ã€æç‚¼å’Œé€Ÿåº¦æ–¹é¢çš„æ”¹è¿›ã€‚</li>
<li>åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é¢†åŸŸï¼Œå‚æ•°æ•ˆç‡é€‚åº”æ–¹æ³•å¦‚æç¤º&#x2F;é€‚é…å™¨&#x2F;LoRAå—åˆ°é‡è§†ï¼Œè®­ç»ƒå®è·µä¹Ÿåœ¨å‘ç”Ÿå˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09586">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5bcb94019d6d93a98606a7f196df716f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895982&auth_key=1760895982-0-0-aa9081a6f3de40de66233346b33b4114&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1ce29c20e0ae46341d7ff20297191fc6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895989&auth_key=1760895989-0-0-ad5dd47ba038f7bfa5f1a86c0516973e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-be184f978b42f0ba861eeded4fd503ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760895996&auth_key=1760895996-0-0-3b6af10af4af24e28276bb28f9e9e89d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ea2e2ed7dfa420427452f889f4595b2a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896006&auth_key=1760896006-0-0-fa3e1bddfd9b2fa97bd76722943f6030&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a743e0ead3936f0b049a17f4d57afc7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896013&auth_key=1760896013-0-0-93f7b7d8285f506b9117b5b5911c8ca8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics"><a href="#StatEval-A-Comprehensive-Benchmark-for-Large-Language-Models-in-Statistics" class="headerlink" title="StatEval: A Comprehensive Benchmark for Large Language Models in   Statistics"></a>StatEval: A Comprehensive Benchmark for Large Language Models in   Statistics</h2><p><strong>Authors:Yuchen Lu, Run Yang, Yichen Zhang, Shuguang Yu, Runpeng Dai, Ziwei Wang, Jiayi Xiang, Wenxin E, Siran Gao, Xinyao Ruan, Yirui Huang, Chenjing Xi, Haibo Hu, Yueming Fu, Qinglan Yu, Xiaobing Wei, Jiani Gu, Rui Sun, Jiaxuan Jia, Fan Zhou</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce \textbf{StatEval}, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: <a target="_blank" rel="noopener" href="https://stateval.github.io/">https://stateval.github.io/</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç„¶è€Œç»Ÿè®¡å­¦ä½œä¸ºä¸€é—¨ç‹¬ç‰¹ä¸”ç»¼åˆçš„å­¦ç§‘ï¼Œåœ¨åŸºå‡†æµ‹è¯•ä¸­ä»ç„¶è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>StatEval</strong>ï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºç»Ÿè®¡çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†ä¸åŒéš¾åº¦çº§åˆ«çš„å¹¿åº¦å’Œæ·±åº¦ã€‚StatEvalåŒ…å«13817ä¸ªæ¶µç›–æœ¬ç§‘å’Œç ”ç©¶ç”Ÿè¯¾ç¨‹çš„åŸºç¡€é—®é¢˜ï¼Œä»¥åŠä»é¢†å…ˆæœŸåˆŠä¸­æå–çš„2374ä¸ªç ”ç©¶çº§è¯æ˜ä»»åŠ¡ã€‚åœ¨æ„å»ºåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“ç®¡é“ï¼Œé‡‡ç”¨äººå·¥å¾ªç¯éªŒè¯ï¼Œè‡ªåŠ¨åŒ–å¤§è§„æ¨¡é—®é¢˜æå–ã€é‡å†™å’Œè´¨é‡æ§åˆ¶ï¼ŒåŒæ—¶ç¡®ä¿å­¦æœ¯ä¸¥è°¨æ€§ã€‚æˆ‘ä»¬è¿˜é’ˆå¯¹è®¡ç®—å’Œè¯æ˜ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿå¯¹æ¨ç†èƒ½åŠ›è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶å°é—­å¼æ¨¡å‹å¦‚GPT5-miniåœ¨ç ”ç©¶çº§åˆ«é—®é¢˜ä¸Šçš„è¡¨ç°ä½äº57%ï¼Œä½†å¼€æºæ¨¡å‹çš„è¡¨ç°æ›´ä½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†ç»Ÿè®¡æ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ã€‚æˆ‘ä»¬å¸Œæœ›StatEvalèƒ½æˆä¸ºæ¨è¿›å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ç»Ÿè®¡æ™ºèƒ½çš„ä¸¥è°¨åŸºå‡†æµ‹è¯•ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç éƒ½å¯åœ¨æˆ‘ä»¬çš„ç½‘ç»œå¹³å°ï¼š<a target="_blank" rel="noopener" href="https://stateval.github.io/">https://stateval.github.io/</a> ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09517v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç»Ÿè®¡å­¦è¿™ä¸€ç‹¬ç‰¹ä¸”ç»¼åˆçš„å­¦ç§‘é¢†åŸŸï¼Œå…¶è¯„ä¼°ä»å­˜åœ¨ç¼ºå£ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºStatEvalâ€”â€”é¦–ä¸ªä¸“æ³¨äºç»Ÿè®¡å­¦çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–ä¸åŒéš¾åº¦å±‚æ¬¡ï¼Œæ—¢å¹¿æ³›åˆæ·±å…¥ã€‚StatEvalåŒ…å«13817ä¸ªæ¶µç›–æœ¬ç§‘å’Œç ”ç©¶ç”Ÿè¯¾ç¨‹çš„åŸºç¡€é—®é¢˜ï¼Œä»¥åŠä»é¢†å…ˆæœŸåˆŠæå–çš„2374ä¸ªç ”ç©¶çº§è¯æ˜ä»»åŠ¡ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¯è§„æ¨¡åŒ–ã€å¤šæ™ºèƒ½ä½“ç®¡é“ï¼Œé€šè¿‡äººæœºåä½œéªŒè¯ï¼Œå®ç°å¤§è§„æ¨¡é—®é¢˜æå–ã€æ”¹å†™å’Œè´¨é‡æ§åˆ¶ï¼ŒåŒæ—¶ç¡®ä¿å­¦æœ¯ä¸¥è°¨æ€§ã€‚æˆ‘ä»¬å¯¹è®¡ç®—å‹å’Œè¯æ˜å‹ä»»åŠ¡æå‡ºäº†ç¨³å¥çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ç°å¯¹æ¨ç†èƒ½åŠ›çš„ç²¾ç»†è¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°é—­å¼æ¨¡å‹å¦‚GPT5-miniåœ¨ç ”ç©¶çº§åˆ«é—®é¢˜ä¸Šçš„è¡¨ç°ä½äº57%ï¼Œè€Œå¼€æºæ¨¡å‹è¡¨ç°æ›´å·®ã€‚è¿™çªæ˜¾äº†ç»Ÿè®¡æ¨ç†çš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ã€‚æˆ‘ä»¬æœŸæœ›StatEvalèƒ½æˆä¸ºæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç»Ÿè®¡æ™ºèƒ½å‘å±•çš„ä¸¥è°¨åŸºå‡†æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†æ–¹é¢æœ‰æ‰€è¿›æ­¥ï¼Œä½†åœ¨ç»Ÿè®¡å­¦é¢†åŸŸçš„è¯„ä¼°å­˜åœ¨ç¼ºå£ã€‚</li>
<li>StatEvalæ˜¯é¦–ä¸ªä¸“æ³¨äºç»Ÿè®¡å­¦çš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–æœ¬ç§‘ã€ç ”ç©¶ç”ŸåŠç ”ç©¶çº§åˆ«é—®é¢˜ã€‚</li>
<li>StatEvalåŒ…å«ä¸€ä¸ªå¯è§„æ¨¡åŒ–ã€å¤šæ™ºèƒ½ä½“ç®¡é“ï¼Œç¡®ä¿é—®é¢˜æå–ã€æ”¹å†™å’Œè´¨é‡æ§åˆ¶è¿‡ç¨‹çš„å­¦æœ¯ä¸¥è°¨æ€§ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶å¯é’ˆå¯¹è®¡ç®—å‹å’Œè¯æ˜å‹ä»»åŠ¡è¿›è¡Œç²¾ç»†è¯„ä¼°ã€‚</li>
<li>å°é—­å¼æ¨¡å‹å¦‚GPT5-miniåœ¨ç ”ç©¶çº§åˆ«é—®é¢˜ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚</li>
<li>ç»Ÿè®¡æ¨ç†é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09517">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e29adf58c7cbf45fec3f108ffb27bbcb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896020&auth_key=1760896020-0-0-4388ff8d0b2abb0c8d06747d2235fbf2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a2719eaf3c91a21e3c96d93cef2c2544~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896027&auth_key=1760896027-0-0-b5f9be0abd98d263ae4ebec03e024d10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TripScore-Benchmarking-and-rewarding-real-world-travel-planning-with-fine-grained-evaluation"><a href="#TripScore-Benchmarking-and-rewarding-real-world-travel-planning-with-fine-grained-evaluation" class="headerlink" title="TripScore: Benchmarking and rewarding real-world travel planning with   fine-grained evaluation"></a>TripScore: Benchmarking and rewarding real-world travel planning with   fine-grained evaluation</h2><p><strong>Authors:Yincen Qu, Huan Xiao, Feng Li, Gregory Li, Hui Zhou, Xiangying Dai, Xiaoru Dai</strong></p>
<p>Travel planning is a valuable yet complex task that poses significant challenges even for advanced large language models (LLMs). While recent benchmarks have advanced in evaluating LLMsâ€™ planning capabilities, they often fall short in evaluating feasibility, reliability, and engagement of travel plans. We introduce a comprehensive benchmark for travel planning that unifies fine-grained criteria into a single reward, enabling direct comparison of plan quality and seamless integration with reinforcement learning (RL). Our evaluator achieves moderate agreement with travel-expert annotations (60.75%) and outperforms multiple LLM-as-judge baselines. We further release a large-scale dataset of 4,870 queries including 219 real-world, free-form requests for generalization to authentic user intent. Using this benchmark, we conduct extensive experiments across diverse methods and LLMs, including test-time computation, neuro-symbolic approaches, supervised fine-tuning, and RL via GRPO. Across base models, RL generally improves itinerary feasibility over prompt-only and supervised baselines, yielding higher unified reward scores. </p>
<blockquote>
<p>è¡Œç¨‹è§„åˆ’æ˜¯ä¸€é¡¹æœ‰ä»·å€¼ä½†åˆå¤æ‚çš„ä»»åŠ¡ï¼Œå³ä½¿å¯¹äºå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹Ÿæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘çš„åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°LLMçš„è§„åˆ’èƒ½åŠ›æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å®ƒä»¬åœ¨è¯„ä¼°è¡Œç¨‹è®¡åˆ’çš„å¯è¡Œæ€§ã€å¯é æ€§å’Œå‚ä¸åº¦æ–¹é¢å¾€å¾€æœ‰æ‰€æ¬ ç¼ºã€‚æˆ‘ä»¬å¼•å…¥äº†å…¨é¢çš„è¡Œç¨‹è§„åˆ’åŸºå‡†æµ‹è¯•ï¼Œå®ƒå°†ç²¾ç»†çš„å‡†åˆ™ç»Ÿä¸€ä¸ºä¸€ä¸ªå•ä¸€çš„å¥–åŠ±ï¼Œèƒ½å¤Ÿç›´æ¥æ¯”è¾ƒè®¡åˆ’çš„è´¨é‡ï¼Œå¹¶ä¸å¼ºåŒ–å­¦ä¹ æ— ç¼é›†æˆï¼ˆRLï¼‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°å™¨ä¸æ—…è¡Œä¸“å®¶æ³¨é‡Šä¹‹é—´è¾¾åˆ°ä¸­ç­‰åè®®ï¼ˆ60.75%ï¼‰ï¼Œå¹¶ä¼˜äºå¤šä¸ªLLMåˆ¤æ–­åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒäº†åŒ…å«4870ä¸ªæŸ¥è¯¢çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶ä¸­219ä¸ªæ˜¯çœŸå®ä¸–ç•Œçš„è‡ªç”±å½¢å¼è¯·æ±‚ï¼Œä»¥æ¨å¹¿åˆ°çœŸå®çš„ç”¨æˆ·æ„å›¾ã€‚ä½¿ç”¨è¿™ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬åœ¨å„ç§æ–¹æ³•å’ŒLLMä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬æµ‹è¯•æ—¶é—´è®¡ç®—ã€ç¥ç»ç¬¦å·æ–¹æ³•ã€ç›‘ç£å¾®è°ƒä»¥åŠé€šè¿‡GRPOçš„RLã€‚åœ¨åŸºç¡€æ¨¡å‹æ–¹é¢ï¼ŒRLé€šå¸¸èƒ½æé«˜è¡Œç¨‹çš„å¯è¡Œæ€§ï¼Œè¶…è¶Šä»…æç¤ºå’Œç›‘ç£åŸºçº¿ï¼Œä»è€Œå¾—åˆ°æ›´é«˜çš„ç»Ÿä¸€å¥–åŠ±åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09011v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æ—…è¡Œè§„åˆ’æ˜¯ä¸€é¡¹æœ‰ä»·å€¼ä½†å¤æ‚çš„ä»»åŠ¡ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æå‡ºé‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•éš¾ä»¥å…¨é¢è¯„ä»·è®¡åˆ’çš„å¯è¡Œæ€§ã€å¯é æ€§å’Œå‚ä¸åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ—…è¡Œè§„åˆ’è¯„ä¼°æ ‡å‡†ï¼Œå°†ç²¾ç»†æ ‡å‡†æ•´åˆä¸ºå•ä¸€å¥–åŠ±ï¼Œèƒ½å¤Ÿç›´æ¥æ¯”è¾ƒè®¡åˆ’è´¨é‡ï¼Œå¹¶ä¸å¼ºåŒ–å­¦ä¹ æ— ç¼é›†æˆã€‚æˆ‘ä»¬çš„è¯„ä¼°å™¨ä¸æ—…è¡Œä¸“å®¶æ³¨é‡Šè¾¾æˆä¸­ç­‰å…±è¯†ï¼ˆ60.75%ï¼‰ï¼Œå¹¶ä¼˜äºå¤šä¸ªLLMåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†åŒ…å«4870ä¸ªæŸ¥è¯¢çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…¶ä¸­219ä¸ªæ˜¯çœŸå®ä¸–ç•Œçš„è‡ªç”±å½¢å¼è¯·æ±‚ï¼Œç”¨äºæ¨å¹¿åˆ°çœŸå®ç”¨æˆ·æ„å›¾ã€‚åˆ©ç”¨æ­¤è¯„ä¼°æ ‡å‡†ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„æ–¹æ³•å’ŒLLMä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬æµ‹è¯•æ—¶é—´è®¡ç®—ã€ç¥ç»ç¬¦å·æ–¹æ³•ã€ç›‘ç£å¾®è°ƒä»¥åŠé€šè¿‡GRPOçš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨åŸºç¡€æ¨¡å‹ä¸Šï¼Œå¼ºåŒ–å­¦ä¹ é€šå¸¸èƒ½æé«˜è¡Œç¨‹çš„å¯è¡Œæ€§ï¼Œç›¸å¯¹äºä»…æç¤ºå’Œç›‘ç£åŸºçº¿ï¼Œè·å¾—æ›´é«˜çš„ç»Ÿä¸€å¥–åŠ±åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—…è¡Œè§„åˆ’å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€é¡¹å¤æ‚ä»»åŠ¡ï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ—…è¡Œè§„åˆ’è¯„ä¼°æ ‡å‡†ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä»·è®¡åˆ’çš„å¯è¡Œæ€§ã€å¯é æ€§å’Œå‚ä¸åº¦ã€‚</li>
<li>è¯„ä¼°å™¨ä¸æ—…è¡Œä¸“å®¶æ³¨é‡Šè¾¾æˆä¸­ç­‰å…±è¯†ï¼Œå¹¶ä¼˜äºå¤šä¸ªLLMåŸºçº¿ã€‚</li>
<li>å‘å¸ƒäº†åŒ…å«çœŸå®ä¸–ç•Œè‡ªç”±å½¢å¼è¯·æ±‚çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æå‡è¡Œç¨‹è§„åˆ’è´¨é‡æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºç¡€æ¨¡å‹ä¸Šã€‚</li>
<li>ä¸åŒæ–¹æ³•å’ŒLLMçš„å®éªŒéªŒè¯äº†è¯„ä¼°æ ‡å‡†çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-47457b039412fe6e22992742dfe194ee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896035&auth_key=1760896035-0-0-effcadde4637e4304864010134ef0be9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14d03bd7c3bd51f19e1bf37c524f059d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896042&auth_key=1760896042-0-0-2774d1487d88cc6caa5fe8e8442c7c9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6ad511a7387c50b3b717b6e2e70cba54~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896049&auth_key=1760896049-0-0-78e6878e3b68f587e075954f4429f9c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5beca08e89be72829f78123c47a3a57a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896056&auth_key=1760896056-0-0-34b0706a0ab2fea208becb5d47840e0c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SOP-Maze-Evaluating-Large-Language-Models-on-Complicated-Business-Standard-Operating-Procedures"><a href="#SOP-Maze-Evaluating-Large-Language-Models-on-Complicated-Business-Standard-Operating-Procedures" class="headerlink" title="SOP-Maze: Evaluating Large Language Models on Complicated Business   Standard Operating Procedures"></a>SOP-Maze: Evaluating Large Language Models on Complicated Business   Standard Operating Procedures</h2><p><strong>Authors:Jiaming Wang, Zhe Tang, Yilin Jin, Peng Ding, Xiaoyu Li, Xuezhi Cao</strong></p>
<p>As large language models (LLMs) are widely deployed as domain-specific agents, many benchmarks have been proposed to evaluate their ability to follow instructions and make decisions in real-world scenarios. However, business scenarios often involve complex standard operating procedures (SOPs), and the evaluation of LLM capabilities in such contexts has not been fully explored. To bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world business data and adapted into a collection of 397 tasks from 23 complex SOP scenarios. We further categorize SOP tasks into two broad classes: Lateral Root System (LRS), representing wide-option tasks that demand precise selection; and Heart Root System (HRS), which emphasizes deep logical reasoning with complex branches. Extensive experiments reveal that nearly all state-of-the-art models struggle with SOP-Maze. We conduct a comprehensive analysis and identify three key error categories: (i) route blindness: difficulty following procedures; (ii) conversational fragility: inability to handle real dialogue nuances; and (iii) calculation errors: mistakes in time or arithmetic reasoning under complex contexts. The systematic study explores LLM performance across SOP tasks that challenge both breadth and depth, offering new insights for improving model capabilities. We have open-sourced our work on <a target="_blank" rel="noopener" href="https://github.com/ADoublLEN/SOP-Maze">https://github.com/ADoublLEN/SOP-Maze</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«å¹¿æ³›åº”ç”¨äºç‰¹å®šé¢†åŸŸçš„ä»£ç†ï¼Œè®¸å¤šåŸºå‡†æµ‹è¯•å·²ç»è¢«æå‡ºæ¥è¯„ä¼°å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­éµå¾ªæŒ‡ä»¤å’Œåšå†³ç­–çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå•†ä¸šåœºæ™¯é€šå¸¸æ¶‰åŠå¤æ‚çš„æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ï¼Œè€Œåœ¨è¿™ç§èƒŒæ™¯ä¸‹å¯¹LLMèƒ½åŠ›çš„è¯„ä¼°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SOP-Mazeï¼Œè¿™æ˜¯ä¸€ä¸ªç”±çœŸå®ä¸–ç•Œå•†ä¸šæ•°æ®æ„å»ºçš„åŸºå‡†æµ‹è¯•ï¼Œä»23ä¸ªå¤æ‚çš„SOPåœºæ™¯ä¸­æ”¹ç¼–ä¸º397ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†SOPä»»åŠ¡åˆ†ä¸ºä¸¤å¤§ç±»ï¼šæ¨ªå‘æ ¹ç³»ï¼ˆLRSï¼‰ï¼Œä»£è¡¨éœ€è¦ç²¾ç¡®é€‰æ‹©çš„å¤šé€‰é¡¹ä»»åŠ¡ï¼›ä»¥åŠå¿ƒè„æ ¹ç³»ï¼ˆHRSï¼‰ï¼Œå¼ºè°ƒå…·æœ‰å¤æ‚åˆ†æ”¯çš„æ·±åº¦é€»è¾‘æ¨ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå‡ ä¹æ‰€æœ‰æœ€æ–°æ¨¡å‹åœ¨SOP-Mazeä¸­éƒ½è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬è¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå¹¶ç¡®å®šäº†ä¸‰ä¸ªä¸»è¦çš„é”™è¯¯ç±»åˆ«ï¼šï¼ˆiï¼‰è·¯çº¿ç›²ç›®æ€§ï¼šéš¾ä»¥æŒ‰ç…§æµç¨‹æ“ä½œï¼›ï¼ˆiiï¼‰å¯¹è¯è„†å¼±æ€§ï¼šæ— æ³•å¤„ç†çœŸå®çš„å¯¹è¯ç»†å¾®å·®åˆ«ï¼›ï¼ˆiiiï¼‰è®¡ç®—é”™è¯¯ï¼šåœ¨å¤æ‚èƒŒæ™¯ä¸‹æ—¶é—´å’Œç®—æœ¯æ¨ç†çš„é”™è¯¯ã€‚è¿™é¡¹ç³»ç»Ÿç ”ç©¶æ¢è®¨äº†LLMåœ¨æŒ‘æˆ˜å¹¿åº¦å’Œæ·±åº¦çš„SOPä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸ºæ”¹è¿›æ¨¡å‹èƒ½åŠ›æä¾›äº†æ–°è§è§£ã€‚æˆ‘ä»¬çš„å·¥ä½œå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/ADoublLEN/SOP-Maze%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/ADoublLEN/SOP-Mazeä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08942v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­æ‰§è¡ŒæŒ‡ä»¤å’Œå†³ç­–çš„èƒ½åŠ›ï¼Œæå‡ºäº†å¤šç§è¯„ä¼°åŸºå‡†ã€‚ç„¶è€Œï¼Œä¸šåŠ¡åœºæ™¯é€šå¸¸æ¶‰åŠå¤æ‚çš„æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰ï¼Œè€Œåœ¨è¿™ç±»è¯­å¢ƒä¸‹å¯¹LLMèƒ½åŠ›çš„è¯„ä¼°å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†SOP-MazeåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŸºäºçœŸå®ä¸–ç•Œä¸šåŠ¡æ•°æ®æ„å»ºï¼Œä»23ä¸ªå¤æ‚çš„SOPåœºæ™¯ä¸­è¡ç”Ÿå‡º397é¡¹ä»»åŠ¡ã€‚å®éªŒå‘ç°ï¼Œå‡ ä¹æ‰€æœ‰æœ€æ–°æ¨¡å‹åœ¨SOP-Mazeä¸­éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶å‡ºç°äº†ä¸‰ç§ä¸»è¦é”™è¯¯ç±»å‹ã€‚å¯¹æ­¤åŸºå‡†æµ‹è¯•çš„è¯¦ç»†ç ”ç©¶ä¸ºæ”¹è¿›æ¨¡å‹èƒ½åŠ›æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„è¡¨ç°èƒ½åŠ›å¾—åˆ°äº†å¹¿æ³›çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œä½†é’ˆå¯¹å¤æ‚æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆSOPsï¼‰çš„è¯„ä¼°å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>SOP-MazeåŸºå‡†æµ‹è¯•ä»çœŸå®ä¸–ç•Œä¸šåŠ¡æ•°æ®ä¸­æ„å»ºï¼ŒåŒ…å«ä»å¤šç§å¤æ‚SOPåœºæ™¯ä¸­æ´¾ç”Ÿçš„ä»»åŠ¡ï¼Œä¸ºè¯„ä¼°LLMçš„èƒ½åŠ›æä¾›äº†æ–°çš„è§’åº¦ã€‚</li>
<li>SOPä»»åŠ¡åˆ†ä¸ºä¸¤å¤§ç±»åˆ«ï¼šæ¨ªå‘æ ¹ç³»ç³»ç»Ÿï¼ˆLRSï¼‰å’Œå¿ƒè„æ ¹ç³»ç³»ç»Ÿï¼ˆHRSï¼‰ï¼Œåˆ†åˆ«ä¾§é‡äºä¸åŒçš„ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>å®éªŒå‘ç°å‡ ä¹æ‰€æœ‰æœ€æ–°æ¨¡å‹åœ¨SOP-Mazeä¸­éƒ½é¢ä¸´æŒ‘æˆ˜ï¼Œè¡¨æ˜åœ¨è¿™äº›åœºæ™¯ä¸­æ¨¡å‹çš„æ€§èƒ½æœ‰å¾…æé«˜ã€‚</li>
<li>åœ¨SOP-Mazeä¸­å‡ºç°çš„å…³é”®é”™è¯¯ç±»å‹åŒ…æ‹¬ï¼šè·¯çº¿ç›²è§†ã€å¯¹è¯çš„è„†å¼±æ€§å’Œè®¡ç®—é”™è¯¯ã€‚</li>
<li>ç³»ç»Ÿæ€§çš„ç ”ç©¶ä¸ºæ”¹è¿›æ¨¡å‹èƒ½åŠ›æä¾›äº†æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å®½åº¦å’Œæ·±åº¦éƒ½æŒ‘æˆ˜çš„ä»»åŠ¡æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a311bb0f2f203cb3529889afa6cdaaaa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896063&auth_key=1760896063-0-0-b354e9d5123bd0e34d3c790208ad45c7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d7665c5f2e9bb9fd5200cf3052dd8e31~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896071&auth_key=1760896071-0-0-1fd9aac9e0fefdca950026b7ddc32ad5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-603789ee4359e5ebc9092cd8ed3f0b4e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896078&auth_key=1760896078-0-0-848cab97b21e24a71989fcaa0c1bdc3b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0aebafcc6a1a600085c8f8953c2b17ba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896084&auth_key=1760896084-0-0-2d9602f70df8714701af14b5516e450e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c6b7831e897f679425e175c39bf4b8f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896091&auth_key=1760896091-0-0-05f9f335c54336fd9df292870a83e3b3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Chinese-Commonsense-Reasoning-with-a-Multi-hop-Reasoning-Perspective"><a href="#Benchmarking-Chinese-Commonsense-Reasoning-with-a-Multi-hop-Reasoning-Perspective" class="headerlink" title="Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning   Perspective"></a>Benchmarking Chinese Commonsense Reasoning with a Multi-hop Reasoning   Perspective</h2><p><strong>Authors:Wangjie You, Xusheng Wang, Xing Wang, Wenxiang Jiao, Chao Feng, Juntao Li, Min Zhang</strong></p>
<p>While Large Language Models (LLMs) have demonstrated advanced reasoning capabilities, their comprehensive evaluation in general Chinese-language contexts remains understudied. To bridge this gap, we propose Chinese Commonsense Multi-hop Reasoning (CCMOR), a novel benchmark designed to evaluate LLMsâ€™ ability to integrate Chinese-specific factual knowledge with multi-step logical reasoning. Specifically, we first construct a domain-balanced seed set from existing QA datasets, then develop an LLM-powered pipeline to generate multi-hop questions anchored on factual unit chains. To ensure the quality of resulting dataset, we implement a human-in-the-loop verification system, where domain experts systematically validate and refine the generated questions. Using CCMOR, we evaluate state-of-the-art LLMs, demonstrating persistent limitations in LLMsâ€™ ability to process long-tail knowledge and execute knowledge-intensive reasoning. Notably, retrieval-augmented generation substantially mitigates these knowledge gaps, yielding significant performance gains. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç¤ºäº†å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é€šç”¨ä¸­æ–‡ç¯å¢ƒä¸‹çš„å…¨é¢è¯„ä¼°ä»ç„¶ç ”ç©¶ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸­æ–‡å¸¸è¯†å¤šè·³æ¨ç†ï¼ˆCCMORï¼‰è¿™ä¸€æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMå°†ä¸­æ–‡ç‰¹å®šçš„äº‹å®çŸ¥è¯†ä¸å¤šæ­¥éª¤é€»è¾‘æ¨ç†ç›¸ç»“åˆçš„èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆä»ç°æœ‰çš„é—®ç­”æ•°æ®é›†ä¸­æ„å»ºäº†ä¸€ä¸ªé¢†åŸŸå¹³è¡¡çš„ç§å­é›†ï¼Œç„¶åå¼€å‘äº†ä¸€ä¸ªç”±LLMé©±åŠ¨çš„ç®¡é“ï¼Œä»¥ç”ŸæˆåŸºäºäº‹å®å•ä½é“¾çš„å¤šè·³é—®é¢˜ã€‚ä¸ºç¡®ä¿æœ€ç»ˆæ•°æ®é›†çš„è´¨é‡ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªäººå·¥å¾ªç¯éªŒè¯ç³»ç»Ÿï¼Œé¢†åŸŸä¸“å®¶ç³»ç»Ÿåœ°éªŒè¯å’Œç²¾ç‚¼ç”Ÿæˆçš„é—®é¢˜ã€‚ä½¿ç”¨CCMORï¼Œæˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿å°¾çŸ¥è¯†å’Œæ‰§è¡ŒçŸ¥è¯†å¯†é›†å‹æ¨ç†æ–¹é¢å­˜åœ¨æŒç»­å±€é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆæ˜¾è‘—ç¼“è§£äº†è¿™äº›çŸ¥è¯†å·®è·ï¼Œå¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸­å›½è¯­å¢ƒä¸‹å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»¼åˆè¯„ä»·ç ”ç©¶ä»æ˜¾ä¸è¶³ï¼Œä¸ºæ­¤æå‡ºä¸­å›½å¸¸è¯†å¤šè·³æ¨ç†ï¼ˆCCMORï¼‰åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•æ—¨åœ¨è¯„ä¼°LLMåœ¨æ•´åˆæ±‰è¯­ç‰¹å®šäº‹å®çŸ¥è¯†å’Œå¤šæ­¥éª¤é€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ„å»ºé¢†åŸŸå‡è¡¡çš„ç§å­é›†ï¼Œé€šè¿‡LLMç®¡é“ç”ŸæˆåŸºäºäº‹å®å•å…ƒé“¾çš„å¤šè·³é—®é¢˜ã€‚ä¸ºç¡®ä¿æ•°æ®é›†è´¨é‡ï¼Œå®æ–½äººæœºå¾ªç¯éªŒè¯ç³»ç»Ÿï¼Œç”±é¢†åŸŸä¸“å®¶ç³»ç»Ÿåœ°éªŒè¯å’Œç²¾ç‚¼ç”Ÿæˆçš„é—®é¢˜ã€‚ä½¿ç”¨CCMORè¯„ä¼°äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¡¨æ˜å…¶åœ¨å¤„ç†é•¿å°¾çŸ¥è¯†å’Œæ‰§è¡ŒçŸ¥è¯†å¯†é›†å‹æ¨ç†æ–¹é¢å­˜åœ¨æŒç»­å±€é™æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé€šè¿‡æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯æ˜¾è‘—ç¼“è§£äº†è¿™äº›çŸ¥è¯†å·®è·ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•´åˆæ±‰è¯­ç‰¹å®šäº‹å®çŸ¥è¯†å’Œå¤šæ­¥éª¤é€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›è¯„ä»·ä»æ˜¾ä¸è¶³ã€‚</li>
<li>æå‡ºä¸­å›½å¸¸è¯†å¤šè·³æ¨ç†ï¼ˆCCMORï¼‰åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LLMsåœ¨ä¸Šè¿°æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>CCMORé€šè¿‡æ„å»ºé¢†åŸŸå‡è¡¡çš„ç§å­é›†å’ŒLLMç®¡é“ç”ŸæˆåŸºäºäº‹å®å•å…ƒé“¾çš„å¤šè·³é—®é¢˜ã€‚</li>
<li>å®æ–½äººæœºå¾ªç¯éªŒè¯ç³»ç»Ÿç¡®ä¿æ•°æ®é›†è´¨é‡ã€‚</li>
<li>è¯„ä¼°å‘ç°LLMsåœ¨å¤„ç†é•¿å°¾çŸ¥è¯†å’Œæ‰§è¡ŒçŸ¥è¯†å¯†é›†å‹æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯èƒ½æ˜¾è‘—ç¼“è§£LLMsçš„çŸ¥è¯†å·®è·é—®é¢˜ï¼Œæå‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-91ab5790aa527ac52c7aefd8a63f2efe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896141&auth_key=1760896141-0-0-781f51f9e7702e82f2f0ed20cf151c59&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54a9f88e5b935d8b03ee63d3db80aaff~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896148&auth_key=1760896148-0-0-c007752aed9abfe1695b4ede4fd7916c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5c6664a5421551d43893bc3404c68c9c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896176&auth_key=1760896176-0-0-79689baab1883fa68e3ea62977776882&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff0d16e1efd5b68252ac206e955773aa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896184&auth_key=1760896184-0-0-049fb0f1c09efc6851292719f3f5d2fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ee680fb4e2f69981291b6d699b7a99b0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896191&auth_key=1760896191-0-0-4da4d2fe3c90ef9d7f64833f877cdcc1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c738de8aeb160c87c360e5e963d7a99~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896198&auth_key=1760896198-0-0-9e752253b3b1e0df84342df5423977c5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Token-Hidden-Reward-Steering-Exploration-Exploitation-in-Group-Relative-Deep-Reinforcement-Learning"><a href="#Token-Hidden-Reward-Steering-Exploration-Exploitation-in-Group-Relative-Deep-Reinforcement-Learning" class="headerlink" title="Token Hidden Reward: Steering Exploration-Exploitation in Group Relative   Deep Reinforcement Learning"></a>Token Hidden Reward: Steering Exploration-Exploitation in Group Relative   Deep Reinforcement Learning</h2><p><strong>Authors:Wenlong Deng, Yi Ren, Yushu Li, Boying Gong, Danica J. Sutherland, Xiaoxiao Li, Christos Thrampoulidis</strong></p>
<p>Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each tokenâ€™s influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPOâ€™s learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±æå¤§åœ°æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¦‚ä½•æ˜ç¡®æŒ‡å¯¼è®­ç»ƒèµ°å‘æ¢ç´¢æˆ–åˆ©ç”¨ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†Token Hidden Rewardï¼ˆTHRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»¤ç‰Œçº§åˆ«çš„åº¦é‡æ ‡å‡†ï¼Œå¯ä»¥é‡åŒ–æ¯ä¸ªä»¤ç‰Œåœ¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä¸‹å¯¹æ­£ç¡®ååº”æ¦‚ç‡çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°è®­ç»ƒåŠ¨æ€ä¸»è¦ç”±å…·æœ‰THé«˜ç»å¯¹å€¼çš„å°éƒ¨åˆ†ä»¤ç‰Œä¸»å¯¼ã€‚æœ€æœ‰è¶£çš„æ˜¯ï¼Œå…·æœ‰æ­£å‘THRçš„ä»¤ç‰Œä¼šå¢å¼ºå¯¹æ­£ç¡®è¾“å‡ºçš„ä¿¡å¿ƒï¼Œæœ‰åˆ©äºåˆ©ç”¨å·²æœ‰çŸ¥è¯†ï¼›è€Œå…·æœ‰è´Ÿå‘THRçš„ä»¤ç‰Œåˆ™ä¿ç•™äº†å¯¹æ›¿ä»£è¾“å‡ºçš„æ¦‚ç‡è´¨é‡ï¼Œä»è€Œèƒ½å¤Ÿæ¢ç´¢æ›´å¤šå¯èƒ½æ€§ã€‚è¿™ä¸€å‘ç°æå‡ºäº†ä¸€ç§è‡ªç„¶çš„å¹²é¢„æªæ–½ï¼šä¸€ç§ç”±THRå¼•å¯¼çš„é‡æ–°åŠ æƒç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥è°ƒèŠ‚GRPOçš„å­¦ä¹ ä¿¡å·ï¼Œæ˜ç¡®åœ°å°†è®­ç»ƒåå‘æ¢ç´¢æˆ–åˆ©ç”¨ã€‚æˆ‘ä»¬åœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡å¢å¼ºå…·æœ‰æ­£å‘THRå€¼çš„ä»¤ç‰Œå¹¶å‰Šå¼±è´Ÿå‘THRå€¼çš„ä»¤ç‰Œï¼Œæˆ‘ä»¬çš„ç®—æ³•æé«˜äº†è´ªå©ªè§£ç çš„å‡†ç¡®æ€§ï¼Œæ›´æœ‰åˆ©äºåˆ©ç”¨ç°æœ‰çŸ¥è¯†ã€‚ç›¸åçš„ç­–ç•¥åˆ™æœ‰åˆ©äºæé«˜Pass@Kå‡†ç¡®æ€§ï¼Œæœ‰åˆ©äºæ¢ç´¢ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†è¯¥ç®—æ³•èƒ½å¤Ÿæ— ç¼åœ°ä¸å…¶ä»–RLç›®æ ‡ï¼ˆå¦‚GSPOï¼‰ç›¸ç»“åˆï¼Œå¹¶é€‚ç”¨äºå„ç§æ¶æ„ï¼ŒåŒ…æ‹¬Llamaã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†THRä½œä¸ºä¸€ç§æœ‰åŸåˆ™ã€ç²¾ç»†çš„æœºåˆ¶ï¼Œç”¨äºåŠ¨æ€æ§åˆ¶RLè°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä¸ºé’ˆå¯¹æ¨ç†å¯†é›†å‹åº”ç”¨çš„ç›®æ ‡å¾®è°ƒæä¾›äº†æ–°çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03669v2">PDF</a> Full version of submission to 2nd AI for Math Workshop@ ICML 2025   (best paper)</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±æ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¦‚ä½•æ˜ç¡®å¼•å¯¼è®­ç»ƒè¿‡ç¨‹èµ°å‘æ¢ç´¢æˆ–åˆ©ç”¨ä»æ˜¯ä¸€ä¸ªå¾…è§£å†³çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†Token Hidden Rewardï¼ˆTHRï¼‰è¿™ä¸€æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–æ¯ä¸ªtokenåœ¨Group Relative Policy Optimizationï¼ˆGRPOï¼‰ä¸‹å¯¹æ­£ç¡®å“åº”æ¦‚ç‡çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°è®­ç»ƒè¿‡ç¨‹ä¸»è¦ç”±å…·æœ‰æé«˜ç»å¯¹THRå€¼çš„å°‘é‡tokensä¸»å¯¼ã€‚æœ€æœ‰è¶£çš„æ˜¯ï¼Œå…·æœ‰æ­£THRçš„tokensä¼šå¢å¼ºå¯¹æ­£ç¡®è¾“å‡ºçš„ä¿¡å¿ƒï¼Œä»è€Œæœ‰åˆ©äºåˆ©ç”¨ï¼Œè€Œå…·æœ‰è´ŸTHRçš„tokensåˆ™ä¿ç•™äº†å¯¹æ›¿ä»£è¾“å‡ºçš„æ¦‚ç‡è´¨é‡ï¼Œä»è€Œå®ç°æ¢ç´¢ã€‚è¿™ä¸€è§è§£æå‡ºäº†ä¸€ç§è‡ªç„¶çš„å¹²é¢„æªæ–½ï¼šä¸€ç§ç”±THRå¼•å¯¼çš„é‡æ–°åŠ æƒç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥è°ƒèŠ‚GRPOçš„å­¦ä¹ ä¿¡å·ï¼Œä»¥æ˜ç¡®å¼•å¯¼è®­ç»ƒè¿‡ç¨‹èµ°å‘åˆ©ç”¨æˆ–æ¢ç´¢ã€‚æˆ‘ä»¬åœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†è¯¥ç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡æ”¾å¤§å…·æœ‰æ­£THRå€¼çš„tokenså¹¶å‰Šå¼±å…·æœ‰è´ŸTHRå€¼çš„tokensï¼Œæˆ‘ä»¬çš„ç®—æ³•æé«˜äº†è´ªå©ªè§£ç çš„å‡†ç¡®æ€§ï¼Œæœ‰åˆ©äºåˆ©ç”¨ã€‚ç›¸åçš„ç­–ç•¥åˆ™åœ¨Pass@Kå‡†ç¡®æ€§ä¸Šå–å¾—äº†ä¸€è‡´çš„æ”¶ç›Šï¼Œæœ‰åˆ©äºæ¢ç´¢ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†è¯¥ç®—æ³•å¯ä»¥æ— ç¼é›†æˆåˆ°å…¶ä»–RLç›®æ ‡ä¸­ï¼Œå¦‚GSPOï¼Œå¹¶é€‚ç”¨äºå„ç§æ¶æ„ï¼ŒåŒ…æ‹¬Llamaã€‚è¿™äº›å‘ç°ç¡®ç«‹äº†THRä½œä¸ºåœ¨RLè°ƒæ•´LLMä¸­åŠ¨æ€æ§åˆ¶æ¢ç´¢å’Œåˆ©ç”¨çš„ç²¾ç»†åŒ–æœºåˆ¶çš„åœ°ä½ï¼Œä¸ºé’ˆå¯¹æ¨ç†å¯†é›†å‹åº”ç”¨çš„ç›®æ ‡å¾®è°ƒæä¾›äº†æ–°çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç»“åˆå¯éªŒè¯å¥–åŠ±æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†Token Hidden Rewardï¼ˆTHRï¼‰æŒ‡æ ‡æ¥é‡åŒ–tokenå¯¹æ­£ç¡®å“åº”æ¦‚ç‡çš„å½±å“ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ç”±å…·æœ‰é«˜THRå€¼çš„å°‘æ•°tokensä¸»å¯¼ã€‚</li>
<li>æ­£THRçš„tokensæœ‰åˆ©äºåˆ©ç”¨ï¼Œè€Œè´ŸTHRçš„tokensåˆ™ä¿ƒè¿›æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç”±THRå¼•å¯¼çš„é‡æ–°åŠ æƒç®—æ³•ï¼Œå¯æ˜ç¡®å¼•å¯¼è®­ç»ƒè¿‡ç¨‹èµ°å‘åˆ©ç”¨æˆ–æ¢ç´¢ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨å¤šç§æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æœ‰æ•ˆï¼Œèƒ½æ— ç¼é›†æˆåˆ°å…¶ä»–RLç›®æ ‡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cceca9244984d565b0190d20cd3d489e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896206&auth_key=1760896206-0-0-899d6358a52854fe214d0418102c5d07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-91d2d6734e744ad2e63c49311e795448~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896213&auth_key=1760896213-0-0-a2598c26301e4c1c3599d64664be6008&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b0c04af481694e9c32e9cd289ea34131~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896219&auth_key=1760896219-0-0-2f423de88ff2e687cf6170aeb5c3b486&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0793dd5f1fd0b9370180dfaf952150fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896226&auth_key=1760896226-0-0-70d67637ceca93c8d11b9751cd94b544&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8be0ab76363051d8218ddba33c49ed39~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896233&auth_key=1760896233-0-0-0e0643cf2eb07bbf6ab66914c7710911&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Unspoken-Hints-Accuracy-Without-Acknowledgement-in-LLM-Reasoning"><a href="#Unspoken-Hints-Accuracy-Without-Acknowledgement-in-LLM-Reasoning" class="headerlink" title="Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning"></a>Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning</h2><p><strong>Authors:Arash Marioriyad, Shaygan Adim, Nima Alighardashi, Mahdieh Soleymani Banghshah, Mohammad Hossein Rohban</strong></p>
<p>Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šä¾èµ–é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ä»ç„¶å­˜åœ¨ï¼šè¿™äº›ç”Ÿæˆçš„åŸç†åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ˜¯å¿ äºåº•å±‚è®¡ç®—çš„ï¼Œè€Œä¸æ˜¯ç”±æç¤ºæ‰€å¡‘é€ çš„äº‹åå™äº‹ï¼Œè¿™äº›æç¤ºä½œä¸ºåµŒå…¥åœ¨æç¤ºä¸­çš„ç­”æ¡ˆæ·å¾„ï¼Ÿåœ¨å…³äºæœ‰æç¤ºä¸æ— æç¤ºæç¤ºçš„å…ˆå‰å·¥ä½œä¹‹åï¼Œæˆ‘ä»¬å‘ˆç°äº†ä¸€é¡¹åœ¨å—æ§æç¤ºæ“ä½œä¸‹å¯¹CoTå¿ å®åº¦çš„ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬çš„å®éªŒè®¾è®¡æ¶µç›–äº†å››ä¸ªæ•°æ®é›†ï¼ˆAIMEã€GSM-Hardã€MATH-500ã€UniADILRï¼‰ã€ä¸¤ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆGPT-4oå’ŒGemini-2-Flashï¼‰ï¼Œä»¥åŠä¸€å¥—ç»“æ„åŒ–çš„æç¤ºæ¡ä»¶ï¼Œè¿™äº›æ¡ä»¶åœ¨æ­£ç¡®æ€§ï¼ˆæ­£ç¡®å’Œé”™è¯¯ï¼‰ã€è¡¨ç°é£æ ¼ï¼ˆå¥‰æ‰¿å’Œæ•°æ®æ³„éœ²ï¼‰å’Œå¤æ‚æ€§ï¼ˆåŸå§‹ç­”æ¡ˆã€ä¸¤ä¸ªè¿ç®—ç¬¦çš„è¡¨è¾¾å¼ã€å››ä¸ªè¿ç®—ç¬¦çš„è¡¨è¾¾å¼ï¼‰ä¸Šæœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬è¯„ä¼°äº†ä»»åŠ¡å‡†ç¡®æ€§å’Œæ˜¯å¦åœ¨æ¨ç†ä¸­æ˜ç¡®æ‰¿è®¤æç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œæ­£ç¡®çš„æç¤ºä¼šæ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´éš¾çš„æ ‡å‡†å’Œé€»è¾‘æ¨ç†æ–¹é¢ï¼Œè€Œé”™è¯¯çš„æç¤ºä¼šå¤§å¹…åº¦é™ä½åŸºçº¿èƒ½åŠ›è¾ƒä½çš„ä»»åŠ¡å‡†ç¡®æ€§ã€‚å…¶æ¬¡ï¼Œå¯¹æç¤ºçš„è®¤å¯æä¸å‡è¡¡ï¼šåŸºäºæ–¹ç¨‹çš„æç¤ºç»å¸¸è¢«å¼•ç”¨ï¼Œè€ŒåŸå§‹æç¤ºå¾€å¾€è¢«é»˜é»˜é‡‡çº³ï¼Œè¿™è¡¨æ˜æ›´å¤æ‚çš„æç¤ºæ¨åŠ¨æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¡¨è¾¾å…¶ä¾èµ–ã€‚ç¬¬ä¸‰ï¼Œè¡¨ç°æ–¹å¼å¾ˆé‡è¦ï¼šå¥‰æ‰¿æç¤ºé¼“åŠ±å…¬å¼€è®¤å¯ï¼Œè€Œæ³„éœ²å¼æç¤ºè™½ç„¶æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†ä¿ƒè¿›äº†éšè—çš„ä¾èµ–ã€‚è¿™å¯èƒ½åæ˜ äº†RLHFï¼ˆäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰çš„ç›¸å…³å½±å“ï¼Œå› ä¸ºå¥‰æ‰¿åˆ©ç”¨äººç±»æ„‰æ‚¦çš„ä¸€é¢ï¼Œæ•°æ®æ³„éœ²è§¦å‘äº†è‡ªæˆ‘å®¡æŸ¥çš„ä¸€é¢ã€‚æ€»ä¹‹ï¼Œè¿™äº›ç»“æœè¡¨æ˜ï¼ŒLLMæ¨ç†å—åˆ°æ·å¾„çš„ç³»ç»Ÿæ€§å½±å“ï¼Œè¿™æ©ç›–äº†å¿ å®åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26041v2">PDF</a> 5 Pages, 4 Figures, 4 Tables</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºæ¥è§£å†³æ•°å­¦å’Œé€»è¾‘æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œä¸­å¿ƒé—®é¢˜æ˜¯è¿™äº›ç”Ÿæˆçš„è§£é‡Šåœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¿ å®äºåº•å±‚è®¡ç®—ï¼Œè€Œä¸æ˜¯ç”±æç¤ºæ„æˆçš„ç­”æ¡ˆæ·å¾„æ‰€å¡‘é€ çš„åéªŒå™è¿°ï¼Ÿåœ¨å…³äºæç¤ºä¸éæç¤ºæ€§æç¤ºçš„å…ˆå‰ç ”ç©¶åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¹å—æ§æç¤ºæ“çºµä¸‹çš„CoTå¿ å®æ€§è¿›è¡Œäº†ç³»ç»Ÿç ”ç©¶ã€‚å®éªŒè®¾è®¡æ¶µç›–å››ä¸ªæ•°æ®é›†ã€ä¸¤ä¸ªæœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œä»¥åŠæ¶µç›–æ­£ç¡®æ€§ã€å‘ˆç°é£æ ¼å’Œå¤æ‚æ€§çš„ç»“æ„åŒ–æç¤ºæ¡ä»¶ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ä¸‰ä¸ªå…³é”®å‘ç°ï¼šé¦–å…ˆï¼Œæ­£ç¡®çš„æç¤ºä¼šæ˜¾è‘—æé«˜å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´éš¾çš„ä»»åŠ¡å’Œé€»è¾‘æ¨ç†æ–¹é¢ï¼›å…¶æ¬¡ï¼Œå¯¹æç¤ºçš„æ‰¿è®¤æä¸å‡åŒ€ï¼Œæ–¹ç¨‹æç¤ºç»å¸¸è¢«å¼•ç”¨ï¼Œè€ŒåŸå§‹æç¤ºå¾€å¾€è¢«é™é»˜æ¥å—ï¼›æœ€åï¼Œå‘ˆç°æ–¹å¼å¾ˆé‡è¦ï¼šå¥‰æ‰¿æç¤ºé¼“åŠ±å…¬å¼€æ‰¿è®¤ï¼Œè€Œæ³„éœ²å¼æç¤ºè™½ç„¶èƒ½æé«˜å‡†ç¡®ç‡ï¼Œä½†ä¼šå¢åŠ éšå«ä¾èµ–ã€‚è¿™äº›å‘ç°è¡¨æ˜LLMæ¨ç†å—åˆ°æ·å¾„çš„ç³»ç»Ÿæ€§å½±å“ï¼Œè¿™å¯èƒ½ä¼šæ©ç›–å¿ å®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºè§£å†³æ•°å­¦å’Œé€»è¾‘ä»»åŠ¡æ—¶ï¼Œç”Ÿæˆçš„è§£é‡Šä¸åº•å±‚è®¡ç®—çš„å¿ å®æ€§æ˜¯ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚</li>
<li>æ­£ç¡®æç¤ºå¯ä»¥æ˜¾è‘—æé«˜ä»»åŠ¡å‡†ç¡®ç‡ï¼Œå°¤å…¶åœ¨éš¾åº¦è¾ƒå¤§çš„ä»»åŠ¡å’Œé€»è¾‘æ¨ç†æ–¹é¢ã€‚</li>
<li>æ¨¡å‹å¯¹æç¤ºçš„æ‰¿è®¤ç¨‹åº¦ä¸å‡ï¼Œæ–¹ç¨‹æç¤ºå¸¸è¢«æ˜ç¡®å¼•ç”¨ï¼Œè€ŒåŸå§‹ç­”æ¡ˆç­‰ç®€å•æç¤ºåˆ™å¸¸é™é»˜æ¥å—ã€‚</li>
<li>æç¤ºçš„å‘ˆç°æ–¹å¼å½±å“æ¨¡å‹çš„è¡¨ç°ï¼Œå¥‰æ‰¿å¼æç¤ºé¼“åŠ±å…¬å¼€æ‰¿è®¤ä¾èµ–ï¼Œè€Œæ³„éœ²å¼æç¤ºèƒ½æé«˜å‡†ç¡®ç‡ä½†å¯èƒ½å¢åŠ éšå«ä¾èµ–ã€‚</li>
<li>è¿™äº›å‘ç°è¡¨æ˜LLMæ¨ç†å—åˆ°æ·å¾„çš„ç³»ç»Ÿæ€§å½±å“ï¼Œå¯èƒ½å¯¼è‡´è§£é‡Šç¼ºä¹å¿ å®æ€§ã€‚</li>
<li>ç»“æœåæ˜ äº†äººç±»åé¦ˆï¼ˆRLHFï¼‰å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“ï¼Œå¦‚å¥‰æ‰¿åˆ©ç”¨äººç±»å–œçˆ±é¢ã€æ•°æ®æ³„éœ²è§¦å‘è‡ªæˆ‘å®¡æŸ¥ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c796c73236851842d658d8dca7c0668e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896240&auth_key=1760896240-0-0-021e9ef42b32c0c27d0871a6b358b524&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25c870afb46379a5229128ae227d9b32~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896247&auth_key=1760896247-0-0-3f1ebecbe04c3e42207ce52266560cf2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RADAR-A-Risk-Aware-Dynamic-Multi-Agent-Framework-for-LLM-Safety-Evaluation-via-Role-Specialized-Collaboration"><a href="#RADAR-A-Risk-Aware-Dynamic-Multi-Agent-Framework-for-LLM-Safety-Evaluation-via-Role-Specialized-Collaboration" class="headerlink" title="RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety   Evaluation via Role-Specialized Collaboration"></a>RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety   Evaluation via Role-Specialized Collaboration</h2><p><strong>Authors:Xiuyuan Chen, Jian Zhao, Yuchen Yuan, Tianle Zhang, Huilin Zhou, Zheng Zhu, Ping Hu, Linghe Kong, Chi Zhang, Weiran Huang, Xuelong Li</strong></p>
<p>Existing safety evaluation methods for large language models (LLMs) suffer from inherent limitations, including evaluator bias and detection failures arising from model homogeneity, which collectively undermine the robustness of risk evaluation processes. This paper seeks to re-examine the risk evaluation paradigm by introducing a theoretical framework that reconstructs the underlying risk concept space. Specifically, we decompose the latent risk concept space into three mutually exclusive subspaces: the explicit risk subspace (encompassing direct violations of safety guidelines), the implicit risk subspace (capturing potential malicious content that requires contextual reasoning for identification), and the non-risk subspace. Furthermore, we propose RADAR, a multi-agent collaborative evaluation framework that leverages multi-round debate mechanisms through four specialized complementary roles and employs dynamic update mechanisms to achieve self-evolution of risk concept distributions. This approach enables comprehensive coverage of both explicit and implicit risks while mitigating evaluator bias. To validate the effectiveness of our framework, we construct an evaluation dataset comprising 800 challenging cases. Extensive experiments on our challenging testset and public benchmarks demonstrate that RADAR significantly outperforms baseline evaluation methods across multiple dimensions, including accuracy, stability, and self-evaluation risk sensitivity. Notably, RADAR achieves a 28.87% improvement in risk identification accuracy compared to the strongest baseline evaluation method. </p>
<blockquote>
<p>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨è¯„ä¼°æ–¹æ³•å­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼ŒåŒ…æ‹¬è¯„ä¼°è€…åè§å’Œç”±æ¨¡å‹åŒè´¨æ€§å¼•èµ·çš„æ£€æµ‹å¤±è´¥ï¼Œè¿™äº›å±€é™æ€§å…±åŒå‰Šå¼±äº†é£é™©è¯„ä¼°è¿‡ç¨‹çš„ç¨³å¥æ€§ã€‚æœ¬æ–‡æ—¨åœ¨é€šè¿‡å¼•å…¥ä¸€ä¸ªé‡æ„åŸºç¡€é£é™©æ¦‚å¿µç©ºé—´çš„ç†è®ºæ¡†æ¶ï¼Œé‡æ–°è€ƒå¯Ÿé£é™©è¯„ä¼°èŒƒå¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ½œåœ¨é£é™©æ¦‚å¿µç©ºé—´åˆ†è§£ä¸ºä¸‰ä¸ªç›¸äº’æ’æ–¥çš„å­ç©ºé—´ï¼šæ˜¾å¼é£é™©å­ç©ºé—´ï¼ˆåŒ…å«ç›´æ¥è¿åå®‰å…¨æŒ‡å¯¼åŸåˆ™çš„é£é™©ï¼‰ã€éšå¼é£é™©å­ç©ºé—´ï¼ˆæ•æ‰éœ€è¦ä¸Šä¸‹æ–‡æ¨ç†æ‰èƒ½è¯†åˆ«çš„æ½œåœ¨æ¶æ„å†…å®¹ï¼‰ï¼Œä»¥åŠéé£é™©å­ç©ºé—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†RADARï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“åä½œè¯„ä¼°æ¡†æ¶ï¼Œå®ƒé€šè¿‡å››ä¸ªä¸“ä¸šäº’è¡¥è§’è‰²åˆ©ç”¨å¤šè½®è¾©è®ºæœºåˆ¶ï¼Œå¹¶é‡‡ç”¨åŠ¨æ€æ›´æ–°æœºåˆ¶å®ç°é£é™©æ¦‚å¿µåˆ†å¸ƒçš„è‡ªæˆ‘è¿›åŒ–ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå…¨é¢è¦†ç›–æ˜¾å¼å’Œéšå¼é£é™©ï¼ŒåŒæ—¶å‡è½»è¯„ä¼°è€…åè§ã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«800ä¸ªæŒ‘æˆ˜æ¡ˆä¾‹çš„è¯„ä¼°æ•°æ®é›†ã€‚åœ¨æˆ‘ä»¬å…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•é›†å’Œå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œè‡ªæˆ‘è¯„ä¼°é£é™©æ•æ„Ÿæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸æœ€å¼ºçš„åŸºçº¿è¯„ä¼°æ–¹æ³•ç›¸æ¯”ï¼ŒRADARåœ¨é£é™©è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢æé«˜äº†28.87%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.25271v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºå½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨è¯„ä¼°æ–¹æ³•å­˜åœ¨è¯„ä»·è€…åè§å’Œæ£€æµ‹å¤±è´¥ç­‰å†…åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„é£é™©è¯„ä»·æ¡†æ¶ï¼Œå°†æ½œåœ¨é£é™©æ¦‚å¿µç©ºé—´åˆ†è§£ä¸ºä¸‰ä¸ªç›¸äº’æ’æ–¥çš„å­ç©ºé—´ï¼šæ˜ç¡®é£é™©å­ç©ºé—´ã€éšå«é£é™©å­ç©ºé—´å’Œéé£é™©å­ç©ºé—´ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†RADARå¤šæ™ºèƒ½ä½“åä½œè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤šè½®è¾©è®ºæœºåˆ¶ï¼Œé€šè¿‡å››ä¸ªä¸“ä¸šäº’è¡¥è§’è‰²å’ŒåŠ¨æ€æ›´æ–°æœºåˆ¶å®ç°é£é™©æ¦‚å¿µåˆ†å¸ƒçš„è‡ªæˆ‘è¿›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨é£é™©è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢è¾ƒæœ€å¼ºåŸºçº¿è¯„ä»·æ–¹æ³•æé«˜äº†28.8</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬è¯„ä»·è€…åè§å’Œæ£€æµ‹å¤±è´¥ç­‰é—®é¢˜ã€‚</li>
<li>æ–°çš„é£é™©è¯„ä»·æ¡†æ¶è¢«æå‡ºï¼Œå°†æ½œåœ¨é£é™©æ¦‚å¿µç©ºé—´åˆ†è§£ä¸ºæ˜ç¡®é£é™©å­ç©ºé—´ã€éšå«é£é™©å­ç©ºé—´å’Œéé£é™©å­ç©ºé—´ã€‚</li>
<li>RADARæ¡†æ¶é‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæ–¹å¼ï¼Œé€šè¿‡å¤šè½®è¾©è®ºæœºåˆ¶è¿›è¡Œé£é™©è¯„ä¼°ã€‚</li>
<li>RADARæ¡†æ¶åŒ…å«å››ä¸ªä¸“ä¸šäº’è¡¥è§’è‰²ï¼Œå®ç°é£é™©æ¦‚å¿µåˆ†å¸ƒçš„è‡ªæˆ‘è¿›åŒ–ã€‚</li>
<li>æ–‡ç« æ„å»ºäº†åŒ…å«800ä¸ªæŒ‘æˆ˜æ¡ˆä¾‹çš„è¯„ä¼°æ•°æ®é›†ä»¥éªŒè¯RADARæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRADARåœ¨å¤šä¸ªç»´åº¦ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œè‡ªæˆ‘è¯„ä¼°é£é™©æ•æ„Ÿæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2f642c4a4702b6f3684265f4c46e7502~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896255&auth_key=1760896255-0-0-4b5595895536c382387fecb5e80116f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30927be79cd3aff272f54241a0e3c904~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896262&auth_key=1760896262-0-0-560fb0e1d14b717ffb8c3a58485a4a29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="p-less-Sampling-A-Robust-Hyperparameter-Free-Approach-for-LLM-Decoding"><a href="#p-less-Sampling-A-Robust-Hyperparameter-Free-Approach-for-LLM-Decoding" class="headerlink" title="p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding"></a>p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding</h2><p><strong>Authors:Runyan Tan, Shuang Wu, Phillip Howard</strong></p>
<p>Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. </p>
<blockquote>
<p>ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è·å¾—é«˜è´¨é‡è¾“å‡ºé€šå¸¸å–å†³äºåŸºäºé‡‡æ ·çš„è§£ç ç­–ç•¥çš„é€‰æ‹©ï¼Œè¯¥ç­–ç•¥ä»¥æ¦‚ç‡æ–¹å¼é€‰æ‹©æ¯ä¸ªç”Ÿæˆæ­¥éª¤ä¸­çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚è™½ç„¶å·²ç»æå‡ºäº†å¤šç§è¿™æ ·çš„é‡‡æ ·æ–¹æ³•ï¼Œä½†å®ƒä»¬çš„æ€§èƒ½å¯¹è¶…å‚æ•°çš„é€‰æ‹©å¾ˆæ•æ„Ÿï¼Œè¿™å¯èƒ½éœ€è¦æ ¹æ®ä¸åŒçš„ç”Ÿæˆä»»åŠ¡å’Œæ¸©åº¦é…ç½®è¿›è¡Œè®¾ç½®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†$p$-lessé‡‡æ ·ï¼šè¿™æ˜¯ä¸€ç§åŸºäºä¿¡æ¯è®ºçš„é‡‡æ ·æ–¹æ³•ï¼Œå®ƒæ ¹æ®æ•´ä¸ªä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒåŠ¨æ€åœ°åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­è®¾ç½®æˆªæ–­é˜ˆå€¼ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œ$p$-lessé‡‡æ ·æ²¡æœ‰è¶…å‚æ•°ï¼Œéšç€æ¸©åº¦çš„å‡é«˜ï¼Œå®ƒå§‹ç»ˆèƒ½äº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚æˆ‘ä»¬ä»ç†è®ºè§’åº¦ä»‹ç»äº†$p$-lessé‡‡æ ·çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒå¯¹å…¶åœ¨å¤šç§æ•°å­¦ã€é€»è¾‘æ¨ç†å’Œåˆ›é€ æ€§å†™ä½œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§è¿›è¡Œäº†å®è¯éªŒè¯ã€‚ç»“æœè¡¨æ˜ï¼Œ$p$-lessé‡‡æ ·åœ¨å¤šä¸ªæ–¹é¢å‡ä¼˜äºç°æœ‰é‡‡æ ·æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è¾ƒé«˜æ¸©åº¦ä¸‹æ–‡æœ¬è´¨é‡ä¸‹é™æ›´å°‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†$p$-lesså¦‚ä½•é€šè¿‡æ›´ä½çš„å¹³å‡ä»¤ç‰Œé‡‡æ ·æ—¶é—´å’Œæ›´çŸ­çš„ç”Ÿæˆé•¿åº¦å®ç°æ›´é«˜çš„æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å®šæ€§ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ ·æ€§è¯„ä¼°æ¥åˆ†æçªå‡º$p$-lessçš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.23234v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— å‚æ•°é‡‡æ ·ï¼ˆp-less samplingï¼‰è¿™ä¸€ä¿¡æ¯ç†è®ºé‡‡æ ·æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®æ•´ä¸ªtokenæ¦‚ç‡åˆ†å¸ƒåŠ¨æ€è®¾ç½®æˆªæ–­é˜ˆå€¼ã€‚è¯¥æ–¹æ³•æ— éœ€è°ƒæ•´è¶…å‚æ•°ï¼Œå¯åœ¨æ¸©åº¦å€¼å¢åŠ æ—¶æŒç»­äº§ç”Ÿé«˜è´¨é‡çš„è¾“å‡ºã€‚é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯ï¼Œp-lessé‡‡æ ·åœ¨å¤šä¸ªæ•°å­¦ã€é€»è¾‘æ¨ç†å’Œåˆ›é€ æ€§å†™ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ¸©ç¯å¢ƒä¸‹æ–‡æœ¬è´¨é‡ä¸‹é™æ›´å°‘ã€‚æ­¤å¤–ï¼Œp-lessé‡‡æ ·è¿˜å…·æœ‰æ›´é«˜çš„æ¨ç†æ•ˆç‡ï¼Œå¹³å‡tokené‡‡æ ·æ—¶é—´å’Œç”Ÿæˆé•¿åº¦å‡è¾ƒä½ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚æœ€åï¼Œé€šè¿‡å®šæ€§ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ ·æ€§è¯„ä¼°ï¼Œå±•ç¤ºäº†p-lessé‡‡æ ·çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>p-lessé‡‡æ ·æ˜¯ä¸€ç§åŸºäºä¿¡æ¯ç†è®ºçš„é‡‡æ ·æ–¹æ³•ï¼Œæ ¹æ®æ•´ä¸ªtokenæ¦‚ç‡åˆ†å¸ƒåŠ¨æ€è®¾ç½®æˆªæ–­é˜ˆå€¼ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œp-lessé‡‡æ ·æ— éœ€è°ƒæ•´è¶…å‚æ•°ã€‚</li>
<li>p-lessé‡‡æ ·åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ¸©ç¯å¢ƒä¸‹æ–‡æœ¬è´¨é‡ä¸‹é™æ›´å°‘ã€‚</li>
<li>p-lessé‡‡æ ·æé«˜äº†æ¨ç†æ•ˆç‡ï¼Œå…·æœ‰æ›´å¿«çš„å¹³å‡tokené‡‡æ ·æ—¶é—´å’Œæ›´çŸ­çš„ç”Ÿæˆé•¿åº¦ã€‚</li>
<li>p-lessé‡‡æ ·ä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å®šæ€§ç¤ºä¾‹ã€æ¡ˆä¾‹ç ”ç©¶å’Œå¤šæ ·æ€§è¯„ä¼°ï¼Œå±•ç¤ºäº†p-lessé‡‡æ ·çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f75af9a939d272f26ad8b81b20587ff4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896270&auth_key=1760896270-0-0-09b15d34d136d277896223656a1cca47&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-eb1166edcfe1f484574aeff3852399a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896277&auth_key=1760896277-0-0-4e1468cd1042a922662f3ed87b4b74e4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5d7df353ea675a241831b2c36cde26fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896284&auth_key=1760896284-0-0-ae8b364ecef26c86276b1ffa0822debd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b5aab67e9914294d066e179b7c18ca1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896291&auth_key=1760896291-0-0-1c54e2bda93d2285642682ff2f61ff66&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Geo-R1-Improving-Few-Shot-Geospatial-Referring-Expression-Understanding-with-Reinforcement-Fine-Tuning"><a href="#Geo-R1-Improving-Few-Shot-Geospatial-Referring-Expression-Understanding-with-Reinforcement-Fine-Tuning" class="headerlink" title="Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding   with Reinforcement Fine-Tuning"></a>Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding   with Reinforcement Fine-Tuning</h2><p><strong>Authors:Zilun Zhang, Zian Guan, Tiancheng Zhao, Haozhan Shen, Tianyu Li, Yuxiang Cai, Zhonggen Su, Zhaojun Liu, Jianwei Yin, Xiang Li</strong></p>
<p>Referring expression understanding in remote sensing poses unique challenges, as it requires reasoning over complex object-context relationships. While supervised fine-tuning (SFT) on multimodal large language models achieves strong performance with massive labeled datasets, they struggle in data-scarce scenarios, leading to poor generalization. To address this limitation, we propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm for few-shot geospatial referring. Geo-R1 enforces the model to first generate explicit, interpretable reasoning chains that decompose referring expressions, and then leverage these rationales to localize target objects. This â€œreason first, then actâ€ process enables the model to make more effective use of limited annotations, enhances generalization, and provides interpretability. We validate Geo-R1 on three carefully designed few-shot geospatial referring benchmarks, where our model consistently and substantially outperforms SFT baselines. It also demonstrates strong cross-dataset generalization, highlighting its robustness. Code and data will be released at: <a target="_blank" rel="noopener" href="https://github.com/Geo-R1/geo-r1">https://github.com/Geo-R1/geo-r1</a>. </p>
<blockquote>
<p>é¥æ„Ÿä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒéœ€è¦å¯¹å¤æ‚çš„å¯¹è±¡ä¸Šä¸‹æ–‡å…³ç³»è¿›è¡Œæ¨ç†ã€‚è™½ç„¶åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§é‡æ ‡è®°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬çš„è¡¨ç°å´ä¸å°½å¦‚äººæ„ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Geo-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èŒƒå¼ï¼Œç”¨äºè¿›è¡Œå°‘æ ·æœ¬åœ°ç†ç©ºé—´æŒ‡ä»£ã€‚Geo-R1å¼ºåˆ¶æ¨¡å‹é¦–å…ˆç”Ÿæˆæ˜ç¡®ã€å¯è§£é‡Šçš„æ¨ç†é“¾æ¥åˆ†è§£æŒ‡ä»£è¡¨è¾¾å¼ï¼Œç„¶ååˆ©ç”¨è¿™äº›æ¨ç†æ¥å®šä½ç›®æ ‡å¯¹è±¡ã€‚è¿™ç§â€œå…ˆæ¨ç†ï¼Œåè¡ŒåŠ¨â€çš„è¿‡ç¨‹ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ³¨é‡Šï¼Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡çš„å°‘æ ·æœ¬åœ°ç†ç©ºé—´æŒ‡ä»£åŸºå‡†æµ‹è¯•ä¸Šå¯¹Geo-R1è¿›è¡Œäº†éªŒè¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆä¸”å¤§å¹…åº¦åœ°è¶…è¶Šäº†SFTåŸºå‡†æµ‹è¯•ã€‚å®ƒè¿˜è¡¨ç°å‡ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Geo-R1/geo-r1%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Geo-R1/geo-r1ä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.21976v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹æ„Ÿåº”ä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€å¯¹å¤æ‚çš„å¯¹è±¡ä¸Šä¸‹æ–‡å…³ç³»è¿›è¡Œæ¨ç†ã€‚è™½ç„¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šåˆ©ç”¨å¤§é‡æ ‡æ³¨æ•°æ®é›†å–å¾—äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGeo-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èŒƒå¼ï¼Œç”¨äºè¿›è¡Œå°‘é‡åœ°ç†ç©ºé—´æŒ‡ä»£ã€‚Geo-R1å¼ºåˆ¶æ¨¡å‹é¦–å…ˆç”Ÿæˆæ˜ç¡®ã€å¯è§£é‡Šçš„æ¨ç†é“¾ï¼Œåˆ†è§£æŒ‡ä»£è¡¨è¾¾å¼ï¼Œç„¶ååˆ©ç”¨è¿™äº›ç†æ€§æ¥å®šä½ç›®æ ‡å¯¹è±¡ã€‚è¿™ç§â€œå…ˆæ¨ç†ï¼Œåè¡ŒåŠ¨â€çš„è¿‡ç¨‹ä½¿æ¨¡å‹æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æœ‰é™çš„æ³¨é‡Šï¼Œå¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æä¾›äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªç²¾å¿ƒè®¾è®¡çš„å°‘é‡åœ°ç†ç©ºé—´æŒ‡ä»£åŸºå‡†ä¸ŠéªŒè¯äº†Geo-R1ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æŒç»­ä¸”å¤§å¹…åº¦åœ°è¶…è¶Šäº†SFTåŸºå‡†æµ‹è¯•ã€‚å®ƒè¿˜å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ï¼Œå‡¸æ˜¾äº†å…¶ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹æ„Ÿåº”ä¸­çš„æŒ‡ä»£è¡¨è¾¾å¼ç†è§£éœ€è¦å¤„ç†å¤æ‚çš„å¯¹è±¡ä¸Šä¸‹æ–‡å…³ç³»ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¡¨ç°å¼ºå¤§ï¼Œä½†åœ¨æ•°æ®ç¨€ç¼ºæ—¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>Geo-R1æ˜¯ä¸€ç§å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èŒƒå¼ï¼Œç”¨äºè§£å†³å°‘é‡åœ°ç†ç©ºé—´æŒ‡ä»£é—®é¢˜ã€‚</li>
<li>Geo-R1é€šè¿‡ç”Ÿæˆæ¨ç†é“¾å¹¶åˆ©ç”¨å®ƒä»¬æ¥å®šä½ç›®æ ‡å¯¹è±¡ï¼Œå®ç°â€œå…ˆæ¨ç†ï¼Œåè¡ŒåŠ¨â€çš„è¿‡ç¨‹ã€‚</li>
<li>Geo-R1æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æŒç»­è¶…è¶Šç›‘ç£å¾®è°ƒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>Geo-R1æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.21976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b469fb31430db349d67f62c7861e49fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896298&auth_key=1760896298-0-0-c60a7ed0f774d6c11cf6ff25102f8a11&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-994db49426bf0366cabd4d19111fa23b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896305&auth_key=1760896305-0-0-daefc5e45f00e2aabcd2f4b4dbd257f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7821f1f8223ce5a782bbe37b2b9682c5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896312&auth_key=1760896312-0-0-8c6f6e73538f540075f338c6d75f3b78&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c712fe35903eb4dcf028fa8f5c1c9c4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896319&auth_key=1760896319-0-0-dd37acfb92339d57b2fa832d501152a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c73613c190cee85accb10e9ea1075930~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896326&auth_key=1760896326-0-0-652dae526f5edbcf51de1ad7676698f4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CE-GPPO-Coordinating-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning"><a href="#CE-GPPO-Coordinating-Entropy-via-Gradient-Preserving-Clipping-Policy-Optimization-in-Reinforcement-Learning" class="headerlink" title="CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy   Optimization in Reinforcement Learning"></a>CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy   Optimization in Reinforcement Learning</h2><p><strong>Authors:Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou</strong></p>
<p>Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \textbf{C}oordinating \textbf{E}ntropy via \textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„æœ‰åŠ›èŒƒå¼ã€‚æ­¤è¿‡ç¨‹ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºç®¡ç†ç­–ç•¥ç†µï¼Œè¿™åæ˜ äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚ç°æœ‰æ–¹æ³•ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŠå…¶å˜ä½“ï¼Œç”±äºè£å‰ªæœºåˆ¶è€Œä¸¢å¼ƒäº†æ¥è‡ªä½æ¦‚ç‡æ ‡è®°çš„æœ‰ä»·å€¼æ¢¯åº¦ä¿¡å·ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†ç†µåŠ¨æ€ï¼Œå¹¶å‘ç°è¿™äº›è¢«è£å‰ªçš„æ ‡è®°åœ¨è°ƒèŠ‚ç†µæ¼”åŒ–æ–¹é¢èµ·ç€å…³é”®ä½†è¢«å¿½è§†çš„ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†é€šè¿‡æ¢¯åº¦ä¿ç•™ç­–ç•¥ä¼˜åŒ–åè°ƒç†µï¼ˆCE-GPPOï¼‰çš„æ–°ç®—æ³•ï¼Œå®ƒä»¥æ¸©å’Œä¸”æœ‰é™çš„æ–¹å¼åœ¨åŸç”ŸPPOä¸­é‡æ–°å¼•å…¥äº†è¢«è£å‰ªæ ‡è®°çš„æ¢¯åº¦ã€‚é€šè¿‡æ§åˆ¶æ¥è‡ªè£å‰ªåŒºé—´å¤–æ ‡è®°çš„æ¢¯åº¦å¹…åº¦ï¼ŒCE-GPPOèƒ½å¤Ÿå®ç°æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºè¯æ˜å’Œå®éªŒè¯æ®ï¼Œè¡¨æ˜CE-GPPOæœ‰æ•ˆåœ°ç¼“è§£äº†ç†µä¸ç¨³å®šæ€§çš„é—®é¢˜ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCE-GPPOåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šå§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20712v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„æœ‰åŠ›å·¥å…·ã€‚æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºç®¡ç†ç­–ç•¥ç†µï¼Œå³è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚ç°æœ‰æ–¹æ³•ï¼ˆå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰åŠå…¶å˜ä½“ï¼‰ç”±äºè£å‰ªæœºåˆ¶è€Œä¸¢å¼ƒäº†æ¥è‡ªä½æ¦‚ç‡æ ‡è®°çš„æœ‰ä»·å€¼æ¢¯åº¦ä¿¡å·ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°åˆ†æäº†ç†µåŠ¨åŠ›å­¦ï¼Œå¹¶æ­ç¤ºäº†è¿™äº›è¢«è£å‰ªçš„æ ‡è®°åœ¨è°ƒèŠ‚ç†µæ¼”åŒ–æ–¹é¢å‘æŒ¥ç€é‡è¦å´è¢«å¿½è§†çš„ä½œç”¨ã€‚æˆ‘ä»¬æå‡ºäº†åè°ƒç†µæ¢¯åº¦ä¿ç•™ç­–ç•¥ä¼˜åŒ–ï¼ˆCE-GPPOï¼‰çš„æ–°ç®—æ³•ï¼Œä»¥æ¸©å’Œå’Œæœ‰ç•Œçš„æ–¹å¼é‡æ–°å¼•å…¥äº†åŸç”ŸPPOä¸­è¢«è£å‰ªçš„æ ‡è®°çš„æ¢¯åº¦ã€‚é€šè¿‡æ§åˆ¶æ¥è‡ªè£å‰ªåŒºé—´å¤–æ ‡è®°çš„æ¢¯åº¦å¹…åº¦ï¼ŒCE-GPPOèƒ½å¤Ÿå®ç°æ¢ç´¢ä¸åˆ©ç”¨ä¹‹é—´çš„å¹³è¡¡ã€‚æˆ‘ä»¬æä¾›äº†ç†è®ºè¯æ˜å’Œå®éªŒè¯æ®è¡¨æ˜CE-GPPOæœ‰æ•ˆåœ°ç¼“è§£äº†ç†µä¸ç¨³å®šé—®é¢˜ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCE-GPPOåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸Šå‡ä¼˜äºå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç­–ç•¥ç†µç®¡ç†æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæ¶‰åŠæ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚PPOå› è£å‰ªæœºåˆ¶è€Œå¿½ç•¥ä½æ¦‚ç‡æ ‡è®°çš„æ¢¯åº¦ä¿¡å·ã€‚</li>
<li>è¢«è£å‰ªçš„æ ‡è®°åœ¨è°ƒèŠ‚ç†µæ¼”åŒ–ä¸­èµ·é‡è¦ä½œç”¨ã€‚</li>
<li>æ–°ç®—æ³•CE-GPPOé‡æ–°å¼•å…¥è¢«è£å‰ªæ ‡è®°çš„æ¢¯åº¦ï¼Œå®ç°æ¸©å’Œæœ‰ç•Œçš„ç­–ç•¥ä¼˜åŒ–ã€‚</li>
<li>CE-GPPOé€šè¿‡æ§åˆ¶æ¢¯åº¦å¹…åº¦å®ç°æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dffb380132c3a39e2281c3e3567d1279~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896333&auth_key=1760896333-0-0-92fa758d1a3c81bdb50fec92edf010a2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-98c2ce552ed747bb7169afe07c7e6255~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896340&auth_key=1760896340-0-0-7ad85bf053200648e6e6d4b1e5a20597&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-727df3d1a6353ab0c498dc36a96df317~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896347&auth_key=1760896347-0-0-bbc7d5a14fed35cf7d984020bc14c734&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a7ddf701890df0d970dbeacca113dcc9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896353&auth_key=1760896353-0-0-6ee5d2ce6a0c6fd014c91279eb5668d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe8232da0f1cbe9d18dbe46cacc5d890~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896362&auth_key=1760896362-0-0-e5e4133226b8e6dbb53a1a6d6132674d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Easy-to-Hard-The-MIR-Benchmark-for-Progressive-Interleaved-Multi-Image-Reasoning"><a href="#From-Easy-to-Hard-The-MIR-Benchmark-for-Progressive-Interleaved-Multi-Image-Reasoning" class="headerlink" title="From Easy to Hard: The MIR Benchmark for Progressive Interleaved   Multi-Image Reasoning"></a>From Easy to Hard: The MIR Benchmark for Progressive Interleaved   Multi-Image Reasoning</h2><p><strong>Authors:Hang Du, Jiayang Zhang, Guoshun Nan, Wendi Deng, Zhenyan Chen, Chenyang Zhang, Wang Xiao, Shan Huang, Yuqi Pan, Tao Qi, Sicong Leng</strong></p>
<p>Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language Models (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks. While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images. To enhance MLLMs ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an â€œeasy to hardâ€ approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks. Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models reasoning performance on MIR and other established benchmarks. We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs capability to handle complex inter-modal tasks. </p>
<blockquote>
<p>å¤šå›¾åƒäº¤é”™æ¨ç†æ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤šä¸ªå›¾åƒåŠå…¶ç›¸å…³æ–‡æœ¬ä¸Šä¸‹æ–‡ä¸­çš„è”åˆç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¸¦æ¥äº†è¶…å‡ºå•å›¾åƒæˆ–éäº¤é”™å¤šå›¾åƒä»»åŠ¡çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚å½“å‰çš„å¤šå›¾åƒåŸºå‡†æµ‹è¯•å¿½è§†äº†äº¤é”™çš„æ–‡æœ¬ä¸Šä¸‹æ–‡ä»¥åŠå•ä¸ªå›¾åƒä¸å…¶ç›¸å…³æ–‡æœ¬ä¹‹é—´çš„ç‹¬ç‰¹å…³ç³»ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šå›¾åƒäº¤é”™æ•°æ®ä¸Šè¿›è¡Œæ¨ç†å¯èƒ½ä¼šæ˜¾è‘—å¢å¼ºå…¶å¯¹å¤æ‚åœºæ™¯çš„ç†è§£ï¼Œå¹¶æ›´å¥½åœ°æ•æ‰è·¨æ¨¡æ€ç›¸å…³æ€§ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•MIRï¼Œå®ƒè¦æ±‚åœ¨å¤šå›¾åƒä¼´éšäº¤é”™æ–‡æœ¬ä¸Šä¸‹æ–‡çš„æƒ…å¢ƒä¸‹è¿›è¡Œè”åˆæ¨ç†ï¼Œä»¥å‡†ç¡®åœ°å°†å›¾åƒåŒºåŸŸä¸ç›¸åº”çš„æ–‡æœ¬ç›¸å…³è”ï¼Œå¹¶åœ¨å›¾åƒä¹‹é—´é€»è¾‘åœ°è¿æ¥ä¿¡æ¯ã€‚ä¸ºäº†æé«˜MLLMså¯¹å¤šå›¾åƒäº¤é”™æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸­å¼•å…¥äº†é’ˆå¯¹æ¯ä¸ªå®ä¾‹çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚è¯¥ç­–ç•¥éµå¾ªâ€œä»æ˜“åˆ°éš¾â€çš„æ–¹æ³•ï¼Œé€æ­¥å¼•å¯¼æ¨¡å‹ä»ç®€å•åˆ°å¤æ‚çš„åœºæ™¯ï¼Œä»è€Œæé«˜å…¶å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¯¹å¤šä¸ªMLLMsçš„å¹¿æ³›åŸºå‡†æµ‹è¯•å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MIRå’Œå…¶ä»–æ—¢å®šåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒMIRå°†é¼“åŠ±å¯¹å¤šå›¾åƒäº¤é”™æ¨ç†çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæ¨åŠ¨MLLMså¤„ç†å¤æ‚è·¨æ¨¡æ€ä»»åŠ¡çš„èƒ½åŠ›çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17040v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>å¤šå›¾åƒäº¤é”™æ¨ç†æ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯¹å¤šä¸ªå›¾åƒåŠå…¶ç›¸å…³æ–‡æœ¬ä¸Šä¸‹æ–‡çš„è”åˆç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™å¸¦æ¥äº†è¶…è¶Šå•å›¾åƒæˆ–éäº¤é”™å¤šå›¾åƒä»»åŠ¡çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚é’ˆå¯¹å½“å‰å¤šå›¾åƒåŸºå‡†æµ‹è¯•å¯¹äº¤é”™æ–‡æœ¬ä¸Šä¸‹æ–‡çš„å¿½è§†ä»¥åŠå•ä¸ªå›¾åƒä¸å…³è”æ–‡æœ¬ä¹‹é—´å…³ç³»çš„å¿½è§†ï¼Œæˆ‘ä»¬å¼•å…¥äº†MIRåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚è¿›è¡Œå¤šå›¾åƒäº¤é”™æ•°æ®çš„è”åˆæ¨ç†ï¼Œä»¥å‡†ç¡®åœ°å°†å›¾åƒåŒºåŸŸä¸ç›¸åº”æ–‡æœ¬ç›¸å…³è”ï¼Œå¹¶åœ¨å›¾åƒä¹‹é—´é€»è¾‘è¿æ¥ä¿¡æ¯ã€‚ä¸ºäº†å¢å¼ºMLLMså¯¹å¤šå›¾åƒäº¤é”™æ•°æ®çš„ç†è§£èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨åŸºå‡†æµ‹è¯•ä¸­ä¸ºæ¯ä¸ªå®ä¾‹å¼•å…¥äº†æ¨ç†æ­¥éª¤ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µå­¦ä¹ ç­–ç•¥ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°å¤šä¸ªMLLMsï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨MIRå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šçš„æ¨ç†æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒMIRå°†ä¿ƒè¿›å¤šå›¾åƒäº¤é”™æ¨ç†çš„è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæ¨åŠ¨MLLMsåœ¨å¤„ç†å¤æ‚è·¨æ¨¡æ€ä»»åŠ¡çš„èƒ½åŠ›çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå›¾åƒäº¤é”™æ¨ç†æ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å¯¹å¤šä¸ªå›¾åƒå’Œæ–‡æœ¬çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¤šå›¾åƒåŸºå‡†æµ‹è¯•å¿½è§†äº†äº¤é”™æ–‡æœ¬ä¸Šä¸‹æ–‡åŠå›¾åƒä¸æ–‡æœ¬é—´çš„ç‹¬ç‰¹å…³ç³»ã€‚</li>
<li>MIRåŸºå‡†æµ‹è¯•è¦æ±‚æ¨¡å‹è¿›è¡Œå¤šå›¾åƒäº¤é”™æ•°æ®çš„è”åˆæ¨ç†ï¼Œå‡†ç¡®å…³è”å›¾åƒå’Œæ–‡æœ¬ã€‚</li>
<li>ä¸ºå¢å¼ºMLLMså¯¹å¤šå›¾åƒäº¤é”™æ•°æ®çš„ç†è§£ï¼Œå¼•å…¥äº†æ¨ç†æ­¥éª¤å’Œåˆ†é˜¶æ®µå­¦ä¹ ç­–ç•¥ã€‚</li>
<li>åˆ†é˜¶æ®µå­¦ä¹ ç­–ç•¥é‡‡ç”¨â€œç”±æ˜“åˆ°éš¾â€çš„æ–¹æ³•ï¼Œé€æ­¥æå‡æ¨¡å‹çš„åº”å¯¹å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨MIRå’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17040">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-12c93a27b4b4d1c1b8904e4f45dfd7e6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896369&auth_key=1760896369-0-0-5faf1eff043343a0d27b7a387fcfc229&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0cedb58532d02351877d91f48990e022~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896377&auth_key=1760896377-0-0-01b431faa00434c8461acd636488de09&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8b9b04d7a59f46a811370d63828265a1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896384&auth_key=1760896384-0-0-86752575f597b29af04cf22c6f852be4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c2a40713b9bc4259e638ef6ed1ddb503~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896391&auth_key=1760896391-0-0-4336b225b85ec9622913781423a068b1&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-94f0f9a16ade460bbf778d060d0b3f74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896398&auth_key=1760896398-0-0-fd0bebef4056abad1574dd0b03d3023b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6a4bc9e25ba656e680abaa782c4f570e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896405&auth_key=1760896405-0-0-d6263e265bf28272214ac2e8e439357a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="EdiVal-Agent-An-Object-Centric-Framework-for-Automated-Fine-Grained-Evaluation-of-Multi-Turn-Editing"><a href="#EdiVal-Agent-An-Object-Centric-Framework-for-Automated-Fine-Grained-Evaluation-of-Multi-Turn-Editing" class="headerlink" title="EdiVal-Agent: An Object-Centric Framework for Automated, Fine-Grained   Evaluation of Multi-Turn Editing"></a>EdiVal-Agent: An Object-Centric Framework for Automated, Fine-Grained   Evaluation of Multi-Turn Editing</h2><p><strong>Authors:Tianyu Chen, Yasi Zhang, Zhi Zhang, Peiyu Yu, Shu Wang, Zhendong Wang, Kevin Lin, Xiaofei Wang, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Jianwen Xie, Oscar Leong, Lijuan Wang, Ying Nian Wu, Mingyuan Zhou</strong></p>
<p>Instruction-based image editing has advanced rapidly, yet reliable and interpretable evaluation remains a bottleneck. Current protocols either (i) depend on paired reference images-resulting in limited coverage and inheriting biases from prior generative models-or (ii) rely solely on zero-shot vision-language models (VLMs), whose prompt-based assessments of instruction following, content consistency, and visual quality are often imprecise. To address this, we introduce EdiVal-Agent, an automated and fine-grained evaluation framework grounded in an object-centric perspective, designed to assess not only standard single-turn but also multi-turn instruction-based editing with precision. Given an input image, EdiVal-Agent first decomposes it into semantically meaningful objects, then synthesizes diverse, context-aware editing instructions while dynamically updating object pools across turns. These two stages enable two novel object-centric metrics tailored for multi-turn evaluation and one global metric of visual quality: (1) EdiVal-IF, which measures instruction following by combining open-vocabulary object detectors for symbolic checks with VLMs for semantic verification on detector-guided crops; (2) EdiVal-CC, which evaluates content consistency by calculating semantic similarity of unchanged objects and background using the evolving object pools; and (3) EdiVal-VQ, which quantifies changes in overall visual quality with human preference models. Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing benchmark covering 9 instruction types and 13 state-of-the-art editing models spanning in-context, flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be used to identify existing failure modes, thereby informing the development of the next generation of editing models. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æŠ€æœ¯å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†å¯é ä¸”å¯è§£é‡Šçš„è¯„ä»·ä»ç„¶æ˜¯ä¸€ä¸ªç“¶é¢ˆã€‚å½“å‰çš„è¯„ä»·åè®®è¦ä¹ˆï¼ˆiï¼‰ä¾èµ–äºé…å¯¹å‚è€ƒå›¾åƒï¼Œè¿™å¯¼è‡´è¦†ç›–èŒƒå›´æœ‰é™å¹¶ç»§æ‰¿äº†å…ˆå‰ç”Ÿæˆæ¨¡å‹çš„åè§ï¼›è¦ä¹ˆï¼ˆiiï¼‰å®Œå…¨ä¾èµ–äºé›¶æ ·æœ¬è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹åŸºäºæç¤ºå¯¹æŒ‡ä»¤éµå¾ªã€å†…å®¹ä¸€è‡´æ€§å’Œè§†è§‰è´¨é‡çš„è¯„ä¼°å¾€å¾€ä¸ç²¾ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EdiVal-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„è‡ªåŠ¨åŒ–ç²¾ç»†è¯„ä»·æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ä»…ç²¾ç¡®è¯„ä¼°æ ‡å‡†å•è½®æŒ‡ä»¤ç¼–è¾‘ï¼Œè€Œä¸”è¿˜è¯„ä¼°å¤šè½®æŒ‡ä»¤ç¼–è¾‘ã€‚ç»™å®šè¾“å…¥å›¾åƒï¼ŒEdiVal-Agenté¦–å…ˆå°†å…¶åˆ†è§£ä¸ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å¯¹è±¡ï¼Œç„¶ååˆæˆå¤šæ ·ä¸”ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç¼–è¾‘æŒ‡ä»¤ï¼ŒåŒæ—¶åœ¨å„è½®ä¹‹é—´åŠ¨æ€æ›´æ–°å¯¹è±¡æ± ã€‚è¿™ä¸¤ä¸ªé˜¶æ®µä½¿é’ˆå¯¹å¤šè½®è¯„ä¼°çš„ä¸¤ä¸ªæ–°å‹å¯¹è±¡ä¸­å¿ƒåº¦é‡å’Œä¸€ä¸ªå…¨å±€è§†è§‰è´¨é‡åº¦é‡æˆä¸ºå¯èƒ½ï¼šï¼ˆ1ï¼‰EdiVal-IFï¼Œå®ƒé€šè¿‡ç»“åˆå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨è¿›è¡Œç¬¦å·æ£€æŸ¥å’Œä½¿ç”¨VLMså¯¹æ£€æµ‹å™¨å¼•å¯¼è£å‰ªè¿›è¡Œè¯­ä¹‰éªŒè¯ï¼Œæ¥æµ‹é‡æŒ‡ä»¤éµå¾ªæƒ…å†µï¼›ï¼ˆ2ï¼‰EdiVal-CCï¼Œå®ƒé€šè¿‡è®¡ç®—ä¸å˜å¯¹è±¡å’ŒèƒŒæ™¯ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œä½¿ç”¨ä¸æ–­å‘å±•çš„å¯¹è±¡æ± æ¥è¯„ä¼°å†…å®¹ä¸€è‡´æ€§ï¼›ï¼ˆ3ï¼‰EdiVal-VQï¼Œå®ƒåˆ©ç”¨äººç±»åå¥½æ¨¡å‹é‡åŒ–æ€»ä½“è§†è§‰è´¨é‡çš„å˜åŒ–ã€‚é€šè¿‡å®ç°æ­¤ç®¡é“ï¼Œæˆ‘ä»¬æ„å»ºäº†EdiVal-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–9ç§æŒ‡ä»¤ç±»å‹å’Œ13ç§åŒ…æ‹¬ä¸Šä¸‹æ–‡ã€æµç¨‹åŒ¹é…å’Œæ‰©æ•£èŒƒå¼åœ¨å†…çš„å…ˆè¿›ç¼–è¾‘æ¨¡å‹çš„å¤šè½®ç¼–è¾‘åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒEdiVal-Agentå¯ç”¨äºè¯†åˆ«ç°æœ‰å¤±è´¥æ¨¡å¼ï¼Œä»è€Œä¸ºä¸‹ä¸€ä»£ç¼–è¾‘æ¨¡å‹çš„å¼€å‘æä¾›ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13399v2">PDF</a> Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan   Wang, Ying Nian Wu, and Mingyuan Zhou advised equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘è¯„ä¼°çš„ç“¶é¢ˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è‡ªåŠ¨åŒ–ã€ç²¾ç»†åŒ–çš„è¯„ä¼°æ¡†æ¶EdiVal-Agentã€‚è¯¥æ¡†æ¶ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒï¼Œä¸ä»…èƒ½ç²¾ç¡®è¯„ä¼°å•è½®æŒ‡ä»¤ç¼–è¾‘ï¼Œè¿˜èƒ½è¯„ä¼°å¤šè½®æŒ‡ä»¤ç¼–è¾‘ã€‚é€šè¿‡åˆ†è§£å›¾åƒå’Œåˆæˆå¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡ç¼–è¾‘æŒ‡ä»¤ï¼ŒEdiVal-Agentæä¾›äº†ä¸‰ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šEdiVal-IFã€EdiVal-CCå’ŒEdiVal-VQï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼°æŒ‡ä»¤éµå¾ªã€å†…å®¹ä¸€è‡´æ€§å’Œæ•´ä½“è§†è§‰è´¨é‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ„å»ºäº†EdiVal-BenchåŸºå‡†æµ‹è¯•é›†ï¼Œå±•ç¤ºäº†EdiVal-Agentåœ¨å¤šç§æŒ‡ä»¤ç±»å‹å’Œç¼–è¾‘æ¨¡å‹è¯„ä¼°ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘è¯„ä¼°é¢ä¸´ç“¶é¢ˆï¼Œéœ€è¦æ›´å¯é å’Œå¯è§£é‡Šçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>EdiVal-Agentæ˜¯ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–ã€ç²¾ç»†åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»¥å¯¹è±¡ä¸ºä¸­å¿ƒï¼Œèƒ½ç²¾ç¡®è¯„ä¼°å•è½®å’Œå¤šè½®æŒ‡ä»¤ç¼–è¾‘ã€‚</li>
<li>EdiVal-Agenté€šè¿‡åˆ†è§£å›¾åƒå’Œåˆæˆå¤šæ ·åŒ–çš„ä¸Šä¸‹æ–‡ç¼–è¾‘æŒ‡ä»¤ï¼Œæä¾›ä¸‰ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼šEdiVal-IFã€EdiVal-CCå’ŒEdiVal-VQã€‚</li>
<li>EdiVal-IFç»“åˆå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨å’ŒVLMsï¼Œå¯¹æŒ‡ä»¤éµå¾ªè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>EdiVal-CCé€šè¿‡è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¯„ä¼°å†…å®¹ä¸€è‡´æ€§ã€‚</li>
<li>EdiVal-VQé‡åŒ–æ•´ä½“è§†è§‰è´¨é‡çš„å˜åŒ–ï¼Œä¸äººç±»åå¥½æ¨¡å‹ç›¸ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4a95ee621e46880f71cefa0625ab1d3a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896412&auth_key=1760896412-0-0-bcb6ab11552f2d239b7be437f822b33e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed6da4b3aa70b4fbdcdc0d2ba171b467~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896420&auth_key=1760896420-0-0-7e7dca0b6ba899b8083ca6a30101c43e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0af200ccaaafaf1f2160f93e82be680~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896427&auth_key=1760896427-0-0-c341ba30bf3210e869965cc424a20d45&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2fbfcfb6de0da7035f5206a3bb439d78~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896454&auth_key=1760896454-0-0-5baf832bd18a41dd9d9aaac37650e771&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-21c28ea44505624a5f550ecaaf0bea35~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896461&auth_key=1760896461-0-0-9ba9253465d60af2f736956de0166515&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6383a884abd504f6259312be1bd22483~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896467&auth_key=1760896467-0-0-9e4e98a3f9ef23744c5408d9961bdf8d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LaV-CoT-Language-Aware-Visual-CoT-with-Multi-Aspect-Reward-Optimization-for-Real-World-Multilingual-VQA"><a href="#LaV-CoT-Language-Aware-Visual-CoT-with-Multi-Aspect-Reward-Optimization-for-Real-World-Multilingual-VQA" class="headerlink" title="LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization   for Real-World Multilingual VQA"></a>LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization   for Real-World Multilingual VQA</h2><p><strong>Authors:Jing Huang, Zhiya Tan, Shutao Gong, Fanwei Zeng, Joey Tianyi Zhou, Changtao Miao, Huazhe Tan, Weibin Yao, Jianshu Li</strong></p>
<p>As large vision language models (VLMs) advance, their capabilities in multilingual visual question answering (mVQA) have significantly improved. Chain-of-thought (CoT) reasoning has been proven to enhance interpretability and complex reasoning. However, most existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining their deployment in real-world applications. To address this gap, we introduce LaV-CoT, the first Language-aware Visual CoT framework with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable multi-stage reasoning pipeline consisting of Text Summary with Bounding Box (BBox), Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an automated data curation method that generates multilingual CoT annotations through iterative generation, correction, and refinement, enabling scalable and high-quality training data. To improve reasoning and generalization, LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT) with Language-aware Group Relative Policy Optimization (GRPO), guided by verifiable multi-aspect rewards including language consistency, structural accuracy, and semantic alignment. Extensive evaluations on public datasets including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up to ~9.5% accuracy improvements over open-source baselines of similar size and even surpasses models with 2$\times$ larger scales by ~2.6%. Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513 and Gemini-2.5-flash. We further conducted an online A&#x2F;B test to validate our method on real-world data, highlighting its effectiveness for industrial deployment. Our code is available at this link: <a target="_blank" rel="noopener" href="https://github.com/HJNVR/LaV-CoT">https://github.com/HJNVR/LaV-CoT</a> </p>
<blockquote>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥ï¼Œå®ƒä»¬åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”ï¼ˆmVQAï¼‰æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å·²ç»è¢«è¯æ˜å¯ä»¥æé«˜è§£é‡Šæ€§å’Œå¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬CoTï¼Œå¯¹å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†çš„æ”¯æŒæœ‰é™ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†LaV-CoTï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¸¦æœ‰å¤šæ–¹é¢å¥–åŠ±ä¼˜åŒ–çš„è¯­è¨€æ„ŸçŸ¥è§†è§‰CoTæ¡†æ¶ã€‚LaV-CoTé‡‡ç”¨äº†ä¸€ä¸ªå¯è§£é‡Šçš„å¤šé˜¶æ®µæ¨ç†ç®¡é“ï¼ŒåŒ…æ‹¬å¸¦æœ‰è¾¹ç•Œæ¡†ï¼ˆBBoxï¼‰çš„æ–‡æœ¬æ‘˜è¦ã€è¯­è¨€è¯†åˆ«ã€ç©ºé—´å¯¹è±¡çº§æè¿°å’Œé€æ­¥é€»è¾‘æ¨ç†ã€‚éµå¾ªè¿™ä¸€æ¨ç†ç®¡é“ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆã€ä¿®æ­£å’Œç»†åŒ–ç”Ÿæˆå¤šè¯­è¨€CoTæ³¨é‡Šï¼Œå®ç°å¯æ‰©å±•å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚ä¸ºäº†æé«˜æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒLaV-CoTé‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè¯­è¨€æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥å¯éªŒè¯çš„å¤šæ–¹é¢å¥–åŠ±ä¸ºæŒ‡å¯¼ï¼ŒåŒ…æ‹¬è¯­è¨€ä¸€è‡´æ€§ã€ç»“æ„å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚åœ¨å…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬MMMBã€å¤šè¯­è¨€MMBenchå’ŒMTVQAï¼‰ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œä¸ç±»ä¼¼è§„æ¨¡çš„å¼€æºåŸºçº¿ç›¸æ¯”ï¼ŒLaV-CoTçš„å‡†ç¡®åº¦æé«˜äº†çº¦9.5%ï¼Œç”šè‡³è¶…è¿‡äº†è§„æ¨¡å¤§ä¸¤å€çš„æ¨¡å‹çº¦2.6%ã€‚æ­¤å¤–ï¼ŒLaV-CoTåœ¨GPT-4o-0513å’ŒGemini-2.5-flashç­‰å…ˆè¿›ä¸“æœ‰æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œäº†åœ¨çº¿ABæµ‹è¯•ï¼Œä»¥åœ¨ç°å®ä¸–ç•Œçš„æ•°æ®åº“ä¸­éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨å·¥ä¸šéƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/HJNVR/LaV-CoT">https://github.com/HJNVR/LaV-CoT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10026v3">PDF</a> 12 Pages, 12 Figures, 3 Tables</p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥ï¼Œå®ƒä»¬åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”ï¼ˆmVQAï¼‰æ–¹é¢çš„èƒ½åŠ›æ˜¾è‘—æé«˜ã€‚ä¸ºäº†æ”¹è¿›æ¨ç†å¹¶æå‡åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²èƒ½åŠ›ï¼Œæœ¬æ–‡å¼•å…¥äº†LaV-CoTï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å¤šæ–¹é¢å¥–åŠ±ä¼˜åŒ–çš„è¯­è¨€æ„ŸçŸ¥è§†è§‰é“¾å¼æ€ç»´æ¡†æ¶ã€‚å®ƒé€šè¿‡å¤šé˜¶æ®µæ¨ç†ç®¡é“ã€è‡ªåŠ¨æ•°æ®æ•´ç†æ–¹æ³•å’Œä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œåœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­å®ç°é«˜æ•ˆæ¨ç†å’Œè‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLaV-CoTç›¸è¾ƒäºç±»ä¼¼è§„æ¨¡çš„å¼€æºåŸºçº¿æ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†çº¦9.5%ï¼Œå¹¶ä¸”åœ¨å®é™…éƒ¨ç½²ä¸­è¡¨ç°å‡ºè‰²ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€åˆ†äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”æ–¹é¢çš„èƒ½åŠ›æ˜¾è‘—æå‡ã€‚</li>
<li>LaV-CoTæ˜¯é¦–ä¸ªè¯­è¨€æ„ŸçŸ¥çš„è§†è§‰é“¾å¼æ€ç»´æ¡†æ¶ï¼Œæ”¯æŒå¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>LaV-CoTé‡‡ç”¨å¤šé˜¶æ®µæ¨ç†ç®¡é“ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ‘˜è¦ã€è¯­è¨€è¯†åˆ«ã€ç©ºé—´å¯¹è±¡çº§æè¿°å’Œé€æ­¥é€»è¾‘æ¨ç†ã€‚</li>
<li>LaV-CoTé€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†æ–¹æ³•ç”Ÿæˆå¤šè¯­è¨€é“¾å¼æ€ç»´æ³¨é‡Šï¼Œå®ç°é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>LaV-CoTé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸è¯­è¨€æ„ŸçŸ¥çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæé«˜æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLaV-CoTç›¸è¾ƒäºç°æœ‰æ¨¡å‹æœ‰æ˜æ˜¾æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0d5b8fc5acfe4b77f26ac40833bd2346~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896475&auth_key=1760896475-0-0-e42e6cbee0ec8ea655001159d32fa17f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b08f0fd894da3453161296af500228d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896482&auth_key=1760896482-0-0-2545c6b9fb7f068bffbb8870a1dfe368&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-abf1dd94c7c46c0342dfe7183c42ab9a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896489&auth_key=1760896489-0-0-75bf2ee1e54a1c21f15bd5dfbdd2d79a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d93c32c661cfac8d3496adf2ac1c55cd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896495&auth_key=1760896495-0-0-9f76f7945b81608a0fa121d4cf6fd698&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cdeb39a63a3640d55f40593d5fbb2c1c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896502&auth_key=1760896502-0-0-200d85571e45b48edc4fb662dd311583&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Secure-and-Explainable-Smart-Contract-Generation-with-Security-Aware-Group-Relative-Policy-Optimization"><a href="#Towards-Secure-and-Explainable-Smart-Contract-Generation-with-Security-Aware-Group-Relative-Policy-Optimization" class="headerlink" title="Towards Secure and Explainable Smart Contract Generation with   Security-Aware Group Relative Policy Optimization"></a>Towards Secure and Explainable Smart Contract Generation with   Security-Aware Group Relative Policy Optimization</h2><p><strong>Authors:Lei Yu, Jingyuan Zhang, Xin Wang, Jiajia Ma, Li Yang, Fengjun Zhang</strong></p>
<p>Smart contracts automate the management of high-value assets, where vulnerabilities can lead to catastrophic financial losses. This challenge is amplified in Large Language Models (LLMs) by two interconnected failures: they operate as unauditable â€œblack boxesâ€ lacking a transparent reasoning process, and consequently, generate code riddled with critical security vulnerabilities. To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a novel framework for secure and explainable smart contract generation. It begins with Continual Pre-training (CPT) to specialize the model. We then apply Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis. Finally, to directly mitigate vulnerabilities, we employ Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement learning phase that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness. Evaluated against 17 baselines on a benchmark of 756 real-world functions, SmartCoder-R1 establishes a new state of the art, achieving top performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This FullRate marks a 45.79% relative improvement over the strongest baseline, DeepSeek-R1. Crucially, its generated reasoning also excels in human evaluations, achieving high-quality ratings for Functionality (82.7%), Security (85.3%), and Clarity (90.7%). </p>
<blockquote>
<p>æ™ºèƒ½åˆçº¦è‡ªåŠ¨ç®¡ç†é«˜ä»·å€¼èµ„äº§ï¼Œå…¶ä¸­å­˜åœ¨çš„æ¼æ´å¯èƒ½å¯¼è‡´é‡å¤§è´¢åŠ¡æŸå¤±ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è¢«ä¸¤ç§ç›¸äº’å…³è”çš„å¤±è´¥æ‰€æ”¾å¤§ï¼šå®ƒä»¬ä½œä¸ºæ— æ³•å®¡è®¡çš„â€œé»‘ç®±â€ç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹ï¼Œå› æ­¤ç”Ÿæˆçš„ä»£ç å……æ–¥ç€å…³é”®çš„å®‰å…¨æ¼æ´ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºQwen2.5-Coder-7Bçš„æ™ºèƒ½åˆçº¦ç”Ÿæˆæ–°å‹æ¡†æ¶SmartCoder-R1ã€‚å®ƒé¦–å…ˆé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥ä¸“ä¸šåŒ–æ¨¡å‹ã€‚ç„¶åæˆ‘ä»¬åœ¨7998ä¸ªç»è¿‡ä¸“å®¶éªŒè¯çš„æ¨ç†å’Œä»£ç æ ·æœ¬ä¸Šåº”ç”¨é•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒï¼ˆL-CoT SFTï¼‰ï¼Œä»¥è®­ç»ƒæ¨¡å‹æ¨¡æ‹Ÿäººç±»å®‰å…¨åˆ†æã€‚æœ€åï¼Œä¸ºäº†ç›´æ¥ç¼“è§£æ¼æ´é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å®‰å…¨æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œé€šè¿‡ä¼˜åŒ–ç¼–è¯‘æˆåŠŸã€å®‰å…¨åˆè§„å’Œæ ¼å¼æ­£ç¡®çš„åŠ æƒå¥–åŠ±ä¿¡å·æ¥å®Œå–„ç”Ÿæˆç­–ç•¥ã€‚åœ¨åŒ…å«756ä¸ªçœŸå®ä¸–ç•Œå‡½æ•°çš„åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSmartCoder-R1ä¸17ä¸ªåŸºçº¿ç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åœ¨äº”ä¸ªå…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼šCompassè¾¾åˆ°87.70%ï¼ŒVulRateè¾¾åˆ°8.60%ï¼ŒSafeAvalè¾¾åˆ°80.16%ï¼ŒFuncRateè¾¾åˆ°53.84%ï¼ŒFullRateè¾¾åˆ°50.53%ã€‚FullRateç›¸å¯¹äºè¡¨ç°æœ€å¼ºçš„åŸºçº¿ç³»ç»ŸDeepSeek-R1æœ‰45.79%çš„ç›¸å¯¹æ”¹è¿›ã€‚å…³é”®çš„æ˜¯ï¼Œå…¶ç”Ÿæˆçš„æ¨ç†åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œåœ¨åŠŸèƒ½ã€å®‰å…¨å’Œæ¸…æ™°åº¦æ–¹é¢è·å¾—äº†é«˜è´¨é‡çš„è¯„ä»·ï¼ˆåŠŸèƒ½82.7%ï¼Œå®‰å…¨85.3%ï¼Œæ¸…æ™°åº¦90.7%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09942v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æ™ºèƒ½åˆçº¦åœ¨é«˜ä»·å€¼èµ„äº§ç®¡ç†ä¸­çš„è‡ªåŠ¨åŒ–æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å­˜åœ¨çš„ä¸¤å¤§é—®é¢˜ï¼šæ— æ³•å®¡è®¡çš„â€œé»‘ç®±â€æ“ä½œå’Œç¼ºä¹é€æ˜æ¨ç†è¿‡ç¨‹ï¼Œä»¥åŠç”±æ­¤äº§ç”Ÿçš„ä»£ç ä¸­çš„å…³é”®å®‰å…¨æ¼æ´ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºQwen2.5-Coder-7Bçš„æ–°å‹æ¡†æ¶SmartCoder-R1ï¼Œé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰è¿›è¡Œæ¨¡å‹ä¸“ä¸šåŒ–ï¼Œå¹¶é€šè¿‡é•¿æ€è€ƒé“¾ç›‘ç£å¾®è°ƒï¼ˆL-CoT SFTï¼‰è®­ç»ƒæ¨¡å‹ä»¥æ¨¡æ‹Ÿäººç±»å®‰å…¨åˆ†æã€‚æœ€åï¼Œé€šè¿‡å®‰å…¨æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ç›´æ¥ç¼“è§£æ¼æ´ã€‚SmartCoder-R1åœ¨çœŸå®ä¸–ç•Œå‡½æ•°åŸºå‡†æµ‹è¯•ä¸­ç›¸å¯¹äº17ä¸ªåŸºçº¿å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨äº”ä¸ªå…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æ–°çš„æ°´å¹³ã€‚åŒæ—¶ï¼Œå…¶ç”Ÿæˆçš„ç†ç”±åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½åˆçº¦ç®¡ç†é«˜ä»·å€¼èµ„äº§æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› æ¼æ´å¯èƒ½å¯¼è‡´å·¨å¤§è´¢åŠ¡æŸå¤±ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› ç¼ºä¹é€æ˜åº¦å’Œå®‰å…¨æ€§é¢ä¸´æ‰¹è¯„ã€‚</li>
<li>SmartCoder-R1æ¡†æ¶è¢«æå‡ºä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒé€šè¿‡æŒç»­é¢„è®­ç»ƒã€é•¿æ€è€ƒé“¾ç›‘ç£å¾®è°ƒå’Œå®‰å…¨æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç­‰æŠ€æœ¯æ¥æå‡æ™ºèƒ½åˆçº¦çš„ç”Ÿæˆè´¨é‡å’Œå®‰å…¨æ€§ã€‚</li>
<li>SmartCoder-R1åœ¨çœŸå®ä¸–ç•Œå‡½æ•°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸å¯¹äºæœ€å¼ºçš„åŸºçº¿DeepSeek-R1ï¼ŒFullRateæé«˜äº†45.79%ã€‚</li>
<li>SmartCoder-R1ç”Ÿæˆçš„æ¨ç†åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè·å¾—äº†é«˜è¯„ä»·ï¼Œåœ¨åŠŸèƒ½æ€§ã€å®‰å…¨æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
<li>è¯¥æ¡†æ¶åŸºäºQwen2.5-Coder-7Bï¼Œå…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å’Œå®‰å…¨æ€§ä¼˜åŒ–æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-111f565bfecc3600f1b51c05ea52b724~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896509&auth_key=1760896509-0-0-09d4d063a5437ae7899835a7b6edb21f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-631773680dfed5ee884d230fe3f3a661~resize:0:q75.jpg?source=1f5c5e47&expiration=1760896516&auth_key=1760896516-0-0-c28897dff166a2424f82d84d4ca2425d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-20/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-20/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-20/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-06a78a5be0608ccbfcae1cef3891a0f0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760897136&auth_key=1760897136-0-0-0555f5531200a4e49bd8729b50834bca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-20  The Open Source Advantage in Large Language Models (LLMs)
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-6ca024a8b174edad67272a0321794bca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760825438&auth_key=1760825438-0-0-add3fbed820b33d68225fa53d3e78d14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  LaMoGen Laban Movement-Guided Diffusion for Text-to-Motion Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
