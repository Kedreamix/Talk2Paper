<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-20  Uncertainty-Supervised Interpretable and Robust Evidential Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-35c7aae62c8327e7f356d73aff02312d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    59 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-20-æ›´æ–°"><a href="#2025-10-20-æ›´æ–°" class="headerlink" title="2025-10-20 æ›´æ–°"></a>2025-10-20 æ›´æ–°</h1><h2 id="Uncertainty-Supervised-Interpretable-and-Robust-Evidential-Segmentation"><a href="#Uncertainty-Supervised-Interpretable-and-Robust-Evidential-Segmentation" class="headerlink" title="Uncertainty-Supervised Interpretable and Robust Evidential Segmentation"></a>Uncertainty-Supervised Interpretable and Robust Evidential Segmentation</h2><p><strong>Authors:Yuzhu Li, An Sui, Fuping Wu, Xiahai Zhuang</strong></p>
<p>Uncertainty estimation has been widely studied in medical image segmentation as a tool to provide reliability, particularly in deep learning approaches. However, previous methods generally lack effective supervision in uncertainty estimation, leading to low interpretability and robustness of the predictions. In this work, we propose a self-supervised approach to guide the learning of uncertainty. Specifically, we introduce three principles about the relationships between the uncertainty and the image gradients around boundaries and noise. Based on these principles, two uncertainty supervision losses are designed. These losses enhance the alignment between model predictions and human interpretation. Accordingly, we introduce novel quantitative metrics for evaluating the interpretability and robustness of uncertainty. Experimental results demonstrate that compared to state-of-the-art approaches, the proposed method can achieve competitive segmentation performance and superior results in out-of-distribution (OOD) scenarios while significantly improving the interpretability and robustness of uncertainty estimation. Code is available via <a target="_blank" rel="noopener" href="https://github.com/suiannaius/SURE">https://github.com/suiannaius/SURE</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œä¸ç¡®å®šæ€§ä¼°è®¡è¢«å¹¿æ³›ç ”ç©¶ä½œä¸ºä¸€ç§æä¾›å¯é æ€§çš„å·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„æ–¹æ³•åœ¨ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹é¢é€šå¸¸ç¼ºä¹æœ‰æ•ˆçš„ç›‘ç£ï¼Œå¯¼è‡´é¢„æµ‹çš„è§£è¯»æ€§å’Œç¨³å¥æ€§è¾ƒä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„æ–¹æ³•æ¥æŒ‡å¯¼ä¸ç¡®å®šæ€§å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸‰ä¸ªå…³äºä¸ç¡®å®šæ€§ä¸è¾¹ç•Œå‘¨å›´å›¾åƒæ¢¯åº¦å’Œå™ªå£°ä¹‹é—´å…³ç³»çš„åŸåˆ™ã€‚åŸºäºè¿™äº›åŸåˆ™ï¼Œè®¾è®¡äº†ä¸¤ä¸ªä¸ç¡®å®šæ€§ç›‘ç£æŸå¤±ã€‚è¿™äº›æŸå¤±æé«˜äº†æ¨¡å‹é¢„æµ‹ä¸äººç±»è§£è¯»ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºè¯„ä¼°ä¸ç¡®å®šæ€§è§£è¯»æ€§å’Œç¨³å¥æ€§çš„æ–°å‹å®šé‡æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ç«äº‰æ¿€çƒˆçš„åˆ†å‰²æ€§èƒ½å’Œè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰åœºæ™¯ä¸­éƒ½èƒ½å®ç°å“è¶Šçš„ç»“æœï¼Œå¹¶æ˜¾è‘—æé«˜äº†ä¸ç¡®å®šæ€§ä¼°è®¡çš„è§£è¯»æ€§å’Œç¨³å¥æ€§ã€‚ä»£ç å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/suiannaius/SURE">https://github.com/suiannaius/SURE</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.17098v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œç”¨äºæŒ‡å¯¼ä¸ç¡®å®šæ€§å­¦ä¹ ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¹¿æ³›åº”ç”¨ã€‚é€šè¿‡å¼•å…¥å…³äºä¸ç¡®å®šæ€§ä¸å›¾åƒè¾¹ç•Œå’Œå™ªå£°æ¢¯åº¦ä¹‹é—´å…³ç³»çš„ä¸‰ä¸ªåŸåˆ™ï¼Œè®¾è®¡ä¸¤ç§ä¸ç¡®å®šæ€§ç›‘ç£æŸå¤±ï¼Œæé«˜æ¨¡å‹é¢„æµ‹ä¸äººç±»è§£è¯»ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚åŒæ—¶ï¼Œå¼•å…¥æ–°å‹å®šé‡æŒ‡æ ‡è¯„ä¼°ä¸ç¡®å®šæ€§è§£è¯»å’Œç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒåœºæ™¯ä¸‹è¡¨ç°æ›´ä¼˜è¶Šï¼Œå¹¶æ˜¾è‘—æé«˜ä¸ç¡®å®šæ€§ä¼°è®¡çš„è§£è¯»æ€§å’Œç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/suiannaius/SURE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/suiannaius/SUREè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ—¨åœ¨æé«˜é¢„æµ‹ç»“æœçš„å¯é æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>æå‡ºä¸€ç§è‡ªç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å…³äºä¸ç¡®å®šæ€§ä¸å›¾åƒè¾¹ç•Œå’Œå™ªå£°æ¢¯åº¦å…³ç³»çš„åŸåˆ™æ¥æŒ‡å¯¼ä¸ç¡®å®šæ€§å­¦ä¹ ã€‚</li>
<li>è®¾è®¡ä¸¤ç§ä¸ç¡®å®šæ€§ç›‘ç£æŸå¤±ï¼Œæé«˜æ¨¡å‹é¢„æµ‹ä¸äººç±»è§£è¯»çš„å¯¹é½ç¨‹åº¦ã€‚</li>
<li>å¼•å…¥æ–°å‹å®šé‡æŒ‡æ ‡æ¥è¯„ä¼°ä¸ç¡®å®šæ€§ä¼°è®¡çš„è§£è¯»æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²æ€§èƒ½ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”åœ¨è¶…å‡ºåˆ†å¸ƒçš„åœºæ™¯ä¸‹è¡¨ç°æ›´ä¼˜è¶Šã€‚</li>
<li>å…¬å¼€ç›¸å…³ä»£ç ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80bdb64e0c1706262992fb66ac77ee21" align="middle">
<img src="https://picx.zhimg.com/v2-2a7972404881d7dfa811cf464c0bcb23" align="middle">
<img src="https://picx.zhimg.com/v2-1a15cf6c9dfb5c4a767c61567556fd6f" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cryo-RL-automating-prostate-cancer-cryoablation-planning-with-reinforcement-learning"><a href="#Cryo-RL-automating-prostate-cancer-cryoablation-planning-with-reinforcement-learning" class="headerlink" title="Cryo-RL: automating prostate cancer cryoablation planning with   reinforcement learning"></a>Cryo-RL: automating prostate cancer cryoablation planning with   reinforcement learning</h2><p><strong>Authors:Trixia Simangan, Ahmed Nadeem Abbasi, Yipeng Hu, Shaheer U. Saeed</strong></p>
<p>Cryoablation is a minimally invasive localised treatment for prostate cancer that destroys malignant tissue during de-freezing, while sparing surrounding healthy structures. Its success depends on accurate preoperative planning of cryoprobe placements to fully cover the tumour and avoid critical anatomy. This planning is currently manual, expertise-dependent, and time-consuming, leading to variability in treatment quality and limited scalability. In this work, we introduce Cryo-RL, a reinforcement learning framework that models cryoablation planning as a Markov decision process and learns an optimal policy for cryoprobe placement. Within a simulated environment that models clinical constraints and stochastic intraoperative variability, an agent sequentially selects cryoprobe positions and ice sphere diameters. Guided by a reward function based on tumour coverage, this agent learns a cryoablation strategy that leads to optimal cryoprobe placements without the need for any manually-designed plans. Evaluated on 583 retrospective prostate cancer cases, Cryo-RL achieved over 8 percentage-point Dice improvements compared with the best automated baselines, based on geometric optimisation, and matched human expert performance while requiring substantially less planning time. These results highlight the potential of reinforcement learning to deliver clinically viable, reproducible, and efficient cryoablation plans. </p>
<blockquote>
<p>å†·å†»æ¶ˆèæ˜¯ä¸€ç§å¯¹å‰åˆ—è…ºç™Œçš„å¾®åˆ›å±€éƒ¨æ²»ç–—æ–¹æ³•ï¼Œå®ƒèƒ½åœ¨è§£å†»è¿‡ç¨‹ä¸­ç ´åæ¶æ€§ç»„ç»‡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´çš„å¥åº·ç»“æ„ã€‚å…¶æˆåŠŸå–å†³äºå†·å†»æ¢é’ˆæ”¾ç½®çš„æœ¯å‰è®¡åˆ’å‡†ç¡®ï¼Œä»¥å®Œå…¨è¦†ç›–è‚¿ç˜¤å¹¶é¿å…å…³é”®è§£å‰–ç»“æ„ã€‚å½“å‰çš„è§„åˆ’æ˜¯æ‰‹åŠ¨è¿›è¡Œçš„ï¼Œä¾èµ–äºä¸“å®¶ï¼Œå¹¶ä¸”è€—æ—¶ï¼Œå¯¼è‡´æ²»ç–—è´¨é‡å­˜åœ¨å·®å¼‚æ€§ä¸”æ‰©å±•æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†·å†»å¼ºåŒ–å­¦ä¹ ï¼ˆCryo-RLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†å†·å†»æ¶ˆèè®¡åˆ’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å­¦ä¹ å†·å†»æ¢é’ˆæ”¾ç½®çš„æœ€ä¼˜ç­–ç•¥ã€‚åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„ä¸´åºŠçº¦æŸå’Œæœ¯ä¸­éšæœºå˜åŒ–çš„ç¯å¢ƒä¸­ï¼Œä»£ç†ç¨‹åºä¼šä¾æ¬¡é€‰æ‹©å†·å†»æ¢é’ˆçš„ä½ç½®å’Œå†°çƒç›´å¾„ã€‚åœ¨è‚¿ç˜¤è¦†ç›–çš„å¥–åŠ±å‡½æ•°æŒ‡å¯¼ä¸‹ï¼Œè¯¥ä»£ç†ç¨‹åºå­¦ä¹ äº†ä¸€ç§å†·å†»æ¶ˆèç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿå¯¼è‡´æœ€ä¼˜çš„å†·å†»æ¢é’ˆæ”¾ç½®ï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨è®¾è®¡çš„è®¡åˆ’ã€‚åœ¨å›é¡¾æ€§å‰åˆ—è…ºç™Œç—…ä¾‹ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œå†·å†»å¼ºåŒ–å­¦ä¹ ç›¸è¾ƒäºåŸºäºå‡ ä½•ä¼˜åŒ–çš„æœ€ä½³è‡ªåŠ¨åŒ–åŸºçº¿ï¼ŒDiceæŒ‡æ•°æé«˜äº†è¶…è¿‡8ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶ä¸”ä¸äººç±»ä¸“å®¶è¡¨ç°ç›¸åŒ¹é…ï¼ŒåŒæ—¶è§„åˆ’æ—¶é—´å¤§å¤§å‡å°‘ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æä¾›ä¸´åºŠå¯è¡Œã€å¯é‡å¤å’Œé«˜æ•ˆçš„å†·å†»æ¶ˆèè®¡åˆ’æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04886v2">PDF</a> Accepted at MICAD (Medical Imaging and Computer-Aided Diagnosis) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Cryo-RLè¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶åœ¨å‰åˆ—è…ºç™Œå†·å†»æ¶ˆèæ²»ç–—è®¡åˆ’ä¸­çš„åº”ç”¨ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ¨¡æ‹Ÿæ‰‹æœ¯ç¯å¢ƒï¼Œè‡ªä¸»è§„åˆ’å†·å†»æ¢é’ˆä½ç½®å’Œå†°çƒç›´å¾„ï¼Œä»è€Œæé«˜è‚¿ç˜¤è¦†ç›–ç‡å¹¶ç¼©çŸ­è§„åˆ’æ—¶é—´ã€‚åœ¨å›é¡¾æ€§ç ”ç©¶ä¸­ï¼Œä¸å‡ ä½•ä¼˜åŒ–ç­‰è‡ªåŠ¨åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒCryo-RLåœ¨583ä¾‹å‰åˆ—è…ºç™Œæ‚£è€…çš„åº”ç”¨ä¸­å–å¾—äº†è¶…è¿‡8ä¸ªç™¾åˆ†ç‚¹çš„Diceæ”¹å–„å€¼ï¼Œå¹¶è¾¾åˆ°äº†ä¸äººç±»ä¸“å®¶ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Cryoablationæ˜¯ä¸€ç§å¾®åˆ›çš„å±€éƒ¨å‰åˆ—è…ºç™Œæ²»ç–—æ–¹æ³•ï¼Œé€šè¿‡å†·å†»å’Œè§£å†»è¿‡ç¨‹ç ´åæ¶æ€§ç»„ç»‡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´å¥åº·ç»“æ„ã€‚</li>
<li>å½“å‰Cryoablationçš„æ²»ç–—è®¡åˆ’ä¸»è¦ä¾èµ–äºä¸“å®¶çš„æ‰‹åŠ¨è§„åˆ’ï¼Œå­˜åœ¨æ—¶é—´é•¿ã€è´¨é‡ä¸ä¸€å’Œéš¾ä»¥æ‰©å±•çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†Cryo-RLè¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ¨¡æ‹Ÿå†·å†»æ¶ˆèæ‰‹æœ¯ç¯å¢ƒå¹¶è‡ªä¸»è§„åˆ’å†·å†»æ¢é’ˆä½ç½®å’Œå†°çƒç›´å¾„ã€‚</li>
<li>è¯¥æ¡†æ¶ä»¥è‚¿ç˜¤è¦†ç›–ç‡ä¸ºå¥–åŠ±å‡½æ•°ï¼Œå­¦ä¹ ä¼˜åŒ–å†·å†»æ¶ˆèç­–ç•¥ï¼Œå®ç°æ— éœ€æ‰‹åŠ¨è®¾è®¡çš„è®¡åˆ’ã€‚</li>
<li>åœ¨å›é¡¾æ€§ç ”ç©¶ä¸­ï¼Œä¸å‡ ä½•ä¼˜åŒ–ç­‰è‡ªåŠ¨åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒCryo-RLåœ¨å‰åˆ—è…ºç™Œæ‚£è€…çš„æ²»ç–—ä¸­å–å¾—äº†æ˜¾è‘—æ•ˆæœï¼Œæé«˜äº†Diceæ”¹å–„å€¼ã€‚</li>
<li>Cryo-RLçš„æ€§èƒ½ä¸äººç±»ä¸“å®¶ç›¸å½“ï¼Œä½†è§„åˆ’æ—¶é—´å¤§å¹…ç¼©çŸ­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f12f44cb93f7d19caea0900430941d0a" align="middle">
<img src="https://picx.zhimg.com/v2-8711b3b95e78473a90ad9a2f64e6e4fe" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation"><a href="#MedDINOv3-How-to-adapt-vision-foundation-models-for-medical-image-segmentation" class="headerlink" title="MedDINOv3: How to adapt vision foundation models for medical image   segmentation?"></a>MedDINOv3: How to adapt vision foundation models for medical image   segmentation?</h2><p><strong>Authors:Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang</strong></p>
<p>Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3">https://github.com/ricklisz/MedDINOv3</a>. </p>
<blockquote>
<p>åœ¨CTå’ŒMRIæ‰«æä¸­ï¼Œå™¨å®˜å’Œè‚¿ç˜¤çš„ç²¾ç¡®åˆ†å‰²å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»æ¨åŠ¨äº†è‡ªåŠ¨åŒ–åˆ†å‰²çš„å‘å±•ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»ç„¶é’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼Œåœ¨ä¸åŒçš„æˆåƒæ¨¡å¼å’Œæœºæ„ä¹‹é—´ç¼ºä¹é€šç”¨æ€§ã€‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰ï¼Œåœ¨ç™¾äº¿çº§è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæä¾›äº†å¼ºå¤§ä¸”å¯è¿ç§»çš„è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå°†å…¶é€‚åº”åŒ»å­¦æˆåƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¤§å¤šæ•°åŸºç¡€æ¨¡å‹çš„ViTä¸»å¹²åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä»ç„¶è¡¨ç°ä¸ä½³ï¼Œä¸åŠä¸“ä¸šåŒ–çš„CNNï¼›ï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·è¾ƒå¤§ï¼Œé™åˆ¶äº†å¯è¿ç§»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MedDINOv3ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå°†DINOv3é€‚åº”åŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬é¦–å…ˆå›é¡¾äº†æ™®é€šçš„ViTsï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¶æ„ï¼Œå…·æœ‰å¤šå°ºåº¦ä»¤ç‰Œèšåˆã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨CT-3Mï¼ˆä¸€ä¸ªç²¾é€‰çš„åŒ…å«387ä¸‡è½´å‘CTåˆ‡ç‰‡çš„é›†åˆï¼‰è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œå¹¶ä½¿ç”¨å¤šé˜¶æ®µDINOv3é…æ–¹æ¥å­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç»Ÿä¸€ä¸»å¹²çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ricklisz/MedDINOv3%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ricklisz/MedDINOv3æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02379v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨CTå’ŒMRIæ‰«æä¸­å‡†ç¡®åˆ†å‰²å™¨å®˜å’Œè‚¿ç˜¤çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæ·±åº¦å­¦ä¹ åœ¨è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹é¢çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹ç¼ºä¹è·¨æ¨¡æ€å’Œè·¨æœºæ„çš„é€šç”¨æ€§ã€‚æ–‡ç« æå‡ºå°†é¢„è®­ç»ƒäºç™¾äº¿çº§è‡ªç„¶å›¾åƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰åº”ç”¨äºåŒ»å­¦æˆåƒï¼Œå¹¶ä»‹ç»äº†MedDINOv3æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¤šé˜¶æ®µDINOv3é…æ–¹åœ¨CT-3Mæ•°æ®é›†ä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œä»¥å­¦ä¹ ç¨³å¥çš„å¯†é›†ç‰¹å¾ã€‚MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è§†è§‰åŸºç¡€æ¨¡å‹ä½œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç»Ÿä¸€ä¸»å¹²æ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®åˆ†å‰²CTå’ŒMRIæ‰«æä¸­çš„å™¨å®˜å’Œè‚¿ç˜¤å¯¹è¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œç–¾ç—…ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç¼ºä¹è·¨æ¨¡æ€å’Œè·¨æœºæ„çš„é€šç”¨æ€§ã€‚</li>
<li>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å¯ä»¥åŸºäºè‡ªç„¶å›¾åƒçš„å¼ºå¤§å’Œå¯è¿ç§»è¡¨ç¤ºæ¥å¢å¼ºåŒ»å­¦æˆåƒã€‚</li>
<li>MedDINOv3æ¡†æ¶æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„å°†DINOv3é€‚åº”åŒ»å­¦åˆ†å‰²çš„æ–¹æ³•ã€‚</li>
<li>MedDINOv3é€šè¿‡å¤šé˜¶æ®µDINOv3é…æ–¹åœ¨CT-3Mæ•°æ®é›†ä¸Šè¿›è¡ŒåŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œè®¾è®¡å‡ºå…·æœ‰å¤šå°ºåº¦ä»¤ç‰Œèšåˆçš„ç®€å•æœ‰æ•ˆæ¶æ„ã€‚</li>
<li>MedDINOv3åœ¨å››ä¸ªåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff28c5cb1f2aef55e210576b226bdad3" align="middle">
<img src="https://picx.zhimg.com/v2-69eb7b8f98c17f7f1a653c65dd53dc9f" align="middle">
<img src="https://picx.zhimg.com/v2-8a4306bae32e38dc7777c065bb007ab5" align="middle">
<img src="https://picx.zhimg.com/v2-5f3f2b82643e7fec0a1229c41ce5fb1c" align="middle">
<img src="https://picx.zhimg.com/v2-cfe3be72dd8124edfca2d135d61d1066" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="KonfAI-A-Modular-and-Fully-Configurable-Framework-for-Deep-Learning-in-Medical-Imaging"><a href="#KonfAI-A-Modular-and-Fully-Configurable-Framework-for-Deep-Learning-in-Medical-Imaging" class="headerlink" title="KonfAI: A Modular and Fully Configurable Framework for Deep Learning in   Medical Imaging"></a>KonfAI: A Modular and Fully Configurable Framework for Deep Learning in   Medical Imaging</h2><p><strong>Authors:Valentin Boussot, Jean-Louis Dillenseger</strong></p>
<p>KonfAI is a modular, extensible, and fully configurable deep learning framework specifically designed for medical imaging tasks. It enables users to define complete training, inference, and evaluation workflows through structured YAML configuration files, without modifying the underlying code. This declarative approach enhances reproducibility, transparency, and experimental traceability while reducing development time. Beyond the capabilities of standard pipelines, KonfAI provides native abstractions for advanced strategies including patch-based learning, test-time augmentation, model ensembling, and direct access to intermediate feature representations for deep supervision. It also supports complex multi-model training setups such as generative adversarial architectures. Thanks to its modular and extensible architecture, KonfAI can easily accommodate custom models, loss functions, and data processing components. The framework has been successfully applied to segmentation, registration, and image synthesis tasks, and has contributed to top-ranking results in several international medical imaging challenges. KonfAI is open source and available at <a target="_blank" rel="noopener" href="https://github.com/vboussot/KonfAI">https://github.com/vboussot/KonfAI</a>. </p>
<blockquote>
<p>KonfAIæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–ã€å¯æ‰©å±•å’Œå®Œå…¨å¯é…ç½®çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºåŒ»å­¦å½±åƒä»»åŠ¡è®¾è®¡ã€‚å®ƒä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–çš„YAMLé…ç½®æ–‡ä»¶å®šä¹‰å®Œæ•´çš„è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°å·¥ä½œæµç¨‹ï¼Œè€Œæ— éœ€ä¿®æ”¹åº•å±‚ä»£ç ã€‚è¿™ç§å£°æ˜å¼æ–¹æ³•æé«˜äº†å¯é‡å¤æ€§ã€é€æ˜åº¦å’Œå®éªŒå¯è¿½æº¯æ€§ï¼ŒåŒæ—¶å‡å°‘äº†å¼€å‘æ—¶é—´ã€‚é™¤äº†æ ‡å‡†ç®¡é“çš„åŠŸèƒ½å¤–ï¼ŒKonfAIè¿˜æä¾›é«˜çº§ç­–ç•¥çš„æœ¬æœºæŠ½è±¡ï¼ŒåŒ…æ‹¬åŸºäºè¡¥ä¸çš„å­¦ä¹ ã€æµ‹è¯•æ—¶å¢å¼ºã€æ¨¡å‹é›†æˆå’Œæ·±åº¦ç›‘ç£çš„ç›´æ¥è®¿é—®ä¸­é—´ç‰¹å¾è¡¨ç¤ºã€‚å®ƒè¿˜æ”¯æŒå¤æ‚çš„å¤šæ¨¡å‹è®­ç»ƒè®¾ç½®ï¼Œä¾‹å¦‚ç”Ÿæˆå¯¹æŠ—æ¶æ„ã€‚ç”±äºå…¶æ¨¡å—åŒ–å’Œå¯æ‰©å±•çš„æ¶æ„ï¼ŒKonfAIå¯ä»¥è½»æ¾å®¹çº³è‡ªå®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®å¤„ç†ç»„ä»¶ã€‚è¯¥æ¡†æ¶å·²æˆåŠŸåº”ç”¨äºåˆ†å‰²ã€æ³¨å†Œå’Œå›¾åƒåˆæˆä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªå›½é™…åŒ»å­¦å½±åƒæŒ‘æˆ˜ä¸­è·å¾—äº†é¡¶å°–ç»“æœã€‚KonfAIæ˜¯å¼€æºçš„ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vboussot/KonfAI">https://github.com/vboussot/KonfAI</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09823v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/vboussot/KonfAI">https://github.com/vboussot/KonfAI</a></p>
<p><strong>Summary</strong></p>
<p>åº·æ–åŒ»ç–—æˆåƒæ·±åº¦å­¦ä¹ æ¡†æ¶æ˜¯ä¸€ç§æ¨¡å—åŒ–ã€å¯æ‰©å±•å’Œå®Œå…¨å¯é…ç½®çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸“ä¸ºåŒ»å­¦å½±åƒä»»åŠ¡è®¾è®¡ã€‚å®ƒé‡‡ç”¨å£°æ˜å¼æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–YAMLé…ç½®æ–‡ä»¶å®šä¹‰è®­ç»ƒã€æ¨ç†å’Œè¯„ä¼°æµç¨‹ï¼Œæ— éœ€ä¿®æ”¹åº•å±‚ä»£ç ã€‚è¯¥æ¡†æ¶æ”¯æŒé«˜çº§ç­–ç•¥æŠ½è±¡ã€å¤æ‚å¤šæ¨¡å‹è®­ç»ƒè®¾ç½®ä»¥åŠè‡ªå®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®å¤„ç†ç»„ä»¶çš„çµæ´»é›†æˆã€‚åº·æ–æ¡†æ¶å·²æˆåŠŸåº”ç”¨äºåˆ†å‰²ã€æ³¨å†Œå’Œå›¾åƒåˆæˆä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªå›½é™…åŒ»å­¦å½±åƒæŒ‘æˆ˜ä¸­å–å¾—é¡¶å°–æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åº·æ–æ˜¯ä¸€ä¸ªä¸“ä¸ºåŒ»å­¦å½±åƒä»»åŠ¡è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>å®ƒé‡‡ç”¨å£°æ˜å¼æ–¹æ³•ï¼Œé€šè¿‡ç»“æ„åŒ–YAMLé…ç½®æ–‡ä»¶è¿›è¡Œé…ç½®ï¼Œæé«˜å®éªŒçš„å¯é‡å¤æ€§ã€é€æ˜æ€§å’Œè¿½è¸ªæ€§ã€‚</li>
<li>åº·æ–æä¾›æ¨¡å—åŒ–å’Œå¯æ‰©å±•æ¶æ„ï¼Œæ”¯æŒé«˜çº§ç­–ç•¥æŠ½è±¡å¦‚åŸºäºè¡¥ä¸çš„å­¦ä¹ ã€æµ‹è¯•æ—¶å¢å¼ºã€æ¨¡å‹é›†æˆå’Œæ·±åº¦ç›‘ç£çš„ç›´æ¥è®¿é—®ã€‚</li>
<li>å®ƒæ”¯æŒå¤æ‚çš„å¤šæ¨¡å‹è®­ç»ƒè®¾ç½®ï¼ŒåŒ…æ‹¬ç”Ÿæˆå¯¹æŠ—æ¶æ„ã€‚</li>
<li>åº·æ–æ¡†æ¶å…è®¸è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œæ•°æ®å¤„ç†ç»„ä»¶ã€‚</li>
<li>è¯¥æ¡†æ¶å·²æˆåŠŸåº”ç”¨äºåŒ»å­¦å½±åƒä¸­çš„åˆ†å‰²ã€æ³¨å†Œå’Œå›¾åƒåˆæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09823">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a175514cda14de99005a57079102f56e" align="middle">
<img src="https://picx.zhimg.com/v2-56b53b7a4dc0e51be42ec7ba9ee09d1c" align="middle">
<img src="https://picx.zhimg.com/v2-35c7aae62c8327e7f356d73aff02312d" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation"><a href="#Boosting-Generic-Semi-Supervised-Medical-Image-Segmentation-via-Diverse-Teaching-and-Label-Propagation" class="headerlink" title="Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse   Teaching and Label Propagation"></a>Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse   Teaching and Label Propagation</h2><p><strong>Authors:Wei Li, Pengcheng Zhou, Linye Ma, Wenyi Zhao, Huihua Yang</strong></p>
<p>Both limited annotation and domain shift are significant challenges frequently encountered in medical image segmentation, leading to derivative scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). Conventional methods are generally tailored to specific tasks in isolation, the error accumulation hinders the effective utilization of unlabeled data and limits further improvements, resulting in suboptimal performance when these issues occur. In this paper, we aim to develop a generic framework that masters all three tasks. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data and increasing the diversity of the model. To tackle this issue, we employ a Diverse Teaching and Label Propagation Network (DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation. Our DTLP-Net involves a single student model and two diverse teacher models, which can generate reliable pseudo-labels for the student model. The first teacher model decouple the training process with labeled and unlabeled data, The second teacher is momentum-updated periodically, thus generating reliable yet divers pseudo-labels. To fully utilize the information within the data, we adopt inter-sample and intra-sample data augmentation to learn the global and local knowledge. In addition, to further capture the voxel-level correlations, we propose label propagation to enhance the model robust. We evaluate our proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks. The results showcase notable improvements compared to state-of-the-art methods across all five settings, indicating the potential of our framework to tackle more challenging SSL scenarios. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œç»å¸¸ä¼šé‡åˆ°æœ‰é™çš„æ ‡æ³¨å’Œé¢†åŸŸåç§»è¿™ä¸¤ä¸ªé‡è¦çš„æŒ‘æˆ˜ï¼Œè¿™å¯¼è‡´äº†è¯¸å¦‚åŠç›‘ç£åŒ»å­¦ï¼ˆSSMISï¼‰ã€åŠç›‘ç£åŒ»å­¦é¢†åŸŸæ³›åŒ–ï¼ˆSemi-MDGï¼‰å’Œæ— ç›‘ç£åŒ»å­¦é¢†åŸŸé€‚åº”ï¼ˆUMDAï¼‰ç­‰è¡ç”Ÿåœºæ™¯ã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„ä»»åŠ¡è¿›è¡Œå®šåˆ¶ï¼Œè¯¯å·®ç´¯ç§¯é˜»ç¢äº†æ— æ ‡ç­¾æ•°æ®çš„æœ‰æ•ˆåˆ©ç”¨ï¼Œé™åˆ¶äº†è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚å½“è¿™äº›é—®é¢˜å‘ç”Ÿæ—¶ï¼Œæ€§èƒ½å¾€å¾€è¾¾ä¸åˆ°æœ€ä¼˜ã€‚æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªæŒæ¡æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡é€šç”¨æ¡†æ¶ã€‚æˆ‘ä»¬å‘ç°è§£å†³é—®é¢˜çš„å…³é”®åœ¨äºå¦‚ä½•åœ¨å­˜åœ¨é¢†åŸŸåç§»çš„æœ‰æ ‡ç­¾æ•°æ®æƒ…å†µä¸‹ï¼Œä¸ºæ— æ ‡ç­¾æ•°æ®ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ï¼Œå¹¶å¢åŠ æ¨¡å‹çš„å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ ·åŒ–çš„æ•™å­¦å’Œæ ‡ç­¾ä¼ æ’­ç½‘ç»œï¼ˆDTLP-Netï¼‰æ¥æå‡é€šç”¨çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚æˆ‘ä»¬çš„DTLP-NetåŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªä¸åŒçš„æ•™å¸ˆæ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥ä¸ºå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ã€‚ç¬¬ä¸€ä¸ªæ•™å¸ˆæ¨¡å‹å°†æ ‡æ³¨å’Œæ— æ ‡æ³¨æ•°æ®çš„è®­ç»ƒè¿‡ç¨‹è§£è€¦ï¼›ç¬¬äºŒä¸ªæ•™å¸ˆæ¨¡å‹å®šæœŸæ›´æ–°åŠ¨é‡ï¼Œä»è€Œç”Ÿæˆå¯é ä¸”å¤šæ ·çš„ä¼ªæ ‡ç­¾ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ ·æœ¬é—´å’Œæ ·æœ¬å†…çš„æ•°æ®å¢å¼ºæ¥å­¦ä¹ å…¨å±€å’Œå±€éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥æ•æ‰ä½“ç´ çº§çš„å…³è”ï¼Œæˆ‘ä»¬æå‡ºæ ‡ç­¾ä¼ æ’­ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬æå‡ºçš„æ¡†æ¶åœ¨SSMISã€UMDAå’ŒSemi-MDGä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ‰€æœ‰äº”ä¸ªè®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬æ¡†æ¶åœ¨åº”å¯¹æ›´å…·æŒ‘æˆ˜æ€§çš„SSLåœºæ™¯æ—¶çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08549v2">PDF</a> Under Review</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é‡åˆ°çš„æœ‰é™æ ‡æ³¨ä¸é¢†åŸŸåç§»ä¸¤å¤§æŒ‘æˆ˜ï¼Œå¹¶è¡ç”Ÿå‡ºåŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆSSMISï¼‰ã€åŠç›‘ç£åŒ»å­¦é¢†åŸŸæ³›åŒ–ï¼ˆSemi-MDGï¼‰å’Œæ— ç›‘ç£åŒ»å­¦é¢†åŸŸè‡ªé€‚åº”ï¼ˆUMDAï¼‰ç­‰åœºæ™¯ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶ç§¯ç´¯è¯¯å·®ã€æ— æ³•æœ‰æ•ˆåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨æŒæ¡æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ã€‚ä¸ºè§£å†³é¢†åŸŸåç§»ä¸‹æ— æ ‡ç­¾æ•°æ®çš„å¯é ä¼ªæ ‡ç­¾ç”Ÿæˆé—®é¢˜ï¼Œæœ¬æ–‡é‡‡ç”¨å¤šæ ·åŒ–æ•™å­¦åŠæ ‡ç­¾ä¼ æ’­ç½‘ç»œï¼ˆDTLP-Netï¼‰å¢å¼ºé€šç”¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚DTLP-NetåŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªå¤šæ ·åŒ–çš„æ•™å¸ˆæ¨¡å‹ï¼Œèƒ½ä¸ºå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ã€‚åŒæ—¶ï¼Œé‡‡ç”¨æ ·æœ¬é—´å’Œæ ·æœ¬å†…çš„æ•°æ®å¢å¼ºï¼Œå­¦ä¹ å…¨å±€å’Œå±€éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºæ•æ‰ä½“ç´ çº§å…³è”ï¼Œæœ¬æ–‡æå‡ºæ ‡ç­¾ä¼ æ’­ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€ä½³æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ¡†æ¶åœ¨æ‰€æœ‰äº”ä¸ªè®¾ç½®ä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæ˜¾ç¤ºå‡ºè§£å†³æ›´å¤æ‚çš„SSLåœºæ™¯çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æœ‰é™æ ‡æ³¨å’Œé¢†åŸŸåç§»çš„æŒ‘æˆ˜ï¼Œè¡ç”Ÿå‡ºSSMISã€Semi-MDGå’ŒUMDAç­‰åœºæ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™äº›ä»»åŠ¡æ—¶å­˜åœ¨è¯¯å·®ç§¯ç´¯çš„é—®é¢˜ï¼Œæ— æ³•æœ‰æ•ˆåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®ã€‚</li>
<li>æå‡ºä¸€ç§é€šç”¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ï¼Œå¼ºè°ƒç”Ÿæˆå¯é ä¼ªæ ‡ç­¾å’Œå¢åŠ æ¨¡å‹å¤šæ ·æ€§çš„é‡è¦æ€§ã€‚</li>
<li>é‡‡ç”¨DTLP-Netï¼ŒåŒ…æ‹¬ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹å’Œä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹ï¼Œä»¥ç”Ÿæˆå¯é çš„ä¼ªæ ‡ç­¾ã€‚</li>
<li>é€šè¿‡æ ·æœ¬é—´å’Œæ ·æœ¬å†…çš„æ•°æ®å¢å¼ºï¼Œå……åˆ†åˆ©ç”¨æ•°æ®ä¸­çš„ä¿¡æ¯ï¼Œå­¦ä¹ å…¨å±€å’Œå±€éƒ¨çŸ¥è¯†ã€‚</li>
<li>æå‡ºæ ‡ç­¾ä¼ æ’­ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæ•æ‰ä½“ç´ çº§å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08549">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-842eb7b008c3a2e363de8bdcb9219708" align="middle">
<img src="https://picx.zhimg.com/v2-e8a433a53ea879373523f52da3856230" align="middle">
<img src="https://picx.zhimg.com/v2-a212884ac4c572fe611a306824bc0fc5" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedCAL-Bench-A-Comprehensive-Benchmark-on-Cold-Start-Active-Learning-with-Foundation-Models-for-Medical-Image-Analysis"><a href="#MedCAL-Bench-A-Comprehensive-Benchmark-on-Cold-Start-Active-Learning-with-Foundation-Models-for-Medical-Image-Analysis" class="headerlink" title="MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning   with Foundation Models for Medical Image Analysis"></a>MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning   with Foundation Models for Medical Image Analysis</h2><p><strong>Authors:Ning Zhu, Xiaochuan Ma, Shaoting Zhang, Guotai Wang</strong></p>
<p>Cold-Start Active Learning (CSAL) aims to select informative samples for annotation without prior knowledge, which is important for improving annotation efficiency and model performance under a limited annotation budget in medical image analysis. Most existing CSAL methods rely on Self-Supervised Learning (SSL) on the target dataset for feature extraction, which is inefficient and limited by insufficient feature representation. Recently, pre-trained Foundation Models (FMs) have shown powerful feature extraction ability with a potential for better CSAL. However, this paradigm has been rarely investigated, with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we propose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical image analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets under different annotation budgets, covering classification and segmentation tasks from diverse medical modalities. It is also the first CSAL benchmark that evaluates both the feature extraction and sample selection stages. Our experimental results reveal that: 1) Most FMs are effective feature extractors for CSAL, with DINO family performing the best in segmentation; 2) The performance differences of these FMs are large in segmentation tasks, while small for classification; 3) Different sample selection strategies should be considered in CSAL on different datasets, with Active Learning by Processing Surprisal (ALPS) performing the best in segmentation while RepDiv leading for classification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MedCAL-Bench">https://github.com/HiLab-git/MedCAL-Bench</a>. </p>
<blockquote>
<p>å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ ï¼ˆCSALï¼‰æ—¨åœ¨åœ¨æ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹é€‰æ‹©ä¿¡æ¯æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œè¿™å¯¹äºåœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ä½¿ç”¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—æé«˜æ ‡æ³¨æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½éå¸¸é‡è¦ã€‚å¤§å¤šæ•°ç°æœ‰çš„CSALæ–¹æ³•ä¾èµ–äºç›®æ ‡æ•°æ®é›†ä¸Šçš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¿›è¡Œç‰¹å¾æå–ï¼Œè¿™æ˜¯ä½æ•ˆçš„ï¼Œå¹¶ä¸”å—åˆ°ç‰¹å¾è¡¨ç¤ºä¸è¶³çš„é™åˆ¶ã€‚æœ€è¿‘ï¼Œé¢„è®­ç»ƒçš„Foundation Modelsï¼ˆFMsï¼‰æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå…·æœ‰æ›´å¥½çš„CSALæ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§èŒƒå¼å¾ˆå°‘å—åˆ°ç ”ç©¶ï¼Œå¹¶ä¸”åœ¨CSALä»»åŠ¡ä¸­ç¼ºä¹å¯¹FMè¿›è¡Œæ¯”è¾ƒçš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MedCAL-Benchï¼Œè¿™æ˜¯åŸºäºFMçš„åŒ»å­¦å›¾åƒåˆ†æçš„é¦–ä¸ªç³»ç»Ÿæ€§CSALåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨7ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†14ä¸ªFMå’Œ7ä¸ªCSALç­–ç•¥ï¼Œæ¶‰åŠä¸åŒæ ‡æ³¨é¢„ç®—çš„åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ï¼Œæ¶µç›–äº†å¤šç§åŒ»å­¦æ¨¡æ€ã€‚å®ƒä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªåŒæ—¶è¯„ä¼°ç‰¹å¾æå–å’Œæ ·æœ¬é€‰æ‹©é˜¶æ®µçš„CSALåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼š1ï¼‰å¤§å¤šæ•°FMå¯¹äºCSALéƒ½æ˜¯æœ‰æ•ˆçš„ç‰¹å¾æå–å™¨ï¼ŒDINOç³»åˆ—åœ¨åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ï¼›2ï¼‰è¿™äº›FMåœ¨åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼Œè€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„å·®å¼‚è¾ƒå°ï¼›3ï¼‰åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡ŒCSALæ—¶ï¼Œåº”è€ƒè™‘ä¸åŒçš„æ ·æœ¬é€‰æ‹©ç­–ç•¥ï¼Œå¤„ç†æƒŠå–œï¼ˆALPSï¼‰åœ¨åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒRepDivåœ¨åˆ†ç±»æ–¹é¢é¢†å…ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MedCAL-Bench">https://github.com/HiLab-git/MedCAL-Bench</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03441v2">PDF</a> 23 pages, 6 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼ˆCSALï¼‰çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†åœ¨æ— å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹é€‰æ‹©ä¿¡æ¯æ ·æœ¬çš„é‡è¦æ€§ã€‚æ–‡ç« æŒ‡å‡ºä¼ ç»ŸCSALæ–¹æ³•ä¾èµ–ç›®æ ‡æ•°æ®é›†çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¿›è¡Œç‰¹å¾æå–å­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œç‰¹å¾è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†é¢„è®­ç»ƒçš„Foundation Modelsï¼ˆFMsï¼‰ï¼Œå±•ç°äº†å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå¹¶åœ¨CSALä¸­å…·æœ‰æ½œåŠ›ã€‚ç„¶è€Œï¼ŒFMsåœ¨CSALä¸­çš„åº”ç”¨é²œæœ‰ç ”ç©¶ï¼Œç¼ºä¹ç›¸åº”çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MedCAL-Benchï¼Œé¦–ä¸ªåŸºäºFMçš„CSALåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚é€šè¿‡è¯„ä¼°14ä¸ªFMså’Œ7ç§CSALç­–ç•¥åœ¨ä¸åŒæ•°æ®é›†å’Œä¸åŒæ ‡æ³¨é¢„ç®—ä¸‹çš„è¡¨ç°ï¼Œå®éªŒç»“æœæ˜¾ç¤ºä¸åŒFMsåœ¨åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œè€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šå·®å¼‚è¾ƒå°ï¼›ä¸åŒæ•°æ®é›†çš„CSALåº”è€ƒè™‘ä¸åŒçš„æ ·æœ¬é€‰æ‹©ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼ˆCSALï¼‰åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå¯¹äºæé«˜æ ‡æ³¨æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ã€‚</li>
<li>ç°æœ‰CSALæ–¹æ³•å¤§å¤šä¾èµ–ç›®æ ‡æ•°æ®é›†çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ï¼Œå­˜åœ¨æ•ˆç‡ä½ä¸‹å’Œç‰¹å¾è¡¨ç¤ºä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é¢„è®­ç»ƒçš„Foundation Modelsï¼ˆFMsï¼‰åœ¨ç‰¹å¾æå–æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå…·æœ‰æ”¹å–„CSALçš„æ½œåŠ›ã€‚</li>
<li>FMsåœ¨CSALä¸­çš„åº”ç”¨é²œæœ‰ç ”ç©¶ï¼Œç¼ºä¹ç›¸åº”çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¸ºæ­¤æœ¬æ–‡æå‡ºäº†MedCAL-Benchã€‚</li>
<li>è¯„ä¼°äº†å¤šç§FMså’ŒCSALç­–ç•¥åœ¨ä¸åŒæ•°æ®é›†ã€ä¸åŒæ ‡æ³¨é¢„ç®—ä¸‹çš„è¡¨ç°ï¼Œæ¶µç›–äº†åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒFMsåœ¨åˆ†å‰²ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œè€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„å·®å¼‚è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8a493a27698d912f371d2820a0b4d532" align="middle">
<img src="https://picx.zhimg.com/v2-7118426f2e02b3c65a469b1075d78b51" align="middle">
<img src="https://picx.zhimg.com/v2-afd49a7a484f945dfbe809ed8fc1e28b" align="middle">
<img src="https://picx.zhimg.com/v2-6de8b4cb73d02ce4bb9fe59105ed730f" align="middle">
<img src="https://picx.zhimg.com/v2-a227b1fd14b2d231ff17284f4fd53d35" align="middle">
<img src="https://picx.zhimg.com/v2-93fc52fc51f03e003cb51b096c8bc416" align="middle">
<img src="https://picx.zhimg.com/v2-0c386da8e1f4dd1bfb1fe5880c640eee" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EMedNeXt-An-Enhanced-Brain-Tumor-Segmentation-Framework-for-Sub-Saharan-Africa-using-MedNeXt-V2-with-Deep-Supervision"><a href="#EMedNeXt-An-Enhanced-Brain-Tumor-Segmentation-Framework-for-Sub-Saharan-Africa-using-MedNeXt-V2-with-Deep-Supervision" class="headerlink" title="EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan   Africa using MedNeXt V2 with Deep Supervision"></a>EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan   Africa using MedNeXt V2 with Deep Supervision</h2><p><strong>Authors:Ahmed Jaheen, Abdelrahman Elsayed, Damir Kim, Daniil Tikhonov, Matheus Scatolin, Mohor Banerjee, Qiankun Ji, Mostafa Salem, Hu Wang, Sarim Hashmi, Mohammad Yaqub</strong></p>
<p>Brain cancer affects millions worldwide, and in nearly every clinical setting, doctors rely on magnetic resonance imaging (MRI) to diagnose and monitor gliomas. However, the current standard for tumor quantification through manual segmentation of multi-parametric MRI is time-consuming, requires expert radiologists, and is often infeasible in under-resourced healthcare systems. This problem is especially pronounced in low-income regions, where MRI scanners are of lower quality and radiology expertise is scarce, leading to incorrect segmentation and quantification. In addition, the number of acquired MRI scans in Africa is typically small. To address these challenges, the BraTS-Lighthouse 2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa (SSA), where resource constraints and image quality degradation introduce significant shifts. In this study, we present EMedNeXt â€“ an enhanced brain tumor segmentation framework based on MedNeXt V2 with deep supervision and optimized post-processing pipelines tailored for SSA. EMedNeXt introduces three key contributions: a larger region of interest, an improved nnU-Net v2-based architectural skeleton, and a robust model ensembling system. Evaluated on the hidden validation set, our solution achieved an average LesionWise DSC of 0.897 with an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and 1.0 mm, respectively. </p>
<blockquote>
<p>è„‘ç™Œå½±å“å…¨çƒæ•°ç™¾ä¸‡äººï¼Œåœ¨å‡ ä¹æ‰€æœ‰çš„ä¸´åºŠç¯å¢ƒä¸­ï¼ŒåŒ»ç”Ÿéƒ½ä¾èµ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ¥è¯Šæ–­å’Œç›‘æµ‹èƒ¶è´¨ç˜¤ã€‚ç„¶è€Œï¼Œé€šè¿‡å¤šå‚æ•°MRIè¿›è¡Œè‚¿ç˜¤é‡åŒ–çš„æ‰‹åŠ¨åˆ†å‰²æ˜¯ç›®å‰çš„æ ‡å‡†ï¼Œè¿™ä¸ä»…è€—æ—¶ï¼Œéœ€è¦ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œè€Œä¸”åœ¨èµ„æºä¸è¶³çš„å«ç”Ÿç³»ç»Ÿä¸­é€šå¸¸ä¸å¯è¡Œã€‚è¿™ä¸€é—®é¢˜åœ¨ä½æ”¶å…¥åœ°åŒºå°¤ä¸ºçªå‡ºï¼Œé‚£é‡Œçš„MRIæ‰«æä»ªè´¨é‡è¾ƒä½ï¼Œæ”¾å°„å­¦ä¸“å®¶ç¨€ç¼ºï¼Œå¯¼è‡´åˆ†å‰²å’Œé‡åŒ–ä¸å‡†ç¡®ã€‚æ­¤å¤–ï¼Œéæ´²è·å¾—çš„MRIæ‰«ææ¬¡æ•°é€šå¸¸è¾ƒå°‘ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼ŒBraTS-Lighthouse 2025 Challengeä¸“æ³¨äºæ’’å“ˆæ‹‰ä»¥å—éæ´²ï¼ˆSSAï¼‰çš„ç¨³å¥è‚¿ç˜¤åˆ†å‰²ï¼Œèµ„æºçº¦æŸå’Œå›¾åƒè´¨é‡é€€åŒ–å¼•å…¥äº†é‡å¤§å˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EMedNeXtâ€”â€”ä¸€ä¸ªåŸºäºMedNeXt V2çš„å¢å¼ºå‹è„‘è‚¿ç˜¤åˆ†å‰²æ¡†æ¶ï¼Œå…·æœ‰æ·±åº¦ç›‘ç£å’Œé’ˆå¯¹SSAä¼˜åŒ–çš„åå¤„ç†æµæ°´çº¿ã€‚EMedNeXtå¼•å…¥äº†ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šæ›´å¤§çš„æ„Ÿå…´è¶£åŒºåŸŸã€æ”¹è¿›çš„nnU-Net v2åŸºç¡€æ¶æ„å’Œç¨³å¥çš„æ¨¡å‹é›†æˆç³»ç»Ÿã€‚åœ¨éšè—éªŒè¯é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆè¾¾åˆ°äº†LesionWise DSCçš„å¹³å‡å€¼ä¸º0.897ï¼ŒLesionWise NSDçš„å¹³å‡å€¼ä¸º0.541ï¼Œåœ¨0.5æ¯«ç±³å’Œ1.0æ¯«ç±³çš„å®¹å¿åº¦ä¸‹åˆ†åˆ«ä¸º0.84ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23256v3">PDF</a> Won Third Place Award at Challenge 5 at BraTS-Lighthouse 2025   Challenge (MICCAI 2025)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹éæ´²åœ°åŒºåŒ»ç–—èµ„æºåŒ®ä¹ã€MRIæ‰«æè´¨é‡ä¸ä½³ç­‰é—®é¢˜ï¼ŒEMedNeXtæ¡†æ¶é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°äº†å¯¹è„‘èƒ¶è´¨ç˜¤çš„ç²¾å‡†åˆ†å‰²ã€‚è¯¥æ¡†æ¶ç»“åˆäº†MedNeXt V2çš„æ·±åº¦å­¦ä¹ ç›‘ç£å’Œåå¤„ç†ä¼˜åŒ–æµç¨‹ï¼Œå–å¾—äº†è¾ƒé«˜çš„åˆ†å‰²å‡†ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘ç™Œè¯Šæ–­ä¸­ï¼ŒMRIæ˜¯åŒ»ç”Ÿå¸¸ç”¨çš„è¯Šæ–­å·¥å…·ï¼Œå°¤å…¶åœ¨ä½èµ„æºç¯å¢ƒä¸‹ã€‚</li>
<li>å½“å‰çš„æ‰‹åŠ¨åˆ†å‰²å¤šå‚æ•°MRIæ–¹æ³•å­˜åœ¨æ—¶é—´æˆæœ¬é«˜ã€ä¾èµ–ä¸“å®¶åŠåœ¨èµ„æºåŒ®ä¹åœ°åŒºéš¾ä»¥å®æ–½çš„é—®é¢˜ã€‚</li>
<li>ä½æ”¶å…¥åœ°åŒºç‰¹åˆ«æ˜¯éæ´²çš„MRIæ‰«æèµ„æºåŠè´¨é‡è¾ƒä½ï¼Œå¯¼è‡´è‚¿ç˜¤åˆ†å‰²å’Œé‡åŒ–çš„ä¸å‡†ç¡®ã€‚</li>
<li>BraTS-Lighthouse 2025 Challengeæ—¨åœ¨è§£å†³æ’’å“ˆæ‹‰ä»¥å—éæ´²åœ°åŒºçš„è‚¿ç˜¤åˆ†å‰²é—®é¢˜ã€‚</li>
<li>EMedNeXtæ¡†æ¶åŸºäºMedNeXt V2æ„å»ºï¼Œæ‹¥æœ‰æ·±åº¦å­¦ä¹ ç›‘ç£å’Œä¼˜åŒ–åå¤„ç†æµç¨‹ã€‚</li>
<li>EMedNeXtæ¡†æ¶å¯¹éæ´²åœ°åŒºçš„è„‘èƒ¶è´¨ç˜¤åˆ†å‰²è¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®åº¦ï¼Œå¹³å‡LesionWise DSCè¾¾åˆ°0.897ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b7338ca7b141fbe1baecee20605eccc" align="middle">
<img src="https://picx.zhimg.com/v2-ca8811a6cc12935992263eb4eeff0a8e" align="middle">
<img src="https://picx.zhimg.com/v2-cf377d2ed4e195e8ff779c92d967cfba" align="middle">
<img src="https://picx.zhimg.com/v2-ae128f5648daa610889e8ac3d0ea4314" align="middle">
<img src="https://picx.zhimg.com/v2-d7287f66981914f6072452faeb537131" align="middle">
<img src="https://picx.zhimg.com/v2-dc984c7a0532b7be8dfa071e32958b8e" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MedVKAN-Efficient-Feature-Extraction-with-Mamba-and-KAN-for-Medical-Image-Segmentation"><a href="#MedVKAN-Efficient-Feature-Extraction-with-Mamba-and-KAN-for-Medical-Image-Segmentation" class="headerlink" title="MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical   Image Segmentation"></a>MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical   Image Segmentation</h2><p><strong>Authors:Hancan Zhu, Jinhao Chen, Guanghua He</strong></p>
<p>Medical image segmentation has traditionally relied on convolutional neural networks (CNNs) and Transformer-based models. CNNs, however, are constrained by limited receptive fields, while Transformers face scalability challenges due to quadratic computational complexity. To over-come these issues, recent studies have explored alternative architectures. The Mamba model, a selective state-space design, achieves near-linear complexity and effectively captures long-range dependencies. Its vision-oriented variant, the Visual State Space (VSS) model, extends these strengths to image feature learning. In parallel, the Kolmogorov-Arnold Network (KAN) enhanc-es nonlinear expressiveness by replacing fixed activation functions with learnable ones. Moti-vated by these advances, we propose the VSS-Enhanced KAN (VKAN) module, which integrates VSS with the Expanded Field Convolutional KAN (EFC-KAN) as a replacement for Transformer modules, thereby strengthening feature extraction. We further embed VKAN into a U-Net frame-work, resulting in MedVKAN, an efficient medical image segmentation model. Extensive exper-iments on five public datasets demonstrate that MedVKAN achieves state-of-the-art performance on four datasets and ranks second on the remaining one. These results underscore the effective-ness of combining Mamba and KAN while introducing a novel and computationally efficient feature extraction framework. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/beginner-cjh/MedVKAN">https://github.com/beginner-cjh/MedVKAN</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¼ ç»Ÿä¸Šä¾èµ–äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒåŸºäºTransformerçš„æ¨¡å‹ã€‚ç„¶è€Œï¼ŒCNNså—é™äºæœ‰é™çš„æ„Ÿå—é‡ï¼Œè€ŒTransformeråˆ™ç”±äºäºŒæ¬¡è®¡ç®—å¤æ‚æ€§è€Œé¢ä¸´å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†æ›¿ä»£æ¶æ„ã€‚Mambaæ¨¡å‹æ˜¯ä¸€ç§é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´è®¾è®¡ï¼Œå®ç°äº†è¿‘çº¿æ€§å¤æ‚åº¦ï¼Œå¹¶æœ‰æ•ˆåœ°æ•æ‰äº†é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å…¶é¢å‘è§†è§‰çš„å˜ä½“â€”â€”è§†è§‰çŠ¶æ€ç©ºé—´ï¼ˆVSSï¼‰æ¨¡å‹ï¼Œå°†è¿™äº›ä¼˜ç‚¹æ‰©å±•åˆ°äº†å›¾åƒç‰¹å¾å­¦ä¹ ã€‚åŒæ—¶ï¼ŒKolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰é€šè¿‡ç”¨å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°æ›¿æ¢å›ºå®šçš„æ¿€æ´»å‡½æ•°ï¼Œå¢å¼ºäº†éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚å—åˆ°è¿™äº›è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†VSSå¢å¼ºKANï¼ˆVKANï¼‰æ¨¡å—ï¼Œå®ƒå°†VSSä¸æ‰©å±•å­—æ®µå·ç§¯KANï¼ˆEFC-KANï¼‰ç›¸ç»“åˆï¼Œä½œä¸ºTransformeræ¨¡å—çš„æ›¿ä»£å“ï¼Œä»è€ŒåŠ å¼ºäº†ç‰¹å¾æå–èƒ½åŠ›ã€‚æˆ‘ä»¬å°†VKANè¿›ä¸€æ­¥åµŒå…¥åˆ°U-Netæ¡†æ¶ä¸­ï¼Œå½¢æˆäº†é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹MedVKANã€‚åœ¨äº”ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedVKANåœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨å‰©ä½™çš„ä¸€ä¸ªæ•°æ®é›†ä¸Šæ’åç¬¬äºŒã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ç»“åˆMambaå’ŒKANçš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è®¡ç®—é«˜æ•ˆçš„ç‰¹å¾æå–æ¡†æ¶ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/beginner-cjh/MedVKAN%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/beginner-cjh/MedVKANè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11797v2">PDF</a> This preprint has been published in Biomedical Signal Processing and   Control, Volume 112, 2026, Article 108821</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°æ¨¡å‹MedVKANã€‚è¯¥æ¨¡å‹ç»“åˆMambaå’ŒKANç½‘ç»œçš„ä¼˜ç‚¹ï¼Œé€šè¿‡VKANæ¨¡å—å’ŒEFC-KANæ›¿ä»£Transformeræ¨¡å—ï¼Œå¼ºåŒ–ç‰¹å¾æå–èƒ½åŠ›ã€‚MedVKANåœ¨äº”ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå®ƒåœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨å‰©ä½™ä¸€ä¸ªæ•°æ®é›†ä¸Šæ’åç¬¬äºŒã€‚è¯¥æ¨¡å‹ç»“åˆäº†è®¡ç®—æ•ˆç‡å’Œç‰¹å¾æå–çš„æ–°é¢–æ€§ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¼ ç»Ÿæ–¹æ³•ä¸»è¦ä¾èµ–äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’ŒåŸºäºTransformerçš„æ¨¡å‹ã€‚</li>
<li>CNNå—é™äºæœ‰é™çš„æ„Ÿå—é‡ï¼Œè€ŒTransformeré¢ä¸´ç”±äºäºŒæ¬¡è®¡ç®—å¤æ‚æ€§å¯¼è‡´çš„å¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚</li>
<li>Mambaæ¨¡å‹å’ŒVisual State Spaceï¼ˆVSSï¼‰æ¨¡å‹ä¸ºå…‹æœè¿™äº›é—®é¢˜æä¾›äº†æ–°çš„æ¶æ„é€‰æ‹©ã€‚</li>
<li>Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰é€šè¿‡ç”¨å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°æ›¿æ¢å›ºå®šçš„æ¿€æ´»å‡½æ•°ï¼Œå¢å¼ºäº†éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„VKANæ¨¡å—ç»“åˆäº†VSSå’ŒExpanded Field Convolutional KANï¼ˆEFC-KANï¼‰ï¼Œä½œä¸ºTransformeræ¨¡å—çš„æ›¿ä»£å“ã€‚</li>
<li>MedVKANæ¨¡å‹å°†VKANåµŒå…¥U-Netæ¡†æ¶ä¸­ï¼Œå®ç°äº†é«˜æ•ˆçš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-730784d0c8c85f129617c036eb3412f2" align="middle">
<img src="https://picx.zhimg.com/v2-06bbafe20ae42d032a852c6af5b3c2d9" align="middle">
<img src="https://picx.zhimg.com/v2-774972cd70ff584382bc7dfa1f981e6f" align="middle">
<img src="https://picx.zhimg.com/v2-280ef3a0f6807bf8f727c01a314799f9" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Calibration-and-Uncertainty-for-multiRater-Volume-Assessment-in-multiorgan-Segmentation-CURVAS-challenge-results"><a href="#Calibration-and-Uncertainty-for-multiRater-Volume-Assessment-in-multiorgan-Segmentation-CURVAS-challenge-results" class="headerlink" title="Calibration and Uncertainty for multiRater Volume Assessment in   multiorgan Segmentation (CURVAS) challenge results"></a>Calibration and Uncertainty for multiRater Volume Assessment in   multiorgan Segmentation (CURVAS) challenge results</h2><p><strong>Authors:Meritxell Riera-Marin, Sikha O K, Julia Rodriguez-Comas, Matthias Stefan May, Zhaohong Pan, Xiang Zhou, Xiaokun Liang, Franciskus Xaverius Erick, Andrea Prenner, Cedric Hemon, Valentin Boussot, Jean-Louis Dillenseger, Jean-Claude Nunes, Abdul Qayyum, Moona Mazher, Steven A Niederer, Kaisar Kushibar, Carlos Martin-Isla, Petia Radeva, Karim Lekadir, Theodore Barfoot, Luis C. Garcia Peraza Herrera, Ben Glocker, Tom Vercauteren, Lucas Gago, Justin Englemann, Joy-Marie Kleiss, Anton Aubanell, Andreu Antolin, Javier Garcia-Lopez, Miguel A. Gonzalez Ballester, Adrian Galdran</strong></p>
<p>Deep learning (DL) has become the dominant approach for medical image segmentation, yet ensuring the reliability and clinical applicability of these models requires addressing key challenges such as annotation variability, calibration, and uncertainty estimation. This is why we created the Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS), which highlights the critical role of multiple annotators in establishing a more comprehensive ground truth, emphasizing that segmentation is inherently subjective and that leveraging inter-annotator variability is essential for robust model evaluation. Seven teams participated in the challenge, submitting a variety of DL models evaluated using metrics such as Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and Continuous Ranked Probability Score (CRPS). By incorporating consensus and dissensus ground truth, we assess how DL models handle uncertainty and whether their confidence estimates align with true segmentation performance. Our findings reinforce the importance of well-calibrated models, as better calibration is strongly correlated with the quality of the results. Furthermore, we demonstrate that segmentation models trained on diverse datasets and enriched with pre-trained knowledge exhibit greater robustness, particularly in cases deviating from standard anatomical structures. Notably, the best-performing models achieved high DSC and well-calibrated uncertainty estimates. This work underscores the need for multi-annotator ground truth, thorough calibration assessments, and uncertainty-aware evaluations to develop trustworthy and clinically reliable DL-based medical image segmentation models. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸»æµæ–¹æ³•ï¼Œä½†è¦ç¡®ä¿è¿™äº›æ¨¡å‹å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§ï¼Œéœ€è¦è§£å†³æ ‡æ³¨å˜åŒ–ã€æ ¡å‡†å’Œä¸ç¡®å®šæ€§ä¼°è®¡ç­‰å…³é”®æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šè¯„ä¼°è€…ä½“ç§¯è¯„ä¼°ä¸­çš„æ ¡å‡†å’Œä¸ç¡®å®šæ€§ï¼ˆCURVASï¼‰ï¼Œå®ƒçªå‡ºäº†å¤šè¯„ä¼°è€…åœ¨å»ºç«‹æ›´å…¨é¢çœŸå®æƒ…å†µä¸­çš„å…³é”®ä½œç”¨ï¼Œå¼ºè°ƒåˆ†å‰²æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œåˆ©ç”¨è¯„ä¼°è€…ä¹‹é—´çš„å˜åŒ–å¯¹äºç¨³å¥çš„æ¨¡å‹è¯„ä¼°è‡³å…³é‡è¦ã€‚ä¸ƒä¸ªå›¢é˜Ÿå‚ä¸äº†æ­¤æ¬¡æŒ‘æˆ˜ï¼Œä½¿ç”¨è¯¸å¦‚Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ã€æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰å’Œè¿ç»­æ’åæ¦‚ç‡åˆ†æ•°ï¼ˆCRPSï¼‰ç­‰æŒ‡æ ‡è¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚é€šè¿‡çº³å…¥å…±è¯†å’Œéå…±è¯†çœŸå®æƒ…å†µï¼Œæˆ‘ä»¬è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ä½•å¤„ç†ä¸ç¡®å®šæ€§ï¼Œä»¥åŠå®ƒä»¬çš„ç½®ä¿¡åº¦ä¼°è®¡æ˜¯å¦ä¸çœŸæ­£çš„åˆ†å‰²æ€§èƒ½ç›¸ç¬¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå†æ¬¡å¼ºè°ƒäº†è‰¯å¥½æ ¡å‡†æ¨¡å‹çš„é‡è¦æ€§ï¼Œå› ä¸ºæ›´å¥½çš„æ ¡å‡†ä¸ç»“æœè´¨é‡å¯†åˆ‡ç›¸å…³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜è¡¨æ˜ï¼Œåœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒå¹¶ä¸°å¯Œé¢„è®­ç»ƒçŸ¥è¯†çš„åˆ†å‰²æ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åç¦»æ ‡å‡†è§£å‰–ç»“æ„çš„æƒ…å†µä¸‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹å…·æœ‰é«˜DSCå’Œè‰¯å¥½çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ ¡å‡†ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤šè¯„ä¼°è€…çœŸå®æƒ…å†µã€å…¨é¢çš„æ ¡å‡†è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯„ä¼°çš„éœ€æ±‚ï¼Œä»¥å¼€å‘å¯ä¿¡èµ–å’Œä¸´åºŠå¯é çš„åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08685v2">PDF</a> This challenge was hosted in MICCAI 2024</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„ä¸»å¯¼åœ°ä½ï¼ŒåŒæ—¶æŒ‡å‡ºäº†æ¨¡å‹åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„å¯é æ€§å’Œå¯é æ€§æŒ‘æˆ˜ï¼Œå¦‚æ ‡æ³¨çš„å·®å¼‚æ€§ã€æ ¡å‡†å’Œä¸ç¡®å®šæ€§ä¼°è®¡ç­‰ã€‚ä¸ºæ­¤ï¼Œåˆ›å»ºäº†CURVASæŒ‘æˆ˜ï¼Œå¼ºè°ƒå¤šè¯„ä¼°è€…å¯¹å»ºç«‹æ›´å…¨é¢çœŸå®çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºåˆ†å‰²æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œåˆ©ç”¨è¯„ä¼°è€…ä¹‹é—´çš„å·®å¼‚æ€§å¯¹ç¨³å¥æ¨¡å‹è¯„ä¼°è‡³å…³é‡è¦ã€‚ç ”ç©¶å‘ç°ï¼Œè‰¯å¥½çš„æ ¡å‡†æ¨¡å‹ä¸ç»“æœè´¨é‡å¯†åˆ‡ç›¸å…³ï¼Œç»è¿‡å¤šæ ·åŒ–æ•°æ®é›†è®­ç»ƒå’Œé¢„è®­ç»ƒçŸ¥è¯†å¢å¼ºçš„åˆ†å‰²æ¨¡å‹å…·æœ‰æ›´å¤§çš„ç¨³å¥æ€§ã€‚æœ€å¥½çš„æ¨¡å‹å®ç°äº†é«˜DSCå’Œè‰¯å¥½çš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å¼ºè°ƒéœ€è¦å¤šè¯„ä¼°è€…çœŸå®ã€å…¨é¢çš„æ ¡å‡†è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„è¯†è¯„ä¼°ï¼Œä»¥å¼€å‘å¯ä¿¡ä¸”ä¸´åºŠå¯é çš„åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ å·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸»è¦æ–¹æ³•ï¼Œä½†ç¡®ä¿å…¶å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åˆ›å»ºCURVASæŒ‘æˆ˜ï¼Œå¼ºè°ƒå¤šè¯„ä¼°è€…å¯¹å»ºç«‹æ›´å…¨é¢çœŸå®çš„é‡è¦æ€§ã€‚</li>
<li>åˆ†å‰²æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œåˆ©ç”¨è¯„ä¼°è€…ä¹‹é—´çš„å·®å¼‚æ€§å¯¹ç¨³å¥æ¨¡å‹è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>è‰¯å¥½çš„æ ¡å‡†æ¨¡å‹ä¸ç»“æœè´¨é‡å¯†åˆ‡ç›¸å…³ã€‚</li>
<li>ç»è¿‡å¤šæ ·åŒ–æ•°æ®é›†è®­ç»ƒå’Œé¢„è®­ç»ƒçŸ¥è¯†å¢å¼ºçš„åˆ†å‰²æ¨¡å‹å…·æœ‰æ›´å¤§çš„ç¨³å¥æ€§ã€‚</li>
<li>æœ€ä½³æ¨¡å‹å…·æœ‰é«˜DSCå’Œè‰¯å¥½çš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-846eb22a6a4ebcff605b4520ba256901" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SAIP-Net-Enhancing-Remote-Sensing-Image-Segmentation-via-Spectral-Adaptive-Information-Propagation"><a href="#SAIP-Net-Enhancing-Remote-Sensing-Image-Segmentation-via-Spectral-Adaptive-Information-Propagation" class="headerlink" title="SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral   Adaptive Information Propagation"></a>SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral   Adaptive Information Propagation</h2><p><strong>Authors:Zhongtao Wang, Xizhe Cao, Yisong Chen, Guoping Wang</strong></p>
<p>Semantic segmentation of remote sensing imagery demands precise spatial boundaries and robust intra-class consistency, challenging conventional hierarchical models. To address limitations arising from spatial domain feature fusion and insufficient receptive fields, this paper introduces SAIP-Net, a novel frequency-aware segmentation framework that leverages Spectral Adaptive Information Propagation. SAIP-Net employs adaptive frequency filtering and multi-scale receptive field enhancement to effectively suppress intra-class feature inconsistencies and sharpen boundary lines. Comprehensive experiments demonstrate significant performance improvements over state-of-the-art methods, highlighting the effectiveness of spectral-adaptive strategies combined with expanded receptive fields for remote sensing image segmentation. </p>
<blockquote>
<p>é¥æ„Ÿå½±åƒçš„è¯­ä¹‰åˆ†å‰²è¦æ±‚ç²¾ç¡®çš„ç©ºé—´è¾¹ç•Œå’Œç¨³å¥çš„ç±»å†…ä¸€è‡´æ€§ï¼Œè¿™å¯¹ä¼ ç»Ÿçš„å±‚æ¬¡æ¨¡å‹æå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³ç”±ç©ºé—´åŸŸç‰¹å¾èåˆå’Œæ„Ÿå—é‡ä¸è¶³è€Œäº§ç”Ÿçš„å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†SAIP-Netï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„é¢‘ç‡æ„ŸçŸ¥åˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨è°±è‡ªé€‚åº”ä¿¡æ¯ä¼ æ’­ã€‚SAIP-Neté‡‡ç”¨è‡ªé€‚åº”é¢‘ç‡æ»¤æ³¢å’Œå¤šå°ºåº¦æ„Ÿå—é‡å¢å¼ºï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†ç±»å†…ç‰¹å¾çš„ä¸ä¸€è‡´æ€§ï¼Œæé«˜äº†è¾¹ç•Œçº¿çš„æ¸…æ™°åº¦ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ€æ–°æ–¹æ³•çš„åŸºç¡€ä¸Šæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œçªå‡ºäº†è°±è‡ªé€‚åº”ç­–ç•¥ä¸æ‰©å±•æ„Ÿå—é‡åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16564v2">PDF</a> </p>
<p><strong>Summary</strong><br>    æ–°å‹é¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²æ¨¡å‹SAIP-Netæå‡ºåŸºäºé¢‘åŸŸè‡ªé€‚åº”ä¿¡æ¯å¤„ç†æŠ€æœ¯çš„æ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”é¢‘ç‡è¿‡æ»¤å’Œå¤šå°ºåº¦æ„Ÿå—é‡å¢å¼ºæŠ€æœ¯ï¼Œæé«˜äº†ç©ºé—´è¾¹ç•Œçš„ç²¾ç¡®åº¦å¹¶å¢å¼ºäº†ç±»å†…ä¸€è‡´æ€§ï¼Œæ˜¾è‘—æé«˜äº†é¥æ„Ÿå½±åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAIP-Netæ˜¯ä¸€ç§é’ˆå¯¹é¥æ„Ÿå½±åƒè¯­ä¹‰åˆ†å‰²çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>å®ƒåˆ©ç”¨é¢‘åŸŸè‡ªé€‚åº”ä¿¡æ¯å¤„ç†æŠ€æœ¯æ¥è§£å†³ä¼ ç»Ÿæ¨¡å‹åœ¨ç©ºé—´åŸŸç‰¹å¾èåˆå’Œæ„Ÿå—é‡ä¸è¶³æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>SAIP-Neté€šè¿‡è‡ªé€‚åº”é¢‘ç‡è¿‡æ»¤æ¥æŠ‘åˆ¶ç±»å†…ç‰¹å¾çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>å¤šå°ºåº¦æ„Ÿå—é‡å¢å¼ºæŠ€æœ¯è¢«ç”¨æ¥æé«˜è¾¹ç•Œçº¿çš„æ¸…æ™°åº¦ã€‚</li>
<li>SAIP-Neté€šè¿‡æ‰©å¤§æ„Ÿå—é‡å’Œç»“åˆå…‰è°±è‡ªé€‚åº”ç­–ç•¥ï¼Œå®ç°äº†å¯¹å…ˆè¿›æ–¹æ³•çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨é¥æ„Ÿå½±åƒåˆ†å‰²é¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a08c53deef9ed66d9bd87a30bc929889" align="middle">
<img src="https://picx.zhimg.com/v2-109f3edd26b945536a1b9b9274d96c8f" align="middle">
<img src="https://picx.zhimg.com/v2-4eab25a3b84f45c6594ed88f6aa76aca" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="X-ray-Polarimetry-in-the-Low-Statistics-Regime-using-the-Bayesian-Approach-Reveals-Polarization-Angle-Variations"><a href="#X-ray-Polarimetry-in-the-Low-Statistics-Regime-using-the-Bayesian-Approach-Reveals-Polarization-Angle-Variations" class="headerlink" title="X-ray Polarimetry in the Low Statistics Regime using the Bayesian   Approach Reveals Polarization Angle Variations"></a>X-ray Polarimetry in the Low Statistics Regime using the Bayesian   Approach Reveals Polarization Angle Variations</h2><p><strong>Authors:Hong Li, Qing-Chang Zhao, Hua Feng, Lian Tao, Sergey S. Tsygankov</strong></p>
<p>X-ray polarimetry of accreting compact objects has revealed fast time variations in the polarization angle (PA), suggesting that the geometry and&#x2F;or optical depth of the Comptonization region is changing rapidly. This prompts investigations into how fast such variability can be. Conventionally, the data are often binned to examine the time variability such that the measurement in each bin is above the minimum detectable polarization (MDP). Here we demonstrate that this is unnecessary, and even below the MDP, one can infer the posterior distribution of PA reliably using the Bayesian approach and still be able to place useful constraints on the physics in many cases, due to small relative uncertainties on PA (e.g., $\Delta$PA $\approx$ 10â€“30$^\circ$ compared with a dynamical range of 180$^\circ$). With this approach, we discovered that the PA variation in one of the Imaging X-ray Polarimetry Explorer (IXPE) observations of GX 13+1 is not following a linear rotation mode as suggested previously. Instead, the PA swings between two discrete angles, suggesting that there are two emitting components, e.g., the boundary layer and the spreading layer, competing with each other. In XTE J1701-462, we confirmed previous results for a variable PA in the normal branch, and furthermore, revealed that the variation timescale could be as short as 1.5 hours. During the IXPE observation of Sco X-1, a hint is found for the PA in the highest flux level to be different from the average but consistent with previous measurement results with PolarLight and OSO-8. </p>
<blockquote>
<p>å¯¹è‡´å¯†å¸ç§¯å¤©ä½“è¿›è¡ŒXå°„çº¿åæŒ¯æµ‹é‡æ­ç¤ºäº†åæŒ¯è§’ï¼ˆPAï¼‰çš„å¿«é€Ÿæ—¶é—´å˜åŒ–ï¼Œè¿™è¡¨æ˜åº·æ™®é¡¿åŒ–åŒºåŸŸçš„å‡ ä½•å½¢çŠ¶å’Œ&#x2F;æˆ–å…‰å­¦æ·±åº¦æ­£åœ¨å¿«é€Ÿå˜åŒ–ã€‚è¿™ä¿ƒä½¿äººä»¬ç ”ç©¶è¿™ç§å˜åŒ–é€Ÿåº¦æœ‰å¤šå¿«ã€‚é€šå¸¸ï¼Œæ•°æ®é€šå¸¸ä¼šè¢«åˆ†ç®±å¤„ç†ä»¥æ£€æŸ¥æ—¶é—´å˜åŒ–ï¼Œä»¥ä¾¿æ¯ä¸ªç®±å†…çš„æµ‹é‡å€¼éƒ½é«˜äºæœ€å°å¯æ£€æµ‹åæŒ¯åº¦ï¼ˆMDPï¼‰ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è¯æ˜è¿™æ˜¯ä¸å¿…è¦çš„ï¼Œå³ä½¿ä½äºMDPï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨è´å¶æ–¯æ–¹æ³•å¯é åœ°æ¨æ–­PAçš„åéªŒåˆ†å¸ƒï¼Œå¹¶ä¸”åœ¨è®¸å¤šæƒ…å†µä¸‹ä»ç„¶å¯ä»¥å¯¹ç‰©ç†å­¦æ–½åŠ æœ‰ç”¨çš„çº¦æŸï¼Œè¿™æ˜¯ç”±äºPAçš„ç›¸å¯¹ä¸ç¡®å®šæ€§è¾ƒå°ï¼ˆä¾‹å¦‚ï¼Œä¸180Â°çš„åŠ¨æ€èŒƒå›´ç›¸æ¯”ï¼ŒÎ”PAçº¦ä¸º10-30Â°ï¼‰ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°GX 13+1æˆåƒXå°„çº¿åæŒ¯ä»ªï¼ˆIXPEï¼‰è§‚æµ‹çš„PAå˜åŒ–å¹¶æ²¡æœ‰éµå¾ªå…ˆå‰æå‡ºçš„çº¿æ€§æ—‹è½¬æ¨¡å¼ã€‚ç›¸åï¼ŒPAåœ¨ä¸¤ä¸ªç¦»æ•£è§’åº¦ä¹‹é—´æ‘†åŠ¨ï¼Œè¿™è¡¨æ˜å­˜åœ¨ä¸¤ä¸ªå‘å°„åˆ†é‡ï¼Œä¾‹å¦‚è¾¹ç•Œå±‚å’Œæ‰©å±•å±‚ç›¸äº’ç«äº‰ã€‚åœ¨XTE J1701-462ä¸­ï¼Œæˆ‘ä»¬è¯å®äº†æ­£å¸¸åˆ†æ”¯ä¸­å¯å˜PAçš„å…ˆå‰ç»“æœï¼Œå¹¶ä¸”è¿›ä¸€æ­¥æ­ç¤ºå˜åŒ–æ—¶é—´å°ºåº¦å¯èƒ½çŸ­è‡³1.5å°æ—¶ã€‚åœ¨Sco X-1çš„IXPEè§‚æµ‹ä¸­ï¼Œå‘ç°æœ€é«˜é€šé‡æ°´å¹³çš„PAä¸å¹³å‡å€¼ä¸åŒï¼Œä½†ä¸ä¹‹å‰ä½¿ç”¨PolarLightå’ŒOSO-8çš„æµ‹é‡ç»“æœä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04775v2">PDF</a> 9 pages, 7 figures. Accepted for publication in ApJ</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Xå°„çº¿åæŒ¯æµ‹é‡æ­ç¤ºäº†è‡´å¯†å¤©ä½“åœ¨åæŒ¯è§’ï¼ˆPAï¼‰ä¸Šçš„å¿«é€Ÿæ—¶é—´å˜åŒ–ï¼Œæš—ç¤ºåº·æ™®é¡¿åŒ–åŒºåŸŸçš„å‡ ä½•ç»“æ„å’Œå…‰å­¦åšåº¦åœ¨è¿…é€Ÿå˜åŒ–ã€‚ç ”ç©¶èšç„¦äºè¿™ç§å˜åŒ–æ€§çš„é€Ÿåº¦æœ‰å¤šå¿«ã€‚ä¼ ç»Ÿä¸Šï¼Œä¸ºäº†ç ”ç©¶æ—¶é—´å˜åŒ–æ€§ï¼Œæ•°æ®é€šå¸¸ä»¥è¶…å‡ºæœ€å°å¯æ£€æµ‹åæŒ¯åº¦ï¼ˆMDPï¼‰çš„æ–¹å¼è¿›è¡Œåˆ†ç®±å¤„ç†ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å±•ç¤ºäº†è¿™æ˜¯ä¸å¿…è¦çš„ï¼Œå¹¶ä¸”å³ä¾¿ä½äºMDPï¼Œä»å¯ä»¥åˆ©ç”¨è´å¶æ–¯æ–¹æ³•å¯é æ¨æ–­PAçš„åéªŒåˆ†å¸ƒï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ä»èƒ½å¯¹ç‰©ç†é—®é¢˜æå‡ºæœ‰ç”¨çš„é™åˆ¶ã€‚åˆ©ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°GX 13+1çš„ä¸€ä¸ªæˆåƒXå°„çº¿åæŒ¯ä»ªï¼ˆIXPEï¼‰è§‚æµ‹ä¸­PAå˜åŒ–å¹¶ééµå¾ªå…ˆå‰æ¨æµ‹çš„çº¿æ€§æ—‹è½¬æ¨¡å¼ï¼Œè€Œæ˜¯ä»‹äºä¸¤ä¸ªç¦»æ•£è§’åº¦ä¹‹é—´æ‘†åŠ¨ï¼Œæš—ç¤ºå­˜åœ¨ä¸¤ä¸ªå‘å°„æˆåˆ†ï¼ˆå¦‚è¾¹ç•Œå±‚å’Œæ‰©æ•£å±‚ï¼‰åœ¨ç›¸äº’ç«äº‰ã€‚åœ¨XTE J1701-462ä¸­ï¼Œæˆ‘ä»¬ç¡®è®¤äº†æ­£å¸¸æ”¯è·¯ä¸­å¯å˜PAçš„å…ˆå‰ç»“æœï¼Œå¹¶ä¸”è¿›ä¸€æ­¥æ­ç¤ºå˜åŒ–æ—¶é—´å°ºåº¦å¯èƒ½çŸ­è‡³1.5å°æ—¶ã€‚åœ¨Sco X-1çš„IXPEè§‚æµ‹ä¸­ï¼Œå‘ç°æœ€é«˜é€šé‡æ°´å¹³æ—¶çš„PAä¸å¹³å‡å€¼ä¸åŒä½†ä¸ä¹‹å‰çš„æµ‹é‡ç»“æœä¸€è‡´ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜äº†å¯¹åæŒ¯æ•°æ®çš„è´å¶æ–¯åˆ†æå¯¹äºæ­ç¤ºè‡´å¯†å¤©ä½“ä¸­çš„å¤æ‚ç‰©ç†ç°è±¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è¦ç‚¹</strong></p>
<ol>
<li>Xå°„çº¿åæŒ¯æµ‹é‡æ­ç¤ºäº†è‡´å¯†å¤©ä½“åœ¨åæŒ¯è§’ä¸Šçš„å¿«é€Ÿå˜åŒ–ã€‚</li>
<li>è´å¶æ–¯æ–¹æ³•å…è®¸åœ¨ä½äºæœ€å°å¯æ£€æµ‹åæŒ¯åº¦çš„æƒ…å†µä¸‹æ¨æ–­åæŒ¯è§’åéªŒåˆ†å¸ƒã€‚</li>
<li>åœ¨GX 13+1ä¸­è§‚æµ‹åˆ°åæŒ¯è§’å˜åŒ–å¹¶éçº¿æ€§æ—‹è½¬æ¨¡å¼ï¼Œæš—ç¤ºå­˜åœ¨ä¸¤ä¸ªå‘å°„æˆåˆ†ç«äº‰ã€‚</li>
<li>åœ¨XTE J1701-462ä¸­ç¡®è®¤äº†å¯å˜åæŒ¯è§’å¹¶å‘ç°å˜åŒ–æ—¶é—´å°ºåº¦å¯çŸ­è‡³å‡ å°æ—¶ã€‚</li>
<li>åœ¨Sco X-1çš„è§‚æµ‹ä¸­å‘ç°æœ€é«˜é€šé‡æ°´å¹³æ—¶çš„åæŒ¯è§’ä¸å…ˆå‰æµ‹é‡ç»“æœä¸€è‡´ã€‚</li>
<li>è´å¶æ–¯åˆ†æå¯¹æ­ç¤ºè‡´å¯†å¤©ä½“ä¸­çš„å¤æ‚ç‰©ç†ç°è±¡è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec5fe083f7b68acf1ead0532b28ce508" align="middle">
<img src="https://picx.zhimg.com/v2-aec00f22a25ba2390a4cf607b869f27b" align="middle">
<img src="https://picx.zhimg.com/v2-077876af540df9481ac7141432df8d29" align="middle">
<img src="https://picx.zhimg.com/v2-b16a0de1bf00bb3c6da5e4e8043c962e" align="middle">
<img src="https://picx.zhimg.com/v2-798058107a2d0cdad1c0bf843e3097e0" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis"><a href="#A-Survey-on-Self-supervised-Contrastive-Learning-for-Multimodal-Text-Image-Analysis" class="headerlink" title="A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis"></a>A Survey on Self-supervised Contrastive Learning for Multimodal   Text-Image Analysis</h2><p><strong>Authors:Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif</strong></p>
<p>Self-supervised learning is a machine learning approach that generates implicit labels by learning underlined patterns and extracting discriminative features from unlabeled data without manual labelling. Contrastive learning introduces the concept of â€œpositiveâ€ and â€œnegativeâ€ samples, where positive pairs (e.g., variation of the same image&#x2F;object) are brought together in the embedding space, and negative pairs (e.g., views from different images&#x2F;objects) are pushed farther away. This methodology has shown significant improvements in image understanding and image text analysis without much reliance on labeled data. In this paper, we comprehensively discuss the terminologies, recent developments and applications of contrastive learning with respect to text-image models. Specifically, we provide an overview of the approaches of contrastive learning in text-image models in recent years. Secondly, we categorize the approaches based on different model structures. Thirdly, we further introduce and discuss the latest advances of the techniques used in the process such as pretext tasks for both images and text, architectural structures, and key trends. Lastly, we discuss the recent state-of-art applications of self-supervised contrastive learning Text-Image based models. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä»éæ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨çš„æ¨¡å¼å¹¶æå–åŒºåˆ†ç‰¹å¾ï¼Œç”Ÿæˆéšå«çš„æ ‡ç­¾ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå…¶ä¸­æ­£æ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚åŒä¸€å›¾åƒ&#x2F;å¯¹è±¡çš„å˜ä½“ï¼‰åœ¨åµŒå…¥ç©ºé—´ä¸­èšé›†åœ¨ä¸€èµ·ï¼Œè€Œè´Ÿæ ·æœ¬å¯¹ï¼ˆä¾‹å¦‚æ¥è‡ªä¸åŒå›¾åƒ&#x2F;å¯¹è±¡çš„è§†å›¾ï¼‰åˆ™è¢«æ¨å¼€ã€‚è¿™ç§æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œå›¾åƒæ–‡æœ¬åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œæå¤§åœ°å‡å°‘äº†å¯¹äºæ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ–‡æœ¬-å›¾åƒæ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ è¿›è¡Œäº†å…¨é¢çš„æœ¯è¯­è®¨è®ºã€è¿‘æœŸå‘å±•ä»¥åŠåº”ç”¨æ–¹é¢çš„ä»‹ç»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†è¿‘å¹´æ¥æ–‡æœ¬-å›¾åƒæ¨¡å‹ä¸­å¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„æ¨¡å‹ç»“æ„å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†åˆ†ç±»ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»äº†åœ¨è¯¥è¿‡ç¨‹ä¸­ä½¿ç”¨çš„æœ€æ–°æŠ€æœ¯çš„è¿›å±•ï¼Œå¦‚å›¾åƒå’Œæ–‡æœ¬çš„é¢„è®­ç»ƒä»»åŠ¡ã€æ¶æ„ç»“æ„å’Œå…³é”®è¶‹åŠ¿ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åŸºäºæ–‡æœ¬-å›¾åƒæ¨¡å‹çš„è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æœ€æ–°åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11101v5">PDF</a> 38 pages, 8 figures, survey paper</p>
<p><strong>Summary</strong><br>     è‡ªç›‘ç£å­¦ä¹ é€šè¿‡ä»éæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æ½œåœ¨æ¨¡å¼å’Œæå–åˆ¤åˆ«ç‰¹å¾ï¼Œç”Ÿæˆéšå¼æ ‡ç­¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚å¯¹æ¯”å­¦ä¹ å¼•å…¥äº†â€œæ­£æ ·æœ¬â€å’Œâ€œè´Ÿæ ·æœ¬â€çš„æ¦‚å¿µï¼Œå°†æ­£æ ·æœ¬å¯¹æ‹‰è¿‘åµŒå…¥ç©ºé—´ï¼Œå°†è´Ÿæ ·æœ¬å¯¹æ¨å¼€ã€‚æ­¤æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†ææ–¹é¢æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚æœ¬æ–‡å…¨é¢æ¢è®¨äº†å¯¹æ¯”å­¦ä¹ åœ¨æ–‡æœ¬å›¾åƒæ¨¡å‹ä¸­çš„æœ¯è¯­ã€æœ€æ–°å‘å±•å’Œåº”ç”¨ï¼Œæ¦‚è¿°äº†è¿‘å¹´æ¥çš„æ–¹æ³•ï¼ŒæŒ‰æ¨¡å‹ç»“æ„åˆ†ç±»ï¼Œå¹¶ä»‹ç»äº†æœ€æ–°çš„æŠ€æœ¯å’Œè¶‹åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ é€šè¿‡éæ ‡è®°æ•°æ®å­¦ä¹ æ½œåœ¨æ¨¡å¼å’Œæå–ç‰¹å¾ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ åœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­å¼•å…¥æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬æ¦‚å¿µã€‚</li>
<li>æ­£æ ·æœ¬å¯¹åœ¨åµŒå…¥ç©ºé—´ä¸­ç›¸äº’æ¥è¿‘ï¼Œè´Ÿæ ·æœ¬å¯¹åˆ™ç›¸äº’è¿œç¦»ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ æ–¹æ³•åœ¨å›¾åƒç†è§£å’Œæ–‡æœ¬åˆ†ææ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå‡å°‘å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
<li>æ–‡æœ¬å›¾åƒæ¨¡å‹çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•è¿‘æœŸå¾—åˆ°å…¨é¢å‘å±•ã€‚</li>
<li>æœ¬æ–‡æŒ‰æ¨¡å‹ç»“æ„åˆ†ç±»äº†å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35b9b4a11f71855bba70d7517f93a159" align="middle">
<img src="https://picx.zhimg.com/v2-2313a47b591bc59ade6a1c8862ed1fe2" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Convergence-of-Ray-and-Pixel-Driven-Discretization-Frameworks-in-the-Strong-Operator-Topology"><a href="#Convergence-of-Ray-and-Pixel-Driven-Discretization-Frameworks-in-the-Strong-Operator-Topology" class="headerlink" title="Convergence of Ray- and Pixel-Driven Discretization Frameworks in the   Strong Operator Topology"></a>Convergence of Ray- and Pixel-Driven Discretization Frameworks in the   Strong Operator Topology</h2><p><strong>Authors:Richard Huber</strong></p>
<p>Tomography is a central tool in medical applications, allowing doctors to investigate patientsâ€™ interior features. The Radon transform (in two dimensions) is commonly used to model the measurement process in parallel-beam CT. Suitable discretization of the Radon transform and its adjoint (called the backprojection) is crucial. The most commonly used discretization approach combines what we refer to as the ray-driven Radon transform with what we refer to as the pixel-driven backprojection, as anecdotal reports describe these as showing the best approximation performance. However, there is little rigorous understanding of induced approximation errors. These methods involve three discretization parameters: the spatial-, detector-, and angular resolutions. Most commonly, balanced resolutions are used, i.e., the same (or similar) spatial- and detector resolutions are employed. We present an interpretation of ray- and pixel-driven discretizations as &#96;convolutional methodsâ€™, a special class of finite-rank operators. This allows for a structured analysis that can explain observed behavior. In particular, we prove convergence in the strong operator topology of the ray-driven Radon transform and the pixel-driven backprojection under balanced resolutions, thus theoretically justifying this approach. In particular, with high enough resolutions one can approximate the Radon transform arbitrarily well. Numerical experiments corroborate these theoretical findings. </p>
<blockquote>
<p>æ–­å±‚æ‰«ææ˜¯åŒ»å­¦åº”ç”¨ä¸­çš„æ ¸å¿ƒå·¥å…·ï¼Œå…è®¸åŒ»ç”Ÿæ£€æŸ¥æ‚£è€…çš„å†…éƒ¨ç»“æ„ã€‚Radonå˜æ¢ï¼ˆäºŒç»´ï¼‰å¸¸ç”¨äºå¹³è¡ŒæŸCTçš„æµ‹é‡è¿‡ç¨‹å»ºæ¨¡ã€‚Radonå˜æ¢åŠå…¶ä¼´éšï¼ˆç§°ä¸ºåå‘æŠ•å½±ï¼‰çš„é€‚å½“ç¦»æ•£åŒ–è‡³å…³é‡è¦ã€‚æœ€å¸¸ç”¨çš„ç¦»æ•£åŒ–æ–¹æ³•ç»“åˆäº†æ‰€è°“çš„å°„çº¿é©±åŠ¨Radonå˜æ¢å’Œæ‰€è°“çš„åƒç´ é©±åŠ¨åå‘æŠ•å½±ï¼Œå¦‚ä¸ªåˆ«æŠ¥å‘Šæ‰€è¿°ï¼Œè¿™äº›æ–¹æ³•è¡¨ç°å‡ºæœ€ä½³çš„è¿‘ä¼¼æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹è¯±å¯¼çš„è¿‘ä¼¼è¯¯å·®çš„ä¸¥æ ¼ç†è§£å¾ˆå°‘ã€‚è¿™äº›æ–¹æ³•æ¶‰åŠä¸‰ä¸ªç¦»æ•£åŒ–å‚æ•°ï¼šç©ºé—´åˆ†è¾¨ç‡ã€æ£€æµ‹å™¨åˆ†è¾¨ç‡å’Œè§’åº¦åˆ†è¾¨ç‡ã€‚é€šå¸¸ä½¿ç”¨çš„æ˜¯å¹³è¡¡åˆ†è¾¨ç‡ï¼Œå³é‡‡ç”¨ç›¸åŒï¼ˆæˆ–ç›¸ä¼¼ï¼‰çš„ç©ºé—´åˆ†è¾¨ç‡å’Œæ£€æµ‹å™¨åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬å°†å°„çº¿é©±åŠ¨å’Œåƒç´ é©±åŠ¨çš„ç¦»æ•£åŒ–è§£é‡Šä¸ºâ€œå·ç§¯æ–¹æ³•â€ï¼Œè¿™æ˜¯ä¸€ç±»ç‰¹æ®Šçš„æœ‰é™ç§©ç®—å­ã€‚è¿™å…è®¸è¿›è¡Œæœ‰ç»“æ„çš„åˆ†æï¼Œå¯ä»¥è§£é‡Šè§‚å¯Ÿåˆ°çš„è¡Œä¸ºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨å¹³è¡¡åˆ†è¾¨ç‡ä¸‹ï¼Œå°„çº¿é©±åŠ¨çš„Radonå˜æ¢å’Œåƒç´ é©±åŠ¨çš„åå‘æŠ•å½±åœ¨å¼ºç®—å­æ‹“æ‰‘ä¸­æ˜¯æ”¶æ•›çš„ï¼Œä»è€Œä¸ºè¿™ç§æ–¹æ³•æä¾›äº†ç†è®ºæ”¯æŒã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨è¶³å¤Ÿé«˜çš„åˆ†è¾¨ç‡ä¸‹ï¼Œå¯ä»¥ä»»æ„å¥½åœ°è¿‘ä¼¼Radonå˜æ¢ã€‚æ•°å€¼å®éªŒè¯å®äº†è¿™äº›ç†è®ºå‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03069v3">PDF</a> 38 pages, 14 figures, Preprint was substantially updated with   inclusion of section 4.2.2 concerning numerical experiments for the   backprojection, as well as improvements in all sections</p>
<p><strong>Summary</strong><br>     åŒ»å­¦æ–­å±‚æˆåƒä¸­Radonå˜æ¢åŠå…¶ç¦»æ•£åŒ–çš„é‡è¦æ€§ã€‚ç»“åˆåƒç´ é©±åŠ¨å’Œå°„çº¿é©±åŠ¨æ–¹æ³•ï¼Œé€šè¿‡å·ç§¯æ–¹æ³•è§£æç¦»æ•£åŒ–è¿‡ç¨‹ï¼Œè¯æ˜åœ¨å¹³è¡¡åˆ†è¾¨ç‡ä¸‹å…¶æ”¶æ•›æ€§ï¼Œæ•°å€¼å®éªŒæ”¯æŒè¿™äº›ç†è®ºå‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–­å±‚æˆåƒä¸­Radonå˜æ¢ç”¨äºå»ºæ¨¡æµ‹é‡è¿‡ç¨‹çš„é‡è¦æ€§ã€‚</li>
<li>Radonå˜æ¢çš„ç¦»æ•£åŒ–åŠå…¶ä¼´éšçš„èƒŒæŠ•å½±æ˜¯å…³é”®æ­¥éª¤ã€‚</li>
<li>åƒç´ é©±åŠ¨å’Œå°„çº¿é©±åŠ¨æ–¹æ³•æ˜¯æœ€å¸¸ç”¨çš„ç¦»æ•£åŒ–æ–¹æ³•ï¼Œè¢«è¯æ˜åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ç¼ºä¹å…³äºè¿‘ä¼¼è¯¯å·®çš„ä¸¥æ ¼ç†è§£ã€‚</li>
<li>ä»‹ç»äº†å°†åƒç´ é©±åŠ¨å’Œå°„çº¿é©±åŠ¨ç¦»æ•£åŒ–è§†ä¸ºå·ç§¯æ–¹æ³•çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œå³æœ‰é™ç§©ç®—å­ã€‚</li>
<li>è¯æ˜åœ¨å¹³è¡¡åˆ†è¾¨ç‡ä¸‹ï¼Œå°„çº¿é©±åŠ¨çš„Radonå˜æ¢å’Œåƒç´ é©±åŠ¨çš„èƒŒæŠ•å½±åœ¨å¼ºç®—å­æ‹“æ‰‘ä¸­æ”¶æ•›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3688fa93c7e45ff1f4a89c30ae5e7a3" align="middle">
<img src="https://picx.zhimg.com/v2-b1a80c115e9f5df83b532efe7fc6fba0" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RadVLM-A-Multitask-Conversational-Vision-Language-Model-for-Radiology"><a href="#RadVLM-A-Multitask-Conversational-Vision-Language-Model-for-Radiology" class="headerlink" title="RadVLM: A Multitask Conversational Vision-Language Model for Radiology"></a>RadVLM: A Multitask Conversational Vision-Language Model for Radiology</h2><p><strong>Authors:Nicolas Deperrois, Hidetoshi Matsuo, Samuel RuipÃ©rez-Campillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser, Koji Fujimoto, Mizuho Nishio, Thomas M. Sutter, Julia E. Vogt, Jonas Kluckert, Thomas Frauenfelder, Christian BlÃ¼thgen, Farhad Nooralahzadeh, Michael Krauthammer</strong></p>
<p>The widespread use of chest X-rays (CXRs), coupled with a shortage of radiologists, has driven growing interest in automated CXR analysis and AI-assisted reporting. While existing vision-language models (VLMs) show promise in specific tasks such as report generation or abnormality detection, they often lack support for interactive diagnostic capabilities. In this work we present RadVLM, a compact, multitask conversational foundation model designed for CXR interpretation. To this end, we curate a large-scale instruction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks â€“ such as report generation, abnormality classification, and visual grounding â€“ and multi-turn, multi-task conversational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. Our results show that RadVLM achieves state-of-the-art performance in conversational capabilities and visual grounding while remaining competitive in other radiology tasks. Ablation studies further highlight the benefit of joint training across multiple tasks, particularly for scenarios with limited annotated data. Together, these findings highlight the potential of RadVLM as a clinically relevant AI assistant, providing structured CXR interpretation and conversational capabilities to support more effective and accessible diagnostic workflows. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰ï¼ˆCXRï¼‰çš„å¹¿æ³›åº”ç”¨ä»¥åŠæ”¾å°„ç§‘åŒ»ç”Ÿçš„çŸ­ç¼ºï¼Œå¼•å‘äº†äººä»¬å¯¹è‡ªåŠ¨CXRåˆ†æå’ŒAIè¾…åŠ©æŠ¥å‘Šç”ŸæˆæŠ€æœ¯çš„æ—¥ç›Šæµ“åšçš„å…´è¶£ã€‚è™½ç„¶ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æŠ¥å‘Šç”Ÿæˆæˆ–å¼‚å¸¸æ£€æµ‹ç­‰ç‰¹å®šä»»åŠ¡ä¸Šå±•ç°å‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹æ”¯æŒäº¤äº’è¯Šæ–­çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†RadVLMï¼Œè¿™æ˜¯ä¸€ä¸ªç´§å‡‘çš„å¤šä»»åŠ¡å¯¹è¯åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºCXRè§£è¯»è®¾è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡1ç™¾ä¸‡å¼ å›¾åƒå’ŒæŒ‡ä»¤å¯¹ï¼ŒåŒ…å«å¦‚æŠ¥å‘Šç”Ÿæˆã€å¼‚å¸¸åˆ†ç±»å’Œè§†è§‰å®šä½ç­‰å•å›åˆä»»åŠ¡ï¼Œä»¥åŠå¤šå›åˆå¤šä»»åŠ¡å¯¹è¯äº¤äº’ã€‚åœ¨è¿™ä¸ªæŒ‡ä»¤æ•°æ®é›†ä¸Šå¾®è°ƒRadVLMåï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸é‡æ–°å®ç°çš„åŸºçº¿VLMsè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒRadVLMåœ¨å¯¹è¯èƒ½åŠ›å’Œè§†è§‰å®šä½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å…¶ä»–æ”¾å°„å­¦ä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥å¼ºè°ƒäº†è·¨å¤šä¸ªä»»åŠ¡è”åˆè®­ç»ƒçš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚è¿™äº›å‘ç°å…±åŒçªå‡ºäº†RadVLMä½œä¸ºä¸´åºŠç›¸å…³AIåŠ©ç†çš„æ½œåŠ›ï¼Œæä¾›ç»“æ„åŒ–CXRè§£è¯»å’Œå¯¹è¯èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´æœ‰æ•ˆå’Œå¯è®¿é—®çš„è¯Šæ–­å·¥ä½œæµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.03333v2">PDF</a> 21 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RadVLMæ¨¡å‹åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åˆ›æ–°åº”ç”¨ã€‚é’ˆå¯¹æ”¾å°„ç§‘åŒ»ç”ŸçŸ­ç¼ºå’Œèƒ¸Xå…‰ï¼ˆCXRï¼‰å¹¿æ³›åº”ç”¨çš„é—®é¢˜ï¼Œè¯¥æ¨¡å‹å±•ç¤ºäº†å¼ºå¤§çš„å¤šä»»åŠ¡å¯¹è¯èƒ½åŠ›ï¼ŒåŒ…æ‹¬æŠ¥å‘Šç”Ÿæˆã€å¼‚å¸¸æ£€æµ‹ç­‰ã€‚é€šè¿‡å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†çš„è®­ç»ƒï¼ŒRadVLMåœ¨å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è¯èƒ½åŠ›å’Œè§†è§‰å®šä½æ–¹é¢ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨æœ‰é™æ ‡æ³¨æ•°æ®åœºæ™¯ä¸‹ä¹Ÿå±•ç°å‡ºä¼˜åŠ¿ï¼Œæœ‰æœ›æˆä¸ºä¸´åºŠç›¸å…³çš„AIåŠ©æ‰‹ï¼Œä¸ºè¯Šæ–­æµç¨‹æä¾›ç»“æ„åŒ–è§£è¯»å’Œå¯¹è¯æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RadVLMæ˜¯ä¸€ä¸ªé’ˆå¯¹CXRè§£è¯»çš„å¤šä»»åŠ¡å¯¹è¯åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ç»è¿‡å¤§è§„æ¨¡æŒ‡ä»¤æ•°æ®é›†è®­ç»ƒï¼ŒåŒ…å«å•ä»»åŠ¡å’Œå¤šä»»åŠ¡å¯¹è¯äº¤äº’ã€‚</li>
<li>RadVLMåœ¨å¯¹è¯èƒ½åŠ›å’Œè§†è§‰å®šä½æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä»»åŠ¡è”åˆè®­ç»ƒæ–¹é¢å±•ç°å‡ºä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>RadVLMæœ‰æœ›æˆä¸ºä¸´åºŠç›¸å…³çš„AIåŠ©æ‰‹ï¼Œæ”¯æŒæ›´æœ‰æ•ˆå’Œå¯è®¿é—®çš„è¯Šæ–­æµç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.03333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cde84d74faa16925b63baab005e3d20d" align="middle">
<img src="https://picx.zhimg.com/v2-48946c34802168fd687642b3a9b8b1d2" align="middle">
<img src="https://picx.zhimg.com/v2-797dfd62cc84ede80267babb794951c6" align="middle">
<img src="https://picx.zhimg.com/v2-c7857b19a1c2983d9ddc63a5b2545478" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TABSurfer-a-Hybrid-Deep-Learning-Architecture-for-Subcortical-Segmentation"><a href="#TABSurfer-a-Hybrid-Deep-Learning-Architecture-for-Subcortical-Segmentation" class="headerlink" title="TABSurfer: a Hybrid Deep Learning Architecture for Subcortical   Segmentation"></a>TABSurfer: a Hybrid Deep Learning Architecture for Subcortical   Segmentation</h2><p><strong>Authors:Aaron Cao, Vishwanatha M. Rao, Kejia Liu, Xinrui Liu, Andrew F. Laine, Jia Guo</strong></p>
<p>Subcortical segmentation remains challenging despite its important applications in quantitative structural analysis of brain MRI scans. The most accurate method, manual segmentation, is highly labor intensive, so automated tools like FreeSurfer have been adopted to handle this task. However, these traditional pipelines are slow and inefficient for processing large datasets. In this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformer hybrid deep learning model designed for superior subcortical segmentation compared to existing state-of-the-art tools. To evaluate, we first demonstrate TABSurferâ€™s consistent performance across various T1w MRI datasets with significantly shorter processing times compared to FreeSurfer. Then, we validate against manual segmentations, where TABSurfer outperforms FreeSurfer based on the manual ground truth. In each test, we also establish TABSurferâ€™s advantage over a leading deep learning benchmark, FastSurferVINN. Together, these studies highlight TABSurferâ€™s utility as a powerful tool for fully automated subcortical segmentation with high fidelity. </p>
<blockquote>
<p>å°½ç®¡åœ¨å¤§è„‘MRIæ‰«æçš„å®šé‡ç»“æ„åˆ†æä¸­ï¼Œäºšçš®å±‚åˆ†å‰²å…·æœ‰é‡è¦çš„åº”ç”¨ï¼Œä½†å…¶ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ€å‡†ç¡®çš„æ–¹æ³•æ˜¯æ‰‹åŠ¨åˆ†å‰²ï¼Œä½†è¿™ç§æ–¹æ³•éå¸¸è€—æ—¶ï¼Œå› æ­¤å·²ç»é‡‡ç”¨äº†å¦‚FreeSurferä¹‹ç±»çš„è‡ªåŠ¨åŒ–å·¥å…·æ¥å¤„ç†è¿™é¡¹ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›ä¼ ç»Ÿç®¡é“åœ¨å¤„ç†å¤§æ•°æ®é›†æ—¶é€Ÿåº¦è¾ƒæ…¢ä¸”æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TABSurferï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸‰ç»´è¡¥ä¸åŸºCNN-Transformeræ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨ä»¥å‡ºè‰²çš„è¡¨ç°è¿›è¡Œäºšçš®å±‚åˆ†å‰²ï¼Œä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›å·¥å…·ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†TABSurferåœ¨å„ç§T1w MRIæ•°æ®é›†ä¸Šçš„ä¸€è‡´æ€§èƒ½ï¼Œå…¶å¤„ç†æ—¶é—´æ˜¾è‘—çŸ­äºFreeSurferã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸æ‰‹åŠ¨åˆ†å‰²è¿›è¡ŒéªŒè¯ï¼ŒTABSurferåœ¨åŸºäºæ‰‹åŠ¨çœŸå®å€¼çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºFreeSurferã€‚åœ¨æ¯æ¬¡æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†TABSurferç›¸å¯¹äºé¢†å…ˆçš„æ·±åº¦å­¦ä¹ åŸºå‡†æµ‹è¯•FastSurferVINNçš„ä¼˜åŠ¿ã€‚è¿™äº›ç ”ç©¶å…±åŒå‡¸æ˜¾äº†TABSurferä½œä¸ºå¼ºå¤§å·¥å…·çš„é‡è¦æ€§ï¼Œå¯ä½œä¸ºå…¨è‡ªåŠ¨é«˜ä¿çœŸäºšçš®å±‚åˆ†å‰²çš„æœ‰åŠ›å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.08267v2">PDF</a> 5 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŸºäºæ·±åº¦å­¦ä¹ çš„å­çš®å±‚åˆ†å‰²æ–¹æ³•TABSurferï¼Œä¸ä¼ ç»Ÿçš„è‡ªåŠ¨åŒ–å·¥å…·FreeSurferç›¸æ¯”ï¼ŒTABSurferåœ¨æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶æ•ˆç‡æ›´é«˜ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTABSurferåœ¨å¤šç§T1w MRIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸€è‡´ï¼Œä¸”å¤„ç†æ—¶é—´æ˜¾è‘—ç¼©çŸ­ã€‚ç›¸è¾ƒäºæ‰‹åŠ¨åˆ†å‰²ï¼ŒTABSurferåŒæ ·å±•ç°äº†ä¼˜è¶Šæ€§ã€‚æ€»çš„æ¥è¯´ï¼ŒTABSurferæ˜¯ä¸€ç§é«˜æ•ˆã€å‡†ç¡®çš„è‡ªåŠ¨åŒ–å­çš®å±‚åˆ†å‰²å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TABSurferæ˜¯ä¸€ç§æ–°å‹çš„3Dè¡¥ä¸å‹CNN-Transformeræ··åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºå­çš®å±‚åˆ†å‰²ã€‚</li>
<li>ä¸ç°æœ‰å…ˆè¿›å·¥å…·ç›¸æ¯”ï¼ŒTABSurferåœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶æ›´é«˜æ•ˆã€‚</li>
<li>TABSurferåœ¨å„ç§T1w MRIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸€è‡´ã€‚</li>
<li>ä¸FreeSurferç›¸æ¯”ï¼ŒTABSurferå¤„ç†æ—¶é—´æ›´çŸ­ã€‚</li>
<li>ç›¸è¾ƒäºæ‰‹åŠ¨åˆ†å‰²ï¼ŒTABSurferå±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>TABSurferç›¸è¾ƒäºåŸºå‡†æ·±åº¦å­¦ä¹ æ¨¡å‹FastSurferVINNæœ‰æ‰€ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.08267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51ae90ea6b14d4b58d3055d56de0b6fb" align="middle">
<img src="https://picx.zhimg.com/v2-874deb8a6a721e70825db8ebfb0972d2" align="middle">
<img src="https://picx.zhimg.com/v2-0f800dc6341ca4d2d4bb2dcce086a3c7" align="middle">
<img src="https://picx.zhimg.com/v2-6770c0103830f29dc338ce94814d4f4f" align="middle">
<img src="https://picx.zhimg.com/v2-dbefe4e5b77d6af2afbffa81c7d5a0fe" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-20/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-21/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2562c828eaffb0e0e3a13596ae7a73e4" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-21  PokeeResearch Effective Deep Research via Reinforcement Learning from   AI Feedback and Robust Reasoning Scaffold
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-20/Speech/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ee1750007411d3b30109bebebec20175" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-20  Seeing Hate Differently Hate Subspace Modeling for Culture-Aware Hate   Speech Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
