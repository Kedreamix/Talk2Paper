<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  Quantization Meets dLLMs A Systematic Study of Post-training   Quantization for Diffusion LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-063dd8fa5c39abd6d572370d41d89a25.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-22-æ›´æ–°"><a href="#2025-08-22-æ›´æ–°" class="headerlink" title="2025-08-22 æ›´æ–°"></a>2025-08-22 æ›´æ–°</h1><h2 id="Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs"><a href="#Quantization-Meets-dLLMs-A-Systematic-Study-of-Post-training-Quantization-for-Diffusion-LLMs" class="headerlink" title="Quantization Meets dLLMs: A Systematic Study of Post-training   Quantization for Diffusion LLMs"></a>Quantization Meets dLLMs: A Systematic Study of Post-training   Quantization for Diffusion LLMs</h2><p><strong>Authors:Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun</strong></p>
<p>Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community. </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰çš„è¿›å±•ä¸ºè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„è‡ªå›å½’ï¼ˆARï¼‰LLMsçš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ©ç”¨å…¨å…³æ³¨åŠ›å’ŒåŸºäºå»å™ªçš„è§£ç ç­–ç•¥ã€‚ç„¶è€Œï¼Œç”±äºè¿™äº›æ¨¡å‹çš„å‚æ•°è§„æ¨¡åºå¤§ä¸”èµ„æºéœ€æ±‚é«˜ï¼Œå› æ­¤åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å®ƒä»¬ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰å·²ç»æˆä¸ºå‹ç¼©AR LLMsçš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ï¼Œä½†å…¶å¯¹dLLMsçš„é€‚ç”¨æ€§ä»å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹çš„é‡åŒ–è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆç¡®å®šäº†æ¿€æ´»å¼‚å¸¸å€¼çš„å­˜åœ¨ï¼Œè¿™äº›å¼‚å¸¸å€¼å…·æœ‰å¼‚å¸¸å¤§çš„æ¿€æ´»å€¼å¹¶ä¸»å¯¼åŠ¨æ€èŒƒå›´ã€‚è¿™äº›å¼‚å¸¸å€¼å¯¹ä½æ¯”ç‰¹é‡åŒ–æ„æˆäº†å…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¾ˆéš¾ä¿æŒå¤§å¤šæ•°å€¼çš„ç²¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å®ç°äº†æœ€å…ˆè¿›çš„PTQæ–¹æ³•ï¼Œå¹¶å¯¹å¤šç§ä»»åŠ¡ç±»å‹å’Œæ¨¡å‹å˜ä½“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„åˆ†ææ²¿ç€å››ä¸ªå…³é”®ç»´åº¦å±•å¼€ï¼šä½å®½ã€é‡åŒ–æ–¹æ³•ã€ä»»åŠ¡ç±»åˆ«å’Œæ¨¡å‹ç±»å‹ã€‚é€šè¿‡å¤šè§’åº¦è¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºä¸åŒé…ç½®ä¸‹dLLMé‡åŒ–è¡Œä¸ºçš„å®ç”¨è§è§£ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ç»“æœèƒ½ä¸ºæœªæ¥åœ¨æœ‰æ•ˆéƒ¨ç½²dLLMæ–¹é¢çš„ç›¸å…³ç ”ç©¶å¥ å®šåŸºç¡€ã€‚æ‰€æœ‰ä»£ç å’Œå®éªŒè®¾ç½®éƒ½å°†å‘å¸ƒï¼Œä»¥æ”¯æŒç ”ç©¶ç¤¾åŒºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14896v1">PDF</a> Technical Report, Work in Progress</p>
<p><strong>Summary</strong><br>     æœ€è¿‘æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰çš„è¿›å±•ä¸ºè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡æä¾›äº†æœ‰å‰é€”çš„è‡ªå›å½’ï¼ˆARï¼‰LLMsæ›¿ä»£æ–¹æ¡ˆï¼Œåˆ©ç”¨å…¨æ³¨æ„åŠ›æœºåˆ¶å’ŒåŸºäºå»å™ªçš„è§£ç ç­–ç•¥ã€‚ç„¶è€Œï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²è¿™äº›æ¨¡å‹ä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå…¶å‚æ•°è§„æ¨¡åºå¤§ä¸”èµ„æºéœ€æ±‚é«˜ã€‚å°½ç®¡å·²æœ‰å…³äºå‹ç¼©è‡ªå›å½’LLMsçš„æ¨¡å‹åé‡åŒ–ï¼ˆPTQï¼‰æŠ€æœ¯è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†å…¶å¯¹dLLMsçš„åº”ç”¨ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†æ‰©æ•£å‹è¯­è¨€æ¨¡å‹çš„é‡åŒ–é—®é¢˜ã€‚é¦–å…ˆè¯†åˆ«äº†æ¿€æ´»å¼‚å¸¸å€¼çš„å­˜åœ¨ï¼Œè¿™äº›å¼‚å¸¸å€¼è¡¨ç°ä¸ºä¸»å¯¼åŠ¨æ€èŒƒå›´çš„å¼‚å¸¸å¤§çš„æ¿€æ´»å€¼ã€‚è¿™äº›å¼‚å¸¸å€¼å¯¹ä½æ¯”ç‰¹é‡åŒ–æ„æˆäº†å…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¾ˆéš¾ä¿æŒå¤§å¤šæ•°å€¼çš„ç²¾åº¦ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å®æ–½äº†æœ€å…ˆè¿›çš„PTQæ–¹æ³•ï¼Œå¹¶å¯¹å¤šç§ä»»åŠ¡ç±»å‹å’Œæ¨¡å‹å˜ä½“è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬çš„åˆ†æä»å››ä¸ªå…³é”®ç»´åº¦å±•å¼€ï¼šä½å®½ã€é‡åŒ–æ–¹æ³•ã€ä»»åŠ¡ç±»åˆ«å’Œæ¨¡å‹ç±»å‹ã€‚é€šè¿‡å¤šè§’åº¦è¯„ä¼°ï¼Œæˆ‘ä»¬æä¾›äº†åœ¨ä¸åŒé…ç½®ä¸‹dLLMé‡åŒ–çš„å®ç”¨è§è§£ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ç»“æœèƒ½ä¸ºæœªæ¥é«˜æ•ˆdLLMéƒ¨ç½²çš„ç ”ç©¶æä¾›åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>dLLMsä½œä¸ºä¸€ç§è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡çš„æ›¿ä»£æ–¹æ¡ˆå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>æ¿€æ´»å¼‚å¸¸å€¼åœ¨dLLMsä¸­çš„å­˜åœ¨æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå¯¹ä½æ¯”ç‰¹é‡åŒ–æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>å¯¹ä¸åŒä½å®½ã€é‡åŒ–æ–¹æ³•ã€ä»»åŠ¡ç±»åˆ«å’Œæ¨¡å‹ç±»å‹çš„dLLMsè¿›è¡Œäº†å…¨é¢çš„é‡åŒ–è¯„ä¼°ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†æ‰©æ•£å‹è¯­è¨€æ¨¡å‹çš„é‡åŒ–é—®é¢˜ï¼Œå¹¶æä¾›å®ç”¨è§è§£ã€‚</li>
<li>é€šè¿‡å¤šè§’åº¦è¯„ä¼°ï¼Œä¸ºé«˜æ•ˆdLLMéƒ¨ç½²çš„æœªæ¥ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
<li>å…¬å¼€æ‰€æœ‰ä»£ç å’Œå®éªŒè®¾ç½®ä»¥æ”¯æŒç ”ç©¶ç¤¾åŒºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-523b4c4daced49ce9f4d0846d1ed4764.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa03f777b006e9d85639405f5dfea5de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-160b5a66b3461899174f36defbd33853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-455de72a791f3c95dba79fe2feb4d153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d25246c90219c4be021f9e83ce24c3b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework"><a href="#MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework" class="headerlink" title="MedReseacher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework"></a>MedReseacher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework</h2><p><strong>Authors:Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</strong></p>
<p>Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts.We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains. </p>
<blockquote>
<p>è¿‘æœŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½“ç°åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿåœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢å’Œç»¼åˆä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡é€šç”¨æ·±åº¦ç ”ç©¶ä»£ç†äººåœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸçš„æŒ‘æˆ˜é¢å‰ï¼Œå®ƒä»¬é‡åˆ°äº†å¾ˆå¤§çš„å›°éš¾ï¼Œé¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿè™½ç„¶åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¸€å®šçš„å‡†ç¡®æ€§ï¼Œä½†ä»æœ‰å¾ˆå¤§çš„å±€é™æ€§ã€‚ä¸»è¦é™åˆ¶å› ç´ æœ‰ï¼šï¼ˆ1ï¼‰æ¨¡å‹ç¼ºä¹ç”¨äºä¸´åºŠæ¨ç†çš„å……è¶³å¯†é›†åŒ»å­¦çŸ¥è¯†ï¼›ï¼ˆ2ï¼‰æ¡†æ¶å—åˆ°ç¼ºä¹é’ˆå¯¹åŒ»å­¦ä¸Šä¸‹æ–‡é‡èº«å®šåˆ¶çš„ä¸“ç”¨æ£€ç´¢å·¥å…·çš„åˆ¶çº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ»å­¦æ·±åº¦ç ”ç©¶ä»£ç†äººï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨åŒ»å­¦çŸ¥è¯†å›¾è°±å¼€å‘äº†ä¸€ç§æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œä»å›´ç»•ç½•è§åŒ»å­¦å®ä½“çš„å­å›¾ä¸­æå–æœ€é•¿çš„é“¾æ¥ç”Ÿæˆå¤æ‚çš„å¤šè·³é—®ç­”å¯¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†è‡ªå®šä¹‰çš„ç§äººåŒ»å­¦æ£€ç´¢å¼•æ“ä¸é€šç”¨å·¥å…·ç›¸ç»“åˆï¼Œå®ç°å‡†ç¡®åŒ»å­¦ä¿¡æ¯çš„ç»¼åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨12ä¸ªåŒ»å­¦ä¸“ä¸šé¢†åŸŸç”Ÿæˆäº†2100å¤šä¸ªä¸åŒçš„è½¨è¿¹ï¼Œæ¯ä¸ªè½¨è¿¹å¹³å‡äº¤äº’4.2ä¸ªå·¥å…·ã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä»¥åŠå¤åˆå¥–åŠ±ï¼Œæˆ‘ä»¬çš„MedResearcher-R1-32Bæ¨¡å‹åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œåœ¨æ¶æ„ã€å·¥å…·è®¾è®¡å’Œè®­ç»ƒæ•°æ®æ„å»ºæ–¹é¢é‡‡ç”¨æœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸç‰¹å®šåˆ›æ–°ï¼Œå¯ä»¥ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨ä¸“ç”¨é¢†åŸŸè¶…è¶Šæ›´å¤§çš„ä¸“æœ‰ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14880v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°å‘å±•çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ç³»ç»Ÿåœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä¾‹å¦‚åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­ï¼Œå…¶åœ¨å¤æ‚ä¿¡æ¯æ£€ç´¢å’Œåˆæˆä»»åŠ¡ä¸Šçš„è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚å°½ç®¡é€šç”¨æ·±åº¦ç ”ç©¶ä»£ç†å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨åŒ»å­¦é¢†åŸŸé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿåœ¨è¿™æ–¹é¢å®ç°çš„å‡†ç¡®ç‡æœ‰é™ã€‚ä¸»è¦å±€é™åœ¨äºæ¨¡å‹ç¼ºä¹ç”¨äºä¸´åºŠæ¨ç†çš„å¯†é›†åŒ»å­¦çŸ¥è¯†ï¼Œä»¥åŠæ¡†æ¶å—é™äºç¼ºä¹é’ˆå¯¹åŒ»å­¦ç¯å¢ƒçš„ä¸“é—¨æ£€ç´¢å·¥å…·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ»å­¦æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œä½¿ç”¨åŒ»å­¦çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤æ‚çš„å¤šè·³é—®ç­”å¯¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é›†æˆäº†å®šåˆ¶çš„ç§äººåŒ»å­¦æ£€ç´¢å¼•æ“å’Œé€šç”¨å·¥å…·ï¼Œä»¥å®ç°å‡†ç¡®çš„ä¿¡æ¯åˆæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†è¶…è¿‡ä¸¤åƒä¸€ç™¾ä¸ªè·¨è¶ŠåäºŒç§åŒ»å­¦ä¸“ä¸šçš„ä¸åŒè½¨è¿¹ï¼Œæ¯ä¸ªè½¨è¿¹å¹³å‡ä½¿ç”¨å·¥å…·äº¤äº’å››æ¬¡ä»¥ä¸Šã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ å¤åˆå¥–åŠ±çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œæˆ‘ä»¬çš„MedResearcher-R1-32Bæ¨¡å‹åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šä¿æŒäº†ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œåœ¨æ¶æ„ã€å·¥å…·è®¾è®¡å’Œè®­ç»ƒæ•°æ®æ„å»ºæ–¹é¢æœ‰é’ˆå¯¹æ€§çš„é¢†åŸŸç‰¹å®šåˆ›æ–°å¯ä»¥ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨ä¸“é—¨é¢†åŸŸä¸­è¶…è¶Šæ›´å¤§çš„ä¸“æœ‰ç³»ç»Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å¼ºå¤§èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸­ã€‚</li>
<li>é€šç”¨æ·±åº¦ç ”ç©¶ä»£ç†åœ¨åŒ»å­¦é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºåŒ»å­¦çŸ¥è¯†çš„å¤æ‚æ€§å’Œç‰¹å®šè¯­å¢ƒçš„éœ€æ±‚ã€‚</li>
<li>åŒ»å­¦æ·±åº¦ç ”ç©¶ä»£ç†çš„æ ¸å¿ƒæŒ‘æˆ˜åŒ…æ‹¬ç¼ºä¹ä¸´åºŠæ¨ç†çš„å¯†é›†åŒ»å­¦çŸ¥è¯†å’Œç¼ºä¹ä¸“é—¨ç”¨äºåŒ»å­¦ç¯å¢ƒçš„æ£€ç´¢å·¥å…·ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„åŒ»å­¦æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œé€šè¿‡æ•°æ®åˆæˆæ¡†æ¶å’Œå®šåˆ¶æ£€ç´¢å¼•æ“çš„ç»“åˆæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ–¹æ³•ç”Ÿæˆäº†è·¨è¶Šå¤šä¸ªåŒ»å­¦ä¸“ä¸šçš„å¤šæ ·åŒ–è½¨è¿¹ï¼Œå¹¶å®ç°äº†åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ç»“åˆç›‘ç£å¾®è°ƒå’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-145333c4ecb400359c97d17f875ee0b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1601c553312993feb55ef505b074d85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e93679d38feb0e25a65e0ea77deb8081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cff4303881eefc0dbb6dd240bfb6871.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent"><a href="#Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent" class="headerlink" title="Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent"></a>Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent</h2><p><strong>Authors:Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu</strong></p>
<p>As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œç¡®ä¿å…¶é²æ£’æ€§å’Œå®‰å…¨å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰å¯¹é½æŠ€æœ¯åœ¨å…¸å‹æç¤ºä¸Šçš„æ€»ä½“æˆåŠŸï¼ŒLLMä»å®¹æ˜“å—åˆ°é€šè¿‡ç”¨æˆ·æç¤ºé™„åŠ çš„ç²¾å¿ƒåˆ¶ä½œçš„å¯¹æŠ—è§¦å‘è€Œå¯åŠ¨çš„è¶Šç‹±æ”»å‡»ã€‚å¤§å¤šæ•°ç°æœ‰çš„è¶Šç‹±æ–¹æ³•è¦ä¹ˆä¾èµ–äºç¦»æ•£æ ‡è®°ç©ºé—´ä¸Šçš„ä½æ•ˆæœç´¢ï¼Œè¦ä¹ˆç›´æ¥ä¼˜åŒ–è¿ç»­åµŒå…¥ã€‚è™½ç„¶å¯ä»¥å°†è¿ç»­åµŒå…¥ç›´æ¥ä½œä¸ºé€‰å®šå¼€æºæ¨¡å‹çš„è¾“å…¥ï¼Œä½†å¯¹äºä¸“æœ‰æ¨¡å‹æ¥è¯´ï¼Œè¿™æ ·åšå¹¶ä¸å¯è¡Œã€‚å¦ä¸€æ–¹é¢ï¼Œå°†è¿™äº›åµŒå…¥æŠ•å½±å›æœ‰æ•ˆçš„ç¦»æ•£æ ‡è®°åˆå¼•å…¥äº†é¢å¤–çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”é€šå¸¸ä¼šé™ä½æ”»å‡»æ•ˆæœã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…åœ¨çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™æ³•ç»“åˆBregmanæŠ•å½±ä¼˜åŒ–å¯¹æŠ—æ€§åç¼€æ ‡è®°çš„æ¾å¼›ç‹¬çƒ­ç¼–ç ï¼Œç¡®ä¿æ¯ä¸ªæ ‡è®°çš„ä¼˜åŒ–ç‹¬çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡å•çº¯å½¢å†…ã€‚æˆ‘ä»¬ä¸ºæ‰€æå‡ºçš„æ–¹æ³•æä¾›äº†æ”¶æ•›æ€§çš„ç†è®ºè¯æ˜ï¼Œå¹¶å®ç°äº†ä¸€ç§æœ‰æ•ˆçš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°çªç ´å‡ ä¸ªå¹¿æ³›ä½¿ç”¨çš„LLMã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¸‰ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼ŒæˆåŠŸç‡æ›´é«˜ï¼Œæ”¶æ•›æ›´å¿«ï¼Œå¹¶åœ¨äº”ä¸ªå¼€æºLLMå’Œå››ä¸ªä¸“ä¸ºè¯„ä¼°è¶Šç‹±æ–¹æ³•ç­–åˆ’çš„å¯¹æŠ—æ€§è¡Œä¸ºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚é™¤äº†å•ä¸ªæç¤ºæ”»å‡»å¤–ï¼Œæˆ‘ä»¬è¿˜ç”Ÿæˆäº†è·¨å¤šä¸ªæç¤ºæœ‰æ•ˆçš„é€šç”¨å¯¹æŠ—æ€§åç¼€ï¼Œå¹¶è¯æ˜äº†ä¼˜åŒ–åç¼€åœ¨ä¸åŒLLMä¹‹é—´çš„å¯è½¬ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²æ—¥ç›Šå¢å¤šï¼Œä½†å…¶ç¨³å¥æ€§å’Œå®‰å…¨å¯¹é½æ€§ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰å¯¹é½æŠ€æœ¯åœ¨å…¸å‹æç¤ºä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†LLMä»å®¹æ˜“å—åˆ°ç›‘ç‹±çªç ´æ”»å‡»ï¼Œè¿™äº›æ”»å‡»é€šè¿‡æ·»åŠ ç²¾å¿ƒåˆ¶ä½œçš„å¯¹æŠ—è§¦å‘å™¨æ¥è§¦å‘ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šåœ¨ç¦»æ•£ä»¤ç‰Œç©ºé—´è¿›è¡Œä½æ•ˆæœç´¢æˆ–ç›´æ¥ä¼˜åŒ–è¿ç»­åµŒå…¥ã€‚è™½ç„¶è¿ç»­åµŒå…¥å¯ä»¥ç›´æ¥ä½œä¸ºé€‰å®šå¼€æºæ¨¡å‹çš„è¾“å…¥ï¼Œä½†å¯¹äºä¸“æœ‰æ¨¡å‹å´ä¸å¯è¡Œã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å†…åœ¨çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–å¯¹æŠ—åç¼€ä»¤ç‰Œçš„æ¾å¼›ç‹¬çƒ­ç¼–ç ï¼Œå¹¶é€šè¿‡BregmanæŠ•å½±ç¡®ä¿æ¯ä¸ªä»¤ç‰Œçš„ä¼˜åŒ–ç‹¬çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡ç®€å•å½¢å†…ã€‚è¯¥æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œç›¸è¾ƒäºä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå¹¶åœ¨äº”ä¸ªå¼€æºLLMå’Œå››ä¸ªç”¨äºè¯„ä¼°ç›‘ç‹±çªç ´æ–¹æ³•çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æœ‰æ•ˆè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹ä¸ªåˆ«æç¤ºæ”»å‡»è¿›è¡Œäº†é€šç”¨å¯¹æŠ—åç¼€çš„ç”Ÿæˆï¼Œå¹¶è¯æ˜äº†ä¼˜åŒ–åç¼€åœ¨ä¸åŒLLMä¹‹é—´çš„å¯è½¬ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²é¢ä¸´ç¨³å¥æ€§å’Œå®‰å…¨å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>LLMå®¹æ˜“å—åˆ°é€šè¿‡æ·»åŠ å¯¹æŠ—è§¦å‘å™¨è¿›è¡Œçš„ç›‘ç‹±çªç ´æ”»å‡»ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šåœ¨ç¦»æ•£ä»¤ç‰Œç©ºé—´è¿›è¡Œä½æ•ˆæœç´¢æˆ–ç›´æ¥ä¼˜åŒ–è¿ç»­åµŒå…¥ï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„å†…åœ¨ä¼˜åŒ–æ–¹æ³•ç›´æ¥ä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–å¯¹æŠ—åç¼€ä»¤ç‰Œçš„æ¾å¼›ç‹¬çƒ­ç¼–ç ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡BregmanæŠ•å½±ç¡®ä¿ä¼˜åŒ–åçš„ç‹¬çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡ç®€å•å½¢å†…ã€‚</li>
<li>æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03ece2ac5ecc22a45f090986029199a1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs"><a href="#Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs" class="headerlink" title="Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs"></a>Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs</h2><p><strong>Authors:Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar</strong></p>
<p>Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art modelsâ€™ extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the modelsâ€™ full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰å†…å®¹å†—é•¿ã€ç¹æ‚ï¼Œå¯¹äºéœ€è¦æŸ¥é˜…è¿™äº›è®°å½•çš„åŒ»æŠ¤äººå‘˜æ¥è¯´æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§£å†³è¿™ä¸€éš¾é¢˜æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–å¹¶è¿›è¡Œåˆ†æã€‚ç„¶è€Œï¼Œä¸´åºŠç¬”è®°çš„é•¿åº¦ç”šè‡³è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é•¿åº¦ã€‚é€šè¿‡ä»æ•´ä¸ªç”µå­å¥åº·è®°å½•ä¸­æ£€ç´¢ä¸ä»»åŠ¡ç›¸å…³çš„æ®µè½ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œå¯èƒ½ä¼šå‡å°‘æ‰€éœ€è¾“å…¥æ ‡è®°çš„æ•°é‡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªå¯åœ¨å¥åº·ç³»ç»Ÿä¸­è¿›è¡Œå¤åˆ¶çš„ä¸´åºŠä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡æ‰€éœ€åŠªåŠ›æå°ï¼š1ï¼‰æå–æˆåƒç¨‹åºï¼›2ï¼‰ç”ŸæˆæŠ—ç”Ÿç´ ä½¿ç”¨çš„æ—¶é—´çº¿ï¼›3ï¼‰è¯†åˆ«å…³é”®è¯Šæ–­ã€‚æˆ‘ä»¬ä½¿ç”¨å®é™…ä½é™¢æ‚£è€…çš„ç”µå­å¥åº·è®°å½•è¿›è¡Œæµ‹è¯•ï¼Œæµ‹è¯•äº†ä¸‰ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æä¾›çš„ä¸Šä¸‹æ–‡å„ä¸ç›¸åŒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æœ‰é’ˆå¯¹æ€§çš„æ–‡æœ¬æ£€ç´¢æˆ–æœ€æ–°çš„ä¸´åºŠç¬”è®°ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ³•çš„æ•ˆæœæ¥è¿‘æˆ–è¶…è¿‡äº†ä½¿ç”¨æœ€æ–°ç¬”è®°çš„æ•ˆæœï¼Œå¹¶æ¥è¿‘ä½¿ç”¨æ¨¡å‹å…¨è¯­å¢ƒæ—¶çš„è¡¨ç°ï¼ŒåŒæ—¶æ‰€éœ€çš„è¾“å…¥ä»¤ç‰Œå¤§å¤§å‡å°‘ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ–°çš„æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¶Šæ¥è¶Šé•¿çš„æ–‡æœ¬é‡æ—¶ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆæ³•ä»ç„¶æ˜¯ä¸€ä¸ªæœ‰ç«äº‰åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç”µå­ç—…å†ï¼ˆEHRsï¼‰é•¿ä¸”å¤æ‚ï¼Œå¸¸å«æœ‰å¤§é‡å†—ä½™ä¿¡æ¯ï¼Œå¯¹åŒ»ç”Ÿæ¥è¯´æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†å¯èƒ½æ–¹æ¡ˆï¼Œä½†ä¸´åºŠç¬”è®°çš„é•¿åº¦å¸¸è¶…å‡ºæ¨¡å‹çš„è¯­å¢ƒå¤„ç†èŒƒå›´ã€‚æœ¬ç ”ç©¶æå‡ºé€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œä»æ•´ä¸ªç”µå­ç—…å†ä¸­æ£€ç´¢ä»»åŠ¡ç›¸å…³æ®µè½ï¼Œå¯èƒ½å‡å°‘æ‰€éœ€è¾“å…¥æ ‡è®°çš„æ•°é‡ã€‚æœ¬ç ”ç©¶è®¾è®¡äº†ä¸‰é¡¹å¯åœ¨ä¸åŒåŒ»ç–—ç³»ç»Ÿä¸­å¤åˆ¶çš„ä»»åŠ¡ï¼šæå–æˆåƒç¨‹åºã€ç”ŸæˆæŠ—ç”Ÿç´ ä½¿ç”¨çš„æ—¶é—´çº¿ä»¥åŠè¯†åˆ«å…³é”®è¯Šæ–­ã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸‰ç§æœ€æ–°LLMæ¨¡å‹å¤„ç†å®é™…ä½é™¢æ‚£è€…ç”µå­ç—…å†çš„èƒ½åŠ›ï¼Œä½¿ç”¨ç›®æ ‡æ–‡æœ¬æ£€ç´¢æˆ–æœ€æ–°ä¸´åºŠç¬”è®°ä½œä¸ºä¸Šä¸‹æ–‡ã€‚ç ”ç©¶å‘ç°ï¼ŒRAGçš„æ€§èƒ½æ¥è¿‘æˆ–è¶…è¿‡ä½¿ç”¨æœ€æ–°ç¬”è®°çš„æ•ˆæœï¼Œä¸”ä½¿ç”¨è¾“å…¥æ ‡è®°çš„æ•°é‡å¤§å¤§å‡å°‘ã€‚ç»“æœæç¤ºï¼Œå³ä½¿åœ¨æ–°æ¨¡å‹å¤„ç†æ–‡æœ¬é•¿åº¦å¢åŠ çš„æƒ…å†µä¸‹ï¼ŒRAGä»ç„¶æ˜¯ä¸€ç§æœ‰ç«äº‰åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­ç—…å†ï¼ˆEHRsï¼‰å­˜åœ¨ä¿¡æ¯å†—é•¿ã€å¤æ‚çš„é—®é¢˜ï¼Œå¯¹åŒ»ç”Ÿé€ æˆæŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³EHRsé—®é¢˜ä¸Šå…·æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´å¤„ç†é•¿æ–‡æœ¬çš„æŒ‘æˆ˜ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡ä»æ•´ä¸ªEHRä¸­æ£€ç´¢ä»»åŠ¡ç›¸å…³æ®µè½ï¼Œå‡å°‘äº†æ‰€éœ€çš„è¾“å…¥æ ‡è®°æ•°é‡ã€‚</li>
<li>æœ¬ç ”ç©¶è®¾è®¡äº†ä¸‰é¡¹ä¸´åºŠä»»åŠ¡ç”¨äºæµ‹è¯•LLMæ¨¡å‹æ€§èƒ½ï¼šæå–æˆåƒç¨‹åºã€ç”ŸæˆæŠ—ç”Ÿç´ ä½¿ç”¨çš„æ—¶é—´çº¿ä»¥åŠè¯†åˆ«å…³é”®è¯Šæ–­ã€‚</li>
<li>RAGæ€§èƒ½æ¥è¿‘ä½¿ç”¨æœ€æ–°ä¸´åºŠç¬”è®°çš„æ•ˆæœï¼ŒåŒæ—¶å‡å°‘è¾“å…¥æ ‡è®°æ•°é‡ã€‚</li>
<li>RAGæ–¹æ³•å³ä½¿åœ¨å¤„ç†æ›´é•¿çš„æ–‡æœ¬æ—¶ä»ç„¶æœ‰æ•ˆä¸”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83dbe07bf1a698277068bd104136a117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bd56f80139dafae901b30bf9e82c84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbafa5f04d5e1123682364af991d1bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d1c0016de24c71db12663581fc3e7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6463600292bf3e6cffe9bab881ae1d63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-184f4ffb59e77c944b686be9be4d311f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e50ac77c394de771017b1d053eec8186.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting"><a href="#TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting" class="headerlink" title="TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting"></a>TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting</h2><p><strong>Authors:Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang</strong></p>
<p>Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BiYunying/TransLLM">https://github.com/BiYunying/TransLLM</a>. </p>
<blockquote>
<p>åŸå¸‚äº¤é€šç³»ç»Ÿé¢ä¸´å¤šç§ä»»åŠ¡ä¸­çš„å¤šæ ·åŒ–æŒ‘æˆ˜ï¼Œå¦‚äº¤é€šé¢„æµ‹ã€ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰å……ç”µéœ€æ±‚é¢„æµ‹å’Œå‡ºç§Ÿè½¦è°ƒåº¦ç­‰ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šå°è§„æ¨¡æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯ç‰¹å®šä»»åŠ¡çš„ï¼Œå¹¶ä¸”ä¾èµ–å¤§é‡æ•°æ®ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤šç§åœºæ™¯ä¸­çš„é€šç”¨æ€§ï¼›è€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°½ç®¡é€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£æä¾›äº†çµæ´»æ€§ï¼Œä½†åœ¨äº¤é€šé¢†åŸŸçš„ç»“æ„åŒ–æ—¶ç©ºæ•°æ®å’Œæ•°å€¼æ¨ç†æ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TransLLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¡†æ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æç¤ºç»„åˆï¼Œå°†æ—¶ç©ºå»ºæ¨¡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆèµ·æ¥ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§çš„æ—¶ç©ºç¼–ç å™¨ï¼Œé€šè¿‡æ‰©å¼ çš„ä¸´æ—¶å·ç§¯å’ŒåŒé‡é‚»æ¥å›¾æ³¨æ„åŠ›ç½‘ç»œæ¥æ•æ‰å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œä¸LLMæ— ç¼æ¥å£é€šè¿‡ç»“æ„åŒ–åµŒå…¥ã€‚ä¸€ç§æ–°çš„åŸºäºå®ä¾‹çº§åˆ«çš„æç¤ºè·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œæ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€ä¸ªæ€§åŒ–æç¤ºï¼Œè¶…è¶Šäº†å›ºå®šçš„ä»»åŠ¡ç‰¹å®šæ¨¡æ¿ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ—¶ç©ºæ¨¡å¼ç¼–ç ä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ŒåŠ¨æ€ç»„åˆä¸ªæ€§åŒ–æç¤ºæ¥æŒ‡å¯¼LLMæ¨ç†ï¼Œå¹¶é€šè¿‡ä¸“ç”¨è¾“å‡ºå±‚æŠ•å½±è¡¨ç¤ºæ¥ç”Ÿæˆç‰¹å®šä»»åŠ¡çš„é¢„æµ‹ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransLLMåœ¨ç›‘ç£å­¦ä¹ å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­çš„å‡ºè‰²æœ‰æ•ˆæ€§ã€‚ä¸åä¸ªåŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å›å½’å’Œè§„åˆ’é—®é¢˜ä¸Šå‡è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–å’Œè·¨ä»»åŠ¡é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BiYunying/TransLLM%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/BiYunying/TransLLMä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹åŸå¸‚äº¤é€šç³»ç»Ÿä¸­çš„å¤šæ ·æŒ‘æˆ˜ï¼Œå¦‚äº¤é€šé¢„æµ‹ã€ç”µåŠ¨æ±½è½¦å……ç”µéœ€æ±‚é¢„æµ‹å’Œå‡ºç§Ÿè½¦è°ƒåº¦ç­‰é—®é¢˜ï¼Œæå‡ºäº†TransLLMæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ—¶ç©ºå»ºæ¨¡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æç¤ºç»„æˆæ¥å®ç°ç»Ÿä¸€çš„ä»»åŠ¡å¤„ç†ã€‚TransLLMæ¡†æ¶å…·æœ‰è½»é‡çº§çš„æ—¶ç©ºç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ—¶ç©ºä¾èµ–å…³ç³»ï¼Œå¹¶ä¸LLMæ— ç¼æ¥å£ã€‚å®éªŒè¡¨æ˜ï¼ŒTransLLMåœ¨ç›‘ç£å­¦ä¹ å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸åŸºå‡†æ¨¡å‹ç›¸æ¯”å…·æœ‰å¼ºå¤§çš„æ³›åŒ–å’Œè·¨ä»»åŠ¡é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransLLMæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¡†æ¶ï¼Œç”¨äºå¤„ç†åŸå¸‚äº¤é€šç³»ç»Ÿä¸­çš„å¤šæ ·æŒ‘æˆ˜ï¼Œå¦‚äº¤é€šé¢„æµ‹ã€ç”µåŠ¨æ±½è½¦å……ç”µéœ€æ±‚é¢„æµ‹å’Œå‡ºç§Ÿè½¦è°ƒåº¦ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†æ—¶ç©ºå»ºæ¨¡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥åº”å¯¹ä¸åŒä»»åŠ¡ã€‚</li>
<li>TransLLMå…·æœ‰è½»é‡çº§çš„æ—¶ç©ºç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„æ—¶ç©ºä¾èµ–å…³ç³»ã€‚</li>
<li>é€šè¿‡å¯å­¦ä¹ çš„æç¤ºç»„æˆï¼ŒTransLLMå¯ä»¥åŠ¨æ€åœ°ä¸ªæ€§åŒ–æç¤ºï¼Œè€Œæ— éœ€å›ºå®šçš„ä»»åŠ¡ç‰¹å®šæ¨¡æ¿ã€‚</li>
<li>TransLLMæ¡†æ¶åœ¨ç›‘ç£å­¦ä¹ å’Œé›¶æ ·æœ¬è®¾ç½®ä¸‹å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ä¸å¤šç§åŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼ŒTransLLMå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½åœ¨ä¸åŒçš„äº¤é€šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>TransLLMæ¡†æ¶çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-685171622748fd32dcbc1648e3fcaeba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-755b2642c499f73f93d942309e2db41a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-319c0e6ddb6cf30c1e7a49f45d8d3713.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1c85d6d70c90a4421fd965078a7a817.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Investigation-of-the-Inter-Rater-Reliability-between-Large-Language-Models-and-Human-Raters-in-Qualitative-Analysis"><a href="#Investigation-of-the-Inter-Rater-Reliability-between-Large-Language-Models-and-Human-Raters-in-Qualitative-Analysis" class="headerlink" title="Investigation of the Inter-Rater Reliability between Large Language   Models and Human Raters in Qualitative Analysis"></a>Investigation of the Inter-Rater Reliability between Large Language   Models and Human Raters in Qualitative Analysis</h2><p><strong>Authors:Nikhil Sanjay Borse, Ravishankar Chatta Subramaniam, N. Sanjay Rebello</strong></p>
<p>Qualitative analysis is typically limited to small datasets because it is time-intensive. Moreover, a second human rater is required to ensure reliable findings. Artificial intelligence tools may replace human raters if we demonstrate high reliability compared to human ratings. We investigated the inter-rater reliability of state-of-the-art Large Language Models (LLMs), ChatGPT-4o and ChatGPT-4.5-preview, in rating audio transcripts coded manually. We explored prompts and hyperparameters to optimize model performance. The participants were 14 undergraduate student groups from a university in the midwestern United States who discussed problem-solving strategies for a project. We prompted an LLM to replicate manual coding, and calculated Cohenâ€™s Kappa for inter-rater reliability. After optimizing model hyperparameters and prompts, the results showed substantial agreement (${\kappa}&gt;0.6$) for three themes and moderate agreement on one. Our findings demonstrate the potential of GPT-4o and GPT-4.5 for efficient, scalable qualitative analysis in physics education and identify their limitations in rating domain-general constructs. </p>
<blockquote>
<p>å®šæ€§åˆ†æé€šå¸¸ä»…é™äºå°æ•°æ®é›†ï¼Œå› ä¸ºå®ƒéå¸¸è€—æ—¶ã€‚æ­¤å¤–ï¼Œéœ€è¦ç¬¬äºŒä½äººç±»è¯„åˆ†è€…ä»¥ç¡®ä¿ç»“æœå¯é ã€‚å¦‚æœæˆ‘ä»¬è¯æ˜ä¸äººå·¥è¯„åˆ†ç›¸æ¯”å…·æœ‰é«˜åº¦çš„å¯é æ€§ï¼Œäººå·¥æ™ºèƒ½å·¥å…·å¯èƒ½ä¼šå–ä»£äººç±»è¯„åˆ†è€…ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ChatGPT-4oå’ŒChatGPT-4.5é¢„è§ˆç‰ˆåœ¨æ‰‹åŠ¨ç¼–ç éŸ³é¢‘è½¬å½•æ—¶çš„è¯„åˆ†å‘˜é—´å¯é æ€§ã€‚æˆ‘ä»¬æ¢ç´¢äº†æç¤ºå’Œè¶…å‚æ•°ä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ã€‚å‚ä¸è€…ä¸ºæ¥è‡ªç¾å›½ä¸­è¥¿éƒ¨ä¸€æ‰€å¤§å­¦çš„14ä¸ªæœ¬ç§‘ç”Ÿå°ç»„ï¼Œä»–ä»¬è®¨è®ºäº†ä¸€ä¸ªé¡¹ç›®çš„è§£å†³é—®é¢˜ç­–ç•¥ã€‚æˆ‘ä»¬æç¤ºLLMå¤åˆ¶æ‰‹åŠ¨ç¼–ç ï¼Œå¹¶è®¡ç®—äº†Cohençš„Kappaå€¼ä»¥è¡¡é‡è¯„åˆ†å‘˜é—´å¯é æ€§ã€‚åœ¨ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°å’Œæç¤ºåï¼Œç»“æœæ˜¾ç¤ºä¸‰ä¸ªä¸»é¢˜ä¸Šæœ‰æ˜¾è‘—çš„å…±è¯†ï¼ˆÎº&gt; 0.6ï¼‰ï¼Œä¸€ä¸ªä¸»é¢˜ä¸Šä¸­åº¦å…±è¯†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜GPT-4oå’ŒGPT-4.5åœ¨ç‰©ç†æ•™è‚²ä¸­çš„é«˜æ•ˆã€å¯æ‰©å±•çš„å®šæ€§åˆ†ææ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†ä»–ä»¬åœ¨è¯„ä»·é¢†åŸŸé€šç”¨ç»“æ„æ–¹é¢çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14764v1">PDF</a> 7 pages, 4 figures, Physics Education Research Conference 2025</p>
<p><strong>æ‘˜è¦</strong><br>åœ¨æ‰‹åŠ¨ç¼–ç éŸ³é¢‘è½¬å½•æ—¶ä½¿ç”¨æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ¢è®¨äº†LLMä¹‹é—´çš„è¯„ä¼°å¯é æ€§ã€‚å‘ç°é€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œæç¤ºå’Œè¶…å‚æ•°ä¼˜åŒ–åï¼Œèƒ½å¤Ÿå¾—å‡ºæ˜¾è‘—çš„åè®®ï¼Œä½†ä»å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚è¿™ä¸ºç‰©ç†æ•™è‚²ä¸­çš„å®šæ€§åˆ†ææä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„æ½œåœ¨å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å®šæ€§åˆ†æå—é™äºå°æ•°æ®é›†ï¼Œå› ä¸ºå®ƒè€—æ—¶ä¸”éœ€è¦ç¬¬äºŒä¸ªäººç±»è¯„ä¼°è€…ä»¥ç¡®ä¿ç»“æœçš„å¯é æ€§ã€‚</li>
<li>äººå·¥æ™ºèƒ½å·¥å…·å¯ä»¥å–ä»£äººç±»è¯„ä¼°è€…ï¼Œå‰ææ˜¯è¯æ˜å…¶ä¸äººç±»è¯„ä¼°å…·æœ‰é«˜åº¦çš„å¯é æ€§ã€‚</li>
<li>å¯¹æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº†è¯„ä¼°è€…é—´å¯é æ€§çš„ç ”ç©¶ï¼Œæ¶‰åŠChatGPT-4oå’ŒChatGPT-4.5é¢„è§ˆç‰ˆã€‚</li>
<li>åœ¨å¯¹éŸ³é¢‘è½¬å½•è¿›è¡Œæ‰‹åŠ¨ç¼–ç æ—¶ï¼Œæ¢è®¨äº†å¦‚ä½•ä¼˜åŒ–æ¨¡å‹çš„æç¤ºå’Œè¶…å‚æ•°ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å®éªŒå¯¹è±¡åŒ…æ‹¬æ¥è‡ªç¾å›½ä¸­è¥¿éƒ¨æŸå¤§å­¦çš„åå››ç»„æœ¬ç§‘ç”Ÿï¼Œä»–ä»¬è®¨è®ºäº†ä¸€ä¸ªå…³äºè§£å†³é¡¹ç›®é—®é¢˜çš„ç­–ç•¥ã€‚</li>
<li>åœ¨æ¨¡å‹æç¤ºå’Œè¶…å‚æ•°ä¼˜åŒ–åï¼Œå¯¹ä¸‰ä¸ªä¸»é¢˜çš„è¯„ä¼°æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¸€è‡´æ€§ï¼ˆÎº&gt; 0.6ï¼‰ï¼Œè€Œå¯¹ä¸€ä¸ªä¸»é¢˜çš„è¯„ä¼°æ˜¾ç¤ºå‡ºä¸­ç­‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14764">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-113988460ff3da4d8ae351655bd40d8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-980f5d9f0987db5202c3382c51f52148.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95bf9af15fc5c88169215f065396b01c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-281a36f716f8fc2db469f729f3ce9fcf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference"><a href="#Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference" class="headerlink" title="Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference"></a>Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference</h2><p><strong>Authors:Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</strong></p>
<p>Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/nli-stress-testing">https://github.com/KurbanIntelligenceLab/nli-stress-testing</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œç„¶è€Œï¼Œå®ƒä»¬åœ¨è·¨è¯­è¨€çš„ä¸€è‡´æ€§å’Œé€»è¾‘åŸºç¡€å¯¹é½æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰çš„æ§åˆ¶è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”ŸæˆåŸºäºé€»è¾‘çš„å‰æå‡è®¾å¯¹ï¼Œå¹¶å°†å…¶ç¿»è¯‘æˆç±»å‹å¤šæ ·çš„è¯­è¨€é›†ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è¯­ä¹‰å…³ç³»ï¼Œå¹¶å…è®¸åœ¨å•è¯­å’Œæ··åˆè¯­è¨€ï¼ˆä»£ç åˆ‡æ¢ï¼‰æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»£ç åˆ‡æ¢å¹¶ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æœ‰æ‰€æé«˜ï¼Œè¿™è¡¨æ˜ç¿»è¯‘å¼•èµ·çš„è¯æ±‡å˜åŒ–å¯èƒ½ä½œä¸ºä¸€ç§æ­£åˆ™åŒ–ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼åº¦åˆ†æå’Œè·¨è¯­è¨€å¯¹é½å¯è§†åŒ–æ¥éªŒè¯è¯­ä¹‰ä¿ç•™ï¼Œè¯å®äº†ç¿»è¯‘å¯¹çš„ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ—¢æš´éœ²äº†å½“å‰LLMè·¨è¯­è¨€æ¨ç†çš„æ½œåŠ›å’Œè„†å¼±æ€§ï¼Œä¹Ÿè¯†åˆ«å‡ºä»£ç åˆ‡æ¢æ˜¯æé«˜å¤šè¯­è¨€ç¨³å¥æ€§çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ æ†ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/nli-stress-testing">https://github.com/KurbanIntelligenceLab/nli-stress-testing</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14735v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶è·¨è¯­è¨€çš„é€»è¾‘ä¸€è‡´æ€§å¯¹é½èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ§åˆ¶è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ï¼Œç”ŸæˆåŸºäºé€»è¾‘çš„åˆæˆå‰æå‡è®¾å¯¹ï¼Œå¹¶å°†å…¶ç¿»è¯‘æˆå¤šç§ç±»å‹è¯­è¨€ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è¯­ä¹‰å…³ç³»ï¼Œå¹¶å…è®¸åœ¨å•è¯­å’Œæ··åˆè¯­è¨€ï¼ˆä»£ç åˆ‡æ¢ï¼‰æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»£ç åˆ‡æ¢å¹¶ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æé«˜æ€§èƒ½ï¼Œè¿™è¡¨æ˜ç¿»è¯‘å¼•èµ·çš„è¯æ±‡å˜åŒ–å¯èƒ½ä½œä¸ºä¸€ç§å¸¸è§„ä¿¡å·ã€‚é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§åˆ†æå’Œè·¨è¯­è¨€å¯¹é½å¯è§†åŒ–éªŒè¯è¯­ä¹‰çš„ä¿ç•™ï¼Œç¡®è®¤äº†ç¿»è¯‘å¯¹çš„ä¿çœŸæ€§ã€‚æœ¬ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰LLMè·¨è¯­è¨€æ¨ç†çš„æ½œåŠ›å’Œè„†å¼±æ€§ï¼Œå¹¶å°†ä»£ç åˆ‡æ¢ä½œä¸ºæé«˜å¤šè¯­è¨€ç¨³å¥æ€§çš„æœ‰åŠ›æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨é€æ¸æ™®åŠï¼Œä½†å…¶åœ¨é€»è¾‘ä¸€è‡´æ€§æ–¹é¢çš„è·¨è¯­è¨€å¯¹é½èƒ½åŠ›å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ§åˆ¶è¯„ä¼°æ¡†æ¶ç”¨äºå¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†ï¼ŒåŒ…æ‹¬ç”Ÿæˆåˆæˆé€»è¾‘å‰æå‡è®¾å¯¹å¹¶å°†å…¶ç¿»è¯‘æˆå¤šç§è¯­è¨€ã€‚</li>
<li>ä»£ç åˆ‡æ¢ï¼ˆå³ä½¿ç”¨å¤šç§è¯­è¨€è¿›è¡Œäº¤æµï¼‰å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æœ‰ç§¯æå½±å“ï¼Œå¹¶ä¸ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç¿»è¯‘å¼•èµ·çš„è¯æ±‡å˜åŒ–å¯èƒ½ä½œä¸ºä¸€ç§å¸¸è§„ä¿¡å·ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åµŒå…¥ç›¸ä¼¼æ€§åˆ†æå’Œè·¨è¯­è¨€å¯¹é½å¯è§†åŒ–éªŒè¯äº†è¯­ä¹‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­çš„ä¿ç•™ã€‚</li>
<li>ç ”ç©¶ç»“æœæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹è·¨è¯­è¨€æ¨ç†çš„æ½œåŠ›å’Œè„†å¼±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6cf81e6d49a7795c75a7b96866051b91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-129375487d873b774c2af356fd64c8a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94d2f70e80d68c07f2ccc991876ce4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2da2bf87a33006695a07d47a252ec39.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multiscale-Video-Transformers-for-Class-Agnostic-Segmentation-in-Autonomous-Driving"><a href="#Multiscale-Video-Transformers-for-Class-Agnostic-Segmentation-in-Autonomous-Driving" class="headerlink" title="Multiscale Video Transformers for Class Agnostic Segmentation in   Autonomous Driving"></a>Multiscale Video Transformers for Class Agnostic Segmentation in   Autonomous Driving</h2><p><strong>Authors:Leila Cheshmi, Mennatullah Siam</strong></p>
<p>Ensuring safety in autonomous driving is a complex challenge requiring handling unknown objects and unforeseen driving scenarios. We develop multiscale video transformers capable of detecting unknown objects using only motion cues. Video semantic and panoptic segmentation often relies on known classes seen during training, overlooking novel categories. Recent visual grounding with large language models is computationally expensive, especially for pixel-level output. We propose an efficient video transformer trained end-to-end for class-agnostic segmentation without optical flow. Our method uses multi-stage multiscale query-memory decoding and a scale-specific random drop-token to ensure efficiency and accuracy, maintaining detailed spatiotemporal features with a shared, learnable memory module. Unlike conventional decoders that compress features, our memory-centric design preserves high-resolution information at multiple scales. We evaluate on DAVISâ€™16, KITTI, and Cityscapes. Our method consistently outperforms multiscale baselines while being efficient in GPU memory and run-time, demonstrating a promising direction for real-time, robust dense prediction in safety-critical robotics. </p>
<blockquote>
<p>ç¡®ä¿è‡ªåŠ¨é©¾é©¶çš„å®‰å…¨æ€§æ˜¯ä¸€é¡¹å¤æ‚çš„æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†æœªçŸ¥ç‰©ä½“å’Œæœªæ›¾é¢„è§çš„é©¾é©¶åœºæ™¯ã€‚æˆ‘ä»¬å¼€å‘äº†å¤šå°ºåº¦è§†é¢‘è½¬æ¢å™¨ï¼Œä»…ä½¿ç”¨è¿åŠ¨çº¿ç´¢å³å¯æ£€æµ‹æœªçŸ¥ç‰©ä½“ã€‚è§†é¢‘è¯­ä¹‰å’Œå…¨æ™¯åˆ†å‰²é€šå¸¸ä¾èµ–äºè®­ç»ƒæœŸé—´çœ‹åˆ°çš„å·²çŸ¥ç±»åˆ«ï¼Œä»è€Œå¿½ç•¥äº†æ–°ç±»åˆ«ã€‚æœ€è¿‘ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸€èµ·è¿›è¡Œçš„è§†è§‰å®šä½è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå°¤å…¶æ˜¯å¯¹äºåƒç´ çº§è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆè§†é¢‘è½¬æ¢å™¨ï¼Œé’ˆå¯¹ç±»åˆ«ä¸å¯çŸ¥åˆ†å‰²è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œæ— éœ€å…‰å­¦æµã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¤šé˜¶æ®µå¤šå°ºåº¦æŸ¥è¯¢å†…å­˜è§£ç å’Œç‰¹å®šè§„æ¨¡çš„éšæœºä¸¢å¼ƒä»¤ç‰Œï¼Œä»¥ç¡®ä¿æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œåˆ©ç”¨å…±äº«çš„å¯å­¦ä¹ å†…å­˜æ¨¡å—ï¼Œä¿ç•™è¯¦ç»†çš„æ—¶ç©ºç‰¹å¾ã€‚ä¸ä¼ ç»Ÿçš„å‹ç¼©ç‰¹å¾çš„è§£ç å™¨ä¸åŒï¼Œæˆ‘ä»¬ä»¥å†…å­˜ä¸ºä¸­å¿ƒçš„è®¾è®¡åœ¨å¤šä¸ªå°ºåº¦ä¸Šä¿ç•™äº†é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨DAVISâ€™16ã€KITTIå’ŒCityscapesä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨GPUå†…å­˜å’Œè¿è¡Œæ—¶æ–¹é¢å§‹ç»ˆä¼˜äºå¤šå°ºåº¦åŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºåœ¨å®æ—¶ã€ç¨³å¥çš„å¯†é›†é¢„æµ‹æ–¹é¢å¾ˆæœ‰å‰é€”ï¼Œå¯¹äºå®‰å…¨å…³é”®çš„æœºå™¨äººæŠ€æœ¯å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14729v1">PDF</a> 6 pages, 2 figures, 1 table</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„å®‰å…¨æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºä¸€ç§å¤šå°ºåº¦è§†é¢‘è½¬æ¢å™¨ï¼Œåˆ©ç”¨è¿åŠ¨çº¿ç´¢æ£€æµ‹æœªçŸ¥ç‰©ä½“ã€‚è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ç§é«˜æ•ˆè§†é¢‘è½¬æ¢å™¨ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µå¤šå°ºåº¦æŸ¥è¯¢-å†…å­˜è§£ç å’Œå°ºåº¦ç‰¹å®šéšæœºä¸¢å¼ƒä»¤ç‰ŒæŠ€æœ¯ï¼Œç”¨äºç±»åˆ«ä¸å¯çŸ¥çš„åˆ†å‰²ï¼Œæ— éœ€å…‰å­¦æµåŠ¨ã€‚è¯¥æ–¹æ³•åœ¨ç»†èŠ‚æ—¶ç©ºç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡ä¸€ä¸ªå…±äº«çš„å¯å­¦ä¹ å†…å­˜æ¨¡å—è¿›è¡Œç»´æŠ¤ã€‚è¯¥ç ”ç©¶åœ¨DAVISâ€™16ã€KITTIå’ŒCityscapesç­‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šå°ºåº¦åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§ï¼ŒåŒæ—¶æé«˜äº†GPUå†…å­˜å’Œè¿è¡Œæ—¶é—´çš„æ•ˆç‡ï¼Œä¸ºå®‰å…¨å…³é”®çš„æœºå™¨äººå®æ—¶ã€ç¨³å¥çš„å¯†é›†é¢„æµ‹æä¾›äº†æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„å®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šå°ºåº¦è§†é¢‘è½¬æ¢å™¨æ¥æ£€æµ‹æœªçŸ¥ç‰©ä½“ã€‚</li>
<li>åˆ©ç”¨è¿åŠ¨çº¿ç´¢è¿›è¡Œç±»åˆ«ä¸å¯çŸ¥çš„åˆ†å‰²ï¼Œæ— éœ€å…‰å­¦æµåŠ¨ã€‚</li>
<li>æå‡ºä¸€ç§é«˜æ•ˆè§†é¢‘è½¬æ¢å™¨ï¼Œé‡‡ç”¨åˆ†é˜¶æ®µå¤šå°ºåº¦æŸ¥è¯¢-å†…å­˜è§£ç æŠ€æœ¯ã€‚</li>
<li>é‡‡ç”¨å°ºåº¦ç‰¹å®šéšæœºä¸¢å¼ƒä»¤ç‰ŒæŠ€æœ¯ï¼Œä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å…±äº«çš„å¯å­¦ä¹ å†…å­˜æ¨¡å—æ¥ç»´æŠ¤ç»†èŠ‚æ—¶ç©ºç‰¹å¾ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ˜¾ç¤ºå‡ºåœ¨å¤šå°ºåº¦åŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7bf3540a6f808858dbd60b9175977cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-063dd8fa5c39abd6d572370d41d89a25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f5e57737087a1eb17a7d105ed7c89b4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine"><a href="#ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine" class="headerlink" title="ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"></a>ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</h2><p><strong>Authors:Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu, Haizhou Li, Benyou Wang</strong></p>
<p>Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†ç”±äºä¸¤ä¸ªå…³é”®éšœç¢ï¼Œå®ƒä»¬åœ¨ä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ä»è¢«å¤§å¤§å¿½è§†ï¼šä¸€æ˜¯é«˜è´¨é‡ä¸­åŒ»æ•°æ®çš„ç¨€ç¼ºæ€§ï¼›äºŒæ˜¯ä¸­åŒ»è¯Šæ–­çš„å›ºæœ‰è·¨æ¨¡æ€æ€§è´¨ï¼Œæ¶‰åŠè§‚å¯Ÿã€è†å¬ã€é—»è¯Šå’Œè„‰è¯Šã€‚è¿™äº›æ„Ÿå®˜ä¸°å¯Œçš„æ¨¡æ€è¶…å‡ºäº†ä¼ ç»ŸLLMçš„èŒƒå›´ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é€‚ç”¨äºä¸­åŒ»çš„å¤šæ¨¡æ€LLMâ€”â€”çŸ³é˜µGPTã€‚ä¸ºäº†å…‹æœæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ•´ç†äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸­åŒ»æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡100GBçš„æ–‡æœ¬æ•°æ®å’Œè¶…è¿‡200GBçš„å¤šæ¨¡æ€æ•°æ®ï¼ŒåŒ…æ‹¬120ä¸‡å¼ å›¾åƒã€200å°æ—¶çš„éŸ³é¢‘å’Œç”Ÿç†ä¿¡å·ã€‚çŸ³é˜µGPTç»è¿‡é¢„è®­ç»ƒå’Œæ‰§è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å®ç°æ·±å…¥çš„ä¸­åŒ»çŸ¥è¯†å’Œå¤šæ¨¡æ€æ¨ç†ã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬æ”¶é›†äº†æœ€è¿‘çš„ä¸­åŒ»èµ„æ ¼è€ƒè¯•ï¼Œå¹¶å»ºç«‹äº†è¯ç‰©è¯†åˆ«å’Œè§†è§‰è¯Šæ–­çš„è§†è§‰åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼ŒçŸ³é˜µGPTåœ¨åŒç±»è§„æ¨¡çš„LLMä¸­è¡¨ç°çªå‡ºï¼Œå¹¶ä¸æ›´å¤§çš„ä¸“æœ‰æ¨¡å‹ç›¸ç«äº‰ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ç°æœ‰çš„å¤šæ¨¡æ€LLMä¸­é¢†å…ˆä¸­åŒ»è§†è§‰ç†è§£ï¼Œå¹¶å±•ç¤ºäº†è·¨å£°éŸ³ã€è„‰æã€æ°”å‘³å’Œè§†è§‰ç­‰æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥ï¼Œä¸ºä¸­åŒ»çš„æ•´ä½“å¤šæ¨¡æ€æ„ŸçŸ¥å’Œè¯Šæ–­é“ºå¹³äº†é“è·¯ã€‚æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å…¬å¼€å¯ç”¨ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå°†æ¿€å‘è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14706v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¼ ç»Ÿä¸­åŒ»é¢†åŸŸæ¨å‡ºçš„é¦–ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹â€”â€”çŸ³é˜µGPTã€‚è¯¥æ¨¡å‹æ—¨åœ¨å…‹æœä¸­åŒ»é¢†åŸŸä¸¤å¤§æŒ‘æˆ˜ï¼šé«˜è´¨é‡æ•°æ®çš„ç¨€ç¼ºæ€§å’Œä¸­åŒ»è¯Šæ–­çš„æ„Ÿå®˜ä¸°å¯Œæ€§ã€‚é€šè¿‡æ„å»ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸­åŒ»æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¶…è¿‡100GBçš„æ–‡æœ¬æ•°æ®å’Œè¶…è¿‡200GBçš„å¤šæ¨¡æ€æ•°æ®ï¼ˆåŒ…æ‹¬å›¾åƒã€éŸ³é¢‘å’Œç”Ÿç†ä¿¡å·ï¼‰ï¼ŒShizhenGPTå®ç°äº†æ·±åº¦ä¸­åŒ»çŸ¥è¯†å’Œå¤šæ¨¡æ€æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œå®ƒåœ¨ä¸­åŒ»è§†è§‰ç†è§£æ–¹é¢é¢†å…ˆç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†è·¨å£°éŸ³ã€è„‰è±¡ã€æ°”å‘³å’Œè§†è§‰ç­‰æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºä¸­åŒ»é¢†åŸŸçš„å…¨æ¯å¤šæ¨¡æ€æ„ŸçŸ¥å’Œè¯Šæ–­é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ³é˜µGPTæ˜¯é¦–ä¸ªé’ˆå¯¹ä¼ ç»Ÿä¸­åŒ»é¢†åŸŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ä¸­åŒ»é¢†åŸŸé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šé«˜è´¨é‡æ•°æ®çš„ç¨€ç¼ºæ€§å’Œä¸­åŒ»è¯Šæ–­çš„æ„Ÿå®˜ä¸°å¯Œæ€§ã€‚</li>
<li>çŸ³é˜µGPTé€šè¿‡æ„å»ºè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸­åŒ»æ•°æ®é›†æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>ShizhenGPTå®ç°äº†æ·±åº¦ä¸­åŒ»çŸ¥è¯†å’Œå¤šæ¨¡æ€æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒçŸ³é˜µGPTåœ¨ä¸­åŒ»è§†è§‰ç†è§£æ–¹é¢é¢†å…ˆç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>çŸ³é˜µGPTå±•ç¤ºäº†è·¨å£°éŸ³ã€è„‰è±¡ã€æ°”å‘³å’Œè§†è§‰ç­‰æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-47460fbb72445ff6077ce716a2e6e2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69877002d649150cb83d1634ef7ced3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06977a8a2e667eb13790fe426eed1bb8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71fb63110c7981d1e1bf48cd4d757508.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65aa1d8909e397f2b6fd7dc06368436c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b663604c68d1ba6d3e82ef5a0ec1df1e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adi Renduchintala, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Nemotron-Nano-9B-v2ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶ä¸åŒç±»æ¨¡å‹ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚Nemotron-Nano-9B-v2å»ºç«‹åœ¨Nemotron-Hæ¶æ„çš„åŸºç¡€ä¸Šï¼Œå°†Transformeræ¶æ„ä¸­å¤§éƒ¨åˆ†çš„è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œä»¥å®ç°åœ¨ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€è€ƒè½¨è¿¹æ—¶æé«˜æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆä½¿ç”¨FP8è®­ç»ƒé…æ–¹åœ¨20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šé¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰æ¥åˆ›å»ºNemotron-Nano-9B-v2ã€‚åœ¨å¯¹Nemotron-Nano-12B-v2-Baseè¿›è¡Œå¯¹é½åï¼Œæˆ‘ä»¬é‡‡ç”¨Minitronç­–ç•¥æ¥å‹ç¼©å’Œè’¸é¦æ¨¡å‹ï¼Œæ—¨åœ¨èƒ½å¤Ÿåœ¨å•ä¸ªNVIDIA A10G GPUï¼ˆå…·æœ‰22GiBå†…å­˜ï¼Œbfloat16ç²¾åº¦ï¼‰ä¸Šè¿›è¡Œé«˜è¾¾128kä»¤ç‰Œçš„æ¨ç†ã€‚ä¸ç°æœ‰çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen3-8Bï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†Nemotron-Nano-9B-v2çš„åŒç­‰æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨è¯¸å¦‚8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œçš„æ¨ç†è®¾ç½®ä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚æˆ‘ä»¬å°†Nemotron-Nano-9B-v2ã€Nemotron-Nano12B-v2-Baseå’ŒNemotron-Nano-9B-v2-Baseæ£€æŸ¥ç‚¹ä»¥åŠæˆ‘ä»¬å¤§éƒ¨åˆ†é¢„è®­ç»ƒå’Œåè®­ç»ƒæ•°æ®é›†åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºMamba-Transformeræ¶æ„çš„Nemotron-Nano-9B-v2æ··åˆè¯­è¨€æ¨¡å‹æ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶åœ¨åŒç±»æ¨¡å‹ä¸­å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚å®ƒé€šè¿‡å¼•å…¥Mamba-2å±‚æ›¿ä»£Transformeræ¶æ„ä¸­çš„å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»¥åŠ å¿«ç”Ÿæˆé•¿æœŸæ¨ç†è½¨è¿¹æ—¶çš„æ¨ç†é€Ÿåº¦ã€‚ç»è¿‡åœ¨20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šé¢„è®­ç»ƒçš„12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰ï¼Œå¹¶é‡‡ç”¨Minitronç­–ç•¥è¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼Œå®ç°äº†åœ¨å•ä¸ªNVIDIA A10G GPUä¸Šé«˜è¾¾128kä»¤ç‰Œçš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºå…¶ä»–ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ç›¸å½“æˆ–æ›´å¥½çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œç­‰æ¨ç†åœºæ™¯ä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚è¯¥æ¨¡å‹åŠç›¸å…³æ•°æ®é›†å·²åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nemotron-Nano-9B-v2æ˜¯ä¸€ä¸ªåŸºäºMamba-Transformeræ¶æ„çš„æ··åˆè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†çš„ååé‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥Mamba-2å±‚æ›¿ä»£Transformeræ¶æ„ä¸­çš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚</li>
<li>Nemotron-Nano-9B-v2ç»è¿‡åœ¨å¤§é‡æ•°æ®ä¸Šçš„é¢„è®­ç»ƒï¼Œç„¶åé‡‡ç”¨Minitronç­–ç•¥è¿›è¡Œå‹ç¼©å’Œè’¸é¦ã€‚</li>
<li>æ¨¡å‹å¯ä»¥åœ¨å•ä¸ªNVIDIA A10G GPUä¸Šå¤„ç†é«˜è¾¾128kä»¤ç‰Œçš„æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼ŒNemotron-Nano-9B-v2åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨ç‰¹å®šæ¨ç†åœºæ™¯ä¸‹å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ac5d6aea88ce279e01ebbd730851f0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3dbd8c7b29fda38f4dde86f49428b106.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Letâ€™s-Use-ChatGPT-To-Write-Our-Paper-Benchmarking-LLMs-To-Write-the-Introduction-of-a-Research-Paper"><a href="#Letâ€™s-Use-ChatGPT-To-Write-Our-Paper-Benchmarking-LLMs-To-Write-the-Introduction-of-a-Research-Paper" class="headerlink" title="Letâ€™s Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the   Introduction of a Research Paper"></a>Letâ€™s Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the   Introduction of a Research Paper</h2><p><strong>Authors:Krishna Garg, Firoz Shaikh, Sambaran Bandyopadhyay, Cornelia Caragea</strong></p>
<p>As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMsâ€™ ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverickâ€™s superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets. </p>
<blockquote>
<p>éšç€è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è€…é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå†™ä½œåŠ©æ‰‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶è®ºæ–‡å¼•è¨€ä»ç„¶æ˜¯ä¸€é¡¹æ—¢å…·æŒ‘æˆ˜æ€§åˆå¿…ä¸å¯å°‘çš„å·¥ä½œã€‚æˆ‘ä»¬ä»‹ç»äº†ç§‘å­¦å¼•è¨€ç”Ÿæˆï¼ˆSciIGï¼‰ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä»æ ‡é¢˜ã€æ‘˜è¦å’Œç›¸å…³å·¥ä½œä¸­ç”Ÿæˆè¿è´¯å¼•è¨€çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªNAACL 2025å’ŒICLR 2025è®ºæ–‡çš„æ–°æ•°æ®é›†ï¼Œè¯„ä¼°äº†äº”ä¸ªæœ€æ–°æ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ï¼ˆDeepSeek-v3ã€Gemma-3-12Bã€LLaMA 4-Maverickã€MistralAI Small 3.1ï¼‰å’Œé—­æºGPT-4oç³»ç»Ÿï¼Œæ¶‰åŠå¤šä¸ªç»´åº¦ï¼šè¯æ±‡é‡å ã€è¯­ä¹‰ç›¸ä¼¼æ€§ã€å†…å®¹è¦†ç›–ã€å¿ å®æ€§ã€ä¸€è‡´æ€§ã€å¼•ç”¨æ­£ç¡®æ€§å’Œå™è¿°è´¨é‡ã€‚æˆ‘ä»¬çš„ç»¼åˆæ¡†æ¶ç»“åˆäº†è‡ªåŠ¨åº¦é‡æŒ‡æ ‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜çš„è¯„ä»·ã€‚ç»“æœè¡¨æ˜ï¼ŒLLaMA-4 Maverickåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¿ å®æ€§æ–¹é¢ã€‚æ­¤å¤–ï¼Œä¸‰æç¤ºæ³•å§‹ç»ˆå¦‚ä¸€åœ°ä¼˜äºå°‘æç¤ºæ³•ã€‚è¿™äº›å‘ç°ä¸ºå¼€å‘æœ‰æ•ˆçš„ç ”ç©¶å†™ä½œåŠ©æ‰‹æä¾›äº†å®é™…è§è§£ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©å­¦æœ¯å†™ä½œè®¾å®šäº†ç°å®æœŸæœ›ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†å…¬å¼€å‘å¸ƒæ‰€æœ‰ä»£ç å’Œæ•°æ®é›†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14273v1">PDF</a> 20 pages, 15 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬å†…å®¹ï¼Œæ‘˜è¦å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šâ€œç ”ç©¶äººå‘˜é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå†™ä½œåŠ©æ‰‹ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç ”ç©¶è®ºæ–‡å¼•è¨€ä»ç„¶å……æ»¡æŒ‘æˆ˜ä¸”è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ç§‘å­¦å¼•è¨€ç”Ÿæˆï¼ˆSciIGï¼‰ä»»åŠ¡ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä»æ ‡é¢˜ã€æ‘˜è¦å’Œç›¸å…³å·¥ä½œä¸­ç”Ÿæˆè¿è´¯å¼•è¨€çš„èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨æ–°çš„æ•°æ®é›†è¯„ä¼°äº†äº”ä¸ªå…ˆè¿›æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¼€æºæ¨¡å‹ï¼ˆDeepSeek-v3ç­‰ï¼‰å’Œé—­æºGPT-4oç³»ç»Ÿï¼Œæ¶µç›–å¤šä¸ªç»´åº¦ã€‚ç ”ç©¶ç»“æœè¡¨æ˜LLaMA-4 Maverickæ¨¡å‹åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¿ å®åº¦æ–¹é¢ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜å‘ç°ä¸‰æç¤ºæ³•ä¼˜äºå°‘æç¤ºæ³•ã€‚è¿™äº›å‘ç°å¯¹äºå¼€å‘æœ‰æ•ˆçš„ç ”ç©¶å†™ä½œåŠ©æ‰‹æä¾›äº†å®é™…è§è§£ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©å­¦æœ¯å†™ä½œè®¾å®šäº†åˆç†çš„æœŸæœ›ã€‚â€</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³äºè¯¥æ–‡æœ¬çš„ä¸»è¦è§è§£åˆ—è¡¨ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è¢«è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜ç”¨ä½œå†™ä½œåŠ©æ‰‹ã€‚ç”Ÿæˆé«˜è´¨é‡ç ”ç©¶è®ºæ–‡å¼•è¨€æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶ä»‹ç»äº†ç§‘å­¦å¼•è¨€ç”Ÿæˆï¼ˆSciIGï¼‰ä»»åŠ¡ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆè¿è´¯å¼•è¨€çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨æ–°çš„æ•°æ®é›†è¯„ä¼°äº†äº”ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹ã€‚è¯„ä¼°æ¶µç›–å¤šä¸ªç»´åº¦ï¼Œå¦‚è¯æ±‡é‡å ã€è¯­ä¹‰ç›¸ä¼¼æ€§ã€å†…å®¹è¦†ç›–ç­‰ã€‚</li>
<li>LLaMA-4 Maverickæ¨¡å‹åœ¨å¤šæ•°è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå¿ å®åº¦æ–¹é¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-585536e3e2684a0d309547bb1f0b8695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c363bee2704d4e4da3640c9be22f37ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30e3e68b8bb3405bf8cdced532fd833a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1084e06a8d0f0414a34efda6ffe32772.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation"><a href="#RotBench-Evaluating-Multimodal-Large-Language-Models-on-Identifying-Image-Rotation" class="headerlink" title="RotBench: Evaluating Multimodal Large Language Models on Identifying   Image Rotation"></a>RotBench: Evaluating Multimodal Large Language Models on Identifying   Image Rotation</h2><p><strong>Authors:Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal</strong></p>
<p>We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\deg}, 90{\deg}, 180{\deg}, and 270{\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench â€“ a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information â€“ including captions, depth maps, and more â€“ or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\deg}) images, while certain models are able to identify upside-down (180{\deg}) images. None can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve modelsâ€™ ability to distinguish 90{\deg} and 270{\deg} rotations, despite substantially improving the identification of 180{\deg} images. Together, these results reveal a significant gap between MLLMsâ€™ spatial reasoning capabilities and human perception in identifying rotation. </p>
<blockquote>
<p>æˆ‘ä»¬è°ƒæŸ¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿåœ¨å¤šå¤§ç¨‹åº¦ä¸Šå‡†ç¡®è¯†åˆ«è¾“å…¥å›¾åƒæ—‹è½¬0Â°ã€90Â°ã€180Â°å’Œ270Â°çš„æ–¹å‘ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦å¼ºå¤§çš„è§†è§‰æ¨ç†èƒ½åŠ›æ¥æ£€æµ‹æ—‹è½¬çº¿ç´¢å¹¶ç†è§£å›¾åƒå†…çš„ç©ºé—´å…³ç³»ï¼Œæ— è®ºå…¶æ–¹å‘å¦‚ä½•ã€‚ä¸ºäº†è¯„ä¼°MLLMsçš„è¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†RotBenchâ€”â€”ä¸€ä¸ªåŒ…å«ç”Ÿæ´»æ–¹å¼ã€è‚–åƒå’Œé£æ™¯å›¾åƒçš„350å¼ æ‰‹åŠ¨è¿‡æ»¤çš„åŸºå‡†æµ‹è¯•é›†ã€‚å°½ç®¡è¿™é¡¹ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°åŒ…æ‹¬GPT-5ã€o3å’ŒGemini-2.5-Proåœ¨å†…çš„è‹¥å¹²æœ€å…ˆè¿›çš„å¼€æºå’Œä¸“æœ‰MLLMså¹¶ä¸èƒ½å¯é åœ°è¯†åˆ«è¾“å…¥å›¾åƒä¸­çš„æ—‹è½¬ã€‚ä¸ºæ¨¡å‹æä¾›è¾…åŠ©ä¿¡æ¯ï¼Œå¦‚å­—å¹•ã€æ·±åº¦å›¾ç­‰ï¼Œæˆ–ä½¿ç”¨æ€ç»´é“¾æç¤ºåªèƒ½å¸¦æ¥å¾®å°ä¸”ä¸ä¸€è‡´çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è¯†åˆ«æ­£é¢æœä¸Šï¼ˆ0Â°ï¼‰çš„å›¾åƒï¼Œè€ŒæŸäº›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å€’ç½®ï¼ˆ180Â°ï¼‰çš„å›¾åƒã€‚æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå¯é åœ°åŒºåˆ†90Â°å’Œ270Â°çš„å›¾åƒã€‚åŒæ—¶å±•ç¤ºä»¥ä¸åŒæ–¹å‘æ—‹è½¬çš„å›¾åƒä¼šå¯¼è‡´æ¨ç†æ¨¡å‹çš„æ€§èƒ½é€‚åº¦æå‡ï¼Œè€Œä½¿ç”¨æŠ•ç¥¨çš„ä¿®æ”¹è®¾ç½®åˆ™ä¼šæé«˜è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¡¨æ˜ï¼Œå¾®è°ƒå¹¶ä¸ä¼šæé«˜æ¨¡å‹åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬çš„èƒ½åŠ›ï¼Œå°½ç®¡å®ƒå¤§å¤§æé«˜äº†å¯¹180Â°å›¾åƒçš„è¯†åˆ«èƒ½åŠ›ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç»“æœæ­ç¤ºäº†MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›ä¸äººç±»æ„ŸçŸ¥åœ¨è¯†åˆ«æ—‹è½¬æ–¹é¢çš„é‡å¤§å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13968v2">PDF</a> 20 pages. Code and data: <a target="_blank" rel="noopener" href="https://github.com/tianyiniu/RotBench">https://github.com/tianyiniu/RotBench</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶è°ƒæŸ¥äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å‡†ç¡®è¯†åˆ«è¾“å…¥å›¾åƒæ—‹è½¬è§’åº¦æ–¹é¢çš„èƒ½åŠ›ï¼Œæ¶‰åŠçš„æ—‹è½¬è§’åº¦åŒ…æ‹¬0Â°ã€90Â°ã€180Â°å’Œ270Â°ã€‚è¿™ä¸€ä»»åŠ¡è¦æ±‚æ¨¡å‹å…·å¤‡ç¨³å¥çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä»¥æ£€æµ‹æ—‹è½¬çº¿ç´¢å¹¶åœ¨å›¾åƒä¸­ä¸Šä¸‹æ–‡åŒ–ç©ºé—´å…³ç³»ï¼Œæ— è®ºå…¶æ–¹å‘å¦‚ä½•ã€‚ä¸ºäº†è¯„ä¼°MLLMsçš„è¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†RotBenchâ€”â€”ä¸€ä¸ªåŒ…å«350å¼ æ‰‹åŠ¨ç­›é€‰çš„å›¾åƒåŸºå‡†æµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…æ‹¬ç”Ÿæ´»æ–¹å¼ã€è‚–åƒå’Œé£æ™¯å›¾åƒã€‚å°½ç®¡ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°ä¸€äº›å…ˆè¿›çš„å¼€æºå’Œä¸“æœ‰MLLMsï¼ŒåŒ…æ‹¬GPT-5ã€o3å’ŒGemini-2.5-Proï¼Œæ— æ³•å¯é åœ°è¯†åˆ«è¾“å…¥å›¾åƒçš„æ—‹è½¬è§’åº¦ã€‚ä¸ºæ¨¡å‹æä¾›è¾…åŠ©ä¿¡æ¯ï¼ˆåŒ…æ‹¬å­—å¹•ã€æ·±åº¦å›¾ç­‰ï¼‰æˆ–ä½¿ç”¨é“¾å¼æ€ç»´æç¤ºåªèƒ½å¸¦æ¥å¾®å°ä¸”ä¸ä¸€è‡´çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨¡å‹èƒ½å¤Ÿå¯é åœ°è¯†åˆ«æ­£é¢æœä¸Šï¼ˆ0Â°ï¼‰çš„å›¾åƒï¼Œè€ŒæŸäº›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å€’ç½®ï¼ˆ180Â°ï¼‰çš„å›¾åƒã€‚æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå¯é åœ°åŒºåˆ†90Â°å’Œ270Â°çš„å›¾åƒã€‚åŒæ—¶å±•ç¤ºä»¥ä¸åŒæ–¹å‘æ—‹è½¬çš„å›¾åƒä¼šå¯¼è‡´æ¨ç†æ¨¡å‹çš„æ€§èƒ½é€‚åº¦æå‡ï¼Œè€Œä½¿ç”¨æŠ•ç¥¨çš„æ”¹è¿›è®¾ç½®åˆ™æé«˜äº†è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°å¾®è°ƒå¹¶ä¸èƒ½æé«˜æ¨¡å‹åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬çš„èƒ½åŠ›ï¼Œå°½ç®¡å®ƒåœ¨è¯†åˆ«180Â°å›¾åƒæ–¹é¢æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚æ€»ä½“è€Œè¨€ï¼Œè¿™äº›ç»“æœæ­ç¤ºäº†MLLMsçš„ç©ºé—´æ¨ç†èƒ½åŠ›å’Œäººç±»æ„ŸçŸ¥åœ¨è¯†åˆ«æ—‹è½¬æ–¹é¢çš„å·¨å¤§å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯†åˆ«å›¾åƒæ—‹è½¬è§’åº¦æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†RotBenchåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°MLLMsçš„è§†è§‰æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å‘ç°å¤§å¤šæ•°MLLMsæ— æ³•å¯é åœ°åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬çš„å›¾åƒã€‚</li>
<li>è¾…åŠ©ä¿¡æ¯å’Œé“¾å¼æ€ç»´æç¤ºåªèƒ½å¸¦æ¥æœ‰é™çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
<li>åŒæ—¶å±•ç¤ºä¸åŒæ—‹è½¬è§’åº¦çš„å›¾åƒå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æŠ•ç¥¨çš„æ”¹è¿›è®¾ç½®æœ‰åŠ©äºæé«˜è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¾®è°ƒæ— æ³•æ˜¾è‘—æé«˜æ¨¡å‹åŒºåˆ†90Â°å’Œ270Â°æ—‹è½¬çš„èƒ½åŠ›ï¼Œä½†å¯æ˜¾è‘—æ”¹å–„å¯¹180Â°å›¾åƒçš„è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59089beaf374738874a5569337fe32ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68e2f3bf7b1db8cc8f161d9f77c08946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ba828cb94fd6b696fc21510dafc7eeb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PROV-AGENT-Unified-Provenance-for-Tracking-AI-Agent-Interactions-in-Agentic-Workflows"><a href="#PROV-AGENT-Unified-Provenance-for-Tracking-AI-Agent-Interactions-in-Agentic-Workflows" class="headerlink" title="PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in   Agentic Workflows"></a>PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in   Agentic Workflows</h2><p><strong>Authors:Renan Souza, Amal Gueroudji, Stephen DeWitt, Daniel Rosendo, Tirthankar Ghosal, Robert Ross, Prasanna Balaprakash, Rafael Ferreira da Silva</strong></p>
<p>Large Language Models (LLMs) and other foundation models are increasingly used as the core of AI agents. In agentic workflows, these agents plan tasks, interact with humans and peers, and influence scientific outcomes across federated and heterogeneous environments. However, agents can hallucinate or reason incorrectly, propagating errors when one agentâ€™s output becomes anotherâ€™s input. Thus, assuring that agentsâ€™ actions are transparent, traceable, reproducible, and reliable is critical to assess hallucination risks and mitigate their workflow impacts. While provenance techniques have long supported these principles, existing methods fail to capture and relate agent-centric metadata such as prompts, responses, and decisions with the broader workflow context and downstream outcomes. In this paper, we introduce PROV-AGENT, a provenance model that extends W3C PROV and leverages the Model Context Protocol (MCP) and data observability to integrate agent interactions into end-to-end workflow provenance. Our contributions include: (1) a provenance model tailored for agentic workflows, (2) a near real-time, open-source system for capturing agentic provenance, and (3) a cross-facility evaluation spanning edge, cloud, and HPC environments, demonstrating support for critical provenance queries and agent reliability analysis. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå…¶ä»–åŸºç¡€æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œäººå·¥æ™ºèƒ½ä»£ç†çš„æ ¸å¿ƒã€‚åœ¨ä»£ç†å·¥ä½œæµç¨‹ä¸­ï¼Œè¿™äº›ä»£ç†è®¡åˆ’ä»»åŠ¡ã€ä¸äººç±»å’ŒåŒé¾„äººäº’åŠ¨ï¼Œå¹¶åœ¨è”é‚¦å’Œå¼‚æ„ç¯å¢ƒä¸­å½±å“ç§‘å­¦ç»“æœã€‚ç„¶è€Œï¼Œä»£ç†å¯èƒ½ä¼šå‡ºç°å¹»è§‰æˆ–æ¨ç†é”™è¯¯ï¼Œå½“ä¸€ä¸ªä»£ç†çš„è¾“å‡ºæˆä¸ºå¦ä¸€ä¸ªä»£ç†çš„è¾“å…¥æ—¶ï¼Œé”™è¯¯å°±ä¼šä¼ æ’­ã€‚å› æ­¤ï¼Œç¡®ä¿ä»£ç†çš„è¡ŒåŠ¨é€æ˜ã€å¯è¿½è¸ªã€å¯é‡ç°å’Œå¯é ï¼Œå¯¹äºè¯„ä¼°å¹»è§‰é£é™©å’Œå‡è½»å…¶å·¥ä½œæµç¨‹å½±å“è‡³å…³é‡è¦ã€‚å°½ç®¡æº¯æºæŠ€æœ¯é•¿æœŸæ”¯æŒè¿™äº›åŸåˆ™ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•æ•è·å¹¶ä¸ä»¥ä»£ç†ä¸ºä¸­å¿ƒçš„å…ƒæ•°æ®ï¼ˆå¦‚æç¤ºã€å“åº”å’Œå†³ç­–ï¼‰ä¸æ›´å¹¿æ³›çš„å·¥ä½œæµä¸Šä¸‹æ–‡å’Œä¸‹æ¸¸ç»“æœç›¸å…³è”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PROV-AGENTï¼Œè¿™æ˜¯ä¸€ä¸ªæº¯æºæ¨¡å‹ï¼Œå®ƒæ‰©å±•äº†W3Cçš„PROVï¼Œå¹¶åˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å’Œæ•°æ®å¯è§‚æ€§å°†ä»£ç†äº’åŠ¨æ•´åˆåˆ°ç«¯åˆ°ç«¯çš„å·¥ä½œæµæº¯æºä¸­ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹ä»£ç†å·¥ä½œæµçš„æº¯æºæ¨¡å‹ï¼Œï¼ˆ2ï¼‰ä¸€ä¸ªè¿‘å®æ—¶çš„å¼€æºç³»ç»Ÿï¼Œç”¨äºæ•è·ä»£ç†æº¯æºï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸€é¡¹è·¨è¶Šè¾¹ç¼˜ã€äº‘å’Œé«˜æ€§èƒ½è®¡ç®—ç¯å¢ƒçš„è·¨è®¾æ–½è¯„ä¼°ï¼Œæ¼”ç¤ºäº†å¯¹å…³é”®æº¯æºæŸ¥è¯¢å’Œä»£ç†å¯é æ€§åˆ†æçš„æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02866v3">PDF</a> Paper accepted for publication in the Proceedings of the 2025 IEEE   21st International Conference on e-Science. Cite it as: R. Souza, A.   Gueroudji, S. DeWitt, D. Rosendo, T. Ghosal, R. Ross, P. Balaprakash, R. F.   da Silva, â€œPROV-AGENT: Unified Provenance for Tracking AI Agent Interactions   in Agentic Workflows,â€ IEEE International Conference on e-Science, Chicago,   IL, USA, 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç­‰åŸºç¡€æ¨¡å‹ä½œä¸ºAIä»£ç†çš„æ ¸å¿ƒå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚åœ¨ä»£ç†å·¥ä½œæµç¨‹ä¸­ï¼Œè¿™äº›ä»£ç†è®¡åˆ’ä»»åŠ¡ã€ä¸äººç±»å’ŒåŒè¡Œäº’åŠ¨ï¼Œå¹¶åœ¨è”é‚¦å’Œå¼‚æ„ç¯å¢ƒä¸­å½±å“ç§‘å­¦ç»“æœã€‚ç„¶è€Œï¼Œä»£ç†å¯èƒ½ä¼šå‡ºç°å¹»è§‰æˆ–é”™è¯¯æ¨ç†ï¼Œå½“ä¸€ä¸ªä»£ç†çš„è¾“å‡ºæˆä¸ºå¦ä¸€ä¸ªä»£ç†çš„è¾“å…¥æ—¶ï¼Œé”™è¯¯ä¼šä¼ æ’­ã€‚å› æ­¤ï¼Œç¡®ä¿ä»£ç†çš„è¡ŒåŠ¨é€æ˜ã€å¯è¿½æº¯ã€å¯é‡ç°å’Œå¯é ï¼Œå¯¹äºè¯„ä¼°å¹»è§‰é£é™©å’Œå‡è½»å·¥ä½œæµç¨‹å½±å“è‡³å…³é‡è¦ã€‚å°½ç®¡PROVæ¨¡å‹é•¿ä¹…ä»¥æ¥æ”¯æŒè¿™äº›åŸåˆ™ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•æ•è·å¹¶ä¸æ›´å¹¿æ³›çš„å·¥ä½œæµä¸Šä¸‹æ–‡å’Œä¸‹æ¸¸ç»“æœç›¸å…³çš„ä»£ç†ä¸­å¿ƒå…ƒæ•°æ®ï¼Œå¦‚æç¤ºã€å“åº”å’Œå†³ç­–ã€‚æœ¬æ–‡ä»‹ç»PROV-AGENTï¼Œä¸€ä¸ªæ‰©å±•W3C PROVçš„PROVæ¨¡å‹ï¼Œåˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å’Œæ•°æ®å¯è§‚æ€§å°†ä»£ç†äº¤äº’é›†æˆåˆ°ç«¯åˆ°ç«¯çš„å·¥ä½œæµPROVä¸­ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹ä»£ç†å·¥ä½œæµç¨‹çš„PROVæ¨¡å‹ï¼Œï¼ˆ2ï¼‰ä¸€ä¸ªè¿‘å®æ—¶ã€å¼€æºçš„ä»£ç†PROVæ•è·ç³»ç»Ÿï¼Œï¼ˆ3ï¼‰è·¨è¾¹ç¼˜ã€äº‘å’Œé«˜æ€§èƒ½è®¡ç®—ç¯å¢ƒçš„è·¨è®¾æ–½è¯„ä¼°ï¼Œè¯æ˜æ”¯æŒå…³é”®PROVæŸ¥è¯¢å’Œä»£ç†å¯é æ€§åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså’Œå…¶ä»–åŸºç¡€æ¨¡å‹è¢«ç”¨ä½œAIä»£ç†çš„æ ¸å¿ƒï¼Œå¹¿æ³›åº”ç”¨äºä»£ç†å·¥ä½œæµç¨‹ä¸­ã€‚</li>
<li>ä»£ç†åœ¨å·¥ä½œä¸­å¯èƒ½ä¼šäº§ç”Ÿå¹»è§‰æˆ–é”™è¯¯æ¨ç†ï¼Œéœ€è¦ç¡®ä¿å…¶è¡Œä¸ºé€æ˜ã€å¯è¿½æº¯ã€å¯é‡ç°å’Œå¯é ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•å…¨é¢æ•è·ä¸ä»£ç†ç›¸å…³çš„å…ƒæ•°æ®ï¼Œå¦‚æç¤ºã€å“åº”å’Œå†³ç­–ï¼Œä»¥åŠä¸å·¥ä½œæµä¸Šä¸‹æ–‡å’Œä¸‹æ¸¸ç»“æœçš„å…³è”ã€‚</li>
<li>PROV-AGENTæ˜¯ä¸€ä¸ªæ‰©å±•W3C PROVçš„PROVæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>PROV-AGENTåˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å’Œæ•°æ®å¯è§‚æ€§ï¼Œå°†ä»£ç†äº¤äº’é›†æˆåˆ°ç«¯åˆ°ç«¯çš„å·¥ä½œæµä¸­ã€‚</li>
<li>è¿‘å®æ—¶ã€å¼€æºçš„ä»£ç†PROVæ•è·ç³»ç»Ÿè¢«å¼€å‘å‡ºæ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9ef35ba91e357e3a0563c9991707a50e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e03f75d5d7126fa8feb936583b0e70d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cdae6ad8727f2e553be22bb4ee4b9cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c02a33bc3b755fcc0c58e269872b6e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9002c7a4081594ebd4630c7a0dc59da.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agentsâ€™ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æœ€è¿‘æ˜¾ç¤ºå‡ºé€šè¿‡ä¸å…¶ç¯å¢ƒçš„å¤šæ­¥äº¤äº’è¿›è¡Œå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚è™½ç„¶è¿™äº›ä»£ç†æœ‰æ½œåŠ›å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†å®ƒä»¬çš„è§£å†³é—®é¢˜çš„è¿‡ç¨‹ï¼Œå³ä»£ç†å®Œæˆä»»åŠ¡çš„äº¤äº’è½¨è¿¹ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥å¼•å¯¼ä»£ç†æ­£ç¡®è§£å†³é—®é¢˜ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•ï¼ˆå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼‰å¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å„ç§è½¨è¿¹ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶ä¸”ç¼ºä¹æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œè¿™å¯¼è‡´å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentï¼Œä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–ä»–ä»¬çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå…³é”®æ“ä½œï¼šä¿®è®¢ã€é‡ç»„å’Œç²¾ç‚¼ï¼Œæ¥é‡æ–°å®¡è§†å’Œæ”¹è¿›ä¹‹å‰çš„è½¨è¿¹ã€‚è¿™ç§è¿›åŒ–æœºåˆ¶å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒé€šè¿‡æ™ºèƒ½åœ°æ¢ç´¢å—å…ˆå‰è½¨è¿¹æŒ‡å¯¼çš„å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œè¶…è¶Šäº†å±€éƒ¨æœ€ä¼˜ï¼›ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨è·¨è½¨è¿¹çš„çµæ„Ÿæ¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡è½»æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒSE-Agentå®ç°äº†è¿ç»­çš„è‡ªæˆ‘è¿›åŒ–ï¼Œé€æ­¥æé«˜äº†æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬åœ¨SWE-bench Verifiedä¸Šè¯„ä¼°äº†SE-Agentï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„GitHubé—®é¢˜ã€‚åœ¨äº”ä¸ªå¼ºå¤§çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆSE-Agentå¸¦æ¥äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨SWE-bench Verifiedä¸Šçš„å¼€æºä»£ç†ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/JARVIS-Xs/SE-Agentä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›ä»£ç†å…·æœ‰å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„é—®é¢˜è§£å†³è¿‡ç¨‹ï¼Œå³ä»£ç†å®Œæˆä»»åŠ¡çš„äº’åŠ¨è½¨è¿¹ï¼Œä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥ä¸ºè§£å†³é—®é¢˜æä¾›æ­£ç¡®çš„æ–¹å‘ã€‚ç°æœ‰æ–¹æ³•å¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è™½ç„¶å¯ä»¥æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä½†å®ƒä»¬å¿½è§†äº†ä¸åŒè½¨è¿¹ä¹‹é—´çš„ç›¸äº’ä½œç”¨åŠæœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentï¼Œä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–å…¶æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–ä¸‰ä¸ªå…³é”®æ“ä½œï¼Œè¯¥è¿›åŒ–æœºåˆ¶å®ç°äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šä¸€æ˜¯é€šè¿‡æ™ºèƒ½æ¢ç´¢ç”±å…ˆå‰è½¨è¿¹å¼•å¯¼çš„å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥æ‰©å±•æœç´¢ç©ºé—´è¶…è¶Šå±€éƒ¨æœ€ä¼˜ï¼›äºŒæ˜¯åˆ©ç”¨è·¨è½¨è¿¹çµæ„Ÿæ¥æœ‰æ•ˆæé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒSE-Agentåœ¨è§£å†³çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä»£ç†å…·æœ‰å¤„ç†å¤æ‚ä»»åŠ¡çš„èƒ½åŠ›ï¼Œä½†åœ¨é—®é¢˜è§£å†³è¿‡ç¨‹ä¸­çš„äº’åŠ¨è½¨è¿¹å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢åœ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ—¶å¿½è§†äº†è½¨è¿¹çš„å¤šæ ·æ€§å’Œç›¸äº’ä½œç”¨ã€‚</li>
<li>SE-Agentæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–æ“ä½œå®ç°æ™ºèƒ½æ¢ç´¢å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„ã€‚</li>
<li>SE-Agentæ‰©å¤§äº†æœç´¢ç©ºé—´å¹¶è¶…è¶Šäº†å±€éƒ¨æœ€ä¼˜ï¼Œæé«˜äº†æ€§èƒ½å¹¶å‡å°‘äº†æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚</li>
<li>SE-Agentåœ¨è§£å†³çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜ä¸Šå®ç°äº†æ˜¾è‘—çš„æ•ˆæœæ”¹è¿›ã€‚</li>
<li>SE-Agentçš„ä»£ç å’Œæ¼”ç¤ºææ–™å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9bd726db11fbc16937e1c35c985be8d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Quantification-for-Language-Models-A-Suite-of-Black-Box-White-Box-LLM-Judge-and-Ensemble-Scorers"><a href="#Uncertainty-Quantification-for-Language-Models-A-Suite-of-Black-Box-White-Box-LLM-Judge-and-Ensemble-Scorers" class="headerlink" title="Uncertainty Quantification for Language Models: A Suite of Black-Box,   White-Box, LLM Judge, and Ensemble Scorers"></a>Uncertainty Quantification for Language Models: A Suite of Black-Box,   White-Box, LLM Judge, and Ensemble Scorers</h2><p><strong>Authors:Dylan Bouchard, Mohit Singh Chauhan</strong></p>
<p>Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paperâ€™s companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs. </p>
<blockquote>
<p>å¹»è§‰ï¼ˆhallucinationsï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„é—®é¢˜ã€‚éšç€è¿™äº›æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹æœ‰æ•ˆçš„å¹»è§‰æ£€æµ‹çš„éœ€æ±‚å˜å¾—è‡³å…³é‡è¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¦‚è¿°äº†ä¸€ä¸ªç”¨äºé›¶èµ„æºå¹»è§‰æ£€æµ‹çš„é€šç”¨æ¡†æ¶ï¼Œä»ä¸šè€…å¯ä»¥å°†å…¶åº”ç”¨äºå®é™…åœºæ™¯ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šç§ç°æœ‰çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆUQï¼‰æŠ€æœ¯ï¼ŒåŒ…æ‹¬é»‘ç›’UQã€ç™½ç›’UQå’ŒLLMä½œä¸ºæ³•å®˜ï¼Œæ ¹æ®éœ€è¦å°†å…¶è½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„å“åº”çº§åˆ«ç½®ä¿¡åº¦åˆ†æ•°ï¼ŒèŒƒå›´ä»0åˆ°1ã€‚ä¸ºäº†æé«˜çµæ´»æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯è°ƒæ•´çš„é›†æˆæ–¹æ³•ï¼Œå¯ä»¥èå…¥å„ç§ä¸ªä½“ç½®ä¿¡åº¦åˆ†æ•°ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—ä»ä¸šè€…èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šç”¨ä¾‹ä¼˜åŒ–é›†æˆï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚ä¸ºäº†ç®€åŒ–å®ç°è¿‡ç¨‹ï¼Œæ‰€æœ‰è¯„åˆ†è€…é›†åˆéƒ½åŒ…å«åœ¨æœ¬æ–‡çš„é…å¥—Pythonå·¥å…·åŒ…UQLMä¸­ã€‚ä¸ºäº†è¯„ä¼°å„ç§è¯„åˆ†è€…çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šä¸ªLLMé—®ç­”åŸºå‡†è¿›è¡Œäº†ä¸€ç³»åˆ—å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„å¯è°ƒé›†æˆé€šå¸¸è¶…è¿‡äº†å…¶å„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œå¹¶ä¸”ä¼˜äºç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†ä¸ºæ”¹å–„LLMçš„å‡†ç¡®æ€§å’Œå¯é æ€§è€Œå®šåˆ¶å¹»è§‰æ£€æµ‹ç­–ç•¥çš„å¥½å¤„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.19254v3">PDF</a> UQLM repository: <a target="_blank" rel="noopener" href="https://github.com/cvs-health/uqlm">https://github.com/cvs-health/uqlm</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºé›¶èµ„æºç¯å¢ƒå¹»è§‰æ£€æµ‹çš„é€šç”¨æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”ç°æœ‰çš„ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬é»‘ç›’UQã€ç™½ç›’UQå’ŒLLMä½œä¸ºæ³•å®˜ï¼Œè½¬åŒ–ä¸ºæ ‡å‡†åŒ–çš„å“åº”çº§åˆ«ä¿¡å¿ƒåˆ†æ•°ã€‚æå‡ºä¸€ç§å¯è°ƒé›†æˆæ–¹æ³•ï¼Œç»“åˆå„ç§ä¿¡å¿ƒåˆ†æ•°ï¼Œä»¥ä¼˜åŒ–ç‰¹å®šç”¨ä¾‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯è°ƒé›†æˆé€šå¸¸ä¼˜äºå…¶å•ä¸ªç»„ä»¶å¹¶ä¼˜äºç°æœ‰çš„å¹»è§‰æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsä¸­çš„å¹»è§‰æ£€æµ‹åœ¨åŒ»ç–—ä¿å¥å’Œé‡‘èç­‰é«˜é£é™©é¢†åŸŸå˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§ç”¨äºé›¶èµ„æºå¹»è§‰æ£€æµ‹çš„é€šç”¨æ¡†æ¶ï¼Œé‡‡ç”¨å¤šç§ä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¯è°ƒé›†æˆæ–¹æ³•ï¼Œç»“åˆå„ç§ä¿¡å¿ƒåˆ†æ•°ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬é»‘ç›’UQã€ç™½ç›’UQå’ŒLLMä½œä¸ºæ³•å®˜çš„æŠ€æœ¯ã€‚</li>
<li>å“åº”çº§åˆ«çš„ä¿¡å¿ƒåˆ†æ•°è¢«æ ‡å‡†åŒ–ï¼ŒèŒƒå›´ä»0åˆ°1ã€‚</li>
<li>æä¾›çš„Pythonå·¥å…·åŒ…UQLMç®€åŒ–äº†å®æ–½è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¯è°ƒé›†æˆæ–¹æ³•çš„æ€§èƒ½é€šå¸¸ä¼˜äºå•ä¸ªç»„ä»¶å’Œç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.19254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d38d9f9b1d3a82fb22abd4bd10b393c6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Boosting-Chart-to-Code-Generation-in-MLLM-via-Dual-Preference-Guided-Refinement"><a href="#Boosting-Chart-to-Code-Generation-in-MLLM-via-Dual-Preference-Guided-Refinement" class="headerlink" title="Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided   Refinement"></a>Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided   Refinement</h2><p><strong>Authors:Zhihan Zhang, Yixin Cao, Lizi Liao</strong></p>
<p>Translating chart images into executable plotting scripts-referred to as the chart-to-code generation task-requires Multimodal Large Language Models (MLLMs) to perform fine-grained visual parsing, precise code synthesis, and robust cross-modal reasoning. However, this task is inherently under-constrained: multiple valid code implementations can produce the same visual chart, and evaluation must consider both code correctness and visual fidelity across diverse dimensions. This makes it difficult to learn accurate and generalizable mappings through standard supervised fine-tuning. To address these challenges, we propose a dual preference-guided refinement framework that combines a feedback-driven, dual-modality reward mechanism with iterative preference learning. Our approach introduces a structured variant generation strategy and a visual reward model to efficiently produce high-quality, aspect-aware preference pairs-making preference collection scalable and supervision more targeted. These preferences are used in an offline reinforcement learning setup to optimize the model toward multi-dimensional fidelity. Experimental results show that our framework significantly enhances the performance of general-purpose open-source MLLMs, enabling them to generate high-quality plotting code that rivals specialized chart-centric models and even some proprietary systems. The code and datasets are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Zhihan72/Chart2Code">https://github.com/Zhihan72/Chart2Code</a>. </p>
<blockquote>
<p>å°†å›¾è¡¨å›¾åƒç¿»è¯‘æˆå¯æ‰§è¡Œçš„ç»˜å›¾è„šæœ¬ï¼Œå³æ‰€è°“çš„å›¾è¡¨åˆ°ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œéœ€è¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œç²¾ç»†çš„è§†è§‰è§£æã€ç²¾ç¡®çš„ä»£ç åˆæˆå’Œç¨³å¥çš„è·¨æ¨¡æ€æ¨ç†ã€‚ç„¶è€Œï¼Œè¿™ä¸ªä»»åŠ¡æœ¬è´¨ä¸Šæ˜¯æ¬ çº¦æŸçš„ï¼šå¤šä¸ªæœ‰æ•ˆçš„ä»£ç å®ç°å¯ä»¥äº§ç”Ÿç›¸åŒçš„è§†è§‰å›¾è¡¨ï¼Œè¯„ä¼°å¿…é¡»è€ƒè™‘ä»£ç æ­£ç¡®æ€§å’Œè·¨å¤šä¸ªç»´åº¦çš„è§†è§‰ä¿çœŸåº¦ã€‚è¿™ä½¿å¾—é€šè¿‡æ ‡å‡†ç›‘ç£å¾®è°ƒå­¦ä¹ å‡†ç¡®å’Œé€šç”¨çš„æ˜ å°„å˜å¾—å›°éš¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒåå¥½å¼•å¯¼ä¼˜åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åé¦ˆé©±åŠ¨çš„åŒæ¨¡æ€å¥–åŠ±æœºåˆ¶å’Œè¿­ä»£åå¥½å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ç»“æ„åŒ–å˜ä½“ç”Ÿæˆç­–ç•¥å’Œè§†è§‰å¥–åŠ±æ¨¡å‹ï¼Œä»¥æœ‰æ•ˆåœ°äº§ç”Ÿé«˜è´¨é‡ã€æ–¹é¢æ„ŸçŸ¥çš„åå¥½å¯¹ï¼Œä½¿åå¥½æ”¶é›†å¯æ‰©å±•ï¼Œç›‘ç£æ›´å…·é’ˆå¯¹æ€§ã€‚è¿™äº›åå¥½è¢«ç”¨äºçº¿ä¸‹å¼ºåŒ–å­¦ä¹ è®¾ç½®ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹çš„å¤šç»´ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—æé«˜äº†é€šç”¨å¼€æºMLLMçš„æ€§èƒ½ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ç»˜å›¾ä»£ç ï¼Œä¸ä¸“æ³¨äºå›¾è¡¨çš„æ¨¡å‹ç”šè‡³æŸäº›ä¸“æœ‰ç³»ç»Ÿç›¸ç«äº‰ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Zhihan72/Chart2Code%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Zhihan72/Chart2Codeå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02906v2">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ‰§è¡Œå›¾è¡¨å›¾åƒåˆ°ç»˜å›¾ä»£ç çš„è½¬æ¢ä»»åŠ¡ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªåŒå‘åå¥½å¼•å¯¼ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡åé¦ˆé©±åŠ¨çš„åŒæ¨¡æ€å¥–åŠ±æœºåˆ¶å’Œè¿­ä»£åå¥½å­¦ä¹ æ¥è§£å†³è¿™ä¸€ä»»åŠ¡çš„å†…åœ¨æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶èƒ½é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡ã€é¢å‘æ–¹é¢çš„åå¥½å¯¹ï¼Œä½¿åå¥½æ”¶é›†æ›´å…·å¯æ‰©å±•æ€§ï¼Œç›‘ç£æ›´åŠ æœ‰é’ˆå¯¹æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜äº†é€šç”¨å¼€æºMLLMsçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ç»˜å›¾ä»£ç ï¼Œä¸ä¸“æ³¨äºå›¾è¡¨çš„æ¨¡å‹ç”šè‡³æŸäº›ä¸“æœ‰ç³»ç»Ÿç›¸åª²ç¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¢«ç”¨äºå°†å›¾è¡¨å›¾åƒè½¬æ¢ä¸ºå¯æ‰§è¡Œç»˜å›¾ä»£ç ã€‚</li>
<li>å›¾è¡¨åˆ°ä»£ç çš„ç”Ÿæˆä»»åŠ¡éœ€è¦ç²¾ç»†çš„è§†è§‰è§£æã€ç²¾ç¡®çš„ä»£ç åˆæˆå’Œè·¨æ¨¡æ€æ¨ç†ã€‚</li>
<li>æ­¤ä»»åŠ¡å…·æœ‰å†…åœ¨çš„ä¸ç¡®å®šæ€§ï¼Œå› ä¸ºå¤šä¸ªæœ‰æ•ˆçš„ä»£ç å®ç°å¯ä»¥ç”Ÿæˆç›¸åŒçš„å›¾è¡¨ã€‚</li>
<li>è¯„ä¼°å¿…é¡»åŒæ—¶è€ƒè™‘ä»£ç çš„æ­£ç¡®æ€§å’Œè§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŒå‘åå¥½å¼•å¯¼ä¼˜åŒ–æ¡†æ¶ï¼Œç»“åˆåé¦ˆé©±åŠ¨çš„åŒæ¨¡æ€å¥–åŠ±æœºåˆ¶å’Œè¿­ä»£åå¥½å­¦ä¹ æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>æ¡†æ¶é€šè¿‡ç»“æ„åŒ–å˜ä½“ç”Ÿæˆç­–ç•¥å’Œè§†è§‰å¥–åŠ±æ¨¡å‹æ¥é«˜æ•ˆäº§ç”Ÿé«˜è´¨é‡ã€é¢å‘æ–¹é¢çš„åå¥½å¯¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-361e5d40f6f390e82c48d496637cfb4a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b5b88996aa28767c4fce653d64c1a96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7745ef5f99543ae6ab7a010f4e0632a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41e7879bf00b7b099f279b647e5b417f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="JudgeLRM-Large-Reasoning-Models-as-a-Judge"><a href="#JudgeLRM-Large-Reasoning-Models-as-a-Judge" class="headerlink" title="JudgeLRM: Large Reasoning Models as a Judge"></a>JudgeLRM: Large Reasoning Models as a Judge</h2><p><strong>Authors:Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He</strong></p>
<p>The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°å™¨çš„å…´èµ·ä¸ºäººç±»æ ‡æ³¨æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç„¶è€Œï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸå¾€å¾€è¡¨ç°ä¸è¶³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†LLMè¯„ä¼°å™¨æ˜¯å¦çœŸçš„å—ç›Šäºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¯„ä¼°ä»»åŠ¡ä¸­æ¨ç†éœ€æ±‚çš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç›‘ç£å¾®è°ƒæ€§èƒ½æå‡ä¸éœ€è¦æ¨ç†çš„æ ·æœ¬æ¯”ä¾‹ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³ï¼Œè¿™çªå‡ºäº†ç›‘ç£å¾®è°ƒåœ¨è¿™äº›åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†JudgeLRMï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥åˆ¤æ–­ä¸ºå¯¼å‘çš„LLMå®¶æ—ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä»¥è¯„ä¼°è€…ä¸ºå¯¼å‘ã€ä»¥ç»“æœé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ã€‚JudgeLRMæ¨¡å‹åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šäº†ç›‘ç£å¾®è°ƒæ¨¡å‹å’Œæœ€æ–°çš„æ¨ç†æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒJudgeLRM-3Bè¶…è¶Šäº†GPT-4ï¼Œè€ŒJudgeLRM-7Båœ¨F1åˆ†æ•°ä¸Šè¶…è¶Šäº†DeepSeek-R1è¾¾2.79%ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ·±åº¦æ¨ç†çš„è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00050v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°è€…æä¾›äº†å¯è§„æ¨¡åŒ–æ›¿ä»£äººå·¥æ³¨é‡Šçš„æ–¹æ³•ï¼Œä½†åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å¾€å¾€æ•ˆæœä¸ä½³ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†LLMè¯„ä¼°è€…æ˜¯å¦çœŸæ­£å—ç›Šäºå¢å¼ºçš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¯¹è¯„ä¼°ä»»åŠ¡ä¸­æ¨ç†éœ€æ±‚çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å‘ç°SFTæ€§èƒ½æå‡ä¸éœ€è¦æ¨ç†çš„æ ·æœ¬æ¯”ä¾‹ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³ï¼Œçªæ˜¾äº†SFTåœ¨è¿™äº›åœºæ™¯ä¸­çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†JudgeLRMç³»åˆ—åˆ¤æ–­å¯¼å‘å‹LLMï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒï¼Œä»¥è¯„ä¼°è€…ä¸ºä¸­å¿ƒã€ç»“æœé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ã€‚JudgeLRMæ¨¡å‹åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶ŠSFTè°ƒä¼˜æ¨¡å‹å’Œç°æœ‰å…ˆè¿›æ¨ç†æ¨¡å‹ã€‚ç‰¹åˆ«åœ°ï¼ŒJudgeLRM-3Bè¶…è¶Šäº†GPT-4ï¼Œè€ŒJudgeLRM-7Båœ¨F1åˆ†æ•°ä¸Šè¶…è¶Šäº†DeepSeek-R1è¾¾2.79%ï¼Œåœ¨éœ€è¦æ·±åº¦æ¨ç†çš„è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä½œä¸ºè¯„ä¼°è€…æä¾›äº†å¯è§„æ¨¡åŒ–æ›¿ä»£äººå·¥æ³¨é‡Šçš„æ–¹æ³•ã€‚</li>
<li>åœ¨éœ€è¦å¤æ‚æ¨ç†çš„é¢†åŸŸï¼Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>SFTæ€§èƒ½æå‡ä¸éœ€è¦æ¨ç†çš„æ ·æœ¬æ¯”ä¾‹ä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³ã€‚</li>
<li>JudgeLRMç³»åˆ—æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œä»¥è¯„ä¼°è€…ä¸ºä¸­å¿ƒã€ç»“æœé©±åŠ¨çš„å¥–åŠ±æœºåˆ¶ã€‚</li>
<li>JudgeLRMæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†SFTè°ƒä¼˜æ¨¡å‹å’Œç°æœ‰å…ˆè¿›æ¨ç†æ¨¡å‹ã€‚</li>
<li>JudgeLRM-3Båœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†GPT-4ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0a69b65f9e5086b2f0c21a815cd8c49a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4affed156a5e9a516bdf3fb8c27483c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f23b9fa49cf843e0d5562b5fdc8019dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fad37b911fa36736ec79d216890425d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="On-Fusing-ChatGPT-and-Ensemble-Learning-in-Discon-tinuous-Named-Entity-Recognition-in-Health-Corpora"><a href="#On-Fusing-ChatGPT-and-Ensemble-Learning-in-Discon-tinuous-Named-Entity-Recognition-in-Health-Corpora" class="headerlink" title="On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity   Recognition in Health Corpora"></a>On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity   Recognition in Health Corpora</h2><p><strong>Authors:Tzu-Chieh Chen, Wen-Yang Lin</strong></p>
<p>Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain. </p>
<blockquote>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸€ç›´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­è¯†åˆ«å’Œæå–é‡è¦æœ¯è¯­ã€‚ç„¶è€Œï¼Œå¯¹äºå½“ä»£æ·±åº¦å­¦ä¹ çš„NERæ¨¡å‹æ¥è¯´ï¼Œè¯†åˆ«ä¸è¿ç»­çš„å®ä½“æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æŒ‘æˆ˜ï¼Œè¿™äº›å®ä½“é€šå¸¸åœ¨æ–‡æœ¬ä¸­æ˜¯ç¢ç‰‡åŒ–çš„ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿„ä»Šä¸ºæ­¢ï¼Œå°šæœªæœ‰æ–¹æ³•å°è¯•ä½¿ç”¨é›†æˆå­¦ä¹ æ¥è§£å†³ä¸è¿ç»­å‘½åå®ä½“è¯†åˆ«é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿‘å¹´æ¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰çš„å´›èµ·åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å°†ChatGPTç”¨ä½œé—®é¢˜è§£å†³å·¥å…·ï¼Œè€Œæ²¡æœ‰æ¢ç´¢å…¶åœ¨é›†æˆå­¦ä¹ ç®—æ³•ä¸­ä½œä¸ºæ•´åˆå…ƒç´ çš„å¯èƒ½æ€§ã€‚æœ¬ç ”ç©¶è°ƒæŸ¥äº†ChatGPTä½œä¸ºä»²è£è€…åœ¨é›†æˆæ–¹æ³•ä¸­çš„é›†æˆï¼Œæ—¨åœ¨æé«˜å…¶åœ¨DNERä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†äº”ä¸ªæœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œä½¿ç”¨è‡ªå®šä¹‰æç¤ºå·¥ç¨‹æ¥è¯„ä¼°é›†æˆç®—æ³•çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†åŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸äº”ä¸ªæœ€å…ˆè¿›æ¨¡å‹ã€GPT-3.5å’ŒGPT-4çš„ä¸ªåˆ«åº”ç”¨ä»¥åŠæŠ•ç¥¨é›†æˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„å°†ChatGPTä¸é›†æˆå­¦ä¹ ç®—æ³•ç›¸ç»“åˆçš„æ–¹æ³•åœ¨CADECã€ShARe13å’ŒShARe14æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨å¢å¼ºåŒ»ç–—ä¿å¥é¢†åŸŸNLPåº”ç”¨çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16976v3">PDF</a> 13 pages; a short version named â€œBeyond GPT-NER: ChatGPT as Ensemble   Arbitrator for Discontinuous Named Entity Recognition in Health Corporaâ€ has   been accpeted for presentation at MedInfo2025</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢ç´¢äº†å°†ChatGPTä½œä¸ºé›†æˆå­¦ä¹ ç®—æ³•ä¸­çš„ä»²è£è€…ï¼Œä»¥æé«˜åœ¨æ–­æ–­ç»­å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ç»“åˆäº†äº”ç§æœ€å…ˆè¿›çš„NERæ¨¡å‹ä¸ChatGPTï¼Œé€šè¿‡è‡ªå®šä¹‰æç¤ºå·¥ç¨‹æ¥è¯„ä¼°é›†æˆç®—æ³•çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥èåˆæ–¹æ³•åœ¨ä¸‰ä¸ªåŒ»ç–—åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºå•ç‹¬æ¨¡å‹åŠé›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå±•ç¤ºå…¶åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>NERæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®ä»»åŠ¡ï¼Œæ—¨åœ¨ä»éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ä¸­è¯†åˆ«å’Œæå–é‡è¦æœ¯è¯­ã€‚</li>
<li>æ–­æ–­ç»­ç»­å‘½åå®ä½“è¯†åˆ«ï¼ˆDNERï¼‰æ˜¯å½“ä»£æ·±åº¦å­¦ä¹ NERæ¨¡å‹é¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨è¯†åˆ«è¢«æ–‡æœ¬ä¸­æ–­çš„å®ä½“æ—¶ã€‚</li>
<li>é›†æˆå­¦ä¹ åœ¨è§£å†³DNERé—®é¢˜æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ChatGPTç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶ç»“åˆäº”ç§æœ€å…ˆè¿›NERæ¨¡å‹ä¸ChatGPTï¼Œåˆ©ç”¨è‡ªå®šä¹‰æç¤ºå·¥ç¨‹æ¥æé«˜é›†æˆç®—æ³•çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒèåˆChatGPTçš„é›†æˆå­¦ä¹ ç®—æ³•åœ¨ä¸‰ä¸ªåŒ»ç–—åŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå•ç‹¬æ¨¡å‹å’ŒæŠ•ç¥¨é›†æˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-477def4275c8323142435da7a5ab6f20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5222cd38bf98268e1acae0a50ca59a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7d4f5fd2ddbbc3c743755dc1283a155.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c2e43ec4832a7061528d472d82437f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc64ea5c2d80a69eeb6499ab523003b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c762c332ad6f28bad1e4d4f2994d82dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6da0d74aadc638116f422397c32a4da7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ff11fe6129cfd9ee5e2a8ddc2ca2b67.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness"><a href="#Mastering-Collaborative-Multi-modal-Data-Selection-A-Focus-on-Informativeness-Uniqueness-and-Representativeness" class="headerlink" title="Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness"></a>Mastering Collaborative Multi-modal Data Selection: A Focus on   Informativeness, Uniqueness, and Representativeness</h2><p><strong>Authors:Qifan Yu, Zhebei Shen, Zhongqi Yue, Yang Wu, Bosheng Qin, Wenqiao Zhang, Yunfei Li, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Instruction tuning fine-tunes pre-trained Multi-modal Large Language Models (MLLMs) to handle real-world tasks. However, the rapid expansion of visual instruction datasets introduces data redundancy, leading to excessive computational costs. We propose a collaborative framework, DataTailor, which leverages three key principlesâ€“informativeness, uniqueness, and representativenessâ€“for effective data selection. We argue that a valuable sample should be informative of the task, non-redundant, and represent the sample distribution (i.e., not an outlier). We further propose practical ways to score against each principle, which automatically adapts to a given dataset without tedious hyperparameter tuning. Comprehensive experiments on various benchmarks demonstrate that DataTailor achieves 101.3% of the performance of full-data fine-tuning with only 15% of the data, significantly reducing computational costs while maintaining superior results. This exemplifies the â€œLess is Moreâ€ philosophy in MLLM development. The code and data is available in this \href{<a target="_blank" rel="noopener" href="https://github.com/Yuqifan1117/DataTailor%7D%7BURL%7D">https://github.com/Yuqifan1117/DataTailor}{URL}</a>. </p>
<blockquote>
<p>æŒ‡ä»¤å¾®è°ƒå¯¹é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œå¾®è°ƒä»¥å¤„ç†ç°å®ä¸–ç•Œä»»åŠ¡ã€‚ç„¶è€Œï¼Œè§†è§‰æŒ‡ä»¤æ•°æ®é›†çš„å¿«é€Ÿæ‰©å¼ å¯¼è‡´äº†æ•°æ®å†—ä½™ï¼Œä»è€Œå¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä½œæ¡†æ¶DataTailorï¼Œå®ƒåˆ©ç”¨ä¸‰ä¸ªå…³é”®åŸåˆ™â€”â€”ä¿¡æ¯æ€§ã€å”¯ä¸€æ€§å’Œä»£è¡¨æ€§â€”â€”è¿›è¡Œæœ‰æ•ˆçš„æ•°æ®é€‰æ‹©ã€‚æˆ‘ä»¬è®¤ä¸ºæœ‰ä»·å€¼çš„æ ·æœ¬åº”è¯¥å…·æœ‰ä»»åŠ¡çš„ä¿¡æ¯æ€§ã€éå†—ä½™æ€§ï¼Œå¹¶ä»£è¡¨æ ·æœ¬åˆ†å¸ƒï¼ˆå³ä¸æ˜¯å¼‚å¸¸å€¼ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†é’ˆå¯¹æ¯æ¡åŸåˆ™è¿›è¡Œè¯„åˆ†çš„å®é™…æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥è‡ªåŠ¨é€‚åº”ç»™å®šçš„æ•°æ®é›†ï¼Œè€Œæ— éœ€ç¹ççš„è¶…å‚æ•°è°ƒæ•´ã€‚åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒDataTailorä»…ä½¿ç”¨15%çš„æ•°æ®å°±å®ç°äº†å…¨æ•°æ®å¾®è°ƒæ€§èƒ½çš„101.3%ï¼Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒäº†å“è¶Šçš„ç»“æœã€‚è¿™ä½“ç°äº†MLLMå‘å±•ä¸­çš„â€œå°‘å³æ˜¯å¤šâ€ç†å¿µã€‚ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€ä¸­æ‰¾åˆ°ï¼š[URL]ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Yuqifan1117/DataTailor%EF%BC%89%E3%80%82">https://github.com/Yuqifan1117/DataTailorï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06293v2">PDF</a> ICCV 2025 Highlight</p>
<p><strong>Summary</strong><br>     æ•°æ®å†—ä½™ä½¿å¾—å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤è°ƒä¼˜é¢ä¸´è®¡ç®—æˆæœ¬è¿‡é«˜çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä½œæ¡†æ¶DataTailorï¼Œåˆ©ç”¨ä¿¡æ¯æ€§ã€ç‹¬ç‰¹æ€§å’Œä»£è¡¨æ€§ä¸‰ä¸ªå…³é”®åŸåˆ™è¿›è¡Œé«˜æ•ˆæ•°æ®é€‰æ‹©ã€‚DataTailorèƒ½åœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ç»´æŒæ¨¡å‹æ€§èƒ½ï¼Œå®ç°â€œå°‘å³æ˜¯å¤šâ€çš„ç†å¿µã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ•°æ®å†—ä½™å¯¼è‡´å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è®¡ç®—æˆæœ¬æ¿€å¢ã€‚è¿™æˆä¸ºäº†å¯¹ç°å®ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒçš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºçš„DataTailoræ¡†æ¶åŸºäºä¸‰ä¸ªåŸåˆ™ï¼šä¿¡æ¯æ€§ã€ç‹¬ç‰¹æ€§å’Œä»£è¡¨æ€§æ¥é€‰æ‹©æ•°æ®æ ·æœ¬ï¼Œä»¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å¹¶é™ä½è®¡ç®—æˆæœ¬ã€‚è¿™ä¸‰ä¸ªåŸåˆ™ç¡®ä¿æ‰€é€‰æ ·æœ¬åŒ…å«æœ‰å…³ä»»åŠ¡çš„ä¿¡æ¯é‡ï¼Œå¹¶ä¸”éå†—ä½™ä¸”å…·æœ‰ä»£è¡¨æ€§ã€‚</li>
<li>DataTailorè‡ªåŠ¨é€‚åº”ç»™å®šæ•°æ®é›†ï¼Œæ— éœ€ç¹ççš„è¶…å‚æ•°è°ƒæ•´ã€‚è¿™ä¸€ç‰¹æ€§ç®€åŒ–äº†æ•°æ®å¤„ç†è¿‡ç¨‹å¹¶æé«˜äº†æ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06293">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4d3b226d93d1e5ffb84962c0bf0ebb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2350cd77319064cefa40374e80fedd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46d2a0b9d71138d82f217371311b9f2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f518afee771672b5cf5ba7ffbeaa14d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-539753372556623e2e1327da2e82c54c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="WSI-LLaVA-A-Multimodal-Large-Language-Model-for-Whole-Slide-Image"><a href="#WSI-LLaVA-A-Multimodal-Large-Language-Model-for-Whole-Slide-Image" class="headerlink" title="WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image"></a>WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image</h2><p><strong>Authors:Yuci Liang, Xinheng Lyu, Wenting Chen, Meidan Ding, Jipeng Zhang, Xiangjian He, Song Wu, Xiaohan Xing, Sen Yang, Xiyue Wang, Linlin Shen</strong></p>
<p>Recent advancements in computational pathology have produced patch-level Multi-modal Large Language Models (MLLMs), but these models are limited by their inability to analyze whole slide images (WSIs) comprehensively and their tendency to bypass crucial morphological features that pathologists rely on for diagnosis. To address these challenges, we first introduce WSI-Bench, a large-scale morphology-aware benchmark containing 180k VQA pairs from 9,850 WSIs across 30 cancer types, designed to evaluate MLLMsâ€™ understanding of morphological characteristics crucial for accurate diagnosis. Building upon this benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI understanding that employs a three-stage training approach: WSI-text alignment, feature space alignment, and task-specific instruction tuning. To better assess model performance in pathological contexts, we develop two specialized WSI metrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that WSI-LLaVA outperforms existing models across all capability dimensions, with a significant improvement in morphological analysis, establishing a clear correlation between morphological understanding and diagnostic accuracy. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„æœ€æ–°è¿›å±•å·²ç»äº§ç”Ÿäº†è¡¥ä¸çº§åˆ«çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä½†è¿™äº›æ¨¡å‹å—åˆ°æ— æ³•å…¨é¢åˆ†æå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰çš„å±€é™ï¼Œå¹¶ä¸”å€¾å‘äºç»•è¿‡ç—…ç†å­¦å®¶ä¾èµ–çš„ç”¨äºè¯Šæ–­çš„å…³é”®å½¢æ€ç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†WSI-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å½¢æ€æ„ŸçŸ¥åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ¥è‡ª9850å¼ å¹»ç¯ç‰‡å›¾åƒçš„18ä¸‡å¯¹é—®ç­”ï¼ˆVQAï¼‰ï¼Œæ¶µç›–30ç§ç™Œç—‡ç±»å‹ï¼Œæ—¨åœ¨è¯„ä¼°MLLMså¯¹å½¢æ€ç‰¹å¾çš„å‡†ç¡®ç†è§£ï¼Œè¿™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ã€‚åŸºäºæ­¤åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†WSI-LLaVAï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºåƒå…†åƒç´ WSIç†è§£çš„å…¨æ–°æ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼šWSIæ–‡æœ¬å¯¹é½ã€ç‰¹å¾ç©ºé—´å¯¹é½å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ã€‚ä¸ºäº†æ›´å¥½åœ°è¯„ä¼°æ¨¡å‹åœ¨ç—…ç†èƒŒæ™¯ä¸‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªä¸“é—¨çš„WSIæŒ‡æ ‡ï¼šWSIç²¾ç¡®åº¦å’ŒWSIç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWSI-LLaVAåœ¨æ‰€æœ‰èƒ½åŠ›ç»´åº¦ä¸Šéƒ½ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåœ¨å½¢æ€åˆ†ææ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå»ºç«‹äº†å½¢æ€ç†è§£ä¸è¯Šæ–­å‡†ç¡®æ€§ä¹‹é—´çš„æ˜ç¡®ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.02141v5">PDF</a> ICCV 2025, 38 pages, 22 figures, 35 tables</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè®¡ç®—ç—…ç†å­¦é¢†åŸŸçš„è¿›å±•æ¨åŠ¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‘å—å±‚é¢çš„åº”ç”¨ï¼Œä½†å…¶åœ¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰åˆ†ææ–¹é¢çš„ç»¼åˆèƒ½åŠ›å’Œå¯¹å…³é”®å½¢æ€ç‰¹å¾çš„è¯†åˆ«ä¸Šä»æœ‰ä¸è¶³ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†WSI-BenchåŸºå‡†æµ‹è¯•é›†ï¼Œå¹¶åŸºäºè¯¥åŸºå‡†æµ‹è¯•é›†æå‡ºWSI-LLaVAæ¡†æ¶ï¼Œç”¨äºç†è§£å‰åƒç´ ï¼ˆgigapixelï¼‰çº§åˆ«çš„WSIã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬WSIæ–‡æœ¬å¯¹é½ã€ç‰¹å¾ç©ºé—´å¯¹é½å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸¤ä¸ªé’ˆå¯¹WSIçš„ä¸“ä¸šè¯„ä¼°æŒ‡æ ‡ï¼šWSIç²¾åº¦å’ŒWSIç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWSI-LLaVAåœ¨å„æ–¹é¢æ€§èƒ½å‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå½¢æ€åˆ†ææ˜¾è‘—æ”¹å–„ï¼Œå¹¶å»ºç«‹äº†å½¢æ€ç†è§£ä¸è¯Šæ–­å‡†ç¡®æ€§ä¹‹é—´çš„æ˜ç¡®å…³è”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦é¢†åŸŸå‘å±•å‡ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œä½†å…¶åœ¨å…¨åˆ‡ç‰‡å›¾åƒåˆ†ææ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥WSI-BenchåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«æ¥è‡ª9850ä¸ªWSIsçš„18ä¸‡é—®ç­”å¯¹ï¼Œæ—¨åœ¨è¯„ä¼°MLLMså¯¹å…³é”®å½¢æ€ç‰¹å¾çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>æå‡ºWSI-LLaVAæ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæ–¹æ³•è¿›è¡Œå…¨åˆ‡ç‰‡å›¾åƒç†è§£ã€‚åŒ…æ‹¬WSIæ–‡æœ¬å¯¹é½ã€ç‰¹å¾ç©ºé—´å¯¹é½å’Œä»»åŠ¡ç‰¹å®šæŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>å¼€å‘ä¸¤ä¸ªé’ˆå¯¹å…¨åˆ‡ç‰‡å›¾åƒçš„è¯„ä¼°æŒ‡æ ‡ï¼šWSIç²¾åº¦å’ŒWSIç›¸å…³æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒWSI-LLaVAæ¡†æ¶åœ¨å½¢æ€åˆ†ææ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.02141">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad874326368fd345fa99229530da1c5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb28db29e76d6685187aea3aea3428fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e89f945a4cced96220b5ef2bda4abb90.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-caa8b63272b4de47beec9e6e7fff7408.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a843eaefd0f146f33a11a106abf8ef6c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b36b540912b3009c465b093cccc012dd.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  Challenges and Opportunities for Participatory Design of Conversational   Agents for Young People's Wellbeing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-e93679d38feb0e25a65e0ea77deb8081.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  MedReseacher-R1 Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
