<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  MedReseacher-R1 Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-e93679d38feb0e25a65e0ea77deb8081.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-22
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-22-æ›´æ–°"><a href="#2025-08-22-æ›´æ–°" class="headerlink" title="2025-08-22 æ›´æ–°"></a>2025-08-22 æ›´æ–°</h1><h2 id="MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework"><a href="#MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework" class="headerlink" title="MedReseacher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework"></a>MedReseacher-R1: Expert-Level Medical Deep Researcher via A   Knowledge-Informed Trajectory Synthesis Framework</h2><p><strong>Authors:Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</strong></p>
<p>Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts.We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains. </p>
<blockquote>
<p>æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†äººçš„å‘å±•æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„è·¨å¤šä¸ªé¢†åŸŸçš„èƒ½åŠ›ï¼Œä»¥æ·±åº¦ç ”ç©¶ç³»ç»Ÿä¸ºä¾‹ï¼Œå®ƒä»¬åœ¨å¤æ‚çš„ä¿¡æ¯æ£€ç´¢å’Œç»¼åˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è™½ç„¶é€šç”¨æ·±åº¦ç ”ç©¶ä»£ç†äººåœ¨è®¸å¤šé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»–ä»¬åœ¨åŒ»å­¦é¢†åŸŸé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œé¢†å…ˆçš„ä¸“æœ‰ç³»ç»Ÿåœ¨æœ€å…ˆè¿›çš„åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æœ‰é™ï¼Œè¯æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸»è¦é™åˆ¶å› ç´ æœ‰ï¼šï¼ˆ1ï¼‰æ¨¡å‹ç¼ºä¹ç”¨äºä¸´åºŠæ¨ç†çš„å……è¶³å¯†é›†åŒ»å­¦çŸ¥è¯†ï¼›ï¼ˆ2ï¼‰æ¡†æ¶å—åˆ°ç¼ºä¹é’ˆå¯¹åŒ»å­¦ä¸Šä¸‹æ–‡å®šåˆ¶çš„ä¸“ç”¨æ£€ç´¢å·¥å…·çš„åˆ¶çº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ»å­¦æ·±åº¦ç ”ç©¶ä»£ç†äººï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨åŒ»å­¦çŸ¥è¯†å›¾è°±å¼€å‘äº†ä¸€ç§æ–°å‹æ•°æ®åˆæˆæ¡†æ¶ï¼Œä»å›´ç»•ç½•è§åŒ»å­¦å®ä½“çš„å­å›¾ä¸­æå–æœ€é•¿çš„é“¾æ¥ç”Ÿæˆå¤æ‚çš„å¤šè·³é—®ç­”å¯¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†è‡ªå®šä¹‰çš„ç§æœ‰åŒ»å­¦æ£€ç´¢å¼•æ“ä¸é€šç”¨å·¥å…·é›†æˆåœ¨ä¸€èµ·ï¼Œä»¥å®ç°å‡†ç¡®åŒ»å­¦ä¿¡æ¯çš„ç»¼åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨12ä¸ªåŒ»å­¦ä¸“ä¸šé¢†åŸŸç”Ÿæˆäº†è¶…è¿‡2100æ¡ä¸åŒçš„è½¨è¿¹ï¼Œæ¯ä¸ªè½¨è¿¹å¹³å‡äº¤äº’4.2ä¸ªå·¥å…·ã€‚é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒçš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ ä»¥åŠå¤åˆå¥–åŠ±ï¼Œæˆ‘ä»¬çš„MedResearcher-R1-32Bæ¨¡å‹åœ¨åŒ»å­¦åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œå»ºç«‹äº†æ–°çš„æœ€æ–°ç»“æœï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬æ·±åº¦ç ”ç©¶ä»»åŠ¡ä¸Šä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼Œåœ¨æ¶æ„ã€å·¥å…·è®¾è®¡å’Œè®­ç»ƒæ•°æ®æ„å»ºæ–¹é¢çš„æˆ˜ç•¥é¢†åŸŸç‰¹å®šåˆ›æ–°ï¼Œå¯ä»¥ä½¿è¾ƒå°çš„å¼€æºæ¨¡å‹åœ¨ä¸“é—¨é¢†åŸŸä¸­è¶…è¶Šæ›´å¤§çš„ä¸“æœ‰ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14880v1">PDF</a> 13 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡åˆ›æ–°çš„æ•°æ®åˆæˆæ¡†æ¶å’Œå®šåˆ¶åŒ»ç–—æ£€ç´¢å¼•æ“ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒ»ç–—æ·±åº¦ç ”ç©¶ä»£ç†äººæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥ä»£ç†äººç”Ÿæˆäº†è¶…è¿‡2100æ¡è·¨è¶Š12ä¸ªåŒ»å­¦ä¸“ä¸šçš„ä¸åŒè½¨è¿¹ï¼Œå¹¶é€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œå®ç°äº†åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šçš„å“è¶Šè¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>LLMåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åŒ»ç–—æ·±åº¦ç ”ç©¶ä»£ç†äººé€šè¿‡æ•°æ®åˆæˆæ¡†æ¶å’Œå®šåˆ¶åŒ»ç–—æ£€ç´¢å¼•æ“è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ä»£ç†äººç”Ÿæˆäº†è¶…è¿‡2100æ¡åŒ»å­¦ä¸“ä¸šçš„ä¸åŒè½¨è¿¹ã€‚</li>
<li>è¯¥ä»£ç†äººç»“åˆç›‘ç£å¾®è°ƒä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¸¤é˜¶æ®µè®­ç»ƒã€‚</li>
<li>MedResearcher-R1-32Bæ¨¡å‹åœ¨åŒ»ç–—åŸºå‡†æµ‹è¯•ä¸Šå®ç°å“è¶Šè¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå¯¹ä¸€èˆ¬æ·±åº¦ç ”ç©¶ä»»åŠ¡çš„ç«äº‰åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-145333c4ecb400359c97d17f875ee0b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1601c553312993feb55ef505b074d85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e93679d38feb0e25a65e0ea77deb8081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cff4303881eefc0dbb6dd240bfb6871.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent"><a href="#Universal-and-Transferable-Adversarial-Attack-on-Large-Language-Models-Using-Exponentiated-Gradient-Descent" class="headerlink" title="Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent"></a>Universal and Transferable Adversarial Attack on Large Language Models   Using Exponentiated Gradient Descent</h2><p><strong>Authors:Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu</strong></p>
<p>As large language models (LLMs) are increasingly deployed in critical applications, ensuring their robustness and safety alignment remains a major challenge. Despite the overall success of alignment techniques such as reinforcement learning from human feedback (RLHF) on typical prompts, LLMs remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers appended to user prompts. Most existing jailbreak methods either rely on inefficient searches over discrete token spaces or direct optimization of continuous embeddings. While continuous embeddings can be given directly to selected open-source models as input, doing so is not feasible for proprietary models. On the other hand, projecting these embeddings back into valid discrete tokens introduces additional complexity and often reduces attack effectiveness. We propose an intrinsic optimization method which directly optimizes relaxed one-hot encodings of the adversarial suffix tokens using exponentiated gradient descent coupled with Bregman projection, ensuring that the optimized one-hot encoding of each token always remains within the probability simplex. We provide theoretical proof of convergence for our proposed method and implement an efficient algorithm that effectively jailbreaks several widely used LLMs. Our method achieves higher success rates and faster convergence compared to three state-of-the-art baselines, evaluated on five open-source LLMs and four adversarial behavior datasets curated for evaluating jailbreak methods. In addition to individual prompt attacks, we also generate universal adversarial suffixes effective across multiple prompts and demonstrate transferability of optimized suffixes to different LLMs. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œç¡®ä¿å…¶é²æ£’æ€§å’Œå®‰å…¨å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰å¯¹é½æŠ€æœ¯åœ¨å…¸å‹æç¤ºä¸Šçš„æ€»ä½“æˆåŠŸï¼Œä½†LLMä»å®¹æ˜“å—åˆ°é€šè¿‡ç”¨æˆ·æç¤ºé™„åŠ çš„ç²¾å¿ƒåˆ¶ä½œçš„å¯¹æŠ—æ€§è§¦å‘è€Œå¯åŠ¨çš„è¶Šç‹±æ”»å‡»ï¼ˆjailbreak attacksï¼‰çš„å½±å“ã€‚ç°æœ‰çš„å¤§å¤šæ•°è¶Šç‹±æ–¹æ³•è¦ä¹ˆä¾èµ–äºç¦»æ•£ä»¤ç‰Œç©ºé—´ä¸Šçš„ä½æ•ˆæœç´¢ï¼Œè¦ä¹ˆç›´æ¥ä¼˜åŒ–è¿ç»­åµŒå…¥ã€‚è™½ç„¶å¯ä»¥å°†è¿™äº›è¿ç»­åµŒå…¥ç›´æ¥ä½œä¸ºé€‰å®šå¼€æºæ¨¡å‹çš„è¾“å…¥ï¼Œä½†å¯¹äºä¸“æœ‰æ¨¡å‹è¿™æ ·åšå¹¶ä¸å¯è¡Œã€‚å¦ä¸€æ–¹é¢ï¼Œå°†è¿™äº›åµŒå…¥æŠ•å½±å›æœ‰æ•ˆçš„ç¦»æ•£ä»¤ç‰Œå¼•å…¥äº†é¢å¤–çš„å¤æ‚æ€§ï¼Œå¹¶ä¸”å¾€å¾€ä¼šé™ä½æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å†…åœ¨çš„ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥ä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™æ³•ç»“åˆBregmanæŠ•å½±ä¼˜åŒ–å¯¹æŠ—æ€§åç¼€ä»¤ç‰Œçš„æ¾å¼›ç‹¬çƒ­ç¼–ç ï¼Œç¡®ä¿æ¯ä¸ªä»¤ç‰Œä¼˜åŒ–åçš„ç‹¬çƒ­ç¼–ç å§‹ç»ˆä¿æŒåœ¨æ¦‚ç‡å•çº¯å½¢å†…ã€‚æˆ‘ä»¬ä¸ºæ‰€æå‡ºçš„æ–¹æ³•æä¾›äº†æ”¶æ•›æ€§çš„ç†è®ºè¯æ˜ï¼Œå¹¶å®ç°äº†ä¸€ç§æœ‰æ•ˆçš„ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°çªç ´å‡ ç§å¹¿æ³›ä½¿ç”¨çš„LLMã€‚ä¸ä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ä¸ªå¼€æºLLMå’Œå››ä¸ªç”¨äºè¯„ä¼°è¶Šç‹±æ–¹æ³•çš„å¯¹æŠ—è¡Œä¸ºæ•°æ®é›†ä¸Šå®ç°äº†æ›´é«˜çš„æˆåŠŸç‡å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚é™¤äº†å•ä¸ªæç¤ºæ”»å‡»ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç”Ÿæˆäº†è·¨å¤šä¸ªæç¤ºæœ‰æ•ˆçš„é€šç”¨å¯¹æŠ—æ€§åç¼€ï¼Œå¹¶è¯æ˜äº†ä¼˜åŒ–åç¼€åœ¨ä¸åŒLLMä¹‹é—´çš„å¯è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²æ—¥ç›Šå¢åŠ ï¼Œä½†å…¶é²æ£’æ€§å’Œå®‰å…¨å¯¹é½æ€§ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚å°½ç®¡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰å¯¹é½æŠ€æœ¯åœ¨å…¸å‹æç¤ºä¸Šå–å¾—äº†æ€»ä½“æˆåŠŸï¼Œä½†LLMsä»å®¹æ˜“é­å—ç”±ç‰¹å®šæ¶æ„è§¦å‘è¯è§¦å‘çš„è¶Šç‹±æ”»å‡»ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹LLMsçš„å†…åœ¨ä¼˜åŒ–æ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–æ¾å¼›çš„one-hotç¼–ç å¯¹æŠ—åç¼€ä»¤ç‰Œï¼Œä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™ä¸å¸ƒé›·æ ¼æ›¼æŠ•å½±ç›¸ç»“åˆçš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡å¹¶å®ç°äº†å¿«é€Ÿæ”¶æ•›ï¼Œä¸ä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ï¼Œå¹¶åœ¨äº”ä¸ªå¼€æºLLMså’Œå››ä¸ªè¶Šç‹±æ–¹æ³•è¯„ä¼°çš„å¯¹æŠ—è¡Œä¸ºæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚é™¤äº†é’ˆå¯¹ä¸ªåˆ«æç¤ºçš„æ”»å‡»å¤–ï¼Œæˆ‘ä»¬è¿˜ç”Ÿæˆäº†æœ‰æ•ˆçš„è·¨å¤šä¸ªæç¤ºçš„é€šç”¨å¯¹æŠ—åç¼€ï¼Œå¹¶è¯æ˜äº†ä¼˜åŒ–åç¼€åœ¨ä¸åŒLLMsä¹‹é—´çš„å¯è½¬ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…³é”®åº”ç”¨ä¸­çš„éƒ¨ç½²é¢ä¸´é²æ£’æ€§å’Œå®‰å…¨æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å°½ç®¡æœ‰å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰å¯¹é½æŠ€æœ¯ï¼ŒLLMsä»æ˜“å—ç‰¹å®šæ¶æ„è§¦å‘è¯è§¦å‘çš„è¶Šç‹±æ”»å‡»ã€‚</li>
<li>ç°æœ‰è¶Šç‹±æ–¹æ³•åŒ…æ‹¬åœ¨ç¦»æ•£ä»¤ç‰Œç©ºé—´ä¸Šçš„ä½æ•ˆæœç´¢æˆ–ç›´æ¥ä¼˜åŒ–è¿ç»­åµŒå…¥ï¼Œä½†å¯¹äºä¸“æœ‰æ¨¡å‹ä¸å¯è¡Œã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å†…åœ¨ä¼˜åŒ–æ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–æ¾å¼›çš„one-hotç¼–ç å¯¹æŠ—åç¼€ä»¤ç‰Œï¼Œä½¿ç”¨æŒ‡æ•°æ¢¯åº¦ä¸‹é™ä¸å¸ƒé›·æ ¼æ›¼æŠ•å½±ç»“åˆã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†è¾ƒé«˜çš„æˆåŠŸç‡å¹¶å¿«é€Ÿæ”¶æ•›ï¼Œä¼˜äºä¸‰ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åœ¨äº”ä¸ªå¼€æºLLMså’Œå››ä¸ªè¶Šç‹±æ–¹æ³•è¯„ä¼°çš„å¯¹æŠ—è¡Œä¸ºæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03ece2ac5ecc22a45f090986029199a1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs"><a href="#Evaluating-Retrieval-Augmented-Generation-vs-Long-Context-Input-for-Clinical-Reasoning-over-EHRs" class="headerlink" title="Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs"></a>Evaluating Retrieval-Augmented Generation vs. Long-Context Input for   Clinical Reasoning over EHRs</h2><p><strong>Authors:Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar</strong></p>
<p>Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art modelsâ€™ extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the modelsâ€™ full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰å†…å®¹å†—é•¿ã€ç¹æ‚ï¼Œå¯¹ä¸´åºŠåŒ»ç”Ÿæ¥è¯´æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿé€šè¿‡æå–å’Œæ¨ç†è¿™äº›éç»“æ„åŒ–æ–‡æœ¬æ¥è¿›è¡Œå¤„ç†ã€‚ç„¶è€Œï¼Œä¸´åºŠç¬”è®°çš„é•¿åº¦å¾€å¾€è¶…å‡ºäº†æœ€å…ˆè¿›æ¨¡å‹çš„æ‰©å±•ä¸Šä¸‹æ–‡çª—å£ã€‚é€šè¿‡ä»æ•´ä¸ªç”µå­å¥åº·è®°å½•ä¸­æ£€ç´¢ä»»åŠ¡ç›¸å…³æ®µè½ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æä¾›äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥æ½œåœ¨åœ°å‡å°‘æ‰€éœ€è¾“å…¥æ ‡è®°çš„æ•°é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰é¡¹å¯åœ¨å¥åº·ç³»ç»Ÿä¸­è½»æ¾å¤åˆ¶çš„ä¸´åºŠä»»åŠ¡ï¼š1ï¼‰æå–æˆåƒç¨‹åºï¼Œ2ï¼‰ç”ŸæˆæŠ—ç”Ÿç´ ä½¿ç”¨çš„æ—¶é—´çº¿ï¼Œä»¥åŠ3ï¼‰è¯†åˆ«å…³é”®è¯Šæ–­ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªå®é™…ä½é™¢æ‚£è€…çš„ç”µå­å¥åº·è®°å½•è¿›è¡Œæµ‹è¯•ï¼Œæµ‹è¯•äº†ä¸‰ç§æœ€å…ˆè¿›çš„LLMsï¼Œå®ƒä»¬æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å„ä¸ç›¸åŒï¼Œè¦ä¹ˆé‡‡ç”¨æœ‰é’ˆå¯¹æ€§çš„æ–‡æœ¬æ£€ç´¢ï¼Œè¦ä¹ˆé‡‡ç”¨æœ€æ–°çš„ä¸´åºŠç¬”è®°ã€‚æˆ‘ä»¬å‘ç°ï¼ŒRAGçš„è¡¨ç°åœ¨ä½¿ç”¨æœ€æ–°ç¬”è®°çš„æƒ…å†µä¸‹éå¸¸æ¥è¿‘æˆ–è¶…è¿‡å…¶è¡¨ç°ï¼Œå¹¶ä¸”åœ¨éœ€è¦å¤§å¤§å‡å°‘è¾“å…¥æ ‡è®°çš„æƒ…å†µä¸‹æ¥è¿‘ä½¿ç”¨æ¨¡å‹å…¨è¯­å¢ƒçš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ–°æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¶Šæ¥è¶Šé•¿çš„æ–‡æœ¬æ—¶ï¼ŒRAGä»ç„¶æ˜¯ä¸€ç§å…·æœ‰ç«äº‰åŠ›å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14817v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰å†…å®¹å†—é•¿ã€ä¿¡æ¯ç¹æ‚ï¼Œç»™ä¸´åºŠåŒ»ç”Ÿå¸¦æ¥å¾ˆå¤§æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†å¯èƒ½æ–¹æ¡ˆï¼Œä½†ä¸´åºŠç¬”è®°çš„é•¿åº¦å¸¸å¸¸è¶…å‡ºè¿™äº›æ¨¡å‹çš„è¯­å¢ƒçª—å£é™åˆ¶ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡ä»æ•´ä¸ªEHRä¸­æ£€ç´¢ä»»åŠ¡ç›¸å…³æ®µè½ï¼Œå¯èƒ½å‡å°‘æ‰€éœ€è¾“å…¥æ ‡è®°çš„æ•°é‡ã€‚æœ¬ç ”ç©¶æå‡ºä¸‰é¡¹å¯åœ¨å„åŒ»ç–—ç³»ç»Ÿä¸­è½»æ¾å¤åˆ¶çš„ä¸´åºŠä»»åŠ¡ï¼š1ï¼‰æå–æˆåƒç¨‹åºï¼›2ï¼‰ç”ŸæˆæŠ—ç”Ÿç´ ä½¿ç”¨çš„æ—¶é—´çº¿ï¼›3ï¼‰è¯†åˆ«å…³é”®è¯Šæ–­ã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸‰ç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä¸åŒè¯­å¢ƒé‡ã€é’ˆå¯¹æ€§æ–‡æœ¬æ£€ç´¢æˆ–æœ€æ–°ä¸´åºŠç¬”è®°è¿›è¡Œè¯•éªŒã€‚ç»“æœå‘ç°ï¼ŒRAGçš„è¡¨ç°æ¥è¿‘æˆ–è¶…è¶Šäº†ä½¿ç”¨æœ€æ–°ç¬”è®°çš„æ•ˆæœï¼Œä¸”èƒ½åœ¨æ›´å°‘çš„è¾“å…¥æ ‡è®°ä¸‹æ¥è¿‘ä½¿ç”¨å…¨è¯­å¢ƒæ¨¡å‹çš„æ•ˆæœã€‚è¿™è¡¨æ˜å³ä½¿åœ¨å¤„ç†æ›´é•¿çš„æ–‡æœ¬æ—¶ï¼ŒRAGä»ç„¶æ˜¯ä¸€ç§æœ‰ç«äº‰åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰å†…å®¹å†—é•¿ã€ç¹æ‚ï¼Œç»™ä¸´åºŠåŒ»ç”Ÿå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†ä¸´åºŠç¬”è®°é•¿åº¦è¶…å‡ºæ¨¡å‹è¯­å¢ƒçª—å£é™åˆ¶ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•é€šè¿‡æ£€ç´¢ä»»åŠ¡ç›¸å…³æ®µè½å‡å°‘è¾“å…¥æ ‡è®°éœ€æ±‚ã€‚</li>
<li>ä¸‰é¡¹ä¸´åºŠä»»åŠ¡åŒ…æ‹¬æå–æˆåƒç¨‹åºã€ç”ŸæˆæŠ—ç”Ÿç´ ä½¿ç”¨çš„æ—¶é—´çº¿ã€è¯†åˆ«å…³é”®è¯Šæ–­ã€‚</li>
<li>RAGè¡¨ç°æ¥è¿‘æˆ–è¶…è¶Šä½¿ç”¨æœ€æ–°ç¬”è®°çš„æ•ˆæœï¼Œä¸”èƒ½åœ¨æ›´å°‘è¾“å…¥æ ‡è®°ä¸‹æ¥è¿‘å…¨è¯­å¢ƒæ¨¡å‹çš„æ•ˆæœã€‚</li>
<li>RAGæ˜¯ä¸€ç§æœ‰ç«äº‰åŠ›çš„å¤„ç†é•¿æ–‡æœ¬çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83dbe07bf1a698277068bd104136a117.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32bd56f80139dafae901b30bf9e82c84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbafa5f04d5e1123682364af991d1bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49d1c0016de24c71db12663581fc3e7d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6463600292bf3e6cffe9bab881ae1d63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-184f4ffb59e77c944b686be9be4d311f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e50ac77c394de771017b1d053eec8186.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting"><a href="#TransLLM-A-Unified-Multi-Task-Foundation-Framework-for-Urban-Transportation-via-Learnable-Prompting" class="headerlink" title="TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting"></a>TransLLM: A Unified Multi-Task Foundation Framework for Urban   Transportation via Learnable Prompting</h2><p><strong>Authors:Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang</strong></p>
<p>Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BiYunying/TransLLM">https://github.com/BiYunying/TransLLM</a>. </p>
<blockquote>
<p>åŸå¸‚äº¤é€šç³»ç»Ÿé¢ä¸´å¤šç§ä»»åŠ¡ä¸­çš„å¤šæ ·åŒ–æŒ‘æˆ˜ï¼Œå¦‚äº¤é€šé¢„æµ‹ã€ç”µåŠ¨æ±½è½¦ï¼ˆEVï¼‰å……ç”µéœ€æ±‚é¢„æµ‹å’Œå‡ºç§Ÿè½¦è°ƒåº¦ç­‰ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™æ€§ï¼šå°è§„æ¨¡æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¯ä»»åŠ¡ç‰¹å®šçš„ä¸”æ•°æ®ä¾èµ–æ€§å¼ºï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨å¤šç§åœºæ™¯ä¸­çš„é€šç”¨æ€§ï¼›è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è‡ªç„¶è¯­è¨€æ¥å£æä¾›äº†çµæ´»æ€§ï¼Œä½†åœ¨äº¤é€šé¢†åŸŸçš„ç»“æ„åŒ–æ—¶ç©ºæ•°æ®å’Œæ•°å€¼æ¨ç†æ–¹é¢å´é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TransLLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¡†æ¶ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æç¤ºç»„åˆå°†æ—¶ç©ºå»ºæ¨¡ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è½»é‡çº§çš„æ—¶ç©ºç¼–ç å™¨ï¼Œé€šè¿‡æ‰©å¼ çš„ä¸´æ—¶å·ç§¯å’ŒåŒé‡é‚»æ¥å›¾æ³¨æ„åŠ›ç½‘ç»œæ•æ‰å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œä¸LLMæ— ç¼æ¥å£é€šè¿‡ç»“æ„åŒ–åµŒå…¥ã€‚ä¸€ç§æ–°å‹çš„å®ä¾‹çº§æç¤ºè·¯ç”±æœºåˆ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œæ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€ä¸ªæ€§åŒ–æç¤ºï¼Œè¶…è¶Šäº†å›ºå®šçš„ä»»åŠ¡ç‰¹å®šæ¨¡æ¿ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ—¶ç©ºæ¨¡å¼ç¼–ç ä¸ºä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ŒåŠ¨æ€ç»„åˆä¸ªæ€§åŒ–æç¤ºä»¥å¼•å¯¼LLMæ¨ç†ï¼Œå¹¶é€šè¿‡ä¸“ç”¨è¾“å‡ºå±‚æŠ•å½±ç”Ÿæˆä»»åŠ¡ç‰¹å®šé¢„æµ‹ã€‚åœ¨ä¸ƒä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTransLLMåœ¨ç›‘ç£å­¦ä¹ å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­çš„è¡¨ç°éƒ½éå¸¸å‡ºè‰²ã€‚ä¸åä¸ªåŸºå‡†æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨å›å½’å’Œè§„åˆ’é—®é¢˜ä¸Šå‡è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–å’Œè·¨ä»»åŠ¡é€‚åº”æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BiYunying/TransLLM%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BiYunying/TransLLMè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹åŸå¸‚äº¤é€šç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚äº¤é€šé¢„æµ‹ã€ç”µåŠ¨æ±½è½¦å……ç”µéœ€æ±‚é¢„æµ‹å’Œå‡ºç§Ÿè½¦è°ƒåº¦ç­‰ä»»åŠ¡ï¼Œæå‡ºäº†TransLLMæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆæ—¶ç©ºå»ºæ¨¡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æç¤ºç»„åˆè¿›è¡Œç»Ÿä¸€å¤„ç†ã€‚é‡‡ç”¨è½»é‡çº§æ—¶ç©ºç¼–ç å™¨ï¼Œé€šè¿‡è†¨èƒ€æ—¶é—´å·ç§¯å’ŒåŒé‚»æ¥å›¾æ³¨æ„åŠ›ç½‘ç»œæ•æ‰å¤æ‚ä¾èµ–æ€§ï¼Œä¸å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼æ¥å£ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å®ä¾‹çº§æç¤ºè·¯ç”±æœºåˆ¶ï¼Œæ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€ä¸ªæ€§åŒ–æç¤ºï¼Œè¶…è¶Šäº†å›ºå®šä»»åŠ¡ç‰¹å®šæ¨¡æ¿ã€‚è¯¥æ¡†æ¶åœ¨ä¸ƒä¸ªæ•°æ®é›†å’Œä¸‰ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨æœ‰ç›‘ç£å’Œé›¶æ ·æœ¬è®¾ç½®ä¸­å…·æœ‰å‡ºè‰²çš„æ•ˆæœï¼Œä¸åç§åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œåœ¨å›å½’å’Œè§„åˆ’é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–å’Œè·¨ä»»åŠ¡é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransLLMæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºå¤„ç†åŸå¸‚äº¤é€šç³»ç»Ÿçš„å¤šæ ·æŒ‘æˆ˜ï¼Œå¦‚äº¤é€šé¢„æµ‹ã€ç”µåŠ¨æ±½è½¦å……ç”µéœ€æ±‚é¢„æµ‹å’Œå‡ºç§Ÿè½¦è°ƒåº¦ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†æ—¶ç©ºå»ºæ¨¡å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡å¯å­¦ä¹ çš„æç¤ºç»„åˆè¿›è¡Œèåˆã€‚</li>
<li>TransLLMä½¿ç”¨è½»é‡çº§æ—¶ç©ºç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚ä¾èµ–æ€§ï¼Œå¹¶ä¸LLMæ— ç¼å¯¹æ¥ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å®ä¾‹çº§æç¤ºè·¯ç”±æœºåˆ¶ï¼Œä½¿æ¡†æ¶èƒ½æ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€ä¸ªæ€§åŒ–æç¤ºã€‚</li>
<li>æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†å’Œä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šæ•ˆæœï¼ŒåŒ…æ‹¬æœ‰ç›‘ç£å’Œé›¶æ ·æœ¬è®¾ç½®ã€‚</li>
<li>TransLLMåœ¨å›å½’å’Œè§„åˆ’é—®é¢˜ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œè·¨ä»»åŠ¡é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-685171622748fd32dcbc1648e3fcaeba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-755b2642c499f73f93d942309e2db41a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-319c0e6ddb6cf30c1e7a49f45d8d3713.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c85d6d70c90a4421fd965078a7a817.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference"><a href="#Evaluating-Multilingual-and-Code-Switched-Alignment-in-LLMs-via-Synthetic-Natural-Language-Inference" class="headerlink" title="Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference"></a>Evaluating Multilingual and Code-Switched Alignment in LLMs via   Synthetic Natural Language Inference</h2><p><strong>Authors:Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban</strong></p>
<p>Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/nli-stress-testing">https://github.com/KurbanIntelligenceLab/nli-stress-testing</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œç„¶è€Œå…¶åœ¨ä¸åŒè¯­è¨€ä¹‹é—´ä¿æŒä¸€è‡´æ€§ã€é€»è¾‘æ€§çš„å¯¹é½èƒ½åŠ›ä»è¢«ä½ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰çš„æ§åˆ¶è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”ŸæˆåŸºäºé€»è¾‘çš„å‰æå‡è®¾å¯¹ï¼Œå¹¶å°†å…¶ç¿»è¯‘æˆç±»å‹å¤šæ ·çš„è¯­è¨€é›†ã€‚è¿™ç§è®¾è®¡èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶è¯­ä¹‰å…³ç³»ï¼Œå¹¶å…è®¸åœ¨å•è¯­å’Œæ··åˆè¯­è¨€ï¼ˆä»£ç åˆ‡æ¢ï¼‰æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»£ç åˆ‡æ¢å¹¶ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æœ‰æ‰€æé«˜ï¼Œè¿™è¡¨æ˜ç¿»è¯‘å¼•èµ·çš„è¯æ±‡å˜åŒ–å¯ä»¥ä½œä¸ºæ­£åˆ™åŒ–ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§åˆ†æå’Œè·¨è¯­è¨€å¯¹é½å¯è§†åŒ–æ¥éªŒè¯è¯­ä¹‰çš„ä¿ç•™ï¼Œè¯å®äº†ç¿»è¯‘å¯¹çš„ä¿çœŸæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰LLMè·¨è¯­è¨€æ¨ç†çš„æ½œåŠ›å’Œè„†å¼±æ€§ï¼Œå¹¶å°†ä»£ç åˆ‡æ¢è¯†åˆ«ä¸ºæé«˜å¤šè¯­è¨€ç¨³å¥æ€§çš„æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç›¸å…³ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/KurbanIntelligenceLab/nli-stress-testing">https://github.com/KurbanIntelligenceLab/nli-stress-testing</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14735v1">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶è·¨è¯­è¨€ä¸€è‡´ã€é€»è¾‘ä¸¥è°¨çš„å¯¹é½èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ§åˆ¶è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†ï¼Œç”Ÿæˆåˆæˆé€»è¾‘åŸºç¡€çš„å‰æå‡è®¾å¯¹ï¼Œå¹¶å°†å…¶ç¿»è¯‘æˆç±»å‹å¤šæ ·çš„è¯­è¨€é›†ã€‚è¯¥ç ”ç©¶è®¾è®¡èƒ½ç²¾ç¡®æ§åˆ¶è¯­ä¹‰å…³ç³»ï¼Œå¹¶å¯åœ¨å•è¯­å’Œæ··åˆè¯­è¨€ï¼ˆè¯­è¨€è½¬ç ï¼‰æ¡ä»¶ä¸‹è¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œè¯­è¨€è½¬ç å¹¶ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æœ‰æ‰€æå‡ï¼Œè¿™è¡¨æ˜ç¿»è¯‘å¼•èµ·çš„è¯æ±‡å˜åŒ–å¯èƒ½ä½œä¸ºä¸€ç§æ­£åˆ™åŒ–ä¿¡å·ã€‚é€šè¿‡åµŒå…¥ç›¸ä¼¼æ€§åˆ†æå’Œè·¨è¯­è¨€å¯¹é½å¯è§†åŒ–éªŒè¯äº†è¯­ä¹‰çš„ä¿ç•™ï¼Œè¯å®äº†ç¿»è¯‘å¯¹çš„å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†å½“å‰è¯­è¨€æ¨¡å‹è·¨è¯­è¨€æ¨ç†çš„æ½œåŠ›å’Œè„†å¼±æ€§ï¼Œå¹¶ç¡®å®šäº†è¯­è¨€è½¬ç ä½œä¸ºæé«˜å¤šè¯­è¨€ç¨³å¥æ€§çš„æœ‰åŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­çš„åº”ç”¨éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œä½†è·¨è¯­è¨€é€»è¾‘å¯¹é½èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚</li>
<li>æå‡ºä¸€ç§æ§åˆ¶è¯„ä¼°æ¡†æ¶ç”¨äºå¤šè¯­è¨€è‡ªç„¶è¯­è¨€æ¨ç†ï¼Œèƒ½ç”Ÿæˆå¹¶æ§åˆ¶è¯­ä¹‰å…³ç³»çš„åˆæˆæ•°æ®ã€‚</li>
<li>å‘ç°è¯­è¨€è½¬ç ä¸ä¼šé™ä½æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½æå‡æ¨¡å‹è¡¨ç°ï¼Œè¿™æš—ç¤ºäº†ç¿»è¯‘å¼•èµ·çš„è¯æ±‡å˜åŒ–æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚</li>
<li>è¯­ä¹‰ä¿ç•™çš„éªŒè¯é€šè¿‡åµŒå…¥ç›¸ä¼¼æ€§åˆ†æå’Œè·¨è¯­è¨€å¯¹é½å¯è§†åŒ–å¾—åˆ°ç¡®è®¤ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å½“å‰è¯­è¨€æ¨¡å‹è·¨è¯­è¨€æ¨ç†çš„æ½œåŠ›ä¸è„†å¼±æ€§ã€‚</li>
<li>è¯­è¨€è½¬ç è¢«è¯†åˆ«ä¸ºæé«˜å¤šè¯­è¨€ç¨³å¥æ€§çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6cf81e6d49a7795c75a7b96866051b91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-129375487d873b774c2af356fd64c8a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e94d2f70e80d68c07f2ccc991876ce4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2da2bf87a33006695a07d47a252ec39.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine"><a href="#ShizhenGPT-Towards-Multimodal-LLMs-for-Traditional-Chinese-Medicine" class="headerlink" title="ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine"></a>ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</h2><p><strong>Authors:Junying Chen, Zhenyang Cai, Zhiheng Liu, Yunjin Yang, Rongsheng Wang, Qingying Xiao, Xiangyi Feng, Zhan Su, Jing Guo, Xiang Wan, Guangjun Yu, Haizhou Li, Benyou Wang</strong></p>
<p>Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†ç”±äºä¸¤ä¸ªå…³é”®éšœç¢ï¼Œå®ƒä»¬åœ¨ä¼ ç»Ÿä¸­åŒ»ï¼ˆTCMï¼‰é¢†åŸŸçš„åº”ç”¨æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼šï¼ˆ1ï¼‰ç¼ºä¹é«˜è´¨é‡çš„ä¸­åŒ»æ•°æ®å’Œï¼ˆ2ï¼‰ä¸­åŒ»è¯Šæ–­çš„å›ºæœ‰è·¨æ¨¡æ€ç‰¹æ€§ï¼Œæ¶‰åŠè§‚å¯Ÿã€è†å¬ã€é—»è¯Šå’Œè¯Šè„‰ã€‚è¿™äº›æ„Ÿå®˜ä¸°å¯Œçš„æ¨¡æ€è¶…å‡ºäº†ä¼ ç»ŸLLMçš„èŒƒå›´ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†é€‚ç”¨äºä¸­åŒ»çš„å¤šæ¨¡æ€LLMâ€œæ—¶çGPTâ€ã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ•´ç†äº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ä¸­åŒ»æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡100GBçš„æ–‡æœ¬å’Œè¶…è¿‡200GBçš„å¤šæ¨¡æ€æ•°æ®ï¼ŒåŒ…æ‹¬120ä¸‡å¼ å›¾åƒã€200å°æ—¶çš„éŸ³é¢‘å’Œç”Ÿç†ä¿¡å·ã€‚æ—¶çGPTç»è¿‡é¢„è®­ç»ƒå’Œæ‰§è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å®ç°æ·±å…¥çš„ä¸­åŒ»çŸ¥è¯†å’Œå¤šæ¨¡æ€æ¨ç†ã€‚ä¸ºäº†è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ”¶é›†äº†æœ€è¿‘çš„ä¸­åŒ»èµ„æ ¼è€ƒè¯•ï¼Œå¹¶å»ºç«‹äº†è¯ç‰©è¯†åˆ«å’Œè§†è§‰è¯Šæ–­çš„è§†è§‰åŸºå‡†æµ‹è¯•ã€‚å®éªŒè¡¨æ˜ï¼Œæ—¶çGPTåœ¨åŒç±»è§„æ¨¡çš„LLMä¸­è¡¨ç°çªå‡ºï¼Œå¹¶ä¸æ›´å¤§çš„ä¸“æœ‰æ¨¡å‹ç›¸ç«äº‰ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ä¸­åŒ»è§†è§‰ç†è§£æ–¹é¢é¢†å…ˆäºç°æœ‰çš„å¤šæ¨¡æ€LLMï¼Œå¹¶å±•ç¤ºäº†è·¨å£°éŸ³ã€è„‰è±¡ã€æ°”å‘³å’Œè§†è§‰ç­‰æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºä¸­åŒ»çš„æ•´ä½“å¤šæ¨¡æ€æ„ŸçŸ¥å’Œè¯Šæ–­é“ºå¹³äº†é“è·¯ã€‚æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å…¬å¼€å¯ç”¨ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå°†æ¿€å‘è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14706v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¼—å¤šé¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œå®ƒä»¬åœ¨ä¸­åŒ»é¢†åŸŸçš„åº”ç”¨æ½œåŠ›ä»æœªè¢«å……åˆ†å‘æ˜ã€‚ä¸ºè§£å†³é«˜è´¨é‡ä¸­åŒ»æ•°æ®ç¼ºä¹ä»¥åŠä¸­åŒ»è¯Šæ–­æœ¬èº«çš„å¤šæ¨¡æ€ç‰¹æ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†é’ˆå¯¹ä¸­åŒ»çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ShizhenGPTã€‚ShizhenGPTé€šè¿‡é¢„è®­ç»ƒä¸æŒ‡ä»¤å¾®è°ƒï¼Œæ·±å…¥ç†è§£äº†ä¸­åŒ»çŸ¥è¯†å¹¶å…·å¤‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒShizhenGPTåœ¨ä¸­åŒ»è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å±•ç°å‡ºè·¨å£°éŸ³ã€è„‰è±¡ã€æ°”å‘³å’Œè§†è§‰ç­‰æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥èƒ½åŠ›ï¼Œä¸ºä¸­åŒ»çš„å…¨æ–¹ä½å¤šæ¨¡æ€æ„ŸçŸ¥ä¸è¯Šæ–­å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚è¯¥ç ”ç©¶æˆæœå…¬å¼€äº†æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ï¼Œå¸Œæœ›è¿›ä¸€æ­¥æ¿€å‘è¯¥é¢†åŸŸçš„ç ”ç©¶æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­åŒ»é¢†åŸŸçš„åº”ç”¨ä»å¤„äºæ¢ç´¢é˜¶æ®µã€‚</li>
<li>ç¼ºå°‘é«˜è´¨é‡ä¸­åŒ»æ•°æ®å’Œä¸­åŒ»è¯Šæ–­çš„å¤šæ¨¡æ€ç‰¹æ€§æ˜¯ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>ShizhenGPTæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸­åŒ»é¢†åŸŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ShizhenGPTé€šè¿‡é¢„è®­ç»ƒå’ŒæŒ‡ä»¤å¾®è°ƒè¾¾åˆ°æ·±å…¥ç†è§£ä¸­åŒ»çŸ¥è¯†å’Œå¤šæ¨¡æ€æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>ShizhenGPTåœ¨ä¸­åŒ»è§†è§‰ç†è§£æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œå±•ç°å‡ºè·¨å¤šç§æ¨¡æ€çš„ç»Ÿä¸€æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æˆæœå…¬å¼€äº†æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç ï¼Œä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14706">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-47460fbb72445ff6077ce716a2e6e2e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e69877002d649150cb83d1634ef7ced3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06977a8a2e667eb13790fe426eed1bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71fb63110c7981d1e1bf48cd4d757508.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65aa1d8909e397f2b6fd7dc06368436c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b663604c68d1ba6d3e82ef5a0ec1df1e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers"><a href="#MCP-Universe-Benchmarking-Large-Language-Models-with-Real-World-Model-Context-Protocol-Servers" class="headerlink" title="MCP-Universe: Benchmarking Large Language Models with Real-World Model   Context Protocol Servers"></a>MCP-Universe: Benchmarking Large Language Models with Real-World Model   Context Protocol Servers</h2><p><strong>Authors:Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li</strong></p>
<p>The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem. </p>
<blockquote>
<p>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆModel Context Protocolï¼‰å·²ç»æˆä¸ºè¿æ¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·çš„é©å‘½æ€§æ ‡å‡†ï¼Œè¿…é€Ÿè¢«å„å¤§äººå·¥æ™ºèƒ½æä¾›å•†å’Œå¼€å‘å¹³å°é‡‡çº³ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚é•¿æœŸæ¨ç†å’Œåºå¤§ã€é™Œç”Ÿçš„å·¥å…·ç©ºé—´ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€å…³é”®ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MCPå®‡å®™ï¼ˆMCP-Universeï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸”å›°éš¾ä»»åŠ¡ä¸­ä¸çœŸå®ä¸–ç•ŒMCPæœåŠ¡å™¨äº¤äº’çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†6ä¸ªæ ¸å¿ƒé¢†åŸŸï¼Œè·¨è¶Š11ä¸ªä¸åŒçš„MCPæœåŠ¡å™¨ï¼šä½ç½®å¯¼èˆªã€ä»“åº“ç®¡ç†ã€è´¢åŠ¡åˆ†æã€3Dè®¾è®¡ã€æµè§ˆå™¨è‡ªåŠ¨åŒ–å’Œç½‘é¡µæœç´¢ã€‚ä¸ºäº†ç¡®ä¿ä¸¥æ ¼çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å®æ–½äº†åŸºäºæ‰§è¡Œçš„è¯„ä¼°å™¨ï¼ŒåŒ…æ‹¬ç”¨äºä»£ç†æ ¼å¼åˆè§„æ€§çš„æ ¼å¼è¯„ä¼°å™¨ã€ç”¨äºæ—¶é—´ä¸å˜å†…å®¹åŒ¹é…çš„é™æ€è¯„ä¼°å™¨ï¼Œä»¥åŠå¯è‡ªåŠ¨æ£€ç´¢å®æ—¶çœŸå®æ•°æ®çš„åŠ¨æ€è¯„ä¼°å™¨ï¼Œé€‚ç”¨äºæ—¶é—´æ•æ„Ÿçš„ä»»åŠ¡ã€‚é€šè¿‡å¯¹é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€ä½³æ¨¡å‹ï¼Œå¦‚GPT-5ï¼ˆ43.72%ï¼‰ã€Grok-4ï¼ˆ33.33%ï¼‰å’ŒClaude-4.0-Sonnetï¼ˆ29.44%ï¼‰ï¼Œä¹Ÿå­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½é™åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†æå‡ºäº†ä¸€ä¸ªé‡å¤§çš„é•¿æœŸä¸Šä¸‹æ–‡æŒ‘æˆ˜ï¼Œéšç€äº¤äº’æ­¥éª¤çš„å¢åŠ ï¼Œè¾“å…¥ä»¤ç‰Œçš„æ•°é‡ä¼šè¿…é€Ÿå¢åŠ ã€‚è€Œä¸”ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªæœªçŸ¥å·¥å…·æŒ‘æˆ˜ï¼Œå› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†å¾€å¾€ä¸ç†Ÿæ‚‰MCPæœåŠ¡å™¨çš„ç²¾ç¡®ç”¨é€”ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåƒå…‰æ ‡è¿™æ ·çš„ä¼ä¸šçº§ä»£ç†å¹¶ä¸èƒ½æ¯”æ ‡å‡†çš„ReActæ¡†æ¶å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚é™¤äº†è¯„ä¼°ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä»¥æ”¯æŒUIçš„æ–¹å¼å¼€æºäº†å¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿ç ”ç©¶è€…å’Œå®è·µè€…èƒ½å¤Ÿæ— ç¼é›†æˆæ–°çš„ä»£ç†å’ŒMCPæœåŠ¡å™¨ï¼ŒåŒæ—¶ä¿ƒè¿›åœ¨ä¸æ–­å‘å±•çš„MCPç”Ÿæ€ç³»ç»Ÿä¸­çš„åˆ›æ–°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14704v1">PDF</a> Website: <a target="_blank" rel="noopener" href="https://mcp-universe.github.io/">https://mcp-universe.github.io</a></p>
<p><strong>Summary</strong><br>åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·çš„è¿æ¥æ ‡å‡†ä¸­ï¼Œæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·²å´­éœ²å¤´è§’ï¼Œæˆä¸ºå˜é©æ€§æ ‡å‡†ã€‚ä½†ç°æœ‰åŸºå‡†æµ‹è¯•è¿‡äºç®€å•ï¼Œæ— æ³•æ•æ‰å®é™…åº”ç”¨ç¨‹åºçš„æŒ‘æˆ˜ï¼Œå¦‚é•¿æœŸæ¨ç†å’Œå¤§å‹ã€ä¸ç†Ÿæ‚‰çš„å·¥å…·ç©ºé—´ã€‚ä¸ºè§£å†³è¿™ä¸€å…³é”®å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MCP-Universeï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°è¯­è¨€å¤§æ¨¡å‹åœ¨çœŸå®å’Œå›°éš¾ä»»åŠ¡ä¸­ä¸çœŸå®ä¸–ç•ŒMCPæœåŠ¡å™¨äº¤äº’çš„åŸºå‡†æµ‹è¯•ã€‚æ¶µç›–6ä¸ªæ ¸å¿ƒé¢†åŸŸï¼Œè·¨è¶Š11ä¸ªä¸åŒçš„MCPæœåŠ¡å™¨ã€‚å°½ç®¡å¯¹é¡¶å°–æ¨¡å‹å¦‚GPT-5ç­‰è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼Œä½†å‘ç°å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½é™åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å¯¹LLMä»£ç†æå‡ºäº†é•¿æœŸä¸Šä¸‹æ–‡å’ŒæœªçŸ¥å·¥å…·çš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œå¼€æºäº†å¯æ‰©å±•çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒç•Œé¢ï¼Œä¿ƒè¿›æ–°ä»£ç†å’ŒMCPæœåŠ¡å™¨çš„æ— ç¼é›†æˆï¼Œæ¨åŠ¨MCPç”Ÿæ€ç³»ç»Ÿå¿«é€Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å·²æˆä¸ºè¿æ¥å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·çš„é‡è¦æ ‡å‡†ï¼Œå—åˆ°å„å¤§AIä¾›åº”å•†å’Œå¼€å‘å¹³å°çš„å¹¿æ³›é‡‡çº³ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†æ¨¡æ‹Ÿå®é™…åº”ç”¨ç¨‹åºçš„æŒ‘æˆ˜ï¼Œå¦‚é•¿æœŸæ¨ç†å’Œå¤§å‹ã€ä¸ç†Ÿæ‚‰å·¥å…·ç©ºé—´çš„åº”ç”¨æŒ‘æˆ˜ã€‚</li>
<li>MCP-Universeæ˜¯é¦–ä¸ªä¸“ä¸ºè¯„ä¼°LLMsåœ¨çœŸå®å’Œå›°éš¾ä»»åŠ¡ä¸­è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸå’Œä¸åŒçš„MCPæœåŠ¡å™¨ã€‚</li>
<li>é¡¶å°–æ¨¡å‹å¦‚GPT-5ç­‰è™½ç»å¹¿æ³›è¯„ä¼°ï¼Œä½†ä»å­˜åœ¨æ˜¾è‘—æ€§èƒ½é™åˆ¶ï¼Œé¢ä¸´é•¿æœŸä¸Šä¸‹æ–‡å’ŒæœªçŸ¥å·¥å…·çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ä¸šçº§ä»£ç†åœ¨å¤„ç†å¤§é‡äº¤äº’æ­¥éª¤æ—¶é¢ä¸´è¾“å…¥ä»¤ç‰Œæ•°é‡è¿…é€Ÿå¢åŠ çš„é—®é¢˜ã€‚</li>
<li>å¼€æºçš„è¯„ä¼°æ¡†æ¶æ”¯æŒç•Œé¢ï¼Œä¾¿äºç ”ç©¶äººå‘˜å’Œå®è·µè€…æ— ç¼é›†æˆæ–°ä»£ç†å’ŒMCPæœåŠ¡å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f0828be6817961439f551e432382603c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f77fee2dc4a17e80f9c5b1e00ff21b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf20e9916a34924a141d1d9bfb010cac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f2054fc6941363614657be0cfe236d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ddded5442449553444bf8cae05510d2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles"><a href="#Adversarial-Generation-and-Collaborative-Evolution-of-Safety-Critical-Scenarios-for-Autonomous-Vehicles" class="headerlink" title="Adversarial Generation and Collaborative Evolution of Safety-Critical   Scenarios for Autonomous Vehicles"></a>Adversarial Generation and Collaborative Evolution of Safety-Critical   Scenarios for Autonomous Vehicles</h2><p><strong>Authors:Jiangfan Liu, Yongkang Guo, Fangzhi Zhong, Tianyuan Zhang, Zonglei Jing, Siyuan Liang, Jiakai Wang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</strong></p>
<p>The generation of safety-critical scenarios in simulation has become increasingly crucial for safety evaluation in autonomous vehicles prior to road deployment in society. However, current approaches largely rely on predefined threat patterns or rule-based strategies, which limit their ability to expose diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a framework that can generate plentiful safety-critical scenarios by reasoning novel adversarial cases and then amplifying them with complex traffic flows. Given a simple prompt of a benign scene, it first performs Meta-Scenario Generation, where a large language model, grounded in structured driving knowledge, infers an adversarial agent whose behavior poses a threat that is both plausible and deliberately challenging. This meta-scenario is then specified in executable code for precise in-simulator control. Subsequently, Complex Scenario Evolution uses background vehicles to amplify the core threat introduced by Meta-Scenario. It builds an adversarial collaborator graph to identify key agent trajectories for optimization. These perturbations are designed to simultaneously reduce the ego vehicleâ€™s maneuvering space and create critical occlusions. Extensive experiments conducted on multiple reinforcement learning based AV models show that ScenGE uncovers more severe collision cases (+31.96%) on average than SoTA baselines. Additionally, our ScenGE can be applied to large model based AV systems and deployed on different simulators; we further observe that adversarial training on our scenarios improves the model robustness. Finally, we validate our framework through real-world vehicle tests and human evaluation, confirming that the generated scenarios are both plausible and critical. We hope our paper can build up a critical step towards building public trust and ensuring their safe deployment. </p>
<blockquote>
<p>åœ¨ä»¿çœŸä¸­ç”Ÿæˆå®‰å…¨å…³é”®åœºæ™¯å¯¹äºåœ¨é“è·¯ä¸Šéƒ¨ç½²ç¤¾ä¼šè‡ªåŠ¨é©¾é©¶æ±½è½¦ä¹‹å‰çš„å®‰å…¨è¯„ä¼°å˜å¾—æ„ˆå‘å…³é”®ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¤§å¤šä¾èµ–äºé¢„å…ˆå®šä¹‰çš„å¨èƒæ¨¡å¼æˆ–åŸºäºè§„åˆ™çš„ç­–ç•¥ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æš´éœ²å¤šæ ·å’Œæœªæ›¾é¢„è§åˆ°çš„æ•…éšœæ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ScenGEæ¡†æ¶ï¼Œå®ƒå¯ä»¥é€šè¿‡æ¨ç†æ–°çš„å¯¹æŠ—æ€§æ¡ˆä¾‹å¹¶ä½¿ç”¨å¤æ‚çš„äº¤é€šæµæ¥æ”¾å¤§è¿™äº›æ¡ˆä¾‹ï¼Œä»è€Œç”Ÿæˆå¤§é‡çš„å®‰å…¨å…³é”®åœºæ™¯ã€‚ç»™å®šä¸€ä¸ªè‰¯æ€§åœºæ™¯çš„ç®€å•æç¤ºï¼Œå®ƒé¦–å…ˆæ‰§è¡Œå…ƒåœºæ™¯ç”Ÿæˆï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ä»¥ç»“æ„åŒ–é©¾é©¶çŸ¥è¯†ä¸ºåŸºç¡€ï¼Œæ¨æ–­å‡ºå¯¹æŠ—æ€§ä»£ç†çš„è¡Œä¸ºæ„æˆæ—¢åˆç†åˆæ•…æ„å…·æœ‰æŒ‘æˆ˜æ€§çš„å¨èƒã€‚è¯¥å…ƒåœºæ™¯éšåè¢«æŒ‡å®šä¸ºå¯æ‰§è¡Œä»£ç ï¼Œä»¥åœ¨æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œç²¾ç¡®æ§åˆ¶ã€‚éšåï¼Œå¤æ‚åœºæ™¯æ¼”å˜ä½¿ç”¨èƒŒæ™¯è½¦è¾†æ¥æ”¾å¤§å…ƒåœºæ™¯å¼•å…¥çš„æ ¸å¿ƒå¨èƒã€‚å®ƒå»ºç«‹äº†ä¸€ä¸ªå¯¹æŠ—æ€§ååŒå›¾æ¥è¯†åˆ«å…³é”®ä»£ç†è½¨è¿¹è¿›è¡Œä¼˜åŒ–ã€‚è¿™äº›æ‰°åŠ¨æ—¨åœ¨åŒæ—¶å‡å°‘è‡ªæˆ‘è½¦è¾†çš„æœºåŠ¨ç©ºé—´å¹¶é€ æˆå…³é”®é®æŒ¡ã€‚åœ¨å¤šä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒScenGEå¹³å‡å‘ç°äº†æ¯”æœ€æ–°æŠ€æœ¯åŸºå‡†æ›´å¤šçš„ä¸¥é‡ç¢°æ’æƒ…å†µï¼ˆ+31.96%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ScenGEå¯åº”ç”¨äºåŸºäºå¤§å‹æ¨¡å‹çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¹¶å¯åœ¨ä¸åŒçš„æ¨¡æ‹Ÿå™¨ä¸Šéƒ¨ç½²ï¼›æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œåœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸Šè¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒå¯æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡çœŸå®è½¦è¾†æµ‹è¯•å’Œäººç±»è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œè¯å®äº†ç”Ÿæˆçš„åœºæ™¯æ—¢åˆç†åˆå…³é”®ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„è®ºæ–‡èƒ½ä¸ºå»ºç«‹å…¬ä¼—ä¿¡ä»»å¹¶ç¡®ä¿å…¶å®‰å…¨éƒ¨ç½²æ­å»ºå…³é”®ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14527v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†åœ¨æŠ•æ”¾ç¤¾ä¼šå‰çš„å®‰å…¨è¯„ä¼°ï¼Œæ¨¡æ‹Ÿç”Ÿæˆå®‰å…¨å…³é”®åœºæ™¯å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–äºé¢„è®¾çš„å¨èƒæ¨¡å¼æˆ–è§„åˆ™ç­–ç•¥ï¼Œæ— æ³•æš´éœ²å¤šæ ·ä¸”æœªé¢„è§åˆ°çš„æ•…éšœæ¨¡å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºScenGEæ¡†æ¶ï¼Œé€šè¿‡æ¨ç†æ–°å‹å¯¹æŠ—æ¡ˆä¾‹å¹¶å€ŸåŠ©å¤æ‚çš„äº¤é€šæµè¿›è¡Œæ”¾å¤§ï¼Œç”Ÿæˆä¸°å¯Œçš„å®‰å…¨å…³é”®åœºæ™¯ã€‚ç»™å®šä¸€ä¸ªè‰¯æ€§åœºæ™¯çš„ç®€å•æç¤ºï¼Œå®ƒé¦–å…ˆè¿›è¡Œå…ƒåœºæ™¯ç”Ÿæˆï¼Œä½¿ç”¨åŸºäºç»“æ„åŒ–é©¾é©¶çŸ¥è¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨æ–­ä¸€ä¸ªå¯¹æŠ—æ€§ä»£ç†çš„è¡Œä¸ºå¨èƒï¼Œæ—¢å…·æœ‰å¯è¡Œæ€§åˆå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç„¶åï¼Œå¤æ‚åœºæ™¯æ¼”åŒ–åˆ©ç”¨èƒŒæ™¯è½¦è¾†æ”¾å¤§å…ƒåœºæ™¯å¼•å…¥çš„æ ¸å¿ƒå¨èƒã€‚å®ƒå»ºç«‹å¯¹æŠ—æ€§åä½œå›¾æ¥ä¼˜åŒ–å…³é”®ä»£ç†è½¨è¿¹ã€‚è¿™äº›æ‰°åŠ¨æ—¨åœ¨åŒæ—¶å‡å°‘è½¦è¾†çš„æ“ä½œç©ºé—´å¹¶é€ æˆå…³é”®é®æŒ¡ã€‚åœ¨å¤šä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒScenGEå¹³å‡æ¯”æœ€æ–°æŠ€æœ¯åŸºçº¿å‘ç°æ›´ä¸¥é‡çš„ç¢°æ’æƒ…å†µï¼ˆ+31.96%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ScenGEå¯åº”ç”¨äºå¤§å‹æ¨¡å‹ä¸ºåŸºç¡€çš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿå¹¶éƒ¨ç½²åœ¨ä¸åŒçš„æ¨¡æ‹Ÿå™¨ä¸Šï¼›æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°åœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸Šè¿›è¡Œå¯¹æŠ—æ€§è®­ç»ƒå¯æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡çœŸå®è½¦è¾†æµ‹è¯•å’Œäººç±»è¯„ä¼°éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œç¡®è®¤ç”Ÿæˆçš„åœºæ™¯æ—¢å¯è¡Œåˆå…³é”®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªä¸»è½¦è¾†çš„æ¨¡æ‹Ÿå®‰å…¨å…³é”®åœºæ™¯ç”Ÿæˆå¯¹äºç¤¾ä¼šé“è·¯éƒ¨ç½²å‰çš„å®‰å…¨è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•å—é™äºé¢„è®¾å¨èƒæ¨¡å¼å’Œè§„åˆ™ç­–ç•¥ï¼Œéš¾ä»¥å±•ç°å¤šæ ·ä¸”æœªé¢„è§çš„å¤±è´¥æ¨¡å¼ã€‚</li>
<li>ScenGEæ¡†æ¶é€šè¿‡æ¨ç†æ–°å‹å¯¹æŠ—æ¡ˆä¾‹å¹¶ç»“åˆå¤æ‚äº¤é€šæµè¿›è¡Œåœºæ™¯ç”Ÿæˆï¼Œä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>ScenGEåŒ…å«å…ƒåœºæ™¯ç”Ÿæˆå’Œå¤æ‚åœºæ™¯æ¼”åŒ–ä¸¤ä¸ªé˜¶æ®µï¼Œå¯æœ‰æ•ˆæ¨¡æ‹Ÿå¯¹æŠ—æ€§ä»£ç†è¡Œä¸ºå’Œäº¤é€šæµäº’åŠ¨ã€‚</li>
<li>ScenGEåœ¨å¤šç§è‡ªåŠ¨é©¾é©¶æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå¹³å‡å‘ç°æ›´ä¸¥é‡çš„ç¢°æ’æƒ…å†µã€‚</li>
<li>ScenGEå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œå¯éƒ¨ç½²åœ¨ä¸åŒçš„æ¨¡æ‹Ÿå™¨ä¸Šï¼Œå¹¶ä¸”é€šè¿‡å¯¹æŠ—è®­ç»ƒæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7a2c99a0e7c7ea35e74faf63180bef78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b73d672a0a69d3c12a2fe79cb2a2b33f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44b29f6ba57e703ff5210a61a83e06aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8c9bbf44bc7607c0fbb857e05832a2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model"><a href="#NVIDIA-Nemotron-Nano-2-An-Accurate-and-Efficient-Hybrid-Mamba-Transformer-Reasoning-Model" class="headerlink" title="NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model"></a>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid   Mamba-Transformer Reasoning Model</h2><p><strong>Authors: NVIDIA,  :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adi Renduchintala, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen</strong></p>
<p>We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Nemotron-Nano-9B-v2ï¼Œè¿™æ˜¯ä¸€æ¬¾æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜æ¨ç†å·¥ä½œè´Ÿè½½çš„ååé‡ï¼ŒåŒæ—¶ä¸ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”å®ç°æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚Nemotron-Nano-9B-v2å»ºç«‹åœ¨Nemotron-Hæ¶æ„çš„åŸºç¡€ä¸Šï¼Œè¯¥æ¶æ„å°†Transformeræ¶æ„ä¸­å¤§éƒ¨åˆ†çš„è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºMamba-2å±‚ï¼Œä»¥å®ç°åœ¨ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€è€ƒè½¨è¿¹æ—¶æé«˜æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬é€šè¿‡é¦–å…ˆåœ¨20ä¸‡äº¿ä¸ªä»¤ç‰Œä¸Šä½¿ç”¨FP8è®­ç»ƒé…æ–¹é¢„è®­ç»ƒä¸€ä¸ª12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰æ¥åˆ›å»ºNemotron-Nano-9B-v2ã€‚åœ¨å¯¹Nemotron-Nano-12B-v2-Baseè¿›è¡Œå¯¹é½åï¼Œæˆ‘ä»¬é‡‡ç”¨Minitronç­–ç•¥å¯¹æ¨¡å‹è¿›è¡Œå‹ç¼©å’Œè’¸é¦ï¼Œæ—¨åœ¨ä½¿ç”¨å•ä¸ªNVIDIA A10G GPUï¼ˆå…·æœ‰22GBå†…å­˜ï¼Œbfloat16ç²¾åº¦ï¼‰è¿›è¡Œæœ€å¤šè¾¾128kä»¤ç‰Œçš„æ¨ç†ã€‚ä¸ç°æœ‰çš„ç±»ä¼¼è§„æ¨¡æ¨¡å‹ï¼ˆä¾‹å¦‚Qwen3-8Bï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¯æ˜äº†Nemotron-Nano-9B-v2çš„å‡†ç¡®ç‡ç›¸å½“æˆ–æ›´é«˜ï¼ŒåŒæ—¶åœ¨å¦‚8kè¾“å…¥å’Œ16kè¾“å‡ºä»¤ç‰Œç­‰æ¨ç†è®¾ç½®ä¸­å®ç°äº†é«˜è¾¾6å€çš„æ¨ç†ååé‡ã€‚æˆ‘ä»¬å°†Nemotron-Nano-9B-v2ã€Nemotron-Nano12B-v2-Baseä»¥åŠNemotron-Nano-9B-v2çš„checkpointå’Œå¤§éƒ¨åˆ†é¢„è®­ç»ƒå’Œåç»­è®­ç»ƒæ•°æ®é›†ä¸€èµ·åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14444v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºNemotron-Hæ¶æ„çš„Nemotron-Nano-9B-v2æ··åˆMamba-Transformerè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æå‡æ¨ç†å·¥ä½œè´Ÿè½½çš„å¤„ç†èƒ½åŠ›å¹¶è¾¾åˆ°ç±»ä¼¼æ¨¡å‹çš„é¡¶å°–å‡†ç¡®åº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡ç”¨Mamba-2å±‚æ›¿æ¢Transformeræ¶æ„ä¸­çš„å¤§éƒ¨åˆ†è‡ªæ³¨æ„åŠ›å±‚ï¼Œåœ¨ç”Ÿæˆæ¨ç†æ‰€éœ€çš„é•¿æ€è€ƒè½¨è¿¹æ—¶å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚é€šè¿‡é¢„è®­ç»ƒä¸€ä¸ªåŸºäºFP8è®­ç»ƒé£Ÿè°±çš„12äº¿å‚æ•°æ¨¡å‹ï¼ˆNemotron-Nano-12B-v2-Baseï¼‰ï¼Œå†é€šè¿‡Minitronç­–ç•¥å‹ç¼©å’Œè’¸é¦æ¨¡å‹ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦ï¼Œä¸ç°æœ‰ç±»ä¼¼è§„æ¨¡çš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒåœ¨æ¨ç†è®¾ç½®æ–¹é¢å–å¾—äº†æ›´å¥½çš„å‡†ç¡®åº¦å¹¶å®ç°äº†æ›´é«˜çš„æ¨ç†ååé‡ã€‚æ¨¡å‹å·²åœ¨Hugging Faceä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ac5d6aea88ce279e01ebbd730851f0f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fa504cec80d675359af61cce57535a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3dbd8c7b29fda38f4dde86f49428b106.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning"><a href="#Automated-Optimization-Modeling-through-Expert-Guided-Large-Language-Model-Reasoning" class="headerlink" title="Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning"></a>Automated Optimization Modeling through Expert-Guided Large Language   Model Reasoning</h2><p><strong>Authors:Beinuo Yang, Qishen Zhou, Junyi Li, Xingchen Su, Simon Hu</strong></p>
<p>Optimization Modeling (OM) is essential for solving complex decision-making problems. However, the process remains time-consuming and error-prone, heavily relying on domain experts. While Large Language Models (LLMs) show promise in addressing these challenges through their natural language understanding and reasoning capabilities, current approaches face three critical limitations: high benchmark labeling error rates reaching up to 42%, narrow evaluation scope that only considers optimal values, and computational inefficiency due to heavy reliance on multi-agent systems or model fine-tuning. In this work, we first enhance existing datasets through systematic error correction and more comprehensive annotation. Additionally, we introduce LogiOR, a new optimization modeling benchmark from the logistics domain, containing more complex problems with standardized annotations. Furthermore, we present ORThought, a novel framework that leverages expert-level optimization modeling principles through chain-of-thought reasoning to automate the OM process. Through extensive empirical evaluation, we demonstrate that ORThought outperforms existing approaches, including multi-agent frameworks, with particularly significant advantages on complex optimization problems. Finally, we provide a systematic analysis of our method, identifying critical success factors and failure modes, providing valuable insights for future research on LLM-based optimization modeling. </p>
<blockquote>
<p>ä¼˜åŒ–å»ºæ¨¡ï¼ˆOMï¼‰å¯¹äºè§£å†³å¤æ‚çš„å†³ç­–é—®é¢˜è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™ä¸€è¿‡ç¨‹ä»ç„¶è€—æ—¶ä¸”å®¹æ˜“å‡ºé”™ï¼Œä¸¥é‡ä¾èµ–äºé¢†åŸŸä¸“å®¶ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å…¶è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨åº”å¯¹è¿™äº›æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šé«˜åŸºå‡†æ ‡ç­¾é”™è¯¯ç‡ï¼Œæœ€é«˜å¯è¾¾42%ï¼Œè¯„ä¼°èŒƒå›´ç‹­çª„ï¼Œåªè€ƒè™‘æœ€ä¼˜å€¼ï¼Œä»¥åŠç”±äºä¸¥é‡ä¾èµ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæˆ–æ¨¡å‹å¾®è°ƒè€Œå¯¼è‡´çš„è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14410v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è§£å†³å¤æ‚çš„å†³ç­–é—®é¢˜æ–¹é¢ï¼Œä¼˜åŒ–å»ºæ¨¡ï¼ˆOMï¼‰ååˆ†é‡è¦ï¼Œä½†å…¶è¿‡ç¨‹è€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºé¢†åŸŸä¸“å®¶ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºé€šè¿‡è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥è§£å†³è¿™äº›æŒ‘æˆ˜çš„æ½œåŠ›ï¼Œä½†å½“å‰çš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šé«˜åŸºå‡†æ ‡ç­¾é”™è¯¯ç‡ã€è¯„ä¼°èŒƒå›´ç‹­çª„åªè€ƒè™‘æœ€ä¼˜å€¼å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚ä¸ºæ”¹å–„è¿™äº›é—®é¢˜ï¼Œæœ¬ç ”ç©¶é¦–å…ˆé€šè¿‡ç³»ç»Ÿè¯¯å·®æ ¡æ­£å’Œæ›´å…¨é¢çš„æ³¨é‡Šå¢å¼ºç°æœ‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥LogiORè¿™ä¸€æ–°çš„ç‰©æµé¢†åŸŸä¼˜åŒ–å»ºæ¨¡åŸºå‡†ã€‚æ­¤å¤–ï¼Œæå‡ºORThoughtæ¡†æ¶ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ¨ç†åˆ©ç”¨ä¸“å®¶çº§ä¼˜åŒ–å»ºæ¨¡åŸåˆ™æ¥è‡ªåŠ¨åŒ–OMè¿‡ç¨‹ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼Œè¯æ˜ORThoughtä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚æœ€åï¼Œå¯¹æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œè¯†åˆ«å‡ºæˆåŠŸå› ç´ å’Œå¤±è´¥æ¨¡å¼ï¼Œä¸ºæœªæ¥åŸºäºLLMçš„ä¼˜åŒ–å»ºæ¨¡ç ”ç©¶æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼˜åŒ–å»ºæ¨¡ï¼ˆOMï¼‰æ˜¯è§£å†³å¤æ‚å†³ç­–é—®é¢˜çš„å…³é”®ï¼Œä½†è¿‡ç¨‹è€—æ—¶ä¸”æ˜“å‡ºé”™ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³OMæŒ‘æˆ˜æ–¹é¢å…·æ½œåŠ›ï¼Œä½†å½“å‰æ–¹æ³•å­˜åœ¨é«˜é”™è¯¯ç‡ã€ç‹­çª„è¯„ä¼°èŒƒå›´å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç³»ç»Ÿè¯¯å·®æ ¡æ­£å’Œæ›´å…¨é¢çš„æ³¨é‡Šå¢å¼ºäº†ç°æœ‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†æ–°çš„ç‰©æµé¢†åŸŸä¼˜åŒ–å»ºæ¨¡åŸºå‡†LogiORã€‚</li>
<li>æå‡ºORThoughtæ¡†æ¶ï¼Œç»“åˆä¸“å®¶çº§ä¼˜åŒ–å»ºæ¨¡åŸåˆ™ï¼Œé€šè¿‡é“¾å¼æ€ç»´æ¨ç†è‡ªåŠ¨åŒ–OMè¿‡ç¨‹ã€‚</li>
<li>ORThoughtæ¡†æ¶åœ¨å¤æ‚ä¼˜åŒ–é—®é¢˜ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•çš„æˆåŠŸä¸å¤±è´¥å› ç´ è¢«è¯†åˆ«ï¼Œä¸ºæœªæ¥åŸºäºLLMçš„ä¼˜åŒ–å»ºæ¨¡ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14410">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fb6edcbd2c2399bc8abbc7fdec25249.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1217039f00592b49c11bcaf7a2836893.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1909c69cdea75d6dc96d29bc8f00a82b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96093fc8129788b6df44f9cc5a24d7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ade61ce866f06c45c73bb60746837150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-127fbbe12fd8e6fb16f6105490a870ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55206638a9556e0a725174980723d238.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS"><a href="#Your-Reward-Function-for-RL-is-Your-Best-PRM-for-Search-Unifying-RL-and-Search-Based-TTS" class="headerlink" title="Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS"></a>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and   Search-Based TTS</h2><p><strong>Authors:Can Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei Hong, Tong Che, Dimitris N. Metaxas</strong></p>
<p>Test-time scaling (TTS) for large language models (LLMs) has thus far fallen into two largely separate paradigms: (1) reinforcement learning (RL) methods that optimize sparse outcome-based rewards, yet suffer from instability and low sample efficiency; and (2) search-based techniques guided by independently trained, static process reward models (PRMs), which require expensive human- or LLM-generated labels and often degrade under distribution shifts. In this paper, we introduce AIRL-S, the first natural unification of RL-based and search-based TTS. Central to AIRL-S is the insight that the reward function learned during RL training inherently represents the ideal PRM for guiding downstream search. Specifically, we leverage adversarial inverse reinforcement learning (AIRL) combined with group relative policy optimization (GRPO) to learn a dense, dynamic PRM directly from correct reasoning traces, entirely eliminating the need for labeled intermediate process data. At inference, the resulting PRM simultaneously serves as the critic for RL rollouts and as a heuristic to effectively guide search procedures, facilitating robust reasoning chain extension, mitigating reward hacking, and enhancing cross-task generalization. Experimental results across eight benchmarks, including mathematics, scientific reasoning, and code generation, demonstrate that our unified approach improves performance by 9 % on average over the base model, matching GPT-4o. Furthermore, when integrated into multiple search algorithms, our PRM consistently outperforms all baseline PRMs trained with labeled data. These results underscore that, indeed, your reward function for RL is your best PRM for search, providing a robust and cost-effective solution to complex reasoning tasks in LLMs. </p>
<blockquote>
<p>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿„ä»Šä¸ºæ­¢ä¸»è¦åˆ†ä¸ºäº†ä¸¤ç§æˆªç„¶ä¸åŒçš„èŒƒå¼ï¼šä¸€æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–åŸºäºç¨€ç–ç»“æœçš„å¥–åŠ±ï¼Œä½†å­˜åœ¨ä¸ç¨³å®šå’Œä½æ ·æœ¬æ•ˆç‡çš„é—®é¢˜ï¼›äºŒæ˜¯åŸºäºç‹¬ç«‹è®­ç»ƒçš„é™æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰æŒ‡å¯¼çš„æœç´¢æŠ€æœ¯ï¼Œè¿™éœ€è¦æ˜‚è´µçš„äººå·¥æˆ–LLMç”Ÿæˆçš„æ ‡ç­¾ï¼Œå¹¶ä¸”åœ¨åˆ†å¸ƒè½¬ç§»çš„æƒ…å†µä¸‹ç»å¸¸ä¼šæ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†AIRL-Sï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºRLå’Œæœç´¢çš„TTSçš„è‡ªç„¶ç»Ÿä¸€ã€‚AIRL-Sçš„æ ¸å¿ƒè§è§£æ˜¯ï¼Œåœ¨RLè®­ç»ƒæœŸé—´å­¦ä¹ çš„å¥–åŠ±å‡½æ•°æœ¬è´¨ä¸Šä»£è¡¨äº†ç†æƒ³çš„PRMï¼Œç”¨äºæŒ‡å¯¼ä¸‹æ¸¸æœç´¢ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨å¯¹æŠ—æ€§é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¯†é›†ã€åŠ¨æ€çš„PRMï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹æ ‡è®°çš„ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚åœ¨æ¨ç†æ—¶ï¼Œæ‰€å¾—çš„PRMåŒæ—¶ä½œä¸ºRLæ¼”ç»ƒçš„æ‰¹è¯„å®¶ï¼Œå¹¶ä½œä¸ºæœ‰æ•ˆæŒ‡å¯¼æœç´¢ç¨‹åºçš„å¯å‘å¼æ–¹æ³•ï¼Œä¿ƒè¿›ç¨³å¥çš„æ¨ç†é“¾æ‰©å±•ï¼Œç¼“è§£å¥–åŠ±ç ´è§£ï¼Œå¹¶å¢å¼ºè·¨ä»»åŠ¡æ³›åŒ–ã€‚åœ¨åŒ…æ‹¬æ•°å­¦ã€ç§‘å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆç­‰åœ¨å†…çš„å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ–¹æ³•å¹³å‡æ¯”åŸºç¡€æ¨¡å‹æé«˜äº†9%çš„æ€§èƒ½ï¼Œä¸GPT-4oç›¸åŒ¹é…ã€‚æ­¤å¤–ï¼Œå½“é›†æˆåˆ°å¤šç§æœç´¢ç®—æ³•ä¸­æ—¶ï¼Œæˆ‘ä»¬çš„PRMå§‹ç»ˆä¼˜äºä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒçš„æ‰€æœ‰åŸºçº¿PRMã€‚è¿™äº›ç»“æœç¡®å®è¡¨æ˜ï¼Œä½ çš„RLå¥–åŠ±å‡½æ•°æ˜¯ä½ çš„æœ€ä½³PRMæœç´¢å·¥å…·ï¼Œä¸ºLLMä¸­çš„å¤æ‚æ¨ç†ä»»åŠ¡æä¾›äº†ç¨³å¥ä¸”ç»æµçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰çš„æ–°æ–¹æ³•â€”â€”AIRL-Sã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŸºäºæœç´¢çš„æŠ€æœ¯ï¼Œé€šè¿‡åˆ©ç”¨å¯¹æŠ—æ€§é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥å­¦ä¹ å¯†é›†ã€åŠ¨æ€çš„PRMï¼Œç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ ï¼Œå®Œå…¨æ¶ˆé™¤äº†å¯¹æ ‡è®°çš„ä¸­é—´è¿‡ç¨‹æ•°æ®çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†9%çš„æ€§èƒ½ï¼Œä¸GPT-4oç›¸åŒ¹é…ã€‚å½“é›†æˆåˆ°å¤šç§æœç´¢ç®—æ³•ä¸­æ—¶ï¼Œæœ¬æ–‡çš„PRMå§‹ç»ˆä¼˜äºä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒçš„åŸºçº¿PRMã€‚è¿™è¯æ˜å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†ä»»åŠ¡æ¥è¯´ï¼Œå¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°æ˜¯æœ€å¥½çš„æœç´¢PRMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIRL-Sç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæœç´¢æŠ€æœ¯ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTTSï¼‰æä¾›äº†æ–°çš„æ–¹æ³•ã€‚</li>
<li>AIRL-Sé€šè¿‡åˆ©ç”¨å¯¹æŠ—æ€§é€†å‘å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›´æ¥ä»æ­£ç¡®çš„æ¨ç†è½¨è¿¹ä¸­å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œæ¶ˆé™¤äº†å¯¹æ ‡è®°æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAIRL-Såœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ€§èƒ½æå‡æ˜¾è‘—ï¼Œå¹³å‡æé«˜9%ï¼Œä¸GPT-4oç›¸åŒ¹é…ã€‚</li>
<li>å½“é›†æˆåˆ°å¤šç§æœç´¢ç®—æ³•ä¸­æ—¶ï¼ŒAIRL-Sçš„å¥–åŠ±å‡½æ•°ä½œä¸ºæœç´¢è¿‡ç¨‹çš„æŒ‡å¯¼ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒçš„PRMã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5198d26e3e08dc7b2acf49f570b44b5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f57c37203deab202935d2ac2bd89c171.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GRILE-A-Benchmark-for-Grammar-Reasoning-and-Explanation-in-Romanian-LLMs"><a href="#GRILE-A-Benchmark-for-Grammar-Reasoning-and-Explanation-in-Romanian-LLMs" class="headerlink" title="GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian   LLMs"></a>GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian   LLMs</h2><p><strong>Authors:Adrian-Marius Dumitran, Alexandra-Mihaela Danila, Angela-Liliana Dumitran</strong></p>
<p>LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œç„¶è€Œå®ƒä»¬å¯¹ä½èµ„æºè¯­è¨€çš„æ•™å­¦ä»·å€¼ä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬æ¨å‡ºäº†GRILEï¼ˆç½—é©¬å°¼äºšè¯­æ³•æ¨ç†å’Œè¯­è¨€è§£é‡Šï¼‰ï¼Œè¿™æ˜¯ä»ç½—é©¬å°¼äºšé«˜é£é™©è€ƒè¯•ï¼ˆå›½å®¶è¯„ä¼°ã€å­¦å£«å­¦ä½ã€å¤§å­¦å…¥å­¦ï¼‰ä¸­æ”¶é›†çš„é¦–ä¸ªå¼€æ”¾çš„1151é“é€‰æ‹©é¢˜åŸºå‡†æµ‹è¯•ã€‚GRILEä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¢ç´¢æœ€å‰æ²¿çš„ä¸ƒç§å¤šè¯­è¨€ç½—é©¬å°¼äºšç‰¹å®šå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸¤ç§äº’è¡¥èƒ½åŠ›ï¼šï¼ˆiï¼‰é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼Œï¼ˆiiï¼‰äº§ç”Ÿè¯­è¨€ä¸Šå‡†ç¡®çš„è§£é‡Šã€‚è™½ç„¶Gemini 2.5 Proçš„å‡†ç¡®ç‡è¾¾åˆ°83%ï¼Œä½†å¤§å¤šæ•°å¼€æ”¾å¼æ¨¡å‹çš„å‡†ç¡®ç‡ä»åœ¨65%ä»¥ä¸‹ï¼Œæ ¹æ®ä¸“å®¶è¯„å®¡ï¼Œ48%çš„è§£é‡Šä¸­å«æœ‰äº‹å®æˆ–æ•™å­¦ä¸Šçš„é”™è¯¯ã€‚è¯¦ç»†çš„é”™è¯¯åˆ†ææŒ‡å‡ºäº†å½¢æ€å­¦ä»¥åŠåº”ç”¨æœ€æ–°DOOM3æ­£å­—æ³•è§„èŒƒçš„ç³»ç»Ÿæ€§å¼±ç‚¹ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œå…¬å…±ç½‘ç»œæ¼”ç¤ºéƒ½å·²å‘å¸ƒï¼Œä»¥åˆºæ¿€æœªæ¥çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ä½èµ„æºç¯å¢ƒä¸­å¯ä¿¡æ•™è‚²NLPçš„å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶å°†GRILEç¡®ç«‹ä¸ºå¯æ§è§£é‡Šç”Ÿæˆå’Œè¯„ä¼°çš„æ–°æµ‹è¯•å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14279v1">PDF</a> Accepted as long paper @RANLP2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¯¹äºä½èµ„æºè¯­è¨€çš„æ•™å­¦ä»·å€¼å°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶æ¨å‡ºäº†GRILEï¼ˆç½—é©¬å°¼äºšè¯­æ³•æ¨ç†ä¸è¯­è¨€è§£é‡ŠåŸºå‡†æµ‹è¯•ï¼‰ï¼ŒåŒ…å«ä»ç½—é©¬å°¼äºšé«˜é£é™©è€ƒè¯•ï¼ˆå›½å®¶è¯„ä¼°ã€é«˜ä¸­æ¯•ä¸šè€ƒè¯•ã€å¤§å­¦å…¥å­¦è€ƒè¯•ï¼‰ä¸­æ”¶é›†çš„1151é“é€‰æ‹©é¢˜ã€‚GRILEä½¿æˆ‘ä»¬èƒ½å¤Ÿæµ‹è¯•ä¸ƒç§æœ€å…ˆè¿›çš„è·¨è¯­ç§å’Œç½—é©¬å°¼äºšç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸¤ç§äº’è¡¥èƒ½åŠ›ï¼šä¸€æ˜¯é€‰æ‹©æ­£ç¡®ç­”æ¡ˆçš„èƒ½åŠ›ï¼ŒäºŒæ˜¯äº§ç”Ÿè¯­è¨€ä¸Šå‡†ç¡®è§£é‡Šçš„èƒ½åŠ›ã€‚è™½ç„¶Gemini 2.5 Proçš„å‡†ç¡®ç‡è¾¾åˆ°83%ï¼Œä½†å¤§å¤šæ•°å¼€æ”¾å¼æƒé‡æ¨¡å‹çš„å‡†ç¡®ç‡ä»åœ¨65%ä»¥ä¸‹ï¼Œæ ¹æ®ä¸“å®¶è¯„å®¡ï¼Œå…¶è§£é‡Šä¸­æœ‰48%å­˜åœ¨äº‹å®æˆ–æ•™å­¦ä¸Šçš„ç¼ºé™·ã€‚è¯¦ç»†çš„é”™è¯¯åˆ†ææŒ‡å‡ºäº†å½¢æ€å­¦ä»¥åŠéµå¾ªæœ€æ–°çš„DOOM3æ­£å­—æ³•è§„èŒƒæ–¹é¢çš„ç³»ç»Ÿæ€§å¼±ç‚¹ã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œå…¬å…±ç½‘ç»œæ¼”ç¤ºéƒ½å·²å‘å¸ƒï¼Œä»¥æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†ä½èµ„æºç¯å¢ƒä¸­å¯ä¿¡æ•™è‚²è‡ªç„¶è¯­è¨€å¤„ç†çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶å°†GRILEç¡®ç«‹ä¸ºæ–°çš„å¯æ§è§£é‡Šç”Ÿæˆä¸è¯„ä»·æµ‹è¯•å¹³å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä½èµ„æºè¯­è¨€æ•™å­¦ä¸­çš„ä»·å€¼å°šä¸æ¸…æ¥šã€‚</li>
<li>æ¨å‡ºäº†GRILEåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»ç½—é©¬å°¼äºšé«˜é£é™©è€ƒè¯•ä¸­æ”¶é›†çš„é€‰æ‹©é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>å¤§éƒ¨åˆ†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨GRILEä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå‡†ç¡®ç‡ä½ï¼Œä¸”å…¶è§£é‡Šå­˜åœ¨å¤§é‡ç¼ºé™·ã€‚</li>
<li>æ¨¡å‹åœ¨å½¢æ€å­¦å’Œæ­£å­—æ³•è§„èŒƒæ–¹é¢çš„ç³»ç»Ÿæ€§å¼±ç‚¹è¢«æŒ‡å‡ºã€‚</li>
<li>æ‰€æœ‰æ•°æ®ã€ä»£ç å’Œå…¬å…±ç½‘ç»œæ¼”ç¤ºéƒ½å·²å…¬å¼€ï¼Œä»¥ä¿ƒè¿›æœªæ¥ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†ä½èµ„æºç¯å¢ƒä¸­æ•™è‚²è‡ªç„¶è¯­è¨€å¤„ç†çš„æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81c09cf8f656a2eb7b9c941daa4f1c3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d8117f19f6cc427cd3520093b17462c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-389251d83667435cefc0a7b6ca3db930.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e829d3458f917c844330a3e77b0af516.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning"><a href="#LENS-Learning-to-Segment-Anything-with-Unified-Reinforced-Reasoning" class="headerlink" title="LENS: Learning to Segment Anything with Unified Reinforced Reasoning"></a>LENS: Learning to Segment Anything with Unified Reinforced Reasoning</h2><p><strong>Authors:Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang</strong></p>
<p>Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a>. </p>
<blockquote>
<p>æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²èƒ½å¤Ÿå®ç°ç²¾ç»†çš„è§†è§‰ç†è§£ï¼Œå¯¹äºäººæœºäº¤äº’å’Œæœºå™¨äººç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç›‘ç£å¾®è°ƒæ–¹æ³•é€šå¸¸åœ¨æµ‹è¯•æ—¶å¿½ç•¥äº†æ˜ç¡®çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬å¯¹æœªè§æç¤ºå’Œé¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LENSï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ã€‚æˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ï¼Œæ¶µç›–å¥å­ã€æ¡†å’Œåˆ†æ®µçº§åˆ«çš„çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç»†åŒ–æ©è†œè´¨é‡çš„åŒæ—¶ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„CoTç†ç”±ã€‚æˆ‘ä»¬ä½¿ç”¨å…¬å¼€çš„3äº¿å‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå³Qwen2.5-VL-3B-Instructï¼‰ï¼Œåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†æµ‹è¯•ä¸Šï¼ŒLENSçš„å¹³å‡å®Œå…¨äº¤é›†ï¼ˆcIoUï¼‰è¾¾åˆ°81.2%ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„å¾®è°ƒæ–¹æ³•GLaMMï¼Œæœ€é«˜æå‡äº†5.6%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„CoTæ¨ç†æ˜¯æ–‡æœ¬æç¤ºåˆ†å‰²çš„ç¨³å¥å…ˆéªŒï¼Œä¸ºå®ç°æ›´é€šç”¨çš„Segment Anythingæ¨¡å‹æä¾›äº†å®é™…è·¯å¾„ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hustvl/LENSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14153v1">PDF</a> Code is released at <a target="_blank" rel="noopener" href="https://github.com/hustvl/LENS">https://github.com/hustvl/LENS</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬å†…å®¹å…³äºé€šè¿‡æç¤ºè¿›è¡Œå›¾åƒåˆ†å‰²çš„æŠ€æœ¯ï¼Œå¼ºè°ƒäº†ç²¾ç»†è§†è§‰ç†è§£çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨äººæœºäº¤äº’å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸçš„åº”ç”¨ä»·å€¼ã€‚é’ˆå¯¹ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•å¿½ç•¥æµ‹è¯•æ—¶çš„æ˜ç¡®æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLENSçš„å¯æ‰©å±•å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ã€‚é€šè¿‡ä½¿ç”¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¥æ¶µç›–å¥å­ã€æ¡†å’Œåˆ†æ®µçº§åˆ«çš„çº¿ç´¢ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„CoTç†ç”±ï¼ŒåŒæ—¶æé«˜æ©è†œè´¨é‡ã€‚ä½¿ç”¨å…¬å¼€å¯ç”¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒLENSåœ¨RefCOCOã€RefCOCO+å’ŒRefCOCOgåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å¹³å‡å®Œå…¨äº¤å¹¶æ¯”ï¼ˆcIoUï¼‰ä¸º81.2%çš„æˆç»©ï¼Œä¼˜äºç²¾ç»†è°ƒå‚æ–¹æ³•GLaMMï¼Œæœ€é«˜æå‡äº†5.6%ã€‚è¿™è¡¨æ˜å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„CoTæ¨ç†ä¸ºæ–‡æœ¬æç¤ºçš„åˆ†å‰²æä¾›äº†ä¸€ä¸ªç¨³å¥çš„å…ˆéªŒï¼Œå¹¶ä¸ºæ›´é€šç”¨çš„åˆ†æ®µæ¨¡å‹æä¾›äº†å®é™…è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æç¤ºçš„å›¾åƒåˆ†å‰²æŠ€æœ¯å¯¹äºç²¾ç»†è§†è§‰ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶åœ¨äººæœºäº¤äº’å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸã€‚</li>
<li>ç°æœ‰ç›‘ç£å¾®è°ƒæ–¹æ³•å¿½ç•¥æµ‹è¯•æ—¶çš„æ˜ç¡®æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LENSæ¡†æ¶é€šè¿‡è”åˆä¼˜åŒ–æ¨ç†è¿‡ç¨‹å’Œåˆ†å‰²ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</li>
<li>LENSæ¡†æ¶é‡‡ç”¨ç»Ÿä¸€å¼ºåŒ–å­¦ä¹ å¥–åŠ±æ¥æ¶µç›–ä¸åŒçº§åˆ«çš„çº¿ç´¢ï¼Œæå‡æ¨¡å‹ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„CoTç†ç”±çš„èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨å…¬å¼€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒLENSåœ¨åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜å¹³å‡å®Œå…¨äº¤å¹¶æ¯”ï¼ˆcIoUï¼‰ã€‚</li>
<li>LENSç›¸è¾ƒäºç²¾ç»†è°ƒå‚æ–¹æ³•GLaMMæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½“ç°äº†å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„CoTæ¨ç†çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14153">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14c9c396f0f288498bb5a56bead6bf3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ed583e264da878a7f9a7d740cb51e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09f46af34f623f7a916628bb36649136.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e31edca7299d20e3515309d7541ec41b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dba8c9f444615125675a7152a246e9a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdbca333e616a340fe8f5d9f17d0baca.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="T-REX-Table-â€“-Refute-or-Entail-eXplainer"><a href="#T-REX-Table-â€“-Refute-or-Entail-eXplainer" class="headerlink" title="T-REX: Table â€“ Refute or Entail eXplainer"></a>T-REX: Table â€“ Refute or Entail eXplainer</h2><p><strong>Authors:Tim Luka Horstmann, Baptiste Geisenberger, Mehwish Alam</strong></p>
<p>Verifying textual claims against structured tabular data is a critical yet challenging task in Natural Language Processing with broad real-world impact. While recent advances in Large Language Models (LLMs) have enabled significant progress in table fact-checking, current solutions remain inaccessible to non-experts. We introduce T-REX (T-REX: Table â€“ Refute or Entail eXplainer), the first live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs. Designed for accuracy and transparency, T-REX empowers non-experts by providing access to advanced fact-checking technology. The system is openly available online. </p>
<blockquote>
<p>éªŒè¯æ–‡æœ¬å£°æ˜ä¸ç»“æ„åŒ–è¡¨æ ¼æ•°æ®æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªå…³é”®è€Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¯¹ç°å®ä¸–ç•Œå…·æœ‰å¹¿æ³›çš„å½±å“ã€‚è™½ç„¶æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä¸ºè¡¨æ ¼äº‹å®æ ¸æŸ¥å¸¦æ¥äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰è§£å†³æ–¹æ¡ˆå¯¹éä¸“ä¸šäººå£«æ¥è¯´ä»ç„¶éš¾ä»¥æ¥è§¦ã€‚æˆ‘ä»¬æ¨å‡ºT-REXï¼ˆT-REXï¼šè¡¨æ ¼â€”â€”åé©³æˆ–é˜è¿°è§£é‡Šå™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾é¦–ä¸ªé’ˆå¯¹å¤šåª’ä½“ã€å¤šè¯­è¨€è¡¨æ ¼å£°æ˜éªŒè¯çš„å®æ—¶äº’åŠ¨å·¥å…·ï¼Œå®ƒä½¿ç”¨æœ€æ–°çš„æŒ‡ä»¤è°ƒä¼˜æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚T-REXæ—¨åœ¨å‡†ç¡®æ€§å’Œé€æ˜åº¦ï¼Œä¸ºéä¸“ä¸šäººå£«æä¾›æ¥è§¦å…ˆè¿›äº‹å®æ ¸æŸ¥æŠ€æœ¯çš„èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿå·²åœ¨çº¿å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14055v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬éªŒè¯ä¸ç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„å¯¹æ¯”åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä½†å¯¹éä¸“ä¸šäººå£«æ¥è¯´å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†T-REXå·¥å…·ï¼Œè¿™æ˜¯é¦–ä¸ªä½¿ç”¨æœ€å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€ã€å¤šè¯­è¨€è¡¨æ ¼ä¸­çš„å£°æ˜éªŒè¯çš„å®æ—¶äº’åŠ¨å·¥å…·ã€‚å®ƒä¸ºéä¸“ä¸šäººå£«æä¾›äº†è®¿é—®å…ˆè¿›çš„äº‹å®æ ¸æŸ¥æŠ€æœ¯çš„èƒ½åŠ›ï¼Œå¹¶è®¾è®¡ç”¨äºå‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚ç³»ç»Ÿå·²åœ¨çº¿å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬éªŒè¯ä¸ç»“æ„åŒ–è¡¨æ ¼æ•°æ®çš„å¯¹æ¯”åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ˜¯é‡è¦çš„ä»»åŠ¡ã€‚</li>
<li>T-REXå·¥å…·æ˜¯ç¬¬ä¸€æ¬¾ç»“åˆäº†æœ€æ–°è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯çš„å®æ—¶äº’åŠ¨å·¥å…·ï¼Œä¸“é—¨ç”¨äºè¡¨æ ¼å£°æ˜éªŒè¯ã€‚</li>
<li>T-REXæ”¯æŒå¤šæ¨¡æ€å’Œå¤šè¯­è¨€è¡¨æ ¼çš„äº‹å®æ ¸æŸ¥ã€‚</li>
<li>è¯¥å·¥å…·åŸºäºå…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>T-REXä¸ºéä¸“ä¸šäººå£«æä¾›äº†è®¿é—®å…ˆè¿›äº‹å®æ ¸æŸ¥æŠ€æœ¯çš„æœºä¼šã€‚</li>
<li>è¯¥ç³»ç»Ÿè®¾è®¡æ³¨é‡å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-10cc43798e6eeb28ec0fefffcb8b6166.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c613312987bceff72dbe913d37a3e22e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f89c0885b713906a361ae6ec9b199acc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Input-Time-Scaling"><a href="#Input-Time-Scaling" class="headerlink" title="Input Time Scaling"></a>Input Time Scaling</h2><p><strong>Authors:Rapheal Huang, Weilong Guo</strong></p>
<p>Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data &amp; training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, â€œgarbage in, garbage outâ€. Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åœ¨åè®­ç»ƒé˜¶æ®µåœ¨å¤§é‡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆæ•°æ®å’Œè®­ç»ƒæ‰©å±•ï¼‰ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é—´ï¼ˆæ¨ç†æ—¶é—´æ‰©å±•ï¼‰è¿›è¡Œæ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰©å±•èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´æ‰©å±•ï¼Œä»¥è¡¥å……å…ˆå‰çš„æ‰©å±•æ–¹æ³•ï¼Œå°†èµ„æºæ”¾åœ¨æŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰ä¸Šã€‚åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ç»“åˆLLMçš„å…ƒçŸ¥è¯†ï¼Œé‡‡ç”¨ä¸åŒç­–ç•¥æ¥ä¼˜åŒ–è¾“å…¥ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†ä¸€ä¸ªæ–°ç°è±¡â€”â€”è®­ç»ƒæµ‹è¯•ååŒè®¾è®¡ã€‚æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­éƒ½åº”ç”¨æŸ¥è¯¢ç­–ç•¥ã€‚åªåœ¨è®­ç»ƒæˆ–æµ‹è¯•é˜¶æ®µåº”ç”¨ç­–ç•¥ä¼šä¸¥é‡é™ä½æ€§èƒ½ã€‚æˆ‘ä»¬æƒŠè®¶åœ°å‘ç°ï¼Œçœ‹ä¼¼æ•°æ®è´¨é‡ä¸é«˜çš„æ•°æ®é›†ä¹Ÿèƒ½è·å¾—é«˜æ€§èƒ½ã€‚å‘æŸ¥è¯¢ä¸­æ·»åŠ æ— å…³ä¿¡æ¯ï¼Œä»ç»è¿‡è½»å¾®è¿‡æ»¤çš„æ•°æ®é›†ä¸­éšæœºé€‰æ‹©ç¤ºä¾‹ï¼Œç”šè‡³å¯èƒ½è¡¨ç°æœ€ä½³ã€‚è¿™äº›å‘ç°ä¸å¹¿ä¸ºæ¥å—çš„å½’çº³åè§â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ç›¸çŸ›ç›¾ã€‚ä½¿ç”¨çœ‹ä¼¼é«˜è´¨é‡æ•°æ®æ•´ç†çš„æ•°æ®é›†ç”šè‡³å¯èƒ½é™åˆ¶æ€§èƒ½ä¸Šé™ã€‚æ­¤å¤–ï¼Œåœ¨ç±»ä¼¼è´¨é‡çš„æ›´å¤šæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ˆ15kå¯¹1kï¼‰è¡¨ç°æ›´å·®ï¼Œä¹Ÿéœ€è¦è°¨æ…æ£€æŸ¥ç®€å•çš„æ•°æ®é›†å¤§å°æ‰©å±•ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬çš„å‘ç°ä¸â€œå°‘å³æ˜¯å¤šâ€ç°è±¡ç›¸å»åˆã€‚å°‘é‡ç¤ºä¾‹è¶³ä»¥æ¿€å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚åœ¨Qwen2.5-32B-Instructè®­ç»ƒçš„æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒï¼Œæˆ‘ä»¬åœ¨AIME24ï¼ˆ76.7%ï¼‰å’ŒAIME25ï¼ˆ76.7%ï¼‰pass@1ä¸Šè¾¾åˆ°äº†32Bæ¨¡å‹ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚é€šè¿‡ä¸‰ä¸ªæ¨¡å‹çš„å¤šæ•°æŠ•ç¥¨ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å®ç°äº†AIME24ï¼ˆ76.7%ï¼‰å’ŒAIME25ï¼ˆ80%ï¼‰ã€‚ä»DeepSeek-R1-Distill-Qwen-32Bå¼€å§‹ï¼ŒAIME24çš„æœ€ä½³ç»“æœå°†è¾¾åˆ°86.7%ï¼ŒAIME25çš„ç»“æœå°†è¾¾åˆ°76.7%ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¼€æºæˆ‘ä»¬çš„æ•°æ®é›†ã€æ•°æ®ç®¡é“ã€è¯„ä¼°ç»“æœå’Œæ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13654v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°çš„ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œä»¥è¡¥å……å…ˆå‰çš„ç¼©æ”¾æ–¹æ³•ã€‚ç ”ç©¶ç»“åˆLLMçš„å…ƒçŸ¥è¯†å¯¹è¾“å…¥è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œå¹¶å‘ç°è®­ç»ƒä¸æµ‹è¯•ååŒè®¾è®¡çš„é‡è¦æ€§ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œçœ‹ä¼¼ä½è´¨é‡çš„æ•°æ®é›†ä¹Ÿèƒ½è·å¾—é«˜æ€§èƒ½ï¼Œæ·»åŠ ä¸æŸ¥è¯¢æ— å…³çš„ä¿¡æ¯ã€ä»æœ€å°‘è¿‡æ»¤çš„éšæœºé€‰æ‹©æ•°æ®ç”šè‡³å¯èƒ½è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶ä¹Ÿå‘ç°ç®€å•æ•°æ®é›†è§„æ¨¡çš„ç¼©æ”¾éœ€è°¨æ…è€ƒè™‘ã€‚ç ”ç©¶æˆæœå…¼å®¹â€œå°‘å³æ˜¯å¤šâ€ç°è±¡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºä¾¿äºå¤åˆ¶å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œç ”ç©¶å›¢é˜Ÿæ­£åœ¨å¼€æºæ•°æ®é›†ã€æ•°æ®ç®¡é“ã€è¯„ä¼°ç»“æœå’Œæ£€æŸ¥ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œä¾§é‡äºæŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰çš„èµ„æºåˆ†é…ï¼Œä»¥è¡¥å……ç°æœ‰çš„æ•°æ®è§„æ¨¡æ‰©å¤§å’Œæ¨ç†æ—¶é—´æ‰©å¤§æ–¹æ³•ã€‚</li>
<li>ç»“åˆLLMçš„å…ƒçŸ¥è¯†ï¼Œé€šè¿‡ä¸åŒçš„ç­–ç•¥å¯¹è¾“å…¥è¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
<li>å‘ç°è®­ç»ƒä¸æµ‹è¯•ååŒè®¾è®¡çš„é‡è¦æ€§ï¼Œéœ€è¦åœ¨ä¸¤è€…ä¸­éƒ½åº”ç”¨æŸ¥è¯¢ç­–ç•¥ï¼Œå¦åˆ™æ€§èƒ½ä¼šä¸¥é‡ä¸‹é™ã€‚</li>
<li>çªç ´å¹¿æ³›æŒæœ‰çš„å½’çº³åè§ï¼Œâ€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ï¼Œå³ä½¿æ•°æ®é›†çœ‹ä¼¼ä½è´¨é‡ï¼Œä¹Ÿå¯èƒ½è·å¾—é«˜æ€§èƒ½ã€‚</li>
<li>ç®€å•æ•°æ®é›†è§„æ¨¡çš„æ‰©å¤§å¹¶ä¸ä¸€å®šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ï¼Œéœ€è¦è°¨æ…è€ƒè™‘ã€‚</li>
<li>ç ”ç©¶æˆæœä¸â€œå°‘å³æ˜¯å¤šâ€ç°è±¡å…¼å®¹ï¼Œå°é›†ä¾‹å­è¶³ä»¥æ¿€å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f906f3c7b17a16258ef3b5c4e5e13af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d3a38472ce46ece768864f7587f2cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8407b7d1ddf9b5d3aa7a361427b909f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9048ed3662a42e86a1252400be874d4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3217e2975586c188dd7c1c691a55b077.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="STEM-Efficient-Relative-Capability-Evaluation-of-LLMs-through-Structured-Transition-Samples"><a href="#STEM-Efficient-Relative-Capability-Evaluation-of-LLMs-through-Structured-Transition-Samples" class="headerlink" title="STEM: Efficient Relative Capability Evaluation of LLMs through   Structured Transition Samples"></a>STEM: Efficient Relative Capability Evaluation of LLMs through   Structured Transition Samples</h2><p><strong>Authors:Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang</strong></p>
<p>Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \textbf{S}tructured \textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs. </p>
<blockquote>
<p>éšç€æ¨¡å‹èƒ½åŠ›çš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶æœ€è¿‘çš„æ¨¡å‹é€šå¸¸åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè·å¾—æ›´é«˜çš„åˆ†æ•°ï¼Œä½†è¿™äº›æ”¹è¿›å¹¶ä¸ä¸€è‡´åœ°åæ˜ å‡ºç°å®ä¸–ç•Œæ¨ç†èƒ½åŠ›çš„å¢å¼ºã€‚æ­¤å¤–ï¼Œå¯¹å…¬å…±åŸºå‡†æµ‹è¯•çš„è¿‡åº¦æ‹Ÿåˆä»¥åŠå®Œæ•´è¯„ä¼°çš„é«˜è®¡ç®—æˆæœ¬ï¼Œä½¿å¾—åœ¨æ¨¡å‹ä¹‹é—´åŒºåˆ†æœ‰æ„ä¹‰çš„å·®å¼‚æ—¢æ˜‚è´µåˆæ•ˆæœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„åŒ–è½¬æ¢è¯„ä¼°æ–¹æ³•ï¼ˆSTEMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§å’Œå¯è§£é‡Šçš„è¯„ä»·æ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆåœ°ä¼°è®¡LLMçš„ç›¸å¯¹èƒ½åŠ›ã€‚STEMé€šè¿‡åˆ†æç›¸åŒæ¶æ„ä½†å‚æ•°è§„æ¨¡ä¸åŒçš„LLMä¹‹é—´çš„ä¸€è‡´æ€§èƒ½è½¬å˜æ¥è¯†åˆ«é‡è¦è½¬æ¢æ ·æœ¬ï¼ˆSTSï¼‰ã€‚è¿™äº›æ ·æœ¬ä½¿STEMèƒ½å¤Ÿæœ‰æ•ˆåœ°ä¼°è®¡æœªçŸ¥æ¨¡å‹çš„èƒ½åŠ›ä½ç½®ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªå¤šæ ·ä¸”å…·æœ‰ä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šåº”ç”¨Qwen3æ¨¡å‹å®¶æ—æ¥æ„å»ºSTSæ± ï¼Œä»¥è¯„ä¼°å…¶é€šç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEMèƒ½å¤Ÿå¯é åœ°æ•æ‰æ€§èƒ½è¶‹åŠ¿ï¼Œä¸æ¨¡å‹èƒ½åŠ›çš„åœ°é¢çœŸå®æ’åç›¸ç¬¦ã€‚è¿™äº›å‘ç°çªæ˜¾äº†STEMä½œä¸ºä¸€ç§å®ç”¨ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œç”¨äºå¯¹LLMè¿›è¡Œç²¾ç»†ç²’åº¦çš„ã€ä¸å—æ¶æ„é™åˆ¶çš„è¯„ä»·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.12096v2">PDF</a> Submit to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æŒ‘æˆ˜åœ¨äºæ¨¡å‹èƒ½åŠ›è¿…é€Ÿæå‡ï¼Œæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„é«˜åˆ†æ•°å¹¶ä¸èƒ½å§‹ç»ˆåæ˜ å…¶åœ¨ç°å®ä¸–ç•Œçš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ç»“æ„åŒ–è½¬æ¢è¯„ä¼°æ–¹æ³•ï¼ˆSTEMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§ä¸”å¯è§£é‡Šçš„è¯„ä»·æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆä¼°è®¡LLMçš„ç›¸å¯¹èƒ½åŠ›ã€‚STEMé€šè¿‡åˆ†æåŒä¸€æ¶æ„ä½†å‚æ•°è§„æ¨¡ä¸åŒçš„LLMä¹‹é—´çš„æ€§èƒ½è¿‡æ¸¡ï¼Œç¡®å®šé‡è¦è½¬æ¢æ ·æœ¬ï¼ˆSTSï¼‰ï¼Œä»è€Œä¼°è®¡æœªçŸ¥æ¨¡å‹çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTEMå¯é åœ°æ•æ‰æ€§èƒ½è¶‹åŠ¿ï¼Œä¸æ¨¡å‹èƒ½åŠ›çš„çœŸå®æ’åç›¸ç¬¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯„ä¼°é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºæ¨¡å‹èƒ½åŠ›è¿…é€Ÿæå‡ã€‚</li>
<li>æ ‡å‡†åŸºå‡†æµ‹è¯•çš„é«˜åˆ†æ•°å¹¶ä¸æ€»èƒ½åæ˜ LLMåœ¨ç°å®ä¸–ç•Œçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>STEMæ˜¯ä¸€ç§è½»é‡çº§ã€å¯è§£é‡Šçš„è¯„ä»·æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆä¼°è®¡LLMçš„ç›¸å¯¹èƒ½åŠ›ã€‚</li>
<li>STEMé€šè¿‡åˆ†æLLMä¹‹é—´çš„æ€§èƒ½è¿‡æ¸¡æ¥ç¡®å®šé‡è¦è½¬æ¢æ ·æœ¬ï¼ˆSTSï¼‰ã€‚</li>
<li>STEMå¯ä»¥ä¼°è®¡æœªçŸ¥æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>STEMåœ¨Qwen3æ¨¡å‹å®¶æ—ä¸Šæ„å»ºSTSæ± ï¼Œæ¶‰åŠå…­ä¸ªå¤šæ ·ä¸”å…·æœ‰ä»£è¡¨æ€§çš„åŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.12096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f236e69e4f9282e46a432f4f76b7e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3207f0ef9aaf932c63dab9bc0e832c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85f909cc2539d6a8b9f62854ab54961d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22cd6d6231da09a3784b41c3d8c20713.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd915b1b1eab5081df93764e7dfc1e0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5440b93db6bd453efb4b62840a347ce.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Ovis2-5-Technical-Report"><a href="#Ovis2-5-Technical-Report" class="headerlink" title="Ovis2.5 Technical Report"></a>Ovis2.5 Technical Report</h2><p><strong>Authors:Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, Jiahe Li, Wen Li, Gui Hu, Yiliang Gu, Siran Yang, Jiamang Wang, Hailong Sun, Yibo Wang, Hui Sun, Jinlong Huang, Yuping He, Shengze Shi, Weihong Zhang, Guodong Zheng, Junpeng Jiang, Sensen Gao, Yi-Feng Wu, Sijia Chen, Yuhui Chen, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</strong></p>
<p>We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout â€“ crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection â€“ including self-checking and revision. This advanced capability is exposed as an optional â€œthinking modeâ€ at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the â€œsmall model, big performanceâ€ philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Ovis2.5ï¼Œå®ƒæ˜¯Ovis2çš„å‡çº§ç‰ˆï¼Œä¸“ä¸ºåŸç”Ÿåˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥å’Œå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†è€Œè®¾è®¡ã€‚Ovis2.5é›†æˆäº†ä¸€ä¸ªåŸç”Ÿåˆ†è¾¨ç‡è§†è§‰è½¬æ¢å™¨ï¼Œè¯¥è½¬æ¢å™¨ä»¥å›¾åƒçš„åŸå§‹å¯å˜åˆ†è¾¨ç‡å¤„ç†å›¾åƒï¼Œé¿å…äº†å›ºå®šåˆ†è¾¨ç‡åˆ†å—å¸¦æ¥çš„é™çº§é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™äº†ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å¸ƒå±€ï¼Œè¿™å¯¹äºå¤æ‚å›¾è¡¨ç­‰è§†è§‰å¯†é›†å†…å®¹è‡³å…³é‡è¦ã€‚ä¸ºäº†åŠ å¼ºæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹è¶…è¶Šçº¿æ€§æ€ç»´é“¾ï¼Œè¿›è¡Œåæ€ï¼ŒåŒ…æ‹¬è‡ªæˆ‘æ£€æŸ¥å’Œä¿®è®¢ã€‚è¿™ç§é«˜çº§åŠŸèƒ½åœ¨æ¨ç†æ—¶ä½œä¸ºå¯é€‰çš„â€œæ€ç»´æ¨¡å¼â€æš´éœ²å‡ºæ¥ï¼Œå…è®¸ç”¨æˆ·åœ¨æ²¡æœ‰å»¶è¿Ÿçš„æƒ…å†µä¸‹æé«˜å›°éš¾è¾“å…¥çš„å‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡å…¨é¢çš„äº”é˜¶æ®µè¯¾ç¨‹è¿›è¡ŒåŸ¹è®­ï¼Œé€æ­¥å»ºç«‹å…¶æŠ€èƒ½ã€‚è¿™ä¸ªè¿‡ç¨‹ä»åŸºæœ¬çš„è§†è§‰å’Œå¤šæ¨¡æ€é¢„è®­ç»ƒå¼€å§‹ï¼Œé€šè¿‡å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´è€Œå‘å±•ï¼Œæœ€ç»ˆä½¿ç”¨DPOå’ŒGRPOè¿›è¡Œå¯¹é½å’Œæ¨ç†å¢å¼ºã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ‰©å±•è¿™äº›å‡çº§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šæ¨¡æ€æ•°æ®æ‰“åŒ…å’Œæ··åˆå¹¶è¡Œæ€§ï¼Œä»è€Œå®ç°äº†ç«¯åˆ°ç«¯çš„æ˜¾è‘—åŠ é€Ÿã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸¤ä¸ªå¼€æºæ¨¡å‹ï¼šOvis2.5-9Bå’ŒOvis2.5-2Bã€‚åè€…ç»§ç»­ç§‰æ‰¿Ovis2çš„â€œå°æ¨¡å‹ã€å¤§æ€§èƒ½â€ç†å¿µï¼Œä½¿å…¶æˆä¸ºèµ„æºå—é™ã€è®¾å¤‡ç«¯çš„ç†æƒ³é€‰æ‹©ã€‚åœ¨OpenCompasså¤šæ¨¡æ€æ’è¡Œæ¦œä¸Šï¼ŒOvis2.5-9Bå¹³å‡å¾—åˆ†ä¸º78.3ï¼Œè¾ƒå…¶å‰èº«Ovis2-8Bæœ‰äº†æ˜¾è‘—æ”¹è¿›ï¼Œå¹¶åœ¨å¼€æ”¾å¼å°å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼›Ovis2.5-2Bå¾—åˆ†ä¸º73.9ï¼Œåœ¨å…¶è§„æ¨¡å†…å»ºç«‹äº†æœ€ä½³çŠ¶æ€ã€‚é™¤äº†æ€»ä½“å¾—åˆ†å¤–ï¼ŒOvis2.5åœ¨STEMåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†é¢†å…ˆçš„ç»“æœï¼Œåœ¨æ¥åœ°å’Œè§†é¢‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¤æ‚å›¾è¡¨åˆ†ææ–¹é¢è¾¾åˆ°äº†å…¶è§„æ¨¡çš„å¼€æºæœ€ä½³çŠ¶æ€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.11737v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æœ¬æ–‡ä»‹ç»äº†Ovis2.5çš„è®¾è®¡å’ŒåŠŸèƒ½ã€‚å®ƒæ˜¯ä¸€æ¬¾è®¾è®¡ç”¨äºæœ¬åœ°åˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥å’Œå¼ºå¤§è·¨æ¨¡æ€æ¨ç†çš„å·¥å…·çš„å‡çº§ç‰ˆã€‚Ovis2.5é‡‡ç”¨æœ¬åœ°åˆ†è¾¨ç‡è§†è§‰è½¬æ¢å™¨ï¼Œå¯åœ¨å›¾åƒçš„åŸç”Ÿå¯å˜åˆ†è¾¨ç‡ä¸‹è¿›è¡Œå¤„ç†ï¼Œé¿å…å›ºå®šåˆ†è¾¨ç‡åˆ†å—é€ æˆçš„ç”»è´¨æŸå¤±ï¼Œå¹¶ä¿ç•™ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å¸ƒå±€ï¼Œå¯¹äºå¤æ‚å›¾è¡¨ç­‰è§†è§‰å¯†é›†å†…å®¹è‡³å…³é‡è¦ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹ï¼Œæé«˜å…¶è¶…è¶Šçº¿æ€§æ€ç»´é“¾çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¿›è¡Œåæ€ï¼ŒåŒ…æ‹¬è‡ªæˆ‘æ£€æŸ¥å’Œä¿®è®¢ã€‚è¯¥æ¨¡å‹é€šè¿‡äº”ä¸ªé˜¶æ®µçš„ç»¼åˆè¯¾ç¨‹è¿›è¡ŒåŸ¹è®­ï¼Œé€æ­¥å»ºç«‹æŠ€èƒ½ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†è·¨æ¨¡æ€æ•°æ®æ‰“åŒ…å’Œæ··åˆå¹¶è¡ŒæŠ€æœ¯ï¼Œå®ç°äº†ç«¯åˆ°ç«¯çš„æ˜¾è‘—åŠ é€Ÿã€‚æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªå¼€æºç‰ˆæœ¬å‘å¸ƒï¼šOvis2.5-9Bå’ŒOvis2.5-2Bã€‚Ovis2.5åœ¨OpenCompassè·¨æ¨¡æ€æ’è¡Œæ¦œä¸Šçš„å¹³å‡å¾—åˆ†ä¸º78.3ï¼Œç›¸è¾ƒäºå…¶å‰èº«Ovis2-8Bæœ‰äº†æ˜¾è‘—çš„æå‡ï¼Œå¹¶åœ¨å…¬å¼€æºä»£ç çš„MLLMsä¸­è¾¾åˆ°äº†æ¬¡40Bå‚æ•°èŒƒå›´å†…çš„æœ€ä½³æ°´å¹³ï¼›Ovis2.5-2Bå¾—åˆ†ä¸º73.9ï¼Œåœ¨å…¶è§„æ¨¡å†…è¾¾åˆ°å¼€æºæœ€ä½³æ°´å¹³ã€‚æ­¤å¤–ï¼ŒOvis2.5åœ¨STEMåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†é¢†å…ˆçš„æˆæœï¼Œåœ¨æ¥åœ°å’Œè§†é¢‘ä»»åŠ¡ä¸Šå±•ç°äº†å¼ºå¤§çš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¤æ‚å›¾è¡¨åˆ†ææ–¹é¢å–å¾—äº†å…¬å¼€æºä»£ç çš„æœ€ä½³æˆç»©ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ovis2.5æ˜¯Ovis2çš„å‡çº§ç‰ˆï¼Œæ”¯æŒåŸç”Ÿåˆ†è¾¨ç‡è§†è§‰æ„ŸçŸ¥å’Œå¼ºå¤§çš„è·¨æ¨¡æ€æ¨ç†ã€‚</li>
<li>Ovis2.5é‡‡ç”¨åŸç”Ÿåˆ†è¾¨ç‡è§†è§‰è½¬æ¢å™¨ï¼Œå¤„ç†å›¾åƒæ—¶é¿å…å›ºå®šåˆ†è¾¨ç‡åˆ†å—ï¼Œä¿ç•™ç²¾ç»†ç»†èŠ‚å’Œå…¨å±€å¸ƒå±€ã€‚</li>
<li>æ¨¡å‹å…·å¤‡åæ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬è‡ªæˆ‘æ£€æŸ¥å’Œä¿®è®¢ï¼Œæä¾›ç”¨æˆ·åœ¨æ¨ç†æ—¶é—´é€‰æ‹©å¼€å¯â€œæ€è€ƒæ¨¡å¼â€ä»¥è·å–æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>æ¨¡å‹é€šè¿‡äº”ä¸ªé˜¶æ®µçš„ç»¼åˆè¯¾ç¨‹è¿›è¡ŒåŸ¹è®­ï¼ŒåŒ…æ‹¬åŸºç¡€è§†è§‰å’Œè·¨æ¨¡æ€é¢„è®­ç»ƒã€å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´ã€å¯¹é½å’Œæ¨ç†å¢å¼ºç­‰ã€‚</li>
<li>é€šè¿‡è·¨æ¨¡æ€æ•°æ®æ‰“åŒ…å’Œæ··åˆå¹¶è¡ŒæŠ€æœ¯ï¼Œå®ç°äº†æ¨¡å‹æ•ˆç‡çš„æå‡å’Œç«¯åˆ°ç«¯çš„åŠ é€Ÿã€‚</li>
<li>Ovis2.5æœ‰ä¸¤ä¸ªå¼€æºç‰ˆæœ¬ï¼šOvis2.5-9Bå’ŒOvis2.5-2Bï¼Œåˆ†åˆ«é€‚ç”¨äºä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.11737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d878db4ab62bbb2a770e756fbff75be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85ba831cc6386aff7392e72fc9defe33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eda7bb4aaea9fbd68ee9af5bbf7af400.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de7f3e024ee70fcbce51b4e1b72063fb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TFRank-Think-Free-Reasoning-Enables-Practical-Pointwise-LLM-Ranking"><a href="#TFRank-Think-Free-Reasoning-Enables-Practical-Pointwise-LLM-Ranking" class="headerlink" title="TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking"></a>TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking</h2><p><strong>Authors:Yongqi Fan, Xiaoyang Chen, Dezhi Ye, Jie Liu, Haijin Liang, Jin Ma, Ben He, Yingfei Sun, Tong Ruan</strong></p>
<p>Reasoning-intensive ranking models built on Large Language Models (LLMs) have made notable progress, but existing approaches often rely on large-scale LLMs and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational cost and latency that limit real-world use. To address this, we propose \textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale LLMs. To improve ranking performance, TFRank effectively integrates CoT data, fine-grained score supervision, and multi-task training. Furthermore, it achieves an efficient <code>\textbf&#123;T&#125;hink-\textbf&#123;F&#125;ree&quot; reasoning capability by employing a </code>think-mode switchâ€™â€™ and pointwise format constraints. Specifically, this allows the model to leverage explicit reasoning during training while delivering precise relevance scores for complex queries at inference without generating any reasoning chains. Experiments show that TFRank (e.g., 1.7B) achieves performance comparable to models with four times more parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on the BEIR benchmark. Further analysis shows that TFRank achieves an effective balance between performance and efficiency, providing a practical solution for integrating advanced reasoning into real-world systems. Our code and data are released in the repository: <a target="_blank" rel="noopener" href="https://github.com/JOHNNY-fans/TFRank">https://github.com/JOHNNY-fans/TFRank</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å¯†é›†å‹æ’åæ¨¡å‹å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤§è§„æ¨¡LLMå’Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜å’Œå»¶è¿Ÿï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°è§„æ¨¡LLMçš„é«˜æ•ˆç‚¹å¼æ¨ç†æ’åæ¨¡å‹TFRankã€‚ä¸ºäº†æé«˜æ’åæ€§èƒ½ï¼ŒTFRankæœ‰æ•ˆåœ°æ•´åˆäº†CoTæ•°æ®ã€ç²¾ç»†åˆ†æ•°ç›‘ç£å’Œå¤šä»»åŠ¡è®­ç»ƒã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡é‡‡ç”¨â€œæ€è€ƒæ¨¡å¼åˆ‡æ¢â€å’Œç‚¹æ ¼å¼çº¦æŸï¼Œå®ç°äº†é«˜æ•ˆçš„â€œæ— æ€è€ƒâ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™å…è®¸æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨æ˜ç¡®çš„æ¨ç†ï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ºå¤æ‚æŸ¥è¯¢æä¾›ç²¾ç¡®çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œè€Œæ— éœ€ç”Ÿæˆä»»ä½•æ¨ç†é“¾ã€‚å®éªŒè¡¨æ˜ï¼ŒTFRankï¼ˆä¾‹å¦‚1.7Bå‚æ•°ï¼‰åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸å‚æ•°å¤šå››å€çš„æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒTFRankåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æœ‰æ•ˆçš„å¹³è¡¡ï¼Œä¸ºå°†é«˜çº§æ¨ç†é›†æˆåˆ°ç°å®ç³»ç»Ÿä¸­æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²åœ¨ä»“åº“ä¸­å‘å¸ƒï¼š<a target="_blank" rel="noopener" href="https://github.com/JOHNNY-fans/TFRank%E3%80%82">https://github.com/JOHNNY-fans/TFRankã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.09539v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºç¡€ä¸Šæ„å»ºçš„æ¨ç†å¯†é›†å‹æ’åæ¨¡å‹å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§è§„æ¨¡LLMå’Œæ˜¾å¼çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜å’Œå»¶è¿Ÿï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°è§„æ¨¡LLMçš„é«˜æ•ˆç‚¹å¼æ¨ç†æ’åå™¨TFRankã€‚TFRanké€šè¿‡æ•´åˆCoTæ•°æ®ã€ç²¾ç»†åˆ†æ•°ç›‘ç£å’Œå¤šä»»åŠ¡è®­ç»ƒï¼Œæé«˜æ’åæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨â€œæ€è€ƒæ¨¡å¼åˆ‡æ¢â€å’Œç‚¹å¼æ ¼å¼çº¦æŸï¼Œå®ç°äº†â€œæ— æ€è€ƒâ€æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒTFRankï¼ˆå¦‚1.7Bå‚æ•°ï¼‰åœ¨BRIGHTåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸å‚æ•°å¤§å››å€çš„æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒTFRankåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼Œä¸ºå°†é«˜çº§æ¨ç†é›†æˆåˆ°ç°å®ç³»ç»Ÿä¸­æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ¨ç†å¯†é›†å‹æ’åæ¨¡å‹ä¾èµ–äºå¤§è§„æ¨¡LLMå’Œæ˜¾å¼CoTæ¨ç†ï¼Œå¯¼è‡´é«˜è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚</li>
<li>TFRankæ˜¯ä¸€ç§åŸºäºå°è§„æ¨¡LLMçš„ç‚¹å¼æ¨ç†æ’åå™¨ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TFRanké€šè¿‡æ•´åˆCoTæ•°æ®ã€ç²¾ç»†åˆ†æ•°ç›‘ç£å’Œå¤šä»»åŠ¡è®­ç»ƒæ¥æé«˜æ’åæ€§èƒ½ã€‚</li>
<li>TFRankå®ç°â€œæ— æ€è€ƒâ€æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡â€œæ€è€ƒæ¨¡å¼åˆ‡æ¢â€å’Œç‚¹å¼æ ¼å¼çº¦æŸã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒTFRankåœ¨æ€§èƒ½ä¸Šä¸è¾ƒå¤§çš„æ¨¡å‹ç›¸å½“ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç«äº‰åŠ›ã€‚</li>
<li>TFRankåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.09539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e423eb68c6df71ce51a4511b84a5f1c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0170cc77d2dbf33a7ffcafe1d7e398c9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8230866c3c2131249a41ad5b3b9eb998.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b546969c8f726dffa7bc1ccf55054d3f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e22dd0c2da4da40a8cff1b3d13f5e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-861d349d6932281a0758772fa57321ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe0047398983aa4149762a427575013c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-acfe59fc074f1d991bc01715ff1870c2.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Interpretable-Reward-Model-via-Sparse-Autoencoder"><a href="#Interpretable-Reward-Model-via-Sparse-Autoencoder" class="headerlink" title="Interpretable Reward Model via Sparse Autoencoder"></a>Interpretable Reward Model via Sparse Autoencoder</h2><p><strong>Authors:Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang</strong></p>
<p>Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm">https://github.com/schrieffer-z/sarm</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä½¿LLMè¡Œä¸ºä¸äººç±»ç¤¾ä¼šä»·å€¼ç›¸å»åˆï¼Œä»è€Œä½¿RMçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹äºæœ‰æ•ˆå¯¹é½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹äºå¥–åŠ±åˆ†é…èƒŒåçš„æ¨ç†æä¾›æœ‰é™çš„è§è§£ï¼Œå¹¶ä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚è™½ç„¶æœ€è¿‘çš„å¤šç»´RMæ—¨åœ¨æé«˜å¯è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æä¾›ç‰¹å¾çº§åˆ«çš„å½’å±ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„æ³¨é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨€ç–è‡ªç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œå°†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰é›†æˆåˆ°å¥–åŠ±æ¨¡å‹ä¸­ã€‚SARMå°†åŸºäºLLMçš„RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–å’Œå•è¯­ä¹‰ç‰¹å¾ç©ºé—´ï¼Œå…¶ä¸­æ ‡é‡å¤´èšåˆç‰¹å¾æ¿€æ´»ä»¥äº§ç”Ÿé€æ˜ä¸”æ¦‚å¿µä¸Šæœ‰æ„ä¹‰çš„å¥–åŠ±åˆ†æ•°ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSARMä¿ƒè¿›äº†å¥–åŠ±åˆ†é…çš„ç‰¹å¾çº§åˆ«å½’å±ï¼Œå…è®¸åŠ¨æ€è°ƒæ•´åå¥½å˜åŒ–ï¼Œä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç›¸æ¯”å®ç°äº†ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/schrieffer-z/sarm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/schrieffer-z/sarmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08746v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºäººç±»åå¥½çš„ä»£ç†ï¼Œä½¿LLMè¡Œä¸ºä¸äººçš„ä»·å€¼è§‚ä¿æŒä¸€è‡´ï¼Œå› æ­¤RMçš„å‡†ç¡®æ€§ã€å¯é æ€§å’Œå¯è§£é‡Šæ€§å¯¹äºæœ‰æ•ˆçš„å¯¹é½è‡³å…³é‡è¦ã€‚é’ˆå¯¹ä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ã€å¯¹äººç±»åå¥½èƒŒåçš„æ¨ç†ç†è§£æœ‰é™ä»¥åŠåœ¨ç”¨æˆ·åå¥½å˜åŒ–æ—¶çš„çµæ´»æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ç»“åˆé¢„è®­ç»ƒç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰çš„å¥–åŠ±æ¨¡å‹ï¼ˆSARMï¼‰ã€‚SARMå°†LLM-based RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–ä¸”è¯­ä¹‰å•ä¸€çš„ç‰¹æ€§ç©ºé—´ï¼Œä»ä¸­äº§ç”Ÿæ¸…æ™°ä¸”å…·æœ‰æ¦‚å¿µæ„ä¹‰çš„å¥–åŠ±åˆ†æ•°ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSARMä¿ƒè¿›äº†å¥–åŠ±åˆ†é…çš„ç›´æ¥ç‰¹å¾çº§å½’å› ã€å…è®¸åŠ¨æ€è°ƒæ•´åå¥½å˜åŒ–ï¼Œå¹¶å®ç°äº†ä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ç›¸æ¯”æ›´ä¼˜è¶Šçš„å¯¹é½æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå¹¿æ³›åº”ç”¨ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰åˆ©ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>ä¼ ç»ŸRMç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå¯¹ç”¨æˆ·åå¥½èƒŒåçš„æ¨ç†ç†è§£æœ‰é™ï¼Œä¸”å¯¹ç”¨æˆ·åå¥½å˜åŒ–ä¸å¤Ÿçµæ´»ã€‚</li>
<li>æ–°å‹çš„å¤šç»´åº¦RMsè™½ç„¶è¿½æ±‚æ›´å¥½çš„å¯è§£é‡Šæ€§ï¼Œä½†å¾€å¾€æ— æ³•æä¾›ç‰¹å¾çº§åˆ«çš„å½’å±ä¿¡æ¯ï¼Œå¹¶ä¸”éœ€è¦æ˜‚è´µçš„æ³¨é‡Šæˆæœ¬ã€‚</li>
<li>SARMç»“åˆäº†é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œå°†RMçš„éšè—æ¿€æ´»æ˜ å°„åˆ°å¯è§£é‡Šã€ç¨€ç–å’Œå•ä¸€çš„ç‰¹æ€§ç©ºé—´ã€‚</li>
<li>SARMèƒ½å¤Ÿå®ç°å¥–åŠ±åˆ†é…çš„ç›´æ¥ç‰¹å¾çº§å½’å› ï¼ŒåŠ¨æ€é€‚åº”åå¥½å˜åŒ–ï¼Œå¹¶åœ¨å¯¹é½æ€§èƒ½ä¸Šè¶…è¶Šä¼ ç»ŸRMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3afe35bcae7acf75065e50ccc9d050c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e38c8e6b12330c78ec551a169893282.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-926930e1ed1f7c34ee897abec757b01f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-491c62cf4888b77b5071074321f6bb81.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-in-Vision-A-Survey"><a href="#Reinforcement-Learning-in-Vision-A-Survey" class="headerlink" title="Reinforcement Learning in Vision: A Survey"></a>Reinforcement Learning in Vision: A Survey</h2><p><strong>Authors:Weijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu, Hong Zhou, Mike Zheng Shou</strong></p>
<p>Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: <a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>. </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸è§†è§‰æ™ºèƒ½äº¤å‰é¢†åŸŸçš„è¿›å±•ï¼Œä½¿å¾—æ™ºèƒ½ä½“ä¸ä»…èƒ½å¤Ÿæ„ŸçŸ¥å¤æ‚çš„è§†è§‰åœºæ™¯ï¼Œè¿˜èƒ½åœ¨å…¶ä¸­è¿›è¡Œæ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚è¿™ç¯‡ç»¼è¿°å¯¹è¯¥é¢†åŸŸè¿›è¡Œäº†æ‰¹åˆ¤æ€§å’Œæœ€æ–°çš„ç»¼åˆã€‚æˆ‘ä»¬é¦–å…ˆæ­£å¼æå‡ºè§†è§‰å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå¹¶è¿½æº¯ç­–ç•¥ä¼˜åŒ–ç­–ç•¥ä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±èŒƒå¼ï¼Œä»è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–åˆ°ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„æ¼”å˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¶…è¿‡200ç¯‡å…·æœ‰ä»£è¡¨æ€§çš„ä½œå“æ•´ç†ä¸ºå››ä¸ªä¸»é¢˜æ”¯æŸ±ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜æ”¯æŸ±ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç®—æ³•è®¾è®¡ã€å¥–åŠ±å·¥ç¨‹ã€åŸºå‡†è¿›åº¦ï¼Œå¹¶æ€»ç»“äº†è¶‹åŠ¿ï¼Œå¦‚è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ã€‚æœ€åï¼Œæˆ‘ä»¬å›é¡¾äº†åŒ…æ‹¬é›†åˆçº§ä¿çœŸåº¦ã€æ ·æœ¬çº§åå¥½å’ŒçŠ¶æ€çº§ç¨³å®šæ€§åœ¨å†…çš„è¯„ä¼°åè®®ï¼Œå¹¶ç¡®å®šäº†å¼€æ”¾æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ·æœ¬æ•ˆç‡ã€æ¨å¹¿å’Œå®‰å…¨éƒ¨ç½²ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›å¿«é€Ÿæ‰©å±•çš„è§†è§‰å¼ºåŒ–å­¦ä¹ æ™¯è§‚çš„è¿è´¯åœ°å›¾ï¼Œå¹¶çªå‡ºæœªæ¥æŸ¥è¯¢çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚èµ„æºå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning">https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.08189v2">PDF</a> 22 pages</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰æ™ºèƒ½çš„äº¤å‰èåˆä½¿å¾—æ™ºèƒ½ä½“ä¸ä»…èƒ½æ„ŸçŸ¥å¤æ‚çš„è§†è§‰åœºæ™¯ï¼Œè¿˜èƒ½è¿›è¡Œæ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨ã€‚æœ¬æ–‡ç»¼è¿°äº†è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œä»‹ç»äº†è§†è§‰å¼ºåŒ–å­¦ä¹ çš„é—®é¢˜å½¢å¼åŒ–å®šä¹‰ï¼Œå›é¡¾äº†ä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±èŒƒå¼çš„ç­–ç•¥ä¼˜åŒ–ç­–ç•¥æ¼”å˜ï¼Œä»¥åŠè¶…è¿‡200é¡¹ä»£è¡¨æ€§å·¥ä½œçš„å››ä¸ªä¸»é¢˜æ”¯æŸ±ï¼šå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ã€‚æœ¬æ–‡è¿˜å®¡æŸ¥äº†è¯„ä¼°åè®®å¹¶æŒ‡å‡ºäº†å¼€æ”¾æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ·æœ¬æ•ˆç‡ã€é€šç”¨æ€§å’Œå®‰å…¨éƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸è§†è§‰æ™ºèƒ½çš„èåˆä½¿æ™ºèƒ½ä½“å…·å¤‡æ„ŸçŸ¥ã€æ¨ç†ã€ç”Ÿæˆå’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚</li>
<li>è§†è§‰å¼ºåŒ–å­¦ä¹ çš„é—®é¢˜å½¢å¼åŒ–å®šä¹‰åŠå…¶åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€è§†è§‰ç”Ÿæˆã€ç»Ÿä¸€æ¨¡å‹æ¡†æ¶å’Œè§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹ç­‰æ–¹é¢çš„åº”ç”¨å¾—åˆ°è¯¦ç»†ä»‹ç»ã€‚</li>
<li>ç­–ç•¥ä¼˜åŒ–ç­–ç•¥ä»RLHFåˆ°å¯éªŒè¯å¥–åŠ±èŒƒå¼çš„æ¼”å˜è¢«å›é¡¾ã€‚</li>
<li>æ–‡ä¸­æåˆ°äº†å¤šç§ç®—æ³•è®¾è®¡è¶‹åŠ¿ï¼Œå¦‚è¯¾ç¨‹é©±åŠ¨è®­ç»ƒã€åå¥½å¯¹é½æ‰©æ•£å’Œç»Ÿä¸€å¥–åŠ±å»ºæ¨¡ã€‚</li>
<li>ç»¼è¿°äº†åŒ…æ‹¬é›†åˆçº§ä¿çœŸåº¦ã€æ ·æœ¬çº§åå¥½å’ŒçŠ¶æ€çº§ç¨³å®šæ€§åœ¨å†…çš„è¯„ä¼°åè®®ã€‚</li>
<li>æŒ‡å‡ºæ ·æœ¬æ•ˆç‡ã€é€šç”¨æ€§å’Œå®‰å…¨éƒ¨ç½²ç­‰å¼€æ”¾æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.08189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bbe3a630483f68d5dd9b93050bd2f0aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0bcd259c33592786111b52901dd13f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afd40acd9948d5a52371a5e494c50ca8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95f89a4e2c0b25c1fade35b3973523da.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e6699aee2862070236411b972d3c41f.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-22/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-063dd8fa5c39abd6d572370d41d89a25.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  Quantization Meets dLLMs A Systematic Study of Post-training   Quantization for Diffusion LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-22/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4dadaeb1284a27cfb99f761cd4d22539.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-22  D^3-Talker Dual-Branch Decoupled Deformation Fields for Few-Shot 3D   Talking Head Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
